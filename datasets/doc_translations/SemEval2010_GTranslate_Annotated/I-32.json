{
    "id": "I-32",
    "original_text": "An Adversarial Environment Model for Bounded Rational Agents in Zero-Sum Interactions Inon Zuckerman1 , Sarit Kraus1 , Jeffrey S. Rosenschein2 , Gal Kaminka1 1 Department of Computer Science 2 The School of Engineering Bar-Ilan University and Computer Science Ramat-Gan, Israel Hebrew University, Jerusalem, Israel {zukermi,sarit,galk}@cs.biu.ac.il jeff@cs.huji.ac.il ABSTRACT Multiagent environments are often not cooperative nor collaborative; in many cases, agents have conflicting interests, leading to adversarial interactions. This paper presents a formal Adversarial Environment model for bounded rational agents operating in a zero-sum environment. In such environments, attempts to use classical utility-based search methods can raise a variety of difficulties (e.g., implicitly modeling the opponent as an omniscient utility maximizer, rather than leveraging a more nuanced, explicit opponent model). We define an Adversarial Environment by describing the mental states of an agent in such an environment. We then present behavioral axioms that are intended to serve as design principles for building such adversarial agents. We explore the application of our approach by analyzing log files of completed Connect-Four games, and present an empirical analysis of the axioms appropriateness. Categories and Subject Descriptors I.2.11 [Artificial Intelligence]: Distributed Artificial Intelligence-Intelligent agents,Multiagent Systems; I.2.4 [Artificial Intelligence]: Knowledge Representation Formalisms and Methods -Modal logic General Terms Design, Theory 1. INTRODUCTION Early research in multiagent systems (MAS) considered cooperative groups of agents; because individual agents had limited resources, or limited access to information (e.g., limited processing power, limited sensor coverage), they worked together by design to solve problems that individually they could not solve, or at least could not solve as efficiently. MAS research, however, soon began to consider interacting agents with individuated interests, as representatives of different humans or organizations with non-identical interests. When interactions are guided by diverse interests, participants may have to overcome disagreements, uncooperative interactions, and even intentional attempts to damage one another. When these types of interactions occur, environments require appropriate behavior from the agents situated in them. We call these environments Adversarial Environments, and call the clashing agents Adversaries. Models of cooperation and teamwork have been extensively explored in MAS through the axiomatization of mental states (e.g., [8, 4, 5]). However, none of this research dealt with adversarial domains and their implications for agent behavior. Our paper addresses this issue by providing a formal, axiomatized mental state model for a subset of adversarial domains, namely simple zero-sum adversarial environments. Simple zero-sum encounters exist of course in various twoplayer games (e.g., Chess, Checkers), but they also exist in n-player games (e.g., Risk, Diplomacy), auctions for a single good, and elsewhere. In these latter environments especially, using a utility-based adversarial search (such as the Min-Max algorithm) does not always provide an adequate solution; the payoff function might be quite complex or difficult to quantify, and there are natural computational limitations on bounded rational agents. In addition, traditional search methods (like Min-Max) do not make use of a model of the opponent, which has proven to be a valuable addition to adversarial planning [9, 3, 11]. In this paper, we develop a formal, axiomatized model for bounded rational agents that are situated in a zero-sum adversarial environment. The model uses different modality operators, and its main foundations are the SharedPlans [4] model for collaborative behavior. We explore environment properties and the mental states of agents to derive behavioral axioms; these behavioral axioms constitute a formal model that serves as a specification and design guideline for agent design in such settings. We then investigate the behavior of our model empirically using the Connect-Four board game. We show that this game conforms to our environment definition, and analyze players behavior using a large set of completed match log 550 978-81-904262-7-5 (RPS) c 2007 IFAAMAS files. In addition, we use the results presented in [9] to discuss the importance of opponent modeling in our ConnectFour adversarial domain. The paper proceeds as follows. Section 2 presents the models formalization. Section 3 presents the empirical analysis and its results. We discuss related work in Section 4, and conclude and present future directions in Section 5. 2. ADVERSARIAL ENVIRONMENTS The adversarial environment model (denoted as AE) is intended to guide the design of agents by providing a specification of the capabilities and mental attitudes of an agent in an adversarial environment. We focus here on specific types of adversarial environments, specified as follows: 1. Zero-Sum Interactions: positive and negative utilities of all agents sum to zero; 2. Simple AEs: all agents in the environment are adversarial agents; 3. Bilateral AEs: AEs with exactly two agents; 4. Multilateral AEs: AEs of three or more agents. We will work on both bilateral and multilateral instantiations of zero-sum and simple environments. In particular, our adversarial environment model will deal with interactions that consist of N agents (N ≥ 2), where all agents are adversaries, and only one agent can succeed. Examples of such environments range from board games (e.g., Chess, Connect-Four, and Diplomacy) to certain economic environments (e.g., N-bidder auctions over a single good). 2.1 Model Overview Our approach is to formalize the mental attitudes and behaviors of a single adversarial agent; we consider how a single agent perceives the AE. The following list specifies the conditions and mental states of an agent in a simple, zero-sum AE: 1. The agent has an individual intention that its own goal will be completed; 2. The agent has an individual belief that it and its adversaries are pursuing full conflicting goals (defined below)there can be only one winner; 3. The agent has an individual belief that each adversary has an intention to complete its own full conflicting goal; 4. The agent has an individual belief in the (partial) profile of its adversaries. Item 3 is required, since it might be the case that some agent has a full conflicting goal, and is currently considering adopting the intention to complete it, but is, as of yet, not committed to achieving it. This might occur because the agent has not yet deliberated about the effects that adopting that intention might have on the other intentions it is currently holding. In such cases, it might not consider itself to even be in an adversarial environment. Item 4 states that the agent should hold some belief about the profiles of its adversaries. The profile represents all the knowledge the agent has about its adversary: its weaknesses, strategic capabilities, goals, intentions, trustworthiness, and more. It can be given explicitly or can be learned from observations of past encounters. 2.2 Model Definitions for Mental States We use Grosz and Krauss definitions of the modal operators, predicates, and meta-predicates, as defined in their SharedPlan formalization [4]. We recall here some of the predicates and operators that are used in that formalization: Int.To(Ai, α, Tn, Tα, C) represents Ais intentions at time Tn to do an action α at time Tα in the context of C. Int.Th(Ai, prop, Tn, Tprop, C) represents Ais intentions at time Tn that a certain proposition prop holds at time Tprop in the context of C. The potential intention operators, Pot.Int.To(...) and Pot.Int.Th(...), are used to represent the mental state when an agent considers adopting an intention, but has not deliberated about the interaction of the other intentions it holds. The operator Bel(Ai, f, Tf ) represents agent Ai believing in the statement expressed in formula f, at time Tf . MB(A, f, Tf ) represents mutual belief for a group of agents A. A snapshot of the system finds our environment to be in some state e ∈ E of environmental variable states, and each adversary in any LAi ∈ L of possible local states. At any given time step, the system will be in some world w of the set of all possible worlds w ∈ W, where w = E×LA1 ×LA2 × ...LAn , and n is the number of adversaries. For example, in a Texas Holdem poker game, an agents local state might be its own set of cards (which is unknown to its adversary) while the environment will consist of the betting pot and the community cards (which are visible to both players). A utility function under this formalization is defined as a mapping from a possible world w ∈ W to an element in , which expresses the desirability of the world, from a single agent perspective. We usually normalize the range to [0,1], where 0 represents the least desirable possible world, and 1 is the most desirable world. The implementation of the utility function is dependent on the domain in question. The following list specifies new predicates, functions, variables, and constants used in conjunction with the original definitions for the adversarial environment formalization: 1. φ is a null action (the agent does not do anything). 2. GAi is the set of agent Ais goals. Each goal is a set of predicates whose satisfaction makes the goal complete (we use G∗ Ai ∈ GAi to represent an arbitrary goal of agent Ai). 3. gAi is the set of agent Ais subgoals. Subgoals are predicates whose satisfaction represents an important milestone toward achievement of the full goal. gG∗ Ai ⊆ gAi is the set of subgoals that are important to the completion of goal G∗ Ai (we will use g∗ G∗ Ai ∈ gG∗ Ai to represent an arbitrary subgoal). 4. P Aj Ai is the profile object agent Ai holds about agent Aj. 5. CA is a general set of actions for all agents in A which are derived from the environments constraints. CAi ⊆ CA is the set of agent Ais possible actions. 6. Do(Ai, α, Tα, w) holds when Ai performs action α over time interval Tα in world w. 7. Achieve(G∗ Ai , α, w) is true when goal G∗ Ai is achieved following the completion of action α in world w ∈ W, where α ∈ CAi . 8. Profile(Ai, PAi Ai ) is true when agent Ai holds an object profile for agent Aj. Definition 1. Full conflict (FulConf ) describes a zerosum interaction where only a single goal of the goals in conflict can be completed. FulConf(G∗ Ai , G∗ Aj ) ⇒ (∃α ∈ CAi , ∀w, β ∈ CAj ) (Achieve(G∗ Ai , α, w) ⇒ ¬Achieve(G∗ Aj , β, w)) ∨ (∃β ∈ CAj , ∀w, α ∈ CAi )(Achieve(G∗ Aj , β, w) ⇒ ¬Achieve(G∗ Ai , α, w)) Definition 2. Adversarial Knowledge (AdvKnow) is a function returning a value which represents the amount of The Sixth Intl. Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) 551 knowledge agent Ai has on the profile of agent Aj, at time Tn. The higher the value, the more knowledge agent Ai has. AdvKnow : P Aj Ai × Tn → Definition 3. Eval - This evaluation function returns an estimated expected utility value for an agent in A, after completing an action from CA in some world state w. Eval : A × CA × w → Definition 4. TrH - (Threshold) is a numerical constant in the [0,1] range that represents an evaluation function (Eval) threshold value. An action that yields an estimated utility evaluation above the TrH is regarded as a highly beneficial action. The Eval value is an estimation and not the real utility function, which is usually unknown. Using the real utility value for a rational agent would easily yield the best outcome for that agent. However, agents usually do not have the real utility functions, but rather a heuristic estimate of it. There are two important properties that should hold for the evaluation function: Property 1. The evaluation function should state that the most desirable world state is one in which the goal is achieved. Therefore, after the goal has been satisfied, there can be no future action that can put the agent in a world state with higher Eval value. (∀Ai, G∗ Ai , α, β ∈ CAi , w ∈ W) Achieve(G∗ Ai , α, w) ⇒ Eval(Ai, α, w) ≥ Eval(Ai, β, w) Property 2. The evaluation function should project an action that causes a completion of a goal or a subgoal to a value which is greater than TrH (a highly beneficial action). (∀Ai, G∗ Ai ∈ GAi , α ∈ CAi , w ∈ W, g∗ GAi ∈ gGAi ) Achieve(G∗ Ai , α, w) ∨ Achieve(g∗ GAi , α, w) ⇒ Eval(Ai, α, w) ≥ TrH. Definition 5. SetAction We define a set action (SetAction) as a set of action operations (either complex or basic actions) from some action sets CAi and CAj which, according to agent Ais belief, are attached together by a temporal and consequential relationship, forming a chain of events (action, and its following consequent action). (∀α1 , . . . , αu ∈ CAi , β1 , . . . , βv ∈ CAj , w ∈ W) SetAction(α1 , . . . , αu , β1 , . . . , βv , w) ⇒ ((Do(Ai, α1 , Tα1 , w) ⇒ Do(Aj, β1 , Tβ1 , w)) ⇒ Do(Ai, α2 , Tα2 , w) ⇒ . . . ⇒ Do(Ai, αu , Tαu , w)) The consequential relation might exist due to various environmental constraints (when one action forces the adversary to respond with a specific action) or due to the agents knowledge about the profile of its adversary. Property 3. As the knowledge we have about our adversary increases we will have additional beliefs about its behavior in different situations which in turn creates new set actions. Formally, if our AdvKnow at time Tn+1 is greater than AdvKnow at time Tn, then every SetAction known at time Tn is also known at time Tn+1. AdvKnow(P Aj Ai , Tn+1) > AdvKnow(P Aj Ai , Tn) ⇒ (∀α1 , . . . , αu ∈ CAi , β1 , . . . , βv ∈ CAj ) Bel(Aag, SetAction(α1 , . . . , αu , β1 , . . . , βv ), Tn) ⇒ Bel(Aag, SetAction(α1 , . . . , αu , β1 , . . . , βv ), Tn+1) 2.3 The Environment Formulation The following axioms provide the formal definition for a simple, zero-sum Adversarial Environment (AE). Satisfaction of these axioms means that the agent is situated in such an environment. It provides specifications for agent Aag to interact with its set of adversaries A with respect to goals G∗ Aag and G∗ A at time TCo at some world state w. AE(Aag, A, G∗ Aag , A1, . . . , Ak, G∗ A1 , . . . , G∗ Ak , Tn, w) 1. Aag has an Int.Th his goal would be completed: (∃α ∈ CAag , Tα) Int.Th(Aag, Achieve(G∗ Aag , α), Tn, Tα, AE) 2. Aag believes that it and each of its adversaries Ao are pursuing full conflicting goals: (∀Ao ∈ {A1, . . . , Ak}) Bel(Aag, FulConf(G∗ Aag , G∗ Ao ), Tn) 3. Aag believes that each of his adversaries in Ao has the Int.Th his conflicting goal G∗ Aoi will be completed: (∀Ao ∈ {A1, . . . , Ak})(∃β ∈ CAo , Tβ) Bel(Aag, Int.Th(Ao, Achieve(G∗ Ao , β), TCo, Tβ, AE), Tn) 4. Aag has beliefs about the (partial) profiles of its adversaries (∀Ao ∈ {A1, . . . , Ak}) (∃PAo Aag ∈ PAag )Bel(Aag, Profile(Ao, PAo Aag ), Tn) To build an agent that will be able to operate successfully within such an AE, we must specify behavioral guidelines for its interactions. Using a naive Eval maximization strategy to a certain search depth will not always yield satisfactory results for several reasons: (1) the search horizon problem when searching for a fixed depth; (2) the strong assumption of an optimally rational, unbounded resources adversary; (3) using an estimated evaluation function which will not give optimal results in all world states, and can be exploited [9]. The following axioms specify the behavioral principles that can be used to differentiate between successful and less successful agents in the above Adversarial Environment. Those axioms should be used as specification principles when designing and implementing agents that should be able to perform well in such Adversarial Environments. The behavioral axioms represent situations in which the agent will adopt potential intentions to (Pot.Int.To(...)) perform an action, which will typically require some means-end reasoning to select a possible course of action. This reasoning will lead to the adoption of an Int.To(...) (see [4]). A1. Goal Achieving Axiom. The first axiom is the simplest case; when the agent Aag believes that it is one action (α) away from achieving his conflicting goal G∗ Aag , it should adopt the potential intention to do α and complete its goal. (∀Aag, α ∈ CAag , Tn, Tα, w ∈ W) (Bel(Aag, Do(Aag, α, Tα, w) ⇒ Achieve(G∗ Aag , α, w)) ⇒ Pot.Int.To(Aag, α, Tn, Tα, w) This somewhat trivial behavior is the first and strongest axiom. In any situation, when the agent is an action away from completing the goal, it should complete the action. Any fair Eval function would naturally classify α as the maximal value action (property 1). However, without explicit axiomatization of such behavior there might be situations where the agent will decide on taking another action for various reasons, due to its bounded decision resources. A2. Preventive Act Axiom. Being in an adversarial situation, agent Aag might decide to take actions that will damage one of its adversarys plans to complete its goal, even if those actions do not explicitly advance Aag towards its conflicting goal G∗ Aag . Such preventive action will take place when agent Aag has a belief about the possibility of its adversary Ao doing an action β that will give it a high 552 The Sixth Intl. Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) utility evaluation value (> TrH). Believing that taking action α will prevent the opponent from doing its β, it will adopt a potential intention to do α. (∀Aag, Ao ∈ A, α ∈ CAag , β ∈ CAo , Tn, Tβ, w ∈ W) (Bel(Aag, Do(Ao, β, Tβ, w) ∧ Eval(Ao, β, w) > TrH, Tn) ∧ Bel(Aag, Do(Aag, α, Tα, w) ⇒ ¬Do(Ao, β, Tβ, w), Tn) ⇒ Pot.Int.To(Aag, α, Tn, Tα, w) This axiom is a basic component of any adversarial environment. For example, looking at a Chess board game, a player could realize that it is about to be checkmated by its opponent, thus making a preventive move. Another example is a Connect Four game: when a player has a row of three chips, its opponent must block it, or lose. A specific instance of A1 occurs when the adversary is one action away from achieving its goal, and immediate preventive action needs to be taken by the agent. Formally, we have the same beliefs as stated above, with a changed belief that doing action β will cause agent Ao to achieve its goal. Proposition 1: Prevent or lose case. (∀Aag, Ao ∈ A, α ∈ CAag , β ∈ CAo , G∗ Ao , Tn, Tα, Tβ, w ∈ W) Bel(Aag, Do(Ao, β, Tβ, w) ⇒ Achieve(G∗ Ao , β, w), Tn) ∧ Bel(Aag, Do(Aag, α, Tα, w) ⇒ ¬Do(Ao, β, Tβ, w)) ⇒ Pot.Int.To(Aag, α, Tn, Tα, w) Sketch of proof: Proposition 1 can be easily derived from axiom A1 and the property 2 of the Eval function, which states that any action that causes a completion of a goal is a highly beneficial action. The preventive act behavior will occur implicitly when the Eval function is equal to the real world utility function. However, being bounded rational agents and dealing with an estimated evaluation function we need to explicitly axiomatize such behavior, for it will not always occur implicitly from the evaluation function. A3. Suboptimal Tactical Move Axiom. In many scenarios a situation may occur where an agent will decide not to take the current most beneficial action it can take (the action with the maximal utility evaluation value), because it believes that taking another action (with lower utility evaluation value) might yield (depending on the adversarys response) a future possibility for a highly beneficial action. This will occur most often when the Eval function is inaccurate and differs by a large extent from the Utility function. Put formally, agent Aag believes in a certain SetAction that will evolve according to its initial action and will yield a high beneficial value (> TrH) solely for it. (∀Aag, Ao ∈ A, Tn, w ∈ W) (∃α1 , . . . , αu ∈ CAi , β1 , . . . , βv ∈ CAj , Tα1 ) Bel(Aag, SetAction(α1 , . . . , αu , β1 , . . . , βv ), Tn) ∧ Bel(Aag, Eval(Ao, βv , w) < TrH < Eval(Aag, αu , w), Tn) ⇒ Pot.Int.To(Aag, α1 , Tn, Tα1 , w) An agent might believe that a chain of events will occur for various reasons due to the inevitable nature of the domain. For example, in Chess, we often observe the following: a move causes a check position, which in turn limits the opponents moves to avoiding the check, to which the first player might react with another check, and so on. The agent might also believe in a chain of events based on its knowledge of its adversarys profile, which allows it to foresee the adversarys movements with high accuracy. A4. Profile Detection Axiom. The agent can adjust its adversarys profiles by observations and pattern study (specifically, if there are repeated encounters with the same adversary). However, instead of waiting for profile information to be revealed, an agent can also initiate actions that will force its adversary to react in a way that will reveal profile knowledge about it. Formally, the axiom states that if all actions (γ) are not highly beneficial actions (< TrH), the agent can do action α in time Tα if it believes that it will result in a non-highly beneficial action β from its adversary, which in turn teaches it about the adversarys profile, i.e., gives a higher AdvKnow(P Aj Ai , Tβ). (∀Aag, Ao ∈ A, α ∈ CAag , β ∈ CAo , Tn, Tα, Tβ, w ∈ W) Bel(Aag, (∀γ ∈ CAag )Eval(Aag, γ, w) < TrH, Tn) ∧ Bel(Aag, Do(Aag, α, Tα, w) ⇒ Do(Ao, β, Tβ, w), Tn) ∧ Bel(Aag, Eval(Ao, β, w) < TrH) ∧ Bel(Aag, AdvKnow(P Aj Ai , Tβ) > AdvKnow(P Aj Ai , Tn), Tn) ⇒ Pot.Int.To(Aag, α, Tn, Tα, w) For example, going back to the Chess board game scenario, consider starting a game versus an opponent about whom we know nothing, not even if it is a human or a computerized opponent. We might start playing a strategy that will be suitable versus an average opponent, and adjust our game according to its level of play. A5. Alliance Formation Axiom The following behavioral axiom is relevant only in a multilateral instantiation of the adversarial environment (obviously, an alliance cannot be formed in a bilateral, zero-sum encounter). In different situations during a multilateral interaction, a group of agents might believe that it is in their best interests to form a temporary alliance. Such an alliance is an agreement that constrains its members behavior, but is believed by its members to enable them to achieve a higher utility value than the one achievable outside of the alliance. As an example, we can look at the classical Risk board game, where each player has an individual goal of being the sole conquerer of the world, a zero-sum game. However, in order to achieve this goal, it might be strategically wise to make short-term ceasefire agreements with other players, or to join forces and attack an opponent who is stronger than the rest. An alliances terms defines the way its members should act. It is a set of predicates, denoted as Terms, that is agreed upon by the alliance members, and should remain true for the duration of the alliance. For example, the set Terms in the Risk scenario, could contain the following predicates: 1. Alliance members will not attack each other on territories X, Y and Z; 2. Alliance members will contribute C units per turn for attacking adversary Ao; 3. Members are obligated to stay as part of the alliance until time Tk or until adversarys Ao army is smaller than Q. The set Terms specifies inter-group constraints on each of the alliance members (∀Aal i ∈ Aal ⊆ A) set of actions Cal i ⊆ C. Definition 6. Al val - the total evaluation value that agent Ai will achieve while being part of Aal is the sum of Evali (Eval for Ai) of each of Aal j Eval values after taking their own α actions (via the agent(α) predicate): Al val(Ai, Cal , Aal , w) = α∈Cal Evali(Aal j , agent(α), w) Definition 7. Al TrH - is a number representing an Al val The Sixth Intl. Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) 553 threshold; above it, the alliance can be said to be a highly beneficial alliance. The value of Al TrH will be calculated dynamically according to the progress of the interaction, as can be seen in [7]. After an alliance is formed, its members are now working in their normal adversarial environment, as well as according to the mental states and axioms required for their interactions as part of the alliance. The following Alliance model (AL) specifies the conditions under which the group Aal can be said to be in an alliance and working with a new and constrained set of actions Cal , at time Tn. AL(Aal , Cal , w, Tn) 1. Aal has a MB that all members are part of Aal : MB(Aal , (∀Aal i ∈ Aal )member(Aal i , Aal ), Tn) 2. Aal has a MB that the group be maintained: MB(Aal , (∀Aal i ∈ Aal )Int.Th (Ai, member(Ai, Aal ), Tn, Tn+1, Co), Tn) 3. Aal has a MB that being members gives them high utility value: MB(Aal , (∀Aal i ∈ Aal )Al val(Aal i , Cal , Aal , w) ≥ Al TrH, Tn) Members profiles are a crucial part of successful alliances. We assume that agents that have more accurate profiles of their adversaries will be more successful in such environments. Such agents will be able to predict when a member is about to breach the alliances contract (item 2 in the above model), and take counter measures (when item 3 will falsify). The robustness of the alliance is in part a function of its members trustfulness measure, objective position estimation, and other profile properties. We should note that an agent can simultaneously be part of more than one alliance. Such a temporary alliance, where the group members do not have a joint goal but act collaboratively for the interest of their own individual goals, is classified as a Treatment Group by modern psychologists [12] (in contrast to a Task Group, where its members have a joint goal). The Shared Activity model as presented in [5] modeled Treatment Group behavior using the same SharedPlans formalization. When comparing both definitions of an alliance and a Treatment Group we found an unsurprising resemblance between both models: the environment models definitions are almost identical (see SAs definitions in [5]), and their Selfish-Act and Cooperative Act axioms conform to our adversarial agents behavior. The main distinction between both models is the integration of a Helpful-behavior act axiom, in the Shared Activity which cannot be part of ours. This axiom states that an agent will consider taking action that will lower its Eval value (to a certain lower bound), if it believes that a group partner will gain a significant benefit. Such behavior cannot occur in a pure adversarial environment (as a zero-sum game is), where the alliance members are constantly on watch to manipulate their alliance to their own advantage. A6. Evaluation Maximization Axiom. In a case when all other axioms are inapplicable, we will proceed with the action that maximizes the heuristic value as computed in the Eval function. (∀Aag, Ao ∈ A, α ∈ Cag, Tn, w ∈ W) Bel(Aag, (∀γ ∈ Cag)Eval(Aag, α, w) ≥ Eval(Aag, γ, w), Tn) ⇒ Pot.Int.To(Aag, α, Tn, Tα, w) T1. Optimality on Eval = Utility The above axiomatic model handles situations where the Utility is unknown and the agents are bounded rational agents. The following theorem shows that in bilateral interactions, where the agents have the real Utility function (i.e., Eval = Utility) and are rational agents, the axioms provide the same optimal result as classic adversarial search (e.g., Min-Max). Theorem 1. Let Ae ag be an unbounded rational AE agent using the Eval heuristic evaluation function, Au ag be the same agent using the true Utility function, and Ao be a sole unbounded utility-based rational adversary. Given that Eval = Utility: (∀α ∈ CAu ag , α ∈ CAe ag , Tn, w ∈ W) Pot.Int.To(Au ag, α, Tn, Tα, w) → Pot.Int.To(Ae ag, α , Tn, Tα, w) ∧ ((α = α ) ∨ (Utility(Au ag, α, w) = Eval(Ae ag, α , w))) Sketch of proof - Given that Au ag has the real utility function and unbounded resources, it can generate the full game tree and run the optimal MinMax algorithm to choose the highest utility value action, which we denote by, α. The proof will show that Ae ag, using the AE axioms, will select the same or equal utility α (when there is more than one action with the same max utility) when Eval = Utility. (A1) Goal achieving axiom - suppose there is an α such that its completion will achieve Au ags goal. It will obtain the highest utility by Min-Max for Au ag. The Ae ag agent will select α or another action with the same utility value via A1. If such α does not exist, Ae ag cannot apply this axiom, and proceeds to A2. (A2) Preventive act axiom - (1) Looking at the basic case (see Prop1), if there is a β which leads Ao to achieve its goal, then a preventive action α will yield the highest utility for Au ag. Au ag will choose it through the utility, while Ae ag will choose it through A2. (2) In the general case, β is a highly beneficial action for Ao, thus yields low utility for Au ag, which will guide it to select an α that will prevent β, while Ae ag will choose it through A2.1 If such β does not exist for Ao, then A2 is not applicable, and Ae ag can proceed to A3. (A3) Suboptimal tactical move axiom - When using a heuristic Eval function, Ae ag has a partial belief in the profile of its adversary (item 4 in AE model), which may lead it to believe in SetActions (Prop1). In our case, Ae ag is holding a full profile on its optimal adversary and knows that Ao will behave optimally according to the real utility values on the complete search tree, therefore, any belief about suboptimal SetAction cannot exist, yielding this axiom inapplicable. Ae ag will proceed to A4. (A4) Profile detection axiom - Given that Ae ag has the full profile of Ao, none of Ae ags actions can increase its knowledge. That axiom will not be applied, and the agent will proceed with A6 (A5 will be disregarded because the interaction is bilateral). (A6) Evaluation maximization axiom - This axiom will select the max Eval for Ae ag. Given that Eval = Utility, the same α that was selected by Au ag will be selected. 3. EVALUATION The main purpose of our experimental analysis is to evaluate the models behavior and performance in a real adversarial environment. This section investigates whether bounded 1 A case where following the completion of β there exists a γ which gives high utility for Agent Au ag, cannot occur because Ao uses the same utility, and γs existence will cause it to classify β as a low utility action. 554 The Sixth Intl. Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) rational agents situated in such adversarial environments will be better off applying our suggested behavioral axioms. 3.1 The Domain To explore the use of the above model and its behavioral axioms, we decided to use the Connect-Four game as our adversarial environment. Connect-Four is a 2-player, zerosum game which is played using a 6x7 matrix-like board. Each turn, a player drops a disc into one of the 7 columns (the set of 21 discs is usually colored yellow for player 1 and red for player 2; we will use White and Black respectively to avoid confusion). The winner is the first player to complete a horizontal, vertical, or diagonal set of four discs with its color. On very rare occasions, the game might end in a tie if all the empty grids are filled, but no player managed to create a 4-disc set. The Connect-Four game was solved in [1], where it is shown that the first player (playing with the white discs) can force a win by starting in the middle column (column 4) and playing optimally However, the optimal strategy is very complex, and difficult to follow even for complex bounded rational agents, such as human players. Before we can proceed checking agent behavior, we must first verify that the domain conforms to the adversarial environments definition as given above (which the behavioral axioms are based on). First, when playing a Connect-Four game, the agent has an intention to win the game (item 1). Second (item 2), our agent believes that in Connect-Four there can only be one winner (or no winner at all in the rare occurrence of a tie). In addition, our agent believes that its opponent to the game will try to win (item 3), and we hope it has some partial knowledge (item 4) about its adversary (this knowledge can vary from nothing, through simple facts such as age, to strategies and weaknesses). Of course, not all Connect-Four encounters are adversarial. For example, when a parent is playing the game with its child, the following situation might occur: the child, having a strong incentive to win, treats the environment as adversarial (it intends to win, understands that there can only be one winner, and believes that its parent is trying to beat him). However, the parents point of view might see the environment as an educational one, where its goal is not to win the game, but to cause enjoyment or practice strategic reasoning. In such an educational environment, a new set of behavioral axioms might be more beneficial to the parents goals than our suggested adversarial behavioral axioms. 3.2 Axiom Analysis After showing that the Connect-Four game is indeed a zero-sum, bilateral adversarial environment, the next step is to look at players behaviors during the game and check whether behaving according to our model does improve performance. To do so we have collected log files from completed Connect-Four games that were played by human players over the Internet. Our collected log file data came from Play by eMail (PBeM) sites. These are web sites that host email games, where each move is taken by an email exchange between the server and the players. Many such sites archives contain real competitive interactions, and also maintain a ranking system for their members. Most of the data we used can be found in [6]. As can be learned from [1], Connect-Four has an optimal strategy and a considerable advantage for the player who starts the game (which we call the White player). We will concentrate in our analysis on the second players moves (to be called Black). The White player, being the first to act, has the so-called initiative advantage. Having the advantage and a good strategy will keep the Black player busy reacting to its moves, instead of initiating threats. A threat is a combination of three discs of the same color, with an empty spot for the fourth winning disk. An open threat is a threat that can be realized in the opponents next move. In order for the Black player to win, it must somehow turn the tide, take the advantage and start presenting threats to the White player. We will explore Black players behavior and their conformance to our axioms. To do so, we built an application that reads log files and analyzes the Black players moves. The application contains two main components: (1) a Min-Max algorithm for evaluation of moves; (2) open threats detector for the discovering of open threats. The Min-Max algorithm will work to a given depth, d and for each move α will output the heuristic value for the next action taken by the player as written in the log file, h(α), alongside the maximum heuristic value, maxh(α), that could be achieved prior to taking the move (obviously, if h(α) = maxh(α), then the player did not do the optimal move heuristically). The threat detectors job is to notify if some action was taken in order to block an open threat (not blocking an open threat will probably cause the player to lose in the opponents next move). The heuristic function used by Min-Max to evaluate the players utility is the following function, which is simple to compute, yet provides a reasonable challenge to human opponents: Definition 8. Let Group be an adjacent set of four squares that are horizontal, vertical, or diagonal. Groupn b (Groupn w) be a Group with n pieces of the black (white) color and 4−n empty squares. h = ((Group1 b ∗α)+(Group2 b ∗β)+(Group3 b ∗γ)+(Group4 b ∗∞)) − ((Group1 w ∗α)+(Group2 w ∗β)+(Group3 w ∗γ)+(Group4 w ∗∞)) The values of α, β and δ can vary to form any desired linear combination; however, it is important to value them with the α < β < δ ordering in mind (we used 1, 4, and 8 as their respective values). Groups of 4 discs of the same color means victory, thus discovery of such a group will result in ∞ to ensure an extreme value. We now use our estimated evaluation function to evaluate the Black players actions during the Connect-Four adversarial interaction. Each game from the log file was input into the application, which processed and output a reformatted log file containing the h value of the current move, the maxh value that could be achieved, and a notification if an open threat was detected. A total of 123 games were analyzed (57 with White winning, and 66 with Black winning). A few additional games were manually ignored in the experiment, due to these problems: a player abandoning the game while the outcome is not final, or a blunt irrational move in the early stages of the game (e.g., not blocking an obvious winning group in the first opening moves). In addition, a single tie game was also removed. The simulator was run to a search depth of 3 moves. We now proceed to analyze the games with respect to each behavioral axiom. The Sixth Intl. Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) 555 Table 1: Average heuristic difference analysis Black losses Black Won Avg minh -17.62 -12.02 Avg 3 lowest h moves (min3 h) -13.20 -8.70 3.2.1 Affirming the Suboptimal tactical move axiom The following section presents the heuristic evaluations of the Min-Max algorithm for each action, and checks the amount and extent of suboptimal tactical actions and their implications on performance. Table 1 shows results and insights from the games heuristic analysis, when search depth equals 3 (this search depth was selected for the results to be comparable to [9], see Section 3.2.3). The tables heuristic data is the difference between the present maximal heuristic value and the heuristic value of the action that was eventually taken by the player (i.e., the closer the number is to 0, the closer the action was to the maximum heuristic action). The first row presents the difference values of the action that had the maximal difference value among all the Black players actions in a given game, as averaged over all Blacks winning and losing games (see respective columns). In games in which the Black player loses, its average difference value was -17.62, while in games in which the Black player won, its average was -12.02. The second row expands the analysis by considering the 3 highest heuristic difference actions, and averaging them. In that case, we notice an average heuristic difference of 5 points between games which the Black player loses and games in which it wins. Nevertheless, the importance of those numbers is that they allowed us to take an educated guess on a threshold number of 11.5, as the value of the TrH constant, which differentiates between normal actions and highly beneficial ones. After finding an approximated TrH constant, we can proceed with an analysis of the importance of suboptimal moves. To do so we took the subset of games in which the minimum heuristic difference value for Blacks actions was 11.5. As presented in Table 2, we can see the different min3 h average of the 3 largest ranges and the respective percentage of games won. The first row shows that the Black player won only 12% of the games in which the average of its 3 highest heuristically difference actions (min3 h) was smaller than the suggested threshold, TrH = 11.5. The second row shows a surprising result: it seems that when min3 h > −4 the Black player rarely wins. Intuition would suggest that games in which the action evaluation values were closer to the maximal values will result in more winning games for Black. However, it seems that in the Connect-Four domain, merely responding with somewhat easily expected actions, without initiating a few surprising and suboptimal moves, does not yield good results. The last row sums up the main insights from the analysis; most of Blacks wins (83%) came when its min3 h was in the range of -11.5 to -4. A close inspection of those Black winning games shows the following pattern behind the numbers: after standard opening moves, Black suddenly drops a disc into an isolated column, which seems a waste of a move. White continues to build its threats, while usually disregarding Blacks last move, which in turn uses the isolated disc as an anchor for a future winning threat. The results show that it was beneficial for the Black player Table 2: Blacks winnings percentages % of games min3 h < −11.5 12% min3 h > −4 5% −11.5 ≤ min3 h ≤ −4 83% to take suboptimal actions and not give the current highest possible heuristic value, but will not be too harmful for its position (i.e., will not give high beneficial value to its adversary). As it turned out, learning the threshold is an important aspect of success: taking wildly risky moves (min3 h < −11.5) or trying to avoid them (min3 h > −4) reduces the Black players winning chances by a large margin. 3.2.2 Affirming the Profile Monitoring Axiom In the task of showing the importance of monitoring ones adversaries profiles, our log files could not be used because they did not contain repeated interactions between players, which are needed to infer the players knowledge about their adversaries. However, the importance of opponent modeling and its use in attaining tactical advantages was already studied in various domains ([3, 9] are good examples). In a recent paper, Markovitch and Reger [9] explored the notion of learning and exploitation of opponent weakness in competitive interactions. They apply simple learning strategies by analyzing examples from past interactions in a specific domain. They also used the Connect-Four adversarial domain, which can now be used to understand the importance of monitoring the adversarys profile. Following the presentation of their theoretical model, they describe an extensive empirical study and check the agents performance after learning the weakness model with past examples. One of the domains used as a competitive environment was the same Connect-Four game (Checkers was the second domain). Their heuristic function was identical to ours with three different variations (H1, H2, and H3) that are distinguished from one another in their linear combination coefficient values. The search depth for the players was 3 (as in our analysis). Their extensive experiments check and compare various learning strategies, risk factors, predefined feature sets and usage methods. The bottom line is that the Connect-Four domain shows an improvement from a 0.556 winning rate before modeling to a 0.69 after modeling (page 22). Their conclusions, showing improved performance when holding and using the adversarys model, justify the effort to monitor the adversary profile for continuous and repeated interactions. An additional point that came up in their experiments is the following: after the opponent weakness model has been learned, the authors describe different methods of integrating the opponent weakness model into the agents decision strategy. Nevertheless, regardless of the specific method they chose to work with, all integration methods might cause the agent to take suboptimal decisions; it might cause the agent to prefer actions that are suboptimal at the present decision junction, but which might cause the opponent to react in accordance with its weakness model (as represented by our agent) which in turn will be beneficial for us in the future. The agents behavior, as demonstrated in [9] further confirms and strengthens our Suboptimal Tactical Axiom as discussed in the previous section. 556 The Sixth Intl. Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) 3.2.3 Additional Insights The need for the Goal Achieving, Preventive Act, and Evaluation Maximization axioms are obvious, and need no further verification. However, even with respect to those axioms, a few interesting insights came up in the log analysis. The Goal achieving and Preventive Act axioms, though theoretically trivial, seem to provide some challenge to a human player. In the initial inspection of the logs, we encountered few games2 where a player, for inexplicable reasons, did not block the other from winning or failed to execute its own winning move. We can blame those faults on the humans lack of attention, or a typing error in its move reply; nevertheless, those errors might occur in bounded rational agents, and the appropriate behavior needs to be axiomatized. A typical Connect-Four game revolves around generating threats and blocking them. In our analysis we looked for explicit preventive actions, i.e., moves that block a group of 3 discs, or that remove a future threat (in our limited search horizon). We found that in 83% of the total games there was at least one preventive action taken by the Black player. It was also found that Black averaged 2.8 preventive actions per game on the games in which it lost, while averaging 1.5 preventive actions per game when winning. It seems that Black requires 1 or 2 preventive actions to build its initial taking position, before starting to present threats. If it did not manage to win, it will usually prevent an extra threat or two before succumbing to White. 4. RELATED WORK Much research deals with the axiomatization of teamwork and mental states of individuals: some models use knowledge and belief [10], others have models of goals and intentions [8, 4]. However, all these formal theories deal with agent teamwork and cooperation. As far as we know, our model is the first to provide a formalized model for explicit adversarial environments and agents behavior in it. The classical Min-Max adversarial search algorithm was the first attempt to integrate the opponent into the search space with a weak assumption of an optimally playing opponent. Since then, much effort has gone into integrating the opponent model into the decision procedure to predict future behavior. The M∗ algorithm presented by Carmel and Markovitch [2] showed a method of incorporating opponent models into adversary search, while in [3] they used learning to provide a more accurate opponent model in a 2player repeated game environment, where agents strategies were modeled as finite automata. Additional Adversarial planning work was done by Willmott et al. [13], which provided an adversarial planning approach to the game of GO. The research mentioned above dealt with adversarial search and the integration of opponent models into classical utilitybased search methods. That work shows the importance of opponent modeling and the ability to exploit it to an agents advantage. However, the basic limitations of those search methods still apply; our model tries to overcome those limitations by presenting a formal model for a new, mental state-based adversarial specification. 5. CONCLUSIONS We presented an Adversarial Environment model for a 2 These were later removed from the final analysis. bounded rational agent that is situated in an N-player, zerosum environment. We used the SharedPlans formalization to define the model and the axioms that agents can apply as behavioral guidelines. The model is meant to be used as a guideline for designing agents that need to operate in such adversarial environments. We presented empirical results, based on ConnectFour log file analysis, that exemplify the model and the axioms for a bilateral instance of the environment. The results we presented are a first step towards an expanded model that will cover all types of adversarial environments, for example, environments that are non-zero-sum, and environments that contain natural agents that are not part of the direct conflict. Those challenges and more will be dealt with in future research. 6. ACKNOWLEDGMENT This research was supported in part by Israel Science Foundation grants #1211/04 and #898/05. 7. REFERENCES [1] L. V. Allis. A knowledge-based approach of Connect-Four - the game is solved: White wins. Masters thesis, Free University, Amsterdam, The Netherlands, 1988. [2] D. Carmel and S. Markovitch. Incorporating opponent models into adversary search. In Proceedings of the Thirteenth National Conference on Artificial Intelligence, pages 120-125, Portland, OR, 1996. [3] D. Carmel and S. Markovitch. Opponent modeling in multi-agent systems. In G. Weiß and S. Sen, editors, Adaptation and Learning in Multi-Agent Systems, pages 40-52. Springer-Verlag, 1996. [4] B. J. Grosz and S. Kraus. Collaborative plans for complex group action. Artificial Intelligence, 86(2):269-357, 1996. [5] M. Hadad, G. Kaminka, G. Armon, and S. Kraus. Supporting collaborative activity. In Proc. of AAAI-2005, pages 83-88, Pittsburgh, 2005. [6] http://www.gamerz.net/˜pbmserv/. [7] S. Kraus and D. Lehmann. Designing and building a negotiating automated agent. Computational Intelligence, 11:132-171, 1995. [8] H. J. Levesque, P. R. Cohen, and J. H. T. Nunes. On acting together. In Proc. of AAAI-90, pages 94-99, Boston, MA, 1990. [9] S. Markovitch and R. Reger. Learning and exploiting relative weaknesses of opponent agents. Autonomous Agents and Multi-Agent Systems, 10(2):103-130, 2005. [10] Y. M. Ronald Fagin, Joseph Y. Halpern and M. Y. Vardi. Reasoning about knowledge. MIT Press, Cambridge, Mass., 1995. [11] P. Thagard. Adversarial problem solving: Modeling an oponent using explanatory coherence. Cognitive Science, 16(1):123-149, 1992. [12] R. W. Toseland and R. F. Rivas. An Introduction to Group Work Practice. Prentice Hall, Englewood Cliffs, NJ, 2nd edition edition, 1995. [13] S. Willmott, J. Richardson, A. Bundy, and J. Levine. An adversarial planning approach to Go. Lecture Notes in Computer Science, 1558:93-112, 1999. The Sixth Intl. Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) 557",
    "original_translation": "Un modelo de entorno adversario para agentes racionales limitados en interacciones de suma cero en Zuckerman1, Sarit Kraus1, Jeffrey S. Rosenschein2, Gal Kaminka1 1 Departamento de Ciencias de la Computación 2 La Escuela de Ingeniería Bar-Ilan Ciencias de la Computación Ramat-Gan, ISRAEL HEBREO UNIVERSITY HEBREO., Jerusalén, Israel {Zukermi, Sarit, Galk}@cs.biu.ac.il jeff@cs.huji.ac.il Los entornos multiagentes abstractos a menudo no son cooperativos ni colaborativos;En muchos casos, los agentes tienen intereses contradictorios, lo que lleva a interacciones adversas. Este artículo presenta un modelo de entorno adversario formal para agentes racionales acotados que operan en un entorno de suma cero. En tales entornos, los intentos de usar métodos de búsqueda clásicos basados en utilidad pueden generar una variedad de dificultades (por ejemplo, modelar implícitamente al oponente como un maximizador de utilidad omnisciente, en lugar de aprovechar un modelo oponente más matizado y explícito). Definimos un entorno adversario al describir los estados mentales de un agente en dicho entorno. Luego presentamos axiomas conductuales que están destinados a servir como principios de diseño para construir tales agentes adversos. Exploramos la aplicación de nuestro enfoque analizando archivos de registro de juegos completos de Connect-Four y presentamos un análisis empírico de la idoneidad de los axiomas. Categorías y descriptores de sujetos I.2.11 [Inteligencia artificial]: agentes de inteligencia artificial distribuidos, sistemas multiagentes;I.2.4 [Inteligencia artificial]: Formalismos y métodos de representación del conocimiento: lógica moderada Términos generales Diseño, Teoría 1. Introducción Investigación temprana en sistemas multiagentes (MAS) considerados grupos cooperativos de agentes;Debido a que los agentes individuales tenían recursos limitados o acceso limitado a la información (por ejemplo, potencia de procesamiento limitada, cobertura de sensor limitada), trabajaron juntos por diseño para resolver problemas que individualmente no podían resolver, o al menos no podían resolver de manera tan eficiente. La investigación de MAS, sin embargo, pronto comenzó a considerar a los agentes interactivos con intereses individualizados, como representantes de diferentes humanos u organizaciones con intereses no idénticos. Cuando las interacciones son guiadas por diversos intereses, los participantes pueden tener que superar los desacuerdos, las interacciones no cooperativas e incluso los intentos intencionales de dañarse entre sí. Cuando se producen estos tipos de interacciones, los entornos requieren un comportamiento apropiado de los agentes situados en ellas. Llamamos a estos entornos entornos adversos y llamamos a los adversarios de los agentes de enfrentamiento. Los modelos de cooperación y trabajo en equipo se han explorado ampliamente en MAS a través de la axiomatización de estados mentales (por ejemplo, [8, 4, 5]). Sin embargo, ninguna de esta investigación trató con dominios adversos y sus implicaciones para el comportamiento de los agentes. Nuestro documento aborda este problema al proporcionar un modelo de estado mental formal y axiomatizado para un subconjunto de dominios adversos, a saber, entornos adversos de suma cero simples. Los encuentros simples de suma cero existen, por supuesto, en varios juegos de doble capa (por ejemplo, ajedrez, damas), pero también existen en los juegos N-jugadores (por ejemplo, riesgo, diplomacia), subastas para un solo bien y en otros lugares. Especialmente en estos últimos entornos, el uso de una búsqueda adversa basada en utilidad (como el algoritmo Min-Max) no siempre proporciona una solución adecuada;La función de pago puede ser bastante compleja o difícil de cuantificar, y existen limitaciones computacionales naturales en los agentes racionales limitados. Además, los métodos de búsqueda tradicionales (como Min-Max) no utilizan un modelo del oponente, lo que ha demostrado ser una valiosa adición a la planificación adversaria [9, 3, 11]. En este documento, desarrollamos un modelo formal y axiomatizado para agentes racionales limitados que están situados en un entorno adversario de suma cero. El modelo utiliza diferentes operadores de modalidad, y sus principales bases son el modelo Sharedplans [4] para el comportamiento colaborativo. Exploramos las propiedades del medio ambiente y los estados mentales de los agentes para derivar axiomas conductuales;Estos axiomas conductuales constituyen un modelo formal que sirve como guía de especificación y diseño para el diseño de agentes en tales entornos. Luego investigamos el comportamiento de nuestro modelo empíricamente utilizando el juego de mesa Connect-Four. Mostramos que este juego se ajusta a nuestra definición de entorno y analizamos el comportamiento de los jugadores utilizando un gran conjunto de Match Log 550 978-81-904262-7-7 (RPS) C 2007 IFAAMAS. Además, utilizamos los resultados presentados en [9] para discutir la importancia del modelado del oponente en nuestro dominio congresivo ConnectFour. El documento procede de la siguiente manera. La Sección 2 presenta la formalización de los modelos. La Sección 3 presenta el análisis empírico y sus resultados. Discutimos el trabajo relacionado en la Sección 4 y concluyamos y presentamos direcciones futuras en la Sección 5. 2. Entornos adversos El modelo de entorno adversario (denotado como AE) está destinado a guiar el diseño de agentes proporcionando una especificación de las capacidades y actitudes mentales de un agente en un entorno adversario. Nos centramos aquí en tipos específicos de entornos adversos, especificados de la siguiente manera: 1. Interacciones de suma cero: las utilidades positivas y negativas de todos los agentes suman a cero;2. AES simples: todos los agentes en el medio ambiente son agentes adversos;3. AES bilaterales: AES con exactamente dos agentes;4. AES multilaterales: EA de tres o más agentes. Trabajaremos en instancias bilaterales y multilaterales de entornos simples y de suma cero. En particular, nuestro modelo de entorno adversario se ocupará de las interacciones que consisten en agentes N (n ≥ 2), donde todos los agentes son adversarios, y solo un agente puede tener éxito. Ejemplos de tales entornos van desde juegos de mesa (por ejemplo, ajedrez, conexión y diplomacia) hasta ciertos entornos económicos (por ejemplo, subastas de N-bidder en un solo bien).2.1 Descripción general del modelo Nuestro enfoque es formalizar las actitudes y comportamientos mentales de un solo agente adversario;Consideramos cómo un solo agente percibe el AE. La siguiente lista especifica las condiciones y los estados mentales de un agente en un AE simple de suma cero: 1. El agente tiene una intención individual de que su propio objetivo se completará;2. El agente tiene una creencia individual de que él y sus adversarios buscan objetivos conflictivos completos (definidos a continuación) puede haber solo un ganador;3. El agente tiene una creencia individual de que cada adversario tiene la intención de completar su propio objetivo conflictivo;4. El agente tiene una creencia individual en el perfil (parcial) de sus adversarios. Se requiere el ítem 3, ya que podría ser el caso de que algún agente tenga un objetivo conflictivo completo, y actualmente está considerando adoptar la intención de completarlo, pero hasta el momento, no está comprometido a lograrlo. Esto podría ocurrir porque el agente aún no ha deliberado sobre los efectos que adoptar esa intención podría tener en las otras intenciones que está teniendo actualmente. En tales casos, podría no considerarse ni siquiera estar en un entorno adversario. El ítem 4 establece que el agente debe mantener una creencia sobre los perfiles de sus adversarios. El perfil representa todo el conocimiento que el agente tiene sobre su adversario: sus debilidades, capacidades estratégicas, objetivos, intenciones, confiabilidad y más. Se puede administrar explícitamente o se puede aprender de las observaciones de encuentros pasados.2.2 Definiciones del modelo para estados mentales Usamos definiciones de Grosz y Krauss de los operadores modales, predicados y metacredicados, como se define en su formalización del plan compartido [4]. Recordamos aquí algunos de los predicados y operadores que se utilizan en esa formalización: int.to (ai, α, tn, tα, c) representan las intenciones de AIS en el momento TN para hacer una acción α en el tiempo Tα en el contexto de C.Int.th (ai, prop, tn, tprop, c) representa las intenciones de AIS en el momento tn que una cierta proposición se mantiene en el tiempo tprop en el contexto de C. los operadores de intención potenciales, pot.int.to (...)y pot.int.th (...), se utilizan para representar el estado mental cuando un agente considera adoptar una intención, pero no ha deliberado sobre la interacción de las otras intenciones que tiene. El operador Bel (AI, F, TF) representa al agente AI que cree en la declaración expresada en la Fórmula F, en el tiempo TF. MB (A, F, TF) representa la creencia mutua para un grupo de agentes A. Una instantánea del sistema considera que nuestro entorno está en algún estado e ∈ E de estados variables ambientales, y cada adversario en cualquier lai ∈ L de posibles estados locales. En cualquier paso de tiempo, el sistema estará en algún mundo W del conjunto de todos los mundos posibles w ∈ W, donde w = E × LA1 × LA2 × ... LAN, y N es el número de adversarios. Por ejemplo, en un juego de póker de Texas Holdem, un estado local de los agentes podría ser su propio conjunto de cartas (que es desconocido para su adversario), mientras que el entorno consistirá en el bote de apuestas y las cartas comunitarias (que son visibles para ambos jugadores). Una función de utilidad bajo esta formalización se define como un mapeo de un posible mundo w ∈ W a un elemento en, lo que expresa la conveniencia del mundo, desde una perspectiva de un solo agente. Por lo general, normalizamos el rango a [0,1], donde 0 representa el mundo menos deseable posible, y 1 es el mundo más deseable. La implementación de la función de utilidad depende del dominio en cuestión. La siguiente lista especifica nuevos predicados, funciones, variables y constantes utilizadas junto con las definiciones originales para la formalización del entorno adversario: 1. φ es una acción nula (el agente no hace nada).2. GAI es el conjunto de objetivos de agente AIS. Cada objetivo es un conjunto de predicados cuya satisfacción complete el objetivo (usamos g ∗ ai ∈ Gai para representar un objetivo arbitrario del agente ai).3. GAI es el conjunto de subconsportes de agentes AIS. Los subggoal son predicados cuya satisfacción representa un hito importante hacia el logro del objetivo completo.GG ∗ ai ⊆ gai es el conjunto de subggoals que son importantes para la finalización de la meta G ∗ ai (usaremos g ∗ g ∗ ai ∈ Gg ∗ ai para representar un subggoal arbitrario).4. P AJ AI es el perfil de Object Agent Ai sobre el agente AJ.5. CA es un conjunto general de acciones para todos los agentes en A que se derivan de las limitaciones de los entornos. Cai ⊆ Ca es el conjunto de acciones posibles de agentes.6. DO (AI, α, Tα, W) se mantiene cuando AI realiza la acción α en el intervalo de tiempo Tα en el mundo w.7. Lograr (g ∗ ai, α, w) es cierto cuando el objetivo g ∗ ai se logra después de la finalización de la acción α en el mundo w ∈ W, donde α ∈ Cai.8. El perfil (ai, pai ai) es cierto cuando el agente ai contiene un perfil de objeto para el agente AJ. Definición 1. El conflicto completo (FULCONF) describe una interacción Zerosum donde solo se puede completar un objetivo de los objetivos en el conflicto. Fulconf (g ∗ ai, g ∗ aj) ⇒ (∃α ∈ Cai, ∀W, β ∈ Caj) (lograr (g ∗ ai, α, w) ⇒ ¬aChieve (g ∗ aj, β, w)) ∨ ((∃β ∈ Caj, ∀W, α ∈ Cai) (lograr (G ∗ AJ, β, W) ⇒ ¬Acie (G ∗ Ai, α, W)) Definición 2. El conocimiento adversario (ADVKEK) es una función que devuelve un valor que representa la cantidad de la Sexta INTL. Conf.En agentes autónomos y sistemas de múltiples agentes (AAMAS 07) 551 El agente de conocimiento AI tiene en el perfil de Agente AJ, en el momento TN. Cuanto mayor sea el valor, más conocimientos agente de IA. Advnow: P AJ AI × TN → Definición 3. Eval: esta función de evaluación devuelve un valor de utilidad esperado estimado para un agente en A, después de completar una acción de CA en algún estado mundial w.Eval: A × CA × W → Definición 4. TRH - (umbral) es una constante numérica en el rango [0,1] que representa un valor umbral de la función de evaluación (eval). Una acción que produce una evaluación de utilidad estimada por encima de la TRH se considera una acción altamente beneficiosa. El valor de evaluación es una estimación y no la función de utilidad real, que generalmente se desconoce. El uso del valor de utilidad real para un agente racional daría fácilmente el mejor resultado para ese agente. Sin embargo, los agentes generalmente no tienen las funciones de utilidad reales, sino una estimación heurística de ellas. Hay dos propiedades importantes que deben mantener para la función de evaluación: Propiedad 1. La función de evaluación debe indicar que el estado mundial más deseable es uno en el que se logra el objetivo. Por lo tanto, después de que el objetivo se haya cumplido, no puede haber acciones futuras que puedan colocar al agente en un estado mundial con un mayor valor de evaluación.(∀ai, g ∗ ai, α, β ∈ Cai, w ∈ W) lograr (g ∗ ai, α, w) ⇒ eval (ai, α, w) ≥ eval (ai, β, w) propiedad 2. La función de evaluación debe proyectar una acción que cause una finalización de un objetivo o un subggoal a un valor que sea mayor que TRH (una acción altamente beneficiosa).(∀ai, g ∗ ai ∈ Gai, α ∈ Cai, w ∈ W, g ∗ gai ∈ Ggai) alcanzar (g ∗ ai, α, w) ∨ alcanzar (g ∗ gai, α, w) ⇒ eval (ai,α, W) ≥ TRH. Definición 5. SetAction Definimos una acción establecida (setAction) como un conjunto de operaciones de acción (acciones complejas o básicas) de algunas acciones Conjuntos CAI y CAJ que, según la creencia del agente, están unidos por una relación temporal y consecuente, formando una cadenade eventos (acción y su siguiente acción consecuente).(∀α1, ..., αu ∈ Cai, β1, ..., βV ∈ Caj, w ∈ W) setAction (α1, ..., αu, β1, ..., βV, w) ⇒ ((do(Ai, α1, tα1, w) ⇒ do (AJ, β1, Tβ1, W)) ⇒ do (ai, α2, tα2, w) ⇒ ... ⇒ do (ai, αu, tαu, w)) lo consecuencialLa relación puede existir debido a diversas restricciones ambientales (cuando una acción obliga al adversario a responder con una acción específica) o debido al conocimiento de los agentes sobre el perfil de su adversario. Propiedad 3. Como conocimiento que tenemos sobre nuestros aumentos adversarios, tendremos creencias adicionales sobre su comportamiento en diferentes situaciones que a su vez crean nuevas acciones establecidas. Formalmente, si nuestro ADVKEK en el momento TN+1 es mayor que Advnow en el momento TN, entonces cada setaction conocida en el momento TN también se conoce en el tiempo TN+1. Advknow (p aj ai, tn+1)> advknow (p aj ai, tn) ⇒ (∀α1, ..., αu ∈ Cai, β1, ..., βv ∈ Caj) bel (AAG, setAction (α1,.., αu, β1, ..., βV), tn) ⇒ bel (aag, setAction (α1, ..., αu, β1, ..., βV), tn+1) 2.3 la formulación del medio ambiente la formulación del medio ambienteLos siguientes axiomas proporcionan la definición formal para un entorno adversario (AE) simple de suma cero. La satisfacción de estos axiomas significa que el agente está situado en dicho entorno. Proporciona especificaciones para que el agente AAG interactúe con su conjunto de adversarios a con respecto a los objetivos g ∗ aag y g ∗ a en el tiempo tco en algún estado mundial w.Ae (aag, a, g ∗ aag, a1, ..., ak, g ∗ a1, ..., g ∗ ak, tn, w) 1. AAG tiene un int.. AAG cree que y cada uno de sus adversarios está buscando objetivos conflictivos completos: (∀ao ∈ {a1, ..., ak}) bel (aag, fulconf (g ∗ aag, g ∗ ao), tn) 3. AAG cree que cada uno de sus adversarios en AO tiene el int.th su objetivo conflictivo g ∗ aoi se completará: (∀ao ∈ {a1, ..., ak}) (∃β ∈ Cao, tβ) bel (aag,Int.th (ao, alcanzar (g ∗ ao, β), tco, tβ, ae), tn) 4. AAG tiene creencias sobre los perfiles (parciales) de sus adversarios (∀ao ∈ {a1, ..., ak}) (∃pao aag ∈ Paag) bel (aag, perfil (ao, pao aag), tn) para construir unAgente que podrá operar con éxito dentro de dicha AE, debemos especificar las pautas de comportamiento para sus interacciones. El uso de una estrategia de maximización de evaluación ingenua a una determinada profundidad de búsqueda no siempre producirá resultados satisfactorios por varias razones: (1) el problema del horizonte de búsqueda al buscar una profundidad fija;(2) la fuerte suposición de un adversario de recursos óptimamente racional e ilimitado;(3) Uso de una función de evaluación estimada que no dará resultados óptimos en todos los estados del mundo, y puede ser explotada [9]. Los siguientes axiomas especifican los principios de comportamiento que pueden usarse para diferenciar entre agentes exitosos y menos exitosos en el entorno adversario anterior. Esos axiomas deben usarse como principios de especificación al diseñar e implementar agentes que deberían poder funcionar bien en tales entornos adversos. Los axiomas conductuales representan situaciones en las que el agente adoptará intenciones potenciales para (Pot.int.to (...)) realizar una acción, que generalmente requerirá un razonamiento de extremo de medios para seleccionar un posible curso de acción. Este razonamiento conducirá a la adopción de un int.to (...) (ver [4]). A1. Objetivo para lograr el axioma. El primer axioma es el caso más simple;Cuando el agente AAG cree que se trata de una acción (α) de lograr su objetivo conflictivo G ∗ AAG, debe adoptar la intención potencial de hacer α y completar su objetivo.(∀AAG, α ∈ Caag, TN, Tα, W ∈ W) (Bel (AAG, DO (AAG, α, Tα, W) ⇒ Logro (G ∗ AAG, α, W)) ⇒ Pot.int.to ((AAG, α, TN, Tα, W) Este comportamiento algo trivial es el primer y más fuerte axioma. En cualquier situación, cuando el agente está a una acción lejos de completar el objetivo, debe completar la acción. Cualquier función de evaluación justa clasificaría naturalmente α como la acción del valor máximo (propiedad 1). Sin embargo, sin la axiomatización explícita de dicho comportamiento, podría haber situaciones en las que el agente decidirá tomar otra acción por varias razones, debido a sus recursos de decisión limitados. A2. Axioma de acto preventivo. Al estar en una situación adversa, el Agente AAG podría decidir tomar medidas que dañen uno de sus planes de adversario para completar su objetivo, incluso si esas acciones no avanzan explícitamente a AAG hacia su objetivo conflictivo G ∗ AAG. Dicha acción preventiva tendrá lugar cuando el Agente AAG cree sobre la posibilidad de que su adversario AO haga una acción β que le dará un alto 552 el Sexto INTL. Conf.en agentes autónomos y sistemas de múltiples agentes (AAMAS 07) Valor de evaluación de utilidad (> TRH). Creyendo que tomar medidas α evitará que el oponente haga su β, adoptará una intención potencial de hacer α.(∀Aag, ao ∈ A, α ∈ Caag, β ∈ Cao, TN, Tβ, W ∈ W) (BEL (AAG, DO (AO, β, Tβ, W) ∧ Eval (AO, β, W)> TRH, Tn) ∧ bel (aag, do (aag, α, tα, w) ⇒ ¬do (ao, β, tβ, w), tn) ⇒ pot.int.to (aag, α, tn, tα, w)Este axioma es un componente básico de cualquier entorno adversario. Por ejemplo, mirando un juego de mesa de ajedrez, un jugador podría darse cuenta de que está a punto de ser revisado por su oponente, haciendo un movimiento preventivo. Otro ejemplo es un juego Connect Four: cuando un jugador tiene una fila de tres chips, su oponente debe bloquearlo o perder. Una instancia específica de A1 ocurre cuando el adversario está a una acción de lograr su objetivo, y el agente debe tomar medidas preventivas inmediatas. Formalmente, tenemos las mismas creencias que se indican anteriormente, con una creencia cambiada de que hacer una acción β hará que el agente AO alcance su objetivo. Proposición 1: prevenir o perder el caso.(∀Aag, ao ∈ A, α ∈ Caag, β ∈ Cao, G ∗ Ao, TN, Tα, Tβ, W ∈ W) Bel (AAG, DO (AO, β, Tβ, W) ⇒ Logro (G ∗ AO, β, w), tn) ∧ bel (aag, do (aag, α, tα, w) ⇒ ¬do (ao, β, tβ, w)) ⇒ pot.int.to (aag, α, tn, tα, w) Boceto de prueba: la Proposición 1 puede derivarse fácilmente del Axiom A1 y la Propiedad 2 de la función EVEV, que establece que cualquier acción que cause una finalización de un objetivo es una acción altamente beneficiosa. El comportamiento del acto preventivo ocurrirá implícitamente cuando la función EVAL sea igual a la función de utilidad del mundo real. Sin embargo, siendo agentes racionales limitados y lidiar con una función de evaluación estimada, necesitamos axiomatizar explícitamente dicho comportamiento, ya que no siempre ocurrirá implícitamente en la función de evaluación. A3. Axioma de movimiento táctico subóptimo. En muchos escenarios, puede ocurrir una situación en la que un agente decidirá no tomar la acción más beneficiosa actual que puede tomar (la acción con el valor máximo de evaluación de servicios públicos), ya que cree que tomar otra acción (con un valor de evaluación de utilidad más bajo) podría producir(Dependiendo de la respuesta de los adversarios) Una posibilidad futura para una acción altamente beneficiosa. Esto ocurrirá con mayor frecuencia cuando la función EVAL sea inexacta y difiere en gran medida de la función de utilidad. Dicho formalmente, el Agente AAG cree en una cierta medida que evolucionará de acuerdo con su acción inicial y producirá un alto valor beneficioso (> TRH) únicamente para ello.(∀aag, ao ∈ A, tn, w ∈ W) (∃α1, ..., αu ∈ Cai, β1, ..., βv ∈ Caj, tα1) bel (AAG, setAction (α1, ...,.,,,,,,,,,.αu, β1, ..., βV), tn) ∧ bel (aag, eval (ao, βv, w) <trh <eval (aag, αu, w), tn) ⇒ pot.int.to (AAG, α1, TN, Tα1, W) Un agente podría creer que una cadena de eventos ocurrirá por varias razones debido a la naturaleza inevitable del dominio. Por ejemplo, en el ajedrez, a menudo observamos lo siguiente: un movimiento provoca una posición de verificación, lo que a su vez limita que los oponentes se mueven para evitar el cheque, al que el primer jugador podría reaccionar con otro control, y así sucesivamente. El agente también podría creer en una cadena de eventos basados en su conocimiento de su perfil de adversario, lo que le permite prever los movimientos de adversarios con alta precisión. A4. Axioma de detección de perfil. El agente puede ajustar sus perfiles de adversario mediante observaciones y estudio de patrones (específicamente, si hay encuentros repetidos con el mismo adversario). Sin embargo, en lugar de esperar a que se revele la información del perfil, un agente también puede iniciar acciones que obligarán a su adversario a reaccionar de una manera que revele el conocimiento del perfil al respecto. Formalmente, el axioma establece que si todas las acciones (γ) no son acciones altamente beneficiosas (<trh), el agente puede hacer una acción α en el tiempo Tα si cree que dará como resultado una acción no altamente beneficiosa β de su adversario,Lo que a su vez le enseña sobre el perfil de los adversarios, es decir, ofrece un advk más alto (p aj ai, tβ).(∀Aag, ao ∈ A, α ∈ Caag, β ∈ Cao, TN, Tα, Tβ, W ∈ W) Bel (AAG, (∀γ ∈ Caag) Eval (AAG, γ, W) <TRH, TN) ∧BEL (AAG, DO (AAG, α, Tα, W) ⇒ Do (AO, β, Tβ, W), TN) ∧ Bel (AAG, Eval (AO, β, W) <TRH) ∧ Bel (AAG, ADVWOK(P AJ AI, Tβ)> advknow (P AJ AI, TN), TN) ⇒ Pot.int.to (AAG, α, TN, Tα, W) Por ejemplo, volver al escenario del juego de tablero, considerar comenzarUn juego versus un oponente sobre el que no sabemos nada, ni siquiera si es un oponente humano o computarizado. Podríamos comenzar a jugar una estrategia que será adecuada frente a un oponente promedio, y ajustar nuestro juego de acuerdo con su nivel de juego. A5. Axioma de formación de la alianza El siguiente axioma de comportamiento es relevante solo en una instanciación multilateral del entorno adversario (obviamente, una alianza no puede formarse en un encuentro bilateral de suma cero). En diferentes situaciones durante una interacción multilateral, un grupo de agentes podría creer que lo mejor para su mejor interés formará una alianza temporal. Tal alianza es un acuerdo que limita el comportamiento de sus miembros, pero sus miembros cree que les permitan lograr un valor de utilidad más alto que el que se puede lograr fuera de la alianza. Como ejemplo, podemos ver el juego de mesa de riesgo clásico, donde cada jugador tiene un objetivo individual de ser el único conquistador del mundo, un juego de suma cero. Sin embargo, para lograr este objetivo, podría ser estratégicamente sabio hacer acuerdos de alto el fuego a corto plazo con otros jugadores, o unir fuerzas y atacar a un oponente que es más fuerte que el resto. Los términos de alianzas define la forma en que deben actuar sus miembros. Es un conjunto de predicados, denotados como términos, que los miembros de la alianza acuerdan, y debe seguir siendo cierto durante la duración de la alianza. Por ejemplo, los términos establecidos en el escenario de riesgo podrían contener los siguientes predicados: 1. Los miembros de la alianza no se atacan entre sí en los territorios X, Y y Z;2. Los miembros de la alianza contribuirán con unidades C por turno para atacar el adversario AO;3. Los miembros están obligados a permanecer como parte de la alianza hasta el tiempo TK o hasta que el ejército de adversario sea más pequeño que Q. Los términos establecidos especifican restricciones entre grupos en cada uno de los miembros de la alianza (∀aal i ∈ Aal ⊆ a) conjunto de acciones cal i ⊆ C. Definición 6. Al Val: el valor de evaluación total que el Agente AI logrará mientras es parte de AAL es la suma de Evali (Eval para AI) de cada uno de los valores de evaluación AAL J después de tomar sus propias acciones α (a través del agente (α) predicado):Al Val (AI, Cal, AAL, W) = α∈Cal Evali (AAL J, Agente (α), W) Definición 7. AL TRH - es un número que representa un Al Val el sexto intl. Conf.en agentes autónomos y sistemas de múltiples agentes (AAMAS 07) umbral 553;Por encima de él, se puede decir que la alianza es una alianza altamente beneficiosa. El valor de Al TRH se calculará dinámicamente de acuerdo con el progreso de la interacción, como se puede ver en [7]. Después de que se forma una alianza, sus miembros ahora están trabajando en su entorno de confrontación normal, así como de acuerdo con los estados mentales y los axiomas necesarios para sus interacciones como parte de la alianza. El siguiente Modelo de Alianza (AL) especifica las condiciones bajo las cuales se puede decir que el grupo AAL está en una alianza y trabajando con un conjunto nuevo y limitado de acciones Cal, en el momento TN. Al (Aal, Cal, W, Tn) 1. AAL tiene un MB de que todos los miembros son parte de AAL: MB (AAL, (∀AAL I ∈ AAL) Miembro (AAL I, AAL), TN) 2. AAL tiene un MB de que se mantenga el grupo: MB (AAL, (∀aal i ∈ Aal) int.th (ai, miembro (ai, aal), tn, tn+1, co), tn) 3. AAL tiene un MB que ser miembros les da un alto valor de utilidad: MB (AAL, (∀aal i ∈ Aal) Al Val (AAL I, Cal, AAL, W) ≥ Al Trh, TN) Los perfiles de los miembros son una parte crucial de exitososalianzas. Suponemos que los agentes que tienen perfiles más precisos de sus adversarios serán más exitosos en tales entornos. Dichos agentes podrán predecir cuándo un miembro está a punto de violar el contrato de alianzas (ítem 2 en el modelo anterior) y tomar contrarrestaciones (cuando el ítem 3 se falsifique). La robustez de la alianza es en parte una función de la medida de confianza de sus miembros, la estimación de posición objetiva y otras propiedades del perfil. Debemos tener en cuenta que un agente puede ser parte de más de una alianza. Tal alianza temporal, donde los miembros del grupo no tienen un objetivo conjunto, sino que actúan en colaboración por el interés de sus propios objetivos individuales, se clasifica como un grupo de tratamiento por psicólogos modernos [12] (en contraste con un grupo de tareas, donde sus miembrostener un objetivo conjunto). El modelo de actividad compartida como se presenta en [5] el comportamiento del grupo de tratamiento modelado utilizando la misma formalización de planos compartidos. Al comparar ambas definiciones de una alianza y un grupo de tratamiento, encontramos una semejanza no sorprendente entre ambos modelos: las definiciones de los modelos ambientales son casi idénticos (ver definiciones SAS en [5]), y sus axiomas de acto egoístas y cooperativos se ajustan a nuestras adversarios adversoscomportamiento de los agentes. La principal distinción entre ambos modelos es la integración de un axioma de acto de comportamiento útil, en la actividad compartida que no puede ser parte de la nuestra. Este axioma establece que un agente considerará tomar medidas que reducirán su valor de evaluación (para cierto límite inferior), si cree que un socio grupal obtendrá un beneficio significativo. Tal comportamiento no puede ocurrir en un entorno adversario puro (como es un juego de suma cero), donde los miembros de la alianza están constantemente de vigilancia para manipular su alianza para su propio beneficio. A6. Axioma de maximización de evaluación. En un caso en el que todos los demás axiomas no sean aplicables, procederemos con la acción que maximiza el valor heurístico según lo calculado en la función EV.(∀Aag, Ao ∈ A, α ∈ CAG, TN, W ∈ W) Bel (AAG, (∀γ ∈ CAG) Eval (AAG, α, W) ≥ Ev (AAG, γ, W), TN) ⇒ Pot.Int.to (AAG, α, TN, Tα, W) T1. Optimización en eval = utilidad El modelo axiomático anterior maneja situaciones donde se desconoce la utilidad y los agentes son agentes racionales limitados. El siguiente teorema muestra que en las interacciones bilaterales, donde los agentes tienen la función de utilidad real (es decir, eval = utilidad) y son agentes racionales, los axiomas proporcionan el mismo resultado óptimo que la búsqueda adversaria clásica (por ejemplo, min-max). Teorema 1. Deje que AE AG sea un agente AE racional ilimitado que use la función de evaluación heurística Eval, AU AG sea el mismo agente que usa la función de utilidad verdadera y AO es un único adversario racional basado en utilidades no resistidos. Dado que eval = utilidad: (∀α ∈ Cau AG, α ∈ Cae AG, TN, W ∈ W) Pot.int.to (Au Ag, α, TN, Tα, W) → Pot.int.to (AE AG, α, Tn, Tα, W) ∧ ((α = α) ∨ (utilidad (Au Ag, α, W) = Eval (Ae AG, α, W))) Boceto de prueba, dado que Au AG tiene el realFunción de utilidad y recursos ilimitados, puede generar el árbol de juego completo y ejecutar el algoritmo MINMAX óptimo para elegir la acción de valor de utilidad más alto, que denotamos, α. La prueba mostrará que AE AG, utilizando los axiomas AE, seleccionará la misma utilidad α α (cuando hay más de una acción con la misma utilidad máxima) cuando Eval = Utility.(A1) Objetivo que logra el axioma: suponga que hay un α de tal manera que su finalización alcanzará el objetivo de AU AGS. Obtendrá la utilidad más alta por Min-Max para Au AG. El agente AE AG seleccionará α u otra acción con el mismo valor de utilidad a través de A1. Si dicho α no existe, AE AG no puede aplicar este axioma y procede a A2.(A2) Axioma de acto preventivo - (1) Mirando el caso básico (ver PROP1), si hay un β que lleva a AO a alcanzar su objetivo, entonces una acción preventiva α producirá la mayor utilidad para AU AG. AU AG lo elegirá a través de la utilidad, mientras que AE AG lo elegirá a través de A2.(2) En el caso general, β es una acción altamente beneficiosa para AO, por lo tanto, produce baja utilidad para AU AG, que lo guiará para seleccionar un α que evitará β, mientras que AE AG lo elegirá a través de A2.1β no existe para AO, entonces A2 no es aplicable y AE AG puede proceder a A3.(A3) Axioma de movimiento táctico subóptimo: cuando se usa una función de evaluación heurística, AE AG tiene una creencia parcial en el perfil de su adversario (ítem 4 en el modelo AE), lo que puede hacer que crea en las setacciones (PROP1). En nuestro caso, AE AG está manteniendo un perfil completo en su adversario óptimo y sabe que AO se comportará de manera óptima de acuerdo con los valores de utilidad reales en el árbol de búsqueda completo, por lo tanto, cualquier creencia sobre la setaction subóptima no puede existir, produciendo este axioma inaplicable. AE AG procederá a A4.(A4) Axioma de detección de perfil: dado que AE AG tiene el perfil completo de AO, ninguna de las acciones de AE AGS puede aumentar su conocimiento. Ese axioma no se aplicará, y el agente continuará con A6 (A5 se ignorará porque la interacción es bilateral).(A6) Axioma de maximización de la evaluación: este axioma seleccionará la evaluación máxima para AE AG. Dado que eval = utilidad, se seleccionará el mismo α que fue seleccionado por Au AG.3. Evaluación El objetivo principal de nuestro análisis experimental es evaluar el comportamiento y el rendimiento de los modelos en un entorno adversario real. Esta sección investiga si se limita 1 un caso en el que después de la finalización de β existe una γ que proporciona una alta utilidad para el agente AU AG, no puede ocurrir porque AO usa la misma utilidad, y la existencia de γs hará que clasifique β como una acción de baja utilidad.554 El sexto intl. Conf.En agentes autónomos y sistemas de múltiples agentes (AAMAS 07), los agentes racionales situados en tales entornos adversos serán mejor aplicando nuestros axiomas de comportamiento sugeridos.3.1 El dominio para explorar el uso del modelo anterior y sus axiomas de comportamiento, decidimos usar el juego Connect-Four como nuestro entorno adversario. Connect-Four es un juego de 2 jugadores, Zerosum que se juega con un tablero de 6x7 tipo matriz. Cada turno, un jugador deja caer un disco en una de las 7 columnas (el conjunto de 21 discos generalmente es de color amarillo para el jugador 1 y el rojo para el jugador 2; usaremos blanco y negro respectivamente para evitar confusión). El ganador es el primer jugador en completar un conjunto horizontal, vertical o diagonal de cuatro discos con su color. En ocasiones muy raras, el juego podría terminar en un empate si se llenan todas las cuadrículas vacías, pero ningún jugador logró crear un conjunto de 4 discos. El juego Connect-Four se resolvió en [1], donde se muestra que el primer jugador (jugando con los discos blancos) puede forzar una victoria comenzando en la columna central (columna 4) y jugando de manera óptima, la estrategia óptima esMuy complejo y difícil de seguir incluso para agentes racionales limitados complejos, como los jugadores humanos. Antes de que podamos continuar con el comportamiento del agente, primero debemos verificar que el dominio se ajuste a la definición de entornos adversos como se dio anteriormente (en los que se basan los axiomas conductuales). Primero, cuando juega un juego Connect-Four, el agente tiene la intención de ganar el juego (ítem 1). Segundo (ítem 2), nuestro agente cree que en Connect-Four solo puede haber un ganador (o ningún ganador en absoluto en la rara ocurrencia de un empate). Además, nuestro agente cree que su oponente al juego intentará ganar (ítem 3), y esperamos que tenga algún conocimiento parcial (ítem 4) sobre su adversario (este conocimiento puede variar de la nada, a través de hechos simples como la edad., a estrategias y debilidades). Por supuesto, no todos los encuentros de conexión de cuatro son adversos. Por ejemplo, cuando un padre está jugando con su hijo, la siguiente situación puede ocurrir: el niño, que tiene un fuerte incentivo para ganar, trata el medio ambiente como adversario (tiene la intención de ganar, entiende que solo puede haber un ganador,y cree que su padre está tratando de vencerlo). Sin embargo, el punto de vista de los padres podría ver el entorno como educativo, donde su objetivo no es ganar el juego, sino causar disfrute o practicar razonamiento estratégico. En un entorno educativo así, un nuevo conjunto de axiomas conductuales podría ser más beneficioso para los objetivos de los padres que nuestros axiomas conductuales adversos sugeridos.3.2 Análisis de axiomas Después de mostrar que el juego Connect-Four es de hecho un entorno adversario bilateral de suma cero, el siguiente paso es observar los comportamientos de los jugadores durante el juego y verificar si comportarse de acuerdo con nuestro modelo mejora el rendimiento. Para hacerlo, hemos recopilado archivos de registro de los juegos completos de Connect-Four que fueron jugados por jugadores humanos a través de Internet. Nuestros datos de archivo de registro recopilados vinieron de los sitios de juego por correo electrónico (PBEM). Estos son sitios web que alojan juegos de correo electrónico, donde cada movimiento es realizado por un intercambio de correo electrónico entre el servidor y los reproductores. Muchos de estos archivos de sitios contienen interacciones competitivas reales y también mantienen un sistema de clasificación para sus miembros. La mayoría de los datos que utilizamos se pueden encontrar en [6]. Como se puede aprender de [1], Connect-Four tiene una estrategia óptima y una ventaja considerable para el jugador que comienza el juego (que llamamos el jugador blanco). Nos concentraremos en nuestro análisis en los movimientos de los segundos jugadores (para ser llamados negros). El jugador blanco, siendo el primero en actuar, tiene la llamada ventaja de la iniciativa. Tener la ventaja y una buena estrategia mantendrá al jugador negro ocupado reaccionando a sus movimientos, en lugar de iniciar amenazas. Una amenaza es una combinación de tres discos del mismo color, con un lugar vacío para el cuarto disco ganador. Una amenaza abierta es una amenaza que se puede realizar en el oponente el próximo movimiento. Para que el jugador negro gane, de alguna manera debe girar el rumbo, aprovechar la ventaja y comenzar a presentar amenazas al jugador blanco. Exploraremos el comportamiento de los jugadores negros y su conformidad con nuestros axiomas. Para hacerlo, creamos una aplicación que lee archivos de registro y analiza los movimientos de los jugadores negros. La aplicación contiene dos componentes principales: (1) un algoritmo Min-Max para la evaluación de movimientos;(2) Detector de amenazas abiertas para descubrir amenazas abiertas. El algoritmo Min-Max funcionará a una profundidad dada, D y para cada movimiento α generará el valor heurístico para la siguiente acción tomada por el jugador como está escrito en el archivo de registro, H (α), junto con el valor heurístico máximo, Maxh(α), eso podría lograrse antes de tomar el movimiento (obviamente, si h (α) = maxH (α), entonces el jugador no hizo el movimiento óptimo heurísticamente). El trabajo de los detectores de amenazas es notificar si se tomaron algunas medidas para bloquear una amenaza abierta (no bloquear una amenaza abierta probablemente hará que el jugador pierda en el próximo movimiento de los oponentes). La función heurística utilizada por Min-Max para evaluar la utilidad de los jugadores es la siguiente función, que es simple de calcular, pero proporciona un desafío razonable para los oponentes humanos: la definición 8. Deje que el grupo sea un conjunto adyacente de cuatro cuadrados que son horizontales, verticales o diagonales. Groupn B (Groupn W) sea un grupo con n piezas del color negro (blanco) y cuadrados vacíos de 4 -n.h = ((grupo1 b ∗ α)+(grupo2 b ∗ β)+(grupo3 b ∗ γ)+(grupo4 b ∗ ∞ ∞)))∗ γ)+(grupo4 w ∗ ∞)) Los valores de α, β y δ pueden variar para formar cualquier combinación lineal deseada;Sin embargo, es importante valorarlos con el orden de α <β <δ (usamos 1, 4 y 8 como valores respectivos). Grupos de 4 discos del mismo color significa victoria, por lo tanto, el descubrimiento de dicho grupo dará como resultado ∞ para garantizar un valor extremo. Ahora utilizamos nuestra función de evaluación estimada para evaluar las acciones de los jugadores negros durante la interacción conecta y cuatro adversas. Cada juego del archivo de registro se ingresó en la aplicación, que procesó y emitió un archivo de registro reformateado que contiene el valor H del movimiento actual, el valor MAXH que podría lograrse y una notificación si se detectó una amenaza abierta. Se analizaron un total de 123 juegos (57 con blanco ganador y 66 con negro ganador). Algunos juegos adicionales se ignoraron manualmente en el experimento, debido a estos problemas: un jugador que abandona el juego mientras que el resultado no es final, o un movimiento irracional contundente en las primeras etapas del juego (por ejemplo, no bloquear un grupo ganador obvio enla primera apertura se mueve). Además, también se eliminó un solo juego de empate. El simulador se ejecutó a una profundidad de búsqueda de 3 movimientos. Ahora procedemos a analizar los juegos con respecto a cada axioma de comportamiento. El sexto intl. Conf.en agentes autónomos y sistemas de múltiples agentes (AAMAS 07) 555 Tabla 1: Análisis promedio de diferencia heurística Pérdidas negras Negras Won AVG MinH -17.62 -12.02 AVG 3 Movimientos H más bajos (Min3 H) -13.20 -8.70 3.2.1 Afirmación de la táctica suboptimalMover axioma La siguiente sección presenta las evaluaciones heurísticas del algoritmo Min-Max para cada acción, y verifica la cantidad y el alcance de las acciones tácticas subóptimas y sus implicaciones en el rendimiento. La Tabla 1 muestra los resultados y las ideas del análisis heurístico de los juegos, cuando la profundidad de búsqueda es igual a 3 (esta profundidad de búsqueda se seleccionó para que los resultados fueran comparables a [9], ver Sección 3.2.3). Los datos heurísticos de las tablas son la diferencia entre el valor heurístico máximo actual y el valor heurístico de la acción que finalmente tomó el jugador (es decir, cuanto más cerca esté el número de 0, más cerca estaba la acción a la acción heurística máxima). La primera fila presenta los valores de diferencia de la acción que tenían el valor de diferencia máxima entre todas las acciones de los jugadores negros en un juego dado, como se promedió sobre todos los negros ganadores y perdedores (ver columnas respectivas). En los juegos en los que el jugador negro pierde, su valor de diferencia promedio fue -17.62, mientras que en los juegos en los que ganó el jugador negro, su promedio fue -12.02. La segunda fila expande el análisis considerando las 3 acciones de diferencia heurística más altas y promediándolas. En ese caso, notamos una diferencia heurística promedio de 5 puntos entre los juegos que el jugador negro pierde y los juegos en los que gana. Sin embargo, la importancia de esos números es que nos permitieron tomar una suposición educada en un número umbral de 11.5, como el valor de la constante TRH, que diferencia entre acciones normales y las altamente beneficiosas. Después de encontrar una constante TRH aproximada, podemos continuar con un análisis de la importancia de los movimientos subóptimos. Para hacerlo, tomamos el subconjunto de juegos en los que el valor mínimo de diferencia heurística para las acciones de los negros fue de 11.5. Como se presenta en la Tabla 2, podemos ver el promedio Min3 H diferente de los 3 rangos más grandes y el porcentaje respectivo de juegos ganados. La primera fila muestra que el jugador negro ganó solo el 12% de los juegos en los que el promedio de sus 3 acciones heurísticamente de diferencia heurísticamente (min3 h) fue más pequeña que el umbral sugerido, TRH = 11.5. La segunda fila muestra un resultado sorprendente: parece que cuando Min3 H> −4 el jugador negro rara vez gana. La intuición sugeriría que los juegos en los que los valores de evaluación de la acción estaban más cerca de los valores máximos darán como resultado más juegos ganadores para Black. Sin embargo, parece que en el dominio Connect-Four, simplemente responder con acciones algo fácilmente esperadas, sin iniciar algunos movimientos sorprendentes y subóptimos, no produce buenos resultados. La última fila resume las ideas principales del análisis;La mayoría de los negros victorias (83%) llegaron cuando su min3 h estaba en el rango de -11.5 a -4. Una inspección minuciosa de esos juegos ganadores negros muestra el siguiente patrón detrás de los números: después de los movimientos de apertura estándar, Black de repente deja un disco en una columna aislada, lo que parece un desperdicio de un movimiento. White continúa construyendo sus amenazas, mientras que generalmente ignora el último movimiento de los negros, que a su vez usa el disco aislado como un ancla para una futura amenaza ganadora. Los resultados muestran que fue beneficioso para la Tabla 2 de los jugadores negros: porcentajes de ganancias de los negros% de los juegos min3 H <−11.5 12% min3 h> −4 5% −11.5 ≤ min3 h ≤ −4 83% para tomar acciones subóptimas y noDé el valor heurístico más alto posible actual, pero no será demasiado dañino para su posición (es decir, no le dará un alto valor beneficioso a su adversario). Al final resultó que, aprender el umbral es un aspecto importante del éxito: tomar movimientos muy riesgosos (Min3 H <-11.5) o tratar de evitarlos (Min3 H> −4) reduce a los jugadores negros ganando oportunidades por un gran margen.3.2.2 Afirmando el axioma de monitoreo de perfil en la tarea de mostrar la importancia de monitorear los perfiles adversarios, nuestros archivos de registro no pudieron usarse porque no contenían interacciones repetidas entre los jugadores, que son necesarios para inferir el conocimiento de los jugadores sobre sus adversarios. Sin embargo, la importancia del modelado del oponente y su uso para lograr ventajas tácticas ya se estudió en varios dominios ([3, 9] son buenos ejemplos). En un artículo reciente, Markovitch y Reger [9] exploraron la noción de aprendizaje y explotación de la debilidad del oponente en las interacciones competitivas. Aplican estrategias de aprendizaje simples analizando ejemplos de interacciones pasadas en un dominio específico. También utilizaron el dominio de adversario Connect-Four, que ahora se puede utilizar para comprender la importancia de monitorear el perfil de adversario. Después de la presentación de su modelo teórico, describen un estudio empírico extenso y verifican el rendimiento de los agentes después de aprender el modelo de debilidad con ejemplos pasados. Uno de los dominios utilizados como un entorno competitivo fue el mismo juego de Connect-Four (Checkers fue el segundo dominio). Su función heurística era idéntica a la nuestra con tres variaciones diferentes (H1, H2 y H3) que se distinguen entre sí en sus valores de coeficientes de combinación lineal. La profundidad de búsqueda para los jugadores fue de 3 (como en nuestro análisis). Sus extensos experimentos verifican y comparan diversas estrategias de aprendizaje, factores de riesgo, conjuntos de características predefinidas y métodos de uso. La conclusión es que el dominio Connect-Four muestra una mejora de una tasa ganadora de 0.556 antes de modelar a un 0.69 después del modelado (página 22). Sus conclusiones, que muestran un rendimiento mejorado al mantener y usar el modelo de adversario, justifican el esfuerzo para monitorear el perfil adversario para interacciones continuas y repetidas. Un punto adicional que surgió en sus experimentos es el siguiente: después de que se ha aprendido el modelo de debilidad del oponente, los autores describen diferentes métodos para integrar el modelo de debilidad del oponente en la estrategia de decisión de los agentes. Sin embargo, independientemente del método específico con el que eligieron trabajar, todos los métodos de integración pueden hacer que el agente tome decisiones subóptimas;Puede hacer que el agente prefiera acciones que sean subóptimas en la unión de decisión actual, pero que puede hacer que el oponente reaccione de acuerdo con su modelo de debilidad (como lo representa nuestro agente), lo que a su vez será beneficioso para nosotros en el futuro. El comportamiento de los agentes, como se demostró en [9], confirma y fortalece aún más nuestro axioma táctico subóptimo como se discutió en la sección anterior.556 El sexto intl. Conf.En agentes autónomos y sistemas de múltiples agentes (AAMAS 07) 3.2.3 Contensores adicionales La necesidad de lograr el objetivo, el acto preventivo y los axiomas de maximización de la evaluación son obvios y no necesitan más verificación. Sin embargo, incluso con respecto a esos axiomas, surgieron algunas ideas interesantes en el análisis de registro. Los axiomas de acto preventivo y logros preventivos, aunque teóricamente triviales, parecen proporcionar algún desafío a un jugador humano. En la inspección inicial de los registros, encontramos pocos juegos2 donde un jugador, por razones inexplicables, no bloqueó al otro para ganar o no pudo ejecutar su propio movimiento ganador. Podemos culpar a esas fallas a la falta de atención de los humanos, o un error de escritura en su respuesta de movimiento;Sin embargo, esos errores pueden ocurrir en agentes racionales limitados, y el comportamiento apropiado debe ser axiomatizado. Un juego típico de Connect Four gira en torno a generar amenazas y bloquearlas. En nuestro análisis, buscamos acciones preventivas explícitas, es decir, se mueven que bloquean un grupo de 3 discos, o que eliminan una amenaza futura (en nuestro horizonte de búsqueda limitado). Descubrimos que en el 83% del total de juegos había al menos una acción preventiva tomada por el jugador negro. También se descubrió que Black promedió 2.8 acciones preventivas por juego en los juegos en los que perdió, mientras promedió 1.5 acciones preventivas por juego al ganar. Parece que el negro requiere 1 o 2 acciones preventivas para construir su posición inicial, antes de comenzar a presentar amenazas. Si no logró ganar, generalmente evitará una amenaza adicional o dos antes de sucumbir al blanco.4. Trabajo relacionado Mucha investigación se ocupa de la axiomatización del trabajo en equipo y los estados mentales de las personas: algunos modelos usan conocimiento y creencias [10], otros tienen modelos de objetivos e intenciones [8, 4]. Sin embargo, todas estas teorías formales tratan con el trabajo en equipo del agente y la cooperación. Hasta donde sabemos, nuestro modelo es el primero en proporcionar un modelo formalizado para entornos adversos explícitos y comportamiento de los agentes en él. El algoritmo de búsqueda adversario de Min-Max clásico fue el primer intento de integrar al oponente en el espacio de búsqueda con una suposición débil de un oponente que juega de manera óptima. Desde entonces, se ha realizado mucho esfuerzo para integrar el modelo de oponente en el procedimiento de decisión para predecir el comportamiento futuro. El algoritmo M ∗ presentado por Carmel y Markovitch [2] mostró un método para incorporar modelos oponentes en la búsqueda adversaria, mientras que en [3] utilizaron el aprendizaje para proporcionar un modelo oponente más preciso en un entorno de juego repetido de 2 jugadores, donde las estrategias de los agentes fueron modeladoscomo autómatas finitos. Willmott et al. Realizaron un trabajo adicional de planificación adversaria.[13], que proporcionó un enfoque de planificación adversa al juego de GO. La investigación mencionada anteriormente trató la búsqueda adversaria y la integración de los modelos oponentes en métodos de búsqueda basados en servicios públicos clásicos. Ese trabajo muestra la importancia del modelado del oponente y la capacidad de explotarlo a una ventaja de los agentes. Sin embargo, las limitaciones básicas de esos métodos de búsqueda aún se aplican;Nuestro modelo trata de superar esas limitaciones presentando un modelo formal para una nueva especificación adversaria basada en el estado mental.5. Conclusiones Presentamos un modelo de entorno adversario para un 2 Estas luego se eliminaron del análisis final.Agente racional limitado que está situado en un entorno N-Relayer, Zerosum. Utilizamos la formalización de Plans Shared para definir el modelo y los axiomas que los agentes pueden aplicar como pautas de comportamiento. El modelo está destinado a usarse como una guía para diseñar agentes que necesitan operar en tales entornos adversos. Presentamos resultados empíricos, basados en el análisis de archivos de registro de ConnectFour, que ejemplifican el modelo y los axiomas para una instancia bilateral del entorno. Los resultados que presentamos son un primer paso hacia un modelo ampliado que cubrirá todos los tipos de entornos adversos, por ejemplo, entornos que no son de cero y entornos que contienen agentes naturales que no forman parte del conflicto directo. Esos desafíos y más se tratarán en futuras investigaciones.6. Reconocimiento Esta investigación fue apoyada en parte por Israel Science Foundation Otorges #1211/04 y #898/05.7. Referencias [1] L. V. Allis. Un enfoque basado en el conocimiento de Connect-Four: el juego está resuelto: White Wins. Tesis de Masters, Free University, Amsterdam, Países Bajos, 1988. [2] D. Carmel y S. Markovitch. Incorporando modelos oponentes en la búsqueda adversaria. En Actas de la Decimotercera Conferencia Nacional sobre Inteligencia Artificial, páginas 120-125, Portland, OR, 1996. [3] D. Carmel y S. Markovitch. Modelado del oponente en sistemas de múltiples agentes. En G. Weiß y S. Sen, editores, adaptación y aprendizaje en sistemas de múltiples agentes, páginas 40-52. Springer-Verlag, 1996. [4] B. J. Grosz y S. Kraus. Planes de colaboración para una acción grupal compleja. Inteligencia Artificial, 86 (2): 269-357, 1996. [5] M. Hadad, G. Kaminka, G. Armon y S. Kraus. Apoyo a la actividad colaborativa. En Proc.de AAAI-2005, páginas 83-88, Pittsburgh, 2005. [6] http://www.gamerz.net/ightpbmserv/.[7] S. Kraus y D. Lehmann. Diseño y construcción de un agente automatizado de negociación. Computational Intelligence, 11: 132-171, 1995. [8] H. J. Levesque, P. R. Cohen y J. H. T. Nunes. Al actuar juntos. En Proc.de AAAI-90, páginas 94-99, Boston, MA, 1990. [9] S. Markovitch y R. Reger. Aprender y explotar las debilidades relativas de los agentes oponentes. Agentes autónomos y sistemas de múltiples agentes, 10 (2): 103-130, 2005. [10] Y. M. Ronald Fagin, Joseph Y. Halpern y M. Y. Vardi. Razonamiento sobre el conocimiento. MIT Press, Cambridge, Mass., 1995. [11] P. Thagard. Resolución de problemas adversos: modelar un oponente usando coherencia explicativa. Cognitive Science, 16 (1): 123-149, 1992. [12] R. W. Toseland y R. F. Rivas. Una introducción a la práctica laboral grupal. Prentice Hall, Englewood Cliffs, NJ, 2nd Edition Edition, 1995. [13] S. Willmott, J. Richardson, A. Bundy y J. Levine. Un enfoque de planificación adversa para ir. Notas de conferencia en informática, 1558: 93-112, 1999. El sexto intl. Conf.en agentes autónomos y sistemas de múltiples agentes (AAMAS 07) 557",
    "original_sentences": [
        "An Adversarial Environment Model for Bounded Rational Agents in Zero-Sum Interactions Inon Zuckerman1 , Sarit Kraus1 , Jeffrey S. Rosenschein2 , Gal Kaminka1 1 Department of Computer Science 2 The School of Engineering Bar-Ilan University and Computer Science Ramat-Gan, Israel Hebrew University, Jerusalem, Israel {zukermi,sarit,galk}@cs.biu.ac.il jeff@cs.huji.ac.il ABSTRACT Multiagent environments are often not cooperative nor collaborative; in many cases, agents have conflicting interests, leading to adversarial interactions.",
        "This paper presents a formal Adversarial Environment model for bounded rational agents operating in a zero-sum environment.",
        "In such environments, attempts to use classical utility-based search methods can raise a variety of difficulties (e.g., implicitly modeling the opponent as an omniscient utility maximizer, rather than leveraging a more nuanced, explicit opponent model).",
        "We define an Adversarial Environment by describing the mental states of an agent in such an environment.",
        "We then present behavioral axioms that are intended to serve as design principles for building such adversarial agents.",
        "We explore the application of our approach by analyzing log files of completed Connect-Four games, and present an empirical analysis of the axioms appropriateness.",
        "Categories and Subject Descriptors I.2.11 [Artificial Intelligence]: Distributed Artificial Intelligence-Intelligent agents,Multiagent Systems; I.2.4 [Artificial Intelligence]: Knowledge Representation Formalisms and Methods -Modal logic General Terms Design, Theory 1.",
        "INTRODUCTION Early research in multiagent systems (MAS) considered cooperative groups of agents; because individual agents had limited resources, or limited access to information (e.g., limited processing power, limited sensor coverage), they worked together by design to solve problems that individually they could not solve, or at least could not solve as efficiently.",
        "MAS research, however, soon began to consider interacting agents with individuated interests, as representatives of different humans or organizations with non-identical interests.",
        "When interactions are guided by diverse interests, participants may have to overcome disagreements, uncooperative interactions, and even intentional attempts to damage one another.",
        "When these types of interactions occur, environments require appropriate behavior from the agents situated in them.",
        "We call these environments Adversarial Environments, and call the clashing agents Adversaries.",
        "Models of cooperation and teamwork have been extensively explored in MAS through the axiomatization of mental states (e.g., [8, 4, 5]).",
        "However, none of this research dealt with adversarial domains and their implications for agent behavior.",
        "Our paper addresses this issue by providing a formal, axiomatized mental state model for a subset of adversarial domains, namely simple zero-sum adversarial environments.",
        "Simple zero-sum encounters exist of course in various twoplayer games (e.g., Chess, Checkers), but they also exist in n-player games (e.g., Risk, Diplomacy), auctions for a single good, and elsewhere.",
        "In these latter environments especially, using a utility-based adversarial search (such as the Min-Max algorithm) does not always provide an adequate solution; the payoff function might be quite complex or difficult to quantify, and there are natural computational limitations on bounded rational agents.",
        "In addition, traditional search methods (like Min-Max) do not make use of a model of the opponent, which has proven to be a valuable addition to adversarial planning [9, 3, 11].",
        "In this paper, we develop a formal, axiomatized model for bounded rational agents that are situated in a zero-sum adversarial environment.",
        "The model uses different modality operators, and its main foundations are the SharedPlans [4] model for collaborative behavior.",
        "We explore environment properties and the mental states of agents to derive behavioral axioms; these behavioral axioms constitute a formal model that serves as a specification and design guideline for agent design in such settings.",
        "We then investigate the behavior of our model empirically using the Connect-Four board game.",
        "We show that this game conforms to our environment definition, and analyze players behavior using a large set of completed match log 550 978-81-904262-7-5 (RPS) c 2007 IFAAMAS files.",
        "In addition, we use the results presented in [9] to discuss the importance of opponent modeling in our ConnectFour adversarial domain.",
        "The paper proceeds as follows.",
        "Section 2 presents the models formalization.",
        "Section 3 presents the empirical analysis and its results.",
        "We discuss related work in Section 4, and conclude and present future directions in Section 5. 2.",
        "ADVERSARIAL ENVIRONMENTS The adversarial environment model (denoted as AE) is intended to guide the design of agents by providing a specification of the capabilities and mental attitudes of an agent in an adversarial environment.",
        "We focus here on specific types of adversarial environments, specified as follows: 1.",
        "Zero-Sum Interactions: positive and negative utilities of all agents sum to zero; 2.",
        "Simple AEs: all agents in the environment are adversarial agents; 3.",
        "Bilateral AEs: AEs with exactly two agents; 4.",
        "Multilateral AEs: AEs of three or more agents.",
        "We will work on both bilateral and multilateral instantiations of zero-sum and simple environments.",
        "In particular, our adversarial environment model will deal with interactions that consist of N agents (N ≥ 2), where all agents are adversaries, and only one agent can succeed.",
        "Examples of such environments range from board games (e.g., Chess, Connect-Four, and Diplomacy) to certain economic environments (e.g., N-bidder auctions over a single good). 2.1 Model Overview Our approach is to formalize the mental attitudes and behaviors of a single adversarial agent; we consider how a single agent perceives the AE.",
        "The following list specifies the conditions and mental states of an agent in a simple, zero-sum AE: 1.",
        "The agent has an individual intention that its own goal will be completed; 2.",
        "The agent has an individual belief that it and its adversaries are pursuing full conflicting goals (defined below)there can be only one winner; 3.",
        "The agent has an individual belief that each adversary has an intention to complete its own full conflicting goal; 4.",
        "The agent has an individual belief in the (partial) profile of its adversaries.",
        "Item 3 is required, since it might be the case that some agent has a full conflicting goal, and is currently considering adopting the intention to complete it, but is, as of yet, not committed to achieving it.",
        "This might occur because the agent has not yet deliberated about the effects that adopting that intention might have on the other intentions it is currently holding.",
        "In such cases, it might not consider itself to even be in an adversarial environment.",
        "Item 4 states that the agent should hold some belief about the profiles of its adversaries.",
        "The profile represents all the knowledge the agent has about its adversary: its weaknesses, strategic capabilities, goals, intentions, trustworthiness, and more.",
        "It can be given explicitly or can be learned from observations of past encounters. 2.2 Model Definitions for Mental States We use Grosz and Krauss definitions of the modal operators, predicates, and meta-predicates, as defined in their SharedPlan formalization [4].",
        "We recall here some of the predicates and operators that are used in that formalization: Int.To(Ai, α, Tn, Tα, C) represents Ais intentions at time Tn to do an action α at time Tα in the context of C. Int.Th(Ai, prop, Tn, Tprop, C) represents Ais intentions at time Tn that a certain proposition prop holds at time Tprop in the context of C. The potential intention operators, Pot.Int.To(...) and Pot.Int.Th(...), are used to represent the mental state when an agent considers adopting an intention, but has not deliberated about the interaction of the other intentions it holds.",
        "The operator Bel(Ai, f, Tf ) represents agent Ai believing in the statement expressed in formula f, at time Tf .",
        "MB(A, f, Tf ) represents mutual belief for a group of agents A.",
        "A snapshot of the system finds our environment to be in some state e ∈ E of environmental variable states, and each adversary in any LAi ∈ L of possible local states.",
        "At any given time step, the system will be in some world w of the set of all possible worlds w ∈ W, where w = E×LA1 ×LA2 × ...LAn , and n is the number of adversaries.",
        "For example, in a Texas Holdem poker game, an agents local state might be its own set of cards (which is unknown to its adversary) while the environment will consist of the betting pot and the community cards (which are visible to both players).",
        "A utility function under this formalization is defined as a mapping from a possible world w ∈ W to an element in , which expresses the desirability of the world, from a single agent perspective.",
        "We usually normalize the range to [0,1], where 0 represents the least desirable possible world, and 1 is the most desirable world.",
        "The implementation of the utility function is dependent on the domain in question.",
        "The following list specifies new predicates, functions, variables, and constants used in conjunction with the original definitions for the adversarial environment formalization: 1. φ is a null action (the agent does not do anything). 2.",
        "GAi is the set of agent Ais goals.",
        "Each goal is a set of predicates whose satisfaction makes the goal complete (we use G∗ Ai ∈ GAi to represent an arbitrary goal of agent Ai). 3. gAi is the set of agent Ais subgoals.",
        "Subgoals are predicates whose satisfaction represents an important milestone toward achievement of the full goal. gG∗ Ai ⊆ gAi is the set of subgoals that are important to the completion of goal G∗ Ai (we will use g∗ G∗ Ai ∈ gG∗ Ai to represent an arbitrary subgoal). 4.",
        "P Aj Ai is the profile object agent Ai holds about agent Aj. 5.",
        "CA is a general set of actions for all agents in A which are derived from the environments constraints.",
        "CAi ⊆ CA is the set of agent Ais possible actions. 6.",
        "Do(Ai, α, Tα, w) holds when Ai performs action α over time interval Tα in world w. 7.",
        "Achieve(G∗ Ai , α, w) is true when goal G∗ Ai is achieved following the completion of action α in world w ∈ W, where α ∈ CAi . 8.",
        "Profile(Ai, PAi Ai ) is true when agent Ai holds an object profile for agent Aj.",
        "Definition 1.",
        "Full conflict (FulConf ) describes a zerosum interaction where only a single goal of the goals in conflict can be completed.",
        "FulConf(G∗ Ai , G∗ Aj ) ⇒ (∃α ∈ CAi , ∀w, β ∈ CAj ) (Achieve(G∗ Ai , α, w) ⇒ ¬Achieve(G∗ Aj , β, w)) ∨ (∃β ∈ CAj , ∀w, α ∈ CAi )(Achieve(G∗ Aj , β, w) ⇒ ¬Achieve(G∗ Ai , α, w)) Definition 2.",
        "Adversarial Knowledge (AdvKnow) is a function returning a value which represents the amount of The Sixth Intl.",
        "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) 551 knowledge agent Ai has on the profile of agent Aj, at time Tn.",
        "The higher the value, the more knowledge agent Ai has.",
        "AdvKnow : P Aj Ai × Tn → Definition 3.",
        "Eval - This evaluation function returns an estimated expected utility value for an agent in A, after completing an action from CA in some world state w. Eval : A × CA × w → Definition 4.",
        "TrH - (Threshold) is a numerical constant in the [0,1] range that represents an evaluation function (Eval) threshold value.",
        "An action that yields an estimated utility evaluation above the TrH is regarded as a highly beneficial action.",
        "The Eval value is an estimation and not the real utility function, which is usually unknown.",
        "Using the real utility value for a rational agent would easily yield the best outcome for that agent.",
        "However, agents usually do not have the real utility functions, but rather a heuristic estimate of it.",
        "There are two important properties that should hold for the evaluation function: Property 1.",
        "The evaluation function should state that the most desirable world state is one in which the goal is achieved.",
        "Therefore, after the goal has been satisfied, there can be no future action that can put the agent in a world state with higher Eval value. (∀Ai, G∗ Ai , α, β ∈ CAi , w ∈ W) Achieve(G∗ Ai , α, w) ⇒ Eval(Ai, α, w) ≥ Eval(Ai, β, w) Property 2.",
        "The evaluation function should project an action that causes a completion of a goal or a subgoal to a value which is greater than TrH (a highly beneficial action). (∀Ai, G∗ Ai ∈ GAi , α ∈ CAi , w ∈ W, g∗ GAi ∈ gGAi ) Achieve(G∗ Ai , α, w) ∨ Achieve(g∗ GAi , α, w) ⇒ Eval(Ai, α, w) ≥ TrH.",
        "Definition 5.",
        "SetAction We define a set action (SetAction) as a set of action operations (either complex or basic actions) from some action sets CAi and CAj which, according to agent Ais belief, are attached together by a temporal and consequential relationship, forming a chain of events (action, and its following consequent action). (∀α1 , . . . , αu ∈ CAi , β1 , . . . , βv ∈ CAj , w ∈ W) SetAction(α1 , . . . , αu , β1 , . . . , βv , w) ⇒ ((Do(Ai, α1 , Tα1 , w) ⇒ Do(Aj, β1 , Tβ1 , w)) ⇒ Do(Ai, α2 , Tα2 , w) ⇒ . . . ⇒ Do(Ai, αu , Tαu , w)) The consequential relation might exist due to various environmental constraints (when one action forces the adversary to respond with a specific action) or due to the agents knowledge about the profile of its adversary.",
        "Property 3.",
        "As the knowledge we have about our adversary increases we will have additional beliefs about its behavior in different situations which in turn creates new set actions.",
        "Formally, if our AdvKnow at time Tn+1 is greater than AdvKnow at time Tn, then every SetAction known at time Tn is also known at time Tn+1.",
        "AdvKnow(P Aj Ai , Tn+1) > AdvKnow(P Aj Ai , Tn) ⇒ (∀α1 , . . . , αu ∈ CAi , β1 , . . . , βv ∈ CAj ) Bel(Aag, SetAction(α1 , . . . , αu , β1 , . . . , βv ), Tn) ⇒ Bel(Aag, SetAction(α1 , . . . , αu , β1 , . . . , βv ), Tn+1) 2.3 The Environment Formulation The following axioms provide the formal definition for a simple, zero-sum Adversarial Environment (AE).",
        "Satisfaction of these axioms means that the agent is situated in such an environment.",
        "It provides specifications for agent Aag to interact with its set of adversaries A with respect to goals G∗ Aag and G∗ A at time TCo at some world state w. AE(Aag, A, G∗ Aag , A1, . . . , Ak, G∗ A1 , . . . , G∗ Ak , Tn, w) 1.",
        "Aag has an Int.Th his goal would be completed: (∃α ∈ CAag , Tα) Int.Th(Aag, Achieve(G∗ Aag , α), Tn, Tα, AE) 2.",
        "Aag believes that it and each of its adversaries Ao are pursuing full conflicting goals: (∀Ao ∈ {A1, . . . , Ak}) Bel(Aag, FulConf(G∗ Aag , G∗ Ao ), Tn) 3.",
        "Aag believes that each of his adversaries in Ao has the Int.Th his conflicting goal G∗ Aoi will be completed: (∀Ao ∈ {A1, . . . , Ak})(∃β ∈ CAo , Tβ) Bel(Aag, Int.Th(Ao, Achieve(G∗ Ao , β), TCo, Tβ, AE), Tn) 4.",
        "Aag has beliefs about the (partial) profiles of its adversaries (∀Ao ∈ {A1, . . . , Ak}) (∃PAo Aag ∈ PAag )Bel(Aag, Profile(Ao, PAo Aag ), Tn) To build an agent that will be able to operate successfully within such an AE, we must specify behavioral guidelines for its interactions.",
        "Using a naive Eval maximization strategy to a certain search depth will not always yield satisfactory results for several reasons: (1) the search horizon problem when searching for a fixed depth; (2) the strong assumption of an optimally rational, unbounded resources adversary; (3) using an estimated evaluation function which will not give optimal results in all world states, and can be exploited [9].",
        "The following axioms specify the behavioral principles that can be used to differentiate between successful and less successful agents in the above Adversarial Environment.",
        "Those axioms should be used as specification principles when designing and implementing agents that should be able to perform well in such Adversarial Environments.",
        "The behavioral axioms represent situations in which the agent will adopt potential intentions to (Pot.Int.To(...)) perform an action, which will typically require some means-end reasoning to select a possible course of action.",
        "This reasoning will lead to the adoption of an Int.To(...) (see [4]).",
        "A1.",
        "Goal Achieving Axiom.",
        "The first axiom is the simplest case; when the agent Aag believes that it is one action (α) away from achieving his conflicting goal G∗ Aag , it should adopt the potential intention to do α and complete its goal. (∀Aag, α ∈ CAag , Tn, Tα, w ∈ W) (Bel(Aag, Do(Aag, α, Tα, w) ⇒ Achieve(G∗ Aag , α, w)) ⇒ Pot.Int.To(Aag, α, Tn, Tα, w) This somewhat trivial behavior is the first and strongest axiom.",
        "In any situation, when the agent is an action away from completing the goal, it should complete the action.",
        "Any fair Eval function would naturally classify α as the maximal value action (property 1).",
        "However, without explicit axiomatization of such behavior there might be situations where the agent will decide on taking another action for various reasons, due to its bounded decision resources.",
        "A2.",
        "Preventive Act Axiom.",
        "Being in an adversarial situation, agent Aag might decide to take actions that will damage one of its adversarys plans to complete its goal, even if those actions do not explicitly advance Aag towards its conflicting goal G∗ Aag .",
        "Such preventive action will take place when agent Aag has a belief about the possibility of its adversary Ao doing an action β that will give it a high 552 The Sixth Intl.",
        "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) utility evaluation value (> TrH).",
        "Believing that taking action α will prevent the opponent from doing its β, it will adopt a potential intention to do α. (∀Aag, Ao ∈ A, α ∈ CAag , β ∈ CAo , Tn, Tβ, w ∈ W) (Bel(Aag, Do(Ao, β, Tβ, w) ∧ Eval(Ao, β, w) > TrH, Tn) ∧ Bel(Aag, Do(Aag, α, Tα, w) ⇒ ¬Do(Ao, β, Tβ, w), Tn) ⇒ Pot.Int.To(Aag, α, Tn, Tα, w) This axiom is a basic component of any adversarial environment.",
        "For example, looking at a Chess board game, a player could realize that it is about to be checkmated by its opponent, thus making a preventive move.",
        "Another example is a Connect Four game: when a player has a row of three chips, its opponent must block it, or lose.",
        "A specific instance of A1 occurs when the adversary is one action away from achieving its goal, and immediate preventive action needs to be taken by the agent.",
        "Formally, we have the same beliefs as stated above, with a changed belief that doing action β will cause agent Ao to achieve its goal.",
        "Proposition 1: Prevent or lose case. (∀Aag, Ao ∈ A, α ∈ CAag , β ∈ CAo , G∗ Ao , Tn, Tα, Tβ, w ∈ W) Bel(Aag, Do(Ao, β, Tβ, w) ⇒ Achieve(G∗ Ao , β, w), Tn) ∧ Bel(Aag, Do(Aag, α, Tα, w) ⇒ ¬Do(Ao, β, Tβ, w)) ⇒ Pot.Int.To(Aag, α, Tn, Tα, w) Sketch of proof: Proposition 1 can be easily derived from axiom A1 and the property 2 of the Eval function, which states that any action that causes a completion of a goal is a highly beneficial action.",
        "The preventive act behavior will occur implicitly when the Eval function is equal to the real world utility function.",
        "However, being bounded rational agents and dealing with an estimated evaluation function we need to explicitly axiomatize such behavior, for it will not always occur implicitly from the evaluation function.",
        "A3.",
        "Suboptimal Tactical Move Axiom.",
        "In many scenarios a situation may occur where an agent will decide not to take the current most beneficial action it can take (the action with the maximal utility evaluation value), because it believes that taking another action (with lower utility evaluation value) might yield (depending on the adversarys response) a future possibility for a highly beneficial action.",
        "This will occur most often when the Eval function is inaccurate and differs by a large extent from the Utility function.",
        "Put formally, agent Aag believes in a certain SetAction that will evolve according to its initial action and will yield a high beneficial value (> TrH) solely for it. (∀Aag, Ao ∈ A, Tn, w ∈ W) (∃α1 , . . . , αu ∈ CAi , β1 , . . . , βv ∈ CAj , Tα1 ) Bel(Aag, SetAction(α1 , . . . , αu , β1 , . . . , βv ), Tn) ∧ Bel(Aag, Eval(Ao, βv , w) < TrH < Eval(Aag, αu , w), Tn) ⇒ Pot.Int.To(Aag, α1 , Tn, Tα1 , w) An agent might believe that a chain of events will occur for various reasons due to the inevitable nature of the domain.",
        "For example, in Chess, we often observe the following: a move causes a check position, which in turn limits the opponents moves to avoiding the check, to which the first player might react with another check, and so on.",
        "The agent might also believe in a chain of events based on its knowledge of its adversarys profile, which allows it to foresee the adversarys movements with high accuracy.",
        "A4.",
        "Profile Detection Axiom.",
        "The agent can adjust its adversarys profiles by observations and pattern study (specifically, if there are repeated encounters with the same adversary).",
        "However, instead of waiting for profile information to be revealed, an agent can also initiate actions that will force its adversary to react in a way that will reveal profile knowledge about it.",
        "Formally, the axiom states that if all actions (γ) are not highly beneficial actions (< TrH), the agent can do action α in time Tα if it believes that it will result in a non-highly beneficial action β from its adversary, which in turn teaches it about the adversarys profile, i.e., gives a higher AdvKnow(P Aj Ai , Tβ). (∀Aag, Ao ∈ A, α ∈ CAag , β ∈ CAo , Tn, Tα, Tβ, w ∈ W) Bel(Aag, (∀γ ∈ CAag )Eval(Aag, γ, w) < TrH, Tn) ∧ Bel(Aag, Do(Aag, α, Tα, w) ⇒ Do(Ao, β, Tβ, w), Tn) ∧ Bel(Aag, Eval(Ao, β, w) < TrH) ∧ Bel(Aag, AdvKnow(P Aj Ai , Tβ) > AdvKnow(P Aj Ai , Tn), Tn) ⇒ Pot.Int.To(Aag, α, Tn, Tα, w) For example, going back to the Chess board game scenario, consider starting a game versus an opponent about whom we know nothing, not even if it is a human or a computerized opponent.",
        "We might start playing a strategy that will be suitable versus an average opponent, and adjust our game according to its level of play.",
        "A5.",
        "Alliance Formation Axiom The following behavioral axiom is relevant only in a multilateral instantiation of the adversarial environment (obviously, an alliance cannot be formed in a bilateral, zero-sum encounter).",
        "In different situations during a multilateral interaction, a group of agents might believe that it is in their best interests to form a temporary alliance.",
        "Such an alliance is an agreement that constrains its members behavior, but is believed by its members to enable them to achieve a higher utility value than the one achievable outside of the alliance.",
        "As an example, we can look at the classical Risk board game, where each player has an individual goal of being the sole conquerer of the world, a zero-sum game.",
        "However, in order to achieve this goal, it might be strategically wise to make short-term ceasefire agreements with other players, or to join forces and attack an opponent who is stronger than the rest.",
        "An alliances terms defines the way its members should act.",
        "It is a set of predicates, denoted as Terms, that is agreed upon by the alliance members, and should remain true for the duration of the alliance.",
        "For example, the set Terms in the Risk scenario, could contain the following predicates: 1.",
        "Alliance members will not attack each other on territories X, Y and Z; 2.",
        "Alliance members will contribute C units per turn for attacking adversary Ao; 3.",
        "Members are obligated to stay as part of the alliance until time Tk or until adversarys Ao army is smaller than Q.",
        "The set Terms specifies inter-group constraints on each of the alliance members (∀Aal i ∈ Aal ⊆ A) set of actions Cal i ⊆ C. Definition 6.",
        "Al val - the total evaluation value that agent Ai will achieve while being part of Aal is the sum of Evali (Eval for Ai) of each of Aal j Eval values after taking their own α actions (via the agent(α) predicate): Al val(Ai, Cal , Aal , w) = α∈Cal Evali(Aal j , agent(α), w) Definition 7.",
        "Al TrH - is a number representing an Al val The Sixth Intl.",
        "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) 553 threshold; above it, the alliance can be said to be a highly beneficial alliance.",
        "The value of Al TrH will be calculated dynamically according to the progress of the interaction, as can be seen in [7].",
        "After an alliance is formed, its members are now working in their normal adversarial environment, as well as according to the mental states and axioms required for their interactions as part of the alliance.",
        "The following Alliance model (AL) specifies the conditions under which the group Aal can be said to be in an alliance and working with a new and constrained set of actions Cal , at time Tn.",
        "AL(Aal , Cal , w, Tn) 1.",
        "Aal has a MB that all members are part of Aal : MB(Aal , (∀Aal i ∈ Aal )member(Aal i , Aal ), Tn) 2.",
        "Aal has a MB that the group be maintained: MB(Aal , (∀Aal i ∈ Aal )Int.Th (Ai, member(Ai, Aal ), Tn, Tn+1, Co), Tn) 3.",
        "Aal has a MB that being members gives them high utility value: MB(Aal , (∀Aal i ∈ Aal )Al val(Aal i , Cal , Aal , w) ≥ Al TrH, Tn) Members profiles are a crucial part of successful alliances.",
        "We assume that agents that have more accurate profiles of their adversaries will be more successful in such environments.",
        "Such agents will be able to predict when a member is about to breach the alliances contract (item 2 in the above model), and take counter measures (when item 3 will falsify).",
        "The robustness of the alliance is in part a function of its members trustfulness measure, objective position estimation, and other profile properties.",
        "We should note that an agent can simultaneously be part of more than one alliance.",
        "Such a temporary alliance, where the group members do not have a joint goal but act collaboratively for the interest of their own individual goals, is classified as a Treatment Group by modern psychologists [12] (in contrast to a Task Group, where its members have a joint goal).",
        "The Shared Activity model as presented in [5] modeled Treatment Group behavior using the same SharedPlans formalization.",
        "When comparing both definitions of an alliance and a Treatment Group we found an unsurprising resemblance between both models: the environment models definitions are almost identical (see SAs definitions in [5]), and their Selfish-Act and Cooperative Act axioms conform to our adversarial agents behavior.",
        "The main distinction between both models is the integration of a Helpful-behavior act axiom, in the Shared Activity which cannot be part of ours.",
        "This axiom states that an agent will consider taking action that will lower its Eval value (to a certain lower bound), if it believes that a group partner will gain a significant benefit.",
        "Such behavior cannot occur in a pure adversarial environment (as a zero-sum game is), where the alliance members are constantly on watch to manipulate their alliance to their own advantage.",
        "A6.",
        "Evaluation Maximization Axiom.",
        "In a case when all other axioms are inapplicable, we will proceed with the action that maximizes the heuristic value as computed in the Eval function. (∀Aag, Ao ∈ A, α ∈ Cag, Tn, w ∈ W) Bel(Aag, (∀γ ∈ Cag)Eval(Aag, α, w) ≥ Eval(Aag, γ, w), Tn) ⇒ Pot.Int.To(Aag, α, Tn, Tα, w) T1.",
        "Optimality on Eval = Utility The above axiomatic model handles situations where the Utility is unknown and the agents are bounded rational agents.",
        "The following theorem shows that in bilateral interactions, where the agents have the real Utility function (i.e., Eval = Utility) and are rational agents, the axioms provide the same optimal result as classic adversarial search (e.g., Min-Max).",
        "Theorem 1.",
        "Let Ae ag be an unbounded rational AE agent using the Eval heuristic evaluation function, Au ag be the same agent using the true Utility function, and Ao be a sole unbounded utility-based rational adversary.",
        "Given that Eval = Utility: (∀α ∈ CAu ag , α ∈ CAe ag , Tn, w ∈ W) Pot.Int.To(Au ag, α, Tn, Tα, w) → Pot.Int.To(Ae ag, α , Tn, Tα, w) ∧ ((α = α ) ∨ (Utility(Au ag, α, w) = Eval(Ae ag, α , w))) Sketch of proof - Given that Au ag has the real utility function and unbounded resources, it can generate the full game tree and run the optimal MinMax algorithm to choose the highest utility value action, which we denote by, α.",
        "The proof will show that Ae ag, using the AE axioms, will select the same or equal utility α (when there is more than one action with the same max utility) when Eval = Utility. (A1) Goal achieving axiom - suppose there is an α such that its completion will achieve Au ags goal.",
        "It will obtain the highest utility by Min-Max for Au ag.",
        "The Ae ag agent will select α or another action with the same utility value via A1.",
        "If such α does not exist, Ae ag cannot apply this axiom, and proceeds to A2. (A2) Preventive act axiom - (1) Looking at the basic case (see Prop1), if there is a β which leads Ao to achieve its goal, then a preventive action α will yield the highest utility for Au ag.",
        "Au ag will choose it through the utility, while Ae ag will choose it through A2. (2) In the general case, β is a highly beneficial action for Ao, thus yields low utility for Au ag, which will guide it to select an α that will prevent β, while Ae ag will choose it through A2.1 If such β does not exist for Ao, then A2 is not applicable, and Ae ag can proceed to A3. (A3) Suboptimal tactical move axiom - When using a heuristic Eval function, Ae ag has a partial belief in the profile of its adversary (item 4 in AE model), which may lead it to believe in SetActions (Prop1).",
        "In our case, Ae ag is holding a full profile on its optimal adversary and knows that Ao will behave optimally according to the real utility values on the complete search tree, therefore, any belief about suboptimal SetAction cannot exist, yielding this axiom inapplicable.",
        "Ae ag will proceed to A4. (A4) Profile detection axiom - Given that Ae ag has the full profile of Ao, none of Ae ags actions can increase its knowledge.",
        "That axiom will not be applied, and the agent will proceed with A6 (A5 will be disregarded because the interaction is bilateral). (A6) Evaluation maximization axiom - This axiom will select the max Eval for Ae ag.",
        "Given that Eval = Utility, the same α that was selected by Au ag will be selected. 3.",
        "EVALUATION The main purpose of our experimental analysis is to evaluate the models behavior and performance in a real adversarial environment.",
        "This section investigates whether bounded 1 A case where following the completion of β there exists a γ which gives high utility for Agent Au ag, cannot occur because Ao uses the same utility, and γs existence will cause it to classify β as a low utility action. 554 The Sixth Intl.",
        "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) rational agents situated in such adversarial environments will be better off applying our suggested behavioral axioms. 3.1 The Domain To explore the use of the above model and its behavioral axioms, we decided to use the Connect-Four game as our adversarial environment.",
        "Connect-Four is a 2-player, zerosum game which is played using a 6x7 matrix-like board.",
        "Each turn, a player drops a disc into one of the 7 columns (the set of 21 discs is usually colored yellow for player 1 and red for player 2; we will use White and Black respectively to avoid confusion).",
        "The winner is the first player to complete a horizontal, vertical, or diagonal set of four discs with its color.",
        "On very rare occasions, the game might end in a tie if all the empty grids are filled, but no player managed to create a 4-disc set.",
        "The Connect-Four game was solved in [1], where it is shown that the first player (playing with the white discs) can force a win by starting in the middle column (column 4) and playing optimally However, the optimal strategy is very complex, and difficult to follow even for complex bounded rational agents, such as human players.",
        "Before we can proceed checking agent behavior, we must first verify that the domain conforms to the adversarial environments definition as given above (which the behavioral axioms are based on).",
        "First, when playing a Connect-Four game, the agent has an intention to win the game (item 1).",
        "Second (item 2), our agent believes that in Connect-Four there can only be one winner (or no winner at all in the rare occurrence of a tie).",
        "In addition, our agent believes that its opponent to the game will try to win (item 3), and we hope it has some partial knowledge (item 4) about its adversary (this knowledge can vary from nothing, through simple facts such as age, to strategies and weaknesses).",
        "Of course, not all Connect-Four encounters are adversarial.",
        "For example, when a parent is playing the game with its child, the following situation might occur: the child, having a strong incentive to win, treats the environment as adversarial (it intends to win, understands that there can only be one winner, and believes that its parent is trying to beat him).",
        "However, the parents point of view might see the environment as an educational one, where its goal is not to win the game, but to cause enjoyment or practice strategic reasoning.",
        "In such an educational environment, a new set of behavioral axioms might be more beneficial to the parents goals than our suggested adversarial behavioral axioms. 3.2 Axiom Analysis After showing that the Connect-Four game is indeed a zero-sum, bilateral adversarial environment, the next step is to look at players behaviors during the game and check whether behaving according to our model does improve performance.",
        "To do so we have collected log files from completed Connect-Four games that were played by human players over the Internet.",
        "Our collected log file data came from Play by eMail (PBeM) sites.",
        "These are web sites that host email games, where each move is taken by an email exchange between the server and the players.",
        "Many such sites archives contain real competitive interactions, and also maintain a ranking system for their members.",
        "Most of the data we used can be found in [6].",
        "As can be learned from [1], Connect-Four has an optimal strategy and a considerable advantage for the player who starts the game (which we call the White player).",
        "We will concentrate in our analysis on the second players moves (to be called Black).",
        "The White player, being the first to act, has the so-called initiative advantage.",
        "Having the advantage and a good strategy will keep the Black player busy reacting to its moves, instead of initiating threats.",
        "A threat is a combination of three discs of the same color, with an empty spot for the fourth winning disk.",
        "An open threat is a threat that can be realized in the opponents next move.",
        "In order for the Black player to win, it must somehow turn the tide, take the advantage and start presenting threats to the White player.",
        "We will explore Black players behavior and their conformance to our axioms.",
        "To do so, we built an application that reads log files and analyzes the Black players moves.",
        "The application contains two main components: (1) a Min-Max algorithm for evaluation of moves; (2) open threats detector for the discovering of open threats.",
        "The Min-Max algorithm will work to a given depth, d and for each move α will output the heuristic value for the next action taken by the player as written in the log file, h(α), alongside the maximum heuristic value, maxh(α), that could be achieved prior to taking the move (obviously, if h(α) = maxh(α), then the player did not do the optimal move heuristically).",
        "The threat detectors job is to notify if some action was taken in order to block an open threat (not blocking an open threat will probably cause the player to lose in the opponents next move).",
        "The heuristic function used by Min-Max to evaluate the players utility is the following function, which is simple to compute, yet provides a reasonable challenge to human opponents: Definition 8.",
        "Let Group be an adjacent set of four squares that are horizontal, vertical, or diagonal.",
        "Groupn b (Groupn w) be a Group with n pieces of the black (white) color and 4−n empty squares. h = ((Group1 b ∗α)+(Group2 b ∗β)+(Group3 b ∗γ)+(Group4 b ∗∞)) − ((Group1 w ∗α)+(Group2 w ∗β)+(Group3 w ∗γ)+(Group4 w ∗∞)) The values of α, β and δ can vary to form any desired linear combination; however, it is important to value them with the α < β < δ ordering in mind (we used 1, 4, and 8 as their respective values).",
        "Groups of 4 discs of the same color means victory, thus discovery of such a group will result in ∞ to ensure an extreme value.",
        "We now use our estimated evaluation function to evaluate the Black players actions during the Connect-Four adversarial interaction.",
        "Each game from the log file was input into the application, which processed and output a reformatted log file containing the h value of the current move, the maxh value that could be achieved, and a notification if an open threat was detected.",
        "A total of 123 games were analyzed (57 with White winning, and 66 with Black winning).",
        "A few additional games were manually ignored in the experiment, due to these problems: a player abandoning the game while the outcome is not final, or a blunt irrational move in the early stages of the game (e.g., not blocking an obvious winning group in the first opening moves).",
        "In addition, a single tie game was also removed.",
        "The simulator was run to a search depth of 3 moves.",
        "We now proceed to analyze the games with respect to each behavioral axiom.",
        "The Sixth Intl.",
        "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) 555 Table 1: Average heuristic difference analysis Black losses Black Won Avg minh -17.62 -12.02 Avg 3 lowest h moves (min3 h) -13.20 -8.70 3.2.1 Affirming the Suboptimal tactical move axiom The following section presents the heuristic evaluations of the Min-Max algorithm for each action, and checks the amount and extent of suboptimal tactical actions and their implications on performance.",
        "Table 1 shows results and insights from the games heuristic analysis, when search depth equals 3 (this search depth was selected for the results to be comparable to [9], see Section 3.2.3).",
        "The tables heuristic data is the difference between the present maximal heuristic value and the heuristic value of the action that was eventually taken by the player (i.e., the closer the number is to 0, the closer the action was to the maximum heuristic action).",
        "The first row presents the difference values of the action that had the maximal difference value among all the Black players actions in a given game, as averaged over all Blacks winning and losing games (see respective columns).",
        "In games in which the Black player loses, its average difference value was -17.62, while in games in which the Black player won, its average was -12.02.",
        "The second row expands the analysis by considering the 3 highest heuristic difference actions, and averaging them.",
        "In that case, we notice an average heuristic difference of 5 points between games which the Black player loses and games in which it wins.",
        "Nevertheless, the importance of those numbers is that they allowed us to take an educated guess on a threshold number of 11.5, as the value of the TrH constant, which differentiates between normal actions and highly beneficial ones.",
        "After finding an approximated TrH constant, we can proceed with an analysis of the importance of suboptimal moves.",
        "To do so we took the subset of games in which the minimum heuristic difference value for Blacks actions was 11.5.",
        "As presented in Table 2, we can see the different min3 h average of the 3 largest ranges and the respective percentage of games won.",
        "The first row shows that the Black player won only 12% of the games in which the average of its 3 highest heuristically difference actions (min3 h) was smaller than the suggested threshold, TrH = 11.5.",
        "The second row shows a surprising result: it seems that when min3 h > −4 the Black player rarely wins.",
        "Intuition would suggest that games in which the action evaluation values were closer to the maximal values will result in more winning games for Black.",
        "However, it seems that in the Connect-Four domain, merely responding with somewhat easily expected actions, without initiating a few surprising and suboptimal moves, does not yield good results.",
        "The last row sums up the main insights from the analysis; most of Blacks wins (83%) came when its min3 h was in the range of -11.5 to -4.",
        "A close inspection of those Black winning games shows the following pattern behind the numbers: after standard opening moves, Black suddenly drops a disc into an isolated column, which seems a waste of a move.",
        "White continues to build its threats, while usually disregarding Blacks last move, which in turn uses the isolated disc as an anchor for a future winning threat.",
        "The results show that it was beneficial for the Black player Table 2: Blacks winnings percentages % of games min3 h < −11.5 12% min3 h > −4 5% −11.5 ≤ min3 h ≤ −4 83% to take suboptimal actions and not give the current highest possible heuristic value, but will not be too harmful for its position (i.e., will not give high beneficial value to its adversary).",
        "As it turned out, learning the threshold is an important aspect of success: taking wildly risky moves (min3 h < −11.5) or trying to avoid them (min3 h > −4) reduces the Black players winning chances by a large margin. 3.2.2 Affirming the Profile Monitoring Axiom In the task of showing the importance of monitoring ones adversaries profiles, our log files could not be used because they did not contain repeated interactions between players, which are needed to infer the players knowledge about their adversaries.",
        "However, the importance of opponent modeling and its use in attaining tactical advantages was already studied in various domains ([3, 9] are good examples).",
        "In a recent paper, Markovitch and Reger [9] explored the notion of learning and exploitation of opponent weakness in competitive interactions.",
        "They apply simple learning strategies by analyzing examples from past interactions in a specific domain.",
        "They also used the Connect-Four adversarial domain, which can now be used to understand the importance of monitoring the adversarys profile.",
        "Following the presentation of their theoretical model, they describe an extensive empirical study and check the agents performance after learning the weakness model with past examples.",
        "One of the domains used as a competitive environment was the same Connect-Four game (Checkers was the second domain).",
        "Their heuristic function was identical to ours with three different variations (H1, H2, and H3) that are distinguished from one another in their linear combination coefficient values.",
        "The search depth for the players was 3 (as in our analysis).",
        "Their extensive experiments check and compare various learning strategies, risk factors, predefined feature sets and usage methods.",
        "The bottom line is that the Connect-Four domain shows an improvement from a 0.556 winning rate before modeling to a 0.69 after modeling (page 22).",
        "Their conclusions, showing improved performance when holding and using the adversarys model, justify the effort to monitor the adversary profile for continuous and repeated interactions.",
        "An additional point that came up in their experiments is the following: after the opponent weakness model has been learned, the authors describe different methods of integrating the opponent weakness model into the agents decision strategy.",
        "Nevertheless, regardless of the specific method they chose to work with, all integration methods might cause the agent to take suboptimal decisions; it might cause the agent to prefer actions that are suboptimal at the present decision junction, but which might cause the opponent to react in accordance with its weakness model (as represented by our agent) which in turn will be beneficial for us in the future.",
        "The agents behavior, as demonstrated in [9] further confirms and strengthens our Suboptimal Tactical Axiom as discussed in the previous section. 556 The Sixth Intl.",
        "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) 3.2.3 Additional Insights The need for the Goal Achieving, Preventive Act, and Evaluation Maximization axioms are obvious, and need no further verification.",
        "However, even with respect to those axioms, a few interesting insights came up in the log analysis.",
        "The Goal achieving and Preventive Act axioms, though theoretically trivial, seem to provide some challenge to a human player.",
        "In the initial inspection of the logs, we encountered few games2 where a player, for inexplicable reasons, did not block the other from winning or failed to execute its own winning move.",
        "We can blame those faults on the humans lack of attention, or a typing error in its move reply; nevertheless, those errors might occur in bounded rational agents, and the appropriate behavior needs to be axiomatized.",
        "A typical Connect-Four game revolves around generating threats and blocking them.",
        "In our analysis we looked for explicit preventive actions, i.e., moves that block a group of 3 discs, or that remove a future threat (in our limited search horizon).",
        "We found that in 83% of the total games there was at least one preventive action taken by the Black player.",
        "It was also found that Black averaged 2.8 preventive actions per game on the games in which it lost, while averaging 1.5 preventive actions per game when winning.",
        "It seems that Black requires 1 or 2 preventive actions to build its initial taking position, before starting to present threats.",
        "If it did not manage to win, it will usually prevent an extra threat or two before succumbing to White. 4.",
        "RELATED WORK Much research deals with the axiomatization of teamwork and mental states of individuals: some models use knowledge and belief [10], others have models of goals and intentions [8, 4].",
        "However, all these formal theories deal with agent teamwork and cooperation.",
        "As far as we know, our model is the first to provide a formalized model for explicit adversarial environments and agents behavior in it.",
        "The classical Min-Max adversarial search algorithm was the first attempt to integrate the opponent into the search space with a weak assumption of an optimally playing opponent.",
        "Since then, much effort has gone into integrating the opponent model into the decision procedure to predict future behavior.",
        "The M∗ algorithm presented by Carmel and Markovitch [2] showed a method of incorporating opponent models into adversary search, while in [3] they used learning to provide a more accurate opponent model in a 2player repeated game environment, where agents strategies were modeled as finite automata.",
        "Additional Adversarial planning work was done by Willmott et al. [13], which provided an adversarial planning approach to the game of GO.",
        "The research mentioned above dealt with adversarial search and the integration of opponent models into classical utilitybased search methods.",
        "That work shows the importance of opponent modeling and the ability to exploit it to an agents advantage.",
        "However, the basic limitations of those search methods still apply; our model tries to overcome those limitations by presenting a formal model for a new, mental state-based adversarial specification. 5.",
        "CONCLUSIONS We presented an Adversarial Environment model for a 2 These were later removed from the final analysis. bounded rational agent that is situated in an N-player, zerosum environment.",
        "We used the SharedPlans formalization to define the model and the axioms that agents can apply as behavioral guidelines.",
        "The model is meant to be used as a guideline for designing agents that need to operate in such adversarial environments.",
        "We presented empirical results, based on ConnectFour log file analysis, that exemplify the model and the axioms for a bilateral instance of the environment.",
        "The results we presented are a first step towards an expanded model that will cover all types of adversarial environments, for example, environments that are non-zero-sum, and environments that contain natural agents that are not part of the direct conflict.",
        "Those challenges and more will be dealt with in future research. 6.",
        "ACKNOWLEDGMENT This research was supported in part by Israel Science Foundation grants #1211/04 and #898/05. 7.",
        "REFERENCES [1] L. V. Allis.",
        "A knowledge-based approach of Connect-Four - the game is solved: White wins.",
        "Masters thesis, Free University, Amsterdam, The Netherlands, 1988. [2] D. Carmel and S. Markovitch.",
        "Incorporating opponent models into adversary search.",
        "In Proceedings of the Thirteenth National Conference on Artificial Intelligence, pages 120-125, Portland, OR, 1996. [3] D. Carmel and S. Markovitch.",
        "Opponent modeling in multi-agent systems.",
        "In G. Weiß and S. Sen, editors, Adaptation and Learning in Multi-Agent Systems, pages 40-52.",
        "Springer-Verlag, 1996. [4] B. J. Grosz and S. Kraus.",
        "Collaborative plans for complex group action.",
        "Artificial Intelligence, 86(2):269-357, 1996. [5] M. Hadad, G. Kaminka, G. Armon, and S. Kraus.",
        "Supporting collaborative activity.",
        "In Proc. of AAAI-2005, pages 83-88, Pittsburgh, 2005. [6] http://www.gamerz.net/˜pbmserv/. [7] S. Kraus and D. Lehmann.",
        "Designing and building a negotiating automated agent.",
        "Computational Intelligence, 11:132-171, 1995. [8] H. J. Levesque, P. R. Cohen, and J. H. T. Nunes.",
        "On acting together.",
        "In Proc. of AAAI-90, pages 94-99, Boston, MA, 1990. [9] S. Markovitch and R. Reger.",
        "Learning and exploiting relative weaknesses of opponent agents.",
        "Autonomous Agents and Multi-Agent Systems, 10(2):103-130, 2005. [10] Y. M. Ronald Fagin, Joseph Y. Halpern and M. Y. Vardi.",
        "Reasoning about knowledge.",
        "MIT Press, Cambridge, Mass., 1995. [11] P. Thagard.",
        "Adversarial problem solving: Modeling an oponent using explanatory coherence.",
        "Cognitive Science, 16(1):123-149, 1992. [12] R. W. Toseland and R. F. Rivas.",
        "An Introduction to Group Work Practice.",
        "Prentice Hall, Englewood Cliffs, NJ, 2nd edition edition, 1995. [13] S. Willmott, J. Richardson, A. Bundy, and J. Levine.",
        "An adversarial planning approach to Go.",
        "Lecture Notes in Computer Science, 1558:93-112, 1999.",
        "The Sixth Intl.",
        "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) 557"
    ],
    "error_count": 0,
    "keys": {
        "multiagent environment": {
            "translated_key": "",
            "is_in_text": false,
            "original_annotated_sentences": [
                "An Adversarial Environment Model for Bounded Rational Agents in Zero-Sum Interactions Inon Zuckerman1 , Sarit Kraus1 , Jeffrey S. Rosenschein2 , Gal Kaminka1 1 Department of Computer Science 2 The School of Engineering Bar-Ilan University and Computer Science Ramat-Gan, Israel Hebrew University, Jerusalem, Israel {zukermi,sarit,galk}@cs.biu.ac.il jeff@cs.huji.ac.il ABSTRACT Multiagent environments are often not cooperative nor collaborative; in many cases, agents have conflicting interests, leading to adversarial interactions.",
                "This paper presents a formal Adversarial Environment model for bounded rational agents operating in a zero-sum environment.",
                "In such environments, attempts to use classical utility-based search methods can raise a variety of difficulties (e.g., implicitly modeling the opponent as an omniscient utility maximizer, rather than leveraging a more nuanced, explicit opponent model).",
                "We define an Adversarial Environment by describing the mental states of an agent in such an environment.",
                "We then present behavioral axioms that are intended to serve as design principles for building such adversarial agents.",
                "We explore the application of our approach by analyzing log files of completed Connect-Four games, and present an empirical analysis of the axioms appropriateness.",
                "Categories and Subject Descriptors I.2.11 [Artificial Intelligence]: Distributed Artificial Intelligence-Intelligent agents,Multiagent Systems; I.2.4 [Artificial Intelligence]: Knowledge Representation Formalisms and Methods -Modal logic General Terms Design, Theory 1.",
                "INTRODUCTION Early research in multiagent systems (MAS) considered cooperative groups of agents; because individual agents had limited resources, or limited access to information (e.g., limited processing power, limited sensor coverage), they worked together by design to solve problems that individually they could not solve, or at least could not solve as efficiently.",
                "MAS research, however, soon began to consider interacting agents with individuated interests, as representatives of different humans or organizations with non-identical interests.",
                "When interactions are guided by diverse interests, participants may have to overcome disagreements, uncooperative interactions, and even intentional attempts to damage one another.",
                "When these types of interactions occur, environments require appropriate behavior from the agents situated in them.",
                "We call these environments Adversarial Environments, and call the clashing agents Adversaries.",
                "Models of cooperation and teamwork have been extensively explored in MAS through the axiomatization of mental states (e.g., [8, 4, 5]).",
                "However, none of this research dealt with adversarial domains and their implications for agent behavior.",
                "Our paper addresses this issue by providing a formal, axiomatized mental state model for a subset of adversarial domains, namely simple zero-sum adversarial environments.",
                "Simple zero-sum encounters exist of course in various twoplayer games (e.g., Chess, Checkers), but they also exist in n-player games (e.g., Risk, Diplomacy), auctions for a single good, and elsewhere.",
                "In these latter environments especially, using a utility-based adversarial search (such as the Min-Max algorithm) does not always provide an adequate solution; the payoff function might be quite complex or difficult to quantify, and there are natural computational limitations on bounded rational agents.",
                "In addition, traditional search methods (like Min-Max) do not make use of a model of the opponent, which has proven to be a valuable addition to adversarial planning [9, 3, 11].",
                "In this paper, we develop a formal, axiomatized model for bounded rational agents that are situated in a zero-sum adversarial environment.",
                "The model uses different modality operators, and its main foundations are the SharedPlans [4] model for collaborative behavior.",
                "We explore environment properties and the mental states of agents to derive behavioral axioms; these behavioral axioms constitute a formal model that serves as a specification and design guideline for agent design in such settings.",
                "We then investigate the behavior of our model empirically using the Connect-Four board game.",
                "We show that this game conforms to our environment definition, and analyze players behavior using a large set of completed match log 550 978-81-904262-7-5 (RPS) c 2007 IFAAMAS files.",
                "In addition, we use the results presented in [9] to discuss the importance of opponent modeling in our ConnectFour adversarial domain.",
                "The paper proceeds as follows.",
                "Section 2 presents the models formalization.",
                "Section 3 presents the empirical analysis and its results.",
                "We discuss related work in Section 4, and conclude and present future directions in Section 5. 2.",
                "ADVERSARIAL ENVIRONMENTS The adversarial environment model (denoted as AE) is intended to guide the design of agents by providing a specification of the capabilities and mental attitudes of an agent in an adversarial environment.",
                "We focus here on specific types of adversarial environments, specified as follows: 1.",
                "Zero-Sum Interactions: positive and negative utilities of all agents sum to zero; 2.",
                "Simple AEs: all agents in the environment are adversarial agents; 3.",
                "Bilateral AEs: AEs with exactly two agents; 4.",
                "Multilateral AEs: AEs of three or more agents.",
                "We will work on both bilateral and multilateral instantiations of zero-sum and simple environments.",
                "In particular, our adversarial environment model will deal with interactions that consist of N agents (N ≥ 2), where all agents are adversaries, and only one agent can succeed.",
                "Examples of such environments range from board games (e.g., Chess, Connect-Four, and Diplomacy) to certain economic environments (e.g., N-bidder auctions over a single good). 2.1 Model Overview Our approach is to formalize the mental attitudes and behaviors of a single adversarial agent; we consider how a single agent perceives the AE.",
                "The following list specifies the conditions and mental states of an agent in a simple, zero-sum AE: 1.",
                "The agent has an individual intention that its own goal will be completed; 2.",
                "The agent has an individual belief that it and its adversaries are pursuing full conflicting goals (defined below)there can be only one winner; 3.",
                "The agent has an individual belief that each adversary has an intention to complete its own full conflicting goal; 4.",
                "The agent has an individual belief in the (partial) profile of its adversaries.",
                "Item 3 is required, since it might be the case that some agent has a full conflicting goal, and is currently considering adopting the intention to complete it, but is, as of yet, not committed to achieving it.",
                "This might occur because the agent has not yet deliberated about the effects that adopting that intention might have on the other intentions it is currently holding.",
                "In such cases, it might not consider itself to even be in an adversarial environment.",
                "Item 4 states that the agent should hold some belief about the profiles of its adversaries.",
                "The profile represents all the knowledge the agent has about its adversary: its weaknesses, strategic capabilities, goals, intentions, trustworthiness, and more.",
                "It can be given explicitly or can be learned from observations of past encounters. 2.2 Model Definitions for Mental States We use Grosz and Krauss definitions of the modal operators, predicates, and meta-predicates, as defined in their SharedPlan formalization [4].",
                "We recall here some of the predicates and operators that are used in that formalization: Int.To(Ai, α, Tn, Tα, C) represents Ais intentions at time Tn to do an action α at time Tα in the context of C. Int.Th(Ai, prop, Tn, Tprop, C) represents Ais intentions at time Tn that a certain proposition prop holds at time Tprop in the context of C. The potential intention operators, Pot.Int.To(...) and Pot.Int.Th(...), are used to represent the mental state when an agent considers adopting an intention, but has not deliberated about the interaction of the other intentions it holds.",
                "The operator Bel(Ai, f, Tf ) represents agent Ai believing in the statement expressed in formula f, at time Tf .",
                "MB(A, f, Tf ) represents mutual belief for a group of agents A.",
                "A snapshot of the system finds our environment to be in some state e ∈ E of environmental variable states, and each adversary in any LAi ∈ L of possible local states.",
                "At any given time step, the system will be in some world w of the set of all possible worlds w ∈ W, where w = E×LA1 ×LA2 × ...LAn , and n is the number of adversaries.",
                "For example, in a Texas Holdem poker game, an agents local state might be its own set of cards (which is unknown to its adversary) while the environment will consist of the betting pot and the community cards (which are visible to both players).",
                "A utility function under this formalization is defined as a mapping from a possible world w ∈ W to an element in , which expresses the desirability of the world, from a single agent perspective.",
                "We usually normalize the range to [0,1], where 0 represents the least desirable possible world, and 1 is the most desirable world.",
                "The implementation of the utility function is dependent on the domain in question.",
                "The following list specifies new predicates, functions, variables, and constants used in conjunction with the original definitions for the adversarial environment formalization: 1. φ is a null action (the agent does not do anything). 2.",
                "GAi is the set of agent Ais goals.",
                "Each goal is a set of predicates whose satisfaction makes the goal complete (we use G∗ Ai ∈ GAi to represent an arbitrary goal of agent Ai). 3. gAi is the set of agent Ais subgoals.",
                "Subgoals are predicates whose satisfaction represents an important milestone toward achievement of the full goal. gG∗ Ai ⊆ gAi is the set of subgoals that are important to the completion of goal G∗ Ai (we will use g∗ G∗ Ai ∈ gG∗ Ai to represent an arbitrary subgoal). 4.",
                "P Aj Ai is the profile object agent Ai holds about agent Aj. 5.",
                "CA is a general set of actions for all agents in A which are derived from the environments constraints.",
                "CAi ⊆ CA is the set of agent Ais possible actions. 6.",
                "Do(Ai, α, Tα, w) holds when Ai performs action α over time interval Tα in world w. 7.",
                "Achieve(G∗ Ai , α, w) is true when goal G∗ Ai is achieved following the completion of action α in world w ∈ W, where α ∈ CAi . 8.",
                "Profile(Ai, PAi Ai ) is true when agent Ai holds an object profile for agent Aj.",
                "Definition 1.",
                "Full conflict (FulConf ) describes a zerosum interaction where only a single goal of the goals in conflict can be completed.",
                "FulConf(G∗ Ai , G∗ Aj ) ⇒ (∃α ∈ CAi , ∀w, β ∈ CAj ) (Achieve(G∗ Ai , α, w) ⇒ ¬Achieve(G∗ Aj , β, w)) ∨ (∃β ∈ CAj , ∀w, α ∈ CAi )(Achieve(G∗ Aj , β, w) ⇒ ¬Achieve(G∗ Ai , α, w)) Definition 2.",
                "Adversarial Knowledge (AdvKnow) is a function returning a value which represents the amount of The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) 551 knowledge agent Ai has on the profile of agent Aj, at time Tn.",
                "The higher the value, the more knowledge agent Ai has.",
                "AdvKnow : P Aj Ai × Tn → Definition 3.",
                "Eval - This evaluation function returns an estimated expected utility value for an agent in A, after completing an action from CA in some world state w. Eval : A × CA × w → Definition 4.",
                "TrH - (Threshold) is a numerical constant in the [0,1] range that represents an evaluation function (Eval) threshold value.",
                "An action that yields an estimated utility evaluation above the TrH is regarded as a highly beneficial action.",
                "The Eval value is an estimation and not the real utility function, which is usually unknown.",
                "Using the real utility value for a rational agent would easily yield the best outcome for that agent.",
                "However, agents usually do not have the real utility functions, but rather a heuristic estimate of it.",
                "There are two important properties that should hold for the evaluation function: Property 1.",
                "The evaluation function should state that the most desirable world state is one in which the goal is achieved.",
                "Therefore, after the goal has been satisfied, there can be no future action that can put the agent in a world state with higher Eval value. (∀Ai, G∗ Ai , α, β ∈ CAi , w ∈ W) Achieve(G∗ Ai , α, w) ⇒ Eval(Ai, α, w) ≥ Eval(Ai, β, w) Property 2.",
                "The evaluation function should project an action that causes a completion of a goal or a subgoal to a value which is greater than TrH (a highly beneficial action). (∀Ai, G∗ Ai ∈ GAi , α ∈ CAi , w ∈ W, g∗ GAi ∈ gGAi ) Achieve(G∗ Ai , α, w) ∨ Achieve(g∗ GAi , α, w) ⇒ Eval(Ai, α, w) ≥ TrH.",
                "Definition 5.",
                "SetAction We define a set action (SetAction) as a set of action operations (either complex or basic actions) from some action sets CAi and CAj which, according to agent Ais belief, are attached together by a temporal and consequential relationship, forming a chain of events (action, and its following consequent action). (∀α1 , . . . , αu ∈ CAi , β1 , . . . , βv ∈ CAj , w ∈ W) SetAction(α1 , . . . , αu , β1 , . . . , βv , w) ⇒ ((Do(Ai, α1 , Tα1 , w) ⇒ Do(Aj, β1 , Tβ1 , w)) ⇒ Do(Ai, α2 , Tα2 , w) ⇒ . . . ⇒ Do(Ai, αu , Tαu , w)) The consequential relation might exist due to various environmental constraints (when one action forces the adversary to respond with a specific action) or due to the agents knowledge about the profile of its adversary.",
                "Property 3.",
                "As the knowledge we have about our adversary increases we will have additional beliefs about its behavior in different situations which in turn creates new set actions.",
                "Formally, if our AdvKnow at time Tn+1 is greater than AdvKnow at time Tn, then every SetAction known at time Tn is also known at time Tn+1.",
                "AdvKnow(P Aj Ai , Tn+1) > AdvKnow(P Aj Ai , Tn) ⇒ (∀α1 , . . . , αu ∈ CAi , β1 , . . . , βv ∈ CAj ) Bel(Aag, SetAction(α1 , . . . , αu , β1 , . . . , βv ), Tn) ⇒ Bel(Aag, SetAction(α1 , . . . , αu , β1 , . . . , βv ), Tn+1) 2.3 The Environment Formulation The following axioms provide the formal definition for a simple, zero-sum Adversarial Environment (AE).",
                "Satisfaction of these axioms means that the agent is situated in such an environment.",
                "It provides specifications for agent Aag to interact with its set of adversaries A with respect to goals G∗ Aag and G∗ A at time TCo at some world state w. AE(Aag, A, G∗ Aag , A1, . . . , Ak, G∗ A1 , . . . , G∗ Ak , Tn, w) 1.",
                "Aag has an Int.Th his goal would be completed: (∃α ∈ CAag , Tα) Int.Th(Aag, Achieve(G∗ Aag , α), Tn, Tα, AE) 2.",
                "Aag believes that it and each of its adversaries Ao are pursuing full conflicting goals: (∀Ao ∈ {A1, . . . , Ak}) Bel(Aag, FulConf(G∗ Aag , G∗ Ao ), Tn) 3.",
                "Aag believes that each of his adversaries in Ao has the Int.Th his conflicting goal G∗ Aoi will be completed: (∀Ao ∈ {A1, . . . , Ak})(∃β ∈ CAo , Tβ) Bel(Aag, Int.Th(Ao, Achieve(G∗ Ao , β), TCo, Tβ, AE), Tn) 4.",
                "Aag has beliefs about the (partial) profiles of its adversaries (∀Ao ∈ {A1, . . . , Ak}) (∃PAo Aag ∈ PAag )Bel(Aag, Profile(Ao, PAo Aag ), Tn) To build an agent that will be able to operate successfully within such an AE, we must specify behavioral guidelines for its interactions.",
                "Using a naive Eval maximization strategy to a certain search depth will not always yield satisfactory results for several reasons: (1) the search horizon problem when searching for a fixed depth; (2) the strong assumption of an optimally rational, unbounded resources adversary; (3) using an estimated evaluation function which will not give optimal results in all world states, and can be exploited [9].",
                "The following axioms specify the behavioral principles that can be used to differentiate between successful and less successful agents in the above Adversarial Environment.",
                "Those axioms should be used as specification principles when designing and implementing agents that should be able to perform well in such Adversarial Environments.",
                "The behavioral axioms represent situations in which the agent will adopt potential intentions to (Pot.Int.To(...)) perform an action, which will typically require some means-end reasoning to select a possible course of action.",
                "This reasoning will lead to the adoption of an Int.To(...) (see [4]).",
                "A1.",
                "Goal Achieving Axiom.",
                "The first axiom is the simplest case; when the agent Aag believes that it is one action (α) away from achieving his conflicting goal G∗ Aag , it should adopt the potential intention to do α and complete its goal. (∀Aag, α ∈ CAag , Tn, Tα, w ∈ W) (Bel(Aag, Do(Aag, α, Tα, w) ⇒ Achieve(G∗ Aag , α, w)) ⇒ Pot.Int.To(Aag, α, Tn, Tα, w) This somewhat trivial behavior is the first and strongest axiom.",
                "In any situation, when the agent is an action away from completing the goal, it should complete the action.",
                "Any fair Eval function would naturally classify α as the maximal value action (property 1).",
                "However, without explicit axiomatization of such behavior there might be situations where the agent will decide on taking another action for various reasons, due to its bounded decision resources.",
                "A2.",
                "Preventive Act Axiom.",
                "Being in an adversarial situation, agent Aag might decide to take actions that will damage one of its adversarys plans to complete its goal, even if those actions do not explicitly advance Aag towards its conflicting goal G∗ Aag .",
                "Such preventive action will take place when agent Aag has a belief about the possibility of its adversary Ao doing an action β that will give it a high 552 The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) utility evaluation value (> TrH).",
                "Believing that taking action α will prevent the opponent from doing its β, it will adopt a potential intention to do α. (∀Aag, Ao ∈ A, α ∈ CAag , β ∈ CAo , Tn, Tβ, w ∈ W) (Bel(Aag, Do(Ao, β, Tβ, w) ∧ Eval(Ao, β, w) > TrH, Tn) ∧ Bel(Aag, Do(Aag, α, Tα, w) ⇒ ¬Do(Ao, β, Tβ, w), Tn) ⇒ Pot.Int.To(Aag, α, Tn, Tα, w) This axiom is a basic component of any adversarial environment.",
                "For example, looking at a Chess board game, a player could realize that it is about to be checkmated by its opponent, thus making a preventive move.",
                "Another example is a Connect Four game: when a player has a row of three chips, its opponent must block it, or lose.",
                "A specific instance of A1 occurs when the adversary is one action away from achieving its goal, and immediate preventive action needs to be taken by the agent.",
                "Formally, we have the same beliefs as stated above, with a changed belief that doing action β will cause agent Ao to achieve its goal.",
                "Proposition 1: Prevent or lose case. (∀Aag, Ao ∈ A, α ∈ CAag , β ∈ CAo , G∗ Ao , Tn, Tα, Tβ, w ∈ W) Bel(Aag, Do(Ao, β, Tβ, w) ⇒ Achieve(G∗ Ao , β, w), Tn) ∧ Bel(Aag, Do(Aag, α, Tα, w) ⇒ ¬Do(Ao, β, Tβ, w)) ⇒ Pot.Int.To(Aag, α, Tn, Tα, w) Sketch of proof: Proposition 1 can be easily derived from axiom A1 and the property 2 of the Eval function, which states that any action that causes a completion of a goal is a highly beneficial action.",
                "The preventive act behavior will occur implicitly when the Eval function is equal to the real world utility function.",
                "However, being bounded rational agents and dealing with an estimated evaluation function we need to explicitly axiomatize such behavior, for it will not always occur implicitly from the evaluation function.",
                "A3.",
                "Suboptimal Tactical Move Axiom.",
                "In many scenarios a situation may occur where an agent will decide not to take the current most beneficial action it can take (the action with the maximal utility evaluation value), because it believes that taking another action (with lower utility evaluation value) might yield (depending on the adversarys response) a future possibility for a highly beneficial action.",
                "This will occur most often when the Eval function is inaccurate and differs by a large extent from the Utility function.",
                "Put formally, agent Aag believes in a certain SetAction that will evolve according to its initial action and will yield a high beneficial value (> TrH) solely for it. (∀Aag, Ao ∈ A, Tn, w ∈ W) (∃α1 , . . . , αu ∈ CAi , β1 , . . . , βv ∈ CAj , Tα1 ) Bel(Aag, SetAction(α1 , . . . , αu , β1 , . . . , βv ), Tn) ∧ Bel(Aag, Eval(Ao, βv , w) < TrH < Eval(Aag, αu , w), Tn) ⇒ Pot.Int.To(Aag, α1 , Tn, Tα1 , w) An agent might believe that a chain of events will occur for various reasons due to the inevitable nature of the domain.",
                "For example, in Chess, we often observe the following: a move causes a check position, which in turn limits the opponents moves to avoiding the check, to which the first player might react with another check, and so on.",
                "The agent might also believe in a chain of events based on its knowledge of its adversarys profile, which allows it to foresee the adversarys movements with high accuracy.",
                "A4.",
                "Profile Detection Axiom.",
                "The agent can adjust its adversarys profiles by observations and pattern study (specifically, if there are repeated encounters with the same adversary).",
                "However, instead of waiting for profile information to be revealed, an agent can also initiate actions that will force its adversary to react in a way that will reveal profile knowledge about it.",
                "Formally, the axiom states that if all actions (γ) are not highly beneficial actions (< TrH), the agent can do action α in time Tα if it believes that it will result in a non-highly beneficial action β from its adversary, which in turn teaches it about the adversarys profile, i.e., gives a higher AdvKnow(P Aj Ai , Tβ). (∀Aag, Ao ∈ A, α ∈ CAag , β ∈ CAo , Tn, Tα, Tβ, w ∈ W) Bel(Aag, (∀γ ∈ CAag )Eval(Aag, γ, w) < TrH, Tn) ∧ Bel(Aag, Do(Aag, α, Tα, w) ⇒ Do(Ao, β, Tβ, w), Tn) ∧ Bel(Aag, Eval(Ao, β, w) < TrH) ∧ Bel(Aag, AdvKnow(P Aj Ai , Tβ) > AdvKnow(P Aj Ai , Tn), Tn) ⇒ Pot.Int.To(Aag, α, Tn, Tα, w) For example, going back to the Chess board game scenario, consider starting a game versus an opponent about whom we know nothing, not even if it is a human or a computerized opponent.",
                "We might start playing a strategy that will be suitable versus an average opponent, and adjust our game according to its level of play.",
                "A5.",
                "Alliance Formation Axiom The following behavioral axiom is relevant only in a multilateral instantiation of the adversarial environment (obviously, an alliance cannot be formed in a bilateral, zero-sum encounter).",
                "In different situations during a multilateral interaction, a group of agents might believe that it is in their best interests to form a temporary alliance.",
                "Such an alliance is an agreement that constrains its members behavior, but is believed by its members to enable them to achieve a higher utility value than the one achievable outside of the alliance.",
                "As an example, we can look at the classical Risk board game, where each player has an individual goal of being the sole conquerer of the world, a zero-sum game.",
                "However, in order to achieve this goal, it might be strategically wise to make short-term ceasefire agreements with other players, or to join forces and attack an opponent who is stronger than the rest.",
                "An alliances terms defines the way its members should act.",
                "It is a set of predicates, denoted as Terms, that is agreed upon by the alliance members, and should remain true for the duration of the alliance.",
                "For example, the set Terms in the Risk scenario, could contain the following predicates: 1.",
                "Alliance members will not attack each other on territories X, Y and Z; 2.",
                "Alliance members will contribute C units per turn for attacking adversary Ao; 3.",
                "Members are obligated to stay as part of the alliance until time Tk or until adversarys Ao army is smaller than Q.",
                "The set Terms specifies inter-group constraints on each of the alliance members (∀Aal i ∈ Aal ⊆ A) set of actions Cal i ⊆ C. Definition 6.",
                "Al val - the total evaluation value that agent Ai will achieve while being part of Aal is the sum of Evali (Eval for Ai) of each of Aal j Eval values after taking their own α actions (via the agent(α) predicate): Al val(Ai, Cal , Aal , w) = α∈Cal Evali(Aal j , agent(α), w) Definition 7.",
                "Al TrH - is a number representing an Al val The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) 553 threshold; above it, the alliance can be said to be a highly beneficial alliance.",
                "The value of Al TrH will be calculated dynamically according to the progress of the interaction, as can be seen in [7].",
                "After an alliance is formed, its members are now working in their normal adversarial environment, as well as according to the mental states and axioms required for their interactions as part of the alliance.",
                "The following Alliance model (AL) specifies the conditions under which the group Aal can be said to be in an alliance and working with a new and constrained set of actions Cal , at time Tn.",
                "AL(Aal , Cal , w, Tn) 1.",
                "Aal has a MB that all members are part of Aal : MB(Aal , (∀Aal i ∈ Aal )member(Aal i , Aal ), Tn) 2.",
                "Aal has a MB that the group be maintained: MB(Aal , (∀Aal i ∈ Aal )Int.Th (Ai, member(Ai, Aal ), Tn, Tn+1, Co), Tn) 3.",
                "Aal has a MB that being members gives them high utility value: MB(Aal , (∀Aal i ∈ Aal )Al val(Aal i , Cal , Aal , w) ≥ Al TrH, Tn) Members profiles are a crucial part of successful alliances.",
                "We assume that agents that have more accurate profiles of their adversaries will be more successful in such environments.",
                "Such agents will be able to predict when a member is about to breach the alliances contract (item 2 in the above model), and take counter measures (when item 3 will falsify).",
                "The robustness of the alliance is in part a function of its members trustfulness measure, objective position estimation, and other profile properties.",
                "We should note that an agent can simultaneously be part of more than one alliance.",
                "Such a temporary alliance, where the group members do not have a joint goal but act collaboratively for the interest of their own individual goals, is classified as a Treatment Group by modern psychologists [12] (in contrast to a Task Group, where its members have a joint goal).",
                "The Shared Activity model as presented in [5] modeled Treatment Group behavior using the same SharedPlans formalization.",
                "When comparing both definitions of an alliance and a Treatment Group we found an unsurprising resemblance between both models: the environment models definitions are almost identical (see SAs definitions in [5]), and their Selfish-Act and Cooperative Act axioms conform to our adversarial agents behavior.",
                "The main distinction between both models is the integration of a Helpful-behavior act axiom, in the Shared Activity which cannot be part of ours.",
                "This axiom states that an agent will consider taking action that will lower its Eval value (to a certain lower bound), if it believes that a group partner will gain a significant benefit.",
                "Such behavior cannot occur in a pure adversarial environment (as a zero-sum game is), where the alliance members are constantly on watch to manipulate their alliance to their own advantage.",
                "A6.",
                "Evaluation Maximization Axiom.",
                "In a case when all other axioms are inapplicable, we will proceed with the action that maximizes the heuristic value as computed in the Eval function. (∀Aag, Ao ∈ A, α ∈ Cag, Tn, w ∈ W) Bel(Aag, (∀γ ∈ Cag)Eval(Aag, α, w) ≥ Eval(Aag, γ, w), Tn) ⇒ Pot.Int.To(Aag, α, Tn, Tα, w) T1.",
                "Optimality on Eval = Utility The above axiomatic model handles situations where the Utility is unknown and the agents are bounded rational agents.",
                "The following theorem shows that in bilateral interactions, where the agents have the real Utility function (i.e., Eval = Utility) and are rational agents, the axioms provide the same optimal result as classic adversarial search (e.g., Min-Max).",
                "Theorem 1.",
                "Let Ae ag be an unbounded rational AE agent using the Eval heuristic evaluation function, Au ag be the same agent using the true Utility function, and Ao be a sole unbounded utility-based rational adversary.",
                "Given that Eval = Utility: (∀α ∈ CAu ag , α ∈ CAe ag , Tn, w ∈ W) Pot.Int.To(Au ag, α, Tn, Tα, w) → Pot.Int.To(Ae ag, α , Tn, Tα, w) ∧ ((α = α ) ∨ (Utility(Au ag, α, w) = Eval(Ae ag, α , w))) Sketch of proof - Given that Au ag has the real utility function and unbounded resources, it can generate the full game tree and run the optimal MinMax algorithm to choose the highest utility value action, which we denote by, α.",
                "The proof will show that Ae ag, using the AE axioms, will select the same or equal utility α (when there is more than one action with the same max utility) when Eval = Utility. (A1) Goal achieving axiom - suppose there is an α such that its completion will achieve Au ags goal.",
                "It will obtain the highest utility by Min-Max for Au ag.",
                "The Ae ag agent will select α or another action with the same utility value via A1.",
                "If such α does not exist, Ae ag cannot apply this axiom, and proceeds to A2. (A2) Preventive act axiom - (1) Looking at the basic case (see Prop1), if there is a β which leads Ao to achieve its goal, then a preventive action α will yield the highest utility for Au ag.",
                "Au ag will choose it through the utility, while Ae ag will choose it through A2. (2) In the general case, β is a highly beneficial action for Ao, thus yields low utility for Au ag, which will guide it to select an α that will prevent β, while Ae ag will choose it through A2.1 If such β does not exist for Ao, then A2 is not applicable, and Ae ag can proceed to A3. (A3) Suboptimal tactical move axiom - When using a heuristic Eval function, Ae ag has a partial belief in the profile of its adversary (item 4 in AE model), which may lead it to believe in SetActions (Prop1).",
                "In our case, Ae ag is holding a full profile on its optimal adversary and knows that Ao will behave optimally according to the real utility values on the complete search tree, therefore, any belief about suboptimal SetAction cannot exist, yielding this axiom inapplicable.",
                "Ae ag will proceed to A4. (A4) Profile detection axiom - Given that Ae ag has the full profile of Ao, none of Ae ags actions can increase its knowledge.",
                "That axiom will not be applied, and the agent will proceed with A6 (A5 will be disregarded because the interaction is bilateral). (A6) Evaluation maximization axiom - This axiom will select the max Eval for Ae ag.",
                "Given that Eval = Utility, the same α that was selected by Au ag will be selected. 3.",
                "EVALUATION The main purpose of our experimental analysis is to evaluate the models behavior and performance in a real adversarial environment.",
                "This section investigates whether bounded 1 A case where following the completion of β there exists a γ which gives high utility for Agent Au ag, cannot occur because Ao uses the same utility, and γs existence will cause it to classify β as a low utility action. 554 The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) rational agents situated in such adversarial environments will be better off applying our suggested behavioral axioms. 3.1 The Domain To explore the use of the above model and its behavioral axioms, we decided to use the Connect-Four game as our adversarial environment.",
                "Connect-Four is a 2-player, zerosum game which is played using a 6x7 matrix-like board.",
                "Each turn, a player drops a disc into one of the 7 columns (the set of 21 discs is usually colored yellow for player 1 and red for player 2; we will use White and Black respectively to avoid confusion).",
                "The winner is the first player to complete a horizontal, vertical, or diagonal set of four discs with its color.",
                "On very rare occasions, the game might end in a tie if all the empty grids are filled, but no player managed to create a 4-disc set.",
                "The Connect-Four game was solved in [1], where it is shown that the first player (playing with the white discs) can force a win by starting in the middle column (column 4) and playing optimally However, the optimal strategy is very complex, and difficult to follow even for complex bounded rational agents, such as human players.",
                "Before we can proceed checking agent behavior, we must first verify that the domain conforms to the adversarial environments definition as given above (which the behavioral axioms are based on).",
                "First, when playing a Connect-Four game, the agent has an intention to win the game (item 1).",
                "Second (item 2), our agent believes that in Connect-Four there can only be one winner (or no winner at all in the rare occurrence of a tie).",
                "In addition, our agent believes that its opponent to the game will try to win (item 3), and we hope it has some partial knowledge (item 4) about its adversary (this knowledge can vary from nothing, through simple facts such as age, to strategies and weaknesses).",
                "Of course, not all Connect-Four encounters are adversarial.",
                "For example, when a parent is playing the game with its child, the following situation might occur: the child, having a strong incentive to win, treats the environment as adversarial (it intends to win, understands that there can only be one winner, and believes that its parent is trying to beat him).",
                "However, the parents point of view might see the environment as an educational one, where its goal is not to win the game, but to cause enjoyment or practice strategic reasoning.",
                "In such an educational environment, a new set of behavioral axioms might be more beneficial to the parents goals than our suggested adversarial behavioral axioms. 3.2 Axiom Analysis After showing that the Connect-Four game is indeed a zero-sum, bilateral adversarial environment, the next step is to look at players behaviors during the game and check whether behaving according to our model does improve performance.",
                "To do so we have collected log files from completed Connect-Four games that were played by human players over the Internet.",
                "Our collected log file data came from Play by eMail (PBeM) sites.",
                "These are web sites that host email games, where each move is taken by an email exchange between the server and the players.",
                "Many such sites archives contain real competitive interactions, and also maintain a ranking system for their members.",
                "Most of the data we used can be found in [6].",
                "As can be learned from [1], Connect-Four has an optimal strategy and a considerable advantage for the player who starts the game (which we call the White player).",
                "We will concentrate in our analysis on the second players moves (to be called Black).",
                "The White player, being the first to act, has the so-called initiative advantage.",
                "Having the advantage and a good strategy will keep the Black player busy reacting to its moves, instead of initiating threats.",
                "A threat is a combination of three discs of the same color, with an empty spot for the fourth winning disk.",
                "An open threat is a threat that can be realized in the opponents next move.",
                "In order for the Black player to win, it must somehow turn the tide, take the advantage and start presenting threats to the White player.",
                "We will explore Black players behavior and their conformance to our axioms.",
                "To do so, we built an application that reads log files and analyzes the Black players moves.",
                "The application contains two main components: (1) a Min-Max algorithm for evaluation of moves; (2) open threats detector for the discovering of open threats.",
                "The Min-Max algorithm will work to a given depth, d and for each move α will output the heuristic value for the next action taken by the player as written in the log file, h(α), alongside the maximum heuristic value, maxh(α), that could be achieved prior to taking the move (obviously, if h(α) = maxh(α), then the player did not do the optimal move heuristically).",
                "The threat detectors job is to notify if some action was taken in order to block an open threat (not blocking an open threat will probably cause the player to lose in the opponents next move).",
                "The heuristic function used by Min-Max to evaluate the players utility is the following function, which is simple to compute, yet provides a reasonable challenge to human opponents: Definition 8.",
                "Let Group be an adjacent set of four squares that are horizontal, vertical, or diagonal.",
                "Groupn b (Groupn w) be a Group with n pieces of the black (white) color and 4−n empty squares. h = ((Group1 b ∗α)+(Group2 b ∗β)+(Group3 b ∗γ)+(Group4 b ∗∞)) − ((Group1 w ∗α)+(Group2 w ∗β)+(Group3 w ∗γ)+(Group4 w ∗∞)) The values of α, β and δ can vary to form any desired linear combination; however, it is important to value them with the α < β < δ ordering in mind (we used 1, 4, and 8 as their respective values).",
                "Groups of 4 discs of the same color means victory, thus discovery of such a group will result in ∞ to ensure an extreme value.",
                "We now use our estimated evaluation function to evaluate the Black players actions during the Connect-Four adversarial interaction.",
                "Each game from the log file was input into the application, which processed and output a reformatted log file containing the h value of the current move, the maxh value that could be achieved, and a notification if an open threat was detected.",
                "A total of 123 games were analyzed (57 with White winning, and 66 with Black winning).",
                "A few additional games were manually ignored in the experiment, due to these problems: a player abandoning the game while the outcome is not final, or a blunt irrational move in the early stages of the game (e.g., not blocking an obvious winning group in the first opening moves).",
                "In addition, a single tie game was also removed.",
                "The simulator was run to a search depth of 3 moves.",
                "We now proceed to analyze the games with respect to each behavioral axiom.",
                "The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) 555 Table 1: Average heuristic difference analysis Black losses Black Won Avg minh -17.62 -12.02 Avg 3 lowest h moves (min3 h) -13.20 -8.70 3.2.1 Affirming the Suboptimal tactical move axiom The following section presents the heuristic evaluations of the Min-Max algorithm for each action, and checks the amount and extent of suboptimal tactical actions and their implications on performance.",
                "Table 1 shows results and insights from the games heuristic analysis, when search depth equals 3 (this search depth was selected for the results to be comparable to [9], see Section 3.2.3).",
                "The tables heuristic data is the difference between the present maximal heuristic value and the heuristic value of the action that was eventually taken by the player (i.e., the closer the number is to 0, the closer the action was to the maximum heuristic action).",
                "The first row presents the difference values of the action that had the maximal difference value among all the Black players actions in a given game, as averaged over all Blacks winning and losing games (see respective columns).",
                "In games in which the Black player loses, its average difference value was -17.62, while in games in which the Black player won, its average was -12.02.",
                "The second row expands the analysis by considering the 3 highest heuristic difference actions, and averaging them.",
                "In that case, we notice an average heuristic difference of 5 points between games which the Black player loses and games in which it wins.",
                "Nevertheless, the importance of those numbers is that they allowed us to take an educated guess on a threshold number of 11.5, as the value of the TrH constant, which differentiates between normal actions and highly beneficial ones.",
                "After finding an approximated TrH constant, we can proceed with an analysis of the importance of suboptimal moves.",
                "To do so we took the subset of games in which the minimum heuristic difference value for Blacks actions was 11.5.",
                "As presented in Table 2, we can see the different min3 h average of the 3 largest ranges and the respective percentage of games won.",
                "The first row shows that the Black player won only 12% of the games in which the average of its 3 highest heuristically difference actions (min3 h) was smaller than the suggested threshold, TrH = 11.5.",
                "The second row shows a surprising result: it seems that when min3 h > −4 the Black player rarely wins.",
                "Intuition would suggest that games in which the action evaluation values were closer to the maximal values will result in more winning games for Black.",
                "However, it seems that in the Connect-Four domain, merely responding with somewhat easily expected actions, without initiating a few surprising and suboptimal moves, does not yield good results.",
                "The last row sums up the main insights from the analysis; most of Blacks wins (83%) came when its min3 h was in the range of -11.5 to -4.",
                "A close inspection of those Black winning games shows the following pattern behind the numbers: after standard opening moves, Black suddenly drops a disc into an isolated column, which seems a waste of a move.",
                "White continues to build its threats, while usually disregarding Blacks last move, which in turn uses the isolated disc as an anchor for a future winning threat.",
                "The results show that it was beneficial for the Black player Table 2: Blacks winnings percentages % of games min3 h < −11.5 12% min3 h > −4 5% −11.5 ≤ min3 h ≤ −4 83% to take suboptimal actions and not give the current highest possible heuristic value, but will not be too harmful for its position (i.e., will not give high beneficial value to its adversary).",
                "As it turned out, learning the threshold is an important aspect of success: taking wildly risky moves (min3 h < −11.5) or trying to avoid them (min3 h > −4) reduces the Black players winning chances by a large margin. 3.2.2 Affirming the Profile Monitoring Axiom In the task of showing the importance of monitoring ones adversaries profiles, our log files could not be used because they did not contain repeated interactions between players, which are needed to infer the players knowledge about their adversaries.",
                "However, the importance of opponent modeling and its use in attaining tactical advantages was already studied in various domains ([3, 9] are good examples).",
                "In a recent paper, Markovitch and Reger [9] explored the notion of learning and exploitation of opponent weakness in competitive interactions.",
                "They apply simple learning strategies by analyzing examples from past interactions in a specific domain.",
                "They also used the Connect-Four adversarial domain, which can now be used to understand the importance of monitoring the adversarys profile.",
                "Following the presentation of their theoretical model, they describe an extensive empirical study and check the agents performance after learning the weakness model with past examples.",
                "One of the domains used as a competitive environment was the same Connect-Four game (Checkers was the second domain).",
                "Their heuristic function was identical to ours with three different variations (H1, H2, and H3) that are distinguished from one another in their linear combination coefficient values.",
                "The search depth for the players was 3 (as in our analysis).",
                "Their extensive experiments check and compare various learning strategies, risk factors, predefined feature sets and usage methods.",
                "The bottom line is that the Connect-Four domain shows an improvement from a 0.556 winning rate before modeling to a 0.69 after modeling (page 22).",
                "Their conclusions, showing improved performance when holding and using the adversarys model, justify the effort to monitor the adversary profile for continuous and repeated interactions.",
                "An additional point that came up in their experiments is the following: after the opponent weakness model has been learned, the authors describe different methods of integrating the opponent weakness model into the agents decision strategy.",
                "Nevertheless, regardless of the specific method they chose to work with, all integration methods might cause the agent to take suboptimal decisions; it might cause the agent to prefer actions that are suboptimal at the present decision junction, but which might cause the opponent to react in accordance with its weakness model (as represented by our agent) which in turn will be beneficial for us in the future.",
                "The agents behavior, as demonstrated in [9] further confirms and strengthens our Suboptimal Tactical Axiom as discussed in the previous section. 556 The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) 3.2.3 Additional Insights The need for the Goal Achieving, Preventive Act, and Evaluation Maximization axioms are obvious, and need no further verification.",
                "However, even with respect to those axioms, a few interesting insights came up in the log analysis.",
                "The Goal achieving and Preventive Act axioms, though theoretically trivial, seem to provide some challenge to a human player.",
                "In the initial inspection of the logs, we encountered few games2 where a player, for inexplicable reasons, did not block the other from winning or failed to execute its own winning move.",
                "We can blame those faults on the humans lack of attention, or a typing error in its move reply; nevertheless, those errors might occur in bounded rational agents, and the appropriate behavior needs to be axiomatized.",
                "A typical Connect-Four game revolves around generating threats and blocking them.",
                "In our analysis we looked for explicit preventive actions, i.e., moves that block a group of 3 discs, or that remove a future threat (in our limited search horizon).",
                "We found that in 83% of the total games there was at least one preventive action taken by the Black player.",
                "It was also found that Black averaged 2.8 preventive actions per game on the games in which it lost, while averaging 1.5 preventive actions per game when winning.",
                "It seems that Black requires 1 or 2 preventive actions to build its initial taking position, before starting to present threats.",
                "If it did not manage to win, it will usually prevent an extra threat or two before succumbing to White. 4.",
                "RELATED WORK Much research deals with the axiomatization of teamwork and mental states of individuals: some models use knowledge and belief [10], others have models of goals and intentions [8, 4].",
                "However, all these formal theories deal with agent teamwork and cooperation.",
                "As far as we know, our model is the first to provide a formalized model for explicit adversarial environments and agents behavior in it.",
                "The classical Min-Max adversarial search algorithm was the first attempt to integrate the opponent into the search space with a weak assumption of an optimally playing opponent.",
                "Since then, much effort has gone into integrating the opponent model into the decision procedure to predict future behavior.",
                "The M∗ algorithm presented by Carmel and Markovitch [2] showed a method of incorporating opponent models into adversary search, while in [3] they used learning to provide a more accurate opponent model in a 2player repeated game environment, where agents strategies were modeled as finite automata.",
                "Additional Adversarial planning work was done by Willmott et al. [13], which provided an adversarial planning approach to the game of GO.",
                "The research mentioned above dealt with adversarial search and the integration of opponent models into classical utilitybased search methods.",
                "That work shows the importance of opponent modeling and the ability to exploit it to an agents advantage.",
                "However, the basic limitations of those search methods still apply; our model tries to overcome those limitations by presenting a formal model for a new, mental state-based adversarial specification. 5.",
                "CONCLUSIONS We presented an Adversarial Environment model for a 2 These were later removed from the final analysis. bounded rational agent that is situated in an N-player, zerosum environment.",
                "We used the SharedPlans formalization to define the model and the axioms that agents can apply as behavioral guidelines.",
                "The model is meant to be used as a guideline for designing agents that need to operate in such adversarial environments.",
                "We presented empirical results, based on ConnectFour log file analysis, that exemplify the model and the axioms for a bilateral instance of the environment.",
                "The results we presented are a first step towards an expanded model that will cover all types of adversarial environments, for example, environments that are non-zero-sum, and environments that contain natural agents that are not part of the direct conflict.",
                "Those challenges and more will be dealt with in future research. 6.",
                "ACKNOWLEDGMENT This research was supported in part by Israel Science Foundation grants #1211/04 and #898/05. 7.",
                "REFERENCES [1] L. V. Allis.",
                "A knowledge-based approach of Connect-Four - the game is solved: White wins.",
                "Masters thesis, Free University, Amsterdam, The Netherlands, 1988. [2] D. Carmel and S. Markovitch.",
                "Incorporating opponent models into adversary search.",
                "In Proceedings of the Thirteenth National Conference on Artificial Intelligence, pages 120-125, Portland, OR, 1996. [3] D. Carmel and S. Markovitch.",
                "Opponent modeling in multi-agent systems.",
                "In G. Weiß and S. Sen, editors, Adaptation and Learning in Multi-Agent Systems, pages 40-52.",
                "Springer-Verlag, 1996. [4] B. J. Grosz and S. Kraus.",
                "Collaborative plans for complex group action.",
                "Artificial Intelligence, 86(2):269-357, 1996. [5] M. Hadad, G. Kaminka, G. Armon, and S. Kraus.",
                "Supporting collaborative activity.",
                "In Proc. of AAAI-2005, pages 83-88, Pittsburgh, 2005. [6] http://www.gamerz.net/˜pbmserv/. [7] S. Kraus and D. Lehmann.",
                "Designing and building a negotiating automated agent.",
                "Computational Intelligence, 11:132-171, 1995. [8] H. J. Levesque, P. R. Cohen, and J. H. T. Nunes.",
                "On acting together.",
                "In Proc. of AAAI-90, pages 94-99, Boston, MA, 1990. [9] S. Markovitch and R. Reger.",
                "Learning and exploiting relative weaknesses of opponent agents.",
                "Autonomous Agents and Multi-Agent Systems, 10(2):103-130, 2005. [10] Y. M. Ronald Fagin, Joseph Y. Halpern and M. Y. Vardi.",
                "Reasoning about knowledge.",
                "MIT Press, Cambridge, Mass., 1995. [11] P. Thagard.",
                "Adversarial problem solving: Modeling an oponent using explanatory coherence.",
                "Cognitive Science, 16(1):123-149, 1992. [12] R. W. Toseland and R. F. Rivas.",
                "An Introduction to Group Work Practice.",
                "Prentice Hall, Englewood Cliffs, NJ, 2nd edition edition, 1995. [13] S. Willmott, J. Richardson, A. Bundy, and J. Levine.",
                "An adversarial planning approach to Go.",
                "Lecture Notes in Computer Science, 1558:93-112, 1999.",
                "The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) 557"
            ],
            "original_annotated_samples": [],
            "translated_annotated_samples": [],
            "translated_text": "",
            "candidates": [],
            "error": []
        },
        "adversarial interaction": {
            "translated_key": "",
            "is_in_text": true,
            "original_annotated_sentences": [
                "An Adversarial Environment Model for Bounded Rational Agents in Zero-Sum Interactions Inon Zuckerman1 , Sarit Kraus1 , Jeffrey S. Rosenschein2 , Gal Kaminka1 1 Department of Computer Science 2 The School of Engineering Bar-Ilan University and Computer Science Ramat-Gan, Israel Hebrew University, Jerusalem, Israel {zukermi,sarit,galk}@cs.biu.ac.il jeff@cs.huji.ac.il ABSTRACT Multiagent environments are often not cooperative nor collaborative; in many cases, agents have conflicting interests, leading to adversarial interactions.",
                "This paper presents a formal Adversarial Environment model for bounded rational agents operating in a zero-sum environment.",
                "In such environments, attempts to use classical utility-based search methods can raise a variety of difficulties (e.g., implicitly modeling the opponent as an omniscient utility maximizer, rather than leveraging a more nuanced, explicit opponent model).",
                "We define an Adversarial Environment by describing the mental states of an agent in such an environment.",
                "We then present behavioral axioms that are intended to serve as design principles for building such adversarial agents.",
                "We explore the application of our approach by analyzing log files of completed Connect-Four games, and present an empirical analysis of the axioms appropriateness.",
                "Categories and Subject Descriptors I.2.11 [Artificial Intelligence]: Distributed Artificial Intelligence-Intelligent agents,Multiagent Systems; I.2.4 [Artificial Intelligence]: Knowledge Representation Formalisms and Methods -Modal logic General Terms Design, Theory 1.",
                "INTRODUCTION Early research in multiagent systems (MAS) considered cooperative groups of agents; because individual agents had limited resources, or limited access to information (e.g., limited processing power, limited sensor coverage), they worked together by design to solve problems that individually they could not solve, or at least could not solve as efficiently.",
                "MAS research, however, soon began to consider interacting agents with individuated interests, as representatives of different humans or organizations with non-identical interests.",
                "When interactions are guided by diverse interests, participants may have to overcome disagreements, uncooperative interactions, and even intentional attempts to damage one another.",
                "When these types of interactions occur, environments require appropriate behavior from the agents situated in them.",
                "We call these environments Adversarial Environments, and call the clashing agents Adversaries.",
                "Models of cooperation and teamwork have been extensively explored in MAS through the axiomatization of mental states (e.g., [8, 4, 5]).",
                "However, none of this research dealt with adversarial domains and their implications for agent behavior.",
                "Our paper addresses this issue by providing a formal, axiomatized mental state model for a subset of adversarial domains, namely simple zero-sum adversarial environments.",
                "Simple zero-sum encounters exist of course in various twoplayer games (e.g., Chess, Checkers), but they also exist in n-player games (e.g., Risk, Diplomacy), auctions for a single good, and elsewhere.",
                "In these latter environments especially, using a utility-based adversarial search (such as the Min-Max algorithm) does not always provide an adequate solution; the payoff function might be quite complex or difficult to quantify, and there are natural computational limitations on bounded rational agents.",
                "In addition, traditional search methods (like Min-Max) do not make use of a model of the opponent, which has proven to be a valuable addition to adversarial planning [9, 3, 11].",
                "In this paper, we develop a formal, axiomatized model for bounded rational agents that are situated in a zero-sum adversarial environment.",
                "The model uses different modality operators, and its main foundations are the SharedPlans [4] model for collaborative behavior.",
                "We explore environment properties and the mental states of agents to derive behavioral axioms; these behavioral axioms constitute a formal model that serves as a specification and design guideline for agent design in such settings.",
                "We then investigate the behavior of our model empirically using the Connect-Four board game.",
                "We show that this game conforms to our environment definition, and analyze players behavior using a large set of completed match log 550 978-81-904262-7-5 (RPS) c 2007 IFAAMAS files.",
                "In addition, we use the results presented in [9] to discuss the importance of opponent modeling in our ConnectFour adversarial domain.",
                "The paper proceeds as follows.",
                "Section 2 presents the models formalization.",
                "Section 3 presents the empirical analysis and its results.",
                "We discuss related work in Section 4, and conclude and present future directions in Section 5. 2.",
                "ADVERSARIAL ENVIRONMENTS The adversarial environment model (denoted as AE) is intended to guide the design of agents by providing a specification of the capabilities and mental attitudes of an agent in an adversarial environment.",
                "We focus here on specific types of adversarial environments, specified as follows: 1.",
                "Zero-Sum Interactions: positive and negative utilities of all agents sum to zero; 2.",
                "Simple AEs: all agents in the environment are adversarial agents; 3.",
                "Bilateral AEs: AEs with exactly two agents; 4.",
                "Multilateral AEs: AEs of three or more agents.",
                "We will work on both bilateral and multilateral instantiations of zero-sum and simple environments.",
                "In particular, our adversarial environment model will deal with interactions that consist of N agents (N ≥ 2), where all agents are adversaries, and only one agent can succeed.",
                "Examples of such environments range from board games (e.g., Chess, Connect-Four, and Diplomacy) to certain economic environments (e.g., N-bidder auctions over a single good). 2.1 Model Overview Our approach is to formalize the mental attitudes and behaviors of a single adversarial agent; we consider how a single agent perceives the AE.",
                "The following list specifies the conditions and mental states of an agent in a simple, zero-sum AE: 1.",
                "The agent has an individual intention that its own goal will be completed; 2.",
                "The agent has an individual belief that it and its adversaries are pursuing full conflicting goals (defined below)there can be only one winner; 3.",
                "The agent has an individual belief that each adversary has an intention to complete its own full conflicting goal; 4.",
                "The agent has an individual belief in the (partial) profile of its adversaries.",
                "Item 3 is required, since it might be the case that some agent has a full conflicting goal, and is currently considering adopting the intention to complete it, but is, as of yet, not committed to achieving it.",
                "This might occur because the agent has not yet deliberated about the effects that adopting that intention might have on the other intentions it is currently holding.",
                "In such cases, it might not consider itself to even be in an adversarial environment.",
                "Item 4 states that the agent should hold some belief about the profiles of its adversaries.",
                "The profile represents all the knowledge the agent has about its adversary: its weaknesses, strategic capabilities, goals, intentions, trustworthiness, and more.",
                "It can be given explicitly or can be learned from observations of past encounters. 2.2 Model Definitions for Mental States We use Grosz and Krauss definitions of the modal operators, predicates, and meta-predicates, as defined in their SharedPlan formalization [4].",
                "We recall here some of the predicates and operators that are used in that formalization: Int.To(Ai, α, Tn, Tα, C) represents Ais intentions at time Tn to do an action α at time Tα in the context of C. Int.Th(Ai, prop, Tn, Tprop, C) represents Ais intentions at time Tn that a certain proposition prop holds at time Tprop in the context of C. The potential intention operators, Pot.Int.To(...) and Pot.Int.Th(...), are used to represent the mental state when an agent considers adopting an intention, but has not deliberated about the interaction of the other intentions it holds.",
                "The operator Bel(Ai, f, Tf ) represents agent Ai believing in the statement expressed in formula f, at time Tf .",
                "MB(A, f, Tf ) represents mutual belief for a group of agents A.",
                "A snapshot of the system finds our environment to be in some state e ∈ E of environmental variable states, and each adversary in any LAi ∈ L of possible local states.",
                "At any given time step, the system will be in some world w of the set of all possible worlds w ∈ W, where w = E×LA1 ×LA2 × ...LAn , and n is the number of adversaries.",
                "For example, in a Texas Holdem poker game, an agents local state might be its own set of cards (which is unknown to its adversary) while the environment will consist of the betting pot and the community cards (which are visible to both players).",
                "A utility function under this formalization is defined as a mapping from a possible world w ∈ W to an element in , which expresses the desirability of the world, from a single agent perspective.",
                "We usually normalize the range to [0,1], where 0 represents the least desirable possible world, and 1 is the most desirable world.",
                "The implementation of the utility function is dependent on the domain in question.",
                "The following list specifies new predicates, functions, variables, and constants used in conjunction with the original definitions for the adversarial environment formalization: 1. φ is a null action (the agent does not do anything). 2.",
                "GAi is the set of agent Ais goals.",
                "Each goal is a set of predicates whose satisfaction makes the goal complete (we use G∗ Ai ∈ GAi to represent an arbitrary goal of agent Ai). 3. gAi is the set of agent Ais subgoals.",
                "Subgoals are predicates whose satisfaction represents an important milestone toward achievement of the full goal. gG∗ Ai ⊆ gAi is the set of subgoals that are important to the completion of goal G∗ Ai (we will use g∗ G∗ Ai ∈ gG∗ Ai to represent an arbitrary subgoal). 4.",
                "P Aj Ai is the profile object agent Ai holds about agent Aj. 5.",
                "CA is a general set of actions for all agents in A which are derived from the environments constraints.",
                "CAi ⊆ CA is the set of agent Ais possible actions. 6.",
                "Do(Ai, α, Tα, w) holds when Ai performs action α over time interval Tα in world w. 7.",
                "Achieve(G∗ Ai , α, w) is true when goal G∗ Ai is achieved following the completion of action α in world w ∈ W, where α ∈ CAi . 8.",
                "Profile(Ai, PAi Ai ) is true when agent Ai holds an object profile for agent Aj.",
                "Definition 1.",
                "Full conflict (FulConf ) describes a zerosum interaction where only a single goal of the goals in conflict can be completed.",
                "FulConf(G∗ Ai , G∗ Aj ) ⇒ (∃α ∈ CAi , ∀w, β ∈ CAj ) (Achieve(G∗ Ai , α, w) ⇒ ¬Achieve(G∗ Aj , β, w)) ∨ (∃β ∈ CAj , ∀w, α ∈ CAi )(Achieve(G∗ Aj , β, w) ⇒ ¬Achieve(G∗ Ai , α, w)) Definition 2.",
                "Adversarial Knowledge (AdvKnow) is a function returning a value which represents the amount of The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) 551 knowledge agent Ai has on the profile of agent Aj, at time Tn.",
                "The higher the value, the more knowledge agent Ai has.",
                "AdvKnow : P Aj Ai × Tn → Definition 3.",
                "Eval - This evaluation function returns an estimated expected utility value for an agent in A, after completing an action from CA in some world state w. Eval : A × CA × w → Definition 4.",
                "TrH - (Threshold) is a numerical constant in the [0,1] range that represents an evaluation function (Eval) threshold value.",
                "An action that yields an estimated utility evaluation above the TrH is regarded as a highly beneficial action.",
                "The Eval value is an estimation and not the real utility function, which is usually unknown.",
                "Using the real utility value for a rational agent would easily yield the best outcome for that agent.",
                "However, agents usually do not have the real utility functions, but rather a heuristic estimate of it.",
                "There are two important properties that should hold for the evaluation function: Property 1.",
                "The evaluation function should state that the most desirable world state is one in which the goal is achieved.",
                "Therefore, after the goal has been satisfied, there can be no future action that can put the agent in a world state with higher Eval value. (∀Ai, G∗ Ai , α, β ∈ CAi , w ∈ W) Achieve(G∗ Ai , α, w) ⇒ Eval(Ai, α, w) ≥ Eval(Ai, β, w) Property 2.",
                "The evaluation function should project an action that causes a completion of a goal or a subgoal to a value which is greater than TrH (a highly beneficial action). (∀Ai, G∗ Ai ∈ GAi , α ∈ CAi , w ∈ W, g∗ GAi ∈ gGAi ) Achieve(G∗ Ai , α, w) ∨ Achieve(g∗ GAi , α, w) ⇒ Eval(Ai, α, w) ≥ TrH.",
                "Definition 5.",
                "SetAction We define a set action (SetAction) as a set of action operations (either complex or basic actions) from some action sets CAi and CAj which, according to agent Ais belief, are attached together by a temporal and consequential relationship, forming a chain of events (action, and its following consequent action). (∀α1 , . . . , αu ∈ CAi , β1 , . . . , βv ∈ CAj , w ∈ W) SetAction(α1 , . . . , αu , β1 , . . . , βv , w) ⇒ ((Do(Ai, α1 , Tα1 , w) ⇒ Do(Aj, β1 , Tβ1 , w)) ⇒ Do(Ai, α2 , Tα2 , w) ⇒ . . . ⇒ Do(Ai, αu , Tαu , w)) The consequential relation might exist due to various environmental constraints (when one action forces the adversary to respond with a specific action) or due to the agents knowledge about the profile of its adversary.",
                "Property 3.",
                "As the knowledge we have about our adversary increases we will have additional beliefs about its behavior in different situations which in turn creates new set actions.",
                "Formally, if our AdvKnow at time Tn+1 is greater than AdvKnow at time Tn, then every SetAction known at time Tn is also known at time Tn+1.",
                "AdvKnow(P Aj Ai , Tn+1) > AdvKnow(P Aj Ai , Tn) ⇒ (∀α1 , . . . , αu ∈ CAi , β1 , . . . , βv ∈ CAj ) Bel(Aag, SetAction(α1 , . . . , αu , β1 , . . . , βv ), Tn) ⇒ Bel(Aag, SetAction(α1 , . . . , αu , β1 , . . . , βv ), Tn+1) 2.3 The Environment Formulation The following axioms provide the formal definition for a simple, zero-sum Adversarial Environment (AE).",
                "Satisfaction of these axioms means that the agent is situated in such an environment.",
                "It provides specifications for agent Aag to interact with its set of adversaries A with respect to goals G∗ Aag and G∗ A at time TCo at some world state w. AE(Aag, A, G∗ Aag , A1, . . . , Ak, G∗ A1 , . . . , G∗ Ak , Tn, w) 1.",
                "Aag has an Int.Th his goal would be completed: (∃α ∈ CAag , Tα) Int.Th(Aag, Achieve(G∗ Aag , α), Tn, Tα, AE) 2.",
                "Aag believes that it and each of its adversaries Ao are pursuing full conflicting goals: (∀Ao ∈ {A1, . . . , Ak}) Bel(Aag, FulConf(G∗ Aag , G∗ Ao ), Tn) 3.",
                "Aag believes that each of his adversaries in Ao has the Int.Th his conflicting goal G∗ Aoi will be completed: (∀Ao ∈ {A1, . . . , Ak})(∃β ∈ CAo , Tβ) Bel(Aag, Int.Th(Ao, Achieve(G∗ Ao , β), TCo, Tβ, AE), Tn) 4.",
                "Aag has beliefs about the (partial) profiles of its adversaries (∀Ao ∈ {A1, . . . , Ak}) (∃PAo Aag ∈ PAag )Bel(Aag, Profile(Ao, PAo Aag ), Tn) To build an agent that will be able to operate successfully within such an AE, we must specify behavioral guidelines for its interactions.",
                "Using a naive Eval maximization strategy to a certain search depth will not always yield satisfactory results for several reasons: (1) the search horizon problem when searching for a fixed depth; (2) the strong assumption of an optimally rational, unbounded resources adversary; (3) using an estimated evaluation function which will not give optimal results in all world states, and can be exploited [9].",
                "The following axioms specify the behavioral principles that can be used to differentiate between successful and less successful agents in the above Adversarial Environment.",
                "Those axioms should be used as specification principles when designing and implementing agents that should be able to perform well in such Adversarial Environments.",
                "The behavioral axioms represent situations in which the agent will adopt potential intentions to (Pot.Int.To(...)) perform an action, which will typically require some means-end reasoning to select a possible course of action.",
                "This reasoning will lead to the adoption of an Int.To(...) (see [4]).",
                "A1.",
                "Goal Achieving Axiom.",
                "The first axiom is the simplest case; when the agent Aag believes that it is one action (α) away from achieving his conflicting goal G∗ Aag , it should adopt the potential intention to do α and complete its goal. (∀Aag, α ∈ CAag , Tn, Tα, w ∈ W) (Bel(Aag, Do(Aag, α, Tα, w) ⇒ Achieve(G∗ Aag , α, w)) ⇒ Pot.Int.To(Aag, α, Tn, Tα, w) This somewhat trivial behavior is the first and strongest axiom.",
                "In any situation, when the agent is an action away from completing the goal, it should complete the action.",
                "Any fair Eval function would naturally classify α as the maximal value action (property 1).",
                "However, without explicit axiomatization of such behavior there might be situations where the agent will decide on taking another action for various reasons, due to its bounded decision resources.",
                "A2.",
                "Preventive Act Axiom.",
                "Being in an adversarial situation, agent Aag might decide to take actions that will damage one of its adversarys plans to complete its goal, even if those actions do not explicitly advance Aag towards its conflicting goal G∗ Aag .",
                "Such preventive action will take place when agent Aag has a belief about the possibility of its adversary Ao doing an action β that will give it a high 552 The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) utility evaluation value (> TrH).",
                "Believing that taking action α will prevent the opponent from doing its β, it will adopt a potential intention to do α. (∀Aag, Ao ∈ A, α ∈ CAag , β ∈ CAo , Tn, Tβ, w ∈ W) (Bel(Aag, Do(Ao, β, Tβ, w) ∧ Eval(Ao, β, w) > TrH, Tn) ∧ Bel(Aag, Do(Aag, α, Tα, w) ⇒ ¬Do(Ao, β, Tβ, w), Tn) ⇒ Pot.Int.To(Aag, α, Tn, Tα, w) This axiom is a basic component of any adversarial environment.",
                "For example, looking at a Chess board game, a player could realize that it is about to be checkmated by its opponent, thus making a preventive move.",
                "Another example is a Connect Four game: when a player has a row of three chips, its opponent must block it, or lose.",
                "A specific instance of A1 occurs when the adversary is one action away from achieving its goal, and immediate preventive action needs to be taken by the agent.",
                "Formally, we have the same beliefs as stated above, with a changed belief that doing action β will cause agent Ao to achieve its goal.",
                "Proposition 1: Prevent or lose case. (∀Aag, Ao ∈ A, α ∈ CAag , β ∈ CAo , G∗ Ao , Tn, Tα, Tβ, w ∈ W) Bel(Aag, Do(Ao, β, Tβ, w) ⇒ Achieve(G∗ Ao , β, w), Tn) ∧ Bel(Aag, Do(Aag, α, Tα, w) ⇒ ¬Do(Ao, β, Tβ, w)) ⇒ Pot.Int.To(Aag, α, Tn, Tα, w) Sketch of proof: Proposition 1 can be easily derived from axiom A1 and the property 2 of the Eval function, which states that any action that causes a completion of a goal is a highly beneficial action.",
                "The preventive act behavior will occur implicitly when the Eval function is equal to the real world utility function.",
                "However, being bounded rational agents and dealing with an estimated evaluation function we need to explicitly axiomatize such behavior, for it will not always occur implicitly from the evaluation function.",
                "A3.",
                "Suboptimal Tactical Move Axiom.",
                "In many scenarios a situation may occur where an agent will decide not to take the current most beneficial action it can take (the action with the maximal utility evaluation value), because it believes that taking another action (with lower utility evaluation value) might yield (depending on the adversarys response) a future possibility for a highly beneficial action.",
                "This will occur most often when the Eval function is inaccurate and differs by a large extent from the Utility function.",
                "Put formally, agent Aag believes in a certain SetAction that will evolve according to its initial action and will yield a high beneficial value (> TrH) solely for it. (∀Aag, Ao ∈ A, Tn, w ∈ W) (∃α1 , . . . , αu ∈ CAi , β1 , . . . , βv ∈ CAj , Tα1 ) Bel(Aag, SetAction(α1 , . . . , αu , β1 , . . . , βv ), Tn) ∧ Bel(Aag, Eval(Ao, βv , w) < TrH < Eval(Aag, αu , w), Tn) ⇒ Pot.Int.To(Aag, α1 , Tn, Tα1 , w) An agent might believe that a chain of events will occur for various reasons due to the inevitable nature of the domain.",
                "For example, in Chess, we often observe the following: a move causes a check position, which in turn limits the opponents moves to avoiding the check, to which the first player might react with another check, and so on.",
                "The agent might also believe in a chain of events based on its knowledge of its adversarys profile, which allows it to foresee the adversarys movements with high accuracy.",
                "A4.",
                "Profile Detection Axiom.",
                "The agent can adjust its adversarys profiles by observations and pattern study (specifically, if there are repeated encounters with the same adversary).",
                "However, instead of waiting for profile information to be revealed, an agent can also initiate actions that will force its adversary to react in a way that will reveal profile knowledge about it.",
                "Formally, the axiom states that if all actions (γ) are not highly beneficial actions (< TrH), the agent can do action α in time Tα if it believes that it will result in a non-highly beneficial action β from its adversary, which in turn teaches it about the adversarys profile, i.e., gives a higher AdvKnow(P Aj Ai , Tβ). (∀Aag, Ao ∈ A, α ∈ CAag , β ∈ CAo , Tn, Tα, Tβ, w ∈ W) Bel(Aag, (∀γ ∈ CAag )Eval(Aag, γ, w) < TrH, Tn) ∧ Bel(Aag, Do(Aag, α, Tα, w) ⇒ Do(Ao, β, Tβ, w), Tn) ∧ Bel(Aag, Eval(Ao, β, w) < TrH) ∧ Bel(Aag, AdvKnow(P Aj Ai , Tβ) > AdvKnow(P Aj Ai , Tn), Tn) ⇒ Pot.Int.To(Aag, α, Tn, Tα, w) For example, going back to the Chess board game scenario, consider starting a game versus an opponent about whom we know nothing, not even if it is a human or a computerized opponent.",
                "We might start playing a strategy that will be suitable versus an average opponent, and adjust our game according to its level of play.",
                "A5.",
                "Alliance Formation Axiom The following behavioral axiom is relevant only in a multilateral instantiation of the adversarial environment (obviously, an alliance cannot be formed in a bilateral, zero-sum encounter).",
                "In different situations during a multilateral interaction, a group of agents might believe that it is in their best interests to form a temporary alliance.",
                "Such an alliance is an agreement that constrains its members behavior, but is believed by its members to enable them to achieve a higher utility value than the one achievable outside of the alliance.",
                "As an example, we can look at the classical Risk board game, where each player has an individual goal of being the sole conquerer of the world, a zero-sum game.",
                "However, in order to achieve this goal, it might be strategically wise to make short-term ceasefire agreements with other players, or to join forces and attack an opponent who is stronger than the rest.",
                "An alliances terms defines the way its members should act.",
                "It is a set of predicates, denoted as Terms, that is agreed upon by the alliance members, and should remain true for the duration of the alliance.",
                "For example, the set Terms in the Risk scenario, could contain the following predicates: 1.",
                "Alliance members will not attack each other on territories X, Y and Z; 2.",
                "Alliance members will contribute C units per turn for attacking adversary Ao; 3.",
                "Members are obligated to stay as part of the alliance until time Tk or until adversarys Ao army is smaller than Q.",
                "The set Terms specifies inter-group constraints on each of the alliance members (∀Aal i ∈ Aal ⊆ A) set of actions Cal i ⊆ C. Definition 6.",
                "Al val - the total evaluation value that agent Ai will achieve while being part of Aal is the sum of Evali (Eval for Ai) of each of Aal j Eval values after taking their own α actions (via the agent(α) predicate): Al val(Ai, Cal , Aal , w) = α∈Cal Evali(Aal j , agent(α), w) Definition 7.",
                "Al TrH - is a number representing an Al val The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) 553 threshold; above it, the alliance can be said to be a highly beneficial alliance.",
                "The value of Al TrH will be calculated dynamically according to the progress of the interaction, as can be seen in [7].",
                "After an alliance is formed, its members are now working in their normal adversarial environment, as well as according to the mental states and axioms required for their interactions as part of the alliance.",
                "The following Alliance model (AL) specifies the conditions under which the group Aal can be said to be in an alliance and working with a new and constrained set of actions Cal , at time Tn.",
                "AL(Aal , Cal , w, Tn) 1.",
                "Aal has a MB that all members are part of Aal : MB(Aal , (∀Aal i ∈ Aal )member(Aal i , Aal ), Tn) 2.",
                "Aal has a MB that the group be maintained: MB(Aal , (∀Aal i ∈ Aal )Int.Th (Ai, member(Ai, Aal ), Tn, Tn+1, Co), Tn) 3.",
                "Aal has a MB that being members gives them high utility value: MB(Aal , (∀Aal i ∈ Aal )Al val(Aal i , Cal , Aal , w) ≥ Al TrH, Tn) Members profiles are a crucial part of successful alliances.",
                "We assume that agents that have more accurate profiles of their adversaries will be more successful in such environments.",
                "Such agents will be able to predict when a member is about to breach the alliances contract (item 2 in the above model), and take counter measures (when item 3 will falsify).",
                "The robustness of the alliance is in part a function of its members trustfulness measure, objective position estimation, and other profile properties.",
                "We should note that an agent can simultaneously be part of more than one alliance.",
                "Such a temporary alliance, where the group members do not have a joint goal but act collaboratively for the interest of their own individual goals, is classified as a Treatment Group by modern psychologists [12] (in contrast to a Task Group, where its members have a joint goal).",
                "The Shared Activity model as presented in [5] modeled Treatment Group behavior using the same SharedPlans formalization.",
                "When comparing both definitions of an alliance and a Treatment Group we found an unsurprising resemblance between both models: the environment models definitions are almost identical (see SAs definitions in [5]), and their Selfish-Act and Cooperative Act axioms conform to our adversarial agents behavior.",
                "The main distinction between both models is the integration of a Helpful-behavior act axiom, in the Shared Activity which cannot be part of ours.",
                "This axiom states that an agent will consider taking action that will lower its Eval value (to a certain lower bound), if it believes that a group partner will gain a significant benefit.",
                "Such behavior cannot occur in a pure adversarial environment (as a zero-sum game is), where the alliance members are constantly on watch to manipulate their alliance to their own advantage.",
                "A6.",
                "Evaluation Maximization Axiom.",
                "In a case when all other axioms are inapplicable, we will proceed with the action that maximizes the heuristic value as computed in the Eval function. (∀Aag, Ao ∈ A, α ∈ Cag, Tn, w ∈ W) Bel(Aag, (∀γ ∈ Cag)Eval(Aag, α, w) ≥ Eval(Aag, γ, w), Tn) ⇒ Pot.Int.To(Aag, α, Tn, Tα, w) T1.",
                "Optimality on Eval = Utility The above axiomatic model handles situations where the Utility is unknown and the agents are bounded rational agents.",
                "The following theorem shows that in bilateral interactions, where the agents have the real Utility function (i.e., Eval = Utility) and are rational agents, the axioms provide the same optimal result as classic adversarial search (e.g., Min-Max).",
                "Theorem 1.",
                "Let Ae ag be an unbounded rational AE agent using the Eval heuristic evaluation function, Au ag be the same agent using the true Utility function, and Ao be a sole unbounded utility-based rational adversary.",
                "Given that Eval = Utility: (∀α ∈ CAu ag , α ∈ CAe ag , Tn, w ∈ W) Pot.Int.To(Au ag, α, Tn, Tα, w) → Pot.Int.To(Ae ag, α , Tn, Tα, w) ∧ ((α = α ) ∨ (Utility(Au ag, α, w) = Eval(Ae ag, α , w))) Sketch of proof - Given that Au ag has the real utility function and unbounded resources, it can generate the full game tree and run the optimal MinMax algorithm to choose the highest utility value action, which we denote by, α.",
                "The proof will show that Ae ag, using the AE axioms, will select the same or equal utility α (when there is more than one action with the same max utility) when Eval = Utility. (A1) Goal achieving axiom - suppose there is an α such that its completion will achieve Au ags goal.",
                "It will obtain the highest utility by Min-Max for Au ag.",
                "The Ae ag agent will select α or another action with the same utility value via A1.",
                "If such α does not exist, Ae ag cannot apply this axiom, and proceeds to A2. (A2) Preventive act axiom - (1) Looking at the basic case (see Prop1), if there is a β which leads Ao to achieve its goal, then a preventive action α will yield the highest utility for Au ag.",
                "Au ag will choose it through the utility, while Ae ag will choose it through A2. (2) In the general case, β is a highly beneficial action for Ao, thus yields low utility for Au ag, which will guide it to select an α that will prevent β, while Ae ag will choose it through A2.1 If such β does not exist for Ao, then A2 is not applicable, and Ae ag can proceed to A3. (A3) Suboptimal tactical move axiom - When using a heuristic Eval function, Ae ag has a partial belief in the profile of its adversary (item 4 in AE model), which may lead it to believe in SetActions (Prop1).",
                "In our case, Ae ag is holding a full profile on its optimal adversary and knows that Ao will behave optimally according to the real utility values on the complete search tree, therefore, any belief about suboptimal SetAction cannot exist, yielding this axiom inapplicable.",
                "Ae ag will proceed to A4. (A4) Profile detection axiom - Given that Ae ag has the full profile of Ao, none of Ae ags actions can increase its knowledge.",
                "That axiom will not be applied, and the agent will proceed with A6 (A5 will be disregarded because the interaction is bilateral). (A6) Evaluation maximization axiom - This axiom will select the max Eval for Ae ag.",
                "Given that Eval = Utility, the same α that was selected by Au ag will be selected. 3.",
                "EVALUATION The main purpose of our experimental analysis is to evaluate the models behavior and performance in a real adversarial environment.",
                "This section investigates whether bounded 1 A case where following the completion of β there exists a γ which gives high utility for Agent Au ag, cannot occur because Ao uses the same utility, and γs existence will cause it to classify β as a low utility action. 554 The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) rational agents situated in such adversarial environments will be better off applying our suggested behavioral axioms. 3.1 The Domain To explore the use of the above model and its behavioral axioms, we decided to use the Connect-Four game as our adversarial environment.",
                "Connect-Four is a 2-player, zerosum game which is played using a 6x7 matrix-like board.",
                "Each turn, a player drops a disc into one of the 7 columns (the set of 21 discs is usually colored yellow for player 1 and red for player 2; we will use White and Black respectively to avoid confusion).",
                "The winner is the first player to complete a horizontal, vertical, or diagonal set of four discs with its color.",
                "On very rare occasions, the game might end in a tie if all the empty grids are filled, but no player managed to create a 4-disc set.",
                "The Connect-Four game was solved in [1], where it is shown that the first player (playing with the white discs) can force a win by starting in the middle column (column 4) and playing optimally However, the optimal strategy is very complex, and difficult to follow even for complex bounded rational agents, such as human players.",
                "Before we can proceed checking agent behavior, we must first verify that the domain conforms to the adversarial environments definition as given above (which the behavioral axioms are based on).",
                "First, when playing a Connect-Four game, the agent has an intention to win the game (item 1).",
                "Second (item 2), our agent believes that in Connect-Four there can only be one winner (or no winner at all in the rare occurrence of a tie).",
                "In addition, our agent believes that its opponent to the game will try to win (item 3), and we hope it has some partial knowledge (item 4) about its adversary (this knowledge can vary from nothing, through simple facts such as age, to strategies and weaknesses).",
                "Of course, not all Connect-Four encounters are adversarial.",
                "For example, when a parent is playing the game with its child, the following situation might occur: the child, having a strong incentive to win, treats the environment as adversarial (it intends to win, understands that there can only be one winner, and believes that its parent is trying to beat him).",
                "However, the parents point of view might see the environment as an educational one, where its goal is not to win the game, but to cause enjoyment or practice strategic reasoning.",
                "In such an educational environment, a new set of behavioral axioms might be more beneficial to the parents goals than our suggested adversarial behavioral axioms. 3.2 Axiom Analysis After showing that the Connect-Four game is indeed a zero-sum, bilateral adversarial environment, the next step is to look at players behaviors during the game and check whether behaving according to our model does improve performance.",
                "To do so we have collected log files from completed Connect-Four games that were played by human players over the Internet.",
                "Our collected log file data came from Play by eMail (PBeM) sites.",
                "These are web sites that host email games, where each move is taken by an email exchange between the server and the players.",
                "Many such sites archives contain real competitive interactions, and also maintain a ranking system for their members.",
                "Most of the data we used can be found in [6].",
                "As can be learned from [1], Connect-Four has an optimal strategy and a considerable advantage for the player who starts the game (which we call the White player).",
                "We will concentrate in our analysis on the second players moves (to be called Black).",
                "The White player, being the first to act, has the so-called initiative advantage.",
                "Having the advantage and a good strategy will keep the Black player busy reacting to its moves, instead of initiating threats.",
                "A threat is a combination of three discs of the same color, with an empty spot for the fourth winning disk.",
                "An open threat is a threat that can be realized in the opponents next move.",
                "In order for the Black player to win, it must somehow turn the tide, take the advantage and start presenting threats to the White player.",
                "We will explore Black players behavior and their conformance to our axioms.",
                "To do so, we built an application that reads log files and analyzes the Black players moves.",
                "The application contains two main components: (1) a Min-Max algorithm for evaluation of moves; (2) open threats detector for the discovering of open threats.",
                "The Min-Max algorithm will work to a given depth, d and for each move α will output the heuristic value for the next action taken by the player as written in the log file, h(α), alongside the maximum heuristic value, maxh(α), that could be achieved prior to taking the move (obviously, if h(α) = maxh(α), then the player did not do the optimal move heuristically).",
                "The threat detectors job is to notify if some action was taken in order to block an open threat (not blocking an open threat will probably cause the player to lose in the opponents next move).",
                "The heuristic function used by Min-Max to evaluate the players utility is the following function, which is simple to compute, yet provides a reasonable challenge to human opponents: Definition 8.",
                "Let Group be an adjacent set of four squares that are horizontal, vertical, or diagonal.",
                "Groupn b (Groupn w) be a Group with n pieces of the black (white) color and 4−n empty squares. h = ((Group1 b ∗α)+(Group2 b ∗β)+(Group3 b ∗γ)+(Group4 b ∗∞)) − ((Group1 w ∗α)+(Group2 w ∗β)+(Group3 w ∗γ)+(Group4 w ∗∞)) The values of α, β and δ can vary to form any desired linear combination; however, it is important to value them with the α < β < δ ordering in mind (we used 1, 4, and 8 as their respective values).",
                "Groups of 4 discs of the same color means victory, thus discovery of such a group will result in ∞ to ensure an extreme value.",
                "We now use our estimated evaluation function to evaluate the Black players actions during the Connect-Four <br>adversarial interaction</br>.",
                "Each game from the log file was input into the application, which processed and output a reformatted log file containing the h value of the current move, the maxh value that could be achieved, and a notification if an open threat was detected.",
                "A total of 123 games were analyzed (57 with White winning, and 66 with Black winning).",
                "A few additional games were manually ignored in the experiment, due to these problems: a player abandoning the game while the outcome is not final, or a blunt irrational move in the early stages of the game (e.g., not blocking an obvious winning group in the first opening moves).",
                "In addition, a single tie game was also removed.",
                "The simulator was run to a search depth of 3 moves.",
                "We now proceed to analyze the games with respect to each behavioral axiom.",
                "The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) 555 Table 1: Average heuristic difference analysis Black losses Black Won Avg minh -17.62 -12.02 Avg 3 lowest h moves (min3 h) -13.20 -8.70 3.2.1 Affirming the Suboptimal tactical move axiom The following section presents the heuristic evaluations of the Min-Max algorithm for each action, and checks the amount and extent of suboptimal tactical actions and their implications on performance.",
                "Table 1 shows results and insights from the games heuristic analysis, when search depth equals 3 (this search depth was selected for the results to be comparable to [9], see Section 3.2.3).",
                "The tables heuristic data is the difference between the present maximal heuristic value and the heuristic value of the action that was eventually taken by the player (i.e., the closer the number is to 0, the closer the action was to the maximum heuristic action).",
                "The first row presents the difference values of the action that had the maximal difference value among all the Black players actions in a given game, as averaged over all Blacks winning and losing games (see respective columns).",
                "In games in which the Black player loses, its average difference value was -17.62, while in games in which the Black player won, its average was -12.02.",
                "The second row expands the analysis by considering the 3 highest heuristic difference actions, and averaging them.",
                "In that case, we notice an average heuristic difference of 5 points between games which the Black player loses and games in which it wins.",
                "Nevertheless, the importance of those numbers is that they allowed us to take an educated guess on a threshold number of 11.5, as the value of the TrH constant, which differentiates between normal actions and highly beneficial ones.",
                "After finding an approximated TrH constant, we can proceed with an analysis of the importance of suboptimal moves.",
                "To do so we took the subset of games in which the minimum heuristic difference value for Blacks actions was 11.5.",
                "As presented in Table 2, we can see the different min3 h average of the 3 largest ranges and the respective percentage of games won.",
                "The first row shows that the Black player won only 12% of the games in which the average of its 3 highest heuristically difference actions (min3 h) was smaller than the suggested threshold, TrH = 11.5.",
                "The second row shows a surprising result: it seems that when min3 h > −4 the Black player rarely wins.",
                "Intuition would suggest that games in which the action evaluation values were closer to the maximal values will result in more winning games for Black.",
                "However, it seems that in the Connect-Four domain, merely responding with somewhat easily expected actions, without initiating a few surprising and suboptimal moves, does not yield good results.",
                "The last row sums up the main insights from the analysis; most of Blacks wins (83%) came when its min3 h was in the range of -11.5 to -4.",
                "A close inspection of those Black winning games shows the following pattern behind the numbers: after standard opening moves, Black suddenly drops a disc into an isolated column, which seems a waste of a move.",
                "White continues to build its threats, while usually disregarding Blacks last move, which in turn uses the isolated disc as an anchor for a future winning threat.",
                "The results show that it was beneficial for the Black player Table 2: Blacks winnings percentages % of games min3 h < −11.5 12% min3 h > −4 5% −11.5 ≤ min3 h ≤ −4 83% to take suboptimal actions and not give the current highest possible heuristic value, but will not be too harmful for its position (i.e., will not give high beneficial value to its adversary).",
                "As it turned out, learning the threshold is an important aspect of success: taking wildly risky moves (min3 h < −11.5) or trying to avoid them (min3 h > −4) reduces the Black players winning chances by a large margin. 3.2.2 Affirming the Profile Monitoring Axiom In the task of showing the importance of monitoring ones adversaries profiles, our log files could not be used because they did not contain repeated interactions between players, which are needed to infer the players knowledge about their adversaries.",
                "However, the importance of opponent modeling and its use in attaining tactical advantages was already studied in various domains ([3, 9] are good examples).",
                "In a recent paper, Markovitch and Reger [9] explored the notion of learning and exploitation of opponent weakness in competitive interactions.",
                "They apply simple learning strategies by analyzing examples from past interactions in a specific domain.",
                "They also used the Connect-Four adversarial domain, which can now be used to understand the importance of monitoring the adversarys profile.",
                "Following the presentation of their theoretical model, they describe an extensive empirical study and check the agents performance after learning the weakness model with past examples.",
                "One of the domains used as a competitive environment was the same Connect-Four game (Checkers was the second domain).",
                "Their heuristic function was identical to ours with three different variations (H1, H2, and H3) that are distinguished from one another in their linear combination coefficient values.",
                "The search depth for the players was 3 (as in our analysis).",
                "Their extensive experiments check and compare various learning strategies, risk factors, predefined feature sets and usage methods.",
                "The bottom line is that the Connect-Four domain shows an improvement from a 0.556 winning rate before modeling to a 0.69 after modeling (page 22).",
                "Their conclusions, showing improved performance when holding and using the adversarys model, justify the effort to monitor the adversary profile for continuous and repeated interactions.",
                "An additional point that came up in their experiments is the following: after the opponent weakness model has been learned, the authors describe different methods of integrating the opponent weakness model into the agents decision strategy.",
                "Nevertheless, regardless of the specific method they chose to work with, all integration methods might cause the agent to take suboptimal decisions; it might cause the agent to prefer actions that are suboptimal at the present decision junction, but which might cause the opponent to react in accordance with its weakness model (as represented by our agent) which in turn will be beneficial for us in the future.",
                "The agents behavior, as demonstrated in [9] further confirms and strengthens our Suboptimal Tactical Axiom as discussed in the previous section. 556 The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) 3.2.3 Additional Insights The need for the Goal Achieving, Preventive Act, and Evaluation Maximization axioms are obvious, and need no further verification.",
                "However, even with respect to those axioms, a few interesting insights came up in the log analysis.",
                "The Goal achieving and Preventive Act axioms, though theoretically trivial, seem to provide some challenge to a human player.",
                "In the initial inspection of the logs, we encountered few games2 where a player, for inexplicable reasons, did not block the other from winning or failed to execute its own winning move.",
                "We can blame those faults on the humans lack of attention, or a typing error in its move reply; nevertheless, those errors might occur in bounded rational agents, and the appropriate behavior needs to be axiomatized.",
                "A typical Connect-Four game revolves around generating threats and blocking them.",
                "In our analysis we looked for explicit preventive actions, i.e., moves that block a group of 3 discs, or that remove a future threat (in our limited search horizon).",
                "We found that in 83% of the total games there was at least one preventive action taken by the Black player.",
                "It was also found that Black averaged 2.8 preventive actions per game on the games in which it lost, while averaging 1.5 preventive actions per game when winning.",
                "It seems that Black requires 1 or 2 preventive actions to build its initial taking position, before starting to present threats.",
                "If it did not manage to win, it will usually prevent an extra threat or two before succumbing to White. 4.",
                "RELATED WORK Much research deals with the axiomatization of teamwork and mental states of individuals: some models use knowledge and belief [10], others have models of goals and intentions [8, 4].",
                "However, all these formal theories deal with agent teamwork and cooperation.",
                "As far as we know, our model is the first to provide a formalized model for explicit adversarial environments and agents behavior in it.",
                "The classical Min-Max adversarial search algorithm was the first attempt to integrate the opponent into the search space with a weak assumption of an optimally playing opponent.",
                "Since then, much effort has gone into integrating the opponent model into the decision procedure to predict future behavior.",
                "The M∗ algorithm presented by Carmel and Markovitch [2] showed a method of incorporating opponent models into adversary search, while in [3] they used learning to provide a more accurate opponent model in a 2player repeated game environment, where agents strategies were modeled as finite automata.",
                "Additional Adversarial planning work was done by Willmott et al. [13], which provided an adversarial planning approach to the game of GO.",
                "The research mentioned above dealt with adversarial search and the integration of opponent models into classical utilitybased search methods.",
                "That work shows the importance of opponent modeling and the ability to exploit it to an agents advantage.",
                "However, the basic limitations of those search methods still apply; our model tries to overcome those limitations by presenting a formal model for a new, mental state-based adversarial specification. 5.",
                "CONCLUSIONS We presented an Adversarial Environment model for a 2 These were later removed from the final analysis. bounded rational agent that is situated in an N-player, zerosum environment.",
                "We used the SharedPlans formalization to define the model and the axioms that agents can apply as behavioral guidelines.",
                "The model is meant to be used as a guideline for designing agents that need to operate in such adversarial environments.",
                "We presented empirical results, based on ConnectFour log file analysis, that exemplify the model and the axioms for a bilateral instance of the environment.",
                "The results we presented are a first step towards an expanded model that will cover all types of adversarial environments, for example, environments that are non-zero-sum, and environments that contain natural agents that are not part of the direct conflict.",
                "Those challenges and more will be dealt with in future research. 6.",
                "ACKNOWLEDGMENT This research was supported in part by Israel Science Foundation grants #1211/04 and #898/05. 7.",
                "REFERENCES [1] L. V. Allis.",
                "A knowledge-based approach of Connect-Four - the game is solved: White wins.",
                "Masters thesis, Free University, Amsterdam, The Netherlands, 1988. [2] D. Carmel and S. Markovitch.",
                "Incorporating opponent models into adversary search.",
                "In Proceedings of the Thirteenth National Conference on Artificial Intelligence, pages 120-125, Portland, OR, 1996. [3] D. Carmel and S. Markovitch.",
                "Opponent modeling in multi-agent systems.",
                "In G. Weiß and S. Sen, editors, Adaptation and Learning in Multi-Agent Systems, pages 40-52.",
                "Springer-Verlag, 1996. [4] B. J. Grosz and S. Kraus.",
                "Collaborative plans for complex group action.",
                "Artificial Intelligence, 86(2):269-357, 1996. [5] M. Hadad, G. Kaminka, G. Armon, and S. Kraus.",
                "Supporting collaborative activity.",
                "In Proc. of AAAI-2005, pages 83-88, Pittsburgh, 2005. [6] http://www.gamerz.net/˜pbmserv/. [7] S. Kraus and D. Lehmann.",
                "Designing and building a negotiating automated agent.",
                "Computational Intelligence, 11:132-171, 1995. [8] H. J. Levesque, P. R. Cohen, and J. H. T. Nunes.",
                "On acting together.",
                "In Proc. of AAAI-90, pages 94-99, Boston, MA, 1990. [9] S. Markovitch and R. Reger.",
                "Learning and exploiting relative weaknesses of opponent agents.",
                "Autonomous Agents and Multi-Agent Systems, 10(2):103-130, 2005. [10] Y. M. Ronald Fagin, Joseph Y. Halpern and M. Y. Vardi.",
                "Reasoning about knowledge.",
                "MIT Press, Cambridge, Mass., 1995. [11] P. Thagard.",
                "Adversarial problem solving: Modeling an oponent using explanatory coherence.",
                "Cognitive Science, 16(1):123-149, 1992. [12] R. W. Toseland and R. F. Rivas.",
                "An Introduction to Group Work Practice.",
                "Prentice Hall, Englewood Cliffs, NJ, 2nd edition edition, 1995. [13] S. Willmott, J. Richardson, A. Bundy, and J. Levine.",
                "An adversarial planning approach to Go.",
                "Lecture Notes in Computer Science, 1558:93-112, 1999.",
                "The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) 557"
            ],
            "original_annotated_samples": [],
            "translated_annotated_samples": [
                "Ahora utilizamos nuestra función de evaluación estimada para evaluar las acciones de los jugadores negros durante la \"interacción adversa\" de conexión.interacción adversaria"
            ],
            "translated_text": "",
            "candidates": [],
            "error": []
        },
        "adversarial environment": {
            "translated_key": "",
            "is_in_text": true,
            "original_annotated_sentences": [
                "An <br>adversarial environment</br> Model for Bounded Rational Agents in Zero-Sum Interactions Inon Zuckerman1 , Sarit Kraus1 , Jeffrey S. Rosenschein2 , Gal Kaminka1 1 Department of Computer Science 2 The School of Engineering Bar-Ilan University and Computer Science Ramat-Gan, Israel Hebrew University, Jerusalem, Israel {zukermi,sarit,galk}@cs.biu.ac.il jeff@cs.huji.ac.il ABSTRACT Multiagent environments are often not cooperative nor collaborative; in many cases, agents have conflicting interests, leading to adversarial interactions.",
                "This paper presents a formal <br>adversarial environment</br> model for bounded rational agents operating in a zero-sum environment.",
                "In such environments, attempts to use classical utility-based search methods can raise a variety of difficulties (e.g., implicitly modeling the opponent as an omniscient utility maximizer, rather than leveraging a more nuanced, explicit opponent model).",
                "We define an <br>adversarial environment</br> by describing the mental states of an agent in such an environment.",
                "We then present behavioral axioms that are intended to serve as design principles for building such adversarial agents.",
                "We explore the application of our approach by analyzing log files of completed Connect-Four games, and present an empirical analysis of the axioms appropriateness.",
                "Categories and Subject Descriptors I.2.11 [Artificial Intelligence]: Distributed Artificial Intelligence-Intelligent agents,Multiagent Systems; I.2.4 [Artificial Intelligence]: Knowledge Representation Formalisms and Methods -Modal logic General Terms Design, Theory 1.",
                "INTRODUCTION Early research in multiagent systems (MAS) considered cooperative groups of agents; because individual agents had limited resources, or limited access to information (e.g., limited processing power, limited sensor coverage), they worked together by design to solve problems that individually they could not solve, or at least could not solve as efficiently.",
                "MAS research, however, soon began to consider interacting agents with individuated interests, as representatives of different humans or organizations with non-identical interests.",
                "When interactions are guided by diverse interests, participants may have to overcome disagreements, uncooperative interactions, and even intentional attempts to damage one another.",
                "When these types of interactions occur, environments require appropriate behavior from the agents situated in them.",
                "We call these environments Adversarial Environments, and call the clashing agents Adversaries.",
                "Models of cooperation and teamwork have been extensively explored in MAS through the axiomatization of mental states (e.g., [8, 4, 5]).",
                "However, none of this research dealt with adversarial domains and their implications for agent behavior.",
                "Our paper addresses this issue by providing a formal, axiomatized mental state model for a subset of adversarial domains, namely simple zero-sum adversarial environments.",
                "Simple zero-sum encounters exist of course in various twoplayer games (e.g., Chess, Checkers), but they also exist in n-player games (e.g., Risk, Diplomacy), auctions for a single good, and elsewhere.",
                "In these latter environments especially, using a utility-based adversarial search (such as the Min-Max algorithm) does not always provide an adequate solution; the payoff function might be quite complex or difficult to quantify, and there are natural computational limitations on bounded rational agents.",
                "In addition, traditional search methods (like Min-Max) do not make use of a model of the opponent, which has proven to be a valuable addition to adversarial planning [9, 3, 11].",
                "In this paper, we develop a formal, axiomatized model for bounded rational agents that are situated in a zero-sum <br>adversarial environment</br>.",
                "The model uses different modality operators, and its main foundations are the SharedPlans [4] model for collaborative behavior.",
                "We explore environment properties and the mental states of agents to derive behavioral axioms; these behavioral axioms constitute a formal model that serves as a specification and design guideline for agent design in such settings.",
                "We then investigate the behavior of our model empirically using the Connect-Four board game.",
                "We show that this game conforms to our environment definition, and analyze players behavior using a large set of completed match log 550 978-81-904262-7-5 (RPS) c 2007 IFAAMAS files.",
                "In addition, we use the results presented in [9] to discuss the importance of opponent modeling in our ConnectFour adversarial domain.",
                "The paper proceeds as follows.",
                "Section 2 presents the models formalization.",
                "Section 3 presents the empirical analysis and its results.",
                "We discuss related work in Section 4, and conclude and present future directions in Section 5. 2.",
                "ADVERSARIAL ENVIRONMENTS The <br>adversarial environment</br> model (denoted as AE) is intended to guide the design of agents by providing a specification of the capabilities and mental attitudes of an agent in an <br>adversarial environment</br>.",
                "We focus here on specific types of adversarial environments, specified as follows: 1.",
                "Zero-Sum Interactions: positive and negative utilities of all agents sum to zero; 2.",
                "Simple AEs: all agents in the environment are adversarial agents; 3.",
                "Bilateral AEs: AEs with exactly two agents; 4.",
                "Multilateral AEs: AEs of three or more agents.",
                "We will work on both bilateral and multilateral instantiations of zero-sum and simple environments.",
                "In particular, our <br>adversarial environment</br> model will deal with interactions that consist of N agents (N ≥ 2), where all agents are adversaries, and only one agent can succeed.",
                "Examples of such environments range from board games (e.g., Chess, Connect-Four, and Diplomacy) to certain economic environments (e.g., N-bidder auctions over a single good). 2.1 Model Overview Our approach is to formalize the mental attitudes and behaviors of a single adversarial agent; we consider how a single agent perceives the AE.",
                "The following list specifies the conditions and mental states of an agent in a simple, zero-sum AE: 1.",
                "The agent has an individual intention that its own goal will be completed; 2.",
                "The agent has an individual belief that it and its adversaries are pursuing full conflicting goals (defined below)there can be only one winner; 3.",
                "The agent has an individual belief that each adversary has an intention to complete its own full conflicting goal; 4.",
                "The agent has an individual belief in the (partial) profile of its adversaries.",
                "Item 3 is required, since it might be the case that some agent has a full conflicting goal, and is currently considering adopting the intention to complete it, but is, as of yet, not committed to achieving it.",
                "This might occur because the agent has not yet deliberated about the effects that adopting that intention might have on the other intentions it is currently holding.",
                "In such cases, it might not consider itself to even be in an <br>adversarial environment</br>.",
                "Item 4 states that the agent should hold some belief about the profiles of its adversaries.",
                "The profile represents all the knowledge the agent has about its adversary: its weaknesses, strategic capabilities, goals, intentions, trustworthiness, and more.",
                "It can be given explicitly or can be learned from observations of past encounters. 2.2 Model Definitions for Mental States We use Grosz and Krauss definitions of the modal operators, predicates, and meta-predicates, as defined in their SharedPlan formalization [4].",
                "We recall here some of the predicates and operators that are used in that formalization: Int.To(Ai, α, Tn, Tα, C) represents Ais intentions at time Tn to do an action α at time Tα in the context of C. Int.Th(Ai, prop, Tn, Tprop, C) represents Ais intentions at time Tn that a certain proposition prop holds at time Tprop in the context of C. The potential intention operators, Pot.Int.To(...) and Pot.Int.Th(...), are used to represent the mental state when an agent considers adopting an intention, but has not deliberated about the interaction of the other intentions it holds.",
                "The operator Bel(Ai, f, Tf ) represents agent Ai believing in the statement expressed in formula f, at time Tf .",
                "MB(A, f, Tf ) represents mutual belief for a group of agents A.",
                "A snapshot of the system finds our environment to be in some state e ∈ E of environmental variable states, and each adversary in any LAi ∈ L of possible local states.",
                "At any given time step, the system will be in some world w of the set of all possible worlds w ∈ W, where w = E×LA1 ×LA2 × ...LAn , and n is the number of adversaries.",
                "For example, in a Texas Holdem poker game, an agents local state might be its own set of cards (which is unknown to its adversary) while the environment will consist of the betting pot and the community cards (which are visible to both players).",
                "A utility function under this formalization is defined as a mapping from a possible world w ∈ W to an element in , which expresses the desirability of the world, from a single agent perspective.",
                "We usually normalize the range to [0,1], where 0 represents the least desirable possible world, and 1 is the most desirable world.",
                "The implementation of the utility function is dependent on the domain in question.",
                "The following list specifies new predicates, functions, variables, and constants used in conjunction with the original definitions for the <br>adversarial environment</br> formalization: 1. φ is a null action (the agent does not do anything). 2.",
                "GAi is the set of agent Ais goals.",
                "Each goal is a set of predicates whose satisfaction makes the goal complete (we use G∗ Ai ∈ GAi to represent an arbitrary goal of agent Ai). 3. gAi is the set of agent Ais subgoals.",
                "Subgoals are predicates whose satisfaction represents an important milestone toward achievement of the full goal. gG∗ Ai ⊆ gAi is the set of subgoals that are important to the completion of goal G∗ Ai (we will use g∗ G∗ Ai ∈ gG∗ Ai to represent an arbitrary subgoal). 4.",
                "P Aj Ai is the profile object agent Ai holds about agent Aj. 5.",
                "CA is a general set of actions for all agents in A which are derived from the environments constraints.",
                "CAi ⊆ CA is the set of agent Ais possible actions. 6.",
                "Do(Ai, α, Tα, w) holds when Ai performs action α over time interval Tα in world w. 7.",
                "Achieve(G∗ Ai , α, w) is true when goal G∗ Ai is achieved following the completion of action α in world w ∈ W, where α ∈ CAi . 8.",
                "Profile(Ai, PAi Ai ) is true when agent Ai holds an object profile for agent Aj.",
                "Definition 1.",
                "Full conflict (FulConf ) describes a zerosum interaction where only a single goal of the goals in conflict can be completed.",
                "FulConf(G∗ Ai , G∗ Aj ) ⇒ (∃α ∈ CAi , ∀w, β ∈ CAj ) (Achieve(G∗ Ai , α, w) ⇒ ¬Achieve(G∗ Aj , β, w)) ∨ (∃β ∈ CAj , ∀w, α ∈ CAi )(Achieve(G∗ Aj , β, w) ⇒ ¬Achieve(G∗ Ai , α, w)) Definition 2.",
                "Adversarial Knowledge (AdvKnow) is a function returning a value which represents the amount of The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) 551 knowledge agent Ai has on the profile of agent Aj, at time Tn.",
                "The higher the value, the more knowledge agent Ai has.",
                "AdvKnow : P Aj Ai × Tn → Definition 3.",
                "Eval - This evaluation function returns an estimated expected utility value for an agent in A, after completing an action from CA in some world state w. Eval : A × CA × w → Definition 4.",
                "TrH - (Threshold) is a numerical constant in the [0,1] range that represents an evaluation function (Eval) threshold value.",
                "An action that yields an estimated utility evaluation above the TrH is regarded as a highly beneficial action.",
                "The Eval value is an estimation and not the real utility function, which is usually unknown.",
                "Using the real utility value for a rational agent would easily yield the best outcome for that agent.",
                "However, agents usually do not have the real utility functions, but rather a heuristic estimate of it.",
                "There are two important properties that should hold for the evaluation function: Property 1.",
                "The evaluation function should state that the most desirable world state is one in which the goal is achieved.",
                "Therefore, after the goal has been satisfied, there can be no future action that can put the agent in a world state with higher Eval value. (∀Ai, G∗ Ai , α, β ∈ CAi , w ∈ W) Achieve(G∗ Ai , α, w) ⇒ Eval(Ai, α, w) ≥ Eval(Ai, β, w) Property 2.",
                "The evaluation function should project an action that causes a completion of a goal or a subgoal to a value which is greater than TrH (a highly beneficial action). (∀Ai, G∗ Ai ∈ GAi , α ∈ CAi , w ∈ W, g∗ GAi ∈ gGAi ) Achieve(G∗ Ai , α, w) ∨ Achieve(g∗ GAi , α, w) ⇒ Eval(Ai, α, w) ≥ TrH.",
                "Definition 5.",
                "SetAction We define a set action (SetAction) as a set of action operations (either complex or basic actions) from some action sets CAi and CAj which, according to agent Ais belief, are attached together by a temporal and consequential relationship, forming a chain of events (action, and its following consequent action). (∀α1 , . . . , αu ∈ CAi , β1 , . . . , βv ∈ CAj , w ∈ W) SetAction(α1 , . . . , αu , β1 , . . . , βv , w) ⇒ ((Do(Ai, α1 , Tα1 , w) ⇒ Do(Aj, β1 , Tβ1 , w)) ⇒ Do(Ai, α2 , Tα2 , w) ⇒ . . . ⇒ Do(Ai, αu , Tαu , w)) The consequential relation might exist due to various environmental constraints (when one action forces the adversary to respond with a specific action) or due to the agents knowledge about the profile of its adversary.",
                "Property 3.",
                "As the knowledge we have about our adversary increases we will have additional beliefs about its behavior in different situations which in turn creates new set actions.",
                "Formally, if our AdvKnow at time Tn+1 is greater than AdvKnow at time Tn, then every SetAction known at time Tn is also known at time Tn+1.",
                "AdvKnow(P Aj Ai , Tn+1) > AdvKnow(P Aj Ai , Tn) ⇒ (∀α1 , . . . , αu ∈ CAi , β1 , . . . , βv ∈ CAj ) Bel(Aag, SetAction(α1 , . . . , αu , β1 , . . . , βv ), Tn) ⇒ Bel(Aag, SetAction(α1 , . . . , αu , β1 , . . . , βv ), Tn+1) 2.3 The Environment Formulation The following axioms provide the formal definition for a simple, zero-sum <br>adversarial environment</br> (AE).",
                "Satisfaction of these axioms means that the agent is situated in such an environment.",
                "It provides specifications for agent Aag to interact with its set of adversaries A with respect to goals G∗ Aag and G∗ A at time TCo at some world state w. AE(Aag, A, G∗ Aag , A1, . . . , Ak, G∗ A1 , . . . , G∗ Ak , Tn, w) 1.",
                "Aag has an Int.Th his goal would be completed: (∃α ∈ CAag , Tα) Int.Th(Aag, Achieve(G∗ Aag , α), Tn, Tα, AE) 2.",
                "Aag believes that it and each of its adversaries Ao are pursuing full conflicting goals: (∀Ao ∈ {A1, . . . , Ak}) Bel(Aag, FulConf(G∗ Aag , G∗ Ao ), Tn) 3.",
                "Aag believes that each of his adversaries in Ao has the Int.Th his conflicting goal G∗ Aoi will be completed: (∀Ao ∈ {A1, . . . , Ak})(∃β ∈ CAo , Tβ) Bel(Aag, Int.Th(Ao, Achieve(G∗ Ao , β), TCo, Tβ, AE), Tn) 4.",
                "Aag has beliefs about the (partial) profiles of its adversaries (∀Ao ∈ {A1, . . . , Ak}) (∃PAo Aag ∈ PAag )Bel(Aag, Profile(Ao, PAo Aag ), Tn) To build an agent that will be able to operate successfully within such an AE, we must specify behavioral guidelines for its interactions.",
                "Using a naive Eval maximization strategy to a certain search depth will not always yield satisfactory results for several reasons: (1) the search horizon problem when searching for a fixed depth; (2) the strong assumption of an optimally rational, unbounded resources adversary; (3) using an estimated evaluation function which will not give optimal results in all world states, and can be exploited [9].",
                "The following axioms specify the behavioral principles that can be used to differentiate between successful and less successful agents in the above <br>adversarial environment</br>.",
                "Those axioms should be used as specification principles when designing and implementing agents that should be able to perform well in such Adversarial Environments.",
                "The behavioral axioms represent situations in which the agent will adopt potential intentions to (Pot.Int.To(...)) perform an action, which will typically require some means-end reasoning to select a possible course of action.",
                "This reasoning will lead to the adoption of an Int.To(...) (see [4]).",
                "A1.",
                "Goal Achieving Axiom.",
                "The first axiom is the simplest case; when the agent Aag believes that it is one action (α) away from achieving his conflicting goal G∗ Aag , it should adopt the potential intention to do α and complete its goal. (∀Aag, α ∈ CAag , Tn, Tα, w ∈ W) (Bel(Aag, Do(Aag, α, Tα, w) ⇒ Achieve(G∗ Aag , α, w)) ⇒ Pot.Int.To(Aag, α, Tn, Tα, w) This somewhat trivial behavior is the first and strongest axiom.",
                "In any situation, when the agent is an action away from completing the goal, it should complete the action.",
                "Any fair Eval function would naturally classify α as the maximal value action (property 1).",
                "However, without explicit axiomatization of such behavior there might be situations where the agent will decide on taking another action for various reasons, due to its bounded decision resources.",
                "A2.",
                "Preventive Act Axiom.",
                "Being in an adversarial situation, agent Aag might decide to take actions that will damage one of its adversarys plans to complete its goal, even if those actions do not explicitly advance Aag towards its conflicting goal G∗ Aag .",
                "Such preventive action will take place when agent Aag has a belief about the possibility of its adversary Ao doing an action β that will give it a high 552 The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) utility evaluation value (> TrH).",
                "Believing that taking action α will prevent the opponent from doing its β, it will adopt a potential intention to do α. (∀Aag, Ao ∈ A, α ∈ CAag , β ∈ CAo , Tn, Tβ, w ∈ W) (Bel(Aag, Do(Ao, β, Tβ, w) ∧ Eval(Ao, β, w) > TrH, Tn) ∧ Bel(Aag, Do(Aag, α, Tα, w) ⇒ ¬Do(Ao, β, Tβ, w), Tn) ⇒ Pot.Int.To(Aag, α, Tn, Tα, w) This axiom is a basic component of any <br>adversarial environment</br>.",
                "For example, looking at a Chess board game, a player could realize that it is about to be checkmated by its opponent, thus making a preventive move.",
                "Another example is a Connect Four game: when a player has a row of three chips, its opponent must block it, or lose.",
                "A specific instance of A1 occurs when the adversary is one action away from achieving its goal, and immediate preventive action needs to be taken by the agent.",
                "Formally, we have the same beliefs as stated above, with a changed belief that doing action β will cause agent Ao to achieve its goal.",
                "Proposition 1: Prevent or lose case. (∀Aag, Ao ∈ A, α ∈ CAag , β ∈ CAo , G∗ Ao , Tn, Tα, Tβ, w ∈ W) Bel(Aag, Do(Ao, β, Tβ, w) ⇒ Achieve(G∗ Ao , β, w), Tn) ∧ Bel(Aag, Do(Aag, α, Tα, w) ⇒ ¬Do(Ao, β, Tβ, w)) ⇒ Pot.Int.To(Aag, α, Tn, Tα, w) Sketch of proof: Proposition 1 can be easily derived from axiom A1 and the property 2 of the Eval function, which states that any action that causes a completion of a goal is a highly beneficial action.",
                "The preventive act behavior will occur implicitly when the Eval function is equal to the real world utility function.",
                "However, being bounded rational agents and dealing with an estimated evaluation function we need to explicitly axiomatize such behavior, for it will not always occur implicitly from the evaluation function.",
                "A3.",
                "Suboptimal Tactical Move Axiom.",
                "In many scenarios a situation may occur where an agent will decide not to take the current most beneficial action it can take (the action with the maximal utility evaluation value), because it believes that taking another action (with lower utility evaluation value) might yield (depending on the adversarys response) a future possibility for a highly beneficial action.",
                "This will occur most often when the Eval function is inaccurate and differs by a large extent from the Utility function.",
                "Put formally, agent Aag believes in a certain SetAction that will evolve according to its initial action and will yield a high beneficial value (> TrH) solely for it. (∀Aag, Ao ∈ A, Tn, w ∈ W) (∃α1 , . . . , αu ∈ CAi , β1 , . . . , βv ∈ CAj , Tα1 ) Bel(Aag, SetAction(α1 , . . . , αu , β1 , . . . , βv ), Tn) ∧ Bel(Aag, Eval(Ao, βv , w) < TrH < Eval(Aag, αu , w), Tn) ⇒ Pot.Int.To(Aag, α1 , Tn, Tα1 , w) An agent might believe that a chain of events will occur for various reasons due to the inevitable nature of the domain.",
                "For example, in Chess, we often observe the following: a move causes a check position, which in turn limits the opponents moves to avoiding the check, to which the first player might react with another check, and so on.",
                "The agent might also believe in a chain of events based on its knowledge of its adversarys profile, which allows it to foresee the adversarys movements with high accuracy.",
                "A4.",
                "Profile Detection Axiom.",
                "The agent can adjust its adversarys profiles by observations and pattern study (specifically, if there are repeated encounters with the same adversary).",
                "However, instead of waiting for profile information to be revealed, an agent can also initiate actions that will force its adversary to react in a way that will reveal profile knowledge about it.",
                "Formally, the axiom states that if all actions (γ) are not highly beneficial actions (< TrH), the agent can do action α in time Tα if it believes that it will result in a non-highly beneficial action β from its adversary, which in turn teaches it about the adversarys profile, i.e., gives a higher AdvKnow(P Aj Ai , Tβ). (∀Aag, Ao ∈ A, α ∈ CAag , β ∈ CAo , Tn, Tα, Tβ, w ∈ W) Bel(Aag, (∀γ ∈ CAag )Eval(Aag, γ, w) < TrH, Tn) ∧ Bel(Aag, Do(Aag, α, Tα, w) ⇒ Do(Ao, β, Tβ, w), Tn) ∧ Bel(Aag, Eval(Ao, β, w) < TrH) ∧ Bel(Aag, AdvKnow(P Aj Ai , Tβ) > AdvKnow(P Aj Ai , Tn), Tn) ⇒ Pot.Int.To(Aag, α, Tn, Tα, w) For example, going back to the Chess board game scenario, consider starting a game versus an opponent about whom we know nothing, not even if it is a human or a computerized opponent.",
                "We might start playing a strategy that will be suitable versus an average opponent, and adjust our game according to its level of play.",
                "A5.",
                "Alliance Formation Axiom The following behavioral axiom is relevant only in a multilateral instantiation of the <br>adversarial environment</br> (obviously, an alliance cannot be formed in a bilateral, zero-sum encounter).",
                "In different situations during a multilateral interaction, a group of agents might believe that it is in their best interests to form a temporary alliance.",
                "Such an alliance is an agreement that constrains its members behavior, but is believed by its members to enable them to achieve a higher utility value than the one achievable outside of the alliance.",
                "As an example, we can look at the classical Risk board game, where each player has an individual goal of being the sole conquerer of the world, a zero-sum game.",
                "However, in order to achieve this goal, it might be strategically wise to make short-term ceasefire agreements with other players, or to join forces and attack an opponent who is stronger than the rest.",
                "An alliances terms defines the way its members should act.",
                "It is a set of predicates, denoted as Terms, that is agreed upon by the alliance members, and should remain true for the duration of the alliance.",
                "For example, the set Terms in the Risk scenario, could contain the following predicates: 1.",
                "Alliance members will not attack each other on territories X, Y and Z; 2.",
                "Alliance members will contribute C units per turn for attacking adversary Ao; 3.",
                "Members are obligated to stay as part of the alliance until time Tk or until adversarys Ao army is smaller than Q.",
                "The set Terms specifies inter-group constraints on each of the alliance members (∀Aal i ∈ Aal ⊆ A) set of actions Cal i ⊆ C. Definition 6.",
                "Al val - the total evaluation value that agent Ai will achieve while being part of Aal is the sum of Evali (Eval for Ai) of each of Aal j Eval values after taking their own α actions (via the agent(α) predicate): Al val(Ai, Cal , Aal , w) = α∈Cal Evali(Aal j , agent(α), w) Definition 7.",
                "Al TrH - is a number representing an Al val The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) 553 threshold; above it, the alliance can be said to be a highly beneficial alliance.",
                "The value of Al TrH will be calculated dynamically according to the progress of the interaction, as can be seen in [7].",
                "After an alliance is formed, its members are now working in their normal <br>adversarial environment</br>, as well as according to the mental states and axioms required for their interactions as part of the alliance.",
                "The following Alliance model (AL) specifies the conditions under which the group Aal can be said to be in an alliance and working with a new and constrained set of actions Cal , at time Tn.",
                "AL(Aal , Cal , w, Tn) 1.",
                "Aal has a MB that all members are part of Aal : MB(Aal , (∀Aal i ∈ Aal )member(Aal i , Aal ), Tn) 2.",
                "Aal has a MB that the group be maintained: MB(Aal , (∀Aal i ∈ Aal )Int.Th (Ai, member(Ai, Aal ), Tn, Tn+1, Co), Tn) 3.",
                "Aal has a MB that being members gives them high utility value: MB(Aal , (∀Aal i ∈ Aal )Al val(Aal i , Cal , Aal , w) ≥ Al TrH, Tn) Members profiles are a crucial part of successful alliances.",
                "We assume that agents that have more accurate profiles of their adversaries will be more successful in such environments.",
                "Such agents will be able to predict when a member is about to breach the alliances contract (item 2 in the above model), and take counter measures (when item 3 will falsify).",
                "The robustness of the alliance is in part a function of its members trustfulness measure, objective position estimation, and other profile properties.",
                "We should note that an agent can simultaneously be part of more than one alliance.",
                "Such a temporary alliance, where the group members do not have a joint goal but act collaboratively for the interest of their own individual goals, is classified as a Treatment Group by modern psychologists [12] (in contrast to a Task Group, where its members have a joint goal).",
                "The Shared Activity model as presented in [5] modeled Treatment Group behavior using the same SharedPlans formalization.",
                "When comparing both definitions of an alliance and a Treatment Group we found an unsurprising resemblance between both models: the environment models definitions are almost identical (see SAs definitions in [5]), and their Selfish-Act and Cooperative Act axioms conform to our adversarial agents behavior.",
                "The main distinction between both models is the integration of a Helpful-behavior act axiom, in the Shared Activity which cannot be part of ours.",
                "This axiom states that an agent will consider taking action that will lower its Eval value (to a certain lower bound), if it believes that a group partner will gain a significant benefit.",
                "Such behavior cannot occur in a pure <br>adversarial environment</br> (as a zero-sum game is), where the alliance members are constantly on watch to manipulate their alliance to their own advantage.",
                "A6.",
                "Evaluation Maximization Axiom.",
                "In a case when all other axioms are inapplicable, we will proceed with the action that maximizes the heuristic value as computed in the Eval function. (∀Aag, Ao ∈ A, α ∈ Cag, Tn, w ∈ W) Bel(Aag, (∀γ ∈ Cag)Eval(Aag, α, w) ≥ Eval(Aag, γ, w), Tn) ⇒ Pot.Int.To(Aag, α, Tn, Tα, w) T1.",
                "Optimality on Eval = Utility The above axiomatic model handles situations where the Utility is unknown and the agents are bounded rational agents.",
                "The following theorem shows that in bilateral interactions, where the agents have the real Utility function (i.e., Eval = Utility) and are rational agents, the axioms provide the same optimal result as classic adversarial search (e.g., Min-Max).",
                "Theorem 1.",
                "Let Ae ag be an unbounded rational AE agent using the Eval heuristic evaluation function, Au ag be the same agent using the true Utility function, and Ao be a sole unbounded utility-based rational adversary.",
                "Given that Eval = Utility: (∀α ∈ CAu ag , α ∈ CAe ag , Tn, w ∈ W) Pot.Int.To(Au ag, α, Tn, Tα, w) → Pot.Int.To(Ae ag, α , Tn, Tα, w) ∧ ((α = α ) ∨ (Utility(Au ag, α, w) = Eval(Ae ag, α , w))) Sketch of proof - Given that Au ag has the real utility function and unbounded resources, it can generate the full game tree and run the optimal MinMax algorithm to choose the highest utility value action, which we denote by, α.",
                "The proof will show that Ae ag, using the AE axioms, will select the same or equal utility α (when there is more than one action with the same max utility) when Eval = Utility. (A1) Goal achieving axiom - suppose there is an α such that its completion will achieve Au ags goal.",
                "It will obtain the highest utility by Min-Max for Au ag.",
                "The Ae ag agent will select α or another action with the same utility value via A1.",
                "If such α does not exist, Ae ag cannot apply this axiom, and proceeds to A2. (A2) Preventive act axiom - (1) Looking at the basic case (see Prop1), if there is a β which leads Ao to achieve its goal, then a preventive action α will yield the highest utility for Au ag.",
                "Au ag will choose it through the utility, while Ae ag will choose it through A2. (2) In the general case, β is a highly beneficial action for Ao, thus yields low utility for Au ag, which will guide it to select an α that will prevent β, while Ae ag will choose it through A2.1 If such β does not exist for Ao, then A2 is not applicable, and Ae ag can proceed to A3. (A3) Suboptimal tactical move axiom - When using a heuristic Eval function, Ae ag has a partial belief in the profile of its adversary (item 4 in AE model), which may lead it to believe in SetActions (Prop1).",
                "In our case, Ae ag is holding a full profile on its optimal adversary and knows that Ao will behave optimally according to the real utility values on the complete search tree, therefore, any belief about suboptimal SetAction cannot exist, yielding this axiom inapplicable.",
                "Ae ag will proceed to A4. (A4) Profile detection axiom - Given that Ae ag has the full profile of Ao, none of Ae ags actions can increase its knowledge.",
                "That axiom will not be applied, and the agent will proceed with A6 (A5 will be disregarded because the interaction is bilateral). (A6) Evaluation maximization axiom - This axiom will select the max Eval for Ae ag.",
                "Given that Eval = Utility, the same α that was selected by Au ag will be selected. 3.",
                "EVALUATION The main purpose of our experimental analysis is to evaluate the models behavior and performance in a real <br>adversarial environment</br>.",
                "This section investigates whether bounded 1 A case where following the completion of β there exists a γ which gives high utility for Agent Au ag, cannot occur because Ao uses the same utility, and γs existence will cause it to classify β as a low utility action. 554 The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) rational agents situated in such adversarial environments will be better off applying our suggested behavioral axioms. 3.1 The Domain To explore the use of the above model and its behavioral axioms, we decided to use the Connect-Four game as our <br>adversarial environment</br>.",
                "Connect-Four is a 2-player, zerosum game which is played using a 6x7 matrix-like board.",
                "Each turn, a player drops a disc into one of the 7 columns (the set of 21 discs is usually colored yellow for player 1 and red for player 2; we will use White and Black respectively to avoid confusion).",
                "The winner is the first player to complete a horizontal, vertical, or diagonal set of four discs with its color.",
                "On very rare occasions, the game might end in a tie if all the empty grids are filled, but no player managed to create a 4-disc set.",
                "The Connect-Four game was solved in [1], where it is shown that the first player (playing with the white discs) can force a win by starting in the middle column (column 4) and playing optimally However, the optimal strategy is very complex, and difficult to follow even for complex bounded rational agents, such as human players.",
                "Before we can proceed checking agent behavior, we must first verify that the domain conforms to the adversarial environments definition as given above (which the behavioral axioms are based on).",
                "First, when playing a Connect-Four game, the agent has an intention to win the game (item 1).",
                "Second (item 2), our agent believes that in Connect-Four there can only be one winner (or no winner at all in the rare occurrence of a tie).",
                "In addition, our agent believes that its opponent to the game will try to win (item 3), and we hope it has some partial knowledge (item 4) about its adversary (this knowledge can vary from nothing, through simple facts such as age, to strategies and weaknesses).",
                "Of course, not all Connect-Four encounters are adversarial.",
                "For example, when a parent is playing the game with its child, the following situation might occur: the child, having a strong incentive to win, treats the environment as adversarial (it intends to win, understands that there can only be one winner, and believes that its parent is trying to beat him).",
                "However, the parents point of view might see the environment as an educational one, where its goal is not to win the game, but to cause enjoyment or practice strategic reasoning.",
                "In such an educational environment, a new set of behavioral axioms might be more beneficial to the parents goals than our suggested adversarial behavioral axioms. 3.2 Axiom Analysis After showing that the Connect-Four game is indeed a zero-sum, bilateral <br>adversarial environment</br>, the next step is to look at players behaviors during the game and check whether behaving according to our model does improve performance.",
                "To do so we have collected log files from completed Connect-Four games that were played by human players over the Internet.",
                "Our collected log file data came from Play by eMail (PBeM) sites.",
                "These are web sites that host email games, where each move is taken by an email exchange between the server and the players.",
                "Many such sites archives contain real competitive interactions, and also maintain a ranking system for their members.",
                "Most of the data we used can be found in [6].",
                "As can be learned from [1], Connect-Four has an optimal strategy and a considerable advantage for the player who starts the game (which we call the White player).",
                "We will concentrate in our analysis on the second players moves (to be called Black).",
                "The White player, being the first to act, has the so-called initiative advantage.",
                "Having the advantage and a good strategy will keep the Black player busy reacting to its moves, instead of initiating threats.",
                "A threat is a combination of three discs of the same color, with an empty spot for the fourth winning disk.",
                "An open threat is a threat that can be realized in the opponents next move.",
                "In order for the Black player to win, it must somehow turn the tide, take the advantage and start presenting threats to the White player.",
                "We will explore Black players behavior and their conformance to our axioms.",
                "To do so, we built an application that reads log files and analyzes the Black players moves.",
                "The application contains two main components: (1) a Min-Max algorithm for evaluation of moves; (2) open threats detector for the discovering of open threats.",
                "The Min-Max algorithm will work to a given depth, d and for each move α will output the heuristic value for the next action taken by the player as written in the log file, h(α), alongside the maximum heuristic value, maxh(α), that could be achieved prior to taking the move (obviously, if h(α) = maxh(α), then the player did not do the optimal move heuristically).",
                "The threat detectors job is to notify if some action was taken in order to block an open threat (not blocking an open threat will probably cause the player to lose in the opponents next move).",
                "The heuristic function used by Min-Max to evaluate the players utility is the following function, which is simple to compute, yet provides a reasonable challenge to human opponents: Definition 8.",
                "Let Group be an adjacent set of four squares that are horizontal, vertical, or diagonal.",
                "Groupn b (Groupn w) be a Group with n pieces of the black (white) color and 4−n empty squares. h = ((Group1 b ∗α)+(Group2 b ∗β)+(Group3 b ∗γ)+(Group4 b ∗∞)) − ((Group1 w ∗α)+(Group2 w ∗β)+(Group3 w ∗γ)+(Group4 w ∗∞)) The values of α, β and δ can vary to form any desired linear combination; however, it is important to value them with the α < β < δ ordering in mind (we used 1, 4, and 8 as their respective values).",
                "Groups of 4 discs of the same color means victory, thus discovery of such a group will result in ∞ to ensure an extreme value.",
                "We now use our estimated evaluation function to evaluate the Black players actions during the Connect-Four adversarial interaction.",
                "Each game from the log file was input into the application, which processed and output a reformatted log file containing the h value of the current move, the maxh value that could be achieved, and a notification if an open threat was detected.",
                "A total of 123 games were analyzed (57 with White winning, and 66 with Black winning).",
                "A few additional games were manually ignored in the experiment, due to these problems: a player abandoning the game while the outcome is not final, or a blunt irrational move in the early stages of the game (e.g., not blocking an obvious winning group in the first opening moves).",
                "In addition, a single tie game was also removed.",
                "The simulator was run to a search depth of 3 moves.",
                "We now proceed to analyze the games with respect to each behavioral axiom.",
                "The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) 555 Table 1: Average heuristic difference analysis Black losses Black Won Avg minh -17.62 -12.02 Avg 3 lowest h moves (min3 h) -13.20 -8.70 3.2.1 Affirming the Suboptimal tactical move axiom The following section presents the heuristic evaluations of the Min-Max algorithm for each action, and checks the amount and extent of suboptimal tactical actions and their implications on performance.",
                "Table 1 shows results and insights from the games heuristic analysis, when search depth equals 3 (this search depth was selected for the results to be comparable to [9], see Section 3.2.3).",
                "The tables heuristic data is the difference between the present maximal heuristic value and the heuristic value of the action that was eventually taken by the player (i.e., the closer the number is to 0, the closer the action was to the maximum heuristic action).",
                "The first row presents the difference values of the action that had the maximal difference value among all the Black players actions in a given game, as averaged over all Blacks winning and losing games (see respective columns).",
                "In games in which the Black player loses, its average difference value was -17.62, while in games in which the Black player won, its average was -12.02.",
                "The second row expands the analysis by considering the 3 highest heuristic difference actions, and averaging them.",
                "In that case, we notice an average heuristic difference of 5 points between games which the Black player loses and games in which it wins.",
                "Nevertheless, the importance of those numbers is that they allowed us to take an educated guess on a threshold number of 11.5, as the value of the TrH constant, which differentiates between normal actions and highly beneficial ones.",
                "After finding an approximated TrH constant, we can proceed with an analysis of the importance of suboptimal moves.",
                "To do so we took the subset of games in which the minimum heuristic difference value for Blacks actions was 11.5.",
                "As presented in Table 2, we can see the different min3 h average of the 3 largest ranges and the respective percentage of games won.",
                "The first row shows that the Black player won only 12% of the games in which the average of its 3 highest heuristically difference actions (min3 h) was smaller than the suggested threshold, TrH = 11.5.",
                "The second row shows a surprising result: it seems that when min3 h > −4 the Black player rarely wins.",
                "Intuition would suggest that games in which the action evaluation values were closer to the maximal values will result in more winning games for Black.",
                "However, it seems that in the Connect-Four domain, merely responding with somewhat easily expected actions, without initiating a few surprising and suboptimal moves, does not yield good results.",
                "The last row sums up the main insights from the analysis; most of Blacks wins (83%) came when its min3 h was in the range of -11.5 to -4.",
                "A close inspection of those Black winning games shows the following pattern behind the numbers: after standard opening moves, Black suddenly drops a disc into an isolated column, which seems a waste of a move.",
                "White continues to build its threats, while usually disregarding Blacks last move, which in turn uses the isolated disc as an anchor for a future winning threat.",
                "The results show that it was beneficial for the Black player Table 2: Blacks winnings percentages % of games min3 h < −11.5 12% min3 h > −4 5% −11.5 ≤ min3 h ≤ −4 83% to take suboptimal actions and not give the current highest possible heuristic value, but will not be too harmful for its position (i.e., will not give high beneficial value to its adversary).",
                "As it turned out, learning the threshold is an important aspect of success: taking wildly risky moves (min3 h < −11.5) or trying to avoid them (min3 h > −4) reduces the Black players winning chances by a large margin. 3.2.2 Affirming the Profile Monitoring Axiom In the task of showing the importance of monitoring ones adversaries profiles, our log files could not be used because they did not contain repeated interactions between players, which are needed to infer the players knowledge about their adversaries.",
                "However, the importance of opponent modeling and its use in attaining tactical advantages was already studied in various domains ([3, 9] are good examples).",
                "In a recent paper, Markovitch and Reger [9] explored the notion of learning and exploitation of opponent weakness in competitive interactions.",
                "They apply simple learning strategies by analyzing examples from past interactions in a specific domain.",
                "They also used the Connect-Four adversarial domain, which can now be used to understand the importance of monitoring the adversarys profile.",
                "Following the presentation of their theoretical model, they describe an extensive empirical study and check the agents performance after learning the weakness model with past examples.",
                "One of the domains used as a competitive environment was the same Connect-Four game (Checkers was the second domain).",
                "Their heuristic function was identical to ours with three different variations (H1, H2, and H3) that are distinguished from one another in their linear combination coefficient values.",
                "The search depth for the players was 3 (as in our analysis).",
                "Their extensive experiments check and compare various learning strategies, risk factors, predefined feature sets and usage methods.",
                "The bottom line is that the Connect-Four domain shows an improvement from a 0.556 winning rate before modeling to a 0.69 after modeling (page 22).",
                "Their conclusions, showing improved performance when holding and using the adversarys model, justify the effort to monitor the adversary profile for continuous and repeated interactions.",
                "An additional point that came up in their experiments is the following: after the opponent weakness model has been learned, the authors describe different methods of integrating the opponent weakness model into the agents decision strategy.",
                "Nevertheless, regardless of the specific method they chose to work with, all integration methods might cause the agent to take suboptimal decisions; it might cause the agent to prefer actions that are suboptimal at the present decision junction, but which might cause the opponent to react in accordance with its weakness model (as represented by our agent) which in turn will be beneficial for us in the future.",
                "The agents behavior, as demonstrated in [9] further confirms and strengthens our Suboptimal Tactical Axiom as discussed in the previous section. 556 The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) 3.2.3 Additional Insights The need for the Goal Achieving, Preventive Act, and Evaluation Maximization axioms are obvious, and need no further verification.",
                "However, even with respect to those axioms, a few interesting insights came up in the log analysis.",
                "The Goal achieving and Preventive Act axioms, though theoretically trivial, seem to provide some challenge to a human player.",
                "In the initial inspection of the logs, we encountered few games2 where a player, for inexplicable reasons, did not block the other from winning or failed to execute its own winning move.",
                "We can blame those faults on the humans lack of attention, or a typing error in its move reply; nevertheless, those errors might occur in bounded rational agents, and the appropriate behavior needs to be axiomatized.",
                "A typical Connect-Four game revolves around generating threats and blocking them.",
                "In our analysis we looked for explicit preventive actions, i.e., moves that block a group of 3 discs, or that remove a future threat (in our limited search horizon).",
                "We found that in 83% of the total games there was at least one preventive action taken by the Black player.",
                "It was also found that Black averaged 2.8 preventive actions per game on the games in which it lost, while averaging 1.5 preventive actions per game when winning.",
                "It seems that Black requires 1 or 2 preventive actions to build its initial taking position, before starting to present threats.",
                "If it did not manage to win, it will usually prevent an extra threat or two before succumbing to White. 4.",
                "RELATED WORK Much research deals with the axiomatization of teamwork and mental states of individuals: some models use knowledge and belief [10], others have models of goals and intentions [8, 4].",
                "However, all these formal theories deal with agent teamwork and cooperation.",
                "As far as we know, our model is the first to provide a formalized model for explicit adversarial environments and agents behavior in it.",
                "The classical Min-Max adversarial search algorithm was the first attempt to integrate the opponent into the search space with a weak assumption of an optimally playing opponent.",
                "Since then, much effort has gone into integrating the opponent model into the decision procedure to predict future behavior.",
                "The M∗ algorithm presented by Carmel and Markovitch [2] showed a method of incorporating opponent models into adversary search, while in [3] they used learning to provide a more accurate opponent model in a 2player repeated game environment, where agents strategies were modeled as finite automata.",
                "Additional Adversarial planning work was done by Willmott et al. [13], which provided an adversarial planning approach to the game of GO.",
                "The research mentioned above dealt with adversarial search and the integration of opponent models into classical utilitybased search methods.",
                "That work shows the importance of opponent modeling and the ability to exploit it to an agents advantage.",
                "However, the basic limitations of those search methods still apply; our model tries to overcome those limitations by presenting a formal model for a new, mental state-based adversarial specification. 5.",
                "CONCLUSIONS We presented an <br>adversarial environment</br> model for a 2 These were later removed from the final analysis. bounded rational agent that is situated in an N-player, zerosum environment.",
                "We used the SharedPlans formalization to define the model and the axioms that agents can apply as behavioral guidelines.",
                "The model is meant to be used as a guideline for designing agents that need to operate in such adversarial environments.",
                "We presented empirical results, based on ConnectFour log file analysis, that exemplify the model and the axioms for a bilateral instance of the environment.",
                "The results we presented are a first step towards an expanded model that will cover all types of adversarial environments, for example, environments that are non-zero-sum, and environments that contain natural agents that are not part of the direct conflict.",
                "Those challenges and more will be dealt with in future research. 6.",
                "ACKNOWLEDGMENT This research was supported in part by Israel Science Foundation grants #1211/04 and #898/05. 7.",
                "REFERENCES [1] L. V. Allis.",
                "A knowledge-based approach of Connect-Four - the game is solved: White wins.",
                "Masters thesis, Free University, Amsterdam, The Netherlands, 1988. [2] D. Carmel and S. Markovitch.",
                "Incorporating opponent models into adversary search.",
                "In Proceedings of the Thirteenth National Conference on Artificial Intelligence, pages 120-125, Portland, OR, 1996. [3] D. Carmel and S. Markovitch.",
                "Opponent modeling in multi-agent systems.",
                "In G. Weiß and S. Sen, editors, Adaptation and Learning in Multi-Agent Systems, pages 40-52.",
                "Springer-Verlag, 1996. [4] B. J. Grosz and S. Kraus.",
                "Collaborative plans for complex group action.",
                "Artificial Intelligence, 86(2):269-357, 1996. [5] M. Hadad, G. Kaminka, G. Armon, and S. Kraus.",
                "Supporting collaborative activity.",
                "In Proc. of AAAI-2005, pages 83-88, Pittsburgh, 2005. [6] http://www.gamerz.net/˜pbmserv/. [7] S. Kraus and D. Lehmann.",
                "Designing and building a negotiating automated agent.",
                "Computational Intelligence, 11:132-171, 1995. [8] H. J. Levesque, P. R. Cohen, and J. H. T. Nunes.",
                "On acting together.",
                "In Proc. of AAAI-90, pages 94-99, Boston, MA, 1990. [9] S. Markovitch and R. Reger.",
                "Learning and exploiting relative weaknesses of opponent agents.",
                "Autonomous Agents and Multi-Agent Systems, 10(2):103-130, 2005. [10] Y. M. Ronald Fagin, Joseph Y. Halpern and M. Y. Vardi.",
                "Reasoning about knowledge.",
                "MIT Press, Cambridge, Mass., 1995. [11] P. Thagard.",
                "Adversarial problem solving: Modeling an oponent using explanatory coherence.",
                "Cognitive Science, 16(1):123-149, 1992. [12] R. W. Toseland and R. F. Rivas.",
                "An Introduction to Group Work Practice.",
                "Prentice Hall, Englewood Cliffs, NJ, 2nd edition edition, 1995. [13] S. Willmott, J. Richardson, A. Bundy, and J. Levine.",
                "An adversarial planning approach to Go.",
                "Lecture Notes in Computer Science, 1558:93-112, 1999.",
                "The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) 557"
            ],
            "original_annotated_samples": [],
            "translated_annotated_samples": [
                "Un modelo de \"entorno adversario\" para agentes racionales limitados en interacciones de suma cero en Zuckerman1, Sarit Kraus1, Jeffrey S. Rosenschein2, Gal Kaminka1 1 Departamento de Ciencias de la Computación 2 La Facultad de Ingeniería Bar-Ilan Universidad y Ciencias de la Computación Ramat-Gan, Israel, IsraelUniversidad Hebrea, Jerusalén, Israel {Zukermi, Sarit, Galk}@cs.biu.ac.il jeff@cs.huji.ac.il Los entornos multiagentes abstractos a menudo no son cooperativos ni colaborativos;En muchos casos, los agentes tienen intereses contradictorios, lo que lleva a interacciones adversas.entorno adversario",
                "Este documento presenta un modelo formal de \"entorno adversario\" para agentes racionales limitados que operan en un entorno de suma cero.entorno adversario",
                "Definimos un \"entorno adversario\" al describir los estados mentales de un agente en dicho entorno.entorno adversario",
                "En este documento, desarrollamos un modelo formal y axiomatizado para agentes racionales limitados que están situados en un \"entorno adversario\" de suma cero.entorno adversario",
                "Entornos adversos El modelo de \"entorno adversario\" (denotado como AE) está destinado a guiar el diseño de agentes proporcionando una especificación de las capacidades y actitudes mentales de un agente en un \"entorno adversario\".entorno adversario",
                "En particular, nuestro modelo de \"entorno adversario\" se ocupará de las interacciones que consisten en n agentes (n ≥ 2), donde todos los agentes son adversarios, y solo un agente puede tener éxito.entorno adversario",
                "En tales casos, podría no considerarse incluso estar en un \"entorno adversario\".entorno adversario",
                "La siguiente lista especifica nuevos predicados, funciones, variables y constantes utilizadas junto con las definiciones originales para la formalización del \"entorno adversario\": 1. φ es una acción nula (el agente no hace nada).2. Entorno adversario",
                "Advknow (p aj ai, tn+1)> advknow (p aj ai, tn) ⇒ (∀α1, ..., αu ∈ Cai, β1, ..., βv ∈ Caj) bel (AAG, setAction (α1,.., αu, β1, ..., βV), tn) ⇒ bel (aag, setAction (α1, ..., αu, β1, ..., βV), tn+1) 2.3 la formulación del medio ambiente la formulación del medio ambienteLos siguientes axiomas proporcionan la definición formal para un \"entorno adversario\" simple de suma cero (AE).entorno adversario",
                "Los siguientes axiomas especifican los principios de comportamiento que pueden usarse para diferenciar entre agentes exitosos y menos exitosos en el \"entorno adversario\" anterior.entorno adversario",
                "Creyendo que tomar medidas α evitará que el oponente haga su β, adoptará una intención potencial de hacer α.(∀Aag, ao ∈ A, α ∈ Caag, β ∈ Cao, TN, Tβ, W ∈ W) (BEL (AAG, DO (AO, β, Tβ, W) ∧ Eval (AO, β, W)> TRH, Tn) ∧ bel (aag, do (aag, α, tα, w) ⇒ ¬do (ao, β, tβ, w), tn) ⇒ pot.int.to (aag, α, tn, tα, w)Este axioma es un componente básico de cualquier \"entorno adversario\". Entorno adversario",
                "Axioma de formación de la alianza El siguiente axioma de comportamiento es relevante solo en una instanciación multilateral del \"entorno adversario\" (obviamente, una alianza no puede formarse en un encuentro bilateral de suma cero).entorno adversario",
                "Después de que se forma una alianza, sus miembros ahora están trabajando en su \"entorno adversario\" normal, así como de acuerdo con los estados mentales y los axiomas necesarios para sus interacciones como parte de la alianza.entorno adversario",
                "Tal comportamiento no puede ocurrir en un \"entorno adversario\" puro (como es un juego de suma cero), donde los miembros de la alianza están constantemente de vigilancia para manipular su alianza a su propio beneficio.entorno adversario",
                "Evaluación El objetivo principal de nuestro análisis experimental es evaluar el comportamiento y el rendimiento de los modelos en un \"entorno adversario\" real.entorno adversario",
                "Conf.En agentes autónomos y sistemas de múltiples agentes (AAMAS 07), los agentes racionales situados en tales entornos adversos serán mejor aplicando nuestros axiomas de comportamiento sugeridos.3.1 El dominio para explorar el uso del modelo anterior y sus axiomas de comportamiento, decidimos usar el juego Connect-Four como nuestro \"entorno adversario\".entorno adversario",
                "En un entorno educativo así, un nuevo conjunto de axiomas conductuales podría ser más beneficioso para los objetivos de los padres que nuestros axiomas conductuales adversos sugeridos.3.2 Análisis de axiomas Después de mostrar que el juego Connect-Four es de hecho un \"entorno adversario\" bilateral de suma cero, el siguiente paso es observar los comportamientos de los jugadores durante el juego y verificar si comportarse de acuerdo con nuestro modelo mejora el rendimiento.entorno adversario",
                "Conclusiones presentamos un modelo de \"entorno adversario\" para un 2. Estas luego se eliminaron del análisis final.Agente racional limitado que está situado en un entorno N-Relayer, Zerosum.entorno adversario"
            ],
            "translated_text": "",
            "candidates": [],
            "error": []
        },
        "behavioral axiom": {
            "translated_key": "",
            "is_in_text": true,
            "original_annotated_sentences": [
                "An Adversarial Environment Model for Bounded Rational Agents in Zero-Sum Interactions Inon Zuckerman1 , Sarit Kraus1 , Jeffrey S. Rosenschein2 , Gal Kaminka1 1 Department of Computer Science 2 The School of Engineering Bar-Ilan University and Computer Science Ramat-Gan, Israel Hebrew University, Jerusalem, Israel {zukermi,sarit,galk}@cs.biu.ac.il jeff@cs.huji.ac.il ABSTRACT Multiagent environments are often not cooperative nor collaborative; in many cases, agents have conflicting interests, leading to adversarial interactions.",
                "This paper presents a formal Adversarial Environment model for bounded rational agents operating in a zero-sum environment.",
                "In such environments, attempts to use classical utility-based search methods can raise a variety of difficulties (e.g., implicitly modeling the opponent as an omniscient utility maximizer, rather than leveraging a more nuanced, explicit opponent model).",
                "We define an Adversarial Environment by describing the mental states of an agent in such an environment.",
                "We then present behavioral axioms that are intended to serve as design principles for building such adversarial agents.",
                "We explore the application of our approach by analyzing log files of completed Connect-Four games, and present an empirical analysis of the axioms appropriateness.",
                "Categories and Subject Descriptors I.2.11 [Artificial Intelligence]: Distributed Artificial Intelligence-Intelligent agents,Multiagent Systems; I.2.4 [Artificial Intelligence]: Knowledge Representation Formalisms and Methods -Modal logic General Terms Design, Theory 1.",
                "INTRODUCTION Early research in multiagent systems (MAS) considered cooperative groups of agents; because individual agents had limited resources, or limited access to information (e.g., limited processing power, limited sensor coverage), they worked together by design to solve problems that individually they could not solve, or at least could not solve as efficiently.",
                "MAS research, however, soon began to consider interacting agents with individuated interests, as representatives of different humans or organizations with non-identical interests.",
                "When interactions are guided by diverse interests, participants may have to overcome disagreements, uncooperative interactions, and even intentional attempts to damage one another.",
                "When these types of interactions occur, environments require appropriate behavior from the agents situated in them.",
                "We call these environments Adversarial Environments, and call the clashing agents Adversaries.",
                "Models of cooperation and teamwork have been extensively explored in MAS through the axiomatization of mental states (e.g., [8, 4, 5]).",
                "However, none of this research dealt with adversarial domains and their implications for agent behavior.",
                "Our paper addresses this issue by providing a formal, axiomatized mental state model for a subset of adversarial domains, namely simple zero-sum adversarial environments.",
                "Simple zero-sum encounters exist of course in various twoplayer games (e.g., Chess, Checkers), but they also exist in n-player games (e.g., Risk, Diplomacy), auctions for a single good, and elsewhere.",
                "In these latter environments especially, using a utility-based adversarial search (such as the Min-Max algorithm) does not always provide an adequate solution; the payoff function might be quite complex or difficult to quantify, and there are natural computational limitations on bounded rational agents.",
                "In addition, traditional search methods (like Min-Max) do not make use of a model of the opponent, which has proven to be a valuable addition to adversarial planning [9, 3, 11].",
                "In this paper, we develop a formal, axiomatized model for bounded rational agents that are situated in a zero-sum adversarial environment.",
                "The model uses different modality operators, and its main foundations are the SharedPlans [4] model for collaborative behavior.",
                "We explore environment properties and the mental states of agents to derive behavioral axioms; these behavioral axioms constitute a formal model that serves as a specification and design guideline for agent design in such settings.",
                "We then investigate the behavior of our model empirically using the Connect-Four board game.",
                "We show that this game conforms to our environment definition, and analyze players behavior using a large set of completed match log 550 978-81-904262-7-5 (RPS) c 2007 IFAAMAS files.",
                "In addition, we use the results presented in [9] to discuss the importance of opponent modeling in our ConnectFour adversarial domain.",
                "The paper proceeds as follows.",
                "Section 2 presents the models formalization.",
                "Section 3 presents the empirical analysis and its results.",
                "We discuss related work in Section 4, and conclude and present future directions in Section 5. 2.",
                "ADVERSARIAL ENVIRONMENTS The adversarial environment model (denoted as AE) is intended to guide the design of agents by providing a specification of the capabilities and mental attitudes of an agent in an adversarial environment.",
                "We focus here on specific types of adversarial environments, specified as follows: 1.",
                "Zero-Sum Interactions: positive and negative utilities of all agents sum to zero; 2.",
                "Simple AEs: all agents in the environment are adversarial agents; 3.",
                "Bilateral AEs: AEs with exactly two agents; 4.",
                "Multilateral AEs: AEs of three or more agents.",
                "We will work on both bilateral and multilateral instantiations of zero-sum and simple environments.",
                "In particular, our adversarial environment model will deal with interactions that consist of N agents (N ≥ 2), where all agents are adversaries, and only one agent can succeed.",
                "Examples of such environments range from board games (e.g., Chess, Connect-Four, and Diplomacy) to certain economic environments (e.g., N-bidder auctions over a single good). 2.1 Model Overview Our approach is to formalize the mental attitudes and behaviors of a single adversarial agent; we consider how a single agent perceives the AE.",
                "The following list specifies the conditions and mental states of an agent in a simple, zero-sum AE: 1.",
                "The agent has an individual intention that its own goal will be completed; 2.",
                "The agent has an individual belief that it and its adversaries are pursuing full conflicting goals (defined below)there can be only one winner; 3.",
                "The agent has an individual belief that each adversary has an intention to complete its own full conflicting goal; 4.",
                "The agent has an individual belief in the (partial) profile of its adversaries.",
                "Item 3 is required, since it might be the case that some agent has a full conflicting goal, and is currently considering adopting the intention to complete it, but is, as of yet, not committed to achieving it.",
                "This might occur because the agent has not yet deliberated about the effects that adopting that intention might have on the other intentions it is currently holding.",
                "In such cases, it might not consider itself to even be in an adversarial environment.",
                "Item 4 states that the agent should hold some belief about the profiles of its adversaries.",
                "The profile represents all the knowledge the agent has about its adversary: its weaknesses, strategic capabilities, goals, intentions, trustworthiness, and more.",
                "It can be given explicitly or can be learned from observations of past encounters. 2.2 Model Definitions for Mental States We use Grosz and Krauss definitions of the modal operators, predicates, and meta-predicates, as defined in their SharedPlan formalization [4].",
                "We recall here some of the predicates and operators that are used in that formalization: Int.To(Ai, α, Tn, Tα, C) represents Ais intentions at time Tn to do an action α at time Tα in the context of C. Int.Th(Ai, prop, Tn, Tprop, C) represents Ais intentions at time Tn that a certain proposition prop holds at time Tprop in the context of C. The potential intention operators, Pot.Int.To(...) and Pot.Int.Th(...), are used to represent the mental state when an agent considers adopting an intention, but has not deliberated about the interaction of the other intentions it holds.",
                "The operator Bel(Ai, f, Tf ) represents agent Ai believing in the statement expressed in formula f, at time Tf .",
                "MB(A, f, Tf ) represents mutual belief for a group of agents A.",
                "A snapshot of the system finds our environment to be in some state e ∈ E of environmental variable states, and each adversary in any LAi ∈ L of possible local states.",
                "At any given time step, the system will be in some world w of the set of all possible worlds w ∈ W, where w = E×LA1 ×LA2 × ...LAn , and n is the number of adversaries.",
                "For example, in a Texas Holdem poker game, an agents local state might be its own set of cards (which is unknown to its adversary) while the environment will consist of the betting pot and the community cards (which are visible to both players).",
                "A utility function under this formalization is defined as a mapping from a possible world w ∈ W to an element in , which expresses the desirability of the world, from a single agent perspective.",
                "We usually normalize the range to [0,1], where 0 represents the least desirable possible world, and 1 is the most desirable world.",
                "The implementation of the utility function is dependent on the domain in question.",
                "The following list specifies new predicates, functions, variables, and constants used in conjunction with the original definitions for the adversarial environment formalization: 1. φ is a null action (the agent does not do anything). 2.",
                "GAi is the set of agent Ais goals.",
                "Each goal is a set of predicates whose satisfaction makes the goal complete (we use G∗ Ai ∈ GAi to represent an arbitrary goal of agent Ai). 3. gAi is the set of agent Ais subgoals.",
                "Subgoals are predicates whose satisfaction represents an important milestone toward achievement of the full goal. gG∗ Ai ⊆ gAi is the set of subgoals that are important to the completion of goal G∗ Ai (we will use g∗ G∗ Ai ∈ gG∗ Ai to represent an arbitrary subgoal). 4.",
                "P Aj Ai is the profile object agent Ai holds about agent Aj. 5.",
                "CA is a general set of actions for all agents in A which are derived from the environments constraints.",
                "CAi ⊆ CA is the set of agent Ais possible actions. 6.",
                "Do(Ai, α, Tα, w) holds when Ai performs action α over time interval Tα in world w. 7.",
                "Achieve(G∗ Ai , α, w) is true when goal G∗ Ai is achieved following the completion of action α in world w ∈ W, where α ∈ CAi . 8.",
                "Profile(Ai, PAi Ai ) is true when agent Ai holds an object profile for agent Aj.",
                "Definition 1.",
                "Full conflict (FulConf ) describes a zerosum interaction where only a single goal of the goals in conflict can be completed.",
                "FulConf(G∗ Ai , G∗ Aj ) ⇒ (∃α ∈ CAi , ∀w, β ∈ CAj ) (Achieve(G∗ Ai , α, w) ⇒ ¬Achieve(G∗ Aj , β, w)) ∨ (∃β ∈ CAj , ∀w, α ∈ CAi )(Achieve(G∗ Aj , β, w) ⇒ ¬Achieve(G∗ Ai , α, w)) Definition 2.",
                "Adversarial Knowledge (AdvKnow) is a function returning a value which represents the amount of The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) 551 knowledge agent Ai has on the profile of agent Aj, at time Tn.",
                "The higher the value, the more knowledge agent Ai has.",
                "AdvKnow : P Aj Ai × Tn → Definition 3.",
                "Eval - This evaluation function returns an estimated expected utility value for an agent in A, after completing an action from CA in some world state w. Eval : A × CA × w → Definition 4.",
                "TrH - (Threshold) is a numerical constant in the [0,1] range that represents an evaluation function (Eval) threshold value.",
                "An action that yields an estimated utility evaluation above the TrH is regarded as a highly beneficial action.",
                "The Eval value is an estimation and not the real utility function, which is usually unknown.",
                "Using the real utility value for a rational agent would easily yield the best outcome for that agent.",
                "However, agents usually do not have the real utility functions, but rather a heuristic estimate of it.",
                "There are two important properties that should hold for the evaluation function: Property 1.",
                "The evaluation function should state that the most desirable world state is one in which the goal is achieved.",
                "Therefore, after the goal has been satisfied, there can be no future action that can put the agent in a world state with higher Eval value. (∀Ai, G∗ Ai , α, β ∈ CAi , w ∈ W) Achieve(G∗ Ai , α, w) ⇒ Eval(Ai, α, w) ≥ Eval(Ai, β, w) Property 2.",
                "The evaluation function should project an action that causes a completion of a goal or a subgoal to a value which is greater than TrH (a highly beneficial action). (∀Ai, G∗ Ai ∈ GAi , α ∈ CAi , w ∈ W, g∗ GAi ∈ gGAi ) Achieve(G∗ Ai , α, w) ∨ Achieve(g∗ GAi , α, w) ⇒ Eval(Ai, α, w) ≥ TrH.",
                "Definition 5.",
                "SetAction We define a set action (SetAction) as a set of action operations (either complex or basic actions) from some action sets CAi and CAj which, according to agent Ais belief, are attached together by a temporal and consequential relationship, forming a chain of events (action, and its following consequent action). (∀α1 , . . . , αu ∈ CAi , β1 , . . . , βv ∈ CAj , w ∈ W) SetAction(α1 , . . . , αu , β1 , . . . , βv , w) ⇒ ((Do(Ai, α1 , Tα1 , w) ⇒ Do(Aj, β1 , Tβ1 , w)) ⇒ Do(Ai, α2 , Tα2 , w) ⇒ . . . ⇒ Do(Ai, αu , Tαu , w)) The consequential relation might exist due to various environmental constraints (when one action forces the adversary to respond with a specific action) or due to the agents knowledge about the profile of its adversary.",
                "Property 3.",
                "As the knowledge we have about our adversary increases we will have additional beliefs about its behavior in different situations which in turn creates new set actions.",
                "Formally, if our AdvKnow at time Tn+1 is greater than AdvKnow at time Tn, then every SetAction known at time Tn is also known at time Tn+1.",
                "AdvKnow(P Aj Ai , Tn+1) > AdvKnow(P Aj Ai , Tn) ⇒ (∀α1 , . . . , αu ∈ CAi , β1 , . . . , βv ∈ CAj ) Bel(Aag, SetAction(α1 , . . . , αu , β1 , . . . , βv ), Tn) ⇒ Bel(Aag, SetAction(α1 , . . . , αu , β1 , . . . , βv ), Tn+1) 2.3 The Environment Formulation The following axioms provide the formal definition for a simple, zero-sum Adversarial Environment (AE).",
                "Satisfaction of these axioms means that the agent is situated in such an environment.",
                "It provides specifications for agent Aag to interact with its set of adversaries A with respect to goals G∗ Aag and G∗ A at time TCo at some world state w. AE(Aag, A, G∗ Aag , A1, . . . , Ak, G∗ A1 , . . . , G∗ Ak , Tn, w) 1.",
                "Aag has an Int.Th his goal would be completed: (∃α ∈ CAag , Tα) Int.Th(Aag, Achieve(G∗ Aag , α), Tn, Tα, AE) 2.",
                "Aag believes that it and each of its adversaries Ao are pursuing full conflicting goals: (∀Ao ∈ {A1, . . . , Ak}) Bel(Aag, FulConf(G∗ Aag , G∗ Ao ), Tn) 3.",
                "Aag believes that each of his adversaries in Ao has the Int.Th his conflicting goal G∗ Aoi will be completed: (∀Ao ∈ {A1, . . . , Ak})(∃β ∈ CAo , Tβ) Bel(Aag, Int.Th(Ao, Achieve(G∗ Ao , β), TCo, Tβ, AE), Tn) 4.",
                "Aag has beliefs about the (partial) profiles of its adversaries (∀Ao ∈ {A1, . . . , Ak}) (∃PAo Aag ∈ PAag )Bel(Aag, Profile(Ao, PAo Aag ), Tn) To build an agent that will be able to operate successfully within such an AE, we must specify behavioral guidelines for its interactions.",
                "Using a naive Eval maximization strategy to a certain search depth will not always yield satisfactory results for several reasons: (1) the search horizon problem when searching for a fixed depth; (2) the strong assumption of an optimally rational, unbounded resources adversary; (3) using an estimated evaluation function which will not give optimal results in all world states, and can be exploited [9].",
                "The following axioms specify the behavioral principles that can be used to differentiate between successful and less successful agents in the above Adversarial Environment.",
                "Those axioms should be used as specification principles when designing and implementing agents that should be able to perform well in such Adversarial Environments.",
                "The behavioral axioms represent situations in which the agent will adopt potential intentions to (Pot.Int.To(...)) perform an action, which will typically require some means-end reasoning to select a possible course of action.",
                "This reasoning will lead to the adoption of an Int.To(...) (see [4]).",
                "A1.",
                "Goal Achieving Axiom.",
                "The first axiom is the simplest case; when the agent Aag believes that it is one action (α) away from achieving his conflicting goal G∗ Aag , it should adopt the potential intention to do α and complete its goal. (∀Aag, α ∈ CAag , Tn, Tα, w ∈ W) (Bel(Aag, Do(Aag, α, Tα, w) ⇒ Achieve(G∗ Aag , α, w)) ⇒ Pot.Int.To(Aag, α, Tn, Tα, w) This somewhat trivial behavior is the first and strongest axiom.",
                "In any situation, when the agent is an action away from completing the goal, it should complete the action.",
                "Any fair Eval function would naturally classify α as the maximal value action (property 1).",
                "However, without explicit axiomatization of such behavior there might be situations where the agent will decide on taking another action for various reasons, due to its bounded decision resources.",
                "A2.",
                "Preventive Act Axiom.",
                "Being in an adversarial situation, agent Aag might decide to take actions that will damage one of its adversarys plans to complete its goal, even if those actions do not explicitly advance Aag towards its conflicting goal G∗ Aag .",
                "Such preventive action will take place when agent Aag has a belief about the possibility of its adversary Ao doing an action β that will give it a high 552 The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) utility evaluation value (> TrH).",
                "Believing that taking action α will prevent the opponent from doing its β, it will adopt a potential intention to do α. (∀Aag, Ao ∈ A, α ∈ CAag , β ∈ CAo , Tn, Tβ, w ∈ W) (Bel(Aag, Do(Ao, β, Tβ, w) ∧ Eval(Ao, β, w) > TrH, Tn) ∧ Bel(Aag, Do(Aag, α, Tα, w) ⇒ ¬Do(Ao, β, Tβ, w), Tn) ⇒ Pot.Int.To(Aag, α, Tn, Tα, w) This axiom is a basic component of any adversarial environment.",
                "For example, looking at a Chess board game, a player could realize that it is about to be checkmated by its opponent, thus making a preventive move.",
                "Another example is a Connect Four game: when a player has a row of three chips, its opponent must block it, or lose.",
                "A specific instance of A1 occurs when the adversary is one action away from achieving its goal, and immediate preventive action needs to be taken by the agent.",
                "Formally, we have the same beliefs as stated above, with a changed belief that doing action β will cause agent Ao to achieve its goal.",
                "Proposition 1: Prevent or lose case. (∀Aag, Ao ∈ A, α ∈ CAag , β ∈ CAo , G∗ Ao , Tn, Tα, Tβ, w ∈ W) Bel(Aag, Do(Ao, β, Tβ, w) ⇒ Achieve(G∗ Ao , β, w), Tn) ∧ Bel(Aag, Do(Aag, α, Tα, w) ⇒ ¬Do(Ao, β, Tβ, w)) ⇒ Pot.Int.To(Aag, α, Tn, Tα, w) Sketch of proof: Proposition 1 can be easily derived from axiom A1 and the property 2 of the Eval function, which states that any action that causes a completion of a goal is a highly beneficial action.",
                "The preventive act behavior will occur implicitly when the Eval function is equal to the real world utility function.",
                "However, being bounded rational agents and dealing with an estimated evaluation function we need to explicitly axiomatize such behavior, for it will not always occur implicitly from the evaluation function.",
                "A3.",
                "Suboptimal Tactical Move Axiom.",
                "In many scenarios a situation may occur where an agent will decide not to take the current most beneficial action it can take (the action with the maximal utility evaluation value), because it believes that taking another action (with lower utility evaluation value) might yield (depending on the adversarys response) a future possibility for a highly beneficial action.",
                "This will occur most often when the Eval function is inaccurate and differs by a large extent from the Utility function.",
                "Put formally, agent Aag believes in a certain SetAction that will evolve according to its initial action and will yield a high beneficial value (> TrH) solely for it. (∀Aag, Ao ∈ A, Tn, w ∈ W) (∃α1 , . . . , αu ∈ CAi , β1 , . . . , βv ∈ CAj , Tα1 ) Bel(Aag, SetAction(α1 , . . . , αu , β1 , . . . , βv ), Tn) ∧ Bel(Aag, Eval(Ao, βv , w) < TrH < Eval(Aag, αu , w), Tn) ⇒ Pot.Int.To(Aag, α1 , Tn, Tα1 , w) An agent might believe that a chain of events will occur for various reasons due to the inevitable nature of the domain.",
                "For example, in Chess, we often observe the following: a move causes a check position, which in turn limits the opponents moves to avoiding the check, to which the first player might react with another check, and so on.",
                "The agent might also believe in a chain of events based on its knowledge of its adversarys profile, which allows it to foresee the adversarys movements with high accuracy.",
                "A4.",
                "Profile Detection Axiom.",
                "The agent can adjust its adversarys profiles by observations and pattern study (specifically, if there are repeated encounters with the same adversary).",
                "However, instead of waiting for profile information to be revealed, an agent can also initiate actions that will force its adversary to react in a way that will reveal profile knowledge about it.",
                "Formally, the axiom states that if all actions (γ) are not highly beneficial actions (< TrH), the agent can do action α in time Tα if it believes that it will result in a non-highly beneficial action β from its adversary, which in turn teaches it about the adversarys profile, i.e., gives a higher AdvKnow(P Aj Ai , Tβ). (∀Aag, Ao ∈ A, α ∈ CAag , β ∈ CAo , Tn, Tα, Tβ, w ∈ W) Bel(Aag, (∀γ ∈ CAag )Eval(Aag, γ, w) < TrH, Tn) ∧ Bel(Aag, Do(Aag, α, Tα, w) ⇒ Do(Ao, β, Tβ, w), Tn) ∧ Bel(Aag, Eval(Ao, β, w) < TrH) ∧ Bel(Aag, AdvKnow(P Aj Ai , Tβ) > AdvKnow(P Aj Ai , Tn), Tn) ⇒ Pot.Int.To(Aag, α, Tn, Tα, w) For example, going back to the Chess board game scenario, consider starting a game versus an opponent about whom we know nothing, not even if it is a human or a computerized opponent.",
                "We might start playing a strategy that will be suitable versus an average opponent, and adjust our game according to its level of play.",
                "A5.",
                "Alliance Formation Axiom The following <br>behavioral axiom</br> is relevant only in a multilateral instantiation of the adversarial environment (obviously, an alliance cannot be formed in a bilateral, zero-sum encounter).",
                "In different situations during a multilateral interaction, a group of agents might believe that it is in their best interests to form a temporary alliance.",
                "Such an alliance is an agreement that constrains its members behavior, but is believed by its members to enable them to achieve a higher utility value than the one achievable outside of the alliance.",
                "As an example, we can look at the classical Risk board game, where each player has an individual goal of being the sole conquerer of the world, a zero-sum game.",
                "However, in order to achieve this goal, it might be strategically wise to make short-term ceasefire agreements with other players, or to join forces and attack an opponent who is stronger than the rest.",
                "An alliances terms defines the way its members should act.",
                "It is a set of predicates, denoted as Terms, that is agreed upon by the alliance members, and should remain true for the duration of the alliance.",
                "For example, the set Terms in the Risk scenario, could contain the following predicates: 1.",
                "Alliance members will not attack each other on territories X, Y and Z; 2.",
                "Alliance members will contribute C units per turn for attacking adversary Ao; 3.",
                "Members are obligated to stay as part of the alliance until time Tk or until adversarys Ao army is smaller than Q.",
                "The set Terms specifies inter-group constraints on each of the alliance members (∀Aal i ∈ Aal ⊆ A) set of actions Cal i ⊆ C. Definition 6.",
                "Al val - the total evaluation value that agent Ai will achieve while being part of Aal is the sum of Evali (Eval for Ai) of each of Aal j Eval values after taking their own α actions (via the agent(α) predicate): Al val(Ai, Cal , Aal , w) = α∈Cal Evali(Aal j , agent(α), w) Definition 7.",
                "Al TrH - is a number representing an Al val The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) 553 threshold; above it, the alliance can be said to be a highly beneficial alliance.",
                "The value of Al TrH will be calculated dynamically according to the progress of the interaction, as can be seen in [7].",
                "After an alliance is formed, its members are now working in their normal adversarial environment, as well as according to the mental states and axioms required for their interactions as part of the alliance.",
                "The following Alliance model (AL) specifies the conditions under which the group Aal can be said to be in an alliance and working with a new and constrained set of actions Cal , at time Tn.",
                "AL(Aal , Cal , w, Tn) 1.",
                "Aal has a MB that all members are part of Aal : MB(Aal , (∀Aal i ∈ Aal )member(Aal i , Aal ), Tn) 2.",
                "Aal has a MB that the group be maintained: MB(Aal , (∀Aal i ∈ Aal )Int.Th (Ai, member(Ai, Aal ), Tn, Tn+1, Co), Tn) 3.",
                "Aal has a MB that being members gives them high utility value: MB(Aal , (∀Aal i ∈ Aal )Al val(Aal i , Cal , Aal , w) ≥ Al TrH, Tn) Members profiles are a crucial part of successful alliances.",
                "We assume that agents that have more accurate profiles of their adversaries will be more successful in such environments.",
                "Such agents will be able to predict when a member is about to breach the alliances contract (item 2 in the above model), and take counter measures (when item 3 will falsify).",
                "The robustness of the alliance is in part a function of its members trustfulness measure, objective position estimation, and other profile properties.",
                "We should note that an agent can simultaneously be part of more than one alliance.",
                "Such a temporary alliance, where the group members do not have a joint goal but act collaboratively for the interest of their own individual goals, is classified as a Treatment Group by modern psychologists [12] (in contrast to a Task Group, where its members have a joint goal).",
                "The Shared Activity model as presented in [5] modeled Treatment Group behavior using the same SharedPlans formalization.",
                "When comparing both definitions of an alliance and a Treatment Group we found an unsurprising resemblance between both models: the environment models definitions are almost identical (see SAs definitions in [5]), and their Selfish-Act and Cooperative Act axioms conform to our adversarial agents behavior.",
                "The main distinction between both models is the integration of a Helpful-behavior act axiom, in the Shared Activity which cannot be part of ours.",
                "This axiom states that an agent will consider taking action that will lower its Eval value (to a certain lower bound), if it believes that a group partner will gain a significant benefit.",
                "Such behavior cannot occur in a pure adversarial environment (as a zero-sum game is), where the alliance members are constantly on watch to manipulate their alliance to their own advantage.",
                "A6.",
                "Evaluation Maximization Axiom.",
                "In a case when all other axioms are inapplicable, we will proceed with the action that maximizes the heuristic value as computed in the Eval function. (∀Aag, Ao ∈ A, α ∈ Cag, Tn, w ∈ W) Bel(Aag, (∀γ ∈ Cag)Eval(Aag, α, w) ≥ Eval(Aag, γ, w), Tn) ⇒ Pot.Int.To(Aag, α, Tn, Tα, w) T1.",
                "Optimality on Eval = Utility The above axiomatic model handles situations where the Utility is unknown and the agents are bounded rational agents.",
                "The following theorem shows that in bilateral interactions, where the agents have the real Utility function (i.e., Eval = Utility) and are rational agents, the axioms provide the same optimal result as classic adversarial search (e.g., Min-Max).",
                "Theorem 1.",
                "Let Ae ag be an unbounded rational AE agent using the Eval heuristic evaluation function, Au ag be the same agent using the true Utility function, and Ao be a sole unbounded utility-based rational adversary.",
                "Given that Eval = Utility: (∀α ∈ CAu ag , α ∈ CAe ag , Tn, w ∈ W) Pot.Int.To(Au ag, α, Tn, Tα, w) → Pot.Int.To(Ae ag, α , Tn, Tα, w) ∧ ((α = α ) ∨ (Utility(Au ag, α, w) = Eval(Ae ag, α , w))) Sketch of proof - Given that Au ag has the real utility function and unbounded resources, it can generate the full game tree and run the optimal MinMax algorithm to choose the highest utility value action, which we denote by, α.",
                "The proof will show that Ae ag, using the AE axioms, will select the same or equal utility α (when there is more than one action with the same max utility) when Eval = Utility. (A1) Goal achieving axiom - suppose there is an α such that its completion will achieve Au ags goal.",
                "It will obtain the highest utility by Min-Max for Au ag.",
                "The Ae ag agent will select α or another action with the same utility value via A1.",
                "If such α does not exist, Ae ag cannot apply this axiom, and proceeds to A2. (A2) Preventive act axiom - (1) Looking at the basic case (see Prop1), if there is a β which leads Ao to achieve its goal, then a preventive action α will yield the highest utility for Au ag.",
                "Au ag will choose it through the utility, while Ae ag will choose it through A2. (2) In the general case, β is a highly beneficial action for Ao, thus yields low utility for Au ag, which will guide it to select an α that will prevent β, while Ae ag will choose it through A2.1 If such β does not exist for Ao, then A2 is not applicable, and Ae ag can proceed to A3. (A3) Suboptimal tactical move axiom - When using a heuristic Eval function, Ae ag has a partial belief in the profile of its adversary (item 4 in AE model), which may lead it to believe in SetActions (Prop1).",
                "In our case, Ae ag is holding a full profile on its optimal adversary and knows that Ao will behave optimally according to the real utility values on the complete search tree, therefore, any belief about suboptimal SetAction cannot exist, yielding this axiom inapplicable.",
                "Ae ag will proceed to A4. (A4) Profile detection axiom - Given that Ae ag has the full profile of Ao, none of Ae ags actions can increase its knowledge.",
                "That axiom will not be applied, and the agent will proceed with A6 (A5 will be disregarded because the interaction is bilateral). (A6) Evaluation maximization axiom - This axiom will select the max Eval for Ae ag.",
                "Given that Eval = Utility, the same α that was selected by Au ag will be selected. 3.",
                "EVALUATION The main purpose of our experimental analysis is to evaluate the models behavior and performance in a real adversarial environment.",
                "This section investigates whether bounded 1 A case where following the completion of β there exists a γ which gives high utility for Agent Au ag, cannot occur because Ao uses the same utility, and γs existence will cause it to classify β as a low utility action. 554 The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) rational agents situated in such adversarial environments will be better off applying our suggested behavioral axioms. 3.1 The Domain To explore the use of the above model and its behavioral axioms, we decided to use the Connect-Four game as our adversarial environment.",
                "Connect-Four is a 2-player, zerosum game which is played using a 6x7 matrix-like board.",
                "Each turn, a player drops a disc into one of the 7 columns (the set of 21 discs is usually colored yellow for player 1 and red for player 2; we will use White and Black respectively to avoid confusion).",
                "The winner is the first player to complete a horizontal, vertical, or diagonal set of four discs with its color.",
                "On very rare occasions, the game might end in a tie if all the empty grids are filled, but no player managed to create a 4-disc set.",
                "The Connect-Four game was solved in [1], where it is shown that the first player (playing with the white discs) can force a win by starting in the middle column (column 4) and playing optimally However, the optimal strategy is very complex, and difficult to follow even for complex bounded rational agents, such as human players.",
                "Before we can proceed checking agent behavior, we must first verify that the domain conforms to the adversarial environments definition as given above (which the behavioral axioms are based on).",
                "First, when playing a Connect-Four game, the agent has an intention to win the game (item 1).",
                "Second (item 2), our agent believes that in Connect-Four there can only be one winner (or no winner at all in the rare occurrence of a tie).",
                "In addition, our agent believes that its opponent to the game will try to win (item 3), and we hope it has some partial knowledge (item 4) about its adversary (this knowledge can vary from nothing, through simple facts such as age, to strategies and weaknesses).",
                "Of course, not all Connect-Four encounters are adversarial.",
                "For example, when a parent is playing the game with its child, the following situation might occur: the child, having a strong incentive to win, treats the environment as adversarial (it intends to win, understands that there can only be one winner, and believes that its parent is trying to beat him).",
                "However, the parents point of view might see the environment as an educational one, where its goal is not to win the game, but to cause enjoyment or practice strategic reasoning.",
                "In such an educational environment, a new set of behavioral axioms might be more beneficial to the parents goals than our suggested adversarial behavioral axioms. 3.2 Axiom Analysis After showing that the Connect-Four game is indeed a zero-sum, bilateral adversarial environment, the next step is to look at players behaviors during the game and check whether behaving according to our model does improve performance.",
                "To do so we have collected log files from completed Connect-Four games that were played by human players over the Internet.",
                "Our collected log file data came from Play by eMail (PBeM) sites.",
                "These are web sites that host email games, where each move is taken by an email exchange between the server and the players.",
                "Many such sites archives contain real competitive interactions, and also maintain a ranking system for their members.",
                "Most of the data we used can be found in [6].",
                "As can be learned from [1], Connect-Four has an optimal strategy and a considerable advantage for the player who starts the game (which we call the White player).",
                "We will concentrate in our analysis on the second players moves (to be called Black).",
                "The White player, being the first to act, has the so-called initiative advantage.",
                "Having the advantage and a good strategy will keep the Black player busy reacting to its moves, instead of initiating threats.",
                "A threat is a combination of three discs of the same color, with an empty spot for the fourth winning disk.",
                "An open threat is a threat that can be realized in the opponents next move.",
                "In order for the Black player to win, it must somehow turn the tide, take the advantage and start presenting threats to the White player.",
                "We will explore Black players behavior and their conformance to our axioms.",
                "To do so, we built an application that reads log files and analyzes the Black players moves.",
                "The application contains two main components: (1) a Min-Max algorithm for evaluation of moves; (2) open threats detector for the discovering of open threats.",
                "The Min-Max algorithm will work to a given depth, d and for each move α will output the heuristic value for the next action taken by the player as written in the log file, h(α), alongside the maximum heuristic value, maxh(α), that could be achieved prior to taking the move (obviously, if h(α) = maxh(α), then the player did not do the optimal move heuristically).",
                "The threat detectors job is to notify if some action was taken in order to block an open threat (not blocking an open threat will probably cause the player to lose in the opponents next move).",
                "The heuristic function used by Min-Max to evaluate the players utility is the following function, which is simple to compute, yet provides a reasonable challenge to human opponents: Definition 8.",
                "Let Group be an adjacent set of four squares that are horizontal, vertical, or diagonal.",
                "Groupn b (Groupn w) be a Group with n pieces of the black (white) color and 4−n empty squares. h = ((Group1 b ∗α)+(Group2 b ∗β)+(Group3 b ∗γ)+(Group4 b ∗∞)) − ((Group1 w ∗α)+(Group2 w ∗β)+(Group3 w ∗γ)+(Group4 w ∗∞)) The values of α, β and δ can vary to form any desired linear combination; however, it is important to value them with the α < β < δ ordering in mind (we used 1, 4, and 8 as their respective values).",
                "Groups of 4 discs of the same color means victory, thus discovery of such a group will result in ∞ to ensure an extreme value.",
                "We now use our estimated evaluation function to evaluate the Black players actions during the Connect-Four adversarial interaction.",
                "Each game from the log file was input into the application, which processed and output a reformatted log file containing the h value of the current move, the maxh value that could be achieved, and a notification if an open threat was detected.",
                "A total of 123 games were analyzed (57 with White winning, and 66 with Black winning).",
                "A few additional games were manually ignored in the experiment, due to these problems: a player abandoning the game while the outcome is not final, or a blunt irrational move in the early stages of the game (e.g., not blocking an obvious winning group in the first opening moves).",
                "In addition, a single tie game was also removed.",
                "The simulator was run to a search depth of 3 moves.",
                "We now proceed to analyze the games with respect to each <br>behavioral axiom</br>.",
                "The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) 555 Table 1: Average heuristic difference analysis Black losses Black Won Avg minh -17.62 -12.02 Avg 3 lowest h moves (min3 h) -13.20 -8.70 3.2.1 Affirming the Suboptimal tactical move axiom The following section presents the heuristic evaluations of the Min-Max algorithm for each action, and checks the amount and extent of suboptimal tactical actions and their implications on performance.",
                "Table 1 shows results and insights from the games heuristic analysis, when search depth equals 3 (this search depth was selected for the results to be comparable to [9], see Section 3.2.3).",
                "The tables heuristic data is the difference between the present maximal heuristic value and the heuristic value of the action that was eventually taken by the player (i.e., the closer the number is to 0, the closer the action was to the maximum heuristic action).",
                "The first row presents the difference values of the action that had the maximal difference value among all the Black players actions in a given game, as averaged over all Blacks winning and losing games (see respective columns).",
                "In games in which the Black player loses, its average difference value was -17.62, while in games in which the Black player won, its average was -12.02.",
                "The second row expands the analysis by considering the 3 highest heuristic difference actions, and averaging them.",
                "In that case, we notice an average heuristic difference of 5 points between games which the Black player loses and games in which it wins.",
                "Nevertheless, the importance of those numbers is that they allowed us to take an educated guess on a threshold number of 11.5, as the value of the TrH constant, which differentiates between normal actions and highly beneficial ones.",
                "After finding an approximated TrH constant, we can proceed with an analysis of the importance of suboptimal moves.",
                "To do so we took the subset of games in which the minimum heuristic difference value for Blacks actions was 11.5.",
                "As presented in Table 2, we can see the different min3 h average of the 3 largest ranges and the respective percentage of games won.",
                "The first row shows that the Black player won only 12% of the games in which the average of its 3 highest heuristically difference actions (min3 h) was smaller than the suggested threshold, TrH = 11.5.",
                "The second row shows a surprising result: it seems that when min3 h > −4 the Black player rarely wins.",
                "Intuition would suggest that games in which the action evaluation values were closer to the maximal values will result in more winning games for Black.",
                "However, it seems that in the Connect-Four domain, merely responding with somewhat easily expected actions, without initiating a few surprising and suboptimal moves, does not yield good results.",
                "The last row sums up the main insights from the analysis; most of Blacks wins (83%) came when its min3 h was in the range of -11.5 to -4.",
                "A close inspection of those Black winning games shows the following pattern behind the numbers: after standard opening moves, Black suddenly drops a disc into an isolated column, which seems a waste of a move.",
                "White continues to build its threats, while usually disregarding Blacks last move, which in turn uses the isolated disc as an anchor for a future winning threat.",
                "The results show that it was beneficial for the Black player Table 2: Blacks winnings percentages % of games min3 h < −11.5 12% min3 h > −4 5% −11.5 ≤ min3 h ≤ −4 83% to take suboptimal actions and not give the current highest possible heuristic value, but will not be too harmful for its position (i.e., will not give high beneficial value to its adversary).",
                "As it turned out, learning the threshold is an important aspect of success: taking wildly risky moves (min3 h < −11.5) or trying to avoid them (min3 h > −4) reduces the Black players winning chances by a large margin. 3.2.2 Affirming the Profile Monitoring Axiom In the task of showing the importance of monitoring ones adversaries profiles, our log files could not be used because they did not contain repeated interactions between players, which are needed to infer the players knowledge about their adversaries.",
                "However, the importance of opponent modeling and its use in attaining tactical advantages was already studied in various domains ([3, 9] are good examples).",
                "In a recent paper, Markovitch and Reger [9] explored the notion of learning and exploitation of opponent weakness in competitive interactions.",
                "They apply simple learning strategies by analyzing examples from past interactions in a specific domain.",
                "They also used the Connect-Four adversarial domain, which can now be used to understand the importance of monitoring the adversarys profile.",
                "Following the presentation of their theoretical model, they describe an extensive empirical study and check the agents performance after learning the weakness model with past examples.",
                "One of the domains used as a competitive environment was the same Connect-Four game (Checkers was the second domain).",
                "Their heuristic function was identical to ours with three different variations (H1, H2, and H3) that are distinguished from one another in their linear combination coefficient values.",
                "The search depth for the players was 3 (as in our analysis).",
                "Their extensive experiments check and compare various learning strategies, risk factors, predefined feature sets and usage methods.",
                "The bottom line is that the Connect-Four domain shows an improvement from a 0.556 winning rate before modeling to a 0.69 after modeling (page 22).",
                "Their conclusions, showing improved performance when holding and using the adversarys model, justify the effort to monitor the adversary profile for continuous and repeated interactions.",
                "An additional point that came up in their experiments is the following: after the opponent weakness model has been learned, the authors describe different methods of integrating the opponent weakness model into the agents decision strategy.",
                "Nevertheless, regardless of the specific method they chose to work with, all integration methods might cause the agent to take suboptimal decisions; it might cause the agent to prefer actions that are suboptimal at the present decision junction, but which might cause the opponent to react in accordance with its weakness model (as represented by our agent) which in turn will be beneficial for us in the future.",
                "The agents behavior, as demonstrated in [9] further confirms and strengthens our Suboptimal Tactical Axiom as discussed in the previous section. 556 The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) 3.2.3 Additional Insights The need for the Goal Achieving, Preventive Act, and Evaluation Maximization axioms are obvious, and need no further verification.",
                "However, even with respect to those axioms, a few interesting insights came up in the log analysis.",
                "The Goal achieving and Preventive Act axioms, though theoretically trivial, seem to provide some challenge to a human player.",
                "In the initial inspection of the logs, we encountered few games2 where a player, for inexplicable reasons, did not block the other from winning or failed to execute its own winning move.",
                "We can blame those faults on the humans lack of attention, or a typing error in its move reply; nevertheless, those errors might occur in bounded rational agents, and the appropriate behavior needs to be axiomatized.",
                "A typical Connect-Four game revolves around generating threats and blocking them.",
                "In our analysis we looked for explicit preventive actions, i.e., moves that block a group of 3 discs, or that remove a future threat (in our limited search horizon).",
                "We found that in 83% of the total games there was at least one preventive action taken by the Black player.",
                "It was also found that Black averaged 2.8 preventive actions per game on the games in which it lost, while averaging 1.5 preventive actions per game when winning.",
                "It seems that Black requires 1 or 2 preventive actions to build its initial taking position, before starting to present threats.",
                "If it did not manage to win, it will usually prevent an extra threat or two before succumbing to White. 4.",
                "RELATED WORK Much research deals with the axiomatization of teamwork and mental states of individuals: some models use knowledge and belief [10], others have models of goals and intentions [8, 4].",
                "However, all these formal theories deal with agent teamwork and cooperation.",
                "As far as we know, our model is the first to provide a formalized model for explicit adversarial environments and agents behavior in it.",
                "The classical Min-Max adversarial search algorithm was the first attempt to integrate the opponent into the search space with a weak assumption of an optimally playing opponent.",
                "Since then, much effort has gone into integrating the opponent model into the decision procedure to predict future behavior.",
                "The M∗ algorithm presented by Carmel and Markovitch [2] showed a method of incorporating opponent models into adversary search, while in [3] they used learning to provide a more accurate opponent model in a 2player repeated game environment, where agents strategies were modeled as finite automata.",
                "Additional Adversarial planning work was done by Willmott et al. [13], which provided an adversarial planning approach to the game of GO.",
                "The research mentioned above dealt with adversarial search and the integration of opponent models into classical utilitybased search methods.",
                "That work shows the importance of opponent modeling and the ability to exploit it to an agents advantage.",
                "However, the basic limitations of those search methods still apply; our model tries to overcome those limitations by presenting a formal model for a new, mental state-based adversarial specification. 5.",
                "CONCLUSIONS We presented an Adversarial Environment model for a 2 These were later removed from the final analysis. bounded rational agent that is situated in an N-player, zerosum environment.",
                "We used the SharedPlans formalization to define the model and the axioms that agents can apply as behavioral guidelines.",
                "The model is meant to be used as a guideline for designing agents that need to operate in such adversarial environments.",
                "We presented empirical results, based on ConnectFour log file analysis, that exemplify the model and the axioms for a bilateral instance of the environment.",
                "The results we presented are a first step towards an expanded model that will cover all types of adversarial environments, for example, environments that are non-zero-sum, and environments that contain natural agents that are not part of the direct conflict.",
                "Those challenges and more will be dealt with in future research. 6.",
                "ACKNOWLEDGMENT This research was supported in part by Israel Science Foundation grants #1211/04 and #898/05. 7.",
                "REFERENCES [1] L. V. Allis.",
                "A knowledge-based approach of Connect-Four - the game is solved: White wins.",
                "Masters thesis, Free University, Amsterdam, The Netherlands, 1988. [2] D. Carmel and S. Markovitch.",
                "Incorporating opponent models into adversary search.",
                "In Proceedings of the Thirteenth National Conference on Artificial Intelligence, pages 120-125, Portland, OR, 1996. [3] D. Carmel and S. Markovitch.",
                "Opponent modeling in multi-agent systems.",
                "In G. Weiß and S. Sen, editors, Adaptation and Learning in Multi-Agent Systems, pages 40-52.",
                "Springer-Verlag, 1996. [4] B. J. Grosz and S. Kraus.",
                "Collaborative plans for complex group action.",
                "Artificial Intelligence, 86(2):269-357, 1996. [5] M. Hadad, G. Kaminka, G. Armon, and S. Kraus.",
                "Supporting collaborative activity.",
                "In Proc. of AAAI-2005, pages 83-88, Pittsburgh, 2005. [6] http://www.gamerz.net/˜pbmserv/. [7] S. Kraus and D. Lehmann.",
                "Designing and building a negotiating automated agent.",
                "Computational Intelligence, 11:132-171, 1995. [8] H. J. Levesque, P. R. Cohen, and J. H. T. Nunes.",
                "On acting together.",
                "In Proc. of AAAI-90, pages 94-99, Boston, MA, 1990. [9] S. Markovitch and R. Reger.",
                "Learning and exploiting relative weaknesses of opponent agents.",
                "Autonomous Agents and Multi-Agent Systems, 10(2):103-130, 2005. [10] Y. M. Ronald Fagin, Joseph Y. Halpern and M. Y. Vardi.",
                "Reasoning about knowledge.",
                "MIT Press, Cambridge, Mass., 1995. [11] P. Thagard.",
                "Adversarial problem solving: Modeling an oponent using explanatory coherence.",
                "Cognitive Science, 16(1):123-149, 1992. [12] R. W. Toseland and R. F. Rivas.",
                "An Introduction to Group Work Practice.",
                "Prentice Hall, Englewood Cliffs, NJ, 2nd edition edition, 1995. [13] S. Willmott, J. Richardson, A. Bundy, and J. Levine.",
                "An adversarial planning approach to Go.",
                "Lecture Notes in Computer Science, 1558:93-112, 1999.",
                "The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) 557"
            ],
            "original_annotated_samples": [],
            "translated_annotated_samples": [
                "Axioma de formación de la alianza El siguiente \"axioma de comportamiento\" es relevante solo en una instanciación multilateral del entorno adversario (obviamente, una alianza no puede formarse en un encuentro bilateral de suma cero).axioma conductual",
                "Ahora procedemos a analizar los juegos con respecto a cada \"axioma de comportamiento\".axioma conductual"
            ],
            "translated_text": "",
            "candidates": [],
            "error": []
        },
        "bilateral and multilateral instantiation": {
            "translated_key": "",
            "is_in_text": true,
            "original_annotated_sentences": [
                "An Adversarial Environment Model for Bounded Rational Agents in Zero-Sum Interactions Inon Zuckerman1 , Sarit Kraus1 , Jeffrey S. Rosenschein2 , Gal Kaminka1 1 Department of Computer Science 2 The School of Engineering Bar-Ilan University and Computer Science Ramat-Gan, Israel Hebrew University, Jerusalem, Israel {zukermi,sarit,galk}@cs.biu.ac.il jeff@cs.huji.ac.il ABSTRACT Multiagent environments are often not cooperative nor collaborative; in many cases, agents have conflicting interests, leading to adversarial interactions.",
                "This paper presents a formal Adversarial Environment model for bounded rational agents operating in a zero-sum environment.",
                "In such environments, attempts to use classical utility-based search methods can raise a variety of difficulties (e.g., implicitly modeling the opponent as an omniscient utility maximizer, rather than leveraging a more nuanced, explicit opponent model).",
                "We define an Adversarial Environment by describing the mental states of an agent in such an environment.",
                "We then present behavioral axioms that are intended to serve as design principles for building such adversarial agents.",
                "We explore the application of our approach by analyzing log files of completed Connect-Four games, and present an empirical analysis of the axioms appropriateness.",
                "Categories and Subject Descriptors I.2.11 [Artificial Intelligence]: Distributed Artificial Intelligence-Intelligent agents,Multiagent Systems; I.2.4 [Artificial Intelligence]: Knowledge Representation Formalisms and Methods -Modal logic General Terms Design, Theory 1.",
                "INTRODUCTION Early research in multiagent systems (MAS) considered cooperative groups of agents; because individual agents had limited resources, or limited access to information (e.g., limited processing power, limited sensor coverage), they worked together by design to solve problems that individually they could not solve, or at least could not solve as efficiently.",
                "MAS research, however, soon began to consider interacting agents with individuated interests, as representatives of different humans or organizations with non-identical interests.",
                "When interactions are guided by diverse interests, participants may have to overcome disagreements, uncooperative interactions, and even intentional attempts to damage one another.",
                "When these types of interactions occur, environments require appropriate behavior from the agents situated in them.",
                "We call these environments Adversarial Environments, and call the clashing agents Adversaries.",
                "Models of cooperation and teamwork have been extensively explored in MAS through the axiomatization of mental states (e.g., [8, 4, 5]).",
                "However, none of this research dealt with adversarial domains and their implications for agent behavior.",
                "Our paper addresses this issue by providing a formal, axiomatized mental state model for a subset of adversarial domains, namely simple zero-sum adversarial environments.",
                "Simple zero-sum encounters exist of course in various twoplayer games (e.g., Chess, Checkers), but they also exist in n-player games (e.g., Risk, Diplomacy), auctions for a single good, and elsewhere.",
                "In these latter environments especially, using a utility-based adversarial search (such as the Min-Max algorithm) does not always provide an adequate solution; the payoff function might be quite complex or difficult to quantify, and there are natural computational limitations on bounded rational agents.",
                "In addition, traditional search methods (like Min-Max) do not make use of a model of the opponent, which has proven to be a valuable addition to adversarial planning [9, 3, 11].",
                "In this paper, we develop a formal, axiomatized model for bounded rational agents that are situated in a zero-sum adversarial environment.",
                "The model uses different modality operators, and its main foundations are the SharedPlans [4] model for collaborative behavior.",
                "We explore environment properties and the mental states of agents to derive behavioral axioms; these behavioral axioms constitute a formal model that serves as a specification and design guideline for agent design in such settings.",
                "We then investigate the behavior of our model empirically using the Connect-Four board game.",
                "We show that this game conforms to our environment definition, and analyze players behavior using a large set of completed match log 550 978-81-904262-7-5 (RPS) c 2007 IFAAMAS files.",
                "In addition, we use the results presented in [9] to discuss the importance of opponent modeling in our ConnectFour adversarial domain.",
                "The paper proceeds as follows.",
                "Section 2 presents the models formalization.",
                "Section 3 presents the empirical analysis and its results.",
                "We discuss related work in Section 4, and conclude and present future directions in Section 5. 2.",
                "ADVERSARIAL ENVIRONMENTS The adversarial environment model (denoted as AE) is intended to guide the design of agents by providing a specification of the capabilities and mental attitudes of an agent in an adversarial environment.",
                "We focus here on specific types of adversarial environments, specified as follows: 1.",
                "Zero-Sum Interactions: positive and negative utilities of all agents sum to zero; 2.",
                "Simple AEs: all agents in the environment are adversarial agents; 3.",
                "Bilateral AEs: AEs with exactly two agents; 4.",
                "Multilateral AEs: AEs of three or more agents.",
                "We will work on both <br>bilateral and multilateral instantiation</br>s of zero-sum and simple environments.",
                "In particular, our adversarial environment model will deal with interactions that consist of N agents (N ≥ 2), where all agents are adversaries, and only one agent can succeed.",
                "Examples of such environments range from board games (e.g., Chess, Connect-Four, and Diplomacy) to certain economic environments (e.g., N-bidder auctions over a single good). 2.1 Model Overview Our approach is to formalize the mental attitudes and behaviors of a single adversarial agent; we consider how a single agent perceives the AE.",
                "The following list specifies the conditions and mental states of an agent in a simple, zero-sum AE: 1.",
                "The agent has an individual intention that its own goal will be completed; 2.",
                "The agent has an individual belief that it and its adversaries are pursuing full conflicting goals (defined below)there can be only one winner; 3.",
                "The agent has an individual belief that each adversary has an intention to complete its own full conflicting goal; 4.",
                "The agent has an individual belief in the (partial) profile of its adversaries.",
                "Item 3 is required, since it might be the case that some agent has a full conflicting goal, and is currently considering adopting the intention to complete it, but is, as of yet, not committed to achieving it.",
                "This might occur because the agent has not yet deliberated about the effects that adopting that intention might have on the other intentions it is currently holding.",
                "In such cases, it might not consider itself to even be in an adversarial environment.",
                "Item 4 states that the agent should hold some belief about the profiles of its adversaries.",
                "The profile represents all the knowledge the agent has about its adversary: its weaknesses, strategic capabilities, goals, intentions, trustworthiness, and more.",
                "It can be given explicitly or can be learned from observations of past encounters. 2.2 Model Definitions for Mental States We use Grosz and Krauss definitions of the modal operators, predicates, and meta-predicates, as defined in their SharedPlan formalization [4].",
                "We recall here some of the predicates and operators that are used in that formalization: Int.To(Ai, α, Tn, Tα, C) represents Ais intentions at time Tn to do an action α at time Tα in the context of C. Int.Th(Ai, prop, Tn, Tprop, C) represents Ais intentions at time Tn that a certain proposition prop holds at time Tprop in the context of C. The potential intention operators, Pot.Int.To(...) and Pot.Int.Th(...), are used to represent the mental state when an agent considers adopting an intention, but has not deliberated about the interaction of the other intentions it holds.",
                "The operator Bel(Ai, f, Tf ) represents agent Ai believing in the statement expressed in formula f, at time Tf .",
                "MB(A, f, Tf ) represents mutual belief for a group of agents A.",
                "A snapshot of the system finds our environment to be in some state e ∈ E of environmental variable states, and each adversary in any LAi ∈ L of possible local states.",
                "At any given time step, the system will be in some world w of the set of all possible worlds w ∈ W, where w = E×LA1 ×LA2 × ...LAn , and n is the number of adversaries.",
                "For example, in a Texas Holdem poker game, an agents local state might be its own set of cards (which is unknown to its adversary) while the environment will consist of the betting pot and the community cards (which are visible to both players).",
                "A utility function under this formalization is defined as a mapping from a possible world w ∈ W to an element in , which expresses the desirability of the world, from a single agent perspective.",
                "We usually normalize the range to [0,1], where 0 represents the least desirable possible world, and 1 is the most desirable world.",
                "The implementation of the utility function is dependent on the domain in question.",
                "The following list specifies new predicates, functions, variables, and constants used in conjunction with the original definitions for the adversarial environment formalization: 1. φ is a null action (the agent does not do anything). 2.",
                "GAi is the set of agent Ais goals.",
                "Each goal is a set of predicates whose satisfaction makes the goal complete (we use G∗ Ai ∈ GAi to represent an arbitrary goal of agent Ai). 3. gAi is the set of agent Ais subgoals.",
                "Subgoals are predicates whose satisfaction represents an important milestone toward achievement of the full goal. gG∗ Ai ⊆ gAi is the set of subgoals that are important to the completion of goal G∗ Ai (we will use g∗ G∗ Ai ∈ gG∗ Ai to represent an arbitrary subgoal). 4.",
                "P Aj Ai is the profile object agent Ai holds about agent Aj. 5.",
                "CA is a general set of actions for all agents in A which are derived from the environments constraints.",
                "CAi ⊆ CA is the set of agent Ais possible actions. 6.",
                "Do(Ai, α, Tα, w) holds when Ai performs action α over time interval Tα in world w. 7.",
                "Achieve(G∗ Ai , α, w) is true when goal G∗ Ai is achieved following the completion of action α in world w ∈ W, where α ∈ CAi . 8.",
                "Profile(Ai, PAi Ai ) is true when agent Ai holds an object profile for agent Aj.",
                "Definition 1.",
                "Full conflict (FulConf ) describes a zerosum interaction where only a single goal of the goals in conflict can be completed.",
                "FulConf(G∗ Ai , G∗ Aj ) ⇒ (∃α ∈ CAi , ∀w, β ∈ CAj ) (Achieve(G∗ Ai , α, w) ⇒ ¬Achieve(G∗ Aj , β, w)) ∨ (∃β ∈ CAj , ∀w, α ∈ CAi )(Achieve(G∗ Aj , β, w) ⇒ ¬Achieve(G∗ Ai , α, w)) Definition 2.",
                "Adversarial Knowledge (AdvKnow) is a function returning a value which represents the amount of The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) 551 knowledge agent Ai has on the profile of agent Aj, at time Tn.",
                "The higher the value, the more knowledge agent Ai has.",
                "AdvKnow : P Aj Ai × Tn → Definition 3.",
                "Eval - This evaluation function returns an estimated expected utility value for an agent in A, after completing an action from CA in some world state w. Eval : A × CA × w → Definition 4.",
                "TrH - (Threshold) is a numerical constant in the [0,1] range that represents an evaluation function (Eval) threshold value.",
                "An action that yields an estimated utility evaluation above the TrH is regarded as a highly beneficial action.",
                "The Eval value is an estimation and not the real utility function, which is usually unknown.",
                "Using the real utility value for a rational agent would easily yield the best outcome for that agent.",
                "However, agents usually do not have the real utility functions, but rather a heuristic estimate of it.",
                "There are two important properties that should hold for the evaluation function: Property 1.",
                "The evaluation function should state that the most desirable world state is one in which the goal is achieved.",
                "Therefore, after the goal has been satisfied, there can be no future action that can put the agent in a world state with higher Eval value. (∀Ai, G∗ Ai , α, β ∈ CAi , w ∈ W) Achieve(G∗ Ai , α, w) ⇒ Eval(Ai, α, w) ≥ Eval(Ai, β, w) Property 2.",
                "The evaluation function should project an action that causes a completion of a goal or a subgoal to a value which is greater than TrH (a highly beneficial action). (∀Ai, G∗ Ai ∈ GAi , α ∈ CAi , w ∈ W, g∗ GAi ∈ gGAi ) Achieve(G∗ Ai , α, w) ∨ Achieve(g∗ GAi , α, w) ⇒ Eval(Ai, α, w) ≥ TrH.",
                "Definition 5.",
                "SetAction We define a set action (SetAction) as a set of action operations (either complex or basic actions) from some action sets CAi and CAj which, according to agent Ais belief, are attached together by a temporal and consequential relationship, forming a chain of events (action, and its following consequent action). (∀α1 , . . . , αu ∈ CAi , β1 , . . . , βv ∈ CAj , w ∈ W) SetAction(α1 , . . . , αu , β1 , . . . , βv , w) ⇒ ((Do(Ai, α1 , Tα1 , w) ⇒ Do(Aj, β1 , Tβ1 , w)) ⇒ Do(Ai, α2 , Tα2 , w) ⇒ . . . ⇒ Do(Ai, αu , Tαu , w)) The consequential relation might exist due to various environmental constraints (when one action forces the adversary to respond with a specific action) or due to the agents knowledge about the profile of its adversary.",
                "Property 3.",
                "As the knowledge we have about our adversary increases we will have additional beliefs about its behavior in different situations which in turn creates new set actions.",
                "Formally, if our AdvKnow at time Tn+1 is greater than AdvKnow at time Tn, then every SetAction known at time Tn is also known at time Tn+1.",
                "AdvKnow(P Aj Ai , Tn+1) > AdvKnow(P Aj Ai , Tn) ⇒ (∀α1 , . . . , αu ∈ CAi , β1 , . . . , βv ∈ CAj ) Bel(Aag, SetAction(α1 , . . . , αu , β1 , . . . , βv ), Tn) ⇒ Bel(Aag, SetAction(α1 , . . . , αu , β1 , . . . , βv ), Tn+1) 2.3 The Environment Formulation The following axioms provide the formal definition for a simple, zero-sum Adversarial Environment (AE).",
                "Satisfaction of these axioms means that the agent is situated in such an environment.",
                "It provides specifications for agent Aag to interact with its set of adversaries A with respect to goals G∗ Aag and G∗ A at time TCo at some world state w. AE(Aag, A, G∗ Aag , A1, . . . , Ak, G∗ A1 , . . . , G∗ Ak , Tn, w) 1.",
                "Aag has an Int.Th his goal would be completed: (∃α ∈ CAag , Tα) Int.Th(Aag, Achieve(G∗ Aag , α), Tn, Tα, AE) 2.",
                "Aag believes that it and each of its adversaries Ao are pursuing full conflicting goals: (∀Ao ∈ {A1, . . . , Ak}) Bel(Aag, FulConf(G∗ Aag , G∗ Ao ), Tn) 3.",
                "Aag believes that each of his adversaries in Ao has the Int.Th his conflicting goal G∗ Aoi will be completed: (∀Ao ∈ {A1, . . . , Ak})(∃β ∈ CAo , Tβ) Bel(Aag, Int.Th(Ao, Achieve(G∗ Ao , β), TCo, Tβ, AE), Tn) 4.",
                "Aag has beliefs about the (partial) profiles of its adversaries (∀Ao ∈ {A1, . . . , Ak}) (∃PAo Aag ∈ PAag )Bel(Aag, Profile(Ao, PAo Aag ), Tn) To build an agent that will be able to operate successfully within such an AE, we must specify behavioral guidelines for its interactions.",
                "Using a naive Eval maximization strategy to a certain search depth will not always yield satisfactory results for several reasons: (1) the search horizon problem when searching for a fixed depth; (2) the strong assumption of an optimally rational, unbounded resources adversary; (3) using an estimated evaluation function which will not give optimal results in all world states, and can be exploited [9].",
                "The following axioms specify the behavioral principles that can be used to differentiate between successful and less successful agents in the above Adversarial Environment.",
                "Those axioms should be used as specification principles when designing and implementing agents that should be able to perform well in such Adversarial Environments.",
                "The behavioral axioms represent situations in which the agent will adopt potential intentions to (Pot.Int.To(...)) perform an action, which will typically require some means-end reasoning to select a possible course of action.",
                "This reasoning will lead to the adoption of an Int.To(...) (see [4]).",
                "A1.",
                "Goal Achieving Axiom.",
                "The first axiom is the simplest case; when the agent Aag believes that it is one action (α) away from achieving his conflicting goal G∗ Aag , it should adopt the potential intention to do α and complete its goal. (∀Aag, α ∈ CAag , Tn, Tα, w ∈ W) (Bel(Aag, Do(Aag, α, Tα, w) ⇒ Achieve(G∗ Aag , α, w)) ⇒ Pot.Int.To(Aag, α, Tn, Tα, w) This somewhat trivial behavior is the first and strongest axiom.",
                "In any situation, when the agent is an action away from completing the goal, it should complete the action.",
                "Any fair Eval function would naturally classify α as the maximal value action (property 1).",
                "However, without explicit axiomatization of such behavior there might be situations where the agent will decide on taking another action for various reasons, due to its bounded decision resources.",
                "A2.",
                "Preventive Act Axiom.",
                "Being in an adversarial situation, agent Aag might decide to take actions that will damage one of its adversarys plans to complete its goal, even if those actions do not explicitly advance Aag towards its conflicting goal G∗ Aag .",
                "Such preventive action will take place when agent Aag has a belief about the possibility of its adversary Ao doing an action β that will give it a high 552 The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) utility evaluation value (> TrH).",
                "Believing that taking action α will prevent the opponent from doing its β, it will adopt a potential intention to do α. (∀Aag, Ao ∈ A, α ∈ CAag , β ∈ CAo , Tn, Tβ, w ∈ W) (Bel(Aag, Do(Ao, β, Tβ, w) ∧ Eval(Ao, β, w) > TrH, Tn) ∧ Bel(Aag, Do(Aag, α, Tα, w) ⇒ ¬Do(Ao, β, Tβ, w), Tn) ⇒ Pot.Int.To(Aag, α, Tn, Tα, w) This axiom is a basic component of any adversarial environment.",
                "For example, looking at a Chess board game, a player could realize that it is about to be checkmated by its opponent, thus making a preventive move.",
                "Another example is a Connect Four game: when a player has a row of three chips, its opponent must block it, or lose.",
                "A specific instance of A1 occurs when the adversary is one action away from achieving its goal, and immediate preventive action needs to be taken by the agent.",
                "Formally, we have the same beliefs as stated above, with a changed belief that doing action β will cause agent Ao to achieve its goal.",
                "Proposition 1: Prevent or lose case. (∀Aag, Ao ∈ A, α ∈ CAag , β ∈ CAo , G∗ Ao , Tn, Tα, Tβ, w ∈ W) Bel(Aag, Do(Ao, β, Tβ, w) ⇒ Achieve(G∗ Ao , β, w), Tn) ∧ Bel(Aag, Do(Aag, α, Tα, w) ⇒ ¬Do(Ao, β, Tβ, w)) ⇒ Pot.Int.To(Aag, α, Tn, Tα, w) Sketch of proof: Proposition 1 can be easily derived from axiom A1 and the property 2 of the Eval function, which states that any action that causes a completion of a goal is a highly beneficial action.",
                "The preventive act behavior will occur implicitly when the Eval function is equal to the real world utility function.",
                "However, being bounded rational agents and dealing with an estimated evaluation function we need to explicitly axiomatize such behavior, for it will not always occur implicitly from the evaluation function.",
                "A3.",
                "Suboptimal Tactical Move Axiom.",
                "In many scenarios a situation may occur where an agent will decide not to take the current most beneficial action it can take (the action with the maximal utility evaluation value), because it believes that taking another action (with lower utility evaluation value) might yield (depending on the adversarys response) a future possibility for a highly beneficial action.",
                "This will occur most often when the Eval function is inaccurate and differs by a large extent from the Utility function.",
                "Put formally, agent Aag believes in a certain SetAction that will evolve according to its initial action and will yield a high beneficial value (> TrH) solely for it. (∀Aag, Ao ∈ A, Tn, w ∈ W) (∃α1 , . . . , αu ∈ CAi , β1 , . . . , βv ∈ CAj , Tα1 ) Bel(Aag, SetAction(α1 , . . . , αu , β1 , . . . , βv ), Tn) ∧ Bel(Aag, Eval(Ao, βv , w) < TrH < Eval(Aag, αu , w), Tn) ⇒ Pot.Int.To(Aag, α1 , Tn, Tα1 , w) An agent might believe that a chain of events will occur for various reasons due to the inevitable nature of the domain.",
                "For example, in Chess, we often observe the following: a move causes a check position, which in turn limits the opponents moves to avoiding the check, to which the first player might react with another check, and so on.",
                "The agent might also believe in a chain of events based on its knowledge of its adversarys profile, which allows it to foresee the adversarys movements with high accuracy.",
                "A4.",
                "Profile Detection Axiom.",
                "The agent can adjust its adversarys profiles by observations and pattern study (specifically, if there are repeated encounters with the same adversary).",
                "However, instead of waiting for profile information to be revealed, an agent can also initiate actions that will force its adversary to react in a way that will reveal profile knowledge about it.",
                "Formally, the axiom states that if all actions (γ) are not highly beneficial actions (< TrH), the agent can do action α in time Tα if it believes that it will result in a non-highly beneficial action β from its adversary, which in turn teaches it about the adversarys profile, i.e., gives a higher AdvKnow(P Aj Ai , Tβ). (∀Aag, Ao ∈ A, α ∈ CAag , β ∈ CAo , Tn, Tα, Tβ, w ∈ W) Bel(Aag, (∀γ ∈ CAag )Eval(Aag, γ, w) < TrH, Tn) ∧ Bel(Aag, Do(Aag, α, Tα, w) ⇒ Do(Ao, β, Tβ, w), Tn) ∧ Bel(Aag, Eval(Ao, β, w) < TrH) ∧ Bel(Aag, AdvKnow(P Aj Ai , Tβ) > AdvKnow(P Aj Ai , Tn), Tn) ⇒ Pot.Int.To(Aag, α, Tn, Tα, w) For example, going back to the Chess board game scenario, consider starting a game versus an opponent about whom we know nothing, not even if it is a human or a computerized opponent.",
                "We might start playing a strategy that will be suitable versus an average opponent, and adjust our game according to its level of play.",
                "A5.",
                "Alliance Formation Axiom The following behavioral axiom is relevant only in a multilateral instantiation of the adversarial environment (obviously, an alliance cannot be formed in a bilateral, zero-sum encounter).",
                "In different situations during a multilateral interaction, a group of agents might believe that it is in their best interests to form a temporary alliance.",
                "Such an alliance is an agreement that constrains its members behavior, but is believed by its members to enable them to achieve a higher utility value than the one achievable outside of the alliance.",
                "As an example, we can look at the classical Risk board game, where each player has an individual goal of being the sole conquerer of the world, a zero-sum game.",
                "However, in order to achieve this goal, it might be strategically wise to make short-term ceasefire agreements with other players, or to join forces and attack an opponent who is stronger than the rest.",
                "An alliances terms defines the way its members should act.",
                "It is a set of predicates, denoted as Terms, that is agreed upon by the alliance members, and should remain true for the duration of the alliance.",
                "For example, the set Terms in the Risk scenario, could contain the following predicates: 1.",
                "Alliance members will not attack each other on territories X, Y and Z; 2.",
                "Alliance members will contribute C units per turn for attacking adversary Ao; 3.",
                "Members are obligated to stay as part of the alliance until time Tk or until adversarys Ao army is smaller than Q.",
                "The set Terms specifies inter-group constraints on each of the alliance members (∀Aal i ∈ Aal ⊆ A) set of actions Cal i ⊆ C. Definition 6.",
                "Al val - the total evaluation value that agent Ai will achieve while being part of Aal is the sum of Evali (Eval for Ai) of each of Aal j Eval values after taking their own α actions (via the agent(α) predicate): Al val(Ai, Cal , Aal , w) = α∈Cal Evali(Aal j , agent(α), w) Definition 7.",
                "Al TrH - is a number representing an Al val The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) 553 threshold; above it, the alliance can be said to be a highly beneficial alliance.",
                "The value of Al TrH will be calculated dynamically according to the progress of the interaction, as can be seen in [7].",
                "After an alliance is formed, its members are now working in their normal adversarial environment, as well as according to the mental states and axioms required for their interactions as part of the alliance.",
                "The following Alliance model (AL) specifies the conditions under which the group Aal can be said to be in an alliance and working with a new and constrained set of actions Cal , at time Tn.",
                "AL(Aal , Cal , w, Tn) 1.",
                "Aal has a MB that all members are part of Aal : MB(Aal , (∀Aal i ∈ Aal )member(Aal i , Aal ), Tn) 2.",
                "Aal has a MB that the group be maintained: MB(Aal , (∀Aal i ∈ Aal )Int.Th (Ai, member(Ai, Aal ), Tn, Tn+1, Co), Tn) 3.",
                "Aal has a MB that being members gives them high utility value: MB(Aal , (∀Aal i ∈ Aal )Al val(Aal i , Cal , Aal , w) ≥ Al TrH, Tn) Members profiles are a crucial part of successful alliances.",
                "We assume that agents that have more accurate profiles of their adversaries will be more successful in such environments.",
                "Such agents will be able to predict when a member is about to breach the alliances contract (item 2 in the above model), and take counter measures (when item 3 will falsify).",
                "The robustness of the alliance is in part a function of its members trustfulness measure, objective position estimation, and other profile properties.",
                "We should note that an agent can simultaneously be part of more than one alliance.",
                "Such a temporary alliance, where the group members do not have a joint goal but act collaboratively for the interest of their own individual goals, is classified as a Treatment Group by modern psychologists [12] (in contrast to a Task Group, where its members have a joint goal).",
                "The Shared Activity model as presented in [5] modeled Treatment Group behavior using the same SharedPlans formalization.",
                "When comparing both definitions of an alliance and a Treatment Group we found an unsurprising resemblance between both models: the environment models definitions are almost identical (see SAs definitions in [5]), and their Selfish-Act and Cooperative Act axioms conform to our adversarial agents behavior.",
                "The main distinction between both models is the integration of a Helpful-behavior act axiom, in the Shared Activity which cannot be part of ours.",
                "This axiom states that an agent will consider taking action that will lower its Eval value (to a certain lower bound), if it believes that a group partner will gain a significant benefit.",
                "Such behavior cannot occur in a pure adversarial environment (as a zero-sum game is), where the alliance members are constantly on watch to manipulate their alliance to their own advantage.",
                "A6.",
                "Evaluation Maximization Axiom.",
                "In a case when all other axioms are inapplicable, we will proceed with the action that maximizes the heuristic value as computed in the Eval function. (∀Aag, Ao ∈ A, α ∈ Cag, Tn, w ∈ W) Bel(Aag, (∀γ ∈ Cag)Eval(Aag, α, w) ≥ Eval(Aag, γ, w), Tn) ⇒ Pot.Int.To(Aag, α, Tn, Tα, w) T1.",
                "Optimality on Eval = Utility The above axiomatic model handles situations where the Utility is unknown and the agents are bounded rational agents.",
                "The following theorem shows that in bilateral interactions, where the agents have the real Utility function (i.e., Eval = Utility) and are rational agents, the axioms provide the same optimal result as classic adversarial search (e.g., Min-Max).",
                "Theorem 1.",
                "Let Ae ag be an unbounded rational AE agent using the Eval heuristic evaluation function, Au ag be the same agent using the true Utility function, and Ao be a sole unbounded utility-based rational adversary.",
                "Given that Eval = Utility: (∀α ∈ CAu ag , α ∈ CAe ag , Tn, w ∈ W) Pot.Int.To(Au ag, α, Tn, Tα, w) → Pot.Int.To(Ae ag, α , Tn, Tα, w) ∧ ((α = α ) ∨ (Utility(Au ag, α, w) = Eval(Ae ag, α , w))) Sketch of proof - Given that Au ag has the real utility function and unbounded resources, it can generate the full game tree and run the optimal MinMax algorithm to choose the highest utility value action, which we denote by, α.",
                "The proof will show that Ae ag, using the AE axioms, will select the same or equal utility α (when there is more than one action with the same max utility) when Eval = Utility. (A1) Goal achieving axiom - suppose there is an α such that its completion will achieve Au ags goal.",
                "It will obtain the highest utility by Min-Max for Au ag.",
                "The Ae ag agent will select α or another action with the same utility value via A1.",
                "If such α does not exist, Ae ag cannot apply this axiom, and proceeds to A2. (A2) Preventive act axiom - (1) Looking at the basic case (see Prop1), if there is a β which leads Ao to achieve its goal, then a preventive action α will yield the highest utility for Au ag.",
                "Au ag will choose it through the utility, while Ae ag will choose it through A2. (2) In the general case, β is a highly beneficial action for Ao, thus yields low utility for Au ag, which will guide it to select an α that will prevent β, while Ae ag will choose it through A2.1 If such β does not exist for Ao, then A2 is not applicable, and Ae ag can proceed to A3. (A3) Suboptimal tactical move axiom - When using a heuristic Eval function, Ae ag has a partial belief in the profile of its adversary (item 4 in AE model), which may lead it to believe in SetActions (Prop1).",
                "In our case, Ae ag is holding a full profile on its optimal adversary and knows that Ao will behave optimally according to the real utility values on the complete search tree, therefore, any belief about suboptimal SetAction cannot exist, yielding this axiom inapplicable.",
                "Ae ag will proceed to A4. (A4) Profile detection axiom - Given that Ae ag has the full profile of Ao, none of Ae ags actions can increase its knowledge.",
                "That axiom will not be applied, and the agent will proceed with A6 (A5 will be disregarded because the interaction is bilateral). (A6) Evaluation maximization axiom - This axiom will select the max Eval for Ae ag.",
                "Given that Eval = Utility, the same α that was selected by Au ag will be selected. 3.",
                "EVALUATION The main purpose of our experimental analysis is to evaluate the models behavior and performance in a real adversarial environment.",
                "This section investigates whether bounded 1 A case where following the completion of β there exists a γ which gives high utility for Agent Au ag, cannot occur because Ao uses the same utility, and γs existence will cause it to classify β as a low utility action. 554 The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) rational agents situated in such adversarial environments will be better off applying our suggested behavioral axioms. 3.1 The Domain To explore the use of the above model and its behavioral axioms, we decided to use the Connect-Four game as our adversarial environment.",
                "Connect-Four is a 2-player, zerosum game which is played using a 6x7 matrix-like board.",
                "Each turn, a player drops a disc into one of the 7 columns (the set of 21 discs is usually colored yellow for player 1 and red for player 2; we will use White and Black respectively to avoid confusion).",
                "The winner is the first player to complete a horizontal, vertical, or diagonal set of four discs with its color.",
                "On very rare occasions, the game might end in a tie if all the empty grids are filled, but no player managed to create a 4-disc set.",
                "The Connect-Four game was solved in [1], where it is shown that the first player (playing with the white discs) can force a win by starting in the middle column (column 4) and playing optimally However, the optimal strategy is very complex, and difficult to follow even for complex bounded rational agents, such as human players.",
                "Before we can proceed checking agent behavior, we must first verify that the domain conforms to the adversarial environments definition as given above (which the behavioral axioms are based on).",
                "First, when playing a Connect-Four game, the agent has an intention to win the game (item 1).",
                "Second (item 2), our agent believes that in Connect-Four there can only be one winner (or no winner at all in the rare occurrence of a tie).",
                "In addition, our agent believes that its opponent to the game will try to win (item 3), and we hope it has some partial knowledge (item 4) about its adversary (this knowledge can vary from nothing, through simple facts such as age, to strategies and weaknesses).",
                "Of course, not all Connect-Four encounters are adversarial.",
                "For example, when a parent is playing the game with its child, the following situation might occur: the child, having a strong incentive to win, treats the environment as adversarial (it intends to win, understands that there can only be one winner, and believes that its parent is trying to beat him).",
                "However, the parents point of view might see the environment as an educational one, where its goal is not to win the game, but to cause enjoyment or practice strategic reasoning.",
                "In such an educational environment, a new set of behavioral axioms might be more beneficial to the parents goals than our suggested adversarial behavioral axioms. 3.2 Axiom Analysis After showing that the Connect-Four game is indeed a zero-sum, bilateral adversarial environment, the next step is to look at players behaviors during the game and check whether behaving according to our model does improve performance.",
                "To do so we have collected log files from completed Connect-Four games that were played by human players over the Internet.",
                "Our collected log file data came from Play by eMail (PBeM) sites.",
                "These are web sites that host email games, where each move is taken by an email exchange between the server and the players.",
                "Many such sites archives contain real competitive interactions, and also maintain a ranking system for their members.",
                "Most of the data we used can be found in [6].",
                "As can be learned from [1], Connect-Four has an optimal strategy and a considerable advantage for the player who starts the game (which we call the White player).",
                "We will concentrate in our analysis on the second players moves (to be called Black).",
                "The White player, being the first to act, has the so-called initiative advantage.",
                "Having the advantage and a good strategy will keep the Black player busy reacting to its moves, instead of initiating threats.",
                "A threat is a combination of three discs of the same color, with an empty spot for the fourth winning disk.",
                "An open threat is a threat that can be realized in the opponents next move.",
                "In order for the Black player to win, it must somehow turn the tide, take the advantage and start presenting threats to the White player.",
                "We will explore Black players behavior and their conformance to our axioms.",
                "To do so, we built an application that reads log files and analyzes the Black players moves.",
                "The application contains two main components: (1) a Min-Max algorithm for evaluation of moves; (2) open threats detector for the discovering of open threats.",
                "The Min-Max algorithm will work to a given depth, d and for each move α will output the heuristic value for the next action taken by the player as written in the log file, h(α), alongside the maximum heuristic value, maxh(α), that could be achieved prior to taking the move (obviously, if h(α) = maxh(α), then the player did not do the optimal move heuristically).",
                "The threat detectors job is to notify if some action was taken in order to block an open threat (not blocking an open threat will probably cause the player to lose in the opponents next move).",
                "The heuristic function used by Min-Max to evaluate the players utility is the following function, which is simple to compute, yet provides a reasonable challenge to human opponents: Definition 8.",
                "Let Group be an adjacent set of four squares that are horizontal, vertical, or diagonal.",
                "Groupn b (Groupn w) be a Group with n pieces of the black (white) color and 4−n empty squares. h = ((Group1 b ∗α)+(Group2 b ∗β)+(Group3 b ∗γ)+(Group4 b ∗∞)) − ((Group1 w ∗α)+(Group2 w ∗β)+(Group3 w ∗γ)+(Group4 w ∗∞)) The values of α, β and δ can vary to form any desired linear combination; however, it is important to value them with the α < β < δ ordering in mind (we used 1, 4, and 8 as their respective values).",
                "Groups of 4 discs of the same color means victory, thus discovery of such a group will result in ∞ to ensure an extreme value.",
                "We now use our estimated evaluation function to evaluate the Black players actions during the Connect-Four adversarial interaction.",
                "Each game from the log file was input into the application, which processed and output a reformatted log file containing the h value of the current move, the maxh value that could be achieved, and a notification if an open threat was detected.",
                "A total of 123 games were analyzed (57 with White winning, and 66 with Black winning).",
                "A few additional games were manually ignored in the experiment, due to these problems: a player abandoning the game while the outcome is not final, or a blunt irrational move in the early stages of the game (e.g., not blocking an obvious winning group in the first opening moves).",
                "In addition, a single tie game was also removed.",
                "The simulator was run to a search depth of 3 moves.",
                "We now proceed to analyze the games with respect to each behavioral axiom.",
                "The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) 555 Table 1: Average heuristic difference analysis Black losses Black Won Avg minh -17.62 -12.02 Avg 3 lowest h moves (min3 h) -13.20 -8.70 3.2.1 Affirming the Suboptimal tactical move axiom The following section presents the heuristic evaluations of the Min-Max algorithm for each action, and checks the amount and extent of suboptimal tactical actions and their implications on performance.",
                "Table 1 shows results and insights from the games heuristic analysis, when search depth equals 3 (this search depth was selected for the results to be comparable to [9], see Section 3.2.3).",
                "The tables heuristic data is the difference between the present maximal heuristic value and the heuristic value of the action that was eventually taken by the player (i.e., the closer the number is to 0, the closer the action was to the maximum heuristic action).",
                "The first row presents the difference values of the action that had the maximal difference value among all the Black players actions in a given game, as averaged over all Blacks winning and losing games (see respective columns).",
                "In games in which the Black player loses, its average difference value was -17.62, while in games in which the Black player won, its average was -12.02.",
                "The second row expands the analysis by considering the 3 highest heuristic difference actions, and averaging them.",
                "In that case, we notice an average heuristic difference of 5 points between games which the Black player loses and games in which it wins.",
                "Nevertheless, the importance of those numbers is that they allowed us to take an educated guess on a threshold number of 11.5, as the value of the TrH constant, which differentiates between normal actions and highly beneficial ones.",
                "After finding an approximated TrH constant, we can proceed with an analysis of the importance of suboptimal moves.",
                "To do so we took the subset of games in which the minimum heuristic difference value for Blacks actions was 11.5.",
                "As presented in Table 2, we can see the different min3 h average of the 3 largest ranges and the respective percentage of games won.",
                "The first row shows that the Black player won only 12% of the games in which the average of its 3 highest heuristically difference actions (min3 h) was smaller than the suggested threshold, TrH = 11.5.",
                "The second row shows a surprising result: it seems that when min3 h > −4 the Black player rarely wins.",
                "Intuition would suggest that games in which the action evaluation values were closer to the maximal values will result in more winning games for Black.",
                "However, it seems that in the Connect-Four domain, merely responding with somewhat easily expected actions, without initiating a few surprising and suboptimal moves, does not yield good results.",
                "The last row sums up the main insights from the analysis; most of Blacks wins (83%) came when its min3 h was in the range of -11.5 to -4.",
                "A close inspection of those Black winning games shows the following pattern behind the numbers: after standard opening moves, Black suddenly drops a disc into an isolated column, which seems a waste of a move.",
                "White continues to build its threats, while usually disregarding Blacks last move, which in turn uses the isolated disc as an anchor for a future winning threat.",
                "The results show that it was beneficial for the Black player Table 2: Blacks winnings percentages % of games min3 h < −11.5 12% min3 h > −4 5% −11.5 ≤ min3 h ≤ −4 83% to take suboptimal actions and not give the current highest possible heuristic value, but will not be too harmful for its position (i.e., will not give high beneficial value to its adversary).",
                "As it turned out, learning the threshold is an important aspect of success: taking wildly risky moves (min3 h < −11.5) or trying to avoid them (min3 h > −4) reduces the Black players winning chances by a large margin. 3.2.2 Affirming the Profile Monitoring Axiom In the task of showing the importance of monitoring ones adversaries profiles, our log files could not be used because they did not contain repeated interactions between players, which are needed to infer the players knowledge about their adversaries.",
                "However, the importance of opponent modeling and its use in attaining tactical advantages was already studied in various domains ([3, 9] are good examples).",
                "In a recent paper, Markovitch and Reger [9] explored the notion of learning and exploitation of opponent weakness in competitive interactions.",
                "They apply simple learning strategies by analyzing examples from past interactions in a specific domain.",
                "They also used the Connect-Four adversarial domain, which can now be used to understand the importance of monitoring the adversarys profile.",
                "Following the presentation of their theoretical model, they describe an extensive empirical study and check the agents performance after learning the weakness model with past examples.",
                "One of the domains used as a competitive environment was the same Connect-Four game (Checkers was the second domain).",
                "Their heuristic function was identical to ours with three different variations (H1, H2, and H3) that are distinguished from one another in their linear combination coefficient values.",
                "The search depth for the players was 3 (as in our analysis).",
                "Their extensive experiments check and compare various learning strategies, risk factors, predefined feature sets and usage methods.",
                "The bottom line is that the Connect-Four domain shows an improvement from a 0.556 winning rate before modeling to a 0.69 after modeling (page 22).",
                "Their conclusions, showing improved performance when holding and using the adversarys model, justify the effort to monitor the adversary profile for continuous and repeated interactions.",
                "An additional point that came up in their experiments is the following: after the opponent weakness model has been learned, the authors describe different methods of integrating the opponent weakness model into the agents decision strategy.",
                "Nevertheless, regardless of the specific method they chose to work with, all integration methods might cause the agent to take suboptimal decisions; it might cause the agent to prefer actions that are suboptimal at the present decision junction, but which might cause the opponent to react in accordance with its weakness model (as represented by our agent) which in turn will be beneficial for us in the future.",
                "The agents behavior, as demonstrated in [9] further confirms and strengthens our Suboptimal Tactical Axiom as discussed in the previous section. 556 The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) 3.2.3 Additional Insights The need for the Goal Achieving, Preventive Act, and Evaluation Maximization axioms are obvious, and need no further verification.",
                "However, even with respect to those axioms, a few interesting insights came up in the log analysis.",
                "The Goal achieving and Preventive Act axioms, though theoretically trivial, seem to provide some challenge to a human player.",
                "In the initial inspection of the logs, we encountered few games2 where a player, for inexplicable reasons, did not block the other from winning or failed to execute its own winning move.",
                "We can blame those faults on the humans lack of attention, or a typing error in its move reply; nevertheless, those errors might occur in bounded rational agents, and the appropriate behavior needs to be axiomatized.",
                "A typical Connect-Four game revolves around generating threats and blocking them.",
                "In our analysis we looked for explicit preventive actions, i.e., moves that block a group of 3 discs, or that remove a future threat (in our limited search horizon).",
                "We found that in 83% of the total games there was at least one preventive action taken by the Black player.",
                "It was also found that Black averaged 2.8 preventive actions per game on the games in which it lost, while averaging 1.5 preventive actions per game when winning.",
                "It seems that Black requires 1 or 2 preventive actions to build its initial taking position, before starting to present threats.",
                "If it did not manage to win, it will usually prevent an extra threat or two before succumbing to White. 4.",
                "RELATED WORK Much research deals with the axiomatization of teamwork and mental states of individuals: some models use knowledge and belief [10], others have models of goals and intentions [8, 4].",
                "However, all these formal theories deal with agent teamwork and cooperation.",
                "As far as we know, our model is the first to provide a formalized model for explicit adversarial environments and agents behavior in it.",
                "The classical Min-Max adversarial search algorithm was the first attempt to integrate the opponent into the search space with a weak assumption of an optimally playing opponent.",
                "Since then, much effort has gone into integrating the opponent model into the decision procedure to predict future behavior.",
                "The M∗ algorithm presented by Carmel and Markovitch [2] showed a method of incorporating opponent models into adversary search, while in [3] they used learning to provide a more accurate opponent model in a 2player repeated game environment, where agents strategies were modeled as finite automata.",
                "Additional Adversarial planning work was done by Willmott et al. [13], which provided an adversarial planning approach to the game of GO.",
                "The research mentioned above dealt with adversarial search and the integration of opponent models into classical utilitybased search methods.",
                "That work shows the importance of opponent modeling and the ability to exploit it to an agents advantage.",
                "However, the basic limitations of those search methods still apply; our model tries to overcome those limitations by presenting a formal model for a new, mental state-based adversarial specification. 5.",
                "CONCLUSIONS We presented an Adversarial Environment model for a 2 These were later removed from the final analysis. bounded rational agent that is situated in an N-player, zerosum environment.",
                "We used the SharedPlans formalization to define the model and the axioms that agents can apply as behavioral guidelines.",
                "The model is meant to be used as a guideline for designing agents that need to operate in such adversarial environments.",
                "We presented empirical results, based on ConnectFour log file analysis, that exemplify the model and the axioms for a bilateral instance of the environment.",
                "The results we presented are a first step towards an expanded model that will cover all types of adversarial environments, for example, environments that are non-zero-sum, and environments that contain natural agents that are not part of the direct conflict.",
                "Those challenges and more will be dealt with in future research. 6.",
                "ACKNOWLEDGMENT This research was supported in part by Israel Science Foundation grants #1211/04 and #898/05. 7.",
                "REFERENCES [1] L. V. Allis.",
                "A knowledge-based approach of Connect-Four - the game is solved: White wins.",
                "Masters thesis, Free University, Amsterdam, The Netherlands, 1988. [2] D. Carmel and S. Markovitch.",
                "Incorporating opponent models into adversary search.",
                "In Proceedings of the Thirteenth National Conference on Artificial Intelligence, pages 120-125, Portland, OR, 1996. [3] D. Carmel and S. Markovitch.",
                "Opponent modeling in multi-agent systems.",
                "In G. Weiß and S. Sen, editors, Adaptation and Learning in Multi-Agent Systems, pages 40-52.",
                "Springer-Verlag, 1996. [4] B. J. Grosz and S. Kraus.",
                "Collaborative plans for complex group action.",
                "Artificial Intelligence, 86(2):269-357, 1996. [5] M. Hadad, G. Kaminka, G. Armon, and S. Kraus.",
                "Supporting collaborative activity.",
                "In Proc. of AAAI-2005, pages 83-88, Pittsburgh, 2005. [6] http://www.gamerz.net/˜pbmserv/. [7] S. Kraus and D. Lehmann.",
                "Designing and building a negotiating automated agent.",
                "Computational Intelligence, 11:132-171, 1995. [8] H. J. Levesque, P. R. Cohen, and J. H. T. Nunes.",
                "On acting together.",
                "In Proc. of AAAI-90, pages 94-99, Boston, MA, 1990. [9] S. Markovitch and R. Reger.",
                "Learning and exploiting relative weaknesses of opponent agents.",
                "Autonomous Agents and Multi-Agent Systems, 10(2):103-130, 2005. [10] Y. M. Ronald Fagin, Joseph Y. Halpern and M. Y. Vardi.",
                "Reasoning about knowledge.",
                "MIT Press, Cambridge, Mass., 1995. [11] P. Thagard.",
                "Adversarial problem solving: Modeling an oponent using explanatory coherence.",
                "Cognitive Science, 16(1):123-149, 1992. [12] R. W. Toseland and R. F. Rivas.",
                "An Introduction to Group Work Practice.",
                "Prentice Hall, Englewood Cliffs, NJ, 2nd edition edition, 1995. [13] S. Willmott, J. Richardson, A. Bundy, and J. Levine.",
                "An adversarial planning approach to Go.",
                "Lecture Notes in Computer Science, 1558:93-112, 1999.",
                "The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) 557"
            ],
            "original_annotated_samples": [],
            "translated_annotated_samples": [
                "Trabajaremos en \"instanciación bilateral y multilateral\" de entornos simples y de suma cero.Instanciación bilateral y multilateral"
            ],
            "translated_text": "",
            "candidates": [],
            "error": []
        },
        "evaluation function": {
            "translated_key": "",
            "is_in_text": true,
            "original_annotated_sentences": [
                "An Adversarial Environment Model for Bounded Rational Agents in Zero-Sum Interactions Inon Zuckerman1 , Sarit Kraus1 , Jeffrey S. Rosenschein2 , Gal Kaminka1 1 Department of Computer Science 2 The School of Engineering Bar-Ilan University and Computer Science Ramat-Gan, Israel Hebrew University, Jerusalem, Israel {zukermi,sarit,galk}@cs.biu.ac.il jeff@cs.huji.ac.il ABSTRACT Multiagent environments are often not cooperative nor collaborative; in many cases, agents have conflicting interests, leading to adversarial interactions.",
                "This paper presents a formal Adversarial Environment model for bounded rational agents operating in a zero-sum environment.",
                "In such environments, attempts to use classical utility-based search methods can raise a variety of difficulties (e.g., implicitly modeling the opponent as an omniscient utility maximizer, rather than leveraging a more nuanced, explicit opponent model).",
                "We define an Adversarial Environment by describing the mental states of an agent in such an environment.",
                "We then present behavioral axioms that are intended to serve as design principles for building such adversarial agents.",
                "We explore the application of our approach by analyzing log files of completed Connect-Four games, and present an empirical analysis of the axioms appropriateness.",
                "Categories and Subject Descriptors I.2.11 [Artificial Intelligence]: Distributed Artificial Intelligence-Intelligent agents,Multiagent Systems; I.2.4 [Artificial Intelligence]: Knowledge Representation Formalisms and Methods -Modal logic General Terms Design, Theory 1.",
                "INTRODUCTION Early research in multiagent systems (MAS) considered cooperative groups of agents; because individual agents had limited resources, or limited access to information (e.g., limited processing power, limited sensor coverage), they worked together by design to solve problems that individually they could not solve, or at least could not solve as efficiently.",
                "MAS research, however, soon began to consider interacting agents with individuated interests, as representatives of different humans or organizations with non-identical interests.",
                "When interactions are guided by diverse interests, participants may have to overcome disagreements, uncooperative interactions, and even intentional attempts to damage one another.",
                "When these types of interactions occur, environments require appropriate behavior from the agents situated in them.",
                "We call these environments Adversarial Environments, and call the clashing agents Adversaries.",
                "Models of cooperation and teamwork have been extensively explored in MAS through the axiomatization of mental states (e.g., [8, 4, 5]).",
                "However, none of this research dealt with adversarial domains and their implications for agent behavior.",
                "Our paper addresses this issue by providing a formal, axiomatized mental state model for a subset of adversarial domains, namely simple zero-sum adversarial environments.",
                "Simple zero-sum encounters exist of course in various twoplayer games (e.g., Chess, Checkers), but they also exist in n-player games (e.g., Risk, Diplomacy), auctions for a single good, and elsewhere.",
                "In these latter environments especially, using a utility-based adversarial search (such as the Min-Max algorithm) does not always provide an adequate solution; the payoff function might be quite complex or difficult to quantify, and there are natural computational limitations on bounded rational agents.",
                "In addition, traditional search methods (like Min-Max) do not make use of a model of the opponent, which has proven to be a valuable addition to adversarial planning [9, 3, 11].",
                "In this paper, we develop a formal, axiomatized model for bounded rational agents that are situated in a zero-sum adversarial environment.",
                "The model uses different modality operators, and its main foundations are the SharedPlans [4] model for collaborative behavior.",
                "We explore environment properties and the mental states of agents to derive behavioral axioms; these behavioral axioms constitute a formal model that serves as a specification and design guideline for agent design in such settings.",
                "We then investigate the behavior of our model empirically using the Connect-Four board game.",
                "We show that this game conforms to our environment definition, and analyze players behavior using a large set of completed match log 550 978-81-904262-7-5 (RPS) c 2007 IFAAMAS files.",
                "In addition, we use the results presented in [9] to discuss the importance of opponent modeling in our ConnectFour adversarial domain.",
                "The paper proceeds as follows.",
                "Section 2 presents the models formalization.",
                "Section 3 presents the empirical analysis and its results.",
                "We discuss related work in Section 4, and conclude and present future directions in Section 5. 2.",
                "ADVERSARIAL ENVIRONMENTS The adversarial environment model (denoted as AE) is intended to guide the design of agents by providing a specification of the capabilities and mental attitudes of an agent in an adversarial environment.",
                "We focus here on specific types of adversarial environments, specified as follows: 1.",
                "Zero-Sum Interactions: positive and negative utilities of all agents sum to zero; 2.",
                "Simple AEs: all agents in the environment are adversarial agents; 3.",
                "Bilateral AEs: AEs with exactly two agents; 4.",
                "Multilateral AEs: AEs of three or more agents.",
                "We will work on both bilateral and multilateral instantiations of zero-sum and simple environments.",
                "In particular, our adversarial environment model will deal with interactions that consist of N agents (N ≥ 2), where all agents are adversaries, and only one agent can succeed.",
                "Examples of such environments range from board games (e.g., Chess, Connect-Four, and Diplomacy) to certain economic environments (e.g., N-bidder auctions over a single good). 2.1 Model Overview Our approach is to formalize the mental attitudes and behaviors of a single adversarial agent; we consider how a single agent perceives the AE.",
                "The following list specifies the conditions and mental states of an agent in a simple, zero-sum AE: 1.",
                "The agent has an individual intention that its own goal will be completed; 2.",
                "The agent has an individual belief that it and its adversaries are pursuing full conflicting goals (defined below)there can be only one winner; 3.",
                "The agent has an individual belief that each adversary has an intention to complete its own full conflicting goal; 4.",
                "The agent has an individual belief in the (partial) profile of its adversaries.",
                "Item 3 is required, since it might be the case that some agent has a full conflicting goal, and is currently considering adopting the intention to complete it, but is, as of yet, not committed to achieving it.",
                "This might occur because the agent has not yet deliberated about the effects that adopting that intention might have on the other intentions it is currently holding.",
                "In such cases, it might not consider itself to even be in an adversarial environment.",
                "Item 4 states that the agent should hold some belief about the profiles of its adversaries.",
                "The profile represents all the knowledge the agent has about its adversary: its weaknesses, strategic capabilities, goals, intentions, trustworthiness, and more.",
                "It can be given explicitly or can be learned from observations of past encounters. 2.2 Model Definitions for Mental States We use Grosz and Krauss definitions of the modal operators, predicates, and meta-predicates, as defined in their SharedPlan formalization [4].",
                "We recall here some of the predicates and operators that are used in that formalization: Int.To(Ai, α, Tn, Tα, C) represents Ais intentions at time Tn to do an action α at time Tα in the context of C. Int.Th(Ai, prop, Tn, Tprop, C) represents Ais intentions at time Tn that a certain proposition prop holds at time Tprop in the context of C. The potential intention operators, Pot.Int.To(...) and Pot.Int.Th(...), are used to represent the mental state when an agent considers adopting an intention, but has not deliberated about the interaction of the other intentions it holds.",
                "The operator Bel(Ai, f, Tf ) represents agent Ai believing in the statement expressed in formula f, at time Tf .",
                "MB(A, f, Tf ) represents mutual belief for a group of agents A.",
                "A snapshot of the system finds our environment to be in some state e ∈ E of environmental variable states, and each adversary in any LAi ∈ L of possible local states.",
                "At any given time step, the system will be in some world w of the set of all possible worlds w ∈ W, where w = E×LA1 ×LA2 × ...LAn , and n is the number of adversaries.",
                "For example, in a Texas Holdem poker game, an agents local state might be its own set of cards (which is unknown to its adversary) while the environment will consist of the betting pot and the community cards (which are visible to both players).",
                "A utility function under this formalization is defined as a mapping from a possible world w ∈ W to an element in , which expresses the desirability of the world, from a single agent perspective.",
                "We usually normalize the range to [0,1], where 0 represents the least desirable possible world, and 1 is the most desirable world.",
                "The implementation of the utility function is dependent on the domain in question.",
                "The following list specifies new predicates, functions, variables, and constants used in conjunction with the original definitions for the adversarial environment formalization: 1. φ is a null action (the agent does not do anything). 2.",
                "GAi is the set of agent Ais goals.",
                "Each goal is a set of predicates whose satisfaction makes the goal complete (we use G∗ Ai ∈ GAi to represent an arbitrary goal of agent Ai). 3. gAi is the set of agent Ais subgoals.",
                "Subgoals are predicates whose satisfaction represents an important milestone toward achievement of the full goal. gG∗ Ai ⊆ gAi is the set of subgoals that are important to the completion of goal G∗ Ai (we will use g∗ G∗ Ai ∈ gG∗ Ai to represent an arbitrary subgoal). 4.",
                "P Aj Ai is the profile object agent Ai holds about agent Aj. 5.",
                "CA is a general set of actions for all agents in A which are derived from the environments constraints.",
                "CAi ⊆ CA is the set of agent Ais possible actions. 6.",
                "Do(Ai, α, Tα, w) holds when Ai performs action α over time interval Tα in world w. 7.",
                "Achieve(G∗ Ai , α, w) is true when goal G∗ Ai is achieved following the completion of action α in world w ∈ W, where α ∈ CAi . 8.",
                "Profile(Ai, PAi Ai ) is true when agent Ai holds an object profile for agent Aj.",
                "Definition 1.",
                "Full conflict (FulConf ) describes a zerosum interaction where only a single goal of the goals in conflict can be completed.",
                "FulConf(G∗ Ai , G∗ Aj ) ⇒ (∃α ∈ CAi , ∀w, β ∈ CAj ) (Achieve(G∗ Ai , α, w) ⇒ ¬Achieve(G∗ Aj , β, w)) ∨ (∃β ∈ CAj , ∀w, α ∈ CAi )(Achieve(G∗ Aj , β, w) ⇒ ¬Achieve(G∗ Ai , α, w)) Definition 2.",
                "Adversarial Knowledge (AdvKnow) is a function returning a value which represents the amount of The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) 551 knowledge agent Ai has on the profile of agent Aj, at time Tn.",
                "The higher the value, the more knowledge agent Ai has.",
                "AdvKnow : P Aj Ai × Tn → Definition 3.",
                "Eval - This <br>evaluation function</br> returns an estimated expected utility value for an agent in A, after completing an action from CA in some world state w. Eval : A × CA × w → Definition 4.",
                "TrH - (Threshold) is a numerical constant in the [0,1] range that represents an <br>evaluation function</br> (Eval) threshold value.",
                "An action that yields an estimated utility evaluation above the TrH is regarded as a highly beneficial action.",
                "The Eval value is an estimation and not the real utility function, which is usually unknown.",
                "Using the real utility value for a rational agent would easily yield the best outcome for that agent.",
                "However, agents usually do not have the real utility functions, but rather a heuristic estimate of it.",
                "There are two important properties that should hold for the <br>evaluation function</br>: Property 1.",
                "The <br>evaluation function</br> should state that the most desirable world state is one in which the goal is achieved.",
                "Therefore, after the goal has been satisfied, there can be no future action that can put the agent in a world state with higher Eval value. (∀Ai, G∗ Ai , α, β ∈ CAi , w ∈ W) Achieve(G∗ Ai , α, w) ⇒ Eval(Ai, α, w) ≥ Eval(Ai, β, w) Property 2.",
                "The <br>evaluation function</br> should project an action that causes a completion of a goal or a subgoal to a value which is greater than TrH (a highly beneficial action). (∀Ai, G∗ Ai ∈ GAi , α ∈ CAi , w ∈ W, g∗ GAi ∈ gGAi ) Achieve(G∗ Ai , α, w) ∨ Achieve(g∗ GAi , α, w) ⇒ Eval(Ai, α, w) ≥ TrH.",
                "Definition 5.",
                "SetAction We define a set action (SetAction) as a set of action operations (either complex or basic actions) from some action sets CAi and CAj which, according to agent Ais belief, are attached together by a temporal and consequential relationship, forming a chain of events (action, and its following consequent action). (∀α1 , . . . , αu ∈ CAi , β1 , . . . , βv ∈ CAj , w ∈ W) SetAction(α1 , . . . , αu , β1 , . . . , βv , w) ⇒ ((Do(Ai, α1 , Tα1 , w) ⇒ Do(Aj, β1 , Tβ1 , w)) ⇒ Do(Ai, α2 , Tα2 , w) ⇒ . . . ⇒ Do(Ai, αu , Tαu , w)) The consequential relation might exist due to various environmental constraints (when one action forces the adversary to respond with a specific action) or due to the agents knowledge about the profile of its adversary.",
                "Property 3.",
                "As the knowledge we have about our adversary increases we will have additional beliefs about its behavior in different situations which in turn creates new set actions.",
                "Formally, if our AdvKnow at time Tn+1 is greater than AdvKnow at time Tn, then every SetAction known at time Tn is also known at time Tn+1.",
                "AdvKnow(P Aj Ai , Tn+1) > AdvKnow(P Aj Ai , Tn) ⇒ (∀α1 , . . . , αu ∈ CAi , β1 , . . . , βv ∈ CAj ) Bel(Aag, SetAction(α1 , . . . , αu , β1 , . . . , βv ), Tn) ⇒ Bel(Aag, SetAction(α1 , . . . , αu , β1 , . . . , βv ), Tn+1) 2.3 The Environment Formulation The following axioms provide the formal definition for a simple, zero-sum Adversarial Environment (AE).",
                "Satisfaction of these axioms means that the agent is situated in such an environment.",
                "It provides specifications for agent Aag to interact with its set of adversaries A with respect to goals G∗ Aag and G∗ A at time TCo at some world state w. AE(Aag, A, G∗ Aag , A1, . . . , Ak, G∗ A1 , . . . , G∗ Ak , Tn, w) 1.",
                "Aag has an Int.Th his goal would be completed: (∃α ∈ CAag , Tα) Int.Th(Aag, Achieve(G∗ Aag , α), Tn, Tα, AE) 2.",
                "Aag believes that it and each of its adversaries Ao are pursuing full conflicting goals: (∀Ao ∈ {A1, . . . , Ak}) Bel(Aag, FulConf(G∗ Aag , G∗ Ao ), Tn) 3.",
                "Aag believes that each of his adversaries in Ao has the Int.Th his conflicting goal G∗ Aoi will be completed: (∀Ao ∈ {A1, . . . , Ak})(∃β ∈ CAo , Tβ) Bel(Aag, Int.Th(Ao, Achieve(G∗ Ao , β), TCo, Tβ, AE), Tn) 4.",
                "Aag has beliefs about the (partial) profiles of its adversaries (∀Ao ∈ {A1, . . . , Ak}) (∃PAo Aag ∈ PAag )Bel(Aag, Profile(Ao, PAo Aag ), Tn) To build an agent that will be able to operate successfully within such an AE, we must specify behavioral guidelines for its interactions.",
                "Using a naive Eval maximization strategy to a certain search depth will not always yield satisfactory results for several reasons: (1) the search horizon problem when searching for a fixed depth; (2) the strong assumption of an optimally rational, unbounded resources adversary; (3) using an estimated <br>evaluation function</br> which will not give optimal results in all world states, and can be exploited [9].",
                "The following axioms specify the behavioral principles that can be used to differentiate between successful and less successful agents in the above Adversarial Environment.",
                "Those axioms should be used as specification principles when designing and implementing agents that should be able to perform well in such Adversarial Environments.",
                "The behavioral axioms represent situations in which the agent will adopt potential intentions to (Pot.Int.To(...)) perform an action, which will typically require some means-end reasoning to select a possible course of action.",
                "This reasoning will lead to the adoption of an Int.To(...) (see [4]).",
                "A1.",
                "Goal Achieving Axiom.",
                "The first axiom is the simplest case; when the agent Aag believes that it is one action (α) away from achieving his conflicting goal G∗ Aag , it should adopt the potential intention to do α and complete its goal. (∀Aag, α ∈ CAag , Tn, Tα, w ∈ W) (Bel(Aag, Do(Aag, α, Tα, w) ⇒ Achieve(G∗ Aag , α, w)) ⇒ Pot.Int.To(Aag, α, Tn, Tα, w) This somewhat trivial behavior is the first and strongest axiom.",
                "In any situation, when the agent is an action away from completing the goal, it should complete the action.",
                "Any fair Eval function would naturally classify α as the maximal value action (property 1).",
                "However, without explicit axiomatization of such behavior there might be situations where the agent will decide on taking another action for various reasons, due to its bounded decision resources.",
                "A2.",
                "Preventive Act Axiom.",
                "Being in an adversarial situation, agent Aag might decide to take actions that will damage one of its adversarys plans to complete its goal, even if those actions do not explicitly advance Aag towards its conflicting goal G∗ Aag .",
                "Such preventive action will take place when agent Aag has a belief about the possibility of its adversary Ao doing an action β that will give it a high 552 The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) utility evaluation value (> TrH).",
                "Believing that taking action α will prevent the opponent from doing its β, it will adopt a potential intention to do α. (∀Aag, Ao ∈ A, α ∈ CAag , β ∈ CAo , Tn, Tβ, w ∈ W) (Bel(Aag, Do(Ao, β, Tβ, w) ∧ Eval(Ao, β, w) > TrH, Tn) ∧ Bel(Aag, Do(Aag, α, Tα, w) ⇒ ¬Do(Ao, β, Tβ, w), Tn) ⇒ Pot.Int.To(Aag, α, Tn, Tα, w) This axiom is a basic component of any adversarial environment.",
                "For example, looking at a Chess board game, a player could realize that it is about to be checkmated by its opponent, thus making a preventive move.",
                "Another example is a Connect Four game: when a player has a row of three chips, its opponent must block it, or lose.",
                "A specific instance of A1 occurs when the adversary is one action away from achieving its goal, and immediate preventive action needs to be taken by the agent.",
                "Formally, we have the same beliefs as stated above, with a changed belief that doing action β will cause agent Ao to achieve its goal.",
                "Proposition 1: Prevent or lose case. (∀Aag, Ao ∈ A, α ∈ CAag , β ∈ CAo , G∗ Ao , Tn, Tα, Tβ, w ∈ W) Bel(Aag, Do(Ao, β, Tβ, w) ⇒ Achieve(G∗ Ao , β, w), Tn) ∧ Bel(Aag, Do(Aag, α, Tα, w) ⇒ ¬Do(Ao, β, Tβ, w)) ⇒ Pot.Int.To(Aag, α, Tn, Tα, w) Sketch of proof: Proposition 1 can be easily derived from axiom A1 and the property 2 of the Eval function, which states that any action that causes a completion of a goal is a highly beneficial action.",
                "The preventive act behavior will occur implicitly when the Eval function is equal to the real world utility function.",
                "However, being bounded rational agents and dealing with an estimated <br>evaluation function</br> we need to explicitly axiomatize such behavior, for it will not always occur implicitly from the <br>evaluation function</br>.",
                "A3.",
                "Suboptimal Tactical Move Axiom.",
                "In many scenarios a situation may occur where an agent will decide not to take the current most beneficial action it can take (the action with the maximal utility evaluation value), because it believes that taking another action (with lower utility evaluation value) might yield (depending on the adversarys response) a future possibility for a highly beneficial action.",
                "This will occur most often when the Eval function is inaccurate and differs by a large extent from the Utility function.",
                "Put formally, agent Aag believes in a certain SetAction that will evolve according to its initial action and will yield a high beneficial value (> TrH) solely for it. (∀Aag, Ao ∈ A, Tn, w ∈ W) (∃α1 , . . . , αu ∈ CAi , β1 , . . . , βv ∈ CAj , Tα1 ) Bel(Aag, SetAction(α1 , . . . , αu , β1 , . . . , βv ), Tn) ∧ Bel(Aag, Eval(Ao, βv , w) < TrH < Eval(Aag, αu , w), Tn) ⇒ Pot.Int.To(Aag, α1 , Tn, Tα1 , w) An agent might believe that a chain of events will occur for various reasons due to the inevitable nature of the domain.",
                "For example, in Chess, we often observe the following: a move causes a check position, which in turn limits the opponents moves to avoiding the check, to which the first player might react with another check, and so on.",
                "The agent might also believe in a chain of events based on its knowledge of its adversarys profile, which allows it to foresee the adversarys movements with high accuracy.",
                "A4.",
                "Profile Detection Axiom.",
                "The agent can adjust its adversarys profiles by observations and pattern study (specifically, if there are repeated encounters with the same adversary).",
                "However, instead of waiting for profile information to be revealed, an agent can also initiate actions that will force its adversary to react in a way that will reveal profile knowledge about it.",
                "Formally, the axiom states that if all actions (γ) are not highly beneficial actions (< TrH), the agent can do action α in time Tα if it believes that it will result in a non-highly beneficial action β from its adversary, which in turn teaches it about the adversarys profile, i.e., gives a higher AdvKnow(P Aj Ai , Tβ). (∀Aag, Ao ∈ A, α ∈ CAag , β ∈ CAo , Tn, Tα, Tβ, w ∈ W) Bel(Aag, (∀γ ∈ CAag )Eval(Aag, γ, w) < TrH, Tn) ∧ Bel(Aag, Do(Aag, α, Tα, w) ⇒ Do(Ao, β, Tβ, w), Tn) ∧ Bel(Aag, Eval(Ao, β, w) < TrH) ∧ Bel(Aag, AdvKnow(P Aj Ai , Tβ) > AdvKnow(P Aj Ai , Tn), Tn) ⇒ Pot.Int.To(Aag, α, Tn, Tα, w) For example, going back to the Chess board game scenario, consider starting a game versus an opponent about whom we know nothing, not even if it is a human or a computerized opponent.",
                "We might start playing a strategy that will be suitable versus an average opponent, and adjust our game according to its level of play.",
                "A5.",
                "Alliance Formation Axiom The following behavioral axiom is relevant only in a multilateral instantiation of the adversarial environment (obviously, an alliance cannot be formed in a bilateral, zero-sum encounter).",
                "In different situations during a multilateral interaction, a group of agents might believe that it is in their best interests to form a temporary alliance.",
                "Such an alliance is an agreement that constrains its members behavior, but is believed by its members to enable them to achieve a higher utility value than the one achievable outside of the alliance.",
                "As an example, we can look at the classical Risk board game, where each player has an individual goal of being the sole conquerer of the world, a zero-sum game.",
                "However, in order to achieve this goal, it might be strategically wise to make short-term ceasefire agreements with other players, or to join forces and attack an opponent who is stronger than the rest.",
                "An alliances terms defines the way its members should act.",
                "It is a set of predicates, denoted as Terms, that is agreed upon by the alliance members, and should remain true for the duration of the alliance.",
                "For example, the set Terms in the Risk scenario, could contain the following predicates: 1.",
                "Alliance members will not attack each other on territories X, Y and Z; 2.",
                "Alliance members will contribute C units per turn for attacking adversary Ao; 3.",
                "Members are obligated to stay as part of the alliance until time Tk or until adversarys Ao army is smaller than Q.",
                "The set Terms specifies inter-group constraints on each of the alliance members (∀Aal i ∈ Aal ⊆ A) set of actions Cal i ⊆ C. Definition 6.",
                "Al val - the total evaluation value that agent Ai will achieve while being part of Aal is the sum of Evali (Eval for Ai) of each of Aal j Eval values after taking their own α actions (via the agent(α) predicate): Al val(Ai, Cal , Aal , w) = α∈Cal Evali(Aal j , agent(α), w) Definition 7.",
                "Al TrH - is a number representing an Al val The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) 553 threshold; above it, the alliance can be said to be a highly beneficial alliance.",
                "The value of Al TrH will be calculated dynamically according to the progress of the interaction, as can be seen in [7].",
                "After an alliance is formed, its members are now working in their normal adversarial environment, as well as according to the mental states and axioms required for their interactions as part of the alliance.",
                "The following Alliance model (AL) specifies the conditions under which the group Aal can be said to be in an alliance and working with a new and constrained set of actions Cal , at time Tn.",
                "AL(Aal , Cal , w, Tn) 1.",
                "Aal has a MB that all members are part of Aal : MB(Aal , (∀Aal i ∈ Aal )member(Aal i , Aal ), Tn) 2.",
                "Aal has a MB that the group be maintained: MB(Aal , (∀Aal i ∈ Aal )Int.Th (Ai, member(Ai, Aal ), Tn, Tn+1, Co), Tn) 3.",
                "Aal has a MB that being members gives them high utility value: MB(Aal , (∀Aal i ∈ Aal )Al val(Aal i , Cal , Aal , w) ≥ Al TrH, Tn) Members profiles are a crucial part of successful alliances.",
                "We assume that agents that have more accurate profiles of their adversaries will be more successful in such environments.",
                "Such agents will be able to predict when a member is about to breach the alliances contract (item 2 in the above model), and take counter measures (when item 3 will falsify).",
                "The robustness of the alliance is in part a function of its members trustfulness measure, objective position estimation, and other profile properties.",
                "We should note that an agent can simultaneously be part of more than one alliance.",
                "Such a temporary alliance, where the group members do not have a joint goal but act collaboratively for the interest of their own individual goals, is classified as a Treatment Group by modern psychologists [12] (in contrast to a Task Group, where its members have a joint goal).",
                "The Shared Activity model as presented in [5] modeled Treatment Group behavior using the same SharedPlans formalization.",
                "When comparing both definitions of an alliance and a Treatment Group we found an unsurprising resemblance between both models: the environment models definitions are almost identical (see SAs definitions in [5]), and their Selfish-Act and Cooperative Act axioms conform to our adversarial agents behavior.",
                "The main distinction between both models is the integration of a Helpful-behavior act axiom, in the Shared Activity which cannot be part of ours.",
                "This axiom states that an agent will consider taking action that will lower its Eval value (to a certain lower bound), if it believes that a group partner will gain a significant benefit.",
                "Such behavior cannot occur in a pure adversarial environment (as a zero-sum game is), where the alliance members are constantly on watch to manipulate their alliance to their own advantage.",
                "A6.",
                "Evaluation Maximization Axiom.",
                "In a case when all other axioms are inapplicable, we will proceed with the action that maximizes the heuristic value as computed in the Eval function. (∀Aag, Ao ∈ A, α ∈ Cag, Tn, w ∈ W) Bel(Aag, (∀γ ∈ Cag)Eval(Aag, α, w) ≥ Eval(Aag, γ, w), Tn) ⇒ Pot.Int.To(Aag, α, Tn, Tα, w) T1.",
                "Optimality on Eval = Utility The above axiomatic model handles situations where the Utility is unknown and the agents are bounded rational agents.",
                "The following theorem shows that in bilateral interactions, where the agents have the real Utility function (i.e., Eval = Utility) and are rational agents, the axioms provide the same optimal result as classic adversarial search (e.g., Min-Max).",
                "Theorem 1.",
                "Let Ae ag be an unbounded rational AE agent using the Eval heuristic <br>evaluation function</br>, Au ag be the same agent using the true Utility function, and Ao be a sole unbounded utility-based rational adversary.",
                "Given that Eval = Utility: (∀α ∈ CAu ag , α ∈ CAe ag , Tn, w ∈ W) Pot.Int.To(Au ag, α, Tn, Tα, w) → Pot.Int.To(Ae ag, α , Tn, Tα, w) ∧ ((α = α ) ∨ (Utility(Au ag, α, w) = Eval(Ae ag, α , w))) Sketch of proof - Given that Au ag has the real utility function and unbounded resources, it can generate the full game tree and run the optimal MinMax algorithm to choose the highest utility value action, which we denote by, α.",
                "The proof will show that Ae ag, using the AE axioms, will select the same or equal utility α (when there is more than one action with the same max utility) when Eval = Utility. (A1) Goal achieving axiom - suppose there is an α such that its completion will achieve Au ags goal.",
                "It will obtain the highest utility by Min-Max for Au ag.",
                "The Ae ag agent will select α or another action with the same utility value via A1.",
                "If such α does not exist, Ae ag cannot apply this axiom, and proceeds to A2. (A2) Preventive act axiom - (1) Looking at the basic case (see Prop1), if there is a β which leads Ao to achieve its goal, then a preventive action α will yield the highest utility for Au ag.",
                "Au ag will choose it through the utility, while Ae ag will choose it through A2. (2) In the general case, β is a highly beneficial action for Ao, thus yields low utility for Au ag, which will guide it to select an α that will prevent β, while Ae ag will choose it through A2.1 If such β does not exist for Ao, then A2 is not applicable, and Ae ag can proceed to A3. (A3) Suboptimal tactical move axiom - When using a heuristic Eval function, Ae ag has a partial belief in the profile of its adversary (item 4 in AE model), which may lead it to believe in SetActions (Prop1).",
                "In our case, Ae ag is holding a full profile on its optimal adversary and knows that Ao will behave optimally according to the real utility values on the complete search tree, therefore, any belief about suboptimal SetAction cannot exist, yielding this axiom inapplicable.",
                "Ae ag will proceed to A4. (A4) Profile detection axiom - Given that Ae ag has the full profile of Ao, none of Ae ags actions can increase its knowledge.",
                "That axiom will not be applied, and the agent will proceed with A6 (A5 will be disregarded because the interaction is bilateral). (A6) Evaluation maximization axiom - This axiom will select the max Eval for Ae ag.",
                "Given that Eval = Utility, the same α that was selected by Au ag will be selected. 3.",
                "EVALUATION The main purpose of our experimental analysis is to evaluate the models behavior and performance in a real adversarial environment.",
                "This section investigates whether bounded 1 A case where following the completion of β there exists a γ which gives high utility for Agent Au ag, cannot occur because Ao uses the same utility, and γs existence will cause it to classify β as a low utility action. 554 The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) rational agents situated in such adversarial environments will be better off applying our suggested behavioral axioms. 3.1 The Domain To explore the use of the above model and its behavioral axioms, we decided to use the Connect-Four game as our adversarial environment.",
                "Connect-Four is a 2-player, zerosum game which is played using a 6x7 matrix-like board.",
                "Each turn, a player drops a disc into one of the 7 columns (the set of 21 discs is usually colored yellow for player 1 and red for player 2; we will use White and Black respectively to avoid confusion).",
                "The winner is the first player to complete a horizontal, vertical, or diagonal set of four discs with its color.",
                "On very rare occasions, the game might end in a tie if all the empty grids are filled, but no player managed to create a 4-disc set.",
                "The Connect-Four game was solved in [1], where it is shown that the first player (playing with the white discs) can force a win by starting in the middle column (column 4) and playing optimally However, the optimal strategy is very complex, and difficult to follow even for complex bounded rational agents, such as human players.",
                "Before we can proceed checking agent behavior, we must first verify that the domain conforms to the adversarial environments definition as given above (which the behavioral axioms are based on).",
                "First, when playing a Connect-Four game, the agent has an intention to win the game (item 1).",
                "Second (item 2), our agent believes that in Connect-Four there can only be one winner (or no winner at all in the rare occurrence of a tie).",
                "In addition, our agent believes that its opponent to the game will try to win (item 3), and we hope it has some partial knowledge (item 4) about its adversary (this knowledge can vary from nothing, through simple facts such as age, to strategies and weaknesses).",
                "Of course, not all Connect-Four encounters are adversarial.",
                "For example, when a parent is playing the game with its child, the following situation might occur: the child, having a strong incentive to win, treats the environment as adversarial (it intends to win, understands that there can only be one winner, and believes that its parent is trying to beat him).",
                "However, the parents point of view might see the environment as an educational one, where its goal is not to win the game, but to cause enjoyment or practice strategic reasoning.",
                "In such an educational environment, a new set of behavioral axioms might be more beneficial to the parents goals than our suggested adversarial behavioral axioms. 3.2 Axiom Analysis After showing that the Connect-Four game is indeed a zero-sum, bilateral adversarial environment, the next step is to look at players behaviors during the game and check whether behaving according to our model does improve performance.",
                "To do so we have collected log files from completed Connect-Four games that were played by human players over the Internet.",
                "Our collected log file data came from Play by eMail (PBeM) sites.",
                "These are web sites that host email games, where each move is taken by an email exchange between the server and the players.",
                "Many such sites archives contain real competitive interactions, and also maintain a ranking system for their members.",
                "Most of the data we used can be found in [6].",
                "As can be learned from [1], Connect-Four has an optimal strategy and a considerable advantage for the player who starts the game (which we call the White player).",
                "We will concentrate in our analysis on the second players moves (to be called Black).",
                "The White player, being the first to act, has the so-called initiative advantage.",
                "Having the advantage and a good strategy will keep the Black player busy reacting to its moves, instead of initiating threats.",
                "A threat is a combination of three discs of the same color, with an empty spot for the fourth winning disk.",
                "An open threat is a threat that can be realized in the opponents next move.",
                "In order for the Black player to win, it must somehow turn the tide, take the advantage and start presenting threats to the White player.",
                "We will explore Black players behavior and their conformance to our axioms.",
                "To do so, we built an application that reads log files and analyzes the Black players moves.",
                "The application contains two main components: (1) a Min-Max algorithm for evaluation of moves; (2) open threats detector for the discovering of open threats.",
                "The Min-Max algorithm will work to a given depth, d and for each move α will output the heuristic value for the next action taken by the player as written in the log file, h(α), alongside the maximum heuristic value, maxh(α), that could be achieved prior to taking the move (obviously, if h(α) = maxh(α), then the player did not do the optimal move heuristically).",
                "The threat detectors job is to notify if some action was taken in order to block an open threat (not blocking an open threat will probably cause the player to lose in the opponents next move).",
                "The heuristic function used by Min-Max to evaluate the players utility is the following function, which is simple to compute, yet provides a reasonable challenge to human opponents: Definition 8.",
                "Let Group be an adjacent set of four squares that are horizontal, vertical, or diagonal.",
                "Groupn b (Groupn w) be a Group with n pieces of the black (white) color and 4−n empty squares. h = ((Group1 b ∗α)+(Group2 b ∗β)+(Group3 b ∗γ)+(Group4 b ∗∞)) − ((Group1 w ∗α)+(Group2 w ∗β)+(Group3 w ∗γ)+(Group4 w ∗∞)) The values of α, β and δ can vary to form any desired linear combination; however, it is important to value them with the α < β < δ ordering in mind (we used 1, 4, and 8 as their respective values).",
                "Groups of 4 discs of the same color means victory, thus discovery of such a group will result in ∞ to ensure an extreme value.",
                "We now use our estimated <br>evaluation function</br> to evaluate the Black players actions during the Connect-Four adversarial interaction.",
                "Each game from the log file was input into the application, which processed and output a reformatted log file containing the h value of the current move, the maxh value that could be achieved, and a notification if an open threat was detected.",
                "A total of 123 games were analyzed (57 with White winning, and 66 with Black winning).",
                "A few additional games were manually ignored in the experiment, due to these problems: a player abandoning the game while the outcome is not final, or a blunt irrational move in the early stages of the game (e.g., not blocking an obvious winning group in the first opening moves).",
                "In addition, a single tie game was also removed.",
                "The simulator was run to a search depth of 3 moves.",
                "We now proceed to analyze the games with respect to each behavioral axiom.",
                "The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) 555 Table 1: Average heuristic difference analysis Black losses Black Won Avg minh -17.62 -12.02 Avg 3 lowest h moves (min3 h) -13.20 -8.70 3.2.1 Affirming the Suboptimal tactical move axiom The following section presents the heuristic evaluations of the Min-Max algorithm for each action, and checks the amount and extent of suboptimal tactical actions and their implications on performance.",
                "Table 1 shows results and insights from the games heuristic analysis, when search depth equals 3 (this search depth was selected for the results to be comparable to [9], see Section 3.2.3).",
                "The tables heuristic data is the difference between the present maximal heuristic value and the heuristic value of the action that was eventually taken by the player (i.e., the closer the number is to 0, the closer the action was to the maximum heuristic action).",
                "The first row presents the difference values of the action that had the maximal difference value among all the Black players actions in a given game, as averaged over all Blacks winning and losing games (see respective columns).",
                "In games in which the Black player loses, its average difference value was -17.62, while in games in which the Black player won, its average was -12.02.",
                "The second row expands the analysis by considering the 3 highest heuristic difference actions, and averaging them.",
                "In that case, we notice an average heuristic difference of 5 points between games which the Black player loses and games in which it wins.",
                "Nevertheless, the importance of those numbers is that they allowed us to take an educated guess on a threshold number of 11.5, as the value of the TrH constant, which differentiates between normal actions and highly beneficial ones.",
                "After finding an approximated TrH constant, we can proceed with an analysis of the importance of suboptimal moves.",
                "To do so we took the subset of games in which the minimum heuristic difference value for Blacks actions was 11.5.",
                "As presented in Table 2, we can see the different min3 h average of the 3 largest ranges and the respective percentage of games won.",
                "The first row shows that the Black player won only 12% of the games in which the average of its 3 highest heuristically difference actions (min3 h) was smaller than the suggested threshold, TrH = 11.5.",
                "The second row shows a surprising result: it seems that when min3 h > −4 the Black player rarely wins.",
                "Intuition would suggest that games in which the action evaluation values were closer to the maximal values will result in more winning games for Black.",
                "However, it seems that in the Connect-Four domain, merely responding with somewhat easily expected actions, without initiating a few surprising and suboptimal moves, does not yield good results.",
                "The last row sums up the main insights from the analysis; most of Blacks wins (83%) came when its min3 h was in the range of -11.5 to -4.",
                "A close inspection of those Black winning games shows the following pattern behind the numbers: after standard opening moves, Black suddenly drops a disc into an isolated column, which seems a waste of a move.",
                "White continues to build its threats, while usually disregarding Blacks last move, which in turn uses the isolated disc as an anchor for a future winning threat.",
                "The results show that it was beneficial for the Black player Table 2: Blacks winnings percentages % of games min3 h < −11.5 12% min3 h > −4 5% −11.5 ≤ min3 h ≤ −4 83% to take suboptimal actions and not give the current highest possible heuristic value, but will not be too harmful for its position (i.e., will not give high beneficial value to its adversary).",
                "As it turned out, learning the threshold is an important aspect of success: taking wildly risky moves (min3 h < −11.5) or trying to avoid them (min3 h > −4) reduces the Black players winning chances by a large margin. 3.2.2 Affirming the Profile Monitoring Axiom In the task of showing the importance of monitoring ones adversaries profiles, our log files could not be used because they did not contain repeated interactions between players, which are needed to infer the players knowledge about their adversaries.",
                "However, the importance of opponent modeling and its use in attaining tactical advantages was already studied in various domains ([3, 9] are good examples).",
                "In a recent paper, Markovitch and Reger [9] explored the notion of learning and exploitation of opponent weakness in competitive interactions.",
                "They apply simple learning strategies by analyzing examples from past interactions in a specific domain.",
                "They also used the Connect-Four adversarial domain, which can now be used to understand the importance of monitoring the adversarys profile.",
                "Following the presentation of their theoretical model, they describe an extensive empirical study and check the agents performance after learning the weakness model with past examples.",
                "One of the domains used as a competitive environment was the same Connect-Four game (Checkers was the second domain).",
                "Their heuristic function was identical to ours with three different variations (H1, H2, and H3) that are distinguished from one another in their linear combination coefficient values.",
                "The search depth for the players was 3 (as in our analysis).",
                "Their extensive experiments check and compare various learning strategies, risk factors, predefined feature sets and usage methods.",
                "The bottom line is that the Connect-Four domain shows an improvement from a 0.556 winning rate before modeling to a 0.69 after modeling (page 22).",
                "Their conclusions, showing improved performance when holding and using the adversarys model, justify the effort to monitor the adversary profile for continuous and repeated interactions.",
                "An additional point that came up in their experiments is the following: after the opponent weakness model has been learned, the authors describe different methods of integrating the opponent weakness model into the agents decision strategy.",
                "Nevertheless, regardless of the specific method they chose to work with, all integration methods might cause the agent to take suboptimal decisions; it might cause the agent to prefer actions that are suboptimal at the present decision junction, but which might cause the opponent to react in accordance with its weakness model (as represented by our agent) which in turn will be beneficial for us in the future.",
                "The agents behavior, as demonstrated in [9] further confirms and strengthens our Suboptimal Tactical Axiom as discussed in the previous section. 556 The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) 3.2.3 Additional Insights The need for the Goal Achieving, Preventive Act, and Evaluation Maximization axioms are obvious, and need no further verification.",
                "However, even with respect to those axioms, a few interesting insights came up in the log analysis.",
                "The Goal achieving and Preventive Act axioms, though theoretically trivial, seem to provide some challenge to a human player.",
                "In the initial inspection of the logs, we encountered few games2 where a player, for inexplicable reasons, did not block the other from winning or failed to execute its own winning move.",
                "We can blame those faults on the humans lack of attention, or a typing error in its move reply; nevertheless, those errors might occur in bounded rational agents, and the appropriate behavior needs to be axiomatized.",
                "A typical Connect-Four game revolves around generating threats and blocking them.",
                "In our analysis we looked for explicit preventive actions, i.e., moves that block a group of 3 discs, or that remove a future threat (in our limited search horizon).",
                "We found that in 83% of the total games there was at least one preventive action taken by the Black player.",
                "It was also found that Black averaged 2.8 preventive actions per game on the games in which it lost, while averaging 1.5 preventive actions per game when winning.",
                "It seems that Black requires 1 or 2 preventive actions to build its initial taking position, before starting to present threats.",
                "If it did not manage to win, it will usually prevent an extra threat or two before succumbing to White. 4.",
                "RELATED WORK Much research deals with the axiomatization of teamwork and mental states of individuals: some models use knowledge and belief [10], others have models of goals and intentions [8, 4].",
                "However, all these formal theories deal with agent teamwork and cooperation.",
                "As far as we know, our model is the first to provide a formalized model for explicit adversarial environments and agents behavior in it.",
                "The classical Min-Max adversarial search algorithm was the first attempt to integrate the opponent into the search space with a weak assumption of an optimally playing opponent.",
                "Since then, much effort has gone into integrating the opponent model into the decision procedure to predict future behavior.",
                "The M∗ algorithm presented by Carmel and Markovitch [2] showed a method of incorporating opponent models into adversary search, while in [3] they used learning to provide a more accurate opponent model in a 2player repeated game environment, where agents strategies were modeled as finite automata.",
                "Additional Adversarial planning work was done by Willmott et al. [13], which provided an adversarial planning approach to the game of GO.",
                "The research mentioned above dealt with adversarial search and the integration of opponent models into classical utilitybased search methods.",
                "That work shows the importance of opponent modeling and the ability to exploit it to an agents advantage.",
                "However, the basic limitations of those search methods still apply; our model tries to overcome those limitations by presenting a formal model for a new, mental state-based adversarial specification. 5.",
                "CONCLUSIONS We presented an Adversarial Environment model for a 2 These were later removed from the final analysis. bounded rational agent that is situated in an N-player, zerosum environment.",
                "We used the SharedPlans formalization to define the model and the axioms that agents can apply as behavioral guidelines.",
                "The model is meant to be used as a guideline for designing agents that need to operate in such adversarial environments.",
                "We presented empirical results, based on ConnectFour log file analysis, that exemplify the model and the axioms for a bilateral instance of the environment.",
                "The results we presented are a first step towards an expanded model that will cover all types of adversarial environments, for example, environments that are non-zero-sum, and environments that contain natural agents that are not part of the direct conflict.",
                "Those challenges and more will be dealt with in future research. 6.",
                "ACKNOWLEDGMENT This research was supported in part by Israel Science Foundation grants #1211/04 and #898/05. 7.",
                "REFERENCES [1] L. V. Allis.",
                "A knowledge-based approach of Connect-Four - the game is solved: White wins.",
                "Masters thesis, Free University, Amsterdam, The Netherlands, 1988. [2] D. Carmel and S. Markovitch.",
                "Incorporating opponent models into adversary search.",
                "In Proceedings of the Thirteenth National Conference on Artificial Intelligence, pages 120-125, Portland, OR, 1996. [3] D. Carmel and S. Markovitch.",
                "Opponent modeling in multi-agent systems.",
                "In G. Weiß and S. Sen, editors, Adaptation and Learning in Multi-Agent Systems, pages 40-52.",
                "Springer-Verlag, 1996. [4] B. J. Grosz and S. Kraus.",
                "Collaborative plans for complex group action.",
                "Artificial Intelligence, 86(2):269-357, 1996. [5] M. Hadad, G. Kaminka, G. Armon, and S. Kraus.",
                "Supporting collaborative activity.",
                "In Proc. of AAAI-2005, pages 83-88, Pittsburgh, 2005. [6] http://www.gamerz.net/˜pbmserv/. [7] S. Kraus and D. Lehmann.",
                "Designing and building a negotiating automated agent.",
                "Computational Intelligence, 11:132-171, 1995. [8] H. J. Levesque, P. R. Cohen, and J. H. T. Nunes.",
                "On acting together.",
                "In Proc. of AAAI-90, pages 94-99, Boston, MA, 1990. [9] S. Markovitch and R. Reger.",
                "Learning and exploiting relative weaknesses of opponent agents.",
                "Autonomous Agents and Multi-Agent Systems, 10(2):103-130, 2005. [10] Y. M. Ronald Fagin, Joseph Y. Halpern and M. Y. Vardi.",
                "Reasoning about knowledge.",
                "MIT Press, Cambridge, Mass., 1995. [11] P. Thagard.",
                "Adversarial problem solving: Modeling an oponent using explanatory coherence.",
                "Cognitive Science, 16(1):123-149, 1992. [12] R. W. Toseland and R. F. Rivas.",
                "An Introduction to Group Work Practice.",
                "Prentice Hall, Englewood Cliffs, NJ, 2nd edition edition, 1995. [13] S. Willmott, J. Richardson, A. Bundy, and J. Levine.",
                "An adversarial planning approach to Go.",
                "Lecture Notes in Computer Science, 1558:93-112, 1999.",
                "The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) 557"
            ],
            "original_annotated_samples": [],
            "translated_annotated_samples": [
                "Eval: esta \"función de evaluación\" devuelve un valor de utilidad esperado estimado para un agente en A, después de completar una acción de CA en algún estado mundial w.Eval: A × CA × W → Definición 4. Función de evaluación",
                "TRH - (umbral) es una constante numérica en el rango [0,1] que representa un valor umbral de \"función de evaluación\" (eval).función de evaluación",
                "Hay dos propiedades importantes que deben mantener para la \"función de evaluación\": propiedad 1. Función de evaluación",
                "La \"función de evaluación\" debe indicar que el estado mundial más deseable es uno en el que se logra el objetivo.función de evaluación",
                "La \"función de evaluación\" debe proyectar una acción que cause una finalización de un objetivo o un subggoal a un valor que sea mayor que la TRH (una acción altamente beneficiosa).(∀ai, g ∗ ai ∈ Gai, α ∈ Cai, w ∈ W, g ∗ gai ∈ Ggai) alcanzar (g ∗ ai, α, w) ∨ alcanzar (g ∗ gai, α, w) ⇒ eval (ai,α, W) ≥ TRH.función de evaluación",
                "El uso de una estrategia de maximización de evaluación ingenua a una determinada profundidad de búsqueda no siempre producirá resultados satisfactorios por varias razones: (1) el problema del horizonte de búsqueda al buscar una profundidad fija;(2) la fuerte suposición de un adversario de recursos óptimamente racional e ilimitado;(3) Uso de una \"función de evaluación\" estimada que no dará resultados óptimos en todos los estados del mundo, y puede ser explotada [9].función de evaluación",
                "Sin embargo, siendo agentes racionales limitados y lidiar con una \"función de evaluación\" estimada, necesitamos axiomatizar explícitamente dicho comportamiento, ya que no siempre ocurrirá implícitamente de la \"función de evaluación\".función de evaluación",
                "Deje que AE AG sea un agente AE racional ilimitado que use la \"función de evaluación\" Heuristic Heuristic, AU AG sea el mismo agente que usa la función de utilidad verdadera, y AO es un único adversario racional basado en utilidades no resistentes.función de evaluación",
                "Ahora utilizamos nuestra \"función de evaluación\" estimada para evaluar las acciones de los jugadores negros durante la interacción conecta y cuatro adversas.función de evaluación"
            ],
            "translated_text": "",
            "candidates": [],
            "error": []
        },
        "beneficial action": {
            "translated_key": "",
            "is_in_text": true,
            "original_annotated_sentences": [
                "An Adversarial Environment Model for Bounded Rational Agents in Zero-Sum Interactions Inon Zuckerman1 , Sarit Kraus1 , Jeffrey S. Rosenschein2 , Gal Kaminka1 1 Department of Computer Science 2 The School of Engineering Bar-Ilan University and Computer Science Ramat-Gan, Israel Hebrew University, Jerusalem, Israel {zukermi,sarit,galk}@cs.biu.ac.il jeff@cs.huji.ac.il ABSTRACT Multiagent environments are often not cooperative nor collaborative; in many cases, agents have conflicting interests, leading to adversarial interactions.",
                "This paper presents a formal Adversarial Environment model for bounded rational agents operating in a zero-sum environment.",
                "In such environments, attempts to use classical utility-based search methods can raise a variety of difficulties (e.g., implicitly modeling the opponent as an omniscient utility maximizer, rather than leveraging a more nuanced, explicit opponent model).",
                "We define an Adversarial Environment by describing the mental states of an agent in such an environment.",
                "We then present behavioral axioms that are intended to serve as design principles for building such adversarial agents.",
                "We explore the application of our approach by analyzing log files of completed Connect-Four games, and present an empirical analysis of the axioms appropriateness.",
                "Categories and Subject Descriptors I.2.11 [Artificial Intelligence]: Distributed Artificial Intelligence-Intelligent agents,Multiagent Systems; I.2.4 [Artificial Intelligence]: Knowledge Representation Formalisms and Methods -Modal logic General Terms Design, Theory 1.",
                "INTRODUCTION Early research in multiagent systems (MAS) considered cooperative groups of agents; because individual agents had limited resources, or limited access to information (e.g., limited processing power, limited sensor coverage), they worked together by design to solve problems that individually they could not solve, or at least could not solve as efficiently.",
                "MAS research, however, soon began to consider interacting agents with individuated interests, as representatives of different humans or organizations with non-identical interests.",
                "When interactions are guided by diverse interests, participants may have to overcome disagreements, uncooperative interactions, and even intentional attempts to damage one another.",
                "When these types of interactions occur, environments require appropriate behavior from the agents situated in them.",
                "We call these environments Adversarial Environments, and call the clashing agents Adversaries.",
                "Models of cooperation and teamwork have been extensively explored in MAS through the axiomatization of mental states (e.g., [8, 4, 5]).",
                "However, none of this research dealt with adversarial domains and their implications for agent behavior.",
                "Our paper addresses this issue by providing a formal, axiomatized mental state model for a subset of adversarial domains, namely simple zero-sum adversarial environments.",
                "Simple zero-sum encounters exist of course in various twoplayer games (e.g., Chess, Checkers), but they also exist in n-player games (e.g., Risk, Diplomacy), auctions for a single good, and elsewhere.",
                "In these latter environments especially, using a utility-based adversarial search (such as the Min-Max algorithm) does not always provide an adequate solution; the payoff function might be quite complex or difficult to quantify, and there are natural computational limitations on bounded rational agents.",
                "In addition, traditional search methods (like Min-Max) do not make use of a model of the opponent, which has proven to be a valuable addition to adversarial planning [9, 3, 11].",
                "In this paper, we develop a formal, axiomatized model for bounded rational agents that are situated in a zero-sum adversarial environment.",
                "The model uses different modality operators, and its main foundations are the SharedPlans [4] model for collaborative behavior.",
                "We explore environment properties and the mental states of agents to derive behavioral axioms; these behavioral axioms constitute a formal model that serves as a specification and design guideline for agent design in such settings.",
                "We then investigate the behavior of our model empirically using the Connect-Four board game.",
                "We show that this game conforms to our environment definition, and analyze players behavior using a large set of completed match log 550 978-81-904262-7-5 (RPS) c 2007 IFAAMAS files.",
                "In addition, we use the results presented in [9] to discuss the importance of opponent modeling in our ConnectFour adversarial domain.",
                "The paper proceeds as follows.",
                "Section 2 presents the models formalization.",
                "Section 3 presents the empirical analysis and its results.",
                "We discuss related work in Section 4, and conclude and present future directions in Section 5. 2.",
                "ADVERSARIAL ENVIRONMENTS The adversarial environment model (denoted as AE) is intended to guide the design of agents by providing a specification of the capabilities and mental attitudes of an agent in an adversarial environment.",
                "We focus here on specific types of adversarial environments, specified as follows: 1.",
                "Zero-Sum Interactions: positive and negative utilities of all agents sum to zero; 2.",
                "Simple AEs: all agents in the environment are adversarial agents; 3.",
                "Bilateral AEs: AEs with exactly two agents; 4.",
                "Multilateral AEs: AEs of three or more agents.",
                "We will work on both bilateral and multilateral instantiations of zero-sum and simple environments.",
                "In particular, our adversarial environment model will deal with interactions that consist of N agents (N ≥ 2), where all agents are adversaries, and only one agent can succeed.",
                "Examples of such environments range from board games (e.g., Chess, Connect-Four, and Diplomacy) to certain economic environments (e.g., N-bidder auctions over a single good). 2.1 Model Overview Our approach is to formalize the mental attitudes and behaviors of a single adversarial agent; we consider how a single agent perceives the AE.",
                "The following list specifies the conditions and mental states of an agent in a simple, zero-sum AE: 1.",
                "The agent has an individual intention that its own goal will be completed; 2.",
                "The agent has an individual belief that it and its adversaries are pursuing full conflicting goals (defined below)there can be only one winner; 3.",
                "The agent has an individual belief that each adversary has an intention to complete its own full conflicting goal; 4.",
                "The agent has an individual belief in the (partial) profile of its adversaries.",
                "Item 3 is required, since it might be the case that some agent has a full conflicting goal, and is currently considering adopting the intention to complete it, but is, as of yet, not committed to achieving it.",
                "This might occur because the agent has not yet deliberated about the effects that adopting that intention might have on the other intentions it is currently holding.",
                "In such cases, it might not consider itself to even be in an adversarial environment.",
                "Item 4 states that the agent should hold some belief about the profiles of its adversaries.",
                "The profile represents all the knowledge the agent has about its adversary: its weaknesses, strategic capabilities, goals, intentions, trustworthiness, and more.",
                "It can be given explicitly or can be learned from observations of past encounters. 2.2 Model Definitions for Mental States We use Grosz and Krauss definitions of the modal operators, predicates, and meta-predicates, as defined in their SharedPlan formalization [4].",
                "We recall here some of the predicates and operators that are used in that formalization: Int.To(Ai, α, Tn, Tα, C) represents Ais intentions at time Tn to do an action α at time Tα in the context of C. Int.Th(Ai, prop, Tn, Tprop, C) represents Ais intentions at time Tn that a certain proposition prop holds at time Tprop in the context of C. The potential intention operators, Pot.Int.To(...) and Pot.Int.Th(...), are used to represent the mental state when an agent considers adopting an intention, but has not deliberated about the interaction of the other intentions it holds.",
                "The operator Bel(Ai, f, Tf ) represents agent Ai believing in the statement expressed in formula f, at time Tf .",
                "MB(A, f, Tf ) represents mutual belief for a group of agents A.",
                "A snapshot of the system finds our environment to be in some state e ∈ E of environmental variable states, and each adversary in any LAi ∈ L of possible local states.",
                "At any given time step, the system will be in some world w of the set of all possible worlds w ∈ W, where w = E×LA1 ×LA2 × ...LAn , and n is the number of adversaries.",
                "For example, in a Texas Holdem poker game, an agents local state might be its own set of cards (which is unknown to its adversary) while the environment will consist of the betting pot and the community cards (which are visible to both players).",
                "A utility function under this formalization is defined as a mapping from a possible world w ∈ W to an element in , which expresses the desirability of the world, from a single agent perspective.",
                "We usually normalize the range to [0,1], where 0 represents the least desirable possible world, and 1 is the most desirable world.",
                "The implementation of the utility function is dependent on the domain in question.",
                "The following list specifies new predicates, functions, variables, and constants used in conjunction with the original definitions for the adversarial environment formalization: 1. φ is a null action (the agent does not do anything). 2.",
                "GAi is the set of agent Ais goals.",
                "Each goal is a set of predicates whose satisfaction makes the goal complete (we use G∗ Ai ∈ GAi to represent an arbitrary goal of agent Ai). 3. gAi is the set of agent Ais subgoals.",
                "Subgoals are predicates whose satisfaction represents an important milestone toward achievement of the full goal. gG∗ Ai ⊆ gAi is the set of subgoals that are important to the completion of goal G∗ Ai (we will use g∗ G∗ Ai ∈ gG∗ Ai to represent an arbitrary subgoal). 4.",
                "P Aj Ai is the profile object agent Ai holds about agent Aj. 5.",
                "CA is a general set of actions for all agents in A which are derived from the environments constraints.",
                "CAi ⊆ CA is the set of agent Ais possible actions. 6.",
                "Do(Ai, α, Tα, w) holds when Ai performs action α over time interval Tα in world w. 7.",
                "Achieve(G∗ Ai , α, w) is true when goal G∗ Ai is achieved following the completion of action α in world w ∈ W, where α ∈ CAi . 8.",
                "Profile(Ai, PAi Ai ) is true when agent Ai holds an object profile for agent Aj.",
                "Definition 1.",
                "Full conflict (FulConf ) describes a zerosum interaction where only a single goal of the goals in conflict can be completed.",
                "FulConf(G∗ Ai , G∗ Aj ) ⇒ (∃α ∈ CAi , ∀w, β ∈ CAj ) (Achieve(G∗ Ai , α, w) ⇒ ¬Achieve(G∗ Aj , β, w)) ∨ (∃β ∈ CAj , ∀w, α ∈ CAi )(Achieve(G∗ Aj , β, w) ⇒ ¬Achieve(G∗ Ai , α, w)) Definition 2.",
                "Adversarial Knowledge (AdvKnow) is a function returning a value which represents the amount of The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) 551 knowledge agent Ai has on the profile of agent Aj, at time Tn.",
                "The higher the value, the more knowledge agent Ai has.",
                "AdvKnow : P Aj Ai × Tn → Definition 3.",
                "Eval - This evaluation function returns an estimated expected utility value for an agent in A, after completing an action from CA in some world state w. Eval : A × CA × w → Definition 4.",
                "TrH - (Threshold) is a numerical constant in the [0,1] range that represents an evaluation function (Eval) threshold value.",
                "An action that yields an estimated utility evaluation above the TrH is regarded as a highly <br>beneficial action</br>.",
                "The Eval value is an estimation and not the real utility function, which is usually unknown.",
                "Using the real utility value for a rational agent would easily yield the best outcome for that agent.",
                "However, agents usually do not have the real utility functions, but rather a heuristic estimate of it.",
                "There are two important properties that should hold for the evaluation function: Property 1.",
                "The evaluation function should state that the most desirable world state is one in which the goal is achieved.",
                "Therefore, after the goal has been satisfied, there can be no future action that can put the agent in a world state with higher Eval value. (∀Ai, G∗ Ai , α, β ∈ CAi , w ∈ W) Achieve(G∗ Ai , α, w) ⇒ Eval(Ai, α, w) ≥ Eval(Ai, β, w) Property 2.",
                "The evaluation function should project an action that causes a completion of a goal or a subgoal to a value which is greater than TrH (a highly <br>beneficial action</br>). (∀Ai, G∗ Ai ∈ GAi , α ∈ CAi , w ∈ W, g∗ GAi ∈ gGAi ) Achieve(G∗ Ai , α, w) ∨ Achieve(g∗ GAi , α, w) ⇒ Eval(Ai, α, w) ≥ TrH.",
                "Definition 5.",
                "SetAction We define a set action (SetAction) as a set of action operations (either complex or basic actions) from some action sets CAi and CAj which, according to agent Ais belief, are attached together by a temporal and consequential relationship, forming a chain of events (action, and its following consequent action). (∀α1 , . . . , αu ∈ CAi , β1 , . . . , βv ∈ CAj , w ∈ W) SetAction(α1 , . . . , αu , β1 , . . . , βv , w) ⇒ ((Do(Ai, α1 , Tα1 , w) ⇒ Do(Aj, β1 , Tβ1 , w)) ⇒ Do(Ai, α2 , Tα2 , w) ⇒ . . . ⇒ Do(Ai, αu , Tαu , w)) The consequential relation might exist due to various environmental constraints (when one action forces the adversary to respond with a specific action) or due to the agents knowledge about the profile of its adversary.",
                "Property 3.",
                "As the knowledge we have about our adversary increases we will have additional beliefs about its behavior in different situations which in turn creates new set actions.",
                "Formally, if our AdvKnow at time Tn+1 is greater than AdvKnow at time Tn, then every SetAction known at time Tn is also known at time Tn+1.",
                "AdvKnow(P Aj Ai , Tn+1) > AdvKnow(P Aj Ai , Tn) ⇒ (∀α1 , . . . , αu ∈ CAi , β1 , . . . , βv ∈ CAj ) Bel(Aag, SetAction(α1 , . . . , αu , β1 , . . . , βv ), Tn) ⇒ Bel(Aag, SetAction(α1 , . . . , αu , β1 , . . . , βv ), Tn+1) 2.3 The Environment Formulation The following axioms provide the formal definition for a simple, zero-sum Adversarial Environment (AE).",
                "Satisfaction of these axioms means that the agent is situated in such an environment.",
                "It provides specifications for agent Aag to interact with its set of adversaries A with respect to goals G∗ Aag and G∗ A at time TCo at some world state w. AE(Aag, A, G∗ Aag , A1, . . . , Ak, G∗ A1 , . . . , G∗ Ak , Tn, w) 1.",
                "Aag has an Int.Th his goal would be completed: (∃α ∈ CAag , Tα) Int.Th(Aag, Achieve(G∗ Aag , α), Tn, Tα, AE) 2.",
                "Aag believes that it and each of its adversaries Ao are pursuing full conflicting goals: (∀Ao ∈ {A1, . . . , Ak}) Bel(Aag, FulConf(G∗ Aag , G∗ Ao ), Tn) 3.",
                "Aag believes that each of his adversaries in Ao has the Int.Th his conflicting goal G∗ Aoi will be completed: (∀Ao ∈ {A1, . . . , Ak})(∃β ∈ CAo , Tβ) Bel(Aag, Int.Th(Ao, Achieve(G∗ Ao , β), TCo, Tβ, AE), Tn) 4.",
                "Aag has beliefs about the (partial) profiles of its adversaries (∀Ao ∈ {A1, . . . , Ak}) (∃PAo Aag ∈ PAag )Bel(Aag, Profile(Ao, PAo Aag ), Tn) To build an agent that will be able to operate successfully within such an AE, we must specify behavioral guidelines for its interactions.",
                "Using a naive Eval maximization strategy to a certain search depth will not always yield satisfactory results for several reasons: (1) the search horizon problem when searching for a fixed depth; (2) the strong assumption of an optimally rational, unbounded resources adversary; (3) using an estimated evaluation function which will not give optimal results in all world states, and can be exploited [9].",
                "The following axioms specify the behavioral principles that can be used to differentiate between successful and less successful agents in the above Adversarial Environment.",
                "Those axioms should be used as specification principles when designing and implementing agents that should be able to perform well in such Adversarial Environments.",
                "The behavioral axioms represent situations in which the agent will adopt potential intentions to (Pot.Int.To(...)) perform an action, which will typically require some means-end reasoning to select a possible course of action.",
                "This reasoning will lead to the adoption of an Int.To(...) (see [4]).",
                "A1.",
                "Goal Achieving Axiom.",
                "The first axiom is the simplest case; when the agent Aag believes that it is one action (α) away from achieving his conflicting goal G∗ Aag , it should adopt the potential intention to do α and complete its goal. (∀Aag, α ∈ CAag , Tn, Tα, w ∈ W) (Bel(Aag, Do(Aag, α, Tα, w) ⇒ Achieve(G∗ Aag , α, w)) ⇒ Pot.Int.To(Aag, α, Tn, Tα, w) This somewhat trivial behavior is the first and strongest axiom.",
                "In any situation, when the agent is an action away from completing the goal, it should complete the action.",
                "Any fair Eval function would naturally classify α as the maximal value action (property 1).",
                "However, without explicit axiomatization of such behavior there might be situations where the agent will decide on taking another action for various reasons, due to its bounded decision resources.",
                "A2.",
                "Preventive Act Axiom.",
                "Being in an adversarial situation, agent Aag might decide to take actions that will damage one of its adversarys plans to complete its goal, even if those actions do not explicitly advance Aag towards its conflicting goal G∗ Aag .",
                "Such preventive action will take place when agent Aag has a belief about the possibility of its adversary Ao doing an action β that will give it a high 552 The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) utility evaluation value (> TrH).",
                "Believing that taking action α will prevent the opponent from doing its β, it will adopt a potential intention to do α. (∀Aag, Ao ∈ A, α ∈ CAag , β ∈ CAo , Tn, Tβ, w ∈ W) (Bel(Aag, Do(Ao, β, Tβ, w) ∧ Eval(Ao, β, w) > TrH, Tn) ∧ Bel(Aag, Do(Aag, α, Tα, w) ⇒ ¬Do(Ao, β, Tβ, w), Tn) ⇒ Pot.Int.To(Aag, α, Tn, Tα, w) This axiom is a basic component of any adversarial environment.",
                "For example, looking at a Chess board game, a player could realize that it is about to be checkmated by its opponent, thus making a preventive move.",
                "Another example is a Connect Four game: when a player has a row of three chips, its opponent must block it, or lose.",
                "A specific instance of A1 occurs when the adversary is one action away from achieving its goal, and immediate preventive action needs to be taken by the agent.",
                "Formally, we have the same beliefs as stated above, with a changed belief that doing action β will cause agent Ao to achieve its goal.",
                "Proposition 1: Prevent or lose case. (∀Aag, Ao ∈ A, α ∈ CAag , β ∈ CAo , G∗ Ao , Tn, Tα, Tβ, w ∈ W) Bel(Aag, Do(Ao, β, Tβ, w) ⇒ Achieve(G∗ Ao , β, w), Tn) ∧ Bel(Aag, Do(Aag, α, Tα, w) ⇒ ¬Do(Ao, β, Tβ, w)) ⇒ Pot.Int.To(Aag, α, Tn, Tα, w) Sketch of proof: Proposition 1 can be easily derived from axiom A1 and the property 2 of the Eval function, which states that any action that causes a completion of a goal is a highly <br>beneficial action</br>.",
                "The preventive act behavior will occur implicitly when the Eval function is equal to the real world utility function.",
                "However, being bounded rational agents and dealing with an estimated evaluation function we need to explicitly axiomatize such behavior, for it will not always occur implicitly from the evaluation function.",
                "A3.",
                "Suboptimal Tactical Move Axiom.",
                "In many scenarios a situation may occur where an agent will decide not to take the current most <br>beneficial action</br> it can take (the action with the maximal utility evaluation value), because it believes that taking another action (with lower utility evaluation value) might yield (depending on the adversarys response) a future possibility for a highly <br>beneficial action</br>.",
                "This will occur most often when the Eval function is inaccurate and differs by a large extent from the Utility function.",
                "Put formally, agent Aag believes in a certain SetAction that will evolve according to its initial action and will yield a high beneficial value (> TrH) solely for it. (∀Aag, Ao ∈ A, Tn, w ∈ W) (∃α1 , . . . , αu ∈ CAi , β1 , . . . , βv ∈ CAj , Tα1 ) Bel(Aag, SetAction(α1 , . . . , αu , β1 , . . . , βv ), Tn) ∧ Bel(Aag, Eval(Ao, βv , w) < TrH < Eval(Aag, αu , w), Tn) ⇒ Pot.Int.To(Aag, α1 , Tn, Tα1 , w) An agent might believe that a chain of events will occur for various reasons due to the inevitable nature of the domain.",
                "For example, in Chess, we often observe the following: a move causes a check position, which in turn limits the opponents moves to avoiding the check, to which the first player might react with another check, and so on.",
                "The agent might also believe in a chain of events based on its knowledge of its adversarys profile, which allows it to foresee the adversarys movements with high accuracy.",
                "A4.",
                "Profile Detection Axiom.",
                "The agent can adjust its adversarys profiles by observations and pattern study (specifically, if there are repeated encounters with the same adversary).",
                "However, instead of waiting for profile information to be revealed, an agent can also initiate actions that will force its adversary to react in a way that will reveal profile knowledge about it.",
                "Formally, the axiom states that if all actions (γ) are not highly beneficial actions (< TrH), the agent can do action α in time Tα if it believes that it will result in a non-highly <br>beneficial action</br> β from its adversary, which in turn teaches it about the adversarys profile, i.e., gives a higher AdvKnow(P Aj Ai , Tβ). (∀Aag, Ao ∈ A, α ∈ CAag , β ∈ CAo , Tn, Tα, Tβ, w ∈ W) Bel(Aag, (∀γ ∈ CAag )Eval(Aag, γ, w) < TrH, Tn) ∧ Bel(Aag, Do(Aag, α, Tα, w) ⇒ Do(Ao, β, Tβ, w), Tn) ∧ Bel(Aag, Eval(Ao, β, w) < TrH) ∧ Bel(Aag, AdvKnow(P Aj Ai , Tβ) > AdvKnow(P Aj Ai , Tn), Tn) ⇒ Pot.Int.To(Aag, α, Tn, Tα, w) For example, going back to the Chess board game scenario, consider starting a game versus an opponent about whom we know nothing, not even if it is a human or a computerized opponent.",
                "We might start playing a strategy that will be suitable versus an average opponent, and adjust our game according to its level of play.",
                "A5.",
                "Alliance Formation Axiom The following behavioral axiom is relevant only in a multilateral instantiation of the adversarial environment (obviously, an alliance cannot be formed in a bilateral, zero-sum encounter).",
                "In different situations during a multilateral interaction, a group of agents might believe that it is in their best interests to form a temporary alliance.",
                "Such an alliance is an agreement that constrains its members behavior, but is believed by its members to enable them to achieve a higher utility value than the one achievable outside of the alliance.",
                "As an example, we can look at the classical Risk board game, where each player has an individual goal of being the sole conquerer of the world, a zero-sum game.",
                "However, in order to achieve this goal, it might be strategically wise to make short-term ceasefire agreements with other players, or to join forces and attack an opponent who is stronger than the rest.",
                "An alliances terms defines the way its members should act.",
                "It is a set of predicates, denoted as Terms, that is agreed upon by the alliance members, and should remain true for the duration of the alliance.",
                "For example, the set Terms in the Risk scenario, could contain the following predicates: 1.",
                "Alliance members will not attack each other on territories X, Y and Z; 2.",
                "Alliance members will contribute C units per turn for attacking adversary Ao; 3.",
                "Members are obligated to stay as part of the alliance until time Tk or until adversarys Ao army is smaller than Q.",
                "The set Terms specifies inter-group constraints on each of the alliance members (∀Aal i ∈ Aal ⊆ A) set of actions Cal i ⊆ C. Definition 6.",
                "Al val - the total evaluation value that agent Ai will achieve while being part of Aal is the sum of Evali (Eval for Ai) of each of Aal j Eval values after taking their own α actions (via the agent(α) predicate): Al val(Ai, Cal , Aal , w) = α∈Cal Evali(Aal j , agent(α), w) Definition 7.",
                "Al TrH - is a number representing an Al val The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) 553 threshold; above it, the alliance can be said to be a highly beneficial alliance.",
                "The value of Al TrH will be calculated dynamically according to the progress of the interaction, as can be seen in [7].",
                "After an alliance is formed, its members are now working in their normal adversarial environment, as well as according to the mental states and axioms required for their interactions as part of the alliance.",
                "The following Alliance model (AL) specifies the conditions under which the group Aal can be said to be in an alliance and working with a new and constrained set of actions Cal , at time Tn.",
                "AL(Aal , Cal , w, Tn) 1.",
                "Aal has a MB that all members are part of Aal : MB(Aal , (∀Aal i ∈ Aal )member(Aal i , Aal ), Tn) 2.",
                "Aal has a MB that the group be maintained: MB(Aal , (∀Aal i ∈ Aal )Int.Th (Ai, member(Ai, Aal ), Tn, Tn+1, Co), Tn) 3.",
                "Aal has a MB that being members gives them high utility value: MB(Aal , (∀Aal i ∈ Aal )Al val(Aal i , Cal , Aal , w) ≥ Al TrH, Tn) Members profiles are a crucial part of successful alliances.",
                "We assume that agents that have more accurate profiles of their adversaries will be more successful in such environments.",
                "Such agents will be able to predict when a member is about to breach the alliances contract (item 2 in the above model), and take counter measures (when item 3 will falsify).",
                "The robustness of the alliance is in part a function of its members trustfulness measure, objective position estimation, and other profile properties.",
                "We should note that an agent can simultaneously be part of more than one alliance.",
                "Such a temporary alliance, where the group members do not have a joint goal but act collaboratively for the interest of their own individual goals, is classified as a Treatment Group by modern psychologists [12] (in contrast to a Task Group, where its members have a joint goal).",
                "The Shared Activity model as presented in [5] modeled Treatment Group behavior using the same SharedPlans formalization.",
                "When comparing both definitions of an alliance and a Treatment Group we found an unsurprising resemblance between both models: the environment models definitions are almost identical (see SAs definitions in [5]), and their Selfish-Act and Cooperative Act axioms conform to our adversarial agents behavior.",
                "The main distinction between both models is the integration of a Helpful-behavior act axiom, in the Shared Activity which cannot be part of ours.",
                "This axiom states that an agent will consider taking action that will lower its Eval value (to a certain lower bound), if it believes that a group partner will gain a significant benefit.",
                "Such behavior cannot occur in a pure adversarial environment (as a zero-sum game is), where the alliance members are constantly on watch to manipulate their alliance to their own advantage.",
                "A6.",
                "Evaluation Maximization Axiom.",
                "In a case when all other axioms are inapplicable, we will proceed with the action that maximizes the heuristic value as computed in the Eval function. (∀Aag, Ao ∈ A, α ∈ Cag, Tn, w ∈ W) Bel(Aag, (∀γ ∈ Cag)Eval(Aag, α, w) ≥ Eval(Aag, γ, w), Tn) ⇒ Pot.Int.To(Aag, α, Tn, Tα, w) T1.",
                "Optimality on Eval = Utility The above axiomatic model handles situations where the Utility is unknown and the agents are bounded rational agents.",
                "The following theorem shows that in bilateral interactions, where the agents have the real Utility function (i.e., Eval = Utility) and are rational agents, the axioms provide the same optimal result as classic adversarial search (e.g., Min-Max).",
                "Theorem 1.",
                "Let Ae ag be an unbounded rational AE agent using the Eval heuristic evaluation function, Au ag be the same agent using the true Utility function, and Ao be a sole unbounded utility-based rational adversary.",
                "Given that Eval = Utility: (∀α ∈ CAu ag , α ∈ CAe ag , Tn, w ∈ W) Pot.Int.To(Au ag, α, Tn, Tα, w) → Pot.Int.To(Ae ag, α , Tn, Tα, w) ∧ ((α = α ) ∨ (Utility(Au ag, α, w) = Eval(Ae ag, α , w))) Sketch of proof - Given that Au ag has the real utility function and unbounded resources, it can generate the full game tree and run the optimal MinMax algorithm to choose the highest utility value action, which we denote by, α.",
                "The proof will show that Ae ag, using the AE axioms, will select the same or equal utility α (when there is more than one action with the same max utility) when Eval = Utility. (A1) Goal achieving axiom - suppose there is an α such that its completion will achieve Au ags goal.",
                "It will obtain the highest utility by Min-Max for Au ag.",
                "The Ae ag agent will select α or another action with the same utility value via A1.",
                "If such α does not exist, Ae ag cannot apply this axiom, and proceeds to A2. (A2) Preventive act axiom - (1) Looking at the basic case (see Prop1), if there is a β which leads Ao to achieve its goal, then a preventive action α will yield the highest utility for Au ag.",
                "Au ag will choose it through the utility, while Ae ag will choose it through A2. (2) In the general case, β is a highly <br>beneficial action</br> for Ao, thus yields low utility for Au ag, which will guide it to select an α that will prevent β, while Ae ag will choose it through A2.1 If such β does not exist for Ao, then A2 is not applicable, and Ae ag can proceed to A3. (A3) Suboptimal tactical move axiom - When using a heuristic Eval function, Ae ag has a partial belief in the profile of its adversary (item 4 in AE model), which may lead it to believe in SetActions (Prop1).",
                "In our case, Ae ag is holding a full profile on its optimal adversary and knows that Ao will behave optimally according to the real utility values on the complete search tree, therefore, any belief about suboptimal SetAction cannot exist, yielding this axiom inapplicable.",
                "Ae ag will proceed to A4. (A4) Profile detection axiom - Given that Ae ag has the full profile of Ao, none of Ae ags actions can increase its knowledge.",
                "That axiom will not be applied, and the agent will proceed with A6 (A5 will be disregarded because the interaction is bilateral). (A6) Evaluation maximization axiom - This axiom will select the max Eval for Ae ag.",
                "Given that Eval = Utility, the same α that was selected by Au ag will be selected. 3.",
                "EVALUATION The main purpose of our experimental analysis is to evaluate the models behavior and performance in a real adversarial environment.",
                "This section investigates whether bounded 1 A case where following the completion of β there exists a γ which gives high utility for Agent Au ag, cannot occur because Ao uses the same utility, and γs existence will cause it to classify β as a low utility action. 554 The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) rational agents situated in such adversarial environments will be better off applying our suggested behavioral axioms. 3.1 The Domain To explore the use of the above model and its behavioral axioms, we decided to use the Connect-Four game as our adversarial environment.",
                "Connect-Four is a 2-player, zerosum game which is played using a 6x7 matrix-like board.",
                "Each turn, a player drops a disc into one of the 7 columns (the set of 21 discs is usually colored yellow for player 1 and red for player 2; we will use White and Black respectively to avoid confusion).",
                "The winner is the first player to complete a horizontal, vertical, or diagonal set of four discs with its color.",
                "On very rare occasions, the game might end in a tie if all the empty grids are filled, but no player managed to create a 4-disc set.",
                "The Connect-Four game was solved in [1], where it is shown that the first player (playing with the white discs) can force a win by starting in the middle column (column 4) and playing optimally However, the optimal strategy is very complex, and difficult to follow even for complex bounded rational agents, such as human players.",
                "Before we can proceed checking agent behavior, we must first verify that the domain conforms to the adversarial environments definition as given above (which the behavioral axioms are based on).",
                "First, when playing a Connect-Four game, the agent has an intention to win the game (item 1).",
                "Second (item 2), our agent believes that in Connect-Four there can only be one winner (or no winner at all in the rare occurrence of a tie).",
                "In addition, our agent believes that its opponent to the game will try to win (item 3), and we hope it has some partial knowledge (item 4) about its adversary (this knowledge can vary from nothing, through simple facts such as age, to strategies and weaknesses).",
                "Of course, not all Connect-Four encounters are adversarial.",
                "For example, when a parent is playing the game with its child, the following situation might occur: the child, having a strong incentive to win, treats the environment as adversarial (it intends to win, understands that there can only be one winner, and believes that its parent is trying to beat him).",
                "However, the parents point of view might see the environment as an educational one, where its goal is not to win the game, but to cause enjoyment or practice strategic reasoning.",
                "In such an educational environment, a new set of behavioral axioms might be more beneficial to the parents goals than our suggested adversarial behavioral axioms. 3.2 Axiom Analysis After showing that the Connect-Four game is indeed a zero-sum, bilateral adversarial environment, the next step is to look at players behaviors during the game and check whether behaving according to our model does improve performance.",
                "To do so we have collected log files from completed Connect-Four games that were played by human players over the Internet.",
                "Our collected log file data came from Play by eMail (PBeM) sites.",
                "These are web sites that host email games, where each move is taken by an email exchange between the server and the players.",
                "Many such sites archives contain real competitive interactions, and also maintain a ranking system for their members.",
                "Most of the data we used can be found in [6].",
                "As can be learned from [1], Connect-Four has an optimal strategy and a considerable advantage for the player who starts the game (which we call the White player).",
                "We will concentrate in our analysis on the second players moves (to be called Black).",
                "The White player, being the first to act, has the so-called initiative advantage.",
                "Having the advantage and a good strategy will keep the Black player busy reacting to its moves, instead of initiating threats.",
                "A threat is a combination of three discs of the same color, with an empty spot for the fourth winning disk.",
                "An open threat is a threat that can be realized in the opponents next move.",
                "In order for the Black player to win, it must somehow turn the tide, take the advantage and start presenting threats to the White player.",
                "We will explore Black players behavior and their conformance to our axioms.",
                "To do so, we built an application that reads log files and analyzes the Black players moves.",
                "The application contains two main components: (1) a Min-Max algorithm for evaluation of moves; (2) open threats detector for the discovering of open threats.",
                "The Min-Max algorithm will work to a given depth, d and for each move α will output the heuristic value for the next action taken by the player as written in the log file, h(α), alongside the maximum heuristic value, maxh(α), that could be achieved prior to taking the move (obviously, if h(α) = maxh(α), then the player did not do the optimal move heuristically).",
                "The threat detectors job is to notify if some action was taken in order to block an open threat (not blocking an open threat will probably cause the player to lose in the opponents next move).",
                "The heuristic function used by Min-Max to evaluate the players utility is the following function, which is simple to compute, yet provides a reasonable challenge to human opponents: Definition 8.",
                "Let Group be an adjacent set of four squares that are horizontal, vertical, or diagonal.",
                "Groupn b (Groupn w) be a Group with n pieces of the black (white) color and 4−n empty squares. h = ((Group1 b ∗α)+(Group2 b ∗β)+(Group3 b ∗γ)+(Group4 b ∗∞)) − ((Group1 w ∗α)+(Group2 w ∗β)+(Group3 w ∗γ)+(Group4 w ∗∞)) The values of α, β and δ can vary to form any desired linear combination; however, it is important to value them with the α < β < δ ordering in mind (we used 1, 4, and 8 as their respective values).",
                "Groups of 4 discs of the same color means victory, thus discovery of such a group will result in ∞ to ensure an extreme value.",
                "We now use our estimated evaluation function to evaluate the Black players actions during the Connect-Four adversarial interaction.",
                "Each game from the log file was input into the application, which processed and output a reformatted log file containing the h value of the current move, the maxh value that could be achieved, and a notification if an open threat was detected.",
                "A total of 123 games were analyzed (57 with White winning, and 66 with Black winning).",
                "A few additional games were manually ignored in the experiment, due to these problems: a player abandoning the game while the outcome is not final, or a blunt irrational move in the early stages of the game (e.g., not blocking an obvious winning group in the first opening moves).",
                "In addition, a single tie game was also removed.",
                "The simulator was run to a search depth of 3 moves.",
                "We now proceed to analyze the games with respect to each behavioral axiom.",
                "The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) 555 Table 1: Average heuristic difference analysis Black losses Black Won Avg minh -17.62 -12.02 Avg 3 lowest h moves (min3 h) -13.20 -8.70 3.2.1 Affirming the Suboptimal tactical move axiom The following section presents the heuristic evaluations of the Min-Max algorithm for each action, and checks the amount and extent of suboptimal tactical actions and their implications on performance.",
                "Table 1 shows results and insights from the games heuristic analysis, when search depth equals 3 (this search depth was selected for the results to be comparable to [9], see Section 3.2.3).",
                "The tables heuristic data is the difference between the present maximal heuristic value and the heuristic value of the action that was eventually taken by the player (i.e., the closer the number is to 0, the closer the action was to the maximum heuristic action).",
                "The first row presents the difference values of the action that had the maximal difference value among all the Black players actions in a given game, as averaged over all Blacks winning and losing games (see respective columns).",
                "In games in which the Black player loses, its average difference value was -17.62, while in games in which the Black player won, its average was -12.02.",
                "The second row expands the analysis by considering the 3 highest heuristic difference actions, and averaging them.",
                "In that case, we notice an average heuristic difference of 5 points between games which the Black player loses and games in which it wins.",
                "Nevertheless, the importance of those numbers is that they allowed us to take an educated guess on a threshold number of 11.5, as the value of the TrH constant, which differentiates between normal actions and highly beneficial ones.",
                "After finding an approximated TrH constant, we can proceed with an analysis of the importance of suboptimal moves.",
                "To do so we took the subset of games in which the minimum heuristic difference value for Blacks actions was 11.5.",
                "As presented in Table 2, we can see the different min3 h average of the 3 largest ranges and the respective percentage of games won.",
                "The first row shows that the Black player won only 12% of the games in which the average of its 3 highest heuristically difference actions (min3 h) was smaller than the suggested threshold, TrH = 11.5.",
                "The second row shows a surprising result: it seems that when min3 h > −4 the Black player rarely wins.",
                "Intuition would suggest that games in which the action evaluation values were closer to the maximal values will result in more winning games for Black.",
                "However, it seems that in the Connect-Four domain, merely responding with somewhat easily expected actions, without initiating a few surprising and suboptimal moves, does not yield good results.",
                "The last row sums up the main insights from the analysis; most of Blacks wins (83%) came when its min3 h was in the range of -11.5 to -4.",
                "A close inspection of those Black winning games shows the following pattern behind the numbers: after standard opening moves, Black suddenly drops a disc into an isolated column, which seems a waste of a move.",
                "White continues to build its threats, while usually disregarding Blacks last move, which in turn uses the isolated disc as an anchor for a future winning threat.",
                "The results show that it was beneficial for the Black player Table 2: Blacks winnings percentages % of games min3 h < −11.5 12% min3 h > −4 5% −11.5 ≤ min3 h ≤ −4 83% to take suboptimal actions and not give the current highest possible heuristic value, but will not be too harmful for its position (i.e., will not give high beneficial value to its adversary).",
                "As it turned out, learning the threshold is an important aspect of success: taking wildly risky moves (min3 h < −11.5) or trying to avoid them (min3 h > −4) reduces the Black players winning chances by a large margin. 3.2.2 Affirming the Profile Monitoring Axiom In the task of showing the importance of monitoring ones adversaries profiles, our log files could not be used because they did not contain repeated interactions between players, which are needed to infer the players knowledge about their adversaries.",
                "However, the importance of opponent modeling and its use in attaining tactical advantages was already studied in various domains ([3, 9] are good examples).",
                "In a recent paper, Markovitch and Reger [9] explored the notion of learning and exploitation of opponent weakness in competitive interactions.",
                "They apply simple learning strategies by analyzing examples from past interactions in a specific domain.",
                "They also used the Connect-Four adversarial domain, which can now be used to understand the importance of monitoring the adversarys profile.",
                "Following the presentation of their theoretical model, they describe an extensive empirical study and check the agents performance after learning the weakness model with past examples.",
                "One of the domains used as a competitive environment was the same Connect-Four game (Checkers was the second domain).",
                "Their heuristic function was identical to ours with three different variations (H1, H2, and H3) that are distinguished from one another in their linear combination coefficient values.",
                "The search depth for the players was 3 (as in our analysis).",
                "Their extensive experiments check and compare various learning strategies, risk factors, predefined feature sets and usage methods.",
                "The bottom line is that the Connect-Four domain shows an improvement from a 0.556 winning rate before modeling to a 0.69 after modeling (page 22).",
                "Their conclusions, showing improved performance when holding and using the adversarys model, justify the effort to monitor the adversary profile for continuous and repeated interactions.",
                "An additional point that came up in their experiments is the following: after the opponent weakness model has been learned, the authors describe different methods of integrating the opponent weakness model into the agents decision strategy.",
                "Nevertheless, regardless of the specific method they chose to work with, all integration methods might cause the agent to take suboptimal decisions; it might cause the agent to prefer actions that are suboptimal at the present decision junction, but which might cause the opponent to react in accordance with its weakness model (as represented by our agent) which in turn will be beneficial for us in the future.",
                "The agents behavior, as demonstrated in [9] further confirms and strengthens our Suboptimal Tactical Axiom as discussed in the previous section. 556 The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) 3.2.3 Additional Insights The need for the Goal Achieving, Preventive Act, and Evaluation Maximization axioms are obvious, and need no further verification.",
                "However, even with respect to those axioms, a few interesting insights came up in the log analysis.",
                "The Goal achieving and Preventive Act axioms, though theoretically trivial, seem to provide some challenge to a human player.",
                "In the initial inspection of the logs, we encountered few games2 where a player, for inexplicable reasons, did not block the other from winning or failed to execute its own winning move.",
                "We can blame those faults on the humans lack of attention, or a typing error in its move reply; nevertheless, those errors might occur in bounded rational agents, and the appropriate behavior needs to be axiomatized.",
                "A typical Connect-Four game revolves around generating threats and blocking them.",
                "In our analysis we looked for explicit preventive actions, i.e., moves that block a group of 3 discs, or that remove a future threat (in our limited search horizon).",
                "We found that in 83% of the total games there was at least one preventive action taken by the Black player.",
                "It was also found that Black averaged 2.8 preventive actions per game on the games in which it lost, while averaging 1.5 preventive actions per game when winning.",
                "It seems that Black requires 1 or 2 preventive actions to build its initial taking position, before starting to present threats.",
                "If it did not manage to win, it will usually prevent an extra threat or two before succumbing to White. 4.",
                "RELATED WORK Much research deals with the axiomatization of teamwork and mental states of individuals: some models use knowledge and belief [10], others have models of goals and intentions [8, 4].",
                "However, all these formal theories deal with agent teamwork and cooperation.",
                "As far as we know, our model is the first to provide a formalized model for explicit adversarial environments and agents behavior in it.",
                "The classical Min-Max adversarial search algorithm was the first attempt to integrate the opponent into the search space with a weak assumption of an optimally playing opponent.",
                "Since then, much effort has gone into integrating the opponent model into the decision procedure to predict future behavior.",
                "The M∗ algorithm presented by Carmel and Markovitch [2] showed a method of incorporating opponent models into adversary search, while in [3] they used learning to provide a more accurate opponent model in a 2player repeated game environment, where agents strategies were modeled as finite automata.",
                "Additional Adversarial planning work was done by Willmott et al. [13], which provided an adversarial planning approach to the game of GO.",
                "The research mentioned above dealt with adversarial search and the integration of opponent models into classical utilitybased search methods.",
                "That work shows the importance of opponent modeling and the ability to exploit it to an agents advantage.",
                "However, the basic limitations of those search methods still apply; our model tries to overcome those limitations by presenting a formal model for a new, mental state-based adversarial specification. 5.",
                "CONCLUSIONS We presented an Adversarial Environment model for a 2 These were later removed from the final analysis. bounded rational agent that is situated in an N-player, zerosum environment.",
                "We used the SharedPlans formalization to define the model and the axioms that agents can apply as behavioral guidelines.",
                "The model is meant to be used as a guideline for designing agents that need to operate in such adversarial environments.",
                "We presented empirical results, based on ConnectFour log file analysis, that exemplify the model and the axioms for a bilateral instance of the environment.",
                "The results we presented are a first step towards an expanded model that will cover all types of adversarial environments, for example, environments that are non-zero-sum, and environments that contain natural agents that are not part of the direct conflict.",
                "Those challenges and more will be dealt with in future research. 6.",
                "ACKNOWLEDGMENT This research was supported in part by Israel Science Foundation grants #1211/04 and #898/05. 7.",
                "REFERENCES [1] L. V. Allis.",
                "A knowledge-based approach of Connect-Four - the game is solved: White wins.",
                "Masters thesis, Free University, Amsterdam, The Netherlands, 1988. [2] D. Carmel and S. Markovitch.",
                "Incorporating opponent models into adversary search.",
                "In Proceedings of the Thirteenth National Conference on Artificial Intelligence, pages 120-125, Portland, OR, 1996. [3] D. Carmel and S. Markovitch.",
                "Opponent modeling in multi-agent systems.",
                "In G. Weiß and S. Sen, editors, Adaptation and Learning in Multi-Agent Systems, pages 40-52.",
                "Springer-Verlag, 1996. [4] B. J. Grosz and S. Kraus.",
                "Collaborative plans for complex group action.",
                "Artificial Intelligence, 86(2):269-357, 1996. [5] M. Hadad, G. Kaminka, G. Armon, and S. Kraus.",
                "Supporting collaborative activity.",
                "In Proc. of AAAI-2005, pages 83-88, Pittsburgh, 2005. [6] http://www.gamerz.net/˜pbmserv/. [7] S. Kraus and D. Lehmann.",
                "Designing and building a negotiating automated agent.",
                "Computational Intelligence, 11:132-171, 1995. [8] H. J. Levesque, P. R. Cohen, and J. H. T. Nunes.",
                "On acting together.",
                "In Proc. of AAAI-90, pages 94-99, Boston, MA, 1990. [9] S. Markovitch and R. Reger.",
                "Learning and exploiting relative weaknesses of opponent agents.",
                "Autonomous Agents and Multi-Agent Systems, 10(2):103-130, 2005. [10] Y. M. Ronald Fagin, Joseph Y. Halpern and M. Y. Vardi.",
                "Reasoning about knowledge.",
                "MIT Press, Cambridge, Mass., 1995. [11] P. Thagard.",
                "Adversarial problem solving: Modeling an oponent using explanatory coherence.",
                "Cognitive Science, 16(1):123-149, 1992. [12] R. W. Toseland and R. F. Rivas.",
                "An Introduction to Group Work Practice.",
                "Prentice Hall, Englewood Cliffs, NJ, 2nd edition edition, 1995. [13] S. Willmott, J. Richardson, A. Bundy, and J. Levine.",
                "An adversarial planning approach to Go.",
                "Lecture Notes in Computer Science, 1558:93-112, 1999.",
                "The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) 557"
            ],
            "original_annotated_samples": [],
            "translated_annotated_samples": [
                "Una acción que produce una evaluación de utilidad estimada por encima de la TRH se considera una \"acción beneficiosa\" altamente \".acción beneficiosa",
                "La función de evaluación debe proyectar una acción que cause una finalización de un objetivo o un subggoal a un valor que sea mayor que TRH (una \"acción beneficiosa\" altamente \").(∀ai, g ∗ ai ∈ Gai, α ∈ Cai, w ∈ W, g ∗ gai ∈ Ggai) alcanzar (g ∗ ai, α, w) ∨ alcanzar (g ∗ gai, α, w) ⇒ eval (ai,α, W) ≥ TRH.acción beneficiosa",
                "Proposición 1: prevenir o perder el caso.(∀Aag, ao ∈ A, α ∈ Caag, β ∈ Cao, G ∗ Ao, TN, Tα, Tβ, W ∈ W) Bel (AAG, DO (AO, β, Tβ, W) ⇒ Logro (G ∗ AO, β, w), tn) ∧ bel (aag, do (aag, α, tα, w) ⇒ ¬do (ao, β, tβ, w)) ⇒ pot.int.to (aag, α, tn, tα, w) Boceto de prueba: La Proposición 1 puede derivarse fácilmente del Axiom A1 y la Propiedad 2 de la función EVAL, que establece que cualquier acción que cause una finalización de un objetivo es una \"acción beneficiosa\" altamente \".acción beneficiosa",
                "En muchos escenarios, puede ocurrir una situación en la que un agente decidirá no tomar la \"acción beneficiosa\" más actual que puede tomar (la acción con el valor de evaluación máxima de servicios públicos), porque cree que tomar otra acción (con un valor de evaluación de utilidad más bajo)podría producir (dependiendo de la respuesta de los adversarios) una posibilidad futura para una \"acción beneficiosa\" altamente \".acción beneficiosa",
                "Formalmente, el axioma establece que si todas las acciones (γ) no son acciones altamente beneficiosas (<trh), el agente puede hacer una acción α en el tiempo Tα si cree que dará como resultado una acción no altamente \"beneficiosa\" de suEl adversario, que a su vez, le enseña sobre el perfil de los adversarios, es decir, ofrece un mayor advnow (P AJ AI, Tβ).(∀Aag, ao ∈ A, α ∈ Caag, β ∈ Cao, TN, Tα, Tβ, W ∈ W) Bel (AAG, (∀γ ∈ Caag) Eval (AAG, γ, W) <TRH, TN) ∧BEL (AAG, DO (AAG, α, Tα, W) ⇒ Do (AO, β, Tβ, W), TN) ∧ Bel (AAG, Eval (AO, β, W) <TRH) ∧ Bel (AAG, ADVWOK(P AJ AI, Tβ)> advknow (P AJ AI, TN), TN) ⇒ Pot.int.to (AAG, α, TN, Tα, W) Por ejemplo, volver al escenario del juego de tablero, considerar comenzarUn juego versus un oponente sobre el que no sabemos nada, ni siquiera si es un oponente humano o computarizado.acción beneficiosa",
                "AU AG lo elegirá a través de la utilidad, mientras que AE AG lo elegirá a través de A2.(2) En el caso general, β es una \"acción beneficiosa\" altamente \"para AO, por lo tanto, produce baja utilidad para AU AG, lo que lo guiará para seleccionar un α que evitará β, mientras que AE AG lo elegirá a través de A2.1Si dicho β no existe para AO, entonces A2 no es aplicable, y AE AG puede proceder a A3.(A3) Axioma de movimiento táctico subóptimo: cuando se usa una función de evaluación heurística, AE AG tiene una creencia parcial en el perfil de su adversario (ítem 4 en el modelo AE), lo que puede hacer que crea en las setacciones (PROP1).acción beneficiosa"
            ],
            "translated_text": "",
            "candidates": [],
            "error": []
        },
        "connect-four game": {
            "translated_key": "",
            "is_in_text": true,
            "original_annotated_sentences": [
                "An Adversarial Environment Model for Bounded Rational Agents in Zero-Sum Interactions Inon Zuckerman1 , Sarit Kraus1 , Jeffrey S. Rosenschein2 , Gal Kaminka1 1 Department of Computer Science 2 The School of Engineering Bar-Ilan University and Computer Science Ramat-Gan, Israel Hebrew University, Jerusalem, Israel {zukermi,sarit,galk}@cs.biu.ac.il jeff@cs.huji.ac.il ABSTRACT Multiagent environments are often not cooperative nor collaborative; in many cases, agents have conflicting interests, leading to adversarial interactions.",
                "This paper presents a formal Adversarial Environment model for bounded rational agents operating in a zero-sum environment.",
                "In such environments, attempts to use classical utility-based search methods can raise a variety of difficulties (e.g., implicitly modeling the opponent as an omniscient utility maximizer, rather than leveraging a more nuanced, explicit opponent model).",
                "We define an Adversarial Environment by describing the mental states of an agent in such an environment.",
                "We then present behavioral axioms that are intended to serve as design principles for building such adversarial agents.",
                "We explore the application of our approach by analyzing log files of completed Connect-Four games, and present an empirical analysis of the axioms appropriateness.",
                "Categories and Subject Descriptors I.2.11 [Artificial Intelligence]: Distributed Artificial Intelligence-Intelligent agents,Multiagent Systems; I.2.4 [Artificial Intelligence]: Knowledge Representation Formalisms and Methods -Modal logic General Terms Design, Theory 1.",
                "INTRODUCTION Early research in multiagent systems (MAS) considered cooperative groups of agents; because individual agents had limited resources, or limited access to information (e.g., limited processing power, limited sensor coverage), they worked together by design to solve problems that individually they could not solve, or at least could not solve as efficiently.",
                "MAS research, however, soon began to consider interacting agents with individuated interests, as representatives of different humans or organizations with non-identical interests.",
                "When interactions are guided by diverse interests, participants may have to overcome disagreements, uncooperative interactions, and even intentional attempts to damage one another.",
                "When these types of interactions occur, environments require appropriate behavior from the agents situated in them.",
                "We call these environments Adversarial Environments, and call the clashing agents Adversaries.",
                "Models of cooperation and teamwork have been extensively explored in MAS through the axiomatization of mental states (e.g., [8, 4, 5]).",
                "However, none of this research dealt with adversarial domains and their implications for agent behavior.",
                "Our paper addresses this issue by providing a formal, axiomatized mental state model for a subset of adversarial domains, namely simple zero-sum adversarial environments.",
                "Simple zero-sum encounters exist of course in various twoplayer games (e.g., Chess, Checkers), but they also exist in n-player games (e.g., Risk, Diplomacy), auctions for a single good, and elsewhere.",
                "In these latter environments especially, using a utility-based adversarial search (such as the Min-Max algorithm) does not always provide an adequate solution; the payoff function might be quite complex or difficult to quantify, and there are natural computational limitations on bounded rational agents.",
                "In addition, traditional search methods (like Min-Max) do not make use of a model of the opponent, which has proven to be a valuable addition to adversarial planning [9, 3, 11].",
                "In this paper, we develop a formal, axiomatized model for bounded rational agents that are situated in a zero-sum adversarial environment.",
                "The model uses different modality operators, and its main foundations are the SharedPlans [4] model for collaborative behavior.",
                "We explore environment properties and the mental states of agents to derive behavioral axioms; these behavioral axioms constitute a formal model that serves as a specification and design guideline for agent design in such settings.",
                "We then investigate the behavior of our model empirically using the Connect-Four board game.",
                "We show that this game conforms to our environment definition, and analyze players behavior using a large set of completed match log 550 978-81-904262-7-5 (RPS) c 2007 IFAAMAS files.",
                "In addition, we use the results presented in [9] to discuss the importance of opponent modeling in our ConnectFour adversarial domain.",
                "The paper proceeds as follows.",
                "Section 2 presents the models formalization.",
                "Section 3 presents the empirical analysis and its results.",
                "We discuss related work in Section 4, and conclude and present future directions in Section 5. 2.",
                "ADVERSARIAL ENVIRONMENTS The adversarial environment model (denoted as AE) is intended to guide the design of agents by providing a specification of the capabilities and mental attitudes of an agent in an adversarial environment.",
                "We focus here on specific types of adversarial environments, specified as follows: 1.",
                "Zero-Sum Interactions: positive and negative utilities of all agents sum to zero; 2.",
                "Simple AEs: all agents in the environment are adversarial agents; 3.",
                "Bilateral AEs: AEs with exactly two agents; 4.",
                "Multilateral AEs: AEs of three or more agents.",
                "We will work on both bilateral and multilateral instantiations of zero-sum and simple environments.",
                "In particular, our adversarial environment model will deal with interactions that consist of N agents (N ≥ 2), where all agents are adversaries, and only one agent can succeed.",
                "Examples of such environments range from board games (e.g., Chess, Connect-Four, and Diplomacy) to certain economic environments (e.g., N-bidder auctions over a single good). 2.1 Model Overview Our approach is to formalize the mental attitudes and behaviors of a single adversarial agent; we consider how a single agent perceives the AE.",
                "The following list specifies the conditions and mental states of an agent in a simple, zero-sum AE: 1.",
                "The agent has an individual intention that its own goal will be completed; 2.",
                "The agent has an individual belief that it and its adversaries are pursuing full conflicting goals (defined below)there can be only one winner; 3.",
                "The agent has an individual belief that each adversary has an intention to complete its own full conflicting goal; 4.",
                "The agent has an individual belief in the (partial) profile of its adversaries.",
                "Item 3 is required, since it might be the case that some agent has a full conflicting goal, and is currently considering adopting the intention to complete it, but is, as of yet, not committed to achieving it.",
                "This might occur because the agent has not yet deliberated about the effects that adopting that intention might have on the other intentions it is currently holding.",
                "In such cases, it might not consider itself to even be in an adversarial environment.",
                "Item 4 states that the agent should hold some belief about the profiles of its adversaries.",
                "The profile represents all the knowledge the agent has about its adversary: its weaknesses, strategic capabilities, goals, intentions, trustworthiness, and more.",
                "It can be given explicitly or can be learned from observations of past encounters. 2.2 Model Definitions for Mental States We use Grosz and Krauss definitions of the modal operators, predicates, and meta-predicates, as defined in their SharedPlan formalization [4].",
                "We recall here some of the predicates and operators that are used in that formalization: Int.To(Ai, α, Tn, Tα, C) represents Ais intentions at time Tn to do an action α at time Tα in the context of C. Int.Th(Ai, prop, Tn, Tprop, C) represents Ais intentions at time Tn that a certain proposition prop holds at time Tprop in the context of C. The potential intention operators, Pot.Int.To(...) and Pot.Int.Th(...), are used to represent the mental state when an agent considers adopting an intention, but has not deliberated about the interaction of the other intentions it holds.",
                "The operator Bel(Ai, f, Tf ) represents agent Ai believing in the statement expressed in formula f, at time Tf .",
                "MB(A, f, Tf ) represents mutual belief for a group of agents A.",
                "A snapshot of the system finds our environment to be in some state e ∈ E of environmental variable states, and each adversary in any LAi ∈ L of possible local states.",
                "At any given time step, the system will be in some world w of the set of all possible worlds w ∈ W, where w = E×LA1 ×LA2 × ...LAn , and n is the number of adversaries.",
                "For example, in a Texas Holdem poker game, an agents local state might be its own set of cards (which is unknown to its adversary) while the environment will consist of the betting pot and the community cards (which are visible to both players).",
                "A utility function under this formalization is defined as a mapping from a possible world w ∈ W to an element in , which expresses the desirability of the world, from a single agent perspective.",
                "We usually normalize the range to [0,1], where 0 represents the least desirable possible world, and 1 is the most desirable world.",
                "The implementation of the utility function is dependent on the domain in question.",
                "The following list specifies new predicates, functions, variables, and constants used in conjunction with the original definitions for the adversarial environment formalization: 1. φ is a null action (the agent does not do anything). 2.",
                "GAi is the set of agent Ais goals.",
                "Each goal is a set of predicates whose satisfaction makes the goal complete (we use G∗ Ai ∈ GAi to represent an arbitrary goal of agent Ai). 3. gAi is the set of agent Ais subgoals.",
                "Subgoals are predicates whose satisfaction represents an important milestone toward achievement of the full goal. gG∗ Ai ⊆ gAi is the set of subgoals that are important to the completion of goal G∗ Ai (we will use g∗ G∗ Ai ∈ gG∗ Ai to represent an arbitrary subgoal). 4.",
                "P Aj Ai is the profile object agent Ai holds about agent Aj. 5.",
                "CA is a general set of actions for all agents in A which are derived from the environments constraints.",
                "CAi ⊆ CA is the set of agent Ais possible actions. 6.",
                "Do(Ai, α, Tα, w) holds when Ai performs action α over time interval Tα in world w. 7.",
                "Achieve(G∗ Ai , α, w) is true when goal G∗ Ai is achieved following the completion of action α in world w ∈ W, where α ∈ CAi . 8.",
                "Profile(Ai, PAi Ai ) is true when agent Ai holds an object profile for agent Aj.",
                "Definition 1.",
                "Full conflict (FulConf ) describes a zerosum interaction where only a single goal of the goals in conflict can be completed.",
                "FulConf(G∗ Ai , G∗ Aj ) ⇒ (∃α ∈ CAi , ∀w, β ∈ CAj ) (Achieve(G∗ Ai , α, w) ⇒ ¬Achieve(G∗ Aj , β, w)) ∨ (∃β ∈ CAj , ∀w, α ∈ CAi )(Achieve(G∗ Aj , β, w) ⇒ ¬Achieve(G∗ Ai , α, w)) Definition 2.",
                "Adversarial Knowledge (AdvKnow) is a function returning a value which represents the amount of The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) 551 knowledge agent Ai has on the profile of agent Aj, at time Tn.",
                "The higher the value, the more knowledge agent Ai has.",
                "AdvKnow : P Aj Ai × Tn → Definition 3.",
                "Eval - This evaluation function returns an estimated expected utility value for an agent in A, after completing an action from CA in some world state w. Eval : A × CA × w → Definition 4.",
                "TrH - (Threshold) is a numerical constant in the [0,1] range that represents an evaluation function (Eval) threshold value.",
                "An action that yields an estimated utility evaluation above the TrH is regarded as a highly beneficial action.",
                "The Eval value is an estimation and not the real utility function, which is usually unknown.",
                "Using the real utility value for a rational agent would easily yield the best outcome for that agent.",
                "However, agents usually do not have the real utility functions, but rather a heuristic estimate of it.",
                "There are two important properties that should hold for the evaluation function: Property 1.",
                "The evaluation function should state that the most desirable world state is one in which the goal is achieved.",
                "Therefore, after the goal has been satisfied, there can be no future action that can put the agent in a world state with higher Eval value. (∀Ai, G∗ Ai , α, β ∈ CAi , w ∈ W) Achieve(G∗ Ai , α, w) ⇒ Eval(Ai, α, w) ≥ Eval(Ai, β, w) Property 2.",
                "The evaluation function should project an action that causes a completion of a goal or a subgoal to a value which is greater than TrH (a highly beneficial action). (∀Ai, G∗ Ai ∈ GAi , α ∈ CAi , w ∈ W, g∗ GAi ∈ gGAi ) Achieve(G∗ Ai , α, w) ∨ Achieve(g∗ GAi , α, w) ⇒ Eval(Ai, α, w) ≥ TrH.",
                "Definition 5.",
                "SetAction We define a set action (SetAction) as a set of action operations (either complex or basic actions) from some action sets CAi and CAj which, according to agent Ais belief, are attached together by a temporal and consequential relationship, forming a chain of events (action, and its following consequent action). (∀α1 , . . . , αu ∈ CAi , β1 , . . . , βv ∈ CAj , w ∈ W) SetAction(α1 , . . . , αu , β1 , . . . , βv , w) ⇒ ((Do(Ai, α1 , Tα1 , w) ⇒ Do(Aj, β1 , Tβ1 , w)) ⇒ Do(Ai, α2 , Tα2 , w) ⇒ . . . ⇒ Do(Ai, αu , Tαu , w)) The consequential relation might exist due to various environmental constraints (when one action forces the adversary to respond with a specific action) or due to the agents knowledge about the profile of its adversary.",
                "Property 3.",
                "As the knowledge we have about our adversary increases we will have additional beliefs about its behavior in different situations which in turn creates new set actions.",
                "Formally, if our AdvKnow at time Tn+1 is greater than AdvKnow at time Tn, then every SetAction known at time Tn is also known at time Tn+1.",
                "AdvKnow(P Aj Ai , Tn+1) > AdvKnow(P Aj Ai , Tn) ⇒ (∀α1 , . . . , αu ∈ CAi , β1 , . . . , βv ∈ CAj ) Bel(Aag, SetAction(α1 , . . . , αu , β1 , . . . , βv ), Tn) ⇒ Bel(Aag, SetAction(α1 , . . . , αu , β1 , . . . , βv ), Tn+1) 2.3 The Environment Formulation The following axioms provide the formal definition for a simple, zero-sum Adversarial Environment (AE).",
                "Satisfaction of these axioms means that the agent is situated in such an environment.",
                "It provides specifications for agent Aag to interact with its set of adversaries A with respect to goals G∗ Aag and G∗ A at time TCo at some world state w. AE(Aag, A, G∗ Aag , A1, . . . , Ak, G∗ A1 , . . . , G∗ Ak , Tn, w) 1.",
                "Aag has an Int.Th his goal would be completed: (∃α ∈ CAag , Tα) Int.Th(Aag, Achieve(G∗ Aag , α), Tn, Tα, AE) 2.",
                "Aag believes that it and each of its adversaries Ao are pursuing full conflicting goals: (∀Ao ∈ {A1, . . . , Ak}) Bel(Aag, FulConf(G∗ Aag , G∗ Ao ), Tn) 3.",
                "Aag believes that each of his adversaries in Ao has the Int.Th his conflicting goal G∗ Aoi will be completed: (∀Ao ∈ {A1, . . . , Ak})(∃β ∈ CAo , Tβ) Bel(Aag, Int.Th(Ao, Achieve(G∗ Ao , β), TCo, Tβ, AE), Tn) 4.",
                "Aag has beliefs about the (partial) profiles of its adversaries (∀Ao ∈ {A1, . . . , Ak}) (∃PAo Aag ∈ PAag )Bel(Aag, Profile(Ao, PAo Aag ), Tn) To build an agent that will be able to operate successfully within such an AE, we must specify behavioral guidelines for its interactions.",
                "Using a naive Eval maximization strategy to a certain search depth will not always yield satisfactory results for several reasons: (1) the search horizon problem when searching for a fixed depth; (2) the strong assumption of an optimally rational, unbounded resources adversary; (3) using an estimated evaluation function which will not give optimal results in all world states, and can be exploited [9].",
                "The following axioms specify the behavioral principles that can be used to differentiate between successful and less successful agents in the above Adversarial Environment.",
                "Those axioms should be used as specification principles when designing and implementing agents that should be able to perform well in such Adversarial Environments.",
                "The behavioral axioms represent situations in which the agent will adopt potential intentions to (Pot.Int.To(...)) perform an action, which will typically require some means-end reasoning to select a possible course of action.",
                "This reasoning will lead to the adoption of an Int.To(...) (see [4]).",
                "A1.",
                "Goal Achieving Axiom.",
                "The first axiom is the simplest case; when the agent Aag believes that it is one action (α) away from achieving his conflicting goal G∗ Aag , it should adopt the potential intention to do α and complete its goal. (∀Aag, α ∈ CAag , Tn, Tα, w ∈ W) (Bel(Aag, Do(Aag, α, Tα, w) ⇒ Achieve(G∗ Aag , α, w)) ⇒ Pot.Int.To(Aag, α, Tn, Tα, w) This somewhat trivial behavior is the first and strongest axiom.",
                "In any situation, when the agent is an action away from completing the goal, it should complete the action.",
                "Any fair Eval function would naturally classify α as the maximal value action (property 1).",
                "However, without explicit axiomatization of such behavior there might be situations where the agent will decide on taking another action for various reasons, due to its bounded decision resources.",
                "A2.",
                "Preventive Act Axiom.",
                "Being in an adversarial situation, agent Aag might decide to take actions that will damage one of its adversarys plans to complete its goal, even if those actions do not explicitly advance Aag towards its conflicting goal G∗ Aag .",
                "Such preventive action will take place when agent Aag has a belief about the possibility of its adversary Ao doing an action β that will give it a high 552 The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) utility evaluation value (> TrH).",
                "Believing that taking action α will prevent the opponent from doing its β, it will adopt a potential intention to do α. (∀Aag, Ao ∈ A, α ∈ CAag , β ∈ CAo , Tn, Tβ, w ∈ W) (Bel(Aag, Do(Ao, β, Tβ, w) ∧ Eval(Ao, β, w) > TrH, Tn) ∧ Bel(Aag, Do(Aag, α, Tα, w) ⇒ ¬Do(Ao, β, Tβ, w), Tn) ⇒ Pot.Int.To(Aag, α, Tn, Tα, w) This axiom is a basic component of any adversarial environment.",
                "For example, looking at a Chess board game, a player could realize that it is about to be checkmated by its opponent, thus making a preventive move.",
                "Another example is a Connect Four game: when a player has a row of three chips, its opponent must block it, or lose.",
                "A specific instance of A1 occurs when the adversary is one action away from achieving its goal, and immediate preventive action needs to be taken by the agent.",
                "Formally, we have the same beliefs as stated above, with a changed belief that doing action β will cause agent Ao to achieve its goal.",
                "Proposition 1: Prevent or lose case. (∀Aag, Ao ∈ A, α ∈ CAag , β ∈ CAo , G∗ Ao , Tn, Tα, Tβ, w ∈ W) Bel(Aag, Do(Ao, β, Tβ, w) ⇒ Achieve(G∗ Ao , β, w), Tn) ∧ Bel(Aag, Do(Aag, α, Tα, w) ⇒ ¬Do(Ao, β, Tβ, w)) ⇒ Pot.Int.To(Aag, α, Tn, Tα, w) Sketch of proof: Proposition 1 can be easily derived from axiom A1 and the property 2 of the Eval function, which states that any action that causes a completion of a goal is a highly beneficial action.",
                "The preventive act behavior will occur implicitly when the Eval function is equal to the real world utility function.",
                "However, being bounded rational agents and dealing with an estimated evaluation function we need to explicitly axiomatize such behavior, for it will not always occur implicitly from the evaluation function.",
                "A3.",
                "Suboptimal Tactical Move Axiom.",
                "In many scenarios a situation may occur where an agent will decide not to take the current most beneficial action it can take (the action with the maximal utility evaluation value), because it believes that taking another action (with lower utility evaluation value) might yield (depending on the adversarys response) a future possibility for a highly beneficial action.",
                "This will occur most often when the Eval function is inaccurate and differs by a large extent from the Utility function.",
                "Put formally, agent Aag believes in a certain SetAction that will evolve according to its initial action and will yield a high beneficial value (> TrH) solely for it. (∀Aag, Ao ∈ A, Tn, w ∈ W) (∃α1 , . . . , αu ∈ CAi , β1 , . . . , βv ∈ CAj , Tα1 ) Bel(Aag, SetAction(α1 , . . . , αu , β1 , . . . , βv ), Tn) ∧ Bel(Aag, Eval(Ao, βv , w) < TrH < Eval(Aag, αu , w), Tn) ⇒ Pot.Int.To(Aag, α1 , Tn, Tα1 , w) An agent might believe that a chain of events will occur for various reasons due to the inevitable nature of the domain.",
                "For example, in Chess, we often observe the following: a move causes a check position, which in turn limits the opponents moves to avoiding the check, to which the first player might react with another check, and so on.",
                "The agent might also believe in a chain of events based on its knowledge of its adversarys profile, which allows it to foresee the adversarys movements with high accuracy.",
                "A4.",
                "Profile Detection Axiom.",
                "The agent can adjust its adversarys profiles by observations and pattern study (specifically, if there are repeated encounters with the same adversary).",
                "However, instead of waiting for profile information to be revealed, an agent can also initiate actions that will force its adversary to react in a way that will reveal profile knowledge about it.",
                "Formally, the axiom states that if all actions (γ) are not highly beneficial actions (< TrH), the agent can do action α in time Tα if it believes that it will result in a non-highly beneficial action β from its adversary, which in turn teaches it about the adversarys profile, i.e., gives a higher AdvKnow(P Aj Ai , Tβ). (∀Aag, Ao ∈ A, α ∈ CAag , β ∈ CAo , Tn, Tα, Tβ, w ∈ W) Bel(Aag, (∀γ ∈ CAag )Eval(Aag, γ, w) < TrH, Tn) ∧ Bel(Aag, Do(Aag, α, Tα, w) ⇒ Do(Ao, β, Tβ, w), Tn) ∧ Bel(Aag, Eval(Ao, β, w) < TrH) ∧ Bel(Aag, AdvKnow(P Aj Ai , Tβ) > AdvKnow(P Aj Ai , Tn), Tn) ⇒ Pot.Int.To(Aag, α, Tn, Tα, w) For example, going back to the Chess board game scenario, consider starting a game versus an opponent about whom we know nothing, not even if it is a human or a computerized opponent.",
                "We might start playing a strategy that will be suitable versus an average opponent, and adjust our game according to its level of play.",
                "A5.",
                "Alliance Formation Axiom The following behavioral axiom is relevant only in a multilateral instantiation of the adversarial environment (obviously, an alliance cannot be formed in a bilateral, zero-sum encounter).",
                "In different situations during a multilateral interaction, a group of agents might believe that it is in their best interests to form a temporary alliance.",
                "Such an alliance is an agreement that constrains its members behavior, but is believed by its members to enable them to achieve a higher utility value than the one achievable outside of the alliance.",
                "As an example, we can look at the classical Risk board game, where each player has an individual goal of being the sole conquerer of the world, a zero-sum game.",
                "However, in order to achieve this goal, it might be strategically wise to make short-term ceasefire agreements with other players, or to join forces and attack an opponent who is stronger than the rest.",
                "An alliances terms defines the way its members should act.",
                "It is a set of predicates, denoted as Terms, that is agreed upon by the alliance members, and should remain true for the duration of the alliance.",
                "For example, the set Terms in the Risk scenario, could contain the following predicates: 1.",
                "Alliance members will not attack each other on territories X, Y and Z; 2.",
                "Alliance members will contribute C units per turn for attacking adversary Ao; 3.",
                "Members are obligated to stay as part of the alliance until time Tk or until adversarys Ao army is smaller than Q.",
                "The set Terms specifies inter-group constraints on each of the alliance members (∀Aal i ∈ Aal ⊆ A) set of actions Cal i ⊆ C. Definition 6.",
                "Al val - the total evaluation value that agent Ai will achieve while being part of Aal is the sum of Evali (Eval for Ai) of each of Aal j Eval values after taking their own α actions (via the agent(α) predicate): Al val(Ai, Cal , Aal , w) = α∈Cal Evali(Aal j , agent(α), w) Definition 7.",
                "Al TrH - is a number representing an Al val The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) 553 threshold; above it, the alliance can be said to be a highly beneficial alliance.",
                "The value of Al TrH will be calculated dynamically according to the progress of the interaction, as can be seen in [7].",
                "After an alliance is formed, its members are now working in their normal adversarial environment, as well as according to the mental states and axioms required for their interactions as part of the alliance.",
                "The following Alliance model (AL) specifies the conditions under which the group Aal can be said to be in an alliance and working with a new and constrained set of actions Cal , at time Tn.",
                "AL(Aal , Cal , w, Tn) 1.",
                "Aal has a MB that all members are part of Aal : MB(Aal , (∀Aal i ∈ Aal )member(Aal i , Aal ), Tn) 2.",
                "Aal has a MB that the group be maintained: MB(Aal , (∀Aal i ∈ Aal )Int.Th (Ai, member(Ai, Aal ), Tn, Tn+1, Co), Tn) 3.",
                "Aal has a MB that being members gives them high utility value: MB(Aal , (∀Aal i ∈ Aal )Al val(Aal i , Cal , Aal , w) ≥ Al TrH, Tn) Members profiles are a crucial part of successful alliances.",
                "We assume that agents that have more accurate profiles of their adversaries will be more successful in such environments.",
                "Such agents will be able to predict when a member is about to breach the alliances contract (item 2 in the above model), and take counter measures (when item 3 will falsify).",
                "The robustness of the alliance is in part a function of its members trustfulness measure, objective position estimation, and other profile properties.",
                "We should note that an agent can simultaneously be part of more than one alliance.",
                "Such a temporary alliance, where the group members do not have a joint goal but act collaboratively for the interest of their own individual goals, is classified as a Treatment Group by modern psychologists [12] (in contrast to a Task Group, where its members have a joint goal).",
                "The Shared Activity model as presented in [5] modeled Treatment Group behavior using the same SharedPlans formalization.",
                "When comparing both definitions of an alliance and a Treatment Group we found an unsurprising resemblance between both models: the environment models definitions are almost identical (see SAs definitions in [5]), and their Selfish-Act and Cooperative Act axioms conform to our adversarial agents behavior.",
                "The main distinction between both models is the integration of a Helpful-behavior act axiom, in the Shared Activity which cannot be part of ours.",
                "This axiom states that an agent will consider taking action that will lower its Eval value (to a certain lower bound), if it believes that a group partner will gain a significant benefit.",
                "Such behavior cannot occur in a pure adversarial environment (as a zero-sum game is), where the alliance members are constantly on watch to manipulate their alliance to their own advantage.",
                "A6.",
                "Evaluation Maximization Axiom.",
                "In a case when all other axioms are inapplicable, we will proceed with the action that maximizes the heuristic value as computed in the Eval function. (∀Aag, Ao ∈ A, α ∈ Cag, Tn, w ∈ W) Bel(Aag, (∀γ ∈ Cag)Eval(Aag, α, w) ≥ Eval(Aag, γ, w), Tn) ⇒ Pot.Int.To(Aag, α, Tn, Tα, w) T1.",
                "Optimality on Eval = Utility The above axiomatic model handles situations where the Utility is unknown and the agents are bounded rational agents.",
                "The following theorem shows that in bilateral interactions, where the agents have the real Utility function (i.e., Eval = Utility) and are rational agents, the axioms provide the same optimal result as classic adversarial search (e.g., Min-Max).",
                "Theorem 1.",
                "Let Ae ag be an unbounded rational AE agent using the Eval heuristic evaluation function, Au ag be the same agent using the true Utility function, and Ao be a sole unbounded utility-based rational adversary.",
                "Given that Eval = Utility: (∀α ∈ CAu ag , α ∈ CAe ag , Tn, w ∈ W) Pot.Int.To(Au ag, α, Tn, Tα, w) → Pot.Int.To(Ae ag, α , Tn, Tα, w) ∧ ((α = α ) ∨ (Utility(Au ag, α, w) = Eval(Ae ag, α , w))) Sketch of proof - Given that Au ag has the real utility function and unbounded resources, it can generate the full game tree and run the optimal MinMax algorithm to choose the highest utility value action, which we denote by, α.",
                "The proof will show that Ae ag, using the AE axioms, will select the same or equal utility α (when there is more than one action with the same max utility) when Eval = Utility. (A1) Goal achieving axiom - suppose there is an α such that its completion will achieve Au ags goal.",
                "It will obtain the highest utility by Min-Max for Au ag.",
                "The Ae ag agent will select α or another action with the same utility value via A1.",
                "If such α does not exist, Ae ag cannot apply this axiom, and proceeds to A2. (A2) Preventive act axiom - (1) Looking at the basic case (see Prop1), if there is a β which leads Ao to achieve its goal, then a preventive action α will yield the highest utility for Au ag.",
                "Au ag will choose it through the utility, while Ae ag will choose it through A2. (2) In the general case, β is a highly beneficial action for Ao, thus yields low utility for Au ag, which will guide it to select an α that will prevent β, while Ae ag will choose it through A2.1 If such β does not exist for Ao, then A2 is not applicable, and Ae ag can proceed to A3. (A3) Suboptimal tactical move axiom - When using a heuristic Eval function, Ae ag has a partial belief in the profile of its adversary (item 4 in AE model), which may lead it to believe in SetActions (Prop1).",
                "In our case, Ae ag is holding a full profile on its optimal adversary and knows that Ao will behave optimally according to the real utility values on the complete search tree, therefore, any belief about suboptimal SetAction cannot exist, yielding this axiom inapplicable.",
                "Ae ag will proceed to A4. (A4) Profile detection axiom - Given that Ae ag has the full profile of Ao, none of Ae ags actions can increase its knowledge.",
                "That axiom will not be applied, and the agent will proceed with A6 (A5 will be disregarded because the interaction is bilateral). (A6) Evaluation maximization axiom - This axiom will select the max Eval for Ae ag.",
                "Given that Eval = Utility, the same α that was selected by Au ag will be selected. 3.",
                "EVALUATION The main purpose of our experimental analysis is to evaluate the models behavior and performance in a real adversarial environment.",
                "This section investigates whether bounded 1 A case where following the completion of β there exists a γ which gives high utility for Agent Au ag, cannot occur because Ao uses the same utility, and γs existence will cause it to classify β as a low utility action. 554 The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) rational agents situated in such adversarial environments will be better off applying our suggested behavioral axioms. 3.1 The Domain To explore the use of the above model and its behavioral axioms, we decided to use the <br>connect-four game</br> as our adversarial environment.",
                "Connect-Four is a 2-player, zerosum game which is played using a 6x7 matrix-like board.",
                "Each turn, a player drops a disc into one of the 7 columns (the set of 21 discs is usually colored yellow for player 1 and red for player 2; we will use White and Black respectively to avoid confusion).",
                "The winner is the first player to complete a horizontal, vertical, or diagonal set of four discs with its color.",
                "On very rare occasions, the game might end in a tie if all the empty grids are filled, but no player managed to create a 4-disc set.",
                "The <br>connect-four game</br> was solved in [1], where it is shown that the first player (playing with the white discs) can force a win by starting in the middle column (column 4) and playing optimally However, the optimal strategy is very complex, and difficult to follow even for complex bounded rational agents, such as human players.",
                "Before we can proceed checking agent behavior, we must first verify that the domain conforms to the adversarial environments definition as given above (which the behavioral axioms are based on).",
                "First, when playing a <br>connect-four game</br>, the agent has an intention to win the game (item 1).",
                "Second (item 2), our agent believes that in Connect-Four there can only be one winner (or no winner at all in the rare occurrence of a tie).",
                "In addition, our agent believes that its opponent to the game will try to win (item 3), and we hope it has some partial knowledge (item 4) about its adversary (this knowledge can vary from nothing, through simple facts such as age, to strategies and weaknesses).",
                "Of course, not all Connect-Four encounters are adversarial.",
                "For example, when a parent is playing the game with its child, the following situation might occur: the child, having a strong incentive to win, treats the environment as adversarial (it intends to win, understands that there can only be one winner, and believes that its parent is trying to beat him).",
                "However, the parents point of view might see the environment as an educational one, where its goal is not to win the game, but to cause enjoyment or practice strategic reasoning.",
                "In such an educational environment, a new set of behavioral axioms might be more beneficial to the parents goals than our suggested adversarial behavioral axioms. 3.2 Axiom Analysis After showing that the <br>connect-four game</br> is indeed a zero-sum, bilateral adversarial environment, the next step is to look at players behaviors during the game and check whether behaving according to our model does improve performance.",
                "To do so we have collected log files from completed Connect-Four games that were played by human players over the Internet.",
                "Our collected log file data came from Play by eMail (PBeM) sites.",
                "These are web sites that host email games, where each move is taken by an email exchange between the server and the players.",
                "Many such sites archives contain real competitive interactions, and also maintain a ranking system for their members.",
                "Most of the data we used can be found in [6].",
                "As can be learned from [1], Connect-Four has an optimal strategy and a considerable advantage for the player who starts the game (which we call the White player).",
                "We will concentrate in our analysis on the second players moves (to be called Black).",
                "The White player, being the first to act, has the so-called initiative advantage.",
                "Having the advantage and a good strategy will keep the Black player busy reacting to its moves, instead of initiating threats.",
                "A threat is a combination of three discs of the same color, with an empty spot for the fourth winning disk.",
                "An open threat is a threat that can be realized in the opponents next move.",
                "In order for the Black player to win, it must somehow turn the tide, take the advantage and start presenting threats to the White player.",
                "We will explore Black players behavior and their conformance to our axioms.",
                "To do so, we built an application that reads log files and analyzes the Black players moves.",
                "The application contains two main components: (1) a Min-Max algorithm for evaluation of moves; (2) open threats detector for the discovering of open threats.",
                "The Min-Max algorithm will work to a given depth, d and for each move α will output the heuristic value for the next action taken by the player as written in the log file, h(α), alongside the maximum heuristic value, maxh(α), that could be achieved prior to taking the move (obviously, if h(α) = maxh(α), then the player did not do the optimal move heuristically).",
                "The threat detectors job is to notify if some action was taken in order to block an open threat (not blocking an open threat will probably cause the player to lose in the opponents next move).",
                "The heuristic function used by Min-Max to evaluate the players utility is the following function, which is simple to compute, yet provides a reasonable challenge to human opponents: Definition 8.",
                "Let Group be an adjacent set of four squares that are horizontal, vertical, or diagonal.",
                "Groupn b (Groupn w) be a Group with n pieces of the black (white) color and 4−n empty squares. h = ((Group1 b ∗α)+(Group2 b ∗β)+(Group3 b ∗γ)+(Group4 b ∗∞)) − ((Group1 w ∗α)+(Group2 w ∗β)+(Group3 w ∗γ)+(Group4 w ∗∞)) The values of α, β and δ can vary to form any desired linear combination; however, it is important to value them with the α < β < δ ordering in mind (we used 1, 4, and 8 as their respective values).",
                "Groups of 4 discs of the same color means victory, thus discovery of such a group will result in ∞ to ensure an extreme value.",
                "We now use our estimated evaluation function to evaluate the Black players actions during the Connect-Four adversarial interaction.",
                "Each game from the log file was input into the application, which processed and output a reformatted log file containing the h value of the current move, the maxh value that could be achieved, and a notification if an open threat was detected.",
                "A total of 123 games were analyzed (57 with White winning, and 66 with Black winning).",
                "A few additional games were manually ignored in the experiment, due to these problems: a player abandoning the game while the outcome is not final, or a blunt irrational move in the early stages of the game (e.g., not blocking an obvious winning group in the first opening moves).",
                "In addition, a single tie game was also removed.",
                "The simulator was run to a search depth of 3 moves.",
                "We now proceed to analyze the games with respect to each behavioral axiom.",
                "The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) 555 Table 1: Average heuristic difference analysis Black losses Black Won Avg minh -17.62 -12.02 Avg 3 lowest h moves (min3 h) -13.20 -8.70 3.2.1 Affirming the Suboptimal tactical move axiom The following section presents the heuristic evaluations of the Min-Max algorithm for each action, and checks the amount and extent of suboptimal tactical actions and their implications on performance.",
                "Table 1 shows results and insights from the games heuristic analysis, when search depth equals 3 (this search depth was selected for the results to be comparable to [9], see Section 3.2.3).",
                "The tables heuristic data is the difference between the present maximal heuristic value and the heuristic value of the action that was eventually taken by the player (i.e., the closer the number is to 0, the closer the action was to the maximum heuristic action).",
                "The first row presents the difference values of the action that had the maximal difference value among all the Black players actions in a given game, as averaged over all Blacks winning and losing games (see respective columns).",
                "In games in which the Black player loses, its average difference value was -17.62, while in games in which the Black player won, its average was -12.02.",
                "The second row expands the analysis by considering the 3 highest heuristic difference actions, and averaging them.",
                "In that case, we notice an average heuristic difference of 5 points between games which the Black player loses and games in which it wins.",
                "Nevertheless, the importance of those numbers is that they allowed us to take an educated guess on a threshold number of 11.5, as the value of the TrH constant, which differentiates between normal actions and highly beneficial ones.",
                "After finding an approximated TrH constant, we can proceed with an analysis of the importance of suboptimal moves.",
                "To do so we took the subset of games in which the minimum heuristic difference value for Blacks actions was 11.5.",
                "As presented in Table 2, we can see the different min3 h average of the 3 largest ranges and the respective percentage of games won.",
                "The first row shows that the Black player won only 12% of the games in which the average of its 3 highest heuristically difference actions (min3 h) was smaller than the suggested threshold, TrH = 11.5.",
                "The second row shows a surprising result: it seems that when min3 h > −4 the Black player rarely wins.",
                "Intuition would suggest that games in which the action evaluation values were closer to the maximal values will result in more winning games for Black.",
                "However, it seems that in the Connect-Four domain, merely responding with somewhat easily expected actions, without initiating a few surprising and suboptimal moves, does not yield good results.",
                "The last row sums up the main insights from the analysis; most of Blacks wins (83%) came when its min3 h was in the range of -11.5 to -4.",
                "A close inspection of those Black winning games shows the following pattern behind the numbers: after standard opening moves, Black suddenly drops a disc into an isolated column, which seems a waste of a move.",
                "White continues to build its threats, while usually disregarding Blacks last move, which in turn uses the isolated disc as an anchor for a future winning threat.",
                "The results show that it was beneficial for the Black player Table 2: Blacks winnings percentages % of games min3 h < −11.5 12% min3 h > −4 5% −11.5 ≤ min3 h ≤ −4 83% to take suboptimal actions and not give the current highest possible heuristic value, but will not be too harmful for its position (i.e., will not give high beneficial value to its adversary).",
                "As it turned out, learning the threshold is an important aspect of success: taking wildly risky moves (min3 h < −11.5) or trying to avoid them (min3 h > −4) reduces the Black players winning chances by a large margin. 3.2.2 Affirming the Profile Monitoring Axiom In the task of showing the importance of monitoring ones adversaries profiles, our log files could not be used because they did not contain repeated interactions between players, which are needed to infer the players knowledge about their adversaries.",
                "However, the importance of opponent modeling and its use in attaining tactical advantages was already studied in various domains ([3, 9] are good examples).",
                "In a recent paper, Markovitch and Reger [9] explored the notion of learning and exploitation of opponent weakness in competitive interactions.",
                "They apply simple learning strategies by analyzing examples from past interactions in a specific domain.",
                "They also used the Connect-Four adversarial domain, which can now be used to understand the importance of monitoring the adversarys profile.",
                "Following the presentation of their theoretical model, they describe an extensive empirical study and check the agents performance after learning the weakness model with past examples.",
                "One of the domains used as a competitive environment was the same <br>connect-four game</br> (Checkers was the second domain).",
                "Their heuristic function was identical to ours with three different variations (H1, H2, and H3) that are distinguished from one another in their linear combination coefficient values.",
                "The search depth for the players was 3 (as in our analysis).",
                "Their extensive experiments check and compare various learning strategies, risk factors, predefined feature sets and usage methods.",
                "The bottom line is that the Connect-Four domain shows an improvement from a 0.556 winning rate before modeling to a 0.69 after modeling (page 22).",
                "Their conclusions, showing improved performance when holding and using the adversarys model, justify the effort to monitor the adversary profile for continuous and repeated interactions.",
                "An additional point that came up in their experiments is the following: after the opponent weakness model has been learned, the authors describe different methods of integrating the opponent weakness model into the agents decision strategy.",
                "Nevertheless, regardless of the specific method they chose to work with, all integration methods might cause the agent to take suboptimal decisions; it might cause the agent to prefer actions that are suboptimal at the present decision junction, but which might cause the opponent to react in accordance with its weakness model (as represented by our agent) which in turn will be beneficial for us in the future.",
                "The agents behavior, as demonstrated in [9] further confirms and strengthens our Suboptimal Tactical Axiom as discussed in the previous section. 556 The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) 3.2.3 Additional Insights The need for the Goal Achieving, Preventive Act, and Evaluation Maximization axioms are obvious, and need no further verification.",
                "However, even with respect to those axioms, a few interesting insights came up in the log analysis.",
                "The Goal achieving and Preventive Act axioms, though theoretically trivial, seem to provide some challenge to a human player.",
                "In the initial inspection of the logs, we encountered few games2 where a player, for inexplicable reasons, did not block the other from winning or failed to execute its own winning move.",
                "We can blame those faults on the humans lack of attention, or a typing error in its move reply; nevertheless, those errors might occur in bounded rational agents, and the appropriate behavior needs to be axiomatized.",
                "A typical <br>connect-four game</br> revolves around generating threats and blocking them.",
                "In our analysis we looked for explicit preventive actions, i.e., moves that block a group of 3 discs, or that remove a future threat (in our limited search horizon).",
                "We found that in 83% of the total games there was at least one preventive action taken by the Black player.",
                "It was also found that Black averaged 2.8 preventive actions per game on the games in which it lost, while averaging 1.5 preventive actions per game when winning.",
                "It seems that Black requires 1 or 2 preventive actions to build its initial taking position, before starting to present threats.",
                "If it did not manage to win, it will usually prevent an extra threat or two before succumbing to White. 4.",
                "RELATED WORK Much research deals with the axiomatization of teamwork and mental states of individuals: some models use knowledge and belief [10], others have models of goals and intentions [8, 4].",
                "However, all these formal theories deal with agent teamwork and cooperation.",
                "As far as we know, our model is the first to provide a formalized model for explicit adversarial environments and agents behavior in it.",
                "The classical Min-Max adversarial search algorithm was the first attempt to integrate the opponent into the search space with a weak assumption of an optimally playing opponent.",
                "Since then, much effort has gone into integrating the opponent model into the decision procedure to predict future behavior.",
                "The M∗ algorithm presented by Carmel and Markovitch [2] showed a method of incorporating opponent models into adversary search, while in [3] they used learning to provide a more accurate opponent model in a 2player repeated game environment, where agents strategies were modeled as finite automata.",
                "Additional Adversarial planning work was done by Willmott et al. [13], which provided an adversarial planning approach to the game of GO.",
                "The research mentioned above dealt with adversarial search and the integration of opponent models into classical utilitybased search methods.",
                "That work shows the importance of opponent modeling and the ability to exploit it to an agents advantage.",
                "However, the basic limitations of those search methods still apply; our model tries to overcome those limitations by presenting a formal model for a new, mental state-based adversarial specification. 5.",
                "CONCLUSIONS We presented an Adversarial Environment model for a 2 These were later removed from the final analysis. bounded rational agent that is situated in an N-player, zerosum environment.",
                "We used the SharedPlans formalization to define the model and the axioms that agents can apply as behavioral guidelines.",
                "The model is meant to be used as a guideline for designing agents that need to operate in such adversarial environments.",
                "We presented empirical results, based on ConnectFour log file analysis, that exemplify the model and the axioms for a bilateral instance of the environment.",
                "The results we presented are a first step towards an expanded model that will cover all types of adversarial environments, for example, environments that are non-zero-sum, and environments that contain natural agents that are not part of the direct conflict.",
                "Those challenges and more will be dealt with in future research. 6.",
                "ACKNOWLEDGMENT This research was supported in part by Israel Science Foundation grants #1211/04 and #898/05. 7.",
                "REFERENCES [1] L. V. Allis.",
                "A knowledge-based approach of Connect-Four - the game is solved: White wins.",
                "Masters thesis, Free University, Amsterdam, The Netherlands, 1988. [2] D. Carmel and S. Markovitch.",
                "Incorporating opponent models into adversary search.",
                "In Proceedings of the Thirteenth National Conference on Artificial Intelligence, pages 120-125, Portland, OR, 1996. [3] D. Carmel and S. Markovitch.",
                "Opponent modeling in multi-agent systems.",
                "In G. Weiß and S. Sen, editors, Adaptation and Learning in Multi-Agent Systems, pages 40-52.",
                "Springer-Verlag, 1996. [4] B. J. Grosz and S. Kraus.",
                "Collaborative plans for complex group action.",
                "Artificial Intelligence, 86(2):269-357, 1996. [5] M. Hadad, G. Kaminka, G. Armon, and S. Kraus.",
                "Supporting collaborative activity.",
                "In Proc. of AAAI-2005, pages 83-88, Pittsburgh, 2005. [6] http://www.gamerz.net/˜pbmserv/. [7] S. Kraus and D. Lehmann.",
                "Designing and building a negotiating automated agent.",
                "Computational Intelligence, 11:132-171, 1995. [8] H. J. Levesque, P. R. Cohen, and J. H. T. Nunes.",
                "On acting together.",
                "In Proc. of AAAI-90, pages 94-99, Boston, MA, 1990. [9] S. Markovitch and R. Reger.",
                "Learning and exploiting relative weaknesses of opponent agents.",
                "Autonomous Agents and Multi-Agent Systems, 10(2):103-130, 2005. [10] Y. M. Ronald Fagin, Joseph Y. Halpern and M. Y. Vardi.",
                "Reasoning about knowledge.",
                "MIT Press, Cambridge, Mass., 1995. [11] P. Thagard.",
                "Adversarial problem solving: Modeling an oponent using explanatory coherence.",
                "Cognitive Science, 16(1):123-149, 1992. [12] R. W. Toseland and R. F. Rivas.",
                "An Introduction to Group Work Practice.",
                "Prentice Hall, Englewood Cliffs, NJ, 2nd edition edition, 1995. [13] S. Willmott, J. Richardson, A. Bundy, and J. Levine.",
                "An adversarial planning approach to Go.",
                "Lecture Notes in Computer Science, 1558:93-112, 1999.",
                "The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) 557"
            ],
            "original_annotated_samples": [],
            "translated_annotated_samples": [
                "Conf.En agentes autónomos y sistemas de múltiples agentes (AAMAS 07), los agentes racionales situados en tales entornos adversos serán mejor aplicando nuestros axiomas de comportamiento sugeridos.3.1 El dominio para explorar el uso del modelo anterior y sus axiomas de comportamiento, decidimos usar el \"juego de conexión de conexión\" como nuestro entorno adversario.Juego de Connect-Four",
                "El \"Juego de Connect-Four\" se resolvió en [1], donde se muestra que el primer jugador (jugando con los discos blancos) puede forzar una victoria comenzando en la columna central (columna 4) y jugando de manera óptima, sin embargo, el óptimoLa estrategia es muy compleja y difícil de seguir incluso para agentes racionales limitados complejos, como los jugadores humanos.Juego de Connect-Four",
                "Primero, al jugar un \"juego de cuatro años\", el agente tiene la intención de ganar el juego (ítem 1).Juego de Connect-Four",
                "En un entorno educativo así, un nuevo conjunto de axiomas conductuales podría ser más beneficioso para los objetivos de los padres que nuestros axiomas conductuales adversos sugeridos.3.2 Análisis de axiomas Después de mostrar que el \"juego de conexión de conexión\" es de hecho un entorno adversario bilateral de suma cero, el siguiente paso es observar los comportamientos de los jugadores durante el juego y verificar si comportarse de acuerdo con nuestro modelo mejora el rendimiento.Juego de Connect-Four",
                "Uno de los dominios utilizados como un entorno competitivo fue el mismo \"juego Connect-Four\" (Checkers fue el segundo dominio).Juego de Connect-Four",
                "Un típico \"Juego de Connect Four\" gira en torno a generar amenazas y bloquearlas.Juego de Connect-Four"
            ],
            "translated_text": "",
            "candidates": [],
            "error": []
        },
        "empirical study": {
            "translated_key": "",
            "is_in_text": true,
            "original_annotated_sentences": [
                "An Adversarial Environment Model for Bounded Rational Agents in Zero-Sum Interactions Inon Zuckerman1 , Sarit Kraus1 , Jeffrey S. Rosenschein2 , Gal Kaminka1 1 Department of Computer Science 2 The School of Engineering Bar-Ilan University and Computer Science Ramat-Gan, Israel Hebrew University, Jerusalem, Israel {zukermi,sarit,galk}@cs.biu.ac.il jeff@cs.huji.ac.il ABSTRACT Multiagent environments are often not cooperative nor collaborative; in many cases, agents have conflicting interests, leading to adversarial interactions.",
                "This paper presents a formal Adversarial Environment model for bounded rational agents operating in a zero-sum environment.",
                "In such environments, attempts to use classical utility-based search methods can raise a variety of difficulties (e.g., implicitly modeling the opponent as an omniscient utility maximizer, rather than leveraging a more nuanced, explicit opponent model).",
                "We define an Adversarial Environment by describing the mental states of an agent in such an environment.",
                "We then present behavioral axioms that are intended to serve as design principles for building such adversarial agents.",
                "We explore the application of our approach by analyzing log files of completed Connect-Four games, and present an empirical analysis of the axioms appropriateness.",
                "Categories and Subject Descriptors I.2.11 [Artificial Intelligence]: Distributed Artificial Intelligence-Intelligent agents,Multiagent Systems; I.2.4 [Artificial Intelligence]: Knowledge Representation Formalisms and Methods -Modal logic General Terms Design, Theory 1.",
                "INTRODUCTION Early research in multiagent systems (MAS) considered cooperative groups of agents; because individual agents had limited resources, or limited access to information (e.g., limited processing power, limited sensor coverage), they worked together by design to solve problems that individually they could not solve, or at least could not solve as efficiently.",
                "MAS research, however, soon began to consider interacting agents with individuated interests, as representatives of different humans or organizations with non-identical interests.",
                "When interactions are guided by diverse interests, participants may have to overcome disagreements, uncooperative interactions, and even intentional attempts to damage one another.",
                "When these types of interactions occur, environments require appropriate behavior from the agents situated in them.",
                "We call these environments Adversarial Environments, and call the clashing agents Adversaries.",
                "Models of cooperation and teamwork have been extensively explored in MAS through the axiomatization of mental states (e.g., [8, 4, 5]).",
                "However, none of this research dealt with adversarial domains and their implications for agent behavior.",
                "Our paper addresses this issue by providing a formal, axiomatized mental state model for a subset of adversarial domains, namely simple zero-sum adversarial environments.",
                "Simple zero-sum encounters exist of course in various twoplayer games (e.g., Chess, Checkers), but they also exist in n-player games (e.g., Risk, Diplomacy), auctions for a single good, and elsewhere.",
                "In these latter environments especially, using a utility-based adversarial search (such as the Min-Max algorithm) does not always provide an adequate solution; the payoff function might be quite complex or difficult to quantify, and there are natural computational limitations on bounded rational agents.",
                "In addition, traditional search methods (like Min-Max) do not make use of a model of the opponent, which has proven to be a valuable addition to adversarial planning [9, 3, 11].",
                "In this paper, we develop a formal, axiomatized model for bounded rational agents that are situated in a zero-sum adversarial environment.",
                "The model uses different modality operators, and its main foundations are the SharedPlans [4] model for collaborative behavior.",
                "We explore environment properties and the mental states of agents to derive behavioral axioms; these behavioral axioms constitute a formal model that serves as a specification and design guideline for agent design in such settings.",
                "We then investigate the behavior of our model empirically using the Connect-Four board game.",
                "We show that this game conforms to our environment definition, and analyze players behavior using a large set of completed match log 550 978-81-904262-7-5 (RPS) c 2007 IFAAMAS files.",
                "In addition, we use the results presented in [9] to discuss the importance of opponent modeling in our ConnectFour adversarial domain.",
                "The paper proceeds as follows.",
                "Section 2 presents the models formalization.",
                "Section 3 presents the empirical analysis and its results.",
                "We discuss related work in Section 4, and conclude and present future directions in Section 5. 2.",
                "ADVERSARIAL ENVIRONMENTS The adversarial environment model (denoted as AE) is intended to guide the design of agents by providing a specification of the capabilities and mental attitudes of an agent in an adversarial environment.",
                "We focus here on specific types of adversarial environments, specified as follows: 1.",
                "Zero-Sum Interactions: positive and negative utilities of all agents sum to zero; 2.",
                "Simple AEs: all agents in the environment are adversarial agents; 3.",
                "Bilateral AEs: AEs with exactly two agents; 4.",
                "Multilateral AEs: AEs of three or more agents.",
                "We will work on both bilateral and multilateral instantiations of zero-sum and simple environments.",
                "In particular, our adversarial environment model will deal with interactions that consist of N agents (N ≥ 2), where all agents are adversaries, and only one agent can succeed.",
                "Examples of such environments range from board games (e.g., Chess, Connect-Four, and Diplomacy) to certain economic environments (e.g., N-bidder auctions over a single good). 2.1 Model Overview Our approach is to formalize the mental attitudes and behaviors of a single adversarial agent; we consider how a single agent perceives the AE.",
                "The following list specifies the conditions and mental states of an agent in a simple, zero-sum AE: 1.",
                "The agent has an individual intention that its own goal will be completed; 2.",
                "The agent has an individual belief that it and its adversaries are pursuing full conflicting goals (defined below)there can be only one winner; 3.",
                "The agent has an individual belief that each adversary has an intention to complete its own full conflicting goal; 4.",
                "The agent has an individual belief in the (partial) profile of its adversaries.",
                "Item 3 is required, since it might be the case that some agent has a full conflicting goal, and is currently considering adopting the intention to complete it, but is, as of yet, not committed to achieving it.",
                "This might occur because the agent has not yet deliberated about the effects that adopting that intention might have on the other intentions it is currently holding.",
                "In such cases, it might not consider itself to even be in an adversarial environment.",
                "Item 4 states that the agent should hold some belief about the profiles of its adversaries.",
                "The profile represents all the knowledge the agent has about its adversary: its weaknesses, strategic capabilities, goals, intentions, trustworthiness, and more.",
                "It can be given explicitly or can be learned from observations of past encounters. 2.2 Model Definitions for Mental States We use Grosz and Krauss definitions of the modal operators, predicates, and meta-predicates, as defined in their SharedPlan formalization [4].",
                "We recall here some of the predicates and operators that are used in that formalization: Int.To(Ai, α, Tn, Tα, C) represents Ais intentions at time Tn to do an action α at time Tα in the context of C. Int.Th(Ai, prop, Tn, Tprop, C) represents Ais intentions at time Tn that a certain proposition prop holds at time Tprop in the context of C. The potential intention operators, Pot.Int.To(...) and Pot.Int.Th(...), are used to represent the mental state when an agent considers adopting an intention, but has not deliberated about the interaction of the other intentions it holds.",
                "The operator Bel(Ai, f, Tf ) represents agent Ai believing in the statement expressed in formula f, at time Tf .",
                "MB(A, f, Tf ) represents mutual belief for a group of agents A.",
                "A snapshot of the system finds our environment to be in some state e ∈ E of environmental variable states, and each adversary in any LAi ∈ L of possible local states.",
                "At any given time step, the system will be in some world w of the set of all possible worlds w ∈ W, where w = E×LA1 ×LA2 × ...LAn , and n is the number of adversaries.",
                "For example, in a Texas Holdem poker game, an agents local state might be its own set of cards (which is unknown to its adversary) while the environment will consist of the betting pot and the community cards (which are visible to both players).",
                "A utility function under this formalization is defined as a mapping from a possible world w ∈ W to an element in , which expresses the desirability of the world, from a single agent perspective.",
                "We usually normalize the range to [0,1], where 0 represents the least desirable possible world, and 1 is the most desirable world.",
                "The implementation of the utility function is dependent on the domain in question.",
                "The following list specifies new predicates, functions, variables, and constants used in conjunction with the original definitions for the adversarial environment formalization: 1. φ is a null action (the agent does not do anything). 2.",
                "GAi is the set of agent Ais goals.",
                "Each goal is a set of predicates whose satisfaction makes the goal complete (we use G∗ Ai ∈ GAi to represent an arbitrary goal of agent Ai). 3. gAi is the set of agent Ais subgoals.",
                "Subgoals are predicates whose satisfaction represents an important milestone toward achievement of the full goal. gG∗ Ai ⊆ gAi is the set of subgoals that are important to the completion of goal G∗ Ai (we will use g∗ G∗ Ai ∈ gG∗ Ai to represent an arbitrary subgoal). 4.",
                "P Aj Ai is the profile object agent Ai holds about agent Aj. 5.",
                "CA is a general set of actions for all agents in A which are derived from the environments constraints.",
                "CAi ⊆ CA is the set of agent Ais possible actions. 6.",
                "Do(Ai, α, Tα, w) holds when Ai performs action α over time interval Tα in world w. 7.",
                "Achieve(G∗ Ai , α, w) is true when goal G∗ Ai is achieved following the completion of action α in world w ∈ W, where α ∈ CAi . 8.",
                "Profile(Ai, PAi Ai ) is true when agent Ai holds an object profile for agent Aj.",
                "Definition 1.",
                "Full conflict (FulConf ) describes a zerosum interaction where only a single goal of the goals in conflict can be completed.",
                "FulConf(G∗ Ai , G∗ Aj ) ⇒ (∃α ∈ CAi , ∀w, β ∈ CAj ) (Achieve(G∗ Ai , α, w) ⇒ ¬Achieve(G∗ Aj , β, w)) ∨ (∃β ∈ CAj , ∀w, α ∈ CAi )(Achieve(G∗ Aj , β, w) ⇒ ¬Achieve(G∗ Ai , α, w)) Definition 2.",
                "Adversarial Knowledge (AdvKnow) is a function returning a value which represents the amount of The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) 551 knowledge agent Ai has on the profile of agent Aj, at time Tn.",
                "The higher the value, the more knowledge agent Ai has.",
                "AdvKnow : P Aj Ai × Tn → Definition 3.",
                "Eval - This evaluation function returns an estimated expected utility value for an agent in A, after completing an action from CA in some world state w. Eval : A × CA × w → Definition 4.",
                "TrH - (Threshold) is a numerical constant in the [0,1] range that represents an evaluation function (Eval) threshold value.",
                "An action that yields an estimated utility evaluation above the TrH is regarded as a highly beneficial action.",
                "The Eval value is an estimation and not the real utility function, which is usually unknown.",
                "Using the real utility value for a rational agent would easily yield the best outcome for that agent.",
                "However, agents usually do not have the real utility functions, but rather a heuristic estimate of it.",
                "There are two important properties that should hold for the evaluation function: Property 1.",
                "The evaluation function should state that the most desirable world state is one in which the goal is achieved.",
                "Therefore, after the goal has been satisfied, there can be no future action that can put the agent in a world state with higher Eval value. (∀Ai, G∗ Ai , α, β ∈ CAi , w ∈ W) Achieve(G∗ Ai , α, w) ⇒ Eval(Ai, α, w) ≥ Eval(Ai, β, w) Property 2.",
                "The evaluation function should project an action that causes a completion of a goal or a subgoal to a value which is greater than TrH (a highly beneficial action). (∀Ai, G∗ Ai ∈ GAi , α ∈ CAi , w ∈ W, g∗ GAi ∈ gGAi ) Achieve(G∗ Ai , α, w) ∨ Achieve(g∗ GAi , α, w) ⇒ Eval(Ai, α, w) ≥ TrH.",
                "Definition 5.",
                "SetAction We define a set action (SetAction) as a set of action operations (either complex or basic actions) from some action sets CAi and CAj which, according to agent Ais belief, are attached together by a temporal and consequential relationship, forming a chain of events (action, and its following consequent action). (∀α1 , . . . , αu ∈ CAi , β1 , . . . , βv ∈ CAj , w ∈ W) SetAction(α1 , . . . , αu , β1 , . . . , βv , w) ⇒ ((Do(Ai, α1 , Tα1 , w) ⇒ Do(Aj, β1 , Tβ1 , w)) ⇒ Do(Ai, α2 , Tα2 , w) ⇒ . . . ⇒ Do(Ai, αu , Tαu , w)) The consequential relation might exist due to various environmental constraints (when one action forces the adversary to respond with a specific action) or due to the agents knowledge about the profile of its adversary.",
                "Property 3.",
                "As the knowledge we have about our adversary increases we will have additional beliefs about its behavior in different situations which in turn creates new set actions.",
                "Formally, if our AdvKnow at time Tn+1 is greater than AdvKnow at time Tn, then every SetAction known at time Tn is also known at time Tn+1.",
                "AdvKnow(P Aj Ai , Tn+1) > AdvKnow(P Aj Ai , Tn) ⇒ (∀α1 , . . . , αu ∈ CAi , β1 , . . . , βv ∈ CAj ) Bel(Aag, SetAction(α1 , . . . , αu , β1 , . . . , βv ), Tn) ⇒ Bel(Aag, SetAction(α1 , . . . , αu , β1 , . . . , βv ), Tn+1) 2.3 The Environment Formulation The following axioms provide the formal definition for a simple, zero-sum Adversarial Environment (AE).",
                "Satisfaction of these axioms means that the agent is situated in such an environment.",
                "It provides specifications for agent Aag to interact with its set of adversaries A with respect to goals G∗ Aag and G∗ A at time TCo at some world state w. AE(Aag, A, G∗ Aag , A1, . . . , Ak, G∗ A1 , . . . , G∗ Ak , Tn, w) 1.",
                "Aag has an Int.Th his goal would be completed: (∃α ∈ CAag , Tα) Int.Th(Aag, Achieve(G∗ Aag , α), Tn, Tα, AE) 2.",
                "Aag believes that it and each of its adversaries Ao are pursuing full conflicting goals: (∀Ao ∈ {A1, . . . , Ak}) Bel(Aag, FulConf(G∗ Aag , G∗ Ao ), Tn) 3.",
                "Aag believes that each of his adversaries in Ao has the Int.Th his conflicting goal G∗ Aoi will be completed: (∀Ao ∈ {A1, . . . , Ak})(∃β ∈ CAo , Tβ) Bel(Aag, Int.Th(Ao, Achieve(G∗ Ao , β), TCo, Tβ, AE), Tn) 4.",
                "Aag has beliefs about the (partial) profiles of its adversaries (∀Ao ∈ {A1, . . . , Ak}) (∃PAo Aag ∈ PAag )Bel(Aag, Profile(Ao, PAo Aag ), Tn) To build an agent that will be able to operate successfully within such an AE, we must specify behavioral guidelines for its interactions.",
                "Using a naive Eval maximization strategy to a certain search depth will not always yield satisfactory results for several reasons: (1) the search horizon problem when searching for a fixed depth; (2) the strong assumption of an optimally rational, unbounded resources adversary; (3) using an estimated evaluation function which will not give optimal results in all world states, and can be exploited [9].",
                "The following axioms specify the behavioral principles that can be used to differentiate between successful and less successful agents in the above Adversarial Environment.",
                "Those axioms should be used as specification principles when designing and implementing agents that should be able to perform well in such Adversarial Environments.",
                "The behavioral axioms represent situations in which the agent will adopt potential intentions to (Pot.Int.To(...)) perform an action, which will typically require some means-end reasoning to select a possible course of action.",
                "This reasoning will lead to the adoption of an Int.To(...) (see [4]).",
                "A1.",
                "Goal Achieving Axiom.",
                "The first axiom is the simplest case; when the agent Aag believes that it is one action (α) away from achieving his conflicting goal G∗ Aag , it should adopt the potential intention to do α and complete its goal. (∀Aag, α ∈ CAag , Tn, Tα, w ∈ W) (Bel(Aag, Do(Aag, α, Tα, w) ⇒ Achieve(G∗ Aag , α, w)) ⇒ Pot.Int.To(Aag, α, Tn, Tα, w) This somewhat trivial behavior is the first and strongest axiom.",
                "In any situation, when the agent is an action away from completing the goal, it should complete the action.",
                "Any fair Eval function would naturally classify α as the maximal value action (property 1).",
                "However, without explicit axiomatization of such behavior there might be situations where the agent will decide on taking another action for various reasons, due to its bounded decision resources.",
                "A2.",
                "Preventive Act Axiom.",
                "Being in an adversarial situation, agent Aag might decide to take actions that will damage one of its adversarys plans to complete its goal, even if those actions do not explicitly advance Aag towards its conflicting goal G∗ Aag .",
                "Such preventive action will take place when agent Aag has a belief about the possibility of its adversary Ao doing an action β that will give it a high 552 The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) utility evaluation value (> TrH).",
                "Believing that taking action α will prevent the opponent from doing its β, it will adopt a potential intention to do α. (∀Aag, Ao ∈ A, α ∈ CAag , β ∈ CAo , Tn, Tβ, w ∈ W) (Bel(Aag, Do(Ao, β, Tβ, w) ∧ Eval(Ao, β, w) > TrH, Tn) ∧ Bel(Aag, Do(Aag, α, Tα, w) ⇒ ¬Do(Ao, β, Tβ, w), Tn) ⇒ Pot.Int.To(Aag, α, Tn, Tα, w) This axiom is a basic component of any adversarial environment.",
                "For example, looking at a Chess board game, a player could realize that it is about to be checkmated by its opponent, thus making a preventive move.",
                "Another example is a Connect Four game: when a player has a row of three chips, its opponent must block it, or lose.",
                "A specific instance of A1 occurs when the adversary is one action away from achieving its goal, and immediate preventive action needs to be taken by the agent.",
                "Formally, we have the same beliefs as stated above, with a changed belief that doing action β will cause agent Ao to achieve its goal.",
                "Proposition 1: Prevent or lose case. (∀Aag, Ao ∈ A, α ∈ CAag , β ∈ CAo , G∗ Ao , Tn, Tα, Tβ, w ∈ W) Bel(Aag, Do(Ao, β, Tβ, w) ⇒ Achieve(G∗ Ao , β, w), Tn) ∧ Bel(Aag, Do(Aag, α, Tα, w) ⇒ ¬Do(Ao, β, Tβ, w)) ⇒ Pot.Int.To(Aag, α, Tn, Tα, w) Sketch of proof: Proposition 1 can be easily derived from axiom A1 and the property 2 of the Eval function, which states that any action that causes a completion of a goal is a highly beneficial action.",
                "The preventive act behavior will occur implicitly when the Eval function is equal to the real world utility function.",
                "However, being bounded rational agents and dealing with an estimated evaluation function we need to explicitly axiomatize such behavior, for it will not always occur implicitly from the evaluation function.",
                "A3.",
                "Suboptimal Tactical Move Axiom.",
                "In many scenarios a situation may occur where an agent will decide not to take the current most beneficial action it can take (the action with the maximal utility evaluation value), because it believes that taking another action (with lower utility evaluation value) might yield (depending on the adversarys response) a future possibility for a highly beneficial action.",
                "This will occur most often when the Eval function is inaccurate and differs by a large extent from the Utility function.",
                "Put formally, agent Aag believes in a certain SetAction that will evolve according to its initial action and will yield a high beneficial value (> TrH) solely for it. (∀Aag, Ao ∈ A, Tn, w ∈ W) (∃α1 , . . . , αu ∈ CAi , β1 , . . . , βv ∈ CAj , Tα1 ) Bel(Aag, SetAction(α1 , . . . , αu , β1 , . . . , βv ), Tn) ∧ Bel(Aag, Eval(Ao, βv , w) < TrH < Eval(Aag, αu , w), Tn) ⇒ Pot.Int.To(Aag, α1 , Tn, Tα1 , w) An agent might believe that a chain of events will occur for various reasons due to the inevitable nature of the domain.",
                "For example, in Chess, we often observe the following: a move causes a check position, which in turn limits the opponents moves to avoiding the check, to which the first player might react with another check, and so on.",
                "The agent might also believe in a chain of events based on its knowledge of its adversarys profile, which allows it to foresee the adversarys movements with high accuracy.",
                "A4.",
                "Profile Detection Axiom.",
                "The agent can adjust its adversarys profiles by observations and pattern study (specifically, if there are repeated encounters with the same adversary).",
                "However, instead of waiting for profile information to be revealed, an agent can also initiate actions that will force its adversary to react in a way that will reveal profile knowledge about it.",
                "Formally, the axiom states that if all actions (γ) are not highly beneficial actions (< TrH), the agent can do action α in time Tα if it believes that it will result in a non-highly beneficial action β from its adversary, which in turn teaches it about the adversarys profile, i.e., gives a higher AdvKnow(P Aj Ai , Tβ). (∀Aag, Ao ∈ A, α ∈ CAag , β ∈ CAo , Tn, Tα, Tβ, w ∈ W) Bel(Aag, (∀γ ∈ CAag )Eval(Aag, γ, w) < TrH, Tn) ∧ Bel(Aag, Do(Aag, α, Tα, w) ⇒ Do(Ao, β, Tβ, w), Tn) ∧ Bel(Aag, Eval(Ao, β, w) < TrH) ∧ Bel(Aag, AdvKnow(P Aj Ai , Tβ) > AdvKnow(P Aj Ai , Tn), Tn) ⇒ Pot.Int.To(Aag, α, Tn, Tα, w) For example, going back to the Chess board game scenario, consider starting a game versus an opponent about whom we know nothing, not even if it is a human or a computerized opponent.",
                "We might start playing a strategy that will be suitable versus an average opponent, and adjust our game according to its level of play.",
                "A5.",
                "Alliance Formation Axiom The following behavioral axiom is relevant only in a multilateral instantiation of the adversarial environment (obviously, an alliance cannot be formed in a bilateral, zero-sum encounter).",
                "In different situations during a multilateral interaction, a group of agents might believe that it is in their best interests to form a temporary alliance.",
                "Such an alliance is an agreement that constrains its members behavior, but is believed by its members to enable them to achieve a higher utility value than the one achievable outside of the alliance.",
                "As an example, we can look at the classical Risk board game, where each player has an individual goal of being the sole conquerer of the world, a zero-sum game.",
                "However, in order to achieve this goal, it might be strategically wise to make short-term ceasefire agreements with other players, or to join forces and attack an opponent who is stronger than the rest.",
                "An alliances terms defines the way its members should act.",
                "It is a set of predicates, denoted as Terms, that is agreed upon by the alliance members, and should remain true for the duration of the alliance.",
                "For example, the set Terms in the Risk scenario, could contain the following predicates: 1.",
                "Alliance members will not attack each other on territories X, Y and Z; 2.",
                "Alliance members will contribute C units per turn for attacking adversary Ao; 3.",
                "Members are obligated to stay as part of the alliance until time Tk or until adversarys Ao army is smaller than Q.",
                "The set Terms specifies inter-group constraints on each of the alliance members (∀Aal i ∈ Aal ⊆ A) set of actions Cal i ⊆ C. Definition 6.",
                "Al val - the total evaluation value that agent Ai will achieve while being part of Aal is the sum of Evali (Eval for Ai) of each of Aal j Eval values after taking their own α actions (via the agent(α) predicate): Al val(Ai, Cal , Aal , w) = α∈Cal Evali(Aal j , agent(α), w) Definition 7.",
                "Al TrH - is a number representing an Al val The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) 553 threshold; above it, the alliance can be said to be a highly beneficial alliance.",
                "The value of Al TrH will be calculated dynamically according to the progress of the interaction, as can be seen in [7].",
                "After an alliance is formed, its members are now working in their normal adversarial environment, as well as according to the mental states and axioms required for their interactions as part of the alliance.",
                "The following Alliance model (AL) specifies the conditions under which the group Aal can be said to be in an alliance and working with a new and constrained set of actions Cal , at time Tn.",
                "AL(Aal , Cal , w, Tn) 1.",
                "Aal has a MB that all members are part of Aal : MB(Aal , (∀Aal i ∈ Aal )member(Aal i , Aal ), Tn) 2.",
                "Aal has a MB that the group be maintained: MB(Aal , (∀Aal i ∈ Aal )Int.Th (Ai, member(Ai, Aal ), Tn, Tn+1, Co), Tn) 3.",
                "Aal has a MB that being members gives them high utility value: MB(Aal , (∀Aal i ∈ Aal )Al val(Aal i , Cal , Aal , w) ≥ Al TrH, Tn) Members profiles are a crucial part of successful alliances.",
                "We assume that agents that have more accurate profiles of their adversaries will be more successful in such environments.",
                "Such agents will be able to predict when a member is about to breach the alliances contract (item 2 in the above model), and take counter measures (when item 3 will falsify).",
                "The robustness of the alliance is in part a function of its members trustfulness measure, objective position estimation, and other profile properties.",
                "We should note that an agent can simultaneously be part of more than one alliance.",
                "Such a temporary alliance, where the group members do not have a joint goal but act collaboratively for the interest of their own individual goals, is classified as a Treatment Group by modern psychologists [12] (in contrast to a Task Group, where its members have a joint goal).",
                "The Shared Activity model as presented in [5] modeled Treatment Group behavior using the same SharedPlans formalization.",
                "When comparing both definitions of an alliance and a Treatment Group we found an unsurprising resemblance between both models: the environment models definitions are almost identical (see SAs definitions in [5]), and their Selfish-Act and Cooperative Act axioms conform to our adversarial agents behavior.",
                "The main distinction between both models is the integration of a Helpful-behavior act axiom, in the Shared Activity which cannot be part of ours.",
                "This axiom states that an agent will consider taking action that will lower its Eval value (to a certain lower bound), if it believes that a group partner will gain a significant benefit.",
                "Such behavior cannot occur in a pure adversarial environment (as a zero-sum game is), where the alliance members are constantly on watch to manipulate their alliance to their own advantage.",
                "A6.",
                "Evaluation Maximization Axiom.",
                "In a case when all other axioms are inapplicable, we will proceed with the action that maximizes the heuristic value as computed in the Eval function. (∀Aag, Ao ∈ A, α ∈ Cag, Tn, w ∈ W) Bel(Aag, (∀γ ∈ Cag)Eval(Aag, α, w) ≥ Eval(Aag, γ, w), Tn) ⇒ Pot.Int.To(Aag, α, Tn, Tα, w) T1.",
                "Optimality on Eval = Utility The above axiomatic model handles situations where the Utility is unknown and the agents are bounded rational agents.",
                "The following theorem shows that in bilateral interactions, where the agents have the real Utility function (i.e., Eval = Utility) and are rational agents, the axioms provide the same optimal result as classic adversarial search (e.g., Min-Max).",
                "Theorem 1.",
                "Let Ae ag be an unbounded rational AE agent using the Eval heuristic evaluation function, Au ag be the same agent using the true Utility function, and Ao be a sole unbounded utility-based rational adversary.",
                "Given that Eval = Utility: (∀α ∈ CAu ag , α ∈ CAe ag , Tn, w ∈ W) Pot.Int.To(Au ag, α, Tn, Tα, w) → Pot.Int.To(Ae ag, α , Tn, Tα, w) ∧ ((α = α ) ∨ (Utility(Au ag, α, w) = Eval(Ae ag, α , w))) Sketch of proof - Given that Au ag has the real utility function and unbounded resources, it can generate the full game tree and run the optimal MinMax algorithm to choose the highest utility value action, which we denote by, α.",
                "The proof will show that Ae ag, using the AE axioms, will select the same or equal utility α (when there is more than one action with the same max utility) when Eval = Utility. (A1) Goal achieving axiom - suppose there is an α such that its completion will achieve Au ags goal.",
                "It will obtain the highest utility by Min-Max for Au ag.",
                "The Ae ag agent will select α or another action with the same utility value via A1.",
                "If such α does not exist, Ae ag cannot apply this axiom, and proceeds to A2. (A2) Preventive act axiom - (1) Looking at the basic case (see Prop1), if there is a β which leads Ao to achieve its goal, then a preventive action α will yield the highest utility for Au ag.",
                "Au ag will choose it through the utility, while Ae ag will choose it through A2. (2) In the general case, β is a highly beneficial action for Ao, thus yields low utility for Au ag, which will guide it to select an α that will prevent β, while Ae ag will choose it through A2.1 If such β does not exist for Ao, then A2 is not applicable, and Ae ag can proceed to A3. (A3) Suboptimal tactical move axiom - When using a heuristic Eval function, Ae ag has a partial belief in the profile of its adversary (item 4 in AE model), which may lead it to believe in SetActions (Prop1).",
                "In our case, Ae ag is holding a full profile on its optimal adversary and knows that Ao will behave optimally according to the real utility values on the complete search tree, therefore, any belief about suboptimal SetAction cannot exist, yielding this axiom inapplicable.",
                "Ae ag will proceed to A4. (A4) Profile detection axiom - Given that Ae ag has the full profile of Ao, none of Ae ags actions can increase its knowledge.",
                "That axiom will not be applied, and the agent will proceed with A6 (A5 will be disregarded because the interaction is bilateral). (A6) Evaluation maximization axiom - This axiom will select the max Eval for Ae ag.",
                "Given that Eval = Utility, the same α that was selected by Au ag will be selected. 3.",
                "EVALUATION The main purpose of our experimental analysis is to evaluate the models behavior and performance in a real adversarial environment.",
                "This section investigates whether bounded 1 A case where following the completion of β there exists a γ which gives high utility for Agent Au ag, cannot occur because Ao uses the same utility, and γs existence will cause it to classify β as a low utility action. 554 The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) rational agents situated in such adversarial environments will be better off applying our suggested behavioral axioms. 3.1 The Domain To explore the use of the above model and its behavioral axioms, we decided to use the Connect-Four game as our adversarial environment.",
                "Connect-Four is a 2-player, zerosum game which is played using a 6x7 matrix-like board.",
                "Each turn, a player drops a disc into one of the 7 columns (the set of 21 discs is usually colored yellow for player 1 and red for player 2; we will use White and Black respectively to avoid confusion).",
                "The winner is the first player to complete a horizontal, vertical, or diagonal set of four discs with its color.",
                "On very rare occasions, the game might end in a tie if all the empty grids are filled, but no player managed to create a 4-disc set.",
                "The Connect-Four game was solved in [1], where it is shown that the first player (playing with the white discs) can force a win by starting in the middle column (column 4) and playing optimally However, the optimal strategy is very complex, and difficult to follow even for complex bounded rational agents, such as human players.",
                "Before we can proceed checking agent behavior, we must first verify that the domain conforms to the adversarial environments definition as given above (which the behavioral axioms are based on).",
                "First, when playing a Connect-Four game, the agent has an intention to win the game (item 1).",
                "Second (item 2), our agent believes that in Connect-Four there can only be one winner (or no winner at all in the rare occurrence of a tie).",
                "In addition, our agent believes that its opponent to the game will try to win (item 3), and we hope it has some partial knowledge (item 4) about its adversary (this knowledge can vary from nothing, through simple facts such as age, to strategies and weaknesses).",
                "Of course, not all Connect-Four encounters are adversarial.",
                "For example, when a parent is playing the game with its child, the following situation might occur: the child, having a strong incentive to win, treats the environment as adversarial (it intends to win, understands that there can only be one winner, and believes that its parent is trying to beat him).",
                "However, the parents point of view might see the environment as an educational one, where its goal is not to win the game, but to cause enjoyment or practice strategic reasoning.",
                "In such an educational environment, a new set of behavioral axioms might be more beneficial to the parents goals than our suggested adversarial behavioral axioms. 3.2 Axiom Analysis After showing that the Connect-Four game is indeed a zero-sum, bilateral adversarial environment, the next step is to look at players behaviors during the game and check whether behaving according to our model does improve performance.",
                "To do so we have collected log files from completed Connect-Four games that were played by human players over the Internet.",
                "Our collected log file data came from Play by eMail (PBeM) sites.",
                "These are web sites that host email games, where each move is taken by an email exchange between the server and the players.",
                "Many such sites archives contain real competitive interactions, and also maintain a ranking system for their members.",
                "Most of the data we used can be found in [6].",
                "As can be learned from [1], Connect-Four has an optimal strategy and a considerable advantage for the player who starts the game (which we call the White player).",
                "We will concentrate in our analysis on the second players moves (to be called Black).",
                "The White player, being the first to act, has the so-called initiative advantage.",
                "Having the advantage and a good strategy will keep the Black player busy reacting to its moves, instead of initiating threats.",
                "A threat is a combination of three discs of the same color, with an empty spot for the fourth winning disk.",
                "An open threat is a threat that can be realized in the opponents next move.",
                "In order for the Black player to win, it must somehow turn the tide, take the advantage and start presenting threats to the White player.",
                "We will explore Black players behavior and their conformance to our axioms.",
                "To do so, we built an application that reads log files and analyzes the Black players moves.",
                "The application contains two main components: (1) a Min-Max algorithm for evaluation of moves; (2) open threats detector for the discovering of open threats.",
                "The Min-Max algorithm will work to a given depth, d and for each move α will output the heuristic value for the next action taken by the player as written in the log file, h(α), alongside the maximum heuristic value, maxh(α), that could be achieved prior to taking the move (obviously, if h(α) = maxh(α), then the player did not do the optimal move heuristically).",
                "The threat detectors job is to notify if some action was taken in order to block an open threat (not blocking an open threat will probably cause the player to lose in the opponents next move).",
                "The heuristic function used by Min-Max to evaluate the players utility is the following function, which is simple to compute, yet provides a reasonable challenge to human opponents: Definition 8.",
                "Let Group be an adjacent set of four squares that are horizontal, vertical, or diagonal.",
                "Groupn b (Groupn w) be a Group with n pieces of the black (white) color and 4−n empty squares. h = ((Group1 b ∗α)+(Group2 b ∗β)+(Group3 b ∗γ)+(Group4 b ∗∞)) − ((Group1 w ∗α)+(Group2 w ∗β)+(Group3 w ∗γ)+(Group4 w ∗∞)) The values of α, β and δ can vary to form any desired linear combination; however, it is important to value them with the α < β < δ ordering in mind (we used 1, 4, and 8 as their respective values).",
                "Groups of 4 discs of the same color means victory, thus discovery of such a group will result in ∞ to ensure an extreme value.",
                "We now use our estimated evaluation function to evaluate the Black players actions during the Connect-Four adversarial interaction.",
                "Each game from the log file was input into the application, which processed and output a reformatted log file containing the h value of the current move, the maxh value that could be achieved, and a notification if an open threat was detected.",
                "A total of 123 games were analyzed (57 with White winning, and 66 with Black winning).",
                "A few additional games were manually ignored in the experiment, due to these problems: a player abandoning the game while the outcome is not final, or a blunt irrational move in the early stages of the game (e.g., not blocking an obvious winning group in the first opening moves).",
                "In addition, a single tie game was also removed.",
                "The simulator was run to a search depth of 3 moves.",
                "We now proceed to analyze the games with respect to each behavioral axiom.",
                "The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) 555 Table 1: Average heuristic difference analysis Black losses Black Won Avg minh -17.62 -12.02 Avg 3 lowest h moves (min3 h) -13.20 -8.70 3.2.1 Affirming the Suboptimal tactical move axiom The following section presents the heuristic evaluations of the Min-Max algorithm for each action, and checks the amount and extent of suboptimal tactical actions and their implications on performance.",
                "Table 1 shows results and insights from the games heuristic analysis, when search depth equals 3 (this search depth was selected for the results to be comparable to [9], see Section 3.2.3).",
                "The tables heuristic data is the difference between the present maximal heuristic value and the heuristic value of the action that was eventually taken by the player (i.e., the closer the number is to 0, the closer the action was to the maximum heuristic action).",
                "The first row presents the difference values of the action that had the maximal difference value among all the Black players actions in a given game, as averaged over all Blacks winning and losing games (see respective columns).",
                "In games in which the Black player loses, its average difference value was -17.62, while in games in which the Black player won, its average was -12.02.",
                "The second row expands the analysis by considering the 3 highest heuristic difference actions, and averaging them.",
                "In that case, we notice an average heuristic difference of 5 points between games which the Black player loses and games in which it wins.",
                "Nevertheless, the importance of those numbers is that they allowed us to take an educated guess on a threshold number of 11.5, as the value of the TrH constant, which differentiates between normal actions and highly beneficial ones.",
                "After finding an approximated TrH constant, we can proceed with an analysis of the importance of suboptimal moves.",
                "To do so we took the subset of games in which the minimum heuristic difference value for Blacks actions was 11.5.",
                "As presented in Table 2, we can see the different min3 h average of the 3 largest ranges and the respective percentage of games won.",
                "The first row shows that the Black player won only 12% of the games in which the average of its 3 highest heuristically difference actions (min3 h) was smaller than the suggested threshold, TrH = 11.5.",
                "The second row shows a surprising result: it seems that when min3 h > −4 the Black player rarely wins.",
                "Intuition would suggest that games in which the action evaluation values were closer to the maximal values will result in more winning games for Black.",
                "However, it seems that in the Connect-Four domain, merely responding with somewhat easily expected actions, without initiating a few surprising and suboptimal moves, does not yield good results.",
                "The last row sums up the main insights from the analysis; most of Blacks wins (83%) came when its min3 h was in the range of -11.5 to -4.",
                "A close inspection of those Black winning games shows the following pattern behind the numbers: after standard opening moves, Black suddenly drops a disc into an isolated column, which seems a waste of a move.",
                "White continues to build its threats, while usually disregarding Blacks last move, which in turn uses the isolated disc as an anchor for a future winning threat.",
                "The results show that it was beneficial for the Black player Table 2: Blacks winnings percentages % of games min3 h < −11.5 12% min3 h > −4 5% −11.5 ≤ min3 h ≤ −4 83% to take suboptimal actions and not give the current highest possible heuristic value, but will not be too harmful for its position (i.e., will not give high beneficial value to its adversary).",
                "As it turned out, learning the threshold is an important aspect of success: taking wildly risky moves (min3 h < −11.5) or trying to avoid them (min3 h > −4) reduces the Black players winning chances by a large margin. 3.2.2 Affirming the Profile Monitoring Axiom In the task of showing the importance of monitoring ones adversaries profiles, our log files could not be used because they did not contain repeated interactions between players, which are needed to infer the players knowledge about their adversaries.",
                "However, the importance of opponent modeling and its use in attaining tactical advantages was already studied in various domains ([3, 9] are good examples).",
                "In a recent paper, Markovitch and Reger [9] explored the notion of learning and exploitation of opponent weakness in competitive interactions.",
                "They apply simple learning strategies by analyzing examples from past interactions in a specific domain.",
                "They also used the Connect-Four adversarial domain, which can now be used to understand the importance of monitoring the adversarys profile.",
                "Following the presentation of their theoretical model, they describe an extensive <br>empirical study</br> and check the agents performance after learning the weakness model with past examples.",
                "One of the domains used as a competitive environment was the same Connect-Four game (Checkers was the second domain).",
                "Their heuristic function was identical to ours with three different variations (H1, H2, and H3) that are distinguished from one another in their linear combination coefficient values.",
                "The search depth for the players was 3 (as in our analysis).",
                "Their extensive experiments check and compare various learning strategies, risk factors, predefined feature sets and usage methods.",
                "The bottom line is that the Connect-Four domain shows an improvement from a 0.556 winning rate before modeling to a 0.69 after modeling (page 22).",
                "Their conclusions, showing improved performance when holding and using the adversarys model, justify the effort to monitor the adversary profile for continuous and repeated interactions.",
                "An additional point that came up in their experiments is the following: after the opponent weakness model has been learned, the authors describe different methods of integrating the opponent weakness model into the agents decision strategy.",
                "Nevertheless, regardless of the specific method they chose to work with, all integration methods might cause the agent to take suboptimal decisions; it might cause the agent to prefer actions that are suboptimal at the present decision junction, but which might cause the opponent to react in accordance with its weakness model (as represented by our agent) which in turn will be beneficial for us in the future.",
                "The agents behavior, as demonstrated in [9] further confirms and strengthens our Suboptimal Tactical Axiom as discussed in the previous section. 556 The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) 3.2.3 Additional Insights The need for the Goal Achieving, Preventive Act, and Evaluation Maximization axioms are obvious, and need no further verification.",
                "However, even with respect to those axioms, a few interesting insights came up in the log analysis.",
                "The Goal achieving and Preventive Act axioms, though theoretically trivial, seem to provide some challenge to a human player.",
                "In the initial inspection of the logs, we encountered few games2 where a player, for inexplicable reasons, did not block the other from winning or failed to execute its own winning move.",
                "We can blame those faults on the humans lack of attention, or a typing error in its move reply; nevertheless, those errors might occur in bounded rational agents, and the appropriate behavior needs to be axiomatized.",
                "A typical Connect-Four game revolves around generating threats and blocking them.",
                "In our analysis we looked for explicit preventive actions, i.e., moves that block a group of 3 discs, or that remove a future threat (in our limited search horizon).",
                "We found that in 83% of the total games there was at least one preventive action taken by the Black player.",
                "It was also found that Black averaged 2.8 preventive actions per game on the games in which it lost, while averaging 1.5 preventive actions per game when winning.",
                "It seems that Black requires 1 or 2 preventive actions to build its initial taking position, before starting to present threats.",
                "If it did not manage to win, it will usually prevent an extra threat or two before succumbing to White. 4.",
                "RELATED WORK Much research deals with the axiomatization of teamwork and mental states of individuals: some models use knowledge and belief [10], others have models of goals and intentions [8, 4].",
                "However, all these formal theories deal with agent teamwork and cooperation.",
                "As far as we know, our model is the first to provide a formalized model for explicit adversarial environments and agents behavior in it.",
                "The classical Min-Max adversarial search algorithm was the first attempt to integrate the opponent into the search space with a weak assumption of an optimally playing opponent.",
                "Since then, much effort has gone into integrating the opponent model into the decision procedure to predict future behavior.",
                "The M∗ algorithm presented by Carmel and Markovitch [2] showed a method of incorporating opponent models into adversary search, while in [3] they used learning to provide a more accurate opponent model in a 2player repeated game environment, where agents strategies were modeled as finite automata.",
                "Additional Adversarial planning work was done by Willmott et al. [13], which provided an adversarial planning approach to the game of GO.",
                "The research mentioned above dealt with adversarial search and the integration of opponent models into classical utilitybased search methods.",
                "That work shows the importance of opponent modeling and the ability to exploit it to an agents advantage.",
                "However, the basic limitations of those search methods still apply; our model tries to overcome those limitations by presenting a formal model for a new, mental state-based adversarial specification. 5.",
                "CONCLUSIONS We presented an Adversarial Environment model for a 2 These were later removed from the final analysis. bounded rational agent that is situated in an N-player, zerosum environment.",
                "We used the SharedPlans formalization to define the model and the axioms that agents can apply as behavioral guidelines.",
                "The model is meant to be used as a guideline for designing agents that need to operate in such adversarial environments.",
                "We presented empirical results, based on ConnectFour log file analysis, that exemplify the model and the axioms for a bilateral instance of the environment.",
                "The results we presented are a first step towards an expanded model that will cover all types of adversarial environments, for example, environments that are non-zero-sum, and environments that contain natural agents that are not part of the direct conflict.",
                "Those challenges and more will be dealt with in future research. 6.",
                "ACKNOWLEDGMENT This research was supported in part by Israel Science Foundation grants #1211/04 and #898/05. 7.",
                "REFERENCES [1] L. V. Allis.",
                "A knowledge-based approach of Connect-Four - the game is solved: White wins.",
                "Masters thesis, Free University, Amsterdam, The Netherlands, 1988. [2] D. Carmel and S. Markovitch.",
                "Incorporating opponent models into adversary search.",
                "In Proceedings of the Thirteenth National Conference on Artificial Intelligence, pages 120-125, Portland, OR, 1996. [3] D. Carmel and S. Markovitch.",
                "Opponent modeling in multi-agent systems.",
                "In G. Weiß and S. Sen, editors, Adaptation and Learning in Multi-Agent Systems, pages 40-52.",
                "Springer-Verlag, 1996. [4] B. J. Grosz and S. Kraus.",
                "Collaborative plans for complex group action.",
                "Artificial Intelligence, 86(2):269-357, 1996. [5] M. Hadad, G. Kaminka, G. Armon, and S. Kraus.",
                "Supporting collaborative activity.",
                "In Proc. of AAAI-2005, pages 83-88, Pittsburgh, 2005. [6] http://www.gamerz.net/˜pbmserv/. [7] S. Kraus and D. Lehmann.",
                "Designing and building a negotiating automated agent.",
                "Computational Intelligence, 11:132-171, 1995. [8] H. J. Levesque, P. R. Cohen, and J. H. T. Nunes.",
                "On acting together.",
                "In Proc. of AAAI-90, pages 94-99, Boston, MA, 1990. [9] S. Markovitch and R. Reger.",
                "Learning and exploiting relative weaknesses of opponent agents.",
                "Autonomous Agents and Multi-Agent Systems, 10(2):103-130, 2005. [10] Y. M. Ronald Fagin, Joseph Y. Halpern and M. Y. Vardi.",
                "Reasoning about knowledge.",
                "MIT Press, Cambridge, Mass., 1995. [11] P. Thagard.",
                "Adversarial problem solving: Modeling an oponent using explanatory coherence.",
                "Cognitive Science, 16(1):123-149, 1992. [12] R. W. Toseland and R. F. Rivas.",
                "An Introduction to Group Work Practice.",
                "Prentice Hall, Englewood Cliffs, NJ, 2nd edition edition, 1995. [13] S. Willmott, J. Richardson, A. Bundy, and J. Levine.",
                "An adversarial planning approach to Go.",
                "Lecture Notes in Computer Science, 1558:93-112, 1999.",
                "The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) 557"
            ],
            "original_annotated_samples": [],
            "translated_annotated_samples": [
                "Después de la presentación de su modelo teórico, describen un extenso \"estudio empírico\" y verifican el rendimiento de los agentes después de aprender el modelo de debilidad con ejemplos pasados.estudio empírico"
            ],
            "translated_text": "",
            "candidates": [],
            "error": []
        },
        "axiomatized model": {
            "translated_key": "",
            "is_in_text": true,
            "original_annotated_sentences": [
                "An Adversarial Environment Model for Bounded Rational Agents in Zero-Sum Interactions Inon Zuckerman1 , Sarit Kraus1 , Jeffrey S. Rosenschein2 , Gal Kaminka1 1 Department of Computer Science 2 The School of Engineering Bar-Ilan University and Computer Science Ramat-Gan, Israel Hebrew University, Jerusalem, Israel {zukermi,sarit,galk}@cs.biu.ac.il jeff@cs.huji.ac.il ABSTRACT Multiagent environments are often not cooperative nor collaborative; in many cases, agents have conflicting interests, leading to adversarial interactions.",
                "This paper presents a formal Adversarial Environment model for bounded rational agents operating in a zero-sum environment.",
                "In such environments, attempts to use classical utility-based search methods can raise a variety of difficulties (e.g., implicitly modeling the opponent as an omniscient utility maximizer, rather than leveraging a more nuanced, explicit opponent model).",
                "We define an Adversarial Environment by describing the mental states of an agent in such an environment.",
                "We then present behavioral axioms that are intended to serve as design principles for building such adversarial agents.",
                "We explore the application of our approach by analyzing log files of completed Connect-Four games, and present an empirical analysis of the axioms appropriateness.",
                "Categories and Subject Descriptors I.2.11 [Artificial Intelligence]: Distributed Artificial Intelligence-Intelligent agents,Multiagent Systems; I.2.4 [Artificial Intelligence]: Knowledge Representation Formalisms and Methods -Modal logic General Terms Design, Theory 1.",
                "INTRODUCTION Early research in multiagent systems (MAS) considered cooperative groups of agents; because individual agents had limited resources, or limited access to information (e.g., limited processing power, limited sensor coverage), they worked together by design to solve problems that individually they could not solve, or at least could not solve as efficiently.",
                "MAS research, however, soon began to consider interacting agents with individuated interests, as representatives of different humans or organizations with non-identical interests.",
                "When interactions are guided by diverse interests, participants may have to overcome disagreements, uncooperative interactions, and even intentional attempts to damage one another.",
                "When these types of interactions occur, environments require appropriate behavior from the agents situated in them.",
                "We call these environments Adversarial Environments, and call the clashing agents Adversaries.",
                "Models of cooperation and teamwork have been extensively explored in MAS through the axiomatization of mental states (e.g., [8, 4, 5]).",
                "However, none of this research dealt with adversarial domains and their implications for agent behavior.",
                "Our paper addresses this issue by providing a formal, axiomatized mental state model for a subset of adversarial domains, namely simple zero-sum adversarial environments.",
                "Simple zero-sum encounters exist of course in various twoplayer games (e.g., Chess, Checkers), but they also exist in n-player games (e.g., Risk, Diplomacy), auctions for a single good, and elsewhere.",
                "In these latter environments especially, using a utility-based adversarial search (such as the Min-Max algorithm) does not always provide an adequate solution; the payoff function might be quite complex or difficult to quantify, and there are natural computational limitations on bounded rational agents.",
                "In addition, traditional search methods (like Min-Max) do not make use of a model of the opponent, which has proven to be a valuable addition to adversarial planning [9, 3, 11].",
                "In this paper, we develop a formal, <br>axiomatized model</br> for bounded rational agents that are situated in a zero-sum adversarial environment.",
                "The model uses different modality operators, and its main foundations are the SharedPlans [4] model for collaborative behavior.",
                "We explore environment properties and the mental states of agents to derive behavioral axioms; these behavioral axioms constitute a formal model that serves as a specification and design guideline for agent design in such settings.",
                "We then investigate the behavior of our model empirically using the Connect-Four board game.",
                "We show that this game conforms to our environment definition, and analyze players behavior using a large set of completed match log 550 978-81-904262-7-5 (RPS) c 2007 IFAAMAS files.",
                "In addition, we use the results presented in [9] to discuss the importance of opponent modeling in our ConnectFour adversarial domain.",
                "The paper proceeds as follows.",
                "Section 2 presents the models formalization.",
                "Section 3 presents the empirical analysis and its results.",
                "We discuss related work in Section 4, and conclude and present future directions in Section 5. 2.",
                "ADVERSARIAL ENVIRONMENTS The adversarial environment model (denoted as AE) is intended to guide the design of agents by providing a specification of the capabilities and mental attitudes of an agent in an adversarial environment.",
                "We focus here on specific types of adversarial environments, specified as follows: 1.",
                "Zero-Sum Interactions: positive and negative utilities of all agents sum to zero; 2.",
                "Simple AEs: all agents in the environment are adversarial agents; 3.",
                "Bilateral AEs: AEs with exactly two agents; 4.",
                "Multilateral AEs: AEs of three or more agents.",
                "We will work on both bilateral and multilateral instantiations of zero-sum and simple environments.",
                "In particular, our adversarial environment model will deal with interactions that consist of N agents (N ≥ 2), where all agents are adversaries, and only one agent can succeed.",
                "Examples of such environments range from board games (e.g., Chess, Connect-Four, and Diplomacy) to certain economic environments (e.g., N-bidder auctions over a single good). 2.1 Model Overview Our approach is to formalize the mental attitudes and behaviors of a single adversarial agent; we consider how a single agent perceives the AE.",
                "The following list specifies the conditions and mental states of an agent in a simple, zero-sum AE: 1.",
                "The agent has an individual intention that its own goal will be completed; 2.",
                "The agent has an individual belief that it and its adversaries are pursuing full conflicting goals (defined below)there can be only one winner; 3.",
                "The agent has an individual belief that each adversary has an intention to complete its own full conflicting goal; 4.",
                "The agent has an individual belief in the (partial) profile of its adversaries.",
                "Item 3 is required, since it might be the case that some agent has a full conflicting goal, and is currently considering adopting the intention to complete it, but is, as of yet, not committed to achieving it.",
                "This might occur because the agent has not yet deliberated about the effects that adopting that intention might have on the other intentions it is currently holding.",
                "In such cases, it might not consider itself to even be in an adversarial environment.",
                "Item 4 states that the agent should hold some belief about the profiles of its adversaries.",
                "The profile represents all the knowledge the agent has about its adversary: its weaknesses, strategic capabilities, goals, intentions, trustworthiness, and more.",
                "It can be given explicitly or can be learned from observations of past encounters. 2.2 Model Definitions for Mental States We use Grosz and Krauss definitions of the modal operators, predicates, and meta-predicates, as defined in their SharedPlan formalization [4].",
                "We recall here some of the predicates and operators that are used in that formalization: Int.To(Ai, α, Tn, Tα, C) represents Ais intentions at time Tn to do an action α at time Tα in the context of C. Int.Th(Ai, prop, Tn, Tprop, C) represents Ais intentions at time Tn that a certain proposition prop holds at time Tprop in the context of C. The potential intention operators, Pot.Int.To(...) and Pot.Int.Th(...), are used to represent the mental state when an agent considers adopting an intention, but has not deliberated about the interaction of the other intentions it holds.",
                "The operator Bel(Ai, f, Tf ) represents agent Ai believing in the statement expressed in formula f, at time Tf .",
                "MB(A, f, Tf ) represents mutual belief for a group of agents A.",
                "A snapshot of the system finds our environment to be in some state e ∈ E of environmental variable states, and each adversary in any LAi ∈ L of possible local states.",
                "At any given time step, the system will be in some world w of the set of all possible worlds w ∈ W, where w = E×LA1 ×LA2 × ...LAn , and n is the number of adversaries.",
                "For example, in a Texas Holdem poker game, an agents local state might be its own set of cards (which is unknown to its adversary) while the environment will consist of the betting pot and the community cards (which are visible to both players).",
                "A utility function under this formalization is defined as a mapping from a possible world w ∈ W to an element in , which expresses the desirability of the world, from a single agent perspective.",
                "We usually normalize the range to [0,1], where 0 represents the least desirable possible world, and 1 is the most desirable world.",
                "The implementation of the utility function is dependent on the domain in question.",
                "The following list specifies new predicates, functions, variables, and constants used in conjunction with the original definitions for the adversarial environment formalization: 1. φ is a null action (the agent does not do anything). 2.",
                "GAi is the set of agent Ais goals.",
                "Each goal is a set of predicates whose satisfaction makes the goal complete (we use G∗ Ai ∈ GAi to represent an arbitrary goal of agent Ai). 3. gAi is the set of agent Ais subgoals.",
                "Subgoals are predicates whose satisfaction represents an important milestone toward achievement of the full goal. gG∗ Ai ⊆ gAi is the set of subgoals that are important to the completion of goal G∗ Ai (we will use g∗ G∗ Ai ∈ gG∗ Ai to represent an arbitrary subgoal). 4.",
                "P Aj Ai is the profile object agent Ai holds about agent Aj. 5.",
                "CA is a general set of actions for all agents in A which are derived from the environments constraints.",
                "CAi ⊆ CA is the set of agent Ais possible actions. 6.",
                "Do(Ai, α, Tα, w) holds when Ai performs action α over time interval Tα in world w. 7.",
                "Achieve(G∗ Ai , α, w) is true when goal G∗ Ai is achieved following the completion of action α in world w ∈ W, where α ∈ CAi . 8.",
                "Profile(Ai, PAi Ai ) is true when agent Ai holds an object profile for agent Aj.",
                "Definition 1.",
                "Full conflict (FulConf ) describes a zerosum interaction where only a single goal of the goals in conflict can be completed.",
                "FulConf(G∗ Ai , G∗ Aj ) ⇒ (∃α ∈ CAi , ∀w, β ∈ CAj ) (Achieve(G∗ Ai , α, w) ⇒ ¬Achieve(G∗ Aj , β, w)) ∨ (∃β ∈ CAj , ∀w, α ∈ CAi )(Achieve(G∗ Aj , β, w) ⇒ ¬Achieve(G∗ Ai , α, w)) Definition 2.",
                "Adversarial Knowledge (AdvKnow) is a function returning a value which represents the amount of The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) 551 knowledge agent Ai has on the profile of agent Aj, at time Tn.",
                "The higher the value, the more knowledge agent Ai has.",
                "AdvKnow : P Aj Ai × Tn → Definition 3.",
                "Eval - This evaluation function returns an estimated expected utility value for an agent in A, after completing an action from CA in some world state w. Eval : A × CA × w → Definition 4.",
                "TrH - (Threshold) is a numerical constant in the [0,1] range that represents an evaluation function (Eval) threshold value.",
                "An action that yields an estimated utility evaluation above the TrH is regarded as a highly beneficial action.",
                "The Eval value is an estimation and not the real utility function, which is usually unknown.",
                "Using the real utility value for a rational agent would easily yield the best outcome for that agent.",
                "However, agents usually do not have the real utility functions, but rather a heuristic estimate of it.",
                "There are two important properties that should hold for the evaluation function: Property 1.",
                "The evaluation function should state that the most desirable world state is one in which the goal is achieved.",
                "Therefore, after the goal has been satisfied, there can be no future action that can put the agent in a world state with higher Eval value. (∀Ai, G∗ Ai , α, β ∈ CAi , w ∈ W) Achieve(G∗ Ai , α, w) ⇒ Eval(Ai, α, w) ≥ Eval(Ai, β, w) Property 2.",
                "The evaluation function should project an action that causes a completion of a goal or a subgoal to a value which is greater than TrH (a highly beneficial action). (∀Ai, G∗ Ai ∈ GAi , α ∈ CAi , w ∈ W, g∗ GAi ∈ gGAi ) Achieve(G∗ Ai , α, w) ∨ Achieve(g∗ GAi , α, w) ⇒ Eval(Ai, α, w) ≥ TrH.",
                "Definition 5.",
                "SetAction We define a set action (SetAction) as a set of action operations (either complex or basic actions) from some action sets CAi and CAj which, according to agent Ais belief, are attached together by a temporal and consequential relationship, forming a chain of events (action, and its following consequent action). (∀α1 , . . . , αu ∈ CAi , β1 , . . . , βv ∈ CAj , w ∈ W) SetAction(α1 , . . . , αu , β1 , . . . , βv , w) ⇒ ((Do(Ai, α1 , Tα1 , w) ⇒ Do(Aj, β1 , Tβ1 , w)) ⇒ Do(Ai, α2 , Tα2 , w) ⇒ . . . ⇒ Do(Ai, αu , Tαu , w)) The consequential relation might exist due to various environmental constraints (when one action forces the adversary to respond with a specific action) or due to the agents knowledge about the profile of its adversary.",
                "Property 3.",
                "As the knowledge we have about our adversary increases we will have additional beliefs about its behavior in different situations which in turn creates new set actions.",
                "Formally, if our AdvKnow at time Tn+1 is greater than AdvKnow at time Tn, then every SetAction known at time Tn is also known at time Tn+1.",
                "AdvKnow(P Aj Ai , Tn+1) > AdvKnow(P Aj Ai , Tn) ⇒ (∀α1 , . . . , αu ∈ CAi , β1 , . . . , βv ∈ CAj ) Bel(Aag, SetAction(α1 , . . . , αu , β1 , . . . , βv ), Tn) ⇒ Bel(Aag, SetAction(α1 , . . . , αu , β1 , . . . , βv ), Tn+1) 2.3 The Environment Formulation The following axioms provide the formal definition for a simple, zero-sum Adversarial Environment (AE).",
                "Satisfaction of these axioms means that the agent is situated in such an environment.",
                "It provides specifications for agent Aag to interact with its set of adversaries A with respect to goals G∗ Aag and G∗ A at time TCo at some world state w. AE(Aag, A, G∗ Aag , A1, . . . , Ak, G∗ A1 , . . . , G∗ Ak , Tn, w) 1.",
                "Aag has an Int.Th his goal would be completed: (∃α ∈ CAag , Tα) Int.Th(Aag, Achieve(G∗ Aag , α), Tn, Tα, AE) 2.",
                "Aag believes that it and each of its adversaries Ao are pursuing full conflicting goals: (∀Ao ∈ {A1, . . . , Ak}) Bel(Aag, FulConf(G∗ Aag , G∗ Ao ), Tn) 3.",
                "Aag believes that each of his adversaries in Ao has the Int.Th his conflicting goal G∗ Aoi will be completed: (∀Ao ∈ {A1, . . . , Ak})(∃β ∈ CAo , Tβ) Bel(Aag, Int.Th(Ao, Achieve(G∗ Ao , β), TCo, Tβ, AE), Tn) 4.",
                "Aag has beliefs about the (partial) profiles of its adversaries (∀Ao ∈ {A1, . . . , Ak}) (∃PAo Aag ∈ PAag )Bel(Aag, Profile(Ao, PAo Aag ), Tn) To build an agent that will be able to operate successfully within such an AE, we must specify behavioral guidelines for its interactions.",
                "Using a naive Eval maximization strategy to a certain search depth will not always yield satisfactory results for several reasons: (1) the search horizon problem when searching for a fixed depth; (2) the strong assumption of an optimally rational, unbounded resources adversary; (3) using an estimated evaluation function which will not give optimal results in all world states, and can be exploited [9].",
                "The following axioms specify the behavioral principles that can be used to differentiate between successful and less successful agents in the above Adversarial Environment.",
                "Those axioms should be used as specification principles when designing and implementing agents that should be able to perform well in such Adversarial Environments.",
                "The behavioral axioms represent situations in which the agent will adopt potential intentions to (Pot.Int.To(...)) perform an action, which will typically require some means-end reasoning to select a possible course of action.",
                "This reasoning will lead to the adoption of an Int.To(...) (see [4]).",
                "A1.",
                "Goal Achieving Axiom.",
                "The first axiom is the simplest case; when the agent Aag believes that it is one action (α) away from achieving his conflicting goal G∗ Aag , it should adopt the potential intention to do α and complete its goal. (∀Aag, α ∈ CAag , Tn, Tα, w ∈ W) (Bel(Aag, Do(Aag, α, Tα, w) ⇒ Achieve(G∗ Aag , α, w)) ⇒ Pot.Int.To(Aag, α, Tn, Tα, w) This somewhat trivial behavior is the first and strongest axiom.",
                "In any situation, when the agent is an action away from completing the goal, it should complete the action.",
                "Any fair Eval function would naturally classify α as the maximal value action (property 1).",
                "However, without explicit axiomatization of such behavior there might be situations where the agent will decide on taking another action for various reasons, due to its bounded decision resources.",
                "A2.",
                "Preventive Act Axiom.",
                "Being in an adversarial situation, agent Aag might decide to take actions that will damage one of its adversarys plans to complete its goal, even if those actions do not explicitly advance Aag towards its conflicting goal G∗ Aag .",
                "Such preventive action will take place when agent Aag has a belief about the possibility of its adversary Ao doing an action β that will give it a high 552 The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) utility evaluation value (> TrH).",
                "Believing that taking action α will prevent the opponent from doing its β, it will adopt a potential intention to do α. (∀Aag, Ao ∈ A, α ∈ CAag , β ∈ CAo , Tn, Tβ, w ∈ W) (Bel(Aag, Do(Ao, β, Tβ, w) ∧ Eval(Ao, β, w) > TrH, Tn) ∧ Bel(Aag, Do(Aag, α, Tα, w) ⇒ ¬Do(Ao, β, Tβ, w), Tn) ⇒ Pot.Int.To(Aag, α, Tn, Tα, w) This axiom is a basic component of any adversarial environment.",
                "For example, looking at a Chess board game, a player could realize that it is about to be checkmated by its opponent, thus making a preventive move.",
                "Another example is a Connect Four game: when a player has a row of three chips, its opponent must block it, or lose.",
                "A specific instance of A1 occurs when the adversary is one action away from achieving its goal, and immediate preventive action needs to be taken by the agent.",
                "Formally, we have the same beliefs as stated above, with a changed belief that doing action β will cause agent Ao to achieve its goal.",
                "Proposition 1: Prevent or lose case. (∀Aag, Ao ∈ A, α ∈ CAag , β ∈ CAo , G∗ Ao , Tn, Tα, Tβ, w ∈ W) Bel(Aag, Do(Ao, β, Tβ, w) ⇒ Achieve(G∗ Ao , β, w), Tn) ∧ Bel(Aag, Do(Aag, α, Tα, w) ⇒ ¬Do(Ao, β, Tβ, w)) ⇒ Pot.Int.To(Aag, α, Tn, Tα, w) Sketch of proof: Proposition 1 can be easily derived from axiom A1 and the property 2 of the Eval function, which states that any action that causes a completion of a goal is a highly beneficial action.",
                "The preventive act behavior will occur implicitly when the Eval function is equal to the real world utility function.",
                "However, being bounded rational agents and dealing with an estimated evaluation function we need to explicitly axiomatize such behavior, for it will not always occur implicitly from the evaluation function.",
                "A3.",
                "Suboptimal Tactical Move Axiom.",
                "In many scenarios a situation may occur where an agent will decide not to take the current most beneficial action it can take (the action with the maximal utility evaluation value), because it believes that taking another action (with lower utility evaluation value) might yield (depending on the adversarys response) a future possibility for a highly beneficial action.",
                "This will occur most often when the Eval function is inaccurate and differs by a large extent from the Utility function.",
                "Put formally, agent Aag believes in a certain SetAction that will evolve according to its initial action and will yield a high beneficial value (> TrH) solely for it. (∀Aag, Ao ∈ A, Tn, w ∈ W) (∃α1 , . . . , αu ∈ CAi , β1 , . . . , βv ∈ CAj , Tα1 ) Bel(Aag, SetAction(α1 , . . . , αu , β1 , . . . , βv ), Tn) ∧ Bel(Aag, Eval(Ao, βv , w) < TrH < Eval(Aag, αu , w), Tn) ⇒ Pot.Int.To(Aag, α1 , Tn, Tα1 , w) An agent might believe that a chain of events will occur for various reasons due to the inevitable nature of the domain.",
                "For example, in Chess, we often observe the following: a move causes a check position, which in turn limits the opponents moves to avoiding the check, to which the first player might react with another check, and so on.",
                "The agent might also believe in a chain of events based on its knowledge of its adversarys profile, which allows it to foresee the adversarys movements with high accuracy.",
                "A4.",
                "Profile Detection Axiom.",
                "The agent can adjust its adversarys profiles by observations and pattern study (specifically, if there are repeated encounters with the same adversary).",
                "However, instead of waiting for profile information to be revealed, an agent can also initiate actions that will force its adversary to react in a way that will reveal profile knowledge about it.",
                "Formally, the axiom states that if all actions (γ) are not highly beneficial actions (< TrH), the agent can do action α in time Tα if it believes that it will result in a non-highly beneficial action β from its adversary, which in turn teaches it about the adversarys profile, i.e., gives a higher AdvKnow(P Aj Ai , Tβ). (∀Aag, Ao ∈ A, α ∈ CAag , β ∈ CAo , Tn, Tα, Tβ, w ∈ W) Bel(Aag, (∀γ ∈ CAag )Eval(Aag, γ, w) < TrH, Tn) ∧ Bel(Aag, Do(Aag, α, Tα, w) ⇒ Do(Ao, β, Tβ, w), Tn) ∧ Bel(Aag, Eval(Ao, β, w) < TrH) ∧ Bel(Aag, AdvKnow(P Aj Ai , Tβ) > AdvKnow(P Aj Ai , Tn), Tn) ⇒ Pot.Int.To(Aag, α, Tn, Tα, w) For example, going back to the Chess board game scenario, consider starting a game versus an opponent about whom we know nothing, not even if it is a human or a computerized opponent.",
                "We might start playing a strategy that will be suitable versus an average opponent, and adjust our game according to its level of play.",
                "A5.",
                "Alliance Formation Axiom The following behavioral axiom is relevant only in a multilateral instantiation of the adversarial environment (obviously, an alliance cannot be formed in a bilateral, zero-sum encounter).",
                "In different situations during a multilateral interaction, a group of agents might believe that it is in their best interests to form a temporary alliance.",
                "Such an alliance is an agreement that constrains its members behavior, but is believed by its members to enable them to achieve a higher utility value than the one achievable outside of the alliance.",
                "As an example, we can look at the classical Risk board game, where each player has an individual goal of being the sole conquerer of the world, a zero-sum game.",
                "However, in order to achieve this goal, it might be strategically wise to make short-term ceasefire agreements with other players, or to join forces and attack an opponent who is stronger than the rest.",
                "An alliances terms defines the way its members should act.",
                "It is a set of predicates, denoted as Terms, that is agreed upon by the alliance members, and should remain true for the duration of the alliance.",
                "For example, the set Terms in the Risk scenario, could contain the following predicates: 1.",
                "Alliance members will not attack each other on territories X, Y and Z; 2.",
                "Alliance members will contribute C units per turn for attacking adversary Ao; 3.",
                "Members are obligated to stay as part of the alliance until time Tk or until adversarys Ao army is smaller than Q.",
                "The set Terms specifies inter-group constraints on each of the alliance members (∀Aal i ∈ Aal ⊆ A) set of actions Cal i ⊆ C. Definition 6.",
                "Al val - the total evaluation value that agent Ai will achieve while being part of Aal is the sum of Evali (Eval for Ai) of each of Aal j Eval values after taking their own α actions (via the agent(α) predicate): Al val(Ai, Cal , Aal , w) = α∈Cal Evali(Aal j , agent(α), w) Definition 7.",
                "Al TrH - is a number representing an Al val The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) 553 threshold; above it, the alliance can be said to be a highly beneficial alliance.",
                "The value of Al TrH will be calculated dynamically according to the progress of the interaction, as can be seen in [7].",
                "After an alliance is formed, its members are now working in their normal adversarial environment, as well as according to the mental states and axioms required for their interactions as part of the alliance.",
                "The following Alliance model (AL) specifies the conditions under which the group Aal can be said to be in an alliance and working with a new and constrained set of actions Cal , at time Tn.",
                "AL(Aal , Cal , w, Tn) 1.",
                "Aal has a MB that all members are part of Aal : MB(Aal , (∀Aal i ∈ Aal )member(Aal i , Aal ), Tn) 2.",
                "Aal has a MB that the group be maintained: MB(Aal , (∀Aal i ∈ Aal )Int.Th (Ai, member(Ai, Aal ), Tn, Tn+1, Co), Tn) 3.",
                "Aal has a MB that being members gives them high utility value: MB(Aal , (∀Aal i ∈ Aal )Al val(Aal i , Cal , Aal , w) ≥ Al TrH, Tn) Members profiles are a crucial part of successful alliances.",
                "We assume that agents that have more accurate profiles of their adversaries will be more successful in such environments.",
                "Such agents will be able to predict when a member is about to breach the alliances contract (item 2 in the above model), and take counter measures (when item 3 will falsify).",
                "The robustness of the alliance is in part a function of its members trustfulness measure, objective position estimation, and other profile properties.",
                "We should note that an agent can simultaneously be part of more than one alliance.",
                "Such a temporary alliance, where the group members do not have a joint goal but act collaboratively for the interest of their own individual goals, is classified as a Treatment Group by modern psychologists [12] (in contrast to a Task Group, where its members have a joint goal).",
                "The Shared Activity model as presented in [5] modeled Treatment Group behavior using the same SharedPlans formalization.",
                "When comparing both definitions of an alliance and a Treatment Group we found an unsurprising resemblance between both models: the environment models definitions are almost identical (see SAs definitions in [5]), and their Selfish-Act and Cooperative Act axioms conform to our adversarial agents behavior.",
                "The main distinction between both models is the integration of a Helpful-behavior act axiom, in the Shared Activity which cannot be part of ours.",
                "This axiom states that an agent will consider taking action that will lower its Eval value (to a certain lower bound), if it believes that a group partner will gain a significant benefit.",
                "Such behavior cannot occur in a pure adversarial environment (as a zero-sum game is), where the alliance members are constantly on watch to manipulate their alliance to their own advantage.",
                "A6.",
                "Evaluation Maximization Axiom.",
                "In a case when all other axioms are inapplicable, we will proceed with the action that maximizes the heuristic value as computed in the Eval function. (∀Aag, Ao ∈ A, α ∈ Cag, Tn, w ∈ W) Bel(Aag, (∀γ ∈ Cag)Eval(Aag, α, w) ≥ Eval(Aag, γ, w), Tn) ⇒ Pot.Int.To(Aag, α, Tn, Tα, w) T1.",
                "Optimality on Eval = Utility The above axiomatic model handles situations where the Utility is unknown and the agents are bounded rational agents.",
                "The following theorem shows that in bilateral interactions, where the agents have the real Utility function (i.e., Eval = Utility) and are rational agents, the axioms provide the same optimal result as classic adversarial search (e.g., Min-Max).",
                "Theorem 1.",
                "Let Ae ag be an unbounded rational AE agent using the Eval heuristic evaluation function, Au ag be the same agent using the true Utility function, and Ao be a sole unbounded utility-based rational adversary.",
                "Given that Eval = Utility: (∀α ∈ CAu ag , α ∈ CAe ag , Tn, w ∈ W) Pot.Int.To(Au ag, α, Tn, Tα, w) → Pot.Int.To(Ae ag, α , Tn, Tα, w) ∧ ((α = α ) ∨ (Utility(Au ag, α, w) = Eval(Ae ag, α , w))) Sketch of proof - Given that Au ag has the real utility function and unbounded resources, it can generate the full game tree and run the optimal MinMax algorithm to choose the highest utility value action, which we denote by, α.",
                "The proof will show that Ae ag, using the AE axioms, will select the same or equal utility α (when there is more than one action with the same max utility) when Eval = Utility. (A1) Goal achieving axiom - suppose there is an α such that its completion will achieve Au ags goal.",
                "It will obtain the highest utility by Min-Max for Au ag.",
                "The Ae ag agent will select α or another action with the same utility value via A1.",
                "If such α does not exist, Ae ag cannot apply this axiom, and proceeds to A2. (A2) Preventive act axiom - (1) Looking at the basic case (see Prop1), if there is a β which leads Ao to achieve its goal, then a preventive action α will yield the highest utility for Au ag.",
                "Au ag will choose it through the utility, while Ae ag will choose it through A2. (2) In the general case, β is a highly beneficial action for Ao, thus yields low utility for Au ag, which will guide it to select an α that will prevent β, while Ae ag will choose it through A2.1 If such β does not exist for Ao, then A2 is not applicable, and Ae ag can proceed to A3. (A3) Suboptimal tactical move axiom - When using a heuristic Eval function, Ae ag has a partial belief in the profile of its adversary (item 4 in AE model), which may lead it to believe in SetActions (Prop1).",
                "In our case, Ae ag is holding a full profile on its optimal adversary and knows that Ao will behave optimally according to the real utility values on the complete search tree, therefore, any belief about suboptimal SetAction cannot exist, yielding this axiom inapplicable.",
                "Ae ag will proceed to A4. (A4) Profile detection axiom - Given that Ae ag has the full profile of Ao, none of Ae ags actions can increase its knowledge.",
                "That axiom will not be applied, and the agent will proceed with A6 (A5 will be disregarded because the interaction is bilateral). (A6) Evaluation maximization axiom - This axiom will select the max Eval for Ae ag.",
                "Given that Eval = Utility, the same α that was selected by Au ag will be selected. 3.",
                "EVALUATION The main purpose of our experimental analysis is to evaluate the models behavior and performance in a real adversarial environment.",
                "This section investigates whether bounded 1 A case where following the completion of β there exists a γ which gives high utility for Agent Au ag, cannot occur because Ao uses the same utility, and γs existence will cause it to classify β as a low utility action. 554 The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) rational agents situated in such adversarial environments will be better off applying our suggested behavioral axioms. 3.1 The Domain To explore the use of the above model and its behavioral axioms, we decided to use the Connect-Four game as our adversarial environment.",
                "Connect-Four is a 2-player, zerosum game which is played using a 6x7 matrix-like board.",
                "Each turn, a player drops a disc into one of the 7 columns (the set of 21 discs is usually colored yellow for player 1 and red for player 2; we will use White and Black respectively to avoid confusion).",
                "The winner is the first player to complete a horizontal, vertical, or diagonal set of four discs with its color.",
                "On very rare occasions, the game might end in a tie if all the empty grids are filled, but no player managed to create a 4-disc set.",
                "The Connect-Four game was solved in [1], where it is shown that the first player (playing with the white discs) can force a win by starting in the middle column (column 4) and playing optimally However, the optimal strategy is very complex, and difficult to follow even for complex bounded rational agents, such as human players.",
                "Before we can proceed checking agent behavior, we must first verify that the domain conforms to the adversarial environments definition as given above (which the behavioral axioms are based on).",
                "First, when playing a Connect-Four game, the agent has an intention to win the game (item 1).",
                "Second (item 2), our agent believes that in Connect-Four there can only be one winner (or no winner at all in the rare occurrence of a tie).",
                "In addition, our agent believes that its opponent to the game will try to win (item 3), and we hope it has some partial knowledge (item 4) about its adversary (this knowledge can vary from nothing, through simple facts such as age, to strategies and weaknesses).",
                "Of course, not all Connect-Four encounters are adversarial.",
                "For example, when a parent is playing the game with its child, the following situation might occur: the child, having a strong incentive to win, treats the environment as adversarial (it intends to win, understands that there can only be one winner, and believes that its parent is trying to beat him).",
                "However, the parents point of view might see the environment as an educational one, where its goal is not to win the game, but to cause enjoyment or practice strategic reasoning.",
                "In such an educational environment, a new set of behavioral axioms might be more beneficial to the parents goals than our suggested adversarial behavioral axioms. 3.2 Axiom Analysis After showing that the Connect-Four game is indeed a zero-sum, bilateral adversarial environment, the next step is to look at players behaviors during the game and check whether behaving according to our model does improve performance.",
                "To do so we have collected log files from completed Connect-Four games that were played by human players over the Internet.",
                "Our collected log file data came from Play by eMail (PBeM) sites.",
                "These are web sites that host email games, where each move is taken by an email exchange between the server and the players.",
                "Many such sites archives contain real competitive interactions, and also maintain a ranking system for their members.",
                "Most of the data we used can be found in [6].",
                "As can be learned from [1], Connect-Four has an optimal strategy and a considerable advantage for the player who starts the game (which we call the White player).",
                "We will concentrate in our analysis on the second players moves (to be called Black).",
                "The White player, being the first to act, has the so-called initiative advantage.",
                "Having the advantage and a good strategy will keep the Black player busy reacting to its moves, instead of initiating threats.",
                "A threat is a combination of three discs of the same color, with an empty spot for the fourth winning disk.",
                "An open threat is a threat that can be realized in the opponents next move.",
                "In order for the Black player to win, it must somehow turn the tide, take the advantage and start presenting threats to the White player.",
                "We will explore Black players behavior and their conformance to our axioms.",
                "To do so, we built an application that reads log files and analyzes the Black players moves.",
                "The application contains two main components: (1) a Min-Max algorithm for evaluation of moves; (2) open threats detector for the discovering of open threats.",
                "The Min-Max algorithm will work to a given depth, d and for each move α will output the heuristic value for the next action taken by the player as written in the log file, h(α), alongside the maximum heuristic value, maxh(α), that could be achieved prior to taking the move (obviously, if h(α) = maxh(α), then the player did not do the optimal move heuristically).",
                "The threat detectors job is to notify if some action was taken in order to block an open threat (not blocking an open threat will probably cause the player to lose in the opponents next move).",
                "The heuristic function used by Min-Max to evaluate the players utility is the following function, which is simple to compute, yet provides a reasonable challenge to human opponents: Definition 8.",
                "Let Group be an adjacent set of four squares that are horizontal, vertical, or diagonal.",
                "Groupn b (Groupn w) be a Group with n pieces of the black (white) color and 4−n empty squares. h = ((Group1 b ∗α)+(Group2 b ∗β)+(Group3 b ∗γ)+(Group4 b ∗∞)) − ((Group1 w ∗α)+(Group2 w ∗β)+(Group3 w ∗γ)+(Group4 w ∗∞)) The values of α, β and δ can vary to form any desired linear combination; however, it is important to value them with the α < β < δ ordering in mind (we used 1, 4, and 8 as their respective values).",
                "Groups of 4 discs of the same color means victory, thus discovery of such a group will result in ∞ to ensure an extreme value.",
                "We now use our estimated evaluation function to evaluate the Black players actions during the Connect-Four adversarial interaction.",
                "Each game from the log file was input into the application, which processed and output a reformatted log file containing the h value of the current move, the maxh value that could be achieved, and a notification if an open threat was detected.",
                "A total of 123 games were analyzed (57 with White winning, and 66 with Black winning).",
                "A few additional games were manually ignored in the experiment, due to these problems: a player abandoning the game while the outcome is not final, or a blunt irrational move in the early stages of the game (e.g., not blocking an obvious winning group in the first opening moves).",
                "In addition, a single tie game was also removed.",
                "The simulator was run to a search depth of 3 moves.",
                "We now proceed to analyze the games with respect to each behavioral axiom.",
                "The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) 555 Table 1: Average heuristic difference analysis Black losses Black Won Avg minh -17.62 -12.02 Avg 3 lowest h moves (min3 h) -13.20 -8.70 3.2.1 Affirming the Suboptimal tactical move axiom The following section presents the heuristic evaluations of the Min-Max algorithm for each action, and checks the amount and extent of suboptimal tactical actions and their implications on performance.",
                "Table 1 shows results and insights from the games heuristic analysis, when search depth equals 3 (this search depth was selected for the results to be comparable to [9], see Section 3.2.3).",
                "The tables heuristic data is the difference between the present maximal heuristic value and the heuristic value of the action that was eventually taken by the player (i.e., the closer the number is to 0, the closer the action was to the maximum heuristic action).",
                "The first row presents the difference values of the action that had the maximal difference value among all the Black players actions in a given game, as averaged over all Blacks winning and losing games (see respective columns).",
                "In games in which the Black player loses, its average difference value was -17.62, while in games in which the Black player won, its average was -12.02.",
                "The second row expands the analysis by considering the 3 highest heuristic difference actions, and averaging them.",
                "In that case, we notice an average heuristic difference of 5 points between games which the Black player loses and games in which it wins.",
                "Nevertheless, the importance of those numbers is that they allowed us to take an educated guess on a threshold number of 11.5, as the value of the TrH constant, which differentiates between normal actions and highly beneficial ones.",
                "After finding an approximated TrH constant, we can proceed with an analysis of the importance of suboptimal moves.",
                "To do so we took the subset of games in which the minimum heuristic difference value for Blacks actions was 11.5.",
                "As presented in Table 2, we can see the different min3 h average of the 3 largest ranges and the respective percentage of games won.",
                "The first row shows that the Black player won only 12% of the games in which the average of its 3 highest heuristically difference actions (min3 h) was smaller than the suggested threshold, TrH = 11.5.",
                "The second row shows a surprising result: it seems that when min3 h > −4 the Black player rarely wins.",
                "Intuition would suggest that games in which the action evaluation values were closer to the maximal values will result in more winning games for Black.",
                "However, it seems that in the Connect-Four domain, merely responding with somewhat easily expected actions, without initiating a few surprising and suboptimal moves, does not yield good results.",
                "The last row sums up the main insights from the analysis; most of Blacks wins (83%) came when its min3 h was in the range of -11.5 to -4.",
                "A close inspection of those Black winning games shows the following pattern behind the numbers: after standard opening moves, Black suddenly drops a disc into an isolated column, which seems a waste of a move.",
                "White continues to build its threats, while usually disregarding Blacks last move, which in turn uses the isolated disc as an anchor for a future winning threat.",
                "The results show that it was beneficial for the Black player Table 2: Blacks winnings percentages % of games min3 h < −11.5 12% min3 h > −4 5% −11.5 ≤ min3 h ≤ −4 83% to take suboptimal actions and not give the current highest possible heuristic value, but will not be too harmful for its position (i.e., will not give high beneficial value to its adversary).",
                "As it turned out, learning the threshold is an important aspect of success: taking wildly risky moves (min3 h < −11.5) or trying to avoid them (min3 h > −4) reduces the Black players winning chances by a large margin. 3.2.2 Affirming the Profile Monitoring Axiom In the task of showing the importance of monitoring ones adversaries profiles, our log files could not be used because they did not contain repeated interactions between players, which are needed to infer the players knowledge about their adversaries.",
                "However, the importance of opponent modeling and its use in attaining tactical advantages was already studied in various domains ([3, 9] are good examples).",
                "In a recent paper, Markovitch and Reger [9] explored the notion of learning and exploitation of opponent weakness in competitive interactions.",
                "They apply simple learning strategies by analyzing examples from past interactions in a specific domain.",
                "They also used the Connect-Four adversarial domain, which can now be used to understand the importance of monitoring the adversarys profile.",
                "Following the presentation of their theoretical model, they describe an extensive empirical study and check the agents performance after learning the weakness model with past examples.",
                "One of the domains used as a competitive environment was the same Connect-Four game (Checkers was the second domain).",
                "Their heuristic function was identical to ours with three different variations (H1, H2, and H3) that are distinguished from one another in their linear combination coefficient values.",
                "The search depth for the players was 3 (as in our analysis).",
                "Their extensive experiments check and compare various learning strategies, risk factors, predefined feature sets and usage methods.",
                "The bottom line is that the Connect-Four domain shows an improvement from a 0.556 winning rate before modeling to a 0.69 after modeling (page 22).",
                "Their conclusions, showing improved performance when holding and using the adversarys model, justify the effort to monitor the adversary profile for continuous and repeated interactions.",
                "An additional point that came up in their experiments is the following: after the opponent weakness model has been learned, the authors describe different methods of integrating the opponent weakness model into the agents decision strategy.",
                "Nevertheless, regardless of the specific method they chose to work with, all integration methods might cause the agent to take suboptimal decisions; it might cause the agent to prefer actions that are suboptimal at the present decision junction, but which might cause the opponent to react in accordance with its weakness model (as represented by our agent) which in turn will be beneficial for us in the future.",
                "The agents behavior, as demonstrated in [9] further confirms and strengthens our Suboptimal Tactical Axiom as discussed in the previous section. 556 The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) 3.2.3 Additional Insights The need for the Goal Achieving, Preventive Act, and Evaluation Maximization axioms are obvious, and need no further verification.",
                "However, even with respect to those axioms, a few interesting insights came up in the log analysis.",
                "The Goal achieving and Preventive Act axioms, though theoretically trivial, seem to provide some challenge to a human player.",
                "In the initial inspection of the logs, we encountered few games2 where a player, for inexplicable reasons, did not block the other from winning or failed to execute its own winning move.",
                "We can blame those faults on the humans lack of attention, or a typing error in its move reply; nevertheless, those errors might occur in bounded rational agents, and the appropriate behavior needs to be axiomatized.",
                "A typical Connect-Four game revolves around generating threats and blocking them.",
                "In our analysis we looked for explicit preventive actions, i.e., moves that block a group of 3 discs, or that remove a future threat (in our limited search horizon).",
                "We found that in 83% of the total games there was at least one preventive action taken by the Black player.",
                "It was also found that Black averaged 2.8 preventive actions per game on the games in which it lost, while averaging 1.5 preventive actions per game when winning.",
                "It seems that Black requires 1 or 2 preventive actions to build its initial taking position, before starting to present threats.",
                "If it did not manage to win, it will usually prevent an extra threat or two before succumbing to White. 4.",
                "RELATED WORK Much research deals with the axiomatization of teamwork and mental states of individuals: some models use knowledge and belief [10], others have models of goals and intentions [8, 4].",
                "However, all these formal theories deal with agent teamwork and cooperation.",
                "As far as we know, our model is the first to provide a formalized model for explicit adversarial environments and agents behavior in it.",
                "The classical Min-Max adversarial search algorithm was the first attempt to integrate the opponent into the search space with a weak assumption of an optimally playing opponent.",
                "Since then, much effort has gone into integrating the opponent model into the decision procedure to predict future behavior.",
                "The M∗ algorithm presented by Carmel and Markovitch [2] showed a method of incorporating opponent models into adversary search, while in [3] they used learning to provide a more accurate opponent model in a 2player repeated game environment, where agents strategies were modeled as finite automata.",
                "Additional Adversarial planning work was done by Willmott et al. [13], which provided an adversarial planning approach to the game of GO.",
                "The research mentioned above dealt with adversarial search and the integration of opponent models into classical utilitybased search methods.",
                "That work shows the importance of opponent modeling and the ability to exploit it to an agents advantage.",
                "However, the basic limitations of those search methods still apply; our model tries to overcome those limitations by presenting a formal model for a new, mental state-based adversarial specification. 5.",
                "CONCLUSIONS We presented an Adversarial Environment model for a 2 These were later removed from the final analysis. bounded rational agent that is situated in an N-player, zerosum environment.",
                "We used the SharedPlans formalization to define the model and the axioms that agents can apply as behavioral guidelines.",
                "The model is meant to be used as a guideline for designing agents that need to operate in such adversarial environments.",
                "We presented empirical results, based on ConnectFour log file analysis, that exemplify the model and the axioms for a bilateral instance of the environment.",
                "The results we presented are a first step towards an expanded model that will cover all types of adversarial environments, for example, environments that are non-zero-sum, and environments that contain natural agents that are not part of the direct conflict.",
                "Those challenges and more will be dealt with in future research. 6.",
                "ACKNOWLEDGMENT This research was supported in part by Israel Science Foundation grants #1211/04 and #898/05. 7.",
                "REFERENCES [1] L. V. Allis.",
                "A knowledge-based approach of Connect-Four - the game is solved: White wins.",
                "Masters thesis, Free University, Amsterdam, The Netherlands, 1988. [2] D. Carmel and S. Markovitch.",
                "Incorporating opponent models into adversary search.",
                "In Proceedings of the Thirteenth National Conference on Artificial Intelligence, pages 120-125, Portland, OR, 1996. [3] D. Carmel and S. Markovitch.",
                "Opponent modeling in multi-agent systems.",
                "In G. Weiß and S. Sen, editors, Adaptation and Learning in Multi-Agent Systems, pages 40-52.",
                "Springer-Verlag, 1996. [4] B. J. Grosz and S. Kraus.",
                "Collaborative plans for complex group action.",
                "Artificial Intelligence, 86(2):269-357, 1996. [5] M. Hadad, G. Kaminka, G. Armon, and S. Kraus.",
                "Supporting collaborative activity.",
                "In Proc. of AAAI-2005, pages 83-88, Pittsburgh, 2005. [6] http://www.gamerz.net/˜pbmserv/. [7] S. Kraus and D. Lehmann.",
                "Designing and building a negotiating automated agent.",
                "Computational Intelligence, 11:132-171, 1995. [8] H. J. Levesque, P. R. Cohen, and J. H. T. Nunes.",
                "On acting together.",
                "In Proc. of AAAI-90, pages 94-99, Boston, MA, 1990. [9] S. Markovitch and R. Reger.",
                "Learning and exploiting relative weaknesses of opponent agents.",
                "Autonomous Agents and Multi-Agent Systems, 10(2):103-130, 2005. [10] Y. M. Ronald Fagin, Joseph Y. Halpern and M. Y. Vardi.",
                "Reasoning about knowledge.",
                "MIT Press, Cambridge, Mass., 1995. [11] P. Thagard.",
                "Adversarial problem solving: Modeling an oponent using explanatory coherence.",
                "Cognitive Science, 16(1):123-149, 1992. [12] R. W. Toseland and R. F. Rivas.",
                "An Introduction to Group Work Practice.",
                "Prentice Hall, Englewood Cliffs, NJ, 2nd edition edition, 1995. [13] S. Willmott, J. Richardson, A. Bundy, and J. Levine.",
                "An adversarial planning approach to Go.",
                "Lecture Notes in Computer Science, 1558:93-112, 1999.",
                "The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) 557"
            ],
            "original_annotated_samples": [],
            "translated_annotated_samples": [
                "En este artículo, desarrollamos un \"modelo axiomatizado\" formal para agentes racionales acotados que están situados en un entorno adversario de suma cero.modelo axiomatizado"
            ],
            "translated_text": "",
            "candidates": [],
            "error": []
        },
        "zero-sum encounter": {
            "translated_key": "",
            "is_in_text": true,
            "original_annotated_sentences": [
                "An Adversarial Environment Model for Bounded Rational Agents in Zero-Sum Interactions Inon Zuckerman1 , Sarit Kraus1 , Jeffrey S. Rosenschein2 , Gal Kaminka1 1 Department of Computer Science 2 The School of Engineering Bar-Ilan University and Computer Science Ramat-Gan, Israel Hebrew University, Jerusalem, Israel {zukermi,sarit,galk}@cs.biu.ac.il jeff@cs.huji.ac.il ABSTRACT Multiagent environments are often not cooperative nor collaborative; in many cases, agents have conflicting interests, leading to adversarial interactions.",
                "This paper presents a formal Adversarial Environment model for bounded rational agents operating in a zero-sum environment.",
                "In such environments, attempts to use classical utility-based search methods can raise a variety of difficulties (e.g., implicitly modeling the opponent as an omniscient utility maximizer, rather than leveraging a more nuanced, explicit opponent model).",
                "We define an Adversarial Environment by describing the mental states of an agent in such an environment.",
                "We then present behavioral axioms that are intended to serve as design principles for building such adversarial agents.",
                "We explore the application of our approach by analyzing log files of completed Connect-Four games, and present an empirical analysis of the axioms appropriateness.",
                "Categories and Subject Descriptors I.2.11 [Artificial Intelligence]: Distributed Artificial Intelligence-Intelligent agents,Multiagent Systems; I.2.4 [Artificial Intelligence]: Knowledge Representation Formalisms and Methods -Modal logic General Terms Design, Theory 1.",
                "INTRODUCTION Early research in multiagent systems (MAS) considered cooperative groups of agents; because individual agents had limited resources, or limited access to information (e.g., limited processing power, limited sensor coverage), they worked together by design to solve problems that individually they could not solve, or at least could not solve as efficiently.",
                "MAS research, however, soon began to consider interacting agents with individuated interests, as representatives of different humans or organizations with non-identical interests.",
                "When interactions are guided by diverse interests, participants may have to overcome disagreements, uncooperative interactions, and even intentional attempts to damage one another.",
                "When these types of interactions occur, environments require appropriate behavior from the agents situated in them.",
                "We call these environments Adversarial Environments, and call the clashing agents Adversaries.",
                "Models of cooperation and teamwork have been extensively explored in MAS through the axiomatization of mental states (e.g., [8, 4, 5]).",
                "However, none of this research dealt with adversarial domains and their implications for agent behavior.",
                "Our paper addresses this issue by providing a formal, axiomatized mental state model for a subset of adversarial domains, namely simple zero-sum adversarial environments.",
                "Simple zero-sum encounters exist of course in various twoplayer games (e.g., Chess, Checkers), but they also exist in n-player games (e.g., Risk, Diplomacy), auctions for a single good, and elsewhere.",
                "In these latter environments especially, using a utility-based adversarial search (such as the Min-Max algorithm) does not always provide an adequate solution; the payoff function might be quite complex or difficult to quantify, and there are natural computational limitations on bounded rational agents.",
                "In addition, traditional search methods (like Min-Max) do not make use of a model of the opponent, which has proven to be a valuable addition to adversarial planning [9, 3, 11].",
                "In this paper, we develop a formal, axiomatized model for bounded rational agents that are situated in a zero-sum adversarial environment.",
                "The model uses different modality operators, and its main foundations are the SharedPlans [4] model for collaborative behavior.",
                "We explore environment properties and the mental states of agents to derive behavioral axioms; these behavioral axioms constitute a formal model that serves as a specification and design guideline for agent design in such settings.",
                "We then investigate the behavior of our model empirically using the Connect-Four board game.",
                "We show that this game conforms to our environment definition, and analyze players behavior using a large set of completed match log 550 978-81-904262-7-5 (RPS) c 2007 IFAAMAS files.",
                "In addition, we use the results presented in [9] to discuss the importance of opponent modeling in our ConnectFour adversarial domain.",
                "The paper proceeds as follows.",
                "Section 2 presents the models formalization.",
                "Section 3 presents the empirical analysis and its results.",
                "We discuss related work in Section 4, and conclude and present future directions in Section 5. 2.",
                "ADVERSARIAL ENVIRONMENTS The adversarial environment model (denoted as AE) is intended to guide the design of agents by providing a specification of the capabilities and mental attitudes of an agent in an adversarial environment.",
                "We focus here on specific types of adversarial environments, specified as follows: 1.",
                "Zero-Sum Interactions: positive and negative utilities of all agents sum to zero; 2.",
                "Simple AEs: all agents in the environment are adversarial agents; 3.",
                "Bilateral AEs: AEs with exactly two agents; 4.",
                "Multilateral AEs: AEs of three or more agents.",
                "We will work on both bilateral and multilateral instantiations of zero-sum and simple environments.",
                "In particular, our adversarial environment model will deal with interactions that consist of N agents (N ≥ 2), where all agents are adversaries, and only one agent can succeed.",
                "Examples of such environments range from board games (e.g., Chess, Connect-Four, and Diplomacy) to certain economic environments (e.g., N-bidder auctions over a single good). 2.1 Model Overview Our approach is to formalize the mental attitudes and behaviors of a single adversarial agent; we consider how a single agent perceives the AE.",
                "The following list specifies the conditions and mental states of an agent in a simple, zero-sum AE: 1.",
                "The agent has an individual intention that its own goal will be completed; 2.",
                "The agent has an individual belief that it and its adversaries are pursuing full conflicting goals (defined below)there can be only one winner; 3.",
                "The agent has an individual belief that each adversary has an intention to complete its own full conflicting goal; 4.",
                "The agent has an individual belief in the (partial) profile of its adversaries.",
                "Item 3 is required, since it might be the case that some agent has a full conflicting goal, and is currently considering adopting the intention to complete it, but is, as of yet, not committed to achieving it.",
                "This might occur because the agent has not yet deliberated about the effects that adopting that intention might have on the other intentions it is currently holding.",
                "In such cases, it might not consider itself to even be in an adversarial environment.",
                "Item 4 states that the agent should hold some belief about the profiles of its adversaries.",
                "The profile represents all the knowledge the agent has about its adversary: its weaknesses, strategic capabilities, goals, intentions, trustworthiness, and more.",
                "It can be given explicitly or can be learned from observations of past encounters. 2.2 Model Definitions for Mental States We use Grosz and Krauss definitions of the modal operators, predicates, and meta-predicates, as defined in their SharedPlan formalization [4].",
                "We recall here some of the predicates and operators that are used in that formalization: Int.To(Ai, α, Tn, Tα, C) represents Ais intentions at time Tn to do an action α at time Tα in the context of C. Int.Th(Ai, prop, Tn, Tprop, C) represents Ais intentions at time Tn that a certain proposition prop holds at time Tprop in the context of C. The potential intention operators, Pot.Int.To(...) and Pot.Int.Th(...), are used to represent the mental state when an agent considers adopting an intention, but has not deliberated about the interaction of the other intentions it holds.",
                "The operator Bel(Ai, f, Tf ) represents agent Ai believing in the statement expressed in formula f, at time Tf .",
                "MB(A, f, Tf ) represents mutual belief for a group of agents A.",
                "A snapshot of the system finds our environment to be in some state e ∈ E of environmental variable states, and each adversary in any LAi ∈ L of possible local states.",
                "At any given time step, the system will be in some world w of the set of all possible worlds w ∈ W, where w = E×LA1 ×LA2 × ...LAn , and n is the number of adversaries.",
                "For example, in a Texas Holdem poker game, an agents local state might be its own set of cards (which is unknown to its adversary) while the environment will consist of the betting pot and the community cards (which are visible to both players).",
                "A utility function under this formalization is defined as a mapping from a possible world w ∈ W to an element in , which expresses the desirability of the world, from a single agent perspective.",
                "We usually normalize the range to [0,1], where 0 represents the least desirable possible world, and 1 is the most desirable world.",
                "The implementation of the utility function is dependent on the domain in question.",
                "The following list specifies new predicates, functions, variables, and constants used in conjunction with the original definitions for the adversarial environment formalization: 1. φ is a null action (the agent does not do anything). 2.",
                "GAi is the set of agent Ais goals.",
                "Each goal is a set of predicates whose satisfaction makes the goal complete (we use G∗ Ai ∈ GAi to represent an arbitrary goal of agent Ai). 3. gAi is the set of agent Ais subgoals.",
                "Subgoals are predicates whose satisfaction represents an important milestone toward achievement of the full goal. gG∗ Ai ⊆ gAi is the set of subgoals that are important to the completion of goal G∗ Ai (we will use g∗ G∗ Ai ∈ gG∗ Ai to represent an arbitrary subgoal). 4.",
                "P Aj Ai is the profile object agent Ai holds about agent Aj. 5.",
                "CA is a general set of actions for all agents in A which are derived from the environments constraints.",
                "CAi ⊆ CA is the set of agent Ais possible actions. 6.",
                "Do(Ai, α, Tα, w) holds when Ai performs action α over time interval Tα in world w. 7.",
                "Achieve(G∗ Ai , α, w) is true when goal G∗ Ai is achieved following the completion of action α in world w ∈ W, where α ∈ CAi . 8.",
                "Profile(Ai, PAi Ai ) is true when agent Ai holds an object profile for agent Aj.",
                "Definition 1.",
                "Full conflict (FulConf ) describes a zerosum interaction where only a single goal of the goals in conflict can be completed.",
                "FulConf(G∗ Ai , G∗ Aj ) ⇒ (∃α ∈ CAi , ∀w, β ∈ CAj ) (Achieve(G∗ Ai , α, w) ⇒ ¬Achieve(G∗ Aj , β, w)) ∨ (∃β ∈ CAj , ∀w, α ∈ CAi )(Achieve(G∗ Aj , β, w) ⇒ ¬Achieve(G∗ Ai , α, w)) Definition 2.",
                "Adversarial Knowledge (AdvKnow) is a function returning a value which represents the amount of The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) 551 knowledge agent Ai has on the profile of agent Aj, at time Tn.",
                "The higher the value, the more knowledge agent Ai has.",
                "AdvKnow : P Aj Ai × Tn → Definition 3.",
                "Eval - This evaluation function returns an estimated expected utility value for an agent in A, after completing an action from CA in some world state w. Eval : A × CA × w → Definition 4.",
                "TrH - (Threshold) is a numerical constant in the [0,1] range that represents an evaluation function (Eval) threshold value.",
                "An action that yields an estimated utility evaluation above the TrH is regarded as a highly beneficial action.",
                "The Eval value is an estimation and not the real utility function, which is usually unknown.",
                "Using the real utility value for a rational agent would easily yield the best outcome for that agent.",
                "However, agents usually do not have the real utility functions, but rather a heuristic estimate of it.",
                "There are two important properties that should hold for the evaluation function: Property 1.",
                "The evaluation function should state that the most desirable world state is one in which the goal is achieved.",
                "Therefore, after the goal has been satisfied, there can be no future action that can put the agent in a world state with higher Eval value. (∀Ai, G∗ Ai , α, β ∈ CAi , w ∈ W) Achieve(G∗ Ai , α, w) ⇒ Eval(Ai, α, w) ≥ Eval(Ai, β, w) Property 2.",
                "The evaluation function should project an action that causes a completion of a goal or a subgoal to a value which is greater than TrH (a highly beneficial action). (∀Ai, G∗ Ai ∈ GAi , α ∈ CAi , w ∈ W, g∗ GAi ∈ gGAi ) Achieve(G∗ Ai , α, w) ∨ Achieve(g∗ GAi , α, w) ⇒ Eval(Ai, α, w) ≥ TrH.",
                "Definition 5.",
                "SetAction We define a set action (SetAction) as a set of action operations (either complex or basic actions) from some action sets CAi and CAj which, according to agent Ais belief, are attached together by a temporal and consequential relationship, forming a chain of events (action, and its following consequent action). (∀α1 , . . . , αu ∈ CAi , β1 , . . . , βv ∈ CAj , w ∈ W) SetAction(α1 , . . . , αu , β1 , . . . , βv , w) ⇒ ((Do(Ai, α1 , Tα1 , w) ⇒ Do(Aj, β1 , Tβ1 , w)) ⇒ Do(Ai, α2 , Tα2 , w) ⇒ . . . ⇒ Do(Ai, αu , Tαu , w)) The consequential relation might exist due to various environmental constraints (when one action forces the adversary to respond with a specific action) or due to the agents knowledge about the profile of its adversary.",
                "Property 3.",
                "As the knowledge we have about our adversary increases we will have additional beliefs about its behavior in different situations which in turn creates new set actions.",
                "Formally, if our AdvKnow at time Tn+1 is greater than AdvKnow at time Tn, then every SetAction known at time Tn is also known at time Tn+1.",
                "AdvKnow(P Aj Ai , Tn+1) > AdvKnow(P Aj Ai , Tn) ⇒ (∀α1 , . . . , αu ∈ CAi , β1 , . . . , βv ∈ CAj ) Bel(Aag, SetAction(α1 , . . . , αu , β1 , . . . , βv ), Tn) ⇒ Bel(Aag, SetAction(α1 , . . . , αu , β1 , . . . , βv ), Tn+1) 2.3 The Environment Formulation The following axioms provide the formal definition for a simple, zero-sum Adversarial Environment (AE).",
                "Satisfaction of these axioms means that the agent is situated in such an environment.",
                "It provides specifications for agent Aag to interact with its set of adversaries A with respect to goals G∗ Aag and G∗ A at time TCo at some world state w. AE(Aag, A, G∗ Aag , A1, . . . , Ak, G∗ A1 , . . . , G∗ Ak , Tn, w) 1.",
                "Aag has an Int.Th his goal would be completed: (∃α ∈ CAag , Tα) Int.Th(Aag, Achieve(G∗ Aag , α), Tn, Tα, AE) 2.",
                "Aag believes that it and each of its adversaries Ao are pursuing full conflicting goals: (∀Ao ∈ {A1, . . . , Ak}) Bel(Aag, FulConf(G∗ Aag , G∗ Ao ), Tn) 3.",
                "Aag believes that each of his adversaries in Ao has the Int.Th his conflicting goal G∗ Aoi will be completed: (∀Ao ∈ {A1, . . . , Ak})(∃β ∈ CAo , Tβ) Bel(Aag, Int.Th(Ao, Achieve(G∗ Ao , β), TCo, Tβ, AE), Tn) 4.",
                "Aag has beliefs about the (partial) profiles of its adversaries (∀Ao ∈ {A1, . . . , Ak}) (∃PAo Aag ∈ PAag )Bel(Aag, Profile(Ao, PAo Aag ), Tn) To build an agent that will be able to operate successfully within such an AE, we must specify behavioral guidelines for its interactions.",
                "Using a naive Eval maximization strategy to a certain search depth will not always yield satisfactory results for several reasons: (1) the search horizon problem when searching for a fixed depth; (2) the strong assumption of an optimally rational, unbounded resources adversary; (3) using an estimated evaluation function which will not give optimal results in all world states, and can be exploited [9].",
                "The following axioms specify the behavioral principles that can be used to differentiate between successful and less successful agents in the above Adversarial Environment.",
                "Those axioms should be used as specification principles when designing and implementing agents that should be able to perform well in such Adversarial Environments.",
                "The behavioral axioms represent situations in which the agent will adopt potential intentions to (Pot.Int.To(...)) perform an action, which will typically require some means-end reasoning to select a possible course of action.",
                "This reasoning will lead to the adoption of an Int.To(...) (see [4]).",
                "A1.",
                "Goal Achieving Axiom.",
                "The first axiom is the simplest case; when the agent Aag believes that it is one action (α) away from achieving his conflicting goal G∗ Aag , it should adopt the potential intention to do α and complete its goal. (∀Aag, α ∈ CAag , Tn, Tα, w ∈ W) (Bel(Aag, Do(Aag, α, Tα, w) ⇒ Achieve(G∗ Aag , α, w)) ⇒ Pot.Int.To(Aag, α, Tn, Tα, w) This somewhat trivial behavior is the first and strongest axiom.",
                "In any situation, when the agent is an action away from completing the goal, it should complete the action.",
                "Any fair Eval function would naturally classify α as the maximal value action (property 1).",
                "However, without explicit axiomatization of such behavior there might be situations where the agent will decide on taking another action for various reasons, due to its bounded decision resources.",
                "A2.",
                "Preventive Act Axiom.",
                "Being in an adversarial situation, agent Aag might decide to take actions that will damage one of its adversarys plans to complete its goal, even if those actions do not explicitly advance Aag towards its conflicting goal G∗ Aag .",
                "Such preventive action will take place when agent Aag has a belief about the possibility of its adversary Ao doing an action β that will give it a high 552 The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) utility evaluation value (> TrH).",
                "Believing that taking action α will prevent the opponent from doing its β, it will adopt a potential intention to do α. (∀Aag, Ao ∈ A, α ∈ CAag , β ∈ CAo , Tn, Tβ, w ∈ W) (Bel(Aag, Do(Ao, β, Tβ, w) ∧ Eval(Ao, β, w) > TrH, Tn) ∧ Bel(Aag, Do(Aag, α, Tα, w) ⇒ ¬Do(Ao, β, Tβ, w), Tn) ⇒ Pot.Int.To(Aag, α, Tn, Tα, w) This axiom is a basic component of any adversarial environment.",
                "For example, looking at a Chess board game, a player could realize that it is about to be checkmated by its opponent, thus making a preventive move.",
                "Another example is a Connect Four game: when a player has a row of three chips, its opponent must block it, or lose.",
                "A specific instance of A1 occurs when the adversary is one action away from achieving its goal, and immediate preventive action needs to be taken by the agent.",
                "Formally, we have the same beliefs as stated above, with a changed belief that doing action β will cause agent Ao to achieve its goal.",
                "Proposition 1: Prevent or lose case. (∀Aag, Ao ∈ A, α ∈ CAag , β ∈ CAo , G∗ Ao , Tn, Tα, Tβ, w ∈ W) Bel(Aag, Do(Ao, β, Tβ, w) ⇒ Achieve(G∗ Ao , β, w), Tn) ∧ Bel(Aag, Do(Aag, α, Tα, w) ⇒ ¬Do(Ao, β, Tβ, w)) ⇒ Pot.Int.To(Aag, α, Tn, Tα, w) Sketch of proof: Proposition 1 can be easily derived from axiom A1 and the property 2 of the Eval function, which states that any action that causes a completion of a goal is a highly beneficial action.",
                "The preventive act behavior will occur implicitly when the Eval function is equal to the real world utility function.",
                "However, being bounded rational agents and dealing with an estimated evaluation function we need to explicitly axiomatize such behavior, for it will not always occur implicitly from the evaluation function.",
                "A3.",
                "Suboptimal Tactical Move Axiom.",
                "In many scenarios a situation may occur where an agent will decide not to take the current most beneficial action it can take (the action with the maximal utility evaluation value), because it believes that taking another action (with lower utility evaluation value) might yield (depending on the adversarys response) a future possibility for a highly beneficial action.",
                "This will occur most often when the Eval function is inaccurate and differs by a large extent from the Utility function.",
                "Put formally, agent Aag believes in a certain SetAction that will evolve according to its initial action and will yield a high beneficial value (> TrH) solely for it. (∀Aag, Ao ∈ A, Tn, w ∈ W) (∃α1 , . . . , αu ∈ CAi , β1 , . . . , βv ∈ CAj , Tα1 ) Bel(Aag, SetAction(α1 , . . . , αu , β1 , . . . , βv ), Tn) ∧ Bel(Aag, Eval(Ao, βv , w) < TrH < Eval(Aag, αu , w), Tn) ⇒ Pot.Int.To(Aag, α1 , Tn, Tα1 , w) An agent might believe that a chain of events will occur for various reasons due to the inevitable nature of the domain.",
                "For example, in Chess, we often observe the following: a move causes a check position, which in turn limits the opponents moves to avoiding the check, to which the first player might react with another check, and so on.",
                "The agent might also believe in a chain of events based on its knowledge of its adversarys profile, which allows it to foresee the adversarys movements with high accuracy.",
                "A4.",
                "Profile Detection Axiom.",
                "The agent can adjust its adversarys profiles by observations and pattern study (specifically, if there are repeated encounters with the same adversary).",
                "However, instead of waiting for profile information to be revealed, an agent can also initiate actions that will force its adversary to react in a way that will reveal profile knowledge about it.",
                "Formally, the axiom states that if all actions (γ) are not highly beneficial actions (< TrH), the agent can do action α in time Tα if it believes that it will result in a non-highly beneficial action β from its adversary, which in turn teaches it about the adversarys profile, i.e., gives a higher AdvKnow(P Aj Ai , Tβ). (∀Aag, Ao ∈ A, α ∈ CAag , β ∈ CAo , Tn, Tα, Tβ, w ∈ W) Bel(Aag, (∀γ ∈ CAag )Eval(Aag, γ, w) < TrH, Tn) ∧ Bel(Aag, Do(Aag, α, Tα, w) ⇒ Do(Ao, β, Tβ, w), Tn) ∧ Bel(Aag, Eval(Ao, β, w) < TrH) ∧ Bel(Aag, AdvKnow(P Aj Ai , Tβ) > AdvKnow(P Aj Ai , Tn), Tn) ⇒ Pot.Int.To(Aag, α, Tn, Tα, w) For example, going back to the Chess board game scenario, consider starting a game versus an opponent about whom we know nothing, not even if it is a human or a computerized opponent.",
                "We might start playing a strategy that will be suitable versus an average opponent, and adjust our game according to its level of play.",
                "A5.",
                "Alliance Formation Axiom The following behavioral axiom is relevant only in a multilateral instantiation of the adversarial environment (obviously, an alliance cannot be formed in a bilateral, <br>zero-sum encounter</br>).",
                "In different situations during a multilateral interaction, a group of agents might believe that it is in their best interests to form a temporary alliance.",
                "Such an alliance is an agreement that constrains its members behavior, but is believed by its members to enable them to achieve a higher utility value than the one achievable outside of the alliance.",
                "As an example, we can look at the classical Risk board game, where each player has an individual goal of being the sole conquerer of the world, a zero-sum game.",
                "However, in order to achieve this goal, it might be strategically wise to make short-term ceasefire agreements with other players, or to join forces and attack an opponent who is stronger than the rest.",
                "An alliances terms defines the way its members should act.",
                "It is a set of predicates, denoted as Terms, that is agreed upon by the alliance members, and should remain true for the duration of the alliance.",
                "For example, the set Terms in the Risk scenario, could contain the following predicates: 1.",
                "Alliance members will not attack each other on territories X, Y and Z; 2.",
                "Alliance members will contribute C units per turn for attacking adversary Ao; 3.",
                "Members are obligated to stay as part of the alliance until time Tk or until adversarys Ao army is smaller than Q.",
                "The set Terms specifies inter-group constraints on each of the alliance members (∀Aal i ∈ Aal ⊆ A) set of actions Cal i ⊆ C. Definition 6.",
                "Al val - the total evaluation value that agent Ai will achieve while being part of Aal is the sum of Evali (Eval for Ai) of each of Aal j Eval values after taking their own α actions (via the agent(α) predicate): Al val(Ai, Cal , Aal , w) = α∈Cal Evali(Aal j , agent(α), w) Definition 7.",
                "Al TrH - is a number representing an Al val The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) 553 threshold; above it, the alliance can be said to be a highly beneficial alliance.",
                "The value of Al TrH will be calculated dynamically according to the progress of the interaction, as can be seen in [7].",
                "After an alliance is formed, its members are now working in their normal adversarial environment, as well as according to the mental states and axioms required for their interactions as part of the alliance.",
                "The following Alliance model (AL) specifies the conditions under which the group Aal can be said to be in an alliance and working with a new and constrained set of actions Cal , at time Tn.",
                "AL(Aal , Cal , w, Tn) 1.",
                "Aal has a MB that all members are part of Aal : MB(Aal , (∀Aal i ∈ Aal )member(Aal i , Aal ), Tn) 2.",
                "Aal has a MB that the group be maintained: MB(Aal , (∀Aal i ∈ Aal )Int.Th (Ai, member(Ai, Aal ), Tn, Tn+1, Co), Tn) 3.",
                "Aal has a MB that being members gives them high utility value: MB(Aal , (∀Aal i ∈ Aal )Al val(Aal i , Cal , Aal , w) ≥ Al TrH, Tn) Members profiles are a crucial part of successful alliances.",
                "We assume that agents that have more accurate profiles of their adversaries will be more successful in such environments.",
                "Such agents will be able to predict when a member is about to breach the alliances contract (item 2 in the above model), and take counter measures (when item 3 will falsify).",
                "The robustness of the alliance is in part a function of its members trustfulness measure, objective position estimation, and other profile properties.",
                "We should note that an agent can simultaneously be part of more than one alliance.",
                "Such a temporary alliance, where the group members do not have a joint goal but act collaboratively for the interest of their own individual goals, is classified as a Treatment Group by modern psychologists [12] (in contrast to a Task Group, where its members have a joint goal).",
                "The Shared Activity model as presented in [5] modeled Treatment Group behavior using the same SharedPlans formalization.",
                "When comparing both definitions of an alliance and a Treatment Group we found an unsurprising resemblance between both models: the environment models definitions are almost identical (see SAs definitions in [5]), and their Selfish-Act and Cooperative Act axioms conform to our adversarial agents behavior.",
                "The main distinction between both models is the integration of a Helpful-behavior act axiom, in the Shared Activity which cannot be part of ours.",
                "This axiom states that an agent will consider taking action that will lower its Eval value (to a certain lower bound), if it believes that a group partner will gain a significant benefit.",
                "Such behavior cannot occur in a pure adversarial environment (as a zero-sum game is), where the alliance members are constantly on watch to manipulate their alliance to their own advantage.",
                "A6.",
                "Evaluation Maximization Axiom.",
                "In a case when all other axioms are inapplicable, we will proceed with the action that maximizes the heuristic value as computed in the Eval function. (∀Aag, Ao ∈ A, α ∈ Cag, Tn, w ∈ W) Bel(Aag, (∀γ ∈ Cag)Eval(Aag, α, w) ≥ Eval(Aag, γ, w), Tn) ⇒ Pot.Int.To(Aag, α, Tn, Tα, w) T1.",
                "Optimality on Eval = Utility The above axiomatic model handles situations where the Utility is unknown and the agents are bounded rational agents.",
                "The following theorem shows that in bilateral interactions, where the agents have the real Utility function (i.e., Eval = Utility) and are rational agents, the axioms provide the same optimal result as classic adversarial search (e.g., Min-Max).",
                "Theorem 1.",
                "Let Ae ag be an unbounded rational AE agent using the Eval heuristic evaluation function, Au ag be the same agent using the true Utility function, and Ao be a sole unbounded utility-based rational adversary.",
                "Given that Eval = Utility: (∀α ∈ CAu ag , α ∈ CAe ag , Tn, w ∈ W) Pot.Int.To(Au ag, α, Tn, Tα, w) → Pot.Int.To(Ae ag, α , Tn, Tα, w) ∧ ((α = α ) ∨ (Utility(Au ag, α, w) = Eval(Ae ag, α , w))) Sketch of proof - Given that Au ag has the real utility function and unbounded resources, it can generate the full game tree and run the optimal MinMax algorithm to choose the highest utility value action, which we denote by, α.",
                "The proof will show that Ae ag, using the AE axioms, will select the same or equal utility α (when there is more than one action with the same max utility) when Eval = Utility. (A1) Goal achieving axiom - suppose there is an α such that its completion will achieve Au ags goal.",
                "It will obtain the highest utility by Min-Max for Au ag.",
                "The Ae ag agent will select α or another action with the same utility value via A1.",
                "If such α does not exist, Ae ag cannot apply this axiom, and proceeds to A2. (A2) Preventive act axiom - (1) Looking at the basic case (see Prop1), if there is a β which leads Ao to achieve its goal, then a preventive action α will yield the highest utility for Au ag.",
                "Au ag will choose it through the utility, while Ae ag will choose it through A2. (2) In the general case, β is a highly beneficial action for Ao, thus yields low utility for Au ag, which will guide it to select an α that will prevent β, while Ae ag will choose it through A2.1 If such β does not exist for Ao, then A2 is not applicable, and Ae ag can proceed to A3. (A3) Suboptimal tactical move axiom - When using a heuristic Eval function, Ae ag has a partial belief in the profile of its adversary (item 4 in AE model), which may lead it to believe in SetActions (Prop1).",
                "In our case, Ae ag is holding a full profile on its optimal adversary and knows that Ao will behave optimally according to the real utility values on the complete search tree, therefore, any belief about suboptimal SetAction cannot exist, yielding this axiom inapplicable.",
                "Ae ag will proceed to A4. (A4) Profile detection axiom - Given that Ae ag has the full profile of Ao, none of Ae ags actions can increase its knowledge.",
                "That axiom will not be applied, and the agent will proceed with A6 (A5 will be disregarded because the interaction is bilateral). (A6) Evaluation maximization axiom - This axiom will select the max Eval for Ae ag.",
                "Given that Eval = Utility, the same α that was selected by Au ag will be selected. 3.",
                "EVALUATION The main purpose of our experimental analysis is to evaluate the models behavior and performance in a real adversarial environment.",
                "This section investigates whether bounded 1 A case where following the completion of β there exists a γ which gives high utility for Agent Au ag, cannot occur because Ao uses the same utility, and γs existence will cause it to classify β as a low utility action. 554 The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) rational agents situated in such adversarial environments will be better off applying our suggested behavioral axioms. 3.1 The Domain To explore the use of the above model and its behavioral axioms, we decided to use the Connect-Four game as our adversarial environment.",
                "Connect-Four is a 2-player, zerosum game which is played using a 6x7 matrix-like board.",
                "Each turn, a player drops a disc into one of the 7 columns (the set of 21 discs is usually colored yellow for player 1 and red for player 2; we will use White and Black respectively to avoid confusion).",
                "The winner is the first player to complete a horizontal, vertical, or diagonal set of four discs with its color.",
                "On very rare occasions, the game might end in a tie if all the empty grids are filled, but no player managed to create a 4-disc set.",
                "The Connect-Four game was solved in [1], where it is shown that the first player (playing with the white discs) can force a win by starting in the middle column (column 4) and playing optimally However, the optimal strategy is very complex, and difficult to follow even for complex bounded rational agents, such as human players.",
                "Before we can proceed checking agent behavior, we must first verify that the domain conforms to the adversarial environments definition as given above (which the behavioral axioms are based on).",
                "First, when playing a Connect-Four game, the agent has an intention to win the game (item 1).",
                "Second (item 2), our agent believes that in Connect-Four there can only be one winner (or no winner at all in the rare occurrence of a tie).",
                "In addition, our agent believes that its opponent to the game will try to win (item 3), and we hope it has some partial knowledge (item 4) about its adversary (this knowledge can vary from nothing, through simple facts such as age, to strategies and weaknesses).",
                "Of course, not all Connect-Four encounters are adversarial.",
                "For example, when a parent is playing the game with its child, the following situation might occur: the child, having a strong incentive to win, treats the environment as adversarial (it intends to win, understands that there can only be one winner, and believes that its parent is trying to beat him).",
                "However, the parents point of view might see the environment as an educational one, where its goal is not to win the game, but to cause enjoyment or practice strategic reasoning.",
                "In such an educational environment, a new set of behavioral axioms might be more beneficial to the parents goals than our suggested adversarial behavioral axioms. 3.2 Axiom Analysis After showing that the Connect-Four game is indeed a zero-sum, bilateral adversarial environment, the next step is to look at players behaviors during the game and check whether behaving according to our model does improve performance.",
                "To do so we have collected log files from completed Connect-Four games that were played by human players over the Internet.",
                "Our collected log file data came from Play by eMail (PBeM) sites.",
                "These are web sites that host email games, where each move is taken by an email exchange between the server and the players.",
                "Many such sites archives contain real competitive interactions, and also maintain a ranking system for their members.",
                "Most of the data we used can be found in [6].",
                "As can be learned from [1], Connect-Four has an optimal strategy and a considerable advantage for the player who starts the game (which we call the White player).",
                "We will concentrate in our analysis on the second players moves (to be called Black).",
                "The White player, being the first to act, has the so-called initiative advantage.",
                "Having the advantage and a good strategy will keep the Black player busy reacting to its moves, instead of initiating threats.",
                "A threat is a combination of three discs of the same color, with an empty spot for the fourth winning disk.",
                "An open threat is a threat that can be realized in the opponents next move.",
                "In order for the Black player to win, it must somehow turn the tide, take the advantage and start presenting threats to the White player.",
                "We will explore Black players behavior and their conformance to our axioms.",
                "To do so, we built an application that reads log files and analyzes the Black players moves.",
                "The application contains two main components: (1) a Min-Max algorithm for evaluation of moves; (2) open threats detector for the discovering of open threats.",
                "The Min-Max algorithm will work to a given depth, d and for each move α will output the heuristic value for the next action taken by the player as written in the log file, h(α), alongside the maximum heuristic value, maxh(α), that could be achieved prior to taking the move (obviously, if h(α) = maxh(α), then the player did not do the optimal move heuristically).",
                "The threat detectors job is to notify if some action was taken in order to block an open threat (not blocking an open threat will probably cause the player to lose in the opponents next move).",
                "The heuristic function used by Min-Max to evaluate the players utility is the following function, which is simple to compute, yet provides a reasonable challenge to human opponents: Definition 8.",
                "Let Group be an adjacent set of four squares that are horizontal, vertical, or diagonal.",
                "Groupn b (Groupn w) be a Group with n pieces of the black (white) color and 4−n empty squares. h = ((Group1 b ∗α)+(Group2 b ∗β)+(Group3 b ∗γ)+(Group4 b ∗∞)) − ((Group1 w ∗α)+(Group2 w ∗β)+(Group3 w ∗γ)+(Group4 w ∗∞)) The values of α, β and δ can vary to form any desired linear combination; however, it is important to value them with the α < β < δ ordering in mind (we used 1, 4, and 8 as their respective values).",
                "Groups of 4 discs of the same color means victory, thus discovery of such a group will result in ∞ to ensure an extreme value.",
                "We now use our estimated evaluation function to evaluate the Black players actions during the Connect-Four adversarial interaction.",
                "Each game from the log file was input into the application, which processed and output a reformatted log file containing the h value of the current move, the maxh value that could be achieved, and a notification if an open threat was detected.",
                "A total of 123 games were analyzed (57 with White winning, and 66 with Black winning).",
                "A few additional games were manually ignored in the experiment, due to these problems: a player abandoning the game while the outcome is not final, or a blunt irrational move in the early stages of the game (e.g., not blocking an obvious winning group in the first opening moves).",
                "In addition, a single tie game was also removed.",
                "The simulator was run to a search depth of 3 moves.",
                "We now proceed to analyze the games with respect to each behavioral axiom.",
                "The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) 555 Table 1: Average heuristic difference analysis Black losses Black Won Avg minh -17.62 -12.02 Avg 3 lowest h moves (min3 h) -13.20 -8.70 3.2.1 Affirming the Suboptimal tactical move axiom The following section presents the heuristic evaluations of the Min-Max algorithm for each action, and checks the amount and extent of suboptimal tactical actions and their implications on performance.",
                "Table 1 shows results and insights from the games heuristic analysis, when search depth equals 3 (this search depth was selected for the results to be comparable to [9], see Section 3.2.3).",
                "The tables heuristic data is the difference between the present maximal heuristic value and the heuristic value of the action that was eventually taken by the player (i.e., the closer the number is to 0, the closer the action was to the maximum heuristic action).",
                "The first row presents the difference values of the action that had the maximal difference value among all the Black players actions in a given game, as averaged over all Blacks winning and losing games (see respective columns).",
                "In games in which the Black player loses, its average difference value was -17.62, while in games in which the Black player won, its average was -12.02.",
                "The second row expands the analysis by considering the 3 highest heuristic difference actions, and averaging them.",
                "In that case, we notice an average heuristic difference of 5 points between games which the Black player loses and games in which it wins.",
                "Nevertheless, the importance of those numbers is that they allowed us to take an educated guess on a threshold number of 11.5, as the value of the TrH constant, which differentiates between normal actions and highly beneficial ones.",
                "After finding an approximated TrH constant, we can proceed with an analysis of the importance of suboptimal moves.",
                "To do so we took the subset of games in which the minimum heuristic difference value for Blacks actions was 11.5.",
                "As presented in Table 2, we can see the different min3 h average of the 3 largest ranges and the respective percentage of games won.",
                "The first row shows that the Black player won only 12% of the games in which the average of its 3 highest heuristically difference actions (min3 h) was smaller than the suggested threshold, TrH = 11.5.",
                "The second row shows a surprising result: it seems that when min3 h > −4 the Black player rarely wins.",
                "Intuition would suggest that games in which the action evaluation values were closer to the maximal values will result in more winning games for Black.",
                "However, it seems that in the Connect-Four domain, merely responding with somewhat easily expected actions, without initiating a few surprising and suboptimal moves, does not yield good results.",
                "The last row sums up the main insights from the analysis; most of Blacks wins (83%) came when its min3 h was in the range of -11.5 to -4.",
                "A close inspection of those Black winning games shows the following pattern behind the numbers: after standard opening moves, Black suddenly drops a disc into an isolated column, which seems a waste of a move.",
                "White continues to build its threats, while usually disregarding Blacks last move, which in turn uses the isolated disc as an anchor for a future winning threat.",
                "The results show that it was beneficial for the Black player Table 2: Blacks winnings percentages % of games min3 h < −11.5 12% min3 h > −4 5% −11.5 ≤ min3 h ≤ −4 83% to take suboptimal actions and not give the current highest possible heuristic value, but will not be too harmful for its position (i.e., will not give high beneficial value to its adversary).",
                "As it turned out, learning the threshold is an important aspect of success: taking wildly risky moves (min3 h < −11.5) or trying to avoid them (min3 h > −4) reduces the Black players winning chances by a large margin. 3.2.2 Affirming the Profile Monitoring Axiom In the task of showing the importance of monitoring ones adversaries profiles, our log files could not be used because they did not contain repeated interactions between players, which are needed to infer the players knowledge about their adversaries.",
                "However, the importance of opponent modeling and its use in attaining tactical advantages was already studied in various domains ([3, 9] are good examples).",
                "In a recent paper, Markovitch and Reger [9] explored the notion of learning and exploitation of opponent weakness in competitive interactions.",
                "They apply simple learning strategies by analyzing examples from past interactions in a specific domain.",
                "They also used the Connect-Four adversarial domain, which can now be used to understand the importance of monitoring the adversarys profile.",
                "Following the presentation of their theoretical model, they describe an extensive empirical study and check the agents performance after learning the weakness model with past examples.",
                "One of the domains used as a competitive environment was the same Connect-Four game (Checkers was the second domain).",
                "Their heuristic function was identical to ours with three different variations (H1, H2, and H3) that are distinguished from one another in their linear combination coefficient values.",
                "The search depth for the players was 3 (as in our analysis).",
                "Their extensive experiments check and compare various learning strategies, risk factors, predefined feature sets and usage methods.",
                "The bottom line is that the Connect-Four domain shows an improvement from a 0.556 winning rate before modeling to a 0.69 after modeling (page 22).",
                "Their conclusions, showing improved performance when holding and using the adversarys model, justify the effort to monitor the adversary profile for continuous and repeated interactions.",
                "An additional point that came up in their experiments is the following: after the opponent weakness model has been learned, the authors describe different methods of integrating the opponent weakness model into the agents decision strategy.",
                "Nevertheless, regardless of the specific method they chose to work with, all integration methods might cause the agent to take suboptimal decisions; it might cause the agent to prefer actions that are suboptimal at the present decision junction, but which might cause the opponent to react in accordance with its weakness model (as represented by our agent) which in turn will be beneficial for us in the future.",
                "The agents behavior, as demonstrated in [9] further confirms and strengthens our Suboptimal Tactical Axiom as discussed in the previous section. 556 The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) 3.2.3 Additional Insights The need for the Goal Achieving, Preventive Act, and Evaluation Maximization axioms are obvious, and need no further verification.",
                "However, even with respect to those axioms, a few interesting insights came up in the log analysis.",
                "The Goal achieving and Preventive Act axioms, though theoretically trivial, seem to provide some challenge to a human player.",
                "In the initial inspection of the logs, we encountered few games2 where a player, for inexplicable reasons, did not block the other from winning or failed to execute its own winning move.",
                "We can blame those faults on the humans lack of attention, or a typing error in its move reply; nevertheless, those errors might occur in bounded rational agents, and the appropriate behavior needs to be axiomatized.",
                "A typical Connect-Four game revolves around generating threats and blocking them.",
                "In our analysis we looked for explicit preventive actions, i.e., moves that block a group of 3 discs, or that remove a future threat (in our limited search horizon).",
                "We found that in 83% of the total games there was at least one preventive action taken by the Black player.",
                "It was also found that Black averaged 2.8 preventive actions per game on the games in which it lost, while averaging 1.5 preventive actions per game when winning.",
                "It seems that Black requires 1 or 2 preventive actions to build its initial taking position, before starting to present threats.",
                "If it did not manage to win, it will usually prevent an extra threat or two before succumbing to White. 4.",
                "RELATED WORK Much research deals with the axiomatization of teamwork and mental states of individuals: some models use knowledge and belief [10], others have models of goals and intentions [8, 4].",
                "However, all these formal theories deal with agent teamwork and cooperation.",
                "As far as we know, our model is the first to provide a formalized model for explicit adversarial environments and agents behavior in it.",
                "The classical Min-Max adversarial search algorithm was the first attempt to integrate the opponent into the search space with a weak assumption of an optimally playing opponent.",
                "Since then, much effort has gone into integrating the opponent model into the decision procedure to predict future behavior.",
                "The M∗ algorithm presented by Carmel and Markovitch [2] showed a method of incorporating opponent models into adversary search, while in [3] they used learning to provide a more accurate opponent model in a 2player repeated game environment, where agents strategies were modeled as finite automata.",
                "Additional Adversarial planning work was done by Willmott et al. [13], which provided an adversarial planning approach to the game of GO.",
                "The research mentioned above dealt with adversarial search and the integration of opponent models into classical utilitybased search methods.",
                "That work shows the importance of opponent modeling and the ability to exploit it to an agents advantage.",
                "However, the basic limitations of those search methods still apply; our model tries to overcome those limitations by presenting a formal model for a new, mental state-based adversarial specification. 5.",
                "CONCLUSIONS We presented an Adversarial Environment model for a 2 These were later removed from the final analysis. bounded rational agent that is situated in an N-player, zerosum environment.",
                "We used the SharedPlans formalization to define the model and the axioms that agents can apply as behavioral guidelines.",
                "The model is meant to be used as a guideline for designing agents that need to operate in such adversarial environments.",
                "We presented empirical results, based on ConnectFour log file analysis, that exemplify the model and the axioms for a bilateral instance of the environment.",
                "The results we presented are a first step towards an expanded model that will cover all types of adversarial environments, for example, environments that are non-zero-sum, and environments that contain natural agents that are not part of the direct conflict.",
                "Those challenges and more will be dealt with in future research. 6.",
                "ACKNOWLEDGMENT This research was supported in part by Israel Science Foundation grants #1211/04 and #898/05. 7.",
                "REFERENCES [1] L. V. Allis.",
                "A knowledge-based approach of Connect-Four - the game is solved: White wins.",
                "Masters thesis, Free University, Amsterdam, The Netherlands, 1988. [2] D. Carmel and S. Markovitch.",
                "Incorporating opponent models into adversary search.",
                "In Proceedings of the Thirteenth National Conference on Artificial Intelligence, pages 120-125, Portland, OR, 1996. [3] D. Carmel and S. Markovitch.",
                "Opponent modeling in multi-agent systems.",
                "In G. Weiß and S. Sen, editors, Adaptation and Learning in Multi-Agent Systems, pages 40-52.",
                "Springer-Verlag, 1996. [4] B. J. Grosz and S. Kraus.",
                "Collaborative plans for complex group action.",
                "Artificial Intelligence, 86(2):269-357, 1996. [5] M. Hadad, G. Kaminka, G. Armon, and S. Kraus.",
                "Supporting collaborative activity.",
                "In Proc. of AAAI-2005, pages 83-88, Pittsburgh, 2005. [6] http://www.gamerz.net/˜pbmserv/. [7] S. Kraus and D. Lehmann.",
                "Designing and building a negotiating automated agent.",
                "Computational Intelligence, 11:132-171, 1995. [8] H. J. Levesque, P. R. Cohen, and J. H. T. Nunes.",
                "On acting together.",
                "In Proc. of AAAI-90, pages 94-99, Boston, MA, 1990. [9] S. Markovitch and R. Reger.",
                "Learning and exploiting relative weaknesses of opponent agents.",
                "Autonomous Agents and Multi-Agent Systems, 10(2):103-130, 2005. [10] Y. M. Ronald Fagin, Joseph Y. Halpern and M. Y. Vardi.",
                "Reasoning about knowledge.",
                "MIT Press, Cambridge, Mass., 1995. [11] P. Thagard.",
                "Adversarial problem solving: Modeling an oponent using explanatory coherence.",
                "Cognitive Science, 16(1):123-149, 1992. [12] R. W. Toseland and R. F. Rivas.",
                "An Introduction to Group Work Practice.",
                "Prentice Hall, Englewood Cliffs, NJ, 2nd edition edition, 1995. [13] S. Willmott, J. Richardson, A. Bundy, and J. Levine.",
                "An adversarial planning approach to Go.",
                "Lecture Notes in Computer Science, 1558:93-112, 1999.",
                "The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) 557"
            ],
            "original_annotated_samples": [],
            "translated_annotated_samples": [
                "Axioma de formación de la alianza El siguiente axioma de comportamiento es relevante solo en una instanciación multilateral del entorno adversario (obviamente, una alianza no se puede formar en un \"encuentro de suma cero\" bilateral).encuentro de suma cero"
            ],
            "translated_text": "",
            "candidates": [],
            "error": []
        },
        "treatment group": {
            "translated_key": "",
            "is_in_text": true,
            "original_annotated_sentences": [
                "An Adversarial Environment Model for Bounded Rational Agents in Zero-Sum Interactions Inon Zuckerman1 , Sarit Kraus1 , Jeffrey S. Rosenschein2 , Gal Kaminka1 1 Department of Computer Science 2 The School of Engineering Bar-Ilan University and Computer Science Ramat-Gan, Israel Hebrew University, Jerusalem, Israel {zukermi,sarit,galk}@cs.biu.ac.il jeff@cs.huji.ac.il ABSTRACT Multiagent environments are often not cooperative nor collaborative; in many cases, agents have conflicting interests, leading to adversarial interactions.",
                "This paper presents a formal Adversarial Environment model for bounded rational agents operating in a zero-sum environment.",
                "In such environments, attempts to use classical utility-based search methods can raise a variety of difficulties (e.g., implicitly modeling the opponent as an omniscient utility maximizer, rather than leveraging a more nuanced, explicit opponent model).",
                "We define an Adversarial Environment by describing the mental states of an agent in such an environment.",
                "We then present behavioral axioms that are intended to serve as design principles for building such adversarial agents.",
                "We explore the application of our approach by analyzing log files of completed Connect-Four games, and present an empirical analysis of the axioms appropriateness.",
                "Categories and Subject Descriptors I.2.11 [Artificial Intelligence]: Distributed Artificial Intelligence-Intelligent agents,Multiagent Systems; I.2.4 [Artificial Intelligence]: Knowledge Representation Formalisms and Methods -Modal logic General Terms Design, Theory 1.",
                "INTRODUCTION Early research in multiagent systems (MAS) considered cooperative groups of agents; because individual agents had limited resources, or limited access to information (e.g., limited processing power, limited sensor coverage), they worked together by design to solve problems that individually they could not solve, or at least could not solve as efficiently.",
                "MAS research, however, soon began to consider interacting agents with individuated interests, as representatives of different humans or organizations with non-identical interests.",
                "When interactions are guided by diverse interests, participants may have to overcome disagreements, uncooperative interactions, and even intentional attempts to damage one another.",
                "When these types of interactions occur, environments require appropriate behavior from the agents situated in them.",
                "We call these environments Adversarial Environments, and call the clashing agents Adversaries.",
                "Models of cooperation and teamwork have been extensively explored in MAS through the axiomatization of mental states (e.g., [8, 4, 5]).",
                "However, none of this research dealt with adversarial domains and their implications for agent behavior.",
                "Our paper addresses this issue by providing a formal, axiomatized mental state model for a subset of adversarial domains, namely simple zero-sum adversarial environments.",
                "Simple zero-sum encounters exist of course in various twoplayer games (e.g., Chess, Checkers), but they also exist in n-player games (e.g., Risk, Diplomacy), auctions for a single good, and elsewhere.",
                "In these latter environments especially, using a utility-based adversarial search (such as the Min-Max algorithm) does not always provide an adequate solution; the payoff function might be quite complex or difficult to quantify, and there are natural computational limitations on bounded rational agents.",
                "In addition, traditional search methods (like Min-Max) do not make use of a model of the opponent, which has proven to be a valuable addition to adversarial planning [9, 3, 11].",
                "In this paper, we develop a formal, axiomatized model for bounded rational agents that are situated in a zero-sum adversarial environment.",
                "The model uses different modality operators, and its main foundations are the SharedPlans [4] model for collaborative behavior.",
                "We explore environment properties and the mental states of agents to derive behavioral axioms; these behavioral axioms constitute a formal model that serves as a specification and design guideline for agent design in such settings.",
                "We then investigate the behavior of our model empirically using the Connect-Four board game.",
                "We show that this game conforms to our environment definition, and analyze players behavior using a large set of completed match log 550 978-81-904262-7-5 (RPS) c 2007 IFAAMAS files.",
                "In addition, we use the results presented in [9] to discuss the importance of opponent modeling in our ConnectFour adversarial domain.",
                "The paper proceeds as follows.",
                "Section 2 presents the models formalization.",
                "Section 3 presents the empirical analysis and its results.",
                "We discuss related work in Section 4, and conclude and present future directions in Section 5. 2.",
                "ADVERSARIAL ENVIRONMENTS The adversarial environment model (denoted as AE) is intended to guide the design of agents by providing a specification of the capabilities and mental attitudes of an agent in an adversarial environment.",
                "We focus here on specific types of adversarial environments, specified as follows: 1.",
                "Zero-Sum Interactions: positive and negative utilities of all agents sum to zero; 2.",
                "Simple AEs: all agents in the environment are adversarial agents; 3.",
                "Bilateral AEs: AEs with exactly two agents; 4.",
                "Multilateral AEs: AEs of three or more agents.",
                "We will work on both bilateral and multilateral instantiations of zero-sum and simple environments.",
                "In particular, our adversarial environment model will deal with interactions that consist of N agents (N ≥ 2), where all agents are adversaries, and only one agent can succeed.",
                "Examples of such environments range from board games (e.g., Chess, Connect-Four, and Diplomacy) to certain economic environments (e.g., N-bidder auctions over a single good). 2.1 Model Overview Our approach is to formalize the mental attitudes and behaviors of a single adversarial agent; we consider how a single agent perceives the AE.",
                "The following list specifies the conditions and mental states of an agent in a simple, zero-sum AE: 1.",
                "The agent has an individual intention that its own goal will be completed; 2.",
                "The agent has an individual belief that it and its adversaries are pursuing full conflicting goals (defined below)there can be only one winner; 3.",
                "The agent has an individual belief that each adversary has an intention to complete its own full conflicting goal; 4.",
                "The agent has an individual belief in the (partial) profile of its adversaries.",
                "Item 3 is required, since it might be the case that some agent has a full conflicting goal, and is currently considering adopting the intention to complete it, but is, as of yet, not committed to achieving it.",
                "This might occur because the agent has not yet deliberated about the effects that adopting that intention might have on the other intentions it is currently holding.",
                "In such cases, it might not consider itself to even be in an adversarial environment.",
                "Item 4 states that the agent should hold some belief about the profiles of its adversaries.",
                "The profile represents all the knowledge the agent has about its adversary: its weaknesses, strategic capabilities, goals, intentions, trustworthiness, and more.",
                "It can be given explicitly or can be learned from observations of past encounters. 2.2 Model Definitions for Mental States We use Grosz and Krauss definitions of the modal operators, predicates, and meta-predicates, as defined in their SharedPlan formalization [4].",
                "We recall here some of the predicates and operators that are used in that formalization: Int.To(Ai, α, Tn, Tα, C) represents Ais intentions at time Tn to do an action α at time Tα in the context of C. Int.Th(Ai, prop, Tn, Tprop, C) represents Ais intentions at time Tn that a certain proposition prop holds at time Tprop in the context of C. The potential intention operators, Pot.Int.To(...) and Pot.Int.Th(...), are used to represent the mental state when an agent considers adopting an intention, but has not deliberated about the interaction of the other intentions it holds.",
                "The operator Bel(Ai, f, Tf ) represents agent Ai believing in the statement expressed in formula f, at time Tf .",
                "MB(A, f, Tf ) represents mutual belief for a group of agents A.",
                "A snapshot of the system finds our environment to be in some state e ∈ E of environmental variable states, and each adversary in any LAi ∈ L of possible local states.",
                "At any given time step, the system will be in some world w of the set of all possible worlds w ∈ W, where w = E×LA1 ×LA2 × ...LAn , and n is the number of adversaries.",
                "For example, in a Texas Holdem poker game, an agents local state might be its own set of cards (which is unknown to its adversary) while the environment will consist of the betting pot and the community cards (which are visible to both players).",
                "A utility function under this formalization is defined as a mapping from a possible world w ∈ W to an element in , which expresses the desirability of the world, from a single agent perspective.",
                "We usually normalize the range to [0,1], where 0 represents the least desirable possible world, and 1 is the most desirable world.",
                "The implementation of the utility function is dependent on the domain in question.",
                "The following list specifies new predicates, functions, variables, and constants used in conjunction with the original definitions for the adversarial environment formalization: 1. φ is a null action (the agent does not do anything). 2.",
                "GAi is the set of agent Ais goals.",
                "Each goal is a set of predicates whose satisfaction makes the goal complete (we use G∗ Ai ∈ GAi to represent an arbitrary goal of agent Ai). 3. gAi is the set of agent Ais subgoals.",
                "Subgoals are predicates whose satisfaction represents an important milestone toward achievement of the full goal. gG∗ Ai ⊆ gAi is the set of subgoals that are important to the completion of goal G∗ Ai (we will use g∗ G∗ Ai ∈ gG∗ Ai to represent an arbitrary subgoal). 4.",
                "P Aj Ai is the profile object agent Ai holds about agent Aj. 5.",
                "CA is a general set of actions for all agents in A which are derived from the environments constraints.",
                "CAi ⊆ CA is the set of agent Ais possible actions. 6.",
                "Do(Ai, α, Tα, w) holds when Ai performs action α over time interval Tα in world w. 7.",
                "Achieve(G∗ Ai , α, w) is true when goal G∗ Ai is achieved following the completion of action α in world w ∈ W, where α ∈ CAi . 8.",
                "Profile(Ai, PAi Ai ) is true when agent Ai holds an object profile for agent Aj.",
                "Definition 1.",
                "Full conflict (FulConf ) describes a zerosum interaction where only a single goal of the goals in conflict can be completed.",
                "FulConf(G∗ Ai , G∗ Aj ) ⇒ (∃α ∈ CAi , ∀w, β ∈ CAj ) (Achieve(G∗ Ai , α, w) ⇒ ¬Achieve(G∗ Aj , β, w)) ∨ (∃β ∈ CAj , ∀w, α ∈ CAi )(Achieve(G∗ Aj , β, w) ⇒ ¬Achieve(G∗ Ai , α, w)) Definition 2.",
                "Adversarial Knowledge (AdvKnow) is a function returning a value which represents the amount of The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) 551 knowledge agent Ai has on the profile of agent Aj, at time Tn.",
                "The higher the value, the more knowledge agent Ai has.",
                "AdvKnow : P Aj Ai × Tn → Definition 3.",
                "Eval - This evaluation function returns an estimated expected utility value for an agent in A, after completing an action from CA in some world state w. Eval : A × CA × w → Definition 4.",
                "TrH - (Threshold) is a numerical constant in the [0,1] range that represents an evaluation function (Eval) threshold value.",
                "An action that yields an estimated utility evaluation above the TrH is regarded as a highly beneficial action.",
                "The Eval value is an estimation and not the real utility function, which is usually unknown.",
                "Using the real utility value for a rational agent would easily yield the best outcome for that agent.",
                "However, agents usually do not have the real utility functions, but rather a heuristic estimate of it.",
                "There are two important properties that should hold for the evaluation function: Property 1.",
                "The evaluation function should state that the most desirable world state is one in which the goal is achieved.",
                "Therefore, after the goal has been satisfied, there can be no future action that can put the agent in a world state with higher Eval value. (∀Ai, G∗ Ai , α, β ∈ CAi , w ∈ W) Achieve(G∗ Ai , α, w) ⇒ Eval(Ai, α, w) ≥ Eval(Ai, β, w) Property 2.",
                "The evaluation function should project an action that causes a completion of a goal or a subgoal to a value which is greater than TrH (a highly beneficial action). (∀Ai, G∗ Ai ∈ GAi , α ∈ CAi , w ∈ W, g∗ GAi ∈ gGAi ) Achieve(G∗ Ai , α, w) ∨ Achieve(g∗ GAi , α, w) ⇒ Eval(Ai, α, w) ≥ TrH.",
                "Definition 5.",
                "SetAction We define a set action (SetAction) as a set of action operations (either complex or basic actions) from some action sets CAi and CAj which, according to agent Ais belief, are attached together by a temporal and consequential relationship, forming a chain of events (action, and its following consequent action). (∀α1 , . . . , αu ∈ CAi , β1 , . . . , βv ∈ CAj , w ∈ W) SetAction(α1 , . . . , αu , β1 , . . . , βv , w) ⇒ ((Do(Ai, α1 , Tα1 , w) ⇒ Do(Aj, β1 , Tβ1 , w)) ⇒ Do(Ai, α2 , Tα2 , w) ⇒ . . . ⇒ Do(Ai, αu , Tαu , w)) The consequential relation might exist due to various environmental constraints (when one action forces the adversary to respond with a specific action) or due to the agents knowledge about the profile of its adversary.",
                "Property 3.",
                "As the knowledge we have about our adversary increases we will have additional beliefs about its behavior in different situations which in turn creates new set actions.",
                "Formally, if our AdvKnow at time Tn+1 is greater than AdvKnow at time Tn, then every SetAction known at time Tn is also known at time Tn+1.",
                "AdvKnow(P Aj Ai , Tn+1) > AdvKnow(P Aj Ai , Tn) ⇒ (∀α1 , . . . , αu ∈ CAi , β1 , . . . , βv ∈ CAj ) Bel(Aag, SetAction(α1 , . . . , αu , β1 , . . . , βv ), Tn) ⇒ Bel(Aag, SetAction(α1 , . . . , αu , β1 , . . . , βv ), Tn+1) 2.3 The Environment Formulation The following axioms provide the formal definition for a simple, zero-sum Adversarial Environment (AE).",
                "Satisfaction of these axioms means that the agent is situated in such an environment.",
                "It provides specifications for agent Aag to interact with its set of adversaries A with respect to goals G∗ Aag and G∗ A at time TCo at some world state w. AE(Aag, A, G∗ Aag , A1, . . . , Ak, G∗ A1 , . . . , G∗ Ak , Tn, w) 1.",
                "Aag has an Int.Th his goal would be completed: (∃α ∈ CAag , Tα) Int.Th(Aag, Achieve(G∗ Aag , α), Tn, Tα, AE) 2.",
                "Aag believes that it and each of its adversaries Ao are pursuing full conflicting goals: (∀Ao ∈ {A1, . . . , Ak}) Bel(Aag, FulConf(G∗ Aag , G∗ Ao ), Tn) 3.",
                "Aag believes that each of his adversaries in Ao has the Int.Th his conflicting goal G∗ Aoi will be completed: (∀Ao ∈ {A1, . . . , Ak})(∃β ∈ CAo , Tβ) Bel(Aag, Int.Th(Ao, Achieve(G∗ Ao , β), TCo, Tβ, AE), Tn) 4.",
                "Aag has beliefs about the (partial) profiles of its adversaries (∀Ao ∈ {A1, . . . , Ak}) (∃PAo Aag ∈ PAag )Bel(Aag, Profile(Ao, PAo Aag ), Tn) To build an agent that will be able to operate successfully within such an AE, we must specify behavioral guidelines for its interactions.",
                "Using a naive Eval maximization strategy to a certain search depth will not always yield satisfactory results for several reasons: (1) the search horizon problem when searching for a fixed depth; (2) the strong assumption of an optimally rational, unbounded resources adversary; (3) using an estimated evaluation function which will not give optimal results in all world states, and can be exploited [9].",
                "The following axioms specify the behavioral principles that can be used to differentiate between successful and less successful agents in the above Adversarial Environment.",
                "Those axioms should be used as specification principles when designing and implementing agents that should be able to perform well in such Adversarial Environments.",
                "The behavioral axioms represent situations in which the agent will adopt potential intentions to (Pot.Int.To(...)) perform an action, which will typically require some means-end reasoning to select a possible course of action.",
                "This reasoning will lead to the adoption of an Int.To(...) (see [4]).",
                "A1.",
                "Goal Achieving Axiom.",
                "The first axiom is the simplest case; when the agent Aag believes that it is one action (α) away from achieving his conflicting goal G∗ Aag , it should adopt the potential intention to do α and complete its goal. (∀Aag, α ∈ CAag , Tn, Tα, w ∈ W) (Bel(Aag, Do(Aag, α, Tα, w) ⇒ Achieve(G∗ Aag , α, w)) ⇒ Pot.Int.To(Aag, α, Tn, Tα, w) This somewhat trivial behavior is the first and strongest axiom.",
                "In any situation, when the agent is an action away from completing the goal, it should complete the action.",
                "Any fair Eval function would naturally classify α as the maximal value action (property 1).",
                "However, without explicit axiomatization of such behavior there might be situations where the agent will decide on taking another action for various reasons, due to its bounded decision resources.",
                "A2.",
                "Preventive Act Axiom.",
                "Being in an adversarial situation, agent Aag might decide to take actions that will damage one of its adversarys plans to complete its goal, even if those actions do not explicitly advance Aag towards its conflicting goal G∗ Aag .",
                "Such preventive action will take place when agent Aag has a belief about the possibility of its adversary Ao doing an action β that will give it a high 552 The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) utility evaluation value (> TrH).",
                "Believing that taking action α will prevent the opponent from doing its β, it will adopt a potential intention to do α. (∀Aag, Ao ∈ A, α ∈ CAag , β ∈ CAo , Tn, Tβ, w ∈ W) (Bel(Aag, Do(Ao, β, Tβ, w) ∧ Eval(Ao, β, w) > TrH, Tn) ∧ Bel(Aag, Do(Aag, α, Tα, w) ⇒ ¬Do(Ao, β, Tβ, w), Tn) ⇒ Pot.Int.To(Aag, α, Tn, Tα, w) This axiom is a basic component of any adversarial environment.",
                "For example, looking at a Chess board game, a player could realize that it is about to be checkmated by its opponent, thus making a preventive move.",
                "Another example is a Connect Four game: when a player has a row of three chips, its opponent must block it, or lose.",
                "A specific instance of A1 occurs when the adversary is one action away from achieving its goal, and immediate preventive action needs to be taken by the agent.",
                "Formally, we have the same beliefs as stated above, with a changed belief that doing action β will cause agent Ao to achieve its goal.",
                "Proposition 1: Prevent or lose case. (∀Aag, Ao ∈ A, α ∈ CAag , β ∈ CAo , G∗ Ao , Tn, Tα, Tβ, w ∈ W) Bel(Aag, Do(Ao, β, Tβ, w) ⇒ Achieve(G∗ Ao , β, w), Tn) ∧ Bel(Aag, Do(Aag, α, Tα, w) ⇒ ¬Do(Ao, β, Tβ, w)) ⇒ Pot.Int.To(Aag, α, Tn, Tα, w) Sketch of proof: Proposition 1 can be easily derived from axiom A1 and the property 2 of the Eval function, which states that any action that causes a completion of a goal is a highly beneficial action.",
                "The preventive act behavior will occur implicitly when the Eval function is equal to the real world utility function.",
                "However, being bounded rational agents and dealing with an estimated evaluation function we need to explicitly axiomatize such behavior, for it will not always occur implicitly from the evaluation function.",
                "A3.",
                "Suboptimal Tactical Move Axiom.",
                "In many scenarios a situation may occur where an agent will decide not to take the current most beneficial action it can take (the action with the maximal utility evaluation value), because it believes that taking another action (with lower utility evaluation value) might yield (depending on the adversarys response) a future possibility for a highly beneficial action.",
                "This will occur most often when the Eval function is inaccurate and differs by a large extent from the Utility function.",
                "Put formally, agent Aag believes in a certain SetAction that will evolve according to its initial action and will yield a high beneficial value (> TrH) solely for it. (∀Aag, Ao ∈ A, Tn, w ∈ W) (∃α1 , . . . , αu ∈ CAi , β1 , . . . , βv ∈ CAj , Tα1 ) Bel(Aag, SetAction(α1 , . . . , αu , β1 , . . . , βv ), Tn) ∧ Bel(Aag, Eval(Ao, βv , w) < TrH < Eval(Aag, αu , w), Tn) ⇒ Pot.Int.To(Aag, α1 , Tn, Tα1 , w) An agent might believe that a chain of events will occur for various reasons due to the inevitable nature of the domain.",
                "For example, in Chess, we often observe the following: a move causes a check position, which in turn limits the opponents moves to avoiding the check, to which the first player might react with another check, and so on.",
                "The agent might also believe in a chain of events based on its knowledge of its adversarys profile, which allows it to foresee the adversarys movements with high accuracy.",
                "A4.",
                "Profile Detection Axiom.",
                "The agent can adjust its adversarys profiles by observations and pattern study (specifically, if there are repeated encounters with the same adversary).",
                "However, instead of waiting for profile information to be revealed, an agent can also initiate actions that will force its adversary to react in a way that will reveal profile knowledge about it.",
                "Formally, the axiom states that if all actions (γ) are not highly beneficial actions (< TrH), the agent can do action α in time Tα if it believes that it will result in a non-highly beneficial action β from its adversary, which in turn teaches it about the adversarys profile, i.e., gives a higher AdvKnow(P Aj Ai , Tβ). (∀Aag, Ao ∈ A, α ∈ CAag , β ∈ CAo , Tn, Tα, Tβ, w ∈ W) Bel(Aag, (∀γ ∈ CAag )Eval(Aag, γ, w) < TrH, Tn) ∧ Bel(Aag, Do(Aag, α, Tα, w) ⇒ Do(Ao, β, Tβ, w), Tn) ∧ Bel(Aag, Eval(Ao, β, w) < TrH) ∧ Bel(Aag, AdvKnow(P Aj Ai , Tβ) > AdvKnow(P Aj Ai , Tn), Tn) ⇒ Pot.Int.To(Aag, α, Tn, Tα, w) For example, going back to the Chess board game scenario, consider starting a game versus an opponent about whom we know nothing, not even if it is a human or a computerized opponent.",
                "We might start playing a strategy that will be suitable versus an average opponent, and adjust our game according to its level of play.",
                "A5.",
                "Alliance Formation Axiom The following behavioral axiom is relevant only in a multilateral instantiation of the adversarial environment (obviously, an alliance cannot be formed in a bilateral, zero-sum encounter).",
                "In different situations during a multilateral interaction, a group of agents might believe that it is in their best interests to form a temporary alliance.",
                "Such an alliance is an agreement that constrains its members behavior, but is believed by its members to enable them to achieve a higher utility value than the one achievable outside of the alliance.",
                "As an example, we can look at the classical Risk board game, where each player has an individual goal of being the sole conquerer of the world, a zero-sum game.",
                "However, in order to achieve this goal, it might be strategically wise to make short-term ceasefire agreements with other players, or to join forces and attack an opponent who is stronger than the rest.",
                "An alliances terms defines the way its members should act.",
                "It is a set of predicates, denoted as Terms, that is agreed upon by the alliance members, and should remain true for the duration of the alliance.",
                "For example, the set Terms in the Risk scenario, could contain the following predicates: 1.",
                "Alliance members will not attack each other on territories X, Y and Z; 2.",
                "Alliance members will contribute C units per turn for attacking adversary Ao; 3.",
                "Members are obligated to stay as part of the alliance until time Tk or until adversarys Ao army is smaller than Q.",
                "The set Terms specifies inter-group constraints on each of the alliance members (∀Aal i ∈ Aal ⊆ A) set of actions Cal i ⊆ C. Definition 6.",
                "Al val - the total evaluation value that agent Ai will achieve while being part of Aal is the sum of Evali (Eval for Ai) of each of Aal j Eval values after taking their own α actions (via the agent(α) predicate): Al val(Ai, Cal , Aal , w) = α∈Cal Evali(Aal j , agent(α), w) Definition 7.",
                "Al TrH - is a number representing an Al val The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) 553 threshold; above it, the alliance can be said to be a highly beneficial alliance.",
                "The value of Al TrH will be calculated dynamically according to the progress of the interaction, as can be seen in [7].",
                "After an alliance is formed, its members are now working in their normal adversarial environment, as well as according to the mental states and axioms required for their interactions as part of the alliance.",
                "The following Alliance model (AL) specifies the conditions under which the group Aal can be said to be in an alliance and working with a new and constrained set of actions Cal , at time Tn.",
                "AL(Aal , Cal , w, Tn) 1.",
                "Aal has a MB that all members are part of Aal : MB(Aal , (∀Aal i ∈ Aal )member(Aal i , Aal ), Tn) 2.",
                "Aal has a MB that the group be maintained: MB(Aal , (∀Aal i ∈ Aal )Int.Th (Ai, member(Ai, Aal ), Tn, Tn+1, Co), Tn) 3.",
                "Aal has a MB that being members gives them high utility value: MB(Aal , (∀Aal i ∈ Aal )Al val(Aal i , Cal , Aal , w) ≥ Al TrH, Tn) Members profiles are a crucial part of successful alliances.",
                "We assume that agents that have more accurate profiles of their adversaries will be more successful in such environments.",
                "Such agents will be able to predict when a member is about to breach the alliances contract (item 2 in the above model), and take counter measures (when item 3 will falsify).",
                "The robustness of the alliance is in part a function of its members trustfulness measure, objective position estimation, and other profile properties.",
                "We should note that an agent can simultaneously be part of more than one alliance.",
                "Such a temporary alliance, where the group members do not have a joint goal but act collaboratively for the interest of their own individual goals, is classified as a <br>treatment group</br> by modern psychologists [12] (in contrast to a Task Group, where its members have a joint goal).",
                "The Shared Activity model as presented in [5] modeled <br>treatment group</br> behavior using the same SharedPlans formalization.",
                "When comparing both definitions of an alliance and a <br>treatment group</br> we found an unsurprising resemblance between both models: the environment models definitions are almost identical (see SAs definitions in [5]), and their Selfish-Act and Cooperative Act axioms conform to our adversarial agents behavior.",
                "The main distinction between both models is the integration of a Helpful-behavior act axiom, in the Shared Activity which cannot be part of ours.",
                "This axiom states that an agent will consider taking action that will lower its Eval value (to a certain lower bound), if it believes that a group partner will gain a significant benefit.",
                "Such behavior cannot occur in a pure adversarial environment (as a zero-sum game is), where the alliance members are constantly on watch to manipulate their alliance to their own advantage.",
                "A6.",
                "Evaluation Maximization Axiom.",
                "In a case when all other axioms are inapplicable, we will proceed with the action that maximizes the heuristic value as computed in the Eval function. (∀Aag, Ao ∈ A, α ∈ Cag, Tn, w ∈ W) Bel(Aag, (∀γ ∈ Cag)Eval(Aag, α, w) ≥ Eval(Aag, γ, w), Tn) ⇒ Pot.Int.To(Aag, α, Tn, Tα, w) T1.",
                "Optimality on Eval = Utility The above axiomatic model handles situations where the Utility is unknown and the agents are bounded rational agents.",
                "The following theorem shows that in bilateral interactions, where the agents have the real Utility function (i.e., Eval = Utility) and are rational agents, the axioms provide the same optimal result as classic adversarial search (e.g., Min-Max).",
                "Theorem 1.",
                "Let Ae ag be an unbounded rational AE agent using the Eval heuristic evaluation function, Au ag be the same agent using the true Utility function, and Ao be a sole unbounded utility-based rational adversary.",
                "Given that Eval = Utility: (∀α ∈ CAu ag , α ∈ CAe ag , Tn, w ∈ W) Pot.Int.To(Au ag, α, Tn, Tα, w) → Pot.Int.To(Ae ag, α , Tn, Tα, w) ∧ ((α = α ) ∨ (Utility(Au ag, α, w) = Eval(Ae ag, α , w))) Sketch of proof - Given that Au ag has the real utility function and unbounded resources, it can generate the full game tree and run the optimal MinMax algorithm to choose the highest utility value action, which we denote by, α.",
                "The proof will show that Ae ag, using the AE axioms, will select the same or equal utility α (when there is more than one action with the same max utility) when Eval = Utility. (A1) Goal achieving axiom - suppose there is an α such that its completion will achieve Au ags goal.",
                "It will obtain the highest utility by Min-Max for Au ag.",
                "The Ae ag agent will select α or another action with the same utility value via A1.",
                "If such α does not exist, Ae ag cannot apply this axiom, and proceeds to A2. (A2) Preventive act axiom - (1) Looking at the basic case (see Prop1), if there is a β which leads Ao to achieve its goal, then a preventive action α will yield the highest utility for Au ag.",
                "Au ag will choose it through the utility, while Ae ag will choose it through A2. (2) In the general case, β is a highly beneficial action for Ao, thus yields low utility for Au ag, which will guide it to select an α that will prevent β, while Ae ag will choose it through A2.1 If such β does not exist for Ao, then A2 is not applicable, and Ae ag can proceed to A3. (A3) Suboptimal tactical move axiom - When using a heuristic Eval function, Ae ag has a partial belief in the profile of its adversary (item 4 in AE model), which may lead it to believe in SetActions (Prop1).",
                "In our case, Ae ag is holding a full profile on its optimal adversary and knows that Ao will behave optimally according to the real utility values on the complete search tree, therefore, any belief about suboptimal SetAction cannot exist, yielding this axiom inapplicable.",
                "Ae ag will proceed to A4. (A4) Profile detection axiom - Given that Ae ag has the full profile of Ao, none of Ae ags actions can increase its knowledge.",
                "That axiom will not be applied, and the agent will proceed with A6 (A5 will be disregarded because the interaction is bilateral). (A6) Evaluation maximization axiom - This axiom will select the max Eval for Ae ag.",
                "Given that Eval = Utility, the same α that was selected by Au ag will be selected. 3.",
                "EVALUATION The main purpose of our experimental analysis is to evaluate the models behavior and performance in a real adversarial environment.",
                "This section investigates whether bounded 1 A case where following the completion of β there exists a γ which gives high utility for Agent Au ag, cannot occur because Ao uses the same utility, and γs existence will cause it to classify β as a low utility action. 554 The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) rational agents situated in such adversarial environments will be better off applying our suggested behavioral axioms. 3.1 The Domain To explore the use of the above model and its behavioral axioms, we decided to use the Connect-Four game as our adversarial environment.",
                "Connect-Four is a 2-player, zerosum game which is played using a 6x7 matrix-like board.",
                "Each turn, a player drops a disc into one of the 7 columns (the set of 21 discs is usually colored yellow for player 1 and red for player 2; we will use White and Black respectively to avoid confusion).",
                "The winner is the first player to complete a horizontal, vertical, or diagonal set of four discs with its color.",
                "On very rare occasions, the game might end in a tie if all the empty grids are filled, but no player managed to create a 4-disc set.",
                "The Connect-Four game was solved in [1], where it is shown that the first player (playing with the white discs) can force a win by starting in the middle column (column 4) and playing optimally However, the optimal strategy is very complex, and difficult to follow even for complex bounded rational agents, such as human players.",
                "Before we can proceed checking agent behavior, we must first verify that the domain conforms to the adversarial environments definition as given above (which the behavioral axioms are based on).",
                "First, when playing a Connect-Four game, the agent has an intention to win the game (item 1).",
                "Second (item 2), our agent believes that in Connect-Four there can only be one winner (or no winner at all in the rare occurrence of a tie).",
                "In addition, our agent believes that its opponent to the game will try to win (item 3), and we hope it has some partial knowledge (item 4) about its adversary (this knowledge can vary from nothing, through simple facts such as age, to strategies and weaknesses).",
                "Of course, not all Connect-Four encounters are adversarial.",
                "For example, when a parent is playing the game with its child, the following situation might occur: the child, having a strong incentive to win, treats the environment as adversarial (it intends to win, understands that there can only be one winner, and believes that its parent is trying to beat him).",
                "However, the parents point of view might see the environment as an educational one, where its goal is not to win the game, but to cause enjoyment or practice strategic reasoning.",
                "In such an educational environment, a new set of behavioral axioms might be more beneficial to the parents goals than our suggested adversarial behavioral axioms. 3.2 Axiom Analysis After showing that the Connect-Four game is indeed a zero-sum, bilateral adversarial environment, the next step is to look at players behaviors during the game and check whether behaving according to our model does improve performance.",
                "To do so we have collected log files from completed Connect-Four games that were played by human players over the Internet.",
                "Our collected log file data came from Play by eMail (PBeM) sites.",
                "These are web sites that host email games, where each move is taken by an email exchange between the server and the players.",
                "Many such sites archives contain real competitive interactions, and also maintain a ranking system for their members.",
                "Most of the data we used can be found in [6].",
                "As can be learned from [1], Connect-Four has an optimal strategy and a considerable advantage for the player who starts the game (which we call the White player).",
                "We will concentrate in our analysis on the second players moves (to be called Black).",
                "The White player, being the first to act, has the so-called initiative advantage.",
                "Having the advantage and a good strategy will keep the Black player busy reacting to its moves, instead of initiating threats.",
                "A threat is a combination of three discs of the same color, with an empty spot for the fourth winning disk.",
                "An open threat is a threat that can be realized in the opponents next move.",
                "In order for the Black player to win, it must somehow turn the tide, take the advantage and start presenting threats to the White player.",
                "We will explore Black players behavior and their conformance to our axioms.",
                "To do so, we built an application that reads log files and analyzes the Black players moves.",
                "The application contains two main components: (1) a Min-Max algorithm for evaluation of moves; (2) open threats detector for the discovering of open threats.",
                "The Min-Max algorithm will work to a given depth, d and for each move α will output the heuristic value for the next action taken by the player as written in the log file, h(α), alongside the maximum heuristic value, maxh(α), that could be achieved prior to taking the move (obviously, if h(α) = maxh(α), then the player did not do the optimal move heuristically).",
                "The threat detectors job is to notify if some action was taken in order to block an open threat (not blocking an open threat will probably cause the player to lose in the opponents next move).",
                "The heuristic function used by Min-Max to evaluate the players utility is the following function, which is simple to compute, yet provides a reasonable challenge to human opponents: Definition 8.",
                "Let Group be an adjacent set of four squares that are horizontal, vertical, or diagonal.",
                "Groupn b (Groupn w) be a Group with n pieces of the black (white) color and 4−n empty squares. h = ((Group1 b ∗α)+(Group2 b ∗β)+(Group3 b ∗γ)+(Group4 b ∗∞)) − ((Group1 w ∗α)+(Group2 w ∗β)+(Group3 w ∗γ)+(Group4 w ∗∞)) The values of α, β and δ can vary to form any desired linear combination; however, it is important to value them with the α < β < δ ordering in mind (we used 1, 4, and 8 as their respective values).",
                "Groups of 4 discs of the same color means victory, thus discovery of such a group will result in ∞ to ensure an extreme value.",
                "We now use our estimated evaluation function to evaluate the Black players actions during the Connect-Four adversarial interaction.",
                "Each game from the log file was input into the application, which processed and output a reformatted log file containing the h value of the current move, the maxh value that could be achieved, and a notification if an open threat was detected.",
                "A total of 123 games were analyzed (57 with White winning, and 66 with Black winning).",
                "A few additional games were manually ignored in the experiment, due to these problems: a player abandoning the game while the outcome is not final, or a blunt irrational move in the early stages of the game (e.g., not blocking an obvious winning group in the first opening moves).",
                "In addition, a single tie game was also removed.",
                "The simulator was run to a search depth of 3 moves.",
                "We now proceed to analyze the games with respect to each behavioral axiom.",
                "The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) 555 Table 1: Average heuristic difference analysis Black losses Black Won Avg minh -17.62 -12.02 Avg 3 lowest h moves (min3 h) -13.20 -8.70 3.2.1 Affirming the Suboptimal tactical move axiom The following section presents the heuristic evaluations of the Min-Max algorithm for each action, and checks the amount and extent of suboptimal tactical actions and their implications on performance.",
                "Table 1 shows results and insights from the games heuristic analysis, when search depth equals 3 (this search depth was selected for the results to be comparable to [9], see Section 3.2.3).",
                "The tables heuristic data is the difference between the present maximal heuristic value and the heuristic value of the action that was eventually taken by the player (i.e., the closer the number is to 0, the closer the action was to the maximum heuristic action).",
                "The first row presents the difference values of the action that had the maximal difference value among all the Black players actions in a given game, as averaged over all Blacks winning and losing games (see respective columns).",
                "In games in which the Black player loses, its average difference value was -17.62, while in games in which the Black player won, its average was -12.02.",
                "The second row expands the analysis by considering the 3 highest heuristic difference actions, and averaging them.",
                "In that case, we notice an average heuristic difference of 5 points between games which the Black player loses and games in which it wins.",
                "Nevertheless, the importance of those numbers is that they allowed us to take an educated guess on a threshold number of 11.5, as the value of the TrH constant, which differentiates between normal actions and highly beneficial ones.",
                "After finding an approximated TrH constant, we can proceed with an analysis of the importance of suboptimal moves.",
                "To do so we took the subset of games in which the minimum heuristic difference value for Blacks actions was 11.5.",
                "As presented in Table 2, we can see the different min3 h average of the 3 largest ranges and the respective percentage of games won.",
                "The first row shows that the Black player won only 12% of the games in which the average of its 3 highest heuristically difference actions (min3 h) was smaller than the suggested threshold, TrH = 11.5.",
                "The second row shows a surprising result: it seems that when min3 h > −4 the Black player rarely wins.",
                "Intuition would suggest that games in which the action evaluation values were closer to the maximal values will result in more winning games for Black.",
                "However, it seems that in the Connect-Four domain, merely responding with somewhat easily expected actions, without initiating a few surprising and suboptimal moves, does not yield good results.",
                "The last row sums up the main insights from the analysis; most of Blacks wins (83%) came when its min3 h was in the range of -11.5 to -4.",
                "A close inspection of those Black winning games shows the following pattern behind the numbers: after standard opening moves, Black suddenly drops a disc into an isolated column, which seems a waste of a move.",
                "White continues to build its threats, while usually disregarding Blacks last move, which in turn uses the isolated disc as an anchor for a future winning threat.",
                "The results show that it was beneficial for the Black player Table 2: Blacks winnings percentages % of games min3 h < −11.5 12% min3 h > −4 5% −11.5 ≤ min3 h ≤ −4 83% to take suboptimal actions and not give the current highest possible heuristic value, but will not be too harmful for its position (i.e., will not give high beneficial value to its adversary).",
                "As it turned out, learning the threshold is an important aspect of success: taking wildly risky moves (min3 h < −11.5) or trying to avoid them (min3 h > −4) reduces the Black players winning chances by a large margin. 3.2.2 Affirming the Profile Monitoring Axiom In the task of showing the importance of monitoring ones adversaries profiles, our log files could not be used because they did not contain repeated interactions between players, which are needed to infer the players knowledge about their adversaries.",
                "However, the importance of opponent modeling and its use in attaining tactical advantages was already studied in various domains ([3, 9] are good examples).",
                "In a recent paper, Markovitch and Reger [9] explored the notion of learning and exploitation of opponent weakness in competitive interactions.",
                "They apply simple learning strategies by analyzing examples from past interactions in a specific domain.",
                "They also used the Connect-Four adversarial domain, which can now be used to understand the importance of monitoring the adversarys profile.",
                "Following the presentation of their theoretical model, they describe an extensive empirical study and check the agents performance after learning the weakness model with past examples.",
                "One of the domains used as a competitive environment was the same Connect-Four game (Checkers was the second domain).",
                "Their heuristic function was identical to ours with three different variations (H1, H2, and H3) that are distinguished from one another in their linear combination coefficient values.",
                "The search depth for the players was 3 (as in our analysis).",
                "Their extensive experiments check and compare various learning strategies, risk factors, predefined feature sets and usage methods.",
                "The bottom line is that the Connect-Four domain shows an improvement from a 0.556 winning rate before modeling to a 0.69 after modeling (page 22).",
                "Their conclusions, showing improved performance when holding and using the adversarys model, justify the effort to monitor the adversary profile for continuous and repeated interactions.",
                "An additional point that came up in their experiments is the following: after the opponent weakness model has been learned, the authors describe different methods of integrating the opponent weakness model into the agents decision strategy.",
                "Nevertheless, regardless of the specific method they chose to work with, all integration methods might cause the agent to take suboptimal decisions; it might cause the agent to prefer actions that are suboptimal at the present decision junction, but which might cause the opponent to react in accordance with its weakness model (as represented by our agent) which in turn will be beneficial for us in the future.",
                "The agents behavior, as demonstrated in [9] further confirms and strengthens our Suboptimal Tactical Axiom as discussed in the previous section. 556 The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) 3.2.3 Additional Insights The need for the Goal Achieving, Preventive Act, and Evaluation Maximization axioms are obvious, and need no further verification.",
                "However, even with respect to those axioms, a few interesting insights came up in the log analysis.",
                "The Goal achieving and Preventive Act axioms, though theoretically trivial, seem to provide some challenge to a human player.",
                "In the initial inspection of the logs, we encountered few games2 where a player, for inexplicable reasons, did not block the other from winning or failed to execute its own winning move.",
                "We can blame those faults on the humans lack of attention, or a typing error in its move reply; nevertheless, those errors might occur in bounded rational agents, and the appropriate behavior needs to be axiomatized.",
                "A typical Connect-Four game revolves around generating threats and blocking them.",
                "In our analysis we looked for explicit preventive actions, i.e., moves that block a group of 3 discs, or that remove a future threat (in our limited search horizon).",
                "We found that in 83% of the total games there was at least one preventive action taken by the Black player.",
                "It was also found that Black averaged 2.8 preventive actions per game on the games in which it lost, while averaging 1.5 preventive actions per game when winning.",
                "It seems that Black requires 1 or 2 preventive actions to build its initial taking position, before starting to present threats.",
                "If it did not manage to win, it will usually prevent an extra threat or two before succumbing to White. 4.",
                "RELATED WORK Much research deals with the axiomatization of teamwork and mental states of individuals: some models use knowledge and belief [10], others have models of goals and intentions [8, 4].",
                "However, all these formal theories deal with agent teamwork and cooperation.",
                "As far as we know, our model is the first to provide a formalized model for explicit adversarial environments and agents behavior in it.",
                "The classical Min-Max adversarial search algorithm was the first attempt to integrate the opponent into the search space with a weak assumption of an optimally playing opponent.",
                "Since then, much effort has gone into integrating the opponent model into the decision procedure to predict future behavior.",
                "The M∗ algorithm presented by Carmel and Markovitch [2] showed a method of incorporating opponent models into adversary search, while in [3] they used learning to provide a more accurate opponent model in a 2player repeated game environment, where agents strategies were modeled as finite automata.",
                "Additional Adversarial planning work was done by Willmott et al. [13], which provided an adversarial planning approach to the game of GO.",
                "The research mentioned above dealt with adversarial search and the integration of opponent models into classical utilitybased search methods.",
                "That work shows the importance of opponent modeling and the ability to exploit it to an agents advantage.",
                "However, the basic limitations of those search methods still apply; our model tries to overcome those limitations by presenting a formal model for a new, mental state-based adversarial specification. 5.",
                "CONCLUSIONS We presented an Adversarial Environment model for a 2 These were later removed from the final analysis. bounded rational agent that is situated in an N-player, zerosum environment.",
                "We used the SharedPlans formalization to define the model and the axioms that agents can apply as behavioral guidelines.",
                "The model is meant to be used as a guideline for designing agents that need to operate in such adversarial environments.",
                "We presented empirical results, based on ConnectFour log file analysis, that exemplify the model and the axioms for a bilateral instance of the environment.",
                "The results we presented are a first step towards an expanded model that will cover all types of adversarial environments, for example, environments that are non-zero-sum, and environments that contain natural agents that are not part of the direct conflict.",
                "Those challenges and more will be dealt with in future research. 6.",
                "ACKNOWLEDGMENT This research was supported in part by Israel Science Foundation grants #1211/04 and #898/05. 7.",
                "REFERENCES [1] L. V. Allis.",
                "A knowledge-based approach of Connect-Four - the game is solved: White wins.",
                "Masters thesis, Free University, Amsterdam, The Netherlands, 1988. [2] D. Carmel and S. Markovitch.",
                "Incorporating opponent models into adversary search.",
                "In Proceedings of the Thirteenth National Conference on Artificial Intelligence, pages 120-125, Portland, OR, 1996. [3] D. Carmel and S. Markovitch.",
                "Opponent modeling in multi-agent systems.",
                "In G. Weiß and S. Sen, editors, Adaptation and Learning in Multi-Agent Systems, pages 40-52.",
                "Springer-Verlag, 1996. [4] B. J. Grosz and S. Kraus.",
                "Collaborative plans for complex group action.",
                "Artificial Intelligence, 86(2):269-357, 1996. [5] M. Hadad, G. Kaminka, G. Armon, and S. Kraus.",
                "Supporting collaborative activity.",
                "In Proc. of AAAI-2005, pages 83-88, Pittsburgh, 2005. [6] http://www.gamerz.net/˜pbmserv/. [7] S. Kraus and D. Lehmann.",
                "Designing and building a negotiating automated agent.",
                "Computational Intelligence, 11:132-171, 1995. [8] H. J. Levesque, P. R. Cohen, and J. H. T. Nunes.",
                "On acting together.",
                "In Proc. of AAAI-90, pages 94-99, Boston, MA, 1990. [9] S. Markovitch and R. Reger.",
                "Learning and exploiting relative weaknesses of opponent agents.",
                "Autonomous Agents and Multi-Agent Systems, 10(2):103-130, 2005. [10] Y. M. Ronald Fagin, Joseph Y. Halpern and M. Y. Vardi.",
                "Reasoning about knowledge.",
                "MIT Press, Cambridge, Mass., 1995. [11] P. Thagard.",
                "Adversarial problem solving: Modeling an oponent using explanatory coherence.",
                "Cognitive Science, 16(1):123-149, 1992. [12] R. W. Toseland and R. F. Rivas.",
                "An Introduction to Group Work Practice.",
                "Prentice Hall, Englewood Cliffs, NJ, 2nd edition edition, 1995. [13] S. Willmott, J. Richardson, A. Bundy, and J. Levine.",
                "An adversarial planning approach to Go.",
                "Lecture Notes in Computer Science, 1558:93-112, 1999.",
                "The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) 557"
            ],
            "original_annotated_samples": [],
            "translated_annotated_samples": [
                "Tal alianza temporal, donde los miembros del grupo no tienen un objetivo conjunto, sino que actúan en colaboración por el interés de sus propios objetivos individuales, se clasifica como un \"grupo de tratamiento\" por los psicólogos modernos [12] (en contraste con un grupo de tareas, dondesus miembros tienen un objetivo conjunto).grupo de tratamiento",
                "El modelo de actividad compartida como se presenta en [5] comportamiento de \"grupo de tratamiento\" modelado utilizando la misma formalización de planos compartidos.grupo de tratamiento",
                "Al comparar ambas definiciones de una alianza y un \"grupo de tratamiento\", encontramos una semejanza no sorprendente entre ambos modelos: las definiciones de los modelos ambientales son casi idénticos (ver definiciones SAS en [5]), y sus axiomas de acto egoístas y cooperativos se ajustan aNuestro comportamiento de los agentes adversos.grupo de tratamiento"
            ],
            "translated_text": "",
            "candidates": [],
            "error": []
        },
        "eval value": {
            "translated_key": "",
            "is_in_text": true,
            "original_annotated_sentences": [
                "An Adversarial Environment Model for Bounded Rational Agents in Zero-Sum Interactions Inon Zuckerman1 , Sarit Kraus1 , Jeffrey S. Rosenschein2 , Gal Kaminka1 1 Department of Computer Science 2 The School of Engineering Bar-Ilan University and Computer Science Ramat-Gan, Israel Hebrew University, Jerusalem, Israel {zukermi,sarit,galk}@cs.biu.ac.il jeff@cs.huji.ac.il ABSTRACT Multiagent environments are often not cooperative nor collaborative; in many cases, agents have conflicting interests, leading to adversarial interactions.",
                "This paper presents a formal Adversarial Environment model for bounded rational agents operating in a zero-sum environment.",
                "In such environments, attempts to use classical utility-based search methods can raise a variety of difficulties (e.g., implicitly modeling the opponent as an omniscient utility maximizer, rather than leveraging a more nuanced, explicit opponent model).",
                "We define an Adversarial Environment by describing the mental states of an agent in such an environment.",
                "We then present behavioral axioms that are intended to serve as design principles for building such adversarial agents.",
                "We explore the application of our approach by analyzing log files of completed Connect-Four games, and present an empirical analysis of the axioms appropriateness.",
                "Categories and Subject Descriptors I.2.11 [Artificial Intelligence]: Distributed Artificial Intelligence-Intelligent agents,Multiagent Systems; I.2.4 [Artificial Intelligence]: Knowledge Representation Formalisms and Methods -Modal logic General Terms Design, Theory 1.",
                "INTRODUCTION Early research in multiagent systems (MAS) considered cooperative groups of agents; because individual agents had limited resources, or limited access to information (e.g., limited processing power, limited sensor coverage), they worked together by design to solve problems that individually they could not solve, or at least could not solve as efficiently.",
                "MAS research, however, soon began to consider interacting agents with individuated interests, as representatives of different humans or organizations with non-identical interests.",
                "When interactions are guided by diverse interests, participants may have to overcome disagreements, uncooperative interactions, and even intentional attempts to damage one another.",
                "When these types of interactions occur, environments require appropriate behavior from the agents situated in them.",
                "We call these environments Adversarial Environments, and call the clashing agents Adversaries.",
                "Models of cooperation and teamwork have been extensively explored in MAS through the axiomatization of mental states (e.g., [8, 4, 5]).",
                "However, none of this research dealt with adversarial domains and their implications for agent behavior.",
                "Our paper addresses this issue by providing a formal, axiomatized mental state model for a subset of adversarial domains, namely simple zero-sum adversarial environments.",
                "Simple zero-sum encounters exist of course in various twoplayer games (e.g., Chess, Checkers), but they also exist in n-player games (e.g., Risk, Diplomacy), auctions for a single good, and elsewhere.",
                "In these latter environments especially, using a utility-based adversarial search (such as the Min-Max algorithm) does not always provide an adequate solution; the payoff function might be quite complex or difficult to quantify, and there are natural computational limitations on bounded rational agents.",
                "In addition, traditional search methods (like Min-Max) do not make use of a model of the opponent, which has proven to be a valuable addition to adversarial planning [9, 3, 11].",
                "In this paper, we develop a formal, axiomatized model for bounded rational agents that are situated in a zero-sum adversarial environment.",
                "The model uses different modality operators, and its main foundations are the SharedPlans [4] model for collaborative behavior.",
                "We explore environment properties and the mental states of agents to derive behavioral axioms; these behavioral axioms constitute a formal model that serves as a specification and design guideline for agent design in such settings.",
                "We then investigate the behavior of our model empirically using the Connect-Four board game.",
                "We show that this game conforms to our environment definition, and analyze players behavior using a large set of completed match log 550 978-81-904262-7-5 (RPS) c 2007 IFAAMAS files.",
                "In addition, we use the results presented in [9] to discuss the importance of opponent modeling in our ConnectFour adversarial domain.",
                "The paper proceeds as follows.",
                "Section 2 presents the models formalization.",
                "Section 3 presents the empirical analysis and its results.",
                "We discuss related work in Section 4, and conclude and present future directions in Section 5. 2.",
                "ADVERSARIAL ENVIRONMENTS The adversarial environment model (denoted as AE) is intended to guide the design of agents by providing a specification of the capabilities and mental attitudes of an agent in an adversarial environment.",
                "We focus here on specific types of adversarial environments, specified as follows: 1.",
                "Zero-Sum Interactions: positive and negative utilities of all agents sum to zero; 2.",
                "Simple AEs: all agents in the environment are adversarial agents; 3.",
                "Bilateral AEs: AEs with exactly two agents; 4.",
                "Multilateral AEs: AEs of three or more agents.",
                "We will work on both bilateral and multilateral instantiations of zero-sum and simple environments.",
                "In particular, our adversarial environment model will deal with interactions that consist of N agents (N ≥ 2), where all agents are adversaries, and only one agent can succeed.",
                "Examples of such environments range from board games (e.g., Chess, Connect-Four, and Diplomacy) to certain economic environments (e.g., N-bidder auctions over a single good). 2.1 Model Overview Our approach is to formalize the mental attitudes and behaviors of a single adversarial agent; we consider how a single agent perceives the AE.",
                "The following list specifies the conditions and mental states of an agent in a simple, zero-sum AE: 1.",
                "The agent has an individual intention that its own goal will be completed; 2.",
                "The agent has an individual belief that it and its adversaries are pursuing full conflicting goals (defined below)there can be only one winner; 3.",
                "The agent has an individual belief that each adversary has an intention to complete its own full conflicting goal; 4.",
                "The agent has an individual belief in the (partial) profile of its adversaries.",
                "Item 3 is required, since it might be the case that some agent has a full conflicting goal, and is currently considering adopting the intention to complete it, but is, as of yet, not committed to achieving it.",
                "This might occur because the agent has not yet deliberated about the effects that adopting that intention might have on the other intentions it is currently holding.",
                "In such cases, it might not consider itself to even be in an adversarial environment.",
                "Item 4 states that the agent should hold some belief about the profiles of its adversaries.",
                "The profile represents all the knowledge the agent has about its adversary: its weaknesses, strategic capabilities, goals, intentions, trustworthiness, and more.",
                "It can be given explicitly or can be learned from observations of past encounters. 2.2 Model Definitions for Mental States We use Grosz and Krauss definitions of the modal operators, predicates, and meta-predicates, as defined in their SharedPlan formalization [4].",
                "We recall here some of the predicates and operators that are used in that formalization: Int.To(Ai, α, Tn, Tα, C) represents Ais intentions at time Tn to do an action α at time Tα in the context of C. Int.Th(Ai, prop, Tn, Tprop, C) represents Ais intentions at time Tn that a certain proposition prop holds at time Tprop in the context of C. The potential intention operators, Pot.Int.To(...) and Pot.Int.Th(...), are used to represent the mental state when an agent considers adopting an intention, but has not deliberated about the interaction of the other intentions it holds.",
                "The operator Bel(Ai, f, Tf ) represents agent Ai believing in the statement expressed in formula f, at time Tf .",
                "MB(A, f, Tf ) represents mutual belief for a group of agents A.",
                "A snapshot of the system finds our environment to be in some state e ∈ E of environmental variable states, and each adversary in any LAi ∈ L of possible local states.",
                "At any given time step, the system will be in some world w of the set of all possible worlds w ∈ W, where w = E×LA1 ×LA2 × ...LAn , and n is the number of adversaries.",
                "For example, in a Texas Holdem poker game, an agents local state might be its own set of cards (which is unknown to its adversary) while the environment will consist of the betting pot and the community cards (which are visible to both players).",
                "A utility function under this formalization is defined as a mapping from a possible world w ∈ W to an element in , which expresses the desirability of the world, from a single agent perspective.",
                "We usually normalize the range to [0,1], where 0 represents the least desirable possible world, and 1 is the most desirable world.",
                "The implementation of the utility function is dependent on the domain in question.",
                "The following list specifies new predicates, functions, variables, and constants used in conjunction with the original definitions for the adversarial environment formalization: 1. φ is a null action (the agent does not do anything). 2.",
                "GAi is the set of agent Ais goals.",
                "Each goal is a set of predicates whose satisfaction makes the goal complete (we use G∗ Ai ∈ GAi to represent an arbitrary goal of agent Ai). 3. gAi is the set of agent Ais subgoals.",
                "Subgoals are predicates whose satisfaction represents an important milestone toward achievement of the full goal. gG∗ Ai ⊆ gAi is the set of subgoals that are important to the completion of goal G∗ Ai (we will use g∗ G∗ Ai ∈ gG∗ Ai to represent an arbitrary subgoal). 4.",
                "P Aj Ai is the profile object agent Ai holds about agent Aj. 5.",
                "CA is a general set of actions for all agents in A which are derived from the environments constraints.",
                "CAi ⊆ CA is the set of agent Ais possible actions. 6.",
                "Do(Ai, α, Tα, w) holds when Ai performs action α over time interval Tα in world w. 7.",
                "Achieve(G∗ Ai , α, w) is true when goal G∗ Ai is achieved following the completion of action α in world w ∈ W, where α ∈ CAi . 8.",
                "Profile(Ai, PAi Ai ) is true when agent Ai holds an object profile for agent Aj.",
                "Definition 1.",
                "Full conflict (FulConf ) describes a zerosum interaction where only a single goal of the goals in conflict can be completed.",
                "FulConf(G∗ Ai , G∗ Aj ) ⇒ (∃α ∈ CAi , ∀w, β ∈ CAj ) (Achieve(G∗ Ai , α, w) ⇒ ¬Achieve(G∗ Aj , β, w)) ∨ (∃β ∈ CAj , ∀w, α ∈ CAi )(Achieve(G∗ Aj , β, w) ⇒ ¬Achieve(G∗ Ai , α, w)) Definition 2.",
                "Adversarial Knowledge (AdvKnow) is a function returning a value which represents the amount of The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) 551 knowledge agent Ai has on the profile of agent Aj, at time Tn.",
                "The higher the value, the more knowledge agent Ai has.",
                "AdvKnow : P Aj Ai × Tn → Definition 3.",
                "Eval - This evaluation function returns an estimated expected utility value for an agent in A, after completing an action from CA in some world state w. Eval : A × CA × w → Definition 4.",
                "TrH - (Threshold) is a numerical constant in the [0,1] range that represents an evaluation function (Eval) threshold value.",
                "An action that yields an estimated utility evaluation above the TrH is regarded as a highly beneficial action.",
                "The <br>eval value</br> is an estimation and not the real utility function, which is usually unknown.",
                "Using the real utility value for a rational agent would easily yield the best outcome for that agent.",
                "However, agents usually do not have the real utility functions, but rather a heuristic estimate of it.",
                "There are two important properties that should hold for the evaluation function: Property 1.",
                "The evaluation function should state that the most desirable world state is one in which the goal is achieved.",
                "Therefore, after the goal has been satisfied, there can be no future action that can put the agent in a world state with higher <br>eval value</br>. (∀Ai, G∗ Ai , α, β ∈ CAi , w ∈ W) Achieve(G∗ Ai , α, w) ⇒ Eval(Ai, α, w) ≥ Eval(Ai, β, w) Property 2.",
                "The evaluation function should project an action that causes a completion of a goal or a subgoal to a value which is greater than TrH (a highly beneficial action). (∀Ai, G∗ Ai ∈ GAi , α ∈ CAi , w ∈ W, g∗ GAi ∈ gGAi ) Achieve(G∗ Ai , α, w) ∨ Achieve(g∗ GAi , α, w) ⇒ Eval(Ai, α, w) ≥ TrH.",
                "Definition 5.",
                "SetAction We define a set action (SetAction) as a set of action operations (either complex or basic actions) from some action sets CAi and CAj which, according to agent Ais belief, are attached together by a temporal and consequential relationship, forming a chain of events (action, and its following consequent action). (∀α1 , . . . , αu ∈ CAi , β1 , . . . , βv ∈ CAj , w ∈ W) SetAction(α1 , . . . , αu , β1 , . . . , βv , w) ⇒ ((Do(Ai, α1 , Tα1 , w) ⇒ Do(Aj, β1 , Tβ1 , w)) ⇒ Do(Ai, α2 , Tα2 , w) ⇒ . . . ⇒ Do(Ai, αu , Tαu , w)) The consequential relation might exist due to various environmental constraints (when one action forces the adversary to respond with a specific action) or due to the agents knowledge about the profile of its adversary.",
                "Property 3.",
                "As the knowledge we have about our adversary increases we will have additional beliefs about its behavior in different situations which in turn creates new set actions.",
                "Formally, if our AdvKnow at time Tn+1 is greater than AdvKnow at time Tn, then every SetAction known at time Tn is also known at time Tn+1.",
                "AdvKnow(P Aj Ai , Tn+1) > AdvKnow(P Aj Ai , Tn) ⇒ (∀α1 , . . . , αu ∈ CAi , β1 , . . . , βv ∈ CAj ) Bel(Aag, SetAction(α1 , . . . , αu , β1 , . . . , βv ), Tn) ⇒ Bel(Aag, SetAction(α1 , . . . , αu , β1 , . . . , βv ), Tn+1) 2.3 The Environment Formulation The following axioms provide the formal definition for a simple, zero-sum Adversarial Environment (AE).",
                "Satisfaction of these axioms means that the agent is situated in such an environment.",
                "It provides specifications for agent Aag to interact with its set of adversaries A with respect to goals G∗ Aag and G∗ A at time TCo at some world state w. AE(Aag, A, G∗ Aag , A1, . . . , Ak, G∗ A1 , . . . , G∗ Ak , Tn, w) 1.",
                "Aag has an Int.Th his goal would be completed: (∃α ∈ CAag , Tα) Int.Th(Aag, Achieve(G∗ Aag , α), Tn, Tα, AE) 2.",
                "Aag believes that it and each of its adversaries Ao are pursuing full conflicting goals: (∀Ao ∈ {A1, . . . , Ak}) Bel(Aag, FulConf(G∗ Aag , G∗ Ao ), Tn) 3.",
                "Aag believes that each of his adversaries in Ao has the Int.Th his conflicting goal G∗ Aoi will be completed: (∀Ao ∈ {A1, . . . , Ak})(∃β ∈ CAo , Tβ) Bel(Aag, Int.Th(Ao, Achieve(G∗ Ao , β), TCo, Tβ, AE), Tn) 4.",
                "Aag has beliefs about the (partial) profiles of its adversaries (∀Ao ∈ {A1, . . . , Ak}) (∃PAo Aag ∈ PAag )Bel(Aag, Profile(Ao, PAo Aag ), Tn) To build an agent that will be able to operate successfully within such an AE, we must specify behavioral guidelines for its interactions.",
                "Using a naive Eval maximization strategy to a certain search depth will not always yield satisfactory results for several reasons: (1) the search horizon problem when searching for a fixed depth; (2) the strong assumption of an optimally rational, unbounded resources adversary; (3) using an estimated evaluation function which will not give optimal results in all world states, and can be exploited [9].",
                "The following axioms specify the behavioral principles that can be used to differentiate between successful and less successful agents in the above Adversarial Environment.",
                "Those axioms should be used as specification principles when designing and implementing agents that should be able to perform well in such Adversarial Environments.",
                "The behavioral axioms represent situations in which the agent will adopt potential intentions to (Pot.Int.To(...)) perform an action, which will typically require some means-end reasoning to select a possible course of action.",
                "This reasoning will lead to the adoption of an Int.To(...) (see [4]).",
                "A1.",
                "Goal Achieving Axiom.",
                "The first axiom is the simplest case; when the agent Aag believes that it is one action (α) away from achieving his conflicting goal G∗ Aag , it should adopt the potential intention to do α and complete its goal. (∀Aag, α ∈ CAag , Tn, Tα, w ∈ W) (Bel(Aag, Do(Aag, α, Tα, w) ⇒ Achieve(G∗ Aag , α, w)) ⇒ Pot.Int.To(Aag, α, Tn, Tα, w) This somewhat trivial behavior is the first and strongest axiom.",
                "In any situation, when the agent is an action away from completing the goal, it should complete the action.",
                "Any fair Eval function would naturally classify α as the maximal value action (property 1).",
                "However, without explicit axiomatization of such behavior there might be situations where the agent will decide on taking another action for various reasons, due to its bounded decision resources.",
                "A2.",
                "Preventive Act Axiom.",
                "Being in an adversarial situation, agent Aag might decide to take actions that will damage one of its adversarys plans to complete its goal, even if those actions do not explicitly advance Aag towards its conflicting goal G∗ Aag .",
                "Such preventive action will take place when agent Aag has a belief about the possibility of its adversary Ao doing an action β that will give it a high 552 The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) utility evaluation value (> TrH).",
                "Believing that taking action α will prevent the opponent from doing its β, it will adopt a potential intention to do α. (∀Aag, Ao ∈ A, α ∈ CAag , β ∈ CAo , Tn, Tβ, w ∈ W) (Bel(Aag, Do(Ao, β, Tβ, w) ∧ Eval(Ao, β, w) > TrH, Tn) ∧ Bel(Aag, Do(Aag, α, Tα, w) ⇒ ¬Do(Ao, β, Tβ, w), Tn) ⇒ Pot.Int.To(Aag, α, Tn, Tα, w) This axiom is a basic component of any adversarial environment.",
                "For example, looking at a Chess board game, a player could realize that it is about to be checkmated by its opponent, thus making a preventive move.",
                "Another example is a Connect Four game: when a player has a row of three chips, its opponent must block it, or lose.",
                "A specific instance of A1 occurs when the adversary is one action away from achieving its goal, and immediate preventive action needs to be taken by the agent.",
                "Formally, we have the same beliefs as stated above, with a changed belief that doing action β will cause agent Ao to achieve its goal.",
                "Proposition 1: Prevent or lose case. (∀Aag, Ao ∈ A, α ∈ CAag , β ∈ CAo , G∗ Ao , Tn, Tα, Tβ, w ∈ W) Bel(Aag, Do(Ao, β, Tβ, w) ⇒ Achieve(G∗ Ao , β, w), Tn) ∧ Bel(Aag, Do(Aag, α, Tα, w) ⇒ ¬Do(Ao, β, Tβ, w)) ⇒ Pot.Int.To(Aag, α, Tn, Tα, w) Sketch of proof: Proposition 1 can be easily derived from axiom A1 and the property 2 of the Eval function, which states that any action that causes a completion of a goal is a highly beneficial action.",
                "The preventive act behavior will occur implicitly when the Eval function is equal to the real world utility function.",
                "However, being bounded rational agents and dealing with an estimated evaluation function we need to explicitly axiomatize such behavior, for it will not always occur implicitly from the evaluation function.",
                "A3.",
                "Suboptimal Tactical Move Axiom.",
                "In many scenarios a situation may occur where an agent will decide not to take the current most beneficial action it can take (the action with the maximal utility evaluation value), because it believes that taking another action (with lower utility evaluation value) might yield (depending on the adversarys response) a future possibility for a highly beneficial action.",
                "This will occur most often when the Eval function is inaccurate and differs by a large extent from the Utility function.",
                "Put formally, agent Aag believes in a certain SetAction that will evolve according to its initial action and will yield a high beneficial value (> TrH) solely for it. (∀Aag, Ao ∈ A, Tn, w ∈ W) (∃α1 , . . . , αu ∈ CAi , β1 , . . . , βv ∈ CAj , Tα1 ) Bel(Aag, SetAction(α1 , . . . , αu , β1 , . . . , βv ), Tn) ∧ Bel(Aag, Eval(Ao, βv , w) < TrH < Eval(Aag, αu , w), Tn) ⇒ Pot.Int.To(Aag, α1 , Tn, Tα1 , w) An agent might believe that a chain of events will occur for various reasons due to the inevitable nature of the domain.",
                "For example, in Chess, we often observe the following: a move causes a check position, which in turn limits the opponents moves to avoiding the check, to which the first player might react with another check, and so on.",
                "The agent might also believe in a chain of events based on its knowledge of its adversarys profile, which allows it to foresee the adversarys movements with high accuracy.",
                "A4.",
                "Profile Detection Axiom.",
                "The agent can adjust its adversarys profiles by observations and pattern study (specifically, if there are repeated encounters with the same adversary).",
                "However, instead of waiting for profile information to be revealed, an agent can also initiate actions that will force its adversary to react in a way that will reveal profile knowledge about it.",
                "Formally, the axiom states that if all actions (γ) are not highly beneficial actions (< TrH), the agent can do action α in time Tα if it believes that it will result in a non-highly beneficial action β from its adversary, which in turn teaches it about the adversarys profile, i.e., gives a higher AdvKnow(P Aj Ai , Tβ). (∀Aag, Ao ∈ A, α ∈ CAag , β ∈ CAo , Tn, Tα, Tβ, w ∈ W) Bel(Aag, (∀γ ∈ CAag )Eval(Aag, γ, w) < TrH, Tn) ∧ Bel(Aag, Do(Aag, α, Tα, w) ⇒ Do(Ao, β, Tβ, w), Tn) ∧ Bel(Aag, Eval(Ao, β, w) < TrH) ∧ Bel(Aag, AdvKnow(P Aj Ai , Tβ) > AdvKnow(P Aj Ai , Tn), Tn) ⇒ Pot.Int.To(Aag, α, Tn, Tα, w) For example, going back to the Chess board game scenario, consider starting a game versus an opponent about whom we know nothing, not even if it is a human or a computerized opponent.",
                "We might start playing a strategy that will be suitable versus an average opponent, and adjust our game according to its level of play.",
                "A5.",
                "Alliance Formation Axiom The following behavioral axiom is relevant only in a multilateral instantiation of the adversarial environment (obviously, an alliance cannot be formed in a bilateral, zero-sum encounter).",
                "In different situations during a multilateral interaction, a group of agents might believe that it is in their best interests to form a temporary alliance.",
                "Such an alliance is an agreement that constrains its members behavior, but is believed by its members to enable them to achieve a higher utility value than the one achievable outside of the alliance.",
                "As an example, we can look at the classical Risk board game, where each player has an individual goal of being the sole conquerer of the world, a zero-sum game.",
                "However, in order to achieve this goal, it might be strategically wise to make short-term ceasefire agreements with other players, or to join forces and attack an opponent who is stronger than the rest.",
                "An alliances terms defines the way its members should act.",
                "It is a set of predicates, denoted as Terms, that is agreed upon by the alliance members, and should remain true for the duration of the alliance.",
                "For example, the set Terms in the Risk scenario, could contain the following predicates: 1.",
                "Alliance members will not attack each other on territories X, Y and Z; 2.",
                "Alliance members will contribute C units per turn for attacking adversary Ao; 3.",
                "Members are obligated to stay as part of the alliance until time Tk or until adversarys Ao army is smaller than Q.",
                "The set Terms specifies inter-group constraints on each of the alliance members (∀Aal i ∈ Aal ⊆ A) set of actions Cal i ⊆ C. Definition 6.",
                "Al val - the total evaluation value that agent Ai will achieve while being part of Aal is the sum of Evali (Eval for Ai) of each of Aal j Eval values after taking their own α actions (via the agent(α) predicate): Al val(Ai, Cal , Aal , w) = α∈Cal Evali(Aal j , agent(α), w) Definition 7.",
                "Al TrH - is a number representing an Al val The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) 553 threshold; above it, the alliance can be said to be a highly beneficial alliance.",
                "The value of Al TrH will be calculated dynamically according to the progress of the interaction, as can be seen in [7].",
                "After an alliance is formed, its members are now working in their normal adversarial environment, as well as according to the mental states and axioms required for their interactions as part of the alliance.",
                "The following Alliance model (AL) specifies the conditions under which the group Aal can be said to be in an alliance and working with a new and constrained set of actions Cal , at time Tn.",
                "AL(Aal , Cal , w, Tn) 1.",
                "Aal has a MB that all members are part of Aal : MB(Aal , (∀Aal i ∈ Aal )member(Aal i , Aal ), Tn) 2.",
                "Aal has a MB that the group be maintained: MB(Aal , (∀Aal i ∈ Aal )Int.Th (Ai, member(Ai, Aal ), Tn, Tn+1, Co), Tn) 3.",
                "Aal has a MB that being members gives them high utility value: MB(Aal , (∀Aal i ∈ Aal )Al val(Aal i , Cal , Aal , w) ≥ Al TrH, Tn) Members profiles are a crucial part of successful alliances.",
                "We assume that agents that have more accurate profiles of their adversaries will be more successful in such environments.",
                "Such agents will be able to predict when a member is about to breach the alliances contract (item 2 in the above model), and take counter measures (when item 3 will falsify).",
                "The robustness of the alliance is in part a function of its members trustfulness measure, objective position estimation, and other profile properties.",
                "We should note that an agent can simultaneously be part of more than one alliance.",
                "Such a temporary alliance, where the group members do not have a joint goal but act collaboratively for the interest of their own individual goals, is classified as a Treatment Group by modern psychologists [12] (in contrast to a Task Group, where its members have a joint goal).",
                "The Shared Activity model as presented in [5] modeled Treatment Group behavior using the same SharedPlans formalization.",
                "When comparing both definitions of an alliance and a Treatment Group we found an unsurprising resemblance between both models: the environment models definitions are almost identical (see SAs definitions in [5]), and their Selfish-Act and Cooperative Act axioms conform to our adversarial agents behavior.",
                "The main distinction between both models is the integration of a Helpful-behavior act axiom, in the Shared Activity which cannot be part of ours.",
                "This axiom states that an agent will consider taking action that will lower its <br>eval value</br> (to a certain lower bound), if it believes that a group partner will gain a significant benefit.",
                "Such behavior cannot occur in a pure adversarial environment (as a zero-sum game is), where the alliance members are constantly on watch to manipulate their alliance to their own advantage.",
                "A6.",
                "Evaluation Maximization Axiom.",
                "In a case when all other axioms are inapplicable, we will proceed with the action that maximizes the heuristic value as computed in the Eval function. (∀Aag, Ao ∈ A, α ∈ Cag, Tn, w ∈ W) Bel(Aag, (∀γ ∈ Cag)Eval(Aag, α, w) ≥ Eval(Aag, γ, w), Tn) ⇒ Pot.Int.To(Aag, α, Tn, Tα, w) T1.",
                "Optimality on Eval = Utility The above axiomatic model handles situations where the Utility is unknown and the agents are bounded rational agents.",
                "The following theorem shows that in bilateral interactions, where the agents have the real Utility function (i.e., Eval = Utility) and are rational agents, the axioms provide the same optimal result as classic adversarial search (e.g., Min-Max).",
                "Theorem 1.",
                "Let Ae ag be an unbounded rational AE agent using the Eval heuristic evaluation function, Au ag be the same agent using the true Utility function, and Ao be a sole unbounded utility-based rational adversary.",
                "Given that Eval = Utility: (∀α ∈ CAu ag , α ∈ CAe ag , Tn, w ∈ W) Pot.Int.To(Au ag, α, Tn, Tα, w) → Pot.Int.To(Ae ag, α , Tn, Tα, w) ∧ ((α = α ) ∨ (Utility(Au ag, α, w) = Eval(Ae ag, α , w))) Sketch of proof - Given that Au ag has the real utility function and unbounded resources, it can generate the full game tree and run the optimal MinMax algorithm to choose the highest utility value action, which we denote by, α.",
                "The proof will show that Ae ag, using the AE axioms, will select the same or equal utility α (when there is more than one action with the same max utility) when Eval = Utility. (A1) Goal achieving axiom - suppose there is an α such that its completion will achieve Au ags goal.",
                "It will obtain the highest utility by Min-Max for Au ag.",
                "The Ae ag agent will select α or another action with the same utility value via A1.",
                "If such α does not exist, Ae ag cannot apply this axiom, and proceeds to A2. (A2) Preventive act axiom - (1) Looking at the basic case (see Prop1), if there is a β which leads Ao to achieve its goal, then a preventive action α will yield the highest utility for Au ag.",
                "Au ag will choose it through the utility, while Ae ag will choose it through A2. (2) In the general case, β is a highly beneficial action for Ao, thus yields low utility for Au ag, which will guide it to select an α that will prevent β, while Ae ag will choose it through A2.1 If such β does not exist for Ao, then A2 is not applicable, and Ae ag can proceed to A3. (A3) Suboptimal tactical move axiom - When using a heuristic Eval function, Ae ag has a partial belief in the profile of its adversary (item 4 in AE model), which may lead it to believe in SetActions (Prop1).",
                "In our case, Ae ag is holding a full profile on its optimal adversary and knows that Ao will behave optimally according to the real utility values on the complete search tree, therefore, any belief about suboptimal SetAction cannot exist, yielding this axiom inapplicable.",
                "Ae ag will proceed to A4. (A4) Profile detection axiom - Given that Ae ag has the full profile of Ao, none of Ae ags actions can increase its knowledge.",
                "That axiom will not be applied, and the agent will proceed with A6 (A5 will be disregarded because the interaction is bilateral). (A6) Evaluation maximization axiom - This axiom will select the max Eval for Ae ag.",
                "Given that Eval = Utility, the same α that was selected by Au ag will be selected. 3.",
                "EVALUATION The main purpose of our experimental analysis is to evaluate the models behavior and performance in a real adversarial environment.",
                "This section investigates whether bounded 1 A case where following the completion of β there exists a γ which gives high utility for Agent Au ag, cannot occur because Ao uses the same utility, and γs existence will cause it to classify β as a low utility action. 554 The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) rational agents situated in such adversarial environments will be better off applying our suggested behavioral axioms. 3.1 The Domain To explore the use of the above model and its behavioral axioms, we decided to use the Connect-Four game as our adversarial environment.",
                "Connect-Four is a 2-player, zerosum game which is played using a 6x7 matrix-like board.",
                "Each turn, a player drops a disc into one of the 7 columns (the set of 21 discs is usually colored yellow for player 1 and red for player 2; we will use White and Black respectively to avoid confusion).",
                "The winner is the first player to complete a horizontal, vertical, or diagonal set of four discs with its color.",
                "On very rare occasions, the game might end in a tie if all the empty grids are filled, but no player managed to create a 4-disc set.",
                "The Connect-Four game was solved in [1], where it is shown that the first player (playing with the white discs) can force a win by starting in the middle column (column 4) and playing optimally However, the optimal strategy is very complex, and difficult to follow even for complex bounded rational agents, such as human players.",
                "Before we can proceed checking agent behavior, we must first verify that the domain conforms to the adversarial environments definition as given above (which the behavioral axioms are based on).",
                "First, when playing a Connect-Four game, the agent has an intention to win the game (item 1).",
                "Second (item 2), our agent believes that in Connect-Four there can only be one winner (or no winner at all in the rare occurrence of a tie).",
                "In addition, our agent believes that its opponent to the game will try to win (item 3), and we hope it has some partial knowledge (item 4) about its adversary (this knowledge can vary from nothing, through simple facts such as age, to strategies and weaknesses).",
                "Of course, not all Connect-Four encounters are adversarial.",
                "For example, when a parent is playing the game with its child, the following situation might occur: the child, having a strong incentive to win, treats the environment as adversarial (it intends to win, understands that there can only be one winner, and believes that its parent is trying to beat him).",
                "However, the parents point of view might see the environment as an educational one, where its goal is not to win the game, but to cause enjoyment or practice strategic reasoning.",
                "In such an educational environment, a new set of behavioral axioms might be more beneficial to the parents goals than our suggested adversarial behavioral axioms. 3.2 Axiom Analysis After showing that the Connect-Four game is indeed a zero-sum, bilateral adversarial environment, the next step is to look at players behaviors during the game and check whether behaving according to our model does improve performance.",
                "To do so we have collected log files from completed Connect-Four games that were played by human players over the Internet.",
                "Our collected log file data came from Play by eMail (PBeM) sites.",
                "These are web sites that host email games, where each move is taken by an email exchange between the server and the players.",
                "Many such sites archives contain real competitive interactions, and also maintain a ranking system for their members.",
                "Most of the data we used can be found in [6].",
                "As can be learned from [1], Connect-Four has an optimal strategy and a considerable advantage for the player who starts the game (which we call the White player).",
                "We will concentrate in our analysis on the second players moves (to be called Black).",
                "The White player, being the first to act, has the so-called initiative advantage.",
                "Having the advantage and a good strategy will keep the Black player busy reacting to its moves, instead of initiating threats.",
                "A threat is a combination of three discs of the same color, with an empty spot for the fourth winning disk.",
                "An open threat is a threat that can be realized in the opponents next move.",
                "In order for the Black player to win, it must somehow turn the tide, take the advantage and start presenting threats to the White player.",
                "We will explore Black players behavior and their conformance to our axioms.",
                "To do so, we built an application that reads log files and analyzes the Black players moves.",
                "The application contains two main components: (1) a Min-Max algorithm for evaluation of moves; (2) open threats detector for the discovering of open threats.",
                "The Min-Max algorithm will work to a given depth, d and for each move α will output the heuristic value for the next action taken by the player as written in the log file, h(α), alongside the maximum heuristic value, maxh(α), that could be achieved prior to taking the move (obviously, if h(α) = maxh(α), then the player did not do the optimal move heuristically).",
                "The threat detectors job is to notify if some action was taken in order to block an open threat (not blocking an open threat will probably cause the player to lose in the opponents next move).",
                "The heuristic function used by Min-Max to evaluate the players utility is the following function, which is simple to compute, yet provides a reasonable challenge to human opponents: Definition 8.",
                "Let Group be an adjacent set of four squares that are horizontal, vertical, or diagonal.",
                "Groupn b (Groupn w) be a Group with n pieces of the black (white) color and 4−n empty squares. h = ((Group1 b ∗α)+(Group2 b ∗β)+(Group3 b ∗γ)+(Group4 b ∗∞)) − ((Group1 w ∗α)+(Group2 w ∗β)+(Group3 w ∗γ)+(Group4 w ∗∞)) The values of α, β and δ can vary to form any desired linear combination; however, it is important to value them with the α < β < δ ordering in mind (we used 1, 4, and 8 as their respective values).",
                "Groups of 4 discs of the same color means victory, thus discovery of such a group will result in ∞ to ensure an extreme value.",
                "We now use our estimated evaluation function to evaluate the Black players actions during the Connect-Four adversarial interaction.",
                "Each game from the log file was input into the application, which processed and output a reformatted log file containing the h value of the current move, the maxh value that could be achieved, and a notification if an open threat was detected.",
                "A total of 123 games were analyzed (57 with White winning, and 66 with Black winning).",
                "A few additional games were manually ignored in the experiment, due to these problems: a player abandoning the game while the outcome is not final, or a blunt irrational move in the early stages of the game (e.g., not blocking an obvious winning group in the first opening moves).",
                "In addition, a single tie game was also removed.",
                "The simulator was run to a search depth of 3 moves.",
                "We now proceed to analyze the games with respect to each behavioral axiom.",
                "The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) 555 Table 1: Average heuristic difference analysis Black losses Black Won Avg minh -17.62 -12.02 Avg 3 lowest h moves (min3 h) -13.20 -8.70 3.2.1 Affirming the Suboptimal tactical move axiom The following section presents the heuristic evaluations of the Min-Max algorithm for each action, and checks the amount and extent of suboptimal tactical actions and their implications on performance.",
                "Table 1 shows results and insights from the games heuristic analysis, when search depth equals 3 (this search depth was selected for the results to be comparable to [9], see Section 3.2.3).",
                "The tables heuristic data is the difference between the present maximal heuristic value and the heuristic value of the action that was eventually taken by the player (i.e., the closer the number is to 0, the closer the action was to the maximum heuristic action).",
                "The first row presents the difference values of the action that had the maximal difference value among all the Black players actions in a given game, as averaged over all Blacks winning and losing games (see respective columns).",
                "In games in which the Black player loses, its average difference value was -17.62, while in games in which the Black player won, its average was -12.02.",
                "The second row expands the analysis by considering the 3 highest heuristic difference actions, and averaging them.",
                "In that case, we notice an average heuristic difference of 5 points between games which the Black player loses and games in which it wins.",
                "Nevertheless, the importance of those numbers is that they allowed us to take an educated guess on a threshold number of 11.5, as the value of the TrH constant, which differentiates between normal actions and highly beneficial ones.",
                "After finding an approximated TrH constant, we can proceed with an analysis of the importance of suboptimal moves.",
                "To do so we took the subset of games in which the minimum heuristic difference value for Blacks actions was 11.5.",
                "As presented in Table 2, we can see the different min3 h average of the 3 largest ranges and the respective percentage of games won.",
                "The first row shows that the Black player won only 12% of the games in which the average of its 3 highest heuristically difference actions (min3 h) was smaller than the suggested threshold, TrH = 11.5.",
                "The second row shows a surprising result: it seems that when min3 h > −4 the Black player rarely wins.",
                "Intuition would suggest that games in which the action evaluation values were closer to the maximal values will result in more winning games for Black.",
                "However, it seems that in the Connect-Four domain, merely responding with somewhat easily expected actions, without initiating a few surprising and suboptimal moves, does not yield good results.",
                "The last row sums up the main insights from the analysis; most of Blacks wins (83%) came when its min3 h was in the range of -11.5 to -4.",
                "A close inspection of those Black winning games shows the following pattern behind the numbers: after standard opening moves, Black suddenly drops a disc into an isolated column, which seems a waste of a move.",
                "White continues to build its threats, while usually disregarding Blacks last move, which in turn uses the isolated disc as an anchor for a future winning threat.",
                "The results show that it was beneficial for the Black player Table 2: Blacks winnings percentages % of games min3 h < −11.5 12% min3 h > −4 5% −11.5 ≤ min3 h ≤ −4 83% to take suboptimal actions and not give the current highest possible heuristic value, but will not be too harmful for its position (i.e., will not give high beneficial value to its adversary).",
                "As it turned out, learning the threshold is an important aspect of success: taking wildly risky moves (min3 h < −11.5) or trying to avoid them (min3 h > −4) reduces the Black players winning chances by a large margin. 3.2.2 Affirming the Profile Monitoring Axiom In the task of showing the importance of monitoring ones adversaries profiles, our log files could not be used because they did not contain repeated interactions between players, which are needed to infer the players knowledge about their adversaries.",
                "However, the importance of opponent modeling and its use in attaining tactical advantages was already studied in various domains ([3, 9] are good examples).",
                "In a recent paper, Markovitch and Reger [9] explored the notion of learning and exploitation of opponent weakness in competitive interactions.",
                "They apply simple learning strategies by analyzing examples from past interactions in a specific domain.",
                "They also used the Connect-Four adversarial domain, which can now be used to understand the importance of monitoring the adversarys profile.",
                "Following the presentation of their theoretical model, they describe an extensive empirical study and check the agents performance after learning the weakness model with past examples.",
                "One of the domains used as a competitive environment was the same Connect-Four game (Checkers was the second domain).",
                "Their heuristic function was identical to ours with three different variations (H1, H2, and H3) that are distinguished from one another in their linear combination coefficient values.",
                "The search depth for the players was 3 (as in our analysis).",
                "Their extensive experiments check and compare various learning strategies, risk factors, predefined feature sets and usage methods.",
                "The bottom line is that the Connect-Four domain shows an improvement from a 0.556 winning rate before modeling to a 0.69 after modeling (page 22).",
                "Their conclusions, showing improved performance when holding and using the adversarys model, justify the effort to monitor the adversary profile for continuous and repeated interactions.",
                "An additional point that came up in their experiments is the following: after the opponent weakness model has been learned, the authors describe different methods of integrating the opponent weakness model into the agents decision strategy.",
                "Nevertheless, regardless of the specific method they chose to work with, all integration methods might cause the agent to take suboptimal decisions; it might cause the agent to prefer actions that are suboptimal at the present decision junction, but which might cause the opponent to react in accordance with its weakness model (as represented by our agent) which in turn will be beneficial for us in the future.",
                "The agents behavior, as demonstrated in [9] further confirms and strengthens our Suboptimal Tactical Axiom as discussed in the previous section. 556 The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) 3.2.3 Additional Insights The need for the Goal Achieving, Preventive Act, and Evaluation Maximization axioms are obvious, and need no further verification.",
                "However, even with respect to those axioms, a few interesting insights came up in the log analysis.",
                "The Goal achieving and Preventive Act axioms, though theoretically trivial, seem to provide some challenge to a human player.",
                "In the initial inspection of the logs, we encountered few games2 where a player, for inexplicable reasons, did not block the other from winning or failed to execute its own winning move.",
                "We can blame those faults on the humans lack of attention, or a typing error in its move reply; nevertheless, those errors might occur in bounded rational agents, and the appropriate behavior needs to be axiomatized.",
                "A typical Connect-Four game revolves around generating threats and blocking them.",
                "In our analysis we looked for explicit preventive actions, i.e., moves that block a group of 3 discs, or that remove a future threat (in our limited search horizon).",
                "We found that in 83% of the total games there was at least one preventive action taken by the Black player.",
                "It was also found that Black averaged 2.8 preventive actions per game on the games in which it lost, while averaging 1.5 preventive actions per game when winning.",
                "It seems that Black requires 1 or 2 preventive actions to build its initial taking position, before starting to present threats.",
                "If it did not manage to win, it will usually prevent an extra threat or two before succumbing to White. 4.",
                "RELATED WORK Much research deals with the axiomatization of teamwork and mental states of individuals: some models use knowledge and belief [10], others have models of goals and intentions [8, 4].",
                "However, all these formal theories deal with agent teamwork and cooperation.",
                "As far as we know, our model is the first to provide a formalized model for explicit adversarial environments and agents behavior in it.",
                "The classical Min-Max adversarial search algorithm was the first attempt to integrate the opponent into the search space with a weak assumption of an optimally playing opponent.",
                "Since then, much effort has gone into integrating the opponent model into the decision procedure to predict future behavior.",
                "The M∗ algorithm presented by Carmel and Markovitch [2] showed a method of incorporating opponent models into adversary search, while in [3] they used learning to provide a more accurate opponent model in a 2player repeated game environment, where agents strategies were modeled as finite automata.",
                "Additional Adversarial planning work was done by Willmott et al. [13], which provided an adversarial planning approach to the game of GO.",
                "The research mentioned above dealt with adversarial search and the integration of opponent models into classical utilitybased search methods.",
                "That work shows the importance of opponent modeling and the ability to exploit it to an agents advantage.",
                "However, the basic limitations of those search methods still apply; our model tries to overcome those limitations by presenting a formal model for a new, mental state-based adversarial specification. 5.",
                "CONCLUSIONS We presented an Adversarial Environment model for a 2 These were later removed from the final analysis. bounded rational agent that is situated in an N-player, zerosum environment.",
                "We used the SharedPlans formalization to define the model and the axioms that agents can apply as behavioral guidelines.",
                "The model is meant to be used as a guideline for designing agents that need to operate in such adversarial environments.",
                "We presented empirical results, based on ConnectFour log file analysis, that exemplify the model and the axioms for a bilateral instance of the environment.",
                "The results we presented are a first step towards an expanded model that will cover all types of adversarial environments, for example, environments that are non-zero-sum, and environments that contain natural agents that are not part of the direct conflict.",
                "Those challenges and more will be dealt with in future research. 6.",
                "ACKNOWLEDGMENT This research was supported in part by Israel Science Foundation grants #1211/04 and #898/05. 7.",
                "REFERENCES [1] L. V. Allis.",
                "A knowledge-based approach of Connect-Four - the game is solved: White wins.",
                "Masters thesis, Free University, Amsterdam, The Netherlands, 1988. [2] D. Carmel and S. Markovitch.",
                "Incorporating opponent models into adversary search.",
                "In Proceedings of the Thirteenth National Conference on Artificial Intelligence, pages 120-125, Portland, OR, 1996. [3] D. Carmel and S. Markovitch.",
                "Opponent modeling in multi-agent systems.",
                "In G. Weiß and S. Sen, editors, Adaptation and Learning in Multi-Agent Systems, pages 40-52.",
                "Springer-Verlag, 1996. [4] B. J. Grosz and S. Kraus.",
                "Collaborative plans for complex group action.",
                "Artificial Intelligence, 86(2):269-357, 1996. [5] M. Hadad, G. Kaminka, G. Armon, and S. Kraus.",
                "Supporting collaborative activity.",
                "In Proc. of AAAI-2005, pages 83-88, Pittsburgh, 2005. [6] http://www.gamerz.net/˜pbmserv/. [7] S. Kraus and D. Lehmann.",
                "Designing and building a negotiating automated agent.",
                "Computational Intelligence, 11:132-171, 1995. [8] H. J. Levesque, P. R. Cohen, and J. H. T. Nunes.",
                "On acting together.",
                "In Proc. of AAAI-90, pages 94-99, Boston, MA, 1990. [9] S. Markovitch and R. Reger.",
                "Learning and exploiting relative weaknesses of opponent agents.",
                "Autonomous Agents and Multi-Agent Systems, 10(2):103-130, 2005. [10] Y. M. Ronald Fagin, Joseph Y. Halpern and M. Y. Vardi.",
                "Reasoning about knowledge.",
                "MIT Press, Cambridge, Mass., 1995. [11] P. Thagard.",
                "Adversarial problem solving: Modeling an oponent using explanatory coherence.",
                "Cognitive Science, 16(1):123-149, 1992. [12] R. W. Toseland and R. F. Rivas.",
                "An Introduction to Group Work Practice.",
                "Prentice Hall, Englewood Cliffs, NJ, 2nd edition edition, 1995. [13] S. Willmott, J. Richardson, A. Bundy, and J. Levine.",
                "An adversarial planning approach to Go.",
                "Lecture Notes in Computer Science, 1558:93-112, 1999.",
                "The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) 557"
            ],
            "original_annotated_samples": [],
            "translated_annotated_samples": [
                "El \"valor de evaluación\" es una estimación y no la función de utilidad real, que generalmente se desconoce.valor de evaluación",
                "Por lo tanto, después de que el objetivo se haya cumplido, no puede haber acciones futuras que puedan colocar al agente en un estado mundial con un \"valor de evaluación\" más alto.(∀ai, g ∗ ai, α, β ∈ Cai, w ∈ W) lograr (g ∗ ai, α, w) ⇒ eval (ai, α, w) ≥ eval (ai, β, w) propiedad 2. evalvalor",
                "Este axioma establece que un agente considerará tomar medidas que reducirán su \"valor de evaluación\" (para cierto límite inferior), si cree que un socio grupal obtendrá un beneficio significativo.valor de evaluación"
            ],
            "translated_text": "",
            "candidates": [],
            "error": []
        },
        "interaction": {
            "translated_key": "",
            "is_in_text": true,
            "original_annotated_sentences": [
                "An Adversarial Environment Model for Bounded Rational Agents in Zero-Sum Interactions Inon Zuckerman1 , Sarit Kraus1 , Jeffrey S. Rosenschein2 , Gal Kaminka1 1 Department of Computer Science 2 The School of Engineering Bar-Ilan University and Computer Science Ramat-Gan, Israel Hebrew University, Jerusalem, Israel {zukermi,sarit,galk}@cs.biu.ac.il jeff@cs.huji.ac.il ABSTRACT Multiagent environments are often not cooperative nor collaborative; in many cases, agents have conflicting interests, leading to adversarial interactions.",
                "This paper presents a formal Adversarial Environment model for bounded rational agents operating in a zero-sum environment.",
                "In such environments, attempts to use classical utility-based search methods can raise a variety of difficulties (e.g., implicitly modeling the opponent as an omniscient utility maximizer, rather than leveraging a more nuanced, explicit opponent model).",
                "We define an Adversarial Environment by describing the mental states of an agent in such an environment.",
                "We then present behavioral axioms that are intended to serve as design principles for building such adversarial agents.",
                "We explore the application of our approach by analyzing log files of completed Connect-Four games, and present an empirical analysis of the axioms appropriateness.",
                "Categories and Subject Descriptors I.2.11 [Artificial Intelligence]: Distributed Artificial Intelligence-Intelligent agents,Multiagent Systems; I.2.4 [Artificial Intelligence]: Knowledge Representation Formalisms and Methods -Modal logic General Terms Design, Theory 1.",
                "INTRODUCTION Early research in multiagent systems (MAS) considered cooperative groups of agents; because individual agents had limited resources, or limited access to information (e.g., limited processing power, limited sensor coverage), they worked together by design to solve problems that individually they could not solve, or at least could not solve as efficiently.",
                "MAS research, however, soon began to consider interacting agents with individuated interests, as representatives of different humans or organizations with non-identical interests.",
                "When interactions are guided by diverse interests, participants may have to overcome disagreements, uncooperative interactions, and even intentional attempts to damage one another.",
                "When these types of interactions occur, environments require appropriate behavior from the agents situated in them.",
                "We call these environments Adversarial Environments, and call the clashing agents Adversaries.",
                "Models of cooperation and teamwork have been extensively explored in MAS through the axiomatization of mental states (e.g., [8, 4, 5]).",
                "However, none of this research dealt with adversarial domains and their implications for agent behavior.",
                "Our paper addresses this issue by providing a formal, axiomatized mental state model for a subset of adversarial domains, namely simple zero-sum adversarial environments.",
                "Simple zero-sum encounters exist of course in various twoplayer games (e.g., Chess, Checkers), but they also exist in n-player games (e.g., Risk, Diplomacy), auctions for a single good, and elsewhere.",
                "In these latter environments especially, using a utility-based adversarial search (such as the Min-Max algorithm) does not always provide an adequate solution; the payoff function might be quite complex or difficult to quantify, and there are natural computational limitations on bounded rational agents.",
                "In addition, traditional search methods (like Min-Max) do not make use of a model of the opponent, which has proven to be a valuable addition to adversarial planning [9, 3, 11].",
                "In this paper, we develop a formal, axiomatized model for bounded rational agents that are situated in a zero-sum adversarial environment.",
                "The model uses different modality operators, and its main foundations are the SharedPlans [4] model for collaborative behavior.",
                "We explore environment properties and the mental states of agents to derive behavioral axioms; these behavioral axioms constitute a formal model that serves as a specification and design guideline for agent design in such settings.",
                "We then investigate the behavior of our model empirically using the Connect-Four board game.",
                "We show that this game conforms to our environment definition, and analyze players behavior using a large set of completed match log 550 978-81-904262-7-5 (RPS) c 2007 IFAAMAS files.",
                "In addition, we use the results presented in [9] to discuss the importance of opponent modeling in our ConnectFour adversarial domain.",
                "The paper proceeds as follows.",
                "Section 2 presents the models formalization.",
                "Section 3 presents the empirical analysis and its results.",
                "We discuss related work in Section 4, and conclude and present future directions in Section 5. 2.",
                "ADVERSARIAL ENVIRONMENTS The adversarial environment model (denoted as AE) is intended to guide the design of agents by providing a specification of the capabilities and mental attitudes of an agent in an adversarial environment.",
                "We focus here on specific types of adversarial environments, specified as follows: 1.",
                "Zero-Sum Interactions: positive and negative utilities of all agents sum to zero; 2.",
                "Simple AEs: all agents in the environment are adversarial agents; 3.",
                "Bilateral AEs: AEs with exactly two agents; 4.",
                "Multilateral AEs: AEs of three or more agents.",
                "We will work on both bilateral and multilateral instantiations of zero-sum and simple environments.",
                "In particular, our adversarial environment model will deal with interactions that consist of N agents (N ≥ 2), where all agents are adversaries, and only one agent can succeed.",
                "Examples of such environments range from board games (e.g., Chess, Connect-Four, and Diplomacy) to certain economic environments (e.g., N-bidder auctions over a single good). 2.1 Model Overview Our approach is to formalize the mental attitudes and behaviors of a single adversarial agent; we consider how a single agent perceives the AE.",
                "The following list specifies the conditions and mental states of an agent in a simple, zero-sum AE: 1.",
                "The agent has an individual intention that its own goal will be completed; 2.",
                "The agent has an individual belief that it and its adversaries are pursuing full conflicting goals (defined below)there can be only one winner; 3.",
                "The agent has an individual belief that each adversary has an intention to complete its own full conflicting goal; 4.",
                "The agent has an individual belief in the (partial) profile of its adversaries.",
                "Item 3 is required, since it might be the case that some agent has a full conflicting goal, and is currently considering adopting the intention to complete it, but is, as of yet, not committed to achieving it.",
                "This might occur because the agent has not yet deliberated about the effects that adopting that intention might have on the other intentions it is currently holding.",
                "In such cases, it might not consider itself to even be in an adversarial environment.",
                "Item 4 states that the agent should hold some belief about the profiles of its adversaries.",
                "The profile represents all the knowledge the agent has about its adversary: its weaknesses, strategic capabilities, goals, intentions, trustworthiness, and more.",
                "It can be given explicitly or can be learned from observations of past encounters. 2.2 Model Definitions for Mental States We use Grosz and Krauss definitions of the modal operators, predicates, and meta-predicates, as defined in their SharedPlan formalization [4].",
                "We recall here some of the predicates and operators that are used in that formalization: Int.To(Ai, α, Tn, Tα, C) represents Ais intentions at time Tn to do an action α at time Tα in the context of C. Int.Th(Ai, prop, Tn, Tprop, C) represents Ais intentions at time Tn that a certain proposition prop holds at time Tprop in the context of C. The potential intention operators, Pot.Int.To(...) and Pot.Int.Th(...), are used to represent the mental state when an agent considers adopting an intention, but has not deliberated about the <br>interaction</br> of the other intentions it holds.",
                "The operator Bel(Ai, f, Tf ) represents agent Ai believing in the statement expressed in formula f, at time Tf .",
                "MB(A, f, Tf ) represents mutual belief for a group of agents A.",
                "A snapshot of the system finds our environment to be in some state e ∈ E of environmental variable states, and each adversary in any LAi ∈ L of possible local states.",
                "At any given time step, the system will be in some world w of the set of all possible worlds w ∈ W, where w = E×LA1 ×LA2 × ...LAn , and n is the number of adversaries.",
                "For example, in a Texas Holdem poker game, an agents local state might be its own set of cards (which is unknown to its adversary) while the environment will consist of the betting pot and the community cards (which are visible to both players).",
                "A utility function under this formalization is defined as a mapping from a possible world w ∈ W to an element in , which expresses the desirability of the world, from a single agent perspective.",
                "We usually normalize the range to [0,1], where 0 represents the least desirable possible world, and 1 is the most desirable world.",
                "The implementation of the utility function is dependent on the domain in question.",
                "The following list specifies new predicates, functions, variables, and constants used in conjunction with the original definitions for the adversarial environment formalization: 1. φ is a null action (the agent does not do anything). 2.",
                "GAi is the set of agent Ais goals.",
                "Each goal is a set of predicates whose satisfaction makes the goal complete (we use G∗ Ai ∈ GAi to represent an arbitrary goal of agent Ai). 3. gAi is the set of agent Ais subgoals.",
                "Subgoals are predicates whose satisfaction represents an important milestone toward achievement of the full goal. gG∗ Ai ⊆ gAi is the set of subgoals that are important to the completion of goal G∗ Ai (we will use g∗ G∗ Ai ∈ gG∗ Ai to represent an arbitrary subgoal). 4.",
                "P Aj Ai is the profile object agent Ai holds about agent Aj. 5.",
                "CA is a general set of actions for all agents in A which are derived from the environments constraints.",
                "CAi ⊆ CA is the set of agent Ais possible actions. 6.",
                "Do(Ai, α, Tα, w) holds when Ai performs action α over time interval Tα in world w. 7.",
                "Achieve(G∗ Ai , α, w) is true when goal G∗ Ai is achieved following the completion of action α in world w ∈ W, where α ∈ CAi . 8.",
                "Profile(Ai, PAi Ai ) is true when agent Ai holds an object profile for agent Aj.",
                "Definition 1.",
                "Full conflict (FulConf ) describes a zerosum <br>interaction</br> where only a single goal of the goals in conflict can be completed.",
                "FulConf(G∗ Ai , G∗ Aj ) ⇒ (∃α ∈ CAi , ∀w, β ∈ CAj ) (Achieve(G∗ Ai , α, w) ⇒ ¬Achieve(G∗ Aj , β, w)) ∨ (∃β ∈ CAj , ∀w, α ∈ CAi )(Achieve(G∗ Aj , β, w) ⇒ ¬Achieve(G∗ Ai , α, w)) Definition 2.",
                "Adversarial Knowledge (AdvKnow) is a function returning a value which represents the amount of The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) 551 knowledge agent Ai has on the profile of agent Aj, at time Tn.",
                "The higher the value, the more knowledge agent Ai has.",
                "AdvKnow : P Aj Ai × Tn → Definition 3.",
                "Eval - This evaluation function returns an estimated expected utility value for an agent in A, after completing an action from CA in some world state w. Eval : A × CA × w → Definition 4.",
                "TrH - (Threshold) is a numerical constant in the [0,1] range that represents an evaluation function (Eval) threshold value.",
                "An action that yields an estimated utility evaluation above the TrH is regarded as a highly beneficial action.",
                "The Eval value is an estimation and not the real utility function, which is usually unknown.",
                "Using the real utility value for a rational agent would easily yield the best outcome for that agent.",
                "However, agents usually do not have the real utility functions, but rather a heuristic estimate of it.",
                "There are two important properties that should hold for the evaluation function: Property 1.",
                "The evaluation function should state that the most desirable world state is one in which the goal is achieved.",
                "Therefore, after the goal has been satisfied, there can be no future action that can put the agent in a world state with higher Eval value. (∀Ai, G∗ Ai , α, β ∈ CAi , w ∈ W) Achieve(G∗ Ai , α, w) ⇒ Eval(Ai, α, w) ≥ Eval(Ai, β, w) Property 2.",
                "The evaluation function should project an action that causes a completion of a goal or a subgoal to a value which is greater than TrH (a highly beneficial action). (∀Ai, G∗ Ai ∈ GAi , α ∈ CAi , w ∈ W, g∗ GAi ∈ gGAi ) Achieve(G∗ Ai , α, w) ∨ Achieve(g∗ GAi , α, w) ⇒ Eval(Ai, α, w) ≥ TrH.",
                "Definition 5.",
                "SetAction We define a set action (SetAction) as a set of action operations (either complex or basic actions) from some action sets CAi and CAj which, according to agent Ais belief, are attached together by a temporal and consequential relationship, forming a chain of events (action, and its following consequent action). (∀α1 , . . . , αu ∈ CAi , β1 , . . . , βv ∈ CAj , w ∈ W) SetAction(α1 , . . . , αu , β1 , . . . , βv , w) ⇒ ((Do(Ai, α1 , Tα1 , w) ⇒ Do(Aj, β1 , Tβ1 , w)) ⇒ Do(Ai, α2 , Tα2 , w) ⇒ . . . ⇒ Do(Ai, αu , Tαu , w)) The consequential relation might exist due to various environmental constraints (when one action forces the adversary to respond with a specific action) or due to the agents knowledge about the profile of its adversary.",
                "Property 3.",
                "As the knowledge we have about our adversary increases we will have additional beliefs about its behavior in different situations which in turn creates new set actions.",
                "Formally, if our AdvKnow at time Tn+1 is greater than AdvKnow at time Tn, then every SetAction known at time Tn is also known at time Tn+1.",
                "AdvKnow(P Aj Ai , Tn+1) > AdvKnow(P Aj Ai , Tn) ⇒ (∀α1 , . . . , αu ∈ CAi , β1 , . . . , βv ∈ CAj ) Bel(Aag, SetAction(α1 , . . . , αu , β1 , . . . , βv ), Tn) ⇒ Bel(Aag, SetAction(α1 , . . . , αu , β1 , . . . , βv ), Tn+1) 2.3 The Environment Formulation The following axioms provide the formal definition for a simple, zero-sum Adversarial Environment (AE).",
                "Satisfaction of these axioms means that the agent is situated in such an environment.",
                "It provides specifications for agent Aag to interact with its set of adversaries A with respect to goals G∗ Aag and G∗ A at time TCo at some world state w. AE(Aag, A, G∗ Aag , A1, . . . , Ak, G∗ A1 , . . . , G∗ Ak , Tn, w) 1.",
                "Aag has an Int.Th his goal would be completed: (∃α ∈ CAag , Tα) Int.Th(Aag, Achieve(G∗ Aag , α), Tn, Tα, AE) 2.",
                "Aag believes that it and each of its adversaries Ao are pursuing full conflicting goals: (∀Ao ∈ {A1, . . . , Ak}) Bel(Aag, FulConf(G∗ Aag , G∗ Ao ), Tn) 3.",
                "Aag believes that each of his adversaries in Ao has the Int.Th his conflicting goal G∗ Aoi will be completed: (∀Ao ∈ {A1, . . . , Ak})(∃β ∈ CAo , Tβ) Bel(Aag, Int.Th(Ao, Achieve(G∗ Ao , β), TCo, Tβ, AE), Tn) 4.",
                "Aag has beliefs about the (partial) profiles of its adversaries (∀Ao ∈ {A1, . . . , Ak}) (∃PAo Aag ∈ PAag )Bel(Aag, Profile(Ao, PAo Aag ), Tn) To build an agent that will be able to operate successfully within such an AE, we must specify behavioral guidelines for its interactions.",
                "Using a naive Eval maximization strategy to a certain search depth will not always yield satisfactory results for several reasons: (1) the search horizon problem when searching for a fixed depth; (2) the strong assumption of an optimally rational, unbounded resources adversary; (3) using an estimated evaluation function which will not give optimal results in all world states, and can be exploited [9].",
                "The following axioms specify the behavioral principles that can be used to differentiate between successful and less successful agents in the above Adversarial Environment.",
                "Those axioms should be used as specification principles when designing and implementing agents that should be able to perform well in such Adversarial Environments.",
                "The behavioral axioms represent situations in which the agent will adopt potential intentions to (Pot.Int.To(...)) perform an action, which will typically require some means-end reasoning to select a possible course of action.",
                "This reasoning will lead to the adoption of an Int.To(...) (see [4]).",
                "A1.",
                "Goal Achieving Axiom.",
                "The first axiom is the simplest case; when the agent Aag believes that it is one action (α) away from achieving his conflicting goal G∗ Aag , it should adopt the potential intention to do α and complete its goal. (∀Aag, α ∈ CAag , Tn, Tα, w ∈ W) (Bel(Aag, Do(Aag, α, Tα, w) ⇒ Achieve(G∗ Aag , α, w)) ⇒ Pot.Int.To(Aag, α, Tn, Tα, w) This somewhat trivial behavior is the first and strongest axiom.",
                "In any situation, when the agent is an action away from completing the goal, it should complete the action.",
                "Any fair Eval function would naturally classify α as the maximal value action (property 1).",
                "However, without explicit axiomatization of such behavior there might be situations where the agent will decide on taking another action for various reasons, due to its bounded decision resources.",
                "A2.",
                "Preventive Act Axiom.",
                "Being in an adversarial situation, agent Aag might decide to take actions that will damage one of its adversarys plans to complete its goal, even if those actions do not explicitly advance Aag towards its conflicting goal G∗ Aag .",
                "Such preventive action will take place when agent Aag has a belief about the possibility of its adversary Ao doing an action β that will give it a high 552 The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) utility evaluation value (> TrH).",
                "Believing that taking action α will prevent the opponent from doing its β, it will adopt a potential intention to do α. (∀Aag, Ao ∈ A, α ∈ CAag , β ∈ CAo , Tn, Tβ, w ∈ W) (Bel(Aag, Do(Ao, β, Tβ, w) ∧ Eval(Ao, β, w) > TrH, Tn) ∧ Bel(Aag, Do(Aag, α, Tα, w) ⇒ ¬Do(Ao, β, Tβ, w), Tn) ⇒ Pot.Int.To(Aag, α, Tn, Tα, w) This axiom is a basic component of any adversarial environment.",
                "For example, looking at a Chess board game, a player could realize that it is about to be checkmated by its opponent, thus making a preventive move.",
                "Another example is a Connect Four game: when a player has a row of three chips, its opponent must block it, or lose.",
                "A specific instance of A1 occurs when the adversary is one action away from achieving its goal, and immediate preventive action needs to be taken by the agent.",
                "Formally, we have the same beliefs as stated above, with a changed belief that doing action β will cause agent Ao to achieve its goal.",
                "Proposition 1: Prevent or lose case. (∀Aag, Ao ∈ A, α ∈ CAag , β ∈ CAo , G∗ Ao , Tn, Tα, Tβ, w ∈ W) Bel(Aag, Do(Ao, β, Tβ, w) ⇒ Achieve(G∗ Ao , β, w), Tn) ∧ Bel(Aag, Do(Aag, α, Tα, w) ⇒ ¬Do(Ao, β, Tβ, w)) ⇒ Pot.Int.To(Aag, α, Tn, Tα, w) Sketch of proof: Proposition 1 can be easily derived from axiom A1 and the property 2 of the Eval function, which states that any action that causes a completion of a goal is a highly beneficial action.",
                "The preventive act behavior will occur implicitly when the Eval function is equal to the real world utility function.",
                "However, being bounded rational agents and dealing with an estimated evaluation function we need to explicitly axiomatize such behavior, for it will not always occur implicitly from the evaluation function.",
                "A3.",
                "Suboptimal Tactical Move Axiom.",
                "In many scenarios a situation may occur where an agent will decide not to take the current most beneficial action it can take (the action with the maximal utility evaluation value), because it believes that taking another action (with lower utility evaluation value) might yield (depending on the adversarys response) a future possibility for a highly beneficial action.",
                "This will occur most often when the Eval function is inaccurate and differs by a large extent from the Utility function.",
                "Put formally, agent Aag believes in a certain SetAction that will evolve according to its initial action and will yield a high beneficial value (> TrH) solely for it. (∀Aag, Ao ∈ A, Tn, w ∈ W) (∃α1 , . . . , αu ∈ CAi , β1 , . . . , βv ∈ CAj , Tα1 ) Bel(Aag, SetAction(α1 , . . . , αu , β1 , . . . , βv ), Tn) ∧ Bel(Aag, Eval(Ao, βv , w) < TrH < Eval(Aag, αu , w), Tn) ⇒ Pot.Int.To(Aag, α1 , Tn, Tα1 , w) An agent might believe that a chain of events will occur for various reasons due to the inevitable nature of the domain.",
                "For example, in Chess, we often observe the following: a move causes a check position, which in turn limits the opponents moves to avoiding the check, to which the first player might react with another check, and so on.",
                "The agent might also believe in a chain of events based on its knowledge of its adversarys profile, which allows it to foresee the adversarys movements with high accuracy.",
                "A4.",
                "Profile Detection Axiom.",
                "The agent can adjust its adversarys profiles by observations and pattern study (specifically, if there are repeated encounters with the same adversary).",
                "However, instead of waiting for profile information to be revealed, an agent can also initiate actions that will force its adversary to react in a way that will reveal profile knowledge about it.",
                "Formally, the axiom states that if all actions (γ) are not highly beneficial actions (< TrH), the agent can do action α in time Tα if it believes that it will result in a non-highly beneficial action β from its adversary, which in turn teaches it about the adversarys profile, i.e., gives a higher AdvKnow(P Aj Ai , Tβ). (∀Aag, Ao ∈ A, α ∈ CAag , β ∈ CAo , Tn, Tα, Tβ, w ∈ W) Bel(Aag, (∀γ ∈ CAag )Eval(Aag, γ, w) < TrH, Tn) ∧ Bel(Aag, Do(Aag, α, Tα, w) ⇒ Do(Ao, β, Tβ, w), Tn) ∧ Bel(Aag, Eval(Ao, β, w) < TrH) ∧ Bel(Aag, AdvKnow(P Aj Ai , Tβ) > AdvKnow(P Aj Ai , Tn), Tn) ⇒ Pot.Int.To(Aag, α, Tn, Tα, w) For example, going back to the Chess board game scenario, consider starting a game versus an opponent about whom we know nothing, not even if it is a human or a computerized opponent.",
                "We might start playing a strategy that will be suitable versus an average opponent, and adjust our game according to its level of play.",
                "A5.",
                "Alliance Formation Axiom The following behavioral axiom is relevant only in a multilateral instantiation of the adversarial environment (obviously, an alliance cannot be formed in a bilateral, zero-sum encounter).",
                "In different situations during a multilateral <br>interaction</br>, a group of agents might believe that it is in their best interests to form a temporary alliance.",
                "Such an alliance is an agreement that constrains its members behavior, but is believed by its members to enable them to achieve a higher utility value than the one achievable outside of the alliance.",
                "As an example, we can look at the classical Risk board game, where each player has an individual goal of being the sole conquerer of the world, a zero-sum game.",
                "However, in order to achieve this goal, it might be strategically wise to make short-term ceasefire agreements with other players, or to join forces and attack an opponent who is stronger than the rest.",
                "An alliances terms defines the way its members should act.",
                "It is a set of predicates, denoted as Terms, that is agreed upon by the alliance members, and should remain true for the duration of the alliance.",
                "For example, the set Terms in the Risk scenario, could contain the following predicates: 1.",
                "Alliance members will not attack each other on territories X, Y and Z; 2.",
                "Alliance members will contribute C units per turn for attacking adversary Ao; 3.",
                "Members are obligated to stay as part of the alliance until time Tk or until adversarys Ao army is smaller than Q.",
                "The set Terms specifies inter-group constraints on each of the alliance members (∀Aal i ∈ Aal ⊆ A) set of actions Cal i ⊆ C. Definition 6.",
                "Al val - the total evaluation value that agent Ai will achieve while being part of Aal is the sum of Evali (Eval for Ai) of each of Aal j Eval values after taking their own α actions (via the agent(α) predicate): Al val(Ai, Cal , Aal , w) = α∈Cal Evali(Aal j , agent(α), w) Definition 7.",
                "Al TrH - is a number representing an Al val The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) 553 threshold; above it, the alliance can be said to be a highly beneficial alliance.",
                "The value of Al TrH will be calculated dynamically according to the progress of the <br>interaction</br>, as can be seen in [7].",
                "After an alliance is formed, its members are now working in their normal adversarial environment, as well as according to the mental states and axioms required for their interactions as part of the alliance.",
                "The following Alliance model (AL) specifies the conditions under which the group Aal can be said to be in an alliance and working with a new and constrained set of actions Cal , at time Tn.",
                "AL(Aal , Cal , w, Tn) 1.",
                "Aal has a MB that all members are part of Aal : MB(Aal , (∀Aal i ∈ Aal )member(Aal i , Aal ), Tn) 2.",
                "Aal has a MB that the group be maintained: MB(Aal , (∀Aal i ∈ Aal )Int.Th (Ai, member(Ai, Aal ), Tn, Tn+1, Co), Tn) 3.",
                "Aal has a MB that being members gives them high utility value: MB(Aal , (∀Aal i ∈ Aal )Al val(Aal i , Cal , Aal , w) ≥ Al TrH, Tn) Members profiles are a crucial part of successful alliances.",
                "We assume that agents that have more accurate profiles of their adversaries will be more successful in such environments.",
                "Such agents will be able to predict when a member is about to breach the alliances contract (item 2 in the above model), and take counter measures (when item 3 will falsify).",
                "The robustness of the alliance is in part a function of its members trustfulness measure, objective position estimation, and other profile properties.",
                "We should note that an agent can simultaneously be part of more than one alliance.",
                "Such a temporary alliance, where the group members do not have a joint goal but act collaboratively for the interest of their own individual goals, is classified as a Treatment Group by modern psychologists [12] (in contrast to a Task Group, where its members have a joint goal).",
                "The Shared Activity model as presented in [5] modeled Treatment Group behavior using the same SharedPlans formalization.",
                "When comparing both definitions of an alliance and a Treatment Group we found an unsurprising resemblance between both models: the environment models definitions are almost identical (see SAs definitions in [5]), and their Selfish-Act and Cooperative Act axioms conform to our adversarial agents behavior.",
                "The main distinction between both models is the integration of a Helpful-behavior act axiom, in the Shared Activity which cannot be part of ours.",
                "This axiom states that an agent will consider taking action that will lower its Eval value (to a certain lower bound), if it believes that a group partner will gain a significant benefit.",
                "Such behavior cannot occur in a pure adversarial environment (as a zero-sum game is), where the alliance members are constantly on watch to manipulate their alliance to their own advantage.",
                "A6.",
                "Evaluation Maximization Axiom.",
                "In a case when all other axioms are inapplicable, we will proceed with the action that maximizes the heuristic value as computed in the Eval function. (∀Aag, Ao ∈ A, α ∈ Cag, Tn, w ∈ W) Bel(Aag, (∀γ ∈ Cag)Eval(Aag, α, w) ≥ Eval(Aag, γ, w), Tn) ⇒ Pot.Int.To(Aag, α, Tn, Tα, w) T1.",
                "Optimality on Eval = Utility The above axiomatic model handles situations where the Utility is unknown and the agents are bounded rational agents.",
                "The following theorem shows that in bilateral interactions, where the agents have the real Utility function (i.e., Eval = Utility) and are rational agents, the axioms provide the same optimal result as classic adversarial search (e.g., Min-Max).",
                "Theorem 1.",
                "Let Ae ag be an unbounded rational AE agent using the Eval heuristic evaluation function, Au ag be the same agent using the true Utility function, and Ao be a sole unbounded utility-based rational adversary.",
                "Given that Eval = Utility: (∀α ∈ CAu ag , α ∈ CAe ag , Tn, w ∈ W) Pot.Int.To(Au ag, α, Tn, Tα, w) → Pot.Int.To(Ae ag, α , Tn, Tα, w) ∧ ((α = α ) ∨ (Utility(Au ag, α, w) = Eval(Ae ag, α , w))) Sketch of proof - Given that Au ag has the real utility function and unbounded resources, it can generate the full game tree and run the optimal MinMax algorithm to choose the highest utility value action, which we denote by, α.",
                "The proof will show that Ae ag, using the AE axioms, will select the same or equal utility α (when there is more than one action with the same max utility) when Eval = Utility. (A1) Goal achieving axiom - suppose there is an α such that its completion will achieve Au ags goal.",
                "It will obtain the highest utility by Min-Max for Au ag.",
                "The Ae ag agent will select α or another action with the same utility value via A1.",
                "If such α does not exist, Ae ag cannot apply this axiom, and proceeds to A2. (A2) Preventive act axiom - (1) Looking at the basic case (see Prop1), if there is a β which leads Ao to achieve its goal, then a preventive action α will yield the highest utility for Au ag.",
                "Au ag will choose it through the utility, while Ae ag will choose it through A2. (2) In the general case, β is a highly beneficial action for Ao, thus yields low utility for Au ag, which will guide it to select an α that will prevent β, while Ae ag will choose it through A2.1 If such β does not exist for Ao, then A2 is not applicable, and Ae ag can proceed to A3. (A3) Suboptimal tactical move axiom - When using a heuristic Eval function, Ae ag has a partial belief in the profile of its adversary (item 4 in AE model), which may lead it to believe in SetActions (Prop1).",
                "In our case, Ae ag is holding a full profile on its optimal adversary and knows that Ao will behave optimally according to the real utility values on the complete search tree, therefore, any belief about suboptimal SetAction cannot exist, yielding this axiom inapplicable.",
                "Ae ag will proceed to A4. (A4) Profile detection axiom - Given that Ae ag has the full profile of Ao, none of Ae ags actions can increase its knowledge.",
                "That axiom will not be applied, and the agent will proceed with A6 (A5 will be disregarded because the <br>interaction</br> is bilateral). (A6) Evaluation maximization axiom - This axiom will select the max Eval for Ae ag.",
                "Given that Eval = Utility, the same α that was selected by Au ag will be selected. 3.",
                "EVALUATION The main purpose of our experimental analysis is to evaluate the models behavior and performance in a real adversarial environment.",
                "This section investigates whether bounded 1 A case where following the completion of β there exists a γ which gives high utility for Agent Au ag, cannot occur because Ao uses the same utility, and γs existence will cause it to classify β as a low utility action. 554 The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) rational agents situated in such adversarial environments will be better off applying our suggested behavioral axioms. 3.1 The Domain To explore the use of the above model and its behavioral axioms, we decided to use the Connect-Four game as our adversarial environment.",
                "Connect-Four is a 2-player, zerosum game which is played using a 6x7 matrix-like board.",
                "Each turn, a player drops a disc into one of the 7 columns (the set of 21 discs is usually colored yellow for player 1 and red for player 2; we will use White and Black respectively to avoid confusion).",
                "The winner is the first player to complete a horizontal, vertical, or diagonal set of four discs with its color.",
                "On very rare occasions, the game might end in a tie if all the empty grids are filled, but no player managed to create a 4-disc set.",
                "The Connect-Four game was solved in [1], where it is shown that the first player (playing with the white discs) can force a win by starting in the middle column (column 4) and playing optimally However, the optimal strategy is very complex, and difficult to follow even for complex bounded rational agents, such as human players.",
                "Before we can proceed checking agent behavior, we must first verify that the domain conforms to the adversarial environments definition as given above (which the behavioral axioms are based on).",
                "First, when playing a Connect-Four game, the agent has an intention to win the game (item 1).",
                "Second (item 2), our agent believes that in Connect-Four there can only be one winner (or no winner at all in the rare occurrence of a tie).",
                "In addition, our agent believes that its opponent to the game will try to win (item 3), and we hope it has some partial knowledge (item 4) about its adversary (this knowledge can vary from nothing, through simple facts such as age, to strategies and weaknesses).",
                "Of course, not all Connect-Four encounters are adversarial.",
                "For example, when a parent is playing the game with its child, the following situation might occur: the child, having a strong incentive to win, treats the environment as adversarial (it intends to win, understands that there can only be one winner, and believes that its parent is trying to beat him).",
                "However, the parents point of view might see the environment as an educational one, where its goal is not to win the game, but to cause enjoyment or practice strategic reasoning.",
                "In such an educational environment, a new set of behavioral axioms might be more beneficial to the parents goals than our suggested adversarial behavioral axioms. 3.2 Axiom Analysis After showing that the Connect-Four game is indeed a zero-sum, bilateral adversarial environment, the next step is to look at players behaviors during the game and check whether behaving according to our model does improve performance.",
                "To do so we have collected log files from completed Connect-Four games that were played by human players over the Internet.",
                "Our collected log file data came from Play by eMail (PBeM) sites.",
                "These are web sites that host email games, where each move is taken by an email exchange between the server and the players.",
                "Many such sites archives contain real competitive interactions, and also maintain a ranking system for their members.",
                "Most of the data we used can be found in [6].",
                "As can be learned from [1], Connect-Four has an optimal strategy and a considerable advantage for the player who starts the game (which we call the White player).",
                "We will concentrate in our analysis on the second players moves (to be called Black).",
                "The White player, being the first to act, has the so-called initiative advantage.",
                "Having the advantage and a good strategy will keep the Black player busy reacting to its moves, instead of initiating threats.",
                "A threat is a combination of three discs of the same color, with an empty spot for the fourth winning disk.",
                "An open threat is a threat that can be realized in the opponents next move.",
                "In order for the Black player to win, it must somehow turn the tide, take the advantage and start presenting threats to the White player.",
                "We will explore Black players behavior and their conformance to our axioms.",
                "To do so, we built an application that reads log files and analyzes the Black players moves.",
                "The application contains two main components: (1) a Min-Max algorithm for evaluation of moves; (2) open threats detector for the discovering of open threats.",
                "The Min-Max algorithm will work to a given depth, d and for each move α will output the heuristic value for the next action taken by the player as written in the log file, h(α), alongside the maximum heuristic value, maxh(α), that could be achieved prior to taking the move (obviously, if h(α) = maxh(α), then the player did not do the optimal move heuristically).",
                "The threat detectors job is to notify if some action was taken in order to block an open threat (not blocking an open threat will probably cause the player to lose in the opponents next move).",
                "The heuristic function used by Min-Max to evaluate the players utility is the following function, which is simple to compute, yet provides a reasonable challenge to human opponents: Definition 8.",
                "Let Group be an adjacent set of four squares that are horizontal, vertical, or diagonal.",
                "Groupn b (Groupn w) be a Group with n pieces of the black (white) color and 4−n empty squares. h = ((Group1 b ∗α)+(Group2 b ∗β)+(Group3 b ∗γ)+(Group4 b ∗∞)) − ((Group1 w ∗α)+(Group2 w ∗β)+(Group3 w ∗γ)+(Group4 w ∗∞)) The values of α, β and δ can vary to form any desired linear combination; however, it is important to value them with the α < β < δ ordering in mind (we used 1, 4, and 8 as their respective values).",
                "Groups of 4 discs of the same color means victory, thus discovery of such a group will result in ∞ to ensure an extreme value.",
                "We now use our estimated evaluation function to evaluate the Black players actions during the Connect-Four adversarial <br>interaction</br>.",
                "Each game from the log file was input into the application, which processed and output a reformatted log file containing the h value of the current move, the maxh value that could be achieved, and a notification if an open threat was detected.",
                "A total of 123 games were analyzed (57 with White winning, and 66 with Black winning).",
                "A few additional games were manually ignored in the experiment, due to these problems: a player abandoning the game while the outcome is not final, or a blunt irrational move in the early stages of the game (e.g., not blocking an obvious winning group in the first opening moves).",
                "In addition, a single tie game was also removed.",
                "The simulator was run to a search depth of 3 moves.",
                "We now proceed to analyze the games with respect to each behavioral axiom.",
                "The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) 555 Table 1: Average heuristic difference analysis Black losses Black Won Avg minh -17.62 -12.02 Avg 3 lowest h moves (min3 h) -13.20 -8.70 3.2.1 Affirming the Suboptimal tactical move axiom The following section presents the heuristic evaluations of the Min-Max algorithm for each action, and checks the amount and extent of suboptimal tactical actions and their implications on performance.",
                "Table 1 shows results and insights from the games heuristic analysis, when search depth equals 3 (this search depth was selected for the results to be comparable to [9], see Section 3.2.3).",
                "The tables heuristic data is the difference between the present maximal heuristic value and the heuristic value of the action that was eventually taken by the player (i.e., the closer the number is to 0, the closer the action was to the maximum heuristic action).",
                "The first row presents the difference values of the action that had the maximal difference value among all the Black players actions in a given game, as averaged over all Blacks winning and losing games (see respective columns).",
                "In games in which the Black player loses, its average difference value was -17.62, while in games in which the Black player won, its average was -12.02.",
                "The second row expands the analysis by considering the 3 highest heuristic difference actions, and averaging them.",
                "In that case, we notice an average heuristic difference of 5 points between games which the Black player loses and games in which it wins.",
                "Nevertheless, the importance of those numbers is that they allowed us to take an educated guess on a threshold number of 11.5, as the value of the TrH constant, which differentiates between normal actions and highly beneficial ones.",
                "After finding an approximated TrH constant, we can proceed with an analysis of the importance of suboptimal moves.",
                "To do so we took the subset of games in which the minimum heuristic difference value for Blacks actions was 11.5.",
                "As presented in Table 2, we can see the different min3 h average of the 3 largest ranges and the respective percentage of games won.",
                "The first row shows that the Black player won only 12% of the games in which the average of its 3 highest heuristically difference actions (min3 h) was smaller than the suggested threshold, TrH = 11.5.",
                "The second row shows a surprising result: it seems that when min3 h > −4 the Black player rarely wins.",
                "Intuition would suggest that games in which the action evaluation values were closer to the maximal values will result in more winning games for Black.",
                "However, it seems that in the Connect-Four domain, merely responding with somewhat easily expected actions, without initiating a few surprising and suboptimal moves, does not yield good results.",
                "The last row sums up the main insights from the analysis; most of Blacks wins (83%) came when its min3 h was in the range of -11.5 to -4.",
                "A close inspection of those Black winning games shows the following pattern behind the numbers: after standard opening moves, Black suddenly drops a disc into an isolated column, which seems a waste of a move.",
                "White continues to build its threats, while usually disregarding Blacks last move, which in turn uses the isolated disc as an anchor for a future winning threat.",
                "The results show that it was beneficial for the Black player Table 2: Blacks winnings percentages % of games min3 h < −11.5 12% min3 h > −4 5% −11.5 ≤ min3 h ≤ −4 83% to take suboptimal actions and not give the current highest possible heuristic value, but will not be too harmful for its position (i.e., will not give high beneficial value to its adversary).",
                "As it turned out, learning the threshold is an important aspect of success: taking wildly risky moves (min3 h < −11.5) or trying to avoid them (min3 h > −4) reduces the Black players winning chances by a large margin. 3.2.2 Affirming the Profile Monitoring Axiom In the task of showing the importance of monitoring ones adversaries profiles, our log files could not be used because they did not contain repeated interactions between players, which are needed to infer the players knowledge about their adversaries.",
                "However, the importance of opponent modeling and its use in attaining tactical advantages was already studied in various domains ([3, 9] are good examples).",
                "In a recent paper, Markovitch and Reger [9] explored the notion of learning and exploitation of opponent weakness in competitive interactions.",
                "They apply simple learning strategies by analyzing examples from past interactions in a specific domain.",
                "They also used the Connect-Four adversarial domain, which can now be used to understand the importance of monitoring the adversarys profile.",
                "Following the presentation of their theoretical model, they describe an extensive empirical study and check the agents performance after learning the weakness model with past examples.",
                "One of the domains used as a competitive environment was the same Connect-Four game (Checkers was the second domain).",
                "Their heuristic function was identical to ours with three different variations (H1, H2, and H3) that are distinguished from one another in their linear combination coefficient values.",
                "The search depth for the players was 3 (as in our analysis).",
                "Their extensive experiments check and compare various learning strategies, risk factors, predefined feature sets and usage methods.",
                "The bottom line is that the Connect-Four domain shows an improvement from a 0.556 winning rate before modeling to a 0.69 after modeling (page 22).",
                "Their conclusions, showing improved performance when holding and using the adversarys model, justify the effort to monitor the adversary profile for continuous and repeated interactions.",
                "An additional point that came up in their experiments is the following: after the opponent weakness model has been learned, the authors describe different methods of integrating the opponent weakness model into the agents decision strategy.",
                "Nevertheless, regardless of the specific method they chose to work with, all integration methods might cause the agent to take suboptimal decisions; it might cause the agent to prefer actions that are suboptimal at the present decision junction, but which might cause the opponent to react in accordance with its weakness model (as represented by our agent) which in turn will be beneficial for us in the future.",
                "The agents behavior, as demonstrated in [9] further confirms and strengthens our Suboptimal Tactical Axiom as discussed in the previous section. 556 The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) 3.2.3 Additional Insights The need for the Goal Achieving, Preventive Act, and Evaluation Maximization axioms are obvious, and need no further verification.",
                "However, even with respect to those axioms, a few interesting insights came up in the log analysis.",
                "The Goal achieving and Preventive Act axioms, though theoretically trivial, seem to provide some challenge to a human player.",
                "In the initial inspection of the logs, we encountered few games2 where a player, for inexplicable reasons, did not block the other from winning or failed to execute its own winning move.",
                "We can blame those faults on the humans lack of attention, or a typing error in its move reply; nevertheless, those errors might occur in bounded rational agents, and the appropriate behavior needs to be axiomatized.",
                "A typical Connect-Four game revolves around generating threats and blocking them.",
                "In our analysis we looked for explicit preventive actions, i.e., moves that block a group of 3 discs, or that remove a future threat (in our limited search horizon).",
                "We found that in 83% of the total games there was at least one preventive action taken by the Black player.",
                "It was also found that Black averaged 2.8 preventive actions per game on the games in which it lost, while averaging 1.5 preventive actions per game when winning.",
                "It seems that Black requires 1 or 2 preventive actions to build its initial taking position, before starting to present threats.",
                "If it did not manage to win, it will usually prevent an extra threat or two before succumbing to White. 4.",
                "RELATED WORK Much research deals with the axiomatization of teamwork and mental states of individuals: some models use knowledge and belief [10], others have models of goals and intentions [8, 4].",
                "However, all these formal theories deal with agent teamwork and cooperation.",
                "As far as we know, our model is the first to provide a formalized model for explicit adversarial environments and agents behavior in it.",
                "The classical Min-Max adversarial search algorithm was the first attempt to integrate the opponent into the search space with a weak assumption of an optimally playing opponent.",
                "Since then, much effort has gone into integrating the opponent model into the decision procedure to predict future behavior.",
                "The M∗ algorithm presented by Carmel and Markovitch [2] showed a method of incorporating opponent models into adversary search, while in [3] they used learning to provide a more accurate opponent model in a 2player repeated game environment, where agents strategies were modeled as finite automata.",
                "Additional Adversarial planning work was done by Willmott et al. [13], which provided an adversarial planning approach to the game of GO.",
                "The research mentioned above dealt with adversarial search and the integration of opponent models into classical utilitybased search methods.",
                "That work shows the importance of opponent modeling and the ability to exploit it to an agents advantage.",
                "However, the basic limitations of those search methods still apply; our model tries to overcome those limitations by presenting a formal model for a new, mental state-based adversarial specification. 5.",
                "CONCLUSIONS We presented an Adversarial Environment model for a 2 These were later removed from the final analysis. bounded rational agent that is situated in an N-player, zerosum environment.",
                "We used the SharedPlans formalization to define the model and the axioms that agents can apply as behavioral guidelines.",
                "The model is meant to be used as a guideline for designing agents that need to operate in such adversarial environments.",
                "We presented empirical results, based on ConnectFour log file analysis, that exemplify the model and the axioms for a bilateral instance of the environment.",
                "The results we presented are a first step towards an expanded model that will cover all types of adversarial environments, for example, environments that are non-zero-sum, and environments that contain natural agents that are not part of the direct conflict.",
                "Those challenges and more will be dealt with in future research. 6.",
                "ACKNOWLEDGMENT This research was supported in part by Israel Science Foundation grants #1211/04 and #898/05. 7.",
                "REFERENCES [1] L. V. Allis.",
                "A knowledge-based approach of Connect-Four - the game is solved: White wins.",
                "Masters thesis, Free University, Amsterdam, The Netherlands, 1988. [2] D. Carmel and S. Markovitch.",
                "Incorporating opponent models into adversary search.",
                "In Proceedings of the Thirteenth National Conference on Artificial Intelligence, pages 120-125, Portland, OR, 1996. [3] D. Carmel and S. Markovitch.",
                "Opponent modeling in multi-agent systems.",
                "In G. Weiß and S. Sen, editors, Adaptation and Learning in Multi-Agent Systems, pages 40-52.",
                "Springer-Verlag, 1996. [4] B. J. Grosz and S. Kraus.",
                "Collaborative plans for complex group action.",
                "Artificial Intelligence, 86(2):269-357, 1996. [5] M. Hadad, G. Kaminka, G. Armon, and S. Kraus.",
                "Supporting collaborative activity.",
                "In Proc. of AAAI-2005, pages 83-88, Pittsburgh, 2005. [6] http://www.gamerz.net/˜pbmserv/. [7] S. Kraus and D. Lehmann.",
                "Designing and building a negotiating automated agent.",
                "Computational Intelligence, 11:132-171, 1995. [8] H. J. Levesque, P. R. Cohen, and J. H. T. Nunes.",
                "On acting together.",
                "In Proc. of AAAI-90, pages 94-99, Boston, MA, 1990. [9] S. Markovitch and R. Reger.",
                "Learning and exploiting relative weaknesses of opponent agents.",
                "Autonomous Agents and Multi-Agent Systems, 10(2):103-130, 2005. [10] Y. M. Ronald Fagin, Joseph Y. Halpern and M. Y. Vardi.",
                "Reasoning about knowledge.",
                "MIT Press, Cambridge, Mass., 1995. [11] P. Thagard.",
                "Adversarial problem solving: Modeling an oponent using explanatory coherence.",
                "Cognitive Science, 16(1):123-149, 1992. [12] R. W. Toseland and R. F. Rivas.",
                "An Introduction to Group Work Practice.",
                "Prentice Hall, Englewood Cliffs, NJ, 2nd edition edition, 1995. [13] S. Willmott, J. Richardson, A. Bundy, and J. Levine.",
                "An adversarial planning approach to Go.",
                "Lecture Notes in Computer Science, 1558:93-112, 1999.",
                "The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) 557"
            ],
            "original_annotated_samples": [],
            "translated_annotated_samples": [
                "Recordamos aquí algunos de los predicados y operadores que se utilizan en esa formalización: int.to (ai, α, tn, tα, c) representan las intenciones de AIS en el momento TN para hacer una acción α en el tiempo Tα en el contexto de C.Int.th (ai, prop, tn, tprop, c) representa las intenciones de AIS en el momento tn que una cierta proposición se mantiene en el tiempo tprop en el contexto de C. los operadores de intención potenciales, pot.int.to (...)y pot.int.th (...), se usan para representar el estado mental cuando un agente considera adoptar una intención, pero no ha deliberado sobre la \"interacción\" de las otras intenciones que tiene.interacción",
                "El conflicto completo (FULCONF) describe una \"interacción\" de Zerosum donde solo se puede completar un objetivo de los objetivos en el conflicto.interacción",
                "En diferentes situaciones durante una \"interacción\" multilateral, un grupo de agentes podría creer que lo mejor para su mejor interés puede formar una alianza temporal.interacción",
                "El valor de Al TRH se calculará dinámicamente de acuerdo con el progreso de la \"interacción\", como se puede ver en [7].interacción",
                "Ese axioma no se aplicará, y el agente continuará con A6 (A5 se ignorará porque la \"interacción\" es bilateral).(A6) Axioma de maximización de la evaluación: este axioma seleccionará la evaluación máxima para AE AG.interacción",
                "Ahora utilizamos nuestra función de evaluación estimada para evaluar las acciones de los jugadores negros durante la \"interacción\" connversarial connects-four.interacción"
            ],
            "translated_text": "",
            "candidates": [],
            "error": []
        },
        "agent": {
            "translated_key": "",
            "is_in_text": true,
            "original_annotated_sentences": [
                "An Adversarial Environment Model for Bounded Rational Agents in Zero-Sum Interactions Inon Zuckerman1 , Sarit Kraus1 , Jeffrey S. Rosenschein2 , Gal Kaminka1 1 Department of Computer Science 2 The School of Engineering Bar-Ilan University and Computer Science Ramat-Gan, Israel Hebrew University, Jerusalem, Israel {zukermi,sarit,galk}@cs.biu.ac.il jeff@cs.huji.ac.il ABSTRACT Multiagent environments are often not cooperative nor collaborative; in many cases, agents have conflicting interests, leading to adversarial interactions.",
                "This paper presents a formal Adversarial Environment model for bounded rational agents operating in a zero-sum environment.",
                "In such environments, attempts to use classical utility-based search methods can raise a variety of difficulties (e.g., implicitly modeling the opponent as an omniscient utility maximizer, rather than leveraging a more nuanced, explicit opponent model).",
                "We define an Adversarial Environment by describing the mental states of an <br>agent</br> in such an environment.",
                "We then present behavioral axioms that are intended to serve as design principles for building such adversarial agents.",
                "We explore the application of our approach by analyzing log files of completed Connect-Four games, and present an empirical analysis of the axioms appropriateness.",
                "Categories and Subject Descriptors I.2.11 [Artificial Intelligence]: Distributed Artificial Intelligence-Intelligent agents,Multiagent Systems; I.2.4 [Artificial Intelligence]: Knowledge Representation Formalisms and Methods -Modal logic General Terms Design, Theory 1.",
                "INTRODUCTION Early research in multiagent systems (MAS) considered cooperative groups of agents; because individual agents had limited resources, or limited access to information (e.g., limited processing power, limited sensor coverage), they worked together by design to solve problems that individually they could not solve, or at least could not solve as efficiently.",
                "MAS research, however, soon began to consider interacting agents with individuated interests, as representatives of different humans or organizations with non-identical interests.",
                "When interactions are guided by diverse interests, participants may have to overcome disagreements, uncooperative interactions, and even intentional attempts to damage one another.",
                "When these types of interactions occur, environments require appropriate behavior from the agents situated in them.",
                "We call these environments Adversarial Environments, and call the clashing agents Adversaries.",
                "Models of cooperation and teamwork have been extensively explored in MAS through the axiomatization of mental states (e.g., [8, 4, 5]).",
                "However, none of this research dealt with adversarial domains and their implications for <br>agent</br> behavior.",
                "Our paper addresses this issue by providing a formal, axiomatized mental state model for a subset of adversarial domains, namely simple zero-sum adversarial environments.",
                "Simple zero-sum encounters exist of course in various twoplayer games (e.g., Chess, Checkers), but they also exist in n-player games (e.g., Risk, Diplomacy), auctions for a single good, and elsewhere.",
                "In these latter environments especially, using a utility-based adversarial search (such as the Min-Max algorithm) does not always provide an adequate solution; the payoff function might be quite complex or difficult to quantify, and there are natural computational limitations on bounded rational agents.",
                "In addition, traditional search methods (like Min-Max) do not make use of a model of the opponent, which has proven to be a valuable addition to adversarial planning [9, 3, 11].",
                "In this paper, we develop a formal, axiomatized model for bounded rational agents that are situated in a zero-sum adversarial environment.",
                "The model uses different modality operators, and its main foundations are the SharedPlans [4] model for collaborative behavior.",
                "We explore environment properties and the mental states of agents to derive behavioral axioms; these behavioral axioms constitute a formal model that serves as a specification and design guideline for <br>agent</br> design in such settings.",
                "We then investigate the behavior of our model empirically using the Connect-Four board game.",
                "We show that this game conforms to our environment definition, and analyze players behavior using a large set of completed match log 550 978-81-904262-7-5 (RPS) c 2007 IFAAMAS files.",
                "In addition, we use the results presented in [9] to discuss the importance of opponent modeling in our ConnectFour adversarial domain.",
                "The paper proceeds as follows.",
                "Section 2 presents the models formalization.",
                "Section 3 presents the empirical analysis and its results.",
                "We discuss related work in Section 4, and conclude and present future directions in Section 5. 2.",
                "ADVERSARIAL ENVIRONMENTS The adversarial environment model (denoted as AE) is intended to guide the design of agents by providing a specification of the capabilities and mental attitudes of an <br>agent</br> in an adversarial environment.",
                "We focus here on specific types of adversarial environments, specified as follows: 1.",
                "Zero-Sum Interactions: positive and negative utilities of all agents sum to zero; 2.",
                "Simple AEs: all agents in the environment are adversarial agents; 3.",
                "Bilateral AEs: AEs with exactly two agents; 4.",
                "Multilateral AEs: AEs of three or more agents.",
                "We will work on both bilateral and multilateral instantiations of zero-sum and simple environments.",
                "In particular, our adversarial environment model will deal with interactions that consist of N agents (N ≥ 2), where all agents are adversaries, and only one <br>agent</br> can succeed.",
                "Examples of such environments range from board games (e.g., Chess, Connect-Four, and Diplomacy) to certain economic environments (e.g., N-bidder auctions over a single good). 2.1 Model Overview Our approach is to formalize the mental attitudes and behaviors of a single adversarial <br>agent</br>; we consider how a single <br>agent</br> perceives the AE.",
                "The following list specifies the conditions and mental states of an <br>agent</br> in a simple, zero-sum AE: 1.",
                "The <br>agent</br> has an individual intention that its own goal will be completed; 2.",
                "The <br>agent</br> has an individual belief that it and its adversaries are pursuing full conflicting goals (defined below)there can be only one winner; 3.",
                "The <br>agent</br> has an individual belief that each adversary has an intention to complete its own full conflicting goal; 4.",
                "The <br>agent</br> has an individual belief in the (partial) profile of its adversaries.",
                "Item 3 is required, since it might be the case that some <br>agent</br> has a full conflicting goal, and is currently considering adopting the intention to complete it, but is, as of yet, not committed to achieving it.",
                "This might occur because the <br>agent</br> has not yet deliberated about the effects that adopting that intention might have on the other intentions it is currently holding.",
                "In such cases, it might not consider itself to even be in an adversarial environment.",
                "Item 4 states that the <br>agent</br> should hold some belief about the profiles of its adversaries.",
                "The profile represents all the knowledge the <br>agent</br> has about its adversary: its weaknesses, strategic capabilities, goals, intentions, trustworthiness, and more.",
                "It can be given explicitly or can be learned from observations of past encounters. 2.2 Model Definitions for Mental States We use Grosz and Krauss definitions of the modal operators, predicates, and meta-predicates, as defined in their SharedPlan formalization [4].",
                "We recall here some of the predicates and operators that are used in that formalization: Int.To(Ai, α, Tn, Tα, C) represents Ais intentions at time Tn to do an action α at time Tα in the context of C. Int.Th(Ai, prop, Tn, Tprop, C) represents Ais intentions at time Tn that a certain proposition prop holds at time Tprop in the context of C. The potential intention operators, Pot.Int.To(...) and Pot.Int.Th(...), are used to represent the mental state when an <br>agent</br> considers adopting an intention, but has not deliberated about the interaction of the other intentions it holds.",
                "The operator Bel(Ai, f, Tf ) represents <br>agent</br> Ai believing in the statement expressed in formula f, at time Tf .",
                "MB(A, f, Tf ) represents mutual belief for a group of agents A.",
                "A snapshot of the system finds our environment to be in some state e ∈ E of environmental variable states, and each adversary in any LAi ∈ L of possible local states.",
                "At any given time step, the system will be in some world w of the set of all possible worlds w ∈ W, where w = E×LA1 ×LA2 × ...LAn , and n is the number of adversaries.",
                "For example, in a Texas Holdem poker game, an agents local state might be its own set of cards (which is unknown to its adversary) while the environment will consist of the betting pot and the community cards (which are visible to both players).",
                "A utility function under this formalization is defined as a mapping from a possible world w ∈ W to an element in , which expresses the desirability of the world, from a single <br>agent</br> perspective.",
                "We usually normalize the range to [0,1], where 0 represents the least desirable possible world, and 1 is the most desirable world.",
                "The implementation of the utility function is dependent on the domain in question.",
                "The following list specifies new predicates, functions, variables, and constants used in conjunction with the original definitions for the adversarial environment formalization: 1. φ is a null action (the <br>agent</br> does not do anything). 2.",
                "GAi is the set of <br>agent</br> Ais goals.",
                "Each goal is a set of predicates whose satisfaction makes the goal complete (we use G∗ Ai ∈ GAi to represent an arbitrary goal of <br>agent</br> Ai). 3. gAi is the set of <br>agent</br> Ais subgoals.",
                "Subgoals are predicates whose satisfaction represents an important milestone toward achievement of the full goal. gG∗ Ai ⊆ gAi is the set of subgoals that are important to the completion of goal G∗ Ai (we will use g∗ G∗ Ai ∈ gG∗ Ai to represent an arbitrary subgoal). 4.",
                "P Aj Ai is the profile object <br>agent</br> Ai holds about <br>agent</br> Aj. 5.",
                "CA is a general set of actions for all agents in A which are derived from the environments constraints.",
                "CAi ⊆ CA is the set of <br>agent</br> Ais possible actions. 6.",
                "Do(Ai, α, Tα, w) holds when Ai performs action α over time interval Tα in world w. 7.",
                "Achieve(G∗ Ai , α, w) is true when goal G∗ Ai is achieved following the completion of action α in world w ∈ W, where α ∈ CAi . 8.",
                "Profile(Ai, PAi Ai ) is true when <br>agent</br> Ai holds an object profile for <br>agent</br> Aj.",
                "Definition 1.",
                "Full conflict (FulConf ) describes a zerosum interaction where only a single goal of the goals in conflict can be completed.",
                "FulConf(G∗ Ai , G∗ Aj ) ⇒ (∃α ∈ CAi , ∀w, β ∈ CAj ) (Achieve(G∗ Ai , α, w) ⇒ ¬Achieve(G∗ Aj , β, w)) ∨ (∃β ∈ CAj , ∀w, α ∈ CAi )(Achieve(G∗ Aj , β, w) ⇒ ¬Achieve(G∗ Ai , α, w)) Definition 2.",
                "Adversarial Knowledge (AdvKnow) is a function returning a value which represents the amount of The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-<br>agent</br> Systems (AAMAS 07) 551 knowledge <br>agent</br> Ai has on the profile of agent Aj, at time Tn.",
                "The higher the value, the more knowledge <br>agent</br> Ai has.",
                "AdvKnow : P Aj Ai × Tn → Definition 3.",
                "Eval - This evaluation function returns an estimated expected utility value for an <br>agent</br> in A, after completing an action from CA in some world state w. Eval : A × CA × w → Definition 4.",
                "TrH - (Threshold) is a numerical constant in the [0,1] range that represents an evaluation function (Eval) threshold value.",
                "An action that yields an estimated utility evaluation above the TrH is regarded as a highly beneficial action.",
                "The Eval value is an estimation and not the real utility function, which is usually unknown.",
                "Using the real utility value for a rational <br>agent</br> would easily yield the best outcome for that <br>agent</br>.",
                "However, agents usually do not have the real utility functions, but rather a heuristic estimate of it.",
                "There are two important properties that should hold for the evaluation function: Property 1.",
                "The evaluation function should state that the most desirable world state is one in which the goal is achieved.",
                "Therefore, after the goal has been satisfied, there can be no future action that can put the <br>agent</br> in a world state with higher Eval value. (∀Ai, G∗ Ai , α, β ∈ CAi , w ∈ W) Achieve(G∗ Ai , α, w) ⇒ Eval(Ai, α, w) ≥ Eval(Ai, β, w) Property 2.",
                "The evaluation function should project an action that causes a completion of a goal or a subgoal to a value which is greater than TrH (a highly beneficial action). (∀Ai, G∗ Ai ∈ GAi , α ∈ CAi , w ∈ W, g∗ GAi ∈ gGAi ) Achieve(G∗ Ai , α, w) ∨ Achieve(g∗ GAi , α, w) ⇒ Eval(Ai, α, w) ≥ TrH.",
                "Definition 5.",
                "SetAction We define a set action (SetAction) as a set of action operations (either complex or basic actions) from some action sets CAi and CAj which, according to <br>agent</br> Ais belief, are attached together by a temporal and consequential relationship, forming a chain of events (action, and its following consequent action). (∀α1 , . . . , αu ∈ CAi , β1 , . . . , βv ∈ CAj , w ∈ W) SetAction(α1 , . . . , αu , β1 , . . . , βv , w) ⇒ ((Do(Ai, α1 , Tα1 , w) ⇒ Do(Aj, β1 , Tβ1 , w)) ⇒ Do(Ai, α2 , Tα2 , w) ⇒ . . . ⇒ Do(Ai, αu , Tαu , w)) The consequential relation might exist due to various environmental constraints (when one action forces the adversary to respond with a specific action) or due to the agents knowledge about the profile of its adversary.",
                "Property 3.",
                "As the knowledge we have about our adversary increases we will have additional beliefs about its behavior in different situations which in turn creates new set actions.",
                "Formally, if our AdvKnow at time Tn+1 is greater than AdvKnow at time Tn, then every SetAction known at time Tn is also known at time Tn+1.",
                "AdvKnow(P Aj Ai , Tn+1) > AdvKnow(P Aj Ai , Tn) ⇒ (∀α1 , . . . , αu ∈ CAi , β1 , . . . , βv ∈ CAj ) Bel(Aag, SetAction(α1 , . . . , αu , β1 , . . . , βv ), Tn) ⇒ Bel(Aag, SetAction(α1 , . . . , αu , β1 , . . . , βv ), Tn+1) 2.3 The Environment Formulation The following axioms provide the formal definition for a simple, zero-sum Adversarial Environment (AE).",
                "Satisfaction of these axioms means that the <br>agent</br> is situated in such an environment.",
                "It provides specifications for <br>agent</br> Aag to interact with its set of adversaries A with respect to goals G∗ Aag and G∗ A at time TCo at some world state w. AE(Aag, A, G∗ Aag , A1, . . . , Ak, G∗ A1 , . . . , G∗ Ak , Tn, w) 1.",
                "Aag has an Int.Th his goal would be completed: (∃α ∈ CAag , Tα) Int.Th(Aag, Achieve(G∗ Aag , α), Tn, Tα, AE) 2.",
                "Aag believes that it and each of its adversaries Ao are pursuing full conflicting goals: (∀Ao ∈ {A1, . . . , Ak}) Bel(Aag, FulConf(G∗ Aag , G∗ Ao ), Tn) 3.",
                "Aag believes that each of his adversaries in Ao has the Int.Th his conflicting goal G∗ Aoi will be completed: (∀Ao ∈ {A1, . . . , Ak})(∃β ∈ CAo , Tβ) Bel(Aag, Int.Th(Ao, Achieve(G∗ Ao , β), TCo, Tβ, AE), Tn) 4.",
                "Aag has beliefs about the (partial) profiles of its adversaries (∀Ao ∈ {A1, . . . , Ak}) (∃PAo Aag ∈ PAag )Bel(Aag, Profile(Ao, PAo Aag ), Tn) To build an <br>agent</br> that will be able to operate successfully within such an AE, we must specify behavioral guidelines for its interactions.",
                "Using a naive Eval maximization strategy to a certain search depth will not always yield satisfactory results for several reasons: (1) the search horizon problem when searching for a fixed depth; (2) the strong assumption of an optimally rational, unbounded resources adversary; (3) using an estimated evaluation function which will not give optimal results in all world states, and can be exploited [9].",
                "The following axioms specify the behavioral principles that can be used to differentiate between successful and less successful agents in the above Adversarial Environment.",
                "Those axioms should be used as specification principles when designing and implementing agents that should be able to perform well in such Adversarial Environments.",
                "The behavioral axioms represent situations in which the <br>agent</br> will adopt potential intentions to (Pot.Int.To(...)) perform an action, which will typically require some means-end reasoning to select a possible course of action.",
                "This reasoning will lead to the adoption of an Int.To(...) (see [4]).",
                "A1.",
                "Goal Achieving Axiom.",
                "The first axiom is the simplest case; when the <br>agent</br> Aag believes that it is one action (α) away from achieving his conflicting goal G∗ Aag , it should adopt the potential intention to do α and complete its goal. (∀Aag, α ∈ CAag , Tn, Tα, w ∈ W) (Bel(Aag, Do(Aag, α, Tα, w) ⇒ Achieve(G∗ Aag , α, w)) ⇒ Pot.Int.To(Aag, α, Tn, Tα, w) This somewhat trivial behavior is the first and strongest axiom.",
                "In any situation, when the <br>agent</br> is an action away from completing the goal, it should complete the action.",
                "Any fair Eval function would naturally classify α as the maximal value action (property 1).",
                "However, without explicit axiomatization of such behavior there might be situations where the <br>agent</br> will decide on taking another action for various reasons, due to its bounded decision resources.",
                "A2.",
                "Preventive Act Axiom.",
                "Being in an adversarial situation, <br>agent</br> Aag might decide to take actions that will damage one of its adversarys plans to complete its goal, even if those actions do not explicitly advance Aag towards its conflicting goal G∗ Aag .",
                "Such preventive action will take place when <br>agent</br> Aag has a belief about the possibility of its adversary Ao doing an action β that will give it a high 552 The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-<br>agent</br> Systems (AAMAS 07) utility evaluation value (> TrH).",
                "Believing that taking action α will prevent the opponent from doing its β, it will adopt a potential intention to do α. (∀Aag, Ao ∈ A, α ∈ CAag , β ∈ CAo , Tn, Tβ, w ∈ W) (Bel(Aag, Do(Ao, β, Tβ, w) ∧ Eval(Ao, β, w) > TrH, Tn) ∧ Bel(Aag, Do(Aag, α, Tα, w) ⇒ ¬Do(Ao, β, Tβ, w), Tn) ⇒ Pot.Int.To(Aag, α, Tn, Tα, w) This axiom is a basic component of any adversarial environment.",
                "For example, looking at a Chess board game, a player could realize that it is about to be checkmated by its opponent, thus making a preventive move.",
                "Another example is a Connect Four game: when a player has a row of three chips, its opponent must block it, or lose.",
                "A specific instance of A1 occurs when the adversary is one action away from achieving its goal, and immediate preventive action needs to be taken by the <br>agent</br>.",
                "Formally, we have the same beliefs as stated above, with a changed belief that doing action β will cause <br>agent</br> Ao to achieve its goal.",
                "Proposition 1: Prevent or lose case. (∀Aag, Ao ∈ A, α ∈ CAag , β ∈ CAo , G∗ Ao , Tn, Tα, Tβ, w ∈ W) Bel(Aag, Do(Ao, β, Tβ, w) ⇒ Achieve(G∗ Ao , β, w), Tn) ∧ Bel(Aag, Do(Aag, α, Tα, w) ⇒ ¬Do(Ao, β, Tβ, w)) ⇒ Pot.Int.To(Aag, α, Tn, Tα, w) Sketch of proof: Proposition 1 can be easily derived from axiom A1 and the property 2 of the Eval function, which states that any action that causes a completion of a goal is a highly beneficial action.",
                "The preventive act behavior will occur implicitly when the Eval function is equal to the real world utility function.",
                "However, being bounded rational agents and dealing with an estimated evaluation function we need to explicitly axiomatize such behavior, for it will not always occur implicitly from the evaluation function.",
                "A3.",
                "Suboptimal Tactical Move Axiom.",
                "In many scenarios a situation may occur where an <br>agent</br> will decide not to take the current most beneficial action it can take (the action with the maximal utility evaluation value), because it believes that taking another action (with lower utility evaluation value) might yield (depending on the adversarys response) a future possibility for a highly beneficial action.",
                "This will occur most often when the Eval function is inaccurate and differs by a large extent from the Utility function.",
                "Put formally, <br>agent</br> Aag believes in a certain SetAction that will evolve according to its initial action and will yield a high beneficial value (> TrH) solely for it. (∀Aag, Ao ∈ A, Tn, w ∈ W) (∃α1 , . . . , αu ∈ CAi , β1 , . . . , βv ∈ CAj , Tα1 ) Bel(Aag, SetAction(α1 , . . . , αu , β1 , . . . , βv ), Tn) ∧ Bel(Aag, Eval(Ao, βv , w) < TrH < Eval(Aag, αu , w), Tn) ⇒ Pot.Int.To(Aag, α1 , Tn, Tα1 , w) An <br>agent</br> might believe that a chain of events will occur for various reasons due to the inevitable nature of the domain.",
                "For example, in Chess, we often observe the following: a move causes a check position, which in turn limits the opponents moves to avoiding the check, to which the first player might react with another check, and so on.",
                "The <br>agent</br> might also believe in a chain of events based on its knowledge of its adversarys profile, which allows it to foresee the adversarys movements with high accuracy.",
                "A4.",
                "Profile Detection Axiom.",
                "The <br>agent</br> can adjust its adversarys profiles by observations and pattern study (specifically, if there are repeated encounters with the same adversary).",
                "However, instead of waiting for profile information to be revealed, an <br>agent</br> can also initiate actions that will force its adversary to react in a way that will reveal profile knowledge about it.",
                "Formally, the axiom states that if all actions (γ) are not highly beneficial actions (< TrH), the <br>agent</br> can do action α in time Tα if it believes that it will result in a non-highly beneficial action β from its adversary, which in turn teaches it about the adversarys profile, i.e., gives a higher AdvKnow(P Aj Ai , Tβ). (∀Aag, Ao ∈ A, α ∈ CAag , β ∈ CAo , Tn, Tα, Tβ, w ∈ W) Bel(Aag, (∀γ ∈ CAag )Eval(Aag, γ, w) < TrH, Tn) ∧ Bel(Aag, Do(Aag, α, Tα, w) ⇒ Do(Ao, β, Tβ, w), Tn) ∧ Bel(Aag, Eval(Ao, β, w) < TrH) ∧ Bel(Aag, AdvKnow(P Aj Ai , Tβ) > AdvKnow(P Aj Ai , Tn), Tn) ⇒ Pot.Int.To(Aag, α, Tn, Tα, w) For example, going back to the Chess board game scenario, consider starting a game versus an opponent about whom we know nothing, not even if it is a human or a computerized opponent.",
                "We might start playing a strategy that will be suitable versus an average opponent, and adjust our game according to its level of play.",
                "A5.",
                "Alliance Formation Axiom The following behavioral axiom is relevant only in a multilateral instantiation of the adversarial environment (obviously, an alliance cannot be formed in a bilateral, zero-sum encounter).",
                "In different situations during a multilateral interaction, a group of agents might believe that it is in their best interests to form a temporary alliance.",
                "Such an alliance is an agreement that constrains its members behavior, but is believed by its members to enable them to achieve a higher utility value than the one achievable outside of the alliance.",
                "As an example, we can look at the classical Risk board game, where each player has an individual goal of being the sole conquerer of the world, a zero-sum game.",
                "However, in order to achieve this goal, it might be strategically wise to make short-term ceasefire agreements with other players, or to join forces and attack an opponent who is stronger than the rest.",
                "An alliances terms defines the way its members should act.",
                "It is a set of predicates, denoted as Terms, that is agreed upon by the alliance members, and should remain true for the duration of the alliance.",
                "For example, the set Terms in the Risk scenario, could contain the following predicates: 1.",
                "Alliance members will not attack each other on territories X, Y and Z; 2.",
                "Alliance members will contribute C units per turn for attacking adversary Ao; 3.",
                "Members are obligated to stay as part of the alliance until time Tk or until adversarys Ao army is smaller than Q.",
                "The set Terms specifies inter-group constraints on each of the alliance members (∀Aal i ∈ Aal ⊆ A) set of actions Cal i ⊆ C. Definition 6.",
                "Al val - the total evaluation value that <br>agent</br> Ai will achieve while being part of Aal is the sum of Evali (Eval for Ai) of each of Aal j Eval values after taking their own α actions (via the <br>agent</br>(α) predicate): Al val(Ai, Cal , Aal , w) = α∈Cal Evali(Aal j , agent(α), w) Definition 7.",
                "Al TrH - is a number representing an Al val The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-<br>agent</br> Systems (AAMAS 07) 553 threshold; above it, the alliance can be said to be a highly beneficial alliance.",
                "The value of Al TrH will be calculated dynamically according to the progress of the interaction, as can be seen in [7].",
                "After an alliance is formed, its members are now working in their normal adversarial environment, as well as according to the mental states and axioms required for their interactions as part of the alliance.",
                "The following Alliance model (AL) specifies the conditions under which the group Aal can be said to be in an alliance and working with a new and constrained set of actions Cal , at time Tn.",
                "AL(Aal , Cal , w, Tn) 1.",
                "Aal has a MB that all members are part of Aal : MB(Aal , (∀Aal i ∈ Aal )member(Aal i , Aal ), Tn) 2.",
                "Aal has a MB that the group be maintained: MB(Aal , (∀Aal i ∈ Aal )Int.Th (Ai, member(Ai, Aal ), Tn, Tn+1, Co), Tn) 3.",
                "Aal has a MB that being members gives them high utility value: MB(Aal , (∀Aal i ∈ Aal )Al val(Aal i , Cal , Aal , w) ≥ Al TrH, Tn) Members profiles are a crucial part of successful alliances.",
                "We assume that agents that have more accurate profiles of their adversaries will be more successful in such environments.",
                "Such agents will be able to predict when a member is about to breach the alliances contract (item 2 in the above model), and take counter measures (when item 3 will falsify).",
                "The robustness of the alliance is in part a function of its members trustfulness measure, objective position estimation, and other profile properties.",
                "We should note that an <br>agent</br> can simultaneously be part of more than one alliance.",
                "Such a temporary alliance, where the group members do not have a joint goal but act collaboratively for the interest of their own individual goals, is classified as a Treatment Group by modern psychologists [12] (in contrast to a Task Group, where its members have a joint goal).",
                "The Shared Activity model as presented in [5] modeled Treatment Group behavior using the same SharedPlans formalization.",
                "When comparing both definitions of an alliance and a Treatment Group we found an unsurprising resemblance between both models: the environment models definitions are almost identical (see SAs definitions in [5]), and their Selfish-Act and Cooperative Act axioms conform to our adversarial agents behavior.",
                "The main distinction between both models is the integration of a Helpful-behavior act axiom, in the Shared Activity which cannot be part of ours.",
                "This axiom states that an <br>agent</br> will consider taking action that will lower its Eval value (to a certain lower bound), if it believes that a group partner will gain a significant benefit.",
                "Such behavior cannot occur in a pure adversarial environment (as a zero-sum game is), where the alliance members are constantly on watch to manipulate their alliance to their own advantage.",
                "A6.",
                "Evaluation Maximization Axiom.",
                "In a case when all other axioms are inapplicable, we will proceed with the action that maximizes the heuristic value as computed in the Eval function. (∀Aag, Ao ∈ A, α ∈ Cag, Tn, w ∈ W) Bel(Aag, (∀γ ∈ Cag)Eval(Aag, α, w) ≥ Eval(Aag, γ, w), Tn) ⇒ Pot.Int.To(Aag, α, Tn, Tα, w) T1.",
                "Optimality on Eval = Utility The above axiomatic model handles situations where the Utility is unknown and the agents are bounded rational agents.",
                "The following theorem shows that in bilateral interactions, where the agents have the real Utility function (i.e., Eval = Utility) and are rational agents, the axioms provide the same optimal result as classic adversarial search (e.g., Min-Max).",
                "Theorem 1.",
                "Let Ae ag be an unbounded rational AE <br>agent</br> using the Eval heuristic evaluation function, Au ag be the same <br>agent</br> using the true Utility function, and Ao be a sole unbounded utility-based rational adversary.",
                "Given that Eval = Utility: (∀α ∈ CAu ag , α ∈ CAe ag , Tn, w ∈ W) Pot.Int.To(Au ag, α, Tn, Tα, w) → Pot.Int.To(Ae ag, α , Tn, Tα, w) ∧ ((α = α ) ∨ (Utility(Au ag, α, w) = Eval(Ae ag, α , w))) Sketch of proof - Given that Au ag has the real utility function and unbounded resources, it can generate the full game tree and run the optimal MinMax algorithm to choose the highest utility value action, which we denote by, α.",
                "The proof will show that Ae ag, using the AE axioms, will select the same or equal utility α (when there is more than one action with the same max utility) when Eval = Utility. (A1) Goal achieving axiom - suppose there is an α such that its completion will achieve Au ags goal.",
                "It will obtain the highest utility by Min-Max for Au ag.",
                "The Ae ag <br>agent</br> will select α or another action with the same utility value via A1.",
                "If such α does not exist, Ae ag cannot apply this axiom, and proceeds to A2. (A2) Preventive act axiom - (1) Looking at the basic case (see Prop1), if there is a β which leads Ao to achieve its goal, then a preventive action α will yield the highest utility for Au ag.",
                "Au ag will choose it through the utility, while Ae ag will choose it through A2. (2) In the general case, β is a highly beneficial action for Ao, thus yields low utility for Au ag, which will guide it to select an α that will prevent β, while Ae ag will choose it through A2.1 If such β does not exist for Ao, then A2 is not applicable, and Ae ag can proceed to A3. (A3) Suboptimal tactical move axiom - When using a heuristic Eval function, Ae ag has a partial belief in the profile of its adversary (item 4 in AE model), which may lead it to believe in SetActions (Prop1).",
                "In our case, Ae ag is holding a full profile on its optimal adversary and knows that Ao will behave optimally according to the real utility values on the complete search tree, therefore, any belief about suboptimal SetAction cannot exist, yielding this axiom inapplicable.",
                "Ae ag will proceed to A4. (A4) Profile detection axiom - Given that Ae ag has the full profile of Ao, none of Ae ags actions can increase its knowledge.",
                "That axiom will not be applied, and the <br>agent</br> will proceed with A6 (A5 will be disregarded because the interaction is bilateral). (A6) Evaluation maximization axiom - This axiom will select the max Eval for Ae ag.",
                "Given that Eval = Utility, the same α that was selected by Au ag will be selected. 3.",
                "EVALUATION The main purpose of our experimental analysis is to evaluate the models behavior and performance in a real adversarial environment.",
                "This section investigates whether bounded 1 A case where following the completion of β there exists a γ which gives high utility for <br>agent</br> Au ag, cannot occur because Ao uses the same utility, and γs existence will cause it to classify β as a low utility action. 554 The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-<br>agent</br> Systems (AAMAS 07) rational agents situated in such adversarial environments will be better off applying our suggested behavioral axioms. 3.1 The Domain To explore the use of the above model and its behavioral axioms, we decided to use the Connect-Four game as our adversarial environment.",
                "Connect-Four is a 2-player, zerosum game which is played using a 6x7 matrix-like board.",
                "Each turn, a player drops a disc into one of the 7 columns (the set of 21 discs is usually colored yellow for player 1 and red for player 2; we will use White and Black respectively to avoid confusion).",
                "The winner is the first player to complete a horizontal, vertical, or diagonal set of four discs with its color.",
                "On very rare occasions, the game might end in a tie if all the empty grids are filled, but no player managed to create a 4-disc set.",
                "The Connect-Four game was solved in [1], where it is shown that the first player (playing with the white discs) can force a win by starting in the middle column (column 4) and playing optimally However, the optimal strategy is very complex, and difficult to follow even for complex bounded rational agents, such as human players.",
                "Before we can proceed checking <br>agent</br> behavior, we must first verify that the domain conforms to the adversarial environments definition as given above (which the behavioral axioms are based on).",
                "First, when playing a Connect-Four game, the <br>agent</br> has an intention to win the game (item 1).",
                "Second (item 2), our <br>agent</br> believes that in Connect-Four there can only be one winner (or no winner at all in the rare occurrence of a tie).",
                "In addition, our <br>agent</br> believes that its opponent to the game will try to win (item 3), and we hope it has some partial knowledge (item 4) about its adversary (this knowledge can vary from nothing, through simple facts such as age, to strategies and weaknesses).",
                "Of course, not all Connect-Four encounters are adversarial.",
                "For example, when a parent is playing the game with its child, the following situation might occur: the child, having a strong incentive to win, treats the environment as adversarial (it intends to win, understands that there can only be one winner, and believes that its parent is trying to beat him).",
                "However, the parents point of view might see the environment as an educational one, where its goal is not to win the game, but to cause enjoyment or practice strategic reasoning.",
                "In such an educational environment, a new set of behavioral axioms might be more beneficial to the parents goals than our suggested adversarial behavioral axioms. 3.2 Axiom Analysis After showing that the Connect-Four game is indeed a zero-sum, bilateral adversarial environment, the next step is to look at players behaviors during the game and check whether behaving according to our model does improve performance.",
                "To do so we have collected log files from completed Connect-Four games that were played by human players over the Internet.",
                "Our collected log file data came from Play by eMail (PBeM) sites.",
                "These are web sites that host email games, where each move is taken by an email exchange between the server and the players.",
                "Many such sites archives contain real competitive interactions, and also maintain a ranking system for their members.",
                "Most of the data we used can be found in [6].",
                "As can be learned from [1], Connect-Four has an optimal strategy and a considerable advantage for the player who starts the game (which we call the White player).",
                "We will concentrate in our analysis on the second players moves (to be called Black).",
                "The White player, being the first to act, has the so-called initiative advantage.",
                "Having the advantage and a good strategy will keep the Black player busy reacting to its moves, instead of initiating threats.",
                "A threat is a combination of three discs of the same color, with an empty spot for the fourth winning disk.",
                "An open threat is a threat that can be realized in the opponents next move.",
                "In order for the Black player to win, it must somehow turn the tide, take the advantage and start presenting threats to the White player.",
                "We will explore Black players behavior and their conformance to our axioms.",
                "To do so, we built an application that reads log files and analyzes the Black players moves.",
                "The application contains two main components: (1) a Min-Max algorithm for evaluation of moves; (2) open threats detector for the discovering of open threats.",
                "The Min-Max algorithm will work to a given depth, d and for each move α will output the heuristic value for the next action taken by the player as written in the log file, h(α), alongside the maximum heuristic value, maxh(α), that could be achieved prior to taking the move (obviously, if h(α) = maxh(α), then the player did not do the optimal move heuristically).",
                "The threat detectors job is to notify if some action was taken in order to block an open threat (not blocking an open threat will probably cause the player to lose in the opponents next move).",
                "The heuristic function used by Min-Max to evaluate the players utility is the following function, which is simple to compute, yet provides a reasonable challenge to human opponents: Definition 8.",
                "Let Group be an adjacent set of four squares that are horizontal, vertical, or diagonal.",
                "Groupn b (Groupn w) be a Group with n pieces of the black (white) color and 4−n empty squares. h = ((Group1 b ∗α)+(Group2 b ∗β)+(Group3 b ∗γ)+(Group4 b ∗∞)) − ((Group1 w ∗α)+(Group2 w ∗β)+(Group3 w ∗γ)+(Group4 w ∗∞)) The values of α, β and δ can vary to form any desired linear combination; however, it is important to value them with the α < β < δ ordering in mind (we used 1, 4, and 8 as their respective values).",
                "Groups of 4 discs of the same color means victory, thus discovery of such a group will result in ∞ to ensure an extreme value.",
                "We now use our estimated evaluation function to evaluate the Black players actions during the Connect-Four adversarial interaction.",
                "Each game from the log file was input into the application, which processed and output a reformatted log file containing the h value of the current move, the maxh value that could be achieved, and a notification if an open threat was detected.",
                "A total of 123 games were analyzed (57 with White winning, and 66 with Black winning).",
                "A few additional games were manually ignored in the experiment, due to these problems: a player abandoning the game while the outcome is not final, or a blunt irrational move in the early stages of the game (e.g., not blocking an obvious winning group in the first opening moves).",
                "In addition, a single tie game was also removed.",
                "The simulator was run to a search depth of 3 moves.",
                "We now proceed to analyze the games with respect to each behavioral axiom.",
                "The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-<br>agent</br> Systems (AAMAS 07) 555 Table 1: Average heuristic difference analysis Black losses Black Won Avg minh -17.62 -12.02 Avg 3 lowest h moves (min3 h) -13.20 -8.70 3.2.1 Affirming the Suboptimal tactical move axiom The following section presents the heuristic evaluations of the Min-Max algorithm for each action, and checks the amount and extent of suboptimal tactical actions and their implications on performance.",
                "Table 1 shows results and insights from the games heuristic analysis, when search depth equals 3 (this search depth was selected for the results to be comparable to [9], see Section 3.2.3).",
                "The tables heuristic data is the difference between the present maximal heuristic value and the heuristic value of the action that was eventually taken by the player (i.e., the closer the number is to 0, the closer the action was to the maximum heuristic action).",
                "The first row presents the difference values of the action that had the maximal difference value among all the Black players actions in a given game, as averaged over all Blacks winning and losing games (see respective columns).",
                "In games in which the Black player loses, its average difference value was -17.62, while in games in which the Black player won, its average was -12.02.",
                "The second row expands the analysis by considering the 3 highest heuristic difference actions, and averaging them.",
                "In that case, we notice an average heuristic difference of 5 points between games which the Black player loses and games in which it wins.",
                "Nevertheless, the importance of those numbers is that they allowed us to take an educated guess on a threshold number of 11.5, as the value of the TrH constant, which differentiates between normal actions and highly beneficial ones.",
                "After finding an approximated TrH constant, we can proceed with an analysis of the importance of suboptimal moves.",
                "To do so we took the subset of games in which the minimum heuristic difference value for Blacks actions was 11.5.",
                "As presented in Table 2, we can see the different min3 h average of the 3 largest ranges and the respective percentage of games won.",
                "The first row shows that the Black player won only 12% of the games in which the average of its 3 highest heuristically difference actions (min3 h) was smaller than the suggested threshold, TrH = 11.5.",
                "The second row shows a surprising result: it seems that when min3 h > −4 the Black player rarely wins.",
                "Intuition would suggest that games in which the action evaluation values were closer to the maximal values will result in more winning games for Black.",
                "However, it seems that in the Connect-Four domain, merely responding with somewhat easily expected actions, without initiating a few surprising and suboptimal moves, does not yield good results.",
                "The last row sums up the main insights from the analysis; most of Blacks wins (83%) came when its min3 h was in the range of -11.5 to -4.",
                "A close inspection of those Black winning games shows the following pattern behind the numbers: after standard opening moves, Black suddenly drops a disc into an isolated column, which seems a waste of a move.",
                "White continues to build its threats, while usually disregarding Blacks last move, which in turn uses the isolated disc as an anchor for a future winning threat.",
                "The results show that it was beneficial for the Black player Table 2: Blacks winnings percentages % of games min3 h < −11.5 12% min3 h > −4 5% −11.5 ≤ min3 h ≤ −4 83% to take suboptimal actions and not give the current highest possible heuristic value, but will not be too harmful for its position (i.e., will not give high beneficial value to its adversary).",
                "As it turned out, learning the threshold is an important aspect of success: taking wildly risky moves (min3 h < −11.5) or trying to avoid them (min3 h > −4) reduces the Black players winning chances by a large margin. 3.2.2 Affirming the Profile Monitoring Axiom In the task of showing the importance of monitoring ones adversaries profiles, our log files could not be used because they did not contain repeated interactions between players, which are needed to infer the players knowledge about their adversaries.",
                "However, the importance of opponent modeling and its use in attaining tactical advantages was already studied in various domains ([3, 9] are good examples).",
                "In a recent paper, Markovitch and Reger [9] explored the notion of learning and exploitation of opponent weakness in competitive interactions.",
                "They apply simple learning strategies by analyzing examples from past interactions in a specific domain.",
                "They also used the Connect-Four adversarial domain, which can now be used to understand the importance of monitoring the adversarys profile.",
                "Following the presentation of their theoretical model, they describe an extensive empirical study and check the agents performance after learning the weakness model with past examples.",
                "One of the domains used as a competitive environment was the same Connect-Four game (Checkers was the second domain).",
                "Their heuristic function was identical to ours with three different variations (H1, H2, and H3) that are distinguished from one another in their linear combination coefficient values.",
                "The search depth for the players was 3 (as in our analysis).",
                "Their extensive experiments check and compare various learning strategies, risk factors, predefined feature sets and usage methods.",
                "The bottom line is that the Connect-Four domain shows an improvement from a 0.556 winning rate before modeling to a 0.69 after modeling (page 22).",
                "Their conclusions, showing improved performance when holding and using the adversarys model, justify the effort to monitor the adversary profile for continuous and repeated interactions.",
                "An additional point that came up in their experiments is the following: after the opponent weakness model has been learned, the authors describe different methods of integrating the opponent weakness model into the agents decision strategy.",
                "Nevertheless, regardless of the specific method they chose to work with, all integration methods might cause the <br>agent</br> to take suboptimal decisions; it might cause the <br>agent</br> to prefer actions that are suboptimal at the present decision junction, but which might cause the opponent to react in accordance with its weakness model (as represented by our agent) which in turn will be beneficial for us in the future.",
                "The agents behavior, as demonstrated in [9] further confirms and strengthens our Suboptimal Tactical Axiom as discussed in the previous section. 556 The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-<br>agent</br> Systems (AAMAS 07) 3.2.3 Additional Insights The need for the Goal Achieving, Preventive Act, and Evaluation Maximization axioms are obvious, and need no further verification.",
                "However, even with respect to those axioms, a few interesting insights came up in the log analysis.",
                "The Goal achieving and Preventive Act axioms, though theoretically trivial, seem to provide some challenge to a human player.",
                "In the initial inspection of the logs, we encountered few games2 where a player, for inexplicable reasons, did not block the other from winning or failed to execute its own winning move.",
                "We can blame those faults on the humans lack of attention, or a typing error in its move reply; nevertheless, those errors might occur in bounded rational agents, and the appropriate behavior needs to be axiomatized.",
                "A typical Connect-Four game revolves around generating threats and blocking them.",
                "In our analysis we looked for explicit preventive actions, i.e., moves that block a group of 3 discs, or that remove a future threat (in our limited search horizon).",
                "We found that in 83% of the total games there was at least one preventive action taken by the Black player.",
                "It was also found that Black averaged 2.8 preventive actions per game on the games in which it lost, while averaging 1.5 preventive actions per game when winning.",
                "It seems that Black requires 1 or 2 preventive actions to build its initial taking position, before starting to present threats.",
                "If it did not manage to win, it will usually prevent an extra threat or two before succumbing to White. 4.",
                "RELATED WORK Much research deals with the axiomatization of teamwork and mental states of individuals: some models use knowledge and belief [10], others have models of goals and intentions [8, 4].",
                "However, all these formal theories deal with <br>agent</br> teamwork and cooperation.",
                "As far as we know, our model is the first to provide a formalized model for explicit adversarial environments and agents behavior in it.",
                "The classical Min-Max adversarial search algorithm was the first attempt to integrate the opponent into the search space with a weak assumption of an optimally playing opponent.",
                "Since then, much effort has gone into integrating the opponent model into the decision procedure to predict future behavior.",
                "The M∗ algorithm presented by Carmel and Markovitch [2] showed a method of incorporating opponent models into adversary search, while in [3] they used learning to provide a more accurate opponent model in a 2player repeated game environment, where agents strategies were modeled as finite automata.",
                "Additional Adversarial planning work was done by Willmott et al. [13], which provided an adversarial planning approach to the game of GO.",
                "The research mentioned above dealt with adversarial search and the integration of opponent models into classical utilitybased search methods.",
                "That work shows the importance of opponent modeling and the ability to exploit it to an agents advantage.",
                "However, the basic limitations of those search methods still apply; our model tries to overcome those limitations by presenting a formal model for a new, mental state-based adversarial specification. 5.",
                "CONCLUSIONS We presented an Adversarial Environment model for a 2 These were later removed from the final analysis. bounded rational <br>agent</br> that is situated in an N-player, zerosum environment.",
                "We used the SharedPlans formalization to define the model and the axioms that agents can apply as behavioral guidelines.",
                "The model is meant to be used as a guideline for designing agents that need to operate in such adversarial environments.",
                "We presented empirical results, based on ConnectFour log file analysis, that exemplify the model and the axioms for a bilateral instance of the environment.",
                "The results we presented are a first step towards an expanded model that will cover all types of adversarial environments, for example, environments that are non-zero-sum, and environments that contain natural agents that are not part of the direct conflict.",
                "Those challenges and more will be dealt with in future research. 6.",
                "ACKNOWLEDGMENT This research was supported in part by Israel Science Foundation grants #1211/04 and #898/05. 7.",
                "REFERENCES [1] L. V. Allis.",
                "A knowledge-based approach of Connect-Four - the game is solved: White wins.",
                "Masters thesis, Free University, Amsterdam, The Netherlands, 1988. [2] D. Carmel and S. Markovitch.",
                "Incorporating opponent models into adversary search.",
                "In Proceedings of the Thirteenth National Conference on Artificial Intelligence, pages 120-125, Portland, OR, 1996. [3] D. Carmel and S. Markovitch.",
                "Opponent modeling in multi-<br>agent</br> systems.",
                "In G. Weiß and S. Sen, editors, Adaptation and Learning in Multi-<br>agent</br> Systems, pages 40-52.",
                "Springer-Verlag, 1996. [4] B. J. Grosz and S. Kraus.",
                "Collaborative plans for complex group action.",
                "Artificial Intelligence, 86(2):269-357, 1996. [5] M. Hadad, G. Kaminka, G. Armon, and S. Kraus.",
                "Supporting collaborative activity.",
                "In Proc. of AAAI-2005, pages 83-88, Pittsburgh, 2005. [6] http://www.gamerz.net/˜pbmserv/. [7] S. Kraus and D. Lehmann.",
                "Designing and building a negotiating automated <br>agent</br>.",
                "Computational Intelligence, 11:132-171, 1995. [8] H. J. Levesque, P. R. Cohen, and J. H. T. Nunes.",
                "On acting together.",
                "In Proc. of AAAI-90, pages 94-99, Boston, MA, 1990. [9] S. Markovitch and R. Reger.",
                "Learning and exploiting relative weaknesses of opponent agents.",
                "Autonomous Agents and Multi-<br>agent</br> Systems, 10(2):103-130, 2005. [10] Y. M. Ronald Fagin, Joseph Y. Halpern and M. Y. Vardi.",
                "Reasoning about knowledge.",
                "MIT Press, Cambridge, Mass., 1995. [11] P. Thagard.",
                "Adversarial problem solving: Modeling an oponent using explanatory coherence.",
                "Cognitive Science, 16(1):123-149, 1992. [12] R. W. Toseland and R. F. Rivas.",
                "An Introduction to Group Work Practice.",
                "Prentice Hall, Englewood Cliffs, NJ, 2nd edition edition, 1995. [13] S. Willmott, J. Richardson, A. Bundy, and J. Levine.",
                "An adversarial planning approach to Go.",
                "Lecture Notes in Computer Science, 1558:93-112, 1999.",
                "The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-<br>agent</br> Systems (AAMAS 07) 557"
            ],
            "original_annotated_samples": [],
            "translated_annotated_samples": [
                "Definimos un entorno adversario al describir los estados mentales de un \"agente\" en dicho entorno.agente",
                "Sin embargo, ninguna de esta investigación trató con dominios adversos y sus implicaciones para el comportamiento de \"agente\".agente",
                "Exploramos las propiedades del medio ambiente y los estados mentales de los agentes para derivar axiomas conductuales;Estos axiomas conductuales constituyen un modelo formal que sirve como una guía de especificación y diseño para el diseño de \"agente\" en tales entornos.agente",
                "Entornos adversos El modelo de entorno adversario (denotado como AE) está destinado a guiar el diseño de agentes proporcionando una especificación de las capacidades y actitudes mentales de un \"agente\" en un entorno adversario.agente",
                "En particular, nuestro modelo de entorno adversario se ocupará de las interacciones que consisten en n agentes (n ≥ 2), donde todos los agentes son adversarios, y solo un \"agente\" puede tener éxito.agente",
                "Ejemplos de tales entornos van desde juegos de mesa (por ejemplo, ajedrez, conexión y diplomacia) hasta ciertos entornos económicos (por ejemplo, subastas de N-bidder en un solo bien).2.1 Descripción general del modelo Nuestro enfoque es formalizar las actitudes y comportamientos mentales de un solo \"agente\" adversario;Consideramos cómo un solo \"agente\" percibe el AE.agente",
                "La siguiente lista especifica las condiciones y los estados mentales de un \"agente\" en una AE simple y cero: 1. Agente",
                "El \"agente\" tiene una intención individual de que su propio objetivo se completará;2. Agente",
                "El \"agente\" tiene una creencia individual de que él y sus adversarios están buscando objetivos conflictivos completos (definidos a continuación) puede haber solo un ganador;3. Agente",
                "El \"agente\" tiene una creencia individual de que cada adversario tiene la intención de completar su propio objetivo conflictivo completo;4. Agente",
                "El \"agente\" tiene una creencia individual en el perfil (parcial) de sus adversarios.agente",
                "Se requiere el ítem 3, ya que podría ser el caso de que algún \"agente\" tenga un objetivo conflictivo completo, y actualmente está considerando adoptar la intención de completarlo, pero hasta el momento, no está comprometido a lograrlo.agente",
                "Esto podría ocurrir porque el \"agente\" aún no ha deliberado sobre los efectos que la adopción de esa intención podría tener en las otras intenciones que está teniendo actualmente.agente",
                "El ítem 4 establece que el \"agente\" debería mantener una creencia sobre los perfiles de sus adversarios.agente",
                "El perfil representa todo el conocimiento que el \"agente\" tiene sobre su adversario: sus debilidades, capacidades estratégicas, objetivos, intenciones, confiabilidad y más.agente",
                "Recordamos aquí algunos de los predicados y operadores que se utilizan en esa formalización: int.to (ai, α, tn, tα, c) representan las intenciones de AIS en el momento TN para hacer una acción α en el tiempo Tα en el contexto de C.Int.th (ai, prop, tn, tprop, c) representa las intenciones de AIS en el momento tn que una cierta proposición se mantiene en el tiempo tprop en el contexto de C. los operadores de intención potenciales, pot.int.to (...)y Pot.int.th (...), se usan para representar el estado mental cuando un \"agente\" considera adoptar una intención, pero no ha deliberado sobre la interacción de las otras intenciones que tiene.agente",
                "El operador Bel (AI, F, TF) representa el \"agente\" que cree en la declaración expresada en la Fórmula F, en el tiempo TF.agente",
                "Una función de utilidad bajo esta formalización se define como un mapeo de un posible mundo w ∈ W a un elemento en, lo que expresa la conveniencia del mundo, desde una sola perspectiva de \"agente\".agente",
                "La siguiente lista especifica nuevos predicados, funciones, variables y constantes utilizadas junto con las definiciones originales para la formalización del entorno adversario: 1. φ es una acción nula (el \"agente\" no hace nada).2. Agente",
                "GAI es el conjunto de objetivos de AIS \"agente\".agente",
                "Cada objetivo es un conjunto de predicados cuya satisfacción complete el objetivo (usamos G ∗ ai ∈ Gai para representar un objetivo arbitrario de \"agente\" ai).3. GAI es el conjunto de subconsportes de AIS \"agente\".agente",
                "P AJ AI es el objeto de perfil \"agente\" AI contiene sobre \"agente\" AJ.5. Agente",
                "Cai ⊆ Ca es el conjunto de acciones posibles de \"agente\".6. Agente",
                "El perfil (ai, pai ai) es cierto cuando \"agente\" ai contiene un perfil de objeto para \"agente\" AJ.agente",
                "Conf.en agentes autónomos y sistemas múltiples \"agentes\" (AAMAS 07) 551 Conocimiento \"Agente\" que la IA tiene en el perfil de Agent AJ, en el momento TN.agente",
                "Cuanto mayor sea el valor, más conocimiento tiene \"agente\" de conocimiento.agente",
                "Eval: esta función de evaluación devuelve un valor de utilidad esperado estimado para un \"agente\" en A, después de completar una acción de CA en algún estado mundial w.Eval: A × CA × W → Definición 4. Agente",
                "El uso del valor de utilidad real para un \"agente\" racional daría fácilmente el mejor resultado para ese \"agente\".agente",
                "Por lo tanto, después de que el objetivo se haya cumplido, no puede haber acciones futuras que puedan poner al \"agente\" en un estado mundial con un valor de evaluación más alto.(∀ai, g ∗ ai, α, β ∈ Cai, w ∈ W) lograr (g ∗ ai, α, w) ⇒ eval (ai, α, w) ≥ eval (ai, β, w) propiedad 2. agente",
                "SetAction Definimos una acción establecida (setAction) como un conjunto de operaciones de acción (acciones complejas o básicas) de algunas acciones Contintos CAI y CAJ que, según la creencia de \"agente\", están unidos por una relación temporal y consecuente, formando, formandouna cadena de eventos (acción y su siguiente acción consecuente).(∀α1, ..., αu ∈ Cai, β1, ..., βV ∈ Caj, w ∈ W) setAction (α1, ..., αu, β1, ..., βV, w) ⇒ ((do(Ai, α1, tα1, w) ⇒ do (AJ, β1, Tβ1, W)) ⇒ do (ai, α2, tα2, w) ⇒ ... ⇒ do (ai, αu, tαu, w)) lo consecuencialLa relación puede existir debido a diversas restricciones ambientales (cuando una acción obliga al adversario a responder con una acción específica) o debido al conocimiento de los agentes sobre el perfil de su adversario.agente",
                "La satisfacción de estos axiomas significa que el \"agente\" está situado en dicho entorno.agente",
                "Proporciona especificaciones para que el \"agente\" AAG interactúe con su conjunto de adversarios a con respecto a los objetivos G ∗ AAG y G ∗ A en el tiempo TCO en algún estado mundial w.Ae (aag, a, g ∗ aag, a1, ..., ak, g ∗ a1, ..., g ∗ ak, tn, w) 1. agente",
                "AAG tiene creencias sobre los perfiles (parciales) de sus adversarios (∀ao ∈ {a1, ..., ak}) (∃pao aag ∈ Paag) bel (aag, perfil (ao, pao aag), tn) para construir un\"Agente\" que podrá operar con éxito dentro de tal AE, debemos especificar las pautas de comportamiento para sus interacciones.agente",
                "Los axiomas de comportamiento representan situaciones en las que el \"agente\" adoptará intenciones potenciales para (pot.int.to (...)) realizar una acción, que generalmente requerirá un razonamiento de extremo de medios para seleccionar un posible curso de acción.agente",
                "El primer axioma es el caso más simple;Cuando el \"agente\" AAG cree que se trata de una acción (α) de lograr su objetivo conflictivo G ∗ AAG, debe adoptar la intención potencial de hacer α y completar su objetivo.(∀AAG, α ∈ Caag, TN, Tα, W ∈ W) (Bel (AAG, DO (AAG, α, Tα, W) ⇒ Logro (G ∗ AAG, α, W)) ⇒ Pot.int.to ((AAG, α, TN, Tα, W) Este comportamiento algo trivial es el primer y más fuerte axioma. Agente.",
                "En cualquier situación, cuando el \"agente\" está a una acción lejos de completar el objetivo, debe completar la acción.agente",
                "Sin embargo, sin la axiomatización explícita de dicho comportamiento, podría haber situaciones en las que el \"agente\" decidirá tomar otra acción por varias razones, debido a sus recursos de decisión limitados.agente",
                "Al estar en una situación adversa, el \"agente\" AAG podría decidir tomar medidas que dañen uno de sus planes de adversario para completar su objetivo, incluso si esas acciones no avanzan explícitamente a AAG hacia su objetivo conflictivo G ∗ AAG.agente",
                "Dicha acción preventiva tendrá lugar cuando el \"agente\" AAG cree sobre la posibilidad de que su adversario AO haga una acción β que le dará un alto 552 el sexto INTL.agente",
                "Conf.en agentes autónomos y sistemas múltiples \"agentes\" (AAMAS 07) Valor de evaluación de utilidad (> TRH).agente",
                "Una instancia específica de A1 ocurre cuando el adversario está a una acción de lograr su objetivo, y el \"agente\" debe tomar medidas preventivas inmediatas.agente",
                "Formalmente, tenemos las mismas creencias que se indican anteriormente, con una creencia cambiada de que hacer una acción β hará que el \"agente\" AO logre su objetivo.agente",
                "En muchos escenarios, puede ocurrir una situación en la que un \"agente\" decidirá no tomar la acción más beneficiosa actual que puede tomar (la acción con el valor de evaluación máxima de utilidad), porque cree que tomar otra acción (con un valor de evaluación de utilidad más bajo)podría producir (dependiendo de la respuesta de los adversarios) una posibilidad futura para una acción altamente beneficiosa.agente",
                "Dicho formalmente, el \"agente\" AAG cree en una cierta medida que evolucionará de acuerdo con su acción inicial y producirá un alto valor beneficioso (> TRH) únicamente para ello.(∀aag, ao ∈ A, tn, w ∈ W) (∃α1, ..., αu ∈ Cai, β1, ..., βv ∈ Caj, tα1) bel (AAG, setAction (α1, ...,.,,,,,,,,,.αu, β1, ..., βV), tn) ∧ bel (aag, eval (ao, βv, w) <trh <eval (aag, αu, w), tn) ⇒ pot.int.to (AAG, α1, TN, Tα1, W) Un \"agente\" podría creer que una cadena de eventos ocurrirá por varias razones debido a la naturaleza inevitable del dominio.agente",
                "El \"agente\" también podría creer en una cadena de eventos basados en su conocimiento de su perfil de adversario, lo que le permite prever los movimientos de los adversarios con alta precisión.agente",
                "El \"agente\" puede ajustar sus perfiles de adversario mediante observaciones y estudio de patrones (específicamente, si hay encuentros repetidos con el mismo adversario).agente",
                "Sin embargo, en lugar de esperar a que se revele la información del perfil, un \"agente\" también puede iniciar acciones que obligarán a su adversario a reaccionar de una manera que revele el conocimiento del perfil al respecto.agente",
                "Formalmente, el axioma establece que si todas las acciones (γ) no son acciones altamente beneficiosas (<trh), el \"agente\" puede hacer la acción α en el tiempo Tα si cree que dará como resultado una acción no altamente beneficiosa β de suEl adversario, que a su vez, le enseña sobre el perfil de los adversarios, es decir, ofrece un mayor advnow (P AJ AI, Tβ).(∀Aag, ao ∈ A, α ∈ Caag, β ∈ Cao, TN, Tα, Tβ, W ∈ W) Bel (AAG, (∀γ ∈ Caag) Eval (AAG, γ, W) <TRH, TN) ∧BEL (AAG, DO (AAG, α, Tα, W) ⇒ Do (AO, β, Tβ, W), TN) ∧ Bel (AAG, Eval (AO, β, W) <TRH) ∧ Bel (AAG, ADVWOK(P AJ AI, Tβ)> advknow (P AJ AI, TN), TN) ⇒ Pot.int.to (AAG, α, TN, Tα, W) Por ejemplo, volver al escenario del juego de tablero, considerar comenzarUn juego versus un oponente sobre el que no sabemos nada, ni siquiera si es un oponente humano o computarizado.agente",
                "Al Val: el valor de evaluación total que alcanzará el \"agente\" AI mientras es parte de AAL es la suma de Evali (Eval para AI) de cada uno de los valores de evaluación de AAL J después de tomar sus propias acciones α (a través del \"agente\" (α (α) predicado): Al Val (Ai, Cal, Aal, W) = α∈Cal Evali (Aal J, Agente (α), W) Definición 7. Agente",
                "Conf.en agentes autónomos y sistemas múltiples \"agentes\" (AAMAS 07) umbral 553;Por encima de él, se puede decir que la alianza es una alianza altamente beneficiosa.agente",
                "Debemos tener en cuenta que un \"agente\" puede ser simultáneamente parte de más de una alianza.agente",
                "Este axioma establece que un \"agente\" considerará tomar medidas que reducirán su valor de evaluación (a cierto límite inferior), si cree que un socio grupal obtendrá un beneficio significativo.agente",
                "Deje que AE AG sea un \"agente\" racional no resistido utilizando la función de evaluación heurística Eval, AU AG es el mismo \"agente\" que usa la función de utilidad verdadera, y AO es un único adversario racional basado en la utilidad ilimitada.agente",
                "El \"agente\" de AE AG seleccionará α u otra acción con el mismo valor de utilidad a través de A1.agente",
                "Ese axioma no se aplicará, y el \"agente\" procederá con A6 (A5 se ignorará porque la interacción es bilateral).(A6) Axioma de maximización de la evaluación: este axioma seleccionará la evaluación máxima para AE AG.agente",
                "Esta sección investiga si se limita 1 un caso en el que después de la finalización de β existe una γ que proporciona una alta utilidad para el \"agente\" au ag, no puede ocurrir porque AO usa la misma utilidad, y la existencia de γs hará que clasifique β como un bajoacción de utilidad.554 El sexto intl.agente",
                "Conf.En agentes autónomos y sistemas múltiples \"agentes\" (AAMAS 07), los agentes racionales situados en tales entornos adversos serán mejor aplicar nuestros axiomas de comportamiento sugeridos.3.1 El dominio para explorar el uso del modelo anterior y sus axiomas de comportamiento, decidimos usar el juego Connect-Four como nuestro entorno adversario.agente",
                "Antes de que podamos continuar con la verificación del \"agente\", primero debemos verificar que el dominio se ajuste a la definición de entornos adversos como se indicó anteriormente (en los que se basan los axiomas conductuales).agente",
                "Primero, cuando juega un juego Connect-Four, el \"Agente\" tiene la intención de ganar el juego (ítem 1).agente",
                "Segundo (ítem 2), nuestro \"agente\" cree que en Connect Four solo puede haber un ganador (o ningún ganador en la rara ocurrencia de un empate).agente",
                "Además, nuestro \"agente\" cree que su oponente al juego intentará ganar (ítem 3), y esperamos que tenga algún conocimiento parcial (ítem 4) sobre su adversario (este conocimiento puede variar de la nada, a través de hechos simples como talescomo edad, a estrategias y debilidades).agente",
                "Conf.en agentes autónomos y sistemas múltiples de \"agente\" (aamas 07) 555 Tabla 1: Análisis promedio de diferencia heurística Pérdidas negras Negras Won AVG Minh -17.62 -12.02 AVG 3 Movimientos H más bajos (Min3 h) -13.20 -8.70 3.2.1 Afirmación de la afirmación de la afirmación de la afirmación de la afirmación de la afirmación de la afirmación delAxioma de movimiento táctico subóptimo La siguiente sección presenta las evaluaciones heurísticas del algoritmo Min-Max para cada acción, y verifica la cantidad y el alcance de las acciones tácticas subóptimas y sus implicaciones en el rendimiento.agente",
                "Sin embargo, independientemente del método específico con el que eligieron trabajar, todos los métodos de integración pueden hacer que el \"agente\" tome decisiones subóptimas;Puede hacer que el \"agente\" prefiera acciones que sean subóptimas en la unión de decisión actual, pero que puede hacer que el oponente reaccione de acuerdo con su modelo de debilidad (como lo representa nuestro agente), lo que a su vez será beneficioso para nosotros en elfuturo.agente",
                "Conf.En agentes autónomos y sistemas múltiples de \"agente\" (AAMAS 07) 3.2.3 Conocimientos adicionales La necesidad de lograr el objetivo, el acto preventivo y los axiomas de maximización de la evaluación son obvios y no necesitan más verificación.agente",
                "Sin embargo, todas estas teorías formales tratan con el trabajo en equipo y la cooperación de \"agente\".agente",
                "Conclusiones Presentamos un modelo de entorno adversario para un 2 Estas luego se eliminaron del análisis final.\"Agente\" racional limitado que está situado en un entorno N-jugador n Zerosum.agente",
                "Modelado del oponente en sistemas multi-\"agentes\".agente",
                "En G. Weiß y S. Sen, editores, adaptación y aprendizaje en sistemas multi-\"agentes\", páginas 40-52.agente",
                "Diseño y construcción de un \"agente\" automatizado.agente",
                "Agentes autónomos y sistemas múltiples \"agentes\", 10 (2): 103-130, 2005. [10] Y. M. Ronald Fagin, Joseph Y. Halpern y M. Y. Vardi.agente",
                "Conf.en agentes autónomos y sistemas múltiples de \"agente\" (AAMAS 07) 557 Agente"
            ],
            "translated_text": "",
            "candidates": [],
            "error": []
        },
        "multiagent system": {
            "translated_key": "",
            "is_in_text": true,
            "original_annotated_sentences": [
                "An Adversarial Environment Model for Bounded Rational Agents in Zero-Sum Interactions Inon Zuckerman1 , Sarit Kraus1 , Jeffrey S. Rosenschein2 , Gal Kaminka1 1 Department of Computer Science 2 The School of Engineering Bar-Ilan University and Computer Science Ramat-Gan, Israel Hebrew University, Jerusalem, Israel {zukermi,sarit,galk}@cs.biu.ac.il jeff@cs.huji.ac.il ABSTRACT Multiagent environments are often not cooperative nor collaborative; in many cases, agents have conflicting interests, leading to adversarial interactions.",
                "This paper presents a formal Adversarial Environment model for bounded rational agents operating in a zero-sum environment.",
                "In such environments, attempts to use classical utility-based search methods can raise a variety of difficulties (e.g., implicitly modeling the opponent as an omniscient utility maximizer, rather than leveraging a more nuanced, explicit opponent model).",
                "We define an Adversarial Environment by describing the mental states of an agent in such an environment.",
                "We then present behavioral axioms that are intended to serve as design principles for building such adversarial agents.",
                "We explore the application of our approach by analyzing log files of completed Connect-Four games, and present an empirical analysis of the axioms appropriateness.",
                "Categories and Subject Descriptors I.2.11 [Artificial Intelligence]: Distributed Artificial Intelligence-Intelligent agents,Multiagent Systems; I.2.4 [Artificial Intelligence]: Knowledge Representation Formalisms and Methods -Modal logic General Terms Design, Theory 1.",
                "INTRODUCTION Early research in <br>multiagent system</br>s (MAS) considered cooperative groups of agents; because individual agents had limited resources, or limited access to information (e.g., limited processing power, limited sensor coverage), they worked together by design to solve problems that individually they could not solve, or at least could not solve as efficiently.",
                "MAS research, however, soon began to consider interacting agents with individuated interests, as representatives of different humans or organizations with non-identical interests.",
                "When interactions are guided by diverse interests, participants may have to overcome disagreements, uncooperative interactions, and even intentional attempts to damage one another.",
                "When these types of interactions occur, environments require appropriate behavior from the agents situated in them.",
                "We call these environments Adversarial Environments, and call the clashing agents Adversaries.",
                "Models of cooperation and teamwork have been extensively explored in MAS through the axiomatization of mental states (e.g., [8, 4, 5]).",
                "However, none of this research dealt with adversarial domains and their implications for agent behavior.",
                "Our paper addresses this issue by providing a formal, axiomatized mental state model for a subset of adversarial domains, namely simple zero-sum adversarial environments.",
                "Simple zero-sum encounters exist of course in various twoplayer games (e.g., Chess, Checkers), but they also exist in n-player games (e.g., Risk, Diplomacy), auctions for a single good, and elsewhere.",
                "In these latter environments especially, using a utility-based adversarial search (such as the Min-Max algorithm) does not always provide an adequate solution; the payoff function might be quite complex or difficult to quantify, and there are natural computational limitations on bounded rational agents.",
                "In addition, traditional search methods (like Min-Max) do not make use of a model of the opponent, which has proven to be a valuable addition to adversarial planning [9, 3, 11].",
                "In this paper, we develop a formal, axiomatized model for bounded rational agents that are situated in a zero-sum adversarial environment.",
                "The model uses different modality operators, and its main foundations are the SharedPlans [4] model for collaborative behavior.",
                "We explore environment properties and the mental states of agents to derive behavioral axioms; these behavioral axioms constitute a formal model that serves as a specification and design guideline for agent design in such settings.",
                "We then investigate the behavior of our model empirically using the Connect-Four board game.",
                "We show that this game conforms to our environment definition, and analyze players behavior using a large set of completed match log 550 978-81-904262-7-5 (RPS) c 2007 IFAAMAS files.",
                "In addition, we use the results presented in [9] to discuss the importance of opponent modeling in our ConnectFour adversarial domain.",
                "The paper proceeds as follows.",
                "Section 2 presents the models formalization.",
                "Section 3 presents the empirical analysis and its results.",
                "We discuss related work in Section 4, and conclude and present future directions in Section 5. 2.",
                "ADVERSARIAL ENVIRONMENTS The adversarial environment model (denoted as AE) is intended to guide the design of agents by providing a specification of the capabilities and mental attitudes of an agent in an adversarial environment.",
                "We focus here on specific types of adversarial environments, specified as follows: 1.",
                "Zero-Sum Interactions: positive and negative utilities of all agents sum to zero; 2.",
                "Simple AEs: all agents in the environment are adversarial agents; 3.",
                "Bilateral AEs: AEs with exactly two agents; 4.",
                "Multilateral AEs: AEs of three or more agents.",
                "We will work on both bilateral and multilateral instantiations of zero-sum and simple environments.",
                "In particular, our adversarial environment model will deal with interactions that consist of N agents (N ≥ 2), where all agents are adversaries, and only one agent can succeed.",
                "Examples of such environments range from board games (e.g., Chess, Connect-Four, and Diplomacy) to certain economic environments (e.g., N-bidder auctions over a single good). 2.1 Model Overview Our approach is to formalize the mental attitudes and behaviors of a single adversarial agent; we consider how a single agent perceives the AE.",
                "The following list specifies the conditions and mental states of an agent in a simple, zero-sum AE: 1.",
                "The agent has an individual intention that its own goal will be completed; 2.",
                "The agent has an individual belief that it and its adversaries are pursuing full conflicting goals (defined below)there can be only one winner; 3.",
                "The agent has an individual belief that each adversary has an intention to complete its own full conflicting goal; 4.",
                "The agent has an individual belief in the (partial) profile of its adversaries.",
                "Item 3 is required, since it might be the case that some agent has a full conflicting goal, and is currently considering adopting the intention to complete it, but is, as of yet, not committed to achieving it.",
                "This might occur because the agent has not yet deliberated about the effects that adopting that intention might have on the other intentions it is currently holding.",
                "In such cases, it might not consider itself to even be in an adversarial environment.",
                "Item 4 states that the agent should hold some belief about the profiles of its adversaries.",
                "The profile represents all the knowledge the agent has about its adversary: its weaknesses, strategic capabilities, goals, intentions, trustworthiness, and more.",
                "It can be given explicitly or can be learned from observations of past encounters. 2.2 Model Definitions for Mental States We use Grosz and Krauss definitions of the modal operators, predicates, and meta-predicates, as defined in their SharedPlan formalization [4].",
                "We recall here some of the predicates and operators that are used in that formalization: Int.To(Ai, α, Tn, Tα, C) represents Ais intentions at time Tn to do an action α at time Tα in the context of C. Int.Th(Ai, prop, Tn, Tprop, C) represents Ais intentions at time Tn that a certain proposition prop holds at time Tprop in the context of C. The potential intention operators, Pot.Int.To(...) and Pot.Int.Th(...), are used to represent the mental state when an agent considers adopting an intention, but has not deliberated about the interaction of the other intentions it holds.",
                "The operator Bel(Ai, f, Tf ) represents agent Ai believing in the statement expressed in formula f, at time Tf .",
                "MB(A, f, Tf ) represents mutual belief for a group of agents A.",
                "A snapshot of the system finds our environment to be in some state e ∈ E of environmental variable states, and each adversary in any LAi ∈ L of possible local states.",
                "At any given time step, the system will be in some world w of the set of all possible worlds w ∈ W, where w = E×LA1 ×LA2 × ...LAn , and n is the number of adversaries.",
                "For example, in a Texas Holdem poker game, an agents local state might be its own set of cards (which is unknown to its adversary) while the environment will consist of the betting pot and the community cards (which are visible to both players).",
                "A utility function under this formalization is defined as a mapping from a possible world w ∈ W to an element in , which expresses the desirability of the world, from a single agent perspective.",
                "We usually normalize the range to [0,1], where 0 represents the least desirable possible world, and 1 is the most desirable world.",
                "The implementation of the utility function is dependent on the domain in question.",
                "The following list specifies new predicates, functions, variables, and constants used in conjunction with the original definitions for the adversarial environment formalization: 1. φ is a null action (the agent does not do anything). 2.",
                "GAi is the set of agent Ais goals.",
                "Each goal is a set of predicates whose satisfaction makes the goal complete (we use G∗ Ai ∈ GAi to represent an arbitrary goal of agent Ai). 3. gAi is the set of agent Ais subgoals.",
                "Subgoals are predicates whose satisfaction represents an important milestone toward achievement of the full goal. gG∗ Ai ⊆ gAi is the set of subgoals that are important to the completion of goal G∗ Ai (we will use g∗ G∗ Ai ∈ gG∗ Ai to represent an arbitrary subgoal). 4.",
                "P Aj Ai is the profile object agent Ai holds about agent Aj. 5.",
                "CA is a general set of actions for all agents in A which are derived from the environments constraints.",
                "CAi ⊆ CA is the set of agent Ais possible actions. 6.",
                "Do(Ai, α, Tα, w) holds when Ai performs action α over time interval Tα in world w. 7.",
                "Achieve(G∗ Ai , α, w) is true when goal G∗ Ai is achieved following the completion of action α in world w ∈ W, where α ∈ CAi . 8.",
                "Profile(Ai, PAi Ai ) is true when agent Ai holds an object profile for agent Aj.",
                "Definition 1.",
                "Full conflict (FulConf ) describes a zerosum interaction where only a single goal of the goals in conflict can be completed.",
                "FulConf(G∗ Ai , G∗ Aj ) ⇒ (∃α ∈ CAi , ∀w, β ∈ CAj ) (Achieve(G∗ Ai , α, w) ⇒ ¬Achieve(G∗ Aj , β, w)) ∨ (∃β ∈ CAj , ∀w, α ∈ CAi )(Achieve(G∗ Aj , β, w) ⇒ ¬Achieve(G∗ Ai , α, w)) Definition 2.",
                "Adversarial Knowledge (AdvKnow) is a function returning a value which represents the amount of The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) 551 knowledge agent Ai has on the profile of agent Aj, at time Tn.",
                "The higher the value, the more knowledge agent Ai has.",
                "AdvKnow : P Aj Ai × Tn → Definition 3.",
                "Eval - This evaluation function returns an estimated expected utility value for an agent in A, after completing an action from CA in some world state w. Eval : A × CA × w → Definition 4.",
                "TrH - (Threshold) is a numerical constant in the [0,1] range that represents an evaluation function (Eval) threshold value.",
                "An action that yields an estimated utility evaluation above the TrH is regarded as a highly beneficial action.",
                "The Eval value is an estimation and not the real utility function, which is usually unknown.",
                "Using the real utility value for a rational agent would easily yield the best outcome for that agent.",
                "However, agents usually do not have the real utility functions, but rather a heuristic estimate of it.",
                "There are two important properties that should hold for the evaluation function: Property 1.",
                "The evaluation function should state that the most desirable world state is one in which the goal is achieved.",
                "Therefore, after the goal has been satisfied, there can be no future action that can put the agent in a world state with higher Eval value. (∀Ai, G∗ Ai , α, β ∈ CAi , w ∈ W) Achieve(G∗ Ai , α, w) ⇒ Eval(Ai, α, w) ≥ Eval(Ai, β, w) Property 2.",
                "The evaluation function should project an action that causes a completion of a goal or a subgoal to a value which is greater than TrH (a highly beneficial action). (∀Ai, G∗ Ai ∈ GAi , α ∈ CAi , w ∈ W, g∗ GAi ∈ gGAi ) Achieve(G∗ Ai , α, w) ∨ Achieve(g∗ GAi , α, w) ⇒ Eval(Ai, α, w) ≥ TrH.",
                "Definition 5.",
                "SetAction We define a set action (SetAction) as a set of action operations (either complex or basic actions) from some action sets CAi and CAj which, according to agent Ais belief, are attached together by a temporal and consequential relationship, forming a chain of events (action, and its following consequent action). (∀α1 , . . . , αu ∈ CAi , β1 , . . . , βv ∈ CAj , w ∈ W) SetAction(α1 , . . . , αu , β1 , . . . , βv , w) ⇒ ((Do(Ai, α1 , Tα1 , w) ⇒ Do(Aj, β1 , Tβ1 , w)) ⇒ Do(Ai, α2 , Tα2 , w) ⇒ . . . ⇒ Do(Ai, αu , Tαu , w)) The consequential relation might exist due to various environmental constraints (when one action forces the adversary to respond with a specific action) or due to the agents knowledge about the profile of its adversary.",
                "Property 3.",
                "As the knowledge we have about our adversary increases we will have additional beliefs about its behavior in different situations which in turn creates new set actions.",
                "Formally, if our AdvKnow at time Tn+1 is greater than AdvKnow at time Tn, then every SetAction known at time Tn is also known at time Tn+1.",
                "AdvKnow(P Aj Ai , Tn+1) > AdvKnow(P Aj Ai , Tn) ⇒ (∀α1 , . . . , αu ∈ CAi , β1 , . . . , βv ∈ CAj ) Bel(Aag, SetAction(α1 , . . . , αu , β1 , . . . , βv ), Tn) ⇒ Bel(Aag, SetAction(α1 , . . . , αu , β1 , . . . , βv ), Tn+1) 2.3 The Environment Formulation The following axioms provide the formal definition for a simple, zero-sum Adversarial Environment (AE).",
                "Satisfaction of these axioms means that the agent is situated in such an environment.",
                "It provides specifications for agent Aag to interact with its set of adversaries A with respect to goals G∗ Aag and G∗ A at time TCo at some world state w. AE(Aag, A, G∗ Aag , A1, . . . , Ak, G∗ A1 , . . . , G∗ Ak , Tn, w) 1.",
                "Aag has an Int.Th his goal would be completed: (∃α ∈ CAag , Tα) Int.Th(Aag, Achieve(G∗ Aag , α), Tn, Tα, AE) 2.",
                "Aag believes that it and each of its adversaries Ao are pursuing full conflicting goals: (∀Ao ∈ {A1, . . . , Ak}) Bel(Aag, FulConf(G∗ Aag , G∗ Ao ), Tn) 3.",
                "Aag believes that each of his adversaries in Ao has the Int.Th his conflicting goal G∗ Aoi will be completed: (∀Ao ∈ {A1, . . . , Ak})(∃β ∈ CAo , Tβ) Bel(Aag, Int.Th(Ao, Achieve(G∗ Ao , β), TCo, Tβ, AE), Tn) 4.",
                "Aag has beliefs about the (partial) profiles of its adversaries (∀Ao ∈ {A1, . . . , Ak}) (∃PAo Aag ∈ PAag )Bel(Aag, Profile(Ao, PAo Aag ), Tn) To build an agent that will be able to operate successfully within such an AE, we must specify behavioral guidelines for its interactions.",
                "Using a naive Eval maximization strategy to a certain search depth will not always yield satisfactory results for several reasons: (1) the search horizon problem when searching for a fixed depth; (2) the strong assumption of an optimally rational, unbounded resources adversary; (3) using an estimated evaluation function which will not give optimal results in all world states, and can be exploited [9].",
                "The following axioms specify the behavioral principles that can be used to differentiate between successful and less successful agents in the above Adversarial Environment.",
                "Those axioms should be used as specification principles when designing and implementing agents that should be able to perform well in such Adversarial Environments.",
                "The behavioral axioms represent situations in which the agent will adopt potential intentions to (Pot.Int.To(...)) perform an action, which will typically require some means-end reasoning to select a possible course of action.",
                "This reasoning will lead to the adoption of an Int.To(...) (see [4]).",
                "A1.",
                "Goal Achieving Axiom.",
                "The first axiom is the simplest case; when the agent Aag believes that it is one action (α) away from achieving his conflicting goal G∗ Aag , it should adopt the potential intention to do α and complete its goal. (∀Aag, α ∈ CAag , Tn, Tα, w ∈ W) (Bel(Aag, Do(Aag, α, Tα, w) ⇒ Achieve(G∗ Aag , α, w)) ⇒ Pot.Int.To(Aag, α, Tn, Tα, w) This somewhat trivial behavior is the first and strongest axiom.",
                "In any situation, when the agent is an action away from completing the goal, it should complete the action.",
                "Any fair Eval function would naturally classify α as the maximal value action (property 1).",
                "However, without explicit axiomatization of such behavior there might be situations where the agent will decide on taking another action for various reasons, due to its bounded decision resources.",
                "A2.",
                "Preventive Act Axiom.",
                "Being in an adversarial situation, agent Aag might decide to take actions that will damage one of its adversarys plans to complete its goal, even if those actions do not explicitly advance Aag towards its conflicting goal G∗ Aag .",
                "Such preventive action will take place when agent Aag has a belief about the possibility of its adversary Ao doing an action β that will give it a high 552 The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) utility evaluation value (> TrH).",
                "Believing that taking action α will prevent the opponent from doing its β, it will adopt a potential intention to do α. (∀Aag, Ao ∈ A, α ∈ CAag , β ∈ CAo , Tn, Tβ, w ∈ W) (Bel(Aag, Do(Ao, β, Tβ, w) ∧ Eval(Ao, β, w) > TrH, Tn) ∧ Bel(Aag, Do(Aag, α, Tα, w) ⇒ ¬Do(Ao, β, Tβ, w), Tn) ⇒ Pot.Int.To(Aag, α, Tn, Tα, w) This axiom is a basic component of any adversarial environment.",
                "For example, looking at a Chess board game, a player could realize that it is about to be checkmated by its opponent, thus making a preventive move.",
                "Another example is a Connect Four game: when a player has a row of three chips, its opponent must block it, or lose.",
                "A specific instance of A1 occurs when the adversary is one action away from achieving its goal, and immediate preventive action needs to be taken by the agent.",
                "Formally, we have the same beliefs as stated above, with a changed belief that doing action β will cause agent Ao to achieve its goal.",
                "Proposition 1: Prevent or lose case. (∀Aag, Ao ∈ A, α ∈ CAag , β ∈ CAo , G∗ Ao , Tn, Tα, Tβ, w ∈ W) Bel(Aag, Do(Ao, β, Tβ, w) ⇒ Achieve(G∗ Ao , β, w), Tn) ∧ Bel(Aag, Do(Aag, α, Tα, w) ⇒ ¬Do(Ao, β, Tβ, w)) ⇒ Pot.Int.To(Aag, α, Tn, Tα, w) Sketch of proof: Proposition 1 can be easily derived from axiom A1 and the property 2 of the Eval function, which states that any action that causes a completion of a goal is a highly beneficial action.",
                "The preventive act behavior will occur implicitly when the Eval function is equal to the real world utility function.",
                "However, being bounded rational agents and dealing with an estimated evaluation function we need to explicitly axiomatize such behavior, for it will not always occur implicitly from the evaluation function.",
                "A3.",
                "Suboptimal Tactical Move Axiom.",
                "In many scenarios a situation may occur where an agent will decide not to take the current most beneficial action it can take (the action with the maximal utility evaluation value), because it believes that taking another action (with lower utility evaluation value) might yield (depending on the adversarys response) a future possibility for a highly beneficial action.",
                "This will occur most often when the Eval function is inaccurate and differs by a large extent from the Utility function.",
                "Put formally, agent Aag believes in a certain SetAction that will evolve according to its initial action and will yield a high beneficial value (> TrH) solely for it. (∀Aag, Ao ∈ A, Tn, w ∈ W) (∃α1 , . . . , αu ∈ CAi , β1 , . . . , βv ∈ CAj , Tα1 ) Bel(Aag, SetAction(α1 , . . . , αu , β1 , . . . , βv ), Tn) ∧ Bel(Aag, Eval(Ao, βv , w) < TrH < Eval(Aag, αu , w), Tn) ⇒ Pot.Int.To(Aag, α1 , Tn, Tα1 , w) An agent might believe that a chain of events will occur for various reasons due to the inevitable nature of the domain.",
                "For example, in Chess, we often observe the following: a move causes a check position, which in turn limits the opponents moves to avoiding the check, to which the first player might react with another check, and so on.",
                "The agent might also believe in a chain of events based on its knowledge of its adversarys profile, which allows it to foresee the adversarys movements with high accuracy.",
                "A4.",
                "Profile Detection Axiom.",
                "The agent can adjust its adversarys profiles by observations and pattern study (specifically, if there are repeated encounters with the same adversary).",
                "However, instead of waiting for profile information to be revealed, an agent can also initiate actions that will force its adversary to react in a way that will reveal profile knowledge about it.",
                "Formally, the axiom states that if all actions (γ) are not highly beneficial actions (< TrH), the agent can do action α in time Tα if it believes that it will result in a non-highly beneficial action β from its adversary, which in turn teaches it about the adversarys profile, i.e., gives a higher AdvKnow(P Aj Ai , Tβ). (∀Aag, Ao ∈ A, α ∈ CAag , β ∈ CAo , Tn, Tα, Tβ, w ∈ W) Bel(Aag, (∀γ ∈ CAag )Eval(Aag, γ, w) < TrH, Tn) ∧ Bel(Aag, Do(Aag, α, Tα, w) ⇒ Do(Ao, β, Tβ, w), Tn) ∧ Bel(Aag, Eval(Ao, β, w) < TrH) ∧ Bel(Aag, AdvKnow(P Aj Ai , Tβ) > AdvKnow(P Aj Ai , Tn), Tn) ⇒ Pot.Int.To(Aag, α, Tn, Tα, w) For example, going back to the Chess board game scenario, consider starting a game versus an opponent about whom we know nothing, not even if it is a human or a computerized opponent.",
                "We might start playing a strategy that will be suitable versus an average opponent, and adjust our game according to its level of play.",
                "A5.",
                "Alliance Formation Axiom The following behavioral axiom is relevant only in a multilateral instantiation of the adversarial environment (obviously, an alliance cannot be formed in a bilateral, zero-sum encounter).",
                "In different situations during a multilateral interaction, a group of agents might believe that it is in their best interests to form a temporary alliance.",
                "Such an alliance is an agreement that constrains its members behavior, but is believed by its members to enable them to achieve a higher utility value than the one achievable outside of the alliance.",
                "As an example, we can look at the classical Risk board game, where each player has an individual goal of being the sole conquerer of the world, a zero-sum game.",
                "However, in order to achieve this goal, it might be strategically wise to make short-term ceasefire agreements with other players, or to join forces and attack an opponent who is stronger than the rest.",
                "An alliances terms defines the way its members should act.",
                "It is a set of predicates, denoted as Terms, that is agreed upon by the alliance members, and should remain true for the duration of the alliance.",
                "For example, the set Terms in the Risk scenario, could contain the following predicates: 1.",
                "Alliance members will not attack each other on territories X, Y and Z; 2.",
                "Alliance members will contribute C units per turn for attacking adversary Ao; 3.",
                "Members are obligated to stay as part of the alliance until time Tk or until adversarys Ao army is smaller than Q.",
                "The set Terms specifies inter-group constraints on each of the alliance members (∀Aal i ∈ Aal ⊆ A) set of actions Cal i ⊆ C. Definition 6.",
                "Al val - the total evaluation value that agent Ai will achieve while being part of Aal is the sum of Evali (Eval for Ai) of each of Aal j Eval values after taking their own α actions (via the agent(α) predicate): Al val(Ai, Cal , Aal , w) = α∈Cal Evali(Aal j , agent(α), w) Definition 7.",
                "Al TrH - is a number representing an Al val The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) 553 threshold; above it, the alliance can be said to be a highly beneficial alliance.",
                "The value of Al TrH will be calculated dynamically according to the progress of the interaction, as can be seen in [7].",
                "After an alliance is formed, its members are now working in their normal adversarial environment, as well as according to the mental states and axioms required for their interactions as part of the alliance.",
                "The following Alliance model (AL) specifies the conditions under which the group Aal can be said to be in an alliance and working with a new and constrained set of actions Cal , at time Tn.",
                "AL(Aal , Cal , w, Tn) 1.",
                "Aal has a MB that all members are part of Aal : MB(Aal , (∀Aal i ∈ Aal )member(Aal i , Aal ), Tn) 2.",
                "Aal has a MB that the group be maintained: MB(Aal , (∀Aal i ∈ Aal )Int.Th (Ai, member(Ai, Aal ), Tn, Tn+1, Co), Tn) 3.",
                "Aal has a MB that being members gives them high utility value: MB(Aal , (∀Aal i ∈ Aal )Al val(Aal i , Cal , Aal , w) ≥ Al TrH, Tn) Members profiles are a crucial part of successful alliances.",
                "We assume that agents that have more accurate profiles of their adversaries will be more successful in such environments.",
                "Such agents will be able to predict when a member is about to breach the alliances contract (item 2 in the above model), and take counter measures (when item 3 will falsify).",
                "The robustness of the alliance is in part a function of its members trustfulness measure, objective position estimation, and other profile properties.",
                "We should note that an agent can simultaneously be part of more than one alliance.",
                "Such a temporary alliance, where the group members do not have a joint goal but act collaboratively for the interest of their own individual goals, is classified as a Treatment Group by modern psychologists [12] (in contrast to a Task Group, where its members have a joint goal).",
                "The Shared Activity model as presented in [5] modeled Treatment Group behavior using the same SharedPlans formalization.",
                "When comparing both definitions of an alliance and a Treatment Group we found an unsurprising resemblance between both models: the environment models definitions are almost identical (see SAs definitions in [5]), and their Selfish-Act and Cooperative Act axioms conform to our adversarial agents behavior.",
                "The main distinction between both models is the integration of a Helpful-behavior act axiom, in the Shared Activity which cannot be part of ours.",
                "This axiom states that an agent will consider taking action that will lower its Eval value (to a certain lower bound), if it believes that a group partner will gain a significant benefit.",
                "Such behavior cannot occur in a pure adversarial environment (as a zero-sum game is), where the alliance members are constantly on watch to manipulate their alliance to their own advantage.",
                "A6.",
                "Evaluation Maximization Axiom.",
                "In a case when all other axioms are inapplicable, we will proceed with the action that maximizes the heuristic value as computed in the Eval function. (∀Aag, Ao ∈ A, α ∈ Cag, Tn, w ∈ W) Bel(Aag, (∀γ ∈ Cag)Eval(Aag, α, w) ≥ Eval(Aag, γ, w), Tn) ⇒ Pot.Int.To(Aag, α, Tn, Tα, w) T1.",
                "Optimality on Eval = Utility The above axiomatic model handles situations where the Utility is unknown and the agents are bounded rational agents.",
                "The following theorem shows that in bilateral interactions, where the agents have the real Utility function (i.e., Eval = Utility) and are rational agents, the axioms provide the same optimal result as classic adversarial search (e.g., Min-Max).",
                "Theorem 1.",
                "Let Ae ag be an unbounded rational AE agent using the Eval heuristic evaluation function, Au ag be the same agent using the true Utility function, and Ao be a sole unbounded utility-based rational adversary.",
                "Given that Eval = Utility: (∀α ∈ CAu ag , α ∈ CAe ag , Tn, w ∈ W) Pot.Int.To(Au ag, α, Tn, Tα, w) → Pot.Int.To(Ae ag, α , Tn, Tα, w) ∧ ((α = α ) ∨ (Utility(Au ag, α, w) = Eval(Ae ag, α , w))) Sketch of proof - Given that Au ag has the real utility function and unbounded resources, it can generate the full game tree and run the optimal MinMax algorithm to choose the highest utility value action, which we denote by, α.",
                "The proof will show that Ae ag, using the AE axioms, will select the same or equal utility α (when there is more than one action with the same max utility) when Eval = Utility. (A1) Goal achieving axiom - suppose there is an α such that its completion will achieve Au ags goal.",
                "It will obtain the highest utility by Min-Max for Au ag.",
                "The Ae ag agent will select α or another action with the same utility value via A1.",
                "If such α does not exist, Ae ag cannot apply this axiom, and proceeds to A2. (A2) Preventive act axiom - (1) Looking at the basic case (see Prop1), if there is a β which leads Ao to achieve its goal, then a preventive action α will yield the highest utility for Au ag.",
                "Au ag will choose it through the utility, while Ae ag will choose it through A2. (2) In the general case, β is a highly beneficial action for Ao, thus yields low utility for Au ag, which will guide it to select an α that will prevent β, while Ae ag will choose it through A2.1 If such β does not exist for Ao, then A2 is not applicable, and Ae ag can proceed to A3. (A3) Suboptimal tactical move axiom - When using a heuristic Eval function, Ae ag has a partial belief in the profile of its adversary (item 4 in AE model), which may lead it to believe in SetActions (Prop1).",
                "In our case, Ae ag is holding a full profile on its optimal adversary and knows that Ao will behave optimally according to the real utility values on the complete search tree, therefore, any belief about suboptimal SetAction cannot exist, yielding this axiom inapplicable.",
                "Ae ag will proceed to A4. (A4) Profile detection axiom - Given that Ae ag has the full profile of Ao, none of Ae ags actions can increase its knowledge.",
                "That axiom will not be applied, and the agent will proceed with A6 (A5 will be disregarded because the interaction is bilateral). (A6) Evaluation maximization axiom - This axiom will select the max Eval for Ae ag.",
                "Given that Eval = Utility, the same α that was selected by Au ag will be selected. 3.",
                "EVALUATION The main purpose of our experimental analysis is to evaluate the models behavior and performance in a real adversarial environment.",
                "This section investigates whether bounded 1 A case where following the completion of β there exists a γ which gives high utility for Agent Au ag, cannot occur because Ao uses the same utility, and γs existence will cause it to classify β as a low utility action. 554 The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) rational agents situated in such adversarial environments will be better off applying our suggested behavioral axioms. 3.1 The Domain To explore the use of the above model and its behavioral axioms, we decided to use the Connect-Four game as our adversarial environment.",
                "Connect-Four is a 2-player, zerosum game which is played using a 6x7 matrix-like board.",
                "Each turn, a player drops a disc into one of the 7 columns (the set of 21 discs is usually colored yellow for player 1 and red for player 2; we will use White and Black respectively to avoid confusion).",
                "The winner is the first player to complete a horizontal, vertical, or diagonal set of four discs with its color.",
                "On very rare occasions, the game might end in a tie if all the empty grids are filled, but no player managed to create a 4-disc set.",
                "The Connect-Four game was solved in [1], where it is shown that the first player (playing with the white discs) can force a win by starting in the middle column (column 4) and playing optimally However, the optimal strategy is very complex, and difficult to follow even for complex bounded rational agents, such as human players.",
                "Before we can proceed checking agent behavior, we must first verify that the domain conforms to the adversarial environments definition as given above (which the behavioral axioms are based on).",
                "First, when playing a Connect-Four game, the agent has an intention to win the game (item 1).",
                "Second (item 2), our agent believes that in Connect-Four there can only be one winner (or no winner at all in the rare occurrence of a tie).",
                "In addition, our agent believes that its opponent to the game will try to win (item 3), and we hope it has some partial knowledge (item 4) about its adversary (this knowledge can vary from nothing, through simple facts such as age, to strategies and weaknesses).",
                "Of course, not all Connect-Four encounters are adversarial.",
                "For example, when a parent is playing the game with its child, the following situation might occur: the child, having a strong incentive to win, treats the environment as adversarial (it intends to win, understands that there can only be one winner, and believes that its parent is trying to beat him).",
                "However, the parents point of view might see the environment as an educational one, where its goal is not to win the game, but to cause enjoyment or practice strategic reasoning.",
                "In such an educational environment, a new set of behavioral axioms might be more beneficial to the parents goals than our suggested adversarial behavioral axioms. 3.2 Axiom Analysis After showing that the Connect-Four game is indeed a zero-sum, bilateral adversarial environment, the next step is to look at players behaviors during the game and check whether behaving according to our model does improve performance.",
                "To do so we have collected log files from completed Connect-Four games that were played by human players over the Internet.",
                "Our collected log file data came from Play by eMail (PBeM) sites.",
                "These are web sites that host email games, where each move is taken by an email exchange between the server and the players.",
                "Many such sites archives contain real competitive interactions, and also maintain a ranking system for their members.",
                "Most of the data we used can be found in [6].",
                "As can be learned from [1], Connect-Four has an optimal strategy and a considerable advantage for the player who starts the game (which we call the White player).",
                "We will concentrate in our analysis on the second players moves (to be called Black).",
                "The White player, being the first to act, has the so-called initiative advantage.",
                "Having the advantage and a good strategy will keep the Black player busy reacting to its moves, instead of initiating threats.",
                "A threat is a combination of three discs of the same color, with an empty spot for the fourth winning disk.",
                "An open threat is a threat that can be realized in the opponents next move.",
                "In order for the Black player to win, it must somehow turn the tide, take the advantage and start presenting threats to the White player.",
                "We will explore Black players behavior and their conformance to our axioms.",
                "To do so, we built an application that reads log files and analyzes the Black players moves.",
                "The application contains two main components: (1) a Min-Max algorithm for evaluation of moves; (2) open threats detector for the discovering of open threats.",
                "The Min-Max algorithm will work to a given depth, d and for each move α will output the heuristic value for the next action taken by the player as written in the log file, h(α), alongside the maximum heuristic value, maxh(α), that could be achieved prior to taking the move (obviously, if h(α) = maxh(α), then the player did not do the optimal move heuristically).",
                "The threat detectors job is to notify if some action was taken in order to block an open threat (not blocking an open threat will probably cause the player to lose in the opponents next move).",
                "The heuristic function used by Min-Max to evaluate the players utility is the following function, which is simple to compute, yet provides a reasonable challenge to human opponents: Definition 8.",
                "Let Group be an adjacent set of four squares that are horizontal, vertical, or diagonal.",
                "Groupn b (Groupn w) be a Group with n pieces of the black (white) color and 4−n empty squares. h = ((Group1 b ∗α)+(Group2 b ∗β)+(Group3 b ∗γ)+(Group4 b ∗∞)) − ((Group1 w ∗α)+(Group2 w ∗β)+(Group3 w ∗γ)+(Group4 w ∗∞)) The values of α, β and δ can vary to form any desired linear combination; however, it is important to value them with the α < β < δ ordering in mind (we used 1, 4, and 8 as their respective values).",
                "Groups of 4 discs of the same color means victory, thus discovery of such a group will result in ∞ to ensure an extreme value.",
                "We now use our estimated evaluation function to evaluate the Black players actions during the Connect-Four adversarial interaction.",
                "Each game from the log file was input into the application, which processed and output a reformatted log file containing the h value of the current move, the maxh value that could be achieved, and a notification if an open threat was detected.",
                "A total of 123 games were analyzed (57 with White winning, and 66 with Black winning).",
                "A few additional games were manually ignored in the experiment, due to these problems: a player abandoning the game while the outcome is not final, or a blunt irrational move in the early stages of the game (e.g., not blocking an obvious winning group in the first opening moves).",
                "In addition, a single tie game was also removed.",
                "The simulator was run to a search depth of 3 moves.",
                "We now proceed to analyze the games with respect to each behavioral axiom.",
                "The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) 555 Table 1: Average heuristic difference analysis Black losses Black Won Avg minh -17.62 -12.02 Avg 3 lowest h moves (min3 h) -13.20 -8.70 3.2.1 Affirming the Suboptimal tactical move axiom The following section presents the heuristic evaluations of the Min-Max algorithm for each action, and checks the amount and extent of suboptimal tactical actions and their implications on performance.",
                "Table 1 shows results and insights from the games heuristic analysis, when search depth equals 3 (this search depth was selected for the results to be comparable to [9], see Section 3.2.3).",
                "The tables heuristic data is the difference between the present maximal heuristic value and the heuristic value of the action that was eventually taken by the player (i.e., the closer the number is to 0, the closer the action was to the maximum heuristic action).",
                "The first row presents the difference values of the action that had the maximal difference value among all the Black players actions in a given game, as averaged over all Blacks winning and losing games (see respective columns).",
                "In games in which the Black player loses, its average difference value was -17.62, while in games in which the Black player won, its average was -12.02.",
                "The second row expands the analysis by considering the 3 highest heuristic difference actions, and averaging them.",
                "In that case, we notice an average heuristic difference of 5 points between games which the Black player loses and games in which it wins.",
                "Nevertheless, the importance of those numbers is that they allowed us to take an educated guess on a threshold number of 11.5, as the value of the TrH constant, which differentiates between normal actions and highly beneficial ones.",
                "After finding an approximated TrH constant, we can proceed with an analysis of the importance of suboptimal moves.",
                "To do so we took the subset of games in which the minimum heuristic difference value for Blacks actions was 11.5.",
                "As presented in Table 2, we can see the different min3 h average of the 3 largest ranges and the respective percentage of games won.",
                "The first row shows that the Black player won only 12% of the games in which the average of its 3 highest heuristically difference actions (min3 h) was smaller than the suggested threshold, TrH = 11.5.",
                "The second row shows a surprising result: it seems that when min3 h > −4 the Black player rarely wins.",
                "Intuition would suggest that games in which the action evaluation values were closer to the maximal values will result in more winning games for Black.",
                "However, it seems that in the Connect-Four domain, merely responding with somewhat easily expected actions, without initiating a few surprising and suboptimal moves, does not yield good results.",
                "The last row sums up the main insights from the analysis; most of Blacks wins (83%) came when its min3 h was in the range of -11.5 to -4.",
                "A close inspection of those Black winning games shows the following pattern behind the numbers: after standard opening moves, Black suddenly drops a disc into an isolated column, which seems a waste of a move.",
                "White continues to build its threats, while usually disregarding Blacks last move, which in turn uses the isolated disc as an anchor for a future winning threat.",
                "The results show that it was beneficial for the Black player Table 2: Blacks winnings percentages % of games min3 h < −11.5 12% min3 h > −4 5% −11.5 ≤ min3 h ≤ −4 83% to take suboptimal actions and not give the current highest possible heuristic value, but will not be too harmful for its position (i.e., will not give high beneficial value to its adversary).",
                "As it turned out, learning the threshold is an important aspect of success: taking wildly risky moves (min3 h < −11.5) or trying to avoid them (min3 h > −4) reduces the Black players winning chances by a large margin. 3.2.2 Affirming the Profile Monitoring Axiom In the task of showing the importance of monitoring ones adversaries profiles, our log files could not be used because they did not contain repeated interactions between players, which are needed to infer the players knowledge about their adversaries.",
                "However, the importance of opponent modeling and its use in attaining tactical advantages was already studied in various domains ([3, 9] are good examples).",
                "In a recent paper, Markovitch and Reger [9] explored the notion of learning and exploitation of opponent weakness in competitive interactions.",
                "They apply simple learning strategies by analyzing examples from past interactions in a specific domain.",
                "They also used the Connect-Four adversarial domain, which can now be used to understand the importance of monitoring the adversarys profile.",
                "Following the presentation of their theoretical model, they describe an extensive empirical study and check the agents performance after learning the weakness model with past examples.",
                "One of the domains used as a competitive environment was the same Connect-Four game (Checkers was the second domain).",
                "Their heuristic function was identical to ours with three different variations (H1, H2, and H3) that are distinguished from one another in their linear combination coefficient values.",
                "The search depth for the players was 3 (as in our analysis).",
                "Their extensive experiments check and compare various learning strategies, risk factors, predefined feature sets and usage methods.",
                "The bottom line is that the Connect-Four domain shows an improvement from a 0.556 winning rate before modeling to a 0.69 after modeling (page 22).",
                "Their conclusions, showing improved performance when holding and using the adversarys model, justify the effort to monitor the adversary profile for continuous and repeated interactions.",
                "An additional point that came up in their experiments is the following: after the opponent weakness model has been learned, the authors describe different methods of integrating the opponent weakness model into the agents decision strategy.",
                "Nevertheless, regardless of the specific method they chose to work with, all integration methods might cause the agent to take suboptimal decisions; it might cause the agent to prefer actions that are suboptimal at the present decision junction, but which might cause the opponent to react in accordance with its weakness model (as represented by our agent) which in turn will be beneficial for us in the future.",
                "The agents behavior, as demonstrated in [9] further confirms and strengthens our Suboptimal Tactical Axiom as discussed in the previous section. 556 The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) 3.2.3 Additional Insights The need for the Goal Achieving, Preventive Act, and Evaluation Maximization axioms are obvious, and need no further verification.",
                "However, even with respect to those axioms, a few interesting insights came up in the log analysis.",
                "The Goal achieving and Preventive Act axioms, though theoretically trivial, seem to provide some challenge to a human player.",
                "In the initial inspection of the logs, we encountered few games2 where a player, for inexplicable reasons, did not block the other from winning or failed to execute its own winning move.",
                "We can blame those faults on the humans lack of attention, or a typing error in its move reply; nevertheless, those errors might occur in bounded rational agents, and the appropriate behavior needs to be axiomatized.",
                "A typical Connect-Four game revolves around generating threats and blocking them.",
                "In our analysis we looked for explicit preventive actions, i.e., moves that block a group of 3 discs, or that remove a future threat (in our limited search horizon).",
                "We found that in 83% of the total games there was at least one preventive action taken by the Black player.",
                "It was also found that Black averaged 2.8 preventive actions per game on the games in which it lost, while averaging 1.5 preventive actions per game when winning.",
                "It seems that Black requires 1 or 2 preventive actions to build its initial taking position, before starting to present threats.",
                "If it did not manage to win, it will usually prevent an extra threat or two before succumbing to White. 4.",
                "RELATED WORK Much research deals with the axiomatization of teamwork and mental states of individuals: some models use knowledge and belief [10], others have models of goals and intentions [8, 4].",
                "However, all these formal theories deal with agent teamwork and cooperation.",
                "As far as we know, our model is the first to provide a formalized model for explicit adversarial environments and agents behavior in it.",
                "The classical Min-Max adversarial search algorithm was the first attempt to integrate the opponent into the search space with a weak assumption of an optimally playing opponent.",
                "Since then, much effort has gone into integrating the opponent model into the decision procedure to predict future behavior.",
                "The M∗ algorithm presented by Carmel and Markovitch [2] showed a method of incorporating opponent models into adversary search, while in [3] they used learning to provide a more accurate opponent model in a 2player repeated game environment, where agents strategies were modeled as finite automata.",
                "Additional Adversarial planning work was done by Willmott et al. [13], which provided an adversarial planning approach to the game of GO.",
                "The research mentioned above dealt with adversarial search and the integration of opponent models into classical utilitybased search methods.",
                "That work shows the importance of opponent modeling and the ability to exploit it to an agents advantage.",
                "However, the basic limitations of those search methods still apply; our model tries to overcome those limitations by presenting a formal model for a new, mental state-based adversarial specification. 5.",
                "CONCLUSIONS We presented an Adversarial Environment model for a 2 These were later removed from the final analysis. bounded rational agent that is situated in an N-player, zerosum environment.",
                "We used the SharedPlans formalization to define the model and the axioms that agents can apply as behavioral guidelines.",
                "The model is meant to be used as a guideline for designing agents that need to operate in such adversarial environments.",
                "We presented empirical results, based on ConnectFour log file analysis, that exemplify the model and the axioms for a bilateral instance of the environment.",
                "The results we presented are a first step towards an expanded model that will cover all types of adversarial environments, for example, environments that are non-zero-sum, and environments that contain natural agents that are not part of the direct conflict.",
                "Those challenges and more will be dealt with in future research. 6.",
                "ACKNOWLEDGMENT This research was supported in part by Israel Science Foundation grants #1211/04 and #898/05. 7.",
                "REFERENCES [1] L. V. Allis.",
                "A knowledge-based approach of Connect-Four - the game is solved: White wins.",
                "Masters thesis, Free University, Amsterdam, The Netherlands, 1988. [2] D. Carmel and S. Markovitch.",
                "Incorporating opponent models into adversary search.",
                "In Proceedings of the Thirteenth National Conference on Artificial Intelligence, pages 120-125, Portland, OR, 1996. [3] D. Carmel and S. Markovitch.",
                "Opponent modeling in multi-agent systems.",
                "In G. Weiß and S. Sen, editors, Adaptation and Learning in Multi-Agent Systems, pages 40-52.",
                "Springer-Verlag, 1996. [4] B. J. Grosz and S. Kraus.",
                "Collaborative plans for complex group action.",
                "Artificial Intelligence, 86(2):269-357, 1996. [5] M. Hadad, G. Kaminka, G. Armon, and S. Kraus.",
                "Supporting collaborative activity.",
                "In Proc. of AAAI-2005, pages 83-88, Pittsburgh, 2005. [6] http://www.gamerz.net/˜pbmserv/. [7] S. Kraus and D. Lehmann.",
                "Designing and building a negotiating automated agent.",
                "Computational Intelligence, 11:132-171, 1995. [8] H. J. Levesque, P. R. Cohen, and J. H. T. Nunes.",
                "On acting together.",
                "In Proc. of AAAI-90, pages 94-99, Boston, MA, 1990. [9] S. Markovitch and R. Reger.",
                "Learning and exploiting relative weaknesses of opponent agents.",
                "Autonomous Agents and Multi-Agent Systems, 10(2):103-130, 2005. [10] Y. M. Ronald Fagin, Joseph Y. Halpern and M. Y. Vardi.",
                "Reasoning about knowledge.",
                "MIT Press, Cambridge, Mass., 1995. [11] P. Thagard.",
                "Adversarial problem solving: Modeling an oponent using explanatory coherence.",
                "Cognitive Science, 16(1):123-149, 1992. [12] R. W. Toseland and R. F. Rivas.",
                "An Introduction to Group Work Practice.",
                "Prentice Hall, Englewood Cliffs, NJ, 2nd edition edition, 1995. [13] S. Willmott, J. Richardson, A. Bundy, and J. Levine.",
                "An adversarial planning approach to Go.",
                "Lecture Notes in Computer Science, 1558:93-112, 1999.",
                "The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) 557"
            ],
            "original_annotated_samples": [],
            "translated_annotated_samples": [
                "Introducción Investigación temprana en \"Sistemas Multiagentes\" (MAS) considerados grupos cooperativos de agentes;Debido a que los agentes individuales tenían recursos limitados o acceso limitado a la información (por ejemplo, potencia de procesamiento limitada, cobertura de sensor limitada), trabajaron juntos por diseño para resolver problemas que individualmente no podían resolver, o al menos no podían resolver de manera tan eficiente.sistema multiagente"
            ],
            "translated_text": "",
            "candidates": [],
            "error": []
        },
        "modal logic": {
            "translated_key": "",
            "is_in_text": true,
            "original_annotated_sentences": [
                "An Adversarial Environment Model for Bounded Rational Agents in Zero-Sum Interactions Inon Zuckerman1 , Sarit Kraus1 , Jeffrey S. Rosenschein2 , Gal Kaminka1 1 Department of Computer Science 2 The School of Engineering Bar-Ilan University and Computer Science Ramat-Gan, Israel Hebrew University, Jerusalem, Israel {zukermi,sarit,galk}@cs.biu.ac.il jeff@cs.huji.ac.il ABSTRACT Multiagent environments are often not cooperative nor collaborative; in many cases, agents have conflicting interests, leading to adversarial interactions.",
                "This paper presents a formal Adversarial Environment model for bounded rational agents operating in a zero-sum environment.",
                "In such environments, attempts to use classical utility-based search methods can raise a variety of difficulties (e.g., implicitly modeling the opponent as an omniscient utility maximizer, rather than leveraging a more nuanced, explicit opponent model).",
                "We define an Adversarial Environment by describing the mental states of an agent in such an environment.",
                "We then present behavioral axioms that are intended to serve as design principles for building such adversarial agents.",
                "We explore the application of our approach by analyzing log files of completed Connect-Four games, and present an empirical analysis of the axioms appropriateness.",
                "Categories and Subject Descriptors I.2.11 [Artificial Intelligence]: Distributed Artificial Intelligence-Intelligent agents,Multiagent Systems; I.2.4 [Artificial Intelligence]: Knowledge Representation Formalisms and Methods -<br>modal logic</br> General Terms Design, Theory 1.",
                "INTRODUCTION Early research in multiagent systems (MAS) considered cooperative groups of agents; because individual agents had limited resources, or limited access to information (e.g., limited processing power, limited sensor coverage), they worked together by design to solve problems that individually they could not solve, or at least could not solve as efficiently.",
                "MAS research, however, soon began to consider interacting agents with individuated interests, as representatives of different humans or organizations with non-identical interests.",
                "When interactions are guided by diverse interests, participants may have to overcome disagreements, uncooperative interactions, and even intentional attempts to damage one another.",
                "When these types of interactions occur, environments require appropriate behavior from the agents situated in them.",
                "We call these environments Adversarial Environments, and call the clashing agents Adversaries.",
                "Models of cooperation and teamwork have been extensively explored in MAS through the axiomatization of mental states (e.g., [8, 4, 5]).",
                "However, none of this research dealt with adversarial domains and their implications for agent behavior.",
                "Our paper addresses this issue by providing a formal, axiomatized mental state model for a subset of adversarial domains, namely simple zero-sum adversarial environments.",
                "Simple zero-sum encounters exist of course in various twoplayer games (e.g., Chess, Checkers), but they also exist in n-player games (e.g., Risk, Diplomacy), auctions for a single good, and elsewhere.",
                "In these latter environments especially, using a utility-based adversarial search (such as the Min-Max algorithm) does not always provide an adequate solution; the payoff function might be quite complex or difficult to quantify, and there are natural computational limitations on bounded rational agents.",
                "In addition, traditional search methods (like Min-Max) do not make use of a model of the opponent, which has proven to be a valuable addition to adversarial planning [9, 3, 11].",
                "In this paper, we develop a formal, axiomatized model for bounded rational agents that are situated in a zero-sum adversarial environment.",
                "The model uses different modality operators, and its main foundations are the SharedPlans [4] model for collaborative behavior.",
                "We explore environment properties and the mental states of agents to derive behavioral axioms; these behavioral axioms constitute a formal model that serves as a specification and design guideline for agent design in such settings.",
                "We then investigate the behavior of our model empirically using the Connect-Four board game.",
                "We show that this game conforms to our environment definition, and analyze players behavior using a large set of completed match log 550 978-81-904262-7-5 (RPS) c 2007 IFAAMAS files.",
                "In addition, we use the results presented in [9] to discuss the importance of opponent modeling in our ConnectFour adversarial domain.",
                "The paper proceeds as follows.",
                "Section 2 presents the models formalization.",
                "Section 3 presents the empirical analysis and its results.",
                "We discuss related work in Section 4, and conclude and present future directions in Section 5. 2.",
                "ADVERSARIAL ENVIRONMENTS The adversarial environment model (denoted as AE) is intended to guide the design of agents by providing a specification of the capabilities and mental attitudes of an agent in an adversarial environment.",
                "We focus here on specific types of adversarial environments, specified as follows: 1.",
                "Zero-Sum Interactions: positive and negative utilities of all agents sum to zero; 2.",
                "Simple AEs: all agents in the environment are adversarial agents; 3.",
                "Bilateral AEs: AEs with exactly two agents; 4.",
                "Multilateral AEs: AEs of three or more agents.",
                "We will work on both bilateral and multilateral instantiations of zero-sum and simple environments.",
                "In particular, our adversarial environment model will deal with interactions that consist of N agents (N ≥ 2), where all agents are adversaries, and only one agent can succeed.",
                "Examples of such environments range from board games (e.g., Chess, Connect-Four, and Diplomacy) to certain economic environments (e.g., N-bidder auctions over a single good). 2.1 Model Overview Our approach is to formalize the mental attitudes and behaviors of a single adversarial agent; we consider how a single agent perceives the AE.",
                "The following list specifies the conditions and mental states of an agent in a simple, zero-sum AE: 1.",
                "The agent has an individual intention that its own goal will be completed; 2.",
                "The agent has an individual belief that it and its adversaries are pursuing full conflicting goals (defined below)there can be only one winner; 3.",
                "The agent has an individual belief that each adversary has an intention to complete its own full conflicting goal; 4.",
                "The agent has an individual belief in the (partial) profile of its adversaries.",
                "Item 3 is required, since it might be the case that some agent has a full conflicting goal, and is currently considering adopting the intention to complete it, but is, as of yet, not committed to achieving it.",
                "This might occur because the agent has not yet deliberated about the effects that adopting that intention might have on the other intentions it is currently holding.",
                "In such cases, it might not consider itself to even be in an adversarial environment.",
                "Item 4 states that the agent should hold some belief about the profiles of its adversaries.",
                "The profile represents all the knowledge the agent has about its adversary: its weaknesses, strategic capabilities, goals, intentions, trustworthiness, and more.",
                "It can be given explicitly or can be learned from observations of past encounters. 2.2 Model Definitions for Mental States We use Grosz and Krauss definitions of the modal operators, predicates, and meta-predicates, as defined in their SharedPlan formalization [4].",
                "We recall here some of the predicates and operators that are used in that formalization: Int.To(Ai, α, Tn, Tα, C) represents Ais intentions at time Tn to do an action α at time Tα in the context of C. Int.Th(Ai, prop, Tn, Tprop, C) represents Ais intentions at time Tn that a certain proposition prop holds at time Tprop in the context of C. The potential intention operators, Pot.Int.To(...) and Pot.Int.Th(...), are used to represent the mental state when an agent considers adopting an intention, but has not deliberated about the interaction of the other intentions it holds.",
                "The operator Bel(Ai, f, Tf ) represents agent Ai believing in the statement expressed in formula f, at time Tf .",
                "MB(A, f, Tf ) represents mutual belief for a group of agents A.",
                "A snapshot of the system finds our environment to be in some state e ∈ E of environmental variable states, and each adversary in any LAi ∈ L of possible local states.",
                "At any given time step, the system will be in some world w of the set of all possible worlds w ∈ W, where w = E×LA1 ×LA2 × ...LAn , and n is the number of adversaries.",
                "For example, in a Texas Holdem poker game, an agents local state might be its own set of cards (which is unknown to its adversary) while the environment will consist of the betting pot and the community cards (which are visible to both players).",
                "A utility function under this formalization is defined as a mapping from a possible world w ∈ W to an element in , which expresses the desirability of the world, from a single agent perspective.",
                "We usually normalize the range to [0,1], where 0 represents the least desirable possible world, and 1 is the most desirable world.",
                "The implementation of the utility function is dependent on the domain in question.",
                "The following list specifies new predicates, functions, variables, and constants used in conjunction with the original definitions for the adversarial environment formalization: 1. φ is a null action (the agent does not do anything). 2.",
                "GAi is the set of agent Ais goals.",
                "Each goal is a set of predicates whose satisfaction makes the goal complete (we use G∗ Ai ∈ GAi to represent an arbitrary goal of agent Ai). 3. gAi is the set of agent Ais subgoals.",
                "Subgoals are predicates whose satisfaction represents an important milestone toward achievement of the full goal. gG∗ Ai ⊆ gAi is the set of subgoals that are important to the completion of goal G∗ Ai (we will use g∗ G∗ Ai ∈ gG∗ Ai to represent an arbitrary subgoal). 4.",
                "P Aj Ai is the profile object agent Ai holds about agent Aj. 5.",
                "CA is a general set of actions for all agents in A which are derived from the environments constraints.",
                "CAi ⊆ CA is the set of agent Ais possible actions. 6.",
                "Do(Ai, α, Tα, w) holds when Ai performs action α over time interval Tα in world w. 7.",
                "Achieve(G∗ Ai , α, w) is true when goal G∗ Ai is achieved following the completion of action α in world w ∈ W, where α ∈ CAi . 8.",
                "Profile(Ai, PAi Ai ) is true when agent Ai holds an object profile for agent Aj.",
                "Definition 1.",
                "Full conflict (FulConf ) describes a zerosum interaction where only a single goal of the goals in conflict can be completed.",
                "FulConf(G∗ Ai , G∗ Aj ) ⇒ (∃α ∈ CAi , ∀w, β ∈ CAj ) (Achieve(G∗ Ai , α, w) ⇒ ¬Achieve(G∗ Aj , β, w)) ∨ (∃β ∈ CAj , ∀w, α ∈ CAi )(Achieve(G∗ Aj , β, w) ⇒ ¬Achieve(G∗ Ai , α, w)) Definition 2.",
                "Adversarial Knowledge (AdvKnow) is a function returning a value which represents the amount of The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) 551 knowledge agent Ai has on the profile of agent Aj, at time Tn.",
                "The higher the value, the more knowledge agent Ai has.",
                "AdvKnow : P Aj Ai × Tn → Definition 3.",
                "Eval - This evaluation function returns an estimated expected utility value for an agent in A, after completing an action from CA in some world state w. Eval : A × CA × w → Definition 4.",
                "TrH - (Threshold) is a numerical constant in the [0,1] range that represents an evaluation function (Eval) threshold value.",
                "An action that yields an estimated utility evaluation above the TrH is regarded as a highly beneficial action.",
                "The Eval value is an estimation and not the real utility function, which is usually unknown.",
                "Using the real utility value for a rational agent would easily yield the best outcome for that agent.",
                "However, agents usually do not have the real utility functions, but rather a heuristic estimate of it.",
                "There are two important properties that should hold for the evaluation function: Property 1.",
                "The evaluation function should state that the most desirable world state is one in which the goal is achieved.",
                "Therefore, after the goal has been satisfied, there can be no future action that can put the agent in a world state with higher Eval value. (∀Ai, G∗ Ai , α, β ∈ CAi , w ∈ W) Achieve(G∗ Ai , α, w) ⇒ Eval(Ai, α, w) ≥ Eval(Ai, β, w) Property 2.",
                "The evaluation function should project an action that causes a completion of a goal or a subgoal to a value which is greater than TrH (a highly beneficial action). (∀Ai, G∗ Ai ∈ GAi , α ∈ CAi , w ∈ W, g∗ GAi ∈ gGAi ) Achieve(G∗ Ai , α, w) ∨ Achieve(g∗ GAi , α, w) ⇒ Eval(Ai, α, w) ≥ TrH.",
                "Definition 5.",
                "SetAction We define a set action (SetAction) as a set of action operations (either complex or basic actions) from some action sets CAi and CAj which, according to agent Ais belief, are attached together by a temporal and consequential relationship, forming a chain of events (action, and its following consequent action). (∀α1 , . . . , αu ∈ CAi , β1 , . . . , βv ∈ CAj , w ∈ W) SetAction(α1 , . . . , αu , β1 , . . . , βv , w) ⇒ ((Do(Ai, α1 , Tα1 , w) ⇒ Do(Aj, β1 , Tβ1 , w)) ⇒ Do(Ai, α2 , Tα2 , w) ⇒ . . . ⇒ Do(Ai, αu , Tαu , w)) The consequential relation might exist due to various environmental constraints (when one action forces the adversary to respond with a specific action) or due to the agents knowledge about the profile of its adversary.",
                "Property 3.",
                "As the knowledge we have about our adversary increases we will have additional beliefs about its behavior in different situations which in turn creates new set actions.",
                "Formally, if our AdvKnow at time Tn+1 is greater than AdvKnow at time Tn, then every SetAction known at time Tn is also known at time Tn+1.",
                "AdvKnow(P Aj Ai , Tn+1) > AdvKnow(P Aj Ai , Tn) ⇒ (∀α1 , . . . , αu ∈ CAi , β1 , . . . , βv ∈ CAj ) Bel(Aag, SetAction(α1 , . . . , αu , β1 , . . . , βv ), Tn) ⇒ Bel(Aag, SetAction(α1 , . . . , αu , β1 , . . . , βv ), Tn+1) 2.3 The Environment Formulation The following axioms provide the formal definition for a simple, zero-sum Adversarial Environment (AE).",
                "Satisfaction of these axioms means that the agent is situated in such an environment.",
                "It provides specifications for agent Aag to interact with its set of adversaries A with respect to goals G∗ Aag and G∗ A at time TCo at some world state w. AE(Aag, A, G∗ Aag , A1, . . . , Ak, G∗ A1 , . . . , G∗ Ak , Tn, w) 1.",
                "Aag has an Int.Th his goal would be completed: (∃α ∈ CAag , Tα) Int.Th(Aag, Achieve(G∗ Aag , α), Tn, Tα, AE) 2.",
                "Aag believes that it and each of its adversaries Ao are pursuing full conflicting goals: (∀Ao ∈ {A1, . . . , Ak}) Bel(Aag, FulConf(G∗ Aag , G∗ Ao ), Tn) 3.",
                "Aag believes that each of his adversaries in Ao has the Int.Th his conflicting goal G∗ Aoi will be completed: (∀Ao ∈ {A1, . . . , Ak})(∃β ∈ CAo , Tβ) Bel(Aag, Int.Th(Ao, Achieve(G∗ Ao , β), TCo, Tβ, AE), Tn) 4.",
                "Aag has beliefs about the (partial) profiles of its adversaries (∀Ao ∈ {A1, . . . , Ak}) (∃PAo Aag ∈ PAag )Bel(Aag, Profile(Ao, PAo Aag ), Tn) To build an agent that will be able to operate successfully within such an AE, we must specify behavioral guidelines for its interactions.",
                "Using a naive Eval maximization strategy to a certain search depth will not always yield satisfactory results for several reasons: (1) the search horizon problem when searching for a fixed depth; (2) the strong assumption of an optimally rational, unbounded resources adversary; (3) using an estimated evaluation function which will not give optimal results in all world states, and can be exploited [9].",
                "The following axioms specify the behavioral principles that can be used to differentiate between successful and less successful agents in the above Adversarial Environment.",
                "Those axioms should be used as specification principles when designing and implementing agents that should be able to perform well in such Adversarial Environments.",
                "The behavioral axioms represent situations in which the agent will adopt potential intentions to (Pot.Int.To(...)) perform an action, which will typically require some means-end reasoning to select a possible course of action.",
                "This reasoning will lead to the adoption of an Int.To(...) (see [4]).",
                "A1.",
                "Goal Achieving Axiom.",
                "The first axiom is the simplest case; when the agent Aag believes that it is one action (α) away from achieving his conflicting goal G∗ Aag , it should adopt the potential intention to do α and complete its goal. (∀Aag, α ∈ CAag , Tn, Tα, w ∈ W) (Bel(Aag, Do(Aag, α, Tα, w) ⇒ Achieve(G∗ Aag , α, w)) ⇒ Pot.Int.To(Aag, α, Tn, Tα, w) This somewhat trivial behavior is the first and strongest axiom.",
                "In any situation, when the agent is an action away from completing the goal, it should complete the action.",
                "Any fair Eval function would naturally classify α as the maximal value action (property 1).",
                "However, without explicit axiomatization of such behavior there might be situations where the agent will decide on taking another action for various reasons, due to its bounded decision resources.",
                "A2.",
                "Preventive Act Axiom.",
                "Being in an adversarial situation, agent Aag might decide to take actions that will damage one of its adversarys plans to complete its goal, even if those actions do not explicitly advance Aag towards its conflicting goal G∗ Aag .",
                "Such preventive action will take place when agent Aag has a belief about the possibility of its adversary Ao doing an action β that will give it a high 552 The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) utility evaluation value (> TrH).",
                "Believing that taking action α will prevent the opponent from doing its β, it will adopt a potential intention to do α. (∀Aag, Ao ∈ A, α ∈ CAag , β ∈ CAo , Tn, Tβ, w ∈ W) (Bel(Aag, Do(Ao, β, Tβ, w) ∧ Eval(Ao, β, w) > TrH, Tn) ∧ Bel(Aag, Do(Aag, α, Tα, w) ⇒ ¬Do(Ao, β, Tβ, w), Tn) ⇒ Pot.Int.To(Aag, α, Tn, Tα, w) This axiom is a basic component of any adversarial environment.",
                "For example, looking at a Chess board game, a player could realize that it is about to be checkmated by its opponent, thus making a preventive move.",
                "Another example is a Connect Four game: when a player has a row of three chips, its opponent must block it, or lose.",
                "A specific instance of A1 occurs when the adversary is one action away from achieving its goal, and immediate preventive action needs to be taken by the agent.",
                "Formally, we have the same beliefs as stated above, with a changed belief that doing action β will cause agent Ao to achieve its goal.",
                "Proposition 1: Prevent or lose case. (∀Aag, Ao ∈ A, α ∈ CAag , β ∈ CAo , G∗ Ao , Tn, Tα, Tβ, w ∈ W) Bel(Aag, Do(Ao, β, Tβ, w) ⇒ Achieve(G∗ Ao , β, w), Tn) ∧ Bel(Aag, Do(Aag, α, Tα, w) ⇒ ¬Do(Ao, β, Tβ, w)) ⇒ Pot.Int.To(Aag, α, Tn, Tα, w) Sketch of proof: Proposition 1 can be easily derived from axiom A1 and the property 2 of the Eval function, which states that any action that causes a completion of a goal is a highly beneficial action.",
                "The preventive act behavior will occur implicitly when the Eval function is equal to the real world utility function.",
                "However, being bounded rational agents and dealing with an estimated evaluation function we need to explicitly axiomatize such behavior, for it will not always occur implicitly from the evaluation function.",
                "A3.",
                "Suboptimal Tactical Move Axiom.",
                "In many scenarios a situation may occur where an agent will decide not to take the current most beneficial action it can take (the action with the maximal utility evaluation value), because it believes that taking another action (with lower utility evaluation value) might yield (depending on the adversarys response) a future possibility for a highly beneficial action.",
                "This will occur most often when the Eval function is inaccurate and differs by a large extent from the Utility function.",
                "Put formally, agent Aag believes in a certain SetAction that will evolve according to its initial action and will yield a high beneficial value (> TrH) solely for it. (∀Aag, Ao ∈ A, Tn, w ∈ W) (∃α1 , . . . , αu ∈ CAi , β1 , . . . , βv ∈ CAj , Tα1 ) Bel(Aag, SetAction(α1 , . . . , αu , β1 , . . . , βv ), Tn) ∧ Bel(Aag, Eval(Ao, βv , w) < TrH < Eval(Aag, αu , w), Tn) ⇒ Pot.Int.To(Aag, α1 , Tn, Tα1 , w) An agent might believe that a chain of events will occur for various reasons due to the inevitable nature of the domain.",
                "For example, in Chess, we often observe the following: a move causes a check position, which in turn limits the opponents moves to avoiding the check, to which the first player might react with another check, and so on.",
                "The agent might also believe in a chain of events based on its knowledge of its adversarys profile, which allows it to foresee the adversarys movements with high accuracy.",
                "A4.",
                "Profile Detection Axiom.",
                "The agent can adjust its adversarys profiles by observations and pattern study (specifically, if there are repeated encounters with the same adversary).",
                "However, instead of waiting for profile information to be revealed, an agent can also initiate actions that will force its adversary to react in a way that will reveal profile knowledge about it.",
                "Formally, the axiom states that if all actions (γ) are not highly beneficial actions (< TrH), the agent can do action α in time Tα if it believes that it will result in a non-highly beneficial action β from its adversary, which in turn teaches it about the adversarys profile, i.e., gives a higher AdvKnow(P Aj Ai , Tβ). (∀Aag, Ao ∈ A, α ∈ CAag , β ∈ CAo , Tn, Tα, Tβ, w ∈ W) Bel(Aag, (∀γ ∈ CAag )Eval(Aag, γ, w) < TrH, Tn) ∧ Bel(Aag, Do(Aag, α, Tα, w) ⇒ Do(Ao, β, Tβ, w), Tn) ∧ Bel(Aag, Eval(Ao, β, w) < TrH) ∧ Bel(Aag, AdvKnow(P Aj Ai , Tβ) > AdvKnow(P Aj Ai , Tn), Tn) ⇒ Pot.Int.To(Aag, α, Tn, Tα, w) For example, going back to the Chess board game scenario, consider starting a game versus an opponent about whom we know nothing, not even if it is a human or a computerized opponent.",
                "We might start playing a strategy that will be suitable versus an average opponent, and adjust our game according to its level of play.",
                "A5.",
                "Alliance Formation Axiom The following behavioral axiom is relevant only in a multilateral instantiation of the adversarial environment (obviously, an alliance cannot be formed in a bilateral, zero-sum encounter).",
                "In different situations during a multilateral interaction, a group of agents might believe that it is in their best interests to form a temporary alliance.",
                "Such an alliance is an agreement that constrains its members behavior, but is believed by its members to enable them to achieve a higher utility value than the one achievable outside of the alliance.",
                "As an example, we can look at the classical Risk board game, where each player has an individual goal of being the sole conquerer of the world, a zero-sum game.",
                "However, in order to achieve this goal, it might be strategically wise to make short-term ceasefire agreements with other players, or to join forces and attack an opponent who is stronger than the rest.",
                "An alliances terms defines the way its members should act.",
                "It is a set of predicates, denoted as Terms, that is agreed upon by the alliance members, and should remain true for the duration of the alliance.",
                "For example, the set Terms in the Risk scenario, could contain the following predicates: 1.",
                "Alliance members will not attack each other on territories X, Y and Z; 2.",
                "Alliance members will contribute C units per turn for attacking adversary Ao; 3.",
                "Members are obligated to stay as part of the alliance until time Tk or until adversarys Ao army is smaller than Q.",
                "The set Terms specifies inter-group constraints on each of the alliance members (∀Aal i ∈ Aal ⊆ A) set of actions Cal i ⊆ C. Definition 6.",
                "Al val - the total evaluation value that agent Ai will achieve while being part of Aal is the sum of Evali (Eval for Ai) of each of Aal j Eval values after taking their own α actions (via the agent(α) predicate): Al val(Ai, Cal , Aal , w) = α∈Cal Evali(Aal j , agent(α), w) Definition 7.",
                "Al TrH - is a number representing an Al val The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) 553 threshold; above it, the alliance can be said to be a highly beneficial alliance.",
                "The value of Al TrH will be calculated dynamically according to the progress of the interaction, as can be seen in [7].",
                "After an alliance is formed, its members are now working in their normal adversarial environment, as well as according to the mental states and axioms required for their interactions as part of the alliance.",
                "The following Alliance model (AL) specifies the conditions under which the group Aal can be said to be in an alliance and working with a new and constrained set of actions Cal , at time Tn.",
                "AL(Aal , Cal , w, Tn) 1.",
                "Aal has a MB that all members are part of Aal : MB(Aal , (∀Aal i ∈ Aal )member(Aal i , Aal ), Tn) 2.",
                "Aal has a MB that the group be maintained: MB(Aal , (∀Aal i ∈ Aal )Int.Th (Ai, member(Ai, Aal ), Tn, Tn+1, Co), Tn) 3.",
                "Aal has a MB that being members gives them high utility value: MB(Aal , (∀Aal i ∈ Aal )Al val(Aal i , Cal , Aal , w) ≥ Al TrH, Tn) Members profiles are a crucial part of successful alliances.",
                "We assume that agents that have more accurate profiles of their adversaries will be more successful in such environments.",
                "Such agents will be able to predict when a member is about to breach the alliances contract (item 2 in the above model), and take counter measures (when item 3 will falsify).",
                "The robustness of the alliance is in part a function of its members trustfulness measure, objective position estimation, and other profile properties.",
                "We should note that an agent can simultaneously be part of more than one alliance.",
                "Such a temporary alliance, where the group members do not have a joint goal but act collaboratively for the interest of their own individual goals, is classified as a Treatment Group by modern psychologists [12] (in contrast to a Task Group, where its members have a joint goal).",
                "The Shared Activity model as presented in [5] modeled Treatment Group behavior using the same SharedPlans formalization.",
                "When comparing both definitions of an alliance and a Treatment Group we found an unsurprising resemblance between both models: the environment models definitions are almost identical (see SAs definitions in [5]), and their Selfish-Act and Cooperative Act axioms conform to our adversarial agents behavior.",
                "The main distinction between both models is the integration of a Helpful-behavior act axiom, in the Shared Activity which cannot be part of ours.",
                "This axiom states that an agent will consider taking action that will lower its Eval value (to a certain lower bound), if it believes that a group partner will gain a significant benefit.",
                "Such behavior cannot occur in a pure adversarial environment (as a zero-sum game is), where the alliance members are constantly on watch to manipulate their alliance to their own advantage.",
                "A6.",
                "Evaluation Maximization Axiom.",
                "In a case when all other axioms are inapplicable, we will proceed with the action that maximizes the heuristic value as computed in the Eval function. (∀Aag, Ao ∈ A, α ∈ Cag, Tn, w ∈ W) Bel(Aag, (∀γ ∈ Cag)Eval(Aag, α, w) ≥ Eval(Aag, γ, w), Tn) ⇒ Pot.Int.To(Aag, α, Tn, Tα, w) T1.",
                "Optimality on Eval = Utility The above axiomatic model handles situations where the Utility is unknown and the agents are bounded rational agents.",
                "The following theorem shows that in bilateral interactions, where the agents have the real Utility function (i.e., Eval = Utility) and are rational agents, the axioms provide the same optimal result as classic adversarial search (e.g., Min-Max).",
                "Theorem 1.",
                "Let Ae ag be an unbounded rational AE agent using the Eval heuristic evaluation function, Au ag be the same agent using the true Utility function, and Ao be a sole unbounded utility-based rational adversary.",
                "Given that Eval = Utility: (∀α ∈ CAu ag , α ∈ CAe ag , Tn, w ∈ W) Pot.Int.To(Au ag, α, Tn, Tα, w) → Pot.Int.To(Ae ag, α , Tn, Tα, w) ∧ ((α = α ) ∨ (Utility(Au ag, α, w) = Eval(Ae ag, α , w))) Sketch of proof - Given that Au ag has the real utility function and unbounded resources, it can generate the full game tree and run the optimal MinMax algorithm to choose the highest utility value action, which we denote by, α.",
                "The proof will show that Ae ag, using the AE axioms, will select the same or equal utility α (when there is more than one action with the same max utility) when Eval = Utility. (A1) Goal achieving axiom - suppose there is an α such that its completion will achieve Au ags goal.",
                "It will obtain the highest utility by Min-Max for Au ag.",
                "The Ae ag agent will select α or another action with the same utility value via A1.",
                "If such α does not exist, Ae ag cannot apply this axiom, and proceeds to A2. (A2) Preventive act axiom - (1) Looking at the basic case (see Prop1), if there is a β which leads Ao to achieve its goal, then a preventive action α will yield the highest utility for Au ag.",
                "Au ag will choose it through the utility, while Ae ag will choose it through A2. (2) In the general case, β is a highly beneficial action for Ao, thus yields low utility for Au ag, which will guide it to select an α that will prevent β, while Ae ag will choose it through A2.1 If such β does not exist for Ao, then A2 is not applicable, and Ae ag can proceed to A3. (A3) Suboptimal tactical move axiom - When using a heuristic Eval function, Ae ag has a partial belief in the profile of its adversary (item 4 in AE model), which may lead it to believe in SetActions (Prop1).",
                "In our case, Ae ag is holding a full profile on its optimal adversary and knows that Ao will behave optimally according to the real utility values on the complete search tree, therefore, any belief about suboptimal SetAction cannot exist, yielding this axiom inapplicable.",
                "Ae ag will proceed to A4. (A4) Profile detection axiom - Given that Ae ag has the full profile of Ao, none of Ae ags actions can increase its knowledge.",
                "That axiom will not be applied, and the agent will proceed with A6 (A5 will be disregarded because the interaction is bilateral). (A6) Evaluation maximization axiom - This axiom will select the max Eval for Ae ag.",
                "Given that Eval = Utility, the same α that was selected by Au ag will be selected. 3.",
                "EVALUATION The main purpose of our experimental analysis is to evaluate the models behavior and performance in a real adversarial environment.",
                "This section investigates whether bounded 1 A case where following the completion of β there exists a γ which gives high utility for Agent Au ag, cannot occur because Ao uses the same utility, and γs existence will cause it to classify β as a low utility action. 554 The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) rational agents situated in such adversarial environments will be better off applying our suggested behavioral axioms. 3.1 The Domain To explore the use of the above model and its behavioral axioms, we decided to use the Connect-Four game as our adversarial environment.",
                "Connect-Four is a 2-player, zerosum game which is played using a 6x7 matrix-like board.",
                "Each turn, a player drops a disc into one of the 7 columns (the set of 21 discs is usually colored yellow for player 1 and red for player 2; we will use White and Black respectively to avoid confusion).",
                "The winner is the first player to complete a horizontal, vertical, or diagonal set of four discs with its color.",
                "On very rare occasions, the game might end in a tie if all the empty grids are filled, but no player managed to create a 4-disc set.",
                "The Connect-Four game was solved in [1], where it is shown that the first player (playing with the white discs) can force a win by starting in the middle column (column 4) and playing optimally However, the optimal strategy is very complex, and difficult to follow even for complex bounded rational agents, such as human players.",
                "Before we can proceed checking agent behavior, we must first verify that the domain conforms to the adversarial environments definition as given above (which the behavioral axioms are based on).",
                "First, when playing a Connect-Four game, the agent has an intention to win the game (item 1).",
                "Second (item 2), our agent believes that in Connect-Four there can only be one winner (or no winner at all in the rare occurrence of a tie).",
                "In addition, our agent believes that its opponent to the game will try to win (item 3), and we hope it has some partial knowledge (item 4) about its adversary (this knowledge can vary from nothing, through simple facts such as age, to strategies and weaknesses).",
                "Of course, not all Connect-Four encounters are adversarial.",
                "For example, when a parent is playing the game with its child, the following situation might occur: the child, having a strong incentive to win, treats the environment as adversarial (it intends to win, understands that there can only be one winner, and believes that its parent is trying to beat him).",
                "However, the parents point of view might see the environment as an educational one, where its goal is not to win the game, but to cause enjoyment or practice strategic reasoning.",
                "In such an educational environment, a new set of behavioral axioms might be more beneficial to the parents goals than our suggested adversarial behavioral axioms. 3.2 Axiom Analysis After showing that the Connect-Four game is indeed a zero-sum, bilateral adversarial environment, the next step is to look at players behaviors during the game and check whether behaving according to our model does improve performance.",
                "To do so we have collected log files from completed Connect-Four games that were played by human players over the Internet.",
                "Our collected log file data came from Play by eMail (PBeM) sites.",
                "These are web sites that host email games, where each move is taken by an email exchange between the server and the players.",
                "Many such sites archives contain real competitive interactions, and also maintain a ranking system for their members.",
                "Most of the data we used can be found in [6].",
                "As can be learned from [1], Connect-Four has an optimal strategy and a considerable advantage for the player who starts the game (which we call the White player).",
                "We will concentrate in our analysis on the second players moves (to be called Black).",
                "The White player, being the first to act, has the so-called initiative advantage.",
                "Having the advantage and a good strategy will keep the Black player busy reacting to its moves, instead of initiating threats.",
                "A threat is a combination of three discs of the same color, with an empty spot for the fourth winning disk.",
                "An open threat is a threat that can be realized in the opponents next move.",
                "In order for the Black player to win, it must somehow turn the tide, take the advantage and start presenting threats to the White player.",
                "We will explore Black players behavior and their conformance to our axioms.",
                "To do so, we built an application that reads log files and analyzes the Black players moves.",
                "The application contains two main components: (1) a Min-Max algorithm for evaluation of moves; (2) open threats detector for the discovering of open threats.",
                "The Min-Max algorithm will work to a given depth, d and for each move α will output the heuristic value for the next action taken by the player as written in the log file, h(α), alongside the maximum heuristic value, maxh(α), that could be achieved prior to taking the move (obviously, if h(α) = maxh(α), then the player did not do the optimal move heuristically).",
                "The threat detectors job is to notify if some action was taken in order to block an open threat (not blocking an open threat will probably cause the player to lose in the opponents next move).",
                "The heuristic function used by Min-Max to evaluate the players utility is the following function, which is simple to compute, yet provides a reasonable challenge to human opponents: Definition 8.",
                "Let Group be an adjacent set of four squares that are horizontal, vertical, or diagonal.",
                "Groupn b (Groupn w) be a Group with n pieces of the black (white) color and 4−n empty squares. h = ((Group1 b ∗α)+(Group2 b ∗β)+(Group3 b ∗γ)+(Group4 b ∗∞)) − ((Group1 w ∗α)+(Group2 w ∗β)+(Group3 w ∗γ)+(Group4 w ∗∞)) The values of α, β and δ can vary to form any desired linear combination; however, it is important to value them with the α < β < δ ordering in mind (we used 1, 4, and 8 as their respective values).",
                "Groups of 4 discs of the same color means victory, thus discovery of such a group will result in ∞ to ensure an extreme value.",
                "We now use our estimated evaluation function to evaluate the Black players actions during the Connect-Four adversarial interaction.",
                "Each game from the log file was input into the application, which processed and output a reformatted log file containing the h value of the current move, the maxh value that could be achieved, and a notification if an open threat was detected.",
                "A total of 123 games were analyzed (57 with White winning, and 66 with Black winning).",
                "A few additional games were manually ignored in the experiment, due to these problems: a player abandoning the game while the outcome is not final, or a blunt irrational move in the early stages of the game (e.g., not blocking an obvious winning group in the first opening moves).",
                "In addition, a single tie game was also removed.",
                "The simulator was run to a search depth of 3 moves.",
                "We now proceed to analyze the games with respect to each behavioral axiom.",
                "The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) 555 Table 1: Average heuristic difference analysis Black losses Black Won Avg minh -17.62 -12.02 Avg 3 lowest h moves (min3 h) -13.20 -8.70 3.2.1 Affirming the Suboptimal tactical move axiom The following section presents the heuristic evaluations of the Min-Max algorithm for each action, and checks the amount and extent of suboptimal tactical actions and their implications on performance.",
                "Table 1 shows results and insights from the games heuristic analysis, when search depth equals 3 (this search depth was selected for the results to be comparable to [9], see Section 3.2.3).",
                "The tables heuristic data is the difference between the present maximal heuristic value and the heuristic value of the action that was eventually taken by the player (i.e., the closer the number is to 0, the closer the action was to the maximum heuristic action).",
                "The first row presents the difference values of the action that had the maximal difference value among all the Black players actions in a given game, as averaged over all Blacks winning and losing games (see respective columns).",
                "In games in which the Black player loses, its average difference value was -17.62, while in games in which the Black player won, its average was -12.02.",
                "The second row expands the analysis by considering the 3 highest heuristic difference actions, and averaging them.",
                "In that case, we notice an average heuristic difference of 5 points between games which the Black player loses and games in which it wins.",
                "Nevertheless, the importance of those numbers is that they allowed us to take an educated guess on a threshold number of 11.5, as the value of the TrH constant, which differentiates between normal actions and highly beneficial ones.",
                "After finding an approximated TrH constant, we can proceed with an analysis of the importance of suboptimal moves.",
                "To do so we took the subset of games in which the minimum heuristic difference value for Blacks actions was 11.5.",
                "As presented in Table 2, we can see the different min3 h average of the 3 largest ranges and the respective percentage of games won.",
                "The first row shows that the Black player won only 12% of the games in which the average of its 3 highest heuristically difference actions (min3 h) was smaller than the suggested threshold, TrH = 11.5.",
                "The second row shows a surprising result: it seems that when min3 h > −4 the Black player rarely wins.",
                "Intuition would suggest that games in which the action evaluation values were closer to the maximal values will result in more winning games for Black.",
                "However, it seems that in the Connect-Four domain, merely responding with somewhat easily expected actions, without initiating a few surprising and suboptimal moves, does not yield good results.",
                "The last row sums up the main insights from the analysis; most of Blacks wins (83%) came when its min3 h was in the range of -11.5 to -4.",
                "A close inspection of those Black winning games shows the following pattern behind the numbers: after standard opening moves, Black suddenly drops a disc into an isolated column, which seems a waste of a move.",
                "White continues to build its threats, while usually disregarding Blacks last move, which in turn uses the isolated disc as an anchor for a future winning threat.",
                "The results show that it was beneficial for the Black player Table 2: Blacks winnings percentages % of games min3 h < −11.5 12% min3 h > −4 5% −11.5 ≤ min3 h ≤ −4 83% to take suboptimal actions and not give the current highest possible heuristic value, but will not be too harmful for its position (i.e., will not give high beneficial value to its adversary).",
                "As it turned out, learning the threshold is an important aspect of success: taking wildly risky moves (min3 h < −11.5) or trying to avoid them (min3 h > −4) reduces the Black players winning chances by a large margin. 3.2.2 Affirming the Profile Monitoring Axiom In the task of showing the importance of monitoring ones adversaries profiles, our log files could not be used because they did not contain repeated interactions between players, which are needed to infer the players knowledge about their adversaries.",
                "However, the importance of opponent modeling and its use in attaining tactical advantages was already studied in various domains ([3, 9] are good examples).",
                "In a recent paper, Markovitch and Reger [9] explored the notion of learning and exploitation of opponent weakness in competitive interactions.",
                "They apply simple learning strategies by analyzing examples from past interactions in a specific domain.",
                "They also used the Connect-Four adversarial domain, which can now be used to understand the importance of monitoring the adversarys profile.",
                "Following the presentation of their theoretical model, they describe an extensive empirical study and check the agents performance after learning the weakness model with past examples.",
                "One of the domains used as a competitive environment was the same Connect-Four game (Checkers was the second domain).",
                "Their heuristic function was identical to ours with three different variations (H1, H2, and H3) that are distinguished from one another in their linear combination coefficient values.",
                "The search depth for the players was 3 (as in our analysis).",
                "Their extensive experiments check and compare various learning strategies, risk factors, predefined feature sets and usage methods.",
                "The bottom line is that the Connect-Four domain shows an improvement from a 0.556 winning rate before modeling to a 0.69 after modeling (page 22).",
                "Their conclusions, showing improved performance when holding and using the adversarys model, justify the effort to monitor the adversary profile for continuous and repeated interactions.",
                "An additional point that came up in their experiments is the following: after the opponent weakness model has been learned, the authors describe different methods of integrating the opponent weakness model into the agents decision strategy.",
                "Nevertheless, regardless of the specific method they chose to work with, all integration methods might cause the agent to take suboptimal decisions; it might cause the agent to prefer actions that are suboptimal at the present decision junction, but which might cause the opponent to react in accordance with its weakness model (as represented by our agent) which in turn will be beneficial for us in the future.",
                "The agents behavior, as demonstrated in [9] further confirms and strengthens our Suboptimal Tactical Axiom as discussed in the previous section. 556 The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) 3.2.3 Additional Insights The need for the Goal Achieving, Preventive Act, and Evaluation Maximization axioms are obvious, and need no further verification.",
                "However, even with respect to those axioms, a few interesting insights came up in the log analysis.",
                "The Goal achieving and Preventive Act axioms, though theoretically trivial, seem to provide some challenge to a human player.",
                "In the initial inspection of the logs, we encountered few games2 where a player, for inexplicable reasons, did not block the other from winning or failed to execute its own winning move.",
                "We can blame those faults on the humans lack of attention, or a typing error in its move reply; nevertheless, those errors might occur in bounded rational agents, and the appropriate behavior needs to be axiomatized.",
                "A typical Connect-Four game revolves around generating threats and blocking them.",
                "In our analysis we looked for explicit preventive actions, i.e., moves that block a group of 3 discs, or that remove a future threat (in our limited search horizon).",
                "We found that in 83% of the total games there was at least one preventive action taken by the Black player.",
                "It was also found that Black averaged 2.8 preventive actions per game on the games in which it lost, while averaging 1.5 preventive actions per game when winning.",
                "It seems that Black requires 1 or 2 preventive actions to build its initial taking position, before starting to present threats.",
                "If it did not manage to win, it will usually prevent an extra threat or two before succumbing to White. 4.",
                "RELATED WORK Much research deals with the axiomatization of teamwork and mental states of individuals: some models use knowledge and belief [10], others have models of goals and intentions [8, 4].",
                "However, all these formal theories deal with agent teamwork and cooperation.",
                "As far as we know, our model is the first to provide a formalized model for explicit adversarial environments and agents behavior in it.",
                "The classical Min-Max adversarial search algorithm was the first attempt to integrate the opponent into the search space with a weak assumption of an optimally playing opponent.",
                "Since then, much effort has gone into integrating the opponent model into the decision procedure to predict future behavior.",
                "The M∗ algorithm presented by Carmel and Markovitch [2] showed a method of incorporating opponent models into adversary search, while in [3] they used learning to provide a more accurate opponent model in a 2player repeated game environment, where agents strategies were modeled as finite automata.",
                "Additional Adversarial planning work was done by Willmott et al. [13], which provided an adversarial planning approach to the game of GO.",
                "The research mentioned above dealt with adversarial search and the integration of opponent models into classical utilitybased search methods.",
                "That work shows the importance of opponent modeling and the ability to exploit it to an agents advantage.",
                "However, the basic limitations of those search methods still apply; our model tries to overcome those limitations by presenting a formal model for a new, mental state-based adversarial specification. 5.",
                "CONCLUSIONS We presented an Adversarial Environment model for a 2 These were later removed from the final analysis. bounded rational agent that is situated in an N-player, zerosum environment.",
                "We used the SharedPlans formalization to define the model and the axioms that agents can apply as behavioral guidelines.",
                "The model is meant to be used as a guideline for designing agents that need to operate in such adversarial environments.",
                "We presented empirical results, based on ConnectFour log file analysis, that exemplify the model and the axioms for a bilateral instance of the environment.",
                "The results we presented are a first step towards an expanded model that will cover all types of adversarial environments, for example, environments that are non-zero-sum, and environments that contain natural agents that are not part of the direct conflict.",
                "Those challenges and more will be dealt with in future research. 6.",
                "ACKNOWLEDGMENT This research was supported in part by Israel Science Foundation grants #1211/04 and #898/05. 7.",
                "REFERENCES [1] L. V. Allis.",
                "A knowledge-based approach of Connect-Four - the game is solved: White wins.",
                "Masters thesis, Free University, Amsterdam, The Netherlands, 1988. [2] D. Carmel and S. Markovitch.",
                "Incorporating opponent models into adversary search.",
                "In Proceedings of the Thirteenth National Conference on Artificial Intelligence, pages 120-125, Portland, OR, 1996. [3] D. Carmel and S. Markovitch.",
                "Opponent modeling in multi-agent systems.",
                "In G. Weiß and S. Sen, editors, Adaptation and Learning in Multi-Agent Systems, pages 40-52.",
                "Springer-Verlag, 1996. [4] B. J. Grosz and S. Kraus.",
                "Collaborative plans for complex group action.",
                "Artificial Intelligence, 86(2):269-357, 1996. [5] M. Hadad, G. Kaminka, G. Armon, and S. Kraus.",
                "Supporting collaborative activity.",
                "In Proc. of AAAI-2005, pages 83-88, Pittsburgh, 2005. [6] http://www.gamerz.net/˜pbmserv/. [7] S. Kraus and D. Lehmann.",
                "Designing and building a negotiating automated agent.",
                "Computational Intelligence, 11:132-171, 1995. [8] H. J. Levesque, P. R. Cohen, and J. H. T. Nunes.",
                "On acting together.",
                "In Proc. of AAAI-90, pages 94-99, Boston, MA, 1990. [9] S. Markovitch and R. Reger.",
                "Learning and exploiting relative weaknesses of opponent agents.",
                "Autonomous Agents and Multi-Agent Systems, 10(2):103-130, 2005. [10] Y. M. Ronald Fagin, Joseph Y. Halpern and M. Y. Vardi.",
                "Reasoning about knowledge.",
                "MIT Press, Cambridge, Mass., 1995. [11] P. Thagard.",
                "Adversarial problem solving: Modeling an oponent using explanatory coherence.",
                "Cognitive Science, 16(1):123-149, 1992. [12] R. W. Toseland and R. F. Rivas.",
                "An Introduction to Group Work Practice.",
                "Prentice Hall, Englewood Cliffs, NJ, 2nd edition edition, 1995. [13] S. Willmott, J. Richardson, A. Bundy, and J. Levine.",
                "An adversarial planning approach to Go.",
                "Lecture Notes in Computer Science, 1558:93-112, 1999.",
                "The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) 557"
            ],
            "original_annotated_samples": [],
            "translated_annotated_samples": [
                "Categorías y descriptores de sujetos I.2.11 [Inteligencia artificial]: agentes de inteligencia artificial distribuidos, sistemas multiagentes;I.2.4 [Inteligencia artificial]: Formalismos y métodos de representación del conocimiento -Diseño de términos generales de \"lógica modal\", teoría 1. Lógica modal"
            ],
            "translated_text": "",
            "candidates": [],
            "error": []
        }
    }
}