{
    "id": "H-31",
    "original_text": "A Study of Poisson Query Generation Model for Information Retrieval Qiaozhu Mei, Hui Fang, Chengxiang Zhai Department of Computer Science University of Illinois at Urbana-Champaign Urbana,IL 61801 {qmei2,hfang,czhai}@uiuc.edu ABSTRACT Many variants of language models have been proposed for information retrieval. Most existing models are based on multinomial distribution and would score documents based on query likelihood computed based on a query generation probabilistic model. In this paper, we propose and study a new family of query generation models based on Poisson distribution. We show that while in their simplest forms, the new family of models and the existing multinomial models are equivalent, they behave differently for many smoothing methods. We show that the Poisson model has several advantages over the multinomial model, including naturally accommodating per-term smoothing and allowing for more accurate background modeling. We present several variants of the new model corresponding to different smoothing methods, and evaluate them on four representative TREC test collections. The results show that while their basic models perform comparably, the Poisson model can outperform multinomial model with per-term smoothing. The performance can be further improved with two-stage smoothing. Categories and Subject Descriptors: H.3.3 [Information Search and Retrieval]: Retrieval Models General Terms: Algorithms 1. INTRODUCTION As a new type of probabilistic retrieval models, language models have been shown to be effective for many retrieval tasks [21, 28, 14, 4]. Among many variants of language models proposed, the most popular and fundamental one is the query-generation language model [21, 13], which leads to the query-likelihood scoring method for ranking documents. In such a model, given a query q and a document d, we compute the likelihood of generating query q with a model estimated based on document d, i.e., the conditional probability p(q|d). We can then rank documents based on the likelihood of generating the query. Virtually all the existing query generation language models are based on either multinomial distribution [19, 6, 28] or multivariate Bernoulli distribution [21, 18]. The multinomial distribution is especially popular and also shown to be quite effective. The heavy use of multinomial distribution is partly due to the fact that it has been successfully used in speech recognition, where multinomial distribution is a natural choice for modeling the occurrence of a particular word in a particular position in text. Compared with multivariate Bernoulli, multinomial distribution has the advantage of being able to model the frequency of terms in the query; in contrast, multivariate Bernoulli only models the presence and absence of query terms, thus cannot capture different frequencies of query terms. However, multivariate Bernoulli also has one potential advantage over multinomial from the viewpoint of retrieval: in a multinomial distribution, the probabilities of all the terms must sum to 1, making it hard to accommodate per-term smoothing, while in a multivariate Bernoulli, the presence probabilities of different terms are completely independent of each other, easily accommodating per-term smoothing and weighting. Note that term absence is also indirectly captured in a multinomial model through the constraint that all the term probabilities must sum to 1. In this paper, we propose and study a new family of query generation models based on the Poisson distribution. In this new family of models, we model the frequency of each term independently with a Poisson distribution. To score a document, we would first estimate a multivariate Poisson model based on the document, and then score it based on the likelihood of the query given by the estimated Poisson model. In some sense, the Poisson model combines the advantage of multinomial in modeling term frequency and the advantage of the multivariate Bernoulli in accommodating per-term smoothing. Indeed, similar to the multinomial distribution, the Poisson distribution models term frequencies, but without the constraint that all the term probabilities must sum to 1, and similar to multivariate Bernoulli, it models each term independently, thus can easily accommodate per-term smoothing. As in the existing work on multinomial language models, smoothing is critical for this new family of models. We derive several smoothing methods for Poisson model in parallel to those used for multinomial distributions, and compare the corresponding retrieval models with those based on multinomial distributions. We find that while with some smoothing methods, the new model and the multinomial model lead to exactly the same formula, with some other smoothing methods they diverge, and the Poisson model brings in more flexibility for smoothing. In particular, a key difference is that the Poisson model can naturally accommodate perterm smoothing, which is hard to achieve with a multinomial model without heuristic twist of the semantics of a generative model. We exploit this potential advantage to develop a new term-dependent smoothing algorithm for Poisson model and show that this new smoothing algorithm can improve performance over term-independent smoothing algorithms using either Poisson or multinomial model. This advantage is seen for both one-stage and two-stage smoothing. Another potential advantage of the Poisson model is that its corresponding background model for smoothing can be improved through using a mixture model that has a closed form formula. This new background model is shown to outperform the standard background model and reduce the sensitivity of retrieval performance to the smoothing parameter. The rest of the paper is organized as follows. In Section 2, we introduce the new family of query generation models with Poisson distribution, and present various smoothing methods which lead to different retrieval functions. In Section 3, we analytically compare the Poisson language model with the multinomial language model, from the perspective of retrieval. We then design empirical experiments to compare the two families of language models in Section 4. We discuss the related work in 5 and conclude in 6. 2. QUERY GENERATION WITH POISSON PROCESS In the query generation framework, a basic assumption is that a query is generated with a model estimated based on a document. In most existing work [12, 6, 28, 29], people assume that each query word is sampled independently from a multinomial distribution. Alternatively, we assume that a query is generated by sampling the frequency of words from a series of independent Poisson processes [20]. 2.1 The Generation Process Let V = {w1, ..., wn} be a vocabulary set. Let w be a piece of text composed by an author and c(w1), ..., c(wn) be a frequency vector representing w, where c(wi, w) is the frequency count of term wi in text w. In retrieval, w could be either a query or a document. We consider the frequency counts of the n unique terms in w as n different types of events, sampled from n independent homogeneous Poisson processes, respectively. Suppose t is the time period during which the author composed the text. With a homogeneous Poisson process, the frequency count of each event, i.e., the number of occurrences of wi, follows a Poisson distribution with associated parameter λit, where λi is a rate parameter characterizing the expected number of wi in a unit time. The probability density function of such a Poisson Distribution is given by P(c(wi, w) = k|λit) = e−λit (λit)k k! Without losing generality, we set t to the length of the text w (people write one word in a unit time), i.e., t = |w|. With n such independent Poisson processes, each explaining the generation of one term in the vocabulary, the likelihood of w to be generated from such Poisson processes can be written as p(w|Λ) = n i=1 p(c(wi, w)|Λ) = n i=1 e−λi·|w| (λi · |w|)c(wi,w) c(wi, w)! where Λ = {λ1, ..., λn} and |w| = n i=1 c(wi, w). We refer to these n independent Poisson processes with parameter Λ as a Poisson Language Model. Let D = {d1, ..., dm} be an observed set of document samples generated from the Poisson process above. The maximum likelihood estimate (MLE) of λi is ˆλi = d∈D c(wi, d) d∈D w ∈V c(w , d) Note that this MLE is different from the MLE for the Poisson distribution without considering the document lengths, which appears in [22, 24]. Given a document d, we may estimate a Poisson language model Λd using d as a sample. The likelihood that a query q is generated from the document language model Λd can be written as p(q|d) = w∈V p(c(w, q)|Λd) (1) This representation is clearly different from the multinomial query generation model as (1) the likelihood includes all the terms in the vocabulary V , instead of only those appearing in q, and (2) instead of the appearance of terms, the event space of this model is the frequencies of each term. In practice, we have the flexibility to choose the vocabulary V . In one extreme, we can use the vocabulary of the whole collection. However, this may bring in noise and considerable computational cost. In the other extreme, we may focus on the terms in the query and ignore other terms, but some useful information may be lost by ignoring the nonquery terms. As a compromise, we may conflate all the non-query terms as one single pseudo term. In other words, we may assume that there is exactly one non-query term in the vocabulary for each query. In our experiments, we adopt this pseudo non-query term strategy. A document can be scored with the likelihood in Equation 1. However, if a query term is unseen in the document, the MLE of the Poisson distribution would assign zero probability to the term, causing the probability of the query to be zero. As in existing language modeling approaches, the main challenge of constructing a reasonable retrieval model is to find a smoothed language model for p(·|d). 2.2 Smoothing in Poisson Retrieval Model In general, we want to assign non-zero rates for the query terms that are not seen in document d. Many smoothing methods have been proposed for multinomial language models[2, 28, 29]. In general, we have to discount the probabilities of some words seen in the text to leave some extra probability mass to assign to the unseen words. In Poisson language models, however, we do not have the same constraint as in a multinomial model (i.e., w∈V p(w|d) = 1). Thus we do not have to discount the probability of seen words in order to give a non-zero rate to an unseen word. Instead, we only need to guarantee that k=0,1,2,... p(c(w, d) = k|d) = 1. In this section, we introduce three different strategies to smooth a Poisson language model, and show how they lead to different retrieval functions. 2.2.1 Bayesian Smoothing using Gamma Prior Following the risk minimization framework in [11], we assume that a document is generated by the arrival of terms in a time period of |d| according to the document language model, which essentially consists of a vector of Poisson rates for each term, i.e., Λd = λd,1, ..., λd,|V | . A document is assumed to be generated from a potentially different model. Given a particular document d, we want to estimate Λd. The rate of a term is estimated independently of other terms. We use Bayesian estimation with the following Gamma prior, which has two parameters, α and β: Gamma(λ|α, β) = βα Γ(α) λα−1 e−βλ For each term w, the parameters αw and βw are chosen to be αw = µ ∗ λC,w and βw = µ, where µ is a parameter and λC,w is the rate of w estimated from some background language model, usually the collection language model. The posterior distribution of Λd is given by p(Λd|d, C) ∝ w∈V e−λw(|d|+µ) λ c(w,d)+µλC,w−1 w which is a product of |V | Gamma distributions with parameters c(w, d) + µλC,w and |d| + µ for each word w. Given that the Gamma mean is α β , we have ˆλd,w = λd,w λd,wp(λd,w|d, C)dλd,w = c(w, d) + µλC,w |d| + µ This is precisely the smoothed estimate of multinomial language model with Dirichlet prior [28]. 2.2.2 Interpolation (Jelinek-Mercer) Smoothing Another straightforward method is to decompose the query generation model as a mixture of two component models. One is the document language model estimated with maximum likelihood estimator, and the other is a model estimated from the collection background, p(·|C), which assigns non-zero rate to w. For example, we may use an interpolation coefficient between 0 and 1 (i.e., δ ∈ [0, 1]). With this simple interpolation, we can score a document with Score(d, q) = w∈V log((1 − δ)p(c(w, q)|d) + δp(c(w, q)|C)) (2) Using the maximum likelihood estimator for p(·|d), we have λd,w = c(w,d) |d| , thus Equation 2 becomes Score(d, q) ∝ w∈d∩q [log(1 + 1 − δ δ e−λd,w|q| (λd,w|q|)c(w,q) c(w, q)! · p(c(w, q)|C) ) − log (1 − δ)e−λd,w|q| + δp(c(w, q) = 0|C) 1 − δ + δp(c(w, q) = 0|C) ] + w∈d log (1 − δ)e−λd,w|q| + δp(c(w, q) = 0|C) 1 − δ + δp(c(w, q) = 0|C) We can also use a Poisson language model for p(·|C), or use some other frequency-based models. In the retrieval formula above, the first summation can be computed efficiently. The second summation can be actually treated as a document prior, which penalizes long documents. As the second summation is difficult to compute efficiently, we conflate all non-query terms as one pseudo non-queryterm, denoted as N. Using the pseudo-term formulation and a Poisson collection model, we can rewrite the retrieval formula as Score(d, q) ∝ w∈d∩q log(1 + 1 − δ δ e−λd,w (λd,w|q|)c(w,q) e−λd,C |q| (λd,C )c(w,q) ) + log (1 − δ)e−λd,N |q| + δe−λC,N |q| 1 − δ + δe−λC,N |q| (3) where λd,N = |d|− w∈q c(w,d) |d| and λC,N = |C|− w∈q c(w,C) |C| . 2.2.3 Two-Stage Smoothing As discussed in [29], smoothing plays two roles in retrieval: (1) to improve the estimation of the document language model, and (2) to explain the common terms in the query. In order to distinguish the content and non-discriminative words in a query, we follow [29] and assume that a query is generated by sampling from a two-component mixture of Poisson language models, with one component being the document model Λd and the other being a query background language model p(·|U). p(·|U) models the typical term frequencies in the users queries. We may then score each document with the query likelihood computed using the following two-stage smoothing model: p(c(w, q)|Λd, U) = (1 − δ)p(c(w, q)|Λd) + δp(c(w, q)|U) (4) where δ is a parameter, roughly indicating the amount of noise in q. This looks similar to the interpolation smoothing, except that p(·|Λd) now should be a smoothed language model, instead of the one estimated with MLE. With no prior knowledge on p(·|U), we could set it to p(·|C). Any smoothing methods for the document language model can be used to estimate p(·|d) such as the Gamma smoothing as discussed in Section 2.2.1. The empirical study of the smoothing methods is presented in Section 4. 3. ANALYSIS OF POISSON LANGUAGE MODEL From the previous section, we notice that the Poisson language model has a strong connection to the multinomial language model. This is expected since they both belong to the exponential family [26]. However, there are many differences when these two families of models are applied with different smoothing methods. From the perspective of retrieval, will these two language models perform equivalently? If not, which model provides more benefits to retrieval, or provides flexibility which could lead to potential benefits? In this section, we analytically discuss the retrieval features of the Poisson language models, by comparing their behavior with that of the multinomial language models. 3.1 The Equivalence of Basic Models Let us begin with the assumption that all the query terms appear in every document. Under this assumption, no smoothing is needed. A document can be scored by the log likelihood of the query with the maximum likelihood estimate: Score(d, q) = w∈V log e−λd,w|q| (λd,w|q|)c(w,q) c(w, q)! (5) Using the MLE, we have λd,w = c(w,d) w∈V c(w,d) . Thus Score(d, q) ∝ c(w,q)>0 c(w, q) log c(w, d) w∈V c(w, d) This is exactly the log likelihood of the query if the document language model is a multinomial with maximum likelihood estimate. Indeed, even with Gamma smoothing, when plugging λd,w = c(w,d)+µλC,w |d|+µ and λC,w = c(w,C) |C| into Equation 5, it is easy to show that Score(d, q) ∝ w∈q∩d c(w, q) log(1 + c(w, d) µ · c(w,C) |C| ) + |q| log µ |d| + µ (6) which is exactly the Dirichlet retrieval formula in [28]. Note that this equivalence holds only when the document length variation is modeled with Poisson process. This derivation indicates the equivalence of the basic Poisson and multinomial language models for retrieval. With other smoothing strategies, however, the two models would be different. Nevertheless, with this equivalence in basic models, we could expect that the Poisson language model performs comparably to the multinomial language model in retrieval, if only simple smoothing is explored. Based on this equivalence analysis, one may ask, why we should pursue the Poisson language model. In the following sections, we show that despite the equivalence in their basic models, the Poisson language model brings in extra flexibility for exploring advanced techniques on various retrieval features, which could not be achieved with multinomial language models. 3.2 Term Dependent Smoothing One flexibility of the Poisson language model is that it provides a natural framework to accommodate term dependent (per-term) smoothing. Existing work on language model smoothing has already shown that different types of queries should be smoothed differently according to how discriminative the query terms are. [7] also predicted that different terms should have a different smoothing weights. With multinomial query generation models, people usually use a single smoothing coefficient to control the combination of the document model and the background model [28, 29]. This parameter can be made specific for different queries, but always has to be a constant for all the terms. This is mandatory since a multinomial language model has the constraint that w∈V p(w|d) = 1. However, from retrieval perspective, different terms may need to be smoothed differently even if they are in the same query. For example, a non-discriminative term (e.g., the, is) is expected to be explained more with the background model, while a content term (e.g., retrieval, bush) in the query should be explained with the document model. Therefore, a better way of smoothing would be to set the interpolation coefficient (i.e., δ in Formula 2 and Formula 3) specifically for each term. Since the Poisson language model does not have the sum-to-one constraint across terms, it can easily accommodate per-term smoothing without needing to heuristically twist the semantics of a generative model as in the case of multinomial language models. Below we present a possible way to explore term dependent smoothing with Poisson language models. Essentially, we want to use a term-specific smoothing coefficient δ in the linear combination, denoted as δw. This coefficient should intuitively be larger if w is a common word and smaller if it is a content word. The key problem is to find a method to assign reasonable values to δw. Empirical tuning is infeasible for so many parameters. We may instead estimate the parameters ∆ = {δ1, ..., δ|V |} by maximizing the likelihood of the query given the mixture model of p(q|ΛQ) and p(q|U), where ΛQ is the true query model to generate the query and p(q|U) is a query background model as discussed in Section 2.2.3. With the model p(q|ΛQ) hidden, the query likelihood is p(q|∆, U) = ΛQ w∈V ((1 − δw)p(c(w, q)|ΛQ) + δwp(c(w, q)|U))P(ΛQ|U)dΛQ If we have relevant documents for each query, we can approximate the query model space with the language models of all the relevant documents. Without relevant documents, we opt to approximate the query model space with the models of all the documents in the collection. Setting p(·|U) as p(·|C), the query likelihood becomes p(q|∆, U) = d∈C πd w∈V ((1−δw)p(c(w, q)|ˆΛd)+δwp(c(w, q)|C)) where πd = p(ˆΛd|U). p(·|ˆΛd) is an estimated Poisson language model for document d. If we have prior knowledge on p(ˆΛd|U), such as which documents are relevant to the query, we can set πd accordingly, because what we want is to find ∆ that can maximize the likelihood of the query given relevant documents. Without this prior knowledge, we can leave πd as free parameters, and use the EM algorithm to estimate πd and ∆. The updating functions are given as π (k+1) d = πd w∈V ((1 − δw)p(c(w, q)|ˆΛd) + δwp(c(w, q)|C)) d∈C πd w∈V ((1 − δw)p(c(w, q)|ˆΛd) + δwp(c(w, q)|C)) and δ (k+1) w = d∈C πd δwp(c(w, q)|C)) (1 − δw)p(c(w, q)|ˆΛd) + δwp(c(w, q)|C)) As discussed in [29], we only need to run the EM algorithm for several iterations, thus the computational cost is relatively low. We again assume our vocabulary containing all query terms plus a pseudo non-query term. Note that the function does not give an explicit way of estimating the coefficient for the unseen non-query term. In our experiments, we set it to the average over δw of all query terms. With this flexibility, we expect Poisson language models could improve the retrieval performance, especially for verbose queries, where the query terms have various discriminative values. In Section 4, we use empirical experiments to prove this hypothesis. 3.3 Mixture Background Models Another flexibility is to explore different background (collection) models (i.e., p(·|U), or p(·|C)). One common assumption made in language modeling information retrieval is that the background model is a homogeneous model of the document models [28, 29]. Similarly, we can also make the assumption that the collection model is a Poisson language model, with the rates λC,w = d∈C c(w,d) |C| . However, this assumption usually does not hold, since the collection is far more complex than a single document. Indeed, the collection usually consists of a mixture of documents with various genres, authors, and topics, etc. Treating the collection model as a mixture of document models, instead of a single pseudo-document model is more reasonable. Existing work of multinomial language modeling has already shown that a better modeling of background improves the retrieval performance, such as clusters [15, 10], neighbor documents [25], and aspects [8, 27]. All the approaches can be easily adopted using Poisson language models. However, a common problem of these approaches is that they all require heavy computation to construct the background model. With Poisson language modeling, we show that it is possible to model the mixture background without paying for the heavy computational cost. Poisson Mixture [3] has been proposed to model a collection of documents, which can fit the data much better than a single Poisson. The basic idea is to assume that the collection is generated from a mixture of Poisson models, which has the general form of p(x = k|PM) = λ p(λ)p(x = k|λ)dλ p(·|λ) is a single Poisson model and p(λ) is an arbitrary probability density function. There are three well known Poisson mixtures [3]: 2-Poisson, Negative Binomial, and the Katzs K-Mixture [9]. Note that the 2-Poisson model has actually been explored in probabilistic retrieval models, which led to the well-known BM25 formula [22]. All these mixtures have closed forms, and can be estimated from the collection of documents efficiently. This is an advantage over the multinomial mixture models, such as PLSI [8] and LDA [1], for retrieval. For example, the probability density function of Katzs K-Mixture is given as p(c(w) = k|αw, βw) = (1 − αw)ηk,0 + αw βw + 1 ( βw βw + 1 )k where ηk,0 = 1 when k = 0, and 0 otherwise. With the observation of a collection of documents, αw and βw can be estimated as βw = cf(w) − df(w) df(w) and αw = cf(w) Nβw where cf(w) and df(w) are the collection frequency and document frequency of w, and N is the number of documents in the collection. To account for the different document lengths, we assume that βw is a reasonable estimation for generating a document of the average length, and use β = βw avdl |q| to generate the query. This Poisson mixture model can be easily used to replace P(·|C) in the retrieval functions 3 and 4. 3.4 Other Possible Flexibilities In addition to term dependent smoothing and efficient mixture background, a Poisson language model has also some other potential advantages. For example, in Section 2, we see that Formula 2 introduces a component which does document length penalization. Intuitively, when the document has more unique words, it will be penalized more. On the other hand, if a document is exactly n copies of another document, it would not get over penalized. This feature is desirable and not achieved with the Dirichlet model [5]. Potentially, this component could penalize a document according to what types of terms it contains. With term specific settings of δ, we could get even more flexibility for document length normalization. Pseudo-feedback is yet another interesting direction where the Poission model might be able to show its advantage. With model-based feedback, we could again relax the combination coefficients of the feedback model and the background model, and allow different terms to contribute differently to the feedback model. We could also utilize the relevant documents to learn better per-term smoothing coefficients. 4. EVALUATION In Section 3, we analytically compared the Poisson language models and multinomial language models from the perspective of query generation and retrieval. In this section, we compare these two families of models empirically. Experiment results show that the Poisson model with perterm smoothing outperforms multinomial model, and the performance can be further improved with two-stage smoothing. Using Poisson mixture as background model also improves the retrieval performance. 4.1 Datasets Since retrieval performance could significantly vary from one test collection to another, and from one query to another, we select four representative TREC test collections: AP, Trec7, Trec8, and Wt2g(Web). To cover different types of queries, we follow [28, 5], and construct short-keyword (SK, keyword title), short-verbose (SV, one sentence description), and long-verbose (LV, multiple sentences) queries. The documents are stemmed with the Porters stemmer, and we do not remove any stop word. For each parameter, we vary its value to cover a reasonably wide range. 4.2 Comparison to Multinomial We compare the performance of the Poisson retrieval models and multinomial retrieval models using interpolation (JelinekMercer, JM) smoothing and Bayesian smoothing with conjugate priors. Table 1 shows that the two JM-smoothed models perform similarly on all data sets. Since the Dirichlet Smoothing for multinomial language model and the Gamma Smoothing for Poisson language model lead to the same retrieval formula, the performance of these two models are jointly presented. We see that Dirichlet/Gamma smoothing methods outperform both Jelinek-Mercer smoothing methods. The parameter sensitivity curves for two Jelinek-Mercer 0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1 0 0.05 0.1 0.15 0.2 0.25 0.3 Dataset: Trec8 Parameter: δ AveragePrecision JM−Multinomial: LV JM−Multinomial: SV JM−Multinomial: SK JM−Poisson: SK JM−Poisson: SV JM−Poisson: LV Figure 1: Poisson and multinomial performs similarly with Jelinek-Mercer smoothing smoothing methods are shown in Figure 1. Clearly, these two methods perform similarly either in terms of optimality Data Query JM-Multinomial JM-Poisson Dirichlet/Gamma Per-term 2-Stage Poisson MAP InitPr Pr@5d MAP InitPr Pr@5d MAP InitPr Pr@5d MAP InitPr Pr@5d AP88-89 SK 0.203 0.585 0.356 0.203 0.585 0.358 0.224 0.629 0.393 0.226 0.630 0.396 SV 0.187 0.580 0.361 0.183 0.571 0.345 0.204 0.613 0.387 0.217* 0.603 0.390 LV 0.283 0.716 0.480 0.271 0.692 0.470 0.291 0.710 0.496 0.304* 0.695 0.510 Trec7 SK 0.167 0.635 0.400 0.168 0.635 0.404 0.186 0.687 0.428 0.185 0.646 0.436 SV 0.174 0.655 0.432 0.176 0.653 0.432 0.182 0.666 0.432 0.196* 0.660 0.440 LV 0.223 0.730 0.496 0.215 0.766 0.488 0.224 0.748 0.52 0.236* 0.738 0.512 Trec8 SK 0.239 0.621 0.440 0.239 0.621 0.436 0.257 0.718 0.496 0.256 0.704 0.468 SV 0.231 0.686 0.448 0.234 0.702 0.456 0.228 0.691 0.456 0.246* 0.692 0.476 LV 0.265 0.796 0.548 0.261 0.757 0.520 0.260 0.741 0.492 0.274* 0.766 0.508 Web SK 0.250 0.616 0.380 0.250 0.616 0.380 0.302 0.767 0.468 0.307 0.739 0.468 SV 0.214 0.611 0.392 0.217 0.609 0.384 0.273 0.693 0.508 0.292* 0.703 0.480 LV 0.266 0.790 0.464 0.259 0.776 0.452 0.283 0.756 0.496 0.311* 0.759 0.488 Table 1: Performance comparison between Poisson and Multinomial retrieval models: basic models perform comparably; term dependent two-stage smoothing significantly improves Poisson An asterisk (*) indicates that the difference between the performance of the term dependent two-stage smoothing and that of the Dirichlet/Gamma single smoothing is statistically significant according to the Wilcoxon signed rank test at the level of 0.05. or sensitivity. This similarity of performance is expected as we discussed in Section 3.1. Although the Poisson model and multinomial model are similar in terms of the basic model and/or with simple smoothing methods, the Poisson model has great potential and flexibility to be further improved. As shown in the rightmost column of Table 1, term dependent two-stage Poisson model consistently outperforms the basic smoothing models, especially for verbose queries. This model is given in Formula 4, with a Gamma smoothing for the document model p(·|d), and δw, which is term dependent. The parameter µ of the first stage Gamma smoothing is empirically tuned. The combination coefficients (i.e., ∆), are estimated with the EM algorithm in Section 3.2. The parameter sensitivity curves for Dirichlet/Gamma and the per-term two-stage smoothing model are plotted in Figure 2. The per-term two-stage smoothing method is less sensitive to the parameter µ than Dirichlet/Gamma, and yields better optimal performance. 0 1000 2000 3000 4000 5000 6000 7000 8000 9000 10000 0.1 0.12 0.14 0.16 0.18 0.2 0.22 Dataset: AP; Query Type: SV Parameter: µ AveragePrecision Dirichlet/Gamma Smoothing Term Dependent 2−Stage Figure 2: Term dependent two-stage smoothing of Poisson outperforms Dirichlet/Gamma In the following subsections, we conduct experiments to demonstrate how the flexibility of the Poisson model could be utilized to achieve better performance, which we cannot achieve with multinomial language models. 4.3 Term Dependent Smoothing To test the effectiveness of the term dependent smoothing, we conduct the following two experiments. In the first experiment, we relax the constant coefficient in the simple Jelinek-Mercer smoothing formula (i.e., Formula 3), and use the EM algorithm proposed in Section 3.2 to find a δw for each unique term. Since we are using the EM algorithm to iteratively estimate the parameters, we usually do not want the probability of p(·|d) to be zero. We then use a simple Laplace method to slightly smooth the document model before it goes into the EM iterations. The documents are then still scored with Formula 3, but using learnt δw. The results are labeled with JM+L. in Table 2. Data Q JM JM JM+L. 2-Stage 2-Stage (MAP) PT: No Yes Yes No Yes AP SK 0.203 0.204 0.206 0.223 0.226* SV 0.183 0.189 0.214* 0.204 0.217* Trec7 SK 0.168 0.171 0.174 0.186 0.185 SV 0.176 0.147 0.198* 0.194 0.196 Trec8 SK 0.239 0.240 0.227* 0.257 0.256 SV 0.234 0.223 0.249* 0.242 0.246* Web SK 0.250 0.236 0.220* 0.291 0.307* SV 0.217 0.232 0.261* 0.273 0.292* Table 2: Term dependent smoothing improves retrieval performance An asterisk (*) in Column 3 indicates that the difference between the JM+L. method and JM method is statistically significant; an asterisk (*) in Column 5 means that the difference between term dependent two-stage method and query dependent two-stage method is statistically significant; PT stands for per-term. With term dependent coefficients, the performance of the Jelinek-Mercer Poisson model is improved in most cases. However, in some cases (e.g., Trec7/SV), it performs poorly. This might be caused by the problem of EM estimation with unsmoothed document models. Once non-zero probability is assigned to all the terms before entering the EM iteration, the performance on verbose queries can be improved significantly. This indicates that there is still room to find better methods to estimate δw. Please note that neither the perterm JM method nor the JM+L. method has a parameter to tune. As shown in Table 1, the term dependent two-stage smoothing can significantly improve retrieval performance. To understand whether the improvement is contributed by the term dependent smoothing or the two-stage smoothing framework, we design another experiment to compare the perterm two-stage smoothing with the two-stage smoothing method proposed in [29]. Their method managed to find coefficients specific to the query, thus a verbose query would use a higher δ. However, since their model is based on multinomial language modeling, they could not get per-term coefficients. We adopt their method to the Poisson two-stage smoothing, and also estimate a per-query coefficient for all the terms. We compare the performance of such a model with the per-term two-stage smoothing model, and present the results in the right two columns in Table 2. Again, we see that the per-term two-stage smoothing outperforms the per-query two-stage smoothing, especially for verbose queries. The improvement is not as large as how the perterm smoothing method improves over Dirichlet/Gamma. This is expected, since the per-query smoothing has already addressed the query discrimination problem to some extent. This experiment shows that even if the smoothing is already per-query, making it per-term is still beneficial. In brief, the per-term smoothing improved the retrieval performance of both one-stage and two-stage smoothing method. 4.4 Mixture Background Model In this section, we conduct experiments to examine the benefits of using a mixture background model without extra computational cost, which can not be achieved for multinomial models. Specifically, in retrieval formula 3, instead of using a single Poisson distribution to model the background p(·|C), we use Katzs K-Mixture model, which is essentially a mixture of Poisson distributions. p(·|C) can be computed efficiently with simple collection statistics, as discussed in Section 3.3. Data Query JM. Poisson JM. K-Mixture AP SK 0.203 0.204 SV 0.183 0.188* Trec-7 SK 0.168 0.169 SV 0.176 0.178* Trec-8 SK 0.239 0.239 SV 0.234 0.238* Web SK 0.250 0.250 SV 0.217 0.223* Table 3: K-Mixture background model improves retrieval performance The performance of the JM retrieval model with single Poisson background and with Katzs K-Mixture background model is compared in Table 3. Clearly, using K-Mixture to model the background model outperforms the single Poisson background model in most cases, especially for verbose queries where the improvement is statistically significant. Figure 3 shows that the performance changes over different parameters for short verbose queries. The model using K-Mixture background is less sensitive than the one using single Poisson background. Given that this type of mixture 0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1 0 0.05 0.1 0.15 0.2 0.25 Data: Trec8; Query: SV Parameter: δ AveragePrecision Poisson Background K−Mixture Background Figure 3: K-Mixture background model deviates the sensitivity of verbose queries background model does not require any extra computation cost, it would be interesting to study whether using other mixture Poisson models, such as 2-Poisson and negative Binomial, could help the performance. 5. RELATED WORK To the best of our knowledge, there has been no study of query generation models based on Poisson distribution. Language models have been shown to be effective for many retrieval tasks [21, 28, 14, 4]. The most popular and fundamental one is the query-generation language model [21, 13]. All existing query generation language models are based on either multinomial distribution [19, 6, 28, 13] or multivariate Bernoulli distribution [21, 17, 18]. We introduce a new family of language models, based on Poisson distribution. Poisson distribution has been previously studied in the document generation models [16, 22, 3, 24], leading to the development of one of the most effective retrieval formula BM25 [23]. [24] studies the parallel derivation of three different retrieval models which is related to our comparison of Poisson and multinomial. However, the Poisson model in their paper is still under the document generation framework, and also does not account for the document length variation. [26] introduces a way to empirically search for an exponential model for the documents. Poisson mixtures [3] such as 2-Poisson [22], Negative multinomial, and Katzs KMixture [9] has shown to be effective to model and retrieve documents. Once again, none of this work explores Poisson distribution in the query generation framework. Language model smoothing [2, 28, 29] and background structures [15, 10, 25, 27] have been studied with multinomial language models. [7] analytically shows that term specific smoothing could be useful. We show that Poisson language model is natural to accommodate the per-term smoothing without heuristic twist of the semantics of a generative model, and is able to efficiently better model the mixture background, both analytically and empirically. 6. CONCLUSIONS We present a new family of query generation language models for retrieval based on Poisson distribution. We derive several smoothing methods for this family of models, including single-stage smoothing and two-stage smoothing. We compare the new models with the popular multinomial retrieval models both analytically and experimentally. Our analysis shows that while our new models and multinomial models are equivalent under some assumptions, they are generally different with some important differences. In particular, we show that Poisson has an advantage over multinomial in naturally accommodating per-term smoothing. We exploit this property to develop a new per-term smoothing algorithm for Poisson language models, which is shown to outperform term-independent smoothing for both Poisson and multinomial models. Furthermore, we show that a mixture background model for Poisson can be used to improve the performance and robustness over the standard Poisson background model. Our work opens up many interesting directions for further exploration in this new family of models. Further exploring the flexibilities over multinomial language models, such as length normalization and pseudo-feedback could be good future work. It is also appealing to find robust methods to learn the per-term smoothing coefficients without additional computation cost. 7. ACKNOWLEDGMENTS We thank the anonymous SIGIR 07 reviewers for their useful comments. This material is based in part upon work supported by the National Science Foundation under award numbers IIS-0347933 and 0425852. 8. REFERENCES [1] D. Blei, A. Ng, and M. Jordan. Latent dirichlet allocation. Journal of Machine Learning Research, 3:993-1022, 2003. [2] S. F. Chen and J. Goodman. An empirical study of smoothing techniques for language modeling. Technical Report TR-10-98, Harvard University, 1998. [3] K. Church and W. Gale. Poisson mixtures. Nat. Lang. Eng., 1(2):163-190, 1995. [4] W. B. Croft and J. Lafferty, editors. Language Modeling and Information Retrieval. Kluwer Academic Publishers, 2003. [5] H. Fang, T. Tao, and C. Zhai. A formal study of information retrieval heuristics. In Proceedings of the 27th annual international ACM SIGIR conference on Research and development in information retrieval, pages 49-56, 2004. [6] D. Hiemstra. Using Language Models for Information Retrieval. PhD thesis, University of Twente, Enschede, Netherlands, 2001. [7] D. Hiemstra. Term-specific smoothing for the language modeling approach to information retrieval: the importance of a query term. In Proceedings of the 25th annual international ACM SIGIR conference on Research and development in information retrieval, pages 35-41, 2002. [8] T. Hofmann. Probabilistic latent semantic indexing. In Proceedings of ACM SIGIR99, pages 50-57, 1999. [9] S. M. Katz. Distribution of content words and phrases in text and language modelling. Nat. Lang. Eng., 2(1):15-59, 1996. [10] O. Kurland and L. Lee. Corpus structure, language models, and ad-hoc information retrieval. In Proceedings of the 27th annual international ACM SIGIR conference on Research and development in information retrieval, pages 194-201, 2004. [11] J. Lafferty and C. Zhai. Document language models, query models, and risk minimization for information retrieval. In Proceedings of SIGIR01, pages 111-119, Sept 2001. [12] J. Lafferty and C. Zhai. Probabilistic IR models based on query and document generation. In Proceedings of the Language Modeling and IR workshop, pages 1-5, May 31 - June 1 2001. [13] J. Lafferty and C. Zhai. Probabilistic relevance models based on document and query generation. In W. B. Croft and J. Lafferty, editors, Language Modeling and Information Retrieval. Kluwer Academic Publishers, 2003. [14] V. Lavrenko and B. Croft. Relevance-based language models. In Proceedings of SIGIR01, pages 120-127, Sept 2001. [15] X. Liu and W. B. Croft. Cluster-based retrieval using language models. In Proceedings of the 27th annual international ACM SIGIR conference on Research and development in information retrieval, pages 186-193, 2004. [16] E. L. Margulis. Modelling documents with multiple poisson distributions. Inf. Process. Manage., 29(2):215-227, 1993. [17] A. McCallum and K. Nigam. A comparison of event models for naive bayes text classification. In Proceedings of AAAI-98 Workshop on Learning for Text Categorization, 1998. [18] D. Metzler, V. Lavrenko, and W. B. Croft. Formal multiple-bernoulli models for language modeling. In Proceedings of the 27th annual international ACM SIGIR conference on Research and development in information retrieval, pages 540-541, 2004. [19] D. H. Miller, T. Leek, and R. Schwartz. A hidden Markov model information retrieval system. In Proceedings of the 1999 ACM SIGIR Conference on Research and Development in Information Retrieval, pages 214-221, 1999. [20] A. Papoulis. Probability, random variables and stochastic processes. New York: McGraw-Hill, 1984, 2nd ed., 1984. [21] J. M. Ponte and W. B. Croft. A language modeling approach to information retrieval. In Proceedings of the 21st annual international ACM SIGIR conference on Research and development in information retrieval, pages 275-281, 1998. [22] S. Robertson and S. Walker. Some simple effective approximations to the 2-poisson model for probabilistic weighted retrieval. In Proceedings of SIGIR94, pages 232-241, 1994. [23] S. E. Robertson, S. Walker, S. Jones, M. M.Hancock-Beaulieu, and M. Gatford. Okapi at TREC-3. In D. K. Harman, editor, The Third Text REtrieval Conference (TREC-3), pages 109-126, 1995. [24] T. Roelleke and J. Wang. A parallel derivation of probabilistic information retrieval models. In Proceedings of the 29th annual international ACM SIGIR conference on Research and development in information retrieval, pages 107-114, 2006. [25] T. Tao, X. Wang, Q. Mei, and C. Zhai. Language model information retrieval with document expansion. In Proceedings of HLT/NAACL 2006, pages 407-414, 2006. [26] J. Teevan and D. R. Karger. Empirical development of an exponential probabilistic model for text retrieval: using textual analysis to build a better model. In Proceedings of the 26th annual international ACM SIGIR conference on Research and development in informaion retrieval, pages 18-25, 2003. [27] X. Wei and W. B. Croft. Lda-based document models for ad-hoc retrieval. In Proceedings of the 29th annual international ACM SIGIR conference on Research and development in information retrieval, pages 178-185, 2006. [28] C. Zhai and J. Lafferty. A study of smoothing methods for language models applied to ad-hoc information retrieval. In Proceedings of ACM SIGIR01, pages 334-342, Sept 2001. [29] C. Zhai and J. Lafferty. Two-stage language models for information retrieval. In Proceedings of ACM SIGIR02, pages 49-56, Aug 2002.",
    "original_translation": "Un estudio del modelo de generación de consultas de Poisson para la recuperación de información Qiaozhu Mei, Hui Fang, Chengxiang Zhai Departamento de Ciencias de la Computación Universidad de Illinois en Urbana-Champaign Urbana, IL 61801 {QMEI2, HFANG, CZHAI}@uiuc.edu Abstract muchas variadores de idiomas modelos de idiomashan sido propuestos para la recuperación de información. La mayoría de los modelos existentes se basan en la distribución multinomial y obtendrían documentos basados en la probabilidad de consulta calculada en función de un modelo probabilístico de generación de consultas. En este artículo, proponemos y estudiamos una nueva familia de modelos de generación de consultas basados en la distribución de Poisson. Mostramos que, si bien en sus formas más simples, la nueva familia de modelos y los modelos multinomiales existentes son equivalentes, se comportan de manera diferente para muchos métodos de suavizado. Mostramos que el modelo Poisson tiene varias ventajas sobre el modelo multinomial, que incluye un suavizado de acomodación natural por término y que permite un modelado de fondo más preciso. Presentamos varias variantes del nuevo modelo correspondiente a diferentes métodos de suavizado, y las evaluamos en cuatro colecciones representativas de pruebas TREC. Los resultados muestran que, si bien sus modelos básicos funcionan de manera comparable, el modelo Poisson puede superar al modelo multinomial con suavizado por término. El rendimiento se puede mejorar aún más con un suavizado de dos etapas. Categorías y descriptores de asignaturas: H.3.3 [Búsqueda y recuperación de información]: Modelos de recuperación Términos generales: Algoritmos 1. Introducción Como nuevo tipo de modelos de recuperación probabilística, se ha demostrado que los modelos de lenguaje son efectivos para muchas tareas de recuperación [21, 28, 14, 4]. Entre muchas variantes de modelos de idiomas propuestos, el más popular y fundamental es el modelo de lenguaje de generación de consultas [21, 13], lo que conduce al método de puntuación de la probabilidad de consulta para clasificar documentos. En dicho modelo, dada una consulta q y un documento D, calculamos la probabilidad de generar consultas Q con un modelo estimado en base al documento D, es decir, la probabilidad condicional P (Q | D). Luego podemos clasificar los documentos basados en la probabilidad de generar la consulta. Prácticamente todos los modelos de lenguaje de generación de consultas existentes se basan en distribución multinomial [19, 6, 28] o distribución multivariada de Bernoulli [21, 18]. La distribución multinomial es especialmente popular y también se muestra bastante efectiva. El uso intensivo de la distribución multinomial se debe en parte al hecho de que se ha utilizado con éxito en el reconocimiento de voz, donde la distribución multinomial es una opción natural para modelar la aparición de una palabra particular en una posición particular en el texto. En comparación con Bernoulli multivariado, la distribución multinomial tiene la ventaja de poder modelar la frecuencia de los términos en la consulta;Por el contrario, Bernoulli multivariado solo modela la presencia y la ausencia de términos de consulta, por lo que no pueden capturar diferentes frecuencias de términos de consulta. Sin embargo, Bernoulli multivariado también tiene una ventaja potencial sobre multinomial desde el punto de vista de la recuperación: en una distribución multinomial, las probabilidades de todos los términos deben sumar a 1, lo que hace que sea difícil acomodar el suavizado por plazo, mientras que en un Bernoulli multivariado, elLas probabilidades de presencia de diferentes términos son completamente independientes entre sí, acomodando fácilmente el suavizado y la ponderación por término. Tenga en cuenta que la ausencia del término también se captura indirectamente en un modelo multinomial a través de la restricción de que todas las probabilidades del término deben sumar a 1. En este artículo, proponemos y estudiamos una nueva familia de modelos de generación de consultas basados en la distribución de Poisson. En esta nueva familia de modelos, modelamos la frecuencia de cada término independientemente con una distribución de Poisson. Para calificar un documento, primero estimaríamos un modelo de Poisson multivariado basado en el documento, y luego lo puntuaríamos en función de la probabilidad de la consulta dada por el modelo estimado de Poisson. En cierto sentido, el modelo Poisson combina la ventaja de la frecuencia multinomial en el modelado del término y la ventaja de los Bernoulli multivariados para acomodar el suavizado por término. De hecho, de manera similar a la distribución multinomial, los modelos de distribución de Poisson a términos frecuencias, pero sin la restricción de que todas las probabilidades del término deben sumar a 1, y similar a Bernoulli multivariado, modela cada término de forma independiente, por lo tanto, puede acomodar fácilmente el suavizado por término. Al igual que en el trabajo existente en modelos de lenguaje multinomial, el suavizado es fundamental para esta nueva familia de modelos. Derivamos varios métodos de suavizado para el modelo Poisson en paralelo a los utilizados para distribuciones multinomiales, y comparamos los modelos de recuperación correspondientes con los basados en distribuciones multinomiales. Encontramos que, si bien con algunos métodos de suavizado, el nuevo modelo y el modelo multinomial conducen a exactamente la misma fórmula, con algunos otros métodos de suavizado que divergen, y el modelo Poisson brinda más flexibilidad para suavizar. En particular, una diferencia clave es que el modelo de Poisson puede acomodar naturalmente el suavizado perterno, lo cual es difícil de lograr con un modelo multinomial sin giro heurístico de la semántica de un modelo generativo. Explotamos esta ventaja potencial para desarrollar un nuevo algoritmo de suavizado dependiente del término para el modelo Poisson y mostramos que este nuevo algoritmo de suavizado puede mejorar el rendimiento sobre los algoritmos de suavizado independientes del término utilizando el modelo Poisson o multinomial. Esta ventaja se ve tanto para el suavizado de una etapa como en dos etapas. Otra ventaja potencial del modelo Poisson es que su modelo de fondo correspondiente para suavizado se puede mejorar mediante el uso de un modelo de mezcla que tiene una fórmula de forma cerrada. Se muestra que este nuevo modelo de fondo supera al modelo de fondo estándar y reduce la sensibilidad del rendimiento de la recuperación al parámetro de suavizado. El resto del documento está organizado de la siguiente manera. En la Sección 2, presentamos a la nueva familia de modelos de generación de consultas con distribución de Poisson, y presentamos varios métodos de suavizado que conducen a diferentes funciones de recuperación. En la Sección 3, comparamos analíticamente el modelo de lenguaje Poisson con el modelo de lenguaje multinomial, desde la perspectiva de la recuperación. Luego diseñamos experimentos empíricos para comparar las dos familias de los modelos de idiomas en la Sección 4. Discutimos el trabajo relacionado en 5 y concluimos en 6. 2. Generación de consultas con el proceso Poisson En el marco de generación de consultas, una suposición básica es que se genera una consulta con un modelo estimado en base a un documento. En la mayoría de los trabajos existentes [12, 6, 28, 29], las personas suponen que cada palabra de consulta se muestrean independientemente de una distribución multinomial. Alternativamente, suponemos que se genera una consulta muestreando la frecuencia de las palabras de una serie de procesos de Poisson independientes [20].2.1 El proceso de generación Sea v = {w1, ..., wn} un conjunto de vocabulario. Sea w una pieza de texto compuesta por un autor y c (w1), ..., c (wn) un vector de frecuencia que representa w, donde c (wi, w) es el recuento de frecuencia del término wi en el texto w.En recuperación, W podría ser una consulta o un documento. Consideramos los recuentos de frecuencia de los N Términos N únicos en W As N diferentes tipos de eventos, muestreados a partir de n procesos de Poisson homogéneos independientes, respectivamente. Supongamos que T es el período de tiempo durante el cual el autor compuso el texto. Con un proceso de Poisson homogéneo, el recuento de frecuencia de cada evento, es decir, el número de ocurrencias de WI, sigue una distribución de Poisson con el parámetro asociado λit, donde λi es un parámetro de velocidad que caracteriza el número esperado de WI en un tiempo de unidad. La función de densidad de probabilidad de dicha distribución de Poisson viene dada por p (c (wi, w) = k | λit) = e - λit (λit) k k! Sin perder la generalidad, establecemos T a la longitud del texto w (las personas escriben una palabra en un tiempo de unidad), es decir, t = | w |. Con n tales procesos de Poisson independientes, cada uno de los cuales explica la generación de un término en el vocabulario, la probabilidad de que se genere W a partir de dichos procesos de Poisson se puede escribir como p (w | λ) = n i = 1 p (c (wi, w, w, w) | Λ) = n i = 1 e - λi · | w |(λi · | w |) c (wi, w) c (wi, w)!donde λ = {λ1, ..., λn} y | w |= n i = 1 c (wi, w). Nos referimos a estos n procesos Poisson independientes con el parámetro λ como modelo de lenguaje de Poisson. Sea d = {d1, ..., dm} un conjunto observado de muestras de documentos generadas a partir del proceso de Poisson anterior. La estimación de máxima probabilidad (MLE) de λi es ˆλi = d∈D C (wi, d) d∈D w ∈V c (w, d) Tenga en cuenta que este MLE es diferente de la MLE para la distribución de Poisson sin considerar el documentolongitudes, que aparecen en [22, 24]. Dado un documento D, podemos estimar un modelo de lenguaje Poisson λd usando D como muestra. La probabilidad de que una consulta q se genere a partir del modelo de lenguaje de documento λd se puede escribir como p (q | d) = w∈V p (c (w, q) | λd) (1) Esta representación es claramente diferente de la multinomialModelo de generación de consultas AS (1) La probabilidad incluye todos los términos en el vocabulario V, en lugar de solo aquellos que aparecen en Q, y (2) en lugar de la aparición de términos, el espacio de eventos de este modelo son las frecuencias de cada término. En la práctica, tenemos la flexibilidad de elegir el vocabulario v. En un extremo, podemos usar el vocabulario de toda la colección. Sin embargo, esto puede generar ruido y un costo computacional considerable. En el otro extremo, podemos centrarnos en los términos en la consulta e ignorar otros términos, pero se puede perder cierta información útil ignorando los términos que no son Query. Como compromiso, podemos combinar todos los términos no quirales como un solo término pseudo. En otras palabras, podemos suponer que hay exactamente un término no cuidante en el vocabulario para cada consulta. En nuestros experimentos, adoptamos esta estrategia de término pseudo no cuidante. Se puede calificar un documento con la probabilidad de la ecuación 1. Sin embargo, si un término de consulta no se ve en el documento, el MLE de la distribución de Poisson asignaría una probabilidad cero al término, lo que causa la probabilidad de que la consulta sea cero. Al igual que en los enfoques de modelado de idiomas existentes, el principal desafío de construir un modelo de recuperación razonable es encontrar un modelo de lenguaje suavizado para P (· | D).2.2 suavizado en el modelo de recuperación de Poisson En general, queremos asignar tarifas no cero para los términos de consulta que no se ven en el documento d.Se han propuesto muchos métodos de suavizado para modelos de lenguaje multinomial [2, 28, 29]. En general, tenemos que descartar las probabilidades de algunas palabras vistas en el texto para dejar una masa de probabilidad adicional para asignar a las palabras invisibles. Sin embargo, en los modelos de lenguaje de Poisson, no tenemos la misma restricción que en un modelo multinomial (es decir, w∈V P (w | d) = 1). Por lo tanto, no tenemos que descartar la probabilidad de las palabras vistas para dar una tarifa distinta de cero a una palabra invisible. En cambio, solo necesitamos garantizar que K = 0,1,2, ... P (C (W, D) = K | D) = 1. En esta sección, presentamos tres estrategias diferentes para suavizar un modelo de lenguaje de Poisson y mostramos cómo conducen a diferentes funciones de recuperación.2.2.1 suavizado bayesiano utilizando gamma antes del marco de minimización de riesgo en [11], suponemos que un documento se genera mediante la llegada de los términos en un período de tiempo de | D |De acuerdo con el modelo de lenguaje de documento, que esencialmente consiste en un vector de tarifas de Poisson para cada término, es decir, λd = λd, 1, ..., λd, | V |. Se supone que un documento se genera a partir de un modelo potencialmente diferente. Dado un documento en particular D, queremos estimar λd. La tasa de un término se estima independientemente de otros términos. Utilizamos la estimación bayesiana con el siguiente gamma anterior, que tiene dos parámetros, α y β: gamma (λ | α, β) = βα γ (α) λα - 1 e - βλ para cada término W, los parámetros αW y βW sonElegido para ser αW = µ ∗ λc, W y βW = µ, donde µ es un parámetro y λc, W es la velocidad de W estimada a partir de algún modelo de lenguaje de antecedentes, generalmente el modelo de lenguaje de recolección. La distribución posterior de λd viene dada por p (λd | d | d | d | d | d | d | d | d | d | d | d | d | d | d | d | d | d | d | d | d | d | d | d | d | d | d | d | d | d | d | d | d | d | d | d | d | d | d | d | d | d | d | d | d | d | d | d | d | d | d | d | d |V |Distribuciones gamma con parámetros c (w, d) + µλc, w y | d |+ µ para cada palabra w.Dado que la media gamma es α β, tenemos ˆλd, w = λd, w λd, wp (λd, w | d, c) dλd, w = c (w, d) + µλc, w | d |+ µ Esta es precisamente la estimación suavizada del modelo de lenguaje multinomial con Dirichlet Prior [28].2.2.2 Interpolación (Jelinek-Mercer) suavizando otro método directo es descomponer el modelo de generación de consultas como una mezcla de dos modelos de componentes. Uno es el modelo de lenguaje de documento estimado con un estimador de máxima verosimilitud, y el otro es un modelo estimado a partir de los antecedentes de la colección, P (· | C), que asigna una tasa distinta de cero a w.Por ejemplo, podemos usar un coeficiente de interpolación entre 0 y 1 (es decir, Δ ∈ [0, 1]). Con esta interpolación simple, podemos obtener un documento con puntaje (d, q) = w∈V log ((1 - Δ) P (C (W, Q) | D) + ΔP (C (W, Q) | C) (2) Usando el estimador de máxima verosimilitud para P (· | d), tenemos λd, w = c (w, d) | d |, por lo tanto, la ecuación 2 se convierte en la puntuación (d, q) ∝ w∈D∩q [log (1 + 1 - Δ δ e - λd, w | q | (λd, w | q |) c (w, q) c (w, q)! · P (c (w, q) | c)) - log (1 - δ) e - λd, w | q |+ ΔP (C (W, Q) = 0 | C) 1 - Δ + ΔP (C (W, Q) = 0 | C)] + W∈D log (1 - δ) E - λd, W | Q |+ ΔP (C (W, Q) = 0 | C) 1 - Δ + ΔP (C (W, Q) = 0 | C) También podemos usar un modelo de lenguaje Poisson para P (· | C), o usar algunosOtros modelos basados en frecuencia. En la fórmula de recuperación anterior, la primera suma se puede calcular de manera eficiente. La segunda suma puede tratarse como un documento anterior, que penaliza los documentos largos. Como la segunda suma es difícil de calcular de manera eficiente, combinamos todos los términos no quirados como un pseudo no quirado, denotado como N. utilizando la formulación de pseudo a término y un modelo de colección de Poisson, podemos reescribir la fórmula de recuperación como puntaje (D, q) ∝ w∈D∩q log (1 + 1 - Δ δ e - λd, w (λd, w | q |) c (w, q) e - λd, c | q | (λd, c) c) c)(w, q)) + log (1 - δ) e - λd, n | q |+ ΔE - λc, n | Q |1 - Δ + ΔE - λc, n | Q |(3) donde λd, n = | d | - w∈Q c (w, d) | d |y λc, n = | c | - w∈Q c (w, c) | c |.2.2.3 suavizado de dos etapas Como se discutió en [29], el suavizado juega dos roles en la recuperación: (1) para mejorar la estimación del modelo de lenguaje de documentos y (2) explicar los términos comunes en la consulta. Para distinguir el contenido y las palabras no discriminativas en una consulta, seguimos [29] y asumimos que se genera una consulta mediante el muestreo de una mezcla de dos componentes de modelos de lenguaje Poisson, siendo un componente el modelo de documento λd y elOtro es un modelo de lenguaje de antecedentes de consulta P (· | U).P (· | U) Modela las frecuencias de término típicas en las consultas de los usuarios. Luego podemos obtener cada documento con la probabilidad de consulta calculada utilizando el siguiente modelo de suavizado de dos etapas: P (C (W, Q) | λd, U) = (1-δ) P (C (W, Q) | λd)+ ΔP (C (W, Q) | U) (4) donde δ es un parámetro, lo que indica aproximadamente la cantidad de ruido en q. Esto se parece al suavizado de interpolación, excepto que P (· | λd) ahora debería ser un modelo de lenguaje suavizado, en lugar del estimado con MLE. Sin conocimiento previo sobre P (· | U), podríamos establecerlo en P (· | C). Cualquier método de suavizado para el modelo de lenguaje de documento se puede utilizar para estimar P (· | D) como el suavizado gamma como se discute en la Sección 2.2.1. El estudio empírico de los métodos de suavizado se presenta en la Sección 4. 3. Análisis del modelo de lenguaje Poisson De la sección anterior, notamos que el modelo de lenguaje Poisson tiene una fuerte conexión con el modelo de lenguaje multinomial. Esto se espera ya que ambos pertenecen a la familia exponencial [26]. Sin embargo, hay muchas diferencias cuando estas dos familias de modelos se aplican con diferentes métodos de suavizado. Desde la perspectiva de la recuperación, ¿funcionarán estos dos modelos de idiomas de manera equivalente? Si no, ¿qué modelo proporciona más beneficios para la recuperación o proporciona flexibilidad que podría conducir a posibles beneficios? En esta sección, analizamos analíticamente las características de recuperación de los modelos de lenguaje Poisson, comparando su comportamiento con el de los modelos de lenguaje multinomial.3.1 La equivalencia de los modelos básicos comencemos con la suposición de que todos los términos de consulta aparecen en cada documento. Bajo esta suposición, no se necesita suavizado. Un documento puede ser calificado por la probabilidad log de la consulta con la estimación de máxima probabilidad: puntaje (d, q) = w∈V log e - λd, w | q |(λd, w | q |) c (w, q) c (w, q)!(5) Usando el MLE, tenemos λd, w = c (w, d) w∈V c (w, d). Así, puntaje (d, q) ∝ c (w, q)> 0 c (w, q) log c (w, d) w∈V c (w, d) Esta es exactamente la probabilidad log de la consulta si el documentoEl modelo de idioma es un multinomial con una estimación de máxima verosimilitud. De hecho, incluso con suavizado de gamma, al conectar λd, w = c (w, d)+µλc, w | d |+µ y λc, w = c (w, c) | c |En la ecuación 5, es fácil mostrar que la puntuación (d, q) ∝ w∈Q∩d c (w, q) log (1 + c (w, d) µ · c (w, c) | c |) + +| P |log µ | d |+ µ (6) que es exactamente la fórmula de recuperación de Dirichlet en [28]. Tenga en cuenta que esta equivalencia se mantiene solo cuando la variación de la longitud del documento se modela con el proceso Poisson. Esta derivación indica la equivalencia del poisson básico y los modelos de lenguaje multinomial para la recuperación. Sin embargo, con otras estrategias de suavizado, los dos modelos serían diferentes. Sin embargo, con esta equivalencia en modelos básicos, podríamos esperar que el modelo de lenguaje Poisson realice comparablemente el modelo de lenguaje multinomial en recuperación, si solo se explora un suave simple. Basado en este análisis de equivalencia, uno puede preguntar por qué debemos perseguir el modelo de lenguaje Poisson. En las siguientes secciones, mostramos que a pesar de la equivalencia en sus modelos básicos, el modelo de lenguaje Poisson aporta una flexibilidad adicional para explorar técnicas avanzadas en varias características de recuperación, que no se pueden lograr con modelos de lenguaje multinomial.3.2 suavizante dependiente de términos Una flexibilidad del modelo de lenguaje Poisson es que proporciona un marco natural para acomodar el suavizado dependiente del término (por período). El trabajo existente en el suavizado del modelo de lenguaje ya ha demostrado que diferentes tipos de consultas deben suavizarse de manera diferente de acuerdo con cuán discriminatorios son los términos de consulta.[7] también predijo que diferentes términos deberían tener un peso de suavizado diferente. Con modelos de generación de consultas multinomiales, las personas generalmente usan un coeficiente de suavizado único para controlar la combinación del modelo de documento y el modelo de fondo [28, 29]. Este parámetro se puede hacer específico para diferentes consultas, pero siempre tiene que ser una constante para todos los términos. Esto es obligatorio ya que un modelo de lenguaje multinomial tiene la restricción de que w∈V p (w | d) = 1. Sin embargo, desde la perspectiva de recuperación, pueden necesitar diferentes términos para suavizar de manera diferente, incluso si están en la misma consulta. Por ejemplo, se espera que un término no discriminativo (por ejemplo, el, IS) se explique más con el modelo de fondo, mientras que un término de contenido (por ejemplo, recuperación, arbusto) en la consulta debe explicarse con el modelo de documento. Por lo tanto, una mejor manera de suavizar sería establecer el coeficiente de interpolación (es decir, δ en la fórmula 2 y la fórmula 3) específicamente para cada término. Dado que el modelo de lenguaje Poisson no tiene la restricción de suma a uno en los términos, puede acomodar fácilmente el suavizado por término sin necesidad de torcer heurísticamente la semántica de un modelo generativo como en el caso de los modelos de lenguaje multinomial. A continuación presentamos una posible forma de explorar un suavizado dependiente de términos con modelos de lenguaje Poisson. Esencialmente, queremos usar un coeficiente de suavizado específico de término δ en la combinación lineal, denotada como ΔW. Este coeficiente debe ser intuitivamente mayor si W es una palabra común y más pequeña si es una palabra de contenido. El problema clave es encontrar un método para asignar valores razonables a ΔW. La sintonización empírica es inviable para tantos parámetros. En su lugar, podemos estimar los parámetros ∆ = {Δ1, ..., Δ | V |} maximizando la probabilidad de la consulta dado el modelo de mezcla de P (Q | λq) y P (Q | U), donde λq es elEl verdadero modelo de consulta para generar la consulta y P (Q | U) es un modelo de fondo de consulta como se discute en la Sección 2.2.3. Con el modelo P (q | λq) oculto, la probabilidad de consulta es p (q | ∆, u) = λq w∈V ((1 - ΔW) p (c (w, q) | λq) + ΔWp (c (c (c (c (c (c (w, q) | u)) p (λq | u) dλq Si tenemos documentos relevantes para cada consulta, podemos aproximar el espacio del modelo de consulta con los modelos de lenguaje de todos los documentos relevantes. Sin documentos relevantes, optamos por aproximar el espacio del modelo de consulta con los modelos de todos los documentos de la colección. Configuración P (· | U) Como P (· | C), la probabilidad de consulta se convierte en P (Q | ∆, U) = d∈C πd w∈V ((1 - porˆΛd)+ΔWp (c (w, q) | c)) donde πd = p (ˆλd | u).P (· | ˆλd) es un modelo de lenguaje Poisson estimado para el documento d.Si tenemos conocimiento previo sobre p (ˆλd | u), como qué documentos son relevantes para la consulta, podemos establecer πd en consecuencia, porque lo que queremos es encontrar ∆ que pueda maximizar la probabilidad de la consulta dada los documentos relevantes. Sin este conocimiento previo, podemos dejar πd como parámetros libres y usar el algoritmo EM para estimar πd y ∆. Las funciones de actualización se dan como π (k + 1) d = πd w∈V ((1 - ΔW) p (c (w, q) | ˆλd) + ΔWp (c (w, q) | c)) d∈C πd w∈V ((1 - ΔW) p (c (w, q) | ˆλd) + ΔWp (c (w, q) | c)) y Δ (k + 1) w = d∈C πd ΔWP (c (w, q) | c)) (1 - ΔW) P (c (w, q) | ˆλd) + ΔWp (c (w, q) | c)) como se discute en [29], solo necesitamosEjecute el algoritmo EM para varias iteraciones, por lo que el costo computacional es relativamente bajo. Nuevamente asumimos que nuestro vocabulario contiene todos los términos de consulta más un término pseudo no cuidante. Tenga en cuenta que la función no ofrece una forma explícita de estimar el coeficiente para el término no considerado no considerado. En nuestros experimentos, lo establecemos en el promedio de más de ΔW de todos los términos de consulta. Con esta flexibilidad, esperamos que los modelos de lenguaje de Poisson puedan mejorar el rendimiento de la recuperación, especialmente para las consultas detalladas, donde los términos de la consulta tienen varios valores discriminativos. En la Sección 4, utilizamos experimentos empíricos para probar esta hipótesis.3.3 Modelos de fondo de mezcla Otra flexibilidad es explorar diferentes modelos de fondo (colección) (es decir, P (· | U) o P (· | C)). Una suposición común hecha en la recuperación de información de modelado de idiomas es que el modelo de fondo es un modelo homogéneo de los modelos de documentos [28, 29]. Del mismo modo, también podemos suponer que el modelo de recolección es un modelo de lenguaje de Poisson, con las tasas λc, w = d∈C c (w, d) | c |. Sin embargo, esta suposición generalmente no se mantiene, ya que la colección es mucho más compleja que un solo documento. De hecho, la colección generalmente consiste en una mezcla de documentos con varios géneros, autores y temas, etc. Tratar el modelo de recolección como una mezcla de modelos de documentos, en lugar de un solo modelo de pseudo-documento es más razonable. El trabajo existente de modelado de lenguaje multinomial ya ha demostrado que un mejor modelado de antecedentes mejora el rendimiento de la recuperación, como los grupos [15, 10], los documentos vecinos [25] y los aspectos [8, 27]. Todos los enfoques se pueden adoptar fácilmente utilizando modelos de lenguaje Poisson. Sin embargo, un problema común de estos enfoques es que todos requieren un cálculo pesado para construir el modelo de fondo. Con el modelado de idiomas de Poisson, mostramos que es posible modelar el fondo de la mezcla sin pagar el costo computacional pesado. Se ha propuesto la mezcla de Poisson [3] para modelar una colección de documentos, que pueden adaptarse a los datos mucho mejor que un solo Poisson. La idea básica es suponer que la colección se genera a partir de una mezcla de modelos Poisson, que tiene la forma general de p (x = k | pm) = λ p (λ) p (x = k | λ) dλ p (·| λ) es un modelo de Poisson único y P (λ) es una función de densidad de probabilidad arbitraria. Hay tres mezclas de Poisson bien conocidas [3]: 2 poisson, binomial negativo y la mezcla Katzs K [9]. Tenga en cuenta que el modelo de 2 poisson se ha explorado en los modelos de recuperación probabilística, lo que condujo a la conocida fórmula BM25 [22]. Todas estas mezclas tienen formas cerradas y se pueden estimar a partir de la recopilación de documentos de manera eficiente. Esta es una ventaja sobre los modelos de mezcla multinomial, como PLSI [8] y LDA [1], para la recuperación. Por ejemplo, la función de densidad de probabilidad de la mezcla K de Katzs se da como p (c (w) = k | αW, βW) = (1-αW) ηk, 0 + αW βW + 1 (βW βW + 1) K dondeηk, 0 = 1 cuando k = 0 y 0 de lo contrario. Con la observación de una colección de documentos, αW y βW se pueden estimar como βW = cf (w) - df (w) df (w) y αw = cf (w) nβw donde cf (w) y df (w) sonLa frecuencia de recopilación y la frecuencia de documentos de W, y N es el número de documentos en la colección. Para tener en cuenta las diferentes longitudes de documentos, suponemos que βW es una estimación razonable para generar un documento de la longitud promedio, y usar β = βW avdl | Q |para generar la consulta. Este modelo de mezcla de Poisson se puede usar fácilmente para reemplazar P (· | C) en las funciones de recuperación 3 y 4. 3.4 Otras flexibilidades posibles además del suavizado dependiente del término y el fondo de mezcla eficiente, un modelo de lenguaje de Poisson también tiene otras ventajas potenciales. Por ejemplo, en la Sección 2, vemos que la Fórmula 2 introduce un componente que realiza la penalización de la longitud del documento. Intuitivamente, cuando el documento tiene palabras más únicas, será penalizado más. Por otro lado, si un documento es exactamente n copias de otro documento, no se superaría penalizado. Esta característica es deseable y no se logra con el modelo Dirichlet [5]. Potencialmente, este componente podría penalizar un documento de acuerdo con los tipos de términos que contiene. Con la configuración específica a término de δ, podríamos obtener aún más flexibilidad para la normalización de la longitud del documento. La pseudo-retbalda es otra dirección interesante en la que el modelo Poisson podría mostrar su ventaja. Con la retroalimentación basada en modelos, podríamos relajar nuevamente los coeficientes de combinación del modelo de retroalimentación y el modelo de fondo, y permitir que diferentes términos contribuyan de manera diferente al modelo de retroalimentación. También podríamos utilizar los documentos relevantes para aprender mejores coeficientes de suavizado por término.4. Evaluación En la Sección 3, comparamos analíticamente los modelos de lenguaje Poisson y los modelos de lenguaje multinomial desde la perspectiva de la generación y recuperación de consultas. En esta sección, comparamos estas dos familias de modelos empíricamente. Los resultados del experimento muestran que el modelo de Poisson con suavizado Perter supera el modelo multinomial, y el rendimiento puede mejorarse aún más con un suavizado de dos etapas. El uso de la mezcla de Poisson como modelo de fondo también mejora el rendimiento de la recuperación.4.1 conjuntos de datos Dado que el rendimiento de la recuperación podría variar significativamente de una colección de pruebas a otra, y de una consulta a otra, seleccionamos cuatro colecciones representativas de pruebas TREC: AP, TREC7, TREC8 y WT2G (Web). Para cubrir diferentes tipos de consultas, seguimos [28, 5], y construimos una palabra corta (SK, título de palabras clave), consultas de verbose (SV, una descripción de una oración) y consultas de larga data (LV, múltiples oraciones). Los documentos se encuentran con los porteros Stemmer, y no eliminamos ninguna palabra de parada. Para cada parámetro, variamos su valor para cubrir un rango razonablemente amplio.4.2 Comparación con multinomial Comparamos el rendimiento de los modelos de recuperación de Poisson y los modelos de recuperación multinomial utilizando el suavizado de interpolación (Jelinekmercer, JM) y suavizado bayesiano con antugados. La Tabla 1 muestra que los dos modelos suavizado por JM funcionan de manera similar en todos los conjuntos de datos. Dado que el suavizado de Dirichlet para el modelo de lenguaje multinomial y el suavizado de gamma para el modelo de lenguaje Poisson conducen a la misma fórmula de recuperación, el rendimiento de estos dos modelos se presenta conjuntamente. Vemos que los métodos de suavizado de Dirichlet/gamma superan a los métodos de suavizado Jelinek-Mercer. Las curvas de sensibilidad del parámetro para dos Jelinek-Mercer 0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1 0 0.05 0.1 0.15 0.2 0.25 0.3 DataSet: TREC8 Parámetro: Δ Precisión promedio JM-Multinomial: VI JM-Multinomial: SV JM-Multinomio: SK JM JM JM− Poisson: SK JM-Poisson: SV JM-Poisson: LV Figura 1: Poisson y multinomial se realizan de manera similar con los métodos de suavizado de suavizado Jelinek-Mercer se muestran en la Figura 1. Claramente, estos dos métodos funcionan de manera similar en términos de consulta de datos de optimización JM-Multinomial JM-Poisson Dirichlet/Gamma por período de 2 etapas Poisson Map initpr pr@5d Map initpr pr@5d Map Initpr Pr@5D Map PR@5D 5DAP88-89 SK 0.203 0.585 0.356 0.203 0.585 0.358 0.224 0.629 0.393 0.226 0.630 0.396 SV 0.187 0.580 0.361 0.183 0.571 0.345 0.204 0.613 0.387 0.217* 0.603 0.390 LV 0.283 470 0.291 0.710 0.496 0.304* 0.695 0.510 TREC7 SK 0.167 0.635 0.400 0.1680.635 0.404 0.186 0.687 0.428 0.185 0.646 0.436 SV 0.174 0.655 0.432 0.176 0.653 0.432 0.182 0.666 0.432 0.196* 0.660 0.440 VV 0.2223 0.730 0.496 0.215 0.766 0.488888. 738 0.512 TREC8 SK 0.239 0.621 0.440 0.239 0.621 0.436 0.257 0.718 0.496 0.256 0.704 0.468SV 0.231 0.686 0.448 0.234 0.702 0.456 0.228 0.691 0.456 0.246* 0.692 0.476 LV 0.265 0.796 0.548 0.261 0.757 0.520 0.260 0.741 0.492 0.274* 0.766 0.508 Web 0.650 0.650 302 0.767 0.468 0.307 0.739 0.468 SV 0.214 0.611 0.392 0.217 0.609 0.384 0.2730.693 0.508 0.292* 0.703 0.480 LV 0.266 0.790 0.464 0.259 0.776 0.452 0.283 0.756 0.496 0.311* 0.759 0.488 Tabla 1: Comparación de rendimiento entre Poisson y Modelos de recuperación multinomial: Modelos básicos funcionan comparablemente;El suave de dos etapas dependiente del término mejora significativamente Poisson, un asterisco (*) indica que la diferencia entre el rendimiento del suavizado de dos etapas dependiente y el del suavizado único de Dirichlet/gamma es estadísticamente significativa de acuerdo con la prueba de rango firmada de Wilcoxon en la prueba de rango en lanivel de 0.05.o sensibilidad. Se espera esta similitud de rendimiento, como discutimos en la Sección 3.1. Aunque el modelo Poisson y el modelo multinomial son similares en términos del modelo básico y/o con métodos de suavizado simples, el modelo Poisson tiene un gran potencial y flexibilidad para mejorar aún más. Como se muestra en la columna más a la derecha de la Tabla 1, el modelo Poisson de dos etapas dependiente del término supera constantemente los modelos de suavizado básicos, especialmente para las consultas detalladas. Este modelo se da en la Fórmula 4, con un suavizado gamma para el modelo de documento P (· | D), y ΔW, que depende del término. El parámetro µ del suavizado gamma de la primera etapa se ajusta empíricamente. Los coeficientes de combinación (es decir, ∆), se estiman con el algoritmo EM en la Sección 3.2. Las curvas de sensibilidad de parámetros para Dirichlet/Gamma y el modelo de suavizado de dos etapas por plazo se representan en la Figura 2. El método de suavizado de dos etapas por término es menos sensible al parámetro µ que Dirichlet/gamma, y produce un mejor rendimiento óptimo.0 1000 2000 3000 4000 5000 6000 7000 8000 9000 10000 0.1 0.12 0.14 0.16 0.18 0.2 0.22 conjunto de datos: AP;Tipo de consulta: Parámetro SV: µ Precisión promedio Dirichlet/Gamma suave término dependiente 2-etapa Figura 2: suave dependiente de dos etapas de dos etapas de Poisson supera a Dirichlet/gamma En las siguientes subsecciones, realizamos experimentos para demostrar cómo la flexibilidad del modelo Poisson podríaSe utilizará para lograr un mejor rendimiento, que no podemos lograr con modelos de lenguaje multinomial.4.3 suavizante dependiente del término Para probar la efectividad del suave dependiente del término, realizamos los siguientes dos experimentos. En el primer experimento, relajamos el coeficiente constante en la simple fórmula de suavizado de Jelinek-Mercer (es decir, Fórmula 3), y usamos el algoritmo EM propuesto en la Sección 3.2 para encontrar un ΔW para cada término único. Dado que estamos utilizando el algoritmo EM para estimar iterativamente los parámetros, generalmente no queremos que la probabilidad de P (· | D) sea cero. Luego usamos un método de Laplace simple para suavizar ligeramente el modelo de documento antes de entrar en las iteraciones de emergencias. Los documentos todavía se califican con la Fórmula 3, pero utilizan ΔW erudito. Los resultados están etiquetados con JM+L.en la Tabla 2. Datos Q JM JM JM+L.2 etapas de 2 etapas (MAP) PT: No Sí Sí No Sí AP SK 0.203 0.204 0.206 0.223 0.226* SV 0.183 0.189 0.214* 0.204 0.217* TREC7 SK 0.168 0.171 0.174 0.186 0.185 SV 0.176 0.147 0.198* 0.194 0.1960.227* 0.257 0.256 SV 0.234 0.223 0.249* 0.242 0.246* web Sk 0.250 0.236 0.220* 0.291 0.307* SV 0.217 0.232 0.261*entre el jm+l.El método y el método JM es estadísticamente significativo;Un asterisco (*) en la columna 5 significa que la diferencia entre el método de dos etapas dependiente del término y el método de dos etapas dependiente de la consulta es estadísticamente significativa;PT significa por período. Con coeficientes dependientes del término, el rendimiento del modelo Jelinek-Mercer Poisson mejora en la mayoría de los casos. Sin embargo, en algunos casos (por ejemplo, TREC7/SV), funciona mal. Esto podría ser causado por el problema de la estimación de EM con modelos de documentos sin liso. Una vez que se asigna una probabilidad no cero a todos los términos antes de ingresar la iteración EM, el rendimiento de las consultas detalladas se puede mejorar significativamente. Esto indica que todavía hay espacio para encontrar mejores métodos para estimar ΔW. Tenga en cuenta que ni el método JM Perter ni el JM+L.El método tiene un parámetro que sintonizar. Como se muestra en la Tabla 1, el suave suave dependiente de dos etapas depende puede mejorar significativamente el rendimiento de la recuperación. Para comprender si la mejora es contribuida por el suave suave dependiente o el marco de suavizado de dos etapas, diseñamos otro experimento para comparar el suavizado de dos etapas por término con el método de suavizado de dos etapas propuesto en [29]. Su método logró encontrar coeficientes específicos para la consulta, por lo que una consulta detallada usaría un δ más alto. Sin embargo, dado que su modelo se basa en el modelado de lenguaje multinomial, no pudieron obtener coeficientes por término. Adoptamos su método para el suavizado de dos etapas de Poisson, y también estimamos un coeficiente por QUERERY para todos los términos. Comparamos el rendimiento de dicho modelo con el modelo de suavizado de dos etapas por período, y presentamos los resultados en las dos columnas correctas en la Tabla 2. Una vez más, vemos que el suavizado de dos etapas por período supera al suavizado de dos etapas por QUERERY, especialmente para consultas verbosas. La mejora no es tan grande como la forma en que el método de suavizado de término mejora sobre Dirichlet/gamma. Se espera que esto, ya que el suavizado por QUIERY ya ha abordado el problema de discriminación de consultas hasta cierto punto. Este experimento muestra que incluso si el suavizado ya es por QUERERY, lo que lo hace por plazo sigue siendo beneficioso. En resumen, el suavizado por término mejoró el rendimiento de la recuperación del método de suavizado de una etapa y dos etapas.4.4 Modelo de fondo de mezcla En esta sección, realizamos experimentos para examinar los beneficios de usar un modelo de fondo de mezcla sin un costo computacional adicional, que no se puede lograr para modelos multinomiales. Específicamente, en la fórmula 3 de recuperación, en lugar de usar una distribución de Poisson única para modelar el fondo P (· | C), usamos el modelo Katzs K-Micte, que es esencialmente una mezcla de distribuciones de Poisson.P (· | c) se puede calcular de manera eficiente con estadísticas de recolección simples, como se discutió en la Sección 3.3. Consulta de datos JM. Poisson JM. K-MEXCTURA AP SK 0.203 0.204 SV 0.183 0.188* TREC-7 SK 0.168 0.169 SV 0.176 0.178* TREC-8 SK 0.239 0.239 SV 0.234 0.238* Web SK 0.250 0.250 SV 0.217 0.223* Tabla 3: el modelo de fondo K-MIZCE mejora la recuperación de la recuperación mejoraEl rendimiento del modelo de recuperación JM con fondo de Poisson único y con el modelo de fondo de la mezcla K Katzs se compara en la Tabla 3. Claramente, el uso de la mezcla K para modelar el modelo de fondo supera al modelo de fondo de Poisson único en la mayoría de los casos, especialmente para consultas verbosas donde la mejora es estadísticamente significativa. La Figura 3 muestra que el rendimiento cambia en diferentes parámetros para consultas verbosas cortas. El modelo que usa el fondo de la mezcla K es menos sensible que el que usa fondo de Poisson único. Dado que este tipo de mezcla 0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1 0 0.05 0.1 0.15 0.2 0.25 Datos: TREC8;Consulta: Parámetro SV: Δ Precision Precision Poisson Background K-Mezcre Figura Figura 3: El modelo de fondo de la mezcla K desvía la sensibilidad del modelo de fondo de consultas verbosas no requiere ningún costo de cálculo adicional, sería interesante estudiar si utilizando otros modelos Poisson de mezcla,como el 2 poisson y el binomial negativo, podría ayudar al rendimiento.5. Trabajo relacionado según nuestro lo mejor de nuestro conocimiento, no ha habido un estudio de los modelos de generación de consultas basados en la distribución de Poisson. Se ha demostrado que los modelos de lenguaje son efectivos para muchas tareas de recuperación [21, 28, 14, 4]. El más popular y fundamental es el modelo de lenguaje de generación de consultas [21, 13]. Todos los modelos de lenguaje de generación de consultas existentes se basan en distribución multinomial [19, 6, 28, 13] o distribución multivariada de Bernoulli [21, 17, 18]. Presentamos una nueva familia de modelos de idiomas, basado en la distribución de Poisson. La distribución de Poisson se ha estudiado previamente en los modelos de generación de documentos [16, 22, 3, 24], lo que lleva al desarrollo de una de las fórmula de recuperación más efectiva BM25 [23].[24] Estudia la derivación paralela de tres modelos de recuperación diferentes relacionados con nuestra comparación de Poisson y multinomial. Sin embargo, el modelo Poisson en su documento aún está bajo el marco de generación de documentos, y tampoco tiene en cuenta la variación de la longitud del documento.[26] introduce una forma de buscar empíricamente un modelo exponencial para los documentos. Las mezclas de Poisson [3] como 2-poisson [22], multinomial negativo y katzs kmixture [9] ha demostrado ser efectivo para modelar y recuperar documentos. Una vez más, ninguno de este trabajo explora la distribución de Poisson en el marco de generación de consultas. El suavizado del modelo de lenguaje [2, 28, 29] y las estructuras de fondo [15, 10, 25, 27] se han estudiado con modelos de lenguaje multinomial.[7] muestra analíticamente que el suavizado de término específico podría ser útil. Mostramos que el modelo de lenguaje Poisson es natural para acomodar el suavizado por término sin giro heurístico de la semántica de un modelo generativo, y es capaz de modelar mejor el fondo de la mezcla, tanto analíticamente como empíricamente.6. Conclusiones presentamos una nueva familia de modelos de lenguaje de generación de consultas para la recuperación basados en la distribución de Poisson. Derivamos varios métodos de suavizado para esta familia de modelos, incluido el suavizado de una etapa y el suavizado de dos etapas. Comparamos los nuevos modelos con los populares modelos de recuperación multinomial tanto analíticamente como experimentalmente. Nuestro análisis muestra que si bien nuestros nuevos modelos y modelos multinomiales son equivalentes bajo algunos supuestos, generalmente son diferentes con algunas diferencias importantes. En particular, mostramos que Poisson tiene una ventaja sobre multinomial en suavizado por término naturalmente acomodado. Explotamos esta propiedad para desarrollar un nuevo algoritmo de suavizado por término para los modelos de lenguaje Poisson, que se muestra superando el suavizado independiente del término para los modelos Poisson y multinomiales. Además, mostramos que se puede utilizar un modelo de fondo de mezcla para Poisson para mejorar el rendimiento y la robustez sobre el modelo de fondo de Poisson estándar. Nuestro trabajo abre muchas direcciones interesantes para una mayor exploración en esta nueva familia de modelos. Explorar aún más las flexibilidades sobre los modelos de lenguaje multinomial, como la normalización de la longitud y la pseudo-retroalimentación podría ser un buen trabajo futuro. También es atractivo encontrar métodos robustos para aprender los coeficientes de suavizado por término sin un costo de cálculo adicional.7. Agradecimientos Agradecemos a los revisores anónimos Sigir 07 por sus útiles comentarios. Este material se basa en parte en el trabajo apoyado por la National Science Foundation bajo los números de premios IIS-0347933 y 0425852. 8. Referencias [1] D. Blei, A. Ng y M. Jordan. Asignación latente de Dirichlet. Journal of Machine Learning Research, 3: 993-1022, 2003. [2] S. F. Chen y J. Goodman. Un estudio empírico de las técnicas de suavizado para el modelado de idiomas. Informe técnico TR-10-98, Universidad de Harvard, 1998. [3] K. Church y W. Gale. Mezclas de Poisson. Nat. Lang. Eng., 1 (2): 163-190, 1995. [4] W. B. Croft y J. Lafferty, editores. Modelado de idiomas y recuperación de información. Kluwer Academic Publishers, 2003. [5] H. Fang, T. Tao y C. Zhai. Un estudio formal de heurística de recuperación de información. En Actas de la 27ª Conferencia Anual de ACM Sigir sobre investigación y desarrollo en recuperación de información, páginas 49-56, 2004. [6] D. Hiemstra. Uso de modelos de idiomas para la recuperación de información. Tesis doctoral, Universidad de Twente, Enschede, Países Bajos, 2001. [7] D. Hiemstra. Suavizado de término específico para el enfoque de modelado de idiomas para la recuperación de información: la importancia de un término de consulta. En Actas de la 25ª Conferencia Anual de ACM Sigir sobre investigación y desarrollo en recuperación de información, páginas 35-41, 2002. [8] T. Hofmann. Indexación semántica latente probabilística. En Actas de ACM Sigir99, páginas 50-57, 1999. [9] S. M. Katz. Distribución de palabras y frases de contenido en el modelado de texto y lenguaje. Nat. Lang. Eng., 2 (1): 15-59, 1996. [10] O. Kurland y L. Lee. Estructura del corpus, modelos de idiomas y recuperación de información ad-hoc. En Actas de la 27ª Conferencia Anual de ACM Sigir sobre investigación y desarrollo en recuperación de información, páginas 194-201, 2004. [11] J. Lafferty y C. Zhai. Documentar modelos de lenguaje, modelos de consulta y minimización de riesgos para la recuperación de información. En Actas de Sigir01, páginas 111-119, septiembre de 2001. [12] J. Lafferty y C. Zhai. Modelos IR probabilísticos basados en la consulta y la generación de documentos. En Actas del modelado de idiomas y el taller IR, páginas 1-5, 31 de mayo - 1 de junio de 2001. [13] J. Lafferty y C. Zhai. Modelos de relevancia probabilística basados en la generación de documentos y consultas. En W. B. Croft y J. Lafferty, editores, modelado de idiomas y recuperación de información. Kluwer Academic Publishers, 2003. [14] V. Lavrenko y B. Croft. Modelos de idiomas basados en relevancia. En Actas de Sigir01, páginas 120-127, septiembre de 2001. [15] X. Liu y W. B. Croft. Recuperación basada en clúster utilizando modelos de lenguaje. En Actas de la 27ª Conferencia Anual de ACM Sigir sobre investigación y desarrollo en recuperación de información, páginas 186-193, 2004. [16] E. L. margulis. Modelado de documentos con múltiples distribuciones de Poisson. Inf. Proceso. Manage., 29 (2): 215-227, 1993. [17] A. McCallum y K. Nigam. Una comparación de modelos de eventos para la clasificación de texto de Naive Bayes. En Actas del Taller de AAAI-98 sobre el aprendizaje para la categorización de texto, 1998. [18] D. Metzler, V. Lavrenko y W. B. Croft. Modelos formales de múltiples bernoulli para modelado de idiomas. En Actas de la 27ª Conferencia Anual de ACM Sigir sobre investigación y desarrollo en recuperación de información, páginas 540-541, 2004. [19] D. H. Miller, T. Leek y R. Schwartz. Un sistema de recuperación de información del modelo de Markov oculto. En Actas de la Conferencia ACM Sigir sobre investigación y desarrollo de 1999 en recuperación de información, páginas 214-221, 1999. [20] A. papulis. Probabilidad, variables aleatorias y procesos estocásticos. Nueva York: McGraw-Hill, 1984, 2ª ed., 1984. [21] J. M. Ponte y W. B. Croft. Un enfoque de modelado de idiomas para la recuperación de información. En Actas de la 21a Conferencia Anual de ACM Sigir sobre investigación y desarrollo en recuperación de información, páginas 275-281, 1998. [22] S. Robertson y S. Walker. Algunas aproximaciones simples efectivas al modelo de 2 poisson para la recuperación ponderada probabilística. En Actas de Sigir94, páginas 232-241, 1994. [23] S. E. Robertson, S. Walker, S. Jones, M. M.Hancock-Beaulieu y M. Gatford. Okapi en TREC-3. En D. K. Harman, editor, la tercera conferencia de recuperación de texto (TREC-3), páginas 109-126, 1995. [24] T. Roelleke y J. Wang. Una derivación paralela de modelos de recuperación de información probabilística. En Actas de la 29a Conferencia Anual de ACM Sigir sobre investigación y desarrollo en recuperación de información, páginas 107-114, 2006. [25] T. Tao, X. Wang, Q. Mei y C. Zhai. Recuperación de información del modelo de idioma con expansión de documentos. En Actas de HLT/NAACL 2006, páginas 407-414, 2006. [26] J. Teevan y D. R. Karger. Desarrollo empírico de un modelo probabilístico exponencial para la recuperación de texto: utilizando el análisis textual para construir un mejor modelo. En Actas de la 26ª Conferencia Anual de Investigación y Desarrollo de Información de ACM ACM en recuperación de información, páginas 18-25, 2003. [27] X. Wei y W. B. Croft. Modelos de documentos basados en LDA para recuperación ad-hoc. En Actas de la 29a Conferencia Anual de ACM Sigir sobre investigación y desarrollo en recuperación de información, páginas 178-185, 2006. [28] C. Zhai y J. Lafferty. Un estudio de métodos de suavizado para modelos de lenguaje aplicados a la recuperación de información ad-hoc. En Actas de ACM Sigir01, páginas 334-342, septiembre de 2001. [29] C. Zhai y J. Lafferty. Modelos de lenguaje de dos etapas para la recuperación de información. En Actas de ACM Sigir02, páginas 49-56, agosto de 2002.",
    "original_sentences": [
        "A Study of Poisson Query Generation Model for Information Retrieval Qiaozhu Mei, Hui Fang, Chengxiang Zhai Department of Computer Science University of Illinois at Urbana-Champaign Urbana,IL 61801 {qmei2,hfang,czhai}@uiuc.edu ABSTRACT Many variants of language models have been proposed for information retrieval.",
        "Most existing models are based on multinomial distribution and would score documents based on query likelihood computed based on a query generation probabilistic model.",
        "In this paper, we propose and study a new family of query generation models based on Poisson distribution.",
        "We show that while in their simplest forms, the new family of models and the existing multinomial models are equivalent, they behave differently for many smoothing methods.",
        "We show that the Poisson model has several advantages over the multinomial model, including naturally accommodating per-term smoothing and allowing for more accurate background modeling.",
        "We present several variants of the new model corresponding to different smoothing methods, and evaluate them on four representative TREC test collections.",
        "The results show that while their basic models perform comparably, the Poisson model can outperform multinomial model with per-term smoothing.",
        "The performance can be further improved with two-stage smoothing.",
        "Categories and Subject Descriptors: H.3.3 [Information Search and Retrieval]: Retrieval Models General Terms: Algorithms 1.",
        "INTRODUCTION As a new type of probabilistic retrieval models, language models have been shown to be effective for many retrieval tasks [21, 28, 14, 4].",
        "Among many variants of language models proposed, the most popular and fundamental one is the query-generation language model [21, 13], which leads to the query-likelihood scoring method for ranking documents.",
        "In such a model, given a query q and a document d, we compute the likelihood of generating query q with a model estimated based on document d, i.e., the conditional probability p(q|d).",
        "We can then rank documents based on the likelihood of generating the query.",
        "Virtually all the existing query generation language models are based on either multinomial distribution [19, 6, 28] or multivariate Bernoulli distribution [21, 18].",
        "The multinomial distribution is especially popular and also shown to be quite effective.",
        "The heavy use of multinomial distribution is partly due to the fact that it has been successfully used in speech recognition, where multinomial distribution is a natural choice for modeling the occurrence of a particular word in a particular position in text.",
        "Compared with multivariate Bernoulli, multinomial distribution has the advantage of being able to model the frequency of terms in the query; in contrast, multivariate Bernoulli only models the presence and absence of query terms, thus cannot capture different frequencies of query terms.",
        "However, multivariate Bernoulli also has one potential advantage over multinomial from the viewpoint of retrieval: in a multinomial distribution, the probabilities of all the terms must sum to 1, making it hard to accommodate per-term smoothing, while in a multivariate Bernoulli, the presence probabilities of different terms are completely independent of each other, easily accommodating per-term smoothing and weighting.",
        "Note that term absence is also indirectly captured in a multinomial model through the constraint that all the term probabilities must sum to 1.",
        "In this paper, we propose and study a new family of query generation models based on the Poisson distribution.",
        "In this new family of models, we model the frequency of each term independently with a Poisson distribution.",
        "To score a document, we would first estimate a multivariate Poisson model based on the document, and then score it based on the likelihood of the query given by the estimated Poisson model.",
        "In some sense, the Poisson model combines the advantage of multinomial in modeling term frequency and the advantage of the multivariate Bernoulli in accommodating per-term smoothing.",
        "Indeed, similar to the multinomial distribution, the Poisson distribution models term frequencies, but without the constraint that all the term probabilities must sum to 1, and similar to multivariate Bernoulli, it models each term independently, thus can easily accommodate per-term smoothing.",
        "As in the existing work on multinomial language models, smoothing is critical for this new family of models.",
        "We derive several smoothing methods for Poisson model in parallel to those used for multinomial distributions, and compare the corresponding retrieval models with those based on multinomial distributions.",
        "We find that while with some smoothing methods, the new model and the multinomial model lead to exactly the same formula, with some other smoothing methods they diverge, and the Poisson model brings in more flexibility for smoothing.",
        "In particular, a key difference is that the Poisson model can naturally accommodate perterm smoothing, which is hard to achieve with a multinomial model without heuristic twist of the semantics of a generative model.",
        "We exploit this potential advantage to develop a new term-dependent smoothing algorithm for Poisson model and show that this new smoothing algorithm can improve performance over term-independent smoothing algorithms using either Poisson or multinomial model.",
        "This advantage is seen for both one-stage and two-stage smoothing.",
        "Another potential advantage of the Poisson model is that its corresponding background model for smoothing can be improved through using a mixture model that has a closed form formula.",
        "This new background model is shown to outperform the standard background model and reduce the sensitivity of retrieval performance to the smoothing parameter.",
        "The rest of the paper is organized as follows.",
        "In Section 2, we introduce the new family of query generation models with Poisson distribution, and present various smoothing methods which lead to different retrieval functions.",
        "In Section 3, we analytically compare the Poisson language model with the multinomial language model, from the perspective of retrieval.",
        "We then design empirical experiments to compare the two families of language models in Section 4.",
        "We discuss the related work in 5 and conclude in 6. 2.",
        "QUERY GENERATION WITH POISSON PROCESS In the query generation framework, a basic assumption is that a query is generated with a model estimated based on a document.",
        "In most existing work [12, 6, 28, 29], people assume that each query word is sampled independently from a multinomial distribution.",
        "Alternatively, we assume that a query is generated by sampling the frequency of words from a series of independent Poisson processes [20]. 2.1 The Generation Process Let V = {w1, ..., wn} be a vocabulary set.",
        "Let w be a piece of text composed by an author and c(w1), ..., c(wn) be a frequency vector representing w, where c(wi, w) is the frequency count of term wi in text w. In retrieval, w could be either a query or a document.",
        "We consider the frequency counts of the n unique terms in w as n different types of events, sampled from n independent homogeneous Poisson processes, respectively.",
        "Suppose t is the time period during which the author composed the text.",
        "With a homogeneous Poisson process, the frequency count of each event, i.e., the number of occurrences of wi, follows a Poisson distribution with associated parameter λit, where λi is a rate parameter characterizing the expected number of wi in a unit time.",
        "The probability density function of such a Poisson Distribution is given by P(c(wi, w) = k|λit) = e−λit (λit)k k!",
        "Without losing generality, we set t to the length of the text w (people write one word in a unit time), i.e., t = |w|.",
        "With n such independent Poisson processes, each explaining the generation of one term in the vocabulary, the likelihood of w to be generated from such Poisson processes can be written as p(w|Λ) = n i=1 p(c(wi, w)|Λ) = n i=1 e−λi·|w| (λi · |w|)c(wi,w) c(wi, w)! where Λ = {λ1, ..., λn} and |w| = n i=1 c(wi, w).",
        "We refer to these n independent Poisson processes with parameter Λ as a Poisson Language Model.",
        "Let D = {d1, ..., dm} be an observed set of document samples generated from the Poisson process above.",
        "The maximum likelihood estimate (MLE) of λi is ˆλi = d∈D c(wi, d) d∈D w ∈V c(w , d) Note that this MLE is different from the MLE for the Poisson distribution without considering the document lengths, which appears in [22, 24].",
        "Given a document d, we may estimate a Poisson language model Λd using d as a sample.",
        "The likelihood that a query q is generated from the document language model Λd can be written as p(q|d) = w∈V p(c(w, q)|Λd) (1) This representation is clearly different from the multinomial query generation model as (1) the likelihood includes all the terms in the vocabulary V , instead of only those appearing in q, and (2) instead of the appearance of terms, the event space of this model is the frequencies of each term.",
        "In practice, we have the flexibility to choose the vocabulary V .",
        "In one extreme, we can use the vocabulary of the whole collection.",
        "However, this may bring in noise and considerable computational cost.",
        "In the other extreme, we may focus on the terms in the query and ignore other terms, but some useful information may be lost by ignoring the nonquery terms.",
        "As a compromise, we may conflate all the non-query terms as one single pseudo term.",
        "In other words, we may assume that there is exactly one non-query term in the vocabulary for each query.",
        "In our experiments, we adopt this pseudo non-query term strategy.",
        "A document can be scored with the likelihood in Equation 1.",
        "However, if a query term is unseen in the document, the MLE of the Poisson distribution would assign zero probability to the term, causing the probability of the query to be zero.",
        "As in existing language modeling approaches, the main challenge of constructing a reasonable retrieval model is to find a smoothed language model for p(·|d). 2.2 Smoothing in Poisson Retrieval Model In general, we want to assign non-zero rates for the query terms that are not seen in document d. Many smoothing methods have been proposed for multinomial language models[2, 28, 29].",
        "In general, we have to discount the probabilities of some words seen in the text to leave some extra probability mass to assign to the unseen words.",
        "In Poisson language models, however, we do not have the same constraint as in a multinomial model (i.e., w∈V p(w|d) = 1).",
        "Thus we do not have to discount the probability of seen words in order to give a non-zero rate to an unseen word.",
        "Instead, we only need to guarantee that k=0,1,2,... p(c(w, d) = k|d) = 1.",
        "In this section, we introduce three different strategies to smooth a Poisson language model, and show how they lead to different retrieval functions. 2.2.1 Bayesian Smoothing using Gamma Prior Following the risk minimization framework in [11], we assume that a document is generated by the arrival of terms in a time period of |d| according to the document language model, which essentially consists of a vector of Poisson rates for each term, i.e., Λd = λd,1, ..., λd,|V | .",
        "A document is assumed to be generated from a potentially different model.",
        "Given a particular document d, we want to estimate Λd.",
        "The rate of a term is estimated independently of other terms.",
        "We use Bayesian estimation with the following Gamma prior, which has two parameters, α and β: Gamma(λ|α, β) = βα Γ(α) λα−1 e−βλ For each term w, the parameters αw and βw are chosen to be αw = µ ∗ λC,w and βw = µ, where µ is a parameter and λC,w is the rate of w estimated from some background language model, usually the collection language model.",
        "The posterior distribution of Λd is given by p(Λd|d, C) ∝ w∈V e−λw(|d|+µ) λ c(w,d)+µλC,w−1 w which is a product of |V | Gamma distributions with parameters c(w, d) + µλC,w and |d| + µ for each word w. Given that the Gamma mean is α β , we have ˆλd,w = λd,w λd,wp(λd,w|d, C)dλd,w = c(w, d) + µλC,w |d| + µ This is precisely the smoothed estimate of multinomial language model with Dirichlet prior [28]. 2.2.2 Interpolation (Jelinek-Mercer) Smoothing Another straightforward method is to decompose the query generation model as a mixture of two component models.",
        "One is the document language model estimated with maximum likelihood estimator, and the other is a model estimated from the collection background, p(·|C), which assigns non-zero rate to w. For example, we may use an interpolation coefficient between 0 and 1 (i.e., δ ∈ [0, 1]).",
        "With this simple interpolation, we can score a document with Score(d, q) = w∈V log((1 − δ)p(c(w, q)|d) + δp(c(w, q)|C)) (2) Using the maximum likelihood estimator for p(·|d), we have λd,w = c(w,d) |d| , thus Equation 2 becomes Score(d, q) ∝ w∈d∩q [log(1 + 1 − δ δ e−λd,w|q| (λd,w|q|)c(w,q) c(w, q)! · p(c(w, q)|C) ) − log (1 − δ)e−λd,w|q| + δp(c(w, q) = 0|C) 1 − δ + δp(c(w, q) = 0|C) ] + w∈d log (1 − δ)e−λd,w|q| + δp(c(w, q) = 0|C) 1 − δ + δp(c(w, q) = 0|C) We can also use a Poisson language model for p(·|C), or use some other frequency-based models.",
        "In the retrieval formula above, the first summation can be computed efficiently.",
        "The second summation can be actually treated as a document prior, which penalizes long documents.",
        "As the second summation is difficult to compute efficiently, we conflate all non-query terms as one pseudo non-queryterm, denoted as N. Using the pseudo-term formulation and a Poisson collection model, we can rewrite the retrieval formula as Score(d, q) ∝ w∈d∩q log(1 + 1 − δ δ e−λd,w (λd,w|q|)c(w,q) e−λd,C |q| (λd,C )c(w,q) ) + log (1 − δ)e−λd,N |q| + δe−λC,N |q| 1 − δ + δe−λC,N |q| (3) where λd,N = |d|− w∈q c(w,d) |d| and λC,N = |C|− w∈q c(w,C) |C| . 2.2.3 Two-Stage Smoothing As discussed in [29], smoothing plays two roles in retrieval: (1) to improve the estimation of the document language model, and (2) to explain the common terms in the query.",
        "In order to distinguish the content and non-discriminative words in a query, we follow [29] and assume that a query is generated by sampling from a two-component mixture of Poisson language models, with one component being the document model Λd and the other being a query background language model p(·|U). p(·|U) models the typical term frequencies in the users queries.",
        "We may then score each document with the query likelihood computed using the following two-stage smoothing model: p(c(w, q)|Λd, U) = (1 − δ)p(c(w, q)|Λd) + δp(c(w, q)|U) (4) where δ is a parameter, roughly indicating the amount of noise in q.",
        "This looks similar to the interpolation smoothing, except that p(·|Λd) now should be a smoothed language model, instead of the one estimated with MLE.",
        "With no prior knowledge on p(·|U), we could set it to p(·|C).",
        "Any smoothing methods for the document language model can be used to estimate p(·|d) such as the Gamma smoothing as discussed in Section 2.2.1.",
        "The empirical study of the smoothing methods is presented in Section 4. 3.",
        "ANALYSIS OF POISSON LANGUAGE MODEL From the previous section, we notice that the Poisson language model has a strong connection to the multinomial language model.",
        "This is expected since they both belong to the exponential family [26].",
        "However, there are many differences when these two families of models are applied with different smoothing methods.",
        "From the perspective of retrieval, will these two language models perform equivalently?",
        "If not, which model provides more benefits to retrieval, or provides flexibility which could lead to potential benefits?",
        "In this section, we analytically discuss the retrieval features of the Poisson language models, by comparing their behavior with that of the multinomial language models. 3.1 The Equivalence of Basic Models Let us begin with the assumption that all the query terms appear in every document.",
        "Under this assumption, no smoothing is needed.",
        "A document can be scored by the log likelihood of the query with the maximum likelihood estimate: Score(d, q) = w∈V log e−λd,w|q| (λd,w|q|)c(w,q) c(w, q)! (5) Using the MLE, we have λd,w = c(w,d) w∈V c(w,d) .",
        "Thus Score(d, q) ∝ c(w,q)>0 c(w, q) log c(w, d) w∈V c(w, d) This is exactly the log likelihood of the query if the document language model is a multinomial with maximum likelihood estimate.",
        "Indeed, even with Gamma smoothing, when plugging λd,w = c(w,d)+µλC,w |d|+µ and λC,w = c(w,C) |C| into Equation 5, it is easy to show that Score(d, q) ∝ w∈q∩d c(w, q) log(1 + c(w, d) µ · c(w,C) |C| ) + |q| log µ |d| + µ (6) which is exactly the Dirichlet retrieval formula in [28].",
        "Note that this equivalence holds only when the document length variation is modeled with Poisson process.",
        "This derivation indicates the equivalence of the basic Poisson and multinomial language models for retrieval.",
        "With other smoothing strategies, however, the two models would be different.",
        "Nevertheless, with this equivalence in basic models, we could expect that the Poisson language model performs comparably to the multinomial language model in retrieval, if only simple smoothing is explored.",
        "Based on this equivalence analysis, one may ask, why we should pursue the Poisson language model.",
        "In the following sections, we show that despite the equivalence in their basic models, the Poisson language model brings in extra flexibility for exploring advanced techniques on various retrieval features, which could not be achieved with multinomial language models. 3.2 Term Dependent Smoothing One flexibility of the Poisson language model is that it provides a natural framework to accommodate term dependent (per-term) smoothing.",
        "Existing work on language model smoothing has already shown that different types of queries should be smoothed differently according to how discriminative the query terms are. [7] also predicted that different terms should have a different smoothing weights.",
        "With multinomial query generation models, people usually use a single smoothing coefficient to control the combination of the document model and the background model [28, 29].",
        "This parameter can be made specific for different queries, but always has to be a constant for all the terms.",
        "This is mandatory since a multinomial language model has the constraint that w∈V p(w|d) = 1.",
        "However, from retrieval perspective, different terms may need to be smoothed differently even if they are in the same query.",
        "For example, a non-discriminative term (e.g., the, is) is expected to be explained more with the background model, while a content term (e.g., retrieval, bush) in the query should be explained with the document model.",
        "Therefore, a better way of smoothing would be to set the interpolation coefficient (i.e., δ in Formula 2 and Formula 3) specifically for each term.",
        "Since the Poisson language model does not have the sum-to-one constraint across terms, it can easily accommodate per-term smoothing without needing to heuristically twist the semantics of a generative model as in the case of multinomial language models.",
        "Below we present a possible way to explore term dependent smoothing with Poisson language models.",
        "Essentially, we want to use a term-specific smoothing coefficient δ in the linear combination, denoted as δw.",
        "This coefficient should intuitively be larger if w is a common word and smaller if it is a content word.",
        "The key problem is to find a method to assign reasonable values to δw.",
        "Empirical tuning is infeasible for so many parameters.",
        "We may instead estimate the parameters ∆ = {δ1, ..., δ|V |} by maximizing the likelihood of the query given the mixture model of p(q|ΛQ) and p(q|U), where ΛQ is the true query model to generate the query and p(q|U) is a query background model as discussed in Section 2.2.3.",
        "With the model p(q|ΛQ) hidden, the query likelihood is p(q|∆, U) = ΛQ w∈V ((1 − δw)p(c(w, q)|ΛQ) + δwp(c(w, q)|U))P(ΛQ|U)dΛQ If we have relevant documents for each query, we can approximate the query model space with the language models of all the relevant documents.",
        "Without relevant documents, we opt to approximate the query model space with the models of all the documents in the collection.",
        "Setting p(·|U) as p(·|C), the query likelihood becomes p(q|∆, U) = d∈C πd w∈V ((1−δw)p(c(w, q)|ˆΛd)+δwp(c(w, q)|C)) where πd = p(ˆΛd|U). p(·|ˆΛd) is an estimated Poisson language model for document d. If we have prior knowledge on p(ˆΛd|U), such as which documents are relevant to the query, we can set πd accordingly, because what we want is to find ∆ that can maximize the likelihood of the query given relevant documents.",
        "Without this prior knowledge, we can leave πd as free parameters, and use the EM algorithm to estimate πd and ∆.",
        "The updating functions are given as π (k+1) d = πd w∈V ((1 − δw)p(c(w, q)|ˆΛd) + δwp(c(w, q)|C)) d∈C πd w∈V ((1 − δw)p(c(w, q)|ˆΛd) + δwp(c(w, q)|C)) and δ (k+1) w = d∈C πd δwp(c(w, q)|C)) (1 − δw)p(c(w, q)|ˆΛd) + δwp(c(w, q)|C)) As discussed in [29], we only need to run the EM algorithm for several iterations, thus the computational cost is relatively low.",
        "We again assume our vocabulary containing all query terms plus a pseudo non-query term.",
        "Note that the function does not give an explicit way of estimating the coefficient for the unseen non-query term.",
        "In our experiments, we set it to the average over δw of all query terms.",
        "With this flexibility, we expect Poisson language models could improve the retrieval performance, especially for verbose queries, where the query terms have various discriminative values.",
        "In Section 4, we use empirical experiments to prove this hypothesis. 3.3 Mixture Background Models Another flexibility is to explore different background (collection) models (i.e., p(·|U), or p(·|C)).",
        "One common assumption made in language modeling information retrieval is that the background model is a homogeneous model of the document models [28, 29].",
        "Similarly, we can also make the assumption that the collection model is a Poisson language model, with the rates λC,w = d∈C c(w,d) |C| .",
        "However, this assumption usually does not hold, since the collection is far more complex than a single document.",
        "Indeed, the collection usually consists of a mixture of documents with various genres, authors, and topics, etc.",
        "Treating the collection model as a mixture of document models, instead of a single pseudo-document model is more reasonable.",
        "Existing work of multinomial language modeling has already shown that a better modeling of background improves the retrieval performance, such as clusters [15, 10], neighbor documents [25], and aspects [8, 27].",
        "All the approaches can be easily adopted using Poisson language models.",
        "However, a common problem of these approaches is that they all require heavy computation to construct the background model.",
        "With Poisson language modeling, we show that it is possible to model the mixture background without paying for the heavy computational cost.",
        "Poisson Mixture [3] has been proposed to model a collection of documents, which can fit the data much better than a single Poisson.",
        "The basic idea is to assume that the collection is generated from a mixture of Poisson models, which has the general form of p(x = k|PM) = λ p(λ)p(x = k|λ)dλ p(·|λ) is a single Poisson model and p(λ) is an arbitrary probability density function.",
        "There are three well known Poisson mixtures [3]: 2-Poisson, Negative Binomial, and the Katzs K-Mixture [9].",
        "Note that the 2-Poisson model has actually been explored in probabilistic retrieval models, which led to the well-known BM25 formula [22].",
        "All these mixtures have closed forms, and can be estimated from the collection of documents efficiently.",
        "This is an advantage over the multinomial mixture models, such as PLSI [8] and LDA [1], for retrieval.",
        "For example, the probability density function of Katzs K-Mixture is given as p(c(w) = k|αw, βw) = (1 − αw)ηk,0 + αw βw + 1 ( βw βw + 1 )k where ηk,0 = 1 when k = 0, and 0 otherwise.",
        "With the observation of a collection of documents, αw and βw can be estimated as βw = cf(w) − df(w) df(w) and αw = cf(w) Nβw where cf(w) and df(w) are the collection frequency and document frequency of w, and N is the number of documents in the collection.",
        "To account for the different document lengths, we assume that βw is a reasonable estimation for generating a document of the average length, and use β = βw avdl |q| to generate the query.",
        "This Poisson mixture model can be easily used to replace P(·|C) in the retrieval functions 3 and 4. 3.4 Other Possible Flexibilities In addition to term dependent smoothing and efficient mixture background, a Poisson language model has also some other potential advantages.",
        "For example, in Section 2, we see that Formula 2 introduces a component which does document length penalization.",
        "Intuitively, when the document has more unique words, it will be penalized more.",
        "On the other hand, if a document is exactly n copies of another document, it would not get over penalized.",
        "This feature is desirable and not achieved with the Dirichlet model [5].",
        "Potentially, this component could penalize a document according to what types of terms it contains.",
        "With term specific settings of δ, we could get even more flexibility for document length normalization.",
        "Pseudo-feedback is yet another interesting direction where the Poission model might be able to show its advantage.",
        "With model-based feedback, we could again relax the combination coefficients of the feedback model and the background model, and allow different terms to contribute differently to the feedback model.",
        "We could also utilize the relevant documents to learn better per-term smoothing coefficients. 4.",
        "EVALUATION In Section 3, we analytically compared the Poisson language models and multinomial language models from the perspective of query generation and retrieval.",
        "In this section, we compare these two families of models empirically.",
        "Experiment results show that the Poisson model with perterm smoothing outperforms multinomial model, and the performance can be further improved with two-stage smoothing.",
        "Using Poisson mixture as background model also improves the retrieval performance. 4.1 Datasets Since retrieval performance could significantly vary from one test collection to another, and from one query to another, we select four representative TREC test collections: AP, Trec7, Trec8, and Wt2g(Web).",
        "To cover different types of queries, we follow [28, 5], and construct short-keyword (SK, keyword title), short-verbose (SV, one sentence description), and long-verbose (LV, multiple sentences) queries.",
        "The documents are stemmed with the Porters stemmer, and we do not remove any stop word.",
        "For each parameter, we vary its value to cover a reasonably wide range. 4.2 Comparison to Multinomial We compare the performance of the Poisson retrieval models and multinomial retrieval models using interpolation (JelinekMercer, JM) smoothing and Bayesian smoothing with conjugate priors.",
        "Table 1 shows that the two JM-smoothed models perform similarly on all data sets.",
        "Since the Dirichlet Smoothing for multinomial language model and the Gamma Smoothing for Poisson language model lead to the same retrieval formula, the performance of these two models are jointly presented.",
        "We see that Dirichlet/Gamma smoothing methods outperform both Jelinek-Mercer smoothing methods.",
        "The parameter sensitivity curves for two Jelinek-Mercer 0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1 0 0.05 0.1 0.15 0.2 0.25 0.3 Dataset: Trec8 Parameter: δ AveragePrecision JM−Multinomial: LV JM−Multinomial: SV JM−Multinomial: SK JM−Poisson: SK JM−Poisson: SV JM−Poisson: LV Figure 1: Poisson and multinomial performs similarly with Jelinek-Mercer smoothing smoothing methods are shown in Figure 1.",
        "Clearly, these two methods perform similarly either in terms of optimality Data Query JM-Multinomial JM-Poisson Dirichlet/Gamma Per-term 2-Stage Poisson MAP InitPr Pr@5d MAP InitPr Pr@5d MAP InitPr Pr@5d MAP InitPr Pr@5d AP88-89 SK 0.203 0.585 0.356 0.203 0.585 0.358 0.224 0.629 0.393 0.226 0.630 0.396 SV 0.187 0.580 0.361 0.183 0.571 0.345 0.204 0.613 0.387 0.217* 0.603 0.390 LV 0.283 0.716 0.480 0.271 0.692 0.470 0.291 0.710 0.496 0.304* 0.695 0.510 Trec7 SK 0.167 0.635 0.400 0.168 0.635 0.404 0.186 0.687 0.428 0.185 0.646 0.436 SV 0.174 0.655 0.432 0.176 0.653 0.432 0.182 0.666 0.432 0.196* 0.660 0.440 LV 0.223 0.730 0.496 0.215 0.766 0.488 0.224 0.748 0.52 0.236* 0.738 0.512 Trec8 SK 0.239 0.621 0.440 0.239 0.621 0.436 0.257 0.718 0.496 0.256 0.704 0.468 SV 0.231 0.686 0.448 0.234 0.702 0.456 0.228 0.691 0.456 0.246* 0.692 0.476 LV 0.265 0.796 0.548 0.261 0.757 0.520 0.260 0.741 0.492 0.274* 0.766 0.508 Web SK 0.250 0.616 0.380 0.250 0.616 0.380 0.302 0.767 0.468 0.307 0.739 0.468 SV 0.214 0.611 0.392 0.217 0.609 0.384 0.273 0.693 0.508 0.292* 0.703 0.480 LV 0.266 0.790 0.464 0.259 0.776 0.452 0.283 0.756 0.496 0.311* 0.759 0.488 Table 1: Performance comparison between Poisson and Multinomial retrieval models: basic models perform comparably; term dependent two-stage smoothing significantly improves Poisson An asterisk (*) indicates that the difference between the performance of the term dependent two-stage smoothing and that of the Dirichlet/Gamma single smoothing is statistically significant according to the Wilcoxon signed rank test at the level of 0.05. or sensitivity.",
        "This similarity of performance is expected as we discussed in Section 3.1.",
        "Although the Poisson model and multinomial model are similar in terms of the basic model and/or with simple smoothing methods, the Poisson model has great potential and flexibility to be further improved.",
        "As shown in the rightmost column of Table 1, term dependent two-stage Poisson model consistently outperforms the basic smoothing models, especially for verbose queries.",
        "This model is given in Formula 4, with a Gamma smoothing for the document model p(·|d), and δw, which is term dependent.",
        "The parameter µ of the first stage Gamma smoothing is empirically tuned.",
        "The combination coefficients (i.e., ∆), are estimated with the EM algorithm in Section 3.2.",
        "The parameter sensitivity curves for Dirichlet/Gamma and the per-term two-stage smoothing model are plotted in Figure 2.",
        "The per-term two-stage smoothing method is less sensitive to the parameter µ than Dirichlet/Gamma, and yields better optimal performance. 0 1000 2000 3000 4000 5000 6000 7000 8000 9000 10000 0.1 0.12 0.14 0.16 0.18 0.2 0.22 Dataset: AP; Query Type: SV Parameter: µ AveragePrecision Dirichlet/Gamma Smoothing Term Dependent 2−Stage Figure 2: Term dependent two-stage smoothing of Poisson outperforms Dirichlet/Gamma In the following subsections, we conduct experiments to demonstrate how the flexibility of the Poisson model could be utilized to achieve better performance, which we cannot achieve with multinomial language models. 4.3 Term Dependent Smoothing To test the effectiveness of the term dependent smoothing, we conduct the following two experiments.",
        "In the first experiment, we relax the constant coefficient in the simple Jelinek-Mercer smoothing formula (i.e., Formula 3), and use the EM algorithm proposed in Section 3.2 to find a δw for each unique term.",
        "Since we are using the EM algorithm to iteratively estimate the parameters, we usually do not want the probability of p(·|d) to be zero.",
        "We then use a simple Laplace method to slightly smooth the document model before it goes into the EM iterations.",
        "The documents are then still scored with Formula 3, but using learnt δw.",
        "The results are labeled with JM+L. in Table 2.",
        "Data Q JM JM JM+L. 2-Stage 2-Stage (MAP) PT: No Yes Yes No Yes AP SK 0.203 0.204 0.206 0.223 0.226* SV 0.183 0.189 0.214* 0.204 0.217* Trec7 SK 0.168 0.171 0.174 0.186 0.185 SV 0.176 0.147 0.198* 0.194 0.196 Trec8 SK 0.239 0.240 0.227* 0.257 0.256 SV 0.234 0.223 0.249* 0.242 0.246* Web SK 0.250 0.236 0.220* 0.291 0.307* SV 0.217 0.232 0.261* 0.273 0.292* Table 2: Term dependent smoothing improves retrieval performance An asterisk (*) in Column 3 indicates that the difference between the JM+L. method and JM method is statistically significant; an asterisk (*) in Column 5 means that the difference between term dependent two-stage method and query dependent two-stage method is statistically significant; PT stands for per-term.",
        "With term dependent coefficients, the performance of the Jelinek-Mercer Poisson model is improved in most cases.",
        "However, in some cases (e.g., Trec7/SV), it performs poorly.",
        "This might be caused by the problem of EM estimation with unsmoothed document models.",
        "Once non-zero probability is assigned to all the terms before entering the EM iteration, the performance on verbose queries can be improved significantly.",
        "This indicates that there is still room to find better methods to estimate δw.",
        "Please note that neither the perterm JM method nor the JM+L. method has a parameter to tune.",
        "As shown in Table 1, the term dependent two-stage smoothing can significantly improve retrieval performance.",
        "To understand whether the improvement is contributed by the term dependent smoothing or the two-stage smoothing framework, we design another experiment to compare the perterm two-stage smoothing with the two-stage smoothing method proposed in [29].",
        "Their method managed to find coefficients specific to the query, thus a verbose query would use a higher δ.",
        "However, since their model is based on multinomial language modeling, they could not get per-term coefficients.",
        "We adopt their method to the Poisson two-stage smoothing, and also estimate a per-query coefficient for all the terms.",
        "We compare the performance of such a model with the per-term two-stage smoothing model, and present the results in the right two columns in Table 2.",
        "Again, we see that the per-term two-stage smoothing outperforms the per-query two-stage smoothing, especially for verbose queries.",
        "The improvement is not as large as how the perterm smoothing method improves over Dirichlet/Gamma.",
        "This is expected, since the per-query smoothing has already addressed the query discrimination problem to some extent.",
        "This experiment shows that even if the smoothing is already per-query, making it per-term is still beneficial.",
        "In brief, the per-term smoothing improved the retrieval performance of both one-stage and two-stage smoothing method. 4.4 Mixture Background Model In this section, we conduct experiments to examine the benefits of using a mixture background model without extra computational cost, which can not be achieved for multinomial models.",
        "Specifically, in retrieval formula 3, instead of using a single Poisson distribution to model the background p(·|C), we use Katzs K-Mixture model, which is essentially a mixture of Poisson distributions. p(·|C) can be computed efficiently with simple collection statistics, as discussed in Section 3.3.",
        "Data Query JM.",
        "Poisson JM.",
        "K-Mixture AP SK 0.203 0.204 SV 0.183 0.188* Trec-7 SK 0.168 0.169 SV 0.176 0.178* Trec-8 SK 0.239 0.239 SV 0.234 0.238* Web SK 0.250 0.250 SV 0.217 0.223* Table 3: K-Mixture background model improves retrieval performance The performance of the JM retrieval model with single Poisson background and with Katzs K-Mixture background model is compared in Table 3.",
        "Clearly, using K-Mixture to model the background model outperforms the single Poisson background model in most cases, especially for verbose queries where the improvement is statistically significant.",
        "Figure 3 shows that the performance changes over different parameters for short verbose queries.",
        "The model using K-Mixture background is less sensitive than the one using single Poisson background.",
        "Given that this type of mixture 0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1 0 0.05 0.1 0.15 0.2 0.25 Data: Trec8; Query: SV Parameter: δ AveragePrecision Poisson Background K−Mixture Background Figure 3: K-Mixture background model deviates the sensitivity of verbose queries background model does not require any extra computation cost, it would be interesting to study whether using other mixture Poisson models, such as 2-Poisson and negative Binomial, could help the performance. 5.",
        "RELATED WORK To the best of our knowledge, there has been no study of query generation models based on Poisson distribution.",
        "Language models have been shown to be effective for many retrieval tasks [21, 28, 14, 4].",
        "The most popular and fundamental one is the query-generation language model [21, 13].",
        "All existing query generation language models are based on either multinomial distribution [19, 6, 28, 13] or multivariate Bernoulli distribution [21, 17, 18].",
        "We introduce a new family of language models, based on Poisson distribution.",
        "Poisson distribution has been previously studied in the document generation models [16, 22, 3, 24], leading to the development of one of the most effective retrieval formula BM25 [23]. [24] studies the parallel derivation of three different retrieval models which is related to our comparison of Poisson and multinomial.",
        "However, the Poisson model in their paper is still under the document generation framework, and also does not account for the document length variation. [26] introduces a way to empirically search for an exponential model for the documents.",
        "Poisson mixtures [3] such as 2-Poisson [22], Negative multinomial, and Katzs KMixture [9] has shown to be effective to model and retrieve documents.",
        "Once again, none of this work explores Poisson distribution in the query generation framework.",
        "Language model smoothing [2, 28, 29] and background structures [15, 10, 25, 27] have been studied with multinomial language models. [7] analytically shows that term specific smoothing could be useful.",
        "We show that Poisson language model is natural to accommodate the per-term smoothing without heuristic twist of the semantics of a generative model, and is able to efficiently better model the mixture background, both analytically and empirically. 6.",
        "CONCLUSIONS We present a new family of query generation language models for retrieval based on Poisson distribution.",
        "We derive several smoothing methods for this family of models, including single-stage smoothing and two-stage smoothing.",
        "We compare the new models with the popular multinomial retrieval models both analytically and experimentally.",
        "Our analysis shows that while our new models and multinomial models are equivalent under some assumptions, they are generally different with some important differences.",
        "In particular, we show that Poisson has an advantage over multinomial in naturally accommodating per-term smoothing.",
        "We exploit this property to develop a new per-term smoothing algorithm for Poisson language models, which is shown to outperform term-independent smoothing for both Poisson and multinomial models.",
        "Furthermore, we show that a mixture background model for Poisson can be used to improve the performance and robustness over the standard Poisson background model.",
        "Our work opens up many interesting directions for further exploration in this new family of models.",
        "Further exploring the flexibilities over multinomial language models, such as length normalization and pseudo-feedback could be good future work.",
        "It is also appealing to find robust methods to learn the per-term smoothing coefficients without additional computation cost. 7.",
        "ACKNOWLEDGMENTS We thank the anonymous SIGIR 07 reviewers for their useful comments.",
        "This material is based in part upon work supported by the National Science Foundation under award numbers IIS-0347933 and 0425852. 8.",
        "REFERENCES [1] D. Blei, A. Ng, and M. Jordan.",
        "Latent dirichlet allocation.",
        "Journal of Machine Learning Research, 3:993-1022, 2003. [2] S. F. Chen and J. Goodman.",
        "An empirical study of smoothing techniques for language modeling.",
        "Technical Report TR-10-98, Harvard University, 1998. [3] K. Church and W. Gale.",
        "Poisson mixtures.",
        "Nat.",
        "Lang.",
        "Eng., 1(2):163-190, 1995. [4] W. B. Croft and J. Lafferty, editors.",
        "Language Modeling and Information Retrieval.",
        "Kluwer Academic Publishers, 2003. [5] H. Fang, T. Tao, and C. Zhai.",
        "A formal study of information retrieval heuristics.",
        "In Proceedings of the 27th annual international ACM SIGIR conference on Research and development in information retrieval, pages 49-56, 2004. [6] D. Hiemstra.",
        "Using Language Models for Information Retrieval.",
        "PhD thesis, University of Twente, Enschede, Netherlands, 2001. [7] D. Hiemstra.",
        "Term-specific smoothing for the language modeling approach to information retrieval: the importance of a query term.",
        "In Proceedings of the 25th annual international ACM SIGIR conference on Research and development in information retrieval, pages 35-41, 2002. [8] T. Hofmann.",
        "Probabilistic latent semantic indexing.",
        "In Proceedings of ACM SIGIR99, pages 50-57, 1999. [9] S. M. Katz.",
        "Distribution of content words and phrases in text and language modelling.",
        "Nat.",
        "Lang.",
        "Eng., 2(1):15-59, 1996. [10] O. Kurland and L. Lee.",
        "Corpus structure, language models, and ad-hoc information retrieval.",
        "In Proceedings of the 27th annual international ACM SIGIR conference on Research and development in information retrieval, pages 194-201, 2004. [11] J. Lafferty and C. Zhai.",
        "Document language models, query models, and risk minimization for information retrieval.",
        "In Proceedings of SIGIR01, pages 111-119, Sept 2001. [12] J. Lafferty and C. Zhai.",
        "Probabilistic IR models based on query and document generation.",
        "In Proceedings of the Language Modeling and IR workshop, pages 1-5, May 31 - June 1 2001. [13] J. Lafferty and C. Zhai.",
        "Probabilistic relevance models based on document and query generation.",
        "In W. B. Croft and J. Lafferty, editors, Language Modeling and Information Retrieval.",
        "Kluwer Academic Publishers, 2003. [14] V. Lavrenko and B. Croft.",
        "Relevance-based language models.",
        "In Proceedings of SIGIR01, pages 120-127, Sept 2001. [15] X. Liu and W. B. Croft.",
        "Cluster-based retrieval using language models.",
        "In Proceedings of the 27th annual international ACM SIGIR conference on Research and development in information retrieval, pages 186-193, 2004. [16] E. L. Margulis.",
        "Modelling documents with multiple poisson distributions.",
        "Inf.",
        "Process.",
        "Manage., 29(2):215-227, 1993. [17] A. McCallum and K. Nigam.",
        "A comparison of event models for naive bayes text classification.",
        "In Proceedings of AAAI-98 Workshop on Learning for Text Categorization, 1998. [18] D. Metzler, V. Lavrenko, and W. B. Croft.",
        "Formal multiple-bernoulli models for language modeling.",
        "In Proceedings of the 27th annual international ACM SIGIR conference on Research and development in information retrieval, pages 540-541, 2004. [19] D. H. Miller, T. Leek, and R. Schwartz.",
        "A hidden Markov model information retrieval system.",
        "In Proceedings of the 1999 ACM SIGIR Conference on Research and Development in Information Retrieval, pages 214-221, 1999. [20] A. Papoulis.",
        "Probability, random variables and stochastic processes.",
        "New York: McGraw-Hill, 1984, 2nd ed., 1984. [21] J. M. Ponte and W. B. Croft.",
        "A language modeling approach to information retrieval.",
        "In Proceedings of the 21st annual international ACM SIGIR conference on Research and development in information retrieval, pages 275-281, 1998. [22] S. Robertson and S. Walker.",
        "Some simple effective approximations to the 2-poisson model for probabilistic weighted retrieval.",
        "In Proceedings of SIGIR94, pages 232-241, 1994. [23] S. E. Robertson, S. Walker, S. Jones, M. M.Hancock-Beaulieu, and M. Gatford.",
        "Okapi at TREC-3.",
        "In D. K. Harman, editor, The Third Text REtrieval Conference (TREC-3), pages 109-126, 1995. [24] T. Roelleke and J. Wang.",
        "A parallel derivation of probabilistic information retrieval models.",
        "In Proceedings of the 29th annual international ACM SIGIR conference on Research and development in information retrieval, pages 107-114, 2006. [25] T. Tao, X. Wang, Q. Mei, and C. Zhai.",
        "Language model information retrieval with document expansion.",
        "In Proceedings of HLT/NAACL 2006, pages 407-414, 2006. [26] J. Teevan and D. R. Karger.",
        "Empirical development of an exponential probabilistic model for text retrieval: using textual analysis to build a better model.",
        "In Proceedings of the 26th annual international ACM SIGIR conference on Research and development in informaion retrieval, pages 18-25, 2003. [27] X. Wei and W. B. Croft.",
        "Lda-based document models for ad-hoc retrieval.",
        "In Proceedings of the 29th annual international ACM SIGIR conference on Research and development in information retrieval, pages 178-185, 2006. [28] C. Zhai and J. Lafferty.",
        "A study of smoothing methods for language models applied to ad-hoc information retrieval.",
        "In Proceedings of ACM SIGIR01, pages 334-342, Sept 2001. [29] C. Zhai and J. Lafferty.",
        "Two-stage language models for information retrieval.",
        "In Proceedings of ACM SIGIR02, pages 49-56, Aug 2002."
    ],
    "error_count": 0,
    "keys": {
        "multinomial distribution": {
            "translated_key": "",
            "is_in_text": true,
            "original_annotated_sentences": [
                "A Study of Poisson Query Generation Model for Information Retrieval Qiaozhu Mei, Hui Fang, Chengxiang Zhai Department of Computer Science University of Illinois at Urbana-Champaign Urbana,IL 61801 {qmei2,hfang,czhai}@uiuc.edu ABSTRACT Many variants of language models have been proposed for information retrieval.",
                "Most existing models are based on <br>multinomial distribution</br> and would score documents based on query likelihood computed based on a query generation probabilistic model.",
                "In this paper, we propose and study a new family of query generation models based on Poisson distribution.",
                "We show that while in their simplest forms, the new family of models and the existing multinomial models are equivalent, they behave differently for many smoothing methods.",
                "We show that the Poisson model has several advantages over the multinomial model, including naturally accommodating per-term smoothing and allowing for more accurate background modeling.",
                "We present several variants of the new model corresponding to different smoothing methods, and evaluate them on four representative TREC test collections.",
                "The results show that while their basic models perform comparably, the Poisson model can outperform multinomial model with per-term smoothing.",
                "The performance can be further improved with two-stage smoothing.",
                "Categories and Subject Descriptors: H.3.3 [Information Search and Retrieval]: Retrieval Models General Terms: Algorithms 1.",
                "INTRODUCTION As a new type of probabilistic retrieval models, language models have been shown to be effective for many retrieval tasks [21, 28, 14, 4].",
                "Among many variants of language models proposed, the most popular and fundamental one is the query-generation language model [21, 13], which leads to the query-likelihood scoring method for ranking documents.",
                "In such a model, given a query q and a document d, we compute the likelihood of generating query q with a model estimated based on document d, i.e., the conditional probability p(q|d).",
                "We can then rank documents based on the likelihood of generating the query.",
                "Virtually all the existing query generation language models are based on either <br>multinomial distribution</br> [19, 6, 28] or multivariate Bernoulli distribution [21, 18].",
                "The <br>multinomial distribution</br> is especially popular and also shown to be quite effective.",
                "The heavy use of <br>multinomial distribution</br> is partly due to the fact that it has been successfully used in speech recognition, where <br>multinomial distribution</br> is a natural choice for modeling the occurrence of a particular word in a particular position in text.",
                "Compared with multivariate Bernoulli, <br>multinomial distribution</br> has the advantage of being able to model the frequency of terms in the query; in contrast, multivariate Bernoulli only models the presence and absence of query terms, thus cannot capture different frequencies of query terms.",
                "However, multivariate Bernoulli also has one potential advantage over multinomial from the viewpoint of retrieval: in a <br>multinomial distribution</br>, the probabilities of all the terms must sum to 1, making it hard to accommodate per-term smoothing, while in a multivariate Bernoulli, the presence probabilities of different terms are completely independent of each other, easily accommodating per-term smoothing and weighting.",
                "Note that term absence is also indirectly captured in a multinomial model through the constraint that all the term probabilities must sum to 1.",
                "In this paper, we propose and study a new family of query generation models based on the Poisson distribution.",
                "In this new family of models, we model the frequency of each term independently with a Poisson distribution.",
                "To score a document, we would first estimate a multivariate Poisson model based on the document, and then score it based on the likelihood of the query given by the estimated Poisson model.",
                "In some sense, the Poisson model combines the advantage of multinomial in modeling term frequency and the advantage of the multivariate Bernoulli in accommodating per-term smoothing.",
                "Indeed, similar to the <br>multinomial distribution</br>, the Poisson distribution models term frequencies, but without the constraint that all the term probabilities must sum to 1, and similar to multivariate Bernoulli, it models each term independently, thus can easily accommodate per-term smoothing.",
                "As in the existing work on multinomial language models, smoothing is critical for this new family of models.",
                "We derive several smoothing methods for Poisson model in parallel to those used for multinomial distributions, and compare the corresponding retrieval models with those based on multinomial distributions.",
                "We find that while with some smoothing methods, the new model and the multinomial model lead to exactly the same formula, with some other smoothing methods they diverge, and the Poisson model brings in more flexibility for smoothing.",
                "In particular, a key difference is that the Poisson model can naturally accommodate perterm smoothing, which is hard to achieve with a multinomial model without heuristic twist of the semantics of a generative model.",
                "We exploit this potential advantage to develop a new term-dependent smoothing algorithm for Poisson model and show that this new smoothing algorithm can improve performance over term-independent smoothing algorithms using either Poisson or multinomial model.",
                "This advantage is seen for both one-stage and two-stage smoothing.",
                "Another potential advantage of the Poisson model is that its corresponding background model for smoothing can be improved through using a mixture model that has a closed form formula.",
                "This new background model is shown to outperform the standard background model and reduce the sensitivity of retrieval performance to the smoothing parameter.",
                "The rest of the paper is organized as follows.",
                "In Section 2, we introduce the new family of query generation models with Poisson distribution, and present various smoothing methods which lead to different retrieval functions.",
                "In Section 3, we analytically compare the Poisson language model with the multinomial language model, from the perspective of retrieval.",
                "We then design empirical experiments to compare the two families of language models in Section 4.",
                "We discuss the related work in 5 and conclude in 6. 2.",
                "QUERY GENERATION WITH POISSON PROCESS In the query generation framework, a basic assumption is that a query is generated with a model estimated based on a document.",
                "In most existing work [12, 6, 28, 29], people assume that each query word is sampled independently from a <br>multinomial distribution</br>.",
                "Alternatively, we assume that a query is generated by sampling the frequency of words from a series of independent Poisson processes [20]. 2.1 The Generation Process Let V = {w1, ..., wn} be a vocabulary set.",
                "Let w be a piece of text composed by an author and c(w1), ..., c(wn) be a frequency vector representing w, where c(wi, w) is the frequency count of term wi in text w. In retrieval, w could be either a query or a document.",
                "We consider the frequency counts of the n unique terms in w as n different types of events, sampled from n independent homogeneous Poisson processes, respectively.",
                "Suppose t is the time period during which the author composed the text.",
                "With a homogeneous Poisson process, the frequency count of each event, i.e., the number of occurrences of wi, follows a Poisson distribution with associated parameter λit, where λi is a rate parameter characterizing the expected number of wi in a unit time.",
                "The probability density function of such a Poisson Distribution is given by P(c(wi, w) = k|λit) = e−λit (λit)k k!",
                "Without losing generality, we set t to the length of the text w (people write one word in a unit time), i.e., t = |w|.",
                "With n such independent Poisson processes, each explaining the generation of one term in the vocabulary, the likelihood of w to be generated from such Poisson processes can be written as p(w|Λ) = n i=1 p(c(wi, w)|Λ) = n i=1 e−λi·|w| (λi · |w|)c(wi,w) c(wi, w)! where Λ = {λ1, ..., λn} and |w| = n i=1 c(wi, w).",
                "We refer to these n independent Poisson processes with parameter Λ as a Poisson Language Model.",
                "Let D = {d1, ..., dm} be an observed set of document samples generated from the Poisson process above.",
                "The maximum likelihood estimate (MLE) of λi is ˆλi = d∈D c(wi, d) d∈D w ∈V c(w , d) Note that this MLE is different from the MLE for the Poisson distribution without considering the document lengths, which appears in [22, 24].",
                "Given a document d, we may estimate a Poisson language model Λd using d as a sample.",
                "The likelihood that a query q is generated from the document language model Λd can be written as p(q|d) = w∈V p(c(w, q)|Λd) (1) This representation is clearly different from the multinomial query generation model as (1) the likelihood includes all the terms in the vocabulary V , instead of only those appearing in q, and (2) instead of the appearance of terms, the event space of this model is the frequencies of each term.",
                "In practice, we have the flexibility to choose the vocabulary V .",
                "In one extreme, we can use the vocabulary of the whole collection.",
                "However, this may bring in noise and considerable computational cost.",
                "In the other extreme, we may focus on the terms in the query and ignore other terms, but some useful information may be lost by ignoring the nonquery terms.",
                "As a compromise, we may conflate all the non-query terms as one single pseudo term.",
                "In other words, we may assume that there is exactly one non-query term in the vocabulary for each query.",
                "In our experiments, we adopt this pseudo non-query term strategy.",
                "A document can be scored with the likelihood in Equation 1.",
                "However, if a query term is unseen in the document, the MLE of the Poisson distribution would assign zero probability to the term, causing the probability of the query to be zero.",
                "As in existing language modeling approaches, the main challenge of constructing a reasonable retrieval model is to find a smoothed language model for p(·|d). 2.2 Smoothing in Poisson Retrieval Model In general, we want to assign non-zero rates for the query terms that are not seen in document d. Many smoothing methods have been proposed for multinomial language models[2, 28, 29].",
                "In general, we have to discount the probabilities of some words seen in the text to leave some extra probability mass to assign to the unseen words.",
                "In Poisson language models, however, we do not have the same constraint as in a multinomial model (i.e., w∈V p(w|d) = 1).",
                "Thus we do not have to discount the probability of seen words in order to give a non-zero rate to an unseen word.",
                "Instead, we only need to guarantee that k=0,1,2,... p(c(w, d) = k|d) = 1.",
                "In this section, we introduce three different strategies to smooth a Poisson language model, and show how they lead to different retrieval functions. 2.2.1 Bayesian Smoothing using Gamma Prior Following the risk minimization framework in [11], we assume that a document is generated by the arrival of terms in a time period of |d| according to the document language model, which essentially consists of a vector of Poisson rates for each term, i.e., Λd = λd,1, ..., λd,|V | .",
                "A document is assumed to be generated from a potentially different model.",
                "Given a particular document d, we want to estimate Λd.",
                "The rate of a term is estimated independently of other terms.",
                "We use Bayesian estimation with the following Gamma prior, which has two parameters, α and β: Gamma(λ|α, β) = βα Γ(α) λα−1 e−βλ For each term w, the parameters αw and βw are chosen to be αw = µ ∗ λC,w and βw = µ, where µ is a parameter and λC,w is the rate of w estimated from some background language model, usually the collection language model.",
                "The posterior distribution of Λd is given by p(Λd|d, C) ∝ w∈V e−λw(|d|+µ) λ c(w,d)+µλC,w−1 w which is a product of |V | Gamma distributions with parameters c(w, d) + µλC,w and |d| + µ for each word w. Given that the Gamma mean is α β , we have ˆλd,w = λd,w λd,wp(λd,w|d, C)dλd,w = c(w, d) + µλC,w |d| + µ This is precisely the smoothed estimate of multinomial language model with Dirichlet prior [28]. 2.2.2 Interpolation (Jelinek-Mercer) Smoothing Another straightforward method is to decompose the query generation model as a mixture of two component models.",
                "One is the document language model estimated with maximum likelihood estimator, and the other is a model estimated from the collection background, p(·|C), which assigns non-zero rate to w. For example, we may use an interpolation coefficient between 0 and 1 (i.e., δ ∈ [0, 1]).",
                "With this simple interpolation, we can score a document with Score(d, q) = w∈V log((1 − δ)p(c(w, q)|d) + δp(c(w, q)|C)) (2) Using the maximum likelihood estimator for p(·|d), we have λd,w = c(w,d) |d| , thus Equation 2 becomes Score(d, q) ∝ w∈d∩q [log(1 + 1 − δ δ e−λd,w|q| (λd,w|q|)c(w,q) c(w, q)! · p(c(w, q)|C) ) − log (1 − δ)e−λd,w|q| + δp(c(w, q) = 0|C) 1 − δ + δp(c(w, q) = 0|C) ] + w∈d log (1 − δ)e−λd,w|q| + δp(c(w, q) = 0|C) 1 − δ + δp(c(w, q) = 0|C) We can also use a Poisson language model for p(·|C), or use some other frequency-based models.",
                "In the retrieval formula above, the first summation can be computed efficiently.",
                "The second summation can be actually treated as a document prior, which penalizes long documents.",
                "As the second summation is difficult to compute efficiently, we conflate all non-query terms as one pseudo non-queryterm, denoted as N. Using the pseudo-term formulation and a Poisson collection model, we can rewrite the retrieval formula as Score(d, q) ∝ w∈d∩q log(1 + 1 − δ δ e−λd,w (λd,w|q|)c(w,q) e−λd,C |q| (λd,C )c(w,q) ) + log (1 − δ)e−λd,N |q| + δe−λC,N |q| 1 − δ + δe−λC,N |q| (3) where λd,N = |d|− w∈q c(w,d) |d| and λC,N = |C|− w∈q c(w,C) |C| . 2.2.3 Two-Stage Smoothing As discussed in [29], smoothing plays two roles in retrieval: (1) to improve the estimation of the document language model, and (2) to explain the common terms in the query.",
                "In order to distinguish the content and non-discriminative words in a query, we follow [29] and assume that a query is generated by sampling from a two-component mixture of Poisson language models, with one component being the document model Λd and the other being a query background language model p(·|U). p(·|U) models the typical term frequencies in the users queries.",
                "We may then score each document with the query likelihood computed using the following two-stage smoothing model: p(c(w, q)|Λd, U) = (1 − δ)p(c(w, q)|Λd) + δp(c(w, q)|U) (4) where δ is a parameter, roughly indicating the amount of noise in q.",
                "This looks similar to the interpolation smoothing, except that p(·|Λd) now should be a smoothed language model, instead of the one estimated with MLE.",
                "With no prior knowledge on p(·|U), we could set it to p(·|C).",
                "Any smoothing methods for the document language model can be used to estimate p(·|d) such as the Gamma smoothing as discussed in Section 2.2.1.",
                "The empirical study of the smoothing methods is presented in Section 4. 3.",
                "ANALYSIS OF POISSON LANGUAGE MODEL From the previous section, we notice that the Poisson language model has a strong connection to the multinomial language model.",
                "This is expected since they both belong to the exponential family [26].",
                "However, there are many differences when these two families of models are applied with different smoothing methods.",
                "From the perspective of retrieval, will these two language models perform equivalently?",
                "If not, which model provides more benefits to retrieval, or provides flexibility which could lead to potential benefits?",
                "In this section, we analytically discuss the retrieval features of the Poisson language models, by comparing their behavior with that of the multinomial language models. 3.1 The Equivalence of Basic Models Let us begin with the assumption that all the query terms appear in every document.",
                "Under this assumption, no smoothing is needed.",
                "A document can be scored by the log likelihood of the query with the maximum likelihood estimate: Score(d, q) = w∈V log e−λd,w|q| (λd,w|q|)c(w,q) c(w, q)! (5) Using the MLE, we have λd,w = c(w,d) w∈V c(w,d) .",
                "Thus Score(d, q) ∝ c(w,q)>0 c(w, q) log c(w, d) w∈V c(w, d) This is exactly the log likelihood of the query if the document language model is a multinomial with maximum likelihood estimate.",
                "Indeed, even with Gamma smoothing, when plugging λd,w = c(w,d)+µλC,w |d|+µ and λC,w = c(w,C) |C| into Equation 5, it is easy to show that Score(d, q) ∝ w∈q∩d c(w, q) log(1 + c(w, d) µ · c(w,C) |C| ) + |q| log µ |d| + µ (6) which is exactly the Dirichlet retrieval formula in [28].",
                "Note that this equivalence holds only when the document length variation is modeled with Poisson process.",
                "This derivation indicates the equivalence of the basic Poisson and multinomial language models for retrieval.",
                "With other smoothing strategies, however, the two models would be different.",
                "Nevertheless, with this equivalence in basic models, we could expect that the Poisson language model performs comparably to the multinomial language model in retrieval, if only simple smoothing is explored.",
                "Based on this equivalence analysis, one may ask, why we should pursue the Poisson language model.",
                "In the following sections, we show that despite the equivalence in their basic models, the Poisson language model brings in extra flexibility for exploring advanced techniques on various retrieval features, which could not be achieved with multinomial language models. 3.2 Term Dependent Smoothing One flexibility of the Poisson language model is that it provides a natural framework to accommodate term dependent (per-term) smoothing.",
                "Existing work on language model smoothing has already shown that different types of queries should be smoothed differently according to how discriminative the query terms are. [7] also predicted that different terms should have a different smoothing weights.",
                "With multinomial query generation models, people usually use a single smoothing coefficient to control the combination of the document model and the background model [28, 29].",
                "This parameter can be made specific for different queries, but always has to be a constant for all the terms.",
                "This is mandatory since a multinomial language model has the constraint that w∈V p(w|d) = 1.",
                "However, from retrieval perspective, different terms may need to be smoothed differently even if they are in the same query.",
                "For example, a non-discriminative term (e.g., the, is) is expected to be explained more with the background model, while a content term (e.g., retrieval, bush) in the query should be explained with the document model.",
                "Therefore, a better way of smoothing would be to set the interpolation coefficient (i.e., δ in Formula 2 and Formula 3) specifically for each term.",
                "Since the Poisson language model does not have the sum-to-one constraint across terms, it can easily accommodate per-term smoothing without needing to heuristically twist the semantics of a generative model as in the case of multinomial language models.",
                "Below we present a possible way to explore term dependent smoothing with Poisson language models.",
                "Essentially, we want to use a term-specific smoothing coefficient δ in the linear combination, denoted as δw.",
                "This coefficient should intuitively be larger if w is a common word and smaller if it is a content word.",
                "The key problem is to find a method to assign reasonable values to δw.",
                "Empirical tuning is infeasible for so many parameters.",
                "We may instead estimate the parameters ∆ = {δ1, ..., δ|V |} by maximizing the likelihood of the query given the mixture model of p(q|ΛQ) and p(q|U), where ΛQ is the true query model to generate the query and p(q|U) is a query background model as discussed in Section 2.2.3.",
                "With the model p(q|ΛQ) hidden, the query likelihood is p(q|∆, U) = ΛQ w∈V ((1 − δw)p(c(w, q)|ΛQ) + δwp(c(w, q)|U))P(ΛQ|U)dΛQ If we have relevant documents for each query, we can approximate the query model space with the language models of all the relevant documents.",
                "Without relevant documents, we opt to approximate the query model space with the models of all the documents in the collection.",
                "Setting p(·|U) as p(·|C), the query likelihood becomes p(q|∆, U) = d∈C πd w∈V ((1−δw)p(c(w, q)|ˆΛd)+δwp(c(w, q)|C)) where πd = p(ˆΛd|U). p(·|ˆΛd) is an estimated Poisson language model for document d. If we have prior knowledge on p(ˆΛd|U), such as which documents are relevant to the query, we can set πd accordingly, because what we want is to find ∆ that can maximize the likelihood of the query given relevant documents.",
                "Without this prior knowledge, we can leave πd as free parameters, and use the EM algorithm to estimate πd and ∆.",
                "The updating functions are given as π (k+1) d = πd w∈V ((1 − δw)p(c(w, q)|ˆΛd) + δwp(c(w, q)|C)) d∈C πd w∈V ((1 − δw)p(c(w, q)|ˆΛd) + δwp(c(w, q)|C)) and δ (k+1) w = d∈C πd δwp(c(w, q)|C)) (1 − δw)p(c(w, q)|ˆΛd) + δwp(c(w, q)|C)) As discussed in [29], we only need to run the EM algorithm for several iterations, thus the computational cost is relatively low.",
                "We again assume our vocabulary containing all query terms plus a pseudo non-query term.",
                "Note that the function does not give an explicit way of estimating the coefficient for the unseen non-query term.",
                "In our experiments, we set it to the average over δw of all query terms.",
                "With this flexibility, we expect Poisson language models could improve the retrieval performance, especially for verbose queries, where the query terms have various discriminative values.",
                "In Section 4, we use empirical experiments to prove this hypothesis. 3.3 Mixture Background Models Another flexibility is to explore different background (collection) models (i.e., p(·|U), or p(·|C)).",
                "One common assumption made in language modeling information retrieval is that the background model is a homogeneous model of the document models [28, 29].",
                "Similarly, we can also make the assumption that the collection model is a Poisson language model, with the rates λC,w = d∈C c(w,d) |C| .",
                "However, this assumption usually does not hold, since the collection is far more complex than a single document.",
                "Indeed, the collection usually consists of a mixture of documents with various genres, authors, and topics, etc.",
                "Treating the collection model as a mixture of document models, instead of a single pseudo-document model is more reasonable.",
                "Existing work of multinomial language modeling has already shown that a better modeling of background improves the retrieval performance, such as clusters [15, 10], neighbor documents [25], and aspects [8, 27].",
                "All the approaches can be easily adopted using Poisson language models.",
                "However, a common problem of these approaches is that they all require heavy computation to construct the background model.",
                "With Poisson language modeling, we show that it is possible to model the mixture background without paying for the heavy computational cost.",
                "Poisson Mixture [3] has been proposed to model a collection of documents, which can fit the data much better than a single Poisson.",
                "The basic idea is to assume that the collection is generated from a mixture of Poisson models, which has the general form of p(x = k|PM) = λ p(λ)p(x = k|λ)dλ p(·|λ) is a single Poisson model and p(λ) is an arbitrary probability density function.",
                "There are three well known Poisson mixtures [3]: 2-Poisson, Negative Binomial, and the Katzs K-Mixture [9].",
                "Note that the 2-Poisson model has actually been explored in probabilistic retrieval models, which led to the well-known BM25 formula [22].",
                "All these mixtures have closed forms, and can be estimated from the collection of documents efficiently.",
                "This is an advantage over the multinomial mixture models, such as PLSI [8] and LDA [1], for retrieval.",
                "For example, the probability density function of Katzs K-Mixture is given as p(c(w) = k|αw, βw) = (1 − αw)ηk,0 + αw βw + 1 ( βw βw + 1 )k where ηk,0 = 1 when k = 0, and 0 otherwise.",
                "With the observation of a collection of documents, αw and βw can be estimated as βw = cf(w) − df(w) df(w) and αw = cf(w) Nβw where cf(w) and df(w) are the collection frequency and document frequency of w, and N is the number of documents in the collection.",
                "To account for the different document lengths, we assume that βw is a reasonable estimation for generating a document of the average length, and use β = βw avdl |q| to generate the query.",
                "This Poisson mixture model can be easily used to replace P(·|C) in the retrieval functions 3 and 4. 3.4 Other Possible Flexibilities In addition to term dependent smoothing and efficient mixture background, a Poisson language model has also some other potential advantages.",
                "For example, in Section 2, we see that Formula 2 introduces a component which does document length penalization.",
                "Intuitively, when the document has more unique words, it will be penalized more.",
                "On the other hand, if a document is exactly n copies of another document, it would not get over penalized.",
                "This feature is desirable and not achieved with the Dirichlet model [5].",
                "Potentially, this component could penalize a document according to what types of terms it contains.",
                "With term specific settings of δ, we could get even more flexibility for document length normalization.",
                "Pseudo-feedback is yet another interesting direction where the Poission model might be able to show its advantage.",
                "With model-based feedback, we could again relax the combination coefficients of the feedback model and the background model, and allow different terms to contribute differently to the feedback model.",
                "We could also utilize the relevant documents to learn better per-term smoothing coefficients. 4.",
                "EVALUATION In Section 3, we analytically compared the Poisson language models and multinomial language models from the perspective of query generation and retrieval.",
                "In this section, we compare these two families of models empirically.",
                "Experiment results show that the Poisson model with perterm smoothing outperforms multinomial model, and the performance can be further improved with two-stage smoothing.",
                "Using Poisson mixture as background model also improves the retrieval performance. 4.1 Datasets Since retrieval performance could significantly vary from one test collection to another, and from one query to another, we select four representative TREC test collections: AP, Trec7, Trec8, and Wt2g(Web).",
                "To cover different types of queries, we follow [28, 5], and construct short-keyword (SK, keyword title), short-verbose (SV, one sentence description), and long-verbose (LV, multiple sentences) queries.",
                "The documents are stemmed with the Porters stemmer, and we do not remove any stop word.",
                "For each parameter, we vary its value to cover a reasonably wide range. 4.2 Comparison to Multinomial We compare the performance of the Poisson retrieval models and multinomial retrieval models using interpolation (JelinekMercer, JM) smoothing and Bayesian smoothing with conjugate priors.",
                "Table 1 shows that the two JM-smoothed models perform similarly on all data sets.",
                "Since the Dirichlet Smoothing for multinomial language model and the Gamma Smoothing for Poisson language model lead to the same retrieval formula, the performance of these two models are jointly presented.",
                "We see that Dirichlet/Gamma smoothing methods outperform both Jelinek-Mercer smoothing methods.",
                "The parameter sensitivity curves for two Jelinek-Mercer 0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1 0 0.05 0.1 0.15 0.2 0.25 0.3 Dataset: Trec8 Parameter: δ AveragePrecision JM−Multinomial: LV JM−Multinomial: SV JM−Multinomial: SK JM−Poisson: SK JM−Poisson: SV JM−Poisson: LV Figure 1: Poisson and multinomial performs similarly with Jelinek-Mercer smoothing smoothing methods are shown in Figure 1.",
                "Clearly, these two methods perform similarly either in terms of optimality Data Query JM-Multinomial JM-Poisson Dirichlet/Gamma Per-term 2-Stage Poisson MAP InitPr Pr@5d MAP InitPr Pr@5d MAP InitPr Pr@5d MAP InitPr Pr@5d AP88-89 SK 0.203 0.585 0.356 0.203 0.585 0.358 0.224 0.629 0.393 0.226 0.630 0.396 SV 0.187 0.580 0.361 0.183 0.571 0.345 0.204 0.613 0.387 0.217* 0.603 0.390 LV 0.283 0.716 0.480 0.271 0.692 0.470 0.291 0.710 0.496 0.304* 0.695 0.510 Trec7 SK 0.167 0.635 0.400 0.168 0.635 0.404 0.186 0.687 0.428 0.185 0.646 0.436 SV 0.174 0.655 0.432 0.176 0.653 0.432 0.182 0.666 0.432 0.196* 0.660 0.440 LV 0.223 0.730 0.496 0.215 0.766 0.488 0.224 0.748 0.52 0.236* 0.738 0.512 Trec8 SK 0.239 0.621 0.440 0.239 0.621 0.436 0.257 0.718 0.496 0.256 0.704 0.468 SV 0.231 0.686 0.448 0.234 0.702 0.456 0.228 0.691 0.456 0.246* 0.692 0.476 LV 0.265 0.796 0.548 0.261 0.757 0.520 0.260 0.741 0.492 0.274* 0.766 0.508 Web SK 0.250 0.616 0.380 0.250 0.616 0.380 0.302 0.767 0.468 0.307 0.739 0.468 SV 0.214 0.611 0.392 0.217 0.609 0.384 0.273 0.693 0.508 0.292* 0.703 0.480 LV 0.266 0.790 0.464 0.259 0.776 0.452 0.283 0.756 0.496 0.311* 0.759 0.488 Table 1: Performance comparison between Poisson and Multinomial retrieval models: basic models perform comparably; term dependent two-stage smoothing significantly improves Poisson An asterisk (*) indicates that the difference between the performance of the term dependent two-stage smoothing and that of the Dirichlet/Gamma single smoothing is statistically significant according to the Wilcoxon signed rank test at the level of 0.05. or sensitivity.",
                "This similarity of performance is expected as we discussed in Section 3.1.",
                "Although the Poisson model and multinomial model are similar in terms of the basic model and/or with simple smoothing methods, the Poisson model has great potential and flexibility to be further improved.",
                "As shown in the rightmost column of Table 1, term dependent two-stage Poisson model consistently outperforms the basic smoothing models, especially for verbose queries.",
                "This model is given in Formula 4, with a Gamma smoothing for the document model p(·|d), and δw, which is term dependent.",
                "The parameter µ of the first stage Gamma smoothing is empirically tuned.",
                "The combination coefficients (i.e., ∆), are estimated with the EM algorithm in Section 3.2.",
                "The parameter sensitivity curves for Dirichlet/Gamma and the per-term two-stage smoothing model are plotted in Figure 2.",
                "The per-term two-stage smoothing method is less sensitive to the parameter µ than Dirichlet/Gamma, and yields better optimal performance. 0 1000 2000 3000 4000 5000 6000 7000 8000 9000 10000 0.1 0.12 0.14 0.16 0.18 0.2 0.22 Dataset: AP; Query Type: SV Parameter: µ AveragePrecision Dirichlet/Gamma Smoothing Term Dependent 2−Stage Figure 2: Term dependent two-stage smoothing of Poisson outperforms Dirichlet/Gamma In the following subsections, we conduct experiments to demonstrate how the flexibility of the Poisson model could be utilized to achieve better performance, which we cannot achieve with multinomial language models. 4.3 Term Dependent Smoothing To test the effectiveness of the term dependent smoothing, we conduct the following two experiments.",
                "In the first experiment, we relax the constant coefficient in the simple Jelinek-Mercer smoothing formula (i.e., Formula 3), and use the EM algorithm proposed in Section 3.2 to find a δw for each unique term.",
                "Since we are using the EM algorithm to iteratively estimate the parameters, we usually do not want the probability of p(·|d) to be zero.",
                "We then use a simple Laplace method to slightly smooth the document model before it goes into the EM iterations.",
                "The documents are then still scored with Formula 3, but using learnt δw.",
                "The results are labeled with JM+L. in Table 2.",
                "Data Q JM JM JM+L. 2-Stage 2-Stage (MAP) PT: No Yes Yes No Yes AP SK 0.203 0.204 0.206 0.223 0.226* SV 0.183 0.189 0.214* 0.204 0.217* Trec7 SK 0.168 0.171 0.174 0.186 0.185 SV 0.176 0.147 0.198* 0.194 0.196 Trec8 SK 0.239 0.240 0.227* 0.257 0.256 SV 0.234 0.223 0.249* 0.242 0.246* Web SK 0.250 0.236 0.220* 0.291 0.307* SV 0.217 0.232 0.261* 0.273 0.292* Table 2: Term dependent smoothing improves retrieval performance An asterisk (*) in Column 3 indicates that the difference between the JM+L. method and JM method is statistically significant; an asterisk (*) in Column 5 means that the difference between term dependent two-stage method and query dependent two-stage method is statistically significant; PT stands for per-term.",
                "With term dependent coefficients, the performance of the Jelinek-Mercer Poisson model is improved in most cases.",
                "However, in some cases (e.g., Trec7/SV), it performs poorly.",
                "This might be caused by the problem of EM estimation with unsmoothed document models.",
                "Once non-zero probability is assigned to all the terms before entering the EM iteration, the performance on verbose queries can be improved significantly.",
                "This indicates that there is still room to find better methods to estimate δw.",
                "Please note that neither the perterm JM method nor the JM+L. method has a parameter to tune.",
                "As shown in Table 1, the term dependent two-stage smoothing can significantly improve retrieval performance.",
                "To understand whether the improvement is contributed by the term dependent smoothing or the two-stage smoothing framework, we design another experiment to compare the perterm two-stage smoothing with the two-stage smoothing method proposed in [29].",
                "Their method managed to find coefficients specific to the query, thus a verbose query would use a higher δ.",
                "However, since their model is based on multinomial language modeling, they could not get per-term coefficients.",
                "We adopt their method to the Poisson two-stage smoothing, and also estimate a per-query coefficient for all the terms.",
                "We compare the performance of such a model with the per-term two-stage smoothing model, and present the results in the right two columns in Table 2.",
                "Again, we see that the per-term two-stage smoothing outperforms the per-query two-stage smoothing, especially for verbose queries.",
                "The improvement is not as large as how the perterm smoothing method improves over Dirichlet/Gamma.",
                "This is expected, since the per-query smoothing has already addressed the query discrimination problem to some extent.",
                "This experiment shows that even if the smoothing is already per-query, making it per-term is still beneficial.",
                "In brief, the per-term smoothing improved the retrieval performance of both one-stage and two-stage smoothing method. 4.4 Mixture Background Model In this section, we conduct experiments to examine the benefits of using a mixture background model without extra computational cost, which can not be achieved for multinomial models.",
                "Specifically, in retrieval formula 3, instead of using a single Poisson distribution to model the background p(·|C), we use Katzs K-Mixture model, which is essentially a mixture of Poisson distributions. p(·|C) can be computed efficiently with simple collection statistics, as discussed in Section 3.3.",
                "Data Query JM.",
                "Poisson JM.",
                "K-Mixture AP SK 0.203 0.204 SV 0.183 0.188* Trec-7 SK 0.168 0.169 SV 0.176 0.178* Trec-8 SK 0.239 0.239 SV 0.234 0.238* Web SK 0.250 0.250 SV 0.217 0.223* Table 3: K-Mixture background model improves retrieval performance The performance of the JM retrieval model with single Poisson background and with Katzs K-Mixture background model is compared in Table 3.",
                "Clearly, using K-Mixture to model the background model outperforms the single Poisson background model in most cases, especially for verbose queries where the improvement is statistically significant.",
                "Figure 3 shows that the performance changes over different parameters for short verbose queries.",
                "The model using K-Mixture background is less sensitive than the one using single Poisson background.",
                "Given that this type of mixture 0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1 0 0.05 0.1 0.15 0.2 0.25 Data: Trec8; Query: SV Parameter: δ AveragePrecision Poisson Background K−Mixture Background Figure 3: K-Mixture background model deviates the sensitivity of verbose queries background model does not require any extra computation cost, it would be interesting to study whether using other mixture Poisson models, such as 2-Poisson and negative Binomial, could help the performance. 5.",
                "RELATED WORK To the best of our knowledge, there has been no study of query generation models based on Poisson distribution.",
                "Language models have been shown to be effective for many retrieval tasks [21, 28, 14, 4].",
                "The most popular and fundamental one is the query-generation language model [21, 13].",
                "All existing query generation language models are based on either <br>multinomial distribution</br> [19, 6, 28, 13] or multivariate Bernoulli distribution [21, 17, 18].",
                "We introduce a new family of language models, based on Poisson distribution.",
                "Poisson distribution has been previously studied in the document generation models [16, 22, 3, 24], leading to the development of one of the most effective retrieval formula BM25 [23]. [24] studies the parallel derivation of three different retrieval models which is related to our comparison of Poisson and multinomial.",
                "However, the Poisson model in their paper is still under the document generation framework, and also does not account for the document length variation. [26] introduces a way to empirically search for an exponential model for the documents.",
                "Poisson mixtures [3] such as 2-Poisson [22], Negative multinomial, and Katzs KMixture [9] has shown to be effective to model and retrieve documents.",
                "Once again, none of this work explores Poisson distribution in the query generation framework.",
                "Language model smoothing [2, 28, 29] and background structures [15, 10, 25, 27] have been studied with multinomial language models. [7] analytically shows that term specific smoothing could be useful.",
                "We show that Poisson language model is natural to accommodate the per-term smoothing without heuristic twist of the semantics of a generative model, and is able to efficiently better model the mixture background, both analytically and empirically. 6.",
                "CONCLUSIONS We present a new family of query generation language models for retrieval based on Poisson distribution.",
                "We derive several smoothing methods for this family of models, including single-stage smoothing and two-stage smoothing.",
                "We compare the new models with the popular multinomial retrieval models both analytically and experimentally.",
                "Our analysis shows that while our new models and multinomial models are equivalent under some assumptions, they are generally different with some important differences.",
                "In particular, we show that Poisson has an advantage over multinomial in naturally accommodating per-term smoothing.",
                "We exploit this property to develop a new per-term smoothing algorithm for Poisson language models, which is shown to outperform term-independent smoothing for both Poisson and multinomial models.",
                "Furthermore, we show that a mixture background model for Poisson can be used to improve the performance and robustness over the standard Poisson background model.",
                "Our work opens up many interesting directions for further exploration in this new family of models.",
                "Further exploring the flexibilities over multinomial language models, such as length normalization and pseudo-feedback could be good future work.",
                "It is also appealing to find robust methods to learn the per-term smoothing coefficients without additional computation cost. 7.",
                "ACKNOWLEDGMENTS We thank the anonymous SIGIR 07 reviewers for their useful comments.",
                "This material is based in part upon work supported by the National Science Foundation under award numbers IIS-0347933 and 0425852. 8.",
                "REFERENCES [1] D. Blei, A. Ng, and M. Jordan.",
                "Latent dirichlet allocation.",
                "Journal of Machine Learning Research, 3:993-1022, 2003. [2] S. F. Chen and J. Goodman.",
                "An empirical study of smoothing techniques for language modeling.",
                "Technical Report TR-10-98, Harvard University, 1998. [3] K. Church and W. Gale.",
                "Poisson mixtures.",
                "Nat.",
                "Lang.",
                "Eng., 1(2):163-190, 1995. [4] W. B. Croft and J. Lafferty, editors.",
                "Language Modeling and Information Retrieval.",
                "Kluwer Academic Publishers, 2003. [5] H. Fang, T. Tao, and C. Zhai.",
                "A formal study of information retrieval heuristics.",
                "In Proceedings of the 27th annual international ACM SIGIR conference on Research and development in information retrieval, pages 49-56, 2004. [6] D. Hiemstra.",
                "Using Language Models for Information Retrieval.",
                "PhD thesis, University of Twente, Enschede, Netherlands, 2001. [7] D. Hiemstra.",
                "Term-specific smoothing for the language modeling approach to information retrieval: the importance of a query term.",
                "In Proceedings of the 25th annual international ACM SIGIR conference on Research and development in information retrieval, pages 35-41, 2002. [8] T. Hofmann.",
                "Probabilistic latent semantic indexing.",
                "In Proceedings of ACM SIGIR99, pages 50-57, 1999. [9] S. M. Katz.",
                "Distribution of content words and phrases in text and language modelling.",
                "Nat.",
                "Lang.",
                "Eng., 2(1):15-59, 1996. [10] O. Kurland and L. Lee.",
                "Corpus structure, language models, and ad-hoc information retrieval.",
                "In Proceedings of the 27th annual international ACM SIGIR conference on Research and development in information retrieval, pages 194-201, 2004. [11] J. Lafferty and C. Zhai.",
                "Document language models, query models, and risk minimization for information retrieval.",
                "In Proceedings of SIGIR01, pages 111-119, Sept 2001. [12] J. Lafferty and C. Zhai.",
                "Probabilistic IR models based on query and document generation.",
                "In Proceedings of the Language Modeling and IR workshop, pages 1-5, May 31 - June 1 2001. [13] J. Lafferty and C. Zhai.",
                "Probabilistic relevance models based on document and query generation.",
                "In W. B. Croft and J. Lafferty, editors, Language Modeling and Information Retrieval.",
                "Kluwer Academic Publishers, 2003. [14] V. Lavrenko and B. Croft.",
                "Relevance-based language models.",
                "In Proceedings of SIGIR01, pages 120-127, Sept 2001. [15] X. Liu and W. B. Croft.",
                "Cluster-based retrieval using language models.",
                "In Proceedings of the 27th annual international ACM SIGIR conference on Research and development in information retrieval, pages 186-193, 2004. [16] E. L. Margulis.",
                "Modelling documents with multiple poisson distributions.",
                "Inf.",
                "Process.",
                "Manage., 29(2):215-227, 1993. [17] A. McCallum and K. Nigam.",
                "A comparison of event models for naive bayes text classification.",
                "In Proceedings of AAAI-98 Workshop on Learning for Text Categorization, 1998. [18] D. Metzler, V. Lavrenko, and W. B. Croft.",
                "Formal multiple-bernoulli models for language modeling.",
                "In Proceedings of the 27th annual international ACM SIGIR conference on Research and development in information retrieval, pages 540-541, 2004. [19] D. H. Miller, T. Leek, and R. Schwartz.",
                "A hidden Markov model information retrieval system.",
                "In Proceedings of the 1999 ACM SIGIR Conference on Research and Development in Information Retrieval, pages 214-221, 1999. [20] A. Papoulis.",
                "Probability, random variables and stochastic processes.",
                "New York: McGraw-Hill, 1984, 2nd ed., 1984. [21] J. M. Ponte and W. B. Croft.",
                "A language modeling approach to information retrieval.",
                "In Proceedings of the 21st annual international ACM SIGIR conference on Research and development in information retrieval, pages 275-281, 1998. [22] S. Robertson and S. Walker.",
                "Some simple effective approximations to the 2-poisson model for probabilistic weighted retrieval.",
                "In Proceedings of SIGIR94, pages 232-241, 1994. [23] S. E. Robertson, S. Walker, S. Jones, M. M.Hancock-Beaulieu, and M. Gatford.",
                "Okapi at TREC-3.",
                "In D. K. Harman, editor, The Third Text REtrieval Conference (TREC-3), pages 109-126, 1995. [24] T. Roelleke and J. Wang.",
                "A parallel derivation of probabilistic information retrieval models.",
                "In Proceedings of the 29th annual international ACM SIGIR conference on Research and development in information retrieval, pages 107-114, 2006. [25] T. Tao, X. Wang, Q. Mei, and C. Zhai.",
                "Language model information retrieval with document expansion.",
                "In Proceedings of HLT/NAACL 2006, pages 407-414, 2006. [26] J. Teevan and D. R. Karger.",
                "Empirical development of an exponential probabilistic model for text retrieval: using textual analysis to build a better model.",
                "In Proceedings of the 26th annual international ACM SIGIR conference on Research and development in informaion retrieval, pages 18-25, 2003. [27] X. Wei and W. B. Croft.",
                "Lda-based document models for ad-hoc retrieval.",
                "In Proceedings of the 29th annual international ACM SIGIR conference on Research and development in information retrieval, pages 178-185, 2006. [28] C. Zhai and J. Lafferty.",
                "A study of smoothing methods for language models applied to ad-hoc information retrieval.",
                "In Proceedings of ACM SIGIR01, pages 334-342, Sept 2001. [29] C. Zhai and J. Lafferty.",
                "Two-stage language models for information retrieval.",
                "In Proceedings of ACM SIGIR02, pages 49-56, Aug 2002."
            ],
            "original_annotated_samples": [],
            "translated_annotated_samples": [
                "La mayoría de los modelos existentes se basan en \"distribución multinomial\" y obtendrían documentos basados en la probabilidad de consulta calculada en función de un modelo probabilístico de generación de consultas.distribución multinomial",
                "Prácticamente todos los modelos de lenguaje de generación de consultas existentes se basan en \"distribución multinomial\" [19, 6, 28] o distribución multivariada de Bernoulli [21, 18].distribución multinomial",
                "La \"distribución multinomial\" es especialmente popular y también se muestra bastante efectiva.distribución multinomial",
                "El uso intensivo de la \"distribución multinomial\" se debe en parte al hecho de que se ha utilizado con éxito en el reconocimiento de voz, donde la \"distribución multinomial\" es una opción natural para modelar la ocurrencia de una palabra particular en una posición particular en el texto.distribución multinomial",
                "En comparación con Bernoulli multivariado, la \"distribución multinomial\" tiene la ventaja de poder modelar la frecuencia de los términos en la consulta;Por el contrario, Bernoulli multivariado solo modela la presencia y la ausencia de términos de consulta, por lo que no pueden capturar diferentes frecuencias de términos de consulta.distribución multinomial",
                "Sin embargo, Bernoulli multivariado también tiene una ventaja potencial sobre multinomial desde el punto de vista de la recuperación: en una \"distribución multinomial\", las probabilidades de todos los términos deben sumar a 1, lo que hace que sea difícil acomodar el suavizado por término, mientras que en un Bernoulli multivariado, las probabilidades de presencia de diferentes términos son completamente independientes entre sí, acomodando fácilmente el suavizado y la ponderación por término.distribución multinomial",
                "De hecho, de manera similar a la \"distribución multinomial\", la distribución de Poisson modela las frecuencias de términos, pero sin la restricción de que todas las probabilidades del término deben sumar a 1, y similar a Bernoulli multivariado, modela cada término de forma independiente, por lo tanto, puede acomodar fácilmente por período por período por término.suavizado.distribución multinomial",
                "En la mayoría de los trabajos existentes [12, 6, 28, 29], las personas suponen que cada palabra de consulta se muestrean independientemente de una \"distribución multinomial\".distribución multinomial",
                "Todos los modelos de lenguaje de generación de consultas existentes se basan en \"distribución multinomial\" [19, 6, 28, 13] o distribución multivariada de Bernoulli [21, 17, 18].distribución multinomial"
            ],
            "translated_text": "",
            "candidates": [],
            "error": []
        },
        "query generation probabilistic model": {
            "translated_key": "",
            "is_in_text": true,
            "original_annotated_sentences": [
                "A Study of Poisson Query Generation Model for Information Retrieval Qiaozhu Mei, Hui Fang, Chengxiang Zhai Department of Computer Science University of Illinois at Urbana-Champaign Urbana,IL 61801 {qmei2,hfang,czhai}@uiuc.edu ABSTRACT Many variants of language models have been proposed for information retrieval.",
                "Most existing models are based on multinomial distribution and would score documents based on query likelihood computed based on a <br>query generation probabilistic model</br>.",
                "In this paper, we propose and study a new family of query generation models based on Poisson distribution.",
                "We show that while in their simplest forms, the new family of models and the existing multinomial models are equivalent, they behave differently for many smoothing methods.",
                "We show that the Poisson model has several advantages over the multinomial model, including naturally accommodating per-term smoothing and allowing for more accurate background modeling.",
                "We present several variants of the new model corresponding to different smoothing methods, and evaluate them on four representative TREC test collections.",
                "The results show that while their basic models perform comparably, the Poisson model can outperform multinomial model with per-term smoothing.",
                "The performance can be further improved with two-stage smoothing.",
                "Categories and Subject Descriptors: H.3.3 [Information Search and Retrieval]: Retrieval Models General Terms: Algorithms 1.",
                "INTRODUCTION As a new type of probabilistic retrieval models, language models have been shown to be effective for many retrieval tasks [21, 28, 14, 4].",
                "Among many variants of language models proposed, the most popular and fundamental one is the query-generation language model [21, 13], which leads to the query-likelihood scoring method for ranking documents.",
                "In such a model, given a query q and a document d, we compute the likelihood of generating query q with a model estimated based on document d, i.e., the conditional probability p(q|d).",
                "We can then rank documents based on the likelihood of generating the query.",
                "Virtually all the existing query generation language models are based on either multinomial distribution [19, 6, 28] or multivariate Bernoulli distribution [21, 18].",
                "The multinomial distribution is especially popular and also shown to be quite effective.",
                "The heavy use of multinomial distribution is partly due to the fact that it has been successfully used in speech recognition, where multinomial distribution is a natural choice for modeling the occurrence of a particular word in a particular position in text.",
                "Compared with multivariate Bernoulli, multinomial distribution has the advantage of being able to model the frequency of terms in the query; in contrast, multivariate Bernoulli only models the presence and absence of query terms, thus cannot capture different frequencies of query terms.",
                "However, multivariate Bernoulli also has one potential advantage over multinomial from the viewpoint of retrieval: in a multinomial distribution, the probabilities of all the terms must sum to 1, making it hard to accommodate per-term smoothing, while in a multivariate Bernoulli, the presence probabilities of different terms are completely independent of each other, easily accommodating per-term smoothing and weighting.",
                "Note that term absence is also indirectly captured in a multinomial model through the constraint that all the term probabilities must sum to 1.",
                "In this paper, we propose and study a new family of query generation models based on the Poisson distribution.",
                "In this new family of models, we model the frequency of each term independently with a Poisson distribution.",
                "To score a document, we would first estimate a multivariate Poisson model based on the document, and then score it based on the likelihood of the query given by the estimated Poisson model.",
                "In some sense, the Poisson model combines the advantage of multinomial in modeling term frequency and the advantage of the multivariate Bernoulli in accommodating per-term smoothing.",
                "Indeed, similar to the multinomial distribution, the Poisson distribution models term frequencies, but without the constraint that all the term probabilities must sum to 1, and similar to multivariate Bernoulli, it models each term independently, thus can easily accommodate per-term smoothing.",
                "As in the existing work on multinomial language models, smoothing is critical for this new family of models.",
                "We derive several smoothing methods for Poisson model in parallel to those used for multinomial distributions, and compare the corresponding retrieval models with those based on multinomial distributions.",
                "We find that while with some smoothing methods, the new model and the multinomial model lead to exactly the same formula, with some other smoothing methods they diverge, and the Poisson model brings in more flexibility for smoothing.",
                "In particular, a key difference is that the Poisson model can naturally accommodate perterm smoothing, which is hard to achieve with a multinomial model without heuristic twist of the semantics of a generative model.",
                "We exploit this potential advantage to develop a new term-dependent smoothing algorithm for Poisson model and show that this new smoothing algorithm can improve performance over term-independent smoothing algorithms using either Poisson or multinomial model.",
                "This advantage is seen for both one-stage and two-stage smoothing.",
                "Another potential advantage of the Poisson model is that its corresponding background model for smoothing can be improved through using a mixture model that has a closed form formula.",
                "This new background model is shown to outperform the standard background model and reduce the sensitivity of retrieval performance to the smoothing parameter.",
                "The rest of the paper is organized as follows.",
                "In Section 2, we introduce the new family of query generation models with Poisson distribution, and present various smoothing methods which lead to different retrieval functions.",
                "In Section 3, we analytically compare the Poisson language model with the multinomial language model, from the perspective of retrieval.",
                "We then design empirical experiments to compare the two families of language models in Section 4.",
                "We discuss the related work in 5 and conclude in 6. 2.",
                "QUERY GENERATION WITH POISSON PROCESS In the query generation framework, a basic assumption is that a query is generated with a model estimated based on a document.",
                "In most existing work [12, 6, 28, 29], people assume that each query word is sampled independently from a multinomial distribution.",
                "Alternatively, we assume that a query is generated by sampling the frequency of words from a series of independent Poisson processes [20]. 2.1 The Generation Process Let V = {w1, ..., wn} be a vocabulary set.",
                "Let w be a piece of text composed by an author and c(w1), ..., c(wn) be a frequency vector representing w, where c(wi, w) is the frequency count of term wi in text w. In retrieval, w could be either a query or a document.",
                "We consider the frequency counts of the n unique terms in w as n different types of events, sampled from n independent homogeneous Poisson processes, respectively.",
                "Suppose t is the time period during which the author composed the text.",
                "With a homogeneous Poisson process, the frequency count of each event, i.e., the number of occurrences of wi, follows a Poisson distribution with associated parameter λit, where λi is a rate parameter characterizing the expected number of wi in a unit time.",
                "The probability density function of such a Poisson Distribution is given by P(c(wi, w) = k|λit) = e−λit (λit)k k!",
                "Without losing generality, we set t to the length of the text w (people write one word in a unit time), i.e., t = |w|.",
                "With n such independent Poisson processes, each explaining the generation of one term in the vocabulary, the likelihood of w to be generated from such Poisson processes can be written as p(w|Λ) = n i=1 p(c(wi, w)|Λ) = n i=1 e−λi·|w| (λi · |w|)c(wi,w) c(wi, w)! where Λ = {λ1, ..., λn} and |w| = n i=1 c(wi, w).",
                "We refer to these n independent Poisson processes with parameter Λ as a Poisson Language Model.",
                "Let D = {d1, ..., dm} be an observed set of document samples generated from the Poisson process above.",
                "The maximum likelihood estimate (MLE) of λi is ˆλi = d∈D c(wi, d) d∈D w ∈V c(w , d) Note that this MLE is different from the MLE for the Poisson distribution without considering the document lengths, which appears in [22, 24].",
                "Given a document d, we may estimate a Poisson language model Λd using d as a sample.",
                "The likelihood that a query q is generated from the document language model Λd can be written as p(q|d) = w∈V p(c(w, q)|Λd) (1) This representation is clearly different from the multinomial query generation model as (1) the likelihood includes all the terms in the vocabulary V , instead of only those appearing in q, and (2) instead of the appearance of terms, the event space of this model is the frequencies of each term.",
                "In practice, we have the flexibility to choose the vocabulary V .",
                "In one extreme, we can use the vocabulary of the whole collection.",
                "However, this may bring in noise and considerable computational cost.",
                "In the other extreme, we may focus on the terms in the query and ignore other terms, but some useful information may be lost by ignoring the nonquery terms.",
                "As a compromise, we may conflate all the non-query terms as one single pseudo term.",
                "In other words, we may assume that there is exactly one non-query term in the vocabulary for each query.",
                "In our experiments, we adopt this pseudo non-query term strategy.",
                "A document can be scored with the likelihood in Equation 1.",
                "However, if a query term is unseen in the document, the MLE of the Poisson distribution would assign zero probability to the term, causing the probability of the query to be zero.",
                "As in existing language modeling approaches, the main challenge of constructing a reasonable retrieval model is to find a smoothed language model for p(·|d). 2.2 Smoothing in Poisson Retrieval Model In general, we want to assign non-zero rates for the query terms that are not seen in document d. Many smoothing methods have been proposed for multinomial language models[2, 28, 29].",
                "In general, we have to discount the probabilities of some words seen in the text to leave some extra probability mass to assign to the unseen words.",
                "In Poisson language models, however, we do not have the same constraint as in a multinomial model (i.e., w∈V p(w|d) = 1).",
                "Thus we do not have to discount the probability of seen words in order to give a non-zero rate to an unseen word.",
                "Instead, we only need to guarantee that k=0,1,2,... p(c(w, d) = k|d) = 1.",
                "In this section, we introduce three different strategies to smooth a Poisson language model, and show how they lead to different retrieval functions. 2.2.1 Bayesian Smoothing using Gamma Prior Following the risk minimization framework in [11], we assume that a document is generated by the arrival of terms in a time period of |d| according to the document language model, which essentially consists of a vector of Poisson rates for each term, i.e., Λd = λd,1, ..., λd,|V | .",
                "A document is assumed to be generated from a potentially different model.",
                "Given a particular document d, we want to estimate Λd.",
                "The rate of a term is estimated independently of other terms.",
                "We use Bayesian estimation with the following Gamma prior, which has two parameters, α and β: Gamma(λ|α, β) = βα Γ(α) λα−1 e−βλ For each term w, the parameters αw and βw are chosen to be αw = µ ∗ λC,w and βw = µ, where µ is a parameter and λC,w is the rate of w estimated from some background language model, usually the collection language model.",
                "The posterior distribution of Λd is given by p(Λd|d, C) ∝ w∈V e−λw(|d|+µ) λ c(w,d)+µλC,w−1 w which is a product of |V | Gamma distributions with parameters c(w, d) + µλC,w and |d| + µ for each word w. Given that the Gamma mean is α β , we have ˆλd,w = λd,w λd,wp(λd,w|d, C)dλd,w = c(w, d) + µλC,w |d| + µ This is precisely the smoothed estimate of multinomial language model with Dirichlet prior [28]. 2.2.2 Interpolation (Jelinek-Mercer) Smoothing Another straightforward method is to decompose the query generation model as a mixture of two component models.",
                "One is the document language model estimated with maximum likelihood estimator, and the other is a model estimated from the collection background, p(·|C), which assigns non-zero rate to w. For example, we may use an interpolation coefficient between 0 and 1 (i.e., δ ∈ [0, 1]).",
                "With this simple interpolation, we can score a document with Score(d, q) = w∈V log((1 − δ)p(c(w, q)|d) + δp(c(w, q)|C)) (2) Using the maximum likelihood estimator for p(·|d), we have λd,w = c(w,d) |d| , thus Equation 2 becomes Score(d, q) ∝ w∈d∩q [log(1 + 1 − δ δ e−λd,w|q| (λd,w|q|)c(w,q) c(w, q)! · p(c(w, q)|C) ) − log (1 − δ)e−λd,w|q| + δp(c(w, q) = 0|C) 1 − δ + δp(c(w, q) = 0|C) ] + w∈d log (1 − δ)e−λd,w|q| + δp(c(w, q) = 0|C) 1 − δ + δp(c(w, q) = 0|C) We can also use a Poisson language model for p(·|C), or use some other frequency-based models.",
                "In the retrieval formula above, the first summation can be computed efficiently.",
                "The second summation can be actually treated as a document prior, which penalizes long documents.",
                "As the second summation is difficult to compute efficiently, we conflate all non-query terms as one pseudo non-queryterm, denoted as N. Using the pseudo-term formulation and a Poisson collection model, we can rewrite the retrieval formula as Score(d, q) ∝ w∈d∩q log(1 + 1 − δ δ e−λd,w (λd,w|q|)c(w,q) e−λd,C |q| (λd,C )c(w,q) ) + log (1 − δ)e−λd,N |q| + δe−λC,N |q| 1 − δ + δe−λC,N |q| (3) where λd,N = |d|− w∈q c(w,d) |d| and λC,N = |C|− w∈q c(w,C) |C| . 2.2.3 Two-Stage Smoothing As discussed in [29], smoothing plays two roles in retrieval: (1) to improve the estimation of the document language model, and (2) to explain the common terms in the query.",
                "In order to distinguish the content and non-discriminative words in a query, we follow [29] and assume that a query is generated by sampling from a two-component mixture of Poisson language models, with one component being the document model Λd and the other being a query background language model p(·|U). p(·|U) models the typical term frequencies in the users queries.",
                "We may then score each document with the query likelihood computed using the following two-stage smoothing model: p(c(w, q)|Λd, U) = (1 − δ)p(c(w, q)|Λd) + δp(c(w, q)|U) (4) where δ is a parameter, roughly indicating the amount of noise in q.",
                "This looks similar to the interpolation smoothing, except that p(·|Λd) now should be a smoothed language model, instead of the one estimated with MLE.",
                "With no prior knowledge on p(·|U), we could set it to p(·|C).",
                "Any smoothing methods for the document language model can be used to estimate p(·|d) such as the Gamma smoothing as discussed in Section 2.2.1.",
                "The empirical study of the smoothing methods is presented in Section 4. 3.",
                "ANALYSIS OF POISSON LANGUAGE MODEL From the previous section, we notice that the Poisson language model has a strong connection to the multinomial language model.",
                "This is expected since they both belong to the exponential family [26].",
                "However, there are many differences when these two families of models are applied with different smoothing methods.",
                "From the perspective of retrieval, will these two language models perform equivalently?",
                "If not, which model provides more benefits to retrieval, or provides flexibility which could lead to potential benefits?",
                "In this section, we analytically discuss the retrieval features of the Poisson language models, by comparing their behavior with that of the multinomial language models. 3.1 The Equivalence of Basic Models Let us begin with the assumption that all the query terms appear in every document.",
                "Under this assumption, no smoothing is needed.",
                "A document can be scored by the log likelihood of the query with the maximum likelihood estimate: Score(d, q) = w∈V log e−λd,w|q| (λd,w|q|)c(w,q) c(w, q)! (5) Using the MLE, we have λd,w = c(w,d) w∈V c(w,d) .",
                "Thus Score(d, q) ∝ c(w,q)>0 c(w, q) log c(w, d) w∈V c(w, d) This is exactly the log likelihood of the query if the document language model is a multinomial with maximum likelihood estimate.",
                "Indeed, even with Gamma smoothing, when plugging λd,w = c(w,d)+µλC,w |d|+µ and λC,w = c(w,C) |C| into Equation 5, it is easy to show that Score(d, q) ∝ w∈q∩d c(w, q) log(1 + c(w, d) µ · c(w,C) |C| ) + |q| log µ |d| + µ (6) which is exactly the Dirichlet retrieval formula in [28].",
                "Note that this equivalence holds only when the document length variation is modeled with Poisson process.",
                "This derivation indicates the equivalence of the basic Poisson and multinomial language models for retrieval.",
                "With other smoothing strategies, however, the two models would be different.",
                "Nevertheless, with this equivalence in basic models, we could expect that the Poisson language model performs comparably to the multinomial language model in retrieval, if only simple smoothing is explored.",
                "Based on this equivalence analysis, one may ask, why we should pursue the Poisson language model.",
                "In the following sections, we show that despite the equivalence in their basic models, the Poisson language model brings in extra flexibility for exploring advanced techniques on various retrieval features, which could not be achieved with multinomial language models. 3.2 Term Dependent Smoothing One flexibility of the Poisson language model is that it provides a natural framework to accommodate term dependent (per-term) smoothing.",
                "Existing work on language model smoothing has already shown that different types of queries should be smoothed differently according to how discriminative the query terms are. [7] also predicted that different terms should have a different smoothing weights.",
                "With multinomial query generation models, people usually use a single smoothing coefficient to control the combination of the document model and the background model [28, 29].",
                "This parameter can be made specific for different queries, but always has to be a constant for all the terms.",
                "This is mandatory since a multinomial language model has the constraint that w∈V p(w|d) = 1.",
                "However, from retrieval perspective, different terms may need to be smoothed differently even if they are in the same query.",
                "For example, a non-discriminative term (e.g., the, is) is expected to be explained more with the background model, while a content term (e.g., retrieval, bush) in the query should be explained with the document model.",
                "Therefore, a better way of smoothing would be to set the interpolation coefficient (i.e., δ in Formula 2 and Formula 3) specifically for each term.",
                "Since the Poisson language model does not have the sum-to-one constraint across terms, it can easily accommodate per-term smoothing without needing to heuristically twist the semantics of a generative model as in the case of multinomial language models.",
                "Below we present a possible way to explore term dependent smoothing with Poisson language models.",
                "Essentially, we want to use a term-specific smoothing coefficient δ in the linear combination, denoted as δw.",
                "This coefficient should intuitively be larger if w is a common word and smaller if it is a content word.",
                "The key problem is to find a method to assign reasonable values to δw.",
                "Empirical tuning is infeasible for so many parameters.",
                "We may instead estimate the parameters ∆ = {δ1, ..., δ|V |} by maximizing the likelihood of the query given the mixture model of p(q|ΛQ) and p(q|U), where ΛQ is the true query model to generate the query and p(q|U) is a query background model as discussed in Section 2.2.3.",
                "With the model p(q|ΛQ) hidden, the query likelihood is p(q|∆, U) = ΛQ w∈V ((1 − δw)p(c(w, q)|ΛQ) + δwp(c(w, q)|U))P(ΛQ|U)dΛQ If we have relevant documents for each query, we can approximate the query model space with the language models of all the relevant documents.",
                "Without relevant documents, we opt to approximate the query model space with the models of all the documents in the collection.",
                "Setting p(·|U) as p(·|C), the query likelihood becomes p(q|∆, U) = d∈C πd w∈V ((1−δw)p(c(w, q)|ˆΛd)+δwp(c(w, q)|C)) where πd = p(ˆΛd|U). p(·|ˆΛd) is an estimated Poisson language model for document d. If we have prior knowledge on p(ˆΛd|U), such as which documents are relevant to the query, we can set πd accordingly, because what we want is to find ∆ that can maximize the likelihood of the query given relevant documents.",
                "Without this prior knowledge, we can leave πd as free parameters, and use the EM algorithm to estimate πd and ∆.",
                "The updating functions are given as π (k+1) d = πd w∈V ((1 − δw)p(c(w, q)|ˆΛd) + δwp(c(w, q)|C)) d∈C πd w∈V ((1 − δw)p(c(w, q)|ˆΛd) + δwp(c(w, q)|C)) and δ (k+1) w = d∈C πd δwp(c(w, q)|C)) (1 − δw)p(c(w, q)|ˆΛd) + δwp(c(w, q)|C)) As discussed in [29], we only need to run the EM algorithm for several iterations, thus the computational cost is relatively low.",
                "We again assume our vocabulary containing all query terms plus a pseudo non-query term.",
                "Note that the function does not give an explicit way of estimating the coefficient for the unseen non-query term.",
                "In our experiments, we set it to the average over δw of all query terms.",
                "With this flexibility, we expect Poisson language models could improve the retrieval performance, especially for verbose queries, where the query terms have various discriminative values.",
                "In Section 4, we use empirical experiments to prove this hypothesis. 3.3 Mixture Background Models Another flexibility is to explore different background (collection) models (i.e., p(·|U), or p(·|C)).",
                "One common assumption made in language modeling information retrieval is that the background model is a homogeneous model of the document models [28, 29].",
                "Similarly, we can also make the assumption that the collection model is a Poisson language model, with the rates λC,w = d∈C c(w,d) |C| .",
                "However, this assumption usually does not hold, since the collection is far more complex than a single document.",
                "Indeed, the collection usually consists of a mixture of documents with various genres, authors, and topics, etc.",
                "Treating the collection model as a mixture of document models, instead of a single pseudo-document model is more reasonable.",
                "Existing work of multinomial language modeling has already shown that a better modeling of background improves the retrieval performance, such as clusters [15, 10], neighbor documents [25], and aspects [8, 27].",
                "All the approaches can be easily adopted using Poisson language models.",
                "However, a common problem of these approaches is that they all require heavy computation to construct the background model.",
                "With Poisson language modeling, we show that it is possible to model the mixture background without paying for the heavy computational cost.",
                "Poisson Mixture [3] has been proposed to model a collection of documents, which can fit the data much better than a single Poisson.",
                "The basic idea is to assume that the collection is generated from a mixture of Poisson models, which has the general form of p(x = k|PM) = λ p(λ)p(x = k|λ)dλ p(·|λ) is a single Poisson model and p(λ) is an arbitrary probability density function.",
                "There are three well known Poisson mixtures [3]: 2-Poisson, Negative Binomial, and the Katzs K-Mixture [9].",
                "Note that the 2-Poisson model has actually been explored in probabilistic retrieval models, which led to the well-known BM25 formula [22].",
                "All these mixtures have closed forms, and can be estimated from the collection of documents efficiently.",
                "This is an advantage over the multinomial mixture models, such as PLSI [8] and LDA [1], for retrieval.",
                "For example, the probability density function of Katzs K-Mixture is given as p(c(w) = k|αw, βw) = (1 − αw)ηk,0 + αw βw + 1 ( βw βw + 1 )k where ηk,0 = 1 when k = 0, and 0 otherwise.",
                "With the observation of a collection of documents, αw and βw can be estimated as βw = cf(w) − df(w) df(w) and αw = cf(w) Nβw where cf(w) and df(w) are the collection frequency and document frequency of w, and N is the number of documents in the collection.",
                "To account for the different document lengths, we assume that βw is a reasonable estimation for generating a document of the average length, and use β = βw avdl |q| to generate the query.",
                "This Poisson mixture model can be easily used to replace P(·|C) in the retrieval functions 3 and 4. 3.4 Other Possible Flexibilities In addition to term dependent smoothing and efficient mixture background, a Poisson language model has also some other potential advantages.",
                "For example, in Section 2, we see that Formula 2 introduces a component which does document length penalization.",
                "Intuitively, when the document has more unique words, it will be penalized more.",
                "On the other hand, if a document is exactly n copies of another document, it would not get over penalized.",
                "This feature is desirable and not achieved with the Dirichlet model [5].",
                "Potentially, this component could penalize a document according to what types of terms it contains.",
                "With term specific settings of δ, we could get even more flexibility for document length normalization.",
                "Pseudo-feedback is yet another interesting direction where the Poission model might be able to show its advantage.",
                "With model-based feedback, we could again relax the combination coefficients of the feedback model and the background model, and allow different terms to contribute differently to the feedback model.",
                "We could also utilize the relevant documents to learn better per-term smoothing coefficients. 4.",
                "EVALUATION In Section 3, we analytically compared the Poisson language models and multinomial language models from the perspective of query generation and retrieval.",
                "In this section, we compare these two families of models empirically.",
                "Experiment results show that the Poisson model with perterm smoothing outperforms multinomial model, and the performance can be further improved with two-stage smoothing.",
                "Using Poisson mixture as background model also improves the retrieval performance. 4.1 Datasets Since retrieval performance could significantly vary from one test collection to another, and from one query to another, we select four representative TREC test collections: AP, Trec7, Trec8, and Wt2g(Web).",
                "To cover different types of queries, we follow [28, 5], and construct short-keyword (SK, keyword title), short-verbose (SV, one sentence description), and long-verbose (LV, multiple sentences) queries.",
                "The documents are stemmed with the Porters stemmer, and we do not remove any stop word.",
                "For each parameter, we vary its value to cover a reasonably wide range. 4.2 Comparison to Multinomial We compare the performance of the Poisson retrieval models and multinomial retrieval models using interpolation (JelinekMercer, JM) smoothing and Bayesian smoothing with conjugate priors.",
                "Table 1 shows that the two JM-smoothed models perform similarly on all data sets.",
                "Since the Dirichlet Smoothing for multinomial language model and the Gamma Smoothing for Poisson language model lead to the same retrieval formula, the performance of these two models are jointly presented.",
                "We see that Dirichlet/Gamma smoothing methods outperform both Jelinek-Mercer smoothing methods.",
                "The parameter sensitivity curves for two Jelinek-Mercer 0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1 0 0.05 0.1 0.15 0.2 0.25 0.3 Dataset: Trec8 Parameter: δ AveragePrecision JM−Multinomial: LV JM−Multinomial: SV JM−Multinomial: SK JM−Poisson: SK JM−Poisson: SV JM−Poisson: LV Figure 1: Poisson and multinomial performs similarly with Jelinek-Mercer smoothing smoothing methods are shown in Figure 1.",
                "Clearly, these two methods perform similarly either in terms of optimality Data Query JM-Multinomial JM-Poisson Dirichlet/Gamma Per-term 2-Stage Poisson MAP InitPr Pr@5d MAP InitPr Pr@5d MAP InitPr Pr@5d MAP InitPr Pr@5d AP88-89 SK 0.203 0.585 0.356 0.203 0.585 0.358 0.224 0.629 0.393 0.226 0.630 0.396 SV 0.187 0.580 0.361 0.183 0.571 0.345 0.204 0.613 0.387 0.217* 0.603 0.390 LV 0.283 0.716 0.480 0.271 0.692 0.470 0.291 0.710 0.496 0.304* 0.695 0.510 Trec7 SK 0.167 0.635 0.400 0.168 0.635 0.404 0.186 0.687 0.428 0.185 0.646 0.436 SV 0.174 0.655 0.432 0.176 0.653 0.432 0.182 0.666 0.432 0.196* 0.660 0.440 LV 0.223 0.730 0.496 0.215 0.766 0.488 0.224 0.748 0.52 0.236* 0.738 0.512 Trec8 SK 0.239 0.621 0.440 0.239 0.621 0.436 0.257 0.718 0.496 0.256 0.704 0.468 SV 0.231 0.686 0.448 0.234 0.702 0.456 0.228 0.691 0.456 0.246* 0.692 0.476 LV 0.265 0.796 0.548 0.261 0.757 0.520 0.260 0.741 0.492 0.274* 0.766 0.508 Web SK 0.250 0.616 0.380 0.250 0.616 0.380 0.302 0.767 0.468 0.307 0.739 0.468 SV 0.214 0.611 0.392 0.217 0.609 0.384 0.273 0.693 0.508 0.292* 0.703 0.480 LV 0.266 0.790 0.464 0.259 0.776 0.452 0.283 0.756 0.496 0.311* 0.759 0.488 Table 1: Performance comparison between Poisson and Multinomial retrieval models: basic models perform comparably; term dependent two-stage smoothing significantly improves Poisson An asterisk (*) indicates that the difference between the performance of the term dependent two-stage smoothing and that of the Dirichlet/Gamma single smoothing is statistically significant according to the Wilcoxon signed rank test at the level of 0.05. or sensitivity.",
                "This similarity of performance is expected as we discussed in Section 3.1.",
                "Although the Poisson model and multinomial model are similar in terms of the basic model and/or with simple smoothing methods, the Poisson model has great potential and flexibility to be further improved.",
                "As shown in the rightmost column of Table 1, term dependent two-stage Poisson model consistently outperforms the basic smoothing models, especially for verbose queries.",
                "This model is given in Formula 4, with a Gamma smoothing for the document model p(·|d), and δw, which is term dependent.",
                "The parameter µ of the first stage Gamma smoothing is empirically tuned.",
                "The combination coefficients (i.e., ∆), are estimated with the EM algorithm in Section 3.2.",
                "The parameter sensitivity curves for Dirichlet/Gamma and the per-term two-stage smoothing model are plotted in Figure 2.",
                "The per-term two-stage smoothing method is less sensitive to the parameter µ than Dirichlet/Gamma, and yields better optimal performance. 0 1000 2000 3000 4000 5000 6000 7000 8000 9000 10000 0.1 0.12 0.14 0.16 0.18 0.2 0.22 Dataset: AP; Query Type: SV Parameter: µ AveragePrecision Dirichlet/Gamma Smoothing Term Dependent 2−Stage Figure 2: Term dependent two-stage smoothing of Poisson outperforms Dirichlet/Gamma In the following subsections, we conduct experiments to demonstrate how the flexibility of the Poisson model could be utilized to achieve better performance, which we cannot achieve with multinomial language models. 4.3 Term Dependent Smoothing To test the effectiveness of the term dependent smoothing, we conduct the following two experiments.",
                "In the first experiment, we relax the constant coefficient in the simple Jelinek-Mercer smoothing formula (i.e., Formula 3), and use the EM algorithm proposed in Section 3.2 to find a δw for each unique term.",
                "Since we are using the EM algorithm to iteratively estimate the parameters, we usually do not want the probability of p(·|d) to be zero.",
                "We then use a simple Laplace method to slightly smooth the document model before it goes into the EM iterations.",
                "The documents are then still scored with Formula 3, but using learnt δw.",
                "The results are labeled with JM+L. in Table 2.",
                "Data Q JM JM JM+L. 2-Stage 2-Stage (MAP) PT: No Yes Yes No Yes AP SK 0.203 0.204 0.206 0.223 0.226* SV 0.183 0.189 0.214* 0.204 0.217* Trec7 SK 0.168 0.171 0.174 0.186 0.185 SV 0.176 0.147 0.198* 0.194 0.196 Trec8 SK 0.239 0.240 0.227* 0.257 0.256 SV 0.234 0.223 0.249* 0.242 0.246* Web SK 0.250 0.236 0.220* 0.291 0.307* SV 0.217 0.232 0.261* 0.273 0.292* Table 2: Term dependent smoothing improves retrieval performance An asterisk (*) in Column 3 indicates that the difference between the JM+L. method and JM method is statistically significant; an asterisk (*) in Column 5 means that the difference between term dependent two-stage method and query dependent two-stage method is statistically significant; PT stands for per-term.",
                "With term dependent coefficients, the performance of the Jelinek-Mercer Poisson model is improved in most cases.",
                "However, in some cases (e.g., Trec7/SV), it performs poorly.",
                "This might be caused by the problem of EM estimation with unsmoothed document models.",
                "Once non-zero probability is assigned to all the terms before entering the EM iteration, the performance on verbose queries can be improved significantly.",
                "This indicates that there is still room to find better methods to estimate δw.",
                "Please note that neither the perterm JM method nor the JM+L. method has a parameter to tune.",
                "As shown in Table 1, the term dependent two-stage smoothing can significantly improve retrieval performance.",
                "To understand whether the improvement is contributed by the term dependent smoothing or the two-stage smoothing framework, we design another experiment to compare the perterm two-stage smoothing with the two-stage smoothing method proposed in [29].",
                "Their method managed to find coefficients specific to the query, thus a verbose query would use a higher δ.",
                "However, since their model is based on multinomial language modeling, they could not get per-term coefficients.",
                "We adopt their method to the Poisson two-stage smoothing, and also estimate a per-query coefficient for all the terms.",
                "We compare the performance of such a model with the per-term two-stage smoothing model, and present the results in the right two columns in Table 2.",
                "Again, we see that the per-term two-stage smoothing outperforms the per-query two-stage smoothing, especially for verbose queries.",
                "The improvement is not as large as how the perterm smoothing method improves over Dirichlet/Gamma.",
                "This is expected, since the per-query smoothing has already addressed the query discrimination problem to some extent.",
                "This experiment shows that even if the smoothing is already per-query, making it per-term is still beneficial.",
                "In brief, the per-term smoothing improved the retrieval performance of both one-stage and two-stage smoothing method. 4.4 Mixture Background Model In this section, we conduct experiments to examine the benefits of using a mixture background model without extra computational cost, which can not be achieved for multinomial models.",
                "Specifically, in retrieval formula 3, instead of using a single Poisson distribution to model the background p(·|C), we use Katzs K-Mixture model, which is essentially a mixture of Poisson distributions. p(·|C) can be computed efficiently with simple collection statistics, as discussed in Section 3.3.",
                "Data Query JM.",
                "Poisson JM.",
                "K-Mixture AP SK 0.203 0.204 SV 0.183 0.188* Trec-7 SK 0.168 0.169 SV 0.176 0.178* Trec-8 SK 0.239 0.239 SV 0.234 0.238* Web SK 0.250 0.250 SV 0.217 0.223* Table 3: K-Mixture background model improves retrieval performance The performance of the JM retrieval model with single Poisson background and with Katzs K-Mixture background model is compared in Table 3.",
                "Clearly, using K-Mixture to model the background model outperforms the single Poisson background model in most cases, especially for verbose queries where the improvement is statistically significant.",
                "Figure 3 shows that the performance changes over different parameters for short verbose queries.",
                "The model using K-Mixture background is less sensitive than the one using single Poisson background.",
                "Given that this type of mixture 0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1 0 0.05 0.1 0.15 0.2 0.25 Data: Trec8; Query: SV Parameter: δ AveragePrecision Poisson Background K−Mixture Background Figure 3: K-Mixture background model deviates the sensitivity of verbose queries background model does not require any extra computation cost, it would be interesting to study whether using other mixture Poisson models, such as 2-Poisson and negative Binomial, could help the performance. 5.",
                "RELATED WORK To the best of our knowledge, there has been no study of query generation models based on Poisson distribution.",
                "Language models have been shown to be effective for many retrieval tasks [21, 28, 14, 4].",
                "The most popular and fundamental one is the query-generation language model [21, 13].",
                "All existing query generation language models are based on either multinomial distribution [19, 6, 28, 13] or multivariate Bernoulli distribution [21, 17, 18].",
                "We introduce a new family of language models, based on Poisson distribution.",
                "Poisson distribution has been previously studied in the document generation models [16, 22, 3, 24], leading to the development of one of the most effective retrieval formula BM25 [23]. [24] studies the parallel derivation of three different retrieval models which is related to our comparison of Poisson and multinomial.",
                "However, the Poisson model in their paper is still under the document generation framework, and also does not account for the document length variation. [26] introduces a way to empirically search for an exponential model for the documents.",
                "Poisson mixtures [3] such as 2-Poisson [22], Negative multinomial, and Katzs KMixture [9] has shown to be effective to model and retrieve documents.",
                "Once again, none of this work explores Poisson distribution in the query generation framework.",
                "Language model smoothing [2, 28, 29] and background structures [15, 10, 25, 27] have been studied with multinomial language models. [7] analytically shows that term specific smoothing could be useful.",
                "We show that Poisson language model is natural to accommodate the per-term smoothing without heuristic twist of the semantics of a generative model, and is able to efficiently better model the mixture background, both analytically and empirically. 6.",
                "CONCLUSIONS We present a new family of query generation language models for retrieval based on Poisson distribution.",
                "We derive several smoothing methods for this family of models, including single-stage smoothing and two-stage smoothing.",
                "We compare the new models with the popular multinomial retrieval models both analytically and experimentally.",
                "Our analysis shows that while our new models and multinomial models are equivalent under some assumptions, they are generally different with some important differences.",
                "In particular, we show that Poisson has an advantage over multinomial in naturally accommodating per-term smoothing.",
                "We exploit this property to develop a new per-term smoothing algorithm for Poisson language models, which is shown to outperform term-independent smoothing for both Poisson and multinomial models.",
                "Furthermore, we show that a mixture background model for Poisson can be used to improve the performance and robustness over the standard Poisson background model.",
                "Our work opens up many interesting directions for further exploration in this new family of models.",
                "Further exploring the flexibilities over multinomial language models, such as length normalization and pseudo-feedback could be good future work.",
                "It is also appealing to find robust methods to learn the per-term smoothing coefficients without additional computation cost. 7.",
                "ACKNOWLEDGMENTS We thank the anonymous SIGIR 07 reviewers for their useful comments.",
                "This material is based in part upon work supported by the National Science Foundation under award numbers IIS-0347933 and 0425852. 8.",
                "REFERENCES [1] D. Blei, A. Ng, and M. Jordan.",
                "Latent dirichlet allocation.",
                "Journal of Machine Learning Research, 3:993-1022, 2003. [2] S. F. Chen and J. Goodman.",
                "An empirical study of smoothing techniques for language modeling.",
                "Technical Report TR-10-98, Harvard University, 1998. [3] K. Church and W. Gale.",
                "Poisson mixtures.",
                "Nat.",
                "Lang.",
                "Eng., 1(2):163-190, 1995. [4] W. B. Croft and J. Lafferty, editors.",
                "Language Modeling and Information Retrieval.",
                "Kluwer Academic Publishers, 2003. [5] H. Fang, T. Tao, and C. Zhai.",
                "A formal study of information retrieval heuristics.",
                "In Proceedings of the 27th annual international ACM SIGIR conference on Research and development in information retrieval, pages 49-56, 2004. [6] D. Hiemstra.",
                "Using Language Models for Information Retrieval.",
                "PhD thesis, University of Twente, Enschede, Netherlands, 2001. [7] D. Hiemstra.",
                "Term-specific smoothing for the language modeling approach to information retrieval: the importance of a query term.",
                "In Proceedings of the 25th annual international ACM SIGIR conference on Research and development in information retrieval, pages 35-41, 2002. [8] T. Hofmann.",
                "Probabilistic latent semantic indexing.",
                "In Proceedings of ACM SIGIR99, pages 50-57, 1999. [9] S. M. Katz.",
                "Distribution of content words and phrases in text and language modelling.",
                "Nat.",
                "Lang.",
                "Eng., 2(1):15-59, 1996. [10] O. Kurland and L. Lee.",
                "Corpus structure, language models, and ad-hoc information retrieval.",
                "In Proceedings of the 27th annual international ACM SIGIR conference on Research and development in information retrieval, pages 194-201, 2004. [11] J. Lafferty and C. Zhai.",
                "Document language models, query models, and risk minimization for information retrieval.",
                "In Proceedings of SIGIR01, pages 111-119, Sept 2001. [12] J. Lafferty and C. Zhai.",
                "Probabilistic IR models based on query and document generation.",
                "In Proceedings of the Language Modeling and IR workshop, pages 1-5, May 31 - June 1 2001. [13] J. Lafferty and C. Zhai.",
                "Probabilistic relevance models based on document and query generation.",
                "In W. B. Croft and J. Lafferty, editors, Language Modeling and Information Retrieval.",
                "Kluwer Academic Publishers, 2003. [14] V. Lavrenko and B. Croft.",
                "Relevance-based language models.",
                "In Proceedings of SIGIR01, pages 120-127, Sept 2001. [15] X. Liu and W. B. Croft.",
                "Cluster-based retrieval using language models.",
                "In Proceedings of the 27th annual international ACM SIGIR conference on Research and development in information retrieval, pages 186-193, 2004. [16] E. L. Margulis.",
                "Modelling documents with multiple poisson distributions.",
                "Inf.",
                "Process.",
                "Manage., 29(2):215-227, 1993. [17] A. McCallum and K. Nigam.",
                "A comparison of event models for naive bayes text classification.",
                "In Proceedings of AAAI-98 Workshop on Learning for Text Categorization, 1998. [18] D. Metzler, V. Lavrenko, and W. B. Croft.",
                "Formal multiple-bernoulli models for language modeling.",
                "In Proceedings of the 27th annual international ACM SIGIR conference on Research and development in information retrieval, pages 540-541, 2004. [19] D. H. Miller, T. Leek, and R. Schwartz.",
                "A hidden Markov model information retrieval system.",
                "In Proceedings of the 1999 ACM SIGIR Conference on Research and Development in Information Retrieval, pages 214-221, 1999. [20] A. Papoulis.",
                "Probability, random variables and stochastic processes.",
                "New York: McGraw-Hill, 1984, 2nd ed., 1984. [21] J. M. Ponte and W. B. Croft.",
                "A language modeling approach to information retrieval.",
                "In Proceedings of the 21st annual international ACM SIGIR conference on Research and development in information retrieval, pages 275-281, 1998. [22] S. Robertson and S. Walker.",
                "Some simple effective approximations to the 2-poisson model for probabilistic weighted retrieval.",
                "In Proceedings of SIGIR94, pages 232-241, 1994. [23] S. E. Robertson, S. Walker, S. Jones, M. M.Hancock-Beaulieu, and M. Gatford.",
                "Okapi at TREC-3.",
                "In D. K. Harman, editor, The Third Text REtrieval Conference (TREC-3), pages 109-126, 1995. [24] T. Roelleke and J. Wang.",
                "A parallel derivation of probabilistic information retrieval models.",
                "In Proceedings of the 29th annual international ACM SIGIR conference on Research and development in information retrieval, pages 107-114, 2006. [25] T. Tao, X. Wang, Q. Mei, and C. Zhai.",
                "Language model information retrieval with document expansion.",
                "In Proceedings of HLT/NAACL 2006, pages 407-414, 2006. [26] J. Teevan and D. R. Karger.",
                "Empirical development of an exponential probabilistic model for text retrieval: using textual analysis to build a better model.",
                "In Proceedings of the 26th annual international ACM SIGIR conference on Research and development in informaion retrieval, pages 18-25, 2003. [27] X. Wei and W. B. Croft.",
                "Lda-based document models for ad-hoc retrieval.",
                "In Proceedings of the 29th annual international ACM SIGIR conference on Research and development in information retrieval, pages 178-185, 2006. [28] C. Zhai and J. Lafferty.",
                "A study of smoothing methods for language models applied to ad-hoc information retrieval.",
                "In Proceedings of ACM SIGIR01, pages 334-342, Sept 2001. [29] C. Zhai and J. Lafferty.",
                "Two-stage language models for information retrieval.",
                "In Proceedings of ACM SIGIR02, pages 49-56, Aug 2002."
            ],
            "original_annotated_samples": [],
            "translated_annotated_samples": [
                "La mayoría de los modelos existentes se basan en la distribución multinomial y obtendrían documentos basados en la probabilidad de consulta calculada en función de un \"modelo probabilístico de generación de consultas\".modelo probabilístico de la generación de consultas"
            ],
            "translated_text": "",
            "candidates": [],
            "error": []
        },
        "poisson distribution": {
            "translated_key": "",
            "is_in_text": true,
            "original_annotated_sentences": [
                "A Study of Poisson Query Generation Model for Information Retrieval Qiaozhu Mei, Hui Fang, Chengxiang Zhai Department of Computer Science University of Illinois at Urbana-Champaign Urbana,IL 61801 {qmei2,hfang,czhai}@uiuc.edu ABSTRACT Many variants of language models have been proposed for information retrieval.",
                "Most existing models are based on multinomial distribution and would score documents based on query likelihood computed based on a query generation probabilistic model.",
                "In this paper, we propose and study a new family of query generation models based on <br>poisson distribution</br>.",
                "We show that while in their simplest forms, the new family of models and the existing multinomial models are equivalent, they behave differently for many smoothing methods.",
                "We show that the Poisson model has several advantages over the multinomial model, including naturally accommodating per-term smoothing and allowing for more accurate background modeling.",
                "We present several variants of the new model corresponding to different smoothing methods, and evaluate them on four representative TREC test collections.",
                "The results show that while their basic models perform comparably, the Poisson model can outperform multinomial model with per-term smoothing.",
                "The performance can be further improved with two-stage smoothing.",
                "Categories and Subject Descriptors: H.3.3 [Information Search and Retrieval]: Retrieval Models General Terms: Algorithms 1.",
                "INTRODUCTION As a new type of probabilistic retrieval models, language models have been shown to be effective for many retrieval tasks [21, 28, 14, 4].",
                "Among many variants of language models proposed, the most popular and fundamental one is the query-generation language model [21, 13], which leads to the query-likelihood scoring method for ranking documents.",
                "In such a model, given a query q and a document d, we compute the likelihood of generating query q with a model estimated based on document d, i.e., the conditional probability p(q|d).",
                "We can then rank documents based on the likelihood of generating the query.",
                "Virtually all the existing query generation language models are based on either multinomial distribution [19, 6, 28] or multivariate Bernoulli distribution [21, 18].",
                "The multinomial distribution is especially popular and also shown to be quite effective.",
                "The heavy use of multinomial distribution is partly due to the fact that it has been successfully used in speech recognition, where multinomial distribution is a natural choice for modeling the occurrence of a particular word in a particular position in text.",
                "Compared with multivariate Bernoulli, multinomial distribution has the advantage of being able to model the frequency of terms in the query; in contrast, multivariate Bernoulli only models the presence and absence of query terms, thus cannot capture different frequencies of query terms.",
                "However, multivariate Bernoulli also has one potential advantage over multinomial from the viewpoint of retrieval: in a multinomial distribution, the probabilities of all the terms must sum to 1, making it hard to accommodate per-term smoothing, while in a multivariate Bernoulli, the presence probabilities of different terms are completely independent of each other, easily accommodating per-term smoothing and weighting.",
                "Note that term absence is also indirectly captured in a multinomial model through the constraint that all the term probabilities must sum to 1.",
                "In this paper, we propose and study a new family of query generation models based on the <br>poisson distribution</br>.",
                "In this new family of models, we model the frequency of each term independently with a <br>poisson distribution</br>.",
                "To score a document, we would first estimate a multivariate Poisson model based on the document, and then score it based on the likelihood of the query given by the estimated Poisson model.",
                "In some sense, the Poisson model combines the advantage of multinomial in modeling term frequency and the advantage of the multivariate Bernoulli in accommodating per-term smoothing.",
                "Indeed, similar to the multinomial distribution, the <br>poisson distribution</br> models term frequencies, but without the constraint that all the term probabilities must sum to 1, and similar to multivariate Bernoulli, it models each term independently, thus can easily accommodate per-term smoothing.",
                "As in the existing work on multinomial language models, smoothing is critical for this new family of models.",
                "We derive several smoothing methods for Poisson model in parallel to those used for multinomial distributions, and compare the corresponding retrieval models with those based on multinomial distributions.",
                "We find that while with some smoothing methods, the new model and the multinomial model lead to exactly the same formula, with some other smoothing methods they diverge, and the Poisson model brings in more flexibility for smoothing.",
                "In particular, a key difference is that the Poisson model can naturally accommodate perterm smoothing, which is hard to achieve with a multinomial model without heuristic twist of the semantics of a generative model.",
                "We exploit this potential advantage to develop a new term-dependent smoothing algorithm for Poisson model and show that this new smoothing algorithm can improve performance over term-independent smoothing algorithms using either Poisson or multinomial model.",
                "This advantage is seen for both one-stage and two-stage smoothing.",
                "Another potential advantage of the Poisson model is that its corresponding background model for smoothing can be improved through using a mixture model that has a closed form formula.",
                "This new background model is shown to outperform the standard background model and reduce the sensitivity of retrieval performance to the smoothing parameter.",
                "The rest of the paper is organized as follows.",
                "In Section 2, we introduce the new family of query generation models with <br>poisson distribution</br>, and present various smoothing methods which lead to different retrieval functions.",
                "In Section 3, we analytically compare the Poisson language model with the multinomial language model, from the perspective of retrieval.",
                "We then design empirical experiments to compare the two families of language models in Section 4.",
                "We discuss the related work in 5 and conclude in 6. 2.",
                "QUERY GENERATION WITH POISSON PROCESS In the query generation framework, a basic assumption is that a query is generated with a model estimated based on a document.",
                "In most existing work [12, 6, 28, 29], people assume that each query word is sampled independently from a multinomial distribution.",
                "Alternatively, we assume that a query is generated by sampling the frequency of words from a series of independent Poisson processes [20]. 2.1 The Generation Process Let V = {w1, ..., wn} be a vocabulary set.",
                "Let w be a piece of text composed by an author and c(w1), ..., c(wn) be a frequency vector representing w, where c(wi, w) is the frequency count of term wi in text w. In retrieval, w could be either a query or a document.",
                "We consider the frequency counts of the n unique terms in w as n different types of events, sampled from n independent homogeneous Poisson processes, respectively.",
                "Suppose t is the time period during which the author composed the text.",
                "With a homogeneous Poisson process, the frequency count of each event, i.e., the number of occurrences of wi, follows a <br>poisson distribution</br> with associated parameter λit, where λi is a rate parameter characterizing the expected number of wi in a unit time.",
                "The probability density function of such a <br>poisson distribution</br> is given by P(c(wi, w) = k|λit) = e−λit (λit)k k!",
                "Without losing generality, we set t to the length of the text w (people write one word in a unit time), i.e., t = |w|.",
                "With n such independent Poisson processes, each explaining the generation of one term in the vocabulary, the likelihood of w to be generated from such Poisson processes can be written as p(w|Λ) = n i=1 p(c(wi, w)|Λ) = n i=1 e−λi·|w| (λi · |w|)c(wi,w) c(wi, w)! where Λ = {λ1, ..., λn} and |w| = n i=1 c(wi, w).",
                "We refer to these n independent Poisson processes with parameter Λ as a Poisson Language Model.",
                "Let D = {d1, ..., dm} be an observed set of document samples generated from the Poisson process above.",
                "The maximum likelihood estimate (MLE) of λi is ˆλi = d∈D c(wi, d) d∈D w ∈V c(w , d) Note that this MLE is different from the MLE for the <br>poisson distribution</br> without considering the document lengths, which appears in [22, 24].",
                "Given a document d, we may estimate a Poisson language model Λd using d as a sample.",
                "The likelihood that a query q is generated from the document language model Λd can be written as p(q|d) = w∈V p(c(w, q)|Λd) (1) This representation is clearly different from the multinomial query generation model as (1) the likelihood includes all the terms in the vocabulary V , instead of only those appearing in q, and (2) instead of the appearance of terms, the event space of this model is the frequencies of each term.",
                "In practice, we have the flexibility to choose the vocabulary V .",
                "In one extreme, we can use the vocabulary of the whole collection.",
                "However, this may bring in noise and considerable computational cost.",
                "In the other extreme, we may focus on the terms in the query and ignore other terms, but some useful information may be lost by ignoring the nonquery terms.",
                "As a compromise, we may conflate all the non-query terms as one single pseudo term.",
                "In other words, we may assume that there is exactly one non-query term in the vocabulary for each query.",
                "In our experiments, we adopt this pseudo non-query term strategy.",
                "A document can be scored with the likelihood in Equation 1.",
                "However, if a query term is unseen in the document, the MLE of the <br>poisson distribution</br> would assign zero probability to the term, causing the probability of the query to be zero.",
                "As in existing language modeling approaches, the main challenge of constructing a reasonable retrieval model is to find a smoothed language model for p(·|d). 2.2 Smoothing in Poisson Retrieval Model In general, we want to assign non-zero rates for the query terms that are not seen in document d. Many smoothing methods have been proposed for multinomial language models[2, 28, 29].",
                "In general, we have to discount the probabilities of some words seen in the text to leave some extra probability mass to assign to the unseen words.",
                "In Poisson language models, however, we do not have the same constraint as in a multinomial model (i.e., w∈V p(w|d) = 1).",
                "Thus we do not have to discount the probability of seen words in order to give a non-zero rate to an unseen word.",
                "Instead, we only need to guarantee that k=0,1,2,... p(c(w, d) = k|d) = 1.",
                "In this section, we introduce three different strategies to smooth a Poisson language model, and show how they lead to different retrieval functions. 2.2.1 Bayesian Smoothing using Gamma Prior Following the risk minimization framework in [11], we assume that a document is generated by the arrival of terms in a time period of |d| according to the document language model, which essentially consists of a vector of Poisson rates for each term, i.e., Λd = λd,1, ..., λd,|V | .",
                "A document is assumed to be generated from a potentially different model.",
                "Given a particular document d, we want to estimate Λd.",
                "The rate of a term is estimated independently of other terms.",
                "We use Bayesian estimation with the following Gamma prior, which has two parameters, α and β: Gamma(λ|α, β) = βα Γ(α) λα−1 e−βλ For each term w, the parameters αw and βw are chosen to be αw = µ ∗ λC,w and βw = µ, where µ is a parameter and λC,w is the rate of w estimated from some background language model, usually the collection language model.",
                "The posterior distribution of Λd is given by p(Λd|d, C) ∝ w∈V e−λw(|d|+µ) λ c(w,d)+µλC,w−1 w which is a product of |V | Gamma distributions with parameters c(w, d) + µλC,w and |d| + µ for each word w. Given that the Gamma mean is α β , we have ˆλd,w = λd,w λd,wp(λd,w|d, C)dλd,w = c(w, d) + µλC,w |d| + µ This is precisely the smoothed estimate of multinomial language model with Dirichlet prior [28]. 2.2.2 Interpolation (Jelinek-Mercer) Smoothing Another straightforward method is to decompose the query generation model as a mixture of two component models.",
                "One is the document language model estimated with maximum likelihood estimator, and the other is a model estimated from the collection background, p(·|C), which assigns non-zero rate to w. For example, we may use an interpolation coefficient between 0 and 1 (i.e., δ ∈ [0, 1]).",
                "With this simple interpolation, we can score a document with Score(d, q) = w∈V log((1 − δ)p(c(w, q)|d) + δp(c(w, q)|C)) (2) Using the maximum likelihood estimator for p(·|d), we have λd,w = c(w,d) |d| , thus Equation 2 becomes Score(d, q) ∝ w∈d∩q [log(1 + 1 − δ δ e−λd,w|q| (λd,w|q|)c(w,q) c(w, q)! · p(c(w, q)|C) ) − log (1 − δ)e−λd,w|q| + δp(c(w, q) = 0|C) 1 − δ + δp(c(w, q) = 0|C) ] + w∈d log (1 − δ)e−λd,w|q| + δp(c(w, q) = 0|C) 1 − δ + δp(c(w, q) = 0|C) We can also use a Poisson language model for p(·|C), or use some other frequency-based models.",
                "In the retrieval formula above, the first summation can be computed efficiently.",
                "The second summation can be actually treated as a document prior, which penalizes long documents.",
                "As the second summation is difficult to compute efficiently, we conflate all non-query terms as one pseudo non-queryterm, denoted as N. Using the pseudo-term formulation and a Poisson collection model, we can rewrite the retrieval formula as Score(d, q) ∝ w∈d∩q log(1 + 1 − δ δ e−λd,w (λd,w|q|)c(w,q) e−λd,C |q| (λd,C )c(w,q) ) + log (1 − δ)e−λd,N |q| + δe−λC,N |q| 1 − δ + δe−λC,N |q| (3) where λd,N = |d|− w∈q c(w,d) |d| and λC,N = |C|− w∈q c(w,C) |C| . 2.2.3 Two-Stage Smoothing As discussed in [29], smoothing plays two roles in retrieval: (1) to improve the estimation of the document language model, and (2) to explain the common terms in the query.",
                "In order to distinguish the content and non-discriminative words in a query, we follow [29] and assume that a query is generated by sampling from a two-component mixture of Poisson language models, with one component being the document model Λd and the other being a query background language model p(·|U). p(·|U) models the typical term frequencies in the users queries.",
                "We may then score each document with the query likelihood computed using the following two-stage smoothing model: p(c(w, q)|Λd, U) = (1 − δ)p(c(w, q)|Λd) + δp(c(w, q)|U) (4) where δ is a parameter, roughly indicating the amount of noise in q.",
                "This looks similar to the interpolation smoothing, except that p(·|Λd) now should be a smoothed language model, instead of the one estimated with MLE.",
                "With no prior knowledge on p(·|U), we could set it to p(·|C).",
                "Any smoothing methods for the document language model can be used to estimate p(·|d) such as the Gamma smoothing as discussed in Section 2.2.1.",
                "The empirical study of the smoothing methods is presented in Section 4. 3.",
                "ANALYSIS OF POISSON LANGUAGE MODEL From the previous section, we notice that the Poisson language model has a strong connection to the multinomial language model.",
                "This is expected since they both belong to the exponential family [26].",
                "However, there are many differences when these two families of models are applied with different smoothing methods.",
                "From the perspective of retrieval, will these two language models perform equivalently?",
                "If not, which model provides more benefits to retrieval, or provides flexibility which could lead to potential benefits?",
                "In this section, we analytically discuss the retrieval features of the Poisson language models, by comparing their behavior with that of the multinomial language models. 3.1 The Equivalence of Basic Models Let us begin with the assumption that all the query terms appear in every document.",
                "Under this assumption, no smoothing is needed.",
                "A document can be scored by the log likelihood of the query with the maximum likelihood estimate: Score(d, q) = w∈V log e−λd,w|q| (λd,w|q|)c(w,q) c(w, q)! (5) Using the MLE, we have λd,w = c(w,d) w∈V c(w,d) .",
                "Thus Score(d, q) ∝ c(w,q)>0 c(w, q) log c(w, d) w∈V c(w, d) This is exactly the log likelihood of the query if the document language model is a multinomial with maximum likelihood estimate.",
                "Indeed, even with Gamma smoothing, when plugging λd,w = c(w,d)+µλC,w |d|+µ and λC,w = c(w,C) |C| into Equation 5, it is easy to show that Score(d, q) ∝ w∈q∩d c(w, q) log(1 + c(w, d) µ · c(w,C) |C| ) + |q| log µ |d| + µ (6) which is exactly the Dirichlet retrieval formula in [28].",
                "Note that this equivalence holds only when the document length variation is modeled with Poisson process.",
                "This derivation indicates the equivalence of the basic Poisson and multinomial language models for retrieval.",
                "With other smoothing strategies, however, the two models would be different.",
                "Nevertheless, with this equivalence in basic models, we could expect that the Poisson language model performs comparably to the multinomial language model in retrieval, if only simple smoothing is explored.",
                "Based on this equivalence analysis, one may ask, why we should pursue the Poisson language model.",
                "In the following sections, we show that despite the equivalence in their basic models, the Poisson language model brings in extra flexibility for exploring advanced techniques on various retrieval features, which could not be achieved with multinomial language models. 3.2 Term Dependent Smoothing One flexibility of the Poisson language model is that it provides a natural framework to accommodate term dependent (per-term) smoothing.",
                "Existing work on language model smoothing has already shown that different types of queries should be smoothed differently according to how discriminative the query terms are. [7] also predicted that different terms should have a different smoothing weights.",
                "With multinomial query generation models, people usually use a single smoothing coefficient to control the combination of the document model and the background model [28, 29].",
                "This parameter can be made specific for different queries, but always has to be a constant for all the terms.",
                "This is mandatory since a multinomial language model has the constraint that w∈V p(w|d) = 1.",
                "However, from retrieval perspective, different terms may need to be smoothed differently even if they are in the same query.",
                "For example, a non-discriminative term (e.g., the, is) is expected to be explained more with the background model, while a content term (e.g., retrieval, bush) in the query should be explained with the document model.",
                "Therefore, a better way of smoothing would be to set the interpolation coefficient (i.e., δ in Formula 2 and Formula 3) specifically for each term.",
                "Since the Poisson language model does not have the sum-to-one constraint across terms, it can easily accommodate per-term smoothing without needing to heuristically twist the semantics of a generative model as in the case of multinomial language models.",
                "Below we present a possible way to explore term dependent smoothing with Poisson language models.",
                "Essentially, we want to use a term-specific smoothing coefficient δ in the linear combination, denoted as δw.",
                "This coefficient should intuitively be larger if w is a common word and smaller if it is a content word.",
                "The key problem is to find a method to assign reasonable values to δw.",
                "Empirical tuning is infeasible for so many parameters.",
                "We may instead estimate the parameters ∆ = {δ1, ..., δ|V |} by maximizing the likelihood of the query given the mixture model of p(q|ΛQ) and p(q|U), where ΛQ is the true query model to generate the query and p(q|U) is a query background model as discussed in Section 2.2.3.",
                "With the model p(q|ΛQ) hidden, the query likelihood is p(q|∆, U) = ΛQ w∈V ((1 − δw)p(c(w, q)|ΛQ) + δwp(c(w, q)|U))P(ΛQ|U)dΛQ If we have relevant documents for each query, we can approximate the query model space with the language models of all the relevant documents.",
                "Without relevant documents, we opt to approximate the query model space with the models of all the documents in the collection.",
                "Setting p(·|U) as p(·|C), the query likelihood becomes p(q|∆, U) = d∈C πd w∈V ((1−δw)p(c(w, q)|ˆΛd)+δwp(c(w, q)|C)) where πd = p(ˆΛd|U). p(·|ˆΛd) is an estimated Poisson language model for document d. If we have prior knowledge on p(ˆΛd|U), such as which documents are relevant to the query, we can set πd accordingly, because what we want is to find ∆ that can maximize the likelihood of the query given relevant documents.",
                "Without this prior knowledge, we can leave πd as free parameters, and use the EM algorithm to estimate πd and ∆.",
                "The updating functions are given as π (k+1) d = πd w∈V ((1 − δw)p(c(w, q)|ˆΛd) + δwp(c(w, q)|C)) d∈C πd w∈V ((1 − δw)p(c(w, q)|ˆΛd) + δwp(c(w, q)|C)) and δ (k+1) w = d∈C πd δwp(c(w, q)|C)) (1 − δw)p(c(w, q)|ˆΛd) + δwp(c(w, q)|C)) As discussed in [29], we only need to run the EM algorithm for several iterations, thus the computational cost is relatively low.",
                "We again assume our vocabulary containing all query terms plus a pseudo non-query term.",
                "Note that the function does not give an explicit way of estimating the coefficient for the unseen non-query term.",
                "In our experiments, we set it to the average over δw of all query terms.",
                "With this flexibility, we expect Poisson language models could improve the retrieval performance, especially for verbose queries, where the query terms have various discriminative values.",
                "In Section 4, we use empirical experiments to prove this hypothesis. 3.3 Mixture Background Models Another flexibility is to explore different background (collection) models (i.e., p(·|U), or p(·|C)).",
                "One common assumption made in language modeling information retrieval is that the background model is a homogeneous model of the document models [28, 29].",
                "Similarly, we can also make the assumption that the collection model is a Poisson language model, with the rates λC,w = d∈C c(w,d) |C| .",
                "However, this assumption usually does not hold, since the collection is far more complex than a single document.",
                "Indeed, the collection usually consists of a mixture of documents with various genres, authors, and topics, etc.",
                "Treating the collection model as a mixture of document models, instead of a single pseudo-document model is more reasonable.",
                "Existing work of multinomial language modeling has already shown that a better modeling of background improves the retrieval performance, such as clusters [15, 10], neighbor documents [25], and aspects [8, 27].",
                "All the approaches can be easily adopted using Poisson language models.",
                "However, a common problem of these approaches is that they all require heavy computation to construct the background model.",
                "With Poisson language modeling, we show that it is possible to model the mixture background without paying for the heavy computational cost.",
                "Poisson Mixture [3] has been proposed to model a collection of documents, which can fit the data much better than a single Poisson.",
                "The basic idea is to assume that the collection is generated from a mixture of Poisson models, which has the general form of p(x = k|PM) = λ p(λ)p(x = k|λ)dλ p(·|λ) is a single Poisson model and p(λ) is an arbitrary probability density function.",
                "There are three well known Poisson mixtures [3]: 2-Poisson, Negative Binomial, and the Katzs K-Mixture [9].",
                "Note that the 2-Poisson model has actually been explored in probabilistic retrieval models, which led to the well-known BM25 formula [22].",
                "All these mixtures have closed forms, and can be estimated from the collection of documents efficiently.",
                "This is an advantage over the multinomial mixture models, such as PLSI [8] and LDA [1], for retrieval.",
                "For example, the probability density function of Katzs K-Mixture is given as p(c(w) = k|αw, βw) = (1 − αw)ηk,0 + αw βw + 1 ( βw βw + 1 )k where ηk,0 = 1 when k = 0, and 0 otherwise.",
                "With the observation of a collection of documents, αw and βw can be estimated as βw = cf(w) − df(w) df(w) and αw = cf(w) Nβw where cf(w) and df(w) are the collection frequency and document frequency of w, and N is the number of documents in the collection.",
                "To account for the different document lengths, we assume that βw is a reasonable estimation for generating a document of the average length, and use β = βw avdl |q| to generate the query.",
                "This Poisson mixture model can be easily used to replace P(·|C) in the retrieval functions 3 and 4. 3.4 Other Possible Flexibilities In addition to term dependent smoothing and efficient mixture background, a Poisson language model has also some other potential advantages.",
                "For example, in Section 2, we see that Formula 2 introduces a component which does document length penalization.",
                "Intuitively, when the document has more unique words, it will be penalized more.",
                "On the other hand, if a document is exactly n copies of another document, it would not get over penalized.",
                "This feature is desirable and not achieved with the Dirichlet model [5].",
                "Potentially, this component could penalize a document according to what types of terms it contains.",
                "With term specific settings of δ, we could get even more flexibility for document length normalization.",
                "Pseudo-feedback is yet another interesting direction where the Poission model might be able to show its advantage.",
                "With model-based feedback, we could again relax the combination coefficients of the feedback model and the background model, and allow different terms to contribute differently to the feedback model.",
                "We could also utilize the relevant documents to learn better per-term smoothing coefficients. 4.",
                "EVALUATION In Section 3, we analytically compared the Poisson language models and multinomial language models from the perspective of query generation and retrieval.",
                "In this section, we compare these two families of models empirically.",
                "Experiment results show that the Poisson model with perterm smoothing outperforms multinomial model, and the performance can be further improved with two-stage smoothing.",
                "Using Poisson mixture as background model also improves the retrieval performance. 4.1 Datasets Since retrieval performance could significantly vary from one test collection to another, and from one query to another, we select four representative TREC test collections: AP, Trec7, Trec8, and Wt2g(Web).",
                "To cover different types of queries, we follow [28, 5], and construct short-keyword (SK, keyword title), short-verbose (SV, one sentence description), and long-verbose (LV, multiple sentences) queries.",
                "The documents are stemmed with the Porters stemmer, and we do not remove any stop word.",
                "For each parameter, we vary its value to cover a reasonably wide range. 4.2 Comparison to Multinomial We compare the performance of the Poisson retrieval models and multinomial retrieval models using interpolation (JelinekMercer, JM) smoothing and Bayesian smoothing with conjugate priors.",
                "Table 1 shows that the two JM-smoothed models perform similarly on all data sets.",
                "Since the Dirichlet Smoothing for multinomial language model and the Gamma Smoothing for Poisson language model lead to the same retrieval formula, the performance of these two models are jointly presented.",
                "We see that Dirichlet/Gamma smoothing methods outperform both Jelinek-Mercer smoothing methods.",
                "The parameter sensitivity curves for two Jelinek-Mercer 0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1 0 0.05 0.1 0.15 0.2 0.25 0.3 Dataset: Trec8 Parameter: δ AveragePrecision JM−Multinomial: LV JM−Multinomial: SV JM−Multinomial: SK JM−Poisson: SK JM−Poisson: SV JM−Poisson: LV Figure 1: Poisson and multinomial performs similarly with Jelinek-Mercer smoothing smoothing methods are shown in Figure 1.",
                "Clearly, these two methods perform similarly either in terms of optimality Data Query JM-Multinomial JM-Poisson Dirichlet/Gamma Per-term 2-Stage Poisson MAP InitPr Pr@5d MAP InitPr Pr@5d MAP InitPr Pr@5d MAP InitPr Pr@5d AP88-89 SK 0.203 0.585 0.356 0.203 0.585 0.358 0.224 0.629 0.393 0.226 0.630 0.396 SV 0.187 0.580 0.361 0.183 0.571 0.345 0.204 0.613 0.387 0.217* 0.603 0.390 LV 0.283 0.716 0.480 0.271 0.692 0.470 0.291 0.710 0.496 0.304* 0.695 0.510 Trec7 SK 0.167 0.635 0.400 0.168 0.635 0.404 0.186 0.687 0.428 0.185 0.646 0.436 SV 0.174 0.655 0.432 0.176 0.653 0.432 0.182 0.666 0.432 0.196* 0.660 0.440 LV 0.223 0.730 0.496 0.215 0.766 0.488 0.224 0.748 0.52 0.236* 0.738 0.512 Trec8 SK 0.239 0.621 0.440 0.239 0.621 0.436 0.257 0.718 0.496 0.256 0.704 0.468 SV 0.231 0.686 0.448 0.234 0.702 0.456 0.228 0.691 0.456 0.246* 0.692 0.476 LV 0.265 0.796 0.548 0.261 0.757 0.520 0.260 0.741 0.492 0.274* 0.766 0.508 Web SK 0.250 0.616 0.380 0.250 0.616 0.380 0.302 0.767 0.468 0.307 0.739 0.468 SV 0.214 0.611 0.392 0.217 0.609 0.384 0.273 0.693 0.508 0.292* 0.703 0.480 LV 0.266 0.790 0.464 0.259 0.776 0.452 0.283 0.756 0.496 0.311* 0.759 0.488 Table 1: Performance comparison between Poisson and Multinomial retrieval models: basic models perform comparably; term dependent two-stage smoothing significantly improves Poisson An asterisk (*) indicates that the difference between the performance of the term dependent two-stage smoothing and that of the Dirichlet/Gamma single smoothing is statistically significant according to the Wilcoxon signed rank test at the level of 0.05. or sensitivity.",
                "This similarity of performance is expected as we discussed in Section 3.1.",
                "Although the Poisson model and multinomial model are similar in terms of the basic model and/or with simple smoothing methods, the Poisson model has great potential and flexibility to be further improved.",
                "As shown in the rightmost column of Table 1, term dependent two-stage Poisson model consistently outperforms the basic smoothing models, especially for verbose queries.",
                "This model is given in Formula 4, with a Gamma smoothing for the document model p(·|d), and δw, which is term dependent.",
                "The parameter µ of the first stage Gamma smoothing is empirically tuned.",
                "The combination coefficients (i.e., ∆), are estimated with the EM algorithm in Section 3.2.",
                "The parameter sensitivity curves for Dirichlet/Gamma and the per-term two-stage smoothing model are plotted in Figure 2.",
                "The per-term two-stage smoothing method is less sensitive to the parameter µ than Dirichlet/Gamma, and yields better optimal performance. 0 1000 2000 3000 4000 5000 6000 7000 8000 9000 10000 0.1 0.12 0.14 0.16 0.18 0.2 0.22 Dataset: AP; Query Type: SV Parameter: µ AveragePrecision Dirichlet/Gamma Smoothing Term Dependent 2−Stage Figure 2: Term dependent two-stage smoothing of Poisson outperforms Dirichlet/Gamma In the following subsections, we conduct experiments to demonstrate how the flexibility of the Poisson model could be utilized to achieve better performance, which we cannot achieve with multinomial language models. 4.3 Term Dependent Smoothing To test the effectiveness of the term dependent smoothing, we conduct the following two experiments.",
                "In the first experiment, we relax the constant coefficient in the simple Jelinek-Mercer smoothing formula (i.e., Formula 3), and use the EM algorithm proposed in Section 3.2 to find a δw for each unique term.",
                "Since we are using the EM algorithm to iteratively estimate the parameters, we usually do not want the probability of p(·|d) to be zero.",
                "We then use a simple Laplace method to slightly smooth the document model before it goes into the EM iterations.",
                "The documents are then still scored with Formula 3, but using learnt δw.",
                "The results are labeled with JM+L. in Table 2.",
                "Data Q JM JM JM+L. 2-Stage 2-Stage (MAP) PT: No Yes Yes No Yes AP SK 0.203 0.204 0.206 0.223 0.226* SV 0.183 0.189 0.214* 0.204 0.217* Trec7 SK 0.168 0.171 0.174 0.186 0.185 SV 0.176 0.147 0.198* 0.194 0.196 Trec8 SK 0.239 0.240 0.227* 0.257 0.256 SV 0.234 0.223 0.249* 0.242 0.246* Web SK 0.250 0.236 0.220* 0.291 0.307* SV 0.217 0.232 0.261* 0.273 0.292* Table 2: Term dependent smoothing improves retrieval performance An asterisk (*) in Column 3 indicates that the difference between the JM+L. method and JM method is statistically significant; an asterisk (*) in Column 5 means that the difference between term dependent two-stage method and query dependent two-stage method is statistically significant; PT stands for per-term.",
                "With term dependent coefficients, the performance of the Jelinek-Mercer Poisson model is improved in most cases.",
                "However, in some cases (e.g., Trec7/SV), it performs poorly.",
                "This might be caused by the problem of EM estimation with unsmoothed document models.",
                "Once non-zero probability is assigned to all the terms before entering the EM iteration, the performance on verbose queries can be improved significantly.",
                "This indicates that there is still room to find better methods to estimate δw.",
                "Please note that neither the perterm JM method nor the JM+L. method has a parameter to tune.",
                "As shown in Table 1, the term dependent two-stage smoothing can significantly improve retrieval performance.",
                "To understand whether the improvement is contributed by the term dependent smoothing or the two-stage smoothing framework, we design another experiment to compare the perterm two-stage smoothing with the two-stage smoothing method proposed in [29].",
                "Their method managed to find coefficients specific to the query, thus a verbose query would use a higher δ.",
                "However, since their model is based on multinomial language modeling, they could not get per-term coefficients.",
                "We adopt their method to the Poisson two-stage smoothing, and also estimate a per-query coefficient for all the terms.",
                "We compare the performance of such a model with the per-term two-stage smoothing model, and present the results in the right two columns in Table 2.",
                "Again, we see that the per-term two-stage smoothing outperforms the per-query two-stage smoothing, especially for verbose queries.",
                "The improvement is not as large as how the perterm smoothing method improves over Dirichlet/Gamma.",
                "This is expected, since the per-query smoothing has already addressed the query discrimination problem to some extent.",
                "This experiment shows that even if the smoothing is already per-query, making it per-term is still beneficial.",
                "In brief, the per-term smoothing improved the retrieval performance of both one-stage and two-stage smoothing method. 4.4 Mixture Background Model In this section, we conduct experiments to examine the benefits of using a mixture background model without extra computational cost, which can not be achieved for multinomial models.",
                "Specifically, in retrieval formula 3, instead of using a single <br>poisson distribution</br> to model the background p(·|C), we use Katzs K-Mixture model, which is essentially a mixture of Poisson distributions. p(·|C) can be computed efficiently with simple collection statistics, as discussed in Section 3.3.",
                "Data Query JM.",
                "Poisson JM.",
                "K-Mixture AP SK 0.203 0.204 SV 0.183 0.188* Trec-7 SK 0.168 0.169 SV 0.176 0.178* Trec-8 SK 0.239 0.239 SV 0.234 0.238* Web SK 0.250 0.250 SV 0.217 0.223* Table 3: K-Mixture background model improves retrieval performance The performance of the JM retrieval model with single Poisson background and with Katzs K-Mixture background model is compared in Table 3.",
                "Clearly, using K-Mixture to model the background model outperforms the single Poisson background model in most cases, especially for verbose queries where the improvement is statistically significant.",
                "Figure 3 shows that the performance changes over different parameters for short verbose queries.",
                "The model using K-Mixture background is less sensitive than the one using single Poisson background.",
                "Given that this type of mixture 0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1 0 0.05 0.1 0.15 0.2 0.25 Data: Trec8; Query: SV Parameter: δ AveragePrecision Poisson Background K−Mixture Background Figure 3: K-Mixture background model deviates the sensitivity of verbose queries background model does not require any extra computation cost, it would be interesting to study whether using other mixture Poisson models, such as 2-Poisson and negative Binomial, could help the performance. 5.",
                "RELATED WORK To the best of our knowledge, there has been no study of query generation models based on <br>poisson distribution</br>.",
                "Language models have been shown to be effective for many retrieval tasks [21, 28, 14, 4].",
                "The most popular and fundamental one is the query-generation language model [21, 13].",
                "All existing query generation language models are based on either multinomial distribution [19, 6, 28, 13] or multivariate Bernoulli distribution [21, 17, 18].",
                "We introduce a new family of language models, based on <br>poisson distribution</br>.",
                "<br>poisson distribution</br> has been previously studied in the document generation models [16, 22, 3, 24], leading to the development of one of the most effective retrieval formula BM25 [23]. [24] studies the parallel derivation of three different retrieval models which is related to our comparison of Poisson and multinomial.",
                "However, the Poisson model in their paper is still under the document generation framework, and also does not account for the document length variation. [26] introduces a way to empirically search for an exponential model for the documents.",
                "Poisson mixtures [3] such as 2-Poisson [22], Negative multinomial, and Katzs KMixture [9] has shown to be effective to model and retrieve documents.",
                "Once again, none of this work explores <br>poisson distribution</br> in the query generation framework.",
                "Language model smoothing [2, 28, 29] and background structures [15, 10, 25, 27] have been studied with multinomial language models. [7] analytically shows that term specific smoothing could be useful.",
                "We show that Poisson language model is natural to accommodate the per-term smoothing without heuristic twist of the semantics of a generative model, and is able to efficiently better model the mixture background, both analytically and empirically. 6.",
                "CONCLUSIONS We present a new family of query generation language models for retrieval based on <br>poisson distribution</br>.",
                "We derive several smoothing methods for this family of models, including single-stage smoothing and two-stage smoothing.",
                "We compare the new models with the popular multinomial retrieval models both analytically and experimentally.",
                "Our analysis shows that while our new models and multinomial models are equivalent under some assumptions, they are generally different with some important differences.",
                "In particular, we show that Poisson has an advantage over multinomial in naturally accommodating per-term smoothing.",
                "We exploit this property to develop a new per-term smoothing algorithm for Poisson language models, which is shown to outperform term-independent smoothing for both Poisson and multinomial models.",
                "Furthermore, we show that a mixture background model for Poisson can be used to improve the performance and robustness over the standard Poisson background model.",
                "Our work opens up many interesting directions for further exploration in this new family of models.",
                "Further exploring the flexibilities over multinomial language models, such as length normalization and pseudo-feedback could be good future work.",
                "It is also appealing to find robust methods to learn the per-term smoothing coefficients without additional computation cost. 7.",
                "ACKNOWLEDGMENTS We thank the anonymous SIGIR 07 reviewers for their useful comments.",
                "This material is based in part upon work supported by the National Science Foundation under award numbers IIS-0347933 and 0425852. 8.",
                "REFERENCES [1] D. Blei, A. Ng, and M. Jordan.",
                "Latent dirichlet allocation.",
                "Journal of Machine Learning Research, 3:993-1022, 2003. [2] S. F. Chen and J. Goodman.",
                "An empirical study of smoothing techniques for language modeling.",
                "Technical Report TR-10-98, Harvard University, 1998. [3] K. Church and W. Gale.",
                "Poisson mixtures.",
                "Nat.",
                "Lang.",
                "Eng., 1(2):163-190, 1995. [4] W. B. Croft and J. Lafferty, editors.",
                "Language Modeling and Information Retrieval.",
                "Kluwer Academic Publishers, 2003. [5] H. Fang, T. Tao, and C. Zhai.",
                "A formal study of information retrieval heuristics.",
                "In Proceedings of the 27th annual international ACM SIGIR conference on Research and development in information retrieval, pages 49-56, 2004. [6] D. Hiemstra.",
                "Using Language Models for Information Retrieval.",
                "PhD thesis, University of Twente, Enschede, Netherlands, 2001. [7] D. Hiemstra.",
                "Term-specific smoothing for the language modeling approach to information retrieval: the importance of a query term.",
                "In Proceedings of the 25th annual international ACM SIGIR conference on Research and development in information retrieval, pages 35-41, 2002. [8] T. Hofmann.",
                "Probabilistic latent semantic indexing.",
                "In Proceedings of ACM SIGIR99, pages 50-57, 1999. [9] S. M. Katz.",
                "Distribution of content words and phrases in text and language modelling.",
                "Nat.",
                "Lang.",
                "Eng., 2(1):15-59, 1996. [10] O. Kurland and L. Lee.",
                "Corpus structure, language models, and ad-hoc information retrieval.",
                "In Proceedings of the 27th annual international ACM SIGIR conference on Research and development in information retrieval, pages 194-201, 2004. [11] J. Lafferty and C. Zhai.",
                "Document language models, query models, and risk minimization for information retrieval.",
                "In Proceedings of SIGIR01, pages 111-119, Sept 2001. [12] J. Lafferty and C. Zhai.",
                "Probabilistic IR models based on query and document generation.",
                "In Proceedings of the Language Modeling and IR workshop, pages 1-5, May 31 - June 1 2001. [13] J. Lafferty and C. Zhai.",
                "Probabilistic relevance models based on document and query generation.",
                "In W. B. Croft and J. Lafferty, editors, Language Modeling and Information Retrieval.",
                "Kluwer Academic Publishers, 2003. [14] V. Lavrenko and B. Croft.",
                "Relevance-based language models.",
                "In Proceedings of SIGIR01, pages 120-127, Sept 2001. [15] X. Liu and W. B. Croft.",
                "Cluster-based retrieval using language models.",
                "In Proceedings of the 27th annual international ACM SIGIR conference on Research and development in information retrieval, pages 186-193, 2004. [16] E. L. Margulis.",
                "Modelling documents with multiple poisson distributions.",
                "Inf.",
                "Process.",
                "Manage., 29(2):215-227, 1993. [17] A. McCallum and K. Nigam.",
                "A comparison of event models for naive bayes text classification.",
                "In Proceedings of AAAI-98 Workshop on Learning for Text Categorization, 1998. [18] D. Metzler, V. Lavrenko, and W. B. Croft.",
                "Formal multiple-bernoulli models for language modeling.",
                "In Proceedings of the 27th annual international ACM SIGIR conference on Research and development in information retrieval, pages 540-541, 2004. [19] D. H. Miller, T. Leek, and R. Schwartz.",
                "A hidden Markov model information retrieval system.",
                "In Proceedings of the 1999 ACM SIGIR Conference on Research and Development in Information Retrieval, pages 214-221, 1999. [20] A. Papoulis.",
                "Probability, random variables and stochastic processes.",
                "New York: McGraw-Hill, 1984, 2nd ed., 1984. [21] J. M. Ponte and W. B. Croft.",
                "A language modeling approach to information retrieval.",
                "In Proceedings of the 21st annual international ACM SIGIR conference on Research and development in information retrieval, pages 275-281, 1998. [22] S. Robertson and S. Walker.",
                "Some simple effective approximations to the 2-poisson model for probabilistic weighted retrieval.",
                "In Proceedings of SIGIR94, pages 232-241, 1994. [23] S. E. Robertson, S. Walker, S. Jones, M. M.Hancock-Beaulieu, and M. Gatford.",
                "Okapi at TREC-3.",
                "In D. K. Harman, editor, The Third Text REtrieval Conference (TREC-3), pages 109-126, 1995. [24] T. Roelleke and J. Wang.",
                "A parallel derivation of probabilistic information retrieval models.",
                "In Proceedings of the 29th annual international ACM SIGIR conference on Research and development in information retrieval, pages 107-114, 2006. [25] T. Tao, X. Wang, Q. Mei, and C. Zhai.",
                "Language model information retrieval with document expansion.",
                "In Proceedings of HLT/NAACL 2006, pages 407-414, 2006. [26] J. Teevan and D. R. Karger.",
                "Empirical development of an exponential probabilistic model for text retrieval: using textual analysis to build a better model.",
                "In Proceedings of the 26th annual international ACM SIGIR conference on Research and development in informaion retrieval, pages 18-25, 2003. [27] X. Wei and W. B. Croft.",
                "Lda-based document models for ad-hoc retrieval.",
                "In Proceedings of the 29th annual international ACM SIGIR conference on Research and development in information retrieval, pages 178-185, 2006. [28] C. Zhai and J. Lafferty.",
                "A study of smoothing methods for language models applied to ad-hoc information retrieval.",
                "In Proceedings of ACM SIGIR01, pages 334-342, Sept 2001. [29] C. Zhai and J. Lafferty.",
                "Two-stage language models for information retrieval.",
                "In Proceedings of ACM SIGIR02, pages 49-56, Aug 2002."
            ],
            "original_annotated_samples": [],
            "translated_annotated_samples": [
                "En este artículo, proponemos y estudiamos una nueva familia de modelos de generación de consultas basados en \"distribución de Poisson\".distribución de veneno",
                "En este artículo, proponemos y estudiamos una nueva familia de modelos de generación de consultas basados en la \"distribución de Poisson\".distribución de veneno",
                "En esta nueva familia de modelos, modelamos la frecuencia de cada término independientemente con una \"distribución de Poisson\".distribución de veneno",
                "De hecho, de manera similar a la distribución multinomial, la \"distribución de Poisson\" modela las frecuencias de términos, pero sin la restricción de que todas las probabilidades del término deben sumar a 1, y similar a Bernoulli multivariado, modela cada término de forma independiente, por lo tanto, puede acomodar fácilmente por período por término por período.suavizado.distribución de veneno",
                "En la Sección 2, presentamos a la nueva familia de modelos de generación de consultas con \"distribución de Poisson\", y presentamos varios métodos de suavizado que conducen a diferentes funciones de recuperación.distribución de veneno",
                "Con un proceso de Poisson homogéneo, el recuento de frecuencia de cada evento, es decir, el número de ocurrencias de WI, sigue una \"distribución de Poisson\" con el parámetro asociado λIT, donde λi es un parámetro de velocidad que caracteriza el número esperado de WI en un tiempo de unidad.distribución de veneno",
                "La función de densidad de probabilidad de dicha \"distribución de poisson\" viene dada por P (C (Wi, W) = k | λit) = e - λit (λit) k k!distribución de veneno",
                "La estimación de máxima probabilidad (MLE) de λi es ˆλi = d∈D C (wi, d) d∈D w ∈V c (w, d) Tenga en cuenta que este MLE es diferente de la MLE para la \"distribución de Poisson\" sin considerarLas longitudes del documento, que aparecen en [22, 24].distribución de veneno",
                "Sin embargo, si un término de consulta no se ve en el documento, el MLE de la \"distribución de Poisson\" asignaría una probabilidad cero al término, lo que provocará que la probabilidad de la consulta sea cero.distribución de veneno",
                "Específicamente, en la fórmula 3 de recuperación, en lugar de usar una sola \"distribución de Poisson\" para modelar el fondo P (· | C), usamos el modelo Katzs K-Micureza, que es esencialmente una mezcla de distribuciones de Poisson.P (· | c) se puede calcular de manera eficiente con estadísticas de recolección simples, como se discutió en la Sección 3.3.distribución de veneno",
                "Trabajo relacionado según nuestro lo mejor de nuestro conocimiento, no ha habido un estudio de los modelos de generación de consultas basados en la \"distribución de Poisson\".distribución de veneno",
                "Presentamos una nueva familia de modelos de idiomas, basado en \"Poisson Distribution\".distribución de veneno",
                "\"Poisson Distribution\" se ha estudiado previamente en los modelos de generación de documentos [16, 22, 3, 24], lo que lleva al desarrollo de una de las fórmula de recuperación más efectiva BM25 [23].[24] Estudia la derivación paralela de tres modelos de recuperación diferentes relacionados con nuestra comparación de Poisson y multinomial.distribución de veneno",
                "Una vez más, ninguno de este trabajo explora la \"distribución de Poisson\" en el marco de generación de consultas.distribución de veneno",
                "Conclusiones presentamos una nueva familia de modelos de lenguaje de generación de consultas para la recuperación basados en \"distribución de Poisson\".distribución de veneno"
            ],
            "translated_text": "",
            "candidates": [],
            "error": []
        },
        "two-stage smoothing": {
            "translated_key": "",
            "is_in_text": true,
            "original_annotated_sentences": [
                "A Study of Poisson Query Generation Model for Information Retrieval Qiaozhu Mei, Hui Fang, Chengxiang Zhai Department of Computer Science University of Illinois at Urbana-Champaign Urbana,IL 61801 {qmei2,hfang,czhai}@uiuc.edu ABSTRACT Many variants of language models have been proposed for information retrieval.",
                "Most existing models are based on multinomial distribution and would score documents based on query likelihood computed based on a query generation probabilistic model.",
                "In this paper, we propose and study a new family of query generation models based on Poisson distribution.",
                "We show that while in their simplest forms, the new family of models and the existing multinomial models are equivalent, they behave differently for many smoothing methods.",
                "We show that the Poisson model has several advantages over the multinomial model, including naturally accommodating per-term smoothing and allowing for more accurate background modeling.",
                "We present several variants of the new model corresponding to different smoothing methods, and evaluate them on four representative TREC test collections.",
                "The results show that while their basic models perform comparably, the Poisson model can outperform multinomial model with per-term smoothing.",
                "The performance can be further improved with <br>two-stage smoothing</br>.",
                "Categories and Subject Descriptors: H.3.3 [Information Search and Retrieval]: Retrieval Models General Terms: Algorithms 1.",
                "INTRODUCTION As a new type of probabilistic retrieval models, language models have been shown to be effective for many retrieval tasks [21, 28, 14, 4].",
                "Among many variants of language models proposed, the most popular and fundamental one is the query-generation language model [21, 13], which leads to the query-likelihood scoring method for ranking documents.",
                "In such a model, given a query q and a document d, we compute the likelihood of generating query q with a model estimated based on document d, i.e., the conditional probability p(q|d).",
                "We can then rank documents based on the likelihood of generating the query.",
                "Virtually all the existing query generation language models are based on either multinomial distribution [19, 6, 28] or multivariate Bernoulli distribution [21, 18].",
                "The multinomial distribution is especially popular and also shown to be quite effective.",
                "The heavy use of multinomial distribution is partly due to the fact that it has been successfully used in speech recognition, where multinomial distribution is a natural choice for modeling the occurrence of a particular word in a particular position in text.",
                "Compared with multivariate Bernoulli, multinomial distribution has the advantage of being able to model the frequency of terms in the query; in contrast, multivariate Bernoulli only models the presence and absence of query terms, thus cannot capture different frequencies of query terms.",
                "However, multivariate Bernoulli also has one potential advantage over multinomial from the viewpoint of retrieval: in a multinomial distribution, the probabilities of all the terms must sum to 1, making it hard to accommodate per-term smoothing, while in a multivariate Bernoulli, the presence probabilities of different terms are completely independent of each other, easily accommodating per-term smoothing and weighting.",
                "Note that term absence is also indirectly captured in a multinomial model through the constraint that all the term probabilities must sum to 1.",
                "In this paper, we propose and study a new family of query generation models based on the Poisson distribution.",
                "In this new family of models, we model the frequency of each term independently with a Poisson distribution.",
                "To score a document, we would first estimate a multivariate Poisson model based on the document, and then score it based on the likelihood of the query given by the estimated Poisson model.",
                "In some sense, the Poisson model combines the advantage of multinomial in modeling term frequency and the advantage of the multivariate Bernoulli in accommodating per-term smoothing.",
                "Indeed, similar to the multinomial distribution, the Poisson distribution models term frequencies, but without the constraint that all the term probabilities must sum to 1, and similar to multivariate Bernoulli, it models each term independently, thus can easily accommodate per-term smoothing.",
                "As in the existing work on multinomial language models, smoothing is critical for this new family of models.",
                "We derive several smoothing methods for Poisson model in parallel to those used for multinomial distributions, and compare the corresponding retrieval models with those based on multinomial distributions.",
                "We find that while with some smoothing methods, the new model and the multinomial model lead to exactly the same formula, with some other smoothing methods they diverge, and the Poisson model brings in more flexibility for smoothing.",
                "In particular, a key difference is that the Poisson model can naturally accommodate perterm smoothing, which is hard to achieve with a multinomial model without heuristic twist of the semantics of a generative model.",
                "We exploit this potential advantage to develop a new term-dependent smoothing algorithm for Poisson model and show that this new smoothing algorithm can improve performance over term-independent smoothing algorithms using either Poisson or multinomial model.",
                "This advantage is seen for both one-stage and <br>two-stage smoothing</br>.",
                "Another potential advantage of the Poisson model is that its corresponding background model for smoothing can be improved through using a mixture model that has a closed form formula.",
                "This new background model is shown to outperform the standard background model and reduce the sensitivity of retrieval performance to the smoothing parameter.",
                "The rest of the paper is organized as follows.",
                "In Section 2, we introduce the new family of query generation models with Poisson distribution, and present various smoothing methods which lead to different retrieval functions.",
                "In Section 3, we analytically compare the Poisson language model with the multinomial language model, from the perspective of retrieval.",
                "We then design empirical experiments to compare the two families of language models in Section 4.",
                "We discuss the related work in 5 and conclude in 6. 2.",
                "QUERY GENERATION WITH POISSON PROCESS In the query generation framework, a basic assumption is that a query is generated with a model estimated based on a document.",
                "In most existing work [12, 6, 28, 29], people assume that each query word is sampled independently from a multinomial distribution.",
                "Alternatively, we assume that a query is generated by sampling the frequency of words from a series of independent Poisson processes [20]. 2.1 The Generation Process Let V = {w1, ..., wn} be a vocabulary set.",
                "Let w be a piece of text composed by an author and c(w1), ..., c(wn) be a frequency vector representing w, where c(wi, w) is the frequency count of term wi in text w. In retrieval, w could be either a query or a document.",
                "We consider the frequency counts of the n unique terms in w as n different types of events, sampled from n independent homogeneous Poisson processes, respectively.",
                "Suppose t is the time period during which the author composed the text.",
                "With a homogeneous Poisson process, the frequency count of each event, i.e., the number of occurrences of wi, follows a Poisson distribution with associated parameter λit, where λi is a rate parameter characterizing the expected number of wi in a unit time.",
                "The probability density function of such a Poisson Distribution is given by P(c(wi, w) = k|λit) = e−λit (λit)k k!",
                "Without losing generality, we set t to the length of the text w (people write one word in a unit time), i.e., t = |w|.",
                "With n such independent Poisson processes, each explaining the generation of one term in the vocabulary, the likelihood of w to be generated from such Poisson processes can be written as p(w|Λ) = n i=1 p(c(wi, w)|Λ) = n i=1 e−λi·|w| (λi · |w|)c(wi,w) c(wi, w)! where Λ = {λ1, ..., λn} and |w| = n i=1 c(wi, w).",
                "We refer to these n independent Poisson processes with parameter Λ as a Poisson Language Model.",
                "Let D = {d1, ..., dm} be an observed set of document samples generated from the Poisson process above.",
                "The maximum likelihood estimate (MLE) of λi is ˆλi = d∈D c(wi, d) d∈D w ∈V c(w , d) Note that this MLE is different from the MLE for the Poisson distribution without considering the document lengths, which appears in [22, 24].",
                "Given a document d, we may estimate a Poisson language model Λd using d as a sample.",
                "The likelihood that a query q is generated from the document language model Λd can be written as p(q|d) = w∈V p(c(w, q)|Λd) (1) This representation is clearly different from the multinomial query generation model as (1) the likelihood includes all the terms in the vocabulary V , instead of only those appearing in q, and (2) instead of the appearance of terms, the event space of this model is the frequencies of each term.",
                "In practice, we have the flexibility to choose the vocabulary V .",
                "In one extreme, we can use the vocabulary of the whole collection.",
                "However, this may bring in noise and considerable computational cost.",
                "In the other extreme, we may focus on the terms in the query and ignore other terms, but some useful information may be lost by ignoring the nonquery terms.",
                "As a compromise, we may conflate all the non-query terms as one single pseudo term.",
                "In other words, we may assume that there is exactly one non-query term in the vocabulary for each query.",
                "In our experiments, we adopt this pseudo non-query term strategy.",
                "A document can be scored with the likelihood in Equation 1.",
                "However, if a query term is unseen in the document, the MLE of the Poisson distribution would assign zero probability to the term, causing the probability of the query to be zero.",
                "As in existing language modeling approaches, the main challenge of constructing a reasonable retrieval model is to find a smoothed language model for p(·|d). 2.2 Smoothing in Poisson Retrieval Model In general, we want to assign non-zero rates for the query terms that are not seen in document d. Many smoothing methods have been proposed for multinomial language models[2, 28, 29].",
                "In general, we have to discount the probabilities of some words seen in the text to leave some extra probability mass to assign to the unseen words.",
                "In Poisson language models, however, we do not have the same constraint as in a multinomial model (i.e., w∈V p(w|d) = 1).",
                "Thus we do not have to discount the probability of seen words in order to give a non-zero rate to an unseen word.",
                "Instead, we only need to guarantee that k=0,1,2,... p(c(w, d) = k|d) = 1.",
                "In this section, we introduce three different strategies to smooth a Poisson language model, and show how they lead to different retrieval functions. 2.2.1 Bayesian Smoothing using Gamma Prior Following the risk minimization framework in [11], we assume that a document is generated by the arrival of terms in a time period of |d| according to the document language model, which essentially consists of a vector of Poisson rates for each term, i.e., Λd = λd,1, ..., λd,|V | .",
                "A document is assumed to be generated from a potentially different model.",
                "Given a particular document d, we want to estimate Λd.",
                "The rate of a term is estimated independently of other terms.",
                "We use Bayesian estimation with the following Gamma prior, which has two parameters, α and β: Gamma(λ|α, β) = βα Γ(α) λα−1 e−βλ For each term w, the parameters αw and βw are chosen to be αw = µ ∗ λC,w and βw = µ, where µ is a parameter and λC,w is the rate of w estimated from some background language model, usually the collection language model.",
                "The posterior distribution of Λd is given by p(Λd|d, C) ∝ w∈V e−λw(|d|+µ) λ c(w,d)+µλC,w−1 w which is a product of |V | Gamma distributions with parameters c(w, d) + µλC,w and |d| + µ for each word w. Given that the Gamma mean is α β , we have ˆλd,w = λd,w λd,wp(λd,w|d, C)dλd,w = c(w, d) + µλC,w |d| + µ This is precisely the smoothed estimate of multinomial language model with Dirichlet prior [28]. 2.2.2 Interpolation (Jelinek-Mercer) Smoothing Another straightforward method is to decompose the query generation model as a mixture of two component models.",
                "One is the document language model estimated with maximum likelihood estimator, and the other is a model estimated from the collection background, p(·|C), which assigns non-zero rate to w. For example, we may use an interpolation coefficient between 0 and 1 (i.e., δ ∈ [0, 1]).",
                "With this simple interpolation, we can score a document with Score(d, q) = w∈V log((1 − δ)p(c(w, q)|d) + δp(c(w, q)|C)) (2) Using the maximum likelihood estimator for p(·|d), we have λd,w = c(w,d) |d| , thus Equation 2 becomes Score(d, q) ∝ w∈d∩q [log(1 + 1 − δ δ e−λd,w|q| (λd,w|q|)c(w,q) c(w, q)! · p(c(w, q)|C) ) − log (1 − δ)e−λd,w|q| + δp(c(w, q) = 0|C) 1 − δ + δp(c(w, q) = 0|C) ] + w∈d log (1 − δ)e−λd,w|q| + δp(c(w, q) = 0|C) 1 − δ + δp(c(w, q) = 0|C) We can also use a Poisson language model for p(·|C), or use some other frequency-based models.",
                "In the retrieval formula above, the first summation can be computed efficiently.",
                "The second summation can be actually treated as a document prior, which penalizes long documents.",
                "As the second summation is difficult to compute efficiently, we conflate all non-query terms as one pseudo non-queryterm, denoted as N. Using the pseudo-term formulation and a Poisson collection model, we can rewrite the retrieval formula as Score(d, q) ∝ w∈d∩q log(1 + 1 − δ δ e−λd,w (λd,w|q|)c(w,q) e−λd,C |q| (λd,C )c(w,q) ) + log (1 − δ)e−λd,N |q| + δe−λC,N |q| 1 − δ + δe−λC,N |q| (3) where λd,N = |d|− w∈q c(w,d) |d| and λC,N = |C|− w∈q c(w,C) |C| . 2.2.3 <br>two-stage smoothing</br> As discussed in [29], smoothing plays two roles in retrieval: (1) to improve the estimation of the document language model, and (2) to explain the common terms in the query.",
                "In order to distinguish the content and non-discriminative words in a query, we follow [29] and assume that a query is generated by sampling from a two-component mixture of Poisson language models, with one component being the document model Λd and the other being a query background language model p(·|U). p(·|U) models the typical term frequencies in the users queries.",
                "We may then score each document with the query likelihood computed using the following <br>two-stage smoothing</br> model: p(c(w, q)|Λd, U) = (1 − δ)p(c(w, q)|Λd) + δp(c(w, q)|U) (4) where δ is a parameter, roughly indicating the amount of noise in q.",
                "This looks similar to the interpolation smoothing, except that p(·|Λd) now should be a smoothed language model, instead of the one estimated with MLE.",
                "With no prior knowledge on p(·|U), we could set it to p(·|C).",
                "Any smoothing methods for the document language model can be used to estimate p(·|d) such as the Gamma smoothing as discussed in Section 2.2.1.",
                "The empirical study of the smoothing methods is presented in Section 4. 3.",
                "ANALYSIS OF POISSON LANGUAGE MODEL From the previous section, we notice that the Poisson language model has a strong connection to the multinomial language model.",
                "This is expected since they both belong to the exponential family [26].",
                "However, there are many differences when these two families of models are applied with different smoothing methods.",
                "From the perspective of retrieval, will these two language models perform equivalently?",
                "If not, which model provides more benefits to retrieval, or provides flexibility which could lead to potential benefits?",
                "In this section, we analytically discuss the retrieval features of the Poisson language models, by comparing their behavior with that of the multinomial language models. 3.1 The Equivalence of Basic Models Let us begin with the assumption that all the query terms appear in every document.",
                "Under this assumption, no smoothing is needed.",
                "A document can be scored by the log likelihood of the query with the maximum likelihood estimate: Score(d, q) = w∈V log e−λd,w|q| (λd,w|q|)c(w,q) c(w, q)! (5) Using the MLE, we have λd,w = c(w,d) w∈V c(w,d) .",
                "Thus Score(d, q) ∝ c(w,q)>0 c(w, q) log c(w, d) w∈V c(w, d) This is exactly the log likelihood of the query if the document language model is a multinomial with maximum likelihood estimate.",
                "Indeed, even with Gamma smoothing, when plugging λd,w = c(w,d)+µλC,w |d|+µ and λC,w = c(w,C) |C| into Equation 5, it is easy to show that Score(d, q) ∝ w∈q∩d c(w, q) log(1 + c(w, d) µ · c(w,C) |C| ) + |q| log µ |d| + µ (6) which is exactly the Dirichlet retrieval formula in [28].",
                "Note that this equivalence holds only when the document length variation is modeled with Poisson process.",
                "This derivation indicates the equivalence of the basic Poisson and multinomial language models for retrieval.",
                "With other smoothing strategies, however, the two models would be different.",
                "Nevertheless, with this equivalence in basic models, we could expect that the Poisson language model performs comparably to the multinomial language model in retrieval, if only simple smoothing is explored.",
                "Based on this equivalence analysis, one may ask, why we should pursue the Poisson language model.",
                "In the following sections, we show that despite the equivalence in their basic models, the Poisson language model brings in extra flexibility for exploring advanced techniques on various retrieval features, which could not be achieved with multinomial language models. 3.2 Term Dependent Smoothing One flexibility of the Poisson language model is that it provides a natural framework to accommodate term dependent (per-term) smoothing.",
                "Existing work on language model smoothing has already shown that different types of queries should be smoothed differently according to how discriminative the query terms are. [7] also predicted that different terms should have a different smoothing weights.",
                "With multinomial query generation models, people usually use a single smoothing coefficient to control the combination of the document model and the background model [28, 29].",
                "This parameter can be made specific for different queries, but always has to be a constant for all the terms.",
                "This is mandatory since a multinomial language model has the constraint that w∈V p(w|d) = 1.",
                "However, from retrieval perspective, different terms may need to be smoothed differently even if they are in the same query.",
                "For example, a non-discriminative term (e.g., the, is) is expected to be explained more with the background model, while a content term (e.g., retrieval, bush) in the query should be explained with the document model.",
                "Therefore, a better way of smoothing would be to set the interpolation coefficient (i.e., δ in Formula 2 and Formula 3) specifically for each term.",
                "Since the Poisson language model does not have the sum-to-one constraint across terms, it can easily accommodate per-term smoothing without needing to heuristically twist the semantics of a generative model as in the case of multinomial language models.",
                "Below we present a possible way to explore term dependent smoothing with Poisson language models.",
                "Essentially, we want to use a term-specific smoothing coefficient δ in the linear combination, denoted as δw.",
                "This coefficient should intuitively be larger if w is a common word and smaller if it is a content word.",
                "The key problem is to find a method to assign reasonable values to δw.",
                "Empirical tuning is infeasible for so many parameters.",
                "We may instead estimate the parameters ∆ = {δ1, ..., δ|V |} by maximizing the likelihood of the query given the mixture model of p(q|ΛQ) and p(q|U), where ΛQ is the true query model to generate the query and p(q|U) is a query background model as discussed in Section 2.2.3.",
                "With the model p(q|ΛQ) hidden, the query likelihood is p(q|∆, U) = ΛQ w∈V ((1 − δw)p(c(w, q)|ΛQ) + δwp(c(w, q)|U))P(ΛQ|U)dΛQ If we have relevant documents for each query, we can approximate the query model space with the language models of all the relevant documents.",
                "Without relevant documents, we opt to approximate the query model space with the models of all the documents in the collection.",
                "Setting p(·|U) as p(·|C), the query likelihood becomes p(q|∆, U) = d∈C πd w∈V ((1−δw)p(c(w, q)|ˆΛd)+δwp(c(w, q)|C)) where πd = p(ˆΛd|U). p(·|ˆΛd) is an estimated Poisson language model for document d. If we have prior knowledge on p(ˆΛd|U), such as which documents are relevant to the query, we can set πd accordingly, because what we want is to find ∆ that can maximize the likelihood of the query given relevant documents.",
                "Without this prior knowledge, we can leave πd as free parameters, and use the EM algorithm to estimate πd and ∆.",
                "The updating functions are given as π (k+1) d = πd w∈V ((1 − δw)p(c(w, q)|ˆΛd) + δwp(c(w, q)|C)) d∈C πd w∈V ((1 − δw)p(c(w, q)|ˆΛd) + δwp(c(w, q)|C)) and δ (k+1) w = d∈C πd δwp(c(w, q)|C)) (1 − δw)p(c(w, q)|ˆΛd) + δwp(c(w, q)|C)) As discussed in [29], we only need to run the EM algorithm for several iterations, thus the computational cost is relatively low.",
                "We again assume our vocabulary containing all query terms plus a pseudo non-query term.",
                "Note that the function does not give an explicit way of estimating the coefficient for the unseen non-query term.",
                "In our experiments, we set it to the average over δw of all query terms.",
                "With this flexibility, we expect Poisson language models could improve the retrieval performance, especially for verbose queries, where the query terms have various discriminative values.",
                "In Section 4, we use empirical experiments to prove this hypothesis. 3.3 Mixture Background Models Another flexibility is to explore different background (collection) models (i.e., p(·|U), or p(·|C)).",
                "One common assumption made in language modeling information retrieval is that the background model is a homogeneous model of the document models [28, 29].",
                "Similarly, we can also make the assumption that the collection model is a Poisson language model, with the rates λC,w = d∈C c(w,d) |C| .",
                "However, this assumption usually does not hold, since the collection is far more complex than a single document.",
                "Indeed, the collection usually consists of a mixture of documents with various genres, authors, and topics, etc.",
                "Treating the collection model as a mixture of document models, instead of a single pseudo-document model is more reasonable.",
                "Existing work of multinomial language modeling has already shown that a better modeling of background improves the retrieval performance, such as clusters [15, 10], neighbor documents [25], and aspects [8, 27].",
                "All the approaches can be easily adopted using Poisson language models.",
                "However, a common problem of these approaches is that they all require heavy computation to construct the background model.",
                "With Poisson language modeling, we show that it is possible to model the mixture background without paying for the heavy computational cost.",
                "Poisson Mixture [3] has been proposed to model a collection of documents, which can fit the data much better than a single Poisson.",
                "The basic idea is to assume that the collection is generated from a mixture of Poisson models, which has the general form of p(x = k|PM) = λ p(λ)p(x = k|λ)dλ p(·|λ) is a single Poisson model and p(λ) is an arbitrary probability density function.",
                "There are three well known Poisson mixtures [3]: 2-Poisson, Negative Binomial, and the Katzs K-Mixture [9].",
                "Note that the 2-Poisson model has actually been explored in probabilistic retrieval models, which led to the well-known BM25 formula [22].",
                "All these mixtures have closed forms, and can be estimated from the collection of documents efficiently.",
                "This is an advantage over the multinomial mixture models, such as PLSI [8] and LDA [1], for retrieval.",
                "For example, the probability density function of Katzs K-Mixture is given as p(c(w) = k|αw, βw) = (1 − αw)ηk,0 + αw βw + 1 ( βw βw + 1 )k where ηk,0 = 1 when k = 0, and 0 otherwise.",
                "With the observation of a collection of documents, αw and βw can be estimated as βw = cf(w) − df(w) df(w) and αw = cf(w) Nβw where cf(w) and df(w) are the collection frequency and document frequency of w, and N is the number of documents in the collection.",
                "To account for the different document lengths, we assume that βw is a reasonable estimation for generating a document of the average length, and use β = βw avdl |q| to generate the query.",
                "This Poisson mixture model can be easily used to replace P(·|C) in the retrieval functions 3 and 4. 3.4 Other Possible Flexibilities In addition to term dependent smoothing and efficient mixture background, a Poisson language model has also some other potential advantages.",
                "For example, in Section 2, we see that Formula 2 introduces a component which does document length penalization.",
                "Intuitively, when the document has more unique words, it will be penalized more.",
                "On the other hand, if a document is exactly n copies of another document, it would not get over penalized.",
                "This feature is desirable and not achieved with the Dirichlet model [5].",
                "Potentially, this component could penalize a document according to what types of terms it contains.",
                "With term specific settings of δ, we could get even more flexibility for document length normalization.",
                "Pseudo-feedback is yet another interesting direction where the Poission model might be able to show its advantage.",
                "With model-based feedback, we could again relax the combination coefficients of the feedback model and the background model, and allow different terms to contribute differently to the feedback model.",
                "We could also utilize the relevant documents to learn better per-term smoothing coefficients. 4.",
                "EVALUATION In Section 3, we analytically compared the Poisson language models and multinomial language models from the perspective of query generation and retrieval.",
                "In this section, we compare these two families of models empirically.",
                "Experiment results show that the Poisson model with perterm smoothing outperforms multinomial model, and the performance can be further improved with <br>two-stage smoothing</br>.",
                "Using Poisson mixture as background model also improves the retrieval performance. 4.1 Datasets Since retrieval performance could significantly vary from one test collection to another, and from one query to another, we select four representative TREC test collections: AP, Trec7, Trec8, and Wt2g(Web).",
                "To cover different types of queries, we follow [28, 5], and construct short-keyword (SK, keyword title), short-verbose (SV, one sentence description), and long-verbose (LV, multiple sentences) queries.",
                "The documents are stemmed with the Porters stemmer, and we do not remove any stop word.",
                "For each parameter, we vary its value to cover a reasonably wide range. 4.2 Comparison to Multinomial We compare the performance of the Poisson retrieval models and multinomial retrieval models using interpolation (JelinekMercer, JM) smoothing and Bayesian smoothing with conjugate priors.",
                "Table 1 shows that the two JM-smoothed models perform similarly on all data sets.",
                "Since the Dirichlet Smoothing for multinomial language model and the Gamma Smoothing for Poisson language model lead to the same retrieval formula, the performance of these two models are jointly presented.",
                "We see that Dirichlet/Gamma smoothing methods outperform both Jelinek-Mercer smoothing methods.",
                "The parameter sensitivity curves for two Jelinek-Mercer 0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1 0 0.05 0.1 0.15 0.2 0.25 0.3 Dataset: Trec8 Parameter: δ AveragePrecision JM−Multinomial: LV JM−Multinomial: SV JM−Multinomial: SK JM−Poisson: SK JM−Poisson: SV JM−Poisson: LV Figure 1: Poisson and multinomial performs similarly with Jelinek-Mercer smoothing smoothing methods are shown in Figure 1.",
                "Clearly, these two methods perform similarly either in terms of optimality Data Query JM-Multinomial JM-Poisson Dirichlet/Gamma Per-term 2-Stage Poisson MAP InitPr Pr@5d MAP InitPr Pr@5d MAP InitPr Pr@5d MAP InitPr Pr@5d AP88-89 SK 0.203 0.585 0.356 0.203 0.585 0.358 0.224 0.629 0.393 0.226 0.630 0.396 SV 0.187 0.580 0.361 0.183 0.571 0.345 0.204 0.613 0.387 0.217* 0.603 0.390 LV 0.283 0.716 0.480 0.271 0.692 0.470 0.291 0.710 0.496 0.304* 0.695 0.510 Trec7 SK 0.167 0.635 0.400 0.168 0.635 0.404 0.186 0.687 0.428 0.185 0.646 0.436 SV 0.174 0.655 0.432 0.176 0.653 0.432 0.182 0.666 0.432 0.196* 0.660 0.440 LV 0.223 0.730 0.496 0.215 0.766 0.488 0.224 0.748 0.52 0.236* 0.738 0.512 Trec8 SK 0.239 0.621 0.440 0.239 0.621 0.436 0.257 0.718 0.496 0.256 0.704 0.468 SV 0.231 0.686 0.448 0.234 0.702 0.456 0.228 0.691 0.456 0.246* 0.692 0.476 LV 0.265 0.796 0.548 0.261 0.757 0.520 0.260 0.741 0.492 0.274* 0.766 0.508 Web SK 0.250 0.616 0.380 0.250 0.616 0.380 0.302 0.767 0.468 0.307 0.739 0.468 SV 0.214 0.611 0.392 0.217 0.609 0.384 0.273 0.693 0.508 0.292* 0.703 0.480 LV 0.266 0.790 0.464 0.259 0.776 0.452 0.283 0.756 0.496 0.311* 0.759 0.488 Table 1: Performance comparison between Poisson and Multinomial retrieval models: basic models perform comparably; term dependent <br>two-stage smoothing</br> significantly improves Poisson An asterisk (*) indicates that the difference between the performance of the term dependent <br>two-stage smoothing</br> and that of the Dirichlet/Gamma single smoothing is statistically significant according to the Wilcoxon signed rank test at the level of 0.05. or sensitivity.",
                "This similarity of performance is expected as we discussed in Section 3.1.",
                "Although the Poisson model and multinomial model are similar in terms of the basic model and/or with simple smoothing methods, the Poisson model has great potential and flexibility to be further improved.",
                "As shown in the rightmost column of Table 1, term dependent two-stage Poisson model consistently outperforms the basic smoothing models, especially for verbose queries.",
                "This model is given in Formula 4, with a Gamma smoothing for the document model p(·|d), and δw, which is term dependent.",
                "The parameter µ of the first stage Gamma smoothing is empirically tuned.",
                "The combination coefficients (i.e., ∆), are estimated with the EM algorithm in Section 3.2.",
                "The parameter sensitivity curves for Dirichlet/Gamma and the per-term <br>two-stage smoothing</br> model are plotted in Figure 2.",
                "The per-term <br>two-stage smoothing</br> method is less sensitive to the parameter µ than Dirichlet/Gamma, and yields better optimal performance. 0 1000 2000 3000 4000 5000 6000 7000 8000 9000 10000 0.1 0.12 0.14 0.16 0.18 0.2 0.22 Dataset: AP; Query Type: SV Parameter: µ AveragePrecision Dirichlet/Gamma Smoothing Term Dependent 2−Stage Figure 2: Term dependent <br>two-stage smoothing</br> of Poisson outperforms Dirichlet/Gamma In the following subsections, we conduct experiments to demonstrate how the flexibility of the Poisson model could be utilized to achieve better performance, which we cannot achieve with multinomial language models. 4.3 Term Dependent Smoothing To test the effectiveness of the term dependent smoothing, we conduct the following two experiments.",
                "In the first experiment, we relax the constant coefficient in the simple Jelinek-Mercer smoothing formula (i.e., Formula 3), and use the EM algorithm proposed in Section 3.2 to find a δw for each unique term.",
                "Since we are using the EM algorithm to iteratively estimate the parameters, we usually do not want the probability of p(·|d) to be zero.",
                "We then use a simple Laplace method to slightly smooth the document model before it goes into the EM iterations.",
                "The documents are then still scored with Formula 3, but using learnt δw.",
                "The results are labeled with JM+L. in Table 2.",
                "Data Q JM JM JM+L. 2-Stage 2-Stage (MAP) PT: No Yes Yes No Yes AP SK 0.203 0.204 0.206 0.223 0.226* SV 0.183 0.189 0.214* 0.204 0.217* Trec7 SK 0.168 0.171 0.174 0.186 0.185 SV 0.176 0.147 0.198* 0.194 0.196 Trec8 SK 0.239 0.240 0.227* 0.257 0.256 SV 0.234 0.223 0.249* 0.242 0.246* Web SK 0.250 0.236 0.220* 0.291 0.307* SV 0.217 0.232 0.261* 0.273 0.292* Table 2: Term dependent smoothing improves retrieval performance An asterisk (*) in Column 3 indicates that the difference between the JM+L. method and JM method is statistically significant; an asterisk (*) in Column 5 means that the difference between term dependent two-stage method and query dependent two-stage method is statistically significant; PT stands for per-term.",
                "With term dependent coefficients, the performance of the Jelinek-Mercer Poisson model is improved in most cases.",
                "However, in some cases (e.g., Trec7/SV), it performs poorly.",
                "This might be caused by the problem of EM estimation with unsmoothed document models.",
                "Once non-zero probability is assigned to all the terms before entering the EM iteration, the performance on verbose queries can be improved significantly.",
                "This indicates that there is still room to find better methods to estimate δw.",
                "Please note that neither the perterm JM method nor the JM+L. method has a parameter to tune.",
                "As shown in Table 1, the term dependent <br>two-stage smoothing</br> can significantly improve retrieval performance.",
                "To understand whether the improvement is contributed by the term dependent smoothing or the <br>two-stage smoothing</br> framework, we design another experiment to compare the perterm <br>two-stage smoothing</br> with the two-stage smoothing method proposed in [29].",
                "Their method managed to find coefficients specific to the query, thus a verbose query would use a higher δ.",
                "However, since their model is based on multinomial language modeling, they could not get per-term coefficients.",
                "We adopt their method to the Poisson <br>two-stage smoothing</br>, and also estimate a per-query coefficient for all the terms.",
                "We compare the performance of such a model with the per-term <br>two-stage smoothing</br> model, and present the results in the right two columns in Table 2.",
                "Again, we see that the per-term <br>two-stage smoothing</br> outperforms the per-query <br>two-stage smoothing</br>, especially for verbose queries.",
                "The improvement is not as large as how the perterm smoothing method improves over Dirichlet/Gamma.",
                "This is expected, since the per-query smoothing has already addressed the query discrimination problem to some extent.",
                "This experiment shows that even if the smoothing is already per-query, making it per-term is still beneficial.",
                "In brief, the per-term smoothing improved the retrieval performance of both one-stage and <br>two-stage smoothing</br> method. 4.4 Mixture Background Model In this section, we conduct experiments to examine the benefits of using a mixture background model without extra computational cost, which can not be achieved for multinomial models.",
                "Specifically, in retrieval formula 3, instead of using a single Poisson distribution to model the background p(·|C), we use Katzs K-Mixture model, which is essentially a mixture of Poisson distributions. p(·|C) can be computed efficiently with simple collection statistics, as discussed in Section 3.3.",
                "Data Query JM.",
                "Poisson JM.",
                "K-Mixture AP SK 0.203 0.204 SV 0.183 0.188* Trec-7 SK 0.168 0.169 SV 0.176 0.178* Trec-8 SK 0.239 0.239 SV 0.234 0.238* Web SK 0.250 0.250 SV 0.217 0.223* Table 3: K-Mixture background model improves retrieval performance The performance of the JM retrieval model with single Poisson background and with Katzs K-Mixture background model is compared in Table 3.",
                "Clearly, using K-Mixture to model the background model outperforms the single Poisson background model in most cases, especially for verbose queries where the improvement is statistically significant.",
                "Figure 3 shows that the performance changes over different parameters for short verbose queries.",
                "The model using K-Mixture background is less sensitive than the one using single Poisson background.",
                "Given that this type of mixture 0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1 0 0.05 0.1 0.15 0.2 0.25 Data: Trec8; Query: SV Parameter: δ AveragePrecision Poisson Background K−Mixture Background Figure 3: K-Mixture background model deviates the sensitivity of verbose queries background model does not require any extra computation cost, it would be interesting to study whether using other mixture Poisson models, such as 2-Poisson and negative Binomial, could help the performance. 5.",
                "RELATED WORK To the best of our knowledge, there has been no study of query generation models based on Poisson distribution.",
                "Language models have been shown to be effective for many retrieval tasks [21, 28, 14, 4].",
                "The most popular and fundamental one is the query-generation language model [21, 13].",
                "All existing query generation language models are based on either multinomial distribution [19, 6, 28, 13] or multivariate Bernoulli distribution [21, 17, 18].",
                "We introduce a new family of language models, based on Poisson distribution.",
                "Poisson distribution has been previously studied in the document generation models [16, 22, 3, 24], leading to the development of one of the most effective retrieval formula BM25 [23]. [24] studies the parallel derivation of three different retrieval models which is related to our comparison of Poisson and multinomial.",
                "However, the Poisson model in their paper is still under the document generation framework, and also does not account for the document length variation. [26] introduces a way to empirically search for an exponential model for the documents.",
                "Poisson mixtures [3] such as 2-Poisson [22], Negative multinomial, and Katzs KMixture [9] has shown to be effective to model and retrieve documents.",
                "Once again, none of this work explores Poisson distribution in the query generation framework.",
                "Language model smoothing [2, 28, 29] and background structures [15, 10, 25, 27] have been studied with multinomial language models. [7] analytically shows that term specific smoothing could be useful.",
                "We show that Poisson language model is natural to accommodate the per-term smoothing without heuristic twist of the semantics of a generative model, and is able to efficiently better model the mixture background, both analytically and empirically. 6.",
                "CONCLUSIONS We present a new family of query generation language models for retrieval based on Poisson distribution.",
                "We derive several smoothing methods for this family of models, including single-stage smoothing and <br>two-stage smoothing</br>.",
                "We compare the new models with the popular multinomial retrieval models both analytically and experimentally.",
                "Our analysis shows that while our new models and multinomial models are equivalent under some assumptions, they are generally different with some important differences.",
                "In particular, we show that Poisson has an advantage over multinomial in naturally accommodating per-term smoothing.",
                "We exploit this property to develop a new per-term smoothing algorithm for Poisson language models, which is shown to outperform term-independent smoothing for both Poisson and multinomial models.",
                "Furthermore, we show that a mixture background model for Poisson can be used to improve the performance and robustness over the standard Poisson background model.",
                "Our work opens up many interesting directions for further exploration in this new family of models.",
                "Further exploring the flexibilities over multinomial language models, such as length normalization and pseudo-feedback could be good future work.",
                "It is also appealing to find robust methods to learn the per-term smoothing coefficients without additional computation cost. 7.",
                "ACKNOWLEDGMENTS We thank the anonymous SIGIR 07 reviewers for their useful comments.",
                "This material is based in part upon work supported by the National Science Foundation under award numbers IIS-0347933 and 0425852. 8.",
                "REFERENCES [1] D. Blei, A. Ng, and M. Jordan.",
                "Latent dirichlet allocation.",
                "Journal of Machine Learning Research, 3:993-1022, 2003. [2] S. F. Chen and J. Goodman.",
                "An empirical study of smoothing techniques for language modeling.",
                "Technical Report TR-10-98, Harvard University, 1998. [3] K. Church and W. Gale.",
                "Poisson mixtures.",
                "Nat.",
                "Lang.",
                "Eng., 1(2):163-190, 1995. [4] W. B. Croft and J. Lafferty, editors.",
                "Language Modeling and Information Retrieval.",
                "Kluwer Academic Publishers, 2003. [5] H. Fang, T. Tao, and C. Zhai.",
                "A formal study of information retrieval heuristics.",
                "In Proceedings of the 27th annual international ACM SIGIR conference on Research and development in information retrieval, pages 49-56, 2004. [6] D. Hiemstra.",
                "Using Language Models for Information Retrieval.",
                "PhD thesis, University of Twente, Enschede, Netherlands, 2001. [7] D. Hiemstra.",
                "Term-specific smoothing for the language modeling approach to information retrieval: the importance of a query term.",
                "In Proceedings of the 25th annual international ACM SIGIR conference on Research and development in information retrieval, pages 35-41, 2002. [8] T. Hofmann.",
                "Probabilistic latent semantic indexing.",
                "In Proceedings of ACM SIGIR99, pages 50-57, 1999. [9] S. M. Katz.",
                "Distribution of content words and phrases in text and language modelling.",
                "Nat.",
                "Lang.",
                "Eng., 2(1):15-59, 1996. [10] O. Kurland and L. Lee.",
                "Corpus structure, language models, and ad-hoc information retrieval.",
                "In Proceedings of the 27th annual international ACM SIGIR conference on Research and development in information retrieval, pages 194-201, 2004. [11] J. Lafferty and C. Zhai.",
                "Document language models, query models, and risk minimization for information retrieval.",
                "In Proceedings of SIGIR01, pages 111-119, Sept 2001. [12] J. Lafferty and C. Zhai.",
                "Probabilistic IR models based on query and document generation.",
                "In Proceedings of the Language Modeling and IR workshop, pages 1-5, May 31 - June 1 2001. [13] J. Lafferty and C. Zhai.",
                "Probabilistic relevance models based on document and query generation.",
                "In W. B. Croft and J. Lafferty, editors, Language Modeling and Information Retrieval.",
                "Kluwer Academic Publishers, 2003. [14] V. Lavrenko and B. Croft.",
                "Relevance-based language models.",
                "In Proceedings of SIGIR01, pages 120-127, Sept 2001. [15] X. Liu and W. B. Croft.",
                "Cluster-based retrieval using language models.",
                "In Proceedings of the 27th annual international ACM SIGIR conference on Research and development in information retrieval, pages 186-193, 2004. [16] E. L. Margulis.",
                "Modelling documents with multiple poisson distributions.",
                "Inf.",
                "Process.",
                "Manage., 29(2):215-227, 1993. [17] A. McCallum and K. Nigam.",
                "A comparison of event models for naive bayes text classification.",
                "In Proceedings of AAAI-98 Workshop on Learning for Text Categorization, 1998. [18] D. Metzler, V. Lavrenko, and W. B. Croft.",
                "Formal multiple-bernoulli models for language modeling.",
                "In Proceedings of the 27th annual international ACM SIGIR conference on Research and development in information retrieval, pages 540-541, 2004. [19] D. H. Miller, T. Leek, and R. Schwartz.",
                "A hidden Markov model information retrieval system.",
                "In Proceedings of the 1999 ACM SIGIR Conference on Research and Development in Information Retrieval, pages 214-221, 1999. [20] A. Papoulis.",
                "Probability, random variables and stochastic processes.",
                "New York: McGraw-Hill, 1984, 2nd ed., 1984. [21] J. M. Ponte and W. B. Croft.",
                "A language modeling approach to information retrieval.",
                "In Proceedings of the 21st annual international ACM SIGIR conference on Research and development in information retrieval, pages 275-281, 1998. [22] S. Robertson and S. Walker.",
                "Some simple effective approximations to the 2-poisson model for probabilistic weighted retrieval.",
                "In Proceedings of SIGIR94, pages 232-241, 1994. [23] S. E. Robertson, S. Walker, S. Jones, M. M.Hancock-Beaulieu, and M. Gatford.",
                "Okapi at TREC-3.",
                "In D. K. Harman, editor, The Third Text REtrieval Conference (TREC-3), pages 109-126, 1995. [24] T. Roelleke and J. Wang.",
                "A parallel derivation of probabilistic information retrieval models.",
                "In Proceedings of the 29th annual international ACM SIGIR conference on Research and development in information retrieval, pages 107-114, 2006. [25] T. Tao, X. Wang, Q. Mei, and C. Zhai.",
                "Language model information retrieval with document expansion.",
                "In Proceedings of HLT/NAACL 2006, pages 407-414, 2006. [26] J. Teevan and D. R. Karger.",
                "Empirical development of an exponential probabilistic model for text retrieval: using textual analysis to build a better model.",
                "In Proceedings of the 26th annual international ACM SIGIR conference on Research and development in informaion retrieval, pages 18-25, 2003. [27] X. Wei and W. B. Croft.",
                "Lda-based document models for ad-hoc retrieval.",
                "In Proceedings of the 29th annual international ACM SIGIR conference on Research and development in information retrieval, pages 178-185, 2006. [28] C. Zhai and J. Lafferty.",
                "A study of smoothing methods for language models applied to ad-hoc information retrieval.",
                "In Proceedings of ACM SIGIR01, pages 334-342, Sept 2001. [29] C. Zhai and J. Lafferty.",
                "Two-stage language models for information retrieval.",
                "In Proceedings of ACM SIGIR02, pages 49-56, Aug 2002."
            ],
            "original_annotated_samples": [],
            "translated_annotated_samples": [
                "El rendimiento se puede mejorar aún más con el \"suavizado de dos etapas\".suavizado de dos etapas",
                "Esta ventaja se ve tanto para una etapa como para el \"suavizado de dos etapas\".suavizado de dos etapas",
                "Como la segunda suma es difícil de calcular de manera eficiente, combinamos todos los términos no quirados como un pseudo no quirado, denotado como N. utilizando la formulación de pseudo a término y un modelo de colección de Poisson, podemos reescribir la fórmula de recuperación como puntaje (D, q) ∝ w∈D∩q log (1 + 1 - Δ δ e - λd, w (λd, w | q |) c (w, q) e - λd, c | q | (λd, c) c) c)(w, q)) + log (1 - δ) e - λd, n | q |+ ΔE - λc, n | Q |1 - Δ + ΔE - λc, n | Q |(3) donde λd, n = | d | - w∈Q c (w, d) | d |y λc, n = | c | - w∈Q c (w, c) | c |.2.2.3 \"suavizado de dos etapas\" como se discutió en [29], el suavizado juega dos roles en la recuperación: (1) para mejorar la estimación del modelo de lenguaje del documento y (2) para explicar los términos comunes en la consulta.suavizado de dos etapas",
                "Luego podemos obtener cada documento con la probabilidad de consulta calculada utilizando el siguiente modelo de \"suavizado de dos etapas\": P (C (W, Q) | λd, U) = (1-δ) P (C (W, Q) |Λd) + ΔP (c (w, q) | u) (4) donde δ es un parámetro, lo que indica aproximadamente la cantidad de ruido en q.suavizado de dos etapas",
                "Los resultados del experimento muestran que el modelo de Poisson con suavizado Perter supera el modelo multinomial, y el rendimiento puede mejorarse aún más con el \"suavizado de dos etapas\".suavizado de dos etapas",
                "Claramente, estos dos métodos funcionan de manera similar en términos de consulta de datos de optimización JM-Multinomial JM-Poisson Dirichlet/Gamma por período de 2 etapas Poisson Map initpr pr@5d Map initpr pr@5d Map Initpr Pr@5D Map PR@5D 5DAP88-89 SK 0.203 0.585 0.356 0.203 0.585 0.358 0.224 0.629 0.393 0.226 0.630 0.396 SV 0.187 0.580 0.361 0.183 0.571 0.345 0.204 0.613 0.387 0.217* 0.603 0.390 LV 0.283 470 0.291 0.710 0.496 0.304* 0.695 0.510 TREC7 SK 0.167 0.635 0.400 0.1680.635 0.404 0.186 0.687 0.428 0.185 0.646 0.436 SV 0.174 0.655 0.432 0.176 0.653 0.432 0.182 0.666 0.432 0.196* 0.660 0.440 VV 0.2223 0.730 0.496 0.215 0.766 0.488888. 738 0.512 TREC8 SK 0.239 0.621 0.440 0.239 0.621 0.436 0.257 0.718 0.496 0.256 0.704 0.468SV 0.231 0.686 0.448 0.234 0.702 0.456 0.228 0.691 0.456 0.246* 0.692 0.476 LV 0.265 0.796 0.548 0.261 0.757 0.520 0.260 0.741 0.492 0.274* 0.766 0.508 Web 0.650 0.650 302 0.767 0.468 0.307 0.739 0.468 SV 0.214 0.611 0.392 0.217 0.609 0.384 0.2730.693 0.508 0.292* 0.703 0.480 LV 0.266 0.790 0.464 0.259 0.776 0.452 0.283 0.756 0.496 0.311* 0.759 0.488 Tabla 1: Comparación de rendimiento entre Poisson y Modelos de recuperación multinomial: Modelos básicos funcionan comparablemente;Término dependiente \"suavizado de dos etapas\" mejora significativamente Poisson Un asterisco (*) indica que la diferencia entre el rendimiento del término dependiente \"suavizado de dos etapas\" y la del suavizado único de Dirichlet/gamma es estadísticamente significativa según el Wilcoxon firmadoPrueba de rango en el nivel de 0.05.o sensibilidad.suavizado de dos etapas",
                "Las curvas de sensibilidad de los parámetros para dirichlet/gamma y el modelo de \"suavizado de dos etapas\" por término se trazan en la Figura 2. Afisa de dos etapas",
                "El método de \"suavizado de dos etapas\" por término es menos sensible al parámetro µ que Dirichlet/gamma, y produce un mejor rendimiento óptimo.0 1000 2000 3000 4000 5000 6000 7000 8000 9000 10000 0.1 0.12 0.14 0.16 0.18 0.2 0.22 conjunto de datos: AP;Tipo de consulta: Parámetro SV: µ Precisión promedio Precisión Dirichlet/Gamma suave término dependiente 2-etapa Figura 2: Término dependiente \"suavizado de dos etapas\" de Poisson supera a Dirichlet/gamma en las siguientes subsecciones, realizamos experimentos para demostrar cómo la flexibilidad del Poisson del PoissonEl modelo podría utilizarse para lograr un mejor rendimiento, que no podemos lograr con modelos de lenguaje multinomial.4.3 suavizante dependiente del término Para probar la efectividad del suave dependiente del término, realizamos los siguientes dos experimentos.suavizado de dos etapas",
                "Como se muestra en la Tabla 1, el término suavizado de dos etapas dependiente puede mejorar significativamente el rendimiento de la recuperación.suavizado de dos etapas",
                "Para comprender si la mejora es contribuida por el término suavizado dependiente o el marco de \"suavizado de dos etapas\", diseñamos otro experimento para comparar el \"suavizado de dos etapas\" perterable con el método de suavizado de dos etapas propuesto en [29].suavizado de dos etapas",
                "Adoptamos su método para el \"suavizado de dos etapas\" de Poisson, y también estimamos un coeficiente por QUERERY para todos los términos.suavizado de dos etapas",
                "Comparamos el rendimiento de dicho modelo con el modelo de \"suavizado de dos etapas\" por término, y presentamos los resultados en las dos columnas correctas en la Tabla 2. Alisado en dos etapas",
                "Una vez más, vemos que el \"suavizado de dos etapas\" por período supera al \"suavizado de dos etapas\" por QUERERY, especialmente para consultas detalladas.suavizado de dos etapas",
                "En resumen, el suavizado por término mejoró el rendimiento de la recuperación del método de una etapa y \"suavizado de dos etapas\".4.4 Modelo de fondo de mezcla En esta sección, realizamos experimentos para examinar los beneficios de usar un modelo de fondo de mezcla sin un costo computacional adicional, que no se puede lograr para modelos multinomiales.suavizado de dos etapas",
                "Derivamos varios métodos de suavizado para esta familia de modelos, incluido el suavizado de una etapa y el \"suavizado de dos etapas\".suavizado de dos etapas"
            ],
            "translated_text": "",
            "candidates": [],
            "error": []
        },
        "multivariate bernoullus distribution": {
            "translated_key": "",
            "is_in_text": false,
            "original_annotated_sentences": [
                "A Study of Poisson Query Generation Model for Information Retrieval Qiaozhu Mei, Hui Fang, Chengxiang Zhai Department of Computer Science University of Illinois at Urbana-Champaign Urbana,IL 61801 {qmei2,hfang,czhai}@uiuc.edu ABSTRACT Many variants of language models have been proposed for information retrieval.",
                "Most existing models are based on multinomial distribution and would score documents based on query likelihood computed based on a query generation probabilistic model.",
                "In this paper, we propose and study a new family of query generation models based on Poisson distribution.",
                "We show that while in their simplest forms, the new family of models and the existing multinomial models are equivalent, they behave differently for many smoothing methods.",
                "We show that the Poisson model has several advantages over the multinomial model, including naturally accommodating per-term smoothing and allowing for more accurate background modeling.",
                "We present several variants of the new model corresponding to different smoothing methods, and evaluate them on four representative TREC test collections.",
                "The results show that while their basic models perform comparably, the Poisson model can outperform multinomial model with per-term smoothing.",
                "The performance can be further improved with two-stage smoothing.",
                "Categories and Subject Descriptors: H.3.3 [Information Search and Retrieval]: Retrieval Models General Terms: Algorithms 1.",
                "INTRODUCTION As a new type of probabilistic retrieval models, language models have been shown to be effective for many retrieval tasks [21, 28, 14, 4].",
                "Among many variants of language models proposed, the most popular and fundamental one is the query-generation language model [21, 13], which leads to the query-likelihood scoring method for ranking documents.",
                "In such a model, given a query q and a document d, we compute the likelihood of generating query q with a model estimated based on document d, i.e., the conditional probability p(q|d).",
                "We can then rank documents based on the likelihood of generating the query.",
                "Virtually all the existing query generation language models are based on either multinomial distribution [19, 6, 28] or multivariate Bernoulli distribution [21, 18].",
                "The multinomial distribution is especially popular and also shown to be quite effective.",
                "The heavy use of multinomial distribution is partly due to the fact that it has been successfully used in speech recognition, where multinomial distribution is a natural choice for modeling the occurrence of a particular word in a particular position in text.",
                "Compared with multivariate Bernoulli, multinomial distribution has the advantage of being able to model the frequency of terms in the query; in contrast, multivariate Bernoulli only models the presence and absence of query terms, thus cannot capture different frequencies of query terms.",
                "However, multivariate Bernoulli also has one potential advantage over multinomial from the viewpoint of retrieval: in a multinomial distribution, the probabilities of all the terms must sum to 1, making it hard to accommodate per-term smoothing, while in a multivariate Bernoulli, the presence probabilities of different terms are completely independent of each other, easily accommodating per-term smoothing and weighting.",
                "Note that term absence is also indirectly captured in a multinomial model through the constraint that all the term probabilities must sum to 1.",
                "In this paper, we propose and study a new family of query generation models based on the Poisson distribution.",
                "In this new family of models, we model the frequency of each term independently with a Poisson distribution.",
                "To score a document, we would first estimate a multivariate Poisson model based on the document, and then score it based on the likelihood of the query given by the estimated Poisson model.",
                "In some sense, the Poisson model combines the advantage of multinomial in modeling term frequency and the advantage of the multivariate Bernoulli in accommodating per-term smoothing.",
                "Indeed, similar to the multinomial distribution, the Poisson distribution models term frequencies, but without the constraint that all the term probabilities must sum to 1, and similar to multivariate Bernoulli, it models each term independently, thus can easily accommodate per-term smoothing.",
                "As in the existing work on multinomial language models, smoothing is critical for this new family of models.",
                "We derive several smoothing methods for Poisson model in parallel to those used for multinomial distributions, and compare the corresponding retrieval models with those based on multinomial distributions.",
                "We find that while with some smoothing methods, the new model and the multinomial model lead to exactly the same formula, with some other smoothing methods they diverge, and the Poisson model brings in more flexibility for smoothing.",
                "In particular, a key difference is that the Poisson model can naturally accommodate perterm smoothing, which is hard to achieve with a multinomial model without heuristic twist of the semantics of a generative model.",
                "We exploit this potential advantage to develop a new term-dependent smoothing algorithm for Poisson model and show that this new smoothing algorithm can improve performance over term-independent smoothing algorithms using either Poisson or multinomial model.",
                "This advantage is seen for both one-stage and two-stage smoothing.",
                "Another potential advantage of the Poisson model is that its corresponding background model for smoothing can be improved through using a mixture model that has a closed form formula.",
                "This new background model is shown to outperform the standard background model and reduce the sensitivity of retrieval performance to the smoothing parameter.",
                "The rest of the paper is organized as follows.",
                "In Section 2, we introduce the new family of query generation models with Poisson distribution, and present various smoothing methods which lead to different retrieval functions.",
                "In Section 3, we analytically compare the Poisson language model with the multinomial language model, from the perspective of retrieval.",
                "We then design empirical experiments to compare the two families of language models in Section 4.",
                "We discuss the related work in 5 and conclude in 6. 2.",
                "QUERY GENERATION WITH POISSON PROCESS In the query generation framework, a basic assumption is that a query is generated with a model estimated based on a document.",
                "In most existing work [12, 6, 28, 29], people assume that each query word is sampled independently from a multinomial distribution.",
                "Alternatively, we assume that a query is generated by sampling the frequency of words from a series of independent Poisson processes [20]. 2.1 The Generation Process Let V = {w1, ..., wn} be a vocabulary set.",
                "Let w be a piece of text composed by an author and c(w1), ..., c(wn) be a frequency vector representing w, where c(wi, w) is the frequency count of term wi in text w. In retrieval, w could be either a query or a document.",
                "We consider the frequency counts of the n unique terms in w as n different types of events, sampled from n independent homogeneous Poisson processes, respectively.",
                "Suppose t is the time period during which the author composed the text.",
                "With a homogeneous Poisson process, the frequency count of each event, i.e., the number of occurrences of wi, follows a Poisson distribution with associated parameter λit, where λi is a rate parameter characterizing the expected number of wi in a unit time.",
                "The probability density function of such a Poisson Distribution is given by P(c(wi, w) = k|λit) = e−λit (λit)k k!",
                "Without losing generality, we set t to the length of the text w (people write one word in a unit time), i.e., t = |w|.",
                "With n such independent Poisson processes, each explaining the generation of one term in the vocabulary, the likelihood of w to be generated from such Poisson processes can be written as p(w|Λ) = n i=1 p(c(wi, w)|Λ) = n i=1 e−λi·|w| (λi · |w|)c(wi,w) c(wi, w)! where Λ = {λ1, ..., λn} and |w| = n i=1 c(wi, w).",
                "We refer to these n independent Poisson processes with parameter Λ as a Poisson Language Model.",
                "Let D = {d1, ..., dm} be an observed set of document samples generated from the Poisson process above.",
                "The maximum likelihood estimate (MLE) of λi is ˆλi = d∈D c(wi, d) d∈D w ∈V c(w , d) Note that this MLE is different from the MLE for the Poisson distribution without considering the document lengths, which appears in [22, 24].",
                "Given a document d, we may estimate a Poisson language model Λd using d as a sample.",
                "The likelihood that a query q is generated from the document language model Λd can be written as p(q|d) = w∈V p(c(w, q)|Λd) (1) This representation is clearly different from the multinomial query generation model as (1) the likelihood includes all the terms in the vocabulary V , instead of only those appearing in q, and (2) instead of the appearance of terms, the event space of this model is the frequencies of each term.",
                "In practice, we have the flexibility to choose the vocabulary V .",
                "In one extreme, we can use the vocabulary of the whole collection.",
                "However, this may bring in noise and considerable computational cost.",
                "In the other extreme, we may focus on the terms in the query and ignore other terms, but some useful information may be lost by ignoring the nonquery terms.",
                "As a compromise, we may conflate all the non-query terms as one single pseudo term.",
                "In other words, we may assume that there is exactly one non-query term in the vocabulary for each query.",
                "In our experiments, we adopt this pseudo non-query term strategy.",
                "A document can be scored with the likelihood in Equation 1.",
                "However, if a query term is unseen in the document, the MLE of the Poisson distribution would assign zero probability to the term, causing the probability of the query to be zero.",
                "As in existing language modeling approaches, the main challenge of constructing a reasonable retrieval model is to find a smoothed language model for p(·|d). 2.2 Smoothing in Poisson Retrieval Model In general, we want to assign non-zero rates for the query terms that are not seen in document d. Many smoothing methods have been proposed for multinomial language models[2, 28, 29].",
                "In general, we have to discount the probabilities of some words seen in the text to leave some extra probability mass to assign to the unseen words.",
                "In Poisson language models, however, we do not have the same constraint as in a multinomial model (i.e., w∈V p(w|d) = 1).",
                "Thus we do not have to discount the probability of seen words in order to give a non-zero rate to an unseen word.",
                "Instead, we only need to guarantee that k=0,1,2,... p(c(w, d) = k|d) = 1.",
                "In this section, we introduce three different strategies to smooth a Poisson language model, and show how they lead to different retrieval functions. 2.2.1 Bayesian Smoothing using Gamma Prior Following the risk minimization framework in [11], we assume that a document is generated by the arrival of terms in a time period of |d| according to the document language model, which essentially consists of a vector of Poisson rates for each term, i.e., Λd = λd,1, ..., λd,|V | .",
                "A document is assumed to be generated from a potentially different model.",
                "Given a particular document d, we want to estimate Λd.",
                "The rate of a term is estimated independently of other terms.",
                "We use Bayesian estimation with the following Gamma prior, which has two parameters, α and β: Gamma(λ|α, β) = βα Γ(α) λα−1 e−βλ For each term w, the parameters αw and βw are chosen to be αw = µ ∗ λC,w and βw = µ, where µ is a parameter and λC,w is the rate of w estimated from some background language model, usually the collection language model.",
                "The posterior distribution of Λd is given by p(Λd|d, C) ∝ w∈V e−λw(|d|+µ) λ c(w,d)+µλC,w−1 w which is a product of |V | Gamma distributions with parameters c(w, d) + µλC,w and |d| + µ for each word w. Given that the Gamma mean is α β , we have ˆλd,w = λd,w λd,wp(λd,w|d, C)dλd,w = c(w, d) + µλC,w |d| + µ This is precisely the smoothed estimate of multinomial language model with Dirichlet prior [28]. 2.2.2 Interpolation (Jelinek-Mercer) Smoothing Another straightforward method is to decompose the query generation model as a mixture of two component models.",
                "One is the document language model estimated with maximum likelihood estimator, and the other is a model estimated from the collection background, p(·|C), which assigns non-zero rate to w. For example, we may use an interpolation coefficient between 0 and 1 (i.e., δ ∈ [0, 1]).",
                "With this simple interpolation, we can score a document with Score(d, q) = w∈V log((1 − δ)p(c(w, q)|d) + δp(c(w, q)|C)) (2) Using the maximum likelihood estimator for p(·|d), we have λd,w = c(w,d) |d| , thus Equation 2 becomes Score(d, q) ∝ w∈d∩q [log(1 + 1 − δ δ e−λd,w|q| (λd,w|q|)c(w,q) c(w, q)! · p(c(w, q)|C) ) − log (1 − δ)e−λd,w|q| + δp(c(w, q) = 0|C) 1 − δ + δp(c(w, q) = 0|C) ] + w∈d log (1 − δ)e−λd,w|q| + δp(c(w, q) = 0|C) 1 − δ + δp(c(w, q) = 0|C) We can also use a Poisson language model for p(·|C), or use some other frequency-based models.",
                "In the retrieval formula above, the first summation can be computed efficiently.",
                "The second summation can be actually treated as a document prior, which penalizes long documents.",
                "As the second summation is difficult to compute efficiently, we conflate all non-query terms as one pseudo non-queryterm, denoted as N. Using the pseudo-term formulation and a Poisson collection model, we can rewrite the retrieval formula as Score(d, q) ∝ w∈d∩q log(1 + 1 − δ δ e−λd,w (λd,w|q|)c(w,q) e−λd,C |q| (λd,C )c(w,q) ) + log (1 − δ)e−λd,N |q| + δe−λC,N |q| 1 − δ + δe−λC,N |q| (3) where λd,N = |d|− w∈q c(w,d) |d| and λC,N = |C|− w∈q c(w,C) |C| . 2.2.3 Two-Stage Smoothing As discussed in [29], smoothing plays two roles in retrieval: (1) to improve the estimation of the document language model, and (2) to explain the common terms in the query.",
                "In order to distinguish the content and non-discriminative words in a query, we follow [29] and assume that a query is generated by sampling from a two-component mixture of Poisson language models, with one component being the document model Λd and the other being a query background language model p(·|U). p(·|U) models the typical term frequencies in the users queries.",
                "We may then score each document with the query likelihood computed using the following two-stage smoothing model: p(c(w, q)|Λd, U) = (1 − δ)p(c(w, q)|Λd) + δp(c(w, q)|U) (4) where δ is a parameter, roughly indicating the amount of noise in q.",
                "This looks similar to the interpolation smoothing, except that p(·|Λd) now should be a smoothed language model, instead of the one estimated with MLE.",
                "With no prior knowledge on p(·|U), we could set it to p(·|C).",
                "Any smoothing methods for the document language model can be used to estimate p(·|d) such as the Gamma smoothing as discussed in Section 2.2.1.",
                "The empirical study of the smoothing methods is presented in Section 4. 3.",
                "ANALYSIS OF POISSON LANGUAGE MODEL From the previous section, we notice that the Poisson language model has a strong connection to the multinomial language model.",
                "This is expected since they both belong to the exponential family [26].",
                "However, there are many differences when these two families of models are applied with different smoothing methods.",
                "From the perspective of retrieval, will these two language models perform equivalently?",
                "If not, which model provides more benefits to retrieval, or provides flexibility which could lead to potential benefits?",
                "In this section, we analytically discuss the retrieval features of the Poisson language models, by comparing their behavior with that of the multinomial language models. 3.1 The Equivalence of Basic Models Let us begin with the assumption that all the query terms appear in every document.",
                "Under this assumption, no smoothing is needed.",
                "A document can be scored by the log likelihood of the query with the maximum likelihood estimate: Score(d, q) = w∈V log e−λd,w|q| (λd,w|q|)c(w,q) c(w, q)! (5) Using the MLE, we have λd,w = c(w,d) w∈V c(w,d) .",
                "Thus Score(d, q) ∝ c(w,q)>0 c(w, q) log c(w, d) w∈V c(w, d) This is exactly the log likelihood of the query if the document language model is a multinomial with maximum likelihood estimate.",
                "Indeed, even with Gamma smoothing, when plugging λd,w = c(w,d)+µλC,w |d|+µ and λC,w = c(w,C) |C| into Equation 5, it is easy to show that Score(d, q) ∝ w∈q∩d c(w, q) log(1 + c(w, d) µ · c(w,C) |C| ) + |q| log µ |d| + µ (6) which is exactly the Dirichlet retrieval formula in [28].",
                "Note that this equivalence holds only when the document length variation is modeled with Poisson process.",
                "This derivation indicates the equivalence of the basic Poisson and multinomial language models for retrieval.",
                "With other smoothing strategies, however, the two models would be different.",
                "Nevertheless, with this equivalence in basic models, we could expect that the Poisson language model performs comparably to the multinomial language model in retrieval, if only simple smoothing is explored.",
                "Based on this equivalence analysis, one may ask, why we should pursue the Poisson language model.",
                "In the following sections, we show that despite the equivalence in their basic models, the Poisson language model brings in extra flexibility for exploring advanced techniques on various retrieval features, which could not be achieved with multinomial language models. 3.2 Term Dependent Smoothing One flexibility of the Poisson language model is that it provides a natural framework to accommodate term dependent (per-term) smoothing.",
                "Existing work on language model smoothing has already shown that different types of queries should be smoothed differently according to how discriminative the query terms are. [7] also predicted that different terms should have a different smoothing weights.",
                "With multinomial query generation models, people usually use a single smoothing coefficient to control the combination of the document model and the background model [28, 29].",
                "This parameter can be made specific for different queries, but always has to be a constant for all the terms.",
                "This is mandatory since a multinomial language model has the constraint that w∈V p(w|d) = 1.",
                "However, from retrieval perspective, different terms may need to be smoothed differently even if they are in the same query.",
                "For example, a non-discriminative term (e.g., the, is) is expected to be explained more with the background model, while a content term (e.g., retrieval, bush) in the query should be explained with the document model.",
                "Therefore, a better way of smoothing would be to set the interpolation coefficient (i.e., δ in Formula 2 and Formula 3) specifically for each term.",
                "Since the Poisson language model does not have the sum-to-one constraint across terms, it can easily accommodate per-term smoothing without needing to heuristically twist the semantics of a generative model as in the case of multinomial language models.",
                "Below we present a possible way to explore term dependent smoothing with Poisson language models.",
                "Essentially, we want to use a term-specific smoothing coefficient δ in the linear combination, denoted as δw.",
                "This coefficient should intuitively be larger if w is a common word and smaller if it is a content word.",
                "The key problem is to find a method to assign reasonable values to δw.",
                "Empirical tuning is infeasible for so many parameters.",
                "We may instead estimate the parameters ∆ = {δ1, ..., δ|V |} by maximizing the likelihood of the query given the mixture model of p(q|ΛQ) and p(q|U), where ΛQ is the true query model to generate the query and p(q|U) is a query background model as discussed in Section 2.2.3.",
                "With the model p(q|ΛQ) hidden, the query likelihood is p(q|∆, U) = ΛQ w∈V ((1 − δw)p(c(w, q)|ΛQ) + δwp(c(w, q)|U))P(ΛQ|U)dΛQ If we have relevant documents for each query, we can approximate the query model space with the language models of all the relevant documents.",
                "Without relevant documents, we opt to approximate the query model space with the models of all the documents in the collection.",
                "Setting p(·|U) as p(·|C), the query likelihood becomes p(q|∆, U) = d∈C πd w∈V ((1−δw)p(c(w, q)|ˆΛd)+δwp(c(w, q)|C)) where πd = p(ˆΛd|U). p(·|ˆΛd) is an estimated Poisson language model for document d. If we have prior knowledge on p(ˆΛd|U), such as which documents are relevant to the query, we can set πd accordingly, because what we want is to find ∆ that can maximize the likelihood of the query given relevant documents.",
                "Without this prior knowledge, we can leave πd as free parameters, and use the EM algorithm to estimate πd and ∆.",
                "The updating functions are given as π (k+1) d = πd w∈V ((1 − δw)p(c(w, q)|ˆΛd) + δwp(c(w, q)|C)) d∈C πd w∈V ((1 − δw)p(c(w, q)|ˆΛd) + δwp(c(w, q)|C)) and δ (k+1) w = d∈C πd δwp(c(w, q)|C)) (1 − δw)p(c(w, q)|ˆΛd) + δwp(c(w, q)|C)) As discussed in [29], we only need to run the EM algorithm for several iterations, thus the computational cost is relatively low.",
                "We again assume our vocabulary containing all query terms plus a pseudo non-query term.",
                "Note that the function does not give an explicit way of estimating the coefficient for the unseen non-query term.",
                "In our experiments, we set it to the average over δw of all query terms.",
                "With this flexibility, we expect Poisson language models could improve the retrieval performance, especially for verbose queries, where the query terms have various discriminative values.",
                "In Section 4, we use empirical experiments to prove this hypothesis. 3.3 Mixture Background Models Another flexibility is to explore different background (collection) models (i.e., p(·|U), or p(·|C)).",
                "One common assumption made in language modeling information retrieval is that the background model is a homogeneous model of the document models [28, 29].",
                "Similarly, we can also make the assumption that the collection model is a Poisson language model, with the rates λC,w = d∈C c(w,d) |C| .",
                "However, this assumption usually does not hold, since the collection is far more complex than a single document.",
                "Indeed, the collection usually consists of a mixture of documents with various genres, authors, and topics, etc.",
                "Treating the collection model as a mixture of document models, instead of a single pseudo-document model is more reasonable.",
                "Existing work of multinomial language modeling has already shown that a better modeling of background improves the retrieval performance, such as clusters [15, 10], neighbor documents [25], and aspects [8, 27].",
                "All the approaches can be easily adopted using Poisson language models.",
                "However, a common problem of these approaches is that they all require heavy computation to construct the background model.",
                "With Poisson language modeling, we show that it is possible to model the mixture background without paying for the heavy computational cost.",
                "Poisson Mixture [3] has been proposed to model a collection of documents, which can fit the data much better than a single Poisson.",
                "The basic idea is to assume that the collection is generated from a mixture of Poisson models, which has the general form of p(x = k|PM) = λ p(λ)p(x = k|λ)dλ p(·|λ) is a single Poisson model and p(λ) is an arbitrary probability density function.",
                "There are three well known Poisson mixtures [3]: 2-Poisson, Negative Binomial, and the Katzs K-Mixture [9].",
                "Note that the 2-Poisson model has actually been explored in probabilistic retrieval models, which led to the well-known BM25 formula [22].",
                "All these mixtures have closed forms, and can be estimated from the collection of documents efficiently.",
                "This is an advantage over the multinomial mixture models, such as PLSI [8] and LDA [1], for retrieval.",
                "For example, the probability density function of Katzs K-Mixture is given as p(c(w) = k|αw, βw) = (1 − αw)ηk,0 + αw βw + 1 ( βw βw + 1 )k where ηk,0 = 1 when k = 0, and 0 otherwise.",
                "With the observation of a collection of documents, αw and βw can be estimated as βw = cf(w) − df(w) df(w) and αw = cf(w) Nβw where cf(w) and df(w) are the collection frequency and document frequency of w, and N is the number of documents in the collection.",
                "To account for the different document lengths, we assume that βw is a reasonable estimation for generating a document of the average length, and use β = βw avdl |q| to generate the query.",
                "This Poisson mixture model can be easily used to replace P(·|C) in the retrieval functions 3 and 4. 3.4 Other Possible Flexibilities In addition to term dependent smoothing and efficient mixture background, a Poisson language model has also some other potential advantages.",
                "For example, in Section 2, we see that Formula 2 introduces a component which does document length penalization.",
                "Intuitively, when the document has more unique words, it will be penalized more.",
                "On the other hand, if a document is exactly n copies of another document, it would not get over penalized.",
                "This feature is desirable and not achieved with the Dirichlet model [5].",
                "Potentially, this component could penalize a document according to what types of terms it contains.",
                "With term specific settings of δ, we could get even more flexibility for document length normalization.",
                "Pseudo-feedback is yet another interesting direction where the Poission model might be able to show its advantage.",
                "With model-based feedback, we could again relax the combination coefficients of the feedback model and the background model, and allow different terms to contribute differently to the feedback model.",
                "We could also utilize the relevant documents to learn better per-term smoothing coefficients. 4.",
                "EVALUATION In Section 3, we analytically compared the Poisson language models and multinomial language models from the perspective of query generation and retrieval.",
                "In this section, we compare these two families of models empirically.",
                "Experiment results show that the Poisson model with perterm smoothing outperforms multinomial model, and the performance can be further improved with two-stage smoothing.",
                "Using Poisson mixture as background model also improves the retrieval performance. 4.1 Datasets Since retrieval performance could significantly vary from one test collection to another, and from one query to another, we select four representative TREC test collections: AP, Trec7, Trec8, and Wt2g(Web).",
                "To cover different types of queries, we follow [28, 5], and construct short-keyword (SK, keyword title), short-verbose (SV, one sentence description), and long-verbose (LV, multiple sentences) queries.",
                "The documents are stemmed with the Porters stemmer, and we do not remove any stop word.",
                "For each parameter, we vary its value to cover a reasonably wide range. 4.2 Comparison to Multinomial We compare the performance of the Poisson retrieval models and multinomial retrieval models using interpolation (JelinekMercer, JM) smoothing and Bayesian smoothing with conjugate priors.",
                "Table 1 shows that the two JM-smoothed models perform similarly on all data sets.",
                "Since the Dirichlet Smoothing for multinomial language model and the Gamma Smoothing for Poisson language model lead to the same retrieval formula, the performance of these two models are jointly presented.",
                "We see that Dirichlet/Gamma smoothing methods outperform both Jelinek-Mercer smoothing methods.",
                "The parameter sensitivity curves for two Jelinek-Mercer 0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1 0 0.05 0.1 0.15 0.2 0.25 0.3 Dataset: Trec8 Parameter: δ AveragePrecision JM−Multinomial: LV JM−Multinomial: SV JM−Multinomial: SK JM−Poisson: SK JM−Poisson: SV JM−Poisson: LV Figure 1: Poisson and multinomial performs similarly with Jelinek-Mercer smoothing smoothing methods are shown in Figure 1.",
                "Clearly, these two methods perform similarly either in terms of optimality Data Query JM-Multinomial JM-Poisson Dirichlet/Gamma Per-term 2-Stage Poisson MAP InitPr Pr@5d MAP InitPr Pr@5d MAP InitPr Pr@5d MAP InitPr Pr@5d AP88-89 SK 0.203 0.585 0.356 0.203 0.585 0.358 0.224 0.629 0.393 0.226 0.630 0.396 SV 0.187 0.580 0.361 0.183 0.571 0.345 0.204 0.613 0.387 0.217* 0.603 0.390 LV 0.283 0.716 0.480 0.271 0.692 0.470 0.291 0.710 0.496 0.304* 0.695 0.510 Trec7 SK 0.167 0.635 0.400 0.168 0.635 0.404 0.186 0.687 0.428 0.185 0.646 0.436 SV 0.174 0.655 0.432 0.176 0.653 0.432 0.182 0.666 0.432 0.196* 0.660 0.440 LV 0.223 0.730 0.496 0.215 0.766 0.488 0.224 0.748 0.52 0.236* 0.738 0.512 Trec8 SK 0.239 0.621 0.440 0.239 0.621 0.436 0.257 0.718 0.496 0.256 0.704 0.468 SV 0.231 0.686 0.448 0.234 0.702 0.456 0.228 0.691 0.456 0.246* 0.692 0.476 LV 0.265 0.796 0.548 0.261 0.757 0.520 0.260 0.741 0.492 0.274* 0.766 0.508 Web SK 0.250 0.616 0.380 0.250 0.616 0.380 0.302 0.767 0.468 0.307 0.739 0.468 SV 0.214 0.611 0.392 0.217 0.609 0.384 0.273 0.693 0.508 0.292* 0.703 0.480 LV 0.266 0.790 0.464 0.259 0.776 0.452 0.283 0.756 0.496 0.311* 0.759 0.488 Table 1: Performance comparison between Poisson and Multinomial retrieval models: basic models perform comparably; term dependent two-stage smoothing significantly improves Poisson An asterisk (*) indicates that the difference between the performance of the term dependent two-stage smoothing and that of the Dirichlet/Gamma single smoothing is statistically significant according to the Wilcoxon signed rank test at the level of 0.05. or sensitivity.",
                "This similarity of performance is expected as we discussed in Section 3.1.",
                "Although the Poisson model and multinomial model are similar in terms of the basic model and/or with simple smoothing methods, the Poisson model has great potential and flexibility to be further improved.",
                "As shown in the rightmost column of Table 1, term dependent two-stage Poisson model consistently outperforms the basic smoothing models, especially for verbose queries.",
                "This model is given in Formula 4, with a Gamma smoothing for the document model p(·|d), and δw, which is term dependent.",
                "The parameter µ of the first stage Gamma smoothing is empirically tuned.",
                "The combination coefficients (i.e., ∆), are estimated with the EM algorithm in Section 3.2.",
                "The parameter sensitivity curves for Dirichlet/Gamma and the per-term two-stage smoothing model are plotted in Figure 2.",
                "The per-term two-stage smoothing method is less sensitive to the parameter µ than Dirichlet/Gamma, and yields better optimal performance. 0 1000 2000 3000 4000 5000 6000 7000 8000 9000 10000 0.1 0.12 0.14 0.16 0.18 0.2 0.22 Dataset: AP; Query Type: SV Parameter: µ AveragePrecision Dirichlet/Gamma Smoothing Term Dependent 2−Stage Figure 2: Term dependent two-stage smoothing of Poisson outperforms Dirichlet/Gamma In the following subsections, we conduct experiments to demonstrate how the flexibility of the Poisson model could be utilized to achieve better performance, which we cannot achieve with multinomial language models. 4.3 Term Dependent Smoothing To test the effectiveness of the term dependent smoothing, we conduct the following two experiments.",
                "In the first experiment, we relax the constant coefficient in the simple Jelinek-Mercer smoothing formula (i.e., Formula 3), and use the EM algorithm proposed in Section 3.2 to find a δw for each unique term.",
                "Since we are using the EM algorithm to iteratively estimate the parameters, we usually do not want the probability of p(·|d) to be zero.",
                "We then use a simple Laplace method to slightly smooth the document model before it goes into the EM iterations.",
                "The documents are then still scored with Formula 3, but using learnt δw.",
                "The results are labeled with JM+L. in Table 2.",
                "Data Q JM JM JM+L. 2-Stage 2-Stage (MAP) PT: No Yes Yes No Yes AP SK 0.203 0.204 0.206 0.223 0.226* SV 0.183 0.189 0.214* 0.204 0.217* Trec7 SK 0.168 0.171 0.174 0.186 0.185 SV 0.176 0.147 0.198* 0.194 0.196 Trec8 SK 0.239 0.240 0.227* 0.257 0.256 SV 0.234 0.223 0.249* 0.242 0.246* Web SK 0.250 0.236 0.220* 0.291 0.307* SV 0.217 0.232 0.261* 0.273 0.292* Table 2: Term dependent smoothing improves retrieval performance An asterisk (*) in Column 3 indicates that the difference between the JM+L. method and JM method is statistically significant; an asterisk (*) in Column 5 means that the difference between term dependent two-stage method and query dependent two-stage method is statistically significant; PT stands for per-term.",
                "With term dependent coefficients, the performance of the Jelinek-Mercer Poisson model is improved in most cases.",
                "However, in some cases (e.g., Trec7/SV), it performs poorly.",
                "This might be caused by the problem of EM estimation with unsmoothed document models.",
                "Once non-zero probability is assigned to all the terms before entering the EM iteration, the performance on verbose queries can be improved significantly.",
                "This indicates that there is still room to find better methods to estimate δw.",
                "Please note that neither the perterm JM method nor the JM+L. method has a parameter to tune.",
                "As shown in Table 1, the term dependent two-stage smoothing can significantly improve retrieval performance.",
                "To understand whether the improvement is contributed by the term dependent smoothing or the two-stage smoothing framework, we design another experiment to compare the perterm two-stage smoothing with the two-stage smoothing method proposed in [29].",
                "Their method managed to find coefficients specific to the query, thus a verbose query would use a higher δ.",
                "However, since their model is based on multinomial language modeling, they could not get per-term coefficients.",
                "We adopt their method to the Poisson two-stage smoothing, and also estimate a per-query coefficient for all the terms.",
                "We compare the performance of such a model with the per-term two-stage smoothing model, and present the results in the right two columns in Table 2.",
                "Again, we see that the per-term two-stage smoothing outperforms the per-query two-stage smoothing, especially for verbose queries.",
                "The improvement is not as large as how the perterm smoothing method improves over Dirichlet/Gamma.",
                "This is expected, since the per-query smoothing has already addressed the query discrimination problem to some extent.",
                "This experiment shows that even if the smoothing is already per-query, making it per-term is still beneficial.",
                "In brief, the per-term smoothing improved the retrieval performance of both one-stage and two-stage smoothing method. 4.4 Mixture Background Model In this section, we conduct experiments to examine the benefits of using a mixture background model without extra computational cost, which can not be achieved for multinomial models.",
                "Specifically, in retrieval formula 3, instead of using a single Poisson distribution to model the background p(·|C), we use Katzs K-Mixture model, which is essentially a mixture of Poisson distributions. p(·|C) can be computed efficiently with simple collection statistics, as discussed in Section 3.3.",
                "Data Query JM.",
                "Poisson JM.",
                "K-Mixture AP SK 0.203 0.204 SV 0.183 0.188* Trec-7 SK 0.168 0.169 SV 0.176 0.178* Trec-8 SK 0.239 0.239 SV 0.234 0.238* Web SK 0.250 0.250 SV 0.217 0.223* Table 3: K-Mixture background model improves retrieval performance The performance of the JM retrieval model with single Poisson background and with Katzs K-Mixture background model is compared in Table 3.",
                "Clearly, using K-Mixture to model the background model outperforms the single Poisson background model in most cases, especially for verbose queries where the improvement is statistically significant.",
                "Figure 3 shows that the performance changes over different parameters for short verbose queries.",
                "The model using K-Mixture background is less sensitive than the one using single Poisson background.",
                "Given that this type of mixture 0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1 0 0.05 0.1 0.15 0.2 0.25 Data: Trec8; Query: SV Parameter: δ AveragePrecision Poisson Background K−Mixture Background Figure 3: K-Mixture background model deviates the sensitivity of verbose queries background model does not require any extra computation cost, it would be interesting to study whether using other mixture Poisson models, such as 2-Poisson and negative Binomial, could help the performance. 5.",
                "RELATED WORK To the best of our knowledge, there has been no study of query generation models based on Poisson distribution.",
                "Language models have been shown to be effective for many retrieval tasks [21, 28, 14, 4].",
                "The most popular and fundamental one is the query-generation language model [21, 13].",
                "All existing query generation language models are based on either multinomial distribution [19, 6, 28, 13] or multivariate Bernoulli distribution [21, 17, 18].",
                "We introduce a new family of language models, based on Poisson distribution.",
                "Poisson distribution has been previously studied in the document generation models [16, 22, 3, 24], leading to the development of one of the most effective retrieval formula BM25 [23]. [24] studies the parallel derivation of three different retrieval models which is related to our comparison of Poisson and multinomial.",
                "However, the Poisson model in their paper is still under the document generation framework, and also does not account for the document length variation. [26] introduces a way to empirically search for an exponential model for the documents.",
                "Poisson mixtures [3] such as 2-Poisson [22], Negative multinomial, and Katzs KMixture [9] has shown to be effective to model and retrieve documents.",
                "Once again, none of this work explores Poisson distribution in the query generation framework.",
                "Language model smoothing [2, 28, 29] and background structures [15, 10, 25, 27] have been studied with multinomial language models. [7] analytically shows that term specific smoothing could be useful.",
                "We show that Poisson language model is natural to accommodate the per-term smoothing without heuristic twist of the semantics of a generative model, and is able to efficiently better model the mixture background, both analytically and empirically. 6.",
                "CONCLUSIONS We present a new family of query generation language models for retrieval based on Poisson distribution.",
                "We derive several smoothing methods for this family of models, including single-stage smoothing and two-stage smoothing.",
                "We compare the new models with the popular multinomial retrieval models both analytically and experimentally.",
                "Our analysis shows that while our new models and multinomial models are equivalent under some assumptions, they are generally different with some important differences.",
                "In particular, we show that Poisson has an advantage over multinomial in naturally accommodating per-term smoothing.",
                "We exploit this property to develop a new per-term smoothing algorithm for Poisson language models, which is shown to outperform term-independent smoothing for both Poisson and multinomial models.",
                "Furthermore, we show that a mixture background model for Poisson can be used to improve the performance and robustness over the standard Poisson background model.",
                "Our work opens up many interesting directions for further exploration in this new family of models.",
                "Further exploring the flexibilities over multinomial language models, such as length normalization and pseudo-feedback could be good future work.",
                "It is also appealing to find robust methods to learn the per-term smoothing coefficients without additional computation cost. 7.",
                "ACKNOWLEDGMENTS We thank the anonymous SIGIR 07 reviewers for their useful comments.",
                "This material is based in part upon work supported by the National Science Foundation under award numbers IIS-0347933 and 0425852. 8.",
                "REFERENCES [1] D. Blei, A. Ng, and M. Jordan.",
                "Latent dirichlet allocation.",
                "Journal of Machine Learning Research, 3:993-1022, 2003. [2] S. F. Chen and J. Goodman.",
                "An empirical study of smoothing techniques for language modeling.",
                "Technical Report TR-10-98, Harvard University, 1998. [3] K. Church and W. Gale.",
                "Poisson mixtures.",
                "Nat.",
                "Lang.",
                "Eng., 1(2):163-190, 1995. [4] W. B. Croft and J. Lafferty, editors.",
                "Language Modeling and Information Retrieval.",
                "Kluwer Academic Publishers, 2003. [5] H. Fang, T. Tao, and C. Zhai.",
                "A formal study of information retrieval heuristics.",
                "In Proceedings of the 27th annual international ACM SIGIR conference on Research and development in information retrieval, pages 49-56, 2004. [6] D. Hiemstra.",
                "Using Language Models for Information Retrieval.",
                "PhD thesis, University of Twente, Enschede, Netherlands, 2001. [7] D. Hiemstra.",
                "Term-specific smoothing for the language modeling approach to information retrieval: the importance of a query term.",
                "In Proceedings of the 25th annual international ACM SIGIR conference on Research and development in information retrieval, pages 35-41, 2002. [8] T. Hofmann.",
                "Probabilistic latent semantic indexing.",
                "In Proceedings of ACM SIGIR99, pages 50-57, 1999. [9] S. M. Katz.",
                "Distribution of content words and phrases in text and language modelling.",
                "Nat.",
                "Lang.",
                "Eng., 2(1):15-59, 1996. [10] O. Kurland and L. Lee.",
                "Corpus structure, language models, and ad-hoc information retrieval.",
                "In Proceedings of the 27th annual international ACM SIGIR conference on Research and development in information retrieval, pages 194-201, 2004. [11] J. Lafferty and C. Zhai.",
                "Document language models, query models, and risk minimization for information retrieval.",
                "In Proceedings of SIGIR01, pages 111-119, Sept 2001. [12] J. Lafferty and C. Zhai.",
                "Probabilistic IR models based on query and document generation.",
                "In Proceedings of the Language Modeling and IR workshop, pages 1-5, May 31 - June 1 2001. [13] J. Lafferty and C. Zhai.",
                "Probabilistic relevance models based on document and query generation.",
                "In W. B. Croft and J. Lafferty, editors, Language Modeling and Information Retrieval.",
                "Kluwer Academic Publishers, 2003. [14] V. Lavrenko and B. Croft.",
                "Relevance-based language models.",
                "In Proceedings of SIGIR01, pages 120-127, Sept 2001. [15] X. Liu and W. B. Croft.",
                "Cluster-based retrieval using language models.",
                "In Proceedings of the 27th annual international ACM SIGIR conference on Research and development in information retrieval, pages 186-193, 2004. [16] E. L. Margulis.",
                "Modelling documents with multiple poisson distributions.",
                "Inf.",
                "Process.",
                "Manage., 29(2):215-227, 1993. [17] A. McCallum and K. Nigam.",
                "A comparison of event models for naive bayes text classification.",
                "In Proceedings of AAAI-98 Workshop on Learning for Text Categorization, 1998. [18] D. Metzler, V. Lavrenko, and W. B. Croft.",
                "Formal multiple-bernoulli models for language modeling.",
                "In Proceedings of the 27th annual international ACM SIGIR conference on Research and development in information retrieval, pages 540-541, 2004. [19] D. H. Miller, T. Leek, and R. Schwartz.",
                "A hidden Markov model information retrieval system.",
                "In Proceedings of the 1999 ACM SIGIR Conference on Research and Development in Information Retrieval, pages 214-221, 1999. [20] A. Papoulis.",
                "Probability, random variables and stochastic processes.",
                "New York: McGraw-Hill, 1984, 2nd ed., 1984. [21] J. M. Ponte and W. B. Croft.",
                "A language modeling approach to information retrieval.",
                "In Proceedings of the 21st annual international ACM SIGIR conference on Research and development in information retrieval, pages 275-281, 1998. [22] S. Robertson and S. Walker.",
                "Some simple effective approximations to the 2-poisson model for probabilistic weighted retrieval.",
                "In Proceedings of SIGIR94, pages 232-241, 1994. [23] S. E. Robertson, S. Walker, S. Jones, M. M.Hancock-Beaulieu, and M. Gatford.",
                "Okapi at TREC-3.",
                "In D. K. Harman, editor, The Third Text REtrieval Conference (TREC-3), pages 109-126, 1995. [24] T. Roelleke and J. Wang.",
                "A parallel derivation of probabilistic information retrieval models.",
                "In Proceedings of the 29th annual international ACM SIGIR conference on Research and development in information retrieval, pages 107-114, 2006. [25] T. Tao, X. Wang, Q. Mei, and C. Zhai.",
                "Language model information retrieval with document expansion.",
                "In Proceedings of HLT/NAACL 2006, pages 407-414, 2006. [26] J. Teevan and D. R. Karger.",
                "Empirical development of an exponential probabilistic model for text retrieval: using textual analysis to build a better model.",
                "In Proceedings of the 26th annual international ACM SIGIR conference on Research and development in informaion retrieval, pages 18-25, 2003. [27] X. Wei and W. B. Croft.",
                "Lda-based document models for ad-hoc retrieval.",
                "In Proceedings of the 29th annual international ACM SIGIR conference on Research and development in information retrieval, pages 178-185, 2006. [28] C. Zhai and J. Lafferty.",
                "A study of smoothing methods for language models applied to ad-hoc information retrieval.",
                "In Proceedings of ACM SIGIR01, pages 334-342, Sept 2001. [29] C. Zhai and J. Lafferty.",
                "Two-stage language models for information retrieval.",
                "In Proceedings of ACM SIGIR02, pages 49-56, Aug 2002."
            ],
            "original_annotated_samples": [],
            "translated_annotated_samples": [],
            "translated_text": "",
            "candidates": [],
            "error": []
        },
        "speech recognition": {
            "translated_key": "",
            "is_in_text": true,
            "original_annotated_sentences": [
                "A Study of Poisson Query Generation Model for Information Retrieval Qiaozhu Mei, Hui Fang, Chengxiang Zhai Department of Computer Science University of Illinois at Urbana-Champaign Urbana,IL 61801 {qmei2,hfang,czhai}@uiuc.edu ABSTRACT Many variants of language models have been proposed for information retrieval.",
                "Most existing models are based on multinomial distribution and would score documents based on query likelihood computed based on a query generation probabilistic model.",
                "In this paper, we propose and study a new family of query generation models based on Poisson distribution.",
                "We show that while in their simplest forms, the new family of models and the existing multinomial models are equivalent, they behave differently for many smoothing methods.",
                "We show that the Poisson model has several advantages over the multinomial model, including naturally accommodating per-term smoothing and allowing for more accurate background modeling.",
                "We present several variants of the new model corresponding to different smoothing methods, and evaluate them on four representative TREC test collections.",
                "The results show that while their basic models perform comparably, the Poisson model can outperform multinomial model with per-term smoothing.",
                "The performance can be further improved with two-stage smoothing.",
                "Categories and Subject Descriptors: H.3.3 [Information Search and Retrieval]: Retrieval Models General Terms: Algorithms 1.",
                "INTRODUCTION As a new type of probabilistic retrieval models, language models have been shown to be effective for many retrieval tasks [21, 28, 14, 4].",
                "Among many variants of language models proposed, the most popular and fundamental one is the query-generation language model [21, 13], which leads to the query-likelihood scoring method for ranking documents.",
                "In such a model, given a query q and a document d, we compute the likelihood of generating query q with a model estimated based on document d, i.e., the conditional probability p(q|d).",
                "We can then rank documents based on the likelihood of generating the query.",
                "Virtually all the existing query generation language models are based on either multinomial distribution [19, 6, 28] or multivariate Bernoulli distribution [21, 18].",
                "The multinomial distribution is especially popular and also shown to be quite effective.",
                "The heavy use of multinomial distribution is partly due to the fact that it has been successfully used in <br>speech recognition</br>, where multinomial distribution is a natural choice for modeling the occurrence of a particular word in a particular position in text.",
                "Compared with multivariate Bernoulli, multinomial distribution has the advantage of being able to model the frequency of terms in the query; in contrast, multivariate Bernoulli only models the presence and absence of query terms, thus cannot capture different frequencies of query terms.",
                "However, multivariate Bernoulli also has one potential advantage over multinomial from the viewpoint of retrieval: in a multinomial distribution, the probabilities of all the terms must sum to 1, making it hard to accommodate per-term smoothing, while in a multivariate Bernoulli, the presence probabilities of different terms are completely independent of each other, easily accommodating per-term smoothing and weighting.",
                "Note that term absence is also indirectly captured in a multinomial model through the constraint that all the term probabilities must sum to 1.",
                "In this paper, we propose and study a new family of query generation models based on the Poisson distribution.",
                "In this new family of models, we model the frequency of each term independently with a Poisson distribution.",
                "To score a document, we would first estimate a multivariate Poisson model based on the document, and then score it based on the likelihood of the query given by the estimated Poisson model.",
                "In some sense, the Poisson model combines the advantage of multinomial in modeling term frequency and the advantage of the multivariate Bernoulli in accommodating per-term smoothing.",
                "Indeed, similar to the multinomial distribution, the Poisson distribution models term frequencies, but without the constraint that all the term probabilities must sum to 1, and similar to multivariate Bernoulli, it models each term independently, thus can easily accommodate per-term smoothing.",
                "As in the existing work on multinomial language models, smoothing is critical for this new family of models.",
                "We derive several smoothing methods for Poisson model in parallel to those used for multinomial distributions, and compare the corresponding retrieval models with those based on multinomial distributions.",
                "We find that while with some smoothing methods, the new model and the multinomial model lead to exactly the same formula, with some other smoothing methods they diverge, and the Poisson model brings in more flexibility for smoothing.",
                "In particular, a key difference is that the Poisson model can naturally accommodate perterm smoothing, which is hard to achieve with a multinomial model without heuristic twist of the semantics of a generative model.",
                "We exploit this potential advantage to develop a new term-dependent smoothing algorithm for Poisson model and show that this new smoothing algorithm can improve performance over term-independent smoothing algorithms using either Poisson or multinomial model.",
                "This advantage is seen for both one-stage and two-stage smoothing.",
                "Another potential advantage of the Poisson model is that its corresponding background model for smoothing can be improved through using a mixture model that has a closed form formula.",
                "This new background model is shown to outperform the standard background model and reduce the sensitivity of retrieval performance to the smoothing parameter.",
                "The rest of the paper is organized as follows.",
                "In Section 2, we introduce the new family of query generation models with Poisson distribution, and present various smoothing methods which lead to different retrieval functions.",
                "In Section 3, we analytically compare the Poisson language model with the multinomial language model, from the perspective of retrieval.",
                "We then design empirical experiments to compare the two families of language models in Section 4.",
                "We discuss the related work in 5 and conclude in 6. 2.",
                "QUERY GENERATION WITH POISSON PROCESS In the query generation framework, a basic assumption is that a query is generated with a model estimated based on a document.",
                "In most existing work [12, 6, 28, 29], people assume that each query word is sampled independently from a multinomial distribution.",
                "Alternatively, we assume that a query is generated by sampling the frequency of words from a series of independent Poisson processes [20]. 2.1 The Generation Process Let V = {w1, ..., wn} be a vocabulary set.",
                "Let w be a piece of text composed by an author and c(w1), ..., c(wn) be a frequency vector representing w, where c(wi, w) is the frequency count of term wi in text w. In retrieval, w could be either a query or a document.",
                "We consider the frequency counts of the n unique terms in w as n different types of events, sampled from n independent homogeneous Poisson processes, respectively.",
                "Suppose t is the time period during which the author composed the text.",
                "With a homogeneous Poisson process, the frequency count of each event, i.e., the number of occurrences of wi, follows a Poisson distribution with associated parameter λit, where λi is a rate parameter characterizing the expected number of wi in a unit time.",
                "The probability density function of such a Poisson Distribution is given by P(c(wi, w) = k|λit) = e−λit (λit)k k!",
                "Without losing generality, we set t to the length of the text w (people write one word in a unit time), i.e., t = |w|.",
                "With n such independent Poisson processes, each explaining the generation of one term in the vocabulary, the likelihood of w to be generated from such Poisson processes can be written as p(w|Λ) = n i=1 p(c(wi, w)|Λ) = n i=1 e−λi·|w| (λi · |w|)c(wi,w) c(wi, w)! where Λ = {λ1, ..., λn} and |w| = n i=1 c(wi, w).",
                "We refer to these n independent Poisson processes with parameter Λ as a Poisson Language Model.",
                "Let D = {d1, ..., dm} be an observed set of document samples generated from the Poisson process above.",
                "The maximum likelihood estimate (MLE) of λi is ˆλi = d∈D c(wi, d) d∈D w ∈V c(w , d) Note that this MLE is different from the MLE for the Poisson distribution without considering the document lengths, which appears in [22, 24].",
                "Given a document d, we may estimate a Poisson language model Λd using d as a sample.",
                "The likelihood that a query q is generated from the document language model Λd can be written as p(q|d) = w∈V p(c(w, q)|Λd) (1) This representation is clearly different from the multinomial query generation model as (1) the likelihood includes all the terms in the vocabulary V , instead of only those appearing in q, and (2) instead of the appearance of terms, the event space of this model is the frequencies of each term.",
                "In practice, we have the flexibility to choose the vocabulary V .",
                "In one extreme, we can use the vocabulary of the whole collection.",
                "However, this may bring in noise and considerable computational cost.",
                "In the other extreme, we may focus on the terms in the query and ignore other terms, but some useful information may be lost by ignoring the nonquery terms.",
                "As a compromise, we may conflate all the non-query terms as one single pseudo term.",
                "In other words, we may assume that there is exactly one non-query term in the vocabulary for each query.",
                "In our experiments, we adopt this pseudo non-query term strategy.",
                "A document can be scored with the likelihood in Equation 1.",
                "However, if a query term is unseen in the document, the MLE of the Poisson distribution would assign zero probability to the term, causing the probability of the query to be zero.",
                "As in existing language modeling approaches, the main challenge of constructing a reasonable retrieval model is to find a smoothed language model for p(·|d). 2.2 Smoothing in Poisson Retrieval Model In general, we want to assign non-zero rates for the query terms that are not seen in document d. Many smoothing methods have been proposed for multinomial language models[2, 28, 29].",
                "In general, we have to discount the probabilities of some words seen in the text to leave some extra probability mass to assign to the unseen words.",
                "In Poisson language models, however, we do not have the same constraint as in a multinomial model (i.e., w∈V p(w|d) = 1).",
                "Thus we do not have to discount the probability of seen words in order to give a non-zero rate to an unseen word.",
                "Instead, we only need to guarantee that k=0,1,2,... p(c(w, d) = k|d) = 1.",
                "In this section, we introduce three different strategies to smooth a Poisson language model, and show how they lead to different retrieval functions. 2.2.1 Bayesian Smoothing using Gamma Prior Following the risk minimization framework in [11], we assume that a document is generated by the arrival of terms in a time period of |d| according to the document language model, which essentially consists of a vector of Poisson rates for each term, i.e., Λd = λd,1, ..., λd,|V | .",
                "A document is assumed to be generated from a potentially different model.",
                "Given a particular document d, we want to estimate Λd.",
                "The rate of a term is estimated independently of other terms.",
                "We use Bayesian estimation with the following Gamma prior, which has two parameters, α and β: Gamma(λ|α, β) = βα Γ(α) λα−1 e−βλ For each term w, the parameters αw and βw are chosen to be αw = µ ∗ λC,w and βw = µ, where µ is a parameter and λC,w is the rate of w estimated from some background language model, usually the collection language model.",
                "The posterior distribution of Λd is given by p(Λd|d, C) ∝ w∈V e−λw(|d|+µ) λ c(w,d)+µλC,w−1 w which is a product of |V | Gamma distributions with parameters c(w, d) + µλC,w and |d| + µ for each word w. Given that the Gamma mean is α β , we have ˆλd,w = λd,w λd,wp(λd,w|d, C)dλd,w = c(w, d) + µλC,w |d| + µ This is precisely the smoothed estimate of multinomial language model with Dirichlet prior [28]. 2.2.2 Interpolation (Jelinek-Mercer) Smoothing Another straightforward method is to decompose the query generation model as a mixture of two component models.",
                "One is the document language model estimated with maximum likelihood estimator, and the other is a model estimated from the collection background, p(·|C), which assigns non-zero rate to w. For example, we may use an interpolation coefficient between 0 and 1 (i.e., δ ∈ [0, 1]).",
                "With this simple interpolation, we can score a document with Score(d, q) = w∈V log((1 − δ)p(c(w, q)|d) + δp(c(w, q)|C)) (2) Using the maximum likelihood estimator for p(·|d), we have λd,w = c(w,d) |d| , thus Equation 2 becomes Score(d, q) ∝ w∈d∩q [log(1 + 1 − δ δ e−λd,w|q| (λd,w|q|)c(w,q) c(w, q)! · p(c(w, q)|C) ) − log (1 − δ)e−λd,w|q| + δp(c(w, q) = 0|C) 1 − δ + δp(c(w, q) = 0|C) ] + w∈d log (1 − δ)e−λd,w|q| + δp(c(w, q) = 0|C) 1 − δ + δp(c(w, q) = 0|C) We can also use a Poisson language model for p(·|C), or use some other frequency-based models.",
                "In the retrieval formula above, the first summation can be computed efficiently.",
                "The second summation can be actually treated as a document prior, which penalizes long documents.",
                "As the second summation is difficult to compute efficiently, we conflate all non-query terms as one pseudo non-queryterm, denoted as N. Using the pseudo-term formulation and a Poisson collection model, we can rewrite the retrieval formula as Score(d, q) ∝ w∈d∩q log(1 + 1 − δ δ e−λd,w (λd,w|q|)c(w,q) e−λd,C |q| (λd,C )c(w,q) ) + log (1 − δ)e−λd,N |q| + δe−λC,N |q| 1 − δ + δe−λC,N |q| (3) where λd,N = |d|− w∈q c(w,d) |d| and λC,N = |C|− w∈q c(w,C) |C| . 2.2.3 Two-Stage Smoothing As discussed in [29], smoothing plays two roles in retrieval: (1) to improve the estimation of the document language model, and (2) to explain the common terms in the query.",
                "In order to distinguish the content and non-discriminative words in a query, we follow [29] and assume that a query is generated by sampling from a two-component mixture of Poisson language models, with one component being the document model Λd and the other being a query background language model p(·|U). p(·|U) models the typical term frequencies in the users queries.",
                "We may then score each document with the query likelihood computed using the following two-stage smoothing model: p(c(w, q)|Λd, U) = (1 − δ)p(c(w, q)|Λd) + δp(c(w, q)|U) (4) where δ is a parameter, roughly indicating the amount of noise in q.",
                "This looks similar to the interpolation smoothing, except that p(·|Λd) now should be a smoothed language model, instead of the one estimated with MLE.",
                "With no prior knowledge on p(·|U), we could set it to p(·|C).",
                "Any smoothing methods for the document language model can be used to estimate p(·|d) such as the Gamma smoothing as discussed in Section 2.2.1.",
                "The empirical study of the smoothing methods is presented in Section 4. 3.",
                "ANALYSIS OF POISSON LANGUAGE MODEL From the previous section, we notice that the Poisson language model has a strong connection to the multinomial language model.",
                "This is expected since they both belong to the exponential family [26].",
                "However, there are many differences when these two families of models are applied with different smoothing methods.",
                "From the perspective of retrieval, will these two language models perform equivalently?",
                "If not, which model provides more benefits to retrieval, or provides flexibility which could lead to potential benefits?",
                "In this section, we analytically discuss the retrieval features of the Poisson language models, by comparing their behavior with that of the multinomial language models. 3.1 The Equivalence of Basic Models Let us begin with the assumption that all the query terms appear in every document.",
                "Under this assumption, no smoothing is needed.",
                "A document can be scored by the log likelihood of the query with the maximum likelihood estimate: Score(d, q) = w∈V log e−λd,w|q| (λd,w|q|)c(w,q) c(w, q)! (5) Using the MLE, we have λd,w = c(w,d) w∈V c(w,d) .",
                "Thus Score(d, q) ∝ c(w,q)>0 c(w, q) log c(w, d) w∈V c(w, d) This is exactly the log likelihood of the query if the document language model is a multinomial with maximum likelihood estimate.",
                "Indeed, even with Gamma smoothing, when plugging λd,w = c(w,d)+µλC,w |d|+µ and λC,w = c(w,C) |C| into Equation 5, it is easy to show that Score(d, q) ∝ w∈q∩d c(w, q) log(1 + c(w, d) µ · c(w,C) |C| ) + |q| log µ |d| + µ (6) which is exactly the Dirichlet retrieval formula in [28].",
                "Note that this equivalence holds only when the document length variation is modeled with Poisson process.",
                "This derivation indicates the equivalence of the basic Poisson and multinomial language models for retrieval.",
                "With other smoothing strategies, however, the two models would be different.",
                "Nevertheless, with this equivalence in basic models, we could expect that the Poisson language model performs comparably to the multinomial language model in retrieval, if only simple smoothing is explored.",
                "Based on this equivalence analysis, one may ask, why we should pursue the Poisson language model.",
                "In the following sections, we show that despite the equivalence in their basic models, the Poisson language model brings in extra flexibility for exploring advanced techniques on various retrieval features, which could not be achieved with multinomial language models. 3.2 Term Dependent Smoothing One flexibility of the Poisson language model is that it provides a natural framework to accommodate term dependent (per-term) smoothing.",
                "Existing work on language model smoothing has already shown that different types of queries should be smoothed differently according to how discriminative the query terms are. [7] also predicted that different terms should have a different smoothing weights.",
                "With multinomial query generation models, people usually use a single smoothing coefficient to control the combination of the document model and the background model [28, 29].",
                "This parameter can be made specific for different queries, but always has to be a constant for all the terms.",
                "This is mandatory since a multinomial language model has the constraint that w∈V p(w|d) = 1.",
                "However, from retrieval perspective, different terms may need to be smoothed differently even if they are in the same query.",
                "For example, a non-discriminative term (e.g., the, is) is expected to be explained more with the background model, while a content term (e.g., retrieval, bush) in the query should be explained with the document model.",
                "Therefore, a better way of smoothing would be to set the interpolation coefficient (i.e., δ in Formula 2 and Formula 3) specifically for each term.",
                "Since the Poisson language model does not have the sum-to-one constraint across terms, it can easily accommodate per-term smoothing without needing to heuristically twist the semantics of a generative model as in the case of multinomial language models.",
                "Below we present a possible way to explore term dependent smoothing with Poisson language models.",
                "Essentially, we want to use a term-specific smoothing coefficient δ in the linear combination, denoted as δw.",
                "This coefficient should intuitively be larger if w is a common word and smaller if it is a content word.",
                "The key problem is to find a method to assign reasonable values to δw.",
                "Empirical tuning is infeasible for so many parameters.",
                "We may instead estimate the parameters ∆ = {δ1, ..., δ|V |} by maximizing the likelihood of the query given the mixture model of p(q|ΛQ) and p(q|U), where ΛQ is the true query model to generate the query and p(q|U) is a query background model as discussed in Section 2.2.3.",
                "With the model p(q|ΛQ) hidden, the query likelihood is p(q|∆, U) = ΛQ w∈V ((1 − δw)p(c(w, q)|ΛQ) + δwp(c(w, q)|U))P(ΛQ|U)dΛQ If we have relevant documents for each query, we can approximate the query model space with the language models of all the relevant documents.",
                "Without relevant documents, we opt to approximate the query model space with the models of all the documents in the collection.",
                "Setting p(·|U) as p(·|C), the query likelihood becomes p(q|∆, U) = d∈C πd w∈V ((1−δw)p(c(w, q)|ˆΛd)+δwp(c(w, q)|C)) where πd = p(ˆΛd|U). p(·|ˆΛd) is an estimated Poisson language model for document d. If we have prior knowledge on p(ˆΛd|U), such as which documents are relevant to the query, we can set πd accordingly, because what we want is to find ∆ that can maximize the likelihood of the query given relevant documents.",
                "Without this prior knowledge, we can leave πd as free parameters, and use the EM algorithm to estimate πd and ∆.",
                "The updating functions are given as π (k+1) d = πd w∈V ((1 − δw)p(c(w, q)|ˆΛd) + δwp(c(w, q)|C)) d∈C πd w∈V ((1 − δw)p(c(w, q)|ˆΛd) + δwp(c(w, q)|C)) and δ (k+1) w = d∈C πd δwp(c(w, q)|C)) (1 − δw)p(c(w, q)|ˆΛd) + δwp(c(w, q)|C)) As discussed in [29], we only need to run the EM algorithm for several iterations, thus the computational cost is relatively low.",
                "We again assume our vocabulary containing all query terms plus a pseudo non-query term.",
                "Note that the function does not give an explicit way of estimating the coefficient for the unseen non-query term.",
                "In our experiments, we set it to the average over δw of all query terms.",
                "With this flexibility, we expect Poisson language models could improve the retrieval performance, especially for verbose queries, where the query terms have various discriminative values.",
                "In Section 4, we use empirical experiments to prove this hypothesis. 3.3 Mixture Background Models Another flexibility is to explore different background (collection) models (i.e., p(·|U), or p(·|C)).",
                "One common assumption made in language modeling information retrieval is that the background model is a homogeneous model of the document models [28, 29].",
                "Similarly, we can also make the assumption that the collection model is a Poisson language model, with the rates λC,w = d∈C c(w,d) |C| .",
                "However, this assumption usually does not hold, since the collection is far more complex than a single document.",
                "Indeed, the collection usually consists of a mixture of documents with various genres, authors, and topics, etc.",
                "Treating the collection model as a mixture of document models, instead of a single pseudo-document model is more reasonable.",
                "Existing work of multinomial language modeling has already shown that a better modeling of background improves the retrieval performance, such as clusters [15, 10], neighbor documents [25], and aspects [8, 27].",
                "All the approaches can be easily adopted using Poisson language models.",
                "However, a common problem of these approaches is that they all require heavy computation to construct the background model.",
                "With Poisson language modeling, we show that it is possible to model the mixture background without paying for the heavy computational cost.",
                "Poisson Mixture [3] has been proposed to model a collection of documents, which can fit the data much better than a single Poisson.",
                "The basic idea is to assume that the collection is generated from a mixture of Poisson models, which has the general form of p(x = k|PM) = λ p(λ)p(x = k|λ)dλ p(·|λ) is a single Poisson model and p(λ) is an arbitrary probability density function.",
                "There are three well known Poisson mixtures [3]: 2-Poisson, Negative Binomial, and the Katzs K-Mixture [9].",
                "Note that the 2-Poisson model has actually been explored in probabilistic retrieval models, which led to the well-known BM25 formula [22].",
                "All these mixtures have closed forms, and can be estimated from the collection of documents efficiently.",
                "This is an advantage over the multinomial mixture models, such as PLSI [8] and LDA [1], for retrieval.",
                "For example, the probability density function of Katzs K-Mixture is given as p(c(w) = k|αw, βw) = (1 − αw)ηk,0 + αw βw + 1 ( βw βw + 1 )k where ηk,0 = 1 when k = 0, and 0 otherwise.",
                "With the observation of a collection of documents, αw and βw can be estimated as βw = cf(w) − df(w) df(w) and αw = cf(w) Nβw where cf(w) and df(w) are the collection frequency and document frequency of w, and N is the number of documents in the collection.",
                "To account for the different document lengths, we assume that βw is a reasonable estimation for generating a document of the average length, and use β = βw avdl |q| to generate the query.",
                "This Poisson mixture model can be easily used to replace P(·|C) in the retrieval functions 3 and 4. 3.4 Other Possible Flexibilities In addition to term dependent smoothing and efficient mixture background, a Poisson language model has also some other potential advantages.",
                "For example, in Section 2, we see that Formula 2 introduces a component which does document length penalization.",
                "Intuitively, when the document has more unique words, it will be penalized more.",
                "On the other hand, if a document is exactly n copies of another document, it would not get over penalized.",
                "This feature is desirable and not achieved with the Dirichlet model [5].",
                "Potentially, this component could penalize a document according to what types of terms it contains.",
                "With term specific settings of δ, we could get even more flexibility for document length normalization.",
                "Pseudo-feedback is yet another interesting direction where the Poission model might be able to show its advantage.",
                "With model-based feedback, we could again relax the combination coefficients of the feedback model and the background model, and allow different terms to contribute differently to the feedback model.",
                "We could also utilize the relevant documents to learn better per-term smoothing coefficients. 4.",
                "EVALUATION In Section 3, we analytically compared the Poisson language models and multinomial language models from the perspective of query generation and retrieval.",
                "In this section, we compare these two families of models empirically.",
                "Experiment results show that the Poisson model with perterm smoothing outperforms multinomial model, and the performance can be further improved with two-stage smoothing.",
                "Using Poisson mixture as background model also improves the retrieval performance. 4.1 Datasets Since retrieval performance could significantly vary from one test collection to another, and from one query to another, we select four representative TREC test collections: AP, Trec7, Trec8, and Wt2g(Web).",
                "To cover different types of queries, we follow [28, 5], and construct short-keyword (SK, keyword title), short-verbose (SV, one sentence description), and long-verbose (LV, multiple sentences) queries.",
                "The documents are stemmed with the Porters stemmer, and we do not remove any stop word.",
                "For each parameter, we vary its value to cover a reasonably wide range. 4.2 Comparison to Multinomial We compare the performance of the Poisson retrieval models and multinomial retrieval models using interpolation (JelinekMercer, JM) smoothing and Bayesian smoothing with conjugate priors.",
                "Table 1 shows that the two JM-smoothed models perform similarly on all data sets.",
                "Since the Dirichlet Smoothing for multinomial language model and the Gamma Smoothing for Poisson language model lead to the same retrieval formula, the performance of these two models are jointly presented.",
                "We see that Dirichlet/Gamma smoothing methods outperform both Jelinek-Mercer smoothing methods.",
                "The parameter sensitivity curves for two Jelinek-Mercer 0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1 0 0.05 0.1 0.15 0.2 0.25 0.3 Dataset: Trec8 Parameter: δ AveragePrecision JM−Multinomial: LV JM−Multinomial: SV JM−Multinomial: SK JM−Poisson: SK JM−Poisson: SV JM−Poisson: LV Figure 1: Poisson and multinomial performs similarly with Jelinek-Mercer smoothing smoothing methods are shown in Figure 1.",
                "Clearly, these two methods perform similarly either in terms of optimality Data Query JM-Multinomial JM-Poisson Dirichlet/Gamma Per-term 2-Stage Poisson MAP InitPr Pr@5d MAP InitPr Pr@5d MAP InitPr Pr@5d MAP InitPr Pr@5d AP88-89 SK 0.203 0.585 0.356 0.203 0.585 0.358 0.224 0.629 0.393 0.226 0.630 0.396 SV 0.187 0.580 0.361 0.183 0.571 0.345 0.204 0.613 0.387 0.217* 0.603 0.390 LV 0.283 0.716 0.480 0.271 0.692 0.470 0.291 0.710 0.496 0.304* 0.695 0.510 Trec7 SK 0.167 0.635 0.400 0.168 0.635 0.404 0.186 0.687 0.428 0.185 0.646 0.436 SV 0.174 0.655 0.432 0.176 0.653 0.432 0.182 0.666 0.432 0.196* 0.660 0.440 LV 0.223 0.730 0.496 0.215 0.766 0.488 0.224 0.748 0.52 0.236* 0.738 0.512 Trec8 SK 0.239 0.621 0.440 0.239 0.621 0.436 0.257 0.718 0.496 0.256 0.704 0.468 SV 0.231 0.686 0.448 0.234 0.702 0.456 0.228 0.691 0.456 0.246* 0.692 0.476 LV 0.265 0.796 0.548 0.261 0.757 0.520 0.260 0.741 0.492 0.274* 0.766 0.508 Web SK 0.250 0.616 0.380 0.250 0.616 0.380 0.302 0.767 0.468 0.307 0.739 0.468 SV 0.214 0.611 0.392 0.217 0.609 0.384 0.273 0.693 0.508 0.292* 0.703 0.480 LV 0.266 0.790 0.464 0.259 0.776 0.452 0.283 0.756 0.496 0.311* 0.759 0.488 Table 1: Performance comparison between Poisson and Multinomial retrieval models: basic models perform comparably; term dependent two-stage smoothing significantly improves Poisson An asterisk (*) indicates that the difference between the performance of the term dependent two-stage smoothing and that of the Dirichlet/Gamma single smoothing is statistically significant according to the Wilcoxon signed rank test at the level of 0.05. or sensitivity.",
                "This similarity of performance is expected as we discussed in Section 3.1.",
                "Although the Poisson model and multinomial model are similar in terms of the basic model and/or with simple smoothing methods, the Poisson model has great potential and flexibility to be further improved.",
                "As shown in the rightmost column of Table 1, term dependent two-stage Poisson model consistently outperforms the basic smoothing models, especially for verbose queries.",
                "This model is given in Formula 4, with a Gamma smoothing for the document model p(·|d), and δw, which is term dependent.",
                "The parameter µ of the first stage Gamma smoothing is empirically tuned.",
                "The combination coefficients (i.e., ∆), are estimated with the EM algorithm in Section 3.2.",
                "The parameter sensitivity curves for Dirichlet/Gamma and the per-term two-stage smoothing model are plotted in Figure 2.",
                "The per-term two-stage smoothing method is less sensitive to the parameter µ than Dirichlet/Gamma, and yields better optimal performance. 0 1000 2000 3000 4000 5000 6000 7000 8000 9000 10000 0.1 0.12 0.14 0.16 0.18 0.2 0.22 Dataset: AP; Query Type: SV Parameter: µ AveragePrecision Dirichlet/Gamma Smoothing Term Dependent 2−Stage Figure 2: Term dependent two-stage smoothing of Poisson outperforms Dirichlet/Gamma In the following subsections, we conduct experiments to demonstrate how the flexibility of the Poisson model could be utilized to achieve better performance, which we cannot achieve with multinomial language models. 4.3 Term Dependent Smoothing To test the effectiveness of the term dependent smoothing, we conduct the following two experiments.",
                "In the first experiment, we relax the constant coefficient in the simple Jelinek-Mercer smoothing formula (i.e., Formula 3), and use the EM algorithm proposed in Section 3.2 to find a δw for each unique term.",
                "Since we are using the EM algorithm to iteratively estimate the parameters, we usually do not want the probability of p(·|d) to be zero.",
                "We then use a simple Laplace method to slightly smooth the document model before it goes into the EM iterations.",
                "The documents are then still scored with Formula 3, but using learnt δw.",
                "The results are labeled with JM+L. in Table 2.",
                "Data Q JM JM JM+L. 2-Stage 2-Stage (MAP) PT: No Yes Yes No Yes AP SK 0.203 0.204 0.206 0.223 0.226* SV 0.183 0.189 0.214* 0.204 0.217* Trec7 SK 0.168 0.171 0.174 0.186 0.185 SV 0.176 0.147 0.198* 0.194 0.196 Trec8 SK 0.239 0.240 0.227* 0.257 0.256 SV 0.234 0.223 0.249* 0.242 0.246* Web SK 0.250 0.236 0.220* 0.291 0.307* SV 0.217 0.232 0.261* 0.273 0.292* Table 2: Term dependent smoothing improves retrieval performance An asterisk (*) in Column 3 indicates that the difference between the JM+L. method and JM method is statistically significant; an asterisk (*) in Column 5 means that the difference between term dependent two-stage method and query dependent two-stage method is statistically significant; PT stands for per-term.",
                "With term dependent coefficients, the performance of the Jelinek-Mercer Poisson model is improved in most cases.",
                "However, in some cases (e.g., Trec7/SV), it performs poorly.",
                "This might be caused by the problem of EM estimation with unsmoothed document models.",
                "Once non-zero probability is assigned to all the terms before entering the EM iteration, the performance on verbose queries can be improved significantly.",
                "This indicates that there is still room to find better methods to estimate δw.",
                "Please note that neither the perterm JM method nor the JM+L. method has a parameter to tune.",
                "As shown in Table 1, the term dependent two-stage smoothing can significantly improve retrieval performance.",
                "To understand whether the improvement is contributed by the term dependent smoothing or the two-stage smoothing framework, we design another experiment to compare the perterm two-stage smoothing with the two-stage smoothing method proposed in [29].",
                "Their method managed to find coefficients specific to the query, thus a verbose query would use a higher δ.",
                "However, since their model is based on multinomial language modeling, they could not get per-term coefficients.",
                "We adopt their method to the Poisson two-stage smoothing, and also estimate a per-query coefficient for all the terms.",
                "We compare the performance of such a model with the per-term two-stage smoothing model, and present the results in the right two columns in Table 2.",
                "Again, we see that the per-term two-stage smoothing outperforms the per-query two-stage smoothing, especially for verbose queries.",
                "The improvement is not as large as how the perterm smoothing method improves over Dirichlet/Gamma.",
                "This is expected, since the per-query smoothing has already addressed the query discrimination problem to some extent.",
                "This experiment shows that even if the smoothing is already per-query, making it per-term is still beneficial.",
                "In brief, the per-term smoothing improved the retrieval performance of both one-stage and two-stage smoothing method. 4.4 Mixture Background Model In this section, we conduct experiments to examine the benefits of using a mixture background model without extra computational cost, which can not be achieved for multinomial models.",
                "Specifically, in retrieval formula 3, instead of using a single Poisson distribution to model the background p(·|C), we use Katzs K-Mixture model, which is essentially a mixture of Poisson distributions. p(·|C) can be computed efficiently with simple collection statistics, as discussed in Section 3.3.",
                "Data Query JM.",
                "Poisson JM.",
                "K-Mixture AP SK 0.203 0.204 SV 0.183 0.188* Trec-7 SK 0.168 0.169 SV 0.176 0.178* Trec-8 SK 0.239 0.239 SV 0.234 0.238* Web SK 0.250 0.250 SV 0.217 0.223* Table 3: K-Mixture background model improves retrieval performance The performance of the JM retrieval model with single Poisson background and with Katzs K-Mixture background model is compared in Table 3.",
                "Clearly, using K-Mixture to model the background model outperforms the single Poisson background model in most cases, especially for verbose queries where the improvement is statistically significant.",
                "Figure 3 shows that the performance changes over different parameters for short verbose queries.",
                "The model using K-Mixture background is less sensitive than the one using single Poisson background.",
                "Given that this type of mixture 0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1 0 0.05 0.1 0.15 0.2 0.25 Data: Trec8; Query: SV Parameter: δ AveragePrecision Poisson Background K−Mixture Background Figure 3: K-Mixture background model deviates the sensitivity of verbose queries background model does not require any extra computation cost, it would be interesting to study whether using other mixture Poisson models, such as 2-Poisson and negative Binomial, could help the performance. 5.",
                "RELATED WORK To the best of our knowledge, there has been no study of query generation models based on Poisson distribution.",
                "Language models have been shown to be effective for many retrieval tasks [21, 28, 14, 4].",
                "The most popular and fundamental one is the query-generation language model [21, 13].",
                "All existing query generation language models are based on either multinomial distribution [19, 6, 28, 13] or multivariate Bernoulli distribution [21, 17, 18].",
                "We introduce a new family of language models, based on Poisson distribution.",
                "Poisson distribution has been previously studied in the document generation models [16, 22, 3, 24], leading to the development of one of the most effective retrieval formula BM25 [23]. [24] studies the parallel derivation of three different retrieval models which is related to our comparison of Poisson and multinomial.",
                "However, the Poisson model in their paper is still under the document generation framework, and also does not account for the document length variation. [26] introduces a way to empirically search for an exponential model for the documents.",
                "Poisson mixtures [3] such as 2-Poisson [22], Negative multinomial, and Katzs KMixture [9] has shown to be effective to model and retrieve documents.",
                "Once again, none of this work explores Poisson distribution in the query generation framework.",
                "Language model smoothing [2, 28, 29] and background structures [15, 10, 25, 27] have been studied with multinomial language models. [7] analytically shows that term specific smoothing could be useful.",
                "We show that Poisson language model is natural to accommodate the per-term smoothing without heuristic twist of the semantics of a generative model, and is able to efficiently better model the mixture background, both analytically and empirically. 6.",
                "CONCLUSIONS We present a new family of query generation language models for retrieval based on Poisson distribution.",
                "We derive several smoothing methods for this family of models, including single-stage smoothing and two-stage smoothing.",
                "We compare the new models with the popular multinomial retrieval models both analytically and experimentally.",
                "Our analysis shows that while our new models and multinomial models are equivalent under some assumptions, they are generally different with some important differences.",
                "In particular, we show that Poisson has an advantage over multinomial in naturally accommodating per-term smoothing.",
                "We exploit this property to develop a new per-term smoothing algorithm for Poisson language models, which is shown to outperform term-independent smoothing for both Poisson and multinomial models.",
                "Furthermore, we show that a mixture background model for Poisson can be used to improve the performance and robustness over the standard Poisson background model.",
                "Our work opens up many interesting directions for further exploration in this new family of models.",
                "Further exploring the flexibilities over multinomial language models, such as length normalization and pseudo-feedback could be good future work.",
                "It is also appealing to find robust methods to learn the per-term smoothing coefficients without additional computation cost. 7.",
                "ACKNOWLEDGMENTS We thank the anonymous SIGIR 07 reviewers for their useful comments.",
                "This material is based in part upon work supported by the National Science Foundation under award numbers IIS-0347933 and 0425852. 8.",
                "REFERENCES [1] D. Blei, A. Ng, and M. Jordan.",
                "Latent dirichlet allocation.",
                "Journal of Machine Learning Research, 3:993-1022, 2003. [2] S. F. Chen and J. Goodman.",
                "An empirical study of smoothing techniques for language modeling.",
                "Technical Report TR-10-98, Harvard University, 1998. [3] K. Church and W. Gale.",
                "Poisson mixtures.",
                "Nat.",
                "Lang.",
                "Eng., 1(2):163-190, 1995. [4] W. B. Croft and J. Lafferty, editors.",
                "Language Modeling and Information Retrieval.",
                "Kluwer Academic Publishers, 2003. [5] H. Fang, T. Tao, and C. Zhai.",
                "A formal study of information retrieval heuristics.",
                "In Proceedings of the 27th annual international ACM SIGIR conference on Research and development in information retrieval, pages 49-56, 2004. [6] D. Hiemstra.",
                "Using Language Models for Information Retrieval.",
                "PhD thesis, University of Twente, Enschede, Netherlands, 2001. [7] D. Hiemstra.",
                "Term-specific smoothing for the language modeling approach to information retrieval: the importance of a query term.",
                "In Proceedings of the 25th annual international ACM SIGIR conference on Research and development in information retrieval, pages 35-41, 2002. [8] T. Hofmann.",
                "Probabilistic latent semantic indexing.",
                "In Proceedings of ACM SIGIR99, pages 50-57, 1999. [9] S. M. Katz.",
                "Distribution of content words and phrases in text and language modelling.",
                "Nat.",
                "Lang.",
                "Eng., 2(1):15-59, 1996. [10] O. Kurland and L. Lee.",
                "Corpus structure, language models, and ad-hoc information retrieval.",
                "In Proceedings of the 27th annual international ACM SIGIR conference on Research and development in information retrieval, pages 194-201, 2004. [11] J. Lafferty and C. Zhai.",
                "Document language models, query models, and risk minimization for information retrieval.",
                "In Proceedings of SIGIR01, pages 111-119, Sept 2001. [12] J. Lafferty and C. Zhai.",
                "Probabilistic IR models based on query and document generation.",
                "In Proceedings of the Language Modeling and IR workshop, pages 1-5, May 31 - June 1 2001. [13] J. Lafferty and C. Zhai.",
                "Probabilistic relevance models based on document and query generation.",
                "In W. B. Croft and J. Lafferty, editors, Language Modeling and Information Retrieval.",
                "Kluwer Academic Publishers, 2003. [14] V. Lavrenko and B. Croft.",
                "Relevance-based language models.",
                "In Proceedings of SIGIR01, pages 120-127, Sept 2001. [15] X. Liu and W. B. Croft.",
                "Cluster-based retrieval using language models.",
                "In Proceedings of the 27th annual international ACM SIGIR conference on Research and development in information retrieval, pages 186-193, 2004. [16] E. L. Margulis.",
                "Modelling documents with multiple poisson distributions.",
                "Inf.",
                "Process.",
                "Manage., 29(2):215-227, 1993. [17] A. McCallum and K. Nigam.",
                "A comparison of event models for naive bayes text classification.",
                "In Proceedings of AAAI-98 Workshop on Learning for Text Categorization, 1998. [18] D. Metzler, V. Lavrenko, and W. B. Croft.",
                "Formal multiple-bernoulli models for language modeling.",
                "In Proceedings of the 27th annual international ACM SIGIR conference on Research and development in information retrieval, pages 540-541, 2004. [19] D. H. Miller, T. Leek, and R. Schwartz.",
                "A hidden Markov model information retrieval system.",
                "In Proceedings of the 1999 ACM SIGIR Conference on Research and Development in Information Retrieval, pages 214-221, 1999. [20] A. Papoulis.",
                "Probability, random variables and stochastic processes.",
                "New York: McGraw-Hill, 1984, 2nd ed., 1984. [21] J. M. Ponte and W. B. Croft.",
                "A language modeling approach to information retrieval.",
                "In Proceedings of the 21st annual international ACM SIGIR conference on Research and development in information retrieval, pages 275-281, 1998. [22] S. Robertson and S. Walker.",
                "Some simple effective approximations to the 2-poisson model for probabilistic weighted retrieval.",
                "In Proceedings of SIGIR94, pages 232-241, 1994. [23] S. E. Robertson, S. Walker, S. Jones, M. M.Hancock-Beaulieu, and M. Gatford.",
                "Okapi at TREC-3.",
                "In D. K. Harman, editor, The Third Text REtrieval Conference (TREC-3), pages 109-126, 1995. [24] T. Roelleke and J. Wang.",
                "A parallel derivation of probabilistic information retrieval models.",
                "In Proceedings of the 29th annual international ACM SIGIR conference on Research and development in information retrieval, pages 107-114, 2006. [25] T. Tao, X. Wang, Q. Mei, and C. Zhai.",
                "Language model information retrieval with document expansion.",
                "In Proceedings of HLT/NAACL 2006, pages 407-414, 2006. [26] J. Teevan and D. R. Karger.",
                "Empirical development of an exponential probabilistic model for text retrieval: using textual analysis to build a better model.",
                "In Proceedings of the 26th annual international ACM SIGIR conference on Research and development in informaion retrieval, pages 18-25, 2003. [27] X. Wei and W. B. Croft.",
                "Lda-based document models for ad-hoc retrieval.",
                "In Proceedings of the 29th annual international ACM SIGIR conference on Research and development in information retrieval, pages 178-185, 2006. [28] C. Zhai and J. Lafferty.",
                "A study of smoothing methods for language models applied to ad-hoc information retrieval.",
                "In Proceedings of ACM SIGIR01, pages 334-342, Sept 2001. [29] C. Zhai and J. Lafferty.",
                "Two-stage language models for information retrieval.",
                "In Proceedings of ACM SIGIR02, pages 49-56, Aug 2002."
            ],
            "original_annotated_samples": [],
            "translated_annotated_samples": [
                "El uso intensivo de la distribución multinomial se debe en parte al hecho de que se ha utilizado con éxito en el \"reconocimiento de voz\", donde la distribución multinomial es una opción natural para modelar la aparición de una palabra particular en una posición particular en el texto.reconocimiento de voz"
            ],
            "translated_text": "",
            "candidates": [],
            "error": []
        },
        "term frequency": {
            "translated_key": "",
            "is_in_text": true,
            "original_annotated_sentences": [
                "A Study of Poisson Query Generation Model for Information Retrieval Qiaozhu Mei, Hui Fang, Chengxiang Zhai Department of Computer Science University of Illinois at Urbana-Champaign Urbana,IL 61801 {qmei2,hfang,czhai}@uiuc.edu ABSTRACT Many variants of language models have been proposed for information retrieval.",
                "Most existing models are based on multinomial distribution and would score documents based on query likelihood computed based on a query generation probabilistic model.",
                "In this paper, we propose and study a new family of query generation models based on Poisson distribution.",
                "We show that while in their simplest forms, the new family of models and the existing multinomial models are equivalent, they behave differently for many smoothing methods.",
                "We show that the Poisson model has several advantages over the multinomial model, including naturally accommodating per-term smoothing and allowing for more accurate background modeling.",
                "We present several variants of the new model corresponding to different smoothing methods, and evaluate them on four representative TREC test collections.",
                "The results show that while their basic models perform comparably, the Poisson model can outperform multinomial model with per-term smoothing.",
                "The performance can be further improved with two-stage smoothing.",
                "Categories and Subject Descriptors: H.3.3 [Information Search and Retrieval]: Retrieval Models General Terms: Algorithms 1.",
                "INTRODUCTION As a new type of probabilistic retrieval models, language models have been shown to be effective for many retrieval tasks [21, 28, 14, 4].",
                "Among many variants of language models proposed, the most popular and fundamental one is the query-generation language model [21, 13], which leads to the query-likelihood scoring method for ranking documents.",
                "In such a model, given a query q and a document d, we compute the likelihood of generating query q with a model estimated based on document d, i.e., the conditional probability p(q|d).",
                "We can then rank documents based on the likelihood of generating the query.",
                "Virtually all the existing query generation language models are based on either multinomial distribution [19, 6, 28] or multivariate Bernoulli distribution [21, 18].",
                "The multinomial distribution is especially popular and also shown to be quite effective.",
                "The heavy use of multinomial distribution is partly due to the fact that it has been successfully used in speech recognition, where multinomial distribution is a natural choice for modeling the occurrence of a particular word in a particular position in text.",
                "Compared with multivariate Bernoulli, multinomial distribution has the advantage of being able to model the frequency of terms in the query; in contrast, multivariate Bernoulli only models the presence and absence of query terms, thus cannot capture different frequencies of query terms.",
                "However, multivariate Bernoulli also has one potential advantage over multinomial from the viewpoint of retrieval: in a multinomial distribution, the probabilities of all the terms must sum to 1, making it hard to accommodate per-term smoothing, while in a multivariate Bernoulli, the presence probabilities of different terms are completely independent of each other, easily accommodating per-term smoothing and weighting.",
                "Note that term absence is also indirectly captured in a multinomial model through the constraint that all the term probabilities must sum to 1.",
                "In this paper, we propose and study a new family of query generation models based on the Poisson distribution.",
                "In this new family of models, we model the frequency of each term independently with a Poisson distribution.",
                "To score a document, we would first estimate a multivariate Poisson model based on the document, and then score it based on the likelihood of the query given by the estimated Poisson model.",
                "In some sense, the Poisson model combines the advantage of multinomial in modeling <br>term frequency</br> and the advantage of the multivariate Bernoulli in accommodating per-term smoothing.",
                "Indeed, similar to the multinomial distribution, the Poisson distribution models term frequencies, but without the constraint that all the term probabilities must sum to 1, and similar to multivariate Bernoulli, it models each term independently, thus can easily accommodate per-term smoothing.",
                "As in the existing work on multinomial language models, smoothing is critical for this new family of models.",
                "We derive several smoothing methods for Poisson model in parallel to those used for multinomial distributions, and compare the corresponding retrieval models with those based on multinomial distributions.",
                "We find that while with some smoothing methods, the new model and the multinomial model lead to exactly the same formula, with some other smoothing methods they diverge, and the Poisson model brings in more flexibility for smoothing.",
                "In particular, a key difference is that the Poisson model can naturally accommodate perterm smoothing, which is hard to achieve with a multinomial model without heuristic twist of the semantics of a generative model.",
                "We exploit this potential advantage to develop a new term-dependent smoothing algorithm for Poisson model and show that this new smoothing algorithm can improve performance over term-independent smoothing algorithms using either Poisson or multinomial model.",
                "This advantage is seen for both one-stage and two-stage smoothing.",
                "Another potential advantage of the Poisson model is that its corresponding background model for smoothing can be improved through using a mixture model that has a closed form formula.",
                "This new background model is shown to outperform the standard background model and reduce the sensitivity of retrieval performance to the smoothing parameter.",
                "The rest of the paper is organized as follows.",
                "In Section 2, we introduce the new family of query generation models with Poisson distribution, and present various smoothing methods which lead to different retrieval functions.",
                "In Section 3, we analytically compare the Poisson language model with the multinomial language model, from the perspective of retrieval.",
                "We then design empirical experiments to compare the two families of language models in Section 4.",
                "We discuss the related work in 5 and conclude in 6. 2.",
                "QUERY GENERATION WITH POISSON PROCESS In the query generation framework, a basic assumption is that a query is generated with a model estimated based on a document.",
                "In most existing work [12, 6, 28, 29], people assume that each query word is sampled independently from a multinomial distribution.",
                "Alternatively, we assume that a query is generated by sampling the frequency of words from a series of independent Poisson processes [20]. 2.1 The Generation Process Let V = {w1, ..., wn} be a vocabulary set.",
                "Let w be a piece of text composed by an author and c(w1), ..., c(wn) be a frequency vector representing w, where c(wi, w) is the frequency count of term wi in text w. In retrieval, w could be either a query or a document.",
                "We consider the frequency counts of the n unique terms in w as n different types of events, sampled from n independent homogeneous Poisson processes, respectively.",
                "Suppose t is the time period during which the author composed the text.",
                "With a homogeneous Poisson process, the frequency count of each event, i.e., the number of occurrences of wi, follows a Poisson distribution with associated parameter λit, where λi is a rate parameter characterizing the expected number of wi in a unit time.",
                "The probability density function of such a Poisson Distribution is given by P(c(wi, w) = k|λit) = e−λit (λit)k k!",
                "Without losing generality, we set t to the length of the text w (people write one word in a unit time), i.e., t = |w|.",
                "With n such independent Poisson processes, each explaining the generation of one term in the vocabulary, the likelihood of w to be generated from such Poisson processes can be written as p(w|Λ) = n i=1 p(c(wi, w)|Λ) = n i=1 e−λi·|w| (λi · |w|)c(wi,w) c(wi, w)! where Λ = {λ1, ..., λn} and |w| = n i=1 c(wi, w).",
                "We refer to these n independent Poisson processes with parameter Λ as a Poisson Language Model.",
                "Let D = {d1, ..., dm} be an observed set of document samples generated from the Poisson process above.",
                "The maximum likelihood estimate (MLE) of λi is ˆλi = d∈D c(wi, d) d∈D w ∈V c(w , d) Note that this MLE is different from the MLE for the Poisson distribution without considering the document lengths, which appears in [22, 24].",
                "Given a document d, we may estimate a Poisson language model Λd using d as a sample.",
                "The likelihood that a query q is generated from the document language model Λd can be written as p(q|d) = w∈V p(c(w, q)|Λd) (1) This representation is clearly different from the multinomial query generation model as (1) the likelihood includes all the terms in the vocabulary V , instead of only those appearing in q, and (2) instead of the appearance of terms, the event space of this model is the frequencies of each term.",
                "In practice, we have the flexibility to choose the vocabulary V .",
                "In one extreme, we can use the vocabulary of the whole collection.",
                "However, this may bring in noise and considerable computational cost.",
                "In the other extreme, we may focus on the terms in the query and ignore other terms, but some useful information may be lost by ignoring the nonquery terms.",
                "As a compromise, we may conflate all the non-query terms as one single pseudo term.",
                "In other words, we may assume that there is exactly one non-query term in the vocabulary for each query.",
                "In our experiments, we adopt this pseudo non-query term strategy.",
                "A document can be scored with the likelihood in Equation 1.",
                "However, if a query term is unseen in the document, the MLE of the Poisson distribution would assign zero probability to the term, causing the probability of the query to be zero.",
                "As in existing language modeling approaches, the main challenge of constructing a reasonable retrieval model is to find a smoothed language model for p(·|d). 2.2 Smoothing in Poisson Retrieval Model In general, we want to assign non-zero rates for the query terms that are not seen in document d. Many smoothing methods have been proposed for multinomial language models[2, 28, 29].",
                "In general, we have to discount the probabilities of some words seen in the text to leave some extra probability mass to assign to the unseen words.",
                "In Poisson language models, however, we do not have the same constraint as in a multinomial model (i.e., w∈V p(w|d) = 1).",
                "Thus we do not have to discount the probability of seen words in order to give a non-zero rate to an unseen word.",
                "Instead, we only need to guarantee that k=0,1,2,... p(c(w, d) = k|d) = 1.",
                "In this section, we introduce three different strategies to smooth a Poisson language model, and show how they lead to different retrieval functions. 2.2.1 Bayesian Smoothing using Gamma Prior Following the risk minimization framework in [11], we assume that a document is generated by the arrival of terms in a time period of |d| according to the document language model, which essentially consists of a vector of Poisson rates for each term, i.e., Λd = λd,1, ..., λd,|V | .",
                "A document is assumed to be generated from a potentially different model.",
                "Given a particular document d, we want to estimate Λd.",
                "The rate of a term is estimated independently of other terms.",
                "We use Bayesian estimation with the following Gamma prior, which has two parameters, α and β: Gamma(λ|α, β) = βα Γ(α) λα−1 e−βλ For each term w, the parameters αw and βw are chosen to be αw = µ ∗ λC,w and βw = µ, where µ is a parameter and λC,w is the rate of w estimated from some background language model, usually the collection language model.",
                "The posterior distribution of Λd is given by p(Λd|d, C) ∝ w∈V e−λw(|d|+µ) λ c(w,d)+µλC,w−1 w which is a product of |V | Gamma distributions with parameters c(w, d) + µλC,w and |d| + µ for each word w. Given that the Gamma mean is α β , we have ˆλd,w = λd,w λd,wp(λd,w|d, C)dλd,w = c(w, d) + µλC,w |d| + µ This is precisely the smoothed estimate of multinomial language model with Dirichlet prior [28]. 2.2.2 Interpolation (Jelinek-Mercer) Smoothing Another straightforward method is to decompose the query generation model as a mixture of two component models.",
                "One is the document language model estimated with maximum likelihood estimator, and the other is a model estimated from the collection background, p(·|C), which assigns non-zero rate to w. For example, we may use an interpolation coefficient between 0 and 1 (i.e., δ ∈ [0, 1]).",
                "With this simple interpolation, we can score a document with Score(d, q) = w∈V log((1 − δ)p(c(w, q)|d) + δp(c(w, q)|C)) (2) Using the maximum likelihood estimator for p(·|d), we have λd,w = c(w,d) |d| , thus Equation 2 becomes Score(d, q) ∝ w∈d∩q [log(1 + 1 − δ δ e−λd,w|q| (λd,w|q|)c(w,q) c(w, q)! · p(c(w, q)|C) ) − log (1 − δ)e−λd,w|q| + δp(c(w, q) = 0|C) 1 − δ + δp(c(w, q) = 0|C) ] + w∈d log (1 − δ)e−λd,w|q| + δp(c(w, q) = 0|C) 1 − δ + δp(c(w, q) = 0|C) We can also use a Poisson language model for p(·|C), or use some other frequency-based models.",
                "In the retrieval formula above, the first summation can be computed efficiently.",
                "The second summation can be actually treated as a document prior, which penalizes long documents.",
                "As the second summation is difficult to compute efficiently, we conflate all non-query terms as one pseudo non-queryterm, denoted as N. Using the pseudo-term formulation and a Poisson collection model, we can rewrite the retrieval formula as Score(d, q) ∝ w∈d∩q log(1 + 1 − δ δ e−λd,w (λd,w|q|)c(w,q) e−λd,C |q| (λd,C )c(w,q) ) + log (1 − δ)e−λd,N |q| + δe−λC,N |q| 1 − δ + δe−λC,N |q| (3) where λd,N = |d|− w∈q c(w,d) |d| and λC,N = |C|− w∈q c(w,C) |C| . 2.2.3 Two-Stage Smoothing As discussed in [29], smoothing plays two roles in retrieval: (1) to improve the estimation of the document language model, and (2) to explain the common terms in the query.",
                "In order to distinguish the content and non-discriminative words in a query, we follow [29] and assume that a query is generated by sampling from a two-component mixture of Poisson language models, with one component being the document model Λd and the other being a query background language model p(·|U). p(·|U) models the typical term frequencies in the users queries.",
                "We may then score each document with the query likelihood computed using the following two-stage smoothing model: p(c(w, q)|Λd, U) = (1 − δ)p(c(w, q)|Λd) + δp(c(w, q)|U) (4) where δ is a parameter, roughly indicating the amount of noise in q.",
                "This looks similar to the interpolation smoothing, except that p(·|Λd) now should be a smoothed language model, instead of the one estimated with MLE.",
                "With no prior knowledge on p(·|U), we could set it to p(·|C).",
                "Any smoothing methods for the document language model can be used to estimate p(·|d) such as the Gamma smoothing as discussed in Section 2.2.1.",
                "The empirical study of the smoothing methods is presented in Section 4. 3.",
                "ANALYSIS OF POISSON LANGUAGE MODEL From the previous section, we notice that the Poisson language model has a strong connection to the multinomial language model.",
                "This is expected since they both belong to the exponential family [26].",
                "However, there are many differences when these two families of models are applied with different smoothing methods.",
                "From the perspective of retrieval, will these two language models perform equivalently?",
                "If not, which model provides more benefits to retrieval, or provides flexibility which could lead to potential benefits?",
                "In this section, we analytically discuss the retrieval features of the Poisson language models, by comparing their behavior with that of the multinomial language models. 3.1 The Equivalence of Basic Models Let us begin with the assumption that all the query terms appear in every document.",
                "Under this assumption, no smoothing is needed.",
                "A document can be scored by the log likelihood of the query with the maximum likelihood estimate: Score(d, q) = w∈V log e−λd,w|q| (λd,w|q|)c(w,q) c(w, q)! (5) Using the MLE, we have λd,w = c(w,d) w∈V c(w,d) .",
                "Thus Score(d, q) ∝ c(w,q)>0 c(w, q) log c(w, d) w∈V c(w, d) This is exactly the log likelihood of the query if the document language model is a multinomial with maximum likelihood estimate.",
                "Indeed, even with Gamma smoothing, when plugging λd,w = c(w,d)+µλC,w |d|+µ and λC,w = c(w,C) |C| into Equation 5, it is easy to show that Score(d, q) ∝ w∈q∩d c(w, q) log(1 + c(w, d) µ · c(w,C) |C| ) + |q| log µ |d| + µ (6) which is exactly the Dirichlet retrieval formula in [28].",
                "Note that this equivalence holds only when the document length variation is modeled with Poisson process.",
                "This derivation indicates the equivalence of the basic Poisson and multinomial language models for retrieval.",
                "With other smoothing strategies, however, the two models would be different.",
                "Nevertheless, with this equivalence in basic models, we could expect that the Poisson language model performs comparably to the multinomial language model in retrieval, if only simple smoothing is explored.",
                "Based on this equivalence analysis, one may ask, why we should pursue the Poisson language model.",
                "In the following sections, we show that despite the equivalence in their basic models, the Poisson language model brings in extra flexibility for exploring advanced techniques on various retrieval features, which could not be achieved with multinomial language models. 3.2 Term Dependent Smoothing One flexibility of the Poisson language model is that it provides a natural framework to accommodate term dependent (per-term) smoothing.",
                "Existing work on language model smoothing has already shown that different types of queries should be smoothed differently according to how discriminative the query terms are. [7] also predicted that different terms should have a different smoothing weights.",
                "With multinomial query generation models, people usually use a single smoothing coefficient to control the combination of the document model and the background model [28, 29].",
                "This parameter can be made specific for different queries, but always has to be a constant for all the terms.",
                "This is mandatory since a multinomial language model has the constraint that w∈V p(w|d) = 1.",
                "However, from retrieval perspective, different terms may need to be smoothed differently even if they are in the same query.",
                "For example, a non-discriminative term (e.g., the, is) is expected to be explained more with the background model, while a content term (e.g., retrieval, bush) in the query should be explained with the document model.",
                "Therefore, a better way of smoothing would be to set the interpolation coefficient (i.e., δ in Formula 2 and Formula 3) specifically for each term.",
                "Since the Poisson language model does not have the sum-to-one constraint across terms, it can easily accommodate per-term smoothing without needing to heuristically twist the semantics of a generative model as in the case of multinomial language models.",
                "Below we present a possible way to explore term dependent smoothing with Poisson language models.",
                "Essentially, we want to use a term-specific smoothing coefficient δ in the linear combination, denoted as δw.",
                "This coefficient should intuitively be larger if w is a common word and smaller if it is a content word.",
                "The key problem is to find a method to assign reasonable values to δw.",
                "Empirical tuning is infeasible for so many parameters.",
                "We may instead estimate the parameters ∆ = {δ1, ..., δ|V |} by maximizing the likelihood of the query given the mixture model of p(q|ΛQ) and p(q|U), where ΛQ is the true query model to generate the query and p(q|U) is a query background model as discussed in Section 2.2.3.",
                "With the model p(q|ΛQ) hidden, the query likelihood is p(q|∆, U) = ΛQ w∈V ((1 − δw)p(c(w, q)|ΛQ) + δwp(c(w, q)|U))P(ΛQ|U)dΛQ If we have relevant documents for each query, we can approximate the query model space with the language models of all the relevant documents.",
                "Without relevant documents, we opt to approximate the query model space with the models of all the documents in the collection.",
                "Setting p(·|U) as p(·|C), the query likelihood becomes p(q|∆, U) = d∈C πd w∈V ((1−δw)p(c(w, q)|ˆΛd)+δwp(c(w, q)|C)) where πd = p(ˆΛd|U). p(·|ˆΛd) is an estimated Poisson language model for document d. If we have prior knowledge on p(ˆΛd|U), such as which documents are relevant to the query, we can set πd accordingly, because what we want is to find ∆ that can maximize the likelihood of the query given relevant documents.",
                "Without this prior knowledge, we can leave πd as free parameters, and use the EM algorithm to estimate πd and ∆.",
                "The updating functions are given as π (k+1) d = πd w∈V ((1 − δw)p(c(w, q)|ˆΛd) + δwp(c(w, q)|C)) d∈C πd w∈V ((1 − δw)p(c(w, q)|ˆΛd) + δwp(c(w, q)|C)) and δ (k+1) w = d∈C πd δwp(c(w, q)|C)) (1 − δw)p(c(w, q)|ˆΛd) + δwp(c(w, q)|C)) As discussed in [29], we only need to run the EM algorithm for several iterations, thus the computational cost is relatively low.",
                "We again assume our vocabulary containing all query terms plus a pseudo non-query term.",
                "Note that the function does not give an explicit way of estimating the coefficient for the unseen non-query term.",
                "In our experiments, we set it to the average over δw of all query terms.",
                "With this flexibility, we expect Poisson language models could improve the retrieval performance, especially for verbose queries, where the query terms have various discriminative values.",
                "In Section 4, we use empirical experiments to prove this hypothesis. 3.3 Mixture Background Models Another flexibility is to explore different background (collection) models (i.e., p(·|U), or p(·|C)).",
                "One common assumption made in language modeling information retrieval is that the background model is a homogeneous model of the document models [28, 29].",
                "Similarly, we can also make the assumption that the collection model is a Poisson language model, with the rates λC,w = d∈C c(w,d) |C| .",
                "However, this assumption usually does not hold, since the collection is far more complex than a single document.",
                "Indeed, the collection usually consists of a mixture of documents with various genres, authors, and topics, etc.",
                "Treating the collection model as a mixture of document models, instead of a single pseudo-document model is more reasonable.",
                "Existing work of multinomial language modeling has already shown that a better modeling of background improves the retrieval performance, such as clusters [15, 10], neighbor documents [25], and aspects [8, 27].",
                "All the approaches can be easily adopted using Poisson language models.",
                "However, a common problem of these approaches is that they all require heavy computation to construct the background model.",
                "With Poisson language modeling, we show that it is possible to model the mixture background without paying for the heavy computational cost.",
                "Poisson Mixture [3] has been proposed to model a collection of documents, which can fit the data much better than a single Poisson.",
                "The basic idea is to assume that the collection is generated from a mixture of Poisson models, which has the general form of p(x = k|PM) = λ p(λ)p(x = k|λ)dλ p(·|λ) is a single Poisson model and p(λ) is an arbitrary probability density function.",
                "There are three well known Poisson mixtures [3]: 2-Poisson, Negative Binomial, and the Katzs K-Mixture [9].",
                "Note that the 2-Poisson model has actually been explored in probabilistic retrieval models, which led to the well-known BM25 formula [22].",
                "All these mixtures have closed forms, and can be estimated from the collection of documents efficiently.",
                "This is an advantage over the multinomial mixture models, such as PLSI [8] and LDA [1], for retrieval.",
                "For example, the probability density function of Katzs K-Mixture is given as p(c(w) = k|αw, βw) = (1 − αw)ηk,0 + αw βw + 1 ( βw βw + 1 )k where ηk,0 = 1 when k = 0, and 0 otherwise.",
                "With the observation of a collection of documents, αw and βw can be estimated as βw = cf(w) − df(w) df(w) and αw = cf(w) Nβw where cf(w) and df(w) are the collection frequency and document frequency of w, and N is the number of documents in the collection.",
                "To account for the different document lengths, we assume that βw is a reasonable estimation for generating a document of the average length, and use β = βw avdl |q| to generate the query.",
                "This Poisson mixture model can be easily used to replace P(·|C) in the retrieval functions 3 and 4. 3.4 Other Possible Flexibilities In addition to term dependent smoothing and efficient mixture background, a Poisson language model has also some other potential advantages.",
                "For example, in Section 2, we see that Formula 2 introduces a component which does document length penalization.",
                "Intuitively, when the document has more unique words, it will be penalized more.",
                "On the other hand, if a document is exactly n copies of another document, it would not get over penalized.",
                "This feature is desirable and not achieved with the Dirichlet model [5].",
                "Potentially, this component could penalize a document according to what types of terms it contains.",
                "With term specific settings of δ, we could get even more flexibility for document length normalization.",
                "Pseudo-feedback is yet another interesting direction where the Poission model might be able to show its advantage.",
                "With model-based feedback, we could again relax the combination coefficients of the feedback model and the background model, and allow different terms to contribute differently to the feedback model.",
                "We could also utilize the relevant documents to learn better per-term smoothing coefficients. 4.",
                "EVALUATION In Section 3, we analytically compared the Poisson language models and multinomial language models from the perspective of query generation and retrieval.",
                "In this section, we compare these two families of models empirically.",
                "Experiment results show that the Poisson model with perterm smoothing outperforms multinomial model, and the performance can be further improved with two-stage smoothing.",
                "Using Poisson mixture as background model also improves the retrieval performance. 4.1 Datasets Since retrieval performance could significantly vary from one test collection to another, and from one query to another, we select four representative TREC test collections: AP, Trec7, Trec8, and Wt2g(Web).",
                "To cover different types of queries, we follow [28, 5], and construct short-keyword (SK, keyword title), short-verbose (SV, one sentence description), and long-verbose (LV, multiple sentences) queries.",
                "The documents are stemmed with the Porters stemmer, and we do not remove any stop word.",
                "For each parameter, we vary its value to cover a reasonably wide range. 4.2 Comparison to Multinomial We compare the performance of the Poisson retrieval models and multinomial retrieval models using interpolation (JelinekMercer, JM) smoothing and Bayesian smoothing with conjugate priors.",
                "Table 1 shows that the two JM-smoothed models perform similarly on all data sets.",
                "Since the Dirichlet Smoothing for multinomial language model and the Gamma Smoothing for Poisson language model lead to the same retrieval formula, the performance of these two models are jointly presented.",
                "We see that Dirichlet/Gamma smoothing methods outperform both Jelinek-Mercer smoothing methods.",
                "The parameter sensitivity curves for two Jelinek-Mercer 0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1 0 0.05 0.1 0.15 0.2 0.25 0.3 Dataset: Trec8 Parameter: δ AveragePrecision JM−Multinomial: LV JM−Multinomial: SV JM−Multinomial: SK JM−Poisson: SK JM−Poisson: SV JM−Poisson: LV Figure 1: Poisson and multinomial performs similarly with Jelinek-Mercer smoothing smoothing methods are shown in Figure 1.",
                "Clearly, these two methods perform similarly either in terms of optimality Data Query JM-Multinomial JM-Poisson Dirichlet/Gamma Per-term 2-Stage Poisson MAP InitPr Pr@5d MAP InitPr Pr@5d MAP InitPr Pr@5d MAP InitPr Pr@5d AP88-89 SK 0.203 0.585 0.356 0.203 0.585 0.358 0.224 0.629 0.393 0.226 0.630 0.396 SV 0.187 0.580 0.361 0.183 0.571 0.345 0.204 0.613 0.387 0.217* 0.603 0.390 LV 0.283 0.716 0.480 0.271 0.692 0.470 0.291 0.710 0.496 0.304* 0.695 0.510 Trec7 SK 0.167 0.635 0.400 0.168 0.635 0.404 0.186 0.687 0.428 0.185 0.646 0.436 SV 0.174 0.655 0.432 0.176 0.653 0.432 0.182 0.666 0.432 0.196* 0.660 0.440 LV 0.223 0.730 0.496 0.215 0.766 0.488 0.224 0.748 0.52 0.236* 0.738 0.512 Trec8 SK 0.239 0.621 0.440 0.239 0.621 0.436 0.257 0.718 0.496 0.256 0.704 0.468 SV 0.231 0.686 0.448 0.234 0.702 0.456 0.228 0.691 0.456 0.246* 0.692 0.476 LV 0.265 0.796 0.548 0.261 0.757 0.520 0.260 0.741 0.492 0.274* 0.766 0.508 Web SK 0.250 0.616 0.380 0.250 0.616 0.380 0.302 0.767 0.468 0.307 0.739 0.468 SV 0.214 0.611 0.392 0.217 0.609 0.384 0.273 0.693 0.508 0.292* 0.703 0.480 LV 0.266 0.790 0.464 0.259 0.776 0.452 0.283 0.756 0.496 0.311* 0.759 0.488 Table 1: Performance comparison between Poisson and Multinomial retrieval models: basic models perform comparably; term dependent two-stage smoothing significantly improves Poisson An asterisk (*) indicates that the difference between the performance of the term dependent two-stage smoothing and that of the Dirichlet/Gamma single smoothing is statistically significant according to the Wilcoxon signed rank test at the level of 0.05. or sensitivity.",
                "This similarity of performance is expected as we discussed in Section 3.1.",
                "Although the Poisson model and multinomial model are similar in terms of the basic model and/or with simple smoothing methods, the Poisson model has great potential and flexibility to be further improved.",
                "As shown in the rightmost column of Table 1, term dependent two-stage Poisson model consistently outperforms the basic smoothing models, especially for verbose queries.",
                "This model is given in Formula 4, with a Gamma smoothing for the document model p(·|d), and δw, which is term dependent.",
                "The parameter µ of the first stage Gamma smoothing is empirically tuned.",
                "The combination coefficients (i.e., ∆), are estimated with the EM algorithm in Section 3.2.",
                "The parameter sensitivity curves for Dirichlet/Gamma and the per-term two-stage smoothing model are plotted in Figure 2.",
                "The per-term two-stage smoothing method is less sensitive to the parameter µ than Dirichlet/Gamma, and yields better optimal performance. 0 1000 2000 3000 4000 5000 6000 7000 8000 9000 10000 0.1 0.12 0.14 0.16 0.18 0.2 0.22 Dataset: AP; Query Type: SV Parameter: µ AveragePrecision Dirichlet/Gamma Smoothing Term Dependent 2−Stage Figure 2: Term dependent two-stage smoothing of Poisson outperforms Dirichlet/Gamma In the following subsections, we conduct experiments to demonstrate how the flexibility of the Poisson model could be utilized to achieve better performance, which we cannot achieve with multinomial language models. 4.3 Term Dependent Smoothing To test the effectiveness of the term dependent smoothing, we conduct the following two experiments.",
                "In the first experiment, we relax the constant coefficient in the simple Jelinek-Mercer smoothing formula (i.e., Formula 3), and use the EM algorithm proposed in Section 3.2 to find a δw for each unique term.",
                "Since we are using the EM algorithm to iteratively estimate the parameters, we usually do not want the probability of p(·|d) to be zero.",
                "We then use a simple Laplace method to slightly smooth the document model before it goes into the EM iterations.",
                "The documents are then still scored with Formula 3, but using learnt δw.",
                "The results are labeled with JM+L. in Table 2.",
                "Data Q JM JM JM+L. 2-Stage 2-Stage (MAP) PT: No Yes Yes No Yes AP SK 0.203 0.204 0.206 0.223 0.226* SV 0.183 0.189 0.214* 0.204 0.217* Trec7 SK 0.168 0.171 0.174 0.186 0.185 SV 0.176 0.147 0.198* 0.194 0.196 Trec8 SK 0.239 0.240 0.227* 0.257 0.256 SV 0.234 0.223 0.249* 0.242 0.246* Web SK 0.250 0.236 0.220* 0.291 0.307* SV 0.217 0.232 0.261* 0.273 0.292* Table 2: Term dependent smoothing improves retrieval performance An asterisk (*) in Column 3 indicates that the difference between the JM+L. method and JM method is statistically significant; an asterisk (*) in Column 5 means that the difference between term dependent two-stage method and query dependent two-stage method is statistically significant; PT stands for per-term.",
                "With term dependent coefficients, the performance of the Jelinek-Mercer Poisson model is improved in most cases.",
                "However, in some cases (e.g., Trec7/SV), it performs poorly.",
                "This might be caused by the problem of EM estimation with unsmoothed document models.",
                "Once non-zero probability is assigned to all the terms before entering the EM iteration, the performance on verbose queries can be improved significantly.",
                "This indicates that there is still room to find better methods to estimate δw.",
                "Please note that neither the perterm JM method nor the JM+L. method has a parameter to tune.",
                "As shown in Table 1, the term dependent two-stage smoothing can significantly improve retrieval performance.",
                "To understand whether the improvement is contributed by the term dependent smoothing or the two-stage smoothing framework, we design another experiment to compare the perterm two-stage smoothing with the two-stage smoothing method proposed in [29].",
                "Their method managed to find coefficients specific to the query, thus a verbose query would use a higher δ.",
                "However, since their model is based on multinomial language modeling, they could not get per-term coefficients.",
                "We adopt their method to the Poisson two-stage smoothing, and also estimate a per-query coefficient for all the terms.",
                "We compare the performance of such a model with the per-term two-stage smoothing model, and present the results in the right two columns in Table 2.",
                "Again, we see that the per-term two-stage smoothing outperforms the per-query two-stage smoothing, especially for verbose queries.",
                "The improvement is not as large as how the perterm smoothing method improves over Dirichlet/Gamma.",
                "This is expected, since the per-query smoothing has already addressed the query discrimination problem to some extent.",
                "This experiment shows that even if the smoothing is already per-query, making it per-term is still beneficial.",
                "In brief, the per-term smoothing improved the retrieval performance of both one-stage and two-stage smoothing method. 4.4 Mixture Background Model In this section, we conduct experiments to examine the benefits of using a mixture background model without extra computational cost, which can not be achieved for multinomial models.",
                "Specifically, in retrieval formula 3, instead of using a single Poisson distribution to model the background p(·|C), we use Katzs K-Mixture model, which is essentially a mixture of Poisson distributions. p(·|C) can be computed efficiently with simple collection statistics, as discussed in Section 3.3.",
                "Data Query JM.",
                "Poisson JM.",
                "K-Mixture AP SK 0.203 0.204 SV 0.183 0.188* Trec-7 SK 0.168 0.169 SV 0.176 0.178* Trec-8 SK 0.239 0.239 SV 0.234 0.238* Web SK 0.250 0.250 SV 0.217 0.223* Table 3: K-Mixture background model improves retrieval performance The performance of the JM retrieval model with single Poisson background and with Katzs K-Mixture background model is compared in Table 3.",
                "Clearly, using K-Mixture to model the background model outperforms the single Poisson background model in most cases, especially for verbose queries where the improvement is statistically significant.",
                "Figure 3 shows that the performance changes over different parameters for short verbose queries.",
                "The model using K-Mixture background is less sensitive than the one using single Poisson background.",
                "Given that this type of mixture 0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1 0 0.05 0.1 0.15 0.2 0.25 Data: Trec8; Query: SV Parameter: δ AveragePrecision Poisson Background K−Mixture Background Figure 3: K-Mixture background model deviates the sensitivity of verbose queries background model does not require any extra computation cost, it would be interesting to study whether using other mixture Poisson models, such as 2-Poisson and negative Binomial, could help the performance. 5.",
                "RELATED WORK To the best of our knowledge, there has been no study of query generation models based on Poisson distribution.",
                "Language models have been shown to be effective for many retrieval tasks [21, 28, 14, 4].",
                "The most popular and fundamental one is the query-generation language model [21, 13].",
                "All existing query generation language models are based on either multinomial distribution [19, 6, 28, 13] or multivariate Bernoulli distribution [21, 17, 18].",
                "We introduce a new family of language models, based on Poisson distribution.",
                "Poisson distribution has been previously studied in the document generation models [16, 22, 3, 24], leading to the development of one of the most effective retrieval formula BM25 [23]. [24] studies the parallel derivation of three different retrieval models which is related to our comparison of Poisson and multinomial.",
                "However, the Poisson model in their paper is still under the document generation framework, and also does not account for the document length variation. [26] introduces a way to empirically search for an exponential model for the documents.",
                "Poisson mixtures [3] such as 2-Poisson [22], Negative multinomial, and Katzs KMixture [9] has shown to be effective to model and retrieve documents.",
                "Once again, none of this work explores Poisson distribution in the query generation framework.",
                "Language model smoothing [2, 28, 29] and background structures [15, 10, 25, 27] have been studied with multinomial language models. [7] analytically shows that term specific smoothing could be useful.",
                "We show that Poisson language model is natural to accommodate the per-term smoothing without heuristic twist of the semantics of a generative model, and is able to efficiently better model the mixture background, both analytically and empirically. 6.",
                "CONCLUSIONS We present a new family of query generation language models for retrieval based on Poisson distribution.",
                "We derive several smoothing methods for this family of models, including single-stage smoothing and two-stage smoothing.",
                "We compare the new models with the popular multinomial retrieval models both analytically and experimentally.",
                "Our analysis shows that while our new models and multinomial models are equivalent under some assumptions, they are generally different with some important differences.",
                "In particular, we show that Poisson has an advantage over multinomial in naturally accommodating per-term smoothing.",
                "We exploit this property to develop a new per-term smoothing algorithm for Poisson language models, which is shown to outperform term-independent smoothing for both Poisson and multinomial models.",
                "Furthermore, we show that a mixture background model for Poisson can be used to improve the performance and robustness over the standard Poisson background model.",
                "Our work opens up many interesting directions for further exploration in this new family of models.",
                "Further exploring the flexibilities over multinomial language models, such as length normalization and pseudo-feedback could be good future work.",
                "It is also appealing to find robust methods to learn the per-term smoothing coefficients without additional computation cost. 7.",
                "ACKNOWLEDGMENTS We thank the anonymous SIGIR 07 reviewers for their useful comments.",
                "This material is based in part upon work supported by the National Science Foundation under award numbers IIS-0347933 and 0425852. 8.",
                "REFERENCES [1] D. Blei, A. Ng, and M. Jordan.",
                "Latent dirichlet allocation.",
                "Journal of Machine Learning Research, 3:993-1022, 2003. [2] S. F. Chen and J. Goodman.",
                "An empirical study of smoothing techniques for language modeling.",
                "Technical Report TR-10-98, Harvard University, 1998. [3] K. Church and W. Gale.",
                "Poisson mixtures.",
                "Nat.",
                "Lang.",
                "Eng., 1(2):163-190, 1995. [4] W. B. Croft and J. Lafferty, editors.",
                "Language Modeling and Information Retrieval.",
                "Kluwer Academic Publishers, 2003. [5] H. Fang, T. Tao, and C. Zhai.",
                "A formal study of information retrieval heuristics.",
                "In Proceedings of the 27th annual international ACM SIGIR conference on Research and development in information retrieval, pages 49-56, 2004. [6] D. Hiemstra.",
                "Using Language Models for Information Retrieval.",
                "PhD thesis, University of Twente, Enschede, Netherlands, 2001. [7] D. Hiemstra.",
                "Term-specific smoothing for the language modeling approach to information retrieval: the importance of a query term.",
                "In Proceedings of the 25th annual international ACM SIGIR conference on Research and development in information retrieval, pages 35-41, 2002. [8] T. Hofmann.",
                "Probabilistic latent semantic indexing.",
                "In Proceedings of ACM SIGIR99, pages 50-57, 1999. [9] S. M. Katz.",
                "Distribution of content words and phrases in text and language modelling.",
                "Nat.",
                "Lang.",
                "Eng., 2(1):15-59, 1996. [10] O. Kurland and L. Lee.",
                "Corpus structure, language models, and ad-hoc information retrieval.",
                "In Proceedings of the 27th annual international ACM SIGIR conference on Research and development in information retrieval, pages 194-201, 2004. [11] J. Lafferty and C. Zhai.",
                "Document language models, query models, and risk minimization for information retrieval.",
                "In Proceedings of SIGIR01, pages 111-119, Sept 2001. [12] J. Lafferty and C. Zhai.",
                "Probabilistic IR models based on query and document generation.",
                "In Proceedings of the Language Modeling and IR workshop, pages 1-5, May 31 - June 1 2001. [13] J. Lafferty and C. Zhai.",
                "Probabilistic relevance models based on document and query generation.",
                "In W. B. Croft and J. Lafferty, editors, Language Modeling and Information Retrieval.",
                "Kluwer Academic Publishers, 2003. [14] V. Lavrenko and B. Croft.",
                "Relevance-based language models.",
                "In Proceedings of SIGIR01, pages 120-127, Sept 2001. [15] X. Liu and W. B. Croft.",
                "Cluster-based retrieval using language models.",
                "In Proceedings of the 27th annual international ACM SIGIR conference on Research and development in information retrieval, pages 186-193, 2004. [16] E. L. Margulis.",
                "Modelling documents with multiple poisson distributions.",
                "Inf.",
                "Process.",
                "Manage., 29(2):215-227, 1993. [17] A. McCallum and K. Nigam.",
                "A comparison of event models for naive bayes text classification.",
                "In Proceedings of AAAI-98 Workshop on Learning for Text Categorization, 1998. [18] D. Metzler, V. Lavrenko, and W. B. Croft.",
                "Formal multiple-bernoulli models for language modeling.",
                "In Proceedings of the 27th annual international ACM SIGIR conference on Research and development in information retrieval, pages 540-541, 2004. [19] D. H. Miller, T. Leek, and R. Schwartz.",
                "A hidden Markov model information retrieval system.",
                "In Proceedings of the 1999 ACM SIGIR Conference on Research and Development in Information Retrieval, pages 214-221, 1999. [20] A. Papoulis.",
                "Probability, random variables and stochastic processes.",
                "New York: McGraw-Hill, 1984, 2nd ed., 1984. [21] J. M. Ponte and W. B. Croft.",
                "A language modeling approach to information retrieval.",
                "In Proceedings of the 21st annual international ACM SIGIR conference on Research and development in information retrieval, pages 275-281, 1998. [22] S. Robertson and S. Walker.",
                "Some simple effective approximations to the 2-poisson model for probabilistic weighted retrieval.",
                "In Proceedings of SIGIR94, pages 232-241, 1994. [23] S. E. Robertson, S. Walker, S. Jones, M. M.Hancock-Beaulieu, and M. Gatford.",
                "Okapi at TREC-3.",
                "In D. K. Harman, editor, The Third Text REtrieval Conference (TREC-3), pages 109-126, 1995. [24] T. Roelleke and J. Wang.",
                "A parallel derivation of probabilistic information retrieval models.",
                "In Proceedings of the 29th annual international ACM SIGIR conference on Research and development in information retrieval, pages 107-114, 2006. [25] T. Tao, X. Wang, Q. Mei, and C. Zhai.",
                "Language model information retrieval with document expansion.",
                "In Proceedings of HLT/NAACL 2006, pages 407-414, 2006. [26] J. Teevan and D. R. Karger.",
                "Empirical development of an exponential probabilistic model for text retrieval: using textual analysis to build a better model.",
                "In Proceedings of the 26th annual international ACM SIGIR conference on Research and development in informaion retrieval, pages 18-25, 2003. [27] X. Wei and W. B. Croft.",
                "Lda-based document models for ad-hoc retrieval.",
                "In Proceedings of the 29th annual international ACM SIGIR conference on Research and development in information retrieval, pages 178-185, 2006. [28] C. Zhai and J. Lafferty.",
                "A study of smoothing methods for language models applied to ad-hoc information retrieval.",
                "In Proceedings of ACM SIGIR01, pages 334-342, Sept 2001. [29] C. Zhai and J. Lafferty.",
                "Two-stage language models for information retrieval.",
                "In Proceedings of ACM SIGIR02, pages 49-56, Aug 2002."
            ],
            "original_annotated_samples": [],
            "translated_annotated_samples": [
                "En cierto sentido, el modelo Poisson combina la ventaja de multinomial en el modelado de \"frecuencia de término\" y la ventaja de los Bernoulli multivariados para acomodar el suavizado por período.frecuencia"
            ],
            "translated_text": "",
            "candidates": [],
            "error": []
        },
        "perterm smoothing": {
            "translated_key": "",
            "is_in_text": true,
            "original_annotated_sentences": [
                "A Study of Poisson Query Generation Model for Information Retrieval Qiaozhu Mei, Hui Fang, Chengxiang Zhai Department of Computer Science University of Illinois at Urbana-Champaign Urbana,IL 61801 {qmei2,hfang,czhai}@uiuc.edu ABSTRACT Many variants of language models have been proposed for information retrieval.",
                "Most existing models are based on multinomial distribution and would score documents based on query likelihood computed based on a query generation probabilistic model.",
                "In this paper, we propose and study a new family of query generation models based on Poisson distribution.",
                "We show that while in their simplest forms, the new family of models and the existing multinomial models are equivalent, they behave differently for many smoothing methods.",
                "We show that the Poisson model has several advantages over the multinomial model, including naturally accommodating per-term smoothing and allowing for more accurate background modeling.",
                "We present several variants of the new model corresponding to different smoothing methods, and evaluate them on four representative TREC test collections.",
                "The results show that while their basic models perform comparably, the Poisson model can outperform multinomial model with per-term smoothing.",
                "The performance can be further improved with two-stage smoothing.",
                "Categories and Subject Descriptors: H.3.3 [Information Search and Retrieval]: Retrieval Models General Terms: Algorithms 1.",
                "INTRODUCTION As a new type of probabilistic retrieval models, language models have been shown to be effective for many retrieval tasks [21, 28, 14, 4].",
                "Among many variants of language models proposed, the most popular and fundamental one is the query-generation language model [21, 13], which leads to the query-likelihood scoring method for ranking documents.",
                "In such a model, given a query q and a document d, we compute the likelihood of generating query q with a model estimated based on document d, i.e., the conditional probability p(q|d).",
                "We can then rank documents based on the likelihood of generating the query.",
                "Virtually all the existing query generation language models are based on either multinomial distribution [19, 6, 28] or multivariate Bernoulli distribution [21, 18].",
                "The multinomial distribution is especially popular and also shown to be quite effective.",
                "The heavy use of multinomial distribution is partly due to the fact that it has been successfully used in speech recognition, where multinomial distribution is a natural choice for modeling the occurrence of a particular word in a particular position in text.",
                "Compared with multivariate Bernoulli, multinomial distribution has the advantage of being able to model the frequency of terms in the query; in contrast, multivariate Bernoulli only models the presence and absence of query terms, thus cannot capture different frequencies of query terms.",
                "However, multivariate Bernoulli also has one potential advantage over multinomial from the viewpoint of retrieval: in a multinomial distribution, the probabilities of all the terms must sum to 1, making it hard to accommodate per-term smoothing, while in a multivariate Bernoulli, the presence probabilities of different terms are completely independent of each other, easily accommodating per-term smoothing and weighting.",
                "Note that term absence is also indirectly captured in a multinomial model through the constraint that all the term probabilities must sum to 1.",
                "In this paper, we propose and study a new family of query generation models based on the Poisson distribution.",
                "In this new family of models, we model the frequency of each term independently with a Poisson distribution.",
                "To score a document, we would first estimate a multivariate Poisson model based on the document, and then score it based on the likelihood of the query given by the estimated Poisson model.",
                "In some sense, the Poisson model combines the advantage of multinomial in modeling term frequency and the advantage of the multivariate Bernoulli in accommodating per-term smoothing.",
                "Indeed, similar to the multinomial distribution, the Poisson distribution models term frequencies, but without the constraint that all the term probabilities must sum to 1, and similar to multivariate Bernoulli, it models each term independently, thus can easily accommodate per-term smoothing.",
                "As in the existing work on multinomial language models, smoothing is critical for this new family of models.",
                "We derive several smoothing methods for Poisson model in parallel to those used for multinomial distributions, and compare the corresponding retrieval models with those based on multinomial distributions.",
                "We find that while with some smoothing methods, the new model and the multinomial model lead to exactly the same formula, with some other smoothing methods they diverge, and the Poisson model brings in more flexibility for smoothing.",
                "In particular, a key difference is that the Poisson model can naturally accommodate <br>perterm smoothing</br>, which is hard to achieve with a multinomial model without heuristic twist of the semantics of a generative model.",
                "We exploit this potential advantage to develop a new term-dependent smoothing algorithm for Poisson model and show that this new smoothing algorithm can improve performance over term-independent smoothing algorithms using either Poisson or multinomial model.",
                "This advantage is seen for both one-stage and two-stage smoothing.",
                "Another potential advantage of the Poisson model is that its corresponding background model for smoothing can be improved through using a mixture model that has a closed form formula.",
                "This new background model is shown to outperform the standard background model and reduce the sensitivity of retrieval performance to the smoothing parameter.",
                "The rest of the paper is organized as follows.",
                "In Section 2, we introduce the new family of query generation models with Poisson distribution, and present various smoothing methods which lead to different retrieval functions.",
                "In Section 3, we analytically compare the Poisson language model with the multinomial language model, from the perspective of retrieval.",
                "We then design empirical experiments to compare the two families of language models in Section 4.",
                "We discuss the related work in 5 and conclude in 6. 2.",
                "QUERY GENERATION WITH POISSON PROCESS In the query generation framework, a basic assumption is that a query is generated with a model estimated based on a document.",
                "In most existing work [12, 6, 28, 29], people assume that each query word is sampled independently from a multinomial distribution.",
                "Alternatively, we assume that a query is generated by sampling the frequency of words from a series of independent Poisson processes [20]. 2.1 The Generation Process Let V = {w1, ..., wn} be a vocabulary set.",
                "Let w be a piece of text composed by an author and c(w1), ..., c(wn) be a frequency vector representing w, where c(wi, w) is the frequency count of term wi in text w. In retrieval, w could be either a query or a document.",
                "We consider the frequency counts of the n unique terms in w as n different types of events, sampled from n independent homogeneous Poisson processes, respectively.",
                "Suppose t is the time period during which the author composed the text.",
                "With a homogeneous Poisson process, the frequency count of each event, i.e., the number of occurrences of wi, follows a Poisson distribution with associated parameter λit, where λi is a rate parameter characterizing the expected number of wi in a unit time.",
                "The probability density function of such a Poisson Distribution is given by P(c(wi, w) = k|λit) = e−λit (λit)k k!",
                "Without losing generality, we set t to the length of the text w (people write one word in a unit time), i.e., t = |w|.",
                "With n such independent Poisson processes, each explaining the generation of one term in the vocabulary, the likelihood of w to be generated from such Poisson processes can be written as p(w|Λ) = n i=1 p(c(wi, w)|Λ) = n i=1 e−λi·|w| (λi · |w|)c(wi,w) c(wi, w)! where Λ = {λ1, ..., λn} and |w| = n i=1 c(wi, w).",
                "We refer to these n independent Poisson processes with parameter Λ as a Poisson Language Model.",
                "Let D = {d1, ..., dm} be an observed set of document samples generated from the Poisson process above.",
                "The maximum likelihood estimate (MLE) of λi is ˆλi = d∈D c(wi, d) d∈D w ∈V c(w , d) Note that this MLE is different from the MLE for the Poisson distribution without considering the document lengths, which appears in [22, 24].",
                "Given a document d, we may estimate a Poisson language model Λd using d as a sample.",
                "The likelihood that a query q is generated from the document language model Λd can be written as p(q|d) = w∈V p(c(w, q)|Λd) (1) This representation is clearly different from the multinomial query generation model as (1) the likelihood includes all the terms in the vocabulary V , instead of only those appearing in q, and (2) instead of the appearance of terms, the event space of this model is the frequencies of each term.",
                "In practice, we have the flexibility to choose the vocabulary V .",
                "In one extreme, we can use the vocabulary of the whole collection.",
                "However, this may bring in noise and considerable computational cost.",
                "In the other extreme, we may focus on the terms in the query and ignore other terms, but some useful information may be lost by ignoring the nonquery terms.",
                "As a compromise, we may conflate all the non-query terms as one single pseudo term.",
                "In other words, we may assume that there is exactly one non-query term in the vocabulary for each query.",
                "In our experiments, we adopt this pseudo non-query term strategy.",
                "A document can be scored with the likelihood in Equation 1.",
                "However, if a query term is unseen in the document, the MLE of the Poisson distribution would assign zero probability to the term, causing the probability of the query to be zero.",
                "As in existing language modeling approaches, the main challenge of constructing a reasonable retrieval model is to find a smoothed language model for p(·|d). 2.2 Smoothing in Poisson Retrieval Model In general, we want to assign non-zero rates for the query terms that are not seen in document d. Many smoothing methods have been proposed for multinomial language models[2, 28, 29].",
                "In general, we have to discount the probabilities of some words seen in the text to leave some extra probability mass to assign to the unseen words.",
                "In Poisson language models, however, we do not have the same constraint as in a multinomial model (i.e., w∈V p(w|d) = 1).",
                "Thus we do not have to discount the probability of seen words in order to give a non-zero rate to an unseen word.",
                "Instead, we only need to guarantee that k=0,1,2,... p(c(w, d) = k|d) = 1.",
                "In this section, we introduce three different strategies to smooth a Poisson language model, and show how they lead to different retrieval functions. 2.2.1 Bayesian Smoothing using Gamma Prior Following the risk minimization framework in [11], we assume that a document is generated by the arrival of terms in a time period of |d| according to the document language model, which essentially consists of a vector of Poisson rates for each term, i.e., Λd = λd,1, ..., λd,|V | .",
                "A document is assumed to be generated from a potentially different model.",
                "Given a particular document d, we want to estimate Λd.",
                "The rate of a term is estimated independently of other terms.",
                "We use Bayesian estimation with the following Gamma prior, which has two parameters, α and β: Gamma(λ|α, β) = βα Γ(α) λα−1 e−βλ For each term w, the parameters αw and βw are chosen to be αw = µ ∗ λC,w and βw = µ, where µ is a parameter and λC,w is the rate of w estimated from some background language model, usually the collection language model.",
                "The posterior distribution of Λd is given by p(Λd|d, C) ∝ w∈V e−λw(|d|+µ) λ c(w,d)+µλC,w−1 w which is a product of |V | Gamma distributions with parameters c(w, d) + µλC,w and |d| + µ for each word w. Given that the Gamma mean is α β , we have ˆλd,w = λd,w λd,wp(λd,w|d, C)dλd,w = c(w, d) + µλC,w |d| + µ This is precisely the smoothed estimate of multinomial language model with Dirichlet prior [28]. 2.2.2 Interpolation (Jelinek-Mercer) Smoothing Another straightforward method is to decompose the query generation model as a mixture of two component models.",
                "One is the document language model estimated with maximum likelihood estimator, and the other is a model estimated from the collection background, p(·|C), which assigns non-zero rate to w. For example, we may use an interpolation coefficient between 0 and 1 (i.e., δ ∈ [0, 1]).",
                "With this simple interpolation, we can score a document with Score(d, q) = w∈V log((1 − δ)p(c(w, q)|d) + δp(c(w, q)|C)) (2) Using the maximum likelihood estimator for p(·|d), we have λd,w = c(w,d) |d| , thus Equation 2 becomes Score(d, q) ∝ w∈d∩q [log(1 + 1 − δ δ e−λd,w|q| (λd,w|q|)c(w,q) c(w, q)! · p(c(w, q)|C) ) − log (1 − δ)e−λd,w|q| + δp(c(w, q) = 0|C) 1 − δ + δp(c(w, q) = 0|C) ] + w∈d log (1 − δ)e−λd,w|q| + δp(c(w, q) = 0|C) 1 − δ + δp(c(w, q) = 0|C) We can also use a Poisson language model for p(·|C), or use some other frequency-based models.",
                "In the retrieval formula above, the first summation can be computed efficiently.",
                "The second summation can be actually treated as a document prior, which penalizes long documents.",
                "As the second summation is difficult to compute efficiently, we conflate all non-query terms as one pseudo non-queryterm, denoted as N. Using the pseudo-term formulation and a Poisson collection model, we can rewrite the retrieval formula as Score(d, q) ∝ w∈d∩q log(1 + 1 − δ δ e−λd,w (λd,w|q|)c(w,q) e−λd,C |q| (λd,C )c(w,q) ) + log (1 − δ)e−λd,N |q| + δe−λC,N |q| 1 − δ + δe−λC,N |q| (3) where λd,N = |d|− w∈q c(w,d) |d| and λC,N = |C|− w∈q c(w,C) |C| . 2.2.3 Two-Stage Smoothing As discussed in [29], smoothing plays two roles in retrieval: (1) to improve the estimation of the document language model, and (2) to explain the common terms in the query.",
                "In order to distinguish the content and non-discriminative words in a query, we follow [29] and assume that a query is generated by sampling from a two-component mixture of Poisson language models, with one component being the document model Λd and the other being a query background language model p(·|U). p(·|U) models the typical term frequencies in the users queries.",
                "We may then score each document with the query likelihood computed using the following two-stage smoothing model: p(c(w, q)|Λd, U) = (1 − δ)p(c(w, q)|Λd) + δp(c(w, q)|U) (4) where δ is a parameter, roughly indicating the amount of noise in q.",
                "This looks similar to the interpolation smoothing, except that p(·|Λd) now should be a smoothed language model, instead of the one estimated with MLE.",
                "With no prior knowledge on p(·|U), we could set it to p(·|C).",
                "Any smoothing methods for the document language model can be used to estimate p(·|d) such as the Gamma smoothing as discussed in Section 2.2.1.",
                "The empirical study of the smoothing methods is presented in Section 4. 3.",
                "ANALYSIS OF POISSON LANGUAGE MODEL From the previous section, we notice that the Poisson language model has a strong connection to the multinomial language model.",
                "This is expected since they both belong to the exponential family [26].",
                "However, there are many differences when these two families of models are applied with different smoothing methods.",
                "From the perspective of retrieval, will these two language models perform equivalently?",
                "If not, which model provides more benefits to retrieval, or provides flexibility which could lead to potential benefits?",
                "In this section, we analytically discuss the retrieval features of the Poisson language models, by comparing their behavior with that of the multinomial language models. 3.1 The Equivalence of Basic Models Let us begin with the assumption that all the query terms appear in every document.",
                "Under this assumption, no smoothing is needed.",
                "A document can be scored by the log likelihood of the query with the maximum likelihood estimate: Score(d, q) = w∈V log e−λd,w|q| (λd,w|q|)c(w,q) c(w, q)! (5) Using the MLE, we have λd,w = c(w,d) w∈V c(w,d) .",
                "Thus Score(d, q) ∝ c(w,q)>0 c(w, q) log c(w, d) w∈V c(w, d) This is exactly the log likelihood of the query if the document language model is a multinomial with maximum likelihood estimate.",
                "Indeed, even with Gamma smoothing, when plugging λd,w = c(w,d)+µλC,w |d|+µ and λC,w = c(w,C) |C| into Equation 5, it is easy to show that Score(d, q) ∝ w∈q∩d c(w, q) log(1 + c(w, d) µ · c(w,C) |C| ) + |q| log µ |d| + µ (6) which is exactly the Dirichlet retrieval formula in [28].",
                "Note that this equivalence holds only when the document length variation is modeled with Poisson process.",
                "This derivation indicates the equivalence of the basic Poisson and multinomial language models for retrieval.",
                "With other smoothing strategies, however, the two models would be different.",
                "Nevertheless, with this equivalence in basic models, we could expect that the Poisson language model performs comparably to the multinomial language model in retrieval, if only simple smoothing is explored.",
                "Based on this equivalence analysis, one may ask, why we should pursue the Poisson language model.",
                "In the following sections, we show that despite the equivalence in their basic models, the Poisson language model brings in extra flexibility for exploring advanced techniques on various retrieval features, which could not be achieved with multinomial language models. 3.2 Term Dependent Smoothing One flexibility of the Poisson language model is that it provides a natural framework to accommodate term dependent (per-term) smoothing.",
                "Existing work on language model smoothing has already shown that different types of queries should be smoothed differently according to how discriminative the query terms are. [7] also predicted that different terms should have a different smoothing weights.",
                "With multinomial query generation models, people usually use a single smoothing coefficient to control the combination of the document model and the background model [28, 29].",
                "This parameter can be made specific for different queries, but always has to be a constant for all the terms.",
                "This is mandatory since a multinomial language model has the constraint that w∈V p(w|d) = 1.",
                "However, from retrieval perspective, different terms may need to be smoothed differently even if they are in the same query.",
                "For example, a non-discriminative term (e.g., the, is) is expected to be explained more with the background model, while a content term (e.g., retrieval, bush) in the query should be explained with the document model.",
                "Therefore, a better way of smoothing would be to set the interpolation coefficient (i.e., δ in Formula 2 and Formula 3) specifically for each term.",
                "Since the Poisson language model does not have the sum-to-one constraint across terms, it can easily accommodate per-term smoothing without needing to heuristically twist the semantics of a generative model as in the case of multinomial language models.",
                "Below we present a possible way to explore term dependent smoothing with Poisson language models.",
                "Essentially, we want to use a term-specific smoothing coefficient δ in the linear combination, denoted as δw.",
                "This coefficient should intuitively be larger if w is a common word and smaller if it is a content word.",
                "The key problem is to find a method to assign reasonable values to δw.",
                "Empirical tuning is infeasible for so many parameters.",
                "We may instead estimate the parameters ∆ = {δ1, ..., δ|V |} by maximizing the likelihood of the query given the mixture model of p(q|ΛQ) and p(q|U), where ΛQ is the true query model to generate the query and p(q|U) is a query background model as discussed in Section 2.2.3.",
                "With the model p(q|ΛQ) hidden, the query likelihood is p(q|∆, U) = ΛQ w∈V ((1 − δw)p(c(w, q)|ΛQ) + δwp(c(w, q)|U))P(ΛQ|U)dΛQ If we have relevant documents for each query, we can approximate the query model space with the language models of all the relevant documents.",
                "Without relevant documents, we opt to approximate the query model space with the models of all the documents in the collection.",
                "Setting p(·|U) as p(·|C), the query likelihood becomes p(q|∆, U) = d∈C πd w∈V ((1−δw)p(c(w, q)|ˆΛd)+δwp(c(w, q)|C)) where πd = p(ˆΛd|U). p(·|ˆΛd) is an estimated Poisson language model for document d. If we have prior knowledge on p(ˆΛd|U), such as which documents are relevant to the query, we can set πd accordingly, because what we want is to find ∆ that can maximize the likelihood of the query given relevant documents.",
                "Without this prior knowledge, we can leave πd as free parameters, and use the EM algorithm to estimate πd and ∆.",
                "The updating functions are given as π (k+1) d = πd w∈V ((1 − δw)p(c(w, q)|ˆΛd) + δwp(c(w, q)|C)) d∈C πd w∈V ((1 − δw)p(c(w, q)|ˆΛd) + δwp(c(w, q)|C)) and δ (k+1) w = d∈C πd δwp(c(w, q)|C)) (1 − δw)p(c(w, q)|ˆΛd) + δwp(c(w, q)|C)) As discussed in [29], we only need to run the EM algorithm for several iterations, thus the computational cost is relatively low.",
                "We again assume our vocabulary containing all query terms plus a pseudo non-query term.",
                "Note that the function does not give an explicit way of estimating the coefficient for the unseen non-query term.",
                "In our experiments, we set it to the average over δw of all query terms.",
                "With this flexibility, we expect Poisson language models could improve the retrieval performance, especially for verbose queries, where the query terms have various discriminative values.",
                "In Section 4, we use empirical experiments to prove this hypothesis. 3.3 Mixture Background Models Another flexibility is to explore different background (collection) models (i.e., p(·|U), or p(·|C)).",
                "One common assumption made in language modeling information retrieval is that the background model is a homogeneous model of the document models [28, 29].",
                "Similarly, we can also make the assumption that the collection model is a Poisson language model, with the rates λC,w = d∈C c(w,d) |C| .",
                "However, this assumption usually does not hold, since the collection is far more complex than a single document.",
                "Indeed, the collection usually consists of a mixture of documents with various genres, authors, and topics, etc.",
                "Treating the collection model as a mixture of document models, instead of a single pseudo-document model is more reasonable.",
                "Existing work of multinomial language modeling has already shown that a better modeling of background improves the retrieval performance, such as clusters [15, 10], neighbor documents [25], and aspects [8, 27].",
                "All the approaches can be easily adopted using Poisson language models.",
                "However, a common problem of these approaches is that they all require heavy computation to construct the background model.",
                "With Poisson language modeling, we show that it is possible to model the mixture background without paying for the heavy computational cost.",
                "Poisson Mixture [3] has been proposed to model a collection of documents, which can fit the data much better than a single Poisson.",
                "The basic idea is to assume that the collection is generated from a mixture of Poisson models, which has the general form of p(x = k|PM) = λ p(λ)p(x = k|λ)dλ p(·|λ) is a single Poisson model and p(λ) is an arbitrary probability density function.",
                "There are three well known Poisson mixtures [3]: 2-Poisson, Negative Binomial, and the Katzs K-Mixture [9].",
                "Note that the 2-Poisson model has actually been explored in probabilistic retrieval models, which led to the well-known BM25 formula [22].",
                "All these mixtures have closed forms, and can be estimated from the collection of documents efficiently.",
                "This is an advantage over the multinomial mixture models, such as PLSI [8] and LDA [1], for retrieval.",
                "For example, the probability density function of Katzs K-Mixture is given as p(c(w) = k|αw, βw) = (1 − αw)ηk,0 + αw βw + 1 ( βw βw + 1 )k where ηk,0 = 1 when k = 0, and 0 otherwise.",
                "With the observation of a collection of documents, αw and βw can be estimated as βw = cf(w) − df(w) df(w) and αw = cf(w) Nβw where cf(w) and df(w) are the collection frequency and document frequency of w, and N is the number of documents in the collection.",
                "To account for the different document lengths, we assume that βw is a reasonable estimation for generating a document of the average length, and use β = βw avdl |q| to generate the query.",
                "This Poisson mixture model can be easily used to replace P(·|C) in the retrieval functions 3 and 4. 3.4 Other Possible Flexibilities In addition to term dependent smoothing and efficient mixture background, a Poisson language model has also some other potential advantages.",
                "For example, in Section 2, we see that Formula 2 introduces a component which does document length penalization.",
                "Intuitively, when the document has more unique words, it will be penalized more.",
                "On the other hand, if a document is exactly n copies of another document, it would not get over penalized.",
                "This feature is desirable and not achieved with the Dirichlet model [5].",
                "Potentially, this component could penalize a document according to what types of terms it contains.",
                "With term specific settings of δ, we could get even more flexibility for document length normalization.",
                "Pseudo-feedback is yet another interesting direction where the Poission model might be able to show its advantage.",
                "With model-based feedback, we could again relax the combination coefficients of the feedback model and the background model, and allow different terms to contribute differently to the feedback model.",
                "We could also utilize the relevant documents to learn better per-term smoothing coefficients. 4.",
                "EVALUATION In Section 3, we analytically compared the Poisson language models and multinomial language models from the perspective of query generation and retrieval.",
                "In this section, we compare these two families of models empirically.",
                "Experiment results show that the Poisson model with <br>perterm smoothing</br> outperforms multinomial model, and the performance can be further improved with two-stage smoothing.",
                "Using Poisson mixture as background model also improves the retrieval performance. 4.1 Datasets Since retrieval performance could significantly vary from one test collection to another, and from one query to another, we select four representative TREC test collections: AP, Trec7, Trec8, and Wt2g(Web).",
                "To cover different types of queries, we follow [28, 5], and construct short-keyword (SK, keyword title), short-verbose (SV, one sentence description), and long-verbose (LV, multiple sentences) queries.",
                "The documents are stemmed with the Porters stemmer, and we do not remove any stop word.",
                "For each parameter, we vary its value to cover a reasonably wide range. 4.2 Comparison to Multinomial We compare the performance of the Poisson retrieval models and multinomial retrieval models using interpolation (JelinekMercer, JM) smoothing and Bayesian smoothing with conjugate priors.",
                "Table 1 shows that the two JM-smoothed models perform similarly on all data sets.",
                "Since the Dirichlet Smoothing for multinomial language model and the Gamma Smoothing for Poisson language model lead to the same retrieval formula, the performance of these two models are jointly presented.",
                "We see that Dirichlet/Gamma smoothing methods outperform both Jelinek-Mercer smoothing methods.",
                "The parameter sensitivity curves for two Jelinek-Mercer 0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1 0 0.05 0.1 0.15 0.2 0.25 0.3 Dataset: Trec8 Parameter: δ AveragePrecision JM−Multinomial: LV JM−Multinomial: SV JM−Multinomial: SK JM−Poisson: SK JM−Poisson: SV JM−Poisson: LV Figure 1: Poisson and multinomial performs similarly with Jelinek-Mercer smoothing smoothing methods are shown in Figure 1.",
                "Clearly, these two methods perform similarly either in terms of optimality Data Query JM-Multinomial JM-Poisson Dirichlet/Gamma Per-term 2-Stage Poisson MAP InitPr Pr@5d MAP InitPr Pr@5d MAP InitPr Pr@5d MAP InitPr Pr@5d AP88-89 SK 0.203 0.585 0.356 0.203 0.585 0.358 0.224 0.629 0.393 0.226 0.630 0.396 SV 0.187 0.580 0.361 0.183 0.571 0.345 0.204 0.613 0.387 0.217* 0.603 0.390 LV 0.283 0.716 0.480 0.271 0.692 0.470 0.291 0.710 0.496 0.304* 0.695 0.510 Trec7 SK 0.167 0.635 0.400 0.168 0.635 0.404 0.186 0.687 0.428 0.185 0.646 0.436 SV 0.174 0.655 0.432 0.176 0.653 0.432 0.182 0.666 0.432 0.196* 0.660 0.440 LV 0.223 0.730 0.496 0.215 0.766 0.488 0.224 0.748 0.52 0.236* 0.738 0.512 Trec8 SK 0.239 0.621 0.440 0.239 0.621 0.436 0.257 0.718 0.496 0.256 0.704 0.468 SV 0.231 0.686 0.448 0.234 0.702 0.456 0.228 0.691 0.456 0.246* 0.692 0.476 LV 0.265 0.796 0.548 0.261 0.757 0.520 0.260 0.741 0.492 0.274* 0.766 0.508 Web SK 0.250 0.616 0.380 0.250 0.616 0.380 0.302 0.767 0.468 0.307 0.739 0.468 SV 0.214 0.611 0.392 0.217 0.609 0.384 0.273 0.693 0.508 0.292* 0.703 0.480 LV 0.266 0.790 0.464 0.259 0.776 0.452 0.283 0.756 0.496 0.311* 0.759 0.488 Table 1: Performance comparison between Poisson and Multinomial retrieval models: basic models perform comparably; term dependent two-stage smoothing significantly improves Poisson An asterisk (*) indicates that the difference between the performance of the term dependent two-stage smoothing and that of the Dirichlet/Gamma single smoothing is statistically significant according to the Wilcoxon signed rank test at the level of 0.05. or sensitivity.",
                "This similarity of performance is expected as we discussed in Section 3.1.",
                "Although the Poisson model and multinomial model are similar in terms of the basic model and/or with simple smoothing methods, the Poisson model has great potential and flexibility to be further improved.",
                "As shown in the rightmost column of Table 1, term dependent two-stage Poisson model consistently outperforms the basic smoothing models, especially for verbose queries.",
                "This model is given in Formula 4, with a Gamma smoothing for the document model p(·|d), and δw, which is term dependent.",
                "The parameter µ of the first stage Gamma smoothing is empirically tuned.",
                "The combination coefficients (i.e., ∆), are estimated with the EM algorithm in Section 3.2.",
                "The parameter sensitivity curves for Dirichlet/Gamma and the per-term two-stage smoothing model are plotted in Figure 2.",
                "The per-term two-stage smoothing method is less sensitive to the parameter µ than Dirichlet/Gamma, and yields better optimal performance. 0 1000 2000 3000 4000 5000 6000 7000 8000 9000 10000 0.1 0.12 0.14 0.16 0.18 0.2 0.22 Dataset: AP; Query Type: SV Parameter: µ AveragePrecision Dirichlet/Gamma Smoothing Term Dependent 2−Stage Figure 2: Term dependent two-stage smoothing of Poisson outperforms Dirichlet/Gamma In the following subsections, we conduct experiments to demonstrate how the flexibility of the Poisson model could be utilized to achieve better performance, which we cannot achieve with multinomial language models. 4.3 Term Dependent Smoothing To test the effectiveness of the term dependent smoothing, we conduct the following two experiments.",
                "In the first experiment, we relax the constant coefficient in the simple Jelinek-Mercer smoothing formula (i.e., Formula 3), and use the EM algorithm proposed in Section 3.2 to find a δw for each unique term.",
                "Since we are using the EM algorithm to iteratively estimate the parameters, we usually do not want the probability of p(·|d) to be zero.",
                "We then use a simple Laplace method to slightly smooth the document model before it goes into the EM iterations.",
                "The documents are then still scored with Formula 3, but using learnt δw.",
                "The results are labeled with JM+L. in Table 2.",
                "Data Q JM JM JM+L. 2-Stage 2-Stage (MAP) PT: No Yes Yes No Yes AP SK 0.203 0.204 0.206 0.223 0.226* SV 0.183 0.189 0.214* 0.204 0.217* Trec7 SK 0.168 0.171 0.174 0.186 0.185 SV 0.176 0.147 0.198* 0.194 0.196 Trec8 SK 0.239 0.240 0.227* 0.257 0.256 SV 0.234 0.223 0.249* 0.242 0.246* Web SK 0.250 0.236 0.220* 0.291 0.307* SV 0.217 0.232 0.261* 0.273 0.292* Table 2: Term dependent smoothing improves retrieval performance An asterisk (*) in Column 3 indicates that the difference between the JM+L. method and JM method is statistically significant; an asterisk (*) in Column 5 means that the difference between term dependent two-stage method and query dependent two-stage method is statistically significant; PT stands for per-term.",
                "With term dependent coefficients, the performance of the Jelinek-Mercer Poisson model is improved in most cases.",
                "However, in some cases (e.g., Trec7/SV), it performs poorly.",
                "This might be caused by the problem of EM estimation with unsmoothed document models.",
                "Once non-zero probability is assigned to all the terms before entering the EM iteration, the performance on verbose queries can be improved significantly.",
                "This indicates that there is still room to find better methods to estimate δw.",
                "Please note that neither the perterm JM method nor the JM+L. method has a parameter to tune.",
                "As shown in Table 1, the term dependent two-stage smoothing can significantly improve retrieval performance.",
                "To understand whether the improvement is contributed by the term dependent smoothing or the two-stage smoothing framework, we design another experiment to compare the perterm two-stage smoothing with the two-stage smoothing method proposed in [29].",
                "Their method managed to find coefficients specific to the query, thus a verbose query would use a higher δ.",
                "However, since their model is based on multinomial language modeling, they could not get per-term coefficients.",
                "We adopt their method to the Poisson two-stage smoothing, and also estimate a per-query coefficient for all the terms.",
                "We compare the performance of such a model with the per-term two-stage smoothing model, and present the results in the right two columns in Table 2.",
                "Again, we see that the per-term two-stage smoothing outperforms the per-query two-stage smoothing, especially for verbose queries.",
                "The improvement is not as large as how the <br>perterm smoothing</br> method improves over Dirichlet/Gamma.",
                "This is expected, since the per-query smoothing has already addressed the query discrimination problem to some extent.",
                "This experiment shows that even if the smoothing is already per-query, making it per-term is still beneficial.",
                "In brief, the per-term smoothing improved the retrieval performance of both one-stage and two-stage smoothing method. 4.4 Mixture Background Model In this section, we conduct experiments to examine the benefits of using a mixture background model without extra computational cost, which can not be achieved for multinomial models.",
                "Specifically, in retrieval formula 3, instead of using a single Poisson distribution to model the background p(·|C), we use Katzs K-Mixture model, which is essentially a mixture of Poisson distributions. p(·|C) can be computed efficiently with simple collection statistics, as discussed in Section 3.3.",
                "Data Query JM.",
                "Poisson JM.",
                "K-Mixture AP SK 0.203 0.204 SV 0.183 0.188* Trec-7 SK 0.168 0.169 SV 0.176 0.178* Trec-8 SK 0.239 0.239 SV 0.234 0.238* Web SK 0.250 0.250 SV 0.217 0.223* Table 3: K-Mixture background model improves retrieval performance The performance of the JM retrieval model with single Poisson background and with Katzs K-Mixture background model is compared in Table 3.",
                "Clearly, using K-Mixture to model the background model outperforms the single Poisson background model in most cases, especially for verbose queries where the improvement is statistically significant.",
                "Figure 3 shows that the performance changes over different parameters for short verbose queries.",
                "The model using K-Mixture background is less sensitive than the one using single Poisson background.",
                "Given that this type of mixture 0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1 0 0.05 0.1 0.15 0.2 0.25 Data: Trec8; Query: SV Parameter: δ AveragePrecision Poisson Background K−Mixture Background Figure 3: K-Mixture background model deviates the sensitivity of verbose queries background model does not require any extra computation cost, it would be interesting to study whether using other mixture Poisson models, such as 2-Poisson and negative Binomial, could help the performance. 5.",
                "RELATED WORK To the best of our knowledge, there has been no study of query generation models based on Poisson distribution.",
                "Language models have been shown to be effective for many retrieval tasks [21, 28, 14, 4].",
                "The most popular and fundamental one is the query-generation language model [21, 13].",
                "All existing query generation language models are based on either multinomial distribution [19, 6, 28, 13] or multivariate Bernoulli distribution [21, 17, 18].",
                "We introduce a new family of language models, based on Poisson distribution.",
                "Poisson distribution has been previously studied in the document generation models [16, 22, 3, 24], leading to the development of one of the most effective retrieval formula BM25 [23]. [24] studies the parallel derivation of three different retrieval models which is related to our comparison of Poisson and multinomial.",
                "However, the Poisson model in their paper is still under the document generation framework, and also does not account for the document length variation. [26] introduces a way to empirically search for an exponential model for the documents.",
                "Poisson mixtures [3] such as 2-Poisson [22], Negative multinomial, and Katzs KMixture [9] has shown to be effective to model and retrieve documents.",
                "Once again, none of this work explores Poisson distribution in the query generation framework.",
                "Language model smoothing [2, 28, 29] and background structures [15, 10, 25, 27] have been studied with multinomial language models. [7] analytically shows that term specific smoothing could be useful.",
                "We show that Poisson language model is natural to accommodate the per-term smoothing without heuristic twist of the semantics of a generative model, and is able to efficiently better model the mixture background, both analytically and empirically. 6.",
                "CONCLUSIONS We present a new family of query generation language models for retrieval based on Poisson distribution.",
                "We derive several smoothing methods for this family of models, including single-stage smoothing and two-stage smoothing.",
                "We compare the new models with the popular multinomial retrieval models both analytically and experimentally.",
                "Our analysis shows that while our new models and multinomial models are equivalent under some assumptions, they are generally different with some important differences.",
                "In particular, we show that Poisson has an advantage over multinomial in naturally accommodating per-term smoothing.",
                "We exploit this property to develop a new per-term smoothing algorithm for Poisson language models, which is shown to outperform term-independent smoothing for both Poisson and multinomial models.",
                "Furthermore, we show that a mixture background model for Poisson can be used to improve the performance and robustness over the standard Poisson background model.",
                "Our work opens up many interesting directions for further exploration in this new family of models.",
                "Further exploring the flexibilities over multinomial language models, such as length normalization and pseudo-feedback could be good future work.",
                "It is also appealing to find robust methods to learn the per-term smoothing coefficients without additional computation cost. 7.",
                "ACKNOWLEDGMENTS We thank the anonymous SIGIR 07 reviewers for their useful comments.",
                "This material is based in part upon work supported by the National Science Foundation under award numbers IIS-0347933 and 0425852. 8.",
                "REFERENCES [1] D. Blei, A. Ng, and M. Jordan.",
                "Latent dirichlet allocation.",
                "Journal of Machine Learning Research, 3:993-1022, 2003. [2] S. F. Chen and J. Goodman.",
                "An empirical study of smoothing techniques for language modeling.",
                "Technical Report TR-10-98, Harvard University, 1998. [3] K. Church and W. Gale.",
                "Poisson mixtures.",
                "Nat.",
                "Lang.",
                "Eng., 1(2):163-190, 1995. [4] W. B. Croft and J. Lafferty, editors.",
                "Language Modeling and Information Retrieval.",
                "Kluwer Academic Publishers, 2003. [5] H. Fang, T. Tao, and C. Zhai.",
                "A formal study of information retrieval heuristics.",
                "In Proceedings of the 27th annual international ACM SIGIR conference on Research and development in information retrieval, pages 49-56, 2004. [6] D. Hiemstra.",
                "Using Language Models for Information Retrieval.",
                "PhD thesis, University of Twente, Enschede, Netherlands, 2001. [7] D. Hiemstra.",
                "Term-specific smoothing for the language modeling approach to information retrieval: the importance of a query term.",
                "In Proceedings of the 25th annual international ACM SIGIR conference on Research and development in information retrieval, pages 35-41, 2002. [8] T. Hofmann.",
                "Probabilistic latent semantic indexing.",
                "In Proceedings of ACM SIGIR99, pages 50-57, 1999. [9] S. M. Katz.",
                "Distribution of content words and phrases in text and language modelling.",
                "Nat.",
                "Lang.",
                "Eng., 2(1):15-59, 1996. [10] O. Kurland and L. Lee.",
                "Corpus structure, language models, and ad-hoc information retrieval.",
                "In Proceedings of the 27th annual international ACM SIGIR conference on Research and development in information retrieval, pages 194-201, 2004. [11] J. Lafferty and C. Zhai.",
                "Document language models, query models, and risk minimization for information retrieval.",
                "In Proceedings of SIGIR01, pages 111-119, Sept 2001. [12] J. Lafferty and C. Zhai.",
                "Probabilistic IR models based on query and document generation.",
                "In Proceedings of the Language Modeling and IR workshop, pages 1-5, May 31 - June 1 2001. [13] J. Lafferty and C. Zhai.",
                "Probabilistic relevance models based on document and query generation.",
                "In W. B. Croft and J. Lafferty, editors, Language Modeling and Information Retrieval.",
                "Kluwer Academic Publishers, 2003. [14] V. Lavrenko and B. Croft.",
                "Relevance-based language models.",
                "In Proceedings of SIGIR01, pages 120-127, Sept 2001. [15] X. Liu and W. B. Croft.",
                "Cluster-based retrieval using language models.",
                "In Proceedings of the 27th annual international ACM SIGIR conference on Research and development in information retrieval, pages 186-193, 2004. [16] E. L. Margulis.",
                "Modelling documents with multiple poisson distributions.",
                "Inf.",
                "Process.",
                "Manage., 29(2):215-227, 1993. [17] A. McCallum and K. Nigam.",
                "A comparison of event models for naive bayes text classification.",
                "In Proceedings of AAAI-98 Workshop on Learning for Text Categorization, 1998. [18] D. Metzler, V. Lavrenko, and W. B. Croft.",
                "Formal multiple-bernoulli models for language modeling.",
                "In Proceedings of the 27th annual international ACM SIGIR conference on Research and development in information retrieval, pages 540-541, 2004. [19] D. H. Miller, T. Leek, and R. Schwartz.",
                "A hidden Markov model information retrieval system.",
                "In Proceedings of the 1999 ACM SIGIR Conference on Research and Development in Information Retrieval, pages 214-221, 1999. [20] A. Papoulis.",
                "Probability, random variables and stochastic processes.",
                "New York: McGraw-Hill, 1984, 2nd ed., 1984. [21] J. M. Ponte and W. B. Croft.",
                "A language modeling approach to information retrieval.",
                "In Proceedings of the 21st annual international ACM SIGIR conference on Research and development in information retrieval, pages 275-281, 1998. [22] S. Robertson and S. Walker.",
                "Some simple effective approximations to the 2-poisson model for probabilistic weighted retrieval.",
                "In Proceedings of SIGIR94, pages 232-241, 1994. [23] S. E. Robertson, S. Walker, S. Jones, M. M.Hancock-Beaulieu, and M. Gatford.",
                "Okapi at TREC-3.",
                "In D. K. Harman, editor, The Third Text REtrieval Conference (TREC-3), pages 109-126, 1995. [24] T. Roelleke and J. Wang.",
                "A parallel derivation of probabilistic information retrieval models.",
                "In Proceedings of the 29th annual international ACM SIGIR conference on Research and development in information retrieval, pages 107-114, 2006. [25] T. Tao, X. Wang, Q. Mei, and C. Zhai.",
                "Language model information retrieval with document expansion.",
                "In Proceedings of HLT/NAACL 2006, pages 407-414, 2006. [26] J. Teevan and D. R. Karger.",
                "Empirical development of an exponential probabilistic model for text retrieval: using textual analysis to build a better model.",
                "In Proceedings of the 26th annual international ACM SIGIR conference on Research and development in informaion retrieval, pages 18-25, 2003. [27] X. Wei and W. B. Croft.",
                "Lda-based document models for ad-hoc retrieval.",
                "In Proceedings of the 29th annual international ACM SIGIR conference on Research and development in information retrieval, pages 178-185, 2006. [28] C. Zhai and J. Lafferty.",
                "A study of smoothing methods for language models applied to ad-hoc information retrieval.",
                "In Proceedings of ACM SIGIR01, pages 334-342, Sept 2001. [29] C. Zhai and J. Lafferty.",
                "Two-stage language models for information retrieval.",
                "In Proceedings of ACM SIGIR02, pages 49-56, Aug 2002."
            ],
            "original_annotated_samples": [],
            "translated_annotated_samples": [
                "En particular, una diferencia clave es que el modelo de Poisson puede acomodar naturalmente el \"suavizado de temperatura\", que es difícil de lograr con un modelo multinomial sin giro heurístico de la semántica de un modelo generativo.suavizado de perteres",
                "Los resultados del experimento muestran que el modelo de Poisson con \"suavizado perterero\" supera el modelo multinomial, y el rendimiento puede mejorarse aún más con un suavizado de dos etapas.suavizado de perteres",
                "La mejora no es tan grande como la forma en que el método de \"suavizado por término\" mejora sobre Dirichlet/gamma.suavizado de perteres"
            ],
            "translated_text": "",
            "candidates": [],
            "error": []
        },
        "new term-dependent smoothing algorithm": {
            "translated_key": "",
            "is_in_text": true,
            "original_annotated_sentences": [
                "A Study of Poisson Query Generation Model for Information Retrieval Qiaozhu Mei, Hui Fang, Chengxiang Zhai Department of Computer Science University of Illinois at Urbana-Champaign Urbana,IL 61801 {qmei2,hfang,czhai}@uiuc.edu ABSTRACT Many variants of language models have been proposed for information retrieval.",
                "Most existing models are based on multinomial distribution and would score documents based on query likelihood computed based on a query generation probabilistic model.",
                "In this paper, we propose and study a new family of query generation models based on Poisson distribution.",
                "We show that while in their simplest forms, the new family of models and the existing multinomial models are equivalent, they behave differently for many smoothing methods.",
                "We show that the Poisson model has several advantages over the multinomial model, including naturally accommodating per-term smoothing and allowing for more accurate background modeling.",
                "We present several variants of the new model corresponding to different smoothing methods, and evaluate them on four representative TREC test collections.",
                "The results show that while their basic models perform comparably, the Poisson model can outperform multinomial model with per-term smoothing.",
                "The performance can be further improved with two-stage smoothing.",
                "Categories and Subject Descriptors: H.3.3 [Information Search and Retrieval]: Retrieval Models General Terms: Algorithms 1.",
                "INTRODUCTION As a new type of probabilistic retrieval models, language models have been shown to be effective for many retrieval tasks [21, 28, 14, 4].",
                "Among many variants of language models proposed, the most popular and fundamental one is the query-generation language model [21, 13], which leads to the query-likelihood scoring method for ranking documents.",
                "In such a model, given a query q and a document d, we compute the likelihood of generating query q with a model estimated based on document d, i.e., the conditional probability p(q|d).",
                "We can then rank documents based on the likelihood of generating the query.",
                "Virtually all the existing query generation language models are based on either multinomial distribution [19, 6, 28] or multivariate Bernoulli distribution [21, 18].",
                "The multinomial distribution is especially popular and also shown to be quite effective.",
                "The heavy use of multinomial distribution is partly due to the fact that it has been successfully used in speech recognition, where multinomial distribution is a natural choice for modeling the occurrence of a particular word in a particular position in text.",
                "Compared with multivariate Bernoulli, multinomial distribution has the advantage of being able to model the frequency of terms in the query; in contrast, multivariate Bernoulli only models the presence and absence of query terms, thus cannot capture different frequencies of query terms.",
                "However, multivariate Bernoulli also has one potential advantage over multinomial from the viewpoint of retrieval: in a multinomial distribution, the probabilities of all the terms must sum to 1, making it hard to accommodate per-term smoothing, while in a multivariate Bernoulli, the presence probabilities of different terms are completely independent of each other, easily accommodating per-term smoothing and weighting.",
                "Note that term absence is also indirectly captured in a multinomial model through the constraint that all the term probabilities must sum to 1.",
                "In this paper, we propose and study a new family of query generation models based on the Poisson distribution.",
                "In this new family of models, we model the frequency of each term independently with a Poisson distribution.",
                "To score a document, we would first estimate a multivariate Poisson model based on the document, and then score it based on the likelihood of the query given by the estimated Poisson model.",
                "In some sense, the Poisson model combines the advantage of multinomial in modeling term frequency and the advantage of the multivariate Bernoulli in accommodating per-term smoothing.",
                "Indeed, similar to the multinomial distribution, the Poisson distribution models term frequencies, but without the constraint that all the term probabilities must sum to 1, and similar to multivariate Bernoulli, it models each term independently, thus can easily accommodate per-term smoothing.",
                "As in the existing work on multinomial language models, smoothing is critical for this new family of models.",
                "We derive several smoothing methods for Poisson model in parallel to those used for multinomial distributions, and compare the corresponding retrieval models with those based on multinomial distributions.",
                "We find that while with some smoothing methods, the new model and the multinomial model lead to exactly the same formula, with some other smoothing methods they diverge, and the Poisson model brings in more flexibility for smoothing.",
                "In particular, a key difference is that the Poisson model can naturally accommodate perterm smoothing, which is hard to achieve with a multinomial model without heuristic twist of the semantics of a generative model.",
                "We exploit this potential advantage to develop a <br>new term-dependent smoothing algorithm</br> for Poisson model and show that this new smoothing algorithm can improve performance over term-independent smoothing algorithms using either Poisson or multinomial model.",
                "This advantage is seen for both one-stage and two-stage smoothing.",
                "Another potential advantage of the Poisson model is that its corresponding background model for smoothing can be improved through using a mixture model that has a closed form formula.",
                "This new background model is shown to outperform the standard background model and reduce the sensitivity of retrieval performance to the smoothing parameter.",
                "The rest of the paper is organized as follows.",
                "In Section 2, we introduce the new family of query generation models with Poisson distribution, and present various smoothing methods which lead to different retrieval functions.",
                "In Section 3, we analytically compare the Poisson language model with the multinomial language model, from the perspective of retrieval.",
                "We then design empirical experiments to compare the two families of language models in Section 4.",
                "We discuss the related work in 5 and conclude in 6. 2.",
                "QUERY GENERATION WITH POISSON PROCESS In the query generation framework, a basic assumption is that a query is generated with a model estimated based on a document.",
                "In most existing work [12, 6, 28, 29], people assume that each query word is sampled independently from a multinomial distribution.",
                "Alternatively, we assume that a query is generated by sampling the frequency of words from a series of independent Poisson processes [20]. 2.1 The Generation Process Let V = {w1, ..., wn} be a vocabulary set.",
                "Let w be a piece of text composed by an author and c(w1), ..., c(wn) be a frequency vector representing w, where c(wi, w) is the frequency count of term wi in text w. In retrieval, w could be either a query or a document.",
                "We consider the frequency counts of the n unique terms in w as n different types of events, sampled from n independent homogeneous Poisson processes, respectively.",
                "Suppose t is the time period during which the author composed the text.",
                "With a homogeneous Poisson process, the frequency count of each event, i.e., the number of occurrences of wi, follows a Poisson distribution with associated parameter λit, where λi is a rate parameter characterizing the expected number of wi in a unit time.",
                "The probability density function of such a Poisson Distribution is given by P(c(wi, w) = k|λit) = e−λit (λit)k k!",
                "Without losing generality, we set t to the length of the text w (people write one word in a unit time), i.e., t = |w|.",
                "With n such independent Poisson processes, each explaining the generation of one term in the vocabulary, the likelihood of w to be generated from such Poisson processes can be written as p(w|Λ) = n i=1 p(c(wi, w)|Λ) = n i=1 e−λi·|w| (λi · |w|)c(wi,w) c(wi, w)! where Λ = {λ1, ..., λn} and |w| = n i=1 c(wi, w).",
                "We refer to these n independent Poisson processes with parameter Λ as a Poisson Language Model.",
                "Let D = {d1, ..., dm} be an observed set of document samples generated from the Poisson process above.",
                "The maximum likelihood estimate (MLE) of λi is ˆλi = d∈D c(wi, d) d∈D w ∈V c(w , d) Note that this MLE is different from the MLE for the Poisson distribution without considering the document lengths, which appears in [22, 24].",
                "Given a document d, we may estimate a Poisson language model Λd using d as a sample.",
                "The likelihood that a query q is generated from the document language model Λd can be written as p(q|d) = w∈V p(c(w, q)|Λd) (1) This representation is clearly different from the multinomial query generation model as (1) the likelihood includes all the terms in the vocabulary V , instead of only those appearing in q, and (2) instead of the appearance of terms, the event space of this model is the frequencies of each term.",
                "In practice, we have the flexibility to choose the vocabulary V .",
                "In one extreme, we can use the vocabulary of the whole collection.",
                "However, this may bring in noise and considerable computational cost.",
                "In the other extreme, we may focus on the terms in the query and ignore other terms, but some useful information may be lost by ignoring the nonquery terms.",
                "As a compromise, we may conflate all the non-query terms as one single pseudo term.",
                "In other words, we may assume that there is exactly one non-query term in the vocabulary for each query.",
                "In our experiments, we adopt this pseudo non-query term strategy.",
                "A document can be scored with the likelihood in Equation 1.",
                "However, if a query term is unseen in the document, the MLE of the Poisson distribution would assign zero probability to the term, causing the probability of the query to be zero.",
                "As in existing language modeling approaches, the main challenge of constructing a reasonable retrieval model is to find a smoothed language model for p(·|d). 2.2 Smoothing in Poisson Retrieval Model In general, we want to assign non-zero rates for the query terms that are not seen in document d. Many smoothing methods have been proposed for multinomial language models[2, 28, 29].",
                "In general, we have to discount the probabilities of some words seen in the text to leave some extra probability mass to assign to the unseen words.",
                "In Poisson language models, however, we do not have the same constraint as in a multinomial model (i.e., w∈V p(w|d) = 1).",
                "Thus we do not have to discount the probability of seen words in order to give a non-zero rate to an unseen word.",
                "Instead, we only need to guarantee that k=0,1,2,... p(c(w, d) = k|d) = 1.",
                "In this section, we introduce three different strategies to smooth a Poisson language model, and show how they lead to different retrieval functions. 2.2.1 Bayesian Smoothing using Gamma Prior Following the risk minimization framework in [11], we assume that a document is generated by the arrival of terms in a time period of |d| according to the document language model, which essentially consists of a vector of Poisson rates for each term, i.e., Λd = λd,1, ..., λd,|V | .",
                "A document is assumed to be generated from a potentially different model.",
                "Given a particular document d, we want to estimate Λd.",
                "The rate of a term is estimated independently of other terms.",
                "We use Bayesian estimation with the following Gamma prior, which has two parameters, α and β: Gamma(λ|α, β) = βα Γ(α) λα−1 e−βλ For each term w, the parameters αw and βw are chosen to be αw = µ ∗ λC,w and βw = µ, where µ is a parameter and λC,w is the rate of w estimated from some background language model, usually the collection language model.",
                "The posterior distribution of Λd is given by p(Λd|d, C) ∝ w∈V e−λw(|d|+µ) λ c(w,d)+µλC,w−1 w which is a product of |V | Gamma distributions with parameters c(w, d) + µλC,w and |d| + µ for each word w. Given that the Gamma mean is α β , we have ˆλd,w = λd,w λd,wp(λd,w|d, C)dλd,w = c(w, d) + µλC,w |d| + µ This is precisely the smoothed estimate of multinomial language model with Dirichlet prior [28]. 2.2.2 Interpolation (Jelinek-Mercer) Smoothing Another straightforward method is to decompose the query generation model as a mixture of two component models.",
                "One is the document language model estimated with maximum likelihood estimator, and the other is a model estimated from the collection background, p(·|C), which assigns non-zero rate to w. For example, we may use an interpolation coefficient between 0 and 1 (i.e., δ ∈ [0, 1]).",
                "With this simple interpolation, we can score a document with Score(d, q) = w∈V log((1 − δ)p(c(w, q)|d) + δp(c(w, q)|C)) (2) Using the maximum likelihood estimator for p(·|d), we have λd,w = c(w,d) |d| , thus Equation 2 becomes Score(d, q) ∝ w∈d∩q [log(1 + 1 − δ δ e−λd,w|q| (λd,w|q|)c(w,q) c(w, q)! · p(c(w, q)|C) ) − log (1 − δ)e−λd,w|q| + δp(c(w, q) = 0|C) 1 − δ + δp(c(w, q) = 0|C) ] + w∈d log (1 − δ)e−λd,w|q| + δp(c(w, q) = 0|C) 1 − δ + δp(c(w, q) = 0|C) We can also use a Poisson language model for p(·|C), or use some other frequency-based models.",
                "In the retrieval formula above, the first summation can be computed efficiently.",
                "The second summation can be actually treated as a document prior, which penalizes long documents.",
                "As the second summation is difficult to compute efficiently, we conflate all non-query terms as one pseudo non-queryterm, denoted as N. Using the pseudo-term formulation and a Poisson collection model, we can rewrite the retrieval formula as Score(d, q) ∝ w∈d∩q log(1 + 1 − δ δ e−λd,w (λd,w|q|)c(w,q) e−λd,C |q| (λd,C )c(w,q) ) + log (1 − δ)e−λd,N |q| + δe−λC,N |q| 1 − δ + δe−λC,N |q| (3) where λd,N = |d|− w∈q c(w,d) |d| and λC,N = |C|− w∈q c(w,C) |C| . 2.2.3 Two-Stage Smoothing As discussed in [29], smoothing plays two roles in retrieval: (1) to improve the estimation of the document language model, and (2) to explain the common terms in the query.",
                "In order to distinguish the content and non-discriminative words in a query, we follow [29] and assume that a query is generated by sampling from a two-component mixture of Poisson language models, with one component being the document model Λd and the other being a query background language model p(·|U). p(·|U) models the typical term frequencies in the users queries.",
                "We may then score each document with the query likelihood computed using the following two-stage smoothing model: p(c(w, q)|Λd, U) = (1 − δ)p(c(w, q)|Λd) + δp(c(w, q)|U) (4) where δ is a parameter, roughly indicating the amount of noise in q.",
                "This looks similar to the interpolation smoothing, except that p(·|Λd) now should be a smoothed language model, instead of the one estimated with MLE.",
                "With no prior knowledge on p(·|U), we could set it to p(·|C).",
                "Any smoothing methods for the document language model can be used to estimate p(·|d) such as the Gamma smoothing as discussed in Section 2.2.1.",
                "The empirical study of the smoothing methods is presented in Section 4. 3.",
                "ANALYSIS OF POISSON LANGUAGE MODEL From the previous section, we notice that the Poisson language model has a strong connection to the multinomial language model.",
                "This is expected since they both belong to the exponential family [26].",
                "However, there are many differences when these two families of models are applied with different smoothing methods.",
                "From the perspective of retrieval, will these two language models perform equivalently?",
                "If not, which model provides more benefits to retrieval, or provides flexibility which could lead to potential benefits?",
                "In this section, we analytically discuss the retrieval features of the Poisson language models, by comparing their behavior with that of the multinomial language models. 3.1 The Equivalence of Basic Models Let us begin with the assumption that all the query terms appear in every document.",
                "Under this assumption, no smoothing is needed.",
                "A document can be scored by the log likelihood of the query with the maximum likelihood estimate: Score(d, q) = w∈V log e−λd,w|q| (λd,w|q|)c(w,q) c(w, q)! (5) Using the MLE, we have λd,w = c(w,d) w∈V c(w,d) .",
                "Thus Score(d, q) ∝ c(w,q)>0 c(w, q) log c(w, d) w∈V c(w, d) This is exactly the log likelihood of the query if the document language model is a multinomial with maximum likelihood estimate.",
                "Indeed, even with Gamma smoothing, when plugging λd,w = c(w,d)+µλC,w |d|+µ and λC,w = c(w,C) |C| into Equation 5, it is easy to show that Score(d, q) ∝ w∈q∩d c(w, q) log(1 + c(w, d) µ · c(w,C) |C| ) + |q| log µ |d| + µ (6) which is exactly the Dirichlet retrieval formula in [28].",
                "Note that this equivalence holds only when the document length variation is modeled with Poisson process.",
                "This derivation indicates the equivalence of the basic Poisson and multinomial language models for retrieval.",
                "With other smoothing strategies, however, the two models would be different.",
                "Nevertheless, with this equivalence in basic models, we could expect that the Poisson language model performs comparably to the multinomial language model in retrieval, if only simple smoothing is explored.",
                "Based on this equivalence analysis, one may ask, why we should pursue the Poisson language model.",
                "In the following sections, we show that despite the equivalence in their basic models, the Poisson language model brings in extra flexibility for exploring advanced techniques on various retrieval features, which could not be achieved with multinomial language models. 3.2 Term Dependent Smoothing One flexibility of the Poisson language model is that it provides a natural framework to accommodate term dependent (per-term) smoothing.",
                "Existing work on language model smoothing has already shown that different types of queries should be smoothed differently according to how discriminative the query terms are. [7] also predicted that different terms should have a different smoothing weights.",
                "With multinomial query generation models, people usually use a single smoothing coefficient to control the combination of the document model and the background model [28, 29].",
                "This parameter can be made specific for different queries, but always has to be a constant for all the terms.",
                "This is mandatory since a multinomial language model has the constraint that w∈V p(w|d) = 1.",
                "However, from retrieval perspective, different terms may need to be smoothed differently even if they are in the same query.",
                "For example, a non-discriminative term (e.g., the, is) is expected to be explained more with the background model, while a content term (e.g., retrieval, bush) in the query should be explained with the document model.",
                "Therefore, a better way of smoothing would be to set the interpolation coefficient (i.e., δ in Formula 2 and Formula 3) specifically for each term.",
                "Since the Poisson language model does not have the sum-to-one constraint across terms, it can easily accommodate per-term smoothing without needing to heuristically twist the semantics of a generative model as in the case of multinomial language models.",
                "Below we present a possible way to explore term dependent smoothing with Poisson language models.",
                "Essentially, we want to use a term-specific smoothing coefficient δ in the linear combination, denoted as δw.",
                "This coefficient should intuitively be larger if w is a common word and smaller if it is a content word.",
                "The key problem is to find a method to assign reasonable values to δw.",
                "Empirical tuning is infeasible for so many parameters.",
                "We may instead estimate the parameters ∆ = {δ1, ..., δ|V |} by maximizing the likelihood of the query given the mixture model of p(q|ΛQ) and p(q|U), where ΛQ is the true query model to generate the query and p(q|U) is a query background model as discussed in Section 2.2.3.",
                "With the model p(q|ΛQ) hidden, the query likelihood is p(q|∆, U) = ΛQ w∈V ((1 − δw)p(c(w, q)|ΛQ) + δwp(c(w, q)|U))P(ΛQ|U)dΛQ If we have relevant documents for each query, we can approximate the query model space with the language models of all the relevant documents.",
                "Without relevant documents, we opt to approximate the query model space with the models of all the documents in the collection.",
                "Setting p(·|U) as p(·|C), the query likelihood becomes p(q|∆, U) = d∈C πd w∈V ((1−δw)p(c(w, q)|ˆΛd)+δwp(c(w, q)|C)) where πd = p(ˆΛd|U). p(·|ˆΛd) is an estimated Poisson language model for document d. If we have prior knowledge on p(ˆΛd|U), such as which documents are relevant to the query, we can set πd accordingly, because what we want is to find ∆ that can maximize the likelihood of the query given relevant documents.",
                "Without this prior knowledge, we can leave πd as free parameters, and use the EM algorithm to estimate πd and ∆.",
                "The updating functions are given as π (k+1) d = πd w∈V ((1 − δw)p(c(w, q)|ˆΛd) + δwp(c(w, q)|C)) d∈C πd w∈V ((1 − δw)p(c(w, q)|ˆΛd) + δwp(c(w, q)|C)) and δ (k+1) w = d∈C πd δwp(c(w, q)|C)) (1 − δw)p(c(w, q)|ˆΛd) + δwp(c(w, q)|C)) As discussed in [29], we only need to run the EM algorithm for several iterations, thus the computational cost is relatively low.",
                "We again assume our vocabulary containing all query terms plus a pseudo non-query term.",
                "Note that the function does not give an explicit way of estimating the coefficient for the unseen non-query term.",
                "In our experiments, we set it to the average over δw of all query terms.",
                "With this flexibility, we expect Poisson language models could improve the retrieval performance, especially for verbose queries, where the query terms have various discriminative values.",
                "In Section 4, we use empirical experiments to prove this hypothesis. 3.3 Mixture Background Models Another flexibility is to explore different background (collection) models (i.e., p(·|U), or p(·|C)).",
                "One common assumption made in language modeling information retrieval is that the background model is a homogeneous model of the document models [28, 29].",
                "Similarly, we can also make the assumption that the collection model is a Poisson language model, with the rates λC,w = d∈C c(w,d) |C| .",
                "However, this assumption usually does not hold, since the collection is far more complex than a single document.",
                "Indeed, the collection usually consists of a mixture of documents with various genres, authors, and topics, etc.",
                "Treating the collection model as a mixture of document models, instead of a single pseudo-document model is more reasonable.",
                "Existing work of multinomial language modeling has already shown that a better modeling of background improves the retrieval performance, such as clusters [15, 10], neighbor documents [25], and aspects [8, 27].",
                "All the approaches can be easily adopted using Poisson language models.",
                "However, a common problem of these approaches is that they all require heavy computation to construct the background model.",
                "With Poisson language modeling, we show that it is possible to model the mixture background without paying for the heavy computational cost.",
                "Poisson Mixture [3] has been proposed to model a collection of documents, which can fit the data much better than a single Poisson.",
                "The basic idea is to assume that the collection is generated from a mixture of Poisson models, which has the general form of p(x = k|PM) = λ p(λ)p(x = k|λ)dλ p(·|λ) is a single Poisson model and p(λ) is an arbitrary probability density function.",
                "There are three well known Poisson mixtures [3]: 2-Poisson, Negative Binomial, and the Katzs K-Mixture [9].",
                "Note that the 2-Poisson model has actually been explored in probabilistic retrieval models, which led to the well-known BM25 formula [22].",
                "All these mixtures have closed forms, and can be estimated from the collection of documents efficiently.",
                "This is an advantage over the multinomial mixture models, such as PLSI [8] and LDA [1], for retrieval.",
                "For example, the probability density function of Katzs K-Mixture is given as p(c(w) = k|αw, βw) = (1 − αw)ηk,0 + αw βw + 1 ( βw βw + 1 )k where ηk,0 = 1 when k = 0, and 0 otherwise.",
                "With the observation of a collection of documents, αw and βw can be estimated as βw = cf(w) − df(w) df(w) and αw = cf(w) Nβw where cf(w) and df(w) are the collection frequency and document frequency of w, and N is the number of documents in the collection.",
                "To account for the different document lengths, we assume that βw is a reasonable estimation for generating a document of the average length, and use β = βw avdl |q| to generate the query.",
                "This Poisson mixture model can be easily used to replace P(·|C) in the retrieval functions 3 and 4. 3.4 Other Possible Flexibilities In addition to term dependent smoothing and efficient mixture background, a Poisson language model has also some other potential advantages.",
                "For example, in Section 2, we see that Formula 2 introduces a component which does document length penalization.",
                "Intuitively, when the document has more unique words, it will be penalized more.",
                "On the other hand, if a document is exactly n copies of another document, it would not get over penalized.",
                "This feature is desirable and not achieved with the Dirichlet model [5].",
                "Potentially, this component could penalize a document according to what types of terms it contains.",
                "With term specific settings of δ, we could get even more flexibility for document length normalization.",
                "Pseudo-feedback is yet another interesting direction where the Poission model might be able to show its advantage.",
                "With model-based feedback, we could again relax the combination coefficients of the feedback model and the background model, and allow different terms to contribute differently to the feedback model.",
                "We could also utilize the relevant documents to learn better per-term smoothing coefficients. 4.",
                "EVALUATION In Section 3, we analytically compared the Poisson language models and multinomial language models from the perspective of query generation and retrieval.",
                "In this section, we compare these two families of models empirically.",
                "Experiment results show that the Poisson model with perterm smoothing outperforms multinomial model, and the performance can be further improved with two-stage smoothing.",
                "Using Poisson mixture as background model also improves the retrieval performance. 4.1 Datasets Since retrieval performance could significantly vary from one test collection to another, and from one query to another, we select four representative TREC test collections: AP, Trec7, Trec8, and Wt2g(Web).",
                "To cover different types of queries, we follow [28, 5], and construct short-keyword (SK, keyword title), short-verbose (SV, one sentence description), and long-verbose (LV, multiple sentences) queries.",
                "The documents are stemmed with the Porters stemmer, and we do not remove any stop word.",
                "For each parameter, we vary its value to cover a reasonably wide range. 4.2 Comparison to Multinomial We compare the performance of the Poisson retrieval models and multinomial retrieval models using interpolation (JelinekMercer, JM) smoothing and Bayesian smoothing with conjugate priors.",
                "Table 1 shows that the two JM-smoothed models perform similarly on all data sets.",
                "Since the Dirichlet Smoothing for multinomial language model and the Gamma Smoothing for Poisson language model lead to the same retrieval formula, the performance of these two models are jointly presented.",
                "We see that Dirichlet/Gamma smoothing methods outperform both Jelinek-Mercer smoothing methods.",
                "The parameter sensitivity curves for two Jelinek-Mercer 0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1 0 0.05 0.1 0.15 0.2 0.25 0.3 Dataset: Trec8 Parameter: δ AveragePrecision JM−Multinomial: LV JM−Multinomial: SV JM−Multinomial: SK JM−Poisson: SK JM−Poisson: SV JM−Poisson: LV Figure 1: Poisson and multinomial performs similarly with Jelinek-Mercer smoothing smoothing methods are shown in Figure 1.",
                "Clearly, these two methods perform similarly either in terms of optimality Data Query JM-Multinomial JM-Poisson Dirichlet/Gamma Per-term 2-Stage Poisson MAP InitPr Pr@5d MAP InitPr Pr@5d MAP InitPr Pr@5d MAP InitPr Pr@5d AP88-89 SK 0.203 0.585 0.356 0.203 0.585 0.358 0.224 0.629 0.393 0.226 0.630 0.396 SV 0.187 0.580 0.361 0.183 0.571 0.345 0.204 0.613 0.387 0.217* 0.603 0.390 LV 0.283 0.716 0.480 0.271 0.692 0.470 0.291 0.710 0.496 0.304* 0.695 0.510 Trec7 SK 0.167 0.635 0.400 0.168 0.635 0.404 0.186 0.687 0.428 0.185 0.646 0.436 SV 0.174 0.655 0.432 0.176 0.653 0.432 0.182 0.666 0.432 0.196* 0.660 0.440 LV 0.223 0.730 0.496 0.215 0.766 0.488 0.224 0.748 0.52 0.236* 0.738 0.512 Trec8 SK 0.239 0.621 0.440 0.239 0.621 0.436 0.257 0.718 0.496 0.256 0.704 0.468 SV 0.231 0.686 0.448 0.234 0.702 0.456 0.228 0.691 0.456 0.246* 0.692 0.476 LV 0.265 0.796 0.548 0.261 0.757 0.520 0.260 0.741 0.492 0.274* 0.766 0.508 Web SK 0.250 0.616 0.380 0.250 0.616 0.380 0.302 0.767 0.468 0.307 0.739 0.468 SV 0.214 0.611 0.392 0.217 0.609 0.384 0.273 0.693 0.508 0.292* 0.703 0.480 LV 0.266 0.790 0.464 0.259 0.776 0.452 0.283 0.756 0.496 0.311* 0.759 0.488 Table 1: Performance comparison between Poisson and Multinomial retrieval models: basic models perform comparably; term dependent two-stage smoothing significantly improves Poisson An asterisk (*) indicates that the difference between the performance of the term dependent two-stage smoothing and that of the Dirichlet/Gamma single smoothing is statistically significant according to the Wilcoxon signed rank test at the level of 0.05. or sensitivity.",
                "This similarity of performance is expected as we discussed in Section 3.1.",
                "Although the Poisson model and multinomial model are similar in terms of the basic model and/or with simple smoothing methods, the Poisson model has great potential and flexibility to be further improved.",
                "As shown in the rightmost column of Table 1, term dependent two-stage Poisson model consistently outperforms the basic smoothing models, especially for verbose queries.",
                "This model is given in Formula 4, with a Gamma smoothing for the document model p(·|d), and δw, which is term dependent.",
                "The parameter µ of the first stage Gamma smoothing is empirically tuned.",
                "The combination coefficients (i.e., ∆), are estimated with the EM algorithm in Section 3.2.",
                "The parameter sensitivity curves for Dirichlet/Gamma and the per-term two-stage smoothing model are plotted in Figure 2.",
                "The per-term two-stage smoothing method is less sensitive to the parameter µ than Dirichlet/Gamma, and yields better optimal performance. 0 1000 2000 3000 4000 5000 6000 7000 8000 9000 10000 0.1 0.12 0.14 0.16 0.18 0.2 0.22 Dataset: AP; Query Type: SV Parameter: µ AveragePrecision Dirichlet/Gamma Smoothing Term Dependent 2−Stage Figure 2: Term dependent two-stage smoothing of Poisson outperforms Dirichlet/Gamma In the following subsections, we conduct experiments to demonstrate how the flexibility of the Poisson model could be utilized to achieve better performance, which we cannot achieve with multinomial language models. 4.3 Term Dependent Smoothing To test the effectiveness of the term dependent smoothing, we conduct the following two experiments.",
                "In the first experiment, we relax the constant coefficient in the simple Jelinek-Mercer smoothing formula (i.e., Formula 3), and use the EM algorithm proposed in Section 3.2 to find a δw for each unique term.",
                "Since we are using the EM algorithm to iteratively estimate the parameters, we usually do not want the probability of p(·|d) to be zero.",
                "We then use a simple Laplace method to slightly smooth the document model before it goes into the EM iterations.",
                "The documents are then still scored with Formula 3, but using learnt δw.",
                "The results are labeled with JM+L. in Table 2.",
                "Data Q JM JM JM+L. 2-Stage 2-Stage (MAP) PT: No Yes Yes No Yes AP SK 0.203 0.204 0.206 0.223 0.226* SV 0.183 0.189 0.214* 0.204 0.217* Trec7 SK 0.168 0.171 0.174 0.186 0.185 SV 0.176 0.147 0.198* 0.194 0.196 Trec8 SK 0.239 0.240 0.227* 0.257 0.256 SV 0.234 0.223 0.249* 0.242 0.246* Web SK 0.250 0.236 0.220* 0.291 0.307* SV 0.217 0.232 0.261* 0.273 0.292* Table 2: Term dependent smoothing improves retrieval performance An asterisk (*) in Column 3 indicates that the difference between the JM+L. method and JM method is statistically significant; an asterisk (*) in Column 5 means that the difference between term dependent two-stage method and query dependent two-stage method is statistically significant; PT stands for per-term.",
                "With term dependent coefficients, the performance of the Jelinek-Mercer Poisson model is improved in most cases.",
                "However, in some cases (e.g., Trec7/SV), it performs poorly.",
                "This might be caused by the problem of EM estimation with unsmoothed document models.",
                "Once non-zero probability is assigned to all the terms before entering the EM iteration, the performance on verbose queries can be improved significantly.",
                "This indicates that there is still room to find better methods to estimate δw.",
                "Please note that neither the perterm JM method nor the JM+L. method has a parameter to tune.",
                "As shown in Table 1, the term dependent two-stage smoothing can significantly improve retrieval performance.",
                "To understand whether the improvement is contributed by the term dependent smoothing or the two-stage smoothing framework, we design another experiment to compare the perterm two-stage smoothing with the two-stage smoothing method proposed in [29].",
                "Their method managed to find coefficients specific to the query, thus a verbose query would use a higher δ.",
                "However, since their model is based on multinomial language modeling, they could not get per-term coefficients.",
                "We adopt their method to the Poisson two-stage smoothing, and also estimate a per-query coefficient for all the terms.",
                "We compare the performance of such a model with the per-term two-stage smoothing model, and present the results in the right two columns in Table 2.",
                "Again, we see that the per-term two-stage smoothing outperforms the per-query two-stage smoothing, especially for verbose queries.",
                "The improvement is not as large as how the perterm smoothing method improves over Dirichlet/Gamma.",
                "This is expected, since the per-query smoothing has already addressed the query discrimination problem to some extent.",
                "This experiment shows that even if the smoothing is already per-query, making it per-term is still beneficial.",
                "In brief, the per-term smoothing improved the retrieval performance of both one-stage and two-stage smoothing method. 4.4 Mixture Background Model In this section, we conduct experiments to examine the benefits of using a mixture background model without extra computational cost, which can not be achieved for multinomial models.",
                "Specifically, in retrieval formula 3, instead of using a single Poisson distribution to model the background p(·|C), we use Katzs K-Mixture model, which is essentially a mixture of Poisson distributions. p(·|C) can be computed efficiently with simple collection statistics, as discussed in Section 3.3.",
                "Data Query JM.",
                "Poisson JM.",
                "K-Mixture AP SK 0.203 0.204 SV 0.183 0.188* Trec-7 SK 0.168 0.169 SV 0.176 0.178* Trec-8 SK 0.239 0.239 SV 0.234 0.238* Web SK 0.250 0.250 SV 0.217 0.223* Table 3: K-Mixture background model improves retrieval performance The performance of the JM retrieval model with single Poisson background and with Katzs K-Mixture background model is compared in Table 3.",
                "Clearly, using K-Mixture to model the background model outperforms the single Poisson background model in most cases, especially for verbose queries where the improvement is statistically significant.",
                "Figure 3 shows that the performance changes over different parameters for short verbose queries.",
                "The model using K-Mixture background is less sensitive than the one using single Poisson background.",
                "Given that this type of mixture 0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1 0 0.05 0.1 0.15 0.2 0.25 Data: Trec8; Query: SV Parameter: δ AveragePrecision Poisson Background K−Mixture Background Figure 3: K-Mixture background model deviates the sensitivity of verbose queries background model does not require any extra computation cost, it would be interesting to study whether using other mixture Poisson models, such as 2-Poisson and negative Binomial, could help the performance. 5.",
                "RELATED WORK To the best of our knowledge, there has been no study of query generation models based on Poisson distribution.",
                "Language models have been shown to be effective for many retrieval tasks [21, 28, 14, 4].",
                "The most popular and fundamental one is the query-generation language model [21, 13].",
                "All existing query generation language models are based on either multinomial distribution [19, 6, 28, 13] or multivariate Bernoulli distribution [21, 17, 18].",
                "We introduce a new family of language models, based on Poisson distribution.",
                "Poisson distribution has been previously studied in the document generation models [16, 22, 3, 24], leading to the development of one of the most effective retrieval formula BM25 [23]. [24] studies the parallel derivation of three different retrieval models which is related to our comparison of Poisson and multinomial.",
                "However, the Poisson model in their paper is still under the document generation framework, and also does not account for the document length variation. [26] introduces a way to empirically search for an exponential model for the documents.",
                "Poisson mixtures [3] such as 2-Poisson [22], Negative multinomial, and Katzs KMixture [9] has shown to be effective to model and retrieve documents.",
                "Once again, none of this work explores Poisson distribution in the query generation framework.",
                "Language model smoothing [2, 28, 29] and background structures [15, 10, 25, 27] have been studied with multinomial language models. [7] analytically shows that term specific smoothing could be useful.",
                "We show that Poisson language model is natural to accommodate the per-term smoothing without heuristic twist of the semantics of a generative model, and is able to efficiently better model the mixture background, both analytically and empirically. 6.",
                "CONCLUSIONS We present a new family of query generation language models for retrieval based on Poisson distribution.",
                "We derive several smoothing methods for this family of models, including single-stage smoothing and two-stage smoothing.",
                "We compare the new models with the popular multinomial retrieval models both analytically and experimentally.",
                "Our analysis shows that while our new models and multinomial models are equivalent under some assumptions, they are generally different with some important differences.",
                "In particular, we show that Poisson has an advantage over multinomial in naturally accommodating per-term smoothing.",
                "We exploit this property to develop a new per-term smoothing algorithm for Poisson language models, which is shown to outperform term-independent smoothing for both Poisson and multinomial models.",
                "Furthermore, we show that a mixture background model for Poisson can be used to improve the performance and robustness over the standard Poisson background model.",
                "Our work opens up many interesting directions for further exploration in this new family of models.",
                "Further exploring the flexibilities over multinomial language models, such as length normalization and pseudo-feedback could be good future work.",
                "It is also appealing to find robust methods to learn the per-term smoothing coefficients without additional computation cost. 7.",
                "ACKNOWLEDGMENTS We thank the anonymous SIGIR 07 reviewers for their useful comments.",
                "This material is based in part upon work supported by the National Science Foundation under award numbers IIS-0347933 and 0425852. 8.",
                "REFERENCES [1] D. Blei, A. Ng, and M. Jordan.",
                "Latent dirichlet allocation.",
                "Journal of Machine Learning Research, 3:993-1022, 2003. [2] S. F. Chen and J. Goodman.",
                "An empirical study of smoothing techniques for language modeling.",
                "Technical Report TR-10-98, Harvard University, 1998. [3] K. Church and W. Gale.",
                "Poisson mixtures.",
                "Nat.",
                "Lang.",
                "Eng., 1(2):163-190, 1995. [4] W. B. Croft and J. Lafferty, editors.",
                "Language Modeling and Information Retrieval.",
                "Kluwer Academic Publishers, 2003. [5] H. Fang, T. Tao, and C. Zhai.",
                "A formal study of information retrieval heuristics.",
                "In Proceedings of the 27th annual international ACM SIGIR conference on Research and development in information retrieval, pages 49-56, 2004. [6] D. Hiemstra.",
                "Using Language Models for Information Retrieval.",
                "PhD thesis, University of Twente, Enschede, Netherlands, 2001. [7] D. Hiemstra.",
                "Term-specific smoothing for the language modeling approach to information retrieval: the importance of a query term.",
                "In Proceedings of the 25th annual international ACM SIGIR conference on Research and development in information retrieval, pages 35-41, 2002. [8] T. Hofmann.",
                "Probabilistic latent semantic indexing.",
                "In Proceedings of ACM SIGIR99, pages 50-57, 1999. [9] S. M. Katz.",
                "Distribution of content words and phrases in text and language modelling.",
                "Nat.",
                "Lang.",
                "Eng., 2(1):15-59, 1996. [10] O. Kurland and L. Lee.",
                "Corpus structure, language models, and ad-hoc information retrieval.",
                "In Proceedings of the 27th annual international ACM SIGIR conference on Research and development in information retrieval, pages 194-201, 2004. [11] J. Lafferty and C. Zhai.",
                "Document language models, query models, and risk minimization for information retrieval.",
                "In Proceedings of SIGIR01, pages 111-119, Sept 2001. [12] J. Lafferty and C. Zhai.",
                "Probabilistic IR models based on query and document generation.",
                "In Proceedings of the Language Modeling and IR workshop, pages 1-5, May 31 - June 1 2001. [13] J. Lafferty and C. Zhai.",
                "Probabilistic relevance models based on document and query generation.",
                "In W. B. Croft and J. Lafferty, editors, Language Modeling and Information Retrieval.",
                "Kluwer Academic Publishers, 2003. [14] V. Lavrenko and B. Croft.",
                "Relevance-based language models.",
                "In Proceedings of SIGIR01, pages 120-127, Sept 2001. [15] X. Liu and W. B. Croft.",
                "Cluster-based retrieval using language models.",
                "In Proceedings of the 27th annual international ACM SIGIR conference on Research and development in information retrieval, pages 186-193, 2004. [16] E. L. Margulis.",
                "Modelling documents with multiple poisson distributions.",
                "Inf.",
                "Process.",
                "Manage., 29(2):215-227, 1993. [17] A. McCallum and K. Nigam.",
                "A comparison of event models for naive bayes text classification.",
                "In Proceedings of AAAI-98 Workshop on Learning for Text Categorization, 1998. [18] D. Metzler, V. Lavrenko, and W. B. Croft.",
                "Formal multiple-bernoulli models for language modeling.",
                "In Proceedings of the 27th annual international ACM SIGIR conference on Research and development in information retrieval, pages 540-541, 2004. [19] D. H. Miller, T. Leek, and R. Schwartz.",
                "A hidden Markov model information retrieval system.",
                "In Proceedings of the 1999 ACM SIGIR Conference on Research and Development in Information Retrieval, pages 214-221, 1999. [20] A. Papoulis.",
                "Probability, random variables and stochastic processes.",
                "New York: McGraw-Hill, 1984, 2nd ed., 1984. [21] J. M. Ponte and W. B. Croft.",
                "A language modeling approach to information retrieval.",
                "In Proceedings of the 21st annual international ACM SIGIR conference on Research and development in information retrieval, pages 275-281, 1998. [22] S. Robertson and S. Walker.",
                "Some simple effective approximations to the 2-poisson model for probabilistic weighted retrieval.",
                "In Proceedings of SIGIR94, pages 232-241, 1994. [23] S. E. Robertson, S. Walker, S. Jones, M. M.Hancock-Beaulieu, and M. Gatford.",
                "Okapi at TREC-3.",
                "In D. K. Harman, editor, The Third Text REtrieval Conference (TREC-3), pages 109-126, 1995. [24] T. Roelleke and J. Wang.",
                "A parallel derivation of probabilistic information retrieval models.",
                "In Proceedings of the 29th annual international ACM SIGIR conference on Research and development in information retrieval, pages 107-114, 2006. [25] T. Tao, X. Wang, Q. Mei, and C. Zhai.",
                "Language model information retrieval with document expansion.",
                "In Proceedings of HLT/NAACL 2006, pages 407-414, 2006. [26] J. Teevan and D. R. Karger.",
                "Empirical development of an exponential probabilistic model for text retrieval: using textual analysis to build a better model.",
                "In Proceedings of the 26th annual international ACM SIGIR conference on Research and development in informaion retrieval, pages 18-25, 2003. [27] X. Wei and W. B. Croft.",
                "Lda-based document models for ad-hoc retrieval.",
                "In Proceedings of the 29th annual international ACM SIGIR conference on Research and development in information retrieval, pages 178-185, 2006. [28] C. Zhai and J. Lafferty.",
                "A study of smoothing methods for language models applied to ad-hoc information retrieval.",
                "In Proceedings of ACM SIGIR01, pages 334-342, Sept 2001. [29] C. Zhai and J. Lafferty.",
                "Two-stage language models for information retrieval.",
                "In Proceedings of ACM SIGIR02, pages 49-56, Aug 2002."
            ],
            "original_annotated_samples": [],
            "translated_annotated_samples": [
                "Explotamos esta ventaja potencial para desarrollar un \"nuevo algoritmo de suavizado dependiente del término\" para el modelo Poisson y mostramos que este nuevo algoritmo de suavizado puede mejorar el rendimiento sobre los algoritmos de suavizado independientes del término utilizando el modelo Poisson o multinomial.Nuevo algoritmo de suavizado dependiente de término"
            ],
            "translated_text": "",
            "candidates": [],
            "error": []
        },
        "vocabulary set": {
            "translated_key": "",
            "is_in_text": true,
            "original_annotated_sentences": [
                "A Study of Poisson Query Generation Model for Information Retrieval Qiaozhu Mei, Hui Fang, Chengxiang Zhai Department of Computer Science University of Illinois at Urbana-Champaign Urbana,IL 61801 {qmei2,hfang,czhai}@uiuc.edu ABSTRACT Many variants of language models have been proposed for information retrieval.",
                "Most existing models are based on multinomial distribution and would score documents based on query likelihood computed based on a query generation probabilistic model.",
                "In this paper, we propose and study a new family of query generation models based on Poisson distribution.",
                "We show that while in their simplest forms, the new family of models and the existing multinomial models are equivalent, they behave differently for many smoothing methods.",
                "We show that the Poisson model has several advantages over the multinomial model, including naturally accommodating per-term smoothing and allowing for more accurate background modeling.",
                "We present several variants of the new model corresponding to different smoothing methods, and evaluate them on four representative TREC test collections.",
                "The results show that while their basic models perform comparably, the Poisson model can outperform multinomial model with per-term smoothing.",
                "The performance can be further improved with two-stage smoothing.",
                "Categories and Subject Descriptors: H.3.3 [Information Search and Retrieval]: Retrieval Models General Terms: Algorithms 1.",
                "INTRODUCTION As a new type of probabilistic retrieval models, language models have been shown to be effective for many retrieval tasks [21, 28, 14, 4].",
                "Among many variants of language models proposed, the most popular and fundamental one is the query-generation language model [21, 13], which leads to the query-likelihood scoring method for ranking documents.",
                "In such a model, given a query q and a document d, we compute the likelihood of generating query q with a model estimated based on document d, i.e., the conditional probability p(q|d).",
                "We can then rank documents based on the likelihood of generating the query.",
                "Virtually all the existing query generation language models are based on either multinomial distribution [19, 6, 28] or multivariate Bernoulli distribution [21, 18].",
                "The multinomial distribution is especially popular and also shown to be quite effective.",
                "The heavy use of multinomial distribution is partly due to the fact that it has been successfully used in speech recognition, where multinomial distribution is a natural choice for modeling the occurrence of a particular word in a particular position in text.",
                "Compared with multivariate Bernoulli, multinomial distribution has the advantage of being able to model the frequency of terms in the query; in contrast, multivariate Bernoulli only models the presence and absence of query terms, thus cannot capture different frequencies of query terms.",
                "However, multivariate Bernoulli also has one potential advantage over multinomial from the viewpoint of retrieval: in a multinomial distribution, the probabilities of all the terms must sum to 1, making it hard to accommodate per-term smoothing, while in a multivariate Bernoulli, the presence probabilities of different terms are completely independent of each other, easily accommodating per-term smoothing and weighting.",
                "Note that term absence is also indirectly captured in a multinomial model through the constraint that all the term probabilities must sum to 1.",
                "In this paper, we propose and study a new family of query generation models based on the Poisson distribution.",
                "In this new family of models, we model the frequency of each term independently with a Poisson distribution.",
                "To score a document, we would first estimate a multivariate Poisson model based on the document, and then score it based on the likelihood of the query given by the estimated Poisson model.",
                "In some sense, the Poisson model combines the advantage of multinomial in modeling term frequency and the advantage of the multivariate Bernoulli in accommodating per-term smoothing.",
                "Indeed, similar to the multinomial distribution, the Poisson distribution models term frequencies, but without the constraint that all the term probabilities must sum to 1, and similar to multivariate Bernoulli, it models each term independently, thus can easily accommodate per-term smoothing.",
                "As in the existing work on multinomial language models, smoothing is critical for this new family of models.",
                "We derive several smoothing methods for Poisson model in parallel to those used for multinomial distributions, and compare the corresponding retrieval models with those based on multinomial distributions.",
                "We find that while with some smoothing methods, the new model and the multinomial model lead to exactly the same formula, with some other smoothing methods they diverge, and the Poisson model brings in more flexibility for smoothing.",
                "In particular, a key difference is that the Poisson model can naturally accommodate perterm smoothing, which is hard to achieve with a multinomial model without heuristic twist of the semantics of a generative model.",
                "We exploit this potential advantage to develop a new term-dependent smoothing algorithm for Poisson model and show that this new smoothing algorithm can improve performance over term-independent smoothing algorithms using either Poisson or multinomial model.",
                "This advantage is seen for both one-stage and two-stage smoothing.",
                "Another potential advantage of the Poisson model is that its corresponding background model for smoothing can be improved through using a mixture model that has a closed form formula.",
                "This new background model is shown to outperform the standard background model and reduce the sensitivity of retrieval performance to the smoothing parameter.",
                "The rest of the paper is organized as follows.",
                "In Section 2, we introduce the new family of query generation models with Poisson distribution, and present various smoothing methods which lead to different retrieval functions.",
                "In Section 3, we analytically compare the Poisson language model with the multinomial language model, from the perspective of retrieval.",
                "We then design empirical experiments to compare the two families of language models in Section 4.",
                "We discuss the related work in 5 and conclude in 6. 2.",
                "QUERY GENERATION WITH POISSON PROCESS In the query generation framework, a basic assumption is that a query is generated with a model estimated based on a document.",
                "In most existing work [12, 6, 28, 29], people assume that each query word is sampled independently from a multinomial distribution.",
                "Alternatively, we assume that a query is generated by sampling the frequency of words from a series of independent Poisson processes [20]. 2.1 The Generation Process Let V = {w1, ..., wn} be a <br>vocabulary set</br>.",
                "Let w be a piece of text composed by an author and c(w1), ..., c(wn) be a frequency vector representing w, where c(wi, w) is the frequency count of term wi in text w. In retrieval, w could be either a query or a document.",
                "We consider the frequency counts of the n unique terms in w as n different types of events, sampled from n independent homogeneous Poisson processes, respectively.",
                "Suppose t is the time period during which the author composed the text.",
                "With a homogeneous Poisson process, the frequency count of each event, i.e., the number of occurrences of wi, follows a Poisson distribution with associated parameter λit, where λi is a rate parameter characterizing the expected number of wi in a unit time.",
                "The probability density function of such a Poisson Distribution is given by P(c(wi, w) = k|λit) = e−λit (λit)k k!",
                "Without losing generality, we set t to the length of the text w (people write one word in a unit time), i.e., t = |w|.",
                "With n such independent Poisson processes, each explaining the generation of one term in the vocabulary, the likelihood of w to be generated from such Poisson processes can be written as p(w|Λ) = n i=1 p(c(wi, w)|Λ) = n i=1 e−λi·|w| (λi · |w|)c(wi,w) c(wi, w)! where Λ = {λ1, ..., λn} and |w| = n i=1 c(wi, w).",
                "We refer to these n independent Poisson processes with parameter Λ as a Poisson Language Model.",
                "Let D = {d1, ..., dm} be an observed set of document samples generated from the Poisson process above.",
                "The maximum likelihood estimate (MLE) of λi is ˆλi = d∈D c(wi, d) d∈D w ∈V c(w , d) Note that this MLE is different from the MLE for the Poisson distribution without considering the document lengths, which appears in [22, 24].",
                "Given a document d, we may estimate a Poisson language model Λd using d as a sample.",
                "The likelihood that a query q is generated from the document language model Λd can be written as p(q|d) = w∈V p(c(w, q)|Λd) (1) This representation is clearly different from the multinomial query generation model as (1) the likelihood includes all the terms in the vocabulary V , instead of only those appearing in q, and (2) instead of the appearance of terms, the event space of this model is the frequencies of each term.",
                "In practice, we have the flexibility to choose the vocabulary V .",
                "In one extreme, we can use the vocabulary of the whole collection.",
                "However, this may bring in noise and considerable computational cost.",
                "In the other extreme, we may focus on the terms in the query and ignore other terms, but some useful information may be lost by ignoring the nonquery terms.",
                "As a compromise, we may conflate all the non-query terms as one single pseudo term.",
                "In other words, we may assume that there is exactly one non-query term in the vocabulary for each query.",
                "In our experiments, we adopt this pseudo non-query term strategy.",
                "A document can be scored with the likelihood in Equation 1.",
                "However, if a query term is unseen in the document, the MLE of the Poisson distribution would assign zero probability to the term, causing the probability of the query to be zero.",
                "As in existing language modeling approaches, the main challenge of constructing a reasonable retrieval model is to find a smoothed language model for p(·|d). 2.2 Smoothing in Poisson Retrieval Model In general, we want to assign non-zero rates for the query terms that are not seen in document d. Many smoothing methods have been proposed for multinomial language models[2, 28, 29].",
                "In general, we have to discount the probabilities of some words seen in the text to leave some extra probability mass to assign to the unseen words.",
                "In Poisson language models, however, we do not have the same constraint as in a multinomial model (i.e., w∈V p(w|d) = 1).",
                "Thus we do not have to discount the probability of seen words in order to give a non-zero rate to an unseen word.",
                "Instead, we only need to guarantee that k=0,1,2,... p(c(w, d) = k|d) = 1.",
                "In this section, we introduce three different strategies to smooth a Poisson language model, and show how they lead to different retrieval functions. 2.2.1 Bayesian Smoothing using Gamma Prior Following the risk minimization framework in [11], we assume that a document is generated by the arrival of terms in a time period of |d| according to the document language model, which essentially consists of a vector of Poisson rates for each term, i.e., Λd = λd,1, ..., λd,|V | .",
                "A document is assumed to be generated from a potentially different model.",
                "Given a particular document d, we want to estimate Λd.",
                "The rate of a term is estimated independently of other terms.",
                "We use Bayesian estimation with the following Gamma prior, which has two parameters, α and β: Gamma(λ|α, β) = βα Γ(α) λα−1 e−βλ For each term w, the parameters αw and βw are chosen to be αw = µ ∗ λC,w and βw = µ, where µ is a parameter and λC,w is the rate of w estimated from some background language model, usually the collection language model.",
                "The posterior distribution of Λd is given by p(Λd|d, C) ∝ w∈V e−λw(|d|+µ) λ c(w,d)+µλC,w−1 w which is a product of |V | Gamma distributions with parameters c(w, d) + µλC,w and |d| + µ for each word w. Given that the Gamma mean is α β , we have ˆλd,w = λd,w λd,wp(λd,w|d, C)dλd,w = c(w, d) + µλC,w |d| + µ This is precisely the smoothed estimate of multinomial language model with Dirichlet prior [28]. 2.2.2 Interpolation (Jelinek-Mercer) Smoothing Another straightforward method is to decompose the query generation model as a mixture of two component models.",
                "One is the document language model estimated with maximum likelihood estimator, and the other is a model estimated from the collection background, p(·|C), which assigns non-zero rate to w. For example, we may use an interpolation coefficient between 0 and 1 (i.e., δ ∈ [0, 1]).",
                "With this simple interpolation, we can score a document with Score(d, q) = w∈V log((1 − δ)p(c(w, q)|d) + δp(c(w, q)|C)) (2) Using the maximum likelihood estimator for p(·|d), we have λd,w = c(w,d) |d| , thus Equation 2 becomes Score(d, q) ∝ w∈d∩q [log(1 + 1 − δ δ e−λd,w|q| (λd,w|q|)c(w,q) c(w, q)! · p(c(w, q)|C) ) − log (1 − δ)e−λd,w|q| + δp(c(w, q) = 0|C) 1 − δ + δp(c(w, q) = 0|C) ] + w∈d log (1 − δ)e−λd,w|q| + δp(c(w, q) = 0|C) 1 − δ + δp(c(w, q) = 0|C) We can also use a Poisson language model for p(·|C), or use some other frequency-based models.",
                "In the retrieval formula above, the first summation can be computed efficiently.",
                "The second summation can be actually treated as a document prior, which penalizes long documents.",
                "As the second summation is difficult to compute efficiently, we conflate all non-query terms as one pseudo non-queryterm, denoted as N. Using the pseudo-term formulation and a Poisson collection model, we can rewrite the retrieval formula as Score(d, q) ∝ w∈d∩q log(1 + 1 − δ δ e−λd,w (λd,w|q|)c(w,q) e−λd,C |q| (λd,C )c(w,q) ) + log (1 − δ)e−λd,N |q| + δe−λC,N |q| 1 − δ + δe−λC,N |q| (3) where λd,N = |d|− w∈q c(w,d) |d| and λC,N = |C|− w∈q c(w,C) |C| . 2.2.3 Two-Stage Smoothing As discussed in [29], smoothing plays two roles in retrieval: (1) to improve the estimation of the document language model, and (2) to explain the common terms in the query.",
                "In order to distinguish the content and non-discriminative words in a query, we follow [29] and assume that a query is generated by sampling from a two-component mixture of Poisson language models, with one component being the document model Λd and the other being a query background language model p(·|U). p(·|U) models the typical term frequencies in the users queries.",
                "We may then score each document with the query likelihood computed using the following two-stage smoothing model: p(c(w, q)|Λd, U) = (1 − δ)p(c(w, q)|Λd) + δp(c(w, q)|U) (4) where δ is a parameter, roughly indicating the amount of noise in q.",
                "This looks similar to the interpolation smoothing, except that p(·|Λd) now should be a smoothed language model, instead of the one estimated with MLE.",
                "With no prior knowledge on p(·|U), we could set it to p(·|C).",
                "Any smoothing methods for the document language model can be used to estimate p(·|d) such as the Gamma smoothing as discussed in Section 2.2.1.",
                "The empirical study of the smoothing methods is presented in Section 4. 3.",
                "ANALYSIS OF POISSON LANGUAGE MODEL From the previous section, we notice that the Poisson language model has a strong connection to the multinomial language model.",
                "This is expected since they both belong to the exponential family [26].",
                "However, there are many differences when these two families of models are applied with different smoothing methods.",
                "From the perspective of retrieval, will these two language models perform equivalently?",
                "If not, which model provides more benefits to retrieval, or provides flexibility which could lead to potential benefits?",
                "In this section, we analytically discuss the retrieval features of the Poisson language models, by comparing their behavior with that of the multinomial language models. 3.1 The Equivalence of Basic Models Let us begin with the assumption that all the query terms appear in every document.",
                "Under this assumption, no smoothing is needed.",
                "A document can be scored by the log likelihood of the query with the maximum likelihood estimate: Score(d, q) = w∈V log e−λd,w|q| (λd,w|q|)c(w,q) c(w, q)! (5) Using the MLE, we have λd,w = c(w,d) w∈V c(w,d) .",
                "Thus Score(d, q) ∝ c(w,q)>0 c(w, q) log c(w, d) w∈V c(w, d) This is exactly the log likelihood of the query if the document language model is a multinomial with maximum likelihood estimate.",
                "Indeed, even with Gamma smoothing, when plugging λd,w = c(w,d)+µλC,w |d|+µ and λC,w = c(w,C) |C| into Equation 5, it is easy to show that Score(d, q) ∝ w∈q∩d c(w, q) log(1 + c(w, d) µ · c(w,C) |C| ) + |q| log µ |d| + µ (6) which is exactly the Dirichlet retrieval formula in [28].",
                "Note that this equivalence holds only when the document length variation is modeled with Poisson process.",
                "This derivation indicates the equivalence of the basic Poisson and multinomial language models for retrieval.",
                "With other smoothing strategies, however, the two models would be different.",
                "Nevertheless, with this equivalence in basic models, we could expect that the Poisson language model performs comparably to the multinomial language model in retrieval, if only simple smoothing is explored.",
                "Based on this equivalence analysis, one may ask, why we should pursue the Poisson language model.",
                "In the following sections, we show that despite the equivalence in their basic models, the Poisson language model brings in extra flexibility for exploring advanced techniques on various retrieval features, which could not be achieved with multinomial language models. 3.2 Term Dependent Smoothing One flexibility of the Poisson language model is that it provides a natural framework to accommodate term dependent (per-term) smoothing.",
                "Existing work on language model smoothing has already shown that different types of queries should be smoothed differently according to how discriminative the query terms are. [7] also predicted that different terms should have a different smoothing weights.",
                "With multinomial query generation models, people usually use a single smoothing coefficient to control the combination of the document model and the background model [28, 29].",
                "This parameter can be made specific for different queries, but always has to be a constant for all the terms.",
                "This is mandatory since a multinomial language model has the constraint that w∈V p(w|d) = 1.",
                "However, from retrieval perspective, different terms may need to be smoothed differently even if they are in the same query.",
                "For example, a non-discriminative term (e.g., the, is) is expected to be explained more with the background model, while a content term (e.g., retrieval, bush) in the query should be explained with the document model.",
                "Therefore, a better way of smoothing would be to set the interpolation coefficient (i.e., δ in Formula 2 and Formula 3) specifically for each term.",
                "Since the Poisson language model does not have the sum-to-one constraint across terms, it can easily accommodate per-term smoothing without needing to heuristically twist the semantics of a generative model as in the case of multinomial language models.",
                "Below we present a possible way to explore term dependent smoothing with Poisson language models.",
                "Essentially, we want to use a term-specific smoothing coefficient δ in the linear combination, denoted as δw.",
                "This coefficient should intuitively be larger if w is a common word and smaller if it is a content word.",
                "The key problem is to find a method to assign reasonable values to δw.",
                "Empirical tuning is infeasible for so many parameters.",
                "We may instead estimate the parameters ∆ = {δ1, ..., δ|V |} by maximizing the likelihood of the query given the mixture model of p(q|ΛQ) and p(q|U), where ΛQ is the true query model to generate the query and p(q|U) is a query background model as discussed in Section 2.2.3.",
                "With the model p(q|ΛQ) hidden, the query likelihood is p(q|∆, U) = ΛQ w∈V ((1 − δw)p(c(w, q)|ΛQ) + δwp(c(w, q)|U))P(ΛQ|U)dΛQ If we have relevant documents for each query, we can approximate the query model space with the language models of all the relevant documents.",
                "Without relevant documents, we opt to approximate the query model space with the models of all the documents in the collection.",
                "Setting p(·|U) as p(·|C), the query likelihood becomes p(q|∆, U) = d∈C πd w∈V ((1−δw)p(c(w, q)|ˆΛd)+δwp(c(w, q)|C)) where πd = p(ˆΛd|U). p(·|ˆΛd) is an estimated Poisson language model for document d. If we have prior knowledge on p(ˆΛd|U), such as which documents are relevant to the query, we can set πd accordingly, because what we want is to find ∆ that can maximize the likelihood of the query given relevant documents.",
                "Without this prior knowledge, we can leave πd as free parameters, and use the EM algorithm to estimate πd and ∆.",
                "The updating functions are given as π (k+1) d = πd w∈V ((1 − δw)p(c(w, q)|ˆΛd) + δwp(c(w, q)|C)) d∈C πd w∈V ((1 − δw)p(c(w, q)|ˆΛd) + δwp(c(w, q)|C)) and δ (k+1) w = d∈C πd δwp(c(w, q)|C)) (1 − δw)p(c(w, q)|ˆΛd) + δwp(c(w, q)|C)) As discussed in [29], we only need to run the EM algorithm for several iterations, thus the computational cost is relatively low.",
                "We again assume our vocabulary containing all query terms plus a pseudo non-query term.",
                "Note that the function does not give an explicit way of estimating the coefficient for the unseen non-query term.",
                "In our experiments, we set it to the average over δw of all query terms.",
                "With this flexibility, we expect Poisson language models could improve the retrieval performance, especially for verbose queries, where the query terms have various discriminative values.",
                "In Section 4, we use empirical experiments to prove this hypothesis. 3.3 Mixture Background Models Another flexibility is to explore different background (collection) models (i.e., p(·|U), or p(·|C)).",
                "One common assumption made in language modeling information retrieval is that the background model is a homogeneous model of the document models [28, 29].",
                "Similarly, we can also make the assumption that the collection model is a Poisson language model, with the rates λC,w = d∈C c(w,d) |C| .",
                "However, this assumption usually does not hold, since the collection is far more complex than a single document.",
                "Indeed, the collection usually consists of a mixture of documents with various genres, authors, and topics, etc.",
                "Treating the collection model as a mixture of document models, instead of a single pseudo-document model is more reasonable.",
                "Existing work of multinomial language modeling has already shown that a better modeling of background improves the retrieval performance, such as clusters [15, 10], neighbor documents [25], and aspects [8, 27].",
                "All the approaches can be easily adopted using Poisson language models.",
                "However, a common problem of these approaches is that they all require heavy computation to construct the background model.",
                "With Poisson language modeling, we show that it is possible to model the mixture background without paying for the heavy computational cost.",
                "Poisson Mixture [3] has been proposed to model a collection of documents, which can fit the data much better than a single Poisson.",
                "The basic idea is to assume that the collection is generated from a mixture of Poisson models, which has the general form of p(x = k|PM) = λ p(λ)p(x = k|λ)dλ p(·|λ) is a single Poisson model and p(λ) is an arbitrary probability density function.",
                "There are three well known Poisson mixtures [3]: 2-Poisson, Negative Binomial, and the Katzs K-Mixture [9].",
                "Note that the 2-Poisson model has actually been explored in probabilistic retrieval models, which led to the well-known BM25 formula [22].",
                "All these mixtures have closed forms, and can be estimated from the collection of documents efficiently.",
                "This is an advantage over the multinomial mixture models, such as PLSI [8] and LDA [1], for retrieval.",
                "For example, the probability density function of Katzs K-Mixture is given as p(c(w) = k|αw, βw) = (1 − αw)ηk,0 + αw βw + 1 ( βw βw + 1 )k where ηk,0 = 1 when k = 0, and 0 otherwise.",
                "With the observation of a collection of documents, αw and βw can be estimated as βw = cf(w) − df(w) df(w) and αw = cf(w) Nβw where cf(w) and df(w) are the collection frequency and document frequency of w, and N is the number of documents in the collection.",
                "To account for the different document lengths, we assume that βw is a reasonable estimation for generating a document of the average length, and use β = βw avdl |q| to generate the query.",
                "This Poisson mixture model can be easily used to replace P(·|C) in the retrieval functions 3 and 4. 3.4 Other Possible Flexibilities In addition to term dependent smoothing and efficient mixture background, a Poisson language model has also some other potential advantages.",
                "For example, in Section 2, we see that Formula 2 introduces a component which does document length penalization.",
                "Intuitively, when the document has more unique words, it will be penalized more.",
                "On the other hand, if a document is exactly n copies of another document, it would not get over penalized.",
                "This feature is desirable and not achieved with the Dirichlet model [5].",
                "Potentially, this component could penalize a document according to what types of terms it contains.",
                "With term specific settings of δ, we could get even more flexibility for document length normalization.",
                "Pseudo-feedback is yet another interesting direction where the Poission model might be able to show its advantage.",
                "With model-based feedback, we could again relax the combination coefficients of the feedback model and the background model, and allow different terms to contribute differently to the feedback model.",
                "We could also utilize the relevant documents to learn better per-term smoothing coefficients. 4.",
                "EVALUATION In Section 3, we analytically compared the Poisson language models and multinomial language models from the perspective of query generation and retrieval.",
                "In this section, we compare these two families of models empirically.",
                "Experiment results show that the Poisson model with perterm smoothing outperforms multinomial model, and the performance can be further improved with two-stage smoothing.",
                "Using Poisson mixture as background model also improves the retrieval performance. 4.1 Datasets Since retrieval performance could significantly vary from one test collection to another, and from one query to another, we select four representative TREC test collections: AP, Trec7, Trec8, and Wt2g(Web).",
                "To cover different types of queries, we follow [28, 5], and construct short-keyword (SK, keyword title), short-verbose (SV, one sentence description), and long-verbose (LV, multiple sentences) queries.",
                "The documents are stemmed with the Porters stemmer, and we do not remove any stop word.",
                "For each parameter, we vary its value to cover a reasonably wide range. 4.2 Comparison to Multinomial We compare the performance of the Poisson retrieval models and multinomial retrieval models using interpolation (JelinekMercer, JM) smoothing and Bayesian smoothing with conjugate priors.",
                "Table 1 shows that the two JM-smoothed models perform similarly on all data sets.",
                "Since the Dirichlet Smoothing for multinomial language model and the Gamma Smoothing for Poisson language model lead to the same retrieval formula, the performance of these two models are jointly presented.",
                "We see that Dirichlet/Gamma smoothing methods outperform both Jelinek-Mercer smoothing methods.",
                "The parameter sensitivity curves for two Jelinek-Mercer 0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1 0 0.05 0.1 0.15 0.2 0.25 0.3 Dataset: Trec8 Parameter: δ AveragePrecision JM−Multinomial: LV JM−Multinomial: SV JM−Multinomial: SK JM−Poisson: SK JM−Poisson: SV JM−Poisson: LV Figure 1: Poisson and multinomial performs similarly with Jelinek-Mercer smoothing smoothing methods are shown in Figure 1.",
                "Clearly, these two methods perform similarly either in terms of optimality Data Query JM-Multinomial JM-Poisson Dirichlet/Gamma Per-term 2-Stage Poisson MAP InitPr Pr@5d MAP InitPr Pr@5d MAP InitPr Pr@5d MAP InitPr Pr@5d AP88-89 SK 0.203 0.585 0.356 0.203 0.585 0.358 0.224 0.629 0.393 0.226 0.630 0.396 SV 0.187 0.580 0.361 0.183 0.571 0.345 0.204 0.613 0.387 0.217* 0.603 0.390 LV 0.283 0.716 0.480 0.271 0.692 0.470 0.291 0.710 0.496 0.304* 0.695 0.510 Trec7 SK 0.167 0.635 0.400 0.168 0.635 0.404 0.186 0.687 0.428 0.185 0.646 0.436 SV 0.174 0.655 0.432 0.176 0.653 0.432 0.182 0.666 0.432 0.196* 0.660 0.440 LV 0.223 0.730 0.496 0.215 0.766 0.488 0.224 0.748 0.52 0.236* 0.738 0.512 Trec8 SK 0.239 0.621 0.440 0.239 0.621 0.436 0.257 0.718 0.496 0.256 0.704 0.468 SV 0.231 0.686 0.448 0.234 0.702 0.456 0.228 0.691 0.456 0.246* 0.692 0.476 LV 0.265 0.796 0.548 0.261 0.757 0.520 0.260 0.741 0.492 0.274* 0.766 0.508 Web SK 0.250 0.616 0.380 0.250 0.616 0.380 0.302 0.767 0.468 0.307 0.739 0.468 SV 0.214 0.611 0.392 0.217 0.609 0.384 0.273 0.693 0.508 0.292* 0.703 0.480 LV 0.266 0.790 0.464 0.259 0.776 0.452 0.283 0.756 0.496 0.311* 0.759 0.488 Table 1: Performance comparison between Poisson and Multinomial retrieval models: basic models perform comparably; term dependent two-stage smoothing significantly improves Poisson An asterisk (*) indicates that the difference between the performance of the term dependent two-stage smoothing and that of the Dirichlet/Gamma single smoothing is statistically significant according to the Wilcoxon signed rank test at the level of 0.05. or sensitivity.",
                "This similarity of performance is expected as we discussed in Section 3.1.",
                "Although the Poisson model and multinomial model are similar in terms of the basic model and/or with simple smoothing methods, the Poisson model has great potential and flexibility to be further improved.",
                "As shown in the rightmost column of Table 1, term dependent two-stage Poisson model consistently outperforms the basic smoothing models, especially for verbose queries.",
                "This model is given in Formula 4, with a Gamma smoothing for the document model p(·|d), and δw, which is term dependent.",
                "The parameter µ of the first stage Gamma smoothing is empirically tuned.",
                "The combination coefficients (i.e., ∆), are estimated with the EM algorithm in Section 3.2.",
                "The parameter sensitivity curves for Dirichlet/Gamma and the per-term two-stage smoothing model are plotted in Figure 2.",
                "The per-term two-stage smoothing method is less sensitive to the parameter µ than Dirichlet/Gamma, and yields better optimal performance. 0 1000 2000 3000 4000 5000 6000 7000 8000 9000 10000 0.1 0.12 0.14 0.16 0.18 0.2 0.22 Dataset: AP; Query Type: SV Parameter: µ AveragePrecision Dirichlet/Gamma Smoothing Term Dependent 2−Stage Figure 2: Term dependent two-stage smoothing of Poisson outperforms Dirichlet/Gamma In the following subsections, we conduct experiments to demonstrate how the flexibility of the Poisson model could be utilized to achieve better performance, which we cannot achieve with multinomial language models. 4.3 Term Dependent Smoothing To test the effectiveness of the term dependent smoothing, we conduct the following two experiments.",
                "In the first experiment, we relax the constant coefficient in the simple Jelinek-Mercer smoothing formula (i.e., Formula 3), and use the EM algorithm proposed in Section 3.2 to find a δw for each unique term.",
                "Since we are using the EM algorithm to iteratively estimate the parameters, we usually do not want the probability of p(·|d) to be zero.",
                "We then use a simple Laplace method to slightly smooth the document model before it goes into the EM iterations.",
                "The documents are then still scored with Formula 3, but using learnt δw.",
                "The results are labeled with JM+L. in Table 2.",
                "Data Q JM JM JM+L. 2-Stage 2-Stage (MAP) PT: No Yes Yes No Yes AP SK 0.203 0.204 0.206 0.223 0.226* SV 0.183 0.189 0.214* 0.204 0.217* Trec7 SK 0.168 0.171 0.174 0.186 0.185 SV 0.176 0.147 0.198* 0.194 0.196 Trec8 SK 0.239 0.240 0.227* 0.257 0.256 SV 0.234 0.223 0.249* 0.242 0.246* Web SK 0.250 0.236 0.220* 0.291 0.307* SV 0.217 0.232 0.261* 0.273 0.292* Table 2: Term dependent smoothing improves retrieval performance An asterisk (*) in Column 3 indicates that the difference between the JM+L. method and JM method is statistically significant; an asterisk (*) in Column 5 means that the difference between term dependent two-stage method and query dependent two-stage method is statistically significant; PT stands for per-term.",
                "With term dependent coefficients, the performance of the Jelinek-Mercer Poisson model is improved in most cases.",
                "However, in some cases (e.g., Trec7/SV), it performs poorly.",
                "This might be caused by the problem of EM estimation with unsmoothed document models.",
                "Once non-zero probability is assigned to all the terms before entering the EM iteration, the performance on verbose queries can be improved significantly.",
                "This indicates that there is still room to find better methods to estimate δw.",
                "Please note that neither the perterm JM method nor the JM+L. method has a parameter to tune.",
                "As shown in Table 1, the term dependent two-stage smoothing can significantly improve retrieval performance.",
                "To understand whether the improvement is contributed by the term dependent smoothing or the two-stage smoothing framework, we design another experiment to compare the perterm two-stage smoothing with the two-stage smoothing method proposed in [29].",
                "Their method managed to find coefficients specific to the query, thus a verbose query would use a higher δ.",
                "However, since their model is based on multinomial language modeling, they could not get per-term coefficients.",
                "We adopt their method to the Poisson two-stage smoothing, and also estimate a per-query coefficient for all the terms.",
                "We compare the performance of such a model with the per-term two-stage smoothing model, and present the results in the right two columns in Table 2.",
                "Again, we see that the per-term two-stage smoothing outperforms the per-query two-stage smoothing, especially for verbose queries.",
                "The improvement is not as large as how the perterm smoothing method improves over Dirichlet/Gamma.",
                "This is expected, since the per-query smoothing has already addressed the query discrimination problem to some extent.",
                "This experiment shows that even if the smoothing is already per-query, making it per-term is still beneficial.",
                "In brief, the per-term smoothing improved the retrieval performance of both one-stage and two-stage smoothing method. 4.4 Mixture Background Model In this section, we conduct experiments to examine the benefits of using a mixture background model without extra computational cost, which can not be achieved for multinomial models.",
                "Specifically, in retrieval formula 3, instead of using a single Poisson distribution to model the background p(·|C), we use Katzs K-Mixture model, which is essentially a mixture of Poisson distributions. p(·|C) can be computed efficiently with simple collection statistics, as discussed in Section 3.3.",
                "Data Query JM.",
                "Poisson JM.",
                "K-Mixture AP SK 0.203 0.204 SV 0.183 0.188* Trec-7 SK 0.168 0.169 SV 0.176 0.178* Trec-8 SK 0.239 0.239 SV 0.234 0.238* Web SK 0.250 0.250 SV 0.217 0.223* Table 3: K-Mixture background model improves retrieval performance The performance of the JM retrieval model with single Poisson background and with Katzs K-Mixture background model is compared in Table 3.",
                "Clearly, using K-Mixture to model the background model outperforms the single Poisson background model in most cases, especially for verbose queries where the improvement is statistically significant.",
                "Figure 3 shows that the performance changes over different parameters for short verbose queries.",
                "The model using K-Mixture background is less sensitive than the one using single Poisson background.",
                "Given that this type of mixture 0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1 0 0.05 0.1 0.15 0.2 0.25 Data: Trec8; Query: SV Parameter: δ AveragePrecision Poisson Background K−Mixture Background Figure 3: K-Mixture background model deviates the sensitivity of verbose queries background model does not require any extra computation cost, it would be interesting to study whether using other mixture Poisson models, such as 2-Poisson and negative Binomial, could help the performance. 5.",
                "RELATED WORK To the best of our knowledge, there has been no study of query generation models based on Poisson distribution.",
                "Language models have been shown to be effective for many retrieval tasks [21, 28, 14, 4].",
                "The most popular and fundamental one is the query-generation language model [21, 13].",
                "All existing query generation language models are based on either multinomial distribution [19, 6, 28, 13] or multivariate Bernoulli distribution [21, 17, 18].",
                "We introduce a new family of language models, based on Poisson distribution.",
                "Poisson distribution has been previously studied in the document generation models [16, 22, 3, 24], leading to the development of one of the most effective retrieval formula BM25 [23]. [24] studies the parallel derivation of three different retrieval models which is related to our comparison of Poisson and multinomial.",
                "However, the Poisson model in their paper is still under the document generation framework, and also does not account for the document length variation. [26] introduces a way to empirically search for an exponential model for the documents.",
                "Poisson mixtures [3] such as 2-Poisson [22], Negative multinomial, and Katzs KMixture [9] has shown to be effective to model and retrieve documents.",
                "Once again, none of this work explores Poisson distribution in the query generation framework.",
                "Language model smoothing [2, 28, 29] and background structures [15, 10, 25, 27] have been studied with multinomial language models. [7] analytically shows that term specific smoothing could be useful.",
                "We show that Poisson language model is natural to accommodate the per-term smoothing without heuristic twist of the semantics of a generative model, and is able to efficiently better model the mixture background, both analytically and empirically. 6.",
                "CONCLUSIONS We present a new family of query generation language models for retrieval based on Poisson distribution.",
                "We derive several smoothing methods for this family of models, including single-stage smoothing and two-stage smoothing.",
                "We compare the new models with the popular multinomial retrieval models both analytically and experimentally.",
                "Our analysis shows that while our new models and multinomial models are equivalent under some assumptions, they are generally different with some important differences.",
                "In particular, we show that Poisson has an advantage over multinomial in naturally accommodating per-term smoothing.",
                "We exploit this property to develop a new per-term smoothing algorithm for Poisson language models, which is shown to outperform term-independent smoothing for both Poisson and multinomial models.",
                "Furthermore, we show that a mixture background model for Poisson can be used to improve the performance and robustness over the standard Poisson background model.",
                "Our work opens up many interesting directions for further exploration in this new family of models.",
                "Further exploring the flexibilities over multinomial language models, such as length normalization and pseudo-feedback could be good future work.",
                "It is also appealing to find robust methods to learn the per-term smoothing coefficients without additional computation cost. 7.",
                "ACKNOWLEDGMENTS We thank the anonymous SIGIR 07 reviewers for their useful comments.",
                "This material is based in part upon work supported by the National Science Foundation under award numbers IIS-0347933 and 0425852. 8.",
                "REFERENCES [1] D. Blei, A. Ng, and M. Jordan.",
                "Latent dirichlet allocation.",
                "Journal of Machine Learning Research, 3:993-1022, 2003. [2] S. F. Chen and J. Goodman.",
                "An empirical study of smoothing techniques for language modeling.",
                "Technical Report TR-10-98, Harvard University, 1998. [3] K. Church and W. Gale.",
                "Poisson mixtures.",
                "Nat.",
                "Lang.",
                "Eng., 1(2):163-190, 1995. [4] W. B. Croft and J. Lafferty, editors.",
                "Language Modeling and Information Retrieval.",
                "Kluwer Academic Publishers, 2003. [5] H. Fang, T. Tao, and C. Zhai.",
                "A formal study of information retrieval heuristics.",
                "In Proceedings of the 27th annual international ACM SIGIR conference on Research and development in information retrieval, pages 49-56, 2004. [6] D. Hiemstra.",
                "Using Language Models for Information Retrieval.",
                "PhD thesis, University of Twente, Enschede, Netherlands, 2001. [7] D. Hiemstra.",
                "Term-specific smoothing for the language modeling approach to information retrieval: the importance of a query term.",
                "In Proceedings of the 25th annual international ACM SIGIR conference on Research and development in information retrieval, pages 35-41, 2002. [8] T. Hofmann.",
                "Probabilistic latent semantic indexing.",
                "In Proceedings of ACM SIGIR99, pages 50-57, 1999. [9] S. M. Katz.",
                "Distribution of content words and phrases in text and language modelling.",
                "Nat.",
                "Lang.",
                "Eng., 2(1):15-59, 1996. [10] O. Kurland and L. Lee.",
                "Corpus structure, language models, and ad-hoc information retrieval.",
                "In Proceedings of the 27th annual international ACM SIGIR conference on Research and development in information retrieval, pages 194-201, 2004. [11] J. Lafferty and C. Zhai.",
                "Document language models, query models, and risk minimization for information retrieval.",
                "In Proceedings of SIGIR01, pages 111-119, Sept 2001. [12] J. Lafferty and C. Zhai.",
                "Probabilistic IR models based on query and document generation.",
                "In Proceedings of the Language Modeling and IR workshop, pages 1-5, May 31 - June 1 2001. [13] J. Lafferty and C. Zhai.",
                "Probabilistic relevance models based on document and query generation.",
                "In W. B. Croft and J. Lafferty, editors, Language Modeling and Information Retrieval.",
                "Kluwer Academic Publishers, 2003. [14] V. Lavrenko and B. Croft.",
                "Relevance-based language models.",
                "In Proceedings of SIGIR01, pages 120-127, Sept 2001. [15] X. Liu and W. B. Croft.",
                "Cluster-based retrieval using language models.",
                "In Proceedings of the 27th annual international ACM SIGIR conference on Research and development in information retrieval, pages 186-193, 2004. [16] E. L. Margulis.",
                "Modelling documents with multiple poisson distributions.",
                "Inf.",
                "Process.",
                "Manage., 29(2):215-227, 1993. [17] A. McCallum and K. Nigam.",
                "A comparison of event models for naive bayes text classification.",
                "In Proceedings of AAAI-98 Workshop on Learning for Text Categorization, 1998. [18] D. Metzler, V. Lavrenko, and W. B. Croft.",
                "Formal multiple-bernoulli models for language modeling.",
                "In Proceedings of the 27th annual international ACM SIGIR conference on Research and development in information retrieval, pages 540-541, 2004. [19] D. H. Miller, T. Leek, and R. Schwartz.",
                "A hidden Markov model information retrieval system.",
                "In Proceedings of the 1999 ACM SIGIR Conference on Research and Development in Information Retrieval, pages 214-221, 1999. [20] A. Papoulis.",
                "Probability, random variables and stochastic processes.",
                "New York: McGraw-Hill, 1984, 2nd ed., 1984. [21] J. M. Ponte and W. B. Croft.",
                "A language modeling approach to information retrieval.",
                "In Proceedings of the 21st annual international ACM SIGIR conference on Research and development in information retrieval, pages 275-281, 1998. [22] S. Robertson and S. Walker.",
                "Some simple effective approximations to the 2-poisson model for probabilistic weighted retrieval.",
                "In Proceedings of SIGIR94, pages 232-241, 1994. [23] S. E. Robertson, S. Walker, S. Jones, M. M.Hancock-Beaulieu, and M. Gatford.",
                "Okapi at TREC-3.",
                "In D. K. Harman, editor, The Third Text REtrieval Conference (TREC-3), pages 109-126, 1995. [24] T. Roelleke and J. Wang.",
                "A parallel derivation of probabilistic information retrieval models.",
                "In Proceedings of the 29th annual international ACM SIGIR conference on Research and development in information retrieval, pages 107-114, 2006. [25] T. Tao, X. Wang, Q. Mei, and C. Zhai.",
                "Language model information retrieval with document expansion.",
                "In Proceedings of HLT/NAACL 2006, pages 407-414, 2006. [26] J. Teevan and D. R. Karger.",
                "Empirical development of an exponential probabilistic model for text retrieval: using textual analysis to build a better model.",
                "In Proceedings of the 26th annual international ACM SIGIR conference on Research and development in informaion retrieval, pages 18-25, 2003. [27] X. Wei and W. B. Croft.",
                "Lda-based document models for ad-hoc retrieval.",
                "In Proceedings of the 29th annual international ACM SIGIR conference on Research and development in information retrieval, pages 178-185, 2006. [28] C. Zhai and J. Lafferty.",
                "A study of smoothing methods for language models applied to ad-hoc information retrieval.",
                "In Proceedings of ACM SIGIR01, pages 334-342, Sept 2001. [29] C. Zhai and J. Lafferty.",
                "Two-stage language models for information retrieval.",
                "In Proceedings of ACM SIGIR02, pages 49-56, Aug 2002."
            ],
            "original_annotated_samples": [],
            "translated_annotated_samples": [
                "Alternativamente, suponemos que se genera una consulta muestreando la frecuencia de las palabras de una serie de procesos de Poisson independientes [20].2.1 El proceso de generación Sea v = {w1, ..., wn} un \"conjunto de vocabulario\".juego de vocabulario"
            ],
            "translated_text": "",
            "candidates": [],
            "error": []
        },
        "homogeneous poisson process": {
            "translated_key": "",
            "is_in_text": true,
            "original_annotated_sentences": [
                "A Study of Poisson Query Generation Model for Information Retrieval Qiaozhu Mei, Hui Fang, Chengxiang Zhai Department of Computer Science University of Illinois at Urbana-Champaign Urbana,IL 61801 {qmei2,hfang,czhai}@uiuc.edu ABSTRACT Many variants of language models have been proposed for information retrieval.",
                "Most existing models are based on multinomial distribution and would score documents based on query likelihood computed based on a query generation probabilistic model.",
                "In this paper, we propose and study a new family of query generation models based on Poisson distribution.",
                "We show that while in their simplest forms, the new family of models and the existing multinomial models are equivalent, they behave differently for many smoothing methods.",
                "We show that the Poisson model has several advantages over the multinomial model, including naturally accommodating per-term smoothing and allowing for more accurate background modeling.",
                "We present several variants of the new model corresponding to different smoothing methods, and evaluate them on four representative TREC test collections.",
                "The results show that while their basic models perform comparably, the Poisson model can outperform multinomial model with per-term smoothing.",
                "The performance can be further improved with two-stage smoothing.",
                "Categories and Subject Descriptors: H.3.3 [Information Search and Retrieval]: Retrieval Models General Terms: Algorithms 1.",
                "INTRODUCTION As a new type of probabilistic retrieval models, language models have been shown to be effective for many retrieval tasks [21, 28, 14, 4].",
                "Among many variants of language models proposed, the most popular and fundamental one is the query-generation language model [21, 13], which leads to the query-likelihood scoring method for ranking documents.",
                "In such a model, given a query q and a document d, we compute the likelihood of generating query q with a model estimated based on document d, i.e., the conditional probability p(q|d).",
                "We can then rank documents based on the likelihood of generating the query.",
                "Virtually all the existing query generation language models are based on either multinomial distribution [19, 6, 28] or multivariate Bernoulli distribution [21, 18].",
                "The multinomial distribution is especially popular and also shown to be quite effective.",
                "The heavy use of multinomial distribution is partly due to the fact that it has been successfully used in speech recognition, where multinomial distribution is a natural choice for modeling the occurrence of a particular word in a particular position in text.",
                "Compared with multivariate Bernoulli, multinomial distribution has the advantage of being able to model the frequency of terms in the query; in contrast, multivariate Bernoulli only models the presence and absence of query terms, thus cannot capture different frequencies of query terms.",
                "However, multivariate Bernoulli also has one potential advantage over multinomial from the viewpoint of retrieval: in a multinomial distribution, the probabilities of all the terms must sum to 1, making it hard to accommodate per-term smoothing, while in a multivariate Bernoulli, the presence probabilities of different terms are completely independent of each other, easily accommodating per-term smoothing and weighting.",
                "Note that term absence is also indirectly captured in a multinomial model through the constraint that all the term probabilities must sum to 1.",
                "In this paper, we propose and study a new family of query generation models based on the Poisson distribution.",
                "In this new family of models, we model the frequency of each term independently with a Poisson distribution.",
                "To score a document, we would first estimate a multivariate Poisson model based on the document, and then score it based on the likelihood of the query given by the estimated Poisson model.",
                "In some sense, the Poisson model combines the advantage of multinomial in modeling term frequency and the advantage of the multivariate Bernoulli in accommodating per-term smoothing.",
                "Indeed, similar to the multinomial distribution, the Poisson distribution models term frequencies, but without the constraint that all the term probabilities must sum to 1, and similar to multivariate Bernoulli, it models each term independently, thus can easily accommodate per-term smoothing.",
                "As in the existing work on multinomial language models, smoothing is critical for this new family of models.",
                "We derive several smoothing methods for Poisson model in parallel to those used for multinomial distributions, and compare the corresponding retrieval models with those based on multinomial distributions.",
                "We find that while with some smoothing methods, the new model and the multinomial model lead to exactly the same formula, with some other smoothing methods they diverge, and the Poisson model brings in more flexibility for smoothing.",
                "In particular, a key difference is that the Poisson model can naturally accommodate perterm smoothing, which is hard to achieve with a multinomial model without heuristic twist of the semantics of a generative model.",
                "We exploit this potential advantage to develop a new term-dependent smoothing algorithm for Poisson model and show that this new smoothing algorithm can improve performance over term-independent smoothing algorithms using either Poisson or multinomial model.",
                "This advantage is seen for both one-stage and two-stage smoothing.",
                "Another potential advantage of the Poisson model is that its corresponding background model for smoothing can be improved through using a mixture model that has a closed form formula.",
                "This new background model is shown to outperform the standard background model and reduce the sensitivity of retrieval performance to the smoothing parameter.",
                "The rest of the paper is organized as follows.",
                "In Section 2, we introduce the new family of query generation models with Poisson distribution, and present various smoothing methods which lead to different retrieval functions.",
                "In Section 3, we analytically compare the Poisson language model with the multinomial language model, from the perspective of retrieval.",
                "We then design empirical experiments to compare the two families of language models in Section 4.",
                "We discuss the related work in 5 and conclude in 6. 2.",
                "QUERY GENERATION WITH POISSON PROCESS In the query generation framework, a basic assumption is that a query is generated with a model estimated based on a document.",
                "In most existing work [12, 6, 28, 29], people assume that each query word is sampled independently from a multinomial distribution.",
                "Alternatively, we assume that a query is generated by sampling the frequency of words from a series of independent Poisson processes [20]. 2.1 The Generation Process Let V = {w1, ..., wn} be a vocabulary set.",
                "Let w be a piece of text composed by an author and c(w1), ..., c(wn) be a frequency vector representing w, where c(wi, w) is the frequency count of term wi in text w. In retrieval, w could be either a query or a document.",
                "We consider the frequency counts of the n unique terms in w as n different types of events, sampled from n independent homogeneous Poisson processes, respectively.",
                "Suppose t is the time period during which the author composed the text.",
                "With a <br>homogeneous poisson process</br>, the frequency count of each event, i.e., the number of occurrences of wi, follows a Poisson distribution with associated parameter λit, where λi is a rate parameter characterizing the expected number of wi in a unit time.",
                "The probability density function of such a Poisson Distribution is given by P(c(wi, w) = k|λit) = e−λit (λit)k k!",
                "Without losing generality, we set t to the length of the text w (people write one word in a unit time), i.e., t = |w|.",
                "With n such independent Poisson processes, each explaining the generation of one term in the vocabulary, the likelihood of w to be generated from such Poisson processes can be written as p(w|Λ) = n i=1 p(c(wi, w)|Λ) = n i=1 e−λi·|w| (λi · |w|)c(wi,w) c(wi, w)! where Λ = {λ1, ..., λn} and |w| = n i=1 c(wi, w).",
                "We refer to these n independent Poisson processes with parameter Λ as a Poisson Language Model.",
                "Let D = {d1, ..., dm} be an observed set of document samples generated from the Poisson process above.",
                "The maximum likelihood estimate (MLE) of λi is ˆλi = d∈D c(wi, d) d∈D w ∈V c(w , d) Note that this MLE is different from the MLE for the Poisson distribution without considering the document lengths, which appears in [22, 24].",
                "Given a document d, we may estimate a Poisson language model Λd using d as a sample.",
                "The likelihood that a query q is generated from the document language model Λd can be written as p(q|d) = w∈V p(c(w, q)|Λd) (1) This representation is clearly different from the multinomial query generation model as (1) the likelihood includes all the terms in the vocabulary V , instead of only those appearing in q, and (2) instead of the appearance of terms, the event space of this model is the frequencies of each term.",
                "In practice, we have the flexibility to choose the vocabulary V .",
                "In one extreme, we can use the vocabulary of the whole collection.",
                "However, this may bring in noise and considerable computational cost.",
                "In the other extreme, we may focus on the terms in the query and ignore other terms, but some useful information may be lost by ignoring the nonquery terms.",
                "As a compromise, we may conflate all the non-query terms as one single pseudo term.",
                "In other words, we may assume that there is exactly one non-query term in the vocabulary for each query.",
                "In our experiments, we adopt this pseudo non-query term strategy.",
                "A document can be scored with the likelihood in Equation 1.",
                "However, if a query term is unseen in the document, the MLE of the Poisson distribution would assign zero probability to the term, causing the probability of the query to be zero.",
                "As in existing language modeling approaches, the main challenge of constructing a reasonable retrieval model is to find a smoothed language model for p(·|d). 2.2 Smoothing in Poisson Retrieval Model In general, we want to assign non-zero rates for the query terms that are not seen in document d. Many smoothing methods have been proposed for multinomial language models[2, 28, 29].",
                "In general, we have to discount the probabilities of some words seen in the text to leave some extra probability mass to assign to the unseen words.",
                "In Poisson language models, however, we do not have the same constraint as in a multinomial model (i.e., w∈V p(w|d) = 1).",
                "Thus we do not have to discount the probability of seen words in order to give a non-zero rate to an unseen word.",
                "Instead, we only need to guarantee that k=0,1,2,... p(c(w, d) = k|d) = 1.",
                "In this section, we introduce three different strategies to smooth a Poisson language model, and show how they lead to different retrieval functions. 2.2.1 Bayesian Smoothing using Gamma Prior Following the risk minimization framework in [11], we assume that a document is generated by the arrival of terms in a time period of |d| according to the document language model, which essentially consists of a vector of Poisson rates for each term, i.e., Λd = λd,1, ..., λd,|V | .",
                "A document is assumed to be generated from a potentially different model.",
                "Given a particular document d, we want to estimate Λd.",
                "The rate of a term is estimated independently of other terms.",
                "We use Bayesian estimation with the following Gamma prior, which has two parameters, α and β: Gamma(λ|α, β) = βα Γ(α) λα−1 e−βλ For each term w, the parameters αw and βw are chosen to be αw = µ ∗ λC,w and βw = µ, where µ is a parameter and λC,w is the rate of w estimated from some background language model, usually the collection language model.",
                "The posterior distribution of Λd is given by p(Λd|d, C) ∝ w∈V e−λw(|d|+µ) λ c(w,d)+µλC,w−1 w which is a product of |V | Gamma distributions with parameters c(w, d) + µλC,w and |d| + µ for each word w. Given that the Gamma mean is α β , we have ˆλd,w = λd,w λd,wp(λd,w|d, C)dλd,w = c(w, d) + µλC,w |d| + µ This is precisely the smoothed estimate of multinomial language model with Dirichlet prior [28]. 2.2.2 Interpolation (Jelinek-Mercer) Smoothing Another straightforward method is to decompose the query generation model as a mixture of two component models.",
                "One is the document language model estimated with maximum likelihood estimator, and the other is a model estimated from the collection background, p(·|C), which assigns non-zero rate to w. For example, we may use an interpolation coefficient between 0 and 1 (i.e., δ ∈ [0, 1]).",
                "With this simple interpolation, we can score a document with Score(d, q) = w∈V log((1 − δ)p(c(w, q)|d) + δp(c(w, q)|C)) (2) Using the maximum likelihood estimator for p(·|d), we have λd,w = c(w,d) |d| , thus Equation 2 becomes Score(d, q) ∝ w∈d∩q [log(1 + 1 − δ δ e−λd,w|q| (λd,w|q|)c(w,q) c(w, q)! · p(c(w, q)|C) ) − log (1 − δ)e−λd,w|q| + δp(c(w, q) = 0|C) 1 − δ + δp(c(w, q) = 0|C) ] + w∈d log (1 − δ)e−λd,w|q| + δp(c(w, q) = 0|C) 1 − δ + δp(c(w, q) = 0|C) We can also use a Poisson language model for p(·|C), or use some other frequency-based models.",
                "In the retrieval formula above, the first summation can be computed efficiently.",
                "The second summation can be actually treated as a document prior, which penalizes long documents.",
                "As the second summation is difficult to compute efficiently, we conflate all non-query terms as one pseudo non-queryterm, denoted as N. Using the pseudo-term formulation and a Poisson collection model, we can rewrite the retrieval formula as Score(d, q) ∝ w∈d∩q log(1 + 1 − δ δ e−λd,w (λd,w|q|)c(w,q) e−λd,C |q| (λd,C )c(w,q) ) + log (1 − δ)e−λd,N |q| + δe−λC,N |q| 1 − δ + δe−λC,N |q| (3) where λd,N = |d|− w∈q c(w,d) |d| and λC,N = |C|− w∈q c(w,C) |C| . 2.2.3 Two-Stage Smoothing As discussed in [29], smoothing plays two roles in retrieval: (1) to improve the estimation of the document language model, and (2) to explain the common terms in the query.",
                "In order to distinguish the content and non-discriminative words in a query, we follow [29] and assume that a query is generated by sampling from a two-component mixture of Poisson language models, with one component being the document model Λd and the other being a query background language model p(·|U). p(·|U) models the typical term frequencies in the users queries.",
                "We may then score each document with the query likelihood computed using the following two-stage smoothing model: p(c(w, q)|Λd, U) = (1 − δ)p(c(w, q)|Λd) + δp(c(w, q)|U) (4) where δ is a parameter, roughly indicating the amount of noise in q.",
                "This looks similar to the interpolation smoothing, except that p(·|Λd) now should be a smoothed language model, instead of the one estimated with MLE.",
                "With no prior knowledge on p(·|U), we could set it to p(·|C).",
                "Any smoothing methods for the document language model can be used to estimate p(·|d) such as the Gamma smoothing as discussed in Section 2.2.1.",
                "The empirical study of the smoothing methods is presented in Section 4. 3.",
                "ANALYSIS OF POISSON LANGUAGE MODEL From the previous section, we notice that the Poisson language model has a strong connection to the multinomial language model.",
                "This is expected since they both belong to the exponential family [26].",
                "However, there are many differences when these two families of models are applied with different smoothing methods.",
                "From the perspective of retrieval, will these two language models perform equivalently?",
                "If not, which model provides more benefits to retrieval, or provides flexibility which could lead to potential benefits?",
                "In this section, we analytically discuss the retrieval features of the Poisson language models, by comparing their behavior with that of the multinomial language models. 3.1 The Equivalence of Basic Models Let us begin with the assumption that all the query terms appear in every document.",
                "Under this assumption, no smoothing is needed.",
                "A document can be scored by the log likelihood of the query with the maximum likelihood estimate: Score(d, q) = w∈V log e−λd,w|q| (λd,w|q|)c(w,q) c(w, q)! (5) Using the MLE, we have λd,w = c(w,d) w∈V c(w,d) .",
                "Thus Score(d, q) ∝ c(w,q)>0 c(w, q) log c(w, d) w∈V c(w, d) This is exactly the log likelihood of the query if the document language model is a multinomial with maximum likelihood estimate.",
                "Indeed, even with Gamma smoothing, when plugging λd,w = c(w,d)+µλC,w |d|+µ and λC,w = c(w,C) |C| into Equation 5, it is easy to show that Score(d, q) ∝ w∈q∩d c(w, q) log(1 + c(w, d) µ · c(w,C) |C| ) + |q| log µ |d| + µ (6) which is exactly the Dirichlet retrieval formula in [28].",
                "Note that this equivalence holds only when the document length variation is modeled with Poisson process.",
                "This derivation indicates the equivalence of the basic Poisson and multinomial language models for retrieval.",
                "With other smoothing strategies, however, the two models would be different.",
                "Nevertheless, with this equivalence in basic models, we could expect that the Poisson language model performs comparably to the multinomial language model in retrieval, if only simple smoothing is explored.",
                "Based on this equivalence analysis, one may ask, why we should pursue the Poisson language model.",
                "In the following sections, we show that despite the equivalence in their basic models, the Poisson language model brings in extra flexibility for exploring advanced techniques on various retrieval features, which could not be achieved with multinomial language models. 3.2 Term Dependent Smoothing One flexibility of the Poisson language model is that it provides a natural framework to accommodate term dependent (per-term) smoothing.",
                "Existing work on language model smoothing has already shown that different types of queries should be smoothed differently according to how discriminative the query terms are. [7] also predicted that different terms should have a different smoothing weights.",
                "With multinomial query generation models, people usually use a single smoothing coefficient to control the combination of the document model and the background model [28, 29].",
                "This parameter can be made specific for different queries, but always has to be a constant for all the terms.",
                "This is mandatory since a multinomial language model has the constraint that w∈V p(w|d) = 1.",
                "However, from retrieval perspective, different terms may need to be smoothed differently even if they are in the same query.",
                "For example, a non-discriminative term (e.g., the, is) is expected to be explained more with the background model, while a content term (e.g., retrieval, bush) in the query should be explained with the document model.",
                "Therefore, a better way of smoothing would be to set the interpolation coefficient (i.e., δ in Formula 2 and Formula 3) specifically for each term.",
                "Since the Poisson language model does not have the sum-to-one constraint across terms, it can easily accommodate per-term smoothing without needing to heuristically twist the semantics of a generative model as in the case of multinomial language models.",
                "Below we present a possible way to explore term dependent smoothing with Poisson language models.",
                "Essentially, we want to use a term-specific smoothing coefficient δ in the linear combination, denoted as δw.",
                "This coefficient should intuitively be larger if w is a common word and smaller if it is a content word.",
                "The key problem is to find a method to assign reasonable values to δw.",
                "Empirical tuning is infeasible for so many parameters.",
                "We may instead estimate the parameters ∆ = {δ1, ..., δ|V |} by maximizing the likelihood of the query given the mixture model of p(q|ΛQ) and p(q|U), where ΛQ is the true query model to generate the query and p(q|U) is a query background model as discussed in Section 2.2.3.",
                "With the model p(q|ΛQ) hidden, the query likelihood is p(q|∆, U) = ΛQ w∈V ((1 − δw)p(c(w, q)|ΛQ) + δwp(c(w, q)|U))P(ΛQ|U)dΛQ If we have relevant documents for each query, we can approximate the query model space with the language models of all the relevant documents.",
                "Without relevant documents, we opt to approximate the query model space with the models of all the documents in the collection.",
                "Setting p(·|U) as p(·|C), the query likelihood becomes p(q|∆, U) = d∈C πd w∈V ((1−δw)p(c(w, q)|ˆΛd)+δwp(c(w, q)|C)) where πd = p(ˆΛd|U). p(·|ˆΛd) is an estimated Poisson language model for document d. If we have prior knowledge on p(ˆΛd|U), such as which documents are relevant to the query, we can set πd accordingly, because what we want is to find ∆ that can maximize the likelihood of the query given relevant documents.",
                "Without this prior knowledge, we can leave πd as free parameters, and use the EM algorithm to estimate πd and ∆.",
                "The updating functions are given as π (k+1) d = πd w∈V ((1 − δw)p(c(w, q)|ˆΛd) + δwp(c(w, q)|C)) d∈C πd w∈V ((1 − δw)p(c(w, q)|ˆΛd) + δwp(c(w, q)|C)) and δ (k+1) w = d∈C πd δwp(c(w, q)|C)) (1 − δw)p(c(w, q)|ˆΛd) + δwp(c(w, q)|C)) As discussed in [29], we only need to run the EM algorithm for several iterations, thus the computational cost is relatively low.",
                "We again assume our vocabulary containing all query terms plus a pseudo non-query term.",
                "Note that the function does not give an explicit way of estimating the coefficient for the unseen non-query term.",
                "In our experiments, we set it to the average over δw of all query terms.",
                "With this flexibility, we expect Poisson language models could improve the retrieval performance, especially for verbose queries, where the query terms have various discriminative values.",
                "In Section 4, we use empirical experiments to prove this hypothesis. 3.3 Mixture Background Models Another flexibility is to explore different background (collection) models (i.e., p(·|U), or p(·|C)).",
                "One common assumption made in language modeling information retrieval is that the background model is a homogeneous model of the document models [28, 29].",
                "Similarly, we can also make the assumption that the collection model is a Poisson language model, with the rates λC,w = d∈C c(w,d) |C| .",
                "However, this assumption usually does not hold, since the collection is far more complex than a single document.",
                "Indeed, the collection usually consists of a mixture of documents with various genres, authors, and topics, etc.",
                "Treating the collection model as a mixture of document models, instead of a single pseudo-document model is more reasonable.",
                "Existing work of multinomial language modeling has already shown that a better modeling of background improves the retrieval performance, such as clusters [15, 10], neighbor documents [25], and aspects [8, 27].",
                "All the approaches can be easily adopted using Poisson language models.",
                "However, a common problem of these approaches is that they all require heavy computation to construct the background model.",
                "With Poisson language modeling, we show that it is possible to model the mixture background without paying for the heavy computational cost.",
                "Poisson Mixture [3] has been proposed to model a collection of documents, which can fit the data much better than a single Poisson.",
                "The basic idea is to assume that the collection is generated from a mixture of Poisson models, which has the general form of p(x = k|PM) = λ p(λ)p(x = k|λ)dλ p(·|λ) is a single Poisson model and p(λ) is an arbitrary probability density function.",
                "There are three well known Poisson mixtures [3]: 2-Poisson, Negative Binomial, and the Katzs K-Mixture [9].",
                "Note that the 2-Poisson model has actually been explored in probabilistic retrieval models, which led to the well-known BM25 formula [22].",
                "All these mixtures have closed forms, and can be estimated from the collection of documents efficiently.",
                "This is an advantage over the multinomial mixture models, such as PLSI [8] and LDA [1], for retrieval.",
                "For example, the probability density function of Katzs K-Mixture is given as p(c(w) = k|αw, βw) = (1 − αw)ηk,0 + αw βw + 1 ( βw βw + 1 )k where ηk,0 = 1 when k = 0, and 0 otherwise.",
                "With the observation of a collection of documents, αw and βw can be estimated as βw = cf(w) − df(w) df(w) and αw = cf(w) Nβw where cf(w) and df(w) are the collection frequency and document frequency of w, and N is the number of documents in the collection.",
                "To account for the different document lengths, we assume that βw is a reasonable estimation for generating a document of the average length, and use β = βw avdl |q| to generate the query.",
                "This Poisson mixture model can be easily used to replace P(·|C) in the retrieval functions 3 and 4. 3.4 Other Possible Flexibilities In addition to term dependent smoothing and efficient mixture background, a Poisson language model has also some other potential advantages.",
                "For example, in Section 2, we see that Formula 2 introduces a component which does document length penalization.",
                "Intuitively, when the document has more unique words, it will be penalized more.",
                "On the other hand, if a document is exactly n copies of another document, it would not get over penalized.",
                "This feature is desirable and not achieved with the Dirichlet model [5].",
                "Potentially, this component could penalize a document according to what types of terms it contains.",
                "With term specific settings of δ, we could get even more flexibility for document length normalization.",
                "Pseudo-feedback is yet another interesting direction where the Poission model might be able to show its advantage.",
                "With model-based feedback, we could again relax the combination coefficients of the feedback model and the background model, and allow different terms to contribute differently to the feedback model.",
                "We could also utilize the relevant documents to learn better per-term smoothing coefficients. 4.",
                "EVALUATION In Section 3, we analytically compared the Poisson language models and multinomial language models from the perspective of query generation and retrieval.",
                "In this section, we compare these two families of models empirically.",
                "Experiment results show that the Poisson model with perterm smoothing outperforms multinomial model, and the performance can be further improved with two-stage smoothing.",
                "Using Poisson mixture as background model also improves the retrieval performance. 4.1 Datasets Since retrieval performance could significantly vary from one test collection to another, and from one query to another, we select four representative TREC test collections: AP, Trec7, Trec8, and Wt2g(Web).",
                "To cover different types of queries, we follow [28, 5], and construct short-keyword (SK, keyword title), short-verbose (SV, one sentence description), and long-verbose (LV, multiple sentences) queries.",
                "The documents are stemmed with the Porters stemmer, and we do not remove any stop word.",
                "For each parameter, we vary its value to cover a reasonably wide range. 4.2 Comparison to Multinomial We compare the performance of the Poisson retrieval models and multinomial retrieval models using interpolation (JelinekMercer, JM) smoothing and Bayesian smoothing with conjugate priors.",
                "Table 1 shows that the two JM-smoothed models perform similarly on all data sets.",
                "Since the Dirichlet Smoothing for multinomial language model and the Gamma Smoothing for Poisson language model lead to the same retrieval formula, the performance of these two models are jointly presented.",
                "We see that Dirichlet/Gamma smoothing methods outperform both Jelinek-Mercer smoothing methods.",
                "The parameter sensitivity curves for two Jelinek-Mercer 0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1 0 0.05 0.1 0.15 0.2 0.25 0.3 Dataset: Trec8 Parameter: δ AveragePrecision JM−Multinomial: LV JM−Multinomial: SV JM−Multinomial: SK JM−Poisson: SK JM−Poisson: SV JM−Poisson: LV Figure 1: Poisson and multinomial performs similarly with Jelinek-Mercer smoothing smoothing methods are shown in Figure 1.",
                "Clearly, these two methods perform similarly either in terms of optimality Data Query JM-Multinomial JM-Poisson Dirichlet/Gamma Per-term 2-Stage Poisson MAP InitPr Pr@5d MAP InitPr Pr@5d MAP InitPr Pr@5d MAP InitPr Pr@5d AP88-89 SK 0.203 0.585 0.356 0.203 0.585 0.358 0.224 0.629 0.393 0.226 0.630 0.396 SV 0.187 0.580 0.361 0.183 0.571 0.345 0.204 0.613 0.387 0.217* 0.603 0.390 LV 0.283 0.716 0.480 0.271 0.692 0.470 0.291 0.710 0.496 0.304* 0.695 0.510 Trec7 SK 0.167 0.635 0.400 0.168 0.635 0.404 0.186 0.687 0.428 0.185 0.646 0.436 SV 0.174 0.655 0.432 0.176 0.653 0.432 0.182 0.666 0.432 0.196* 0.660 0.440 LV 0.223 0.730 0.496 0.215 0.766 0.488 0.224 0.748 0.52 0.236* 0.738 0.512 Trec8 SK 0.239 0.621 0.440 0.239 0.621 0.436 0.257 0.718 0.496 0.256 0.704 0.468 SV 0.231 0.686 0.448 0.234 0.702 0.456 0.228 0.691 0.456 0.246* 0.692 0.476 LV 0.265 0.796 0.548 0.261 0.757 0.520 0.260 0.741 0.492 0.274* 0.766 0.508 Web SK 0.250 0.616 0.380 0.250 0.616 0.380 0.302 0.767 0.468 0.307 0.739 0.468 SV 0.214 0.611 0.392 0.217 0.609 0.384 0.273 0.693 0.508 0.292* 0.703 0.480 LV 0.266 0.790 0.464 0.259 0.776 0.452 0.283 0.756 0.496 0.311* 0.759 0.488 Table 1: Performance comparison between Poisson and Multinomial retrieval models: basic models perform comparably; term dependent two-stage smoothing significantly improves Poisson An asterisk (*) indicates that the difference between the performance of the term dependent two-stage smoothing and that of the Dirichlet/Gamma single smoothing is statistically significant according to the Wilcoxon signed rank test at the level of 0.05. or sensitivity.",
                "This similarity of performance is expected as we discussed in Section 3.1.",
                "Although the Poisson model and multinomial model are similar in terms of the basic model and/or with simple smoothing methods, the Poisson model has great potential and flexibility to be further improved.",
                "As shown in the rightmost column of Table 1, term dependent two-stage Poisson model consistently outperforms the basic smoothing models, especially for verbose queries.",
                "This model is given in Formula 4, with a Gamma smoothing for the document model p(·|d), and δw, which is term dependent.",
                "The parameter µ of the first stage Gamma smoothing is empirically tuned.",
                "The combination coefficients (i.e., ∆), are estimated with the EM algorithm in Section 3.2.",
                "The parameter sensitivity curves for Dirichlet/Gamma and the per-term two-stage smoothing model are plotted in Figure 2.",
                "The per-term two-stage smoothing method is less sensitive to the parameter µ than Dirichlet/Gamma, and yields better optimal performance. 0 1000 2000 3000 4000 5000 6000 7000 8000 9000 10000 0.1 0.12 0.14 0.16 0.18 0.2 0.22 Dataset: AP; Query Type: SV Parameter: µ AveragePrecision Dirichlet/Gamma Smoothing Term Dependent 2−Stage Figure 2: Term dependent two-stage smoothing of Poisson outperforms Dirichlet/Gamma In the following subsections, we conduct experiments to demonstrate how the flexibility of the Poisson model could be utilized to achieve better performance, which we cannot achieve with multinomial language models. 4.3 Term Dependent Smoothing To test the effectiveness of the term dependent smoothing, we conduct the following two experiments.",
                "In the first experiment, we relax the constant coefficient in the simple Jelinek-Mercer smoothing formula (i.e., Formula 3), and use the EM algorithm proposed in Section 3.2 to find a δw for each unique term.",
                "Since we are using the EM algorithm to iteratively estimate the parameters, we usually do not want the probability of p(·|d) to be zero.",
                "We then use a simple Laplace method to slightly smooth the document model before it goes into the EM iterations.",
                "The documents are then still scored with Formula 3, but using learnt δw.",
                "The results are labeled with JM+L. in Table 2.",
                "Data Q JM JM JM+L. 2-Stage 2-Stage (MAP) PT: No Yes Yes No Yes AP SK 0.203 0.204 0.206 0.223 0.226* SV 0.183 0.189 0.214* 0.204 0.217* Trec7 SK 0.168 0.171 0.174 0.186 0.185 SV 0.176 0.147 0.198* 0.194 0.196 Trec8 SK 0.239 0.240 0.227* 0.257 0.256 SV 0.234 0.223 0.249* 0.242 0.246* Web SK 0.250 0.236 0.220* 0.291 0.307* SV 0.217 0.232 0.261* 0.273 0.292* Table 2: Term dependent smoothing improves retrieval performance An asterisk (*) in Column 3 indicates that the difference between the JM+L. method and JM method is statistically significant; an asterisk (*) in Column 5 means that the difference between term dependent two-stage method and query dependent two-stage method is statistically significant; PT stands for per-term.",
                "With term dependent coefficients, the performance of the Jelinek-Mercer Poisson model is improved in most cases.",
                "However, in some cases (e.g., Trec7/SV), it performs poorly.",
                "This might be caused by the problem of EM estimation with unsmoothed document models.",
                "Once non-zero probability is assigned to all the terms before entering the EM iteration, the performance on verbose queries can be improved significantly.",
                "This indicates that there is still room to find better methods to estimate δw.",
                "Please note that neither the perterm JM method nor the JM+L. method has a parameter to tune.",
                "As shown in Table 1, the term dependent two-stage smoothing can significantly improve retrieval performance.",
                "To understand whether the improvement is contributed by the term dependent smoothing or the two-stage smoothing framework, we design another experiment to compare the perterm two-stage smoothing with the two-stage smoothing method proposed in [29].",
                "Their method managed to find coefficients specific to the query, thus a verbose query would use a higher δ.",
                "However, since their model is based on multinomial language modeling, they could not get per-term coefficients.",
                "We adopt their method to the Poisson two-stage smoothing, and also estimate a per-query coefficient for all the terms.",
                "We compare the performance of such a model with the per-term two-stage smoothing model, and present the results in the right two columns in Table 2.",
                "Again, we see that the per-term two-stage smoothing outperforms the per-query two-stage smoothing, especially for verbose queries.",
                "The improvement is not as large as how the perterm smoothing method improves over Dirichlet/Gamma.",
                "This is expected, since the per-query smoothing has already addressed the query discrimination problem to some extent.",
                "This experiment shows that even if the smoothing is already per-query, making it per-term is still beneficial.",
                "In brief, the per-term smoothing improved the retrieval performance of both one-stage and two-stage smoothing method. 4.4 Mixture Background Model In this section, we conduct experiments to examine the benefits of using a mixture background model without extra computational cost, which can not be achieved for multinomial models.",
                "Specifically, in retrieval formula 3, instead of using a single Poisson distribution to model the background p(·|C), we use Katzs K-Mixture model, which is essentially a mixture of Poisson distributions. p(·|C) can be computed efficiently with simple collection statistics, as discussed in Section 3.3.",
                "Data Query JM.",
                "Poisson JM.",
                "K-Mixture AP SK 0.203 0.204 SV 0.183 0.188* Trec-7 SK 0.168 0.169 SV 0.176 0.178* Trec-8 SK 0.239 0.239 SV 0.234 0.238* Web SK 0.250 0.250 SV 0.217 0.223* Table 3: K-Mixture background model improves retrieval performance The performance of the JM retrieval model with single Poisson background and with Katzs K-Mixture background model is compared in Table 3.",
                "Clearly, using K-Mixture to model the background model outperforms the single Poisson background model in most cases, especially for verbose queries where the improvement is statistically significant.",
                "Figure 3 shows that the performance changes over different parameters for short verbose queries.",
                "The model using K-Mixture background is less sensitive than the one using single Poisson background.",
                "Given that this type of mixture 0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1 0 0.05 0.1 0.15 0.2 0.25 Data: Trec8; Query: SV Parameter: δ AveragePrecision Poisson Background K−Mixture Background Figure 3: K-Mixture background model deviates the sensitivity of verbose queries background model does not require any extra computation cost, it would be interesting to study whether using other mixture Poisson models, such as 2-Poisson and negative Binomial, could help the performance. 5.",
                "RELATED WORK To the best of our knowledge, there has been no study of query generation models based on Poisson distribution.",
                "Language models have been shown to be effective for many retrieval tasks [21, 28, 14, 4].",
                "The most popular and fundamental one is the query-generation language model [21, 13].",
                "All existing query generation language models are based on either multinomial distribution [19, 6, 28, 13] or multivariate Bernoulli distribution [21, 17, 18].",
                "We introduce a new family of language models, based on Poisson distribution.",
                "Poisson distribution has been previously studied in the document generation models [16, 22, 3, 24], leading to the development of one of the most effective retrieval formula BM25 [23]. [24] studies the parallel derivation of three different retrieval models which is related to our comparison of Poisson and multinomial.",
                "However, the Poisson model in their paper is still under the document generation framework, and also does not account for the document length variation. [26] introduces a way to empirically search for an exponential model for the documents.",
                "Poisson mixtures [3] such as 2-Poisson [22], Negative multinomial, and Katzs KMixture [9] has shown to be effective to model and retrieve documents.",
                "Once again, none of this work explores Poisson distribution in the query generation framework.",
                "Language model smoothing [2, 28, 29] and background structures [15, 10, 25, 27] have been studied with multinomial language models. [7] analytically shows that term specific smoothing could be useful.",
                "We show that Poisson language model is natural to accommodate the per-term smoothing without heuristic twist of the semantics of a generative model, and is able to efficiently better model the mixture background, both analytically and empirically. 6.",
                "CONCLUSIONS We present a new family of query generation language models for retrieval based on Poisson distribution.",
                "We derive several smoothing methods for this family of models, including single-stage smoothing and two-stage smoothing.",
                "We compare the new models with the popular multinomial retrieval models both analytically and experimentally.",
                "Our analysis shows that while our new models and multinomial models are equivalent under some assumptions, they are generally different with some important differences.",
                "In particular, we show that Poisson has an advantage over multinomial in naturally accommodating per-term smoothing.",
                "We exploit this property to develop a new per-term smoothing algorithm for Poisson language models, which is shown to outperform term-independent smoothing for both Poisson and multinomial models.",
                "Furthermore, we show that a mixture background model for Poisson can be used to improve the performance and robustness over the standard Poisson background model.",
                "Our work opens up many interesting directions for further exploration in this new family of models.",
                "Further exploring the flexibilities over multinomial language models, such as length normalization and pseudo-feedback could be good future work.",
                "It is also appealing to find robust methods to learn the per-term smoothing coefficients without additional computation cost. 7.",
                "ACKNOWLEDGMENTS We thank the anonymous SIGIR 07 reviewers for their useful comments.",
                "This material is based in part upon work supported by the National Science Foundation under award numbers IIS-0347933 and 0425852. 8.",
                "REFERENCES [1] D. Blei, A. Ng, and M. Jordan.",
                "Latent dirichlet allocation.",
                "Journal of Machine Learning Research, 3:993-1022, 2003. [2] S. F. Chen and J. Goodman.",
                "An empirical study of smoothing techniques for language modeling.",
                "Technical Report TR-10-98, Harvard University, 1998. [3] K. Church and W. Gale.",
                "Poisson mixtures.",
                "Nat.",
                "Lang.",
                "Eng., 1(2):163-190, 1995. [4] W. B. Croft and J. Lafferty, editors.",
                "Language Modeling and Information Retrieval.",
                "Kluwer Academic Publishers, 2003. [5] H. Fang, T. Tao, and C. Zhai.",
                "A formal study of information retrieval heuristics.",
                "In Proceedings of the 27th annual international ACM SIGIR conference on Research and development in information retrieval, pages 49-56, 2004. [6] D. Hiemstra.",
                "Using Language Models for Information Retrieval.",
                "PhD thesis, University of Twente, Enschede, Netherlands, 2001. [7] D. Hiemstra.",
                "Term-specific smoothing for the language modeling approach to information retrieval: the importance of a query term.",
                "In Proceedings of the 25th annual international ACM SIGIR conference on Research and development in information retrieval, pages 35-41, 2002. [8] T. Hofmann.",
                "Probabilistic latent semantic indexing.",
                "In Proceedings of ACM SIGIR99, pages 50-57, 1999. [9] S. M. Katz.",
                "Distribution of content words and phrases in text and language modelling.",
                "Nat.",
                "Lang.",
                "Eng., 2(1):15-59, 1996. [10] O. Kurland and L. Lee.",
                "Corpus structure, language models, and ad-hoc information retrieval.",
                "In Proceedings of the 27th annual international ACM SIGIR conference on Research and development in information retrieval, pages 194-201, 2004. [11] J. Lafferty and C. Zhai.",
                "Document language models, query models, and risk minimization for information retrieval.",
                "In Proceedings of SIGIR01, pages 111-119, Sept 2001. [12] J. Lafferty and C. Zhai.",
                "Probabilistic IR models based on query and document generation.",
                "In Proceedings of the Language Modeling and IR workshop, pages 1-5, May 31 - June 1 2001. [13] J. Lafferty and C. Zhai.",
                "Probabilistic relevance models based on document and query generation.",
                "In W. B. Croft and J. Lafferty, editors, Language Modeling and Information Retrieval.",
                "Kluwer Academic Publishers, 2003. [14] V. Lavrenko and B. Croft.",
                "Relevance-based language models.",
                "In Proceedings of SIGIR01, pages 120-127, Sept 2001. [15] X. Liu and W. B. Croft.",
                "Cluster-based retrieval using language models.",
                "In Proceedings of the 27th annual international ACM SIGIR conference on Research and development in information retrieval, pages 186-193, 2004. [16] E. L. Margulis.",
                "Modelling documents with multiple poisson distributions.",
                "Inf.",
                "Process.",
                "Manage., 29(2):215-227, 1993. [17] A. McCallum and K. Nigam.",
                "A comparison of event models for naive bayes text classification.",
                "In Proceedings of AAAI-98 Workshop on Learning for Text Categorization, 1998. [18] D. Metzler, V. Lavrenko, and W. B. Croft.",
                "Formal multiple-bernoulli models for language modeling.",
                "In Proceedings of the 27th annual international ACM SIGIR conference on Research and development in information retrieval, pages 540-541, 2004. [19] D. H. Miller, T. Leek, and R. Schwartz.",
                "A hidden Markov model information retrieval system.",
                "In Proceedings of the 1999 ACM SIGIR Conference on Research and Development in Information Retrieval, pages 214-221, 1999. [20] A. Papoulis.",
                "Probability, random variables and stochastic processes.",
                "New York: McGraw-Hill, 1984, 2nd ed., 1984. [21] J. M. Ponte and W. B. Croft.",
                "A language modeling approach to information retrieval.",
                "In Proceedings of the 21st annual international ACM SIGIR conference on Research and development in information retrieval, pages 275-281, 1998. [22] S. Robertson and S. Walker.",
                "Some simple effective approximations to the 2-poisson model for probabilistic weighted retrieval.",
                "In Proceedings of SIGIR94, pages 232-241, 1994. [23] S. E. Robertson, S. Walker, S. Jones, M. M.Hancock-Beaulieu, and M. Gatford.",
                "Okapi at TREC-3.",
                "In D. K. Harman, editor, The Third Text REtrieval Conference (TREC-3), pages 109-126, 1995. [24] T. Roelleke and J. Wang.",
                "A parallel derivation of probabilistic information retrieval models.",
                "In Proceedings of the 29th annual international ACM SIGIR conference on Research and development in information retrieval, pages 107-114, 2006. [25] T. Tao, X. Wang, Q. Mei, and C. Zhai.",
                "Language model information retrieval with document expansion.",
                "In Proceedings of HLT/NAACL 2006, pages 407-414, 2006. [26] J. Teevan and D. R. Karger.",
                "Empirical development of an exponential probabilistic model for text retrieval: using textual analysis to build a better model.",
                "In Proceedings of the 26th annual international ACM SIGIR conference on Research and development in informaion retrieval, pages 18-25, 2003. [27] X. Wei and W. B. Croft.",
                "Lda-based document models for ad-hoc retrieval.",
                "In Proceedings of the 29th annual international ACM SIGIR conference on Research and development in information retrieval, pages 178-185, 2006. [28] C. Zhai and J. Lafferty.",
                "A study of smoothing methods for language models applied to ad-hoc information retrieval.",
                "In Proceedings of ACM SIGIR01, pages 334-342, Sept 2001. [29] C. Zhai and J. Lafferty.",
                "Two-stage language models for information retrieval.",
                "In Proceedings of ACM SIGIR02, pages 49-56, Aug 2002."
            ],
            "original_annotated_samples": [],
            "translated_annotated_samples": [
                "Con un \"proceso de Poisson homogéneo\", el recuento de frecuencia de cada evento, es decir, el número de ocurrencias de WI, sigue una distribución de Poisson con el parámetro asociado λit, donde λi es un parámetro de velocidad que caracteriza el número esperado de WI en un tiempo de unidad.proceso de poisson homogéneo"
            ],
            "translated_text": "",
            "candidates": [],
            "error": []
        },
        "single pseudo term": {
            "translated_key": "",
            "is_in_text": true,
            "original_annotated_sentences": [
                "A Study of Poisson Query Generation Model for Information Retrieval Qiaozhu Mei, Hui Fang, Chengxiang Zhai Department of Computer Science University of Illinois at Urbana-Champaign Urbana,IL 61801 {qmei2,hfang,czhai}@uiuc.edu ABSTRACT Many variants of language models have been proposed for information retrieval.",
                "Most existing models are based on multinomial distribution and would score documents based on query likelihood computed based on a query generation probabilistic model.",
                "In this paper, we propose and study a new family of query generation models based on Poisson distribution.",
                "We show that while in their simplest forms, the new family of models and the existing multinomial models are equivalent, they behave differently for many smoothing methods.",
                "We show that the Poisson model has several advantages over the multinomial model, including naturally accommodating per-term smoothing and allowing for more accurate background modeling.",
                "We present several variants of the new model corresponding to different smoothing methods, and evaluate them on four representative TREC test collections.",
                "The results show that while their basic models perform comparably, the Poisson model can outperform multinomial model with per-term smoothing.",
                "The performance can be further improved with two-stage smoothing.",
                "Categories and Subject Descriptors: H.3.3 [Information Search and Retrieval]: Retrieval Models General Terms: Algorithms 1.",
                "INTRODUCTION As a new type of probabilistic retrieval models, language models have been shown to be effective for many retrieval tasks [21, 28, 14, 4].",
                "Among many variants of language models proposed, the most popular and fundamental one is the query-generation language model [21, 13], which leads to the query-likelihood scoring method for ranking documents.",
                "In such a model, given a query q and a document d, we compute the likelihood of generating query q with a model estimated based on document d, i.e., the conditional probability p(q|d).",
                "We can then rank documents based on the likelihood of generating the query.",
                "Virtually all the existing query generation language models are based on either multinomial distribution [19, 6, 28] or multivariate Bernoulli distribution [21, 18].",
                "The multinomial distribution is especially popular and also shown to be quite effective.",
                "The heavy use of multinomial distribution is partly due to the fact that it has been successfully used in speech recognition, where multinomial distribution is a natural choice for modeling the occurrence of a particular word in a particular position in text.",
                "Compared with multivariate Bernoulli, multinomial distribution has the advantage of being able to model the frequency of terms in the query; in contrast, multivariate Bernoulli only models the presence and absence of query terms, thus cannot capture different frequencies of query terms.",
                "However, multivariate Bernoulli also has one potential advantage over multinomial from the viewpoint of retrieval: in a multinomial distribution, the probabilities of all the terms must sum to 1, making it hard to accommodate per-term smoothing, while in a multivariate Bernoulli, the presence probabilities of different terms are completely independent of each other, easily accommodating per-term smoothing and weighting.",
                "Note that term absence is also indirectly captured in a multinomial model through the constraint that all the term probabilities must sum to 1.",
                "In this paper, we propose and study a new family of query generation models based on the Poisson distribution.",
                "In this new family of models, we model the frequency of each term independently with a Poisson distribution.",
                "To score a document, we would first estimate a multivariate Poisson model based on the document, and then score it based on the likelihood of the query given by the estimated Poisson model.",
                "In some sense, the Poisson model combines the advantage of multinomial in modeling term frequency and the advantage of the multivariate Bernoulli in accommodating per-term smoothing.",
                "Indeed, similar to the multinomial distribution, the Poisson distribution models term frequencies, but without the constraint that all the term probabilities must sum to 1, and similar to multivariate Bernoulli, it models each term independently, thus can easily accommodate per-term smoothing.",
                "As in the existing work on multinomial language models, smoothing is critical for this new family of models.",
                "We derive several smoothing methods for Poisson model in parallel to those used for multinomial distributions, and compare the corresponding retrieval models with those based on multinomial distributions.",
                "We find that while with some smoothing methods, the new model and the multinomial model lead to exactly the same formula, with some other smoothing methods they diverge, and the Poisson model brings in more flexibility for smoothing.",
                "In particular, a key difference is that the Poisson model can naturally accommodate perterm smoothing, which is hard to achieve with a multinomial model without heuristic twist of the semantics of a generative model.",
                "We exploit this potential advantage to develop a new term-dependent smoothing algorithm for Poisson model and show that this new smoothing algorithm can improve performance over term-independent smoothing algorithms using either Poisson or multinomial model.",
                "This advantage is seen for both one-stage and two-stage smoothing.",
                "Another potential advantage of the Poisson model is that its corresponding background model for smoothing can be improved through using a mixture model that has a closed form formula.",
                "This new background model is shown to outperform the standard background model and reduce the sensitivity of retrieval performance to the smoothing parameter.",
                "The rest of the paper is organized as follows.",
                "In Section 2, we introduce the new family of query generation models with Poisson distribution, and present various smoothing methods which lead to different retrieval functions.",
                "In Section 3, we analytically compare the Poisson language model with the multinomial language model, from the perspective of retrieval.",
                "We then design empirical experiments to compare the two families of language models in Section 4.",
                "We discuss the related work in 5 and conclude in 6. 2.",
                "QUERY GENERATION WITH POISSON PROCESS In the query generation framework, a basic assumption is that a query is generated with a model estimated based on a document.",
                "In most existing work [12, 6, 28, 29], people assume that each query word is sampled independently from a multinomial distribution.",
                "Alternatively, we assume that a query is generated by sampling the frequency of words from a series of independent Poisson processes [20]. 2.1 The Generation Process Let V = {w1, ..., wn} be a vocabulary set.",
                "Let w be a piece of text composed by an author and c(w1), ..., c(wn) be a frequency vector representing w, where c(wi, w) is the frequency count of term wi in text w. In retrieval, w could be either a query or a document.",
                "We consider the frequency counts of the n unique terms in w as n different types of events, sampled from n independent homogeneous Poisson processes, respectively.",
                "Suppose t is the time period during which the author composed the text.",
                "With a homogeneous Poisson process, the frequency count of each event, i.e., the number of occurrences of wi, follows a Poisson distribution with associated parameter λit, where λi is a rate parameter characterizing the expected number of wi in a unit time.",
                "The probability density function of such a Poisson Distribution is given by P(c(wi, w) = k|λit) = e−λit (λit)k k!",
                "Without losing generality, we set t to the length of the text w (people write one word in a unit time), i.e., t = |w|.",
                "With n such independent Poisson processes, each explaining the generation of one term in the vocabulary, the likelihood of w to be generated from such Poisson processes can be written as p(w|Λ) = n i=1 p(c(wi, w)|Λ) = n i=1 e−λi·|w| (λi · |w|)c(wi,w) c(wi, w)! where Λ = {λ1, ..., λn} and |w| = n i=1 c(wi, w).",
                "We refer to these n independent Poisson processes with parameter Λ as a Poisson Language Model.",
                "Let D = {d1, ..., dm} be an observed set of document samples generated from the Poisson process above.",
                "The maximum likelihood estimate (MLE) of λi is ˆλi = d∈D c(wi, d) d∈D w ∈V c(w , d) Note that this MLE is different from the MLE for the Poisson distribution without considering the document lengths, which appears in [22, 24].",
                "Given a document d, we may estimate a Poisson language model Λd using d as a sample.",
                "The likelihood that a query q is generated from the document language model Λd can be written as p(q|d) = w∈V p(c(w, q)|Λd) (1) This representation is clearly different from the multinomial query generation model as (1) the likelihood includes all the terms in the vocabulary V , instead of only those appearing in q, and (2) instead of the appearance of terms, the event space of this model is the frequencies of each term.",
                "In practice, we have the flexibility to choose the vocabulary V .",
                "In one extreme, we can use the vocabulary of the whole collection.",
                "However, this may bring in noise and considerable computational cost.",
                "In the other extreme, we may focus on the terms in the query and ignore other terms, but some useful information may be lost by ignoring the nonquery terms.",
                "As a compromise, we may conflate all the non-query terms as one <br>single pseudo term</br>.",
                "In other words, we may assume that there is exactly one non-query term in the vocabulary for each query.",
                "In our experiments, we adopt this pseudo non-query term strategy.",
                "A document can be scored with the likelihood in Equation 1.",
                "However, if a query term is unseen in the document, the MLE of the Poisson distribution would assign zero probability to the term, causing the probability of the query to be zero.",
                "As in existing language modeling approaches, the main challenge of constructing a reasonable retrieval model is to find a smoothed language model for p(·|d). 2.2 Smoothing in Poisson Retrieval Model In general, we want to assign non-zero rates for the query terms that are not seen in document d. Many smoothing methods have been proposed for multinomial language models[2, 28, 29].",
                "In general, we have to discount the probabilities of some words seen in the text to leave some extra probability mass to assign to the unseen words.",
                "In Poisson language models, however, we do not have the same constraint as in a multinomial model (i.e., w∈V p(w|d) = 1).",
                "Thus we do not have to discount the probability of seen words in order to give a non-zero rate to an unseen word.",
                "Instead, we only need to guarantee that k=0,1,2,... p(c(w, d) = k|d) = 1.",
                "In this section, we introduce three different strategies to smooth a Poisson language model, and show how they lead to different retrieval functions. 2.2.1 Bayesian Smoothing using Gamma Prior Following the risk minimization framework in [11], we assume that a document is generated by the arrival of terms in a time period of |d| according to the document language model, which essentially consists of a vector of Poisson rates for each term, i.e., Λd = λd,1, ..., λd,|V | .",
                "A document is assumed to be generated from a potentially different model.",
                "Given a particular document d, we want to estimate Λd.",
                "The rate of a term is estimated independently of other terms.",
                "We use Bayesian estimation with the following Gamma prior, which has two parameters, α and β: Gamma(λ|α, β) = βα Γ(α) λα−1 e−βλ For each term w, the parameters αw and βw are chosen to be αw = µ ∗ λC,w and βw = µ, where µ is a parameter and λC,w is the rate of w estimated from some background language model, usually the collection language model.",
                "The posterior distribution of Λd is given by p(Λd|d, C) ∝ w∈V e−λw(|d|+µ) λ c(w,d)+µλC,w−1 w which is a product of |V | Gamma distributions with parameters c(w, d) + µλC,w and |d| + µ for each word w. Given that the Gamma mean is α β , we have ˆλd,w = λd,w λd,wp(λd,w|d, C)dλd,w = c(w, d) + µλC,w |d| + µ This is precisely the smoothed estimate of multinomial language model with Dirichlet prior [28]. 2.2.2 Interpolation (Jelinek-Mercer) Smoothing Another straightforward method is to decompose the query generation model as a mixture of two component models.",
                "One is the document language model estimated with maximum likelihood estimator, and the other is a model estimated from the collection background, p(·|C), which assigns non-zero rate to w. For example, we may use an interpolation coefficient between 0 and 1 (i.e., δ ∈ [0, 1]).",
                "With this simple interpolation, we can score a document with Score(d, q) = w∈V log((1 − δ)p(c(w, q)|d) + δp(c(w, q)|C)) (2) Using the maximum likelihood estimator for p(·|d), we have λd,w = c(w,d) |d| , thus Equation 2 becomes Score(d, q) ∝ w∈d∩q [log(1 + 1 − δ δ e−λd,w|q| (λd,w|q|)c(w,q) c(w, q)! · p(c(w, q)|C) ) − log (1 − δ)e−λd,w|q| + δp(c(w, q) = 0|C) 1 − δ + δp(c(w, q) = 0|C) ] + w∈d log (1 − δ)e−λd,w|q| + δp(c(w, q) = 0|C) 1 − δ + δp(c(w, q) = 0|C) We can also use a Poisson language model for p(·|C), or use some other frequency-based models.",
                "In the retrieval formula above, the first summation can be computed efficiently.",
                "The second summation can be actually treated as a document prior, which penalizes long documents.",
                "As the second summation is difficult to compute efficiently, we conflate all non-query terms as one pseudo non-queryterm, denoted as N. Using the pseudo-term formulation and a Poisson collection model, we can rewrite the retrieval formula as Score(d, q) ∝ w∈d∩q log(1 + 1 − δ δ e−λd,w (λd,w|q|)c(w,q) e−λd,C |q| (λd,C )c(w,q) ) + log (1 − δ)e−λd,N |q| + δe−λC,N |q| 1 − δ + δe−λC,N |q| (3) where λd,N = |d|− w∈q c(w,d) |d| and λC,N = |C|− w∈q c(w,C) |C| . 2.2.3 Two-Stage Smoothing As discussed in [29], smoothing plays two roles in retrieval: (1) to improve the estimation of the document language model, and (2) to explain the common terms in the query.",
                "In order to distinguish the content and non-discriminative words in a query, we follow [29] and assume that a query is generated by sampling from a two-component mixture of Poisson language models, with one component being the document model Λd and the other being a query background language model p(·|U). p(·|U) models the typical term frequencies in the users queries.",
                "We may then score each document with the query likelihood computed using the following two-stage smoothing model: p(c(w, q)|Λd, U) = (1 − δ)p(c(w, q)|Λd) + δp(c(w, q)|U) (4) where δ is a parameter, roughly indicating the amount of noise in q.",
                "This looks similar to the interpolation smoothing, except that p(·|Λd) now should be a smoothed language model, instead of the one estimated with MLE.",
                "With no prior knowledge on p(·|U), we could set it to p(·|C).",
                "Any smoothing methods for the document language model can be used to estimate p(·|d) such as the Gamma smoothing as discussed in Section 2.2.1.",
                "The empirical study of the smoothing methods is presented in Section 4. 3.",
                "ANALYSIS OF POISSON LANGUAGE MODEL From the previous section, we notice that the Poisson language model has a strong connection to the multinomial language model.",
                "This is expected since they both belong to the exponential family [26].",
                "However, there are many differences when these two families of models are applied with different smoothing methods.",
                "From the perspective of retrieval, will these two language models perform equivalently?",
                "If not, which model provides more benefits to retrieval, or provides flexibility which could lead to potential benefits?",
                "In this section, we analytically discuss the retrieval features of the Poisson language models, by comparing their behavior with that of the multinomial language models. 3.1 The Equivalence of Basic Models Let us begin with the assumption that all the query terms appear in every document.",
                "Under this assumption, no smoothing is needed.",
                "A document can be scored by the log likelihood of the query with the maximum likelihood estimate: Score(d, q) = w∈V log e−λd,w|q| (λd,w|q|)c(w,q) c(w, q)! (5) Using the MLE, we have λd,w = c(w,d) w∈V c(w,d) .",
                "Thus Score(d, q) ∝ c(w,q)>0 c(w, q) log c(w, d) w∈V c(w, d) This is exactly the log likelihood of the query if the document language model is a multinomial with maximum likelihood estimate.",
                "Indeed, even with Gamma smoothing, when plugging λd,w = c(w,d)+µλC,w |d|+µ and λC,w = c(w,C) |C| into Equation 5, it is easy to show that Score(d, q) ∝ w∈q∩d c(w, q) log(1 + c(w, d) µ · c(w,C) |C| ) + |q| log µ |d| + µ (6) which is exactly the Dirichlet retrieval formula in [28].",
                "Note that this equivalence holds only when the document length variation is modeled with Poisson process.",
                "This derivation indicates the equivalence of the basic Poisson and multinomial language models for retrieval.",
                "With other smoothing strategies, however, the two models would be different.",
                "Nevertheless, with this equivalence in basic models, we could expect that the Poisson language model performs comparably to the multinomial language model in retrieval, if only simple smoothing is explored.",
                "Based on this equivalence analysis, one may ask, why we should pursue the Poisson language model.",
                "In the following sections, we show that despite the equivalence in their basic models, the Poisson language model brings in extra flexibility for exploring advanced techniques on various retrieval features, which could not be achieved with multinomial language models. 3.2 Term Dependent Smoothing One flexibility of the Poisson language model is that it provides a natural framework to accommodate term dependent (per-term) smoothing.",
                "Existing work on language model smoothing has already shown that different types of queries should be smoothed differently according to how discriminative the query terms are. [7] also predicted that different terms should have a different smoothing weights.",
                "With multinomial query generation models, people usually use a single smoothing coefficient to control the combination of the document model and the background model [28, 29].",
                "This parameter can be made specific for different queries, but always has to be a constant for all the terms.",
                "This is mandatory since a multinomial language model has the constraint that w∈V p(w|d) = 1.",
                "However, from retrieval perspective, different terms may need to be smoothed differently even if they are in the same query.",
                "For example, a non-discriminative term (e.g., the, is) is expected to be explained more with the background model, while a content term (e.g., retrieval, bush) in the query should be explained with the document model.",
                "Therefore, a better way of smoothing would be to set the interpolation coefficient (i.e., δ in Formula 2 and Formula 3) specifically for each term.",
                "Since the Poisson language model does not have the sum-to-one constraint across terms, it can easily accommodate per-term smoothing without needing to heuristically twist the semantics of a generative model as in the case of multinomial language models.",
                "Below we present a possible way to explore term dependent smoothing with Poisson language models.",
                "Essentially, we want to use a term-specific smoothing coefficient δ in the linear combination, denoted as δw.",
                "This coefficient should intuitively be larger if w is a common word and smaller if it is a content word.",
                "The key problem is to find a method to assign reasonable values to δw.",
                "Empirical tuning is infeasible for so many parameters.",
                "We may instead estimate the parameters ∆ = {δ1, ..., δ|V |} by maximizing the likelihood of the query given the mixture model of p(q|ΛQ) and p(q|U), where ΛQ is the true query model to generate the query and p(q|U) is a query background model as discussed in Section 2.2.3.",
                "With the model p(q|ΛQ) hidden, the query likelihood is p(q|∆, U) = ΛQ w∈V ((1 − δw)p(c(w, q)|ΛQ) + δwp(c(w, q)|U))P(ΛQ|U)dΛQ If we have relevant documents for each query, we can approximate the query model space with the language models of all the relevant documents.",
                "Without relevant documents, we opt to approximate the query model space with the models of all the documents in the collection.",
                "Setting p(·|U) as p(·|C), the query likelihood becomes p(q|∆, U) = d∈C πd w∈V ((1−δw)p(c(w, q)|ˆΛd)+δwp(c(w, q)|C)) where πd = p(ˆΛd|U). p(·|ˆΛd) is an estimated Poisson language model for document d. If we have prior knowledge on p(ˆΛd|U), such as which documents are relevant to the query, we can set πd accordingly, because what we want is to find ∆ that can maximize the likelihood of the query given relevant documents.",
                "Without this prior knowledge, we can leave πd as free parameters, and use the EM algorithm to estimate πd and ∆.",
                "The updating functions are given as π (k+1) d = πd w∈V ((1 − δw)p(c(w, q)|ˆΛd) + δwp(c(w, q)|C)) d∈C πd w∈V ((1 − δw)p(c(w, q)|ˆΛd) + δwp(c(w, q)|C)) and δ (k+1) w = d∈C πd δwp(c(w, q)|C)) (1 − δw)p(c(w, q)|ˆΛd) + δwp(c(w, q)|C)) As discussed in [29], we only need to run the EM algorithm for several iterations, thus the computational cost is relatively low.",
                "We again assume our vocabulary containing all query terms plus a pseudo non-query term.",
                "Note that the function does not give an explicit way of estimating the coefficient for the unseen non-query term.",
                "In our experiments, we set it to the average over δw of all query terms.",
                "With this flexibility, we expect Poisson language models could improve the retrieval performance, especially for verbose queries, where the query terms have various discriminative values.",
                "In Section 4, we use empirical experiments to prove this hypothesis. 3.3 Mixture Background Models Another flexibility is to explore different background (collection) models (i.e., p(·|U), or p(·|C)).",
                "One common assumption made in language modeling information retrieval is that the background model is a homogeneous model of the document models [28, 29].",
                "Similarly, we can also make the assumption that the collection model is a Poisson language model, with the rates λC,w = d∈C c(w,d) |C| .",
                "However, this assumption usually does not hold, since the collection is far more complex than a single document.",
                "Indeed, the collection usually consists of a mixture of documents with various genres, authors, and topics, etc.",
                "Treating the collection model as a mixture of document models, instead of a single pseudo-document model is more reasonable.",
                "Existing work of multinomial language modeling has already shown that a better modeling of background improves the retrieval performance, such as clusters [15, 10], neighbor documents [25], and aspects [8, 27].",
                "All the approaches can be easily adopted using Poisson language models.",
                "However, a common problem of these approaches is that they all require heavy computation to construct the background model.",
                "With Poisson language modeling, we show that it is possible to model the mixture background without paying for the heavy computational cost.",
                "Poisson Mixture [3] has been proposed to model a collection of documents, which can fit the data much better than a single Poisson.",
                "The basic idea is to assume that the collection is generated from a mixture of Poisson models, which has the general form of p(x = k|PM) = λ p(λ)p(x = k|λ)dλ p(·|λ) is a single Poisson model and p(λ) is an arbitrary probability density function.",
                "There are three well known Poisson mixtures [3]: 2-Poisson, Negative Binomial, and the Katzs K-Mixture [9].",
                "Note that the 2-Poisson model has actually been explored in probabilistic retrieval models, which led to the well-known BM25 formula [22].",
                "All these mixtures have closed forms, and can be estimated from the collection of documents efficiently.",
                "This is an advantage over the multinomial mixture models, such as PLSI [8] and LDA [1], for retrieval.",
                "For example, the probability density function of Katzs K-Mixture is given as p(c(w) = k|αw, βw) = (1 − αw)ηk,0 + αw βw + 1 ( βw βw + 1 )k where ηk,0 = 1 when k = 0, and 0 otherwise.",
                "With the observation of a collection of documents, αw and βw can be estimated as βw = cf(w) − df(w) df(w) and αw = cf(w) Nβw where cf(w) and df(w) are the collection frequency and document frequency of w, and N is the number of documents in the collection.",
                "To account for the different document lengths, we assume that βw is a reasonable estimation for generating a document of the average length, and use β = βw avdl |q| to generate the query.",
                "This Poisson mixture model can be easily used to replace P(·|C) in the retrieval functions 3 and 4. 3.4 Other Possible Flexibilities In addition to term dependent smoothing and efficient mixture background, a Poisson language model has also some other potential advantages.",
                "For example, in Section 2, we see that Formula 2 introduces a component which does document length penalization.",
                "Intuitively, when the document has more unique words, it will be penalized more.",
                "On the other hand, if a document is exactly n copies of another document, it would not get over penalized.",
                "This feature is desirable and not achieved with the Dirichlet model [5].",
                "Potentially, this component could penalize a document according to what types of terms it contains.",
                "With term specific settings of δ, we could get even more flexibility for document length normalization.",
                "Pseudo-feedback is yet another interesting direction where the Poission model might be able to show its advantage.",
                "With model-based feedback, we could again relax the combination coefficients of the feedback model and the background model, and allow different terms to contribute differently to the feedback model.",
                "We could also utilize the relevant documents to learn better per-term smoothing coefficients. 4.",
                "EVALUATION In Section 3, we analytically compared the Poisson language models and multinomial language models from the perspective of query generation and retrieval.",
                "In this section, we compare these two families of models empirically.",
                "Experiment results show that the Poisson model with perterm smoothing outperforms multinomial model, and the performance can be further improved with two-stage smoothing.",
                "Using Poisson mixture as background model also improves the retrieval performance. 4.1 Datasets Since retrieval performance could significantly vary from one test collection to another, and from one query to another, we select four representative TREC test collections: AP, Trec7, Trec8, and Wt2g(Web).",
                "To cover different types of queries, we follow [28, 5], and construct short-keyword (SK, keyword title), short-verbose (SV, one sentence description), and long-verbose (LV, multiple sentences) queries.",
                "The documents are stemmed with the Porters stemmer, and we do not remove any stop word.",
                "For each parameter, we vary its value to cover a reasonably wide range. 4.2 Comparison to Multinomial We compare the performance of the Poisson retrieval models and multinomial retrieval models using interpolation (JelinekMercer, JM) smoothing and Bayesian smoothing with conjugate priors.",
                "Table 1 shows that the two JM-smoothed models perform similarly on all data sets.",
                "Since the Dirichlet Smoothing for multinomial language model and the Gamma Smoothing for Poisson language model lead to the same retrieval formula, the performance of these two models are jointly presented.",
                "We see that Dirichlet/Gamma smoothing methods outperform both Jelinek-Mercer smoothing methods.",
                "The parameter sensitivity curves for two Jelinek-Mercer 0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1 0 0.05 0.1 0.15 0.2 0.25 0.3 Dataset: Trec8 Parameter: δ AveragePrecision JM−Multinomial: LV JM−Multinomial: SV JM−Multinomial: SK JM−Poisson: SK JM−Poisson: SV JM−Poisson: LV Figure 1: Poisson and multinomial performs similarly with Jelinek-Mercer smoothing smoothing methods are shown in Figure 1.",
                "Clearly, these two methods perform similarly either in terms of optimality Data Query JM-Multinomial JM-Poisson Dirichlet/Gamma Per-term 2-Stage Poisson MAP InitPr Pr@5d MAP InitPr Pr@5d MAP InitPr Pr@5d MAP InitPr Pr@5d AP88-89 SK 0.203 0.585 0.356 0.203 0.585 0.358 0.224 0.629 0.393 0.226 0.630 0.396 SV 0.187 0.580 0.361 0.183 0.571 0.345 0.204 0.613 0.387 0.217* 0.603 0.390 LV 0.283 0.716 0.480 0.271 0.692 0.470 0.291 0.710 0.496 0.304* 0.695 0.510 Trec7 SK 0.167 0.635 0.400 0.168 0.635 0.404 0.186 0.687 0.428 0.185 0.646 0.436 SV 0.174 0.655 0.432 0.176 0.653 0.432 0.182 0.666 0.432 0.196* 0.660 0.440 LV 0.223 0.730 0.496 0.215 0.766 0.488 0.224 0.748 0.52 0.236* 0.738 0.512 Trec8 SK 0.239 0.621 0.440 0.239 0.621 0.436 0.257 0.718 0.496 0.256 0.704 0.468 SV 0.231 0.686 0.448 0.234 0.702 0.456 0.228 0.691 0.456 0.246* 0.692 0.476 LV 0.265 0.796 0.548 0.261 0.757 0.520 0.260 0.741 0.492 0.274* 0.766 0.508 Web SK 0.250 0.616 0.380 0.250 0.616 0.380 0.302 0.767 0.468 0.307 0.739 0.468 SV 0.214 0.611 0.392 0.217 0.609 0.384 0.273 0.693 0.508 0.292* 0.703 0.480 LV 0.266 0.790 0.464 0.259 0.776 0.452 0.283 0.756 0.496 0.311* 0.759 0.488 Table 1: Performance comparison between Poisson and Multinomial retrieval models: basic models perform comparably; term dependent two-stage smoothing significantly improves Poisson An asterisk (*) indicates that the difference between the performance of the term dependent two-stage smoothing and that of the Dirichlet/Gamma single smoothing is statistically significant according to the Wilcoxon signed rank test at the level of 0.05. or sensitivity.",
                "This similarity of performance is expected as we discussed in Section 3.1.",
                "Although the Poisson model and multinomial model are similar in terms of the basic model and/or with simple smoothing methods, the Poisson model has great potential and flexibility to be further improved.",
                "As shown in the rightmost column of Table 1, term dependent two-stage Poisson model consistently outperforms the basic smoothing models, especially for verbose queries.",
                "This model is given in Formula 4, with a Gamma smoothing for the document model p(·|d), and δw, which is term dependent.",
                "The parameter µ of the first stage Gamma smoothing is empirically tuned.",
                "The combination coefficients (i.e., ∆), are estimated with the EM algorithm in Section 3.2.",
                "The parameter sensitivity curves for Dirichlet/Gamma and the per-term two-stage smoothing model are plotted in Figure 2.",
                "The per-term two-stage smoothing method is less sensitive to the parameter µ than Dirichlet/Gamma, and yields better optimal performance. 0 1000 2000 3000 4000 5000 6000 7000 8000 9000 10000 0.1 0.12 0.14 0.16 0.18 0.2 0.22 Dataset: AP; Query Type: SV Parameter: µ AveragePrecision Dirichlet/Gamma Smoothing Term Dependent 2−Stage Figure 2: Term dependent two-stage smoothing of Poisson outperforms Dirichlet/Gamma In the following subsections, we conduct experiments to demonstrate how the flexibility of the Poisson model could be utilized to achieve better performance, which we cannot achieve with multinomial language models. 4.3 Term Dependent Smoothing To test the effectiveness of the term dependent smoothing, we conduct the following two experiments.",
                "In the first experiment, we relax the constant coefficient in the simple Jelinek-Mercer smoothing formula (i.e., Formula 3), and use the EM algorithm proposed in Section 3.2 to find a δw for each unique term.",
                "Since we are using the EM algorithm to iteratively estimate the parameters, we usually do not want the probability of p(·|d) to be zero.",
                "We then use a simple Laplace method to slightly smooth the document model before it goes into the EM iterations.",
                "The documents are then still scored with Formula 3, but using learnt δw.",
                "The results are labeled with JM+L. in Table 2.",
                "Data Q JM JM JM+L. 2-Stage 2-Stage (MAP) PT: No Yes Yes No Yes AP SK 0.203 0.204 0.206 0.223 0.226* SV 0.183 0.189 0.214* 0.204 0.217* Trec7 SK 0.168 0.171 0.174 0.186 0.185 SV 0.176 0.147 0.198* 0.194 0.196 Trec8 SK 0.239 0.240 0.227* 0.257 0.256 SV 0.234 0.223 0.249* 0.242 0.246* Web SK 0.250 0.236 0.220* 0.291 0.307* SV 0.217 0.232 0.261* 0.273 0.292* Table 2: Term dependent smoothing improves retrieval performance An asterisk (*) in Column 3 indicates that the difference between the JM+L. method and JM method is statistically significant; an asterisk (*) in Column 5 means that the difference between term dependent two-stage method and query dependent two-stage method is statistically significant; PT stands for per-term.",
                "With term dependent coefficients, the performance of the Jelinek-Mercer Poisson model is improved in most cases.",
                "However, in some cases (e.g., Trec7/SV), it performs poorly.",
                "This might be caused by the problem of EM estimation with unsmoothed document models.",
                "Once non-zero probability is assigned to all the terms before entering the EM iteration, the performance on verbose queries can be improved significantly.",
                "This indicates that there is still room to find better methods to estimate δw.",
                "Please note that neither the perterm JM method nor the JM+L. method has a parameter to tune.",
                "As shown in Table 1, the term dependent two-stage smoothing can significantly improve retrieval performance.",
                "To understand whether the improvement is contributed by the term dependent smoothing or the two-stage smoothing framework, we design another experiment to compare the perterm two-stage smoothing with the two-stage smoothing method proposed in [29].",
                "Their method managed to find coefficients specific to the query, thus a verbose query would use a higher δ.",
                "However, since their model is based on multinomial language modeling, they could not get per-term coefficients.",
                "We adopt their method to the Poisson two-stage smoothing, and also estimate a per-query coefficient for all the terms.",
                "We compare the performance of such a model with the per-term two-stage smoothing model, and present the results in the right two columns in Table 2.",
                "Again, we see that the per-term two-stage smoothing outperforms the per-query two-stage smoothing, especially for verbose queries.",
                "The improvement is not as large as how the perterm smoothing method improves over Dirichlet/Gamma.",
                "This is expected, since the per-query smoothing has already addressed the query discrimination problem to some extent.",
                "This experiment shows that even if the smoothing is already per-query, making it per-term is still beneficial.",
                "In brief, the per-term smoothing improved the retrieval performance of both one-stage and two-stage smoothing method. 4.4 Mixture Background Model In this section, we conduct experiments to examine the benefits of using a mixture background model without extra computational cost, which can not be achieved for multinomial models.",
                "Specifically, in retrieval formula 3, instead of using a single Poisson distribution to model the background p(·|C), we use Katzs K-Mixture model, which is essentially a mixture of Poisson distributions. p(·|C) can be computed efficiently with simple collection statistics, as discussed in Section 3.3.",
                "Data Query JM.",
                "Poisson JM.",
                "K-Mixture AP SK 0.203 0.204 SV 0.183 0.188* Trec-7 SK 0.168 0.169 SV 0.176 0.178* Trec-8 SK 0.239 0.239 SV 0.234 0.238* Web SK 0.250 0.250 SV 0.217 0.223* Table 3: K-Mixture background model improves retrieval performance The performance of the JM retrieval model with single Poisson background and with Katzs K-Mixture background model is compared in Table 3.",
                "Clearly, using K-Mixture to model the background model outperforms the single Poisson background model in most cases, especially for verbose queries where the improvement is statistically significant.",
                "Figure 3 shows that the performance changes over different parameters for short verbose queries.",
                "The model using K-Mixture background is less sensitive than the one using single Poisson background.",
                "Given that this type of mixture 0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1 0 0.05 0.1 0.15 0.2 0.25 Data: Trec8; Query: SV Parameter: δ AveragePrecision Poisson Background K−Mixture Background Figure 3: K-Mixture background model deviates the sensitivity of verbose queries background model does not require any extra computation cost, it would be interesting to study whether using other mixture Poisson models, such as 2-Poisson and negative Binomial, could help the performance. 5.",
                "RELATED WORK To the best of our knowledge, there has been no study of query generation models based on Poisson distribution.",
                "Language models have been shown to be effective for many retrieval tasks [21, 28, 14, 4].",
                "The most popular and fundamental one is the query-generation language model [21, 13].",
                "All existing query generation language models are based on either multinomial distribution [19, 6, 28, 13] or multivariate Bernoulli distribution [21, 17, 18].",
                "We introduce a new family of language models, based on Poisson distribution.",
                "Poisson distribution has been previously studied in the document generation models [16, 22, 3, 24], leading to the development of one of the most effective retrieval formula BM25 [23]. [24] studies the parallel derivation of three different retrieval models which is related to our comparison of Poisson and multinomial.",
                "However, the Poisson model in their paper is still under the document generation framework, and also does not account for the document length variation. [26] introduces a way to empirically search for an exponential model for the documents.",
                "Poisson mixtures [3] such as 2-Poisson [22], Negative multinomial, and Katzs KMixture [9] has shown to be effective to model and retrieve documents.",
                "Once again, none of this work explores Poisson distribution in the query generation framework.",
                "Language model smoothing [2, 28, 29] and background structures [15, 10, 25, 27] have been studied with multinomial language models. [7] analytically shows that term specific smoothing could be useful.",
                "We show that Poisson language model is natural to accommodate the per-term smoothing without heuristic twist of the semantics of a generative model, and is able to efficiently better model the mixture background, both analytically and empirically. 6.",
                "CONCLUSIONS We present a new family of query generation language models for retrieval based on Poisson distribution.",
                "We derive several smoothing methods for this family of models, including single-stage smoothing and two-stage smoothing.",
                "We compare the new models with the popular multinomial retrieval models both analytically and experimentally.",
                "Our analysis shows that while our new models and multinomial models are equivalent under some assumptions, they are generally different with some important differences.",
                "In particular, we show that Poisson has an advantage over multinomial in naturally accommodating per-term smoothing.",
                "We exploit this property to develop a new per-term smoothing algorithm for Poisson language models, which is shown to outperform term-independent smoothing for both Poisson and multinomial models.",
                "Furthermore, we show that a mixture background model for Poisson can be used to improve the performance and robustness over the standard Poisson background model.",
                "Our work opens up many interesting directions for further exploration in this new family of models.",
                "Further exploring the flexibilities over multinomial language models, such as length normalization and pseudo-feedback could be good future work.",
                "It is also appealing to find robust methods to learn the per-term smoothing coefficients without additional computation cost. 7.",
                "ACKNOWLEDGMENTS We thank the anonymous SIGIR 07 reviewers for their useful comments.",
                "This material is based in part upon work supported by the National Science Foundation under award numbers IIS-0347933 and 0425852. 8.",
                "REFERENCES [1] D. Blei, A. Ng, and M. Jordan.",
                "Latent dirichlet allocation.",
                "Journal of Machine Learning Research, 3:993-1022, 2003. [2] S. F. Chen and J. Goodman.",
                "An empirical study of smoothing techniques for language modeling.",
                "Technical Report TR-10-98, Harvard University, 1998. [3] K. Church and W. Gale.",
                "Poisson mixtures.",
                "Nat.",
                "Lang.",
                "Eng., 1(2):163-190, 1995. [4] W. B. Croft and J. Lafferty, editors.",
                "Language Modeling and Information Retrieval.",
                "Kluwer Academic Publishers, 2003. [5] H. Fang, T. Tao, and C. Zhai.",
                "A formal study of information retrieval heuristics.",
                "In Proceedings of the 27th annual international ACM SIGIR conference on Research and development in information retrieval, pages 49-56, 2004. [6] D. Hiemstra.",
                "Using Language Models for Information Retrieval.",
                "PhD thesis, University of Twente, Enschede, Netherlands, 2001. [7] D. Hiemstra.",
                "Term-specific smoothing for the language modeling approach to information retrieval: the importance of a query term.",
                "In Proceedings of the 25th annual international ACM SIGIR conference on Research and development in information retrieval, pages 35-41, 2002. [8] T. Hofmann.",
                "Probabilistic latent semantic indexing.",
                "In Proceedings of ACM SIGIR99, pages 50-57, 1999. [9] S. M. Katz.",
                "Distribution of content words and phrases in text and language modelling.",
                "Nat.",
                "Lang.",
                "Eng., 2(1):15-59, 1996. [10] O. Kurland and L. Lee.",
                "Corpus structure, language models, and ad-hoc information retrieval.",
                "In Proceedings of the 27th annual international ACM SIGIR conference on Research and development in information retrieval, pages 194-201, 2004. [11] J. Lafferty and C. Zhai.",
                "Document language models, query models, and risk minimization for information retrieval.",
                "In Proceedings of SIGIR01, pages 111-119, Sept 2001. [12] J. Lafferty and C. Zhai.",
                "Probabilistic IR models based on query and document generation.",
                "In Proceedings of the Language Modeling and IR workshop, pages 1-5, May 31 - June 1 2001. [13] J. Lafferty and C. Zhai.",
                "Probabilistic relevance models based on document and query generation.",
                "In W. B. Croft and J. Lafferty, editors, Language Modeling and Information Retrieval.",
                "Kluwer Academic Publishers, 2003. [14] V. Lavrenko and B. Croft.",
                "Relevance-based language models.",
                "In Proceedings of SIGIR01, pages 120-127, Sept 2001. [15] X. Liu and W. B. Croft.",
                "Cluster-based retrieval using language models.",
                "In Proceedings of the 27th annual international ACM SIGIR conference on Research and development in information retrieval, pages 186-193, 2004. [16] E. L. Margulis.",
                "Modelling documents with multiple poisson distributions.",
                "Inf.",
                "Process.",
                "Manage., 29(2):215-227, 1993. [17] A. McCallum and K. Nigam.",
                "A comparison of event models for naive bayes text classification.",
                "In Proceedings of AAAI-98 Workshop on Learning for Text Categorization, 1998. [18] D. Metzler, V. Lavrenko, and W. B. Croft.",
                "Formal multiple-bernoulli models for language modeling.",
                "In Proceedings of the 27th annual international ACM SIGIR conference on Research and development in information retrieval, pages 540-541, 2004. [19] D. H. Miller, T. Leek, and R. Schwartz.",
                "A hidden Markov model information retrieval system.",
                "In Proceedings of the 1999 ACM SIGIR Conference on Research and Development in Information Retrieval, pages 214-221, 1999. [20] A. Papoulis.",
                "Probability, random variables and stochastic processes.",
                "New York: McGraw-Hill, 1984, 2nd ed., 1984. [21] J. M. Ponte and W. B. Croft.",
                "A language modeling approach to information retrieval.",
                "In Proceedings of the 21st annual international ACM SIGIR conference on Research and development in information retrieval, pages 275-281, 1998. [22] S. Robertson and S. Walker.",
                "Some simple effective approximations to the 2-poisson model for probabilistic weighted retrieval.",
                "In Proceedings of SIGIR94, pages 232-241, 1994. [23] S. E. Robertson, S. Walker, S. Jones, M. M.Hancock-Beaulieu, and M. Gatford.",
                "Okapi at TREC-3.",
                "In D. K. Harman, editor, The Third Text REtrieval Conference (TREC-3), pages 109-126, 1995. [24] T. Roelleke and J. Wang.",
                "A parallel derivation of probabilistic information retrieval models.",
                "In Proceedings of the 29th annual international ACM SIGIR conference on Research and development in information retrieval, pages 107-114, 2006. [25] T. Tao, X. Wang, Q. Mei, and C. Zhai.",
                "Language model information retrieval with document expansion.",
                "In Proceedings of HLT/NAACL 2006, pages 407-414, 2006. [26] J. Teevan and D. R. Karger.",
                "Empirical development of an exponential probabilistic model for text retrieval: using textual analysis to build a better model.",
                "In Proceedings of the 26th annual international ACM SIGIR conference on Research and development in informaion retrieval, pages 18-25, 2003. [27] X. Wei and W. B. Croft.",
                "Lda-based document models for ad-hoc retrieval.",
                "In Proceedings of the 29th annual international ACM SIGIR conference on Research and development in information retrieval, pages 178-185, 2006. [28] C. Zhai and J. Lafferty.",
                "A study of smoothing methods for language models applied to ad-hoc information retrieval.",
                "In Proceedings of ACM SIGIR01, pages 334-342, Sept 2001. [29] C. Zhai and J. Lafferty.",
                "Two-stage language models for information retrieval.",
                "In Proceedings of ACM SIGIR02, pages 49-56, Aug 2002."
            ],
            "original_annotated_samples": [],
            "translated_annotated_samples": [
                "Como compromiso, podemos combinar todos los términos no quirales como un \"pseudo término único\".pseudo término"
            ],
            "translated_text": "",
            "candidates": [],
            "error": []
        },
        "language model": {
            "translated_key": "",
            "is_in_text": true,
            "original_annotated_sentences": [
                "A Study of Poisson Query Generation Model for Information Retrieval Qiaozhu Mei, Hui Fang, Chengxiang Zhai Department of Computer Science University of Illinois at Urbana-Champaign Urbana,IL 61801 {qmei2,hfang,czhai}@uiuc.edu ABSTRACT Many variants of language models have been proposed for information retrieval.",
                "Most existing models are based on multinomial distribution and would score documents based on query likelihood computed based on a query generation probabilistic model.",
                "In this paper, we propose and study a new family of query generation models based on Poisson distribution.",
                "We show that while in their simplest forms, the new family of models and the existing multinomial models are equivalent, they behave differently for many smoothing methods.",
                "We show that the Poisson model has several advantages over the multinomial model, including naturally accommodating per-term smoothing and allowing for more accurate background modeling.",
                "We present several variants of the new model corresponding to different smoothing methods, and evaluate them on four representative TREC test collections.",
                "The results show that while their basic models perform comparably, the Poisson model can outperform multinomial model with per-term smoothing.",
                "The performance can be further improved with two-stage smoothing.",
                "Categories and Subject Descriptors: H.3.3 [Information Search and Retrieval]: Retrieval Models General Terms: Algorithms 1.",
                "INTRODUCTION As a new type of probabilistic retrieval models, language models have been shown to be effective for many retrieval tasks [21, 28, 14, 4].",
                "Among many variants of language models proposed, the most popular and fundamental one is the query-generation <br>language model</br> [21, 13], which leads to the query-likelihood scoring method for ranking documents.",
                "In such a model, given a query q and a document d, we compute the likelihood of generating query q with a model estimated based on document d, i.e., the conditional probability p(q|d).",
                "We can then rank documents based on the likelihood of generating the query.",
                "Virtually all the existing query generation language models are based on either multinomial distribution [19, 6, 28] or multivariate Bernoulli distribution [21, 18].",
                "The multinomial distribution is especially popular and also shown to be quite effective.",
                "The heavy use of multinomial distribution is partly due to the fact that it has been successfully used in speech recognition, where multinomial distribution is a natural choice for modeling the occurrence of a particular word in a particular position in text.",
                "Compared with multivariate Bernoulli, multinomial distribution has the advantage of being able to model the frequency of terms in the query; in contrast, multivariate Bernoulli only models the presence and absence of query terms, thus cannot capture different frequencies of query terms.",
                "However, multivariate Bernoulli also has one potential advantage over multinomial from the viewpoint of retrieval: in a multinomial distribution, the probabilities of all the terms must sum to 1, making it hard to accommodate per-term smoothing, while in a multivariate Bernoulli, the presence probabilities of different terms are completely independent of each other, easily accommodating per-term smoothing and weighting.",
                "Note that term absence is also indirectly captured in a multinomial model through the constraint that all the term probabilities must sum to 1.",
                "In this paper, we propose and study a new family of query generation models based on the Poisson distribution.",
                "In this new family of models, we model the frequency of each term independently with a Poisson distribution.",
                "To score a document, we would first estimate a multivariate Poisson model based on the document, and then score it based on the likelihood of the query given by the estimated Poisson model.",
                "In some sense, the Poisson model combines the advantage of multinomial in modeling term frequency and the advantage of the multivariate Bernoulli in accommodating per-term smoothing.",
                "Indeed, similar to the multinomial distribution, the Poisson distribution models term frequencies, but without the constraint that all the term probabilities must sum to 1, and similar to multivariate Bernoulli, it models each term independently, thus can easily accommodate per-term smoothing.",
                "As in the existing work on multinomial language models, smoothing is critical for this new family of models.",
                "We derive several smoothing methods for Poisson model in parallel to those used for multinomial distributions, and compare the corresponding retrieval models with those based on multinomial distributions.",
                "We find that while with some smoothing methods, the new model and the multinomial model lead to exactly the same formula, with some other smoothing methods they diverge, and the Poisson model brings in more flexibility for smoothing.",
                "In particular, a key difference is that the Poisson model can naturally accommodate perterm smoothing, which is hard to achieve with a multinomial model without heuristic twist of the semantics of a generative model.",
                "We exploit this potential advantage to develop a new term-dependent smoothing algorithm for Poisson model and show that this new smoothing algorithm can improve performance over term-independent smoothing algorithms using either Poisson or multinomial model.",
                "This advantage is seen for both one-stage and two-stage smoothing.",
                "Another potential advantage of the Poisson model is that its corresponding background model for smoothing can be improved through using a mixture model that has a closed form formula.",
                "This new background model is shown to outperform the standard background model and reduce the sensitivity of retrieval performance to the smoothing parameter.",
                "The rest of the paper is organized as follows.",
                "In Section 2, we introduce the new family of query generation models with Poisson distribution, and present various smoothing methods which lead to different retrieval functions.",
                "In Section 3, we analytically compare the Poisson <br>language model</br> with the multinomial <br>language model</br>, from the perspective of retrieval.",
                "We then design empirical experiments to compare the two families of language models in Section 4.",
                "We discuss the related work in 5 and conclude in 6. 2.",
                "QUERY GENERATION WITH POISSON PROCESS In the query generation framework, a basic assumption is that a query is generated with a model estimated based on a document.",
                "In most existing work [12, 6, 28, 29], people assume that each query word is sampled independently from a multinomial distribution.",
                "Alternatively, we assume that a query is generated by sampling the frequency of words from a series of independent Poisson processes [20]. 2.1 The Generation Process Let V = {w1, ..., wn} be a vocabulary set.",
                "Let w be a piece of text composed by an author and c(w1), ..., c(wn) be a frequency vector representing w, where c(wi, w) is the frequency count of term wi in text w. In retrieval, w could be either a query or a document.",
                "We consider the frequency counts of the n unique terms in w as n different types of events, sampled from n independent homogeneous Poisson processes, respectively.",
                "Suppose t is the time period during which the author composed the text.",
                "With a homogeneous Poisson process, the frequency count of each event, i.e., the number of occurrences of wi, follows a Poisson distribution with associated parameter λit, where λi is a rate parameter characterizing the expected number of wi in a unit time.",
                "The probability density function of such a Poisson Distribution is given by P(c(wi, w) = k|λit) = e−λit (λit)k k!",
                "Without losing generality, we set t to the length of the text w (people write one word in a unit time), i.e., t = |w|.",
                "With n such independent Poisson processes, each explaining the generation of one term in the vocabulary, the likelihood of w to be generated from such Poisson processes can be written as p(w|Λ) = n i=1 p(c(wi, w)|Λ) = n i=1 e−λi·|w| (λi · |w|)c(wi,w) c(wi, w)! where Λ = {λ1, ..., λn} and |w| = n i=1 c(wi, w).",
                "We refer to these n independent Poisson processes with parameter Λ as a Poisson <br>language model</br>.",
                "Let D = {d1, ..., dm} be an observed set of document samples generated from the Poisson process above.",
                "The maximum likelihood estimate (MLE) of λi is ˆλi = d∈D c(wi, d) d∈D w ∈V c(w , d) Note that this MLE is different from the MLE for the Poisson distribution without considering the document lengths, which appears in [22, 24].",
                "Given a document d, we may estimate a Poisson <br>language model</br> Λd using d as a sample.",
                "The likelihood that a query q is generated from the document <br>language model</br> Λd can be written as p(q|d) = w∈V p(c(w, q)|Λd) (1) This representation is clearly different from the multinomial query generation model as (1) the likelihood includes all the terms in the vocabulary V , instead of only those appearing in q, and (2) instead of the appearance of terms, the event space of this model is the frequencies of each term.",
                "In practice, we have the flexibility to choose the vocabulary V .",
                "In one extreme, we can use the vocabulary of the whole collection.",
                "However, this may bring in noise and considerable computational cost.",
                "In the other extreme, we may focus on the terms in the query and ignore other terms, but some useful information may be lost by ignoring the nonquery terms.",
                "As a compromise, we may conflate all the non-query terms as one single pseudo term.",
                "In other words, we may assume that there is exactly one non-query term in the vocabulary for each query.",
                "In our experiments, we adopt this pseudo non-query term strategy.",
                "A document can be scored with the likelihood in Equation 1.",
                "However, if a query term is unseen in the document, the MLE of the Poisson distribution would assign zero probability to the term, causing the probability of the query to be zero.",
                "As in existing language modeling approaches, the main challenge of constructing a reasonable retrieval model is to find a smoothed <br>language model</br> for p(·|d). 2.2 Smoothing in Poisson Retrieval Model In general, we want to assign non-zero rates for the query terms that are not seen in document d. Many smoothing methods have been proposed for multinomial language models[2, 28, 29].",
                "In general, we have to discount the probabilities of some words seen in the text to leave some extra probability mass to assign to the unseen words.",
                "In Poisson language models, however, we do not have the same constraint as in a multinomial model (i.e., w∈V p(w|d) = 1).",
                "Thus we do not have to discount the probability of seen words in order to give a non-zero rate to an unseen word.",
                "Instead, we only need to guarantee that k=0,1,2,... p(c(w, d) = k|d) = 1.",
                "In this section, we introduce three different strategies to smooth a Poisson <br>language model</br>, and show how they lead to different retrieval functions. 2.2.1 Bayesian Smoothing using Gamma Prior Following the risk minimization framework in [11], we assume that a document is generated by the arrival of terms in a time period of |d| according to the document <br>language model</br>, which essentially consists of a vector of Poisson rates for each term, i.e., Λd = λd,1, ..., λd,|V | .",
                "A document is assumed to be generated from a potentially different model.",
                "Given a particular document d, we want to estimate Λd.",
                "The rate of a term is estimated independently of other terms.",
                "We use Bayesian estimation with the following Gamma prior, which has two parameters, α and β: Gamma(λ|α, β) = βα Γ(α) λα−1 e−βλ For each term w, the parameters αw and βw are chosen to be αw = µ ∗ λC,w and βw = µ, where µ is a parameter and λC,w is the rate of w estimated from some background <br>language model</br>, usually the collection <br>language model</br>.",
                "The posterior distribution of Λd is given by p(Λd|d, C) ∝ w∈V e−λw(|d|+µ) λ c(w,d)+µλC,w−1 w which is a product of |V | Gamma distributions with parameters c(w, d) + µλC,w and |d| + µ for each word w. Given that the Gamma mean is α β , we have ˆλd,w = λd,w λd,wp(λd,w|d, C)dλd,w = c(w, d) + µλC,w |d| + µ This is precisely the smoothed estimate of multinomial <br>language model</br> with Dirichlet prior [28]. 2.2.2 Interpolation (Jelinek-Mercer) Smoothing Another straightforward method is to decompose the query generation model as a mixture of two component models.",
                "One is the document <br>language model</br> estimated with maximum likelihood estimator, and the other is a model estimated from the collection background, p(·|C), which assigns non-zero rate to w. For example, we may use an interpolation coefficient between 0 and 1 (i.e., δ ∈ [0, 1]).",
                "With this simple interpolation, we can score a document with Score(d, q) = w∈V log((1 − δ)p(c(w, q)|d) + δp(c(w, q)|C)) (2) Using the maximum likelihood estimator for p(·|d), we have λd,w = c(w,d) |d| , thus Equation 2 becomes Score(d, q) ∝ w∈d∩q [log(1 + 1 − δ δ e−λd,w|q| (λd,w|q|)c(w,q) c(w, q)! · p(c(w, q)|C) ) − log (1 − δ)e−λd,w|q| + δp(c(w, q) = 0|C) 1 − δ + δp(c(w, q) = 0|C) ] + w∈d log (1 − δ)e−λd,w|q| + δp(c(w, q) = 0|C) 1 − δ + δp(c(w, q) = 0|C) We can also use a Poisson <br>language model</br> for p(·|C), or use some other frequency-based models.",
                "In the retrieval formula above, the first summation can be computed efficiently.",
                "The second summation can be actually treated as a document prior, which penalizes long documents.",
                "As the second summation is difficult to compute efficiently, we conflate all non-query terms as one pseudo non-queryterm, denoted as N. Using the pseudo-term formulation and a Poisson collection model, we can rewrite the retrieval formula as Score(d, q) ∝ w∈d∩q log(1 + 1 − δ δ e−λd,w (λd,w|q|)c(w,q) e−λd,C |q| (λd,C )c(w,q) ) + log (1 − δ)e−λd,N |q| + δe−λC,N |q| 1 − δ + δe−λC,N |q| (3) where λd,N = |d|− w∈q c(w,d) |d| and λC,N = |C|− w∈q c(w,C) |C| . 2.2.3 Two-Stage Smoothing As discussed in [29], smoothing plays two roles in retrieval: (1) to improve the estimation of the document <br>language model</br>, and (2) to explain the common terms in the query.",
                "In order to distinguish the content and non-discriminative words in a query, we follow [29] and assume that a query is generated by sampling from a two-component mixture of Poisson language models, with one component being the document model Λd and the other being a query background <br>language model</br> p(·|U). p(·|U) models the typical term frequencies in the users queries.",
                "We may then score each document with the query likelihood computed using the following two-stage smoothing model: p(c(w, q)|Λd, U) = (1 − δ)p(c(w, q)|Λd) + δp(c(w, q)|U) (4) where δ is a parameter, roughly indicating the amount of noise in q.",
                "This looks similar to the interpolation smoothing, except that p(·|Λd) now should be a smoothed <br>language model</br>, instead of the one estimated with MLE.",
                "With no prior knowledge on p(·|U), we could set it to p(·|C).",
                "Any smoothing methods for the document <br>language model</br> can be used to estimate p(·|d) such as the Gamma smoothing as discussed in Section 2.2.1.",
                "The empirical study of the smoothing methods is presented in Section 4. 3.",
                "ANALYSIS OF POISSON <br>language model</br> From the previous section, we notice that the Poisson <br>language model</br> has a strong connection to the multinomial language model.",
                "This is expected since they both belong to the exponential family [26].",
                "However, there are many differences when these two families of models are applied with different smoothing methods.",
                "From the perspective of retrieval, will these two language models perform equivalently?",
                "If not, which model provides more benefits to retrieval, or provides flexibility which could lead to potential benefits?",
                "In this section, we analytically discuss the retrieval features of the Poisson language models, by comparing their behavior with that of the multinomial language models. 3.1 The Equivalence of Basic Models Let us begin with the assumption that all the query terms appear in every document.",
                "Under this assumption, no smoothing is needed.",
                "A document can be scored by the log likelihood of the query with the maximum likelihood estimate: Score(d, q) = w∈V log e−λd,w|q| (λd,w|q|)c(w,q) c(w, q)! (5) Using the MLE, we have λd,w = c(w,d) w∈V c(w,d) .",
                "Thus Score(d, q) ∝ c(w,q)>0 c(w, q) log c(w, d) w∈V c(w, d) This is exactly the log likelihood of the query if the document <br>language model</br> is a multinomial with maximum likelihood estimate.",
                "Indeed, even with Gamma smoothing, when plugging λd,w = c(w,d)+µλC,w |d|+µ and λC,w = c(w,C) |C| into Equation 5, it is easy to show that Score(d, q) ∝ w∈q∩d c(w, q) log(1 + c(w, d) µ · c(w,C) |C| ) + |q| log µ |d| + µ (6) which is exactly the Dirichlet retrieval formula in [28].",
                "Note that this equivalence holds only when the document length variation is modeled with Poisson process.",
                "This derivation indicates the equivalence of the basic Poisson and multinomial language models for retrieval.",
                "With other smoothing strategies, however, the two models would be different.",
                "Nevertheless, with this equivalence in basic models, we could expect that the Poisson <br>language model</br> performs comparably to the multinomial <br>language model</br> in retrieval, if only simple smoothing is explored.",
                "Based on this equivalence analysis, one may ask, why we should pursue the Poisson <br>language model</br>.",
                "In the following sections, we show that despite the equivalence in their basic models, the Poisson <br>language model</br> brings in extra flexibility for exploring advanced techniques on various retrieval features, which could not be achieved with multinomial language models. 3.2 Term Dependent Smoothing One flexibility of the Poisson <br>language model</br> is that it provides a natural framework to accommodate term dependent (per-term) smoothing.",
                "Existing work on <br>language model</br> smoothing has already shown that different types of queries should be smoothed differently according to how discriminative the query terms are. [7] also predicted that different terms should have a different smoothing weights.",
                "With multinomial query generation models, people usually use a single smoothing coefficient to control the combination of the document model and the background model [28, 29].",
                "This parameter can be made specific for different queries, but always has to be a constant for all the terms.",
                "This is mandatory since a multinomial <br>language model</br> has the constraint that w∈V p(w|d) = 1.",
                "However, from retrieval perspective, different terms may need to be smoothed differently even if they are in the same query.",
                "For example, a non-discriminative term (e.g., the, is) is expected to be explained more with the background model, while a content term (e.g., retrieval, bush) in the query should be explained with the document model.",
                "Therefore, a better way of smoothing would be to set the interpolation coefficient (i.e., δ in Formula 2 and Formula 3) specifically for each term.",
                "Since the Poisson <br>language model</br> does not have the sum-to-one constraint across terms, it can easily accommodate per-term smoothing without needing to heuristically twist the semantics of a generative model as in the case of multinomial language models.",
                "Below we present a possible way to explore term dependent smoothing with Poisson language models.",
                "Essentially, we want to use a term-specific smoothing coefficient δ in the linear combination, denoted as δw.",
                "This coefficient should intuitively be larger if w is a common word and smaller if it is a content word.",
                "The key problem is to find a method to assign reasonable values to δw.",
                "Empirical tuning is infeasible for so many parameters.",
                "We may instead estimate the parameters ∆ = {δ1, ..., δ|V |} by maximizing the likelihood of the query given the mixture model of p(q|ΛQ) and p(q|U), where ΛQ is the true query model to generate the query and p(q|U) is a query background model as discussed in Section 2.2.3.",
                "With the model p(q|ΛQ) hidden, the query likelihood is p(q|∆, U) = ΛQ w∈V ((1 − δw)p(c(w, q)|ΛQ) + δwp(c(w, q)|U))P(ΛQ|U)dΛQ If we have relevant documents for each query, we can approximate the query model space with the language models of all the relevant documents.",
                "Without relevant documents, we opt to approximate the query model space with the models of all the documents in the collection.",
                "Setting p(·|U) as p(·|C), the query likelihood becomes p(q|∆, U) = d∈C πd w∈V ((1−δw)p(c(w, q)|ˆΛd)+δwp(c(w, q)|C)) where πd = p(ˆΛd|U). p(·|ˆΛd) is an estimated Poisson <br>language model</br> for document d. If we have prior knowledge on p(ˆΛd|U), such as which documents are relevant to the query, we can set πd accordingly, because what we want is to find ∆ that can maximize the likelihood of the query given relevant documents.",
                "Without this prior knowledge, we can leave πd as free parameters, and use the EM algorithm to estimate πd and ∆.",
                "The updating functions are given as π (k+1) d = πd w∈V ((1 − δw)p(c(w, q)|ˆΛd) + δwp(c(w, q)|C)) d∈C πd w∈V ((1 − δw)p(c(w, q)|ˆΛd) + δwp(c(w, q)|C)) and δ (k+1) w = d∈C πd δwp(c(w, q)|C)) (1 − δw)p(c(w, q)|ˆΛd) + δwp(c(w, q)|C)) As discussed in [29], we only need to run the EM algorithm for several iterations, thus the computational cost is relatively low.",
                "We again assume our vocabulary containing all query terms plus a pseudo non-query term.",
                "Note that the function does not give an explicit way of estimating the coefficient for the unseen non-query term.",
                "In our experiments, we set it to the average over δw of all query terms.",
                "With this flexibility, we expect Poisson language models could improve the retrieval performance, especially for verbose queries, where the query terms have various discriminative values.",
                "In Section 4, we use empirical experiments to prove this hypothesis. 3.3 Mixture Background Models Another flexibility is to explore different background (collection) models (i.e., p(·|U), or p(·|C)).",
                "One common assumption made in language modeling information retrieval is that the background model is a homogeneous model of the document models [28, 29].",
                "Similarly, we can also make the assumption that the collection model is a Poisson <br>language model</br>, with the rates λC,w = d∈C c(w,d) |C| .",
                "However, this assumption usually does not hold, since the collection is far more complex than a single document.",
                "Indeed, the collection usually consists of a mixture of documents with various genres, authors, and topics, etc.",
                "Treating the collection model as a mixture of document models, instead of a single pseudo-document model is more reasonable.",
                "Existing work of multinomial language modeling has already shown that a better modeling of background improves the retrieval performance, such as clusters [15, 10], neighbor documents [25], and aspects [8, 27].",
                "All the approaches can be easily adopted using Poisson language models.",
                "However, a common problem of these approaches is that they all require heavy computation to construct the background model.",
                "With Poisson language modeling, we show that it is possible to model the mixture background without paying for the heavy computational cost.",
                "Poisson Mixture [3] has been proposed to model a collection of documents, which can fit the data much better than a single Poisson.",
                "The basic idea is to assume that the collection is generated from a mixture of Poisson models, which has the general form of p(x = k|PM) = λ p(λ)p(x = k|λ)dλ p(·|λ) is a single Poisson model and p(λ) is an arbitrary probability density function.",
                "There are three well known Poisson mixtures [3]: 2-Poisson, Negative Binomial, and the Katzs K-Mixture [9].",
                "Note that the 2-Poisson model has actually been explored in probabilistic retrieval models, which led to the well-known BM25 formula [22].",
                "All these mixtures have closed forms, and can be estimated from the collection of documents efficiently.",
                "This is an advantage over the multinomial mixture models, such as PLSI [8] and LDA [1], for retrieval.",
                "For example, the probability density function of Katzs K-Mixture is given as p(c(w) = k|αw, βw) = (1 − αw)ηk,0 + αw βw + 1 ( βw βw + 1 )k where ηk,0 = 1 when k = 0, and 0 otherwise.",
                "With the observation of a collection of documents, αw and βw can be estimated as βw = cf(w) − df(w) df(w) and αw = cf(w) Nβw where cf(w) and df(w) are the collection frequency and document frequency of w, and N is the number of documents in the collection.",
                "To account for the different document lengths, we assume that βw is a reasonable estimation for generating a document of the average length, and use β = βw avdl |q| to generate the query.",
                "This Poisson mixture model can be easily used to replace P(·|C) in the retrieval functions 3 and 4. 3.4 Other Possible Flexibilities In addition to term dependent smoothing and efficient mixture background, a Poisson <br>language model</br> has also some other potential advantages.",
                "For example, in Section 2, we see that Formula 2 introduces a component which does document length penalization.",
                "Intuitively, when the document has more unique words, it will be penalized more.",
                "On the other hand, if a document is exactly n copies of another document, it would not get over penalized.",
                "This feature is desirable and not achieved with the Dirichlet model [5].",
                "Potentially, this component could penalize a document according to what types of terms it contains.",
                "With term specific settings of δ, we could get even more flexibility for document length normalization.",
                "Pseudo-feedback is yet another interesting direction where the Poission model might be able to show its advantage.",
                "With model-based feedback, we could again relax the combination coefficients of the feedback model and the background model, and allow different terms to contribute differently to the feedback model.",
                "We could also utilize the relevant documents to learn better per-term smoothing coefficients. 4.",
                "EVALUATION In Section 3, we analytically compared the Poisson language models and multinomial language models from the perspective of query generation and retrieval.",
                "In this section, we compare these two families of models empirically.",
                "Experiment results show that the Poisson model with perterm smoothing outperforms multinomial model, and the performance can be further improved with two-stage smoothing.",
                "Using Poisson mixture as background model also improves the retrieval performance. 4.1 Datasets Since retrieval performance could significantly vary from one test collection to another, and from one query to another, we select four representative TREC test collections: AP, Trec7, Trec8, and Wt2g(Web).",
                "To cover different types of queries, we follow [28, 5], and construct short-keyword (SK, keyword title), short-verbose (SV, one sentence description), and long-verbose (LV, multiple sentences) queries.",
                "The documents are stemmed with the Porters stemmer, and we do not remove any stop word.",
                "For each parameter, we vary its value to cover a reasonably wide range. 4.2 Comparison to Multinomial We compare the performance of the Poisson retrieval models and multinomial retrieval models using interpolation (JelinekMercer, JM) smoothing and Bayesian smoothing with conjugate priors.",
                "Table 1 shows that the two JM-smoothed models perform similarly on all data sets.",
                "Since the Dirichlet Smoothing for multinomial <br>language model</br> and the Gamma Smoothing for Poisson <br>language model</br> lead to the same retrieval formula, the performance of these two models are jointly presented.",
                "We see that Dirichlet/Gamma smoothing methods outperform both Jelinek-Mercer smoothing methods.",
                "The parameter sensitivity curves for two Jelinek-Mercer 0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1 0 0.05 0.1 0.15 0.2 0.25 0.3 Dataset: Trec8 Parameter: δ AveragePrecision JM−Multinomial: LV JM−Multinomial: SV JM−Multinomial: SK JM−Poisson: SK JM−Poisson: SV JM−Poisson: LV Figure 1: Poisson and multinomial performs similarly with Jelinek-Mercer smoothing smoothing methods are shown in Figure 1.",
                "Clearly, these two methods perform similarly either in terms of optimality Data Query JM-Multinomial JM-Poisson Dirichlet/Gamma Per-term 2-Stage Poisson MAP InitPr Pr@5d MAP InitPr Pr@5d MAP InitPr Pr@5d MAP InitPr Pr@5d AP88-89 SK 0.203 0.585 0.356 0.203 0.585 0.358 0.224 0.629 0.393 0.226 0.630 0.396 SV 0.187 0.580 0.361 0.183 0.571 0.345 0.204 0.613 0.387 0.217* 0.603 0.390 LV 0.283 0.716 0.480 0.271 0.692 0.470 0.291 0.710 0.496 0.304* 0.695 0.510 Trec7 SK 0.167 0.635 0.400 0.168 0.635 0.404 0.186 0.687 0.428 0.185 0.646 0.436 SV 0.174 0.655 0.432 0.176 0.653 0.432 0.182 0.666 0.432 0.196* 0.660 0.440 LV 0.223 0.730 0.496 0.215 0.766 0.488 0.224 0.748 0.52 0.236* 0.738 0.512 Trec8 SK 0.239 0.621 0.440 0.239 0.621 0.436 0.257 0.718 0.496 0.256 0.704 0.468 SV 0.231 0.686 0.448 0.234 0.702 0.456 0.228 0.691 0.456 0.246* 0.692 0.476 LV 0.265 0.796 0.548 0.261 0.757 0.520 0.260 0.741 0.492 0.274* 0.766 0.508 Web SK 0.250 0.616 0.380 0.250 0.616 0.380 0.302 0.767 0.468 0.307 0.739 0.468 SV 0.214 0.611 0.392 0.217 0.609 0.384 0.273 0.693 0.508 0.292* 0.703 0.480 LV 0.266 0.790 0.464 0.259 0.776 0.452 0.283 0.756 0.496 0.311* 0.759 0.488 Table 1: Performance comparison between Poisson and Multinomial retrieval models: basic models perform comparably; term dependent two-stage smoothing significantly improves Poisson An asterisk (*) indicates that the difference between the performance of the term dependent two-stage smoothing and that of the Dirichlet/Gamma single smoothing is statistically significant according to the Wilcoxon signed rank test at the level of 0.05. or sensitivity.",
                "This similarity of performance is expected as we discussed in Section 3.1.",
                "Although the Poisson model and multinomial model are similar in terms of the basic model and/or with simple smoothing methods, the Poisson model has great potential and flexibility to be further improved.",
                "As shown in the rightmost column of Table 1, term dependent two-stage Poisson model consistently outperforms the basic smoothing models, especially for verbose queries.",
                "This model is given in Formula 4, with a Gamma smoothing for the document model p(·|d), and δw, which is term dependent.",
                "The parameter µ of the first stage Gamma smoothing is empirically tuned.",
                "The combination coefficients (i.e., ∆), are estimated with the EM algorithm in Section 3.2.",
                "The parameter sensitivity curves for Dirichlet/Gamma and the per-term two-stage smoothing model are plotted in Figure 2.",
                "The per-term two-stage smoothing method is less sensitive to the parameter µ than Dirichlet/Gamma, and yields better optimal performance. 0 1000 2000 3000 4000 5000 6000 7000 8000 9000 10000 0.1 0.12 0.14 0.16 0.18 0.2 0.22 Dataset: AP; Query Type: SV Parameter: µ AveragePrecision Dirichlet/Gamma Smoothing Term Dependent 2−Stage Figure 2: Term dependent two-stage smoothing of Poisson outperforms Dirichlet/Gamma In the following subsections, we conduct experiments to demonstrate how the flexibility of the Poisson model could be utilized to achieve better performance, which we cannot achieve with multinomial language models. 4.3 Term Dependent Smoothing To test the effectiveness of the term dependent smoothing, we conduct the following two experiments.",
                "In the first experiment, we relax the constant coefficient in the simple Jelinek-Mercer smoothing formula (i.e., Formula 3), and use the EM algorithm proposed in Section 3.2 to find a δw for each unique term.",
                "Since we are using the EM algorithm to iteratively estimate the parameters, we usually do not want the probability of p(·|d) to be zero.",
                "We then use a simple Laplace method to slightly smooth the document model before it goes into the EM iterations.",
                "The documents are then still scored with Formula 3, but using learnt δw.",
                "The results are labeled with JM+L. in Table 2.",
                "Data Q JM JM JM+L. 2-Stage 2-Stage (MAP) PT: No Yes Yes No Yes AP SK 0.203 0.204 0.206 0.223 0.226* SV 0.183 0.189 0.214* 0.204 0.217* Trec7 SK 0.168 0.171 0.174 0.186 0.185 SV 0.176 0.147 0.198* 0.194 0.196 Trec8 SK 0.239 0.240 0.227* 0.257 0.256 SV 0.234 0.223 0.249* 0.242 0.246* Web SK 0.250 0.236 0.220* 0.291 0.307* SV 0.217 0.232 0.261* 0.273 0.292* Table 2: Term dependent smoothing improves retrieval performance An asterisk (*) in Column 3 indicates that the difference between the JM+L. method and JM method is statistically significant; an asterisk (*) in Column 5 means that the difference between term dependent two-stage method and query dependent two-stage method is statistically significant; PT stands for per-term.",
                "With term dependent coefficients, the performance of the Jelinek-Mercer Poisson model is improved in most cases.",
                "However, in some cases (e.g., Trec7/SV), it performs poorly.",
                "This might be caused by the problem of EM estimation with unsmoothed document models.",
                "Once non-zero probability is assigned to all the terms before entering the EM iteration, the performance on verbose queries can be improved significantly.",
                "This indicates that there is still room to find better methods to estimate δw.",
                "Please note that neither the perterm JM method nor the JM+L. method has a parameter to tune.",
                "As shown in Table 1, the term dependent two-stage smoothing can significantly improve retrieval performance.",
                "To understand whether the improvement is contributed by the term dependent smoothing or the two-stage smoothing framework, we design another experiment to compare the perterm two-stage smoothing with the two-stage smoothing method proposed in [29].",
                "Their method managed to find coefficients specific to the query, thus a verbose query would use a higher δ.",
                "However, since their model is based on multinomial language modeling, they could not get per-term coefficients.",
                "We adopt their method to the Poisson two-stage smoothing, and also estimate a per-query coefficient for all the terms.",
                "We compare the performance of such a model with the per-term two-stage smoothing model, and present the results in the right two columns in Table 2.",
                "Again, we see that the per-term two-stage smoothing outperforms the per-query two-stage smoothing, especially for verbose queries.",
                "The improvement is not as large as how the perterm smoothing method improves over Dirichlet/Gamma.",
                "This is expected, since the per-query smoothing has already addressed the query discrimination problem to some extent.",
                "This experiment shows that even if the smoothing is already per-query, making it per-term is still beneficial.",
                "In brief, the per-term smoothing improved the retrieval performance of both one-stage and two-stage smoothing method. 4.4 Mixture Background Model In this section, we conduct experiments to examine the benefits of using a mixture background model without extra computational cost, which can not be achieved for multinomial models.",
                "Specifically, in retrieval formula 3, instead of using a single Poisson distribution to model the background p(·|C), we use Katzs K-Mixture model, which is essentially a mixture of Poisson distributions. p(·|C) can be computed efficiently with simple collection statistics, as discussed in Section 3.3.",
                "Data Query JM.",
                "Poisson JM.",
                "K-Mixture AP SK 0.203 0.204 SV 0.183 0.188* Trec-7 SK 0.168 0.169 SV 0.176 0.178* Trec-8 SK 0.239 0.239 SV 0.234 0.238* Web SK 0.250 0.250 SV 0.217 0.223* Table 3: K-Mixture background model improves retrieval performance The performance of the JM retrieval model with single Poisson background and with Katzs K-Mixture background model is compared in Table 3.",
                "Clearly, using K-Mixture to model the background model outperforms the single Poisson background model in most cases, especially for verbose queries where the improvement is statistically significant.",
                "Figure 3 shows that the performance changes over different parameters for short verbose queries.",
                "The model using K-Mixture background is less sensitive than the one using single Poisson background.",
                "Given that this type of mixture 0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1 0 0.05 0.1 0.15 0.2 0.25 Data: Trec8; Query: SV Parameter: δ AveragePrecision Poisson Background K−Mixture Background Figure 3: K-Mixture background model deviates the sensitivity of verbose queries background model does not require any extra computation cost, it would be interesting to study whether using other mixture Poisson models, such as 2-Poisson and negative Binomial, could help the performance. 5.",
                "RELATED WORK To the best of our knowledge, there has been no study of query generation models based on Poisson distribution.",
                "Language models have been shown to be effective for many retrieval tasks [21, 28, 14, 4].",
                "The most popular and fundamental one is the query-generation <br>language model</br> [21, 13].",
                "All existing query generation language models are based on either multinomial distribution [19, 6, 28, 13] or multivariate Bernoulli distribution [21, 17, 18].",
                "We introduce a new family of language models, based on Poisson distribution.",
                "Poisson distribution has been previously studied in the document generation models [16, 22, 3, 24], leading to the development of one of the most effective retrieval formula BM25 [23]. [24] studies the parallel derivation of three different retrieval models which is related to our comparison of Poisson and multinomial.",
                "However, the Poisson model in their paper is still under the document generation framework, and also does not account for the document length variation. [26] introduces a way to empirically search for an exponential model for the documents.",
                "Poisson mixtures [3] such as 2-Poisson [22], Negative multinomial, and Katzs KMixture [9] has shown to be effective to model and retrieve documents.",
                "Once again, none of this work explores Poisson distribution in the query generation framework.",
                "<br>language model</br> smoothing [2, 28, 29] and background structures [15, 10, 25, 27] have been studied with multinomial language models. [7] analytically shows that term specific smoothing could be useful.",
                "We show that Poisson <br>language model</br> is natural to accommodate the per-term smoothing without heuristic twist of the semantics of a generative model, and is able to efficiently better model the mixture background, both analytically and empirically. 6.",
                "CONCLUSIONS We present a new family of query generation language models for retrieval based on Poisson distribution.",
                "We derive several smoothing methods for this family of models, including single-stage smoothing and two-stage smoothing.",
                "We compare the new models with the popular multinomial retrieval models both analytically and experimentally.",
                "Our analysis shows that while our new models and multinomial models are equivalent under some assumptions, they are generally different with some important differences.",
                "In particular, we show that Poisson has an advantage over multinomial in naturally accommodating per-term smoothing.",
                "We exploit this property to develop a new per-term smoothing algorithm for Poisson language models, which is shown to outperform term-independent smoothing for both Poisson and multinomial models.",
                "Furthermore, we show that a mixture background model for Poisson can be used to improve the performance and robustness over the standard Poisson background model.",
                "Our work opens up many interesting directions for further exploration in this new family of models.",
                "Further exploring the flexibilities over multinomial language models, such as length normalization and pseudo-feedback could be good future work.",
                "It is also appealing to find robust methods to learn the per-term smoothing coefficients without additional computation cost. 7.",
                "ACKNOWLEDGMENTS We thank the anonymous SIGIR 07 reviewers for their useful comments.",
                "This material is based in part upon work supported by the National Science Foundation under award numbers IIS-0347933 and 0425852. 8.",
                "REFERENCES [1] D. Blei, A. Ng, and M. Jordan.",
                "Latent dirichlet allocation.",
                "Journal of Machine Learning Research, 3:993-1022, 2003. [2] S. F. Chen and J. Goodman.",
                "An empirical study of smoothing techniques for language modeling.",
                "Technical Report TR-10-98, Harvard University, 1998. [3] K. Church and W. Gale.",
                "Poisson mixtures.",
                "Nat.",
                "Lang.",
                "Eng., 1(2):163-190, 1995. [4] W. B. Croft and J. Lafferty, editors.",
                "Language Modeling and Information Retrieval.",
                "Kluwer Academic Publishers, 2003. [5] H. Fang, T. Tao, and C. Zhai.",
                "A formal study of information retrieval heuristics.",
                "In Proceedings of the 27th annual international ACM SIGIR conference on Research and development in information retrieval, pages 49-56, 2004. [6] D. Hiemstra.",
                "Using Language Models for Information Retrieval.",
                "PhD thesis, University of Twente, Enschede, Netherlands, 2001. [7] D. Hiemstra.",
                "Term-specific smoothing for the language modeling approach to information retrieval: the importance of a query term.",
                "In Proceedings of the 25th annual international ACM SIGIR conference on Research and development in information retrieval, pages 35-41, 2002. [8] T. Hofmann.",
                "Probabilistic latent semantic indexing.",
                "In Proceedings of ACM SIGIR99, pages 50-57, 1999. [9] S. M. Katz.",
                "Distribution of content words and phrases in text and language modelling.",
                "Nat.",
                "Lang.",
                "Eng., 2(1):15-59, 1996. [10] O. Kurland and L. Lee.",
                "Corpus structure, language models, and ad-hoc information retrieval.",
                "In Proceedings of the 27th annual international ACM SIGIR conference on Research and development in information retrieval, pages 194-201, 2004. [11] J. Lafferty and C. Zhai.",
                "Document language models, query models, and risk minimization for information retrieval.",
                "In Proceedings of SIGIR01, pages 111-119, Sept 2001. [12] J. Lafferty and C. Zhai.",
                "Probabilistic IR models based on query and document generation.",
                "In Proceedings of the Language Modeling and IR workshop, pages 1-5, May 31 - June 1 2001. [13] J. Lafferty and C. Zhai.",
                "Probabilistic relevance models based on document and query generation.",
                "In W. B. Croft and J. Lafferty, editors, Language Modeling and Information Retrieval.",
                "Kluwer Academic Publishers, 2003. [14] V. Lavrenko and B. Croft.",
                "Relevance-based language models.",
                "In Proceedings of SIGIR01, pages 120-127, Sept 2001. [15] X. Liu and W. B. Croft.",
                "Cluster-based retrieval using language models.",
                "In Proceedings of the 27th annual international ACM SIGIR conference on Research and development in information retrieval, pages 186-193, 2004. [16] E. L. Margulis.",
                "Modelling documents with multiple poisson distributions.",
                "Inf.",
                "Process.",
                "Manage., 29(2):215-227, 1993. [17] A. McCallum and K. Nigam.",
                "A comparison of event models for naive bayes text classification.",
                "In Proceedings of AAAI-98 Workshop on Learning for Text Categorization, 1998. [18] D. Metzler, V. Lavrenko, and W. B. Croft.",
                "Formal multiple-bernoulli models for language modeling.",
                "In Proceedings of the 27th annual international ACM SIGIR conference on Research and development in information retrieval, pages 540-541, 2004. [19] D. H. Miller, T. Leek, and R. Schwartz.",
                "A hidden Markov model information retrieval system.",
                "In Proceedings of the 1999 ACM SIGIR Conference on Research and Development in Information Retrieval, pages 214-221, 1999. [20] A. Papoulis.",
                "Probability, random variables and stochastic processes.",
                "New York: McGraw-Hill, 1984, 2nd ed., 1984. [21] J. M. Ponte and W. B. Croft.",
                "A language modeling approach to information retrieval.",
                "In Proceedings of the 21st annual international ACM SIGIR conference on Research and development in information retrieval, pages 275-281, 1998. [22] S. Robertson and S. Walker.",
                "Some simple effective approximations to the 2-poisson model for probabilistic weighted retrieval.",
                "In Proceedings of SIGIR94, pages 232-241, 1994. [23] S. E. Robertson, S. Walker, S. Jones, M. M.Hancock-Beaulieu, and M. Gatford.",
                "Okapi at TREC-3.",
                "In D. K. Harman, editor, The Third Text REtrieval Conference (TREC-3), pages 109-126, 1995. [24] T. Roelleke and J. Wang.",
                "A parallel derivation of probabilistic information retrieval models.",
                "In Proceedings of the 29th annual international ACM SIGIR conference on Research and development in information retrieval, pages 107-114, 2006. [25] T. Tao, X. Wang, Q. Mei, and C. Zhai.",
                "<br>language model</br> information retrieval with document expansion.",
                "In Proceedings of HLT/NAACL 2006, pages 407-414, 2006. [26] J. Teevan and D. R. Karger.",
                "Empirical development of an exponential probabilistic model for text retrieval: using textual analysis to build a better model.",
                "In Proceedings of the 26th annual international ACM SIGIR conference on Research and development in informaion retrieval, pages 18-25, 2003. [27] X. Wei and W. B. Croft.",
                "Lda-based document models for ad-hoc retrieval.",
                "In Proceedings of the 29th annual international ACM SIGIR conference on Research and development in information retrieval, pages 178-185, 2006. [28] C. Zhai and J. Lafferty.",
                "A study of smoothing methods for language models applied to ad-hoc information retrieval.",
                "In Proceedings of ACM SIGIR01, pages 334-342, Sept 2001. [29] C. Zhai and J. Lafferty.",
                "Two-stage language models for information retrieval.",
                "In Proceedings of ACM SIGIR02, pages 49-56, Aug 2002."
            ],
            "original_annotated_samples": [],
            "translated_annotated_samples": [
                "Entre muchas variantes de modelos de idiomas propuestos, el más popular y fundamental es el \"modelo de idioma\" de la generación de consultas [21, 13], lo que conduce al método de puntuación de la probabilidad de consulta para los documentos de clasificación.modelo",
                "En la Sección 3, comparamos analíticamente el \"modelo de idioma\" de Poisson con el \"modelo de idioma\" multinomial, desde la perspectiva de la recuperación.modelo",
                "Nos referimos a estos n procesos de Poisson independientes con el parámetro λ como un \"modelo de lenguaje\" de Poisson.modelo",
                "Dado un documento D, podemos estimar un \"modelo de lenguaje\" de Poisson λd usando D como muestra.modelo",
                "La probabilidad de que una consulta Q se genere a partir del documento \"Modelo de lenguaje\" λd se puede escribir como P (Q | D) = W∈V P (C (W, Q) | λd) (1) Esta representación es claramente diferente deEl modelo de generación de consultas multinomiales como (1) la probabilidad incluye todos los términos en el vocabulario V, en lugar de solo aquellos que aparecen en Q, y (2) en lugar de la aparición de términos, el espacio de eventos de este modelo son las frecuencias de cada unotérmino.modelo",
                "Al igual que en los enfoques de modelado de idiomas existentes, el principal desafío de construir un modelo de recuperación razonable es encontrar un \"modelo de idioma\" suavizado para P (· | D).2.2 suavizado en el modelo de recuperación de Poisson En general, queremos asignar tarifas no cero para los términos de consulta que no se ven en el documento d.Se han propuesto muchos métodos de suavizado para modelos de lenguaje multinomial [2, 28, 29].modelo",
                "En esta sección, presentamos tres estrategias diferentes para suavizar un \"modelo de idioma\" de Poisson y mostramos cómo conducen a diferentes funciones de recuperación.2.2.1 suavizado bayesiano utilizando gamma antes del marco de minimización de riesgo en [11], suponemos que un documento se genera mediante la llegada de los términos en un período de tiempo de | D |De acuerdo con el documento \"Modelo de lenguaje\", que esencialmente consiste en un vector de tasas de Poisson para cada término, es decir, λd = λd, 1, ..., λd, | V |.modelo",
                "Utilizamos la estimación bayesiana con el siguiente gamma anterior, que tiene dos parámetros, α y β: gamma (λ | α, β) = βα γ (α) λα - 1 e - βλ para cada término W, los parámetros αW y βW sonElegido para ser αW = µ ∗ λc, W y βW = µ, donde µ es un parámetro y λc, W es la velocidad de W estimada a partir de algún fondo \"modelo de lenguaje\", generalmente el \"modelo de idioma\" de la colección.modelo",
                "La distribución posterior de λd viene dada por p (λd | d | d | d | d | d | d | d | d | d | d | d | d | d | d | d | d | d | d | d | d | d | d | d | d | d | d | d | d | d | d | d | d | d | d | d | d | d | d | d | d | d | d | d | d | d | d | d | d | d | d | d | d |V |Distribuciones gamma con parámetros c (w, d) + µλc, w y | d |+ µ para cada palabra w.Dado que la media gamma es α β, tenemos ˆλd, w = λd, w λd, wp (λd, w | d, c) dλd, w = c (w, d) + µλc, w | d |+ µ Esta es precisamente la estimación suavizada del \"modelo de lenguaje\" multinomial con Dirichlet Prior [28].2.2.2 Interpolación (Jelinek-Mercer) suavizando otro método directo es descomponer el modelo de generación de consultas como una mezcla de dos modelos de componentes.modelo",
                "Uno es el \"modelo de idioma\" del documento estimado con el estimador de máxima probabilidad, y el otro es un modelo estimado a partir de los antecedentes de la colección, p (· | c), que asigna una tasa distinta de cero a w.Por ejemplo, podemos usar un coeficiente de interpolación entre 0 y 1 (es decir, Δ ∈ [0, 1]).modelo",
                "Con esta interpolación simple, podemos obtener un documento con puntaje (d, q) = w∈V log ((1 - Δ) P (C (W, Q) | D) + ΔP (C (W, Q) | C) (2) Usando el estimador de máxima verosimilitud para P (· | d), tenemos λd, w = c (w, d) | d |, por lo tanto, la ecuación 2 se convierte en la puntuación (d, q) ∝ w∈D∩q [log (1 + 1 - Δ δ e - λd, w | q | (λd, w | q |) c (w, q) c (w, q)! · P (c (w, q) | c)) - log (1 - δ) e - λd, w | q |+ ΔP (C (W, Q) = 0 | C) 1 - Δ + ΔP (C (W, Q) = 0 | C)] + W∈D log (1 - δ) E - λd, W | Q |+ ΔP (c (w, q) = 0 | c) 1 - Δ + ΔP (c (w, q) = 0 | c) También podemos usar un \"modelo de lenguaje\" de Poisson para p (· | c), o oUse otros modelos basados en frecuencia.modelo",
                "Como la segunda suma es difícil de calcular de manera eficiente, combinamos todos los términos no quirados como un pseudo no quirado, denotado como N. utilizando la formulación de pseudo a término y un modelo de colección de Poisson, podemos reescribir la fórmula de recuperación como puntaje (D, q) ∝ w∈D∩q log (1 + 1 - Δ δ e - λd, w (λd, w | q |) c (w, q) e - λd, c | q | (λd, c) c) c)(w, q)) + log (1 - δ) e - λd, n | q |+ ΔE - λc, n | Q |1 - Δ + ΔE - λc, n | Q |(3) donde λd, n = | d | - w∈Q c (w, d) | d |y λc, n = | c | - w∈Q c (w, c) | c |.2.2.3 suavizado de dos etapas Como se discutió en [29], el suavizado juega dos roles en la recuperación: (1) para mejorar la estimación del \"modelo de idioma\" del documento y (2) para explicar los términos comunes en la consulta.modelo",
                "Para distinguir el contenido y las palabras no discriminativas en una consulta, seguimos [29] y asumimos que se genera una consulta mediante el muestreo de una mezcla de dos componentes de modelos de lenguaje Poisson, siendo un componente el modelo de documento λd y elOtro es un fondo de consulta \"Modelo de lenguaje\" P (· | U).P (· | U) Modela las frecuencias de término típicas en las consultas de los usuarios.modelo",
                "Esto se parece al suavizado de interpolación, excepto que P (· | λd) ahora debería ser un \"modelo de lenguaje\" suavizado, en lugar del estimado con MLE.modelo",
                "Cualquier método de suavizado para el \"modelo de lenguaje\" del documento puede usarse para estimar P (· | D) como el suavizado gamma como se discute en la Sección 2.2.1.modelo",
                "Análisis del \"modelo de idioma\" de Poisson de la sección anterior, notamos que el \"modelo de lenguaje\" de Poisson tiene una fuerte conexión con el modelo de lenguaje multinomial.modelo",
                "Así, puntaje (d, q) ∝ c (w, q)> 0 c (w, q) log c (w, d) w∈V c (w, d) Esta es exactamente la probabilidad log de la consulta si el documento\"Modelo de idiomas\" es un multinomial con una estimación de máxima verosimilitud.modelo",
                "Sin embargo, con esta equivalencia en los modelos básicos, podríamos esperar que el \"modelo de lenguaje\" de Poisson se desempeñe comparablemente con el \"modelo de lenguaje\" multinomial en la recuperación, si solo se explora un suave suave simple.modelo",
                "Basado en este análisis de equivalencia, uno puede preguntar por qué debemos perseguir el \"modelo de idioma\" de Poisson.modelo",
                "En las siguientes secciones, mostramos que a pesar de la equivalencia en sus modelos básicos, el \"modelo de lenguaje\" de Poisson aporta una flexibilidad adicional para explorar técnicas avanzadas en diversas características de recuperación, que no se pueden lograr con modelos de idiomas multinomiales.3.2 Afisa dependiente de términos Una flexibilidad del \"modelo de lenguaje\" de Poisson es que proporciona un marco natural para acomodar el suavizado dependiente de los términos (por período).modelo",
                "El trabajo existente en el suavizado del \"modelo de lenguaje\" ya ha demostrado que diferentes tipos de consultas deben suavizarse de manera diferente de acuerdo con cuán discriminatorios son los términos de la consulta.[7] también predijo que diferentes términos deberían tener un peso de suavizado diferente.modelo",
                "Esto es obligatorio ya que un \"modelo de lenguaje\" multinomial tiene la restricción de que w∈V p (w | d) = 1. modelo de lenguaje",
                "Dado que el \"modelo de idioma\" de Poisson no tiene la restricción de suma a uno en los términos, puede acomodar fácilmente el suavizado por término sin necesidad de torcer heurísticamente la semántica de un modelo generativo como en el caso de los modelos de lenguaje multinomial.modelo",
                "Configuración P (· | U) Como P (· | C), la probabilidad de consulta se convierte en P (Q | ∆, U) = d∈C πd w∈V ((1 - porˆΛd)+ΔWp (c (w, q) | c)) donde πd = p (ˆλd | u).P (· | ˆλd) es un \"modelo de idioma\" estimado de Poisson para el documento d.Si tenemos conocimiento previo sobre p (ˆλd | u), como qué documentos son relevantes para la consulta, podemos establecer πd en consecuencia, porque lo que queremos es encontrar ∆ que pueda maximizar la probabilidad de la consulta dada los documentos relevantes.modelo",
                "Del mismo modo, también podemos suponer que el modelo de recolección es un \"modelo de lenguaje\" de Poisson, con las tasas λc, w = d∈C c (w, d) | c |.modelo",
                "Este modelo de mezcla de Poisson se puede usar fácilmente para reemplazar P (· | c) en las funciones de recuperación 3 y 4. 3.4 Otras flexibilidades posibles además del suavizado dependiente del término y el fondo de mezcla eficiente, un \"modelo de lenguaje\" de Poisson también tiene otros potenciales.ventajas.modelo",
                "Dado que el suavizado de Dirichlet para el \"modelo de lenguaje\" multinomial y el suavizado gamma para el \"modelo de lenguaje\" de Poisson conducen a la misma fórmula de recuperación, el rendimiento de estos dos modelos se presenta conjuntamente.modelo",
                "El más popular y fundamental es el \"modelo de idioma\" de la generación de consultas [21, 13].modelo",
                "El suavizado de \"modelo de lenguaje\" [2, 28, 29] y las estructuras de fondo [15, 10, 25, 27] se han estudiado con modelos de lenguaje multinomial.[7] muestra analíticamente que el suavizado de término específico podría ser útil.modelo",
                "Mostramos que el \"modelo de lenguaje\" de Poisson es natural para acomodar el suavizado por término sin giro heurístico de la semántica de un modelo generativo, y es capaz de modelar mejor el fondo de la mezcla, tanto analíticamente como empíricamente.6. Modelo de idiomas",
                "Recuperación de información \"Modelo de idioma\" con expansión de documentos.modelo"
            ],
            "translated_text": "",
            "candidates": [],
            "error": []
        },
        "poisson process": {
            "translated_key": "",
            "is_in_text": true,
            "original_annotated_sentences": [
                "A Study of Poisson Query Generation Model for Information Retrieval Qiaozhu Mei, Hui Fang, Chengxiang Zhai Department of Computer Science University of Illinois at Urbana-Champaign Urbana,IL 61801 {qmei2,hfang,czhai}@uiuc.edu ABSTRACT Many variants of language models have been proposed for information retrieval.",
                "Most existing models are based on multinomial distribution and would score documents based on query likelihood computed based on a query generation probabilistic model.",
                "In this paper, we propose and study a new family of query generation models based on Poisson distribution.",
                "We show that while in their simplest forms, the new family of models and the existing multinomial models are equivalent, they behave differently for many smoothing methods.",
                "We show that the Poisson model has several advantages over the multinomial model, including naturally accommodating per-term smoothing and allowing for more accurate background modeling.",
                "We present several variants of the new model corresponding to different smoothing methods, and evaluate them on four representative TREC test collections.",
                "The results show that while their basic models perform comparably, the Poisson model can outperform multinomial model with per-term smoothing.",
                "The performance can be further improved with two-stage smoothing.",
                "Categories and Subject Descriptors: H.3.3 [Information Search and Retrieval]: Retrieval Models General Terms: Algorithms 1.",
                "INTRODUCTION As a new type of probabilistic retrieval models, language models have been shown to be effective for many retrieval tasks [21, 28, 14, 4].",
                "Among many variants of language models proposed, the most popular and fundamental one is the query-generation language model [21, 13], which leads to the query-likelihood scoring method for ranking documents.",
                "In such a model, given a query q and a document d, we compute the likelihood of generating query q with a model estimated based on document d, i.e., the conditional probability p(q|d).",
                "We can then rank documents based on the likelihood of generating the query.",
                "Virtually all the existing query generation language models are based on either multinomial distribution [19, 6, 28] or multivariate Bernoulli distribution [21, 18].",
                "The multinomial distribution is especially popular and also shown to be quite effective.",
                "The heavy use of multinomial distribution is partly due to the fact that it has been successfully used in speech recognition, where multinomial distribution is a natural choice for modeling the occurrence of a particular word in a particular position in text.",
                "Compared with multivariate Bernoulli, multinomial distribution has the advantage of being able to model the frequency of terms in the query; in contrast, multivariate Bernoulli only models the presence and absence of query terms, thus cannot capture different frequencies of query terms.",
                "However, multivariate Bernoulli also has one potential advantage over multinomial from the viewpoint of retrieval: in a multinomial distribution, the probabilities of all the terms must sum to 1, making it hard to accommodate per-term smoothing, while in a multivariate Bernoulli, the presence probabilities of different terms are completely independent of each other, easily accommodating per-term smoothing and weighting.",
                "Note that term absence is also indirectly captured in a multinomial model through the constraint that all the term probabilities must sum to 1.",
                "In this paper, we propose and study a new family of query generation models based on the Poisson distribution.",
                "In this new family of models, we model the frequency of each term independently with a Poisson distribution.",
                "To score a document, we would first estimate a multivariate Poisson model based on the document, and then score it based on the likelihood of the query given by the estimated Poisson model.",
                "In some sense, the Poisson model combines the advantage of multinomial in modeling term frequency and the advantage of the multivariate Bernoulli in accommodating per-term smoothing.",
                "Indeed, similar to the multinomial distribution, the Poisson distribution models term frequencies, but without the constraint that all the term probabilities must sum to 1, and similar to multivariate Bernoulli, it models each term independently, thus can easily accommodate per-term smoothing.",
                "As in the existing work on multinomial language models, smoothing is critical for this new family of models.",
                "We derive several smoothing methods for Poisson model in parallel to those used for multinomial distributions, and compare the corresponding retrieval models with those based on multinomial distributions.",
                "We find that while with some smoothing methods, the new model and the multinomial model lead to exactly the same formula, with some other smoothing methods they diverge, and the Poisson model brings in more flexibility for smoothing.",
                "In particular, a key difference is that the Poisson model can naturally accommodate perterm smoothing, which is hard to achieve with a multinomial model without heuristic twist of the semantics of a generative model.",
                "We exploit this potential advantage to develop a new term-dependent smoothing algorithm for Poisson model and show that this new smoothing algorithm can improve performance over term-independent smoothing algorithms using either Poisson or multinomial model.",
                "This advantage is seen for both one-stage and two-stage smoothing.",
                "Another potential advantage of the Poisson model is that its corresponding background model for smoothing can be improved through using a mixture model that has a closed form formula.",
                "This new background model is shown to outperform the standard background model and reduce the sensitivity of retrieval performance to the smoothing parameter.",
                "The rest of the paper is organized as follows.",
                "In Section 2, we introduce the new family of query generation models with Poisson distribution, and present various smoothing methods which lead to different retrieval functions.",
                "In Section 3, we analytically compare the Poisson language model with the multinomial language model, from the perspective of retrieval.",
                "We then design empirical experiments to compare the two families of language models in Section 4.",
                "We discuss the related work in 5 and conclude in 6. 2.",
                "QUERY GENERATION WITH <br>poisson process</br> In the query generation framework, a basic assumption is that a query is generated with a model estimated based on a document.",
                "In most existing work [12, 6, 28, 29], people assume that each query word is sampled independently from a multinomial distribution.",
                "Alternatively, we assume that a query is generated by sampling the frequency of words from a series of independent Poisson processes [20]. 2.1 The Generation Process Let V = {w1, ..., wn} be a vocabulary set.",
                "Let w be a piece of text composed by an author and c(w1), ..., c(wn) be a frequency vector representing w, where c(wi, w) is the frequency count of term wi in text w. In retrieval, w could be either a query or a document.",
                "We consider the frequency counts of the n unique terms in w as n different types of events, sampled from n independent homogeneous Poisson processes, respectively.",
                "Suppose t is the time period during which the author composed the text.",
                "With a homogeneous <br>poisson process</br>, the frequency count of each event, i.e., the number of occurrences of wi, follows a Poisson distribution with associated parameter λit, where λi is a rate parameter characterizing the expected number of wi in a unit time.",
                "The probability density function of such a Poisson Distribution is given by P(c(wi, w) = k|λit) = e−λit (λit)k k!",
                "Without losing generality, we set t to the length of the text w (people write one word in a unit time), i.e., t = |w|.",
                "With n such independent Poisson processes, each explaining the generation of one term in the vocabulary, the likelihood of w to be generated from such Poisson processes can be written as p(w|Λ) = n i=1 p(c(wi, w)|Λ) = n i=1 e−λi·|w| (λi · |w|)c(wi,w) c(wi, w)! where Λ = {λ1, ..., λn} and |w| = n i=1 c(wi, w).",
                "We refer to these n independent Poisson processes with parameter Λ as a Poisson Language Model.",
                "Let D = {d1, ..., dm} be an observed set of document samples generated from the <br>poisson process</br> above.",
                "The maximum likelihood estimate (MLE) of λi is ˆλi = d∈D c(wi, d) d∈D w ∈V c(w , d) Note that this MLE is different from the MLE for the Poisson distribution without considering the document lengths, which appears in [22, 24].",
                "Given a document d, we may estimate a Poisson language model Λd using d as a sample.",
                "The likelihood that a query q is generated from the document language model Λd can be written as p(q|d) = w∈V p(c(w, q)|Λd) (1) This representation is clearly different from the multinomial query generation model as (1) the likelihood includes all the terms in the vocabulary V , instead of only those appearing in q, and (2) instead of the appearance of terms, the event space of this model is the frequencies of each term.",
                "In practice, we have the flexibility to choose the vocabulary V .",
                "In one extreme, we can use the vocabulary of the whole collection.",
                "However, this may bring in noise and considerable computational cost.",
                "In the other extreme, we may focus on the terms in the query and ignore other terms, but some useful information may be lost by ignoring the nonquery terms.",
                "As a compromise, we may conflate all the non-query terms as one single pseudo term.",
                "In other words, we may assume that there is exactly one non-query term in the vocabulary for each query.",
                "In our experiments, we adopt this pseudo non-query term strategy.",
                "A document can be scored with the likelihood in Equation 1.",
                "However, if a query term is unseen in the document, the MLE of the Poisson distribution would assign zero probability to the term, causing the probability of the query to be zero.",
                "As in existing language modeling approaches, the main challenge of constructing a reasonable retrieval model is to find a smoothed language model for p(·|d). 2.2 Smoothing in Poisson Retrieval Model In general, we want to assign non-zero rates for the query terms that are not seen in document d. Many smoothing methods have been proposed for multinomial language models[2, 28, 29].",
                "In general, we have to discount the probabilities of some words seen in the text to leave some extra probability mass to assign to the unseen words.",
                "In Poisson language models, however, we do not have the same constraint as in a multinomial model (i.e., w∈V p(w|d) = 1).",
                "Thus we do not have to discount the probability of seen words in order to give a non-zero rate to an unseen word.",
                "Instead, we only need to guarantee that k=0,1,2,... p(c(w, d) = k|d) = 1.",
                "In this section, we introduce three different strategies to smooth a Poisson language model, and show how they lead to different retrieval functions. 2.2.1 Bayesian Smoothing using Gamma Prior Following the risk minimization framework in [11], we assume that a document is generated by the arrival of terms in a time period of |d| according to the document language model, which essentially consists of a vector of Poisson rates for each term, i.e., Λd = λd,1, ..., λd,|V | .",
                "A document is assumed to be generated from a potentially different model.",
                "Given a particular document d, we want to estimate Λd.",
                "The rate of a term is estimated independently of other terms.",
                "We use Bayesian estimation with the following Gamma prior, which has two parameters, α and β: Gamma(λ|α, β) = βα Γ(α) λα−1 e−βλ For each term w, the parameters αw and βw are chosen to be αw = µ ∗ λC,w and βw = µ, where µ is a parameter and λC,w is the rate of w estimated from some background language model, usually the collection language model.",
                "The posterior distribution of Λd is given by p(Λd|d, C) ∝ w∈V e−λw(|d|+µ) λ c(w,d)+µλC,w−1 w which is a product of |V | Gamma distributions with parameters c(w, d) + µλC,w and |d| + µ for each word w. Given that the Gamma mean is α β , we have ˆλd,w = λd,w λd,wp(λd,w|d, C)dλd,w = c(w, d) + µλC,w |d| + µ This is precisely the smoothed estimate of multinomial language model with Dirichlet prior [28]. 2.2.2 Interpolation (Jelinek-Mercer) Smoothing Another straightforward method is to decompose the query generation model as a mixture of two component models.",
                "One is the document language model estimated with maximum likelihood estimator, and the other is a model estimated from the collection background, p(·|C), which assigns non-zero rate to w. For example, we may use an interpolation coefficient between 0 and 1 (i.e., δ ∈ [0, 1]).",
                "With this simple interpolation, we can score a document with Score(d, q) = w∈V log((1 − δ)p(c(w, q)|d) + δp(c(w, q)|C)) (2) Using the maximum likelihood estimator for p(·|d), we have λd,w = c(w,d) |d| , thus Equation 2 becomes Score(d, q) ∝ w∈d∩q [log(1 + 1 − δ δ e−λd,w|q| (λd,w|q|)c(w,q) c(w, q)! · p(c(w, q)|C) ) − log (1 − δ)e−λd,w|q| + δp(c(w, q) = 0|C) 1 − δ + δp(c(w, q) = 0|C) ] + w∈d log (1 − δ)e−λd,w|q| + δp(c(w, q) = 0|C) 1 − δ + δp(c(w, q) = 0|C) We can also use a Poisson language model for p(·|C), or use some other frequency-based models.",
                "In the retrieval formula above, the first summation can be computed efficiently.",
                "The second summation can be actually treated as a document prior, which penalizes long documents.",
                "As the second summation is difficult to compute efficiently, we conflate all non-query terms as one pseudo non-queryterm, denoted as N. Using the pseudo-term formulation and a Poisson collection model, we can rewrite the retrieval formula as Score(d, q) ∝ w∈d∩q log(1 + 1 − δ δ e−λd,w (λd,w|q|)c(w,q) e−λd,C |q| (λd,C )c(w,q) ) + log (1 − δ)e−λd,N |q| + δe−λC,N |q| 1 − δ + δe−λC,N |q| (3) where λd,N = |d|− w∈q c(w,d) |d| and λC,N = |C|− w∈q c(w,C) |C| . 2.2.3 Two-Stage Smoothing As discussed in [29], smoothing plays two roles in retrieval: (1) to improve the estimation of the document language model, and (2) to explain the common terms in the query.",
                "In order to distinguish the content and non-discriminative words in a query, we follow [29] and assume that a query is generated by sampling from a two-component mixture of Poisson language models, with one component being the document model Λd and the other being a query background language model p(·|U). p(·|U) models the typical term frequencies in the users queries.",
                "We may then score each document with the query likelihood computed using the following two-stage smoothing model: p(c(w, q)|Λd, U) = (1 − δ)p(c(w, q)|Λd) + δp(c(w, q)|U) (4) where δ is a parameter, roughly indicating the amount of noise in q.",
                "This looks similar to the interpolation smoothing, except that p(·|Λd) now should be a smoothed language model, instead of the one estimated with MLE.",
                "With no prior knowledge on p(·|U), we could set it to p(·|C).",
                "Any smoothing methods for the document language model can be used to estimate p(·|d) such as the Gamma smoothing as discussed in Section 2.2.1.",
                "The empirical study of the smoothing methods is presented in Section 4. 3.",
                "ANALYSIS OF POISSON LANGUAGE MODEL From the previous section, we notice that the Poisson language model has a strong connection to the multinomial language model.",
                "This is expected since they both belong to the exponential family [26].",
                "However, there are many differences when these two families of models are applied with different smoothing methods.",
                "From the perspective of retrieval, will these two language models perform equivalently?",
                "If not, which model provides more benefits to retrieval, or provides flexibility which could lead to potential benefits?",
                "In this section, we analytically discuss the retrieval features of the Poisson language models, by comparing their behavior with that of the multinomial language models. 3.1 The Equivalence of Basic Models Let us begin with the assumption that all the query terms appear in every document.",
                "Under this assumption, no smoothing is needed.",
                "A document can be scored by the log likelihood of the query with the maximum likelihood estimate: Score(d, q) = w∈V log e−λd,w|q| (λd,w|q|)c(w,q) c(w, q)! (5) Using the MLE, we have λd,w = c(w,d) w∈V c(w,d) .",
                "Thus Score(d, q) ∝ c(w,q)>0 c(w, q) log c(w, d) w∈V c(w, d) This is exactly the log likelihood of the query if the document language model is a multinomial with maximum likelihood estimate.",
                "Indeed, even with Gamma smoothing, when plugging λd,w = c(w,d)+µλC,w |d|+µ and λC,w = c(w,C) |C| into Equation 5, it is easy to show that Score(d, q) ∝ w∈q∩d c(w, q) log(1 + c(w, d) µ · c(w,C) |C| ) + |q| log µ |d| + µ (6) which is exactly the Dirichlet retrieval formula in [28].",
                "Note that this equivalence holds only when the document length variation is modeled with <br>poisson process</br>.",
                "This derivation indicates the equivalence of the basic Poisson and multinomial language models for retrieval.",
                "With other smoothing strategies, however, the two models would be different.",
                "Nevertheless, with this equivalence in basic models, we could expect that the Poisson language model performs comparably to the multinomial language model in retrieval, if only simple smoothing is explored.",
                "Based on this equivalence analysis, one may ask, why we should pursue the Poisson language model.",
                "In the following sections, we show that despite the equivalence in their basic models, the Poisson language model brings in extra flexibility for exploring advanced techniques on various retrieval features, which could not be achieved with multinomial language models. 3.2 Term Dependent Smoothing One flexibility of the Poisson language model is that it provides a natural framework to accommodate term dependent (per-term) smoothing.",
                "Existing work on language model smoothing has already shown that different types of queries should be smoothed differently according to how discriminative the query terms are. [7] also predicted that different terms should have a different smoothing weights.",
                "With multinomial query generation models, people usually use a single smoothing coefficient to control the combination of the document model and the background model [28, 29].",
                "This parameter can be made specific for different queries, but always has to be a constant for all the terms.",
                "This is mandatory since a multinomial language model has the constraint that w∈V p(w|d) = 1.",
                "However, from retrieval perspective, different terms may need to be smoothed differently even if they are in the same query.",
                "For example, a non-discriminative term (e.g., the, is) is expected to be explained more with the background model, while a content term (e.g., retrieval, bush) in the query should be explained with the document model.",
                "Therefore, a better way of smoothing would be to set the interpolation coefficient (i.e., δ in Formula 2 and Formula 3) specifically for each term.",
                "Since the Poisson language model does not have the sum-to-one constraint across terms, it can easily accommodate per-term smoothing without needing to heuristically twist the semantics of a generative model as in the case of multinomial language models.",
                "Below we present a possible way to explore term dependent smoothing with Poisson language models.",
                "Essentially, we want to use a term-specific smoothing coefficient δ in the linear combination, denoted as δw.",
                "This coefficient should intuitively be larger if w is a common word and smaller if it is a content word.",
                "The key problem is to find a method to assign reasonable values to δw.",
                "Empirical tuning is infeasible for so many parameters.",
                "We may instead estimate the parameters ∆ = {δ1, ..., δ|V |} by maximizing the likelihood of the query given the mixture model of p(q|ΛQ) and p(q|U), where ΛQ is the true query model to generate the query and p(q|U) is a query background model as discussed in Section 2.2.3.",
                "With the model p(q|ΛQ) hidden, the query likelihood is p(q|∆, U) = ΛQ w∈V ((1 − δw)p(c(w, q)|ΛQ) + δwp(c(w, q)|U))P(ΛQ|U)dΛQ If we have relevant documents for each query, we can approximate the query model space with the language models of all the relevant documents.",
                "Without relevant documents, we opt to approximate the query model space with the models of all the documents in the collection.",
                "Setting p(·|U) as p(·|C), the query likelihood becomes p(q|∆, U) = d∈C πd w∈V ((1−δw)p(c(w, q)|ˆΛd)+δwp(c(w, q)|C)) where πd = p(ˆΛd|U). p(·|ˆΛd) is an estimated Poisson language model for document d. If we have prior knowledge on p(ˆΛd|U), such as which documents are relevant to the query, we can set πd accordingly, because what we want is to find ∆ that can maximize the likelihood of the query given relevant documents.",
                "Without this prior knowledge, we can leave πd as free parameters, and use the EM algorithm to estimate πd and ∆.",
                "The updating functions are given as π (k+1) d = πd w∈V ((1 − δw)p(c(w, q)|ˆΛd) + δwp(c(w, q)|C)) d∈C πd w∈V ((1 − δw)p(c(w, q)|ˆΛd) + δwp(c(w, q)|C)) and δ (k+1) w = d∈C πd δwp(c(w, q)|C)) (1 − δw)p(c(w, q)|ˆΛd) + δwp(c(w, q)|C)) As discussed in [29], we only need to run the EM algorithm for several iterations, thus the computational cost is relatively low.",
                "We again assume our vocabulary containing all query terms plus a pseudo non-query term.",
                "Note that the function does not give an explicit way of estimating the coefficient for the unseen non-query term.",
                "In our experiments, we set it to the average over δw of all query terms.",
                "With this flexibility, we expect Poisson language models could improve the retrieval performance, especially for verbose queries, where the query terms have various discriminative values.",
                "In Section 4, we use empirical experiments to prove this hypothesis. 3.3 Mixture Background Models Another flexibility is to explore different background (collection) models (i.e., p(·|U), or p(·|C)).",
                "One common assumption made in language modeling information retrieval is that the background model is a homogeneous model of the document models [28, 29].",
                "Similarly, we can also make the assumption that the collection model is a Poisson language model, with the rates λC,w = d∈C c(w,d) |C| .",
                "However, this assumption usually does not hold, since the collection is far more complex than a single document.",
                "Indeed, the collection usually consists of a mixture of documents with various genres, authors, and topics, etc.",
                "Treating the collection model as a mixture of document models, instead of a single pseudo-document model is more reasonable.",
                "Existing work of multinomial language modeling has already shown that a better modeling of background improves the retrieval performance, such as clusters [15, 10], neighbor documents [25], and aspects [8, 27].",
                "All the approaches can be easily adopted using Poisson language models.",
                "However, a common problem of these approaches is that they all require heavy computation to construct the background model.",
                "With Poisson language modeling, we show that it is possible to model the mixture background without paying for the heavy computational cost.",
                "Poisson Mixture [3] has been proposed to model a collection of documents, which can fit the data much better than a single Poisson.",
                "The basic idea is to assume that the collection is generated from a mixture of Poisson models, which has the general form of p(x = k|PM) = λ p(λ)p(x = k|λ)dλ p(·|λ) is a single Poisson model and p(λ) is an arbitrary probability density function.",
                "There are three well known Poisson mixtures [3]: 2-Poisson, Negative Binomial, and the Katzs K-Mixture [9].",
                "Note that the 2-Poisson model has actually been explored in probabilistic retrieval models, which led to the well-known BM25 formula [22].",
                "All these mixtures have closed forms, and can be estimated from the collection of documents efficiently.",
                "This is an advantage over the multinomial mixture models, such as PLSI [8] and LDA [1], for retrieval.",
                "For example, the probability density function of Katzs K-Mixture is given as p(c(w) = k|αw, βw) = (1 − αw)ηk,0 + αw βw + 1 ( βw βw + 1 )k where ηk,0 = 1 when k = 0, and 0 otherwise.",
                "With the observation of a collection of documents, αw and βw can be estimated as βw = cf(w) − df(w) df(w) and αw = cf(w) Nβw where cf(w) and df(w) are the collection frequency and document frequency of w, and N is the number of documents in the collection.",
                "To account for the different document lengths, we assume that βw is a reasonable estimation for generating a document of the average length, and use β = βw avdl |q| to generate the query.",
                "This Poisson mixture model can be easily used to replace P(·|C) in the retrieval functions 3 and 4. 3.4 Other Possible Flexibilities In addition to term dependent smoothing and efficient mixture background, a Poisson language model has also some other potential advantages.",
                "For example, in Section 2, we see that Formula 2 introduces a component which does document length penalization.",
                "Intuitively, when the document has more unique words, it will be penalized more.",
                "On the other hand, if a document is exactly n copies of another document, it would not get over penalized.",
                "This feature is desirable and not achieved with the Dirichlet model [5].",
                "Potentially, this component could penalize a document according to what types of terms it contains.",
                "With term specific settings of δ, we could get even more flexibility for document length normalization.",
                "Pseudo-feedback is yet another interesting direction where the Poission model might be able to show its advantage.",
                "With model-based feedback, we could again relax the combination coefficients of the feedback model and the background model, and allow different terms to contribute differently to the feedback model.",
                "We could also utilize the relevant documents to learn better per-term smoothing coefficients. 4.",
                "EVALUATION In Section 3, we analytically compared the Poisson language models and multinomial language models from the perspective of query generation and retrieval.",
                "In this section, we compare these two families of models empirically.",
                "Experiment results show that the Poisson model with perterm smoothing outperforms multinomial model, and the performance can be further improved with two-stage smoothing.",
                "Using Poisson mixture as background model also improves the retrieval performance. 4.1 Datasets Since retrieval performance could significantly vary from one test collection to another, and from one query to another, we select four representative TREC test collections: AP, Trec7, Trec8, and Wt2g(Web).",
                "To cover different types of queries, we follow [28, 5], and construct short-keyword (SK, keyword title), short-verbose (SV, one sentence description), and long-verbose (LV, multiple sentences) queries.",
                "The documents are stemmed with the Porters stemmer, and we do not remove any stop word.",
                "For each parameter, we vary its value to cover a reasonably wide range. 4.2 Comparison to Multinomial We compare the performance of the Poisson retrieval models and multinomial retrieval models using interpolation (JelinekMercer, JM) smoothing and Bayesian smoothing with conjugate priors.",
                "Table 1 shows that the two JM-smoothed models perform similarly on all data sets.",
                "Since the Dirichlet Smoothing for multinomial language model and the Gamma Smoothing for Poisson language model lead to the same retrieval formula, the performance of these two models are jointly presented.",
                "We see that Dirichlet/Gamma smoothing methods outperform both Jelinek-Mercer smoothing methods.",
                "The parameter sensitivity curves for two Jelinek-Mercer 0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1 0 0.05 0.1 0.15 0.2 0.25 0.3 Dataset: Trec8 Parameter: δ AveragePrecision JM−Multinomial: LV JM−Multinomial: SV JM−Multinomial: SK JM−Poisson: SK JM−Poisson: SV JM−Poisson: LV Figure 1: Poisson and multinomial performs similarly with Jelinek-Mercer smoothing smoothing methods are shown in Figure 1.",
                "Clearly, these two methods perform similarly either in terms of optimality Data Query JM-Multinomial JM-Poisson Dirichlet/Gamma Per-term 2-Stage Poisson MAP InitPr Pr@5d MAP InitPr Pr@5d MAP InitPr Pr@5d MAP InitPr Pr@5d AP88-89 SK 0.203 0.585 0.356 0.203 0.585 0.358 0.224 0.629 0.393 0.226 0.630 0.396 SV 0.187 0.580 0.361 0.183 0.571 0.345 0.204 0.613 0.387 0.217* 0.603 0.390 LV 0.283 0.716 0.480 0.271 0.692 0.470 0.291 0.710 0.496 0.304* 0.695 0.510 Trec7 SK 0.167 0.635 0.400 0.168 0.635 0.404 0.186 0.687 0.428 0.185 0.646 0.436 SV 0.174 0.655 0.432 0.176 0.653 0.432 0.182 0.666 0.432 0.196* 0.660 0.440 LV 0.223 0.730 0.496 0.215 0.766 0.488 0.224 0.748 0.52 0.236* 0.738 0.512 Trec8 SK 0.239 0.621 0.440 0.239 0.621 0.436 0.257 0.718 0.496 0.256 0.704 0.468 SV 0.231 0.686 0.448 0.234 0.702 0.456 0.228 0.691 0.456 0.246* 0.692 0.476 LV 0.265 0.796 0.548 0.261 0.757 0.520 0.260 0.741 0.492 0.274* 0.766 0.508 Web SK 0.250 0.616 0.380 0.250 0.616 0.380 0.302 0.767 0.468 0.307 0.739 0.468 SV 0.214 0.611 0.392 0.217 0.609 0.384 0.273 0.693 0.508 0.292* 0.703 0.480 LV 0.266 0.790 0.464 0.259 0.776 0.452 0.283 0.756 0.496 0.311* 0.759 0.488 Table 1: Performance comparison between Poisson and Multinomial retrieval models: basic models perform comparably; term dependent two-stage smoothing significantly improves Poisson An asterisk (*) indicates that the difference between the performance of the term dependent two-stage smoothing and that of the Dirichlet/Gamma single smoothing is statistically significant according to the Wilcoxon signed rank test at the level of 0.05. or sensitivity.",
                "This similarity of performance is expected as we discussed in Section 3.1.",
                "Although the Poisson model and multinomial model are similar in terms of the basic model and/or with simple smoothing methods, the Poisson model has great potential and flexibility to be further improved.",
                "As shown in the rightmost column of Table 1, term dependent two-stage Poisson model consistently outperforms the basic smoothing models, especially for verbose queries.",
                "This model is given in Formula 4, with a Gamma smoothing for the document model p(·|d), and δw, which is term dependent.",
                "The parameter µ of the first stage Gamma smoothing is empirically tuned.",
                "The combination coefficients (i.e., ∆), are estimated with the EM algorithm in Section 3.2.",
                "The parameter sensitivity curves for Dirichlet/Gamma and the per-term two-stage smoothing model are plotted in Figure 2.",
                "The per-term two-stage smoothing method is less sensitive to the parameter µ than Dirichlet/Gamma, and yields better optimal performance. 0 1000 2000 3000 4000 5000 6000 7000 8000 9000 10000 0.1 0.12 0.14 0.16 0.18 0.2 0.22 Dataset: AP; Query Type: SV Parameter: µ AveragePrecision Dirichlet/Gamma Smoothing Term Dependent 2−Stage Figure 2: Term dependent two-stage smoothing of Poisson outperforms Dirichlet/Gamma In the following subsections, we conduct experiments to demonstrate how the flexibility of the Poisson model could be utilized to achieve better performance, which we cannot achieve with multinomial language models. 4.3 Term Dependent Smoothing To test the effectiveness of the term dependent smoothing, we conduct the following two experiments.",
                "In the first experiment, we relax the constant coefficient in the simple Jelinek-Mercer smoothing formula (i.e., Formula 3), and use the EM algorithm proposed in Section 3.2 to find a δw for each unique term.",
                "Since we are using the EM algorithm to iteratively estimate the parameters, we usually do not want the probability of p(·|d) to be zero.",
                "We then use a simple Laplace method to slightly smooth the document model before it goes into the EM iterations.",
                "The documents are then still scored with Formula 3, but using learnt δw.",
                "The results are labeled with JM+L. in Table 2.",
                "Data Q JM JM JM+L. 2-Stage 2-Stage (MAP) PT: No Yes Yes No Yes AP SK 0.203 0.204 0.206 0.223 0.226* SV 0.183 0.189 0.214* 0.204 0.217* Trec7 SK 0.168 0.171 0.174 0.186 0.185 SV 0.176 0.147 0.198* 0.194 0.196 Trec8 SK 0.239 0.240 0.227* 0.257 0.256 SV 0.234 0.223 0.249* 0.242 0.246* Web SK 0.250 0.236 0.220* 0.291 0.307* SV 0.217 0.232 0.261* 0.273 0.292* Table 2: Term dependent smoothing improves retrieval performance An asterisk (*) in Column 3 indicates that the difference between the JM+L. method and JM method is statistically significant; an asterisk (*) in Column 5 means that the difference between term dependent two-stage method and query dependent two-stage method is statistically significant; PT stands for per-term.",
                "With term dependent coefficients, the performance of the Jelinek-Mercer Poisson model is improved in most cases.",
                "However, in some cases (e.g., Trec7/SV), it performs poorly.",
                "This might be caused by the problem of EM estimation with unsmoothed document models.",
                "Once non-zero probability is assigned to all the terms before entering the EM iteration, the performance on verbose queries can be improved significantly.",
                "This indicates that there is still room to find better methods to estimate δw.",
                "Please note that neither the perterm JM method nor the JM+L. method has a parameter to tune.",
                "As shown in Table 1, the term dependent two-stage smoothing can significantly improve retrieval performance.",
                "To understand whether the improvement is contributed by the term dependent smoothing or the two-stage smoothing framework, we design another experiment to compare the perterm two-stage smoothing with the two-stage smoothing method proposed in [29].",
                "Their method managed to find coefficients specific to the query, thus a verbose query would use a higher δ.",
                "However, since their model is based on multinomial language modeling, they could not get per-term coefficients.",
                "We adopt their method to the Poisson two-stage smoothing, and also estimate a per-query coefficient for all the terms.",
                "We compare the performance of such a model with the per-term two-stage smoothing model, and present the results in the right two columns in Table 2.",
                "Again, we see that the per-term two-stage smoothing outperforms the per-query two-stage smoothing, especially for verbose queries.",
                "The improvement is not as large as how the perterm smoothing method improves over Dirichlet/Gamma.",
                "This is expected, since the per-query smoothing has already addressed the query discrimination problem to some extent.",
                "This experiment shows that even if the smoothing is already per-query, making it per-term is still beneficial.",
                "In brief, the per-term smoothing improved the retrieval performance of both one-stage and two-stage smoothing method. 4.4 Mixture Background Model In this section, we conduct experiments to examine the benefits of using a mixture background model without extra computational cost, which can not be achieved for multinomial models.",
                "Specifically, in retrieval formula 3, instead of using a single Poisson distribution to model the background p(·|C), we use Katzs K-Mixture model, which is essentially a mixture of Poisson distributions. p(·|C) can be computed efficiently with simple collection statistics, as discussed in Section 3.3.",
                "Data Query JM.",
                "Poisson JM.",
                "K-Mixture AP SK 0.203 0.204 SV 0.183 0.188* Trec-7 SK 0.168 0.169 SV 0.176 0.178* Trec-8 SK 0.239 0.239 SV 0.234 0.238* Web SK 0.250 0.250 SV 0.217 0.223* Table 3: K-Mixture background model improves retrieval performance The performance of the JM retrieval model with single Poisson background and with Katzs K-Mixture background model is compared in Table 3.",
                "Clearly, using K-Mixture to model the background model outperforms the single Poisson background model in most cases, especially for verbose queries where the improvement is statistically significant.",
                "Figure 3 shows that the performance changes over different parameters for short verbose queries.",
                "The model using K-Mixture background is less sensitive than the one using single Poisson background.",
                "Given that this type of mixture 0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1 0 0.05 0.1 0.15 0.2 0.25 Data: Trec8; Query: SV Parameter: δ AveragePrecision Poisson Background K−Mixture Background Figure 3: K-Mixture background model deviates the sensitivity of verbose queries background model does not require any extra computation cost, it would be interesting to study whether using other mixture Poisson models, such as 2-Poisson and negative Binomial, could help the performance. 5.",
                "RELATED WORK To the best of our knowledge, there has been no study of query generation models based on Poisson distribution.",
                "Language models have been shown to be effective for many retrieval tasks [21, 28, 14, 4].",
                "The most popular and fundamental one is the query-generation language model [21, 13].",
                "All existing query generation language models are based on either multinomial distribution [19, 6, 28, 13] or multivariate Bernoulli distribution [21, 17, 18].",
                "We introduce a new family of language models, based on Poisson distribution.",
                "Poisson distribution has been previously studied in the document generation models [16, 22, 3, 24], leading to the development of one of the most effective retrieval formula BM25 [23]. [24] studies the parallel derivation of three different retrieval models which is related to our comparison of Poisson and multinomial.",
                "However, the Poisson model in their paper is still under the document generation framework, and also does not account for the document length variation. [26] introduces a way to empirically search for an exponential model for the documents.",
                "Poisson mixtures [3] such as 2-Poisson [22], Negative multinomial, and Katzs KMixture [9] has shown to be effective to model and retrieve documents.",
                "Once again, none of this work explores Poisson distribution in the query generation framework.",
                "Language model smoothing [2, 28, 29] and background structures [15, 10, 25, 27] have been studied with multinomial language models. [7] analytically shows that term specific smoothing could be useful.",
                "We show that Poisson language model is natural to accommodate the per-term smoothing without heuristic twist of the semantics of a generative model, and is able to efficiently better model the mixture background, both analytically and empirically. 6.",
                "CONCLUSIONS We present a new family of query generation language models for retrieval based on Poisson distribution.",
                "We derive several smoothing methods for this family of models, including single-stage smoothing and two-stage smoothing.",
                "We compare the new models with the popular multinomial retrieval models both analytically and experimentally.",
                "Our analysis shows that while our new models and multinomial models are equivalent under some assumptions, they are generally different with some important differences.",
                "In particular, we show that Poisson has an advantage over multinomial in naturally accommodating per-term smoothing.",
                "We exploit this property to develop a new per-term smoothing algorithm for Poisson language models, which is shown to outperform term-independent smoothing for both Poisson and multinomial models.",
                "Furthermore, we show that a mixture background model for Poisson can be used to improve the performance and robustness over the standard Poisson background model.",
                "Our work opens up many interesting directions for further exploration in this new family of models.",
                "Further exploring the flexibilities over multinomial language models, such as length normalization and pseudo-feedback could be good future work.",
                "It is also appealing to find robust methods to learn the per-term smoothing coefficients without additional computation cost. 7.",
                "ACKNOWLEDGMENTS We thank the anonymous SIGIR 07 reviewers for their useful comments.",
                "This material is based in part upon work supported by the National Science Foundation under award numbers IIS-0347933 and 0425852. 8.",
                "REFERENCES [1] D. Blei, A. Ng, and M. Jordan.",
                "Latent dirichlet allocation.",
                "Journal of Machine Learning Research, 3:993-1022, 2003. [2] S. F. Chen and J. Goodman.",
                "An empirical study of smoothing techniques for language modeling.",
                "Technical Report TR-10-98, Harvard University, 1998. [3] K. Church and W. Gale.",
                "Poisson mixtures.",
                "Nat.",
                "Lang.",
                "Eng., 1(2):163-190, 1995. [4] W. B. Croft and J. Lafferty, editors.",
                "Language Modeling and Information Retrieval.",
                "Kluwer Academic Publishers, 2003. [5] H. Fang, T. Tao, and C. Zhai.",
                "A formal study of information retrieval heuristics.",
                "In Proceedings of the 27th annual international ACM SIGIR conference on Research and development in information retrieval, pages 49-56, 2004. [6] D. Hiemstra.",
                "Using Language Models for Information Retrieval.",
                "PhD thesis, University of Twente, Enschede, Netherlands, 2001. [7] D. Hiemstra.",
                "Term-specific smoothing for the language modeling approach to information retrieval: the importance of a query term.",
                "In Proceedings of the 25th annual international ACM SIGIR conference on Research and development in information retrieval, pages 35-41, 2002. [8] T. Hofmann.",
                "Probabilistic latent semantic indexing.",
                "In Proceedings of ACM SIGIR99, pages 50-57, 1999. [9] S. M. Katz.",
                "Distribution of content words and phrases in text and language modelling.",
                "Nat.",
                "Lang.",
                "Eng., 2(1):15-59, 1996. [10] O. Kurland and L. Lee.",
                "Corpus structure, language models, and ad-hoc information retrieval.",
                "In Proceedings of the 27th annual international ACM SIGIR conference on Research and development in information retrieval, pages 194-201, 2004. [11] J. Lafferty and C. Zhai.",
                "Document language models, query models, and risk minimization for information retrieval.",
                "In Proceedings of SIGIR01, pages 111-119, Sept 2001. [12] J. Lafferty and C. Zhai.",
                "Probabilistic IR models based on query and document generation.",
                "In Proceedings of the Language Modeling and IR workshop, pages 1-5, May 31 - June 1 2001. [13] J. Lafferty and C. Zhai.",
                "Probabilistic relevance models based on document and query generation.",
                "In W. B. Croft and J. Lafferty, editors, Language Modeling and Information Retrieval.",
                "Kluwer Academic Publishers, 2003. [14] V. Lavrenko and B. Croft.",
                "Relevance-based language models.",
                "In Proceedings of SIGIR01, pages 120-127, Sept 2001. [15] X. Liu and W. B. Croft.",
                "Cluster-based retrieval using language models.",
                "In Proceedings of the 27th annual international ACM SIGIR conference on Research and development in information retrieval, pages 186-193, 2004. [16] E. L. Margulis.",
                "Modelling documents with multiple poisson distributions.",
                "Inf.",
                "Process.",
                "Manage., 29(2):215-227, 1993. [17] A. McCallum and K. Nigam.",
                "A comparison of event models for naive bayes text classification.",
                "In Proceedings of AAAI-98 Workshop on Learning for Text Categorization, 1998. [18] D. Metzler, V. Lavrenko, and W. B. Croft.",
                "Formal multiple-bernoulli models for language modeling.",
                "In Proceedings of the 27th annual international ACM SIGIR conference on Research and development in information retrieval, pages 540-541, 2004. [19] D. H. Miller, T. Leek, and R. Schwartz.",
                "A hidden Markov model information retrieval system.",
                "In Proceedings of the 1999 ACM SIGIR Conference on Research and Development in Information Retrieval, pages 214-221, 1999. [20] A. Papoulis.",
                "Probability, random variables and stochastic processes.",
                "New York: McGraw-Hill, 1984, 2nd ed., 1984. [21] J. M. Ponte and W. B. Croft.",
                "A language modeling approach to information retrieval.",
                "In Proceedings of the 21st annual international ACM SIGIR conference on Research and development in information retrieval, pages 275-281, 1998. [22] S. Robertson and S. Walker.",
                "Some simple effective approximations to the 2-poisson model for probabilistic weighted retrieval.",
                "In Proceedings of SIGIR94, pages 232-241, 1994. [23] S. E. Robertson, S. Walker, S. Jones, M. M.Hancock-Beaulieu, and M. Gatford.",
                "Okapi at TREC-3.",
                "In D. K. Harman, editor, The Third Text REtrieval Conference (TREC-3), pages 109-126, 1995. [24] T. Roelleke and J. Wang.",
                "A parallel derivation of probabilistic information retrieval models.",
                "In Proceedings of the 29th annual international ACM SIGIR conference on Research and development in information retrieval, pages 107-114, 2006. [25] T. Tao, X. Wang, Q. Mei, and C. Zhai.",
                "Language model information retrieval with document expansion.",
                "In Proceedings of HLT/NAACL 2006, pages 407-414, 2006. [26] J. Teevan and D. R. Karger.",
                "Empirical development of an exponential probabilistic model for text retrieval: using textual analysis to build a better model.",
                "In Proceedings of the 26th annual international ACM SIGIR conference on Research and development in informaion retrieval, pages 18-25, 2003. [27] X. Wei and W. B. Croft.",
                "Lda-based document models for ad-hoc retrieval.",
                "In Proceedings of the 29th annual international ACM SIGIR conference on Research and development in information retrieval, pages 178-185, 2006. [28] C. Zhai and J. Lafferty.",
                "A study of smoothing methods for language models applied to ad-hoc information retrieval.",
                "In Proceedings of ACM SIGIR01, pages 334-342, Sept 2001. [29] C. Zhai and J. Lafferty.",
                "Two-stage language models for information retrieval.",
                "In Proceedings of ACM SIGIR02, pages 49-56, Aug 2002."
            ],
            "original_annotated_samples": [],
            "translated_annotated_samples": [
                "Generación de consultas con \"Poisson Process\" En el marco de generación de consultas, una suposición básica es que se genera una consulta con un modelo estimado en base a un documento.Proceso de Poisson",
                "Con un \"proceso de Poisson\" homogéneo, el recuento de frecuencia de cada evento, es decir, el número de ocurrencias de WI, sigue una distribución de Poisson con el parámetro asociado λit, donde λi es un parámetro de velocidad que caracteriza el número esperado de WI en un tiempo de unidad.Proceso de Poisson",
                "Sea d = {d1, ..., dm} un conjunto observado de muestras de documentos generadas a partir del \"proceso Poisson\" anterior.Proceso de Poisson",
                "Tenga en cuenta que esta equivalencia se mantiene solo cuando la variación de la longitud del documento se modela con el \"proceso Poisson\".Proceso de Poisson"
            ],
            "translated_text": "",
            "candidates": [],
            "error": []
        },
        "query generation": {
            "translated_key": "",
            "is_in_text": true,
            "original_annotated_sentences": [
                "A Study of Poisson <br>query generation</br> Model for Information Retrieval Qiaozhu Mei, Hui Fang, Chengxiang Zhai Department of Computer Science University of Illinois at Urbana-Champaign Urbana,IL 61801 {qmei2,hfang,czhai}@uiuc.edu ABSTRACT Many variants of language models have been proposed for information retrieval.",
                "Most existing models are based on multinomial distribution and would score documents based on query likelihood computed based on a <br>query generation</br> probabilistic model.",
                "In this paper, we propose and study a new family of <br>query generation</br> models based on Poisson distribution.",
                "We show that while in their simplest forms, the new family of models and the existing multinomial models are equivalent, they behave differently for many smoothing methods.",
                "We show that the Poisson model has several advantages over the multinomial model, including naturally accommodating per-term smoothing and allowing for more accurate background modeling.",
                "We present several variants of the new model corresponding to different smoothing methods, and evaluate them on four representative TREC test collections.",
                "The results show that while their basic models perform comparably, the Poisson model can outperform multinomial model with per-term smoothing.",
                "The performance can be further improved with two-stage smoothing.",
                "Categories and Subject Descriptors: H.3.3 [Information Search and Retrieval]: Retrieval Models General Terms: Algorithms 1.",
                "INTRODUCTION As a new type of probabilistic retrieval models, language models have been shown to be effective for many retrieval tasks [21, 28, 14, 4].",
                "Among many variants of language models proposed, the most popular and fundamental one is the query-generation language model [21, 13], which leads to the query-likelihood scoring method for ranking documents.",
                "In such a model, given a query q and a document d, we compute the likelihood of generating query q with a model estimated based on document d, i.e., the conditional probability p(q|d).",
                "We can then rank documents based on the likelihood of generating the query.",
                "Virtually all the existing <br>query generation</br> language models are based on either multinomial distribution [19, 6, 28] or multivariate Bernoulli distribution [21, 18].",
                "The multinomial distribution is especially popular and also shown to be quite effective.",
                "The heavy use of multinomial distribution is partly due to the fact that it has been successfully used in speech recognition, where multinomial distribution is a natural choice for modeling the occurrence of a particular word in a particular position in text.",
                "Compared with multivariate Bernoulli, multinomial distribution has the advantage of being able to model the frequency of terms in the query; in contrast, multivariate Bernoulli only models the presence and absence of query terms, thus cannot capture different frequencies of query terms.",
                "However, multivariate Bernoulli also has one potential advantage over multinomial from the viewpoint of retrieval: in a multinomial distribution, the probabilities of all the terms must sum to 1, making it hard to accommodate per-term smoothing, while in a multivariate Bernoulli, the presence probabilities of different terms are completely independent of each other, easily accommodating per-term smoothing and weighting.",
                "Note that term absence is also indirectly captured in a multinomial model through the constraint that all the term probabilities must sum to 1.",
                "In this paper, we propose and study a new family of <br>query generation</br> models based on the Poisson distribution.",
                "In this new family of models, we model the frequency of each term independently with a Poisson distribution.",
                "To score a document, we would first estimate a multivariate Poisson model based on the document, and then score it based on the likelihood of the query given by the estimated Poisson model.",
                "In some sense, the Poisson model combines the advantage of multinomial in modeling term frequency and the advantage of the multivariate Bernoulli in accommodating per-term smoothing.",
                "Indeed, similar to the multinomial distribution, the Poisson distribution models term frequencies, but without the constraint that all the term probabilities must sum to 1, and similar to multivariate Bernoulli, it models each term independently, thus can easily accommodate per-term smoothing.",
                "As in the existing work on multinomial language models, smoothing is critical for this new family of models.",
                "We derive several smoothing methods for Poisson model in parallel to those used for multinomial distributions, and compare the corresponding retrieval models with those based on multinomial distributions.",
                "We find that while with some smoothing methods, the new model and the multinomial model lead to exactly the same formula, with some other smoothing methods they diverge, and the Poisson model brings in more flexibility for smoothing.",
                "In particular, a key difference is that the Poisson model can naturally accommodate perterm smoothing, which is hard to achieve with a multinomial model without heuristic twist of the semantics of a generative model.",
                "We exploit this potential advantage to develop a new term-dependent smoothing algorithm for Poisson model and show that this new smoothing algorithm can improve performance over term-independent smoothing algorithms using either Poisson or multinomial model.",
                "This advantage is seen for both one-stage and two-stage smoothing.",
                "Another potential advantage of the Poisson model is that its corresponding background model for smoothing can be improved through using a mixture model that has a closed form formula.",
                "This new background model is shown to outperform the standard background model and reduce the sensitivity of retrieval performance to the smoothing parameter.",
                "The rest of the paper is organized as follows.",
                "In Section 2, we introduce the new family of <br>query generation</br> models with Poisson distribution, and present various smoothing methods which lead to different retrieval functions.",
                "In Section 3, we analytically compare the Poisson language model with the multinomial language model, from the perspective of retrieval.",
                "We then design empirical experiments to compare the two families of language models in Section 4.",
                "We discuss the related work in 5 and conclude in 6. 2.",
                "<br>query generation</br> WITH POISSON PROCESS In the <br>query generation</br> framework, a basic assumption is that a query is generated with a model estimated based on a document.",
                "In most existing work [12, 6, 28, 29], people assume that each query word is sampled independently from a multinomial distribution.",
                "Alternatively, we assume that a query is generated by sampling the frequency of words from a series of independent Poisson processes [20]. 2.1 The Generation Process Let V = {w1, ..., wn} be a vocabulary set.",
                "Let w be a piece of text composed by an author and c(w1), ..., c(wn) be a frequency vector representing w, where c(wi, w) is the frequency count of term wi in text w. In retrieval, w could be either a query or a document.",
                "We consider the frequency counts of the n unique terms in w as n different types of events, sampled from n independent homogeneous Poisson processes, respectively.",
                "Suppose t is the time period during which the author composed the text.",
                "With a homogeneous Poisson process, the frequency count of each event, i.e., the number of occurrences of wi, follows a Poisson distribution with associated parameter λit, where λi is a rate parameter characterizing the expected number of wi in a unit time.",
                "The probability density function of such a Poisson Distribution is given by P(c(wi, w) = k|λit) = e−λit (λit)k k!",
                "Without losing generality, we set t to the length of the text w (people write one word in a unit time), i.e., t = |w|.",
                "With n such independent Poisson processes, each explaining the generation of one term in the vocabulary, the likelihood of w to be generated from such Poisson processes can be written as p(w|Λ) = n i=1 p(c(wi, w)|Λ) = n i=1 e−λi·|w| (λi · |w|)c(wi,w) c(wi, w)! where Λ = {λ1, ..., λn} and |w| = n i=1 c(wi, w).",
                "We refer to these n independent Poisson processes with parameter Λ as a Poisson Language Model.",
                "Let D = {d1, ..., dm} be an observed set of document samples generated from the Poisson process above.",
                "The maximum likelihood estimate (MLE) of λi is ˆλi = d∈D c(wi, d) d∈D w ∈V c(w , d) Note that this MLE is different from the MLE for the Poisson distribution without considering the document lengths, which appears in [22, 24].",
                "Given a document d, we may estimate a Poisson language model Λd using d as a sample.",
                "The likelihood that a query q is generated from the document language model Λd can be written as p(q|d) = w∈V p(c(w, q)|Λd) (1) This representation is clearly different from the multinomial <br>query generation</br> model as (1) the likelihood includes all the terms in the vocabulary V , instead of only those appearing in q, and (2) instead of the appearance of terms, the event space of this model is the frequencies of each term.",
                "In practice, we have the flexibility to choose the vocabulary V .",
                "In one extreme, we can use the vocabulary of the whole collection.",
                "However, this may bring in noise and considerable computational cost.",
                "In the other extreme, we may focus on the terms in the query and ignore other terms, but some useful information may be lost by ignoring the nonquery terms.",
                "As a compromise, we may conflate all the non-query terms as one single pseudo term.",
                "In other words, we may assume that there is exactly one non-query term in the vocabulary for each query.",
                "In our experiments, we adopt this pseudo non-query term strategy.",
                "A document can be scored with the likelihood in Equation 1.",
                "However, if a query term is unseen in the document, the MLE of the Poisson distribution would assign zero probability to the term, causing the probability of the query to be zero.",
                "As in existing language modeling approaches, the main challenge of constructing a reasonable retrieval model is to find a smoothed language model for p(·|d). 2.2 Smoothing in Poisson Retrieval Model In general, we want to assign non-zero rates for the query terms that are not seen in document d. Many smoothing methods have been proposed for multinomial language models[2, 28, 29].",
                "In general, we have to discount the probabilities of some words seen in the text to leave some extra probability mass to assign to the unseen words.",
                "In Poisson language models, however, we do not have the same constraint as in a multinomial model (i.e., w∈V p(w|d) = 1).",
                "Thus we do not have to discount the probability of seen words in order to give a non-zero rate to an unseen word.",
                "Instead, we only need to guarantee that k=0,1,2,... p(c(w, d) = k|d) = 1.",
                "In this section, we introduce three different strategies to smooth a Poisson language model, and show how they lead to different retrieval functions. 2.2.1 Bayesian Smoothing using Gamma Prior Following the risk minimization framework in [11], we assume that a document is generated by the arrival of terms in a time period of |d| according to the document language model, which essentially consists of a vector of Poisson rates for each term, i.e., Λd = λd,1, ..., λd,|V | .",
                "A document is assumed to be generated from a potentially different model.",
                "Given a particular document d, we want to estimate Λd.",
                "The rate of a term is estimated independently of other terms.",
                "We use Bayesian estimation with the following Gamma prior, which has two parameters, α and β: Gamma(λ|α, β) = βα Γ(α) λα−1 e−βλ For each term w, the parameters αw and βw are chosen to be αw = µ ∗ λC,w and βw = µ, where µ is a parameter and λC,w is the rate of w estimated from some background language model, usually the collection language model.",
                "The posterior distribution of Λd is given by p(Λd|d, C) ∝ w∈V e−λw(|d|+µ) λ c(w,d)+µλC,w−1 w which is a product of |V | Gamma distributions with parameters c(w, d) + µλC,w and |d| + µ for each word w. Given that the Gamma mean is α β , we have ˆλd,w = λd,w λd,wp(λd,w|d, C)dλd,w = c(w, d) + µλC,w |d| + µ This is precisely the smoothed estimate of multinomial language model with Dirichlet prior [28]. 2.2.2 Interpolation (Jelinek-Mercer) Smoothing Another straightforward method is to decompose the <br>query generation</br> model as a mixture of two component models.",
                "One is the document language model estimated with maximum likelihood estimator, and the other is a model estimated from the collection background, p(·|C), which assigns non-zero rate to w. For example, we may use an interpolation coefficient between 0 and 1 (i.e., δ ∈ [0, 1]).",
                "With this simple interpolation, we can score a document with Score(d, q) = w∈V log((1 − δ)p(c(w, q)|d) + δp(c(w, q)|C)) (2) Using the maximum likelihood estimator for p(·|d), we have λd,w = c(w,d) |d| , thus Equation 2 becomes Score(d, q) ∝ w∈d∩q [log(1 + 1 − δ δ e−λd,w|q| (λd,w|q|)c(w,q) c(w, q)! · p(c(w, q)|C) ) − log (1 − δ)e−λd,w|q| + δp(c(w, q) = 0|C) 1 − δ + δp(c(w, q) = 0|C) ] + w∈d log (1 − δ)e−λd,w|q| + δp(c(w, q) = 0|C) 1 − δ + δp(c(w, q) = 0|C) We can also use a Poisson language model for p(·|C), or use some other frequency-based models.",
                "In the retrieval formula above, the first summation can be computed efficiently.",
                "The second summation can be actually treated as a document prior, which penalizes long documents.",
                "As the second summation is difficult to compute efficiently, we conflate all non-query terms as one pseudo non-queryterm, denoted as N. Using the pseudo-term formulation and a Poisson collection model, we can rewrite the retrieval formula as Score(d, q) ∝ w∈d∩q log(1 + 1 − δ δ e−λd,w (λd,w|q|)c(w,q) e−λd,C |q| (λd,C )c(w,q) ) + log (1 − δ)e−λd,N |q| + δe−λC,N |q| 1 − δ + δe−λC,N |q| (3) where λd,N = |d|− w∈q c(w,d) |d| and λC,N = |C|− w∈q c(w,C) |C| . 2.2.3 Two-Stage Smoothing As discussed in [29], smoothing plays two roles in retrieval: (1) to improve the estimation of the document language model, and (2) to explain the common terms in the query.",
                "In order to distinguish the content and non-discriminative words in a query, we follow [29] and assume that a query is generated by sampling from a two-component mixture of Poisson language models, with one component being the document model Λd and the other being a query background language model p(·|U). p(·|U) models the typical term frequencies in the users queries.",
                "We may then score each document with the query likelihood computed using the following two-stage smoothing model: p(c(w, q)|Λd, U) = (1 − δ)p(c(w, q)|Λd) + δp(c(w, q)|U) (4) where δ is a parameter, roughly indicating the amount of noise in q.",
                "This looks similar to the interpolation smoothing, except that p(·|Λd) now should be a smoothed language model, instead of the one estimated with MLE.",
                "With no prior knowledge on p(·|U), we could set it to p(·|C).",
                "Any smoothing methods for the document language model can be used to estimate p(·|d) such as the Gamma smoothing as discussed in Section 2.2.1.",
                "The empirical study of the smoothing methods is presented in Section 4. 3.",
                "ANALYSIS OF POISSON LANGUAGE MODEL From the previous section, we notice that the Poisson language model has a strong connection to the multinomial language model.",
                "This is expected since they both belong to the exponential family [26].",
                "However, there are many differences when these two families of models are applied with different smoothing methods.",
                "From the perspective of retrieval, will these two language models perform equivalently?",
                "If not, which model provides more benefits to retrieval, or provides flexibility which could lead to potential benefits?",
                "In this section, we analytically discuss the retrieval features of the Poisson language models, by comparing their behavior with that of the multinomial language models. 3.1 The Equivalence of Basic Models Let us begin with the assumption that all the query terms appear in every document.",
                "Under this assumption, no smoothing is needed.",
                "A document can be scored by the log likelihood of the query with the maximum likelihood estimate: Score(d, q) = w∈V log e−λd,w|q| (λd,w|q|)c(w,q) c(w, q)! (5) Using the MLE, we have λd,w = c(w,d) w∈V c(w,d) .",
                "Thus Score(d, q) ∝ c(w,q)>0 c(w, q) log c(w, d) w∈V c(w, d) This is exactly the log likelihood of the query if the document language model is a multinomial with maximum likelihood estimate.",
                "Indeed, even with Gamma smoothing, when plugging λd,w = c(w,d)+µλC,w |d|+µ and λC,w = c(w,C) |C| into Equation 5, it is easy to show that Score(d, q) ∝ w∈q∩d c(w, q) log(1 + c(w, d) µ · c(w,C) |C| ) + |q| log µ |d| + µ (6) which is exactly the Dirichlet retrieval formula in [28].",
                "Note that this equivalence holds only when the document length variation is modeled with Poisson process.",
                "This derivation indicates the equivalence of the basic Poisson and multinomial language models for retrieval.",
                "With other smoothing strategies, however, the two models would be different.",
                "Nevertheless, with this equivalence in basic models, we could expect that the Poisson language model performs comparably to the multinomial language model in retrieval, if only simple smoothing is explored.",
                "Based on this equivalence analysis, one may ask, why we should pursue the Poisson language model.",
                "In the following sections, we show that despite the equivalence in their basic models, the Poisson language model brings in extra flexibility for exploring advanced techniques on various retrieval features, which could not be achieved with multinomial language models. 3.2 Term Dependent Smoothing One flexibility of the Poisson language model is that it provides a natural framework to accommodate term dependent (per-term) smoothing.",
                "Existing work on language model smoothing has already shown that different types of queries should be smoothed differently according to how discriminative the query terms are. [7] also predicted that different terms should have a different smoothing weights.",
                "With multinomial <br>query generation</br> models, people usually use a single smoothing coefficient to control the combination of the document model and the background model [28, 29].",
                "This parameter can be made specific for different queries, but always has to be a constant for all the terms.",
                "This is mandatory since a multinomial language model has the constraint that w∈V p(w|d) = 1.",
                "However, from retrieval perspective, different terms may need to be smoothed differently even if they are in the same query.",
                "For example, a non-discriminative term (e.g., the, is) is expected to be explained more with the background model, while a content term (e.g., retrieval, bush) in the query should be explained with the document model.",
                "Therefore, a better way of smoothing would be to set the interpolation coefficient (i.e., δ in Formula 2 and Formula 3) specifically for each term.",
                "Since the Poisson language model does not have the sum-to-one constraint across terms, it can easily accommodate per-term smoothing without needing to heuristically twist the semantics of a generative model as in the case of multinomial language models.",
                "Below we present a possible way to explore term dependent smoothing with Poisson language models.",
                "Essentially, we want to use a term-specific smoothing coefficient δ in the linear combination, denoted as δw.",
                "This coefficient should intuitively be larger if w is a common word and smaller if it is a content word.",
                "The key problem is to find a method to assign reasonable values to δw.",
                "Empirical tuning is infeasible for so many parameters.",
                "We may instead estimate the parameters ∆ = {δ1, ..., δ|V |} by maximizing the likelihood of the query given the mixture model of p(q|ΛQ) and p(q|U), where ΛQ is the true query model to generate the query and p(q|U) is a query background model as discussed in Section 2.2.3.",
                "With the model p(q|ΛQ) hidden, the query likelihood is p(q|∆, U) = ΛQ w∈V ((1 − δw)p(c(w, q)|ΛQ) + δwp(c(w, q)|U))P(ΛQ|U)dΛQ If we have relevant documents for each query, we can approximate the query model space with the language models of all the relevant documents.",
                "Without relevant documents, we opt to approximate the query model space with the models of all the documents in the collection.",
                "Setting p(·|U) as p(·|C), the query likelihood becomes p(q|∆, U) = d∈C πd w∈V ((1−δw)p(c(w, q)|ˆΛd)+δwp(c(w, q)|C)) where πd = p(ˆΛd|U). p(·|ˆΛd) is an estimated Poisson language model for document d. If we have prior knowledge on p(ˆΛd|U), such as which documents are relevant to the query, we can set πd accordingly, because what we want is to find ∆ that can maximize the likelihood of the query given relevant documents.",
                "Without this prior knowledge, we can leave πd as free parameters, and use the EM algorithm to estimate πd and ∆.",
                "The updating functions are given as π (k+1) d = πd w∈V ((1 − δw)p(c(w, q)|ˆΛd) + δwp(c(w, q)|C)) d∈C πd w∈V ((1 − δw)p(c(w, q)|ˆΛd) + δwp(c(w, q)|C)) and δ (k+1) w = d∈C πd δwp(c(w, q)|C)) (1 − δw)p(c(w, q)|ˆΛd) + δwp(c(w, q)|C)) As discussed in [29], we only need to run the EM algorithm for several iterations, thus the computational cost is relatively low.",
                "We again assume our vocabulary containing all query terms plus a pseudo non-query term.",
                "Note that the function does not give an explicit way of estimating the coefficient for the unseen non-query term.",
                "In our experiments, we set it to the average over δw of all query terms.",
                "With this flexibility, we expect Poisson language models could improve the retrieval performance, especially for verbose queries, where the query terms have various discriminative values.",
                "In Section 4, we use empirical experiments to prove this hypothesis. 3.3 Mixture Background Models Another flexibility is to explore different background (collection) models (i.e., p(·|U), or p(·|C)).",
                "One common assumption made in language modeling information retrieval is that the background model is a homogeneous model of the document models [28, 29].",
                "Similarly, we can also make the assumption that the collection model is a Poisson language model, with the rates λC,w = d∈C c(w,d) |C| .",
                "However, this assumption usually does not hold, since the collection is far more complex than a single document.",
                "Indeed, the collection usually consists of a mixture of documents with various genres, authors, and topics, etc.",
                "Treating the collection model as a mixture of document models, instead of a single pseudo-document model is more reasonable.",
                "Existing work of multinomial language modeling has already shown that a better modeling of background improves the retrieval performance, such as clusters [15, 10], neighbor documents [25], and aspects [8, 27].",
                "All the approaches can be easily adopted using Poisson language models.",
                "However, a common problem of these approaches is that they all require heavy computation to construct the background model.",
                "With Poisson language modeling, we show that it is possible to model the mixture background without paying for the heavy computational cost.",
                "Poisson Mixture [3] has been proposed to model a collection of documents, which can fit the data much better than a single Poisson.",
                "The basic idea is to assume that the collection is generated from a mixture of Poisson models, which has the general form of p(x = k|PM) = λ p(λ)p(x = k|λ)dλ p(·|λ) is a single Poisson model and p(λ) is an arbitrary probability density function.",
                "There are three well known Poisson mixtures [3]: 2-Poisson, Negative Binomial, and the Katzs K-Mixture [9].",
                "Note that the 2-Poisson model has actually been explored in probabilistic retrieval models, which led to the well-known BM25 formula [22].",
                "All these mixtures have closed forms, and can be estimated from the collection of documents efficiently.",
                "This is an advantage over the multinomial mixture models, such as PLSI [8] and LDA [1], for retrieval.",
                "For example, the probability density function of Katzs K-Mixture is given as p(c(w) = k|αw, βw) = (1 − αw)ηk,0 + αw βw + 1 ( βw βw + 1 )k where ηk,0 = 1 when k = 0, and 0 otherwise.",
                "With the observation of a collection of documents, αw and βw can be estimated as βw = cf(w) − df(w) df(w) and αw = cf(w) Nβw where cf(w) and df(w) are the collection frequency and document frequency of w, and N is the number of documents in the collection.",
                "To account for the different document lengths, we assume that βw is a reasonable estimation for generating a document of the average length, and use β = βw avdl |q| to generate the query.",
                "This Poisson mixture model can be easily used to replace P(·|C) in the retrieval functions 3 and 4. 3.4 Other Possible Flexibilities In addition to term dependent smoothing and efficient mixture background, a Poisson language model has also some other potential advantages.",
                "For example, in Section 2, we see that Formula 2 introduces a component which does document length penalization.",
                "Intuitively, when the document has more unique words, it will be penalized more.",
                "On the other hand, if a document is exactly n copies of another document, it would not get over penalized.",
                "This feature is desirable and not achieved with the Dirichlet model [5].",
                "Potentially, this component could penalize a document according to what types of terms it contains.",
                "With term specific settings of δ, we could get even more flexibility for document length normalization.",
                "Pseudo-feedback is yet another interesting direction where the Poission model might be able to show its advantage.",
                "With model-based feedback, we could again relax the combination coefficients of the feedback model and the background model, and allow different terms to contribute differently to the feedback model.",
                "We could also utilize the relevant documents to learn better per-term smoothing coefficients. 4.",
                "EVALUATION In Section 3, we analytically compared the Poisson language models and multinomial language models from the perspective of <br>query generation</br> and retrieval.",
                "In this section, we compare these two families of models empirically.",
                "Experiment results show that the Poisson model with perterm smoothing outperforms multinomial model, and the performance can be further improved with two-stage smoothing.",
                "Using Poisson mixture as background model also improves the retrieval performance. 4.1 Datasets Since retrieval performance could significantly vary from one test collection to another, and from one query to another, we select four representative TREC test collections: AP, Trec7, Trec8, and Wt2g(Web).",
                "To cover different types of queries, we follow [28, 5], and construct short-keyword (SK, keyword title), short-verbose (SV, one sentence description), and long-verbose (LV, multiple sentences) queries.",
                "The documents are stemmed with the Porters stemmer, and we do not remove any stop word.",
                "For each parameter, we vary its value to cover a reasonably wide range. 4.2 Comparison to Multinomial We compare the performance of the Poisson retrieval models and multinomial retrieval models using interpolation (JelinekMercer, JM) smoothing and Bayesian smoothing with conjugate priors.",
                "Table 1 shows that the two JM-smoothed models perform similarly on all data sets.",
                "Since the Dirichlet Smoothing for multinomial language model and the Gamma Smoothing for Poisson language model lead to the same retrieval formula, the performance of these two models are jointly presented.",
                "We see that Dirichlet/Gamma smoothing methods outperform both Jelinek-Mercer smoothing methods.",
                "The parameter sensitivity curves for two Jelinek-Mercer 0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1 0 0.05 0.1 0.15 0.2 0.25 0.3 Dataset: Trec8 Parameter: δ AveragePrecision JM−Multinomial: LV JM−Multinomial: SV JM−Multinomial: SK JM−Poisson: SK JM−Poisson: SV JM−Poisson: LV Figure 1: Poisson and multinomial performs similarly with Jelinek-Mercer smoothing smoothing methods are shown in Figure 1.",
                "Clearly, these two methods perform similarly either in terms of optimality Data Query JM-Multinomial JM-Poisson Dirichlet/Gamma Per-term 2-Stage Poisson MAP InitPr Pr@5d MAP InitPr Pr@5d MAP InitPr Pr@5d MAP InitPr Pr@5d AP88-89 SK 0.203 0.585 0.356 0.203 0.585 0.358 0.224 0.629 0.393 0.226 0.630 0.396 SV 0.187 0.580 0.361 0.183 0.571 0.345 0.204 0.613 0.387 0.217* 0.603 0.390 LV 0.283 0.716 0.480 0.271 0.692 0.470 0.291 0.710 0.496 0.304* 0.695 0.510 Trec7 SK 0.167 0.635 0.400 0.168 0.635 0.404 0.186 0.687 0.428 0.185 0.646 0.436 SV 0.174 0.655 0.432 0.176 0.653 0.432 0.182 0.666 0.432 0.196* 0.660 0.440 LV 0.223 0.730 0.496 0.215 0.766 0.488 0.224 0.748 0.52 0.236* 0.738 0.512 Trec8 SK 0.239 0.621 0.440 0.239 0.621 0.436 0.257 0.718 0.496 0.256 0.704 0.468 SV 0.231 0.686 0.448 0.234 0.702 0.456 0.228 0.691 0.456 0.246* 0.692 0.476 LV 0.265 0.796 0.548 0.261 0.757 0.520 0.260 0.741 0.492 0.274* 0.766 0.508 Web SK 0.250 0.616 0.380 0.250 0.616 0.380 0.302 0.767 0.468 0.307 0.739 0.468 SV 0.214 0.611 0.392 0.217 0.609 0.384 0.273 0.693 0.508 0.292* 0.703 0.480 LV 0.266 0.790 0.464 0.259 0.776 0.452 0.283 0.756 0.496 0.311* 0.759 0.488 Table 1: Performance comparison between Poisson and Multinomial retrieval models: basic models perform comparably; term dependent two-stage smoothing significantly improves Poisson An asterisk (*) indicates that the difference between the performance of the term dependent two-stage smoothing and that of the Dirichlet/Gamma single smoothing is statistically significant according to the Wilcoxon signed rank test at the level of 0.05. or sensitivity.",
                "This similarity of performance is expected as we discussed in Section 3.1.",
                "Although the Poisson model and multinomial model are similar in terms of the basic model and/or with simple smoothing methods, the Poisson model has great potential and flexibility to be further improved.",
                "As shown in the rightmost column of Table 1, term dependent two-stage Poisson model consistently outperforms the basic smoothing models, especially for verbose queries.",
                "This model is given in Formula 4, with a Gamma smoothing for the document model p(·|d), and δw, which is term dependent.",
                "The parameter µ of the first stage Gamma smoothing is empirically tuned.",
                "The combination coefficients (i.e., ∆), are estimated with the EM algorithm in Section 3.2.",
                "The parameter sensitivity curves for Dirichlet/Gamma and the per-term two-stage smoothing model are plotted in Figure 2.",
                "The per-term two-stage smoothing method is less sensitive to the parameter µ than Dirichlet/Gamma, and yields better optimal performance. 0 1000 2000 3000 4000 5000 6000 7000 8000 9000 10000 0.1 0.12 0.14 0.16 0.18 0.2 0.22 Dataset: AP; Query Type: SV Parameter: µ AveragePrecision Dirichlet/Gamma Smoothing Term Dependent 2−Stage Figure 2: Term dependent two-stage smoothing of Poisson outperforms Dirichlet/Gamma In the following subsections, we conduct experiments to demonstrate how the flexibility of the Poisson model could be utilized to achieve better performance, which we cannot achieve with multinomial language models. 4.3 Term Dependent Smoothing To test the effectiveness of the term dependent smoothing, we conduct the following two experiments.",
                "In the first experiment, we relax the constant coefficient in the simple Jelinek-Mercer smoothing formula (i.e., Formula 3), and use the EM algorithm proposed in Section 3.2 to find a δw for each unique term.",
                "Since we are using the EM algorithm to iteratively estimate the parameters, we usually do not want the probability of p(·|d) to be zero.",
                "We then use a simple Laplace method to slightly smooth the document model before it goes into the EM iterations.",
                "The documents are then still scored with Formula 3, but using learnt δw.",
                "The results are labeled with JM+L. in Table 2.",
                "Data Q JM JM JM+L. 2-Stage 2-Stage (MAP) PT: No Yes Yes No Yes AP SK 0.203 0.204 0.206 0.223 0.226* SV 0.183 0.189 0.214* 0.204 0.217* Trec7 SK 0.168 0.171 0.174 0.186 0.185 SV 0.176 0.147 0.198* 0.194 0.196 Trec8 SK 0.239 0.240 0.227* 0.257 0.256 SV 0.234 0.223 0.249* 0.242 0.246* Web SK 0.250 0.236 0.220* 0.291 0.307* SV 0.217 0.232 0.261* 0.273 0.292* Table 2: Term dependent smoothing improves retrieval performance An asterisk (*) in Column 3 indicates that the difference between the JM+L. method and JM method is statistically significant; an asterisk (*) in Column 5 means that the difference between term dependent two-stage method and query dependent two-stage method is statistically significant; PT stands for per-term.",
                "With term dependent coefficients, the performance of the Jelinek-Mercer Poisson model is improved in most cases.",
                "However, in some cases (e.g., Trec7/SV), it performs poorly.",
                "This might be caused by the problem of EM estimation with unsmoothed document models.",
                "Once non-zero probability is assigned to all the terms before entering the EM iteration, the performance on verbose queries can be improved significantly.",
                "This indicates that there is still room to find better methods to estimate δw.",
                "Please note that neither the perterm JM method nor the JM+L. method has a parameter to tune.",
                "As shown in Table 1, the term dependent two-stage smoothing can significantly improve retrieval performance.",
                "To understand whether the improvement is contributed by the term dependent smoothing or the two-stage smoothing framework, we design another experiment to compare the perterm two-stage smoothing with the two-stage smoothing method proposed in [29].",
                "Their method managed to find coefficients specific to the query, thus a verbose query would use a higher δ.",
                "However, since their model is based on multinomial language modeling, they could not get per-term coefficients.",
                "We adopt their method to the Poisson two-stage smoothing, and also estimate a per-query coefficient for all the terms.",
                "We compare the performance of such a model with the per-term two-stage smoothing model, and present the results in the right two columns in Table 2.",
                "Again, we see that the per-term two-stage smoothing outperforms the per-query two-stage smoothing, especially for verbose queries.",
                "The improvement is not as large as how the perterm smoothing method improves over Dirichlet/Gamma.",
                "This is expected, since the per-query smoothing has already addressed the query discrimination problem to some extent.",
                "This experiment shows that even if the smoothing is already per-query, making it per-term is still beneficial.",
                "In brief, the per-term smoothing improved the retrieval performance of both one-stage and two-stage smoothing method. 4.4 Mixture Background Model In this section, we conduct experiments to examine the benefits of using a mixture background model without extra computational cost, which can not be achieved for multinomial models.",
                "Specifically, in retrieval formula 3, instead of using a single Poisson distribution to model the background p(·|C), we use Katzs K-Mixture model, which is essentially a mixture of Poisson distributions. p(·|C) can be computed efficiently with simple collection statistics, as discussed in Section 3.3.",
                "Data Query JM.",
                "Poisson JM.",
                "K-Mixture AP SK 0.203 0.204 SV 0.183 0.188* Trec-7 SK 0.168 0.169 SV 0.176 0.178* Trec-8 SK 0.239 0.239 SV 0.234 0.238* Web SK 0.250 0.250 SV 0.217 0.223* Table 3: K-Mixture background model improves retrieval performance The performance of the JM retrieval model with single Poisson background and with Katzs K-Mixture background model is compared in Table 3.",
                "Clearly, using K-Mixture to model the background model outperforms the single Poisson background model in most cases, especially for verbose queries where the improvement is statistically significant.",
                "Figure 3 shows that the performance changes over different parameters for short verbose queries.",
                "The model using K-Mixture background is less sensitive than the one using single Poisson background.",
                "Given that this type of mixture 0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1 0 0.05 0.1 0.15 0.2 0.25 Data: Trec8; Query: SV Parameter: δ AveragePrecision Poisson Background K−Mixture Background Figure 3: K-Mixture background model deviates the sensitivity of verbose queries background model does not require any extra computation cost, it would be interesting to study whether using other mixture Poisson models, such as 2-Poisson and negative Binomial, could help the performance. 5.",
                "RELATED WORK To the best of our knowledge, there has been no study of <br>query generation</br> models based on Poisson distribution.",
                "Language models have been shown to be effective for many retrieval tasks [21, 28, 14, 4].",
                "The most popular and fundamental one is the query-generation language model [21, 13].",
                "All existing <br>query generation</br> language models are based on either multinomial distribution [19, 6, 28, 13] or multivariate Bernoulli distribution [21, 17, 18].",
                "We introduce a new family of language models, based on Poisson distribution.",
                "Poisson distribution has been previously studied in the document generation models [16, 22, 3, 24], leading to the development of one of the most effective retrieval formula BM25 [23]. [24] studies the parallel derivation of three different retrieval models which is related to our comparison of Poisson and multinomial.",
                "However, the Poisson model in their paper is still under the document generation framework, and also does not account for the document length variation. [26] introduces a way to empirically search for an exponential model for the documents.",
                "Poisson mixtures [3] such as 2-Poisson [22], Negative multinomial, and Katzs KMixture [9] has shown to be effective to model and retrieve documents.",
                "Once again, none of this work explores Poisson distribution in the <br>query generation</br> framework.",
                "Language model smoothing [2, 28, 29] and background structures [15, 10, 25, 27] have been studied with multinomial language models. [7] analytically shows that term specific smoothing could be useful.",
                "We show that Poisson language model is natural to accommodate the per-term smoothing without heuristic twist of the semantics of a generative model, and is able to efficiently better model the mixture background, both analytically and empirically. 6.",
                "CONCLUSIONS We present a new family of <br>query generation</br> language models for retrieval based on Poisson distribution.",
                "We derive several smoothing methods for this family of models, including single-stage smoothing and two-stage smoothing.",
                "We compare the new models with the popular multinomial retrieval models both analytically and experimentally.",
                "Our analysis shows that while our new models and multinomial models are equivalent under some assumptions, they are generally different with some important differences.",
                "In particular, we show that Poisson has an advantage over multinomial in naturally accommodating per-term smoothing.",
                "We exploit this property to develop a new per-term smoothing algorithm for Poisson language models, which is shown to outperform term-independent smoothing for both Poisson and multinomial models.",
                "Furthermore, we show that a mixture background model for Poisson can be used to improve the performance and robustness over the standard Poisson background model.",
                "Our work opens up many interesting directions for further exploration in this new family of models.",
                "Further exploring the flexibilities over multinomial language models, such as length normalization and pseudo-feedback could be good future work.",
                "It is also appealing to find robust methods to learn the per-term smoothing coefficients without additional computation cost. 7.",
                "ACKNOWLEDGMENTS We thank the anonymous SIGIR 07 reviewers for their useful comments.",
                "This material is based in part upon work supported by the National Science Foundation under award numbers IIS-0347933 and 0425852. 8.",
                "REFERENCES [1] D. Blei, A. Ng, and M. Jordan.",
                "Latent dirichlet allocation.",
                "Journal of Machine Learning Research, 3:993-1022, 2003. [2] S. F. Chen and J. Goodman.",
                "An empirical study of smoothing techniques for language modeling.",
                "Technical Report TR-10-98, Harvard University, 1998. [3] K. Church and W. Gale.",
                "Poisson mixtures.",
                "Nat.",
                "Lang.",
                "Eng., 1(2):163-190, 1995. [4] W. B. Croft and J. Lafferty, editors.",
                "Language Modeling and Information Retrieval.",
                "Kluwer Academic Publishers, 2003. [5] H. Fang, T. Tao, and C. Zhai.",
                "A formal study of information retrieval heuristics.",
                "In Proceedings of the 27th annual international ACM SIGIR conference on Research and development in information retrieval, pages 49-56, 2004. [6] D. Hiemstra.",
                "Using Language Models for Information Retrieval.",
                "PhD thesis, University of Twente, Enschede, Netherlands, 2001. [7] D. Hiemstra.",
                "Term-specific smoothing for the language modeling approach to information retrieval: the importance of a query term.",
                "In Proceedings of the 25th annual international ACM SIGIR conference on Research and development in information retrieval, pages 35-41, 2002. [8] T. Hofmann.",
                "Probabilistic latent semantic indexing.",
                "In Proceedings of ACM SIGIR99, pages 50-57, 1999. [9] S. M. Katz.",
                "Distribution of content words and phrases in text and language modelling.",
                "Nat.",
                "Lang.",
                "Eng., 2(1):15-59, 1996. [10] O. Kurland and L. Lee.",
                "Corpus structure, language models, and ad-hoc information retrieval.",
                "In Proceedings of the 27th annual international ACM SIGIR conference on Research and development in information retrieval, pages 194-201, 2004. [11] J. Lafferty and C. Zhai.",
                "Document language models, query models, and risk minimization for information retrieval.",
                "In Proceedings of SIGIR01, pages 111-119, Sept 2001. [12] J. Lafferty and C. Zhai.",
                "Probabilistic IR models based on query and document generation.",
                "In Proceedings of the Language Modeling and IR workshop, pages 1-5, May 31 - June 1 2001. [13] J. Lafferty and C. Zhai.",
                "Probabilistic relevance models based on document and <br>query generation</br>.",
                "In W. B. Croft and J. Lafferty, editors, Language Modeling and Information Retrieval.",
                "Kluwer Academic Publishers, 2003. [14] V. Lavrenko and B. Croft.",
                "Relevance-based language models.",
                "In Proceedings of SIGIR01, pages 120-127, Sept 2001. [15] X. Liu and W. B. Croft.",
                "Cluster-based retrieval using language models.",
                "In Proceedings of the 27th annual international ACM SIGIR conference on Research and development in information retrieval, pages 186-193, 2004. [16] E. L. Margulis.",
                "Modelling documents with multiple poisson distributions.",
                "Inf.",
                "Process.",
                "Manage., 29(2):215-227, 1993. [17] A. McCallum and K. Nigam.",
                "A comparison of event models for naive bayes text classification.",
                "In Proceedings of AAAI-98 Workshop on Learning for Text Categorization, 1998. [18] D. Metzler, V. Lavrenko, and W. B. Croft.",
                "Formal multiple-bernoulli models for language modeling.",
                "In Proceedings of the 27th annual international ACM SIGIR conference on Research and development in information retrieval, pages 540-541, 2004. [19] D. H. Miller, T. Leek, and R. Schwartz.",
                "A hidden Markov model information retrieval system.",
                "In Proceedings of the 1999 ACM SIGIR Conference on Research and Development in Information Retrieval, pages 214-221, 1999. [20] A. Papoulis.",
                "Probability, random variables and stochastic processes.",
                "New York: McGraw-Hill, 1984, 2nd ed., 1984. [21] J. M. Ponte and W. B. Croft.",
                "A language modeling approach to information retrieval.",
                "In Proceedings of the 21st annual international ACM SIGIR conference on Research and development in information retrieval, pages 275-281, 1998. [22] S. Robertson and S. Walker.",
                "Some simple effective approximations to the 2-poisson model for probabilistic weighted retrieval.",
                "In Proceedings of SIGIR94, pages 232-241, 1994. [23] S. E. Robertson, S. Walker, S. Jones, M. M.Hancock-Beaulieu, and M. Gatford.",
                "Okapi at TREC-3.",
                "In D. K. Harman, editor, The Third Text REtrieval Conference (TREC-3), pages 109-126, 1995. [24] T. Roelleke and J. Wang.",
                "A parallel derivation of probabilistic information retrieval models.",
                "In Proceedings of the 29th annual international ACM SIGIR conference on Research and development in information retrieval, pages 107-114, 2006. [25] T. Tao, X. Wang, Q. Mei, and C. Zhai.",
                "Language model information retrieval with document expansion.",
                "In Proceedings of HLT/NAACL 2006, pages 407-414, 2006. [26] J. Teevan and D. R. Karger.",
                "Empirical development of an exponential probabilistic model for text retrieval: using textual analysis to build a better model.",
                "In Proceedings of the 26th annual international ACM SIGIR conference on Research and development in informaion retrieval, pages 18-25, 2003. [27] X. Wei and W. B. Croft.",
                "Lda-based document models for ad-hoc retrieval.",
                "In Proceedings of the 29th annual international ACM SIGIR conference on Research and development in information retrieval, pages 178-185, 2006. [28] C. Zhai and J. Lafferty.",
                "A study of smoothing methods for language models applied to ad-hoc information retrieval.",
                "In Proceedings of ACM SIGIR01, pages 334-342, Sept 2001. [29] C. Zhai and J. Lafferty.",
                "Two-stage language models for information retrieval.",
                "In Proceedings of ACM SIGIR02, pages 49-56, Aug 2002."
            ],
            "original_annotated_samples": [],
            "translated_annotated_samples": [
                "Un estudio del modelo de \"generación de consultas\" de Poisson para la recuperación de información Qiaozhu Mei, Hui Fang, Chengxiang Zhai Departamento de Ciencias de la Computación Universidad de Illinois en Urbana-Champaign Urbana, IL 61801 {QMEI2, Hfang, Czhai}@Uuc.edu Resumen de muchas variantes de las variantes de muchas variantes de las variantes de las variantes de muchas variantes deSe han propuesto modelos de idiomas para la recuperación de la información.generación de consultas",
                "La mayoría de los modelos existentes se basan en la distribución multinomial y obtendrían documentos basados en la probabilidad de consulta calculada en función de un modelo probabilístico de \"generación de consultas\".generación de consultas",
                "En este artículo, proponemos y estudiamos una nueva familia de modelos de \"generación de consultas\" basados en la distribución de Poisson.generación de consultas",
                "Prácticamente todos los modelos de lenguaje existentes de \"generación de consultas\" se basan en distribución multinomial [19, 6, 28] o distribución multivariada de Bernoulli [21, 18].generación de consultas",
                "En este artículo, proponemos y estudiamos una nueva familia de modelos de \"generación de consultas\" basados en la distribución de Poisson.generación de consultas",
                "En la Sección 2, presentamos la nueva familia de modelos de \"generación de consultas\" con distribución de Poisson, y presentamos varios métodos de suavizado que conducen a diferentes funciones de recuperación.generación de consultas",
                "\"Generación de consultas\" con el proceso Poisson en el marco \"Generación de consultas\", una suposición básica es que se genera una consulta con un modelo estimado en base a un documento.generación de consultas",
                "La probabilidad de que una consulta q se genere a partir del modelo de lenguaje de documento λd se puede escribir como p (q | d) = w∈V p (c (w, q) | λd) (1) Esta representación es claramente diferente de la multinomialmodelo de \"generación de consultas\" como (1) La probabilidad incluye todos los términos en el vocabulario V, en lugar de solo aquellos que aparecen en q, y (2) en lugar de la aparición de términos, el espacio de eventos de este modelo son las frecuencias de cada unotérmino.generación de consultas",
                "La distribución posterior de λd viene dada por p (λd | d | d | d | d | d | d | d | d | d | d | d | d | d | d | d | d | d | d | d | d | d | d | d | d | d | d | d | d | d | d | d | d | d | d | d | d | d | d | d | d | d | d | d | d | d | d | d | d | d | d | d | d |V |Distribuciones gamma con parámetros c (w, d) + µλc, w y | d |+ µ para cada palabra w.Dado que la media gamma es α β, tenemos ˆλd, w = λd, w λd, wp (λd, w | d, c) dλd, w = c (w, d) + µλc, w | d |+ µ Esta es precisamente la estimación suavizada del modelo de lenguaje multinomial con Dirichlet Prior [28].2.2.2 Interpolación (Jelinek-Mercer) suavizando otro método directo es descomponer el modelo de \"generación de consultas\" como una mezcla de dos modelos de componentes.generación de consultas",
                "Con modelos multinomiales de \"generación de consultas\", las personas generalmente usan un coeficiente de suavizado único para controlar la combinación del modelo de documento y el modelo de fondo [28, 29].generación de consultas",
                "Evaluación En la Sección 3, comparamos analíticamente los modelos de lenguaje Poisson y los modelos de lenguaje multinomial desde la perspectiva de la \"generación de consultas\" y la recuperación.generación de consultas",
                "Trabajo relacionado según nuestro lo mejor de nuestro conocimiento, no ha habido un estudio de los modelos de \"generación de consultas\" basados en la distribución de Poisson.generación de consultas",
                "Todos los modelos de lenguaje de \"generación de consultas\" existentes se basan en distribución multinomial [19, 6, 28, 13] o distribución multivariada de Bernoulli [21, 17, 18].generación de consultas",
                "Una vez más, ninguno de este trabajo explora la distribución de Poisson en el marco \"Generación de consultas\".generación de consultas",
                "Conclusiones presentamos una nueva familia de modelos de lenguaje de \"generación de consultas\" para la recuperación basados en la distribución de Poisson.generación de consultas",
                "Modelos de relevancia probabilística basados en documento y \"generación de consultas\".generación de consultas"
            ],
            "translated_text": "",
            "candidates": [],
            "error": []
        },
        "formal model": {
            "translated_key": "",
            "is_in_text": false,
            "original_annotated_sentences": [
                "A Study of Poisson Query Generation Model for Information Retrieval Qiaozhu Mei, Hui Fang, Chengxiang Zhai Department of Computer Science University of Illinois at Urbana-Champaign Urbana,IL 61801 {qmei2,hfang,czhai}@uiuc.edu ABSTRACT Many variants of language models have been proposed for information retrieval.",
                "Most existing models are based on multinomial distribution and would score documents based on query likelihood computed based on a query generation probabilistic model.",
                "In this paper, we propose and study a new family of query generation models based on Poisson distribution.",
                "We show that while in their simplest forms, the new family of models and the existing multinomial models are equivalent, they behave differently for many smoothing methods.",
                "We show that the Poisson model has several advantages over the multinomial model, including naturally accommodating per-term smoothing and allowing for more accurate background modeling.",
                "We present several variants of the new model corresponding to different smoothing methods, and evaluate them on four representative TREC test collections.",
                "The results show that while their basic models perform comparably, the Poisson model can outperform multinomial model with per-term smoothing.",
                "The performance can be further improved with two-stage smoothing.",
                "Categories and Subject Descriptors: H.3.3 [Information Search and Retrieval]: Retrieval Models General Terms: Algorithms 1.",
                "INTRODUCTION As a new type of probabilistic retrieval models, language models have been shown to be effective for many retrieval tasks [21, 28, 14, 4].",
                "Among many variants of language models proposed, the most popular and fundamental one is the query-generation language model [21, 13], which leads to the query-likelihood scoring method for ranking documents.",
                "In such a model, given a query q and a document d, we compute the likelihood of generating query q with a model estimated based on document d, i.e., the conditional probability p(q|d).",
                "We can then rank documents based on the likelihood of generating the query.",
                "Virtually all the existing query generation language models are based on either multinomial distribution [19, 6, 28] or multivariate Bernoulli distribution [21, 18].",
                "The multinomial distribution is especially popular and also shown to be quite effective.",
                "The heavy use of multinomial distribution is partly due to the fact that it has been successfully used in speech recognition, where multinomial distribution is a natural choice for modeling the occurrence of a particular word in a particular position in text.",
                "Compared with multivariate Bernoulli, multinomial distribution has the advantage of being able to model the frequency of terms in the query; in contrast, multivariate Bernoulli only models the presence and absence of query terms, thus cannot capture different frequencies of query terms.",
                "However, multivariate Bernoulli also has one potential advantage over multinomial from the viewpoint of retrieval: in a multinomial distribution, the probabilities of all the terms must sum to 1, making it hard to accommodate per-term smoothing, while in a multivariate Bernoulli, the presence probabilities of different terms are completely independent of each other, easily accommodating per-term smoothing and weighting.",
                "Note that term absence is also indirectly captured in a multinomial model through the constraint that all the term probabilities must sum to 1.",
                "In this paper, we propose and study a new family of query generation models based on the Poisson distribution.",
                "In this new family of models, we model the frequency of each term independently with a Poisson distribution.",
                "To score a document, we would first estimate a multivariate Poisson model based on the document, and then score it based on the likelihood of the query given by the estimated Poisson model.",
                "In some sense, the Poisson model combines the advantage of multinomial in modeling term frequency and the advantage of the multivariate Bernoulli in accommodating per-term smoothing.",
                "Indeed, similar to the multinomial distribution, the Poisson distribution models term frequencies, but without the constraint that all the term probabilities must sum to 1, and similar to multivariate Bernoulli, it models each term independently, thus can easily accommodate per-term smoothing.",
                "As in the existing work on multinomial language models, smoothing is critical for this new family of models.",
                "We derive several smoothing methods for Poisson model in parallel to those used for multinomial distributions, and compare the corresponding retrieval models with those based on multinomial distributions.",
                "We find that while with some smoothing methods, the new model and the multinomial model lead to exactly the same formula, with some other smoothing methods they diverge, and the Poisson model brings in more flexibility for smoothing.",
                "In particular, a key difference is that the Poisson model can naturally accommodate perterm smoothing, which is hard to achieve with a multinomial model without heuristic twist of the semantics of a generative model.",
                "We exploit this potential advantage to develop a new term-dependent smoothing algorithm for Poisson model and show that this new smoothing algorithm can improve performance over term-independent smoothing algorithms using either Poisson or multinomial model.",
                "This advantage is seen for both one-stage and two-stage smoothing.",
                "Another potential advantage of the Poisson model is that its corresponding background model for smoothing can be improved through using a mixture model that has a closed form formula.",
                "This new background model is shown to outperform the standard background model and reduce the sensitivity of retrieval performance to the smoothing parameter.",
                "The rest of the paper is organized as follows.",
                "In Section 2, we introduce the new family of query generation models with Poisson distribution, and present various smoothing methods which lead to different retrieval functions.",
                "In Section 3, we analytically compare the Poisson language model with the multinomial language model, from the perspective of retrieval.",
                "We then design empirical experiments to compare the two families of language models in Section 4.",
                "We discuss the related work in 5 and conclude in 6. 2.",
                "QUERY GENERATION WITH POISSON PROCESS In the query generation framework, a basic assumption is that a query is generated with a model estimated based on a document.",
                "In most existing work [12, 6, 28, 29], people assume that each query word is sampled independently from a multinomial distribution.",
                "Alternatively, we assume that a query is generated by sampling the frequency of words from a series of independent Poisson processes [20]. 2.1 The Generation Process Let V = {w1, ..., wn} be a vocabulary set.",
                "Let w be a piece of text composed by an author and c(w1), ..., c(wn) be a frequency vector representing w, where c(wi, w) is the frequency count of term wi in text w. In retrieval, w could be either a query or a document.",
                "We consider the frequency counts of the n unique terms in w as n different types of events, sampled from n independent homogeneous Poisson processes, respectively.",
                "Suppose t is the time period during which the author composed the text.",
                "With a homogeneous Poisson process, the frequency count of each event, i.e., the number of occurrences of wi, follows a Poisson distribution with associated parameter λit, where λi is a rate parameter characterizing the expected number of wi in a unit time.",
                "The probability density function of such a Poisson Distribution is given by P(c(wi, w) = k|λit) = e−λit (λit)k k!",
                "Without losing generality, we set t to the length of the text w (people write one word in a unit time), i.e., t = |w|.",
                "With n such independent Poisson processes, each explaining the generation of one term in the vocabulary, the likelihood of w to be generated from such Poisson processes can be written as p(w|Λ) = n i=1 p(c(wi, w)|Λ) = n i=1 e−λi·|w| (λi · |w|)c(wi,w) c(wi, w)! where Λ = {λ1, ..., λn} and |w| = n i=1 c(wi, w).",
                "We refer to these n independent Poisson processes with parameter Λ as a Poisson Language Model.",
                "Let D = {d1, ..., dm} be an observed set of document samples generated from the Poisson process above.",
                "The maximum likelihood estimate (MLE) of λi is ˆλi = d∈D c(wi, d) d∈D w ∈V c(w , d) Note that this MLE is different from the MLE for the Poisson distribution without considering the document lengths, which appears in [22, 24].",
                "Given a document d, we may estimate a Poisson language model Λd using d as a sample.",
                "The likelihood that a query q is generated from the document language model Λd can be written as p(q|d) = w∈V p(c(w, q)|Λd) (1) This representation is clearly different from the multinomial query generation model as (1) the likelihood includes all the terms in the vocabulary V , instead of only those appearing in q, and (2) instead of the appearance of terms, the event space of this model is the frequencies of each term.",
                "In practice, we have the flexibility to choose the vocabulary V .",
                "In one extreme, we can use the vocabulary of the whole collection.",
                "However, this may bring in noise and considerable computational cost.",
                "In the other extreme, we may focus on the terms in the query and ignore other terms, but some useful information may be lost by ignoring the nonquery terms.",
                "As a compromise, we may conflate all the non-query terms as one single pseudo term.",
                "In other words, we may assume that there is exactly one non-query term in the vocabulary for each query.",
                "In our experiments, we adopt this pseudo non-query term strategy.",
                "A document can be scored with the likelihood in Equation 1.",
                "However, if a query term is unseen in the document, the MLE of the Poisson distribution would assign zero probability to the term, causing the probability of the query to be zero.",
                "As in existing language modeling approaches, the main challenge of constructing a reasonable retrieval model is to find a smoothed language model for p(·|d). 2.2 Smoothing in Poisson Retrieval Model In general, we want to assign non-zero rates for the query terms that are not seen in document d. Many smoothing methods have been proposed for multinomial language models[2, 28, 29].",
                "In general, we have to discount the probabilities of some words seen in the text to leave some extra probability mass to assign to the unseen words.",
                "In Poisson language models, however, we do not have the same constraint as in a multinomial model (i.e., w∈V p(w|d) = 1).",
                "Thus we do not have to discount the probability of seen words in order to give a non-zero rate to an unseen word.",
                "Instead, we only need to guarantee that k=0,1,2,... p(c(w, d) = k|d) = 1.",
                "In this section, we introduce three different strategies to smooth a Poisson language model, and show how they lead to different retrieval functions. 2.2.1 Bayesian Smoothing using Gamma Prior Following the risk minimization framework in [11], we assume that a document is generated by the arrival of terms in a time period of |d| according to the document language model, which essentially consists of a vector of Poisson rates for each term, i.e., Λd = λd,1, ..., λd,|V | .",
                "A document is assumed to be generated from a potentially different model.",
                "Given a particular document d, we want to estimate Λd.",
                "The rate of a term is estimated independently of other terms.",
                "We use Bayesian estimation with the following Gamma prior, which has two parameters, α and β: Gamma(λ|α, β) = βα Γ(α) λα−1 e−βλ For each term w, the parameters αw and βw are chosen to be αw = µ ∗ λC,w and βw = µ, where µ is a parameter and λC,w is the rate of w estimated from some background language model, usually the collection language model.",
                "The posterior distribution of Λd is given by p(Λd|d, C) ∝ w∈V e−λw(|d|+µ) λ c(w,d)+µλC,w−1 w which is a product of |V | Gamma distributions with parameters c(w, d) + µλC,w and |d| + µ for each word w. Given that the Gamma mean is α β , we have ˆλd,w = λd,w λd,wp(λd,w|d, C)dλd,w = c(w, d) + µλC,w |d| + µ This is precisely the smoothed estimate of multinomial language model with Dirichlet prior [28]. 2.2.2 Interpolation (Jelinek-Mercer) Smoothing Another straightforward method is to decompose the query generation model as a mixture of two component models.",
                "One is the document language model estimated with maximum likelihood estimator, and the other is a model estimated from the collection background, p(·|C), which assigns non-zero rate to w. For example, we may use an interpolation coefficient between 0 and 1 (i.e., δ ∈ [0, 1]).",
                "With this simple interpolation, we can score a document with Score(d, q) = w∈V log((1 − δ)p(c(w, q)|d) + δp(c(w, q)|C)) (2) Using the maximum likelihood estimator for p(·|d), we have λd,w = c(w,d) |d| , thus Equation 2 becomes Score(d, q) ∝ w∈d∩q [log(1 + 1 − δ δ e−λd,w|q| (λd,w|q|)c(w,q) c(w, q)! · p(c(w, q)|C) ) − log (1 − δ)e−λd,w|q| + δp(c(w, q) = 0|C) 1 − δ + δp(c(w, q) = 0|C) ] + w∈d log (1 − δ)e−λd,w|q| + δp(c(w, q) = 0|C) 1 − δ + δp(c(w, q) = 0|C) We can also use a Poisson language model for p(·|C), or use some other frequency-based models.",
                "In the retrieval formula above, the first summation can be computed efficiently.",
                "The second summation can be actually treated as a document prior, which penalizes long documents.",
                "As the second summation is difficult to compute efficiently, we conflate all non-query terms as one pseudo non-queryterm, denoted as N. Using the pseudo-term formulation and a Poisson collection model, we can rewrite the retrieval formula as Score(d, q) ∝ w∈d∩q log(1 + 1 − δ δ e−λd,w (λd,w|q|)c(w,q) e−λd,C |q| (λd,C )c(w,q) ) + log (1 − δ)e−λd,N |q| + δe−λC,N |q| 1 − δ + δe−λC,N |q| (3) where λd,N = |d|− w∈q c(w,d) |d| and λC,N = |C|− w∈q c(w,C) |C| . 2.2.3 Two-Stage Smoothing As discussed in [29], smoothing plays two roles in retrieval: (1) to improve the estimation of the document language model, and (2) to explain the common terms in the query.",
                "In order to distinguish the content and non-discriminative words in a query, we follow [29] and assume that a query is generated by sampling from a two-component mixture of Poisson language models, with one component being the document model Λd and the other being a query background language model p(·|U). p(·|U) models the typical term frequencies in the users queries.",
                "We may then score each document with the query likelihood computed using the following two-stage smoothing model: p(c(w, q)|Λd, U) = (1 − δ)p(c(w, q)|Λd) + δp(c(w, q)|U) (4) where δ is a parameter, roughly indicating the amount of noise in q.",
                "This looks similar to the interpolation smoothing, except that p(·|Λd) now should be a smoothed language model, instead of the one estimated with MLE.",
                "With no prior knowledge on p(·|U), we could set it to p(·|C).",
                "Any smoothing methods for the document language model can be used to estimate p(·|d) such as the Gamma smoothing as discussed in Section 2.2.1.",
                "The empirical study of the smoothing methods is presented in Section 4. 3.",
                "ANALYSIS OF POISSON LANGUAGE MODEL From the previous section, we notice that the Poisson language model has a strong connection to the multinomial language model.",
                "This is expected since they both belong to the exponential family [26].",
                "However, there are many differences when these two families of models are applied with different smoothing methods.",
                "From the perspective of retrieval, will these two language models perform equivalently?",
                "If not, which model provides more benefits to retrieval, or provides flexibility which could lead to potential benefits?",
                "In this section, we analytically discuss the retrieval features of the Poisson language models, by comparing their behavior with that of the multinomial language models. 3.1 The Equivalence of Basic Models Let us begin with the assumption that all the query terms appear in every document.",
                "Under this assumption, no smoothing is needed.",
                "A document can be scored by the log likelihood of the query with the maximum likelihood estimate: Score(d, q) = w∈V log e−λd,w|q| (λd,w|q|)c(w,q) c(w, q)! (5) Using the MLE, we have λd,w = c(w,d) w∈V c(w,d) .",
                "Thus Score(d, q) ∝ c(w,q)>0 c(w, q) log c(w, d) w∈V c(w, d) This is exactly the log likelihood of the query if the document language model is a multinomial with maximum likelihood estimate.",
                "Indeed, even with Gamma smoothing, when plugging λd,w = c(w,d)+µλC,w |d|+µ and λC,w = c(w,C) |C| into Equation 5, it is easy to show that Score(d, q) ∝ w∈q∩d c(w, q) log(1 + c(w, d) µ · c(w,C) |C| ) + |q| log µ |d| + µ (6) which is exactly the Dirichlet retrieval formula in [28].",
                "Note that this equivalence holds only when the document length variation is modeled with Poisson process.",
                "This derivation indicates the equivalence of the basic Poisson and multinomial language models for retrieval.",
                "With other smoothing strategies, however, the two models would be different.",
                "Nevertheless, with this equivalence in basic models, we could expect that the Poisson language model performs comparably to the multinomial language model in retrieval, if only simple smoothing is explored.",
                "Based on this equivalence analysis, one may ask, why we should pursue the Poisson language model.",
                "In the following sections, we show that despite the equivalence in their basic models, the Poisson language model brings in extra flexibility for exploring advanced techniques on various retrieval features, which could not be achieved with multinomial language models. 3.2 Term Dependent Smoothing One flexibility of the Poisson language model is that it provides a natural framework to accommodate term dependent (per-term) smoothing.",
                "Existing work on language model smoothing has already shown that different types of queries should be smoothed differently according to how discriminative the query terms are. [7] also predicted that different terms should have a different smoothing weights.",
                "With multinomial query generation models, people usually use a single smoothing coefficient to control the combination of the document model and the background model [28, 29].",
                "This parameter can be made specific for different queries, but always has to be a constant for all the terms.",
                "This is mandatory since a multinomial language model has the constraint that w∈V p(w|d) = 1.",
                "However, from retrieval perspective, different terms may need to be smoothed differently even if they are in the same query.",
                "For example, a non-discriminative term (e.g., the, is) is expected to be explained more with the background model, while a content term (e.g., retrieval, bush) in the query should be explained with the document model.",
                "Therefore, a better way of smoothing would be to set the interpolation coefficient (i.e., δ in Formula 2 and Formula 3) specifically for each term.",
                "Since the Poisson language model does not have the sum-to-one constraint across terms, it can easily accommodate per-term smoothing without needing to heuristically twist the semantics of a generative model as in the case of multinomial language models.",
                "Below we present a possible way to explore term dependent smoothing with Poisson language models.",
                "Essentially, we want to use a term-specific smoothing coefficient δ in the linear combination, denoted as δw.",
                "This coefficient should intuitively be larger if w is a common word and smaller if it is a content word.",
                "The key problem is to find a method to assign reasonable values to δw.",
                "Empirical tuning is infeasible for so many parameters.",
                "We may instead estimate the parameters ∆ = {δ1, ..., δ|V |} by maximizing the likelihood of the query given the mixture model of p(q|ΛQ) and p(q|U), where ΛQ is the true query model to generate the query and p(q|U) is a query background model as discussed in Section 2.2.3.",
                "With the model p(q|ΛQ) hidden, the query likelihood is p(q|∆, U) = ΛQ w∈V ((1 − δw)p(c(w, q)|ΛQ) + δwp(c(w, q)|U))P(ΛQ|U)dΛQ If we have relevant documents for each query, we can approximate the query model space with the language models of all the relevant documents.",
                "Without relevant documents, we opt to approximate the query model space with the models of all the documents in the collection.",
                "Setting p(·|U) as p(·|C), the query likelihood becomes p(q|∆, U) = d∈C πd w∈V ((1−δw)p(c(w, q)|ˆΛd)+δwp(c(w, q)|C)) where πd = p(ˆΛd|U). p(·|ˆΛd) is an estimated Poisson language model for document d. If we have prior knowledge on p(ˆΛd|U), such as which documents are relevant to the query, we can set πd accordingly, because what we want is to find ∆ that can maximize the likelihood of the query given relevant documents.",
                "Without this prior knowledge, we can leave πd as free parameters, and use the EM algorithm to estimate πd and ∆.",
                "The updating functions are given as π (k+1) d = πd w∈V ((1 − δw)p(c(w, q)|ˆΛd) + δwp(c(w, q)|C)) d∈C πd w∈V ((1 − δw)p(c(w, q)|ˆΛd) + δwp(c(w, q)|C)) and δ (k+1) w = d∈C πd δwp(c(w, q)|C)) (1 − δw)p(c(w, q)|ˆΛd) + δwp(c(w, q)|C)) As discussed in [29], we only need to run the EM algorithm for several iterations, thus the computational cost is relatively low.",
                "We again assume our vocabulary containing all query terms plus a pseudo non-query term.",
                "Note that the function does not give an explicit way of estimating the coefficient for the unseen non-query term.",
                "In our experiments, we set it to the average over δw of all query terms.",
                "With this flexibility, we expect Poisson language models could improve the retrieval performance, especially for verbose queries, where the query terms have various discriminative values.",
                "In Section 4, we use empirical experiments to prove this hypothesis. 3.3 Mixture Background Models Another flexibility is to explore different background (collection) models (i.e., p(·|U), or p(·|C)).",
                "One common assumption made in language modeling information retrieval is that the background model is a homogeneous model of the document models [28, 29].",
                "Similarly, we can also make the assumption that the collection model is a Poisson language model, with the rates λC,w = d∈C c(w,d) |C| .",
                "However, this assumption usually does not hold, since the collection is far more complex than a single document.",
                "Indeed, the collection usually consists of a mixture of documents with various genres, authors, and topics, etc.",
                "Treating the collection model as a mixture of document models, instead of a single pseudo-document model is more reasonable.",
                "Existing work of multinomial language modeling has already shown that a better modeling of background improves the retrieval performance, such as clusters [15, 10], neighbor documents [25], and aspects [8, 27].",
                "All the approaches can be easily adopted using Poisson language models.",
                "However, a common problem of these approaches is that they all require heavy computation to construct the background model.",
                "With Poisson language modeling, we show that it is possible to model the mixture background without paying for the heavy computational cost.",
                "Poisson Mixture [3] has been proposed to model a collection of documents, which can fit the data much better than a single Poisson.",
                "The basic idea is to assume that the collection is generated from a mixture of Poisson models, which has the general form of p(x = k|PM) = λ p(λ)p(x = k|λ)dλ p(·|λ) is a single Poisson model and p(λ) is an arbitrary probability density function.",
                "There are three well known Poisson mixtures [3]: 2-Poisson, Negative Binomial, and the Katzs K-Mixture [9].",
                "Note that the 2-Poisson model has actually been explored in probabilistic retrieval models, which led to the well-known BM25 formula [22].",
                "All these mixtures have closed forms, and can be estimated from the collection of documents efficiently.",
                "This is an advantage over the multinomial mixture models, such as PLSI [8] and LDA [1], for retrieval.",
                "For example, the probability density function of Katzs K-Mixture is given as p(c(w) = k|αw, βw) = (1 − αw)ηk,0 + αw βw + 1 ( βw βw + 1 )k where ηk,0 = 1 when k = 0, and 0 otherwise.",
                "With the observation of a collection of documents, αw and βw can be estimated as βw = cf(w) − df(w) df(w) and αw = cf(w) Nβw where cf(w) and df(w) are the collection frequency and document frequency of w, and N is the number of documents in the collection.",
                "To account for the different document lengths, we assume that βw is a reasonable estimation for generating a document of the average length, and use β = βw avdl |q| to generate the query.",
                "This Poisson mixture model can be easily used to replace P(·|C) in the retrieval functions 3 and 4. 3.4 Other Possible Flexibilities In addition to term dependent smoothing and efficient mixture background, a Poisson language model has also some other potential advantages.",
                "For example, in Section 2, we see that Formula 2 introduces a component which does document length penalization.",
                "Intuitively, when the document has more unique words, it will be penalized more.",
                "On the other hand, if a document is exactly n copies of another document, it would not get over penalized.",
                "This feature is desirable and not achieved with the Dirichlet model [5].",
                "Potentially, this component could penalize a document according to what types of terms it contains.",
                "With term specific settings of δ, we could get even more flexibility for document length normalization.",
                "Pseudo-feedback is yet another interesting direction where the Poission model might be able to show its advantage.",
                "With model-based feedback, we could again relax the combination coefficients of the feedback model and the background model, and allow different terms to contribute differently to the feedback model.",
                "We could also utilize the relevant documents to learn better per-term smoothing coefficients. 4.",
                "EVALUATION In Section 3, we analytically compared the Poisson language models and multinomial language models from the perspective of query generation and retrieval.",
                "In this section, we compare these two families of models empirically.",
                "Experiment results show that the Poisson model with perterm smoothing outperforms multinomial model, and the performance can be further improved with two-stage smoothing.",
                "Using Poisson mixture as background model also improves the retrieval performance. 4.1 Datasets Since retrieval performance could significantly vary from one test collection to another, and from one query to another, we select four representative TREC test collections: AP, Trec7, Trec8, and Wt2g(Web).",
                "To cover different types of queries, we follow [28, 5], and construct short-keyword (SK, keyword title), short-verbose (SV, one sentence description), and long-verbose (LV, multiple sentences) queries.",
                "The documents are stemmed with the Porters stemmer, and we do not remove any stop word.",
                "For each parameter, we vary its value to cover a reasonably wide range. 4.2 Comparison to Multinomial We compare the performance of the Poisson retrieval models and multinomial retrieval models using interpolation (JelinekMercer, JM) smoothing and Bayesian smoothing with conjugate priors.",
                "Table 1 shows that the two JM-smoothed models perform similarly on all data sets.",
                "Since the Dirichlet Smoothing for multinomial language model and the Gamma Smoothing for Poisson language model lead to the same retrieval formula, the performance of these two models are jointly presented.",
                "We see that Dirichlet/Gamma smoothing methods outperform both Jelinek-Mercer smoothing methods.",
                "The parameter sensitivity curves for two Jelinek-Mercer 0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1 0 0.05 0.1 0.15 0.2 0.25 0.3 Dataset: Trec8 Parameter: δ AveragePrecision JM−Multinomial: LV JM−Multinomial: SV JM−Multinomial: SK JM−Poisson: SK JM−Poisson: SV JM−Poisson: LV Figure 1: Poisson and multinomial performs similarly with Jelinek-Mercer smoothing smoothing methods are shown in Figure 1.",
                "Clearly, these two methods perform similarly either in terms of optimality Data Query JM-Multinomial JM-Poisson Dirichlet/Gamma Per-term 2-Stage Poisson MAP InitPr Pr@5d MAP InitPr Pr@5d MAP InitPr Pr@5d MAP InitPr Pr@5d AP88-89 SK 0.203 0.585 0.356 0.203 0.585 0.358 0.224 0.629 0.393 0.226 0.630 0.396 SV 0.187 0.580 0.361 0.183 0.571 0.345 0.204 0.613 0.387 0.217* 0.603 0.390 LV 0.283 0.716 0.480 0.271 0.692 0.470 0.291 0.710 0.496 0.304* 0.695 0.510 Trec7 SK 0.167 0.635 0.400 0.168 0.635 0.404 0.186 0.687 0.428 0.185 0.646 0.436 SV 0.174 0.655 0.432 0.176 0.653 0.432 0.182 0.666 0.432 0.196* 0.660 0.440 LV 0.223 0.730 0.496 0.215 0.766 0.488 0.224 0.748 0.52 0.236* 0.738 0.512 Trec8 SK 0.239 0.621 0.440 0.239 0.621 0.436 0.257 0.718 0.496 0.256 0.704 0.468 SV 0.231 0.686 0.448 0.234 0.702 0.456 0.228 0.691 0.456 0.246* 0.692 0.476 LV 0.265 0.796 0.548 0.261 0.757 0.520 0.260 0.741 0.492 0.274* 0.766 0.508 Web SK 0.250 0.616 0.380 0.250 0.616 0.380 0.302 0.767 0.468 0.307 0.739 0.468 SV 0.214 0.611 0.392 0.217 0.609 0.384 0.273 0.693 0.508 0.292* 0.703 0.480 LV 0.266 0.790 0.464 0.259 0.776 0.452 0.283 0.756 0.496 0.311* 0.759 0.488 Table 1: Performance comparison between Poisson and Multinomial retrieval models: basic models perform comparably; term dependent two-stage smoothing significantly improves Poisson An asterisk (*) indicates that the difference between the performance of the term dependent two-stage smoothing and that of the Dirichlet/Gamma single smoothing is statistically significant according to the Wilcoxon signed rank test at the level of 0.05. or sensitivity.",
                "This similarity of performance is expected as we discussed in Section 3.1.",
                "Although the Poisson model and multinomial model are similar in terms of the basic model and/or with simple smoothing methods, the Poisson model has great potential and flexibility to be further improved.",
                "As shown in the rightmost column of Table 1, term dependent two-stage Poisson model consistently outperforms the basic smoothing models, especially for verbose queries.",
                "This model is given in Formula 4, with a Gamma smoothing for the document model p(·|d), and δw, which is term dependent.",
                "The parameter µ of the first stage Gamma smoothing is empirically tuned.",
                "The combination coefficients (i.e., ∆), are estimated with the EM algorithm in Section 3.2.",
                "The parameter sensitivity curves for Dirichlet/Gamma and the per-term two-stage smoothing model are plotted in Figure 2.",
                "The per-term two-stage smoothing method is less sensitive to the parameter µ than Dirichlet/Gamma, and yields better optimal performance. 0 1000 2000 3000 4000 5000 6000 7000 8000 9000 10000 0.1 0.12 0.14 0.16 0.18 0.2 0.22 Dataset: AP; Query Type: SV Parameter: µ AveragePrecision Dirichlet/Gamma Smoothing Term Dependent 2−Stage Figure 2: Term dependent two-stage smoothing of Poisson outperforms Dirichlet/Gamma In the following subsections, we conduct experiments to demonstrate how the flexibility of the Poisson model could be utilized to achieve better performance, which we cannot achieve with multinomial language models. 4.3 Term Dependent Smoothing To test the effectiveness of the term dependent smoothing, we conduct the following two experiments.",
                "In the first experiment, we relax the constant coefficient in the simple Jelinek-Mercer smoothing formula (i.e., Formula 3), and use the EM algorithm proposed in Section 3.2 to find a δw for each unique term.",
                "Since we are using the EM algorithm to iteratively estimate the parameters, we usually do not want the probability of p(·|d) to be zero.",
                "We then use a simple Laplace method to slightly smooth the document model before it goes into the EM iterations.",
                "The documents are then still scored with Formula 3, but using learnt δw.",
                "The results are labeled with JM+L. in Table 2.",
                "Data Q JM JM JM+L. 2-Stage 2-Stage (MAP) PT: No Yes Yes No Yes AP SK 0.203 0.204 0.206 0.223 0.226* SV 0.183 0.189 0.214* 0.204 0.217* Trec7 SK 0.168 0.171 0.174 0.186 0.185 SV 0.176 0.147 0.198* 0.194 0.196 Trec8 SK 0.239 0.240 0.227* 0.257 0.256 SV 0.234 0.223 0.249* 0.242 0.246* Web SK 0.250 0.236 0.220* 0.291 0.307* SV 0.217 0.232 0.261* 0.273 0.292* Table 2: Term dependent smoothing improves retrieval performance An asterisk (*) in Column 3 indicates that the difference between the JM+L. method and JM method is statistically significant; an asterisk (*) in Column 5 means that the difference between term dependent two-stage method and query dependent two-stage method is statistically significant; PT stands for per-term.",
                "With term dependent coefficients, the performance of the Jelinek-Mercer Poisson model is improved in most cases.",
                "However, in some cases (e.g., Trec7/SV), it performs poorly.",
                "This might be caused by the problem of EM estimation with unsmoothed document models.",
                "Once non-zero probability is assigned to all the terms before entering the EM iteration, the performance on verbose queries can be improved significantly.",
                "This indicates that there is still room to find better methods to estimate δw.",
                "Please note that neither the perterm JM method nor the JM+L. method has a parameter to tune.",
                "As shown in Table 1, the term dependent two-stage smoothing can significantly improve retrieval performance.",
                "To understand whether the improvement is contributed by the term dependent smoothing or the two-stage smoothing framework, we design another experiment to compare the perterm two-stage smoothing with the two-stage smoothing method proposed in [29].",
                "Their method managed to find coefficients specific to the query, thus a verbose query would use a higher δ.",
                "However, since their model is based on multinomial language modeling, they could not get per-term coefficients.",
                "We adopt their method to the Poisson two-stage smoothing, and also estimate a per-query coefficient for all the terms.",
                "We compare the performance of such a model with the per-term two-stage smoothing model, and present the results in the right two columns in Table 2.",
                "Again, we see that the per-term two-stage smoothing outperforms the per-query two-stage smoothing, especially for verbose queries.",
                "The improvement is not as large as how the perterm smoothing method improves over Dirichlet/Gamma.",
                "This is expected, since the per-query smoothing has already addressed the query discrimination problem to some extent.",
                "This experiment shows that even if the smoothing is already per-query, making it per-term is still beneficial.",
                "In brief, the per-term smoothing improved the retrieval performance of both one-stage and two-stage smoothing method. 4.4 Mixture Background Model In this section, we conduct experiments to examine the benefits of using a mixture background model without extra computational cost, which can not be achieved for multinomial models.",
                "Specifically, in retrieval formula 3, instead of using a single Poisson distribution to model the background p(·|C), we use Katzs K-Mixture model, which is essentially a mixture of Poisson distributions. p(·|C) can be computed efficiently with simple collection statistics, as discussed in Section 3.3.",
                "Data Query JM.",
                "Poisson JM.",
                "K-Mixture AP SK 0.203 0.204 SV 0.183 0.188* Trec-7 SK 0.168 0.169 SV 0.176 0.178* Trec-8 SK 0.239 0.239 SV 0.234 0.238* Web SK 0.250 0.250 SV 0.217 0.223* Table 3: K-Mixture background model improves retrieval performance The performance of the JM retrieval model with single Poisson background and with Katzs K-Mixture background model is compared in Table 3.",
                "Clearly, using K-Mixture to model the background model outperforms the single Poisson background model in most cases, especially for verbose queries where the improvement is statistically significant.",
                "Figure 3 shows that the performance changes over different parameters for short verbose queries.",
                "The model using K-Mixture background is less sensitive than the one using single Poisson background.",
                "Given that this type of mixture 0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1 0 0.05 0.1 0.15 0.2 0.25 Data: Trec8; Query: SV Parameter: δ AveragePrecision Poisson Background K−Mixture Background Figure 3: K-Mixture background model deviates the sensitivity of verbose queries background model does not require any extra computation cost, it would be interesting to study whether using other mixture Poisson models, such as 2-Poisson and negative Binomial, could help the performance. 5.",
                "RELATED WORK To the best of our knowledge, there has been no study of query generation models based on Poisson distribution.",
                "Language models have been shown to be effective for many retrieval tasks [21, 28, 14, 4].",
                "The most popular and fundamental one is the query-generation language model [21, 13].",
                "All existing query generation language models are based on either multinomial distribution [19, 6, 28, 13] or multivariate Bernoulli distribution [21, 17, 18].",
                "We introduce a new family of language models, based on Poisson distribution.",
                "Poisson distribution has been previously studied in the document generation models [16, 22, 3, 24], leading to the development of one of the most effective retrieval formula BM25 [23]. [24] studies the parallel derivation of three different retrieval models which is related to our comparison of Poisson and multinomial.",
                "However, the Poisson model in their paper is still under the document generation framework, and also does not account for the document length variation. [26] introduces a way to empirically search for an exponential model for the documents.",
                "Poisson mixtures [3] such as 2-Poisson [22], Negative multinomial, and Katzs KMixture [9] has shown to be effective to model and retrieve documents.",
                "Once again, none of this work explores Poisson distribution in the query generation framework.",
                "Language model smoothing [2, 28, 29] and background structures [15, 10, 25, 27] have been studied with multinomial language models. [7] analytically shows that term specific smoothing could be useful.",
                "We show that Poisson language model is natural to accommodate the per-term smoothing without heuristic twist of the semantics of a generative model, and is able to efficiently better model the mixture background, both analytically and empirically. 6.",
                "CONCLUSIONS We present a new family of query generation language models for retrieval based on Poisson distribution.",
                "We derive several smoothing methods for this family of models, including single-stage smoothing and two-stage smoothing.",
                "We compare the new models with the popular multinomial retrieval models both analytically and experimentally.",
                "Our analysis shows that while our new models and multinomial models are equivalent under some assumptions, they are generally different with some important differences.",
                "In particular, we show that Poisson has an advantage over multinomial in naturally accommodating per-term smoothing.",
                "We exploit this property to develop a new per-term smoothing algorithm for Poisson language models, which is shown to outperform term-independent smoothing for both Poisson and multinomial models.",
                "Furthermore, we show that a mixture background model for Poisson can be used to improve the performance and robustness over the standard Poisson background model.",
                "Our work opens up many interesting directions for further exploration in this new family of models.",
                "Further exploring the flexibilities over multinomial language models, such as length normalization and pseudo-feedback could be good future work.",
                "It is also appealing to find robust methods to learn the per-term smoothing coefficients without additional computation cost. 7.",
                "ACKNOWLEDGMENTS We thank the anonymous SIGIR 07 reviewers for their useful comments.",
                "This material is based in part upon work supported by the National Science Foundation under award numbers IIS-0347933 and 0425852. 8.",
                "REFERENCES [1] D. Blei, A. Ng, and M. Jordan.",
                "Latent dirichlet allocation.",
                "Journal of Machine Learning Research, 3:993-1022, 2003. [2] S. F. Chen and J. Goodman.",
                "An empirical study of smoothing techniques for language modeling.",
                "Technical Report TR-10-98, Harvard University, 1998. [3] K. Church and W. Gale.",
                "Poisson mixtures.",
                "Nat.",
                "Lang.",
                "Eng., 1(2):163-190, 1995. [4] W. B. Croft and J. Lafferty, editors.",
                "Language Modeling and Information Retrieval.",
                "Kluwer Academic Publishers, 2003. [5] H. Fang, T. Tao, and C. Zhai.",
                "A formal study of information retrieval heuristics.",
                "In Proceedings of the 27th annual international ACM SIGIR conference on Research and development in information retrieval, pages 49-56, 2004. [6] D. Hiemstra.",
                "Using Language Models for Information Retrieval.",
                "PhD thesis, University of Twente, Enschede, Netherlands, 2001. [7] D. Hiemstra.",
                "Term-specific smoothing for the language modeling approach to information retrieval: the importance of a query term.",
                "In Proceedings of the 25th annual international ACM SIGIR conference on Research and development in information retrieval, pages 35-41, 2002. [8] T. Hofmann.",
                "Probabilistic latent semantic indexing.",
                "In Proceedings of ACM SIGIR99, pages 50-57, 1999. [9] S. M. Katz.",
                "Distribution of content words and phrases in text and language modelling.",
                "Nat.",
                "Lang.",
                "Eng., 2(1):15-59, 1996. [10] O. Kurland and L. Lee.",
                "Corpus structure, language models, and ad-hoc information retrieval.",
                "In Proceedings of the 27th annual international ACM SIGIR conference on Research and development in information retrieval, pages 194-201, 2004. [11] J. Lafferty and C. Zhai.",
                "Document language models, query models, and risk minimization for information retrieval.",
                "In Proceedings of SIGIR01, pages 111-119, Sept 2001. [12] J. Lafferty and C. Zhai.",
                "Probabilistic IR models based on query and document generation.",
                "In Proceedings of the Language Modeling and IR workshop, pages 1-5, May 31 - June 1 2001. [13] J. Lafferty and C. Zhai.",
                "Probabilistic relevance models based on document and query generation.",
                "In W. B. Croft and J. Lafferty, editors, Language Modeling and Information Retrieval.",
                "Kluwer Academic Publishers, 2003. [14] V. Lavrenko and B. Croft.",
                "Relevance-based language models.",
                "In Proceedings of SIGIR01, pages 120-127, Sept 2001. [15] X. Liu and W. B. Croft.",
                "Cluster-based retrieval using language models.",
                "In Proceedings of the 27th annual international ACM SIGIR conference on Research and development in information retrieval, pages 186-193, 2004. [16] E. L. Margulis.",
                "Modelling documents with multiple poisson distributions.",
                "Inf.",
                "Process.",
                "Manage., 29(2):215-227, 1993. [17] A. McCallum and K. Nigam.",
                "A comparison of event models for naive bayes text classification.",
                "In Proceedings of AAAI-98 Workshop on Learning for Text Categorization, 1998. [18] D. Metzler, V. Lavrenko, and W. B. Croft.",
                "Formal multiple-bernoulli models for language modeling.",
                "In Proceedings of the 27th annual international ACM SIGIR conference on Research and development in information retrieval, pages 540-541, 2004. [19] D. H. Miller, T. Leek, and R. Schwartz.",
                "A hidden Markov model information retrieval system.",
                "In Proceedings of the 1999 ACM SIGIR Conference on Research and Development in Information Retrieval, pages 214-221, 1999. [20] A. Papoulis.",
                "Probability, random variables and stochastic processes.",
                "New York: McGraw-Hill, 1984, 2nd ed., 1984. [21] J. M. Ponte and W. B. Croft.",
                "A language modeling approach to information retrieval.",
                "In Proceedings of the 21st annual international ACM SIGIR conference on Research and development in information retrieval, pages 275-281, 1998. [22] S. Robertson and S. Walker.",
                "Some simple effective approximations to the 2-poisson model for probabilistic weighted retrieval.",
                "In Proceedings of SIGIR94, pages 232-241, 1994. [23] S. E. Robertson, S. Walker, S. Jones, M. M.Hancock-Beaulieu, and M. Gatford.",
                "Okapi at TREC-3.",
                "In D. K. Harman, editor, The Third Text REtrieval Conference (TREC-3), pages 109-126, 1995. [24] T. Roelleke and J. Wang.",
                "A parallel derivation of probabilistic information retrieval models.",
                "In Proceedings of the 29th annual international ACM SIGIR conference on Research and development in information retrieval, pages 107-114, 2006. [25] T. Tao, X. Wang, Q. Mei, and C. Zhai.",
                "Language model information retrieval with document expansion.",
                "In Proceedings of HLT/NAACL 2006, pages 407-414, 2006. [26] J. Teevan and D. R. Karger.",
                "Empirical development of an exponential probabilistic model for text retrieval: using textual analysis to build a better model.",
                "In Proceedings of the 26th annual international ACM SIGIR conference on Research and development in informaion retrieval, pages 18-25, 2003. [27] X. Wei and W. B. Croft.",
                "Lda-based document models for ad-hoc retrieval.",
                "In Proceedings of the 29th annual international ACM SIGIR conference on Research and development in information retrieval, pages 178-185, 2006. [28] C. Zhai and J. Lafferty.",
                "A study of smoothing methods for language models applied to ad-hoc information retrieval.",
                "In Proceedings of ACM SIGIR01, pages 334-342, Sept 2001. [29] C. Zhai and J. Lafferty.",
                "Two-stage language models for information retrieval.",
                "In Proceedings of ACM SIGIR02, pages 49-56, Aug 2002."
            ],
            "original_annotated_samples": [],
            "translated_annotated_samples": [],
            "translated_text": "",
            "candidates": [],
            "error": []
        },
        "term dependent smooth": {
            "translated_key": "",
            "is_in_text": true,
            "original_annotated_sentences": [
                "A Study of Poisson Query Generation Model for Information Retrieval Qiaozhu Mei, Hui Fang, Chengxiang Zhai Department of Computer Science University of Illinois at Urbana-Champaign Urbana,IL 61801 {qmei2,hfang,czhai}@uiuc.edu ABSTRACT Many variants of language models have been proposed for information retrieval.",
                "Most existing models are based on multinomial distribution and would score documents based on query likelihood computed based on a query generation probabilistic model.",
                "In this paper, we propose and study a new family of query generation models based on Poisson distribution.",
                "We show that while in their simplest forms, the new family of models and the existing multinomial models are equivalent, they behave differently for many smoothing methods.",
                "We show that the Poisson model has several advantages over the multinomial model, including naturally accommodating per-term smoothing and allowing for more accurate background modeling.",
                "We present several variants of the new model corresponding to different smoothing methods, and evaluate them on four representative TREC test collections.",
                "The results show that while their basic models perform comparably, the Poisson model can outperform multinomial model with per-term smoothing.",
                "The performance can be further improved with two-stage smoothing.",
                "Categories and Subject Descriptors: H.3.3 [Information Search and Retrieval]: Retrieval Models General Terms: Algorithms 1.",
                "INTRODUCTION As a new type of probabilistic retrieval models, language models have been shown to be effective for many retrieval tasks [21, 28, 14, 4].",
                "Among many variants of language models proposed, the most popular and fundamental one is the query-generation language model [21, 13], which leads to the query-likelihood scoring method for ranking documents.",
                "In such a model, given a query q and a document d, we compute the likelihood of generating query q with a model estimated based on document d, i.e., the conditional probability p(q|d).",
                "We can then rank documents based on the likelihood of generating the query.",
                "Virtually all the existing query generation language models are based on either multinomial distribution [19, 6, 28] or multivariate Bernoulli distribution [21, 18].",
                "The multinomial distribution is especially popular and also shown to be quite effective.",
                "The heavy use of multinomial distribution is partly due to the fact that it has been successfully used in speech recognition, where multinomial distribution is a natural choice for modeling the occurrence of a particular word in a particular position in text.",
                "Compared with multivariate Bernoulli, multinomial distribution has the advantage of being able to model the frequency of terms in the query; in contrast, multivariate Bernoulli only models the presence and absence of query terms, thus cannot capture different frequencies of query terms.",
                "However, multivariate Bernoulli also has one potential advantage over multinomial from the viewpoint of retrieval: in a multinomial distribution, the probabilities of all the terms must sum to 1, making it hard to accommodate per-term smoothing, while in a multivariate Bernoulli, the presence probabilities of different terms are completely independent of each other, easily accommodating per-term smoothing and weighting.",
                "Note that term absence is also indirectly captured in a multinomial model through the constraint that all the term probabilities must sum to 1.",
                "In this paper, we propose and study a new family of query generation models based on the Poisson distribution.",
                "In this new family of models, we model the frequency of each term independently with a Poisson distribution.",
                "To score a document, we would first estimate a multivariate Poisson model based on the document, and then score it based on the likelihood of the query given by the estimated Poisson model.",
                "In some sense, the Poisson model combines the advantage of multinomial in modeling term frequency and the advantage of the multivariate Bernoulli in accommodating per-term smoothing.",
                "Indeed, similar to the multinomial distribution, the Poisson distribution models term frequencies, but without the constraint that all the term probabilities must sum to 1, and similar to multivariate Bernoulli, it models each term independently, thus can easily accommodate per-term smoothing.",
                "As in the existing work on multinomial language models, smoothing is critical for this new family of models.",
                "We derive several smoothing methods for Poisson model in parallel to those used for multinomial distributions, and compare the corresponding retrieval models with those based on multinomial distributions.",
                "We find that while with some smoothing methods, the new model and the multinomial model lead to exactly the same formula, with some other smoothing methods they diverge, and the Poisson model brings in more flexibility for smoothing.",
                "In particular, a key difference is that the Poisson model can naturally accommodate perterm smoothing, which is hard to achieve with a multinomial model without heuristic twist of the semantics of a generative model.",
                "We exploit this potential advantage to develop a new term-dependent smoothing algorithm for Poisson model and show that this new smoothing algorithm can improve performance over term-independent smoothing algorithms using either Poisson or multinomial model.",
                "This advantage is seen for both one-stage and two-stage smoothing.",
                "Another potential advantage of the Poisson model is that its corresponding background model for smoothing can be improved through using a mixture model that has a closed form formula.",
                "This new background model is shown to outperform the standard background model and reduce the sensitivity of retrieval performance to the smoothing parameter.",
                "The rest of the paper is organized as follows.",
                "In Section 2, we introduce the new family of query generation models with Poisson distribution, and present various smoothing methods which lead to different retrieval functions.",
                "In Section 3, we analytically compare the Poisson language model with the multinomial language model, from the perspective of retrieval.",
                "We then design empirical experiments to compare the two families of language models in Section 4.",
                "We discuss the related work in 5 and conclude in 6. 2.",
                "QUERY GENERATION WITH POISSON PROCESS In the query generation framework, a basic assumption is that a query is generated with a model estimated based on a document.",
                "In most existing work [12, 6, 28, 29], people assume that each query word is sampled independently from a multinomial distribution.",
                "Alternatively, we assume that a query is generated by sampling the frequency of words from a series of independent Poisson processes [20]. 2.1 The Generation Process Let V = {w1, ..., wn} be a vocabulary set.",
                "Let w be a piece of text composed by an author and c(w1), ..., c(wn) be a frequency vector representing w, where c(wi, w) is the frequency count of term wi in text w. In retrieval, w could be either a query or a document.",
                "We consider the frequency counts of the n unique terms in w as n different types of events, sampled from n independent homogeneous Poisson processes, respectively.",
                "Suppose t is the time period during which the author composed the text.",
                "With a homogeneous Poisson process, the frequency count of each event, i.e., the number of occurrences of wi, follows a Poisson distribution with associated parameter λit, where λi is a rate parameter characterizing the expected number of wi in a unit time.",
                "The probability density function of such a Poisson Distribution is given by P(c(wi, w) = k|λit) = e−λit (λit)k k!",
                "Without losing generality, we set t to the length of the text w (people write one word in a unit time), i.e., t = |w|.",
                "With n such independent Poisson processes, each explaining the generation of one term in the vocabulary, the likelihood of w to be generated from such Poisson processes can be written as p(w|Λ) = n i=1 p(c(wi, w)|Λ) = n i=1 e−λi·|w| (λi · |w|)c(wi,w) c(wi, w)! where Λ = {λ1, ..., λn} and |w| = n i=1 c(wi, w).",
                "We refer to these n independent Poisson processes with parameter Λ as a Poisson Language Model.",
                "Let D = {d1, ..., dm} be an observed set of document samples generated from the Poisson process above.",
                "The maximum likelihood estimate (MLE) of λi is ˆλi = d∈D c(wi, d) d∈D w ∈V c(w , d) Note that this MLE is different from the MLE for the Poisson distribution without considering the document lengths, which appears in [22, 24].",
                "Given a document d, we may estimate a Poisson language model Λd using d as a sample.",
                "The likelihood that a query q is generated from the document language model Λd can be written as p(q|d) = w∈V p(c(w, q)|Λd) (1) This representation is clearly different from the multinomial query generation model as (1) the likelihood includes all the terms in the vocabulary V , instead of only those appearing in q, and (2) instead of the appearance of terms, the event space of this model is the frequencies of each term.",
                "In practice, we have the flexibility to choose the vocabulary V .",
                "In one extreme, we can use the vocabulary of the whole collection.",
                "However, this may bring in noise and considerable computational cost.",
                "In the other extreme, we may focus on the terms in the query and ignore other terms, but some useful information may be lost by ignoring the nonquery terms.",
                "As a compromise, we may conflate all the non-query terms as one single pseudo term.",
                "In other words, we may assume that there is exactly one non-query term in the vocabulary for each query.",
                "In our experiments, we adopt this pseudo non-query term strategy.",
                "A document can be scored with the likelihood in Equation 1.",
                "However, if a query term is unseen in the document, the MLE of the Poisson distribution would assign zero probability to the term, causing the probability of the query to be zero.",
                "As in existing language modeling approaches, the main challenge of constructing a reasonable retrieval model is to find a smoothed language model for p(·|d). 2.2 Smoothing in Poisson Retrieval Model In general, we want to assign non-zero rates for the query terms that are not seen in document d. Many smoothing methods have been proposed for multinomial language models[2, 28, 29].",
                "In general, we have to discount the probabilities of some words seen in the text to leave some extra probability mass to assign to the unseen words.",
                "In Poisson language models, however, we do not have the same constraint as in a multinomial model (i.e., w∈V p(w|d) = 1).",
                "Thus we do not have to discount the probability of seen words in order to give a non-zero rate to an unseen word.",
                "Instead, we only need to guarantee that k=0,1,2,... p(c(w, d) = k|d) = 1.",
                "In this section, we introduce three different strategies to smooth a Poisson language model, and show how they lead to different retrieval functions. 2.2.1 Bayesian Smoothing using Gamma Prior Following the risk minimization framework in [11], we assume that a document is generated by the arrival of terms in a time period of |d| according to the document language model, which essentially consists of a vector of Poisson rates for each term, i.e., Λd = λd,1, ..., λd,|V | .",
                "A document is assumed to be generated from a potentially different model.",
                "Given a particular document d, we want to estimate Λd.",
                "The rate of a term is estimated independently of other terms.",
                "We use Bayesian estimation with the following Gamma prior, which has two parameters, α and β: Gamma(λ|α, β) = βα Γ(α) λα−1 e−βλ For each term w, the parameters αw and βw are chosen to be αw = µ ∗ λC,w and βw = µ, where µ is a parameter and λC,w is the rate of w estimated from some background language model, usually the collection language model.",
                "The posterior distribution of Λd is given by p(Λd|d, C) ∝ w∈V e−λw(|d|+µ) λ c(w,d)+µλC,w−1 w which is a product of |V | Gamma distributions with parameters c(w, d) + µλC,w and |d| + µ for each word w. Given that the Gamma mean is α β , we have ˆλd,w = λd,w λd,wp(λd,w|d, C)dλd,w = c(w, d) + µλC,w |d| + µ This is precisely the smoothed estimate of multinomial language model with Dirichlet prior [28]. 2.2.2 Interpolation (Jelinek-Mercer) Smoothing Another straightforward method is to decompose the query generation model as a mixture of two component models.",
                "One is the document language model estimated with maximum likelihood estimator, and the other is a model estimated from the collection background, p(·|C), which assigns non-zero rate to w. For example, we may use an interpolation coefficient between 0 and 1 (i.e., δ ∈ [0, 1]).",
                "With this simple interpolation, we can score a document with Score(d, q) = w∈V log((1 − δ)p(c(w, q)|d) + δp(c(w, q)|C)) (2) Using the maximum likelihood estimator for p(·|d), we have λd,w = c(w,d) |d| , thus Equation 2 becomes Score(d, q) ∝ w∈d∩q [log(1 + 1 − δ δ e−λd,w|q| (λd,w|q|)c(w,q) c(w, q)! · p(c(w, q)|C) ) − log (1 − δ)e−λd,w|q| + δp(c(w, q) = 0|C) 1 − δ + δp(c(w, q) = 0|C) ] + w∈d log (1 − δ)e−λd,w|q| + δp(c(w, q) = 0|C) 1 − δ + δp(c(w, q) = 0|C) We can also use a Poisson language model for p(·|C), or use some other frequency-based models.",
                "In the retrieval formula above, the first summation can be computed efficiently.",
                "The second summation can be actually treated as a document prior, which penalizes long documents.",
                "As the second summation is difficult to compute efficiently, we conflate all non-query terms as one pseudo non-queryterm, denoted as N. Using the pseudo-term formulation and a Poisson collection model, we can rewrite the retrieval formula as Score(d, q) ∝ w∈d∩q log(1 + 1 − δ δ e−λd,w (λd,w|q|)c(w,q) e−λd,C |q| (λd,C )c(w,q) ) + log (1 − δ)e−λd,N |q| + δe−λC,N |q| 1 − δ + δe−λC,N |q| (3) where λd,N = |d|− w∈q c(w,d) |d| and λC,N = |C|− w∈q c(w,C) |C| . 2.2.3 Two-Stage Smoothing As discussed in [29], smoothing plays two roles in retrieval: (1) to improve the estimation of the document language model, and (2) to explain the common terms in the query.",
                "In order to distinguish the content and non-discriminative words in a query, we follow [29] and assume that a query is generated by sampling from a two-component mixture of Poisson language models, with one component being the document model Λd and the other being a query background language model p(·|U). p(·|U) models the typical term frequencies in the users queries.",
                "We may then score each document with the query likelihood computed using the following two-stage smoothing model: p(c(w, q)|Λd, U) = (1 − δ)p(c(w, q)|Λd) + δp(c(w, q)|U) (4) where δ is a parameter, roughly indicating the amount of noise in q.",
                "This looks similar to the interpolation smoothing, except that p(·|Λd) now should be a smoothed language model, instead of the one estimated with MLE.",
                "With no prior knowledge on p(·|U), we could set it to p(·|C).",
                "Any smoothing methods for the document language model can be used to estimate p(·|d) such as the Gamma smoothing as discussed in Section 2.2.1.",
                "The empirical study of the smoothing methods is presented in Section 4. 3.",
                "ANALYSIS OF POISSON LANGUAGE MODEL From the previous section, we notice that the Poisson language model has a strong connection to the multinomial language model.",
                "This is expected since they both belong to the exponential family [26].",
                "However, there are many differences when these two families of models are applied with different smoothing methods.",
                "From the perspective of retrieval, will these two language models perform equivalently?",
                "If not, which model provides more benefits to retrieval, or provides flexibility which could lead to potential benefits?",
                "In this section, we analytically discuss the retrieval features of the Poisson language models, by comparing their behavior with that of the multinomial language models. 3.1 The Equivalence of Basic Models Let us begin with the assumption that all the query terms appear in every document.",
                "Under this assumption, no smoothing is needed.",
                "A document can be scored by the log likelihood of the query with the maximum likelihood estimate: Score(d, q) = w∈V log e−λd,w|q| (λd,w|q|)c(w,q) c(w, q)! (5) Using the MLE, we have λd,w = c(w,d) w∈V c(w,d) .",
                "Thus Score(d, q) ∝ c(w,q)>0 c(w, q) log c(w, d) w∈V c(w, d) This is exactly the log likelihood of the query if the document language model is a multinomial with maximum likelihood estimate.",
                "Indeed, even with Gamma smoothing, when plugging λd,w = c(w,d)+µλC,w |d|+µ and λC,w = c(w,C) |C| into Equation 5, it is easy to show that Score(d, q) ∝ w∈q∩d c(w, q) log(1 + c(w, d) µ · c(w,C) |C| ) + |q| log µ |d| + µ (6) which is exactly the Dirichlet retrieval formula in [28].",
                "Note that this equivalence holds only when the document length variation is modeled with Poisson process.",
                "This derivation indicates the equivalence of the basic Poisson and multinomial language models for retrieval.",
                "With other smoothing strategies, however, the two models would be different.",
                "Nevertheless, with this equivalence in basic models, we could expect that the Poisson language model performs comparably to the multinomial language model in retrieval, if only simple smoothing is explored.",
                "Based on this equivalence analysis, one may ask, why we should pursue the Poisson language model.",
                "In the following sections, we show that despite the equivalence in their basic models, the Poisson language model brings in extra flexibility for exploring advanced techniques on various retrieval features, which could not be achieved with multinomial language models. 3.2 Term Dependent Smoothing One flexibility of the Poisson language model is that it provides a natural framework to accommodate term dependent (per-term) smoothing.",
                "Existing work on language model smoothing has already shown that different types of queries should be smoothed differently according to how discriminative the query terms are. [7] also predicted that different terms should have a different smoothing weights.",
                "With multinomial query generation models, people usually use a single smoothing coefficient to control the combination of the document model and the background model [28, 29].",
                "This parameter can be made specific for different queries, but always has to be a constant for all the terms.",
                "This is mandatory since a multinomial language model has the constraint that w∈V p(w|d) = 1.",
                "However, from retrieval perspective, different terms may need to be smoothed differently even if they are in the same query.",
                "For example, a non-discriminative term (e.g., the, is) is expected to be explained more with the background model, while a content term (e.g., retrieval, bush) in the query should be explained with the document model.",
                "Therefore, a better way of smoothing would be to set the interpolation coefficient (i.e., δ in Formula 2 and Formula 3) specifically for each term.",
                "Since the Poisson language model does not have the sum-to-one constraint across terms, it can easily accommodate per-term smoothing without needing to heuristically twist the semantics of a generative model as in the case of multinomial language models.",
                "Below we present a possible way to explore <br>term dependent smooth</br>ing with Poisson language models.",
                "Essentially, we want to use a term-specific smoothing coefficient δ in the linear combination, denoted as δw.",
                "This coefficient should intuitively be larger if w is a common word and smaller if it is a content word.",
                "The key problem is to find a method to assign reasonable values to δw.",
                "Empirical tuning is infeasible for so many parameters.",
                "We may instead estimate the parameters ∆ = {δ1, ..., δ|V |} by maximizing the likelihood of the query given the mixture model of p(q|ΛQ) and p(q|U), where ΛQ is the true query model to generate the query and p(q|U) is a query background model as discussed in Section 2.2.3.",
                "With the model p(q|ΛQ) hidden, the query likelihood is p(q|∆, U) = ΛQ w∈V ((1 − δw)p(c(w, q)|ΛQ) + δwp(c(w, q)|U))P(ΛQ|U)dΛQ If we have relevant documents for each query, we can approximate the query model space with the language models of all the relevant documents.",
                "Without relevant documents, we opt to approximate the query model space with the models of all the documents in the collection.",
                "Setting p(·|U) as p(·|C), the query likelihood becomes p(q|∆, U) = d∈C πd w∈V ((1−δw)p(c(w, q)|ˆΛd)+δwp(c(w, q)|C)) where πd = p(ˆΛd|U). p(·|ˆΛd) is an estimated Poisson language model for document d. If we have prior knowledge on p(ˆΛd|U), such as which documents are relevant to the query, we can set πd accordingly, because what we want is to find ∆ that can maximize the likelihood of the query given relevant documents.",
                "Without this prior knowledge, we can leave πd as free parameters, and use the EM algorithm to estimate πd and ∆.",
                "The updating functions are given as π (k+1) d = πd w∈V ((1 − δw)p(c(w, q)|ˆΛd) + δwp(c(w, q)|C)) d∈C πd w∈V ((1 − δw)p(c(w, q)|ˆΛd) + δwp(c(w, q)|C)) and δ (k+1) w = d∈C πd δwp(c(w, q)|C)) (1 − δw)p(c(w, q)|ˆΛd) + δwp(c(w, q)|C)) As discussed in [29], we only need to run the EM algorithm for several iterations, thus the computational cost is relatively low.",
                "We again assume our vocabulary containing all query terms plus a pseudo non-query term.",
                "Note that the function does not give an explicit way of estimating the coefficient for the unseen non-query term.",
                "In our experiments, we set it to the average over δw of all query terms.",
                "With this flexibility, we expect Poisson language models could improve the retrieval performance, especially for verbose queries, where the query terms have various discriminative values.",
                "In Section 4, we use empirical experiments to prove this hypothesis. 3.3 Mixture Background Models Another flexibility is to explore different background (collection) models (i.e., p(·|U), or p(·|C)).",
                "One common assumption made in language modeling information retrieval is that the background model is a homogeneous model of the document models [28, 29].",
                "Similarly, we can also make the assumption that the collection model is a Poisson language model, with the rates λC,w = d∈C c(w,d) |C| .",
                "However, this assumption usually does not hold, since the collection is far more complex than a single document.",
                "Indeed, the collection usually consists of a mixture of documents with various genres, authors, and topics, etc.",
                "Treating the collection model as a mixture of document models, instead of a single pseudo-document model is more reasonable.",
                "Existing work of multinomial language modeling has already shown that a better modeling of background improves the retrieval performance, such as clusters [15, 10], neighbor documents [25], and aspects [8, 27].",
                "All the approaches can be easily adopted using Poisson language models.",
                "However, a common problem of these approaches is that they all require heavy computation to construct the background model.",
                "With Poisson language modeling, we show that it is possible to model the mixture background without paying for the heavy computational cost.",
                "Poisson Mixture [3] has been proposed to model a collection of documents, which can fit the data much better than a single Poisson.",
                "The basic idea is to assume that the collection is generated from a mixture of Poisson models, which has the general form of p(x = k|PM) = λ p(λ)p(x = k|λ)dλ p(·|λ) is a single Poisson model and p(λ) is an arbitrary probability density function.",
                "There are three well known Poisson mixtures [3]: 2-Poisson, Negative Binomial, and the Katzs K-Mixture [9].",
                "Note that the 2-Poisson model has actually been explored in probabilistic retrieval models, which led to the well-known BM25 formula [22].",
                "All these mixtures have closed forms, and can be estimated from the collection of documents efficiently.",
                "This is an advantage over the multinomial mixture models, such as PLSI [8] and LDA [1], for retrieval.",
                "For example, the probability density function of Katzs K-Mixture is given as p(c(w) = k|αw, βw) = (1 − αw)ηk,0 + αw βw + 1 ( βw βw + 1 )k where ηk,0 = 1 when k = 0, and 0 otherwise.",
                "With the observation of a collection of documents, αw and βw can be estimated as βw = cf(w) − df(w) df(w) and αw = cf(w) Nβw where cf(w) and df(w) are the collection frequency and document frequency of w, and N is the number of documents in the collection.",
                "To account for the different document lengths, we assume that βw is a reasonable estimation for generating a document of the average length, and use β = βw avdl |q| to generate the query.",
                "This Poisson mixture model can be easily used to replace P(·|C) in the retrieval functions 3 and 4. 3.4 Other Possible Flexibilities In addition to <br>term dependent smooth</br>ing and efficient mixture background, a Poisson language model has also some other potential advantages.",
                "For example, in Section 2, we see that Formula 2 introduces a component which does document length penalization.",
                "Intuitively, when the document has more unique words, it will be penalized more.",
                "On the other hand, if a document is exactly n copies of another document, it would not get over penalized.",
                "This feature is desirable and not achieved with the Dirichlet model [5].",
                "Potentially, this component could penalize a document according to what types of terms it contains.",
                "With term specific settings of δ, we could get even more flexibility for document length normalization.",
                "Pseudo-feedback is yet another interesting direction where the Poission model might be able to show its advantage.",
                "With model-based feedback, we could again relax the combination coefficients of the feedback model and the background model, and allow different terms to contribute differently to the feedback model.",
                "We could also utilize the relevant documents to learn better per-term smoothing coefficients. 4.",
                "EVALUATION In Section 3, we analytically compared the Poisson language models and multinomial language models from the perspective of query generation and retrieval.",
                "In this section, we compare these two families of models empirically.",
                "Experiment results show that the Poisson model with perterm smoothing outperforms multinomial model, and the performance can be further improved with two-stage smoothing.",
                "Using Poisson mixture as background model also improves the retrieval performance. 4.1 Datasets Since retrieval performance could significantly vary from one test collection to another, and from one query to another, we select four representative TREC test collections: AP, Trec7, Trec8, and Wt2g(Web).",
                "To cover different types of queries, we follow [28, 5], and construct short-keyword (SK, keyword title), short-verbose (SV, one sentence description), and long-verbose (LV, multiple sentences) queries.",
                "The documents are stemmed with the Porters stemmer, and we do not remove any stop word.",
                "For each parameter, we vary its value to cover a reasonably wide range. 4.2 Comparison to Multinomial We compare the performance of the Poisson retrieval models and multinomial retrieval models using interpolation (JelinekMercer, JM) smoothing and Bayesian smoothing with conjugate priors.",
                "Table 1 shows that the two JM-smoothed models perform similarly on all data sets.",
                "Since the Dirichlet Smoothing for multinomial language model and the Gamma Smoothing for Poisson language model lead to the same retrieval formula, the performance of these two models are jointly presented.",
                "We see that Dirichlet/Gamma smoothing methods outperform both Jelinek-Mercer smoothing methods.",
                "The parameter sensitivity curves for two Jelinek-Mercer 0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1 0 0.05 0.1 0.15 0.2 0.25 0.3 Dataset: Trec8 Parameter: δ AveragePrecision JM−Multinomial: LV JM−Multinomial: SV JM−Multinomial: SK JM−Poisson: SK JM−Poisson: SV JM−Poisson: LV Figure 1: Poisson and multinomial performs similarly with Jelinek-Mercer smoothing smoothing methods are shown in Figure 1.",
                "Clearly, these two methods perform similarly either in terms of optimality Data Query JM-Multinomial JM-Poisson Dirichlet/Gamma Per-term 2-Stage Poisson MAP InitPr Pr@5d MAP InitPr Pr@5d MAP InitPr Pr@5d MAP InitPr Pr@5d AP88-89 SK 0.203 0.585 0.356 0.203 0.585 0.358 0.224 0.629 0.393 0.226 0.630 0.396 SV 0.187 0.580 0.361 0.183 0.571 0.345 0.204 0.613 0.387 0.217* 0.603 0.390 LV 0.283 0.716 0.480 0.271 0.692 0.470 0.291 0.710 0.496 0.304* 0.695 0.510 Trec7 SK 0.167 0.635 0.400 0.168 0.635 0.404 0.186 0.687 0.428 0.185 0.646 0.436 SV 0.174 0.655 0.432 0.176 0.653 0.432 0.182 0.666 0.432 0.196* 0.660 0.440 LV 0.223 0.730 0.496 0.215 0.766 0.488 0.224 0.748 0.52 0.236* 0.738 0.512 Trec8 SK 0.239 0.621 0.440 0.239 0.621 0.436 0.257 0.718 0.496 0.256 0.704 0.468 SV 0.231 0.686 0.448 0.234 0.702 0.456 0.228 0.691 0.456 0.246* 0.692 0.476 LV 0.265 0.796 0.548 0.261 0.757 0.520 0.260 0.741 0.492 0.274* 0.766 0.508 Web SK 0.250 0.616 0.380 0.250 0.616 0.380 0.302 0.767 0.468 0.307 0.739 0.468 SV 0.214 0.611 0.392 0.217 0.609 0.384 0.273 0.693 0.508 0.292* 0.703 0.480 LV 0.266 0.790 0.464 0.259 0.776 0.452 0.283 0.756 0.496 0.311* 0.759 0.488 Table 1: Performance comparison between Poisson and Multinomial retrieval models: basic models perform comparably; term dependent two-stage smoothing significantly improves Poisson An asterisk (*) indicates that the difference between the performance of the term dependent two-stage smoothing and that of the Dirichlet/Gamma single smoothing is statistically significant according to the Wilcoxon signed rank test at the level of 0.05. or sensitivity.",
                "This similarity of performance is expected as we discussed in Section 3.1.",
                "Although the Poisson model and multinomial model are similar in terms of the basic model and/or with simple smoothing methods, the Poisson model has great potential and flexibility to be further improved.",
                "As shown in the rightmost column of Table 1, term dependent two-stage Poisson model consistently outperforms the basic smoothing models, especially for verbose queries.",
                "This model is given in Formula 4, with a Gamma smoothing for the document model p(·|d), and δw, which is term dependent.",
                "The parameter µ of the first stage Gamma smoothing is empirically tuned.",
                "The combination coefficients (i.e., ∆), are estimated with the EM algorithm in Section 3.2.",
                "The parameter sensitivity curves for Dirichlet/Gamma and the per-term two-stage smoothing model are plotted in Figure 2.",
                "The per-term two-stage smoothing method is less sensitive to the parameter µ than Dirichlet/Gamma, and yields better optimal performance. 0 1000 2000 3000 4000 5000 6000 7000 8000 9000 10000 0.1 0.12 0.14 0.16 0.18 0.2 0.22 Dataset: AP; Query Type: SV Parameter: µ AveragePrecision Dirichlet/Gamma Smoothing Term Dependent 2−Stage Figure 2: Term dependent two-stage smoothing of Poisson outperforms Dirichlet/Gamma In the following subsections, we conduct experiments to demonstrate how the flexibility of the Poisson model could be utilized to achieve better performance, which we cannot achieve with multinomial language models. 4.3 Term Dependent Smoothing To test the effectiveness of the <br>term dependent smooth</br>ing, we conduct the following two experiments.",
                "In the first experiment, we relax the constant coefficient in the simple Jelinek-Mercer smoothing formula (i.e., Formula 3), and use the EM algorithm proposed in Section 3.2 to find a δw for each unique term.",
                "Since we are using the EM algorithm to iteratively estimate the parameters, we usually do not want the probability of p(·|d) to be zero.",
                "We then use a simple Laplace method to slightly smooth the document model before it goes into the EM iterations.",
                "The documents are then still scored with Formula 3, but using learnt δw.",
                "The results are labeled with JM+L. in Table 2.",
                "Data Q JM JM JM+L. 2-Stage 2-Stage (MAP) PT: No Yes Yes No Yes AP SK 0.203 0.204 0.206 0.223 0.226* SV 0.183 0.189 0.214* 0.204 0.217* Trec7 SK 0.168 0.171 0.174 0.186 0.185 SV 0.176 0.147 0.198* 0.194 0.196 Trec8 SK 0.239 0.240 0.227* 0.257 0.256 SV 0.234 0.223 0.249* 0.242 0.246* Web SK 0.250 0.236 0.220* 0.291 0.307* SV 0.217 0.232 0.261* 0.273 0.292* Table 2: Term dependent smoothing improves retrieval performance An asterisk (*) in Column 3 indicates that the difference between the JM+L. method and JM method is statistically significant; an asterisk (*) in Column 5 means that the difference between term dependent two-stage method and query dependent two-stage method is statistically significant; PT stands for per-term.",
                "With term dependent coefficients, the performance of the Jelinek-Mercer Poisson model is improved in most cases.",
                "However, in some cases (e.g., Trec7/SV), it performs poorly.",
                "This might be caused by the problem of EM estimation with unsmoothed document models.",
                "Once non-zero probability is assigned to all the terms before entering the EM iteration, the performance on verbose queries can be improved significantly.",
                "This indicates that there is still room to find better methods to estimate δw.",
                "Please note that neither the perterm JM method nor the JM+L. method has a parameter to tune.",
                "As shown in Table 1, the term dependent two-stage smoothing can significantly improve retrieval performance.",
                "To understand whether the improvement is contributed by the <br>term dependent smooth</br>ing or the two-stage smoothing framework, we design another experiment to compare the perterm two-stage smoothing with the two-stage smoothing method proposed in [29].",
                "Their method managed to find coefficients specific to the query, thus a verbose query would use a higher δ.",
                "However, since their model is based on multinomial language modeling, they could not get per-term coefficients.",
                "We adopt their method to the Poisson two-stage smoothing, and also estimate a per-query coefficient for all the terms.",
                "We compare the performance of such a model with the per-term two-stage smoothing model, and present the results in the right two columns in Table 2.",
                "Again, we see that the per-term two-stage smoothing outperforms the per-query two-stage smoothing, especially for verbose queries.",
                "The improvement is not as large as how the perterm smoothing method improves over Dirichlet/Gamma.",
                "This is expected, since the per-query smoothing has already addressed the query discrimination problem to some extent.",
                "This experiment shows that even if the smoothing is already per-query, making it per-term is still beneficial.",
                "In brief, the per-term smoothing improved the retrieval performance of both one-stage and two-stage smoothing method. 4.4 Mixture Background Model In this section, we conduct experiments to examine the benefits of using a mixture background model without extra computational cost, which can not be achieved for multinomial models.",
                "Specifically, in retrieval formula 3, instead of using a single Poisson distribution to model the background p(·|C), we use Katzs K-Mixture model, which is essentially a mixture of Poisson distributions. p(·|C) can be computed efficiently with simple collection statistics, as discussed in Section 3.3.",
                "Data Query JM.",
                "Poisson JM.",
                "K-Mixture AP SK 0.203 0.204 SV 0.183 0.188* Trec-7 SK 0.168 0.169 SV 0.176 0.178* Trec-8 SK 0.239 0.239 SV 0.234 0.238* Web SK 0.250 0.250 SV 0.217 0.223* Table 3: K-Mixture background model improves retrieval performance The performance of the JM retrieval model with single Poisson background and with Katzs K-Mixture background model is compared in Table 3.",
                "Clearly, using K-Mixture to model the background model outperforms the single Poisson background model in most cases, especially for verbose queries where the improvement is statistically significant.",
                "Figure 3 shows that the performance changes over different parameters for short verbose queries.",
                "The model using K-Mixture background is less sensitive than the one using single Poisson background.",
                "Given that this type of mixture 0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1 0 0.05 0.1 0.15 0.2 0.25 Data: Trec8; Query: SV Parameter: δ AveragePrecision Poisson Background K−Mixture Background Figure 3: K-Mixture background model deviates the sensitivity of verbose queries background model does not require any extra computation cost, it would be interesting to study whether using other mixture Poisson models, such as 2-Poisson and negative Binomial, could help the performance. 5.",
                "RELATED WORK To the best of our knowledge, there has been no study of query generation models based on Poisson distribution.",
                "Language models have been shown to be effective for many retrieval tasks [21, 28, 14, 4].",
                "The most popular and fundamental one is the query-generation language model [21, 13].",
                "All existing query generation language models are based on either multinomial distribution [19, 6, 28, 13] or multivariate Bernoulli distribution [21, 17, 18].",
                "We introduce a new family of language models, based on Poisson distribution.",
                "Poisson distribution has been previously studied in the document generation models [16, 22, 3, 24], leading to the development of one of the most effective retrieval formula BM25 [23]. [24] studies the parallel derivation of three different retrieval models which is related to our comparison of Poisson and multinomial.",
                "However, the Poisson model in their paper is still under the document generation framework, and also does not account for the document length variation. [26] introduces a way to empirically search for an exponential model for the documents.",
                "Poisson mixtures [3] such as 2-Poisson [22], Negative multinomial, and Katzs KMixture [9] has shown to be effective to model and retrieve documents.",
                "Once again, none of this work explores Poisson distribution in the query generation framework.",
                "Language model smoothing [2, 28, 29] and background structures [15, 10, 25, 27] have been studied with multinomial language models. [7] analytically shows that term specific smoothing could be useful.",
                "We show that Poisson language model is natural to accommodate the per-term smoothing without heuristic twist of the semantics of a generative model, and is able to efficiently better model the mixture background, both analytically and empirically. 6.",
                "CONCLUSIONS We present a new family of query generation language models for retrieval based on Poisson distribution.",
                "We derive several smoothing methods for this family of models, including single-stage smoothing and two-stage smoothing.",
                "We compare the new models with the popular multinomial retrieval models both analytically and experimentally.",
                "Our analysis shows that while our new models and multinomial models are equivalent under some assumptions, they are generally different with some important differences.",
                "In particular, we show that Poisson has an advantage over multinomial in naturally accommodating per-term smoothing.",
                "We exploit this property to develop a new per-term smoothing algorithm for Poisson language models, which is shown to outperform term-independent smoothing for both Poisson and multinomial models.",
                "Furthermore, we show that a mixture background model for Poisson can be used to improve the performance and robustness over the standard Poisson background model.",
                "Our work opens up many interesting directions for further exploration in this new family of models.",
                "Further exploring the flexibilities over multinomial language models, such as length normalization and pseudo-feedback could be good future work.",
                "It is also appealing to find robust methods to learn the per-term smoothing coefficients without additional computation cost. 7.",
                "ACKNOWLEDGMENTS We thank the anonymous SIGIR 07 reviewers for their useful comments.",
                "This material is based in part upon work supported by the National Science Foundation under award numbers IIS-0347933 and 0425852. 8.",
                "REFERENCES [1] D. Blei, A. Ng, and M. Jordan.",
                "Latent dirichlet allocation.",
                "Journal of Machine Learning Research, 3:993-1022, 2003. [2] S. F. Chen and J. Goodman.",
                "An empirical study of smoothing techniques for language modeling.",
                "Technical Report TR-10-98, Harvard University, 1998. [3] K. Church and W. Gale.",
                "Poisson mixtures.",
                "Nat.",
                "Lang.",
                "Eng., 1(2):163-190, 1995. [4] W. B. Croft and J. Lafferty, editors.",
                "Language Modeling and Information Retrieval.",
                "Kluwer Academic Publishers, 2003. [5] H. Fang, T. Tao, and C. Zhai.",
                "A formal study of information retrieval heuristics.",
                "In Proceedings of the 27th annual international ACM SIGIR conference on Research and development in information retrieval, pages 49-56, 2004. [6] D. Hiemstra.",
                "Using Language Models for Information Retrieval.",
                "PhD thesis, University of Twente, Enschede, Netherlands, 2001. [7] D. Hiemstra.",
                "Term-specific smoothing for the language modeling approach to information retrieval: the importance of a query term.",
                "In Proceedings of the 25th annual international ACM SIGIR conference on Research and development in information retrieval, pages 35-41, 2002. [8] T. Hofmann.",
                "Probabilistic latent semantic indexing.",
                "In Proceedings of ACM SIGIR99, pages 50-57, 1999. [9] S. M. Katz.",
                "Distribution of content words and phrases in text and language modelling.",
                "Nat.",
                "Lang.",
                "Eng., 2(1):15-59, 1996. [10] O. Kurland and L. Lee.",
                "Corpus structure, language models, and ad-hoc information retrieval.",
                "In Proceedings of the 27th annual international ACM SIGIR conference on Research and development in information retrieval, pages 194-201, 2004. [11] J. Lafferty and C. Zhai.",
                "Document language models, query models, and risk minimization for information retrieval.",
                "In Proceedings of SIGIR01, pages 111-119, Sept 2001. [12] J. Lafferty and C. Zhai.",
                "Probabilistic IR models based on query and document generation.",
                "In Proceedings of the Language Modeling and IR workshop, pages 1-5, May 31 - June 1 2001. [13] J. Lafferty and C. Zhai.",
                "Probabilistic relevance models based on document and query generation.",
                "In W. B. Croft and J. Lafferty, editors, Language Modeling and Information Retrieval.",
                "Kluwer Academic Publishers, 2003. [14] V. Lavrenko and B. Croft.",
                "Relevance-based language models.",
                "In Proceedings of SIGIR01, pages 120-127, Sept 2001. [15] X. Liu and W. B. Croft.",
                "Cluster-based retrieval using language models.",
                "In Proceedings of the 27th annual international ACM SIGIR conference on Research and development in information retrieval, pages 186-193, 2004. [16] E. L. Margulis.",
                "Modelling documents with multiple poisson distributions.",
                "Inf.",
                "Process.",
                "Manage., 29(2):215-227, 1993. [17] A. McCallum and K. Nigam.",
                "A comparison of event models for naive bayes text classification.",
                "In Proceedings of AAAI-98 Workshop on Learning for Text Categorization, 1998. [18] D. Metzler, V. Lavrenko, and W. B. Croft.",
                "Formal multiple-bernoulli models for language modeling.",
                "In Proceedings of the 27th annual international ACM SIGIR conference on Research and development in information retrieval, pages 540-541, 2004. [19] D. H. Miller, T. Leek, and R. Schwartz.",
                "A hidden Markov model information retrieval system.",
                "In Proceedings of the 1999 ACM SIGIR Conference on Research and Development in Information Retrieval, pages 214-221, 1999. [20] A. Papoulis.",
                "Probability, random variables and stochastic processes.",
                "New York: McGraw-Hill, 1984, 2nd ed., 1984. [21] J. M. Ponte and W. B. Croft.",
                "A language modeling approach to information retrieval.",
                "In Proceedings of the 21st annual international ACM SIGIR conference on Research and development in information retrieval, pages 275-281, 1998. [22] S. Robertson and S. Walker.",
                "Some simple effective approximations to the 2-poisson model for probabilistic weighted retrieval.",
                "In Proceedings of SIGIR94, pages 232-241, 1994. [23] S. E. Robertson, S. Walker, S. Jones, M. M.Hancock-Beaulieu, and M. Gatford.",
                "Okapi at TREC-3.",
                "In D. K. Harman, editor, The Third Text REtrieval Conference (TREC-3), pages 109-126, 1995. [24] T. Roelleke and J. Wang.",
                "A parallel derivation of probabilistic information retrieval models.",
                "In Proceedings of the 29th annual international ACM SIGIR conference on Research and development in information retrieval, pages 107-114, 2006. [25] T. Tao, X. Wang, Q. Mei, and C. Zhai.",
                "Language model information retrieval with document expansion.",
                "In Proceedings of HLT/NAACL 2006, pages 407-414, 2006. [26] J. Teevan and D. R. Karger.",
                "Empirical development of an exponential probabilistic model for text retrieval: using textual analysis to build a better model.",
                "In Proceedings of the 26th annual international ACM SIGIR conference on Research and development in informaion retrieval, pages 18-25, 2003. [27] X. Wei and W. B. Croft.",
                "Lda-based document models for ad-hoc retrieval.",
                "In Proceedings of the 29th annual international ACM SIGIR conference on Research and development in information retrieval, pages 178-185, 2006. [28] C. Zhai and J. Lafferty.",
                "A study of smoothing methods for language models applied to ad-hoc information retrieval.",
                "In Proceedings of ACM SIGIR01, pages 334-342, Sept 2001. [29] C. Zhai and J. Lafferty.",
                "Two-stage language models for information retrieval.",
                "In Proceedings of ACM SIGIR02, pages 49-56, Aug 2002."
            ],
            "original_annotated_samples": [],
            "translated_annotated_samples": [
                "A continuación presentamos una posible forma de explorar el \"término dependiente suave\" con modelos de lenguaje Poisson.término dependiente suave",
                "Este modelo de mezcla de Poisson se puede usar fácilmente para reemplazar P (· | C) en las funciones de recuperación 3 y 4. 3.4 Otras flexibilidades posibles además de un fondo de mezcla eficiente y \"suave dependiente del término\", un modelo de lenguaje Poisson también tiene otrasposibles ventajas.término dependiente suave",
                "El método de suavizado de dos etapas por término es menos sensible al parámetro µ que Dirichlet/gamma, y produce un mejor rendimiento óptimo.0 1000 2000 3000 4000 5000 6000 7000 8000 9000 10000 0.1 0.12 0.14 0.16 0.18 0.2 0.22 conjunto de datos: AP;Tipo de consulta: Parámetro SV: µ Precisión promedio Dirichlet/Gamma suave término dependiente 2-etapa Figura 2: suave dependiente de dos etapas de dos etapas de Poisson supera a Dirichlet/gamma En las siguientes subsecciones, realizamos experimentos para demostrar cómo la flexibilidad del modelo Poisson podríaSe utilizará para lograr un mejor rendimiento, que no podemos lograr con modelos de lenguaje multinomial.4.3 suave dependiente del término Para probar la efectividad del \"término dependiente suave\", realizamos los siguientes dos experimentos.término dependiente suave",
                "Para comprender si la mejora es contribuida por el \"término suave dependiente del término\" o el marco de suavizado de dos etapas, diseñamos otro experimento para comparar el suavizado de dos etapas perterable con el método de suavizado de dos etapas propuesto en [29].término dependiente suave"
            ],
            "translated_text": "",
            "candidates": [],
            "error": []
        }
    }
}