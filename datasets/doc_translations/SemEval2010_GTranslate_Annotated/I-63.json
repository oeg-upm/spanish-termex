{
    "id": "I-63",
    "original_text": "Combinatorial Resource Scheduling for Multiagent MDPs Dmitri A. Dolgov, Michael R. James, and Michael E. Samples AI and Robotics Group Technical Research, Toyota Technical Center USA {ddolgov, michael.r.james, michael.samples}@gmail.com ABSTRACT Optimal resource scheduling in multiagent systems is a computationally challenging task, particularly when the values of resources are not additive. We consider the combinatorial problem of scheduling the usage of multiple resources among agents that operate in stochastic environments, modeled as Markov decision processes (MDPs). In recent years, efficient resource-allocation algorithms have been developed for agents with resource values induced by MDPs. However, this prior work has focused on static resource-allocation problems where resources are distributed once and then utilized in infinite-horizon MDPs. We extend those existing models to the problem of combinatorial resource scheduling, where agents persist only for finite periods between their (predefined) arrival and departure times, requiring resources only for those time periods. We provide a computationally efficient procedure for computing globally optimal resource assignments to agents over time. We illustrate and empirically analyze the method in the context of a stochastic jobscheduling domain. Categories and Subject Descriptors I.2.8 [Artificial Intelligence]: Problem Solving, Control Methods, and Search; I.2.11 [Artificial Intelligence]: Distributed Artificial Intelligence-Multiagent systems General Terms Algorithms, Performance, Design 1. INTRODUCTION The tasks of optimal resource allocation and scheduling are ubiquitous in multiagent systems, but solving such optimization problems can be computationally difficult, due to a number of factors. In particular, when the value of a set of resources to an agent is not additive (as is often the case with resources that are substitutes or complements), the utility function might have to be defined on an exponentially large space of resource bundles, which very quickly becomes computationally intractable. Further, even when each agent has a utility function that is nonzero only on a small subset of the possible resource bundles, obtaining optimal allocation is still computationally prohibitive, as the problem becomes NP-complete [14]. Such computational issues have recently spawned several threads of work in using compact models of agents preferences. One idea is to use any structure present in utility functions to represent them compactly, via, for example, logical formulas [15, 10, 4, 3]. An alternative is to directly model the mechanisms that define the agents utility functions and perform resource allocation directly with these models [9]. A way of accomplishing this is to model the processes by which an agent might utilize the resources and define the utility function as the payoff of these processes. In particular, if an agent uses resources to act in a stochastic environment, its utility function can be naturally modeled with a Markov decision process, whose action set is parameterized by the available resources. This representation can then be used to construct very efficient resource-allocation algorithms that lead to an exponential speedup over a straightforward optimization problem with flat representations of combinatorial preferences [6, 7, 8]. However, this existing work on resource allocation with preferences induced by resource-parameterized MDPs makes an assumption that the resources are only allocated once and are then utilized by the agents independently within their infinite-horizon MDPs. This assumption that no reallocation of resources is possible can be limiting in domains where agents arrive and depart dynamically. In this paper, we extend the work on resource allocation under MDP-induced preferences to discrete-time scheduling problems, where agents are present in the system for finite time intervals and can only use resources within these intervals. In particular, agents arrive and depart at arbitrary (predefined) times and within these intervals use resources to execute tasks in finite-horizon MDPs. We address the problem of globally optimal resource scheduling, where the objective is to find an allocation of resources to the agents across time that maximizes the sum of the expected rewards that they obtain. In this context, our main contribution is a mixed-integerprogramming formulation of the scheduling problem that chooses globally optimal resource assignments, starting times, and execution horizons for all agents (within their arrival1220 978-81-904262-7-5 (RPS) c 2007 IFAAMAS departure intervals). We analyze and empirically compare two flavors of the scheduling problem: one, where agents have static resource assignments within their finite-horizon MDPs, and another, where resources can be dynamically reallocated between agents at every time step. In the rest of the paper, we first lay down the necessary groundwork in Section 2 and then introduce our model and formal problem statement in Section 3. In Section 4.2, we describe our main result, the optimization program for globally optimal resource scheduling. Following the discussion of our experimental results on a job-scheduling problem in Section 5, we conclude in Section 6 with a discussion of possible extensions and generalizations of our method. 2. BACKGROUND Similarly to the model used in previous work on resourceallocation with MDP-induced preferences [6, 7], we define the value of a set of resources to an agent as the value of the best MDP policy that is realizable, given those resources. However, since the focus of our work is on scheduling problems, and a large part of the optimization problem is to decide how resources are allocated in time among agents with finite arrival and departure times, we model the agents planning problems as finite-horizon MDPs, in contrast to previous work that used infinite-horizon discounted MDPs. In the rest of this section, we first introduce some necessary background on finite-horizon MDPs and present a linear-programming formulation that serves as the basis for our solution algorithm developed in Section 4. We also outline the standard methods for combinatorial resource scheduling with flat resource values, which serve as a comparison benchmark for the new model developed here. 2.1 Markov Decision Processes A stationary, finite-domain, discrete-time MDP (see, for example, [13] for a thorough and detailed development) can be described as S, A, p, r , where: S is a finite set of system states; A is a finite set of actions that are available to the agent; p is a stationary stochastic transition function, where p(σ|s, a) is the probability of transitioning to state σ upon executing action a in state s; r is a stationary reward function, where r(s, a) specifies the reward obtained upon executing action a in state s. Given such an MDP, a decision problem under a finite horizon T is to choose an optimal action at every time step to maximize the expected value of the total reward accrued during the agents (finite) lifetime. The agents optimal policy is then a function of current state s and the time until the horizon. An optimal policy for such a problem is to act greedily with respect to the optimal value function, defined recursively by the following system of finite-time Bellman equations [2]: v(s, t) = max a r(s, a) + X σ p(σ|s, a)v(σ, t + 1), ∀s ∈ S, t ∈ [1, T − 1]; v(s, T) = 0, ∀s ∈ S; where v(s, t) is the optimal value of being in state s at time t ∈ [1, T]. This optimal value function can be easily computed using dynamic programming, leading to the following optimal policy π, where π(s, a, t) is the probability of executing action a in state s at time t: π(s, a, t) = ( 1, a = argmaxa r(s, a) + P σ p(σ|s, a)v(σ, t + 1), 0, otherwise. The above is the most common way of computing the optimal value function (and therefore an optimal policy) for a finite-horizon MDP. However, we can also formulate the problem as the following linear program (similarly to the dual LP for infinite-horizon discounted MDPs [13, 6, 7]): max X s X a r(s, a) X t x(s, a, t) subject to: X a x(σ, a, t + 1) = X s,a p(σ|s, a)x(s, a, t) ∀σ, t ∈ [1, T − 1]; X a x(s, a, 1) = α(s), ∀s ∈ S; (1) where α(s) is the initial distribution over the state space, and x is the (non-stationary) occupation measure (x(s, a, t) ∈ [0, 1] is the total expected number of times action a is executed in state s at time t). An optimal (non-stationary) policy is obtained from the occupation measure as follows: π(s, a, t) = x(s, a, t)/ X a x(s, a, t) ∀s ∈ S, t ∈ [1, T]. (2) Note that the standard unconstrained finite-horizon MDP, as described above, always has a uniformly-optimal solution (optimal for any initial distribution α(s)). Therefore, an optimal policy can be obtained by using an arbitrary constant α(s) > 0 (in particular, α(s) = 1 will result in x(s, a, t) = π(s, a, t)). However, for MDPs with resource constraints (as defined below in Section 3), uniformly-optimal policies do not in general exist. In such cases, α becomes a part of the problem input, and a resulting policy is only optimal for that particular α. This result is well known for infinite-horizon MDPs with various types of constraints [1, 6], and it also holds for our finite-horizon model, which can be easily established via a line of reasoning completely analogous to the arguments in [6]. 2.2 Combinatorial Resource Scheduling A straightforward approach to resource scheduling for a set of agents M, whose values for the resources are induced by stochastic planning problems (in our case, finite-horizon MDPs) would be to have each agent enumerate all possible resource assignments over time and, for each one, compute its value by solving the corresponding MDP. Then, each agent would provide valuations for each possible resource bundle over time to a centralized coordinator, who would compute the optimal resource assignments across time based on these valuations. When resources can be allocated at different times to different agents, each agent must submit valuations for every combination of possible time horizons. Let each agent m ∈ M execute its MDP within the arrival-departure time interval τ ∈ [τa m, τd m]. Hence, agent m will execute an MDP with time horizon no greater than Tm = τd m−τa m+1. Let bτ be the global time horizon for the problem, before which all of the agents MDPs must finish. We assume τd m < bτ, ∀m ∈ M. The Sixth Intl. Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) 1221 For the scheduling problem where agents have static resource requirements within their finite-horizon MDPs, the agents provide a valuation for each resource bundle for each possible time horizon (from [1, Tm]) that they may use. Let Ω be the set of resources to be allocated among the agents. An agent will get at most one resource bundle for one of the time horizons. Let the variable ψ ∈ Ψm enumerate all possible pairs of resource bundles and time horizons for agent m, so there are 2|Ω| × Tm values for ψ (the space of bundles is exponential in the number of resource types |Ω|). The agent m must provide a value vψ m for each ψ, and the coordinator will allocate at most one ψ (resource, time horizon) pair to each agent. This allocation is expressed as an indicator variable zψ m ∈ {0, 1} that shows whether ψ is assigned to agent m. For time τ and resource ω, the function nm(ψ, τ, ω) ∈ {0, 1} indicates whether the bundle in ψ uses resource ω at time τ (we make the assumption that agents have binary resource requirements). This allocation problem is NP-complete, even when considering only a single time step, and its difficulty increases significantly with multiple time steps because of the increasing number of values of ψ. The problem of finding an optimal allocation that satisfies the global constraint that the amount of each resource ω allocated to all agents does not exceed the available amount bϕ(ω) can be expressed as the following integer program: max X m∈M X ψ∈Ψm zψ mvψ m subject to: X ψ∈Ψm zψ m ≤ 1, ∀m ∈ M; X m∈M X ψ∈Ψm zψ mnm(ψ, τ, ω) ≤ bϕ(ω), ∀τ ∈ [1, bτ], ∀ω ∈ Ω; (3) The first constraint in equation 3 says that no agent can receive more than one bundle, and the second constraint ensures that the total assignment of resource ω does not, at any time, exceed the resource bound. For the scheduling problem where the agents are able to dynamically reallocate resources, each agent must specify a value for every combination of bundles and time steps within its time horizon. Let the variable ψ ∈ Ψm in this case enumerate all possible resource bundles for which at most one bundle may be assigned to agent m at each time step. Therefore, in this case there are P t∈[1,Tm](2|Ω| )t ∼ 2|Ω|Tm possibilities of resource bundles assigned to different time slots, for the Tm different time horizons. The same set of equations (3) can be used to solve this dynamic scheduling problem, but the integer program is different because of the difference in how ψ is defined. In this case, the number of ψ values is exponential in each agents planning horizon Tm, resulting in a much larger program. This straightforward approach to solving both of these scheduling problems requires an enumeration and solution of either 2|Ω| Tm (static allocation) or P t∈[1,Tm] 2|Ω|t (dynamic reallocation) MDPs for each agent, which very quickly becomes intractable with the growth of the number of resources |Ω| or the time horizon Tm. 3. MODEL AND PROBLEM STATEMENT We now formally introduce our model of the resourcescheduling problem. The problem input consists of the following components: • M, Ω, bϕ, τa m, τd m, bτ are as defined above in Section 2.2. • {Θm} = {S, A, pm, rm, αm} are the MDPs of all agents m ∈ M. Without loss of generality, we assume that state and action spaces of all agents are the same, but each has its own transition function pm, reward function rm, and initial conditions αm. • ϕm : A×Ω → {0, 1} is the mapping of actions to resources for agent m. ϕm(a, ω) indicates whether action a of agent m needs resource ω. An agent m that receives a set of resources that does not include resource ω cannot execute in its MDP policy any action a for which ϕm(a, ω) = 0. We assume all resource requirements are binary; as discussed below in Section 6, this assumption is not limiting. Given the above input, the optimization problem we consider is to find the globally optimal-maximizing the sum of expected rewards-mapping of resources to agents for all time steps: Δ : τ × M × Ω → {0, 1}. A solution is feasible if the corresponding assignment of resources to the agents does not violate the global resource constraint: X m Δm(τ, ω) ≤ bϕ(ω), ∀ω ∈ Ω, τ ∈ [1, bτ]. (4) We consider two flavors of the resource-scheduling problem. The first formulation restricts resource assignments to the space where the allocation of resources to each agent is static during the agents lifetime. The second formulation allows reassignment of resources between agents at every time step within their lifetimes. Figure 1 depicts a resource-scheduling problem with three agents M = {m1, m2, m3}, three resources Ω = {ω1, ω2, ω3}, and a global problem horizon of bτ = 11. The agents arrival and departure times are shown as gray boxes and are {1, 6}, {3, 7}, and {2, 11}, respectively. A solution to this problem is shown via horizontal bars within each agents box, where the bars correspond to the allocation of the three resource types. Figure 1a shows a solution to a static scheduling problem. According to the shown solution, agent m1 begins the execution of its MDP at time τ = 1 and has a lock on all three resources until it finishes execution at time τ = 3. Note that agent m1 relinquishes its hold on the resources before its announced departure time of τd m1 = 6, ostensibly because other agents can utilize the resources more effectively. Thus, at time τ = 4, resources ω1 and ω3 are allocated to agent m2, who then uses them to execute its MDP (using only actions supported by resources ω1 and ω3) until time τ = 7. Agent m3 holds resource ω3 during the interval τ ∈ [4, 10]. Figure 1b shows a possible solution to the dynamic version of the same problem. There, resources can be reallocated between agents at every time step. For example, agent m1 gives up its use of resource ω2 at time τ = 2, although it continues the execution of its MDP until time τ = 6. Notice that an agent is not allowed to stop and restart its MDP, so agent m1 is only able to continue executing in the interval τ ∈ [3, 4] if it has actions that do not require any resources (ϕm(a, ω) = 0). Clearly, the model and problem statement described above make a number of assumptions about the problem and the desired solution properties. We discuss some of those assumptions and their implications in Section 6. 1222 The Sixth Intl. Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) (a) (b) Figure 1: Illustration of a solution to a resource-scheduling problem with three agents and three resources: a) static resource assignments (resource assignments are constant within agents lifetimes; b) dynamic assignment (resource assignments are allowed to change at every time step). 4. RESOURCE SCHEDULING Our resource-scheduling algorithm proceeds in two stages. First, we perform a preprocessing step that augments the agent MDPs; this process is described in Section 4.1. Second, using these augmented MDPs we construct a global optimization problem, which is described in Section 4.2. 4.1 Augmenting Agents MDPs In the model described in the previous section, we assume that if an agent does not possess the necessary resources to perform actions in its MDP, its execution is halted and the agent leaves the system. In other words, the MDPs cannot be paused and resumed. For example, in the problem shown in Figure 1a, agent m1 releases all resources after time τ = 3, at which point the execution of its MDP is halted. Similarly, agents m2 and m3 only execute their MDPs in the intervals τ ∈ [4, 6] and τ ∈ [4, 10], respectively. Therefore, an important part of the global decision-making problem is to decide the window of time during which each of the agents is active (i.e., executing its MDP). To accomplish this, we augment each agents MDP with two new states (start and finish states sb , sf , respectively) and a new start/stop action a∗ , as illustrated in Figure 2. The idea is that an agent stays in the start state sb until it is ready to execute its MDP, at which point it performs the start/stop action a∗ and transitions into the state space of the original MDP with the transition probability that corresponds to the original initial distribution α(s). For example, in Figure 1a, for agent m2 this would happen at time τ = 4. Once the agent gets to the end of its activity window (time τ = 6 for agent m2 in Figure 1a), it performs the start/stop action, which takes it into the sink finish state sf at time τ = 7. More precisely, given an MDP S, A, pm, rm, αm , we define an augmented MDP S , A , pm, rm, αm as follows: S = S ∪ sb ∪ sf ; A = A ∪ a∗ ; p (s|sb , a∗ ) = α(s), ∀s ∈ S; p (sb |sb , a) = 1.0, ∀a ∈ A; p (sf |s, a∗ ) = 1.0, ∀s ∈ S; p (σ|s, a) = p(σ|s, a), ∀s, σ ∈ S, a ∈ A; r (sb , a) = r (sf , a) = 0, ∀a ∈ A ; r (s, a) = r(s, a), ∀s ∈ S, a ∈ A; α (sb ) = 1; α (s) = 0, ∀s ∈ S; where all non-specified transition probabilities are assumed to be zero. Further, in order to account for the new starting state, we begin the MDP one time-step earlier, setting τa m ← τa m − 1. This will not affect the resource allocation due to the resource constraints only being enforced for the original MDP states, as will be discussed in the next section. For example, the augmented MDPs shown in Figure 2b (which starts in state sb at time τ = 2) would be constructed from an MDP with original arrival time τ = 3. Figure 2b also shows a sample trajectory through the state space: the agent starts in state sb , transitions into the state space S of the original MDP, and finally exists into the sink state sf . Note that if we wanted to model a problem where agents could pause their MDPs at arbitrary time steps (which might be useful for domains where dynamic reallocation is possible), we could easily accomplish this by including an extra action that transitions from each state to itself with zero reward. 4.2 MILP for Resource Scheduling Given a set of augmented MDPs, as defined above, the goal of this section is to formulate a global optimization program that solves the resource-scheduling problem. In this section and below, all MDPs are assumed to be the augmented MDPs as defined in Section 4.1. Our approach is similar to the idea used in [6]: we begin with the linear-program formulation of agents MDPs (1) and augment it with constraints that ensure that the corresponding resource allocation across agents and time is valid. The resulting optimization problem then simultaneously solves the agents MDPs and resource-scheduling problems. In the rest of this section, we incrementally develop a mixed integer program (MILP) that achieves this. In the absence of resource constraints, the agents finitehorizon MDPs are completely independent, and the globally optimal solution can be trivially obtained via the following LP, which is simply an aggregation of single-agent finitehorizon LPs: max X m X s X a rm(s, a) X t xm(s, a, t) subject to: X a xm(σ, a, t + 1) = X s,a pm(σ|s, a)xm(s, a, t), ∀m ∈ M, σ ∈ S, t ∈ [1, Tm − 1]; X a xm(s, a, 1) = αm(s), ∀m ∈ M, s ∈ S; (12) where xm(s, a, t) is the occupation measure of agent m, and The Sixth Intl. Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) 1223 (a) (b) Figure 2: Illustration of augmenting an MDP to allow for variable starting and stopping times: a) (left) the original two-state MDP with a single action; (right) the augmented MDP with new states sb and sf and the new action a∗ (note that the origianl transitions are not changed in the augmentation process); b) the augmented MDP displayed as a trajectory through time (grey lines indicate all transitions, while black lines indicate a given trajectory. Objective Function (sum of expected rewards over all agents) max X m X s X a rm(s, a) X t xm(s, a, t) (5) Meaning Implication Linear Constraints Tie x to θ. Agent is only active when occupation measure is nonzero in original MDP states. θm(τ) = 0 =⇒ xm(s, a, τ −τa m+1) = 0 ∀s /∈ {sb , sf }, a ∈ A X s/∈{sb,sf } X a xm(s, a, t) ≤ θm(τa m + t − 1) ∀m ∈ M, ∀t ∈ [1, Tm] (6) Agent can only be active in τ ∈ (τa m, τd m) θm(τ) = 0 ∀m ∈ M, τ /∈ (τa m, τd m) (7) Cannot use resources when not active θm(τ) = 0 =⇒ Δm(τ, ω) = 0 ∀τ ∈ [0, bτ], ω ∈ Ω Δm(τ, ω) ≤ θm(τ) ∀m ∈ M, τ ∈ [0, bτ], ω ∈ Ω (8) Tie x to Δ (nonzero x forces corresponding Δ to be nonzero.) Δm(τ, ω) = 0, ϕm(a, ω) = 1 =⇒ xm(s, a, τ − τa m + 1) = 0 ∀s /∈ {sb , sf } 1/|A| X a ϕm(a, ω) X s/∈{sb,sf } xm(s, a, t) ≤ Δm(t + τa m − 1, ω) ∀m ∈ M, ω ∈ Ω, t ∈ [1, Tm] (9) Resource bounds X m Δm(τ, ω) ≤ bϕ(ω) ∀ω ∈ Ω, τ ∈ [0, bτ] (10) Agent cannot change resources while active. Only enabled for scheduling with static assignments. θm(τ) = 1 and θm(τ + 1) = 1 =⇒ Δm(τ, ω) = Δm(τ + 1, ω) Δm(τ, ω) − Z(1 − θm(τ + 1)) ≤ Δm(τ + 1, ω) + Z(1 − θm(τ)) Δm(τ, ω) + Z(1 − θm(τ + 1)) ≥ Δm(τ + 1, ω) − Z(1 − θm(τ)) ∀m ∈ M, ω ∈ Ω, τ ∈ [0, bτ] (11) Table 1: MILP for globally optimal resource scheduling. Tm = τd m − τa m + 1 is the time horizon for the agents MDP. Using this LP as a basis, we augment it with constraints that ensure that the resource usage implied by the agents occupation measures {xm} does not violate the global resource requirements bϕ at any time step τ ∈ [0, bτ]. To formulate these resource constraints, we use the following binary variables: • Δm(τ, ω) = {0, 1}, ∀m ∈ M, τ ∈ [0, bτ], ω ∈ Ω, which serve as indicator variables that define whether agent m possesses resource ω at time τ. These are analogous to the static indicator variables used in the one-shot static resource-allocation problem in [6]. • θm = {0, 1}, ∀m ∈ M, τ ∈ [0, bτ] are indicator variables that specify whether agent m is active (i.e., executing its MDP) at time τ. The meaning of resource-usage variables Δ is illustrated in Figure 1: Δm(τ, ω) = 1 only if resource ω is allocated to agent m at time τ. The meaning of the activity indicators θ is illustrated in Figure 2b: when agent m is in either the start state sb or the finish state sf , the corresponding θm = 0, but once the agent becomes active and enters one of the other states, we set θm = 1 . This meaning of θ can be enforced with a linear constraint that synchronizes the values of the agents occupation measures xm and the activity 1224 The Sixth Intl. Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) indicators θ, as shown in (6) in Table 1. Another constraint we have to add-because the activity indicators θ are defined on the global timeline τ-is to enforce the fact that the agent is inactive outside of its arrivaldeparture window. This is accomplished by constraint (7) in Table 1. Furthermore, agents should not be using resources while they are inactive. This constraint can also be enforced via a linear inequality on θ and Δ, as shown in (8). Constraint (6) sets the value of θ to match the policy defined by the occupation measure xm. In a similar fashion, we have to make sure that the resource-usage variables Δ are also synchronized with the occupation measure xm. This is done via constraint (9) in Table 1, which is nearly identical to the analogous constraint from [6]. After implementing the above constraint, which enforces the meaning of Δ, we add a constraint that ensures that the agents resource usage never exceeds the amounts of available resources. This condition is also trivially expressed as a linear inequality (10) in Table 1. Finally, for the problem formulation where resource assignments are static during a lifetime of an agent, we add a constraint that ensures that the resource-usage variables Δ do not change their value while the agent is active (θ = 1). This is accomplished via the linear constraint (11), where Z ≥ 2 is a constant that is used to turn off the constraints when θm(τ) = 0 or θm(τ + 1) = 0. This constraint is not used for the dynamic problem formulation, where resources can be reallocated between agents at every time step. To summarize, Table 1 together with the conservationof-flow constraints from (12) defines the MILP that simultaneously computes an optimal resource assignment for all agents across time as well as optimal finite-horizon MDP policies that are valid under that resource assignment. As a rough measure of the complexity of this MILP, let us consider the number of optimization variables and constraints. Let TM = P Tm = P m(τa m − τd m + 1) be the sum of the lengths of the arrival-departure windows across all agents. Then, the number of optimization variables is: TM + bτ|M||Ω| + bτ|M|, TM of which are continuous (xm), and bτ|M||Ω| + bτ|M| are binary (Δ and θ). However, notice that all but TM|M| of the θ are set to zero by constraint (7), which also immediately forces all but TM|M||Ω| of the Δ to be zero via the constraints (8). The number of constraints (not including the degenerate constraints in (7)) in the MILP is: TM + TM|Ω| + bτ|Ω| + bτ|M||Ω|. Despite the fact that the complexity of the MILP is, in the worst case, exponential1 in the number of binary variables, the complexity of this MILP is significantly (exponentially) lower than that of the MILP with flat utility functions, described in Section 2.2. This result echos the efficiency gains reported in [6] for single-shot resource-allocation problems, but is much more pronounced, because of the explosion of the flat utility representation due to the temporal aspect of the problem (recall the prohibitive complexity of the combinatorial optimization in Section 2.2). We empirically analyze the performance of this method in Section 5. 1 Strictly speaking, solving MILPs to optimality is NPcomplete in the number of integer variables. 5. EXPERIMENTAL RESULTS Although the complexity of solving MILPs is in the worst case exponential in the number of integer variables, there are many efficient methods for solving MILPs that allow our algorithm to scale well for parameters common to resource allocation and scheduling problems. In particular, this section introduces a problem domain-the repairshop problem-used to empirically evaluate our algorithms scalability in terms of the number of agents |M|, the number of shared resources |Ω|, and the varied lengths of global time bτ during which agents may enter and exit the system. The repairshop problem is a simple parameterized MDP adopting the metaphor of a vehicular repair shop. Agents in the repair shop are mechanics with a number of independent tasks that yield reward only when completed. In our MDP model of this system, actions taken to advance through the state space are only allowed if the agent holds certain resources that are publicly available to the shop. These resources are in finite supply, and optimal policies for the shop will determine when each agent may hold the limited resources to take actions and earn individual rewards. Each task to be completed is associated with a single action, although the agent is required to repeat the action numerous times before completing the task and earning a reward. This model was parameterized in terms of the number of agents in the system, the number of different types of resources that could be linked to necessary actions, a global time during which agents are allowed to arrive and depart, and a maximum length for the number of time steps an agent may remain in the system. All datapoints in our experiments were obtained with 20 evaluations using CPLEX to solve the MILPs on a Pentium4 computer with 2Gb of RAM. Trials were conducted on both the static and the dynamic version of the resourcescheduling problem, as defined earlier. Figure 3 shows the runtime and policy value for independent modifications to the parameter set. The top row shows how the solution time for the MILP scales as we increase the number of agents |M|, the global time horizon bτ, and the number of resources |Ω|. Increasing the number of agents leads to exponential complexity scaling, which is to be expected for an NP-complete problem. However, increasing the global time limit bτ or the total number of resource types |Ω|-while holding the number of agents constantdoes not lead to decreased performance. This occurs because the problems get easier as they become under-constrained, which is also a common phenomenon for NP-complete problems. We also observe that the solution to the dynamic version of the problem can often be computed much faster than the static version. The bottom row of Figure 3 shows the joint policy value of the policies that correspond to the computed optimal resource-allocation schedules. We can observe that the dynamic version yields higher reward (as expected, since the reward for the dynamic version is always no less than the reward of the static version). We should point out that these graphs should not be viewed as a measure of performance of two different algorithms (both algorithms produce optimal solutions but to different problems), but rather as observations about how the quality of optimal solutions change as more flexibility is allowed in the reallocation of resources. Figure 4 shows runtime and policy value for trials in which common input variables are scaled together. This allows The Sixth Intl. Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) 1225 2 4 6 8 10 10 −3 10 −2 10 −1 10 0 10 1 10 2 10 3 10 4 Number of Agents |M| CPUTime,sec |Ω| = 5, τ = 50 static dynamic 50 100 150 200 10 −2 10 −1 10 0 10 1 10 2 10 3 Global Time Boundary τ CPUTime,sec |M| = 5, |Ω| = 5 static dynamic 10 20 30 40 50 10 −2 10 −1 10 0 10 1 10 2 Number of Resources |Ω| CPUTime,sec |M| = 5, τ = 50 static dynamic 2 4 6 8 10 200 400 600 800 1000 1200 1400 1600 Number of Agents |M| Value |Ω| = 5, τ = 50 static dynamic 50 100 150 200 400 500 600 700 800 900 1000 1100 1200 1300 1400 Global Time Boundary τ Value |M| = 5, |Ω| = 5 static dynamic 10 20 30 40 50 500 600 700 800 900 1000 1100 1200 1300 1400 Number of Resources |Ω| Value |M| = 5, τ = 50 static dynamic Figure 3: Evaluation of our MILP for variable numbers of agents (column 1), lengths of global-time window (column 2), and numbers of resource types (column 3). Top row shows CPU time, and bottom row shows the joint reward of agents MDP policies. Error bars show the 1st and 3rd quartiles (25% and 75%). 2 4 6 8 10 10 −3 10 −2 10 −1 10 0 10 1 10 2 10 3 Number of Agents |M| CPUTime,sec τ = 10|M| static dynamic 2 4 6 8 10 10 −3 10 −2 10 −1 10 0 10 1 10 2 10 3 10 4 Number of Agents |M| CPUTime,sec |Ω| = 2|M| static dynamic 2 4 6 8 10 10 −3 10 −2 10 −1 10 0 10 1 10 2 10 3 10 4 Number of Agents |M| CPUTime,sec |Ω| = 5|M| static dynamic 2 4 6 8 10 200 400 600 800 1000 1200 1400 1600 1800 2000 2200 Number of Agents |M| Value τ = 10|M| static dynamic 2 4 6 8 10 200 400 600 800 1000 1200 1400 1600 1800 2000 Number of Agents |M| Value |Ω| = 2|M| static dynamic 2 4 6 8 10 0 500 1000 1500 2000 2500 Number of Agents |M| Value |Ω| = 5|M| static dynamic Figure 4: Evaluation of our MILP using correlated input variables. The left column tracks the performance and CPU time as the number of agents and global-time window increase together (bτ = 10|M|). The middle and the right column track the performance and CPU time as the number of resources and the number of agents increase together as |Ω| = 2|M| and |Ω| = 5|M|, respectively. Error bars show the 1st and 3rd quartiles (25% and 75%). 1226 The Sixth Intl. Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) us to explore domains where the total number of agents scales proportionally to the total number of resource types or the global time horizon, while keeping constant the average agent density (per unit of global time) or the average number of resources per agent (which commonly occurs in real-life applications). Overall, we believe that these experimental results indicate that our MILP formulation can be used to effectively solve resource-scheduling problems of nontrivial size. 6. DISCUSSION AND CONCLUSIONS Throughout the paper, we have made a number of assumptions in our model and solution algorithm; we discuss their implications below. • Continual execution. We assume that once an agent stops executing its MDP (transitions into state sf ), it exits the system and cannot return. It is easy to relax this assumption for domains where agents MDPs can be paused and restarted. All that is required is to include an additional pause action which transitions from a given state back to itself, and has zero reward. • Indifference to start time. We used a reward model where agents rewards depend only on the time horizon of their MDPs and not the global start time. This is a consequence of our MDP-augmentation procedure from Section 4.1. It is easy to extend the model so that the agents incur an explicit penalty for idling by assigning a non-zero negative reward to the start state sb . • Binary resource requirements. For simplicity, we have assumed that resource costs are binary: ϕm(a, ω) = {0, 1}, but our results generalize in a straightforward manner to non-binary resource mappings, analogously to the procedure used in [5]. • Cooperative agents. The optimization procedure discussed in this paper was developed in the context of cooperative agents, but it can also be used to design a mechanism for scheduling resources among self-interested agents. This optimization procedure can be embedded in a VickreyClarke-Groves auction, completely analogously to the way it was done in [7]. In fact, all the results of [7] about the properties of the auction and information privacy directly carry over to the scheduling domain discussed in this paper, requiring only slight modifications to deal with finitehorizon MDPs. • Known, deterministic arrival and departure times. Finally, we have assumed that agents arrival and departure times (τa m and τd m) are deterministic and known a priori. This assumption is fundamental to our solution method. While there are many domains where this assumption is valid, in many cases agents arrive and depart dynamically and their arrival and departure times can only be predicted probabilistically, leading to online resource-allocation problems. In particular, in the case of self-interested agents, this becomes an interesting version of an online-mechanism-design problem [11, 12]. In summary, we have presented an MILP formulation for the combinatorial resource-scheduling problem where agents values for possible resource assignments are defined by finitehorizon MDPs. This result extends previous work ([6, 7]) on static one-shot resource allocation under MDP-induced preferences to resource-scheduling problems with a temporal aspect. As such, this work takes a step in the direction of designing an online mechanism for agents with combinatorial resource preferences induced by stochastic planning problems. Relaxing the assumption about deterministic arrival and departure times of the agents is a focus of our future work. We would like to thank the anonymous reviewers for their insightful comments and suggestions. 7. REFERENCES [1] E. Altman and A. Shwartz. Adaptive control of constrained Markov chains: Criteria and policies. Annals of Operations Research, special issue on Markov Decision Processes, 28:101-134, 1991. [2] R. Bellman. Dynamic Programming. Princeton University Press, 1957. [3] C. Boutilier. Solving concisely expressed combinatorial auction problems. In Proc. of AAAI-02, pages 359-366, 2002. [4] C. Boutilier and H. H. Hoos. Bidding languages for combinatorial auctions. In Proc. of IJCAI-01, pages 1211-1217, 2001. [5] D. Dolgov. Integrated Resource Allocation and Planning in Stochastic Multiagent Environments. PhD thesis, Computer Science Department, University of Michigan, February 2006. [6] D. A. Dolgov and E. H. Durfee. Optimal resource allocation and policy formulation in loosely-coupled Markov decision processes. In Proc. of ICAPS-04, pages 315-324, June 2004. [7] D. A. Dolgov and E. H. Durfee. Computationally efficient combinatorial auctions for resource allocation in weakly-coupled MDPs. In Proc. of AAMAS-05, New York, NY, USA, 2005. ACM Press. [8] D. A. Dolgov and E. H. Durfee. Resource allocation among agents with preferences induced by factored MDPs. In Proc. of AAMAS-06, 2006. [9] K. Larson and T. Sandholm. Mechanism design and deliberative agents. In Proc. of AAMAS-05, pages 650-656, New York, NY, USA, 2005. ACM Press. [10] N. Nisan. Bidding and allocation in combinatorial auctions. In Electronic Commerce, 2000. [11] D. C. Parkes and S. Singh. An MDP-based approach to Online Mechanism Design. In Proc. of the Seventeenths Annual Conference on Neural Information Processing Systems (NIPS-03), 2003. [12] D. C. Parkes, S. Singh, and D. Yanovsky. Approximately efficient online mechanism design. In Proc. of the Eighteenths Annual Conference on Neural Information Processing Systems (NIPS-04), 2004. [13] M. L. Puterman. Markov Decision Processes. John Wiley & Sons, New York, 1994. [14] M. H. Rothkopf, A. Pekec, and R. M. Harstad. Computationally manageable combinational auctions. Management Science, 44(8):1131-1147, 1998. [15] T. Sandholm. An algorithm for optimal winner determination in combinatorial auctions. In Proc. of IJCAI-99, pages 542-547, San Francisco, CA, USA, 1999. Morgan Kaufmann Publishers Inc. The Sixth Intl. Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) 1227",
    "original_translation": "Programación de recursos combinatorios para MDP múltiples Dmitri A. Dolgov, Michael R. James y Michael E. Investigación técnica de AI y Robotics Group, Toyota Technical Center USA {Ddolgov, Michael.R.James, Michael.samples}@gmail.comLa programación óptima de los recursos en sistemas múltiples es una tarea computacionalmente desafiante, particularmente cuando los valores de los recursos no son aditivos. Consideramos el problema combinatorio de programar el uso de múltiples recursos entre los agentes que operan en entornos estocásticos, modelados como procesos de decisión de Markov (MDP). En los últimos años, se han desarrollado algoritmos eficientes de asignación de recursos para agentes con valores de recursos inducidos por los MDP. Sin embargo, este trabajo anterior se ha centrado en los problemas de asignación de recursos estáticos donde los recursos se distribuyen una vez y luego se utilizan en MDP de Horizon Infinite. Extendemos esos modelos existentes al problema de la programación de recursos combinatorios, donde los agentes persisten solo por períodos finitos entre sus tiempos de llegada y salida (predefinidas), que requieren recursos solo para esos períodos de tiempo. Proporcionamos un procedimiento computacionalmente eficiente para calcular las asignaciones de recursos óptimas a nivel mundial a los agentes a lo largo del tiempo. Ilustramos y analizamos empíricamente el método en el contexto de un dominio estocástico de cambio de empleo. Categorías y descriptores de sujetos I.2.8 [Inteligencia artificial]: resolución de problemas, métodos de control y búsqueda;I.2.11 [Inteligencia artificial]: Sistemas de inteligencia artificial distribuida Sistemas de términos generales Algoritmos, rendimiento, diseño 1. Introducción Las tareas de asignación y programación de recursos óptimos son ubicuas en los sistemas multiagentes, pero resolver tales problemas de optimización puede ser computacionalmente difícil, debido a una serie de factores. En particular, cuando el valor de un conjunto de recursos a un agente no es aditivo (como suele ser el caso con los recursos que son sustitutos o complementos), la función de utilidad podría deberse definirse en un espacio exponencialmente grande de paquetes de recursos queMuy rápidamente se vuelve computacionalmente intratable. Además, incluso cuando cada agente tiene una función de utilidad que no es cero solo en un pequeño subconjunto de los posibles paquetes de recursos, obtener una asignación óptima aún es computacionalmente prohibitiva, a medida que el problema se convierte en NP-complete [14]. Tales problemas computacionales han generado recientemente varios hilos de trabajo en el uso de modelos compactos de preferencias de agentes. Una idea es utilizar cualquier estructura presente en las funciones de utilidad para representarlas de manera compacta, a través de, por ejemplo, fórmulas lógicas [15, 10, 4, 3]. Una alternativa es modelar directamente los mecanismos que definen las funciones de utilidad de los agentes y realizar la asignación de recursos directamente con estos modelos [9]. Una forma de lograr esto es modelar los procesos por los cuales un agente podría utilizar los recursos y definir la función de utilidad como el pago de estos procesos. En particular, si un agente usa recursos para actuar en un entorno estocástico, su función de utilidad puede modelarse naturalmente con un proceso de decisión de Markov, cuyo conjunto de acciones se parametriza con los recursos disponibles. Esta representación se puede usar para construir algoritmos de asignación de recursos muy eficientes que conducen a una aceleración exponencial sobre un problema de optimización sencilla con representaciones planas de preferencias combinatorias [6, 7, 8]. Sin embargo, este trabajo existente sobre la asignación de recursos con preferencias inducidas por los MDP parametrizados por recursos asume que los recursos solo se asignan una vez y luego son utilizados por los agentes de forma independiente dentro de sus MDP infinitos. Esta suposición de que no es posible una reasignación de recursos puede ser limitante en los dominios donde los agentes llegan y salen dinámicamente. En este documento, ampliamos el trabajo sobre la asignación de recursos bajo las preferencias inducidas por MDP a problemas de programación de tiempo discreto, donde los agentes están presentes en el sistema durante intervalos de tiempo finitos y solo pueden usar recursos dentro de estos intervalos. En particular, los agentes llegan y salen en tiempos arbitrarios (predefinidos) y dentro de estos intervalos usan recursos para ejecutar tareas en los MDP de horizones finitos. Abordamos el problema de la programación de recursos a nivel mundial, donde el objetivo es encontrar una asignación de recursos a los agentes a lo largo del tiempo que maximiza la suma de las recompensas esperadas que obtienen. En este contexto, nuestra contribución principal es una formulación de programación de información mixta del problema de programación que elige asignaciones de recursos, tiempos de inicio y de ejecución globalmente óptimos para todos los agentes (dentro de su llegada1220 978-81-904262-7-5 (RPS) C)2007 Intervalos de salida de Ifaamas). Analizamos y comparamos empíricamente dos sabores del problema de programación: uno, donde los agentes tienen asignaciones de recursos estáticos dentro de sus MDP de horizones finitos, y otro, donde los recursos pueden reasignarse dinámicamente entre los agentes en cada paso de tiempo. En el resto del documento, primero establecemos las bases necesarias en la Sección 2 y luego presentamos nuestro modelo y la declaración formal del problema en la Sección 3. En la Sección 4.2, describimos nuestro resultado principal, el programa de optimización para la programación de recursos globalmente óptimo. Tras la discusión de nuestros resultados experimentales sobre un problema de programación de empleo en la Sección 5, concluimos en la Sección 6 con una discusión de posibles extensiones y generalizaciones de nuestro método.2. Antecedentes De manera similar al modelo utilizado en trabajos anteriores sobre recursos de recursos con preferencias inducidas por MDP [6, 7], definimos el valor de un conjunto de recursos a un agente como el valor de la mejor política de MDP que es realizable, dados esos recursos. Sin embargo, dado que el enfoque de nuestro trabajo está en los problemas de programación, y una gran parte del problema de optimización es decidir cómo se asignan los recursos en el tiempo entre los agentes con tiempos de llegada y salida finitos, modelamos los agentes que planean problemas como MDP de oraciones finitas., en contraste con los trabajos anteriores que usaron MDP con descuento de Horizon Infinite-Horizon. En el resto de esta sección, primero introducimos algunos antecedentes necesarios sobre los MDP de horizones finitos y presentamos una formulación de programación lineal que sirve como base para nuestro algoritmo de solución desarrollado en la Sección 4. También describimos los métodos estándar para la programación de recursos combinatorios con valores de recursos planos, que sirven como un punto de referencia de comparación para el nuevo modelo desarrollado aquí.2.1 Procesos de decisión de Markov Un MDP estacionario, de dominio finito y en tiempo discreto (ver, por ejemplo, [13] para un desarrollo exhaustivo y detallado) puede describirse como S, A, P, R, donde: S es un conjunto finitode estados del sistema;A es un conjunto finito de acciones que están disponibles para el agente;P es una función de transición estocástica estacionaria, donde p (σ | s, a) es la probabilidad de transición al estado σ al ejecutar la acción A en el estado s;r es una función de recompensa estacionaria, donde R (s, a) especifica la recompensa obtenida al ejecutar la acción A en el estado s.Dado dicho MDP, un problema de decisión bajo un horizonte finito T es elegir una acción óptima en cada paso de tiempo para maximizar el valor esperado de la recompensa total acumulada durante la vida de los agentes (finitos). La política óptima de los agentes es una función de los estados actuales y la hora hasta el horizonte. Una política óptima para tal problema es actuar con avidez con respecto a la función de valor óptimo, definida de manera recursiva por el siguiente sistema de ecuaciones de Bellman de tiempo finito [2]: V (S, T) = max a r, a) +X σ p (σ | s, a) v (σ, t + 1), ∀s ∈ S, t ∈ [1, t - 1];v (s, t) = 0, ∀s ∈ S;donde v (s, t) es el valor óptimo de estar en el estado s en el momento t ∈ [1, t]. Esta función de valor óptimo se puede calcular fácilmente utilizando una programación dinámica, lo que lleva a la siguiente política óptima π, donde π (s, a, t) es la probabilidad de ejecutar la acción A en el estado s en el tiempo t: π (s, a, t) = (1, a = argmaxa r (s, a) + p σ p (σ | s, a) v (σ, t + 1), 0, de lo contrario. Lo anterior es la forma más común de calcular la función de valor óptimo (y, por lo tanto, una política óptima) para un MDP de horillo finito. Sin embargo, también podemos formular el problema como el siguiente programa lineal (de manera similar al Dual LP para MDP con descuento de Horizon Infinite [13, 6, 7]): Max x s x a r (s, a) x t x (s, a a a a A, t) sujeto a: x a x (σ, a, t + 1) = x s, a p (σ | s, a) x (s, a, t) ∀σ, t ∈ [1, t - 1];X a x (s, a, 1) = α (s), ∀s ∈ S;(1) donde α (s) es la distribución inicial sobre el espacio de estado, y x es la medida de ocupación (no estacionaria) (x (s, a, t) ∈ [0, 1] es el número total esperado de vecesLa acción A se ejecuta en el estado en el momento t). Se obtiene una política óptima (no estacionaria) de la medida de ocupación de la siguiente manera: π (s, a, t) = x (s, a, t)/ x a x (s, a, t) ∀s ∈ S, t∈ [1, t].(2) Tenga en cuenta que el MDP de horizonte finito sin restricciones estándar, como se describió anteriormente, siempre tiene una solución uniformemente óptima (óptima para cualquier α (s) de distribución inicial). Por lo tanto, se puede obtener una política óptima utilizando una constante α (s)> 0 arbitraria (en particular, α (s) = 1 dará como resultado x (s, a, t) = π (s, a, t)). Sin embargo, para los MDP con restricciones de recursos (como se define a continuación en la Sección 3), no existen políticas uniformemente óptimas en general. En tales casos, α se convierte en parte de la entrada del problema, y una política resultante solo es óptima para ese α en particular. Este resultado es bien conocido por los MDP de horizonte infinito con varios tipos de restricciones [1, 6], y también es válida para nuestro modelo de horizonte finito, que puede establecerse fácilmente a través de una línea de razonamiento completamente análoga a los argumentos en [6]2.2 Programación de recursos combinatorios Un enfoque directo para la programación de recursos para un conjunto de agentes M, cuyos valores para los recursos son inducidos por problemas de planificación estocástica (en nuestro caso, los MDP de Horizon finito) sería que cada agente enumere todas las posibles asignaciones de recursos sobretiempo y, para cada uno, calcule su valor resolviendo el MDP correspondiente. Luego, cada agente proporcionaría valoraciones para cada posible paquete de recursos con el tiempo a un coordinador centralizado, que calcularía las tareas óptimas de recursos a través del tiempo en función de estas valoraciones. Cuando los recursos se pueden asignar en diferentes momentos a diferentes agentes, cada agente debe enviar valoraciones para cada combinación de posibles horizontes temporales. Deje que cada agente m ∈ M ejecute su MDP dentro del intervalo de tiempo de partida de llegada τ ∈ [τa m, τd m]. Por lo tanto, el agente M ejecutará un MDP con horizonte de tiempo no mayor que tm = τd m - τa m+1. Sea Bτ el horizonte de tiempo global para el problema, antes del cual todos los MDP de los agentes deben finalizar. Suponemos τd m <bτ, ∀m ∈ M. El sexto intl. Conf.En agentes autónomos y sistemas de múltiples agentes (AAMAS 07) 1221 Para el problema de programación donde los agentes tienen requisitos de recursos estáticos dentro de sus MDP de oraciones finitas, los agentes proporcionan una valoración para cada paquete de recursos para cada posible horizonte de tiempo (de [1, TM]) que pueden usar. Sea Ω el conjunto de recursos que se asignarán entre los agentes. Un agente obtendrá como máximo un paquete de recursos para uno de los horizontes temporales. Deje que la variable ψ ∈ ψm enumere todos los pares posibles de paquetes de recursos y horizontes temporales para el agente M, por lo que hay 2 | Ω |× valores TM para ψ (el espacio de los paquetes es exponencial en el número de tipos de recursos | ω |). El agente m debe proporcionar un valor vψ m para cada ψ, y el coordinador asignará como máximo un par ψ (recurso, horizonte de tiempo) a cada agente. Esta asignación se expresa como una variable indicadora zψ m ∈ {0, 1} que muestra si ψ se asigna al agente m.Para el tiempo τ y el recurso Ω, la función nm (ψ, τ, ω) ∈ {0, 1} indica si el paquete en ψ usa el recurso Ω en el tiempo τ (supuse que los agentes tienen requisitos de recursos binarios). Este problema de asignación es completado NP, incluso cuando se considera solo un paso de tiempo, y su dificultad aumenta significativamente con múltiples pasos de tiempo debido al creciente número de valores de ψ. El problema de encontrar una asignación óptima que satisfaga la restricción global de que la cantidad de cada recurso Ω asignado a todos los agentes no excede la cantidad disponible Bϕ (Ω) puede expresarse como el siguiente programa entero: max x m∈M x ψ∈ψmzψ mvψ m sujeto a: x ψ∈ no zψ m ≤ 1, ∀m ∈ M;X m∈M x ψ∈ no Zψ mnm (ψ, τ, Ω) ≤ bϕ (ω), ∀τ ∈ [1, bτ], ∀Ω ∈ ω;(3) La primera restricción en la ecuación 3 dice que ningún agente puede recibir más de un paquete, y la segunda restricción asegura que la asignación total de recursos ω no exceda, en ningún momento, el límite de recursos. Para el problema de programación donde los agentes pueden reasignar dinámicamente los recursos, cada agente debe especificar un valor para cada combinación de paquetes y pasos de tiempo dentro de su horizonte temporal. Deje que la variable ψ ∈ ψm en este caso enumere todos los paquetes de recursos posibles para los cuales, como máximo, se puede asignar un paquete al agente M en cada paso de tiempo. Por lo tanto, en este caso hay p t∈ [1, tm] (2 | ω |) t ∼ 2 | ω | tm posibilidades de paquetes de recursos asignados a diferentes ranuras de tiempo, para los horizontes de tiempo diferentes. El mismo conjunto de ecuaciones (3) se puede usar para resolver este problema de programación dinámica, pero el programa entero es diferente debido a la diferencia en cómo se define ψ. En este caso, el número de valores ψ es exponencial en cada agente que planee Horizon TM, lo que resulta en un programa mucho más grande. Este enfoque directo para resolver estos dos problemas de programación requiere una enumeración y solución de 2 | Ω |Tm (asignación estática) o p t∈ [1, tm] 2 | ω | t (reasignación dinámica) MDP para cada agente, que rápidamente se vuelve intratable con el crecimiento del número de recursos | ω |o el horizonte de tiempo tm.3. Modelo y declaración del problema ahora presentamos formalmente nuestro modelo del problema de reducción de recursos. La entrada del problema consiste en los siguientes componentes: • M, ω, Bϕ, τa m, τd m, bτ son como se definió anteriormente en la Sección 2.2.• {θm} = {s, a, pm, rm, αm} son los MDP de todos los agentes m ∈ M. Sin pérdida de generalidad, suponemos que los espacios de estado y de acción de todos los agentes son los mismos, pero cada uno tiene su propiofunción de transición PM, función de recompensa RM y condiciones iniciales αm.• ϕM: A × Ω → {0, 1} es la asignación de acciones a los recursos para el agente m.ϕm (a, ω) indica si la acción A del agente M necesita recurso ω. Un agente M que recibe un conjunto de recursos que no incluye el recurso ω no puede ejecutar en su política MDP cualquier acción A para la cual ϕm (a, Ω) = 0. Asumimos que todos los requisitos de recursos son binarios;Como se discute a continuación en la Sección 6, esta suposición no es limitante. Dada la entrada anterior, el problema de optimización que consideramos es encontrar la maximización óptima globalmente óptima, la suma de las recompensas esperadas de los recursos a los agentes para los pasos de tiempo: δ: τ × m × ω → {0, 1}. Es factible una solución si la asignación correspondiente de recursos a los agentes no viola la restricción global de recursos: x m Δm (τ, Ω) ≤ bϕ (ω), ∀ω ∈ ω, τ ∈ [1, bτ].(4) Consideramos dos sabores del problema de programación de recursos. La primera formulación restringe las asignaciones de recursos al espacio donde la asignación de recursos a cada agente es estática durante la vida de los agentes. La segunda formulación permite la reasignación de recursos entre los agentes en cada tiempo dentro de sus vidas. La Figura 1 muestra un problema de programación de recursos con tres agentes M = {M1, M2, M3}, tres recursos ω = {ω1, ω2, ω3}, y un horizonte de problemas global de Bτ = 11. Los tiempos de llegada y salida de los agentes se muestran como cajas grises y son {1, 6}, {3, 7} y {2, 11}, respectivamente. Se muestra una solución a este problema a través de barras horizontales dentro de cada caja de agentes, donde las barras corresponden a la asignación de los tres tipos de recursos. La Figura 1A muestra una solución a un problema de programación estática. Según la solución mostrada, el agente M1 comienza la ejecución de su MDP en el momento τ = 1 y tiene un bloqueo en los tres recursos hasta que termina la ejecución en el momento τ = 3. Tenga en cuenta que el Agente M1 renuncia a su control sobre los recursos antes de su tiempo de salida anunciado de τd M1 = 6, aparentemente porque otros agentes pueden utilizar los recursos de manera más efectiva. Por lo tanto, en el momento τ = 4, los recursos ω1 y ω3 se asignan al Agente M2, quien los usa para ejecutar su MDP (utilizando solo acciones soportadas por los recursos ω1 y ω3) hasta el tiempo τ = 7. El agente M3 contiene el recurso ω3 durante el intervalo τ ∈ [4, 10]. La Figura 1b muestra una posible solución a la versión dinámica del mismo problema. Allí, los recursos pueden reasignarse entre los agentes en cada paso de tiempo. Por ejemplo, el agente M1 da el uso de recursos ω2 en el momento τ = 2, aunque continúa la ejecución de su MDP hasta el tiempo τ = 6. Observe que un agente no puede detener y reiniciar su MDP, por lo que el agente M1 solo puede continuar ejecutándose en el intervalo τ ∈ [3, 4] si tiene acciones que no requieren ningún recurso (ϕm (a, Ω)= 0). Claramente, el modelo y el estado del problema descrito anteriormente hacen una serie de suposiciones sobre el problema y las propiedades de la solución deseadas. Discutimos algunos de esos supuestos y sus implicaciones en la Sección 6. 1222 El sexto intl. Conf.en agentes autónomos y sistemas de múltiples agentes (AAMAS 07) (a) (b) Figura 1: Ilustración de una solución a un problema de programación de recursos con tres agentes y tres recursos: a) Asignaciones de recursos estáticos (las asignaciones de recursos son constantes dentro de los agentesLifetimes; b) Asignación dinámica (las asignaciones de recursos pueden cambiar en cada paso de tiempo).4. Programación de recursos de nuestro algoritmo de programación de recursos en dos etapas. Primero, realizamos un paso de preprocesamiento que aumenta el agente MDP;Este proceso se describe en la Sección 4.1. En segundo lugar, utilizando estos MDP aumentados construimos un problema de optimización global, que se describe en la Sección 4.2.4.1 Agentes de aumento MDP en el modelo descrito en la sección anterior, suponemos que si un agente no posee los recursos necesarios para realizar acciones en su MDP, su ejecución se detiene y el agente deja el sistema. En otras palabras, los MDP no pueden detenerse y reanudarse. Por ejemplo, en el problema que se muestra en la Figura 1A, el Agente M1 libera todos los recursos después del tiempo τ = 3, momento en el cual se detiene la ejecución de su MDP. Del mismo modo, los agentes M2 y M3 solo ejecutan sus MDP en los intervalos τ ∈ [4, 6] y τ ∈ [4, 10], respectivamente. Por lo tanto, una parte importante del problema global de toma de decisiones es decidir la ventana de tiempo durante la cual cada uno de los agentes está activo (es decir, ejecutar su MDP). Para lograr esto, aumentamos cada MDP de los agentes con dos estados nuevos (los estados de inicio y finalización SB, SF, respectivamente) y una nueva acción de inicio/parada A ∗, como se ilustra en la Figura 2. La idea es que un agente permanece en el estado de inicio SB hasta que esté listo para ejecutar su MDP, momento en el que realiza la acción de inicio/parada A ∗ y las transiciones al espacio de estado del MDP original con la probabilidad de transición que corresponde aLa distribución inicial original α (s). Por ejemplo, en la Figura 1a, para el agente M2 esto sucedería en el tiempo τ = 4. Una vez que el agente llega al final de su ventana de actividad (tiempo τ = 6 para el agente M2 en la Figura 1A), realiza la acción de inicio/parada, lo que lo lleva al estado del sumidero SF en el momento τ = 7. Más precisamente, dado un MDP S, A, PM, RM, αM, definimos un MDP S, A, PM, RM, αM como sigue: S = S ∪ Sb ∪ SF;A = a ∪ a ∗;p (s | sb, a ∗) = α (s), ∀s ∈ S;P (Sb | Sb, a) = 1.0, ∀a ∈ A;p (sf | s, a ∗) = 1.0, ∀s ∈ S;p (σ | s, a) = p (σ | s, a), ∀s, σ ∈ S, a ∈ A;r (sb, a) = r (sf, a) = 0, ∀a ∈ A;r (s, a) = r (s, a), ∀s ∈ S, a ∈ A;α (SB) = 1;α (s) = 0, ∀s ∈ S;donde se supone que todas las probabilidades de transición no especificadas son cero. Además, para tener en cuenta el nuevo estado inicial, comenzamos el MDP un paso de tiempo antes, estableciendo τa m ← τa m-1. Esto no afectará la asignación de recursos debido a que las limitaciones de recursos solo se aplican para los estados originales de MDP, como se discutirá en la siguiente sección. Por ejemplo, los MDP aumentados que se muestran en la Figura 2b (que comienza en el estado SB en el momento τ = 2) se construirían a partir de un MDP con tiempo de llegada original τ = 3. La Figura 2b también muestra una trayectoria de muestra a través del espacio de estado: el agente comienza en el estado SB, se transforma al espacio de estado del MDP original y finalmente existe en el estado de sumidero SF. Tenga en cuenta que si quisiéramos modelar un problema en el que los agentes pudieran detener sus MDP en pasos de tiempo arbitrarios (lo que podría ser útil para dominios donde es posible la reasignación dinámica), podríamos lograrlo fácilmente al incluir una acción adicional que pasa de cada estado a sí mismo a sí mismo.con cero recompensa.4.2 MILP para la programación de recursos dado un conjunto de MDP aumentados, como se definió anteriormente, el objetivo de esta sección es formular un programa de optimización global que resuelva el problema de programación de recursos. En esta sección y debajo, se supone que todos los MDP son los MDP aumentados como se define en la Sección 4.1. Nuestro enfoque es similar a la idea utilizada en [6]: comenzamos con la formulación del programa lineal de los MDP de los agentes (1) y la aumentamos con limitaciones que aseguran que la asignación de recursos correspondiente entre los agentes y el tiempo sea válida. El problema de optimización resultante luego resuelve simultáneamente los MDP de los agentes y los problemas de programación de recursos. En el resto de esta sección, desarrollamos incrementalmente un programa entero mixto (MILP) que logra esto. En ausencia de restricciones de recursos, los agentes Finitehorizon MDPS son completamente independientes, y la solución globalmente óptima puede obtenerse de manera trivial a través del siguiente LP, que es simplemente una agregación de LPS de Hinitehorizon de un solo agente: Max X M X S X A RM (RM (RM (s, a) x t xm (s, a, t) sujeto a: x a xm (σ, a, t + 1) = x s, a pm (σ | s, a) xm (s, a, t), ∀m ∈ M, σ ∈ S, t ∈ [1, tm - 1];X a xm (s, a, 1) = αm (s), ∀m ∈ M, s ∈ S;(12) donde xm (s, a, t) es la medida de ocupación del agente m, y el sexto intl. Conf.en agentes autónomos y sistemas de múltiples agentes (AAMAS 07) 1223 (a) (b) Figura 2: Ilustración del aumento de un MDP para permitir tiempos de inicio y detención de variables: a) (izquierda) El MDP de dos estados original con un soloacción;(derecha) el MDP aumentado con los nuevos estados SB y SF y la nueva acción a ∗ (tenga en cuenta que las transiciones de origianl no se cambian en el proceso de aumento);b) El MDP aumentado se muestra como una trayectoria a través del tiempo (las líneas grises indican todas las transiciones, mientras que las líneas negras indican una trayectoria dada. Función objetivo (suma de recompensas esperadas sobre todos los agentes) Max x m x s x a rm (s, a) x t xm (s, a, t) (5) Significa implicaciones de restricciones lineales unte x a θ. El agente solo está activo cuando la medida de ocupación no es cero en los estados originales de MDP.θm (τ) = 0 = ⇒ xm (s, a, τ −τa m+1) = 0 ∀s /∈ {sb, sf}, a ∈ A x s /∈ {sb, sf} x a xm (s,a, t) ≤ θm (τa m + t - 1) ∀m ∈ M, ∀t ∈ [1, tm] (6) El agente solo puede estar activo en τ ∈ (τa m, τd m) θm (τ) = =0 ∀m ∈ M, τ /∈ (τa m, τd m) (7) no puede usar recursos cuando no está activo θm (τ) = 0 = ⇒ ΔM (τ, ω) = 0 ∀τ ∈ [0, bτ],ω ∈ ω ΔM (τ, ω) ≤ θm (τ) ∀m ∈ M, τ ∈ [0, bτ], ω ∈ ω (8) Ate x a δ (fuerzas de cero x correspondientes δ para ser nonoss)) Δm (τ, ω) = 0, ϕm (a, ω) = 1 = ⇒ xm (s, a, τ - τa m + 1) = 0 ∀s /∈ {sb, sf} 1 /| a |X a ϕm (a, ω) x s/∈ {sb, sf} xm (s, a, t) ≤ Δm (t + τa m - 1, ω) ∀m ∈ M, ω ∈ ω, t ∈ [1, Tm] (9) límites de recursos x m Δm (τ, ω) ≤ bϕ (Ω) ∀ω ∈ ω, τ ∈ [0, bτ] (10) El agente no puede cambiar los recursos mientras está activo. Solo habilitado para programar con tareas estáticas.θm (τ) = 1 y θm (τ + 1) = 1 = ⇒ ΔM (τ, ω) = ΔM (τ + 1, ω) ΔM (τ, ω) - z (1 - θm (τ + 1))≤ Δm (τ + 1, Ω) + z (1 - θm (τ)) Δm (τ, ω) + z (1 - θm (τ + 1)) ≥ ΔM (τ + 1, ω) - z (1- θm (τ)) ∀m ∈ M, Ω ∈ ω, τ ∈ [0, Bτ] (11) Tabla 1: MILP para la programación de recursos óptimo globalmente. Tm = τd m - τa m + 1 es el horizonte temporal para los agentes MDP. Utilizando este LP como base, lo aumentamos con limitaciones que aseguran que el uso de recursos implícito por las medidas de ocupación de los agentes {XM} no viole los requisitos de recursos globales Bϕ en ningún momento τ ∈ [0, bτ]. Para formular estas restricciones de recursos, usamos las siguientes variables binarias: • ΔM (τ, ω) = {0, 1}, ∀m ∈ M, τ ∈ [0, bτ], Ω ∈ ω, que sirve como variables indicadoras queDefina si el agente M posee recursos Ω en el momento τ. Estas son análogas a las variables indicadoras estáticas utilizadas en el problema de asignación de recursos estáticos de una sola vez en [6].• θm = {0, 1}, ∀m ∈ M, τ ∈ [0, bτ] son variables indicadoras que especifican si el agente m está activo (es decir, ejecutar su MDP) en el momento τ. El significado de las variables de uso de recursos δ se ilustra en la Figura 1: ΔM (τ, ω) = 1 Solo si el recurso Ω se asigna al agente M en el momento τ. El significado de los indicadores de actividad θ se ilustra en la Figura 2b: cuando el agente M está en el estado de inicio SB o en el estado de finalización SF, el correspondiente θm = 0, pero una vez que el agente se activa y entra en uno de los otros estados, nosotros, nosotros, nosotrosestablecer θm = 1. Este significado de θ puede aplicarse con una restricción lineal que sincroniza los valores de la ocupación de los agentes medidas XM y la actividad 1224 la sexta intl. Conf.en agentes autónomos y sistemas de múltiples agentes (AAMAS 07) Indicadores θ, como se muestra en (6) en la Tabla 1. Otra restricción que tenemos que agregar porque los indicadores de actividad θ se definen en la línea de tiempo global τ-IS para hacer cumplir el hecho de que el agente está inactivo fuera de su ventana de partida de llegada. Esto se logra por restricción (7) en la Tabla 1. Además, los agentes no deberían usar recursos mientras están inactivos. Esta restricción también se puede aplicar a través de una desigualdad lineal en θ y δ, como se muestra en (8). La restricción (6) establece el valor de θ para que coincida con la política definida por la medida de ocupación XM. De manera similar, debemos asegurarnos de que las variables de uso de recursos δ también se sincronizen con la medida de ocupación XM. Esto se realiza a través de la restricción (9) en la Tabla 1, que es casi idéntica a la restricción análoga de [6]. Después de implementar la restricción anterior, que hace cumplir el significado de δ, agregamos una restricción que garantiza que el uso de recursos de los agentes nunca exceda las cantidades de recursos disponibles. Esta condición también se expresa trivialmente como una desigualdad lineal (10) en la Tabla 1. Finalmente, para la formulación del problema donde las asignaciones de recursos son estáticas durante la vida útil de un agente, agregamos una restricción que asegura que las variables de uso de recursos δ no cambien su valor mientras el agente está activo (θ = 1). Esto se logra a través de la restricción lineal (11), donde z ≥ 2 es una constante que se usa para desactivar las restricciones cuando θm (τ) = 0 o θm (τ + 1) = 0. Esta restricción no se usa para la formulación de problemas dinámicos, donde los recursos pueden reasignarse entre los agentes en cada paso de tiempo. Para resumir, la Tabla 1 junto con la conservación de las restricciones de flujo de (12) define el MILP que calcula simultáneamente una asignación de recursos óptima para todos los agentes a lo largo del tiempo, así como las políticas MDP de Horizon finitas óptimas que son válidas bajo esa asignación de recursos. Como una medida aproximada de la complejidad de este MILP, consideremos el número de variables y restricciones de optimización. Sea tm = p tm = p m (τa m-τd m + 1) la suma de las longitudes de las ventanas de partida de llegada en todos los agentes. Entonces, el número de variables de optimización es: tm + bτ | m || Ω |+ bτ | m |, tm de los cuales son continuos (xm) y bτ | m || ω |+ bτ | m |son binarios (δ y θ). Sin embargo, observe que todos menos tm | m |del θ se establecen en cero por restricción (7), lo que también obliga inmediatamente a todos menos tm | m || ω |del δ para ser cero a través de las restricciones (8). El número de restricciones (sin incluir las restricciones degeneradas en (7)) en el MILP es: tm + tm | ω |+ bτ | ω |+ bτ | m || ω |. A pesar del hecho de que la complejidad del MILP es, en el peor de los casos, exponencial1 en el número de variables binarias, la complejidad de este MILP es significativamente (exponencialmente) más baja que la del MILP con funciones de utilidad planas, descrita en la Sección 2.2. Este resultado hace eco de las ganancias de eficiencia informadas en [6] para problemas de asignación de recursos de un solo disparo, pero es mucho más pronunciado, debido a la explosión de la representación de utilidad plana debido al aspecto temporal del problema (recuerde la complejidad prohibitiva delOptimización combinatoria en la Sección 2.2). Analizamos empíricamente el rendimiento de este método en la Sección 5. 1 Estrictamente hablando, resolver MILP a la optimización es NPComplete en el número de variables enteras.5. Resultados experimentales Aunque la complejidad de resolver MILP es en el peor de los casos exponencial en el número de variables enteras, existen muchos métodos eficientes para resolver MILP que permiten que nuestro algoritmo escala bien para los parámetros comunes a los problemas de asignación de recursos y programación. En particular, esta sección introduce un dominio problemático: el problema de la pantalla de reparación utilizado para evaluar empíricamente nuestros algoritmos escalabilidad en términos del número de agentes | M |, el número de recursos compartidos | Ω |qué agentes pueden ingresar y salir del sistema. El problema de reparación es un MDP parametrizado simple que adopta la metáfora de un taller de reparación vehicular. Los agentes en el taller de reparación son mecánicos con una serie de tareas independientes que producen recompensas solo cuando se completan. En nuestro modelo MDP de este sistema, las acciones tomadas para avanzar a través del espacio estatal solo se permiten si el agente posee ciertos recursos que están disponibles públicamente para la tienda. Estos recursos son de suministro finito, y las políticas óptimas para la tienda determinarán cuándo cada agente puede tener los recursos limitados para tomar medidas y obtener recompensas individuales. Cada tarea a completar se asocia con una sola acción, aunque el agente debe repetir la acción numerosas veces antes de completar la tarea y obtener una recompensa. Este modelo fue parametrizado en términos del número de agentes en el sistema, el número de diferentes tipos de recursos que podrían estar vinculados a las acciones necesarias, un tiempo global durante el cual los agentes pueden llegar y partir, y una longitud máxima para el númeroPasos de tiempo Un agente puede permanecer en el sistema. Todos los puntos de datos en nuestros experimentos se obtuvieron con 20 evaluaciones utilizando CPLEX para resolver los MILP en una computadora Pentium4 con 2 GB de RAM. Los ensayos se realizaron en la versión estática y dinámica del problema de reducción de recursos, como se definió anteriormente. La Figura 3 muestra el tiempo de ejecución y el valor de la política para modificaciones independientes al conjunto de parámetros. La fila superior muestra cómo el tiempo de solución para las escalas MILP a medida que aumentamos el número de agentes | M |, el horizonte de tiempo global Bτ y el número de recursos | ω |. El aumento del número de agentes conduce a la escala exponencial de complejidad, lo que se espera para un problema completado de NP. Sin embargo, aumentando el límite de tiempo global Bτ o el número total de tipos de recursos | Ω |-mientras mantiene el número de agentes constantes no conducen a una disminución del rendimiento. Esto ocurre porque los problemas se vuelven más fáciles a medida que se limitan, lo que también es un fenómeno común para los problemas completos de NP. También observamos que la solución a la versión dinámica del problema a menudo se puede calcular mucho más rápido que la versión estática. La fila inferior de la Figura 3 muestra el valor de la política conjunta de las políticas que corresponden a los horarios de asignación de recursos óptimos calculados. Podemos observar que la versión dinámica produce una recompensa más alta (como se esperaba, ya que la recompensa por la versión dinámica no es siempre menor que la recompensa de la versión estática). Debemos señalar que estos gráficos no deben verse como una medida del rendimiento de dos algoritmos diferentes (ambos algoritmos producen soluciones óptimas sino a diferentes problemas), sino como observaciones sobre cómo cambian la calidad de las soluciones óptimas a medida que se permite más flexibilidad enla reasignación de recursos. La Figura 4 muestra el tiempo de ejecución y el valor de la política para las pruebas en los que las variables de entrada comunes se escalan. Esto permite el sexto intl. Conf.En agentes autónomos y sistemas de múltiples agentes (AAMAS 07) 1225 2 4 6 8 10 10 −3 10 −2 10 −1 10 0 10 10 1 10 2 10 3 10 4 Número de agentes | M |Cputime, sec | ω |= 5, τ = 50 dinámica estática 50 100 150 200 10 −2 10 −1 10 0 10 10 1 10 2 10 3 Límite de tiempo global τ CPutime, Sec | M |= 5, | Ω |= 5 dinámica estática 10 20 30 40 50 10 −2 10 −1 10 0 10 1 10 2 Número de recursos | Ω |Cputime, sec | m |= 5, τ = 50 dinámica estática 2 4 6 8 10 200 400 600 800 1000 1200 1400 1600 Número de agentes | M |Valor | Ω |= 5, τ = 50 dinámica estática 50 100 150 200 400 500 600 700 800 900 1000 1100 1200 1300 1400 Tiempo global Límite τ Valor | M |= 5, | Ω |= 5 dinámica estática 10 20 30 40 50 500 600 700 800 900 1000 1100 1200 1300 1400 Número de recursos | Ω |Valor | M |= 5, τ = 50 dinámica estática Figura 3: Evaluación de nuestro MILP para números variables de agentes (columna 1), longitudes de la ventana de tiempo global (columna 2) y números de tipos de recursos (columna 3). La fila superior muestra el tiempo de la CPU, y la fila inferior muestra la recompensa conjunta de las políticas de los agentes MDP. Las barras de error muestran los cuartiles 1er y tercero (25% y 75%).2 4 6 8 10 10 −3 10 −2 10 −1 10 0 10 1 10 2 10 3 Número de agentes | M |Cputime, sec τ = 10 | m |Dinámica estática 2 4 6 8 10 10 −3 10 −2 10 −1 10 0 10 1 10 2 10 3 3 10 4 Número de agentes | M |Cputime, sec | ω |= 2 | m |Dinámica estática 2 4 6 8 10 10 −3 10 −2 10 −1 10 0 10 1 10 2 10 3 3 10 4 Número de agentes | M |Cputime, sec | ω |= 5 | m |Dinámica estática 2 4 6 8 10 200 400 600 800 1000 1200 1400 1600 1800 2000 2200 Número de agentes | M |Valor τ = 10 | m |Dinámica estática 2 4 6 8 10 200 400 600 800 1000 1200 1400 1600 1800 2000 Número de agentes | M |Valor | Ω |= 2 | m |Dinámica estática 2 4 6 8 10 0 500 1000 1500 2000 2500 Número de agentes | M |Valor | Ω |= 5 | m |Dinámica estática Figura 4: Evaluación de nuestro MILP utilizando variables de entrada correlacionadas. La columna izquierda rastrea el rendimiento y el tiempo de la CPU a medida que el número de agentes y la ventana de tiempo global aumentan (bτ = 10 | m |). La columna central y correcta rastrea el rendimiento y el tiempo de la CPU como el número de recursos y el número de agentes aumentan juntos como | ω |= 2 | m |y | ω |= 5 | m |, respectivamente. Las barras de error muestran los cuartiles 1er y tercero (25% y 75%).1226 El sexto intl. Conf.En agentes autónomos y sistemas de múltiples agentes (AAMAS 07) para explorar dominios donde el número total de agentes escala proporcionalmente al número total de tipos de recursos o al horizonte de tiempo global, mientras mantiene constante la densidad de agente promedio (por unidad de tiempo global) o el número promedio de recursos por agente (que se produce comúnmente en aplicaciones de la vida real). En general, creemos que estos resultados experimentales indican que nuestra formulación MILP puede usarse para resolver efectivamente problemas de tamaño no trivial.6. Discusión y conclusiones En todo el documento, hemos hecho una serie de suposiciones en nuestro algoritmo de modelo y solución;Discutimos sus implicaciones a continuación.• Ejecución continua. Suponemos que una vez que un agente deja de ejecutar su MDP (transiciones a State SF), sale del sistema y no puede devolver. Es fácil relajar esta suposición para los dominios donde los MDP de los agentes pueden ser pausados y reiniciados. Todo lo que se requiere es incluir una acción de pausa adicional que transique de un estado dado a sí mismo, y tiene cero recompensa.• Indiferencia a la hora de inicio. Utilizamos un modelo de recompensa donde las recompensas de los agentes dependen solo del horizonte temporal de sus MDP y no del tiempo de inicio global. Esta es una consecuencia de nuestro procedimiento de augatización MDP de la Sección 4.1. Es fácil extender el modelo para que los agentes incurran en una penalización explícita por ralentí al asignar una recompensa negativa no cero al estado de inicio SB.• Requisitos de recursos binarios. Para simplificar, hemos asumido que los costos de recursos son binarios: ϕm (a, ω) = {0, 1}, pero nuestros resultados se generalizan de manera directa a las asignaciones de recursos no binarias, análogos al procedimiento utilizado en [5].• Agentes cooperativos. El procedimiento de optimización discutido en este documento se desarrolló en el contexto de los agentes cooperativos, pero también se puede utilizar para diseñar un mecanismo para programar recursos entre los agentes egoístas. Este procedimiento de optimización se puede incrustar en una subasta Vickreyclarke-Groves, completamente análoga a la forma en que se hizo en [7]. De hecho, todos los resultados de [7] sobre las propiedades de la subasta y la privacidad de la información se trasladan directamente al dominio de programación discutido en este documento, lo que requiere solo pequeñas modificaciones para tratar con los MDP de Horizon Finite• Los tiempos de llegada y salida deterministas conocidas. Finalmente, hemos asumido que los agentes de llegada y tiempos de salida (τa my τd m) son deterministas y conocidos a priori. Esta suposición es fundamental para nuestro método de solución. Si bien hay muchos dominios en los que esta suposición es válida, en muchos casos los agentes llegan y salen dinámicamente y sus tiempos de llegada y salida solo pueden predecirse probabilísticamente, lo que lleva a problemas de asignación de recursos en línea. En particular, en el caso de los agentes egoístas, esto se convierte en una versión interesante de un problema de diseño de mecanismo en línea [11, 12]. En resumen, hemos presentado una formulación MILP para el problema de reducción de recursos combinatorios donde los valores de los agentes para posibles asignaciones de recursos están definidos por los MDP de Horizon de Finite. Este resultado extiende el trabajo anterior ([6, 7]) en la asignación estática de recursos de un solo disparo bajo las preferencias inducidas por MDP a problemas de programación de recursos con un aspecto temporal. Como tal, este trabajo da un paso en la dirección de diseñar un mecanismo en línea para agentes con preferencias de recursos combinatorios inducidas por problemas de planificación estocástica. Relajarse la suposición de la llegada determinista y los tiempos de salida de los agentes es un foco de nuestro trabajo futuro. Nos gustaría agradecer a los revisores anónimos por sus perspicaces comentarios y sugerencias.7. Referencias [1] E. Altman y A. Shwartz. Control adaptativo de las cadenas de Markov restringidas: criterios y políticas. Annals of Operations Research, Número especial sobre los procesos de decisión de Markov, 28: 101-134, 1991. [2] R. Bellman. Programación dinámica. Princeton University Press, 1957. [3] C. Boutilier. Resolución de problemas de subasta combinatorios expresados de manera concisa. En Proc.de AAAI-02, páginas 359-366, 2002. [4] C. Boutilier y H. H. Hoos. Idiomas de licitación para subastas combinatorias. En Proc.de IJCAI-01, páginas 1211-1217, 2001. [5] D. Dolgov. Asignación y planificación de recursos integrados en entornos múltiples estocásticos. Tesis doctoral, Departamento de Ciencias de la Computación, Universidad de Michigan, febrero de 2006. [6] D. A. Dolgov y E. H. Durfee. Asignación óptima de recursos y formulación de políticas en procesos de decisión de Markov libremente acoplados. En Proc.de ICAPS-04, páginas 315-324, junio de 2004. [7] D. A. Dolgov y E. H. Durfee. Subastas combinatorias computacionalmente eficientes para la asignación de recursos en MDP débilmente acoplados. En Proc.de Aamas-05, Nueva York, NY, EE. UU., 2005. ACM Press.[8] D. A. Dolgov y E. H. Durfee. Asignación de recursos entre agentes con preferencias inducidas por los MDP factorizados. En Proc.de Aamas-06, 2006. [9] K. Larson y T. Sandholm. Diseño de mecanismo y agentes deliberativos. En Proc.de AAMAS-05, páginas 650-656, Nueva York, NY, EE. UU., 2005. ACM Press.[10] N. Nisan. Licitación y asignación en subastas combinatorias. En Electronic Commerce, 2000. [11] D. C. Parkes y S. Singh. Un enfoque basado en MDP para el diseño del mecanismo en línea. En Proc.de la Conferencia Anual de los Decideteseciosidades sobre Sistemas de Procesamiento de Información Neural (NIPS-03), 2003. [12] D. C. Parkes, S. Singh y D. Yanovsky. Diseño de mecanismo en línea aproximadamente eficiente. En Proc.de la Decimoctavo Conferencia Anual sobre Sistemas de Procesamiento de Información Neural (NIPS-04), 2004. [13] M. L. Puterman. Procesos de decisión de Markov. John Wiley & Sons, Nueva York, 1994. [14] M. H. Rothkopf, A. Pekec y R. M. Harstad. Subastas combinacionales manejables computacionalmente. Management Science, 44 (8): 1131-1147, 1998. [15] T. Sandholm. Un algoritmo para una determinación óptima del ganador en subastas combinatorias. En Proc.de IJCAI-99, páginas 542-547, San Francisco, CA, EE. UU., 1999. Morgan Kaufmann Publishers Inc. El sexto intl. Conf.en agentes autónomos y sistemas de múltiples agentes (AAMAS 07) 1227",
    "original_sentences": [
        "Combinatorial Resource Scheduling for Multiagent MDPs Dmitri A. Dolgov, Michael R. James, and Michael E. Samples AI and Robotics Group Technical Research, Toyota Technical Center USA {ddolgov, michael.r.james, michael.samples}@gmail.com ABSTRACT Optimal resource scheduling in multiagent systems is a computationally challenging task, particularly when the values of resources are not additive.",
        "We consider the combinatorial problem of scheduling the usage of multiple resources among agents that operate in stochastic environments, modeled as Markov decision processes (MDPs).",
        "In recent years, efficient resource-allocation algorithms have been developed for agents with resource values induced by MDPs.",
        "However, this prior work has focused on static resource-allocation problems where resources are distributed once and then utilized in infinite-horizon MDPs.",
        "We extend those existing models to the problem of combinatorial resource scheduling, where agents persist only for finite periods between their (predefined) arrival and departure times, requiring resources only for those time periods.",
        "We provide a computationally efficient procedure for computing globally optimal resource assignments to agents over time.",
        "We illustrate and empirically analyze the method in the context of a stochastic jobscheduling domain.",
        "Categories and Subject Descriptors I.2.8 [Artificial Intelligence]: Problem Solving, Control Methods, and Search; I.2.11 [Artificial Intelligence]: Distributed Artificial Intelligence-Multiagent systems General Terms Algorithms, Performance, Design 1.",
        "INTRODUCTION The tasks of optimal resource allocation and scheduling are ubiquitous in multiagent systems, but solving such optimization problems can be computationally difficult, due to a number of factors.",
        "In particular, when the value of a set of resources to an agent is not additive (as is often the case with resources that are substitutes or complements), the utility function might have to be defined on an exponentially large space of resource bundles, which very quickly becomes computationally intractable.",
        "Further, even when each agent has a utility function that is nonzero only on a small subset of the possible resource bundles, obtaining optimal allocation is still computationally prohibitive, as the problem becomes NP-complete [14].",
        "Such computational issues have recently spawned several threads of work in using compact models of agents preferences.",
        "One idea is to use any structure present in utility functions to represent them compactly, via, for example, logical formulas [15, 10, 4, 3].",
        "An alternative is to directly model the mechanisms that define the agents utility functions and perform resource allocation directly with these models [9].",
        "A way of accomplishing this is to model the processes by which an agent might utilize the resources and define the utility function as the payoff of these processes.",
        "In particular, if an agent uses resources to act in a stochastic environment, its utility function can be naturally modeled with a Markov decision process, whose action set is parameterized by the available resources.",
        "This representation can then be used to construct very efficient resource-allocation algorithms that lead to an exponential speedup over a straightforward optimization problem with flat representations of combinatorial preferences [6, 7, 8].",
        "However, this existing work on resource allocation with preferences induced by resource-parameterized MDPs makes an assumption that the resources are only allocated once and are then utilized by the agents independently within their infinite-horizon MDPs.",
        "This assumption that no reallocation of resources is possible can be limiting in domains where agents arrive and depart dynamically.",
        "In this paper, we extend the work on resource allocation under MDP-induced preferences to discrete-time scheduling problems, where agents are present in the system for finite time intervals and can only use resources within these intervals.",
        "In particular, agents arrive and depart at arbitrary (predefined) times and within these intervals use resources to execute tasks in finite-horizon MDPs.",
        "We address the problem of globally optimal resource scheduling, where the objective is to find an allocation of resources to the agents across time that maximizes the sum of the expected rewards that they obtain.",
        "In this context, our main contribution is a mixed-integerprogramming formulation of the scheduling problem that chooses globally optimal resource assignments, starting times, and execution horizons for all agents (within their arrival1220 978-81-904262-7-5 (RPS) c 2007 IFAAMAS departure intervals).",
        "We analyze and empirically compare two flavors of the scheduling problem: one, where agents have static resource assignments within their finite-horizon MDPs, and another, where resources can be dynamically reallocated between agents at every time step.",
        "In the rest of the paper, we first lay down the necessary groundwork in Section 2 and then introduce our model and formal problem statement in Section 3.",
        "In Section 4.2, we describe our main result, the optimization program for globally optimal resource scheduling.",
        "Following the discussion of our experimental results on a job-scheduling problem in Section 5, we conclude in Section 6 with a discussion of possible extensions and generalizations of our method. 2.",
        "BACKGROUND Similarly to the model used in previous work on resourceallocation with MDP-induced preferences [6, 7], we define the value of a set of resources to an agent as the value of the best MDP policy that is realizable, given those resources.",
        "However, since the focus of our work is on scheduling problems, and a large part of the optimization problem is to decide how resources are allocated in time among agents with finite arrival and departure times, we model the agents planning problems as finite-horizon MDPs, in contrast to previous work that used infinite-horizon discounted MDPs.",
        "In the rest of this section, we first introduce some necessary background on finite-horizon MDPs and present a linear-programming formulation that serves as the basis for our solution algorithm developed in Section 4.",
        "We also outline the standard methods for combinatorial resource scheduling with flat resource values, which serve as a comparison benchmark for the new model developed here. 2.1 Markov Decision Processes A stationary, finite-domain, discrete-time MDP (see, for example, [13] for a thorough and detailed development) can be described as S, A, p, r , where: S is a finite set of system states; A is a finite set of actions that are available to the agent; p is a stationary stochastic transition function, where p(σ|s, a) is the probability of transitioning to state σ upon executing action a in state s; r is a stationary reward function, where r(s, a) specifies the reward obtained upon executing action a in state s. Given such an MDP, a decision problem under a finite horizon T is to choose an optimal action at every time step to maximize the expected value of the total reward accrued during the agents (finite) lifetime.",
        "The agents optimal policy is then a function of current state s and the time until the horizon.",
        "An optimal policy for such a problem is to act greedily with respect to the optimal value function, defined recursively by the following system of finite-time Bellman equations [2]: v(s, t) = max a r(s, a) + X σ p(σ|s, a)v(σ, t + 1), ∀s ∈ S, t ∈ [1, T − 1]; v(s, T) = 0, ∀s ∈ S; where v(s, t) is the optimal value of being in state s at time t ∈ [1, T].",
        "This optimal value function can be easily computed using dynamic programming, leading to the following optimal policy π, where π(s, a, t) is the probability of executing action a in state s at time t: π(s, a, t) = ( 1, a = argmaxa r(s, a) + P σ p(σ|s, a)v(σ, t + 1), 0, otherwise.",
        "The above is the most common way of computing the optimal value function (and therefore an optimal policy) for a finite-horizon MDP.",
        "However, we can also formulate the problem as the following linear program (similarly to the dual LP for infinite-horizon discounted MDPs [13, 6, 7]): max X s X a r(s, a) X t x(s, a, t) subject to: X a x(σ, a, t + 1) = X s,a p(σ|s, a)x(s, a, t) ∀σ, t ∈ [1, T − 1]; X a x(s, a, 1) = α(s), ∀s ∈ S; (1) where α(s) is the initial distribution over the state space, and x is the (non-stationary) occupation measure (x(s, a, t) ∈ [0, 1] is the total expected number of times action a is executed in state s at time t).",
        "An optimal (non-stationary) policy is obtained from the occupation measure as follows: π(s, a, t) = x(s, a, t)/ X a x(s, a, t) ∀s ∈ S, t ∈ [1, T]. (2) Note that the standard unconstrained finite-horizon MDP, as described above, always has a uniformly-optimal solution (optimal for any initial distribution α(s)).",
        "Therefore, an optimal policy can be obtained by using an arbitrary constant α(s) > 0 (in particular, α(s) = 1 will result in x(s, a, t) = π(s, a, t)).",
        "However, for MDPs with resource constraints (as defined below in Section 3), uniformly-optimal policies do not in general exist.",
        "In such cases, α becomes a part of the problem input, and a resulting policy is only optimal for that particular α.",
        "This result is well known for infinite-horizon MDPs with various types of constraints [1, 6], and it also holds for our finite-horizon model, which can be easily established via a line of reasoning completely analogous to the arguments in [6]. 2.2 Combinatorial Resource Scheduling A straightforward approach to resource scheduling for a set of agents M, whose values for the resources are induced by stochastic planning problems (in our case, finite-horizon MDPs) would be to have each agent enumerate all possible resource assignments over time and, for each one, compute its value by solving the corresponding MDP.",
        "Then, each agent would provide valuations for each possible resource bundle over time to a centralized coordinator, who would compute the optimal resource assignments across time based on these valuations.",
        "When resources can be allocated at different times to different agents, each agent must submit valuations for every combination of possible time horizons.",
        "Let each agent m ∈ M execute its MDP within the arrival-departure time interval τ ∈ [τa m, τd m].",
        "Hence, agent m will execute an MDP with time horizon no greater than Tm = τd m−τa m+1.",
        "Let bτ be the global time horizon for the problem, before which all of the agents MDPs must finish.",
        "We assume τd m < bτ, ∀m ∈ M. The Sixth Intl.",
        "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) 1221 For the scheduling problem where agents have static resource requirements within their finite-horizon MDPs, the agents provide a valuation for each resource bundle for each possible time horizon (from [1, Tm]) that they may use.",
        "Let Ω be the set of resources to be allocated among the agents.",
        "An agent will get at most one resource bundle for one of the time horizons.",
        "Let the variable ψ ∈ Ψm enumerate all possible pairs of resource bundles and time horizons for agent m, so there are 2|Ω| × Tm values for ψ (the space of bundles is exponential in the number of resource types |Ω|).",
        "The agent m must provide a value vψ m for each ψ, and the coordinator will allocate at most one ψ (resource, time horizon) pair to each agent.",
        "This allocation is expressed as an indicator variable zψ m ∈ {0, 1} that shows whether ψ is assigned to agent m. For time τ and resource ω, the function nm(ψ, τ, ω) ∈ {0, 1} indicates whether the bundle in ψ uses resource ω at time τ (we make the assumption that agents have binary resource requirements).",
        "This allocation problem is NP-complete, even when considering only a single time step, and its difficulty increases significantly with multiple time steps because of the increasing number of values of ψ.",
        "The problem of finding an optimal allocation that satisfies the global constraint that the amount of each resource ω allocated to all agents does not exceed the available amount bϕ(ω) can be expressed as the following integer program: max X m∈M X ψ∈Ψm zψ mvψ m subject to: X ψ∈Ψm zψ m ≤ 1, ∀m ∈ M; X m∈M X ψ∈Ψm zψ mnm(ψ, τ, ω) ≤ bϕ(ω), ∀τ ∈ [1, bτ], ∀ω ∈ Ω; (3) The first constraint in equation 3 says that no agent can receive more than one bundle, and the second constraint ensures that the total assignment of resource ω does not, at any time, exceed the resource bound.",
        "For the scheduling problem where the agents are able to dynamically reallocate resources, each agent must specify a value for every combination of bundles and time steps within its time horizon.",
        "Let the variable ψ ∈ Ψm in this case enumerate all possible resource bundles for which at most one bundle may be assigned to agent m at each time step.",
        "Therefore, in this case there are P t∈[1,Tm](2|Ω| )t ∼ 2|Ω|Tm possibilities of resource bundles assigned to different time slots, for the Tm different time horizons.",
        "The same set of equations (3) can be used to solve this dynamic scheduling problem, but the integer program is different because of the difference in how ψ is defined.",
        "In this case, the number of ψ values is exponential in each agents planning horizon Tm, resulting in a much larger program.",
        "This straightforward approach to solving both of these scheduling problems requires an enumeration and solution of either 2|Ω| Tm (static allocation) or P t∈[1,Tm] 2|Ω|t (dynamic reallocation) MDPs for each agent, which very quickly becomes intractable with the growth of the number of resources |Ω| or the time horizon Tm. 3.",
        "MODEL AND PROBLEM STATEMENT We now formally introduce our model of the resourcescheduling problem.",
        "The problem input consists of the following components: • M, Ω, bϕ, τa m, τd m, bτ are as defined above in Section 2.2. • {Θm} = {S, A, pm, rm, αm} are the MDPs of all agents m ∈ M. Without loss of generality, we assume that state and action spaces of all agents are the same, but each has its own transition function pm, reward function rm, and initial conditions αm. • ϕm : A×Ω → {0, 1} is the mapping of actions to resources for agent m. ϕm(a, ω) indicates whether action a of agent m needs resource ω.",
        "An agent m that receives a set of resources that does not include resource ω cannot execute in its MDP policy any action a for which ϕm(a, ω) = 0.",
        "We assume all resource requirements are binary; as discussed below in Section 6, this assumption is not limiting.",
        "Given the above input, the optimization problem we consider is to find the globally optimal-maximizing the sum of expected rewards-mapping of resources to agents for all time steps: Δ : τ × M × Ω → {0, 1}.",
        "A solution is feasible if the corresponding assignment of resources to the agents does not violate the global resource constraint: X m Δm(τ, ω) ≤ bϕ(ω), ∀ω ∈ Ω, τ ∈ [1, bτ]. (4) We consider two flavors of the resource-scheduling problem.",
        "The first formulation restricts resource assignments to the space where the allocation of resources to each agent is static during the agents lifetime.",
        "The second formulation allows reassignment of resources between agents at every time step within their lifetimes.",
        "Figure 1 depicts a resource-scheduling problem with three agents M = {m1, m2, m3}, three resources Ω = {ω1, ω2, ω3}, and a global problem horizon of bτ = 11.",
        "The agents arrival and departure times are shown as gray boxes and are {1, 6}, {3, 7}, and {2, 11}, respectively.",
        "A solution to this problem is shown via horizontal bars within each agents box, where the bars correspond to the allocation of the three resource types.",
        "Figure 1a shows a solution to a static scheduling problem.",
        "According to the shown solution, agent m1 begins the execution of its MDP at time τ = 1 and has a lock on all three resources until it finishes execution at time τ = 3.",
        "Note that agent m1 relinquishes its hold on the resources before its announced departure time of τd m1 = 6, ostensibly because other agents can utilize the resources more effectively.",
        "Thus, at time τ = 4, resources ω1 and ω3 are allocated to agent m2, who then uses them to execute its MDP (using only actions supported by resources ω1 and ω3) until time τ = 7.",
        "Agent m3 holds resource ω3 during the interval τ ∈ [4, 10].",
        "Figure 1b shows a possible solution to the dynamic version of the same problem.",
        "There, resources can be reallocated between agents at every time step.",
        "For example, agent m1 gives up its use of resource ω2 at time τ = 2, although it continues the execution of its MDP until time τ = 6.",
        "Notice that an agent is not allowed to stop and restart its MDP, so agent m1 is only able to continue executing in the interval τ ∈ [3, 4] if it has actions that do not require any resources (ϕm(a, ω) = 0).",
        "Clearly, the model and problem statement described above make a number of assumptions about the problem and the desired solution properties.",
        "We discuss some of those assumptions and their implications in Section 6. 1222 The Sixth Intl.",
        "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) (a) (b) Figure 1: Illustration of a solution to a resource-scheduling problem with three agents and three resources: a) static resource assignments (resource assignments are constant within agents lifetimes; b) dynamic assignment (resource assignments are allowed to change at every time step). 4.",
        "RESOURCE SCHEDULING Our resource-scheduling algorithm proceeds in two stages.",
        "First, we perform a preprocessing step that augments the agent MDPs; this process is described in Section 4.1.",
        "Second, using these augmented MDPs we construct a global optimization problem, which is described in Section 4.2. 4.1 Augmenting Agents MDPs In the model described in the previous section, we assume that if an agent does not possess the necessary resources to perform actions in its MDP, its execution is halted and the agent leaves the system.",
        "In other words, the MDPs cannot be paused and resumed.",
        "For example, in the problem shown in Figure 1a, agent m1 releases all resources after time τ = 3, at which point the execution of its MDP is halted.",
        "Similarly, agents m2 and m3 only execute their MDPs in the intervals τ ∈ [4, 6] and τ ∈ [4, 10], respectively.",
        "Therefore, an important part of the global decision-making problem is to decide the window of time during which each of the agents is active (i.e., executing its MDP).",
        "To accomplish this, we augment each agents MDP with two new states (start and finish states sb , sf , respectively) and a new start/stop action a∗ , as illustrated in Figure 2.",
        "The idea is that an agent stays in the start state sb until it is ready to execute its MDP, at which point it performs the start/stop action a∗ and transitions into the state space of the original MDP with the transition probability that corresponds to the original initial distribution α(s).",
        "For example, in Figure 1a, for agent m2 this would happen at time τ = 4.",
        "Once the agent gets to the end of its activity window (time τ = 6 for agent m2 in Figure 1a), it performs the start/stop action, which takes it into the sink finish state sf at time τ = 7.",
        "More precisely, given an MDP S, A, pm, rm, αm , we define an augmented MDP S , A , pm, rm, αm as follows: S = S ∪ sb ∪ sf ; A = A ∪ a∗ ; p (s|sb , a∗ ) = α(s), ∀s ∈ S; p (sb |sb , a) = 1.0, ∀a ∈ A; p (sf |s, a∗ ) = 1.0, ∀s ∈ S; p (σ|s, a) = p(σ|s, a), ∀s, σ ∈ S, a ∈ A; r (sb , a) = r (sf , a) = 0, ∀a ∈ A ; r (s, a) = r(s, a), ∀s ∈ S, a ∈ A; α (sb ) = 1; α (s) = 0, ∀s ∈ S; where all non-specified transition probabilities are assumed to be zero.",
        "Further, in order to account for the new starting state, we begin the MDP one time-step earlier, setting τa m ← τa m − 1.",
        "This will not affect the resource allocation due to the resource constraints only being enforced for the original MDP states, as will be discussed in the next section.",
        "For example, the augmented MDPs shown in Figure 2b (which starts in state sb at time τ = 2) would be constructed from an MDP with original arrival time τ = 3.",
        "Figure 2b also shows a sample trajectory through the state space: the agent starts in state sb , transitions into the state space S of the original MDP, and finally exists into the sink state sf .",
        "Note that if we wanted to model a problem where agents could pause their MDPs at arbitrary time steps (which might be useful for domains where dynamic reallocation is possible), we could easily accomplish this by including an extra action that transitions from each state to itself with zero reward. 4.2 MILP for Resource Scheduling Given a set of augmented MDPs, as defined above, the goal of this section is to formulate a global optimization program that solves the resource-scheduling problem.",
        "In this section and below, all MDPs are assumed to be the augmented MDPs as defined in Section 4.1.",
        "Our approach is similar to the idea used in [6]: we begin with the linear-program formulation of agents MDPs (1) and augment it with constraints that ensure that the corresponding resource allocation across agents and time is valid.",
        "The resulting optimization problem then simultaneously solves the agents MDPs and resource-scheduling problems.",
        "In the rest of this section, we incrementally develop a mixed integer program (MILP) that achieves this.",
        "In the absence of resource constraints, the agents finitehorizon MDPs are completely independent, and the globally optimal solution can be trivially obtained via the following LP, which is simply an aggregation of single-agent finitehorizon LPs: max X m X s X a rm(s, a) X t xm(s, a, t) subject to: X a xm(σ, a, t + 1) = X s,a pm(σ|s, a)xm(s, a, t), ∀m ∈ M, σ ∈ S, t ∈ [1, Tm − 1]; X a xm(s, a, 1) = αm(s), ∀m ∈ M, s ∈ S; (12) where xm(s, a, t) is the occupation measure of agent m, and The Sixth Intl.",
        "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) 1223 (a) (b) Figure 2: Illustration of augmenting an MDP to allow for variable starting and stopping times: a) (left) the original two-state MDP with a single action; (right) the augmented MDP with new states sb and sf and the new action a∗ (note that the origianl transitions are not changed in the augmentation process); b) the augmented MDP displayed as a trajectory through time (grey lines indicate all transitions, while black lines indicate a given trajectory.",
        "Objective Function (sum of expected rewards over all agents) max X m X s X a rm(s, a) X t xm(s, a, t) (5) Meaning Implication Linear Constraints Tie x to θ.",
        "Agent is only active when occupation measure is nonzero in original MDP states. θm(τ) = 0 =⇒ xm(s, a, τ −τa m+1) = 0 ∀s /∈ {sb , sf }, a ∈ A X s/∈{sb,sf } X a xm(s, a, t) ≤ θm(τa m + t − 1) ∀m ∈ M, ∀t ∈ [1, Tm] (6) Agent can only be active in τ ∈ (τa m, τd m) θm(τ) = 0 ∀m ∈ M, τ /∈ (τa m, τd m) (7) Cannot use resources when not active θm(τ) = 0 =⇒ Δm(τ, ω) = 0 ∀τ ∈ [0, bτ], ω ∈ Ω Δm(τ, ω) ≤ θm(τ) ∀m ∈ M, τ ∈ [0, bτ], ω ∈ Ω (8) Tie x to Δ (nonzero x forces corresponding Δ to be nonzero.)",
        "Δm(τ, ω) = 0, ϕm(a, ω) = 1 =⇒ xm(s, a, τ − τa m + 1) = 0 ∀s /∈ {sb , sf } 1/|A| X a ϕm(a, ω) X s/∈{sb,sf } xm(s, a, t) ≤ Δm(t + τa m − 1, ω) ∀m ∈ M, ω ∈ Ω, t ∈ [1, Tm] (9) Resource bounds X m Δm(τ, ω) ≤ bϕ(ω) ∀ω ∈ Ω, τ ∈ [0, bτ] (10) Agent cannot change resources while active.",
        "Only enabled for scheduling with static assignments. θm(τ) = 1 and θm(τ + 1) = 1 =⇒ Δm(τ, ω) = Δm(τ + 1, ω) Δm(τ, ω) − Z(1 − θm(τ + 1)) ≤ Δm(τ + 1, ω) + Z(1 − θm(τ)) Δm(τ, ω) + Z(1 − θm(τ + 1)) ≥ Δm(τ + 1, ω) − Z(1 − θm(τ)) ∀m ∈ M, ω ∈ Ω, τ ∈ [0, bτ] (11) Table 1: MILP for globally optimal resource scheduling.",
        "Tm = τd m − τa m + 1 is the time horizon for the agents MDP.",
        "Using this LP as a basis, we augment it with constraints that ensure that the resource usage implied by the agents occupation measures {xm} does not violate the global resource requirements bϕ at any time step τ ∈ [0, bτ].",
        "To formulate these resource constraints, we use the following binary variables: • Δm(τ, ω) = {0, 1}, ∀m ∈ M, τ ∈ [0, bτ], ω ∈ Ω, which serve as indicator variables that define whether agent m possesses resource ω at time τ.",
        "These are analogous to the static indicator variables used in the one-shot static resource-allocation problem in [6]. • θm = {0, 1}, ∀m ∈ M, τ ∈ [0, bτ] are indicator variables that specify whether agent m is active (i.e., executing its MDP) at time τ.",
        "The meaning of resource-usage variables Δ is illustrated in Figure 1: Δm(τ, ω) = 1 only if resource ω is allocated to agent m at time τ.",
        "The meaning of the activity indicators θ is illustrated in Figure 2b: when agent m is in either the start state sb or the finish state sf , the corresponding θm = 0, but once the agent becomes active and enters one of the other states, we set θm = 1 .",
        "This meaning of θ can be enforced with a linear constraint that synchronizes the values of the agents occupation measures xm and the activity 1224 The Sixth Intl.",
        "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) indicators θ, as shown in (6) in Table 1.",
        "Another constraint we have to add-because the activity indicators θ are defined on the global timeline τ-is to enforce the fact that the agent is inactive outside of its arrivaldeparture window.",
        "This is accomplished by constraint (7) in Table 1.",
        "Furthermore, agents should not be using resources while they are inactive.",
        "This constraint can also be enforced via a linear inequality on θ and Δ, as shown in (8).",
        "Constraint (6) sets the value of θ to match the policy defined by the occupation measure xm.",
        "In a similar fashion, we have to make sure that the resource-usage variables Δ are also synchronized with the occupation measure xm.",
        "This is done via constraint (9) in Table 1, which is nearly identical to the analogous constraint from [6].",
        "After implementing the above constraint, which enforces the meaning of Δ, we add a constraint that ensures that the agents resource usage never exceeds the amounts of available resources.",
        "This condition is also trivially expressed as a linear inequality (10) in Table 1.",
        "Finally, for the problem formulation where resource assignments are static during a lifetime of an agent, we add a constraint that ensures that the resource-usage variables Δ do not change their value while the agent is active (θ = 1).",
        "This is accomplished via the linear constraint (11), where Z ≥ 2 is a constant that is used to turn off the constraints when θm(τ) = 0 or θm(τ + 1) = 0.",
        "This constraint is not used for the dynamic problem formulation, where resources can be reallocated between agents at every time step.",
        "To summarize, Table 1 together with the conservationof-flow constraints from (12) defines the MILP that simultaneously computes an optimal resource assignment for all agents across time as well as optimal finite-horizon MDP policies that are valid under that resource assignment.",
        "As a rough measure of the complexity of this MILP, let us consider the number of optimization variables and constraints.",
        "Let TM = P Tm = P m(τa m − τd m + 1) be the sum of the lengths of the arrival-departure windows across all agents.",
        "Then, the number of optimization variables is: TM + bτ|M||Ω| + bτ|M|, TM of which are continuous (xm), and bτ|M||Ω| + bτ|M| are binary (Δ and θ).",
        "However, notice that all but TM|M| of the θ are set to zero by constraint (7), which also immediately forces all but TM|M||Ω| of the Δ to be zero via the constraints (8).",
        "The number of constraints (not including the degenerate constraints in (7)) in the MILP is: TM + TM|Ω| + bτ|Ω| + bτ|M||Ω|.",
        "Despite the fact that the complexity of the MILP is, in the worst case, exponential1 in the number of binary variables, the complexity of this MILP is significantly (exponentially) lower than that of the MILP with flat utility functions, described in Section 2.2.",
        "This result echos the efficiency gains reported in [6] for single-shot resource-allocation problems, but is much more pronounced, because of the explosion of the flat utility representation due to the temporal aspect of the problem (recall the prohibitive complexity of the combinatorial optimization in Section 2.2).",
        "We empirically analyze the performance of this method in Section 5. 1 Strictly speaking, solving MILPs to optimality is NPcomplete in the number of integer variables. 5.",
        "EXPERIMENTAL RESULTS Although the complexity of solving MILPs is in the worst case exponential in the number of integer variables, there are many efficient methods for solving MILPs that allow our algorithm to scale well for parameters common to resource allocation and scheduling problems.",
        "In particular, this section introduces a problem domain-the repairshop problem-used to empirically evaluate our algorithms scalability in terms of the number of agents |M|, the number of shared resources |Ω|, and the varied lengths of global time bτ during which agents may enter and exit the system.",
        "The repairshop problem is a simple parameterized MDP adopting the metaphor of a vehicular repair shop.",
        "Agents in the repair shop are mechanics with a number of independent tasks that yield reward only when completed.",
        "In our MDP model of this system, actions taken to advance through the state space are only allowed if the agent holds certain resources that are publicly available to the shop.",
        "These resources are in finite supply, and optimal policies for the shop will determine when each agent may hold the limited resources to take actions and earn individual rewards.",
        "Each task to be completed is associated with a single action, although the agent is required to repeat the action numerous times before completing the task and earning a reward.",
        "This model was parameterized in terms of the number of agents in the system, the number of different types of resources that could be linked to necessary actions, a global time during which agents are allowed to arrive and depart, and a maximum length for the number of time steps an agent may remain in the system.",
        "All datapoints in our experiments were obtained with 20 evaluations using CPLEX to solve the MILPs on a Pentium4 computer with 2Gb of RAM.",
        "Trials were conducted on both the static and the dynamic version of the resourcescheduling problem, as defined earlier.",
        "Figure 3 shows the runtime and policy value for independent modifications to the parameter set.",
        "The top row shows how the solution time for the MILP scales as we increase the number of agents |M|, the global time horizon bτ, and the number of resources |Ω|.",
        "Increasing the number of agents leads to exponential complexity scaling, which is to be expected for an NP-complete problem.",
        "However, increasing the global time limit bτ or the total number of resource types |Ω|-while holding the number of agents constantdoes not lead to decreased performance.",
        "This occurs because the problems get easier as they become under-constrained, which is also a common phenomenon for NP-complete problems.",
        "We also observe that the solution to the dynamic version of the problem can often be computed much faster than the static version.",
        "The bottom row of Figure 3 shows the joint policy value of the policies that correspond to the computed optimal resource-allocation schedules.",
        "We can observe that the dynamic version yields higher reward (as expected, since the reward for the dynamic version is always no less than the reward of the static version).",
        "We should point out that these graphs should not be viewed as a measure of performance of two different algorithms (both algorithms produce optimal solutions but to different problems), but rather as observations about how the quality of optimal solutions change as more flexibility is allowed in the reallocation of resources.",
        "Figure 4 shows runtime and policy value for trials in which common input variables are scaled together.",
        "This allows The Sixth Intl.",
        "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) 1225 2 4 6 8 10 10 −3 10 −2 10 −1 10 0 10 1 10 2 10 3 10 4 Number of Agents |M| CPUTime,sec |Ω| = 5, τ = 50 static dynamic 50 100 150 200 10 −2 10 −1 10 0 10 1 10 2 10 3 Global Time Boundary τ CPUTime,sec |M| = 5, |Ω| = 5 static dynamic 10 20 30 40 50 10 −2 10 −1 10 0 10 1 10 2 Number of Resources |Ω| CPUTime,sec |M| = 5, τ = 50 static dynamic 2 4 6 8 10 200 400 600 800 1000 1200 1400 1600 Number of Agents |M| Value |Ω| = 5, τ = 50 static dynamic 50 100 150 200 400 500 600 700 800 900 1000 1100 1200 1300 1400 Global Time Boundary τ Value |M| = 5, |Ω| = 5 static dynamic 10 20 30 40 50 500 600 700 800 900 1000 1100 1200 1300 1400 Number of Resources |Ω| Value |M| = 5, τ = 50 static dynamic Figure 3: Evaluation of our MILP for variable numbers of agents (column 1), lengths of global-time window (column 2), and numbers of resource types (column 3).",
        "Top row shows CPU time, and bottom row shows the joint reward of agents MDP policies.",
        "Error bars show the 1st and 3rd quartiles (25% and 75%). 2 4 6 8 10 10 −3 10 −2 10 −1 10 0 10 1 10 2 10 3 Number of Agents |M| CPUTime,sec τ = 10|M| static dynamic 2 4 6 8 10 10 −3 10 −2 10 −1 10 0 10 1 10 2 10 3 10 4 Number of Agents |M| CPUTime,sec |Ω| = 2|M| static dynamic 2 4 6 8 10 10 −3 10 −2 10 −1 10 0 10 1 10 2 10 3 10 4 Number of Agents |M| CPUTime,sec |Ω| = 5|M| static dynamic 2 4 6 8 10 200 400 600 800 1000 1200 1400 1600 1800 2000 2200 Number of Agents |M| Value τ = 10|M| static dynamic 2 4 6 8 10 200 400 600 800 1000 1200 1400 1600 1800 2000 Number of Agents |M| Value |Ω| = 2|M| static dynamic 2 4 6 8 10 0 500 1000 1500 2000 2500 Number of Agents |M| Value |Ω| = 5|M| static dynamic Figure 4: Evaluation of our MILP using correlated input variables.",
        "The left column tracks the performance and CPU time as the number of agents and global-time window increase together (bτ = 10|M|).",
        "The middle and the right column track the performance and CPU time as the number of resources and the number of agents increase together as |Ω| = 2|M| and |Ω| = 5|M|, respectively.",
        "Error bars show the 1st and 3rd quartiles (25% and 75%). 1226 The Sixth Intl.",
        "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) us to explore domains where the total number of agents scales proportionally to the total number of resource types or the global time horizon, while keeping constant the average agent density (per unit of global time) or the average number of resources per agent (which commonly occurs in real-life applications).",
        "Overall, we believe that these experimental results indicate that our MILP formulation can be used to effectively solve resource-scheduling problems of nontrivial size. 6.",
        "DISCUSSION AND CONCLUSIONS Throughout the paper, we have made a number of assumptions in our model and solution algorithm; we discuss their implications below. • Continual execution.",
        "We assume that once an agent stops executing its MDP (transitions into state sf ), it exits the system and cannot return.",
        "It is easy to relax this assumption for domains where agents MDPs can be paused and restarted.",
        "All that is required is to include an additional pause action which transitions from a given state back to itself, and has zero reward. • Indifference to start time.",
        "We used a reward model where agents rewards depend only on the time horizon of their MDPs and not the global start time.",
        "This is a consequence of our MDP-augmentation procedure from Section 4.1.",
        "It is easy to extend the model so that the agents incur an explicit penalty for idling by assigning a non-zero negative reward to the start state sb . • Binary resource requirements.",
        "For simplicity, we have assumed that resource costs are binary: ϕm(a, ω) = {0, 1}, but our results generalize in a straightforward manner to non-binary resource mappings, analogously to the procedure used in [5]. • Cooperative agents.",
        "The optimization procedure discussed in this paper was developed in the context of cooperative agents, but it can also be used to design a mechanism for scheduling resources among self-interested agents.",
        "This optimization procedure can be embedded in a VickreyClarke-Groves auction, completely analogously to the way it was done in [7].",
        "In fact, all the results of [7] about the properties of the auction and information privacy directly carry over to the scheduling domain discussed in this paper, requiring only slight modifications to deal with finitehorizon MDPs. • Known, deterministic arrival and departure times.",
        "Finally, we have assumed that agents arrival and departure times (τa m and τd m) are deterministic and known a priori.",
        "This assumption is fundamental to our solution method.",
        "While there are many domains where this assumption is valid, in many cases agents arrive and depart dynamically and their arrival and departure times can only be predicted probabilistically, leading to online resource-allocation problems.",
        "In particular, in the case of self-interested agents, this becomes an interesting version of an online-mechanism-design problem [11, 12].",
        "In summary, we have presented an MILP formulation for the combinatorial resource-scheduling problem where agents values for possible resource assignments are defined by finitehorizon MDPs.",
        "This result extends previous work ([6, 7]) on static one-shot resource allocation under MDP-induced preferences to resource-scheduling problems with a temporal aspect.",
        "As such, this work takes a step in the direction of designing an online mechanism for agents with combinatorial resource preferences induced by stochastic planning problems.",
        "Relaxing the assumption about deterministic arrival and departure times of the agents is a focus of our future work.",
        "We would like to thank the anonymous reviewers for their insightful comments and suggestions. 7.",
        "REFERENCES [1] E. Altman and A. Shwartz.",
        "Adaptive control of constrained Markov chains: Criteria and policies.",
        "Annals of Operations Research, special issue on Markov Decision Processes, 28:101-134, 1991. [2] R. Bellman.",
        "Dynamic Programming.",
        "Princeton University Press, 1957. [3] C. Boutilier.",
        "Solving concisely expressed combinatorial auction problems.",
        "In Proc. of AAAI-02, pages 359-366, 2002. [4] C. Boutilier and H. H. Hoos.",
        "Bidding languages for combinatorial auctions.",
        "In Proc. of IJCAI-01, pages 1211-1217, 2001. [5] D. Dolgov.",
        "Integrated Resource Allocation and Planning in Stochastic Multiagent Environments.",
        "PhD thesis, Computer Science Department, University of Michigan, February 2006. [6] D. A. Dolgov and E. H. Durfee.",
        "Optimal resource allocation and policy formulation in loosely-coupled Markov decision processes.",
        "In Proc. of ICAPS-04, pages 315-324, June 2004. [7] D. A. Dolgov and E. H. Durfee.",
        "Computationally efficient combinatorial auctions for resource allocation in weakly-coupled MDPs.",
        "In Proc. of AAMAS-05, New York, NY, USA, 2005.",
        "ACM Press. [8] D. A. Dolgov and E. H. Durfee.",
        "Resource allocation among agents with preferences induced by factored MDPs.",
        "In Proc. of AAMAS-06, 2006. [9] K. Larson and T. Sandholm.",
        "Mechanism design and deliberative agents.",
        "In Proc. of AAMAS-05, pages 650-656, New York, NY, USA, 2005.",
        "ACM Press. [10] N. Nisan.",
        "Bidding and allocation in combinatorial auctions.",
        "In Electronic Commerce, 2000. [11] D. C. Parkes and S. Singh.",
        "An MDP-based approach to Online Mechanism Design.",
        "In Proc. of the Seventeenths Annual Conference on Neural Information Processing Systems (NIPS-03), 2003. [12] D. C. Parkes, S. Singh, and D. Yanovsky.",
        "Approximately efficient online mechanism design.",
        "In Proc. of the Eighteenths Annual Conference on Neural Information Processing Systems (NIPS-04), 2004. [13] M. L. Puterman.",
        "Markov Decision Processes.",
        "John Wiley & Sons, New York, 1994. [14] M. H. Rothkopf, A. Pekec, and R. M. Harstad.",
        "Computationally manageable combinational auctions.",
        "Management Science, 44(8):1131-1147, 1998. [15] T. Sandholm.",
        "An algorithm for optimal winner determination in combinatorial auctions.",
        "In Proc. of IJCAI-99, pages 542-547, San Francisco, CA, USA, 1999.",
        "Morgan Kaufmann Publishers Inc.",
        "The Sixth Intl.",
        "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) 1227"
    ],
    "error_count": 0,
    "keys": {
        "combinatorial resource scheduling": {
            "translated_key": "",
            "is_in_text": true,
            "original_annotated_sentences": [
                "<br>combinatorial resource scheduling</br> for Multiagent MDPs Dmitri A. Dolgov, Michael R. James, and Michael E. Samples AI and Robotics Group Technical Research, Toyota Technical Center USA {ddolgov, michael.r.james, michael.samples}@gmail.com ABSTRACT Optimal resource scheduling in multiagent systems is a computationally challenging task, particularly when the values of resources are not additive.",
                "We consider the combinatorial problem of scheduling the usage of multiple resources among agents that operate in stochastic environments, modeled as Markov decision processes (MDPs).",
                "In recent years, efficient resource-allocation algorithms have been developed for agents with resource values induced by MDPs.",
                "However, this prior work has focused on static resource-allocation problems where resources are distributed once and then utilized in infinite-horizon MDPs.",
                "We extend those existing models to the problem of <br>combinatorial resource scheduling</br>, where agents persist only for finite periods between their (predefined) arrival and departure times, requiring resources only for those time periods.",
                "We provide a computationally efficient procedure for computing globally optimal resource assignments to agents over time.",
                "We illustrate and empirically analyze the method in the context of a stochastic jobscheduling domain.",
                "Categories and Subject Descriptors I.2.8 [Artificial Intelligence]: Problem Solving, Control Methods, and Search; I.2.11 [Artificial Intelligence]: Distributed Artificial Intelligence-Multiagent systems General Terms Algorithms, Performance, Design 1.",
                "INTRODUCTION The tasks of optimal resource allocation and scheduling are ubiquitous in multiagent systems, but solving such optimization problems can be computationally difficult, due to a number of factors.",
                "In particular, when the value of a set of resources to an agent is not additive (as is often the case with resources that are substitutes or complements), the utility function might have to be defined on an exponentially large space of resource bundles, which very quickly becomes computationally intractable.",
                "Further, even when each agent has a utility function that is nonzero only on a small subset of the possible resource bundles, obtaining optimal allocation is still computationally prohibitive, as the problem becomes NP-complete [14].",
                "Such computational issues have recently spawned several threads of work in using compact models of agents preferences.",
                "One idea is to use any structure present in utility functions to represent them compactly, via, for example, logical formulas [15, 10, 4, 3].",
                "An alternative is to directly model the mechanisms that define the agents utility functions and perform resource allocation directly with these models [9].",
                "A way of accomplishing this is to model the processes by which an agent might utilize the resources and define the utility function as the payoff of these processes.",
                "In particular, if an agent uses resources to act in a stochastic environment, its utility function can be naturally modeled with a Markov decision process, whose action set is parameterized by the available resources.",
                "This representation can then be used to construct very efficient resource-allocation algorithms that lead to an exponential speedup over a straightforward optimization problem with flat representations of combinatorial preferences [6, 7, 8].",
                "However, this existing work on resource allocation with preferences induced by resource-parameterized MDPs makes an assumption that the resources are only allocated once and are then utilized by the agents independently within their infinite-horizon MDPs.",
                "This assumption that no reallocation of resources is possible can be limiting in domains where agents arrive and depart dynamically.",
                "In this paper, we extend the work on resource allocation under MDP-induced preferences to discrete-time scheduling problems, where agents are present in the system for finite time intervals and can only use resources within these intervals.",
                "In particular, agents arrive and depart at arbitrary (predefined) times and within these intervals use resources to execute tasks in finite-horizon MDPs.",
                "We address the problem of globally optimal resource scheduling, where the objective is to find an allocation of resources to the agents across time that maximizes the sum of the expected rewards that they obtain.",
                "In this context, our main contribution is a mixed-integerprogramming formulation of the scheduling problem that chooses globally optimal resource assignments, starting times, and execution horizons for all agents (within their arrival1220 978-81-904262-7-5 (RPS) c 2007 IFAAMAS departure intervals).",
                "We analyze and empirically compare two flavors of the scheduling problem: one, where agents have static resource assignments within their finite-horizon MDPs, and another, where resources can be dynamically reallocated between agents at every time step.",
                "In the rest of the paper, we first lay down the necessary groundwork in Section 2 and then introduce our model and formal problem statement in Section 3.",
                "In Section 4.2, we describe our main result, the optimization program for globally optimal resource scheduling.",
                "Following the discussion of our experimental results on a job-scheduling problem in Section 5, we conclude in Section 6 with a discussion of possible extensions and generalizations of our method. 2.",
                "BACKGROUND Similarly to the model used in previous work on resourceallocation with MDP-induced preferences [6, 7], we define the value of a set of resources to an agent as the value of the best MDP policy that is realizable, given those resources.",
                "However, since the focus of our work is on scheduling problems, and a large part of the optimization problem is to decide how resources are allocated in time among agents with finite arrival and departure times, we model the agents planning problems as finite-horizon MDPs, in contrast to previous work that used infinite-horizon discounted MDPs.",
                "In the rest of this section, we first introduce some necessary background on finite-horizon MDPs and present a linear-programming formulation that serves as the basis for our solution algorithm developed in Section 4.",
                "We also outline the standard methods for <br>combinatorial resource scheduling</br> with flat resource values, which serve as a comparison benchmark for the new model developed here. 2.1 Markov Decision Processes A stationary, finite-domain, discrete-time MDP (see, for example, [13] for a thorough and detailed development) can be described as S, A, p, r , where: S is a finite set of system states; A is a finite set of actions that are available to the agent; p is a stationary stochastic transition function, where p(σ|s, a) is the probability of transitioning to state σ upon executing action a in state s; r is a stationary reward function, where r(s, a) specifies the reward obtained upon executing action a in state s. Given such an MDP, a decision problem under a finite horizon T is to choose an optimal action at every time step to maximize the expected value of the total reward accrued during the agents (finite) lifetime.",
                "The agents optimal policy is then a function of current state s and the time until the horizon.",
                "An optimal policy for such a problem is to act greedily with respect to the optimal value function, defined recursively by the following system of finite-time Bellman equations [2]: v(s, t) = max a r(s, a) + X σ p(σ|s, a)v(σ, t + 1), ∀s ∈ S, t ∈ [1, T − 1]; v(s, T) = 0, ∀s ∈ S; where v(s, t) is the optimal value of being in state s at time t ∈ [1, T].",
                "This optimal value function can be easily computed using dynamic programming, leading to the following optimal policy π, where π(s, a, t) is the probability of executing action a in state s at time t: π(s, a, t) = ( 1, a = argmaxa r(s, a) + P σ p(σ|s, a)v(σ, t + 1), 0, otherwise.",
                "The above is the most common way of computing the optimal value function (and therefore an optimal policy) for a finite-horizon MDP.",
                "However, we can also formulate the problem as the following linear program (similarly to the dual LP for infinite-horizon discounted MDPs [13, 6, 7]): max X s X a r(s, a) X t x(s, a, t) subject to: X a x(σ, a, t + 1) = X s,a p(σ|s, a)x(s, a, t) ∀σ, t ∈ [1, T − 1]; X a x(s, a, 1) = α(s), ∀s ∈ S; (1) where α(s) is the initial distribution over the state space, and x is the (non-stationary) occupation measure (x(s, a, t) ∈ [0, 1] is the total expected number of times action a is executed in state s at time t).",
                "An optimal (non-stationary) policy is obtained from the occupation measure as follows: π(s, a, t) = x(s, a, t)/ X a x(s, a, t) ∀s ∈ S, t ∈ [1, T]. (2) Note that the standard unconstrained finite-horizon MDP, as described above, always has a uniformly-optimal solution (optimal for any initial distribution α(s)).",
                "Therefore, an optimal policy can be obtained by using an arbitrary constant α(s) > 0 (in particular, α(s) = 1 will result in x(s, a, t) = π(s, a, t)).",
                "However, for MDPs with resource constraints (as defined below in Section 3), uniformly-optimal policies do not in general exist.",
                "In such cases, α becomes a part of the problem input, and a resulting policy is only optimal for that particular α.",
                "This result is well known for infinite-horizon MDPs with various types of constraints [1, 6], and it also holds for our finite-horizon model, which can be easily established via a line of reasoning completely analogous to the arguments in [6]. 2.2 <br>combinatorial resource scheduling</br> A straightforward approach to resource scheduling for a set of agents M, whose values for the resources are induced by stochastic planning problems (in our case, finite-horizon MDPs) would be to have each agent enumerate all possible resource assignments over time and, for each one, compute its value by solving the corresponding MDP.",
                "Then, each agent would provide valuations for each possible resource bundle over time to a centralized coordinator, who would compute the optimal resource assignments across time based on these valuations.",
                "When resources can be allocated at different times to different agents, each agent must submit valuations for every combination of possible time horizons.",
                "Let each agent m ∈ M execute its MDP within the arrival-departure time interval τ ∈ [τa m, τd m].",
                "Hence, agent m will execute an MDP with time horizon no greater than Tm = τd m−τa m+1.",
                "Let bτ be the global time horizon for the problem, before which all of the agents MDPs must finish.",
                "We assume τd m < bτ, ∀m ∈ M. The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) 1221 For the scheduling problem where agents have static resource requirements within their finite-horizon MDPs, the agents provide a valuation for each resource bundle for each possible time horizon (from [1, Tm]) that they may use.",
                "Let Ω be the set of resources to be allocated among the agents.",
                "An agent will get at most one resource bundle for one of the time horizons.",
                "Let the variable ψ ∈ Ψm enumerate all possible pairs of resource bundles and time horizons for agent m, so there are 2|Ω| × Tm values for ψ (the space of bundles is exponential in the number of resource types |Ω|).",
                "The agent m must provide a value vψ m for each ψ, and the coordinator will allocate at most one ψ (resource, time horizon) pair to each agent.",
                "This allocation is expressed as an indicator variable zψ m ∈ {0, 1} that shows whether ψ is assigned to agent m. For time τ and resource ω, the function nm(ψ, τ, ω) ∈ {0, 1} indicates whether the bundle in ψ uses resource ω at time τ (we make the assumption that agents have binary resource requirements).",
                "This allocation problem is NP-complete, even when considering only a single time step, and its difficulty increases significantly with multiple time steps because of the increasing number of values of ψ.",
                "The problem of finding an optimal allocation that satisfies the global constraint that the amount of each resource ω allocated to all agents does not exceed the available amount bϕ(ω) can be expressed as the following integer program: max X m∈M X ψ∈Ψm zψ mvψ m subject to: X ψ∈Ψm zψ m ≤ 1, ∀m ∈ M; X m∈M X ψ∈Ψm zψ mnm(ψ, τ, ω) ≤ bϕ(ω), ∀τ ∈ [1, bτ], ∀ω ∈ Ω; (3) The first constraint in equation 3 says that no agent can receive more than one bundle, and the second constraint ensures that the total assignment of resource ω does not, at any time, exceed the resource bound.",
                "For the scheduling problem where the agents are able to dynamically reallocate resources, each agent must specify a value for every combination of bundles and time steps within its time horizon.",
                "Let the variable ψ ∈ Ψm in this case enumerate all possible resource bundles for which at most one bundle may be assigned to agent m at each time step.",
                "Therefore, in this case there are P t∈[1,Tm](2|Ω| )t ∼ 2|Ω|Tm possibilities of resource bundles assigned to different time slots, for the Tm different time horizons.",
                "The same set of equations (3) can be used to solve this dynamic scheduling problem, but the integer program is different because of the difference in how ψ is defined.",
                "In this case, the number of ψ values is exponential in each agents planning horizon Tm, resulting in a much larger program.",
                "This straightforward approach to solving both of these scheduling problems requires an enumeration and solution of either 2|Ω| Tm (static allocation) or P t∈[1,Tm] 2|Ω|t (dynamic reallocation) MDPs for each agent, which very quickly becomes intractable with the growth of the number of resources |Ω| or the time horizon Tm. 3.",
                "MODEL AND PROBLEM STATEMENT We now formally introduce our model of the resourcescheduling problem.",
                "The problem input consists of the following components: • M, Ω, bϕ, τa m, τd m, bτ are as defined above in Section 2.2. • {Θm} = {S, A, pm, rm, αm} are the MDPs of all agents m ∈ M. Without loss of generality, we assume that state and action spaces of all agents are the same, but each has its own transition function pm, reward function rm, and initial conditions αm. • ϕm : A×Ω → {0, 1} is the mapping of actions to resources for agent m. ϕm(a, ω) indicates whether action a of agent m needs resource ω.",
                "An agent m that receives a set of resources that does not include resource ω cannot execute in its MDP policy any action a for which ϕm(a, ω) = 0.",
                "We assume all resource requirements are binary; as discussed below in Section 6, this assumption is not limiting.",
                "Given the above input, the optimization problem we consider is to find the globally optimal-maximizing the sum of expected rewards-mapping of resources to agents for all time steps: Δ : τ × M × Ω → {0, 1}.",
                "A solution is feasible if the corresponding assignment of resources to the agents does not violate the global resource constraint: X m Δm(τ, ω) ≤ bϕ(ω), ∀ω ∈ Ω, τ ∈ [1, bτ]. (4) We consider two flavors of the resource-scheduling problem.",
                "The first formulation restricts resource assignments to the space where the allocation of resources to each agent is static during the agents lifetime.",
                "The second formulation allows reassignment of resources between agents at every time step within their lifetimes.",
                "Figure 1 depicts a resource-scheduling problem with three agents M = {m1, m2, m3}, three resources Ω = {ω1, ω2, ω3}, and a global problem horizon of bτ = 11.",
                "The agents arrival and departure times are shown as gray boxes and are {1, 6}, {3, 7}, and {2, 11}, respectively.",
                "A solution to this problem is shown via horizontal bars within each agents box, where the bars correspond to the allocation of the three resource types.",
                "Figure 1a shows a solution to a static scheduling problem.",
                "According to the shown solution, agent m1 begins the execution of its MDP at time τ = 1 and has a lock on all three resources until it finishes execution at time τ = 3.",
                "Note that agent m1 relinquishes its hold on the resources before its announced departure time of τd m1 = 6, ostensibly because other agents can utilize the resources more effectively.",
                "Thus, at time τ = 4, resources ω1 and ω3 are allocated to agent m2, who then uses them to execute its MDP (using only actions supported by resources ω1 and ω3) until time τ = 7.",
                "Agent m3 holds resource ω3 during the interval τ ∈ [4, 10].",
                "Figure 1b shows a possible solution to the dynamic version of the same problem.",
                "There, resources can be reallocated between agents at every time step.",
                "For example, agent m1 gives up its use of resource ω2 at time τ = 2, although it continues the execution of its MDP until time τ = 6.",
                "Notice that an agent is not allowed to stop and restart its MDP, so agent m1 is only able to continue executing in the interval τ ∈ [3, 4] if it has actions that do not require any resources (ϕm(a, ω) = 0).",
                "Clearly, the model and problem statement described above make a number of assumptions about the problem and the desired solution properties.",
                "We discuss some of those assumptions and their implications in Section 6. 1222 The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) (a) (b) Figure 1: Illustration of a solution to a resource-scheduling problem with three agents and three resources: a) static resource assignments (resource assignments are constant within agents lifetimes; b) dynamic assignment (resource assignments are allowed to change at every time step). 4.",
                "RESOURCE SCHEDULING Our resource-scheduling algorithm proceeds in two stages.",
                "First, we perform a preprocessing step that augments the agent MDPs; this process is described in Section 4.1.",
                "Second, using these augmented MDPs we construct a global optimization problem, which is described in Section 4.2. 4.1 Augmenting Agents MDPs In the model described in the previous section, we assume that if an agent does not possess the necessary resources to perform actions in its MDP, its execution is halted and the agent leaves the system.",
                "In other words, the MDPs cannot be paused and resumed.",
                "For example, in the problem shown in Figure 1a, agent m1 releases all resources after time τ = 3, at which point the execution of its MDP is halted.",
                "Similarly, agents m2 and m3 only execute their MDPs in the intervals τ ∈ [4, 6] and τ ∈ [4, 10], respectively.",
                "Therefore, an important part of the global decision-making problem is to decide the window of time during which each of the agents is active (i.e., executing its MDP).",
                "To accomplish this, we augment each agents MDP with two new states (start and finish states sb , sf , respectively) and a new start/stop action a∗ , as illustrated in Figure 2.",
                "The idea is that an agent stays in the start state sb until it is ready to execute its MDP, at which point it performs the start/stop action a∗ and transitions into the state space of the original MDP with the transition probability that corresponds to the original initial distribution α(s).",
                "For example, in Figure 1a, for agent m2 this would happen at time τ = 4.",
                "Once the agent gets to the end of its activity window (time τ = 6 for agent m2 in Figure 1a), it performs the start/stop action, which takes it into the sink finish state sf at time τ = 7.",
                "More precisely, given an MDP S, A, pm, rm, αm , we define an augmented MDP S , A , pm, rm, αm as follows: S = S ∪ sb ∪ sf ; A = A ∪ a∗ ; p (s|sb , a∗ ) = α(s), ∀s ∈ S; p (sb |sb , a) = 1.0, ∀a ∈ A; p (sf |s, a∗ ) = 1.0, ∀s ∈ S; p (σ|s, a) = p(σ|s, a), ∀s, σ ∈ S, a ∈ A; r (sb , a) = r (sf , a) = 0, ∀a ∈ A ; r (s, a) = r(s, a), ∀s ∈ S, a ∈ A; α (sb ) = 1; α (s) = 0, ∀s ∈ S; where all non-specified transition probabilities are assumed to be zero.",
                "Further, in order to account for the new starting state, we begin the MDP one time-step earlier, setting τa m ← τa m − 1.",
                "This will not affect the resource allocation due to the resource constraints only being enforced for the original MDP states, as will be discussed in the next section.",
                "For example, the augmented MDPs shown in Figure 2b (which starts in state sb at time τ = 2) would be constructed from an MDP with original arrival time τ = 3.",
                "Figure 2b also shows a sample trajectory through the state space: the agent starts in state sb , transitions into the state space S of the original MDP, and finally exists into the sink state sf .",
                "Note that if we wanted to model a problem where agents could pause their MDPs at arbitrary time steps (which might be useful for domains where dynamic reallocation is possible), we could easily accomplish this by including an extra action that transitions from each state to itself with zero reward. 4.2 MILP for Resource Scheduling Given a set of augmented MDPs, as defined above, the goal of this section is to formulate a global optimization program that solves the resource-scheduling problem.",
                "In this section and below, all MDPs are assumed to be the augmented MDPs as defined in Section 4.1.",
                "Our approach is similar to the idea used in [6]: we begin with the linear-program formulation of agents MDPs (1) and augment it with constraints that ensure that the corresponding resource allocation across agents and time is valid.",
                "The resulting optimization problem then simultaneously solves the agents MDPs and resource-scheduling problems.",
                "In the rest of this section, we incrementally develop a mixed integer program (MILP) that achieves this.",
                "In the absence of resource constraints, the agents finitehorizon MDPs are completely independent, and the globally optimal solution can be trivially obtained via the following LP, which is simply an aggregation of single-agent finitehorizon LPs: max X m X s X a rm(s, a) X t xm(s, a, t) subject to: X a xm(σ, a, t + 1) = X s,a pm(σ|s, a)xm(s, a, t), ∀m ∈ M, σ ∈ S, t ∈ [1, Tm − 1]; X a xm(s, a, 1) = αm(s), ∀m ∈ M, s ∈ S; (12) where xm(s, a, t) is the occupation measure of agent m, and The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) 1223 (a) (b) Figure 2: Illustration of augmenting an MDP to allow for variable starting and stopping times: a) (left) the original two-state MDP with a single action; (right) the augmented MDP with new states sb and sf and the new action a∗ (note that the origianl transitions are not changed in the augmentation process); b) the augmented MDP displayed as a trajectory through time (grey lines indicate all transitions, while black lines indicate a given trajectory.",
                "Objective Function (sum of expected rewards over all agents) max X m X s X a rm(s, a) X t xm(s, a, t) (5) Meaning Implication Linear Constraints Tie x to θ.",
                "Agent is only active when occupation measure is nonzero in original MDP states. θm(τ) = 0 =⇒ xm(s, a, τ −τa m+1) = 0 ∀s /∈ {sb , sf }, a ∈ A X s/∈{sb,sf } X a xm(s, a, t) ≤ θm(τa m + t − 1) ∀m ∈ M, ∀t ∈ [1, Tm] (6) Agent can only be active in τ ∈ (τa m, τd m) θm(τ) = 0 ∀m ∈ M, τ /∈ (τa m, τd m) (7) Cannot use resources when not active θm(τ) = 0 =⇒ Δm(τ, ω) = 0 ∀τ ∈ [0, bτ], ω ∈ Ω Δm(τ, ω) ≤ θm(τ) ∀m ∈ M, τ ∈ [0, bτ], ω ∈ Ω (8) Tie x to Δ (nonzero x forces corresponding Δ to be nonzero.)",
                "Δm(τ, ω) = 0, ϕm(a, ω) = 1 =⇒ xm(s, a, τ − τa m + 1) = 0 ∀s /∈ {sb , sf } 1/|A| X a ϕm(a, ω) X s/∈{sb,sf } xm(s, a, t) ≤ Δm(t + τa m − 1, ω) ∀m ∈ M, ω ∈ Ω, t ∈ [1, Tm] (9) Resource bounds X m Δm(τ, ω) ≤ bϕ(ω) ∀ω ∈ Ω, τ ∈ [0, bτ] (10) Agent cannot change resources while active.",
                "Only enabled for scheduling with static assignments. θm(τ) = 1 and θm(τ + 1) = 1 =⇒ Δm(τ, ω) = Δm(τ + 1, ω) Δm(τ, ω) − Z(1 − θm(τ + 1)) ≤ Δm(τ + 1, ω) + Z(1 − θm(τ)) Δm(τ, ω) + Z(1 − θm(τ + 1)) ≥ Δm(τ + 1, ω) − Z(1 − θm(τ)) ∀m ∈ M, ω ∈ Ω, τ ∈ [0, bτ] (11) Table 1: MILP for globally optimal resource scheduling.",
                "Tm = τd m − τa m + 1 is the time horizon for the agents MDP.",
                "Using this LP as a basis, we augment it with constraints that ensure that the resource usage implied by the agents occupation measures {xm} does not violate the global resource requirements bϕ at any time step τ ∈ [0, bτ].",
                "To formulate these resource constraints, we use the following binary variables: • Δm(τ, ω) = {0, 1}, ∀m ∈ M, τ ∈ [0, bτ], ω ∈ Ω, which serve as indicator variables that define whether agent m possesses resource ω at time τ.",
                "These are analogous to the static indicator variables used in the one-shot static resource-allocation problem in [6]. • θm = {0, 1}, ∀m ∈ M, τ ∈ [0, bτ] are indicator variables that specify whether agent m is active (i.e., executing its MDP) at time τ.",
                "The meaning of resource-usage variables Δ is illustrated in Figure 1: Δm(τ, ω) = 1 only if resource ω is allocated to agent m at time τ.",
                "The meaning of the activity indicators θ is illustrated in Figure 2b: when agent m is in either the start state sb or the finish state sf , the corresponding θm = 0, but once the agent becomes active and enters one of the other states, we set θm = 1 .",
                "This meaning of θ can be enforced with a linear constraint that synchronizes the values of the agents occupation measures xm and the activity 1224 The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) indicators θ, as shown in (6) in Table 1.",
                "Another constraint we have to add-because the activity indicators θ are defined on the global timeline τ-is to enforce the fact that the agent is inactive outside of its arrivaldeparture window.",
                "This is accomplished by constraint (7) in Table 1.",
                "Furthermore, agents should not be using resources while they are inactive.",
                "This constraint can also be enforced via a linear inequality on θ and Δ, as shown in (8).",
                "Constraint (6) sets the value of θ to match the policy defined by the occupation measure xm.",
                "In a similar fashion, we have to make sure that the resource-usage variables Δ are also synchronized with the occupation measure xm.",
                "This is done via constraint (9) in Table 1, which is nearly identical to the analogous constraint from [6].",
                "After implementing the above constraint, which enforces the meaning of Δ, we add a constraint that ensures that the agents resource usage never exceeds the amounts of available resources.",
                "This condition is also trivially expressed as a linear inequality (10) in Table 1.",
                "Finally, for the problem formulation where resource assignments are static during a lifetime of an agent, we add a constraint that ensures that the resource-usage variables Δ do not change their value while the agent is active (θ = 1).",
                "This is accomplished via the linear constraint (11), where Z ≥ 2 is a constant that is used to turn off the constraints when θm(τ) = 0 or θm(τ + 1) = 0.",
                "This constraint is not used for the dynamic problem formulation, where resources can be reallocated between agents at every time step.",
                "To summarize, Table 1 together with the conservationof-flow constraints from (12) defines the MILP that simultaneously computes an optimal resource assignment for all agents across time as well as optimal finite-horizon MDP policies that are valid under that resource assignment.",
                "As a rough measure of the complexity of this MILP, let us consider the number of optimization variables and constraints.",
                "Let TM = P Tm = P m(τa m − τd m + 1) be the sum of the lengths of the arrival-departure windows across all agents.",
                "Then, the number of optimization variables is: TM + bτ|M||Ω| + bτ|M|, TM of which are continuous (xm), and bτ|M||Ω| + bτ|M| are binary (Δ and θ).",
                "However, notice that all but TM|M| of the θ are set to zero by constraint (7), which also immediately forces all but TM|M||Ω| of the Δ to be zero via the constraints (8).",
                "The number of constraints (not including the degenerate constraints in (7)) in the MILP is: TM + TM|Ω| + bτ|Ω| + bτ|M||Ω|.",
                "Despite the fact that the complexity of the MILP is, in the worst case, exponential1 in the number of binary variables, the complexity of this MILP is significantly (exponentially) lower than that of the MILP with flat utility functions, described in Section 2.2.",
                "This result echos the efficiency gains reported in [6] for single-shot resource-allocation problems, but is much more pronounced, because of the explosion of the flat utility representation due to the temporal aspect of the problem (recall the prohibitive complexity of the combinatorial optimization in Section 2.2).",
                "We empirically analyze the performance of this method in Section 5. 1 Strictly speaking, solving MILPs to optimality is NPcomplete in the number of integer variables. 5.",
                "EXPERIMENTAL RESULTS Although the complexity of solving MILPs is in the worst case exponential in the number of integer variables, there are many efficient methods for solving MILPs that allow our algorithm to scale well for parameters common to resource allocation and scheduling problems.",
                "In particular, this section introduces a problem domain-the repairshop problem-used to empirically evaluate our algorithms scalability in terms of the number of agents |M|, the number of shared resources |Ω|, and the varied lengths of global time bτ during which agents may enter and exit the system.",
                "The repairshop problem is a simple parameterized MDP adopting the metaphor of a vehicular repair shop.",
                "Agents in the repair shop are mechanics with a number of independent tasks that yield reward only when completed.",
                "In our MDP model of this system, actions taken to advance through the state space are only allowed if the agent holds certain resources that are publicly available to the shop.",
                "These resources are in finite supply, and optimal policies for the shop will determine when each agent may hold the limited resources to take actions and earn individual rewards.",
                "Each task to be completed is associated with a single action, although the agent is required to repeat the action numerous times before completing the task and earning a reward.",
                "This model was parameterized in terms of the number of agents in the system, the number of different types of resources that could be linked to necessary actions, a global time during which agents are allowed to arrive and depart, and a maximum length for the number of time steps an agent may remain in the system.",
                "All datapoints in our experiments were obtained with 20 evaluations using CPLEX to solve the MILPs on a Pentium4 computer with 2Gb of RAM.",
                "Trials were conducted on both the static and the dynamic version of the resourcescheduling problem, as defined earlier.",
                "Figure 3 shows the runtime and policy value for independent modifications to the parameter set.",
                "The top row shows how the solution time for the MILP scales as we increase the number of agents |M|, the global time horizon bτ, and the number of resources |Ω|.",
                "Increasing the number of agents leads to exponential complexity scaling, which is to be expected for an NP-complete problem.",
                "However, increasing the global time limit bτ or the total number of resource types |Ω|-while holding the number of agents constantdoes not lead to decreased performance.",
                "This occurs because the problems get easier as they become under-constrained, which is also a common phenomenon for NP-complete problems.",
                "We also observe that the solution to the dynamic version of the problem can often be computed much faster than the static version.",
                "The bottom row of Figure 3 shows the joint policy value of the policies that correspond to the computed optimal resource-allocation schedules.",
                "We can observe that the dynamic version yields higher reward (as expected, since the reward for the dynamic version is always no less than the reward of the static version).",
                "We should point out that these graphs should not be viewed as a measure of performance of two different algorithms (both algorithms produce optimal solutions but to different problems), but rather as observations about how the quality of optimal solutions change as more flexibility is allowed in the reallocation of resources.",
                "Figure 4 shows runtime and policy value for trials in which common input variables are scaled together.",
                "This allows The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) 1225 2 4 6 8 10 10 −3 10 −2 10 −1 10 0 10 1 10 2 10 3 10 4 Number of Agents |M| CPUTime,sec |Ω| = 5, τ = 50 static dynamic 50 100 150 200 10 −2 10 −1 10 0 10 1 10 2 10 3 Global Time Boundary τ CPUTime,sec |M| = 5, |Ω| = 5 static dynamic 10 20 30 40 50 10 −2 10 −1 10 0 10 1 10 2 Number of Resources |Ω| CPUTime,sec |M| = 5, τ = 50 static dynamic 2 4 6 8 10 200 400 600 800 1000 1200 1400 1600 Number of Agents |M| Value |Ω| = 5, τ = 50 static dynamic 50 100 150 200 400 500 600 700 800 900 1000 1100 1200 1300 1400 Global Time Boundary τ Value |M| = 5, |Ω| = 5 static dynamic 10 20 30 40 50 500 600 700 800 900 1000 1100 1200 1300 1400 Number of Resources |Ω| Value |M| = 5, τ = 50 static dynamic Figure 3: Evaluation of our MILP for variable numbers of agents (column 1), lengths of global-time window (column 2), and numbers of resource types (column 3).",
                "Top row shows CPU time, and bottom row shows the joint reward of agents MDP policies.",
                "Error bars show the 1st and 3rd quartiles (25% and 75%). 2 4 6 8 10 10 −3 10 −2 10 −1 10 0 10 1 10 2 10 3 Number of Agents |M| CPUTime,sec τ = 10|M| static dynamic 2 4 6 8 10 10 −3 10 −2 10 −1 10 0 10 1 10 2 10 3 10 4 Number of Agents |M| CPUTime,sec |Ω| = 2|M| static dynamic 2 4 6 8 10 10 −3 10 −2 10 −1 10 0 10 1 10 2 10 3 10 4 Number of Agents |M| CPUTime,sec |Ω| = 5|M| static dynamic 2 4 6 8 10 200 400 600 800 1000 1200 1400 1600 1800 2000 2200 Number of Agents |M| Value τ = 10|M| static dynamic 2 4 6 8 10 200 400 600 800 1000 1200 1400 1600 1800 2000 Number of Agents |M| Value |Ω| = 2|M| static dynamic 2 4 6 8 10 0 500 1000 1500 2000 2500 Number of Agents |M| Value |Ω| = 5|M| static dynamic Figure 4: Evaluation of our MILP using correlated input variables.",
                "The left column tracks the performance and CPU time as the number of agents and global-time window increase together (bτ = 10|M|).",
                "The middle and the right column track the performance and CPU time as the number of resources and the number of agents increase together as |Ω| = 2|M| and |Ω| = 5|M|, respectively.",
                "Error bars show the 1st and 3rd quartiles (25% and 75%). 1226 The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) us to explore domains where the total number of agents scales proportionally to the total number of resource types or the global time horizon, while keeping constant the average agent density (per unit of global time) or the average number of resources per agent (which commonly occurs in real-life applications).",
                "Overall, we believe that these experimental results indicate that our MILP formulation can be used to effectively solve resource-scheduling problems of nontrivial size. 6.",
                "DISCUSSION AND CONCLUSIONS Throughout the paper, we have made a number of assumptions in our model and solution algorithm; we discuss their implications below. • Continual execution.",
                "We assume that once an agent stops executing its MDP (transitions into state sf ), it exits the system and cannot return.",
                "It is easy to relax this assumption for domains where agents MDPs can be paused and restarted.",
                "All that is required is to include an additional pause action which transitions from a given state back to itself, and has zero reward. • Indifference to start time.",
                "We used a reward model where agents rewards depend only on the time horizon of their MDPs and not the global start time.",
                "This is a consequence of our MDP-augmentation procedure from Section 4.1.",
                "It is easy to extend the model so that the agents incur an explicit penalty for idling by assigning a non-zero negative reward to the start state sb . • Binary resource requirements.",
                "For simplicity, we have assumed that resource costs are binary: ϕm(a, ω) = {0, 1}, but our results generalize in a straightforward manner to non-binary resource mappings, analogously to the procedure used in [5]. • Cooperative agents.",
                "The optimization procedure discussed in this paper was developed in the context of cooperative agents, but it can also be used to design a mechanism for scheduling resources among self-interested agents.",
                "This optimization procedure can be embedded in a VickreyClarke-Groves auction, completely analogously to the way it was done in [7].",
                "In fact, all the results of [7] about the properties of the auction and information privacy directly carry over to the scheduling domain discussed in this paper, requiring only slight modifications to deal with finitehorizon MDPs. • Known, deterministic arrival and departure times.",
                "Finally, we have assumed that agents arrival and departure times (τa m and τd m) are deterministic and known a priori.",
                "This assumption is fundamental to our solution method.",
                "While there are many domains where this assumption is valid, in many cases agents arrive and depart dynamically and their arrival and departure times can only be predicted probabilistically, leading to online resource-allocation problems.",
                "In particular, in the case of self-interested agents, this becomes an interesting version of an online-mechanism-design problem [11, 12].",
                "In summary, we have presented an MILP formulation for the combinatorial resource-scheduling problem where agents values for possible resource assignments are defined by finitehorizon MDPs.",
                "This result extends previous work ([6, 7]) on static one-shot resource allocation under MDP-induced preferences to resource-scheduling problems with a temporal aspect.",
                "As such, this work takes a step in the direction of designing an online mechanism for agents with combinatorial resource preferences induced by stochastic planning problems.",
                "Relaxing the assumption about deterministic arrival and departure times of the agents is a focus of our future work.",
                "We would like to thank the anonymous reviewers for their insightful comments and suggestions. 7.",
                "REFERENCES [1] E. Altman and A. Shwartz.",
                "Adaptive control of constrained Markov chains: Criteria and policies.",
                "Annals of Operations Research, special issue on Markov Decision Processes, 28:101-134, 1991. [2] R. Bellman.",
                "Dynamic Programming.",
                "Princeton University Press, 1957. [3] C. Boutilier.",
                "Solving concisely expressed combinatorial auction problems.",
                "In Proc. of AAAI-02, pages 359-366, 2002. [4] C. Boutilier and H. H. Hoos.",
                "Bidding languages for combinatorial auctions.",
                "In Proc. of IJCAI-01, pages 1211-1217, 2001. [5] D. Dolgov.",
                "Integrated Resource Allocation and Planning in Stochastic Multiagent Environments.",
                "PhD thesis, Computer Science Department, University of Michigan, February 2006. [6] D. A. Dolgov and E. H. Durfee.",
                "Optimal resource allocation and policy formulation in loosely-coupled Markov decision processes.",
                "In Proc. of ICAPS-04, pages 315-324, June 2004. [7] D. A. Dolgov and E. H. Durfee.",
                "Computationally efficient combinatorial auctions for resource allocation in weakly-coupled MDPs.",
                "In Proc. of AAMAS-05, New York, NY, USA, 2005.",
                "ACM Press. [8] D. A. Dolgov and E. H. Durfee.",
                "Resource allocation among agents with preferences induced by factored MDPs.",
                "In Proc. of AAMAS-06, 2006. [9] K. Larson and T. Sandholm.",
                "Mechanism design and deliberative agents.",
                "In Proc. of AAMAS-05, pages 650-656, New York, NY, USA, 2005.",
                "ACM Press. [10] N. Nisan.",
                "Bidding and allocation in combinatorial auctions.",
                "In Electronic Commerce, 2000. [11] D. C. Parkes and S. Singh.",
                "An MDP-based approach to Online Mechanism Design.",
                "In Proc. of the Seventeenths Annual Conference on Neural Information Processing Systems (NIPS-03), 2003. [12] D. C. Parkes, S. Singh, and D. Yanovsky.",
                "Approximately efficient online mechanism design.",
                "In Proc. of the Eighteenths Annual Conference on Neural Information Processing Systems (NIPS-04), 2004. [13] M. L. Puterman.",
                "Markov Decision Processes.",
                "John Wiley & Sons, New York, 1994. [14] M. H. Rothkopf, A. Pekec, and R. M. Harstad.",
                "Computationally manageable combinational auctions.",
                "Management Science, 44(8):1131-1147, 1998. [15] T. Sandholm.",
                "An algorithm for optimal winner determination in combinatorial auctions.",
                "In Proc. of IJCAI-99, pages 542-547, San Francisco, CA, USA, 1999.",
                "Morgan Kaufmann Publishers Inc.",
                "The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) 1227"
            ],
            "original_annotated_samples": [],
            "translated_annotated_samples": [
                "\"Programación de recursos combinatorios\" para MDP múltiples Dmitri A. Dolgov, Michael R. James y Michael E. Investigación técnica de AI y Robotics Group, Toyota Technical Center USA {Ddolgov, Michael.R.James, Michael.samples}@gmail.La programación de recursos óptimo de resumen en resumen en sistemas múltiples es una tarea computacionalmente desafiante, particularmente cuando los valores de los recursos no son aditivos.Programación de recursos combinatorios",
                "Extendemos esos modelos existentes al problema de la \"programación de recursos combinatorios\", donde los agentes persisten solo por períodos finitos entre sus tiempos de llegada y salida (predefinidas), lo que requiere recursos solo para esos períodos de tiempo.Programación de recursos combinatorios",
                "También describimos los métodos estándar para la \"programación de recursos combinatorios\" con valores de recursos planos, que sirven como un punto de referencia de comparación para el nuevo modelo desarrollado aquí.2.1 Procesos de decisión de Markov Un MDP estacionario, de dominio finito y en tiempo discreto (ver, por ejemplo, [13] para un desarrollo exhaustivo y detallado) puede describirse como S, A, P, R, donde: S es un conjunto finitode estados del sistema;A es un conjunto finito de acciones que están disponibles para el agente;P es una función de transición estocástica estacionaria, donde p (σ | s, a) es la probabilidad de transición al estado σ al ejecutar la acción A en el estado s;r es una función de recompensa estacionaria, donde R (s, a) especifica la recompensa obtenida al ejecutar la acción A en el estado s.Dado dicho MDP, un problema de decisión bajo un horizonte finito T es elegir una acción óptima en cada paso de tiempo para maximizar el valor esperado de la recompensa total acumulada durante la vida de los agentes (finitos).Programación de recursos combinatorios",
                "Este resultado es bien conocido por los MDP de horizonte infinito con varios tipos de restricciones [1, 6], y también es válida para nuestro modelo de horizonte finito, que puede establecerse fácilmente a través de una línea de razonamiento completamente análoga a los argumentos en [6]2.2 \"Programación de recursos combinatorios\" Un enfoque directo para la programación de recursos para un conjunto de agentes M, cuyos valores para los recursos son inducidos por problemas de planificación estocástica (en nuestro caso, MDP de horarios de Horizon) sería que cada agente enumere todos los recursos posiblesAsignaciones con el tiempo y, para cada uno, calcule su valor resolviendo el MDP correspondiente.Programación de recursos combinatorios"
            ],
            "translated_text": "",
            "candidates": [],
            "error": []
        },
        "optimal resource scheduling": {
            "translated_key": "",
            "is_in_text": true,
            "original_annotated_sentences": [
                "Combinatorial Resource Scheduling for Multiagent MDPs Dmitri A. Dolgov, Michael R. James, and Michael E. Samples AI and Robotics Group Technical Research, Toyota Technical Center USA {ddolgov, michael.r.james, michael.samples}@gmail.com ABSTRACT <br>optimal resource scheduling</br> in multiagent systems is a computationally challenging task, particularly when the values of resources are not additive.",
                "We consider the combinatorial problem of scheduling the usage of multiple resources among agents that operate in stochastic environments, modeled as Markov decision processes (MDPs).",
                "In recent years, efficient resource-allocation algorithms have been developed for agents with resource values induced by MDPs.",
                "However, this prior work has focused on static resource-allocation problems where resources are distributed once and then utilized in infinite-horizon MDPs.",
                "We extend those existing models to the problem of combinatorial resource scheduling, where agents persist only for finite periods between their (predefined) arrival and departure times, requiring resources only for those time periods.",
                "We provide a computationally efficient procedure for computing globally optimal resource assignments to agents over time.",
                "We illustrate and empirically analyze the method in the context of a stochastic jobscheduling domain.",
                "Categories and Subject Descriptors I.2.8 [Artificial Intelligence]: Problem Solving, Control Methods, and Search; I.2.11 [Artificial Intelligence]: Distributed Artificial Intelligence-Multiagent systems General Terms Algorithms, Performance, Design 1.",
                "INTRODUCTION The tasks of optimal resource allocation and scheduling are ubiquitous in multiagent systems, but solving such optimization problems can be computationally difficult, due to a number of factors.",
                "In particular, when the value of a set of resources to an agent is not additive (as is often the case with resources that are substitutes or complements), the utility function might have to be defined on an exponentially large space of resource bundles, which very quickly becomes computationally intractable.",
                "Further, even when each agent has a utility function that is nonzero only on a small subset of the possible resource bundles, obtaining optimal allocation is still computationally prohibitive, as the problem becomes NP-complete [14].",
                "Such computational issues have recently spawned several threads of work in using compact models of agents preferences.",
                "One idea is to use any structure present in utility functions to represent them compactly, via, for example, logical formulas [15, 10, 4, 3].",
                "An alternative is to directly model the mechanisms that define the agents utility functions and perform resource allocation directly with these models [9].",
                "A way of accomplishing this is to model the processes by which an agent might utilize the resources and define the utility function as the payoff of these processes.",
                "In particular, if an agent uses resources to act in a stochastic environment, its utility function can be naturally modeled with a Markov decision process, whose action set is parameterized by the available resources.",
                "This representation can then be used to construct very efficient resource-allocation algorithms that lead to an exponential speedup over a straightforward optimization problem with flat representations of combinatorial preferences [6, 7, 8].",
                "However, this existing work on resource allocation with preferences induced by resource-parameterized MDPs makes an assumption that the resources are only allocated once and are then utilized by the agents independently within their infinite-horizon MDPs.",
                "This assumption that no reallocation of resources is possible can be limiting in domains where agents arrive and depart dynamically.",
                "In this paper, we extend the work on resource allocation under MDP-induced preferences to discrete-time scheduling problems, where agents are present in the system for finite time intervals and can only use resources within these intervals.",
                "In particular, agents arrive and depart at arbitrary (predefined) times and within these intervals use resources to execute tasks in finite-horizon MDPs.",
                "We address the problem of globally <br>optimal resource scheduling</br>, where the objective is to find an allocation of resources to the agents across time that maximizes the sum of the expected rewards that they obtain.",
                "In this context, our main contribution is a mixed-integerprogramming formulation of the scheduling problem that chooses globally optimal resource assignments, starting times, and execution horizons for all agents (within their arrival1220 978-81-904262-7-5 (RPS) c 2007 IFAAMAS departure intervals).",
                "We analyze and empirically compare two flavors of the scheduling problem: one, where agents have static resource assignments within their finite-horizon MDPs, and another, where resources can be dynamically reallocated between agents at every time step.",
                "In the rest of the paper, we first lay down the necessary groundwork in Section 2 and then introduce our model and formal problem statement in Section 3.",
                "In Section 4.2, we describe our main result, the optimization program for globally <br>optimal resource scheduling</br>.",
                "Following the discussion of our experimental results on a job-scheduling problem in Section 5, we conclude in Section 6 with a discussion of possible extensions and generalizations of our method. 2.",
                "BACKGROUND Similarly to the model used in previous work on resourceallocation with MDP-induced preferences [6, 7], we define the value of a set of resources to an agent as the value of the best MDP policy that is realizable, given those resources.",
                "However, since the focus of our work is on scheduling problems, and a large part of the optimization problem is to decide how resources are allocated in time among agents with finite arrival and departure times, we model the agents planning problems as finite-horizon MDPs, in contrast to previous work that used infinite-horizon discounted MDPs.",
                "In the rest of this section, we first introduce some necessary background on finite-horizon MDPs and present a linear-programming formulation that serves as the basis for our solution algorithm developed in Section 4.",
                "We also outline the standard methods for combinatorial resource scheduling with flat resource values, which serve as a comparison benchmark for the new model developed here. 2.1 Markov Decision Processes A stationary, finite-domain, discrete-time MDP (see, for example, [13] for a thorough and detailed development) can be described as S, A, p, r , where: S is a finite set of system states; A is a finite set of actions that are available to the agent; p is a stationary stochastic transition function, where p(σ|s, a) is the probability of transitioning to state σ upon executing action a in state s; r is a stationary reward function, where r(s, a) specifies the reward obtained upon executing action a in state s. Given such an MDP, a decision problem under a finite horizon T is to choose an optimal action at every time step to maximize the expected value of the total reward accrued during the agents (finite) lifetime.",
                "The agents optimal policy is then a function of current state s and the time until the horizon.",
                "An optimal policy for such a problem is to act greedily with respect to the optimal value function, defined recursively by the following system of finite-time Bellman equations [2]: v(s, t) = max a r(s, a) + X σ p(σ|s, a)v(σ, t + 1), ∀s ∈ S, t ∈ [1, T − 1]; v(s, T) = 0, ∀s ∈ S; where v(s, t) is the optimal value of being in state s at time t ∈ [1, T].",
                "This optimal value function can be easily computed using dynamic programming, leading to the following optimal policy π, where π(s, a, t) is the probability of executing action a in state s at time t: π(s, a, t) = ( 1, a = argmaxa r(s, a) + P σ p(σ|s, a)v(σ, t + 1), 0, otherwise.",
                "The above is the most common way of computing the optimal value function (and therefore an optimal policy) for a finite-horizon MDP.",
                "However, we can also formulate the problem as the following linear program (similarly to the dual LP for infinite-horizon discounted MDPs [13, 6, 7]): max X s X a r(s, a) X t x(s, a, t) subject to: X a x(σ, a, t + 1) = X s,a p(σ|s, a)x(s, a, t) ∀σ, t ∈ [1, T − 1]; X a x(s, a, 1) = α(s), ∀s ∈ S; (1) where α(s) is the initial distribution over the state space, and x is the (non-stationary) occupation measure (x(s, a, t) ∈ [0, 1] is the total expected number of times action a is executed in state s at time t).",
                "An optimal (non-stationary) policy is obtained from the occupation measure as follows: π(s, a, t) = x(s, a, t)/ X a x(s, a, t) ∀s ∈ S, t ∈ [1, T]. (2) Note that the standard unconstrained finite-horizon MDP, as described above, always has a uniformly-optimal solution (optimal for any initial distribution α(s)).",
                "Therefore, an optimal policy can be obtained by using an arbitrary constant α(s) > 0 (in particular, α(s) = 1 will result in x(s, a, t) = π(s, a, t)).",
                "However, for MDPs with resource constraints (as defined below in Section 3), uniformly-optimal policies do not in general exist.",
                "In such cases, α becomes a part of the problem input, and a resulting policy is only optimal for that particular α.",
                "This result is well known for infinite-horizon MDPs with various types of constraints [1, 6], and it also holds for our finite-horizon model, which can be easily established via a line of reasoning completely analogous to the arguments in [6]. 2.2 Combinatorial Resource Scheduling A straightforward approach to resource scheduling for a set of agents M, whose values for the resources are induced by stochastic planning problems (in our case, finite-horizon MDPs) would be to have each agent enumerate all possible resource assignments over time and, for each one, compute its value by solving the corresponding MDP.",
                "Then, each agent would provide valuations for each possible resource bundle over time to a centralized coordinator, who would compute the optimal resource assignments across time based on these valuations.",
                "When resources can be allocated at different times to different agents, each agent must submit valuations for every combination of possible time horizons.",
                "Let each agent m ∈ M execute its MDP within the arrival-departure time interval τ ∈ [τa m, τd m].",
                "Hence, agent m will execute an MDP with time horizon no greater than Tm = τd m−τa m+1.",
                "Let bτ be the global time horizon for the problem, before which all of the agents MDPs must finish.",
                "We assume τd m < bτ, ∀m ∈ M. The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) 1221 For the scheduling problem where agents have static resource requirements within their finite-horizon MDPs, the agents provide a valuation for each resource bundle for each possible time horizon (from [1, Tm]) that they may use.",
                "Let Ω be the set of resources to be allocated among the agents.",
                "An agent will get at most one resource bundle for one of the time horizons.",
                "Let the variable ψ ∈ Ψm enumerate all possible pairs of resource bundles and time horizons for agent m, so there are 2|Ω| × Tm values for ψ (the space of bundles is exponential in the number of resource types |Ω|).",
                "The agent m must provide a value vψ m for each ψ, and the coordinator will allocate at most one ψ (resource, time horizon) pair to each agent.",
                "This allocation is expressed as an indicator variable zψ m ∈ {0, 1} that shows whether ψ is assigned to agent m. For time τ and resource ω, the function nm(ψ, τ, ω) ∈ {0, 1} indicates whether the bundle in ψ uses resource ω at time τ (we make the assumption that agents have binary resource requirements).",
                "This allocation problem is NP-complete, even when considering only a single time step, and its difficulty increases significantly with multiple time steps because of the increasing number of values of ψ.",
                "The problem of finding an optimal allocation that satisfies the global constraint that the amount of each resource ω allocated to all agents does not exceed the available amount bϕ(ω) can be expressed as the following integer program: max X m∈M X ψ∈Ψm zψ mvψ m subject to: X ψ∈Ψm zψ m ≤ 1, ∀m ∈ M; X m∈M X ψ∈Ψm zψ mnm(ψ, τ, ω) ≤ bϕ(ω), ∀τ ∈ [1, bτ], ∀ω ∈ Ω; (3) The first constraint in equation 3 says that no agent can receive more than one bundle, and the second constraint ensures that the total assignment of resource ω does not, at any time, exceed the resource bound.",
                "For the scheduling problem where the agents are able to dynamically reallocate resources, each agent must specify a value for every combination of bundles and time steps within its time horizon.",
                "Let the variable ψ ∈ Ψm in this case enumerate all possible resource bundles for which at most one bundle may be assigned to agent m at each time step.",
                "Therefore, in this case there are P t∈[1,Tm](2|Ω| )t ∼ 2|Ω|Tm possibilities of resource bundles assigned to different time slots, for the Tm different time horizons.",
                "The same set of equations (3) can be used to solve this dynamic scheduling problem, but the integer program is different because of the difference in how ψ is defined.",
                "In this case, the number of ψ values is exponential in each agents planning horizon Tm, resulting in a much larger program.",
                "This straightforward approach to solving both of these scheduling problems requires an enumeration and solution of either 2|Ω| Tm (static allocation) or P t∈[1,Tm] 2|Ω|t (dynamic reallocation) MDPs for each agent, which very quickly becomes intractable with the growth of the number of resources |Ω| or the time horizon Tm. 3.",
                "MODEL AND PROBLEM STATEMENT We now formally introduce our model of the resourcescheduling problem.",
                "The problem input consists of the following components: • M, Ω, bϕ, τa m, τd m, bτ are as defined above in Section 2.2. • {Θm} = {S, A, pm, rm, αm} are the MDPs of all agents m ∈ M. Without loss of generality, we assume that state and action spaces of all agents are the same, but each has its own transition function pm, reward function rm, and initial conditions αm. • ϕm : A×Ω → {0, 1} is the mapping of actions to resources for agent m. ϕm(a, ω) indicates whether action a of agent m needs resource ω.",
                "An agent m that receives a set of resources that does not include resource ω cannot execute in its MDP policy any action a for which ϕm(a, ω) = 0.",
                "We assume all resource requirements are binary; as discussed below in Section 6, this assumption is not limiting.",
                "Given the above input, the optimization problem we consider is to find the globally optimal-maximizing the sum of expected rewards-mapping of resources to agents for all time steps: Δ : τ × M × Ω → {0, 1}.",
                "A solution is feasible if the corresponding assignment of resources to the agents does not violate the global resource constraint: X m Δm(τ, ω) ≤ bϕ(ω), ∀ω ∈ Ω, τ ∈ [1, bτ]. (4) We consider two flavors of the resource-scheduling problem.",
                "The first formulation restricts resource assignments to the space where the allocation of resources to each agent is static during the agents lifetime.",
                "The second formulation allows reassignment of resources between agents at every time step within their lifetimes.",
                "Figure 1 depicts a resource-scheduling problem with three agents M = {m1, m2, m3}, three resources Ω = {ω1, ω2, ω3}, and a global problem horizon of bτ = 11.",
                "The agents arrival and departure times are shown as gray boxes and are {1, 6}, {3, 7}, and {2, 11}, respectively.",
                "A solution to this problem is shown via horizontal bars within each agents box, where the bars correspond to the allocation of the three resource types.",
                "Figure 1a shows a solution to a static scheduling problem.",
                "According to the shown solution, agent m1 begins the execution of its MDP at time τ = 1 and has a lock on all three resources until it finishes execution at time τ = 3.",
                "Note that agent m1 relinquishes its hold on the resources before its announced departure time of τd m1 = 6, ostensibly because other agents can utilize the resources more effectively.",
                "Thus, at time τ = 4, resources ω1 and ω3 are allocated to agent m2, who then uses them to execute its MDP (using only actions supported by resources ω1 and ω3) until time τ = 7.",
                "Agent m3 holds resource ω3 during the interval τ ∈ [4, 10].",
                "Figure 1b shows a possible solution to the dynamic version of the same problem.",
                "There, resources can be reallocated between agents at every time step.",
                "For example, agent m1 gives up its use of resource ω2 at time τ = 2, although it continues the execution of its MDP until time τ = 6.",
                "Notice that an agent is not allowed to stop and restart its MDP, so agent m1 is only able to continue executing in the interval τ ∈ [3, 4] if it has actions that do not require any resources (ϕm(a, ω) = 0).",
                "Clearly, the model and problem statement described above make a number of assumptions about the problem and the desired solution properties.",
                "We discuss some of those assumptions and their implications in Section 6. 1222 The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) (a) (b) Figure 1: Illustration of a solution to a resource-scheduling problem with three agents and three resources: a) static resource assignments (resource assignments are constant within agents lifetimes; b) dynamic assignment (resource assignments are allowed to change at every time step). 4.",
                "RESOURCE SCHEDULING Our resource-scheduling algorithm proceeds in two stages.",
                "First, we perform a preprocessing step that augments the agent MDPs; this process is described in Section 4.1.",
                "Second, using these augmented MDPs we construct a global optimization problem, which is described in Section 4.2. 4.1 Augmenting Agents MDPs In the model described in the previous section, we assume that if an agent does not possess the necessary resources to perform actions in its MDP, its execution is halted and the agent leaves the system.",
                "In other words, the MDPs cannot be paused and resumed.",
                "For example, in the problem shown in Figure 1a, agent m1 releases all resources after time τ = 3, at which point the execution of its MDP is halted.",
                "Similarly, agents m2 and m3 only execute their MDPs in the intervals τ ∈ [4, 6] and τ ∈ [4, 10], respectively.",
                "Therefore, an important part of the global decision-making problem is to decide the window of time during which each of the agents is active (i.e., executing its MDP).",
                "To accomplish this, we augment each agents MDP with two new states (start and finish states sb , sf , respectively) and a new start/stop action a∗ , as illustrated in Figure 2.",
                "The idea is that an agent stays in the start state sb until it is ready to execute its MDP, at which point it performs the start/stop action a∗ and transitions into the state space of the original MDP with the transition probability that corresponds to the original initial distribution α(s).",
                "For example, in Figure 1a, for agent m2 this would happen at time τ = 4.",
                "Once the agent gets to the end of its activity window (time τ = 6 for agent m2 in Figure 1a), it performs the start/stop action, which takes it into the sink finish state sf at time τ = 7.",
                "More precisely, given an MDP S, A, pm, rm, αm , we define an augmented MDP S , A , pm, rm, αm as follows: S = S ∪ sb ∪ sf ; A = A ∪ a∗ ; p (s|sb , a∗ ) = α(s), ∀s ∈ S; p (sb |sb , a) = 1.0, ∀a ∈ A; p (sf |s, a∗ ) = 1.0, ∀s ∈ S; p (σ|s, a) = p(σ|s, a), ∀s, σ ∈ S, a ∈ A; r (sb , a) = r (sf , a) = 0, ∀a ∈ A ; r (s, a) = r(s, a), ∀s ∈ S, a ∈ A; α (sb ) = 1; α (s) = 0, ∀s ∈ S; where all non-specified transition probabilities are assumed to be zero.",
                "Further, in order to account for the new starting state, we begin the MDP one time-step earlier, setting τa m ← τa m − 1.",
                "This will not affect the resource allocation due to the resource constraints only being enforced for the original MDP states, as will be discussed in the next section.",
                "For example, the augmented MDPs shown in Figure 2b (which starts in state sb at time τ = 2) would be constructed from an MDP with original arrival time τ = 3.",
                "Figure 2b also shows a sample trajectory through the state space: the agent starts in state sb , transitions into the state space S of the original MDP, and finally exists into the sink state sf .",
                "Note that if we wanted to model a problem where agents could pause their MDPs at arbitrary time steps (which might be useful for domains where dynamic reallocation is possible), we could easily accomplish this by including an extra action that transitions from each state to itself with zero reward. 4.2 MILP for Resource Scheduling Given a set of augmented MDPs, as defined above, the goal of this section is to formulate a global optimization program that solves the resource-scheduling problem.",
                "In this section and below, all MDPs are assumed to be the augmented MDPs as defined in Section 4.1.",
                "Our approach is similar to the idea used in [6]: we begin with the linear-program formulation of agents MDPs (1) and augment it with constraints that ensure that the corresponding resource allocation across agents and time is valid.",
                "The resulting optimization problem then simultaneously solves the agents MDPs and resource-scheduling problems.",
                "In the rest of this section, we incrementally develop a mixed integer program (MILP) that achieves this.",
                "In the absence of resource constraints, the agents finitehorizon MDPs are completely independent, and the globally optimal solution can be trivially obtained via the following LP, which is simply an aggregation of single-agent finitehorizon LPs: max X m X s X a rm(s, a) X t xm(s, a, t) subject to: X a xm(σ, a, t + 1) = X s,a pm(σ|s, a)xm(s, a, t), ∀m ∈ M, σ ∈ S, t ∈ [1, Tm − 1]; X a xm(s, a, 1) = αm(s), ∀m ∈ M, s ∈ S; (12) where xm(s, a, t) is the occupation measure of agent m, and The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) 1223 (a) (b) Figure 2: Illustration of augmenting an MDP to allow for variable starting and stopping times: a) (left) the original two-state MDP with a single action; (right) the augmented MDP with new states sb and sf and the new action a∗ (note that the origianl transitions are not changed in the augmentation process); b) the augmented MDP displayed as a trajectory through time (grey lines indicate all transitions, while black lines indicate a given trajectory.",
                "Objective Function (sum of expected rewards over all agents) max X m X s X a rm(s, a) X t xm(s, a, t) (5) Meaning Implication Linear Constraints Tie x to θ.",
                "Agent is only active when occupation measure is nonzero in original MDP states. θm(τ) = 0 =⇒ xm(s, a, τ −τa m+1) = 0 ∀s /∈ {sb , sf }, a ∈ A X s/∈{sb,sf } X a xm(s, a, t) ≤ θm(τa m + t − 1) ∀m ∈ M, ∀t ∈ [1, Tm] (6) Agent can only be active in τ ∈ (τa m, τd m) θm(τ) = 0 ∀m ∈ M, τ /∈ (τa m, τd m) (7) Cannot use resources when not active θm(τ) = 0 =⇒ Δm(τ, ω) = 0 ∀τ ∈ [0, bτ], ω ∈ Ω Δm(τ, ω) ≤ θm(τ) ∀m ∈ M, τ ∈ [0, bτ], ω ∈ Ω (8) Tie x to Δ (nonzero x forces corresponding Δ to be nonzero.)",
                "Δm(τ, ω) = 0, ϕm(a, ω) = 1 =⇒ xm(s, a, τ − τa m + 1) = 0 ∀s /∈ {sb , sf } 1/|A| X a ϕm(a, ω) X s/∈{sb,sf } xm(s, a, t) ≤ Δm(t + τa m − 1, ω) ∀m ∈ M, ω ∈ Ω, t ∈ [1, Tm] (9) Resource bounds X m Δm(τ, ω) ≤ bϕ(ω) ∀ω ∈ Ω, τ ∈ [0, bτ] (10) Agent cannot change resources while active.",
                "Only enabled for scheduling with static assignments. θm(τ) = 1 and θm(τ + 1) = 1 =⇒ Δm(τ, ω) = Δm(τ + 1, ω) Δm(τ, ω) − Z(1 − θm(τ + 1)) ≤ Δm(τ + 1, ω) + Z(1 − θm(τ)) Δm(τ, ω) + Z(1 − θm(τ + 1)) ≥ Δm(τ + 1, ω) − Z(1 − θm(τ)) ∀m ∈ M, ω ∈ Ω, τ ∈ [0, bτ] (11) Table 1: MILP for globally <br>optimal resource scheduling</br>.",
                "Tm = τd m − τa m + 1 is the time horizon for the agents MDP.",
                "Using this LP as a basis, we augment it with constraints that ensure that the resource usage implied by the agents occupation measures {xm} does not violate the global resource requirements bϕ at any time step τ ∈ [0, bτ].",
                "To formulate these resource constraints, we use the following binary variables: • Δm(τ, ω) = {0, 1}, ∀m ∈ M, τ ∈ [0, bτ], ω ∈ Ω, which serve as indicator variables that define whether agent m possesses resource ω at time τ.",
                "These are analogous to the static indicator variables used in the one-shot static resource-allocation problem in [6]. • θm = {0, 1}, ∀m ∈ M, τ ∈ [0, bτ] are indicator variables that specify whether agent m is active (i.e., executing its MDP) at time τ.",
                "The meaning of resource-usage variables Δ is illustrated in Figure 1: Δm(τ, ω) = 1 only if resource ω is allocated to agent m at time τ.",
                "The meaning of the activity indicators θ is illustrated in Figure 2b: when agent m is in either the start state sb or the finish state sf , the corresponding θm = 0, but once the agent becomes active and enters one of the other states, we set θm = 1 .",
                "This meaning of θ can be enforced with a linear constraint that synchronizes the values of the agents occupation measures xm and the activity 1224 The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) indicators θ, as shown in (6) in Table 1.",
                "Another constraint we have to add-because the activity indicators θ are defined on the global timeline τ-is to enforce the fact that the agent is inactive outside of its arrivaldeparture window.",
                "This is accomplished by constraint (7) in Table 1.",
                "Furthermore, agents should not be using resources while they are inactive.",
                "This constraint can also be enforced via a linear inequality on θ and Δ, as shown in (8).",
                "Constraint (6) sets the value of θ to match the policy defined by the occupation measure xm.",
                "In a similar fashion, we have to make sure that the resource-usage variables Δ are also synchronized with the occupation measure xm.",
                "This is done via constraint (9) in Table 1, which is nearly identical to the analogous constraint from [6].",
                "After implementing the above constraint, which enforces the meaning of Δ, we add a constraint that ensures that the agents resource usage never exceeds the amounts of available resources.",
                "This condition is also trivially expressed as a linear inequality (10) in Table 1.",
                "Finally, for the problem formulation where resource assignments are static during a lifetime of an agent, we add a constraint that ensures that the resource-usage variables Δ do not change their value while the agent is active (θ = 1).",
                "This is accomplished via the linear constraint (11), where Z ≥ 2 is a constant that is used to turn off the constraints when θm(τ) = 0 or θm(τ + 1) = 0.",
                "This constraint is not used for the dynamic problem formulation, where resources can be reallocated between agents at every time step.",
                "To summarize, Table 1 together with the conservationof-flow constraints from (12) defines the MILP that simultaneously computes an optimal resource assignment for all agents across time as well as optimal finite-horizon MDP policies that are valid under that resource assignment.",
                "As a rough measure of the complexity of this MILP, let us consider the number of optimization variables and constraints.",
                "Let TM = P Tm = P m(τa m − τd m + 1) be the sum of the lengths of the arrival-departure windows across all agents.",
                "Then, the number of optimization variables is: TM + bτ|M||Ω| + bτ|M|, TM of which are continuous (xm), and bτ|M||Ω| + bτ|M| are binary (Δ and θ).",
                "However, notice that all but TM|M| of the θ are set to zero by constraint (7), which also immediately forces all but TM|M||Ω| of the Δ to be zero via the constraints (8).",
                "The number of constraints (not including the degenerate constraints in (7)) in the MILP is: TM + TM|Ω| + bτ|Ω| + bτ|M||Ω|.",
                "Despite the fact that the complexity of the MILP is, in the worst case, exponential1 in the number of binary variables, the complexity of this MILP is significantly (exponentially) lower than that of the MILP with flat utility functions, described in Section 2.2.",
                "This result echos the efficiency gains reported in [6] for single-shot resource-allocation problems, but is much more pronounced, because of the explosion of the flat utility representation due to the temporal aspect of the problem (recall the prohibitive complexity of the combinatorial optimization in Section 2.2).",
                "We empirically analyze the performance of this method in Section 5. 1 Strictly speaking, solving MILPs to optimality is NPcomplete in the number of integer variables. 5.",
                "EXPERIMENTAL RESULTS Although the complexity of solving MILPs is in the worst case exponential in the number of integer variables, there are many efficient methods for solving MILPs that allow our algorithm to scale well for parameters common to resource allocation and scheduling problems.",
                "In particular, this section introduces a problem domain-the repairshop problem-used to empirically evaluate our algorithms scalability in terms of the number of agents |M|, the number of shared resources |Ω|, and the varied lengths of global time bτ during which agents may enter and exit the system.",
                "The repairshop problem is a simple parameterized MDP adopting the metaphor of a vehicular repair shop.",
                "Agents in the repair shop are mechanics with a number of independent tasks that yield reward only when completed.",
                "In our MDP model of this system, actions taken to advance through the state space are only allowed if the agent holds certain resources that are publicly available to the shop.",
                "These resources are in finite supply, and optimal policies for the shop will determine when each agent may hold the limited resources to take actions and earn individual rewards.",
                "Each task to be completed is associated with a single action, although the agent is required to repeat the action numerous times before completing the task and earning a reward.",
                "This model was parameterized in terms of the number of agents in the system, the number of different types of resources that could be linked to necessary actions, a global time during which agents are allowed to arrive and depart, and a maximum length for the number of time steps an agent may remain in the system.",
                "All datapoints in our experiments were obtained with 20 evaluations using CPLEX to solve the MILPs on a Pentium4 computer with 2Gb of RAM.",
                "Trials were conducted on both the static and the dynamic version of the resourcescheduling problem, as defined earlier.",
                "Figure 3 shows the runtime and policy value for independent modifications to the parameter set.",
                "The top row shows how the solution time for the MILP scales as we increase the number of agents |M|, the global time horizon bτ, and the number of resources |Ω|.",
                "Increasing the number of agents leads to exponential complexity scaling, which is to be expected for an NP-complete problem.",
                "However, increasing the global time limit bτ or the total number of resource types |Ω|-while holding the number of agents constantdoes not lead to decreased performance.",
                "This occurs because the problems get easier as they become under-constrained, which is also a common phenomenon for NP-complete problems.",
                "We also observe that the solution to the dynamic version of the problem can often be computed much faster than the static version.",
                "The bottom row of Figure 3 shows the joint policy value of the policies that correspond to the computed optimal resource-allocation schedules.",
                "We can observe that the dynamic version yields higher reward (as expected, since the reward for the dynamic version is always no less than the reward of the static version).",
                "We should point out that these graphs should not be viewed as a measure of performance of two different algorithms (both algorithms produce optimal solutions but to different problems), but rather as observations about how the quality of optimal solutions change as more flexibility is allowed in the reallocation of resources.",
                "Figure 4 shows runtime and policy value for trials in which common input variables are scaled together.",
                "This allows The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) 1225 2 4 6 8 10 10 −3 10 −2 10 −1 10 0 10 1 10 2 10 3 10 4 Number of Agents |M| CPUTime,sec |Ω| = 5, τ = 50 static dynamic 50 100 150 200 10 −2 10 −1 10 0 10 1 10 2 10 3 Global Time Boundary τ CPUTime,sec |M| = 5, |Ω| = 5 static dynamic 10 20 30 40 50 10 −2 10 −1 10 0 10 1 10 2 Number of Resources |Ω| CPUTime,sec |M| = 5, τ = 50 static dynamic 2 4 6 8 10 200 400 600 800 1000 1200 1400 1600 Number of Agents |M| Value |Ω| = 5, τ = 50 static dynamic 50 100 150 200 400 500 600 700 800 900 1000 1100 1200 1300 1400 Global Time Boundary τ Value |M| = 5, |Ω| = 5 static dynamic 10 20 30 40 50 500 600 700 800 900 1000 1100 1200 1300 1400 Number of Resources |Ω| Value |M| = 5, τ = 50 static dynamic Figure 3: Evaluation of our MILP for variable numbers of agents (column 1), lengths of global-time window (column 2), and numbers of resource types (column 3).",
                "Top row shows CPU time, and bottom row shows the joint reward of agents MDP policies.",
                "Error bars show the 1st and 3rd quartiles (25% and 75%). 2 4 6 8 10 10 −3 10 −2 10 −1 10 0 10 1 10 2 10 3 Number of Agents |M| CPUTime,sec τ = 10|M| static dynamic 2 4 6 8 10 10 −3 10 −2 10 −1 10 0 10 1 10 2 10 3 10 4 Number of Agents |M| CPUTime,sec |Ω| = 2|M| static dynamic 2 4 6 8 10 10 −3 10 −2 10 −1 10 0 10 1 10 2 10 3 10 4 Number of Agents |M| CPUTime,sec |Ω| = 5|M| static dynamic 2 4 6 8 10 200 400 600 800 1000 1200 1400 1600 1800 2000 2200 Number of Agents |M| Value τ = 10|M| static dynamic 2 4 6 8 10 200 400 600 800 1000 1200 1400 1600 1800 2000 Number of Agents |M| Value |Ω| = 2|M| static dynamic 2 4 6 8 10 0 500 1000 1500 2000 2500 Number of Agents |M| Value |Ω| = 5|M| static dynamic Figure 4: Evaluation of our MILP using correlated input variables.",
                "The left column tracks the performance and CPU time as the number of agents and global-time window increase together (bτ = 10|M|).",
                "The middle and the right column track the performance and CPU time as the number of resources and the number of agents increase together as |Ω| = 2|M| and |Ω| = 5|M|, respectively.",
                "Error bars show the 1st and 3rd quartiles (25% and 75%). 1226 The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) us to explore domains where the total number of agents scales proportionally to the total number of resource types or the global time horizon, while keeping constant the average agent density (per unit of global time) or the average number of resources per agent (which commonly occurs in real-life applications).",
                "Overall, we believe that these experimental results indicate that our MILP formulation can be used to effectively solve resource-scheduling problems of nontrivial size. 6.",
                "DISCUSSION AND CONCLUSIONS Throughout the paper, we have made a number of assumptions in our model and solution algorithm; we discuss their implications below. • Continual execution.",
                "We assume that once an agent stops executing its MDP (transitions into state sf ), it exits the system and cannot return.",
                "It is easy to relax this assumption for domains where agents MDPs can be paused and restarted.",
                "All that is required is to include an additional pause action which transitions from a given state back to itself, and has zero reward. • Indifference to start time.",
                "We used a reward model where agents rewards depend only on the time horizon of their MDPs and not the global start time.",
                "This is a consequence of our MDP-augmentation procedure from Section 4.1.",
                "It is easy to extend the model so that the agents incur an explicit penalty for idling by assigning a non-zero negative reward to the start state sb . • Binary resource requirements.",
                "For simplicity, we have assumed that resource costs are binary: ϕm(a, ω) = {0, 1}, but our results generalize in a straightforward manner to non-binary resource mappings, analogously to the procedure used in [5]. • Cooperative agents.",
                "The optimization procedure discussed in this paper was developed in the context of cooperative agents, but it can also be used to design a mechanism for scheduling resources among self-interested agents.",
                "This optimization procedure can be embedded in a VickreyClarke-Groves auction, completely analogously to the way it was done in [7].",
                "In fact, all the results of [7] about the properties of the auction and information privacy directly carry over to the scheduling domain discussed in this paper, requiring only slight modifications to deal with finitehorizon MDPs. • Known, deterministic arrival and departure times.",
                "Finally, we have assumed that agents arrival and departure times (τa m and τd m) are deterministic and known a priori.",
                "This assumption is fundamental to our solution method.",
                "While there are many domains where this assumption is valid, in many cases agents arrive and depart dynamically and their arrival and departure times can only be predicted probabilistically, leading to online resource-allocation problems.",
                "In particular, in the case of self-interested agents, this becomes an interesting version of an online-mechanism-design problem [11, 12].",
                "In summary, we have presented an MILP formulation for the combinatorial resource-scheduling problem where agents values for possible resource assignments are defined by finitehorizon MDPs.",
                "This result extends previous work ([6, 7]) on static one-shot resource allocation under MDP-induced preferences to resource-scheduling problems with a temporal aspect.",
                "As such, this work takes a step in the direction of designing an online mechanism for agents with combinatorial resource preferences induced by stochastic planning problems.",
                "Relaxing the assumption about deterministic arrival and departure times of the agents is a focus of our future work.",
                "We would like to thank the anonymous reviewers for their insightful comments and suggestions. 7.",
                "REFERENCES [1] E. Altman and A. Shwartz.",
                "Adaptive control of constrained Markov chains: Criteria and policies.",
                "Annals of Operations Research, special issue on Markov Decision Processes, 28:101-134, 1991. [2] R. Bellman.",
                "Dynamic Programming.",
                "Princeton University Press, 1957. [3] C. Boutilier.",
                "Solving concisely expressed combinatorial auction problems.",
                "In Proc. of AAAI-02, pages 359-366, 2002. [4] C. Boutilier and H. H. Hoos.",
                "Bidding languages for combinatorial auctions.",
                "In Proc. of IJCAI-01, pages 1211-1217, 2001. [5] D. Dolgov.",
                "Integrated Resource Allocation and Planning in Stochastic Multiagent Environments.",
                "PhD thesis, Computer Science Department, University of Michigan, February 2006. [6] D. A. Dolgov and E. H. Durfee.",
                "Optimal resource allocation and policy formulation in loosely-coupled Markov decision processes.",
                "In Proc. of ICAPS-04, pages 315-324, June 2004. [7] D. A. Dolgov and E. H. Durfee.",
                "Computationally efficient combinatorial auctions for resource allocation in weakly-coupled MDPs.",
                "In Proc. of AAMAS-05, New York, NY, USA, 2005.",
                "ACM Press. [8] D. A. Dolgov and E. H. Durfee.",
                "Resource allocation among agents with preferences induced by factored MDPs.",
                "In Proc. of AAMAS-06, 2006. [9] K. Larson and T. Sandholm.",
                "Mechanism design and deliberative agents.",
                "In Proc. of AAMAS-05, pages 650-656, New York, NY, USA, 2005.",
                "ACM Press. [10] N. Nisan.",
                "Bidding and allocation in combinatorial auctions.",
                "In Electronic Commerce, 2000. [11] D. C. Parkes and S. Singh.",
                "An MDP-based approach to Online Mechanism Design.",
                "In Proc. of the Seventeenths Annual Conference on Neural Information Processing Systems (NIPS-03), 2003. [12] D. C. Parkes, S. Singh, and D. Yanovsky.",
                "Approximately efficient online mechanism design.",
                "In Proc. of the Eighteenths Annual Conference on Neural Information Processing Systems (NIPS-04), 2004. [13] M. L. Puterman.",
                "Markov Decision Processes.",
                "John Wiley & Sons, New York, 1994. [14] M. H. Rothkopf, A. Pekec, and R. M. Harstad.",
                "Computationally manageable combinational auctions.",
                "Management Science, 44(8):1131-1147, 1998. [15] T. Sandholm.",
                "An algorithm for optimal winner determination in combinatorial auctions.",
                "In Proc. of IJCAI-99, pages 542-547, San Francisco, CA, USA, 1999.",
                "Morgan Kaufmann Publishers Inc.",
                "The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) 1227"
            ],
            "original_annotated_samples": [],
            "translated_annotated_samples": [
                "Programación de recursos combinatorios para MDP múltiples Dmitri A. Dolgov, Michael R. James y Michael E. Investigación técnica de AI y Robotics Group, Toyota Technical Center USA {Ddolgov, Michael.R.James, Michael.samples}@gmail.comLa \"programación óptima de recursos\" en sistemas multiagentes es una tarea computacionalmente desafiante, particularmente cuando los valores de los recursos no son aditivos.Programación óptima de recursos",
                "Abordamos el problema de la \"programación de recursos óptimo\" global, donde el objetivo es encontrar una asignación de recursos a los agentes a lo largo del tiempo que maximiza la suma de las recompensas esperadas que obtienen.Programación óptima de recursos",
                "En la Sección 4.2, describimos nuestro resultado principal, el programa de optimización para la \"programación óptima de recursos\" mundial.Programación óptima de recursos",
                "Solo habilitado para programar con tareas estáticas.θm (τ) = 1 y θm (τ + 1) = 1 = ⇒ ΔM (τ, ω) = ΔM (τ + 1, ω) ΔM (τ, ω) - z (1 - θm (τ + 1))≤ Δm (τ + 1, Ω) + z (1 - θm (τ)) Δm (τ, ω) + z (1 - θm (τ + 1)) ≥ ΔM (τ + 1, ω) - z (1- θm (τ)) ∀m ∈ M, ω ∈ ω, τ ∈ [0, Bτ] (11) Tabla 1: MILP para la \"programación de recursos óptimo\" globalmente \".Programación óptima de recursos"
            ],
            "translated_text": "",
            "candidates": [],
            "error": []
        },
        "multiagent system": {
            "translated_key": "",
            "is_in_text": true,
            "original_annotated_sentences": [
                "Combinatorial Resource Scheduling for Multiagent MDPs Dmitri A. Dolgov, Michael R. James, and Michael E. Samples AI and Robotics Group Technical Research, Toyota Technical Center USA {ddolgov, michael.r.james, michael.samples}@gmail.com ABSTRACT Optimal resource scheduling in <br>multiagent system</br>s is a computationally challenging task, particularly when the values of resources are not additive.",
                "We consider the combinatorial problem of scheduling the usage of multiple resources among agents that operate in stochastic environments, modeled as Markov decision processes (MDPs).",
                "In recent years, efficient resource-allocation algorithms have been developed for agents with resource values induced by MDPs.",
                "However, this prior work has focused on static resource-allocation problems where resources are distributed once and then utilized in infinite-horizon MDPs.",
                "We extend those existing models to the problem of combinatorial resource scheduling, where agents persist only for finite periods between their (predefined) arrival and departure times, requiring resources only for those time periods.",
                "We provide a computationally efficient procedure for computing globally optimal resource assignments to agents over time.",
                "We illustrate and empirically analyze the method in the context of a stochastic jobscheduling domain.",
                "Categories and Subject Descriptors I.2.8 [Artificial Intelligence]: Problem Solving, Control Methods, and Search; I.2.11 [Artificial Intelligence]: Distributed Artificial Intelligence-Multiagent systems General Terms Algorithms, Performance, Design 1.",
                "INTRODUCTION The tasks of optimal resource allocation and scheduling are ubiquitous in <br>multiagent system</br>s, but solving such optimization problems can be computationally difficult, due to a number of factors.",
                "In particular, when the value of a set of resources to an agent is not additive (as is often the case with resources that are substitutes or complements), the utility function might have to be defined on an exponentially large space of resource bundles, which very quickly becomes computationally intractable.",
                "Further, even when each agent has a utility function that is nonzero only on a small subset of the possible resource bundles, obtaining optimal allocation is still computationally prohibitive, as the problem becomes NP-complete [14].",
                "Such computational issues have recently spawned several threads of work in using compact models of agents preferences.",
                "One idea is to use any structure present in utility functions to represent them compactly, via, for example, logical formulas [15, 10, 4, 3].",
                "An alternative is to directly model the mechanisms that define the agents utility functions and perform resource allocation directly with these models [9].",
                "A way of accomplishing this is to model the processes by which an agent might utilize the resources and define the utility function as the payoff of these processes.",
                "In particular, if an agent uses resources to act in a stochastic environment, its utility function can be naturally modeled with a Markov decision process, whose action set is parameterized by the available resources.",
                "This representation can then be used to construct very efficient resource-allocation algorithms that lead to an exponential speedup over a straightforward optimization problem with flat representations of combinatorial preferences [6, 7, 8].",
                "However, this existing work on resource allocation with preferences induced by resource-parameterized MDPs makes an assumption that the resources are only allocated once and are then utilized by the agents independently within their infinite-horizon MDPs.",
                "This assumption that no reallocation of resources is possible can be limiting in domains where agents arrive and depart dynamically.",
                "In this paper, we extend the work on resource allocation under MDP-induced preferences to discrete-time scheduling problems, where agents are present in the system for finite time intervals and can only use resources within these intervals.",
                "In particular, agents arrive and depart at arbitrary (predefined) times and within these intervals use resources to execute tasks in finite-horizon MDPs.",
                "We address the problem of globally optimal resource scheduling, where the objective is to find an allocation of resources to the agents across time that maximizes the sum of the expected rewards that they obtain.",
                "In this context, our main contribution is a mixed-integerprogramming formulation of the scheduling problem that chooses globally optimal resource assignments, starting times, and execution horizons for all agents (within their arrival1220 978-81-904262-7-5 (RPS) c 2007 IFAAMAS departure intervals).",
                "We analyze and empirically compare two flavors of the scheduling problem: one, where agents have static resource assignments within their finite-horizon MDPs, and another, where resources can be dynamically reallocated between agents at every time step.",
                "In the rest of the paper, we first lay down the necessary groundwork in Section 2 and then introduce our model and formal problem statement in Section 3.",
                "In Section 4.2, we describe our main result, the optimization program for globally optimal resource scheduling.",
                "Following the discussion of our experimental results on a job-scheduling problem in Section 5, we conclude in Section 6 with a discussion of possible extensions and generalizations of our method. 2.",
                "BACKGROUND Similarly to the model used in previous work on resourceallocation with MDP-induced preferences [6, 7], we define the value of a set of resources to an agent as the value of the best MDP policy that is realizable, given those resources.",
                "However, since the focus of our work is on scheduling problems, and a large part of the optimization problem is to decide how resources are allocated in time among agents with finite arrival and departure times, we model the agents planning problems as finite-horizon MDPs, in contrast to previous work that used infinite-horizon discounted MDPs.",
                "In the rest of this section, we first introduce some necessary background on finite-horizon MDPs and present a linear-programming formulation that serves as the basis for our solution algorithm developed in Section 4.",
                "We also outline the standard methods for combinatorial resource scheduling with flat resource values, which serve as a comparison benchmark for the new model developed here. 2.1 Markov Decision Processes A stationary, finite-domain, discrete-time MDP (see, for example, [13] for a thorough and detailed development) can be described as S, A, p, r , where: S is a finite set of system states; A is a finite set of actions that are available to the agent; p is a stationary stochastic transition function, where p(σ|s, a) is the probability of transitioning to state σ upon executing action a in state s; r is a stationary reward function, where r(s, a) specifies the reward obtained upon executing action a in state s. Given such an MDP, a decision problem under a finite horizon T is to choose an optimal action at every time step to maximize the expected value of the total reward accrued during the agents (finite) lifetime.",
                "The agents optimal policy is then a function of current state s and the time until the horizon.",
                "An optimal policy for such a problem is to act greedily with respect to the optimal value function, defined recursively by the following system of finite-time Bellman equations [2]: v(s, t) = max a r(s, a) + X σ p(σ|s, a)v(σ, t + 1), ∀s ∈ S, t ∈ [1, T − 1]; v(s, T) = 0, ∀s ∈ S; where v(s, t) is the optimal value of being in state s at time t ∈ [1, T].",
                "This optimal value function can be easily computed using dynamic programming, leading to the following optimal policy π, where π(s, a, t) is the probability of executing action a in state s at time t: π(s, a, t) = ( 1, a = argmaxa r(s, a) + P σ p(σ|s, a)v(σ, t + 1), 0, otherwise.",
                "The above is the most common way of computing the optimal value function (and therefore an optimal policy) for a finite-horizon MDP.",
                "However, we can also formulate the problem as the following linear program (similarly to the dual LP for infinite-horizon discounted MDPs [13, 6, 7]): max X s X a r(s, a) X t x(s, a, t) subject to: X a x(σ, a, t + 1) = X s,a p(σ|s, a)x(s, a, t) ∀σ, t ∈ [1, T − 1]; X a x(s, a, 1) = α(s), ∀s ∈ S; (1) where α(s) is the initial distribution over the state space, and x is the (non-stationary) occupation measure (x(s, a, t) ∈ [0, 1] is the total expected number of times action a is executed in state s at time t).",
                "An optimal (non-stationary) policy is obtained from the occupation measure as follows: π(s, a, t) = x(s, a, t)/ X a x(s, a, t) ∀s ∈ S, t ∈ [1, T]. (2) Note that the standard unconstrained finite-horizon MDP, as described above, always has a uniformly-optimal solution (optimal for any initial distribution α(s)).",
                "Therefore, an optimal policy can be obtained by using an arbitrary constant α(s) > 0 (in particular, α(s) = 1 will result in x(s, a, t) = π(s, a, t)).",
                "However, for MDPs with resource constraints (as defined below in Section 3), uniformly-optimal policies do not in general exist.",
                "In such cases, α becomes a part of the problem input, and a resulting policy is only optimal for that particular α.",
                "This result is well known for infinite-horizon MDPs with various types of constraints [1, 6], and it also holds for our finite-horizon model, which can be easily established via a line of reasoning completely analogous to the arguments in [6]. 2.2 Combinatorial Resource Scheduling A straightforward approach to resource scheduling for a set of agents M, whose values for the resources are induced by stochastic planning problems (in our case, finite-horizon MDPs) would be to have each agent enumerate all possible resource assignments over time and, for each one, compute its value by solving the corresponding MDP.",
                "Then, each agent would provide valuations for each possible resource bundle over time to a centralized coordinator, who would compute the optimal resource assignments across time based on these valuations.",
                "When resources can be allocated at different times to different agents, each agent must submit valuations for every combination of possible time horizons.",
                "Let each agent m ∈ M execute its MDP within the arrival-departure time interval τ ∈ [τa m, τd m].",
                "Hence, agent m will execute an MDP with time horizon no greater than Tm = τd m−τa m+1.",
                "Let bτ be the global time horizon for the problem, before which all of the agents MDPs must finish.",
                "We assume τd m < bτ, ∀m ∈ M. The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) 1221 For the scheduling problem where agents have static resource requirements within their finite-horizon MDPs, the agents provide a valuation for each resource bundle for each possible time horizon (from [1, Tm]) that they may use.",
                "Let Ω be the set of resources to be allocated among the agents.",
                "An agent will get at most one resource bundle for one of the time horizons.",
                "Let the variable ψ ∈ Ψm enumerate all possible pairs of resource bundles and time horizons for agent m, so there are 2|Ω| × Tm values for ψ (the space of bundles is exponential in the number of resource types |Ω|).",
                "The agent m must provide a value vψ m for each ψ, and the coordinator will allocate at most one ψ (resource, time horizon) pair to each agent.",
                "This allocation is expressed as an indicator variable zψ m ∈ {0, 1} that shows whether ψ is assigned to agent m. For time τ and resource ω, the function nm(ψ, τ, ω) ∈ {0, 1} indicates whether the bundle in ψ uses resource ω at time τ (we make the assumption that agents have binary resource requirements).",
                "This allocation problem is NP-complete, even when considering only a single time step, and its difficulty increases significantly with multiple time steps because of the increasing number of values of ψ.",
                "The problem of finding an optimal allocation that satisfies the global constraint that the amount of each resource ω allocated to all agents does not exceed the available amount bϕ(ω) can be expressed as the following integer program: max X m∈M X ψ∈Ψm zψ mvψ m subject to: X ψ∈Ψm zψ m ≤ 1, ∀m ∈ M; X m∈M X ψ∈Ψm zψ mnm(ψ, τ, ω) ≤ bϕ(ω), ∀τ ∈ [1, bτ], ∀ω ∈ Ω; (3) The first constraint in equation 3 says that no agent can receive more than one bundle, and the second constraint ensures that the total assignment of resource ω does not, at any time, exceed the resource bound.",
                "For the scheduling problem where the agents are able to dynamically reallocate resources, each agent must specify a value for every combination of bundles and time steps within its time horizon.",
                "Let the variable ψ ∈ Ψm in this case enumerate all possible resource bundles for which at most one bundle may be assigned to agent m at each time step.",
                "Therefore, in this case there are P t∈[1,Tm](2|Ω| )t ∼ 2|Ω|Tm possibilities of resource bundles assigned to different time slots, for the Tm different time horizons.",
                "The same set of equations (3) can be used to solve this dynamic scheduling problem, but the integer program is different because of the difference in how ψ is defined.",
                "In this case, the number of ψ values is exponential in each agents planning horizon Tm, resulting in a much larger program.",
                "This straightforward approach to solving both of these scheduling problems requires an enumeration and solution of either 2|Ω| Tm (static allocation) or P t∈[1,Tm] 2|Ω|t (dynamic reallocation) MDPs for each agent, which very quickly becomes intractable with the growth of the number of resources |Ω| or the time horizon Tm. 3.",
                "MODEL AND PROBLEM STATEMENT We now formally introduce our model of the resourcescheduling problem.",
                "The problem input consists of the following components: • M, Ω, bϕ, τa m, τd m, bτ are as defined above in Section 2.2. • {Θm} = {S, A, pm, rm, αm} are the MDPs of all agents m ∈ M. Without loss of generality, we assume that state and action spaces of all agents are the same, but each has its own transition function pm, reward function rm, and initial conditions αm. • ϕm : A×Ω → {0, 1} is the mapping of actions to resources for agent m. ϕm(a, ω) indicates whether action a of agent m needs resource ω.",
                "An agent m that receives a set of resources that does not include resource ω cannot execute in its MDP policy any action a for which ϕm(a, ω) = 0.",
                "We assume all resource requirements are binary; as discussed below in Section 6, this assumption is not limiting.",
                "Given the above input, the optimization problem we consider is to find the globally optimal-maximizing the sum of expected rewards-mapping of resources to agents for all time steps: Δ : τ × M × Ω → {0, 1}.",
                "A solution is feasible if the corresponding assignment of resources to the agents does not violate the global resource constraint: X m Δm(τ, ω) ≤ bϕ(ω), ∀ω ∈ Ω, τ ∈ [1, bτ]. (4) We consider two flavors of the resource-scheduling problem.",
                "The first formulation restricts resource assignments to the space where the allocation of resources to each agent is static during the agents lifetime.",
                "The second formulation allows reassignment of resources between agents at every time step within their lifetimes.",
                "Figure 1 depicts a resource-scheduling problem with three agents M = {m1, m2, m3}, three resources Ω = {ω1, ω2, ω3}, and a global problem horizon of bτ = 11.",
                "The agents arrival and departure times are shown as gray boxes and are {1, 6}, {3, 7}, and {2, 11}, respectively.",
                "A solution to this problem is shown via horizontal bars within each agents box, where the bars correspond to the allocation of the three resource types.",
                "Figure 1a shows a solution to a static scheduling problem.",
                "According to the shown solution, agent m1 begins the execution of its MDP at time τ = 1 and has a lock on all three resources until it finishes execution at time τ = 3.",
                "Note that agent m1 relinquishes its hold on the resources before its announced departure time of τd m1 = 6, ostensibly because other agents can utilize the resources more effectively.",
                "Thus, at time τ = 4, resources ω1 and ω3 are allocated to agent m2, who then uses them to execute its MDP (using only actions supported by resources ω1 and ω3) until time τ = 7.",
                "Agent m3 holds resource ω3 during the interval τ ∈ [4, 10].",
                "Figure 1b shows a possible solution to the dynamic version of the same problem.",
                "There, resources can be reallocated between agents at every time step.",
                "For example, agent m1 gives up its use of resource ω2 at time τ = 2, although it continues the execution of its MDP until time τ = 6.",
                "Notice that an agent is not allowed to stop and restart its MDP, so agent m1 is only able to continue executing in the interval τ ∈ [3, 4] if it has actions that do not require any resources (ϕm(a, ω) = 0).",
                "Clearly, the model and problem statement described above make a number of assumptions about the problem and the desired solution properties.",
                "We discuss some of those assumptions and their implications in Section 6. 1222 The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) (a) (b) Figure 1: Illustration of a solution to a resource-scheduling problem with three agents and three resources: a) static resource assignments (resource assignments are constant within agents lifetimes; b) dynamic assignment (resource assignments are allowed to change at every time step). 4.",
                "RESOURCE SCHEDULING Our resource-scheduling algorithm proceeds in two stages.",
                "First, we perform a preprocessing step that augments the agent MDPs; this process is described in Section 4.1.",
                "Second, using these augmented MDPs we construct a global optimization problem, which is described in Section 4.2. 4.1 Augmenting Agents MDPs In the model described in the previous section, we assume that if an agent does not possess the necessary resources to perform actions in its MDP, its execution is halted and the agent leaves the system.",
                "In other words, the MDPs cannot be paused and resumed.",
                "For example, in the problem shown in Figure 1a, agent m1 releases all resources after time τ = 3, at which point the execution of its MDP is halted.",
                "Similarly, agents m2 and m3 only execute their MDPs in the intervals τ ∈ [4, 6] and τ ∈ [4, 10], respectively.",
                "Therefore, an important part of the global decision-making problem is to decide the window of time during which each of the agents is active (i.e., executing its MDP).",
                "To accomplish this, we augment each agents MDP with two new states (start and finish states sb , sf , respectively) and a new start/stop action a∗ , as illustrated in Figure 2.",
                "The idea is that an agent stays in the start state sb until it is ready to execute its MDP, at which point it performs the start/stop action a∗ and transitions into the state space of the original MDP with the transition probability that corresponds to the original initial distribution α(s).",
                "For example, in Figure 1a, for agent m2 this would happen at time τ = 4.",
                "Once the agent gets to the end of its activity window (time τ = 6 for agent m2 in Figure 1a), it performs the start/stop action, which takes it into the sink finish state sf at time τ = 7.",
                "More precisely, given an MDP S, A, pm, rm, αm , we define an augmented MDP S , A , pm, rm, αm as follows: S = S ∪ sb ∪ sf ; A = A ∪ a∗ ; p (s|sb , a∗ ) = α(s), ∀s ∈ S; p (sb |sb , a) = 1.0, ∀a ∈ A; p (sf |s, a∗ ) = 1.0, ∀s ∈ S; p (σ|s, a) = p(σ|s, a), ∀s, σ ∈ S, a ∈ A; r (sb , a) = r (sf , a) = 0, ∀a ∈ A ; r (s, a) = r(s, a), ∀s ∈ S, a ∈ A; α (sb ) = 1; α (s) = 0, ∀s ∈ S; where all non-specified transition probabilities are assumed to be zero.",
                "Further, in order to account for the new starting state, we begin the MDP one time-step earlier, setting τa m ← τa m − 1.",
                "This will not affect the resource allocation due to the resource constraints only being enforced for the original MDP states, as will be discussed in the next section.",
                "For example, the augmented MDPs shown in Figure 2b (which starts in state sb at time τ = 2) would be constructed from an MDP with original arrival time τ = 3.",
                "Figure 2b also shows a sample trajectory through the state space: the agent starts in state sb , transitions into the state space S of the original MDP, and finally exists into the sink state sf .",
                "Note that if we wanted to model a problem where agents could pause their MDPs at arbitrary time steps (which might be useful for domains where dynamic reallocation is possible), we could easily accomplish this by including an extra action that transitions from each state to itself with zero reward. 4.2 MILP for Resource Scheduling Given a set of augmented MDPs, as defined above, the goal of this section is to formulate a global optimization program that solves the resource-scheduling problem.",
                "In this section and below, all MDPs are assumed to be the augmented MDPs as defined in Section 4.1.",
                "Our approach is similar to the idea used in [6]: we begin with the linear-program formulation of agents MDPs (1) and augment it with constraints that ensure that the corresponding resource allocation across agents and time is valid.",
                "The resulting optimization problem then simultaneously solves the agents MDPs and resource-scheduling problems.",
                "In the rest of this section, we incrementally develop a mixed integer program (MILP) that achieves this.",
                "In the absence of resource constraints, the agents finitehorizon MDPs are completely independent, and the globally optimal solution can be trivially obtained via the following LP, which is simply an aggregation of single-agent finitehorizon LPs: max X m X s X a rm(s, a) X t xm(s, a, t) subject to: X a xm(σ, a, t + 1) = X s,a pm(σ|s, a)xm(s, a, t), ∀m ∈ M, σ ∈ S, t ∈ [1, Tm − 1]; X a xm(s, a, 1) = αm(s), ∀m ∈ M, s ∈ S; (12) where xm(s, a, t) is the occupation measure of agent m, and The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) 1223 (a) (b) Figure 2: Illustration of augmenting an MDP to allow for variable starting and stopping times: a) (left) the original two-state MDP with a single action; (right) the augmented MDP with new states sb and sf and the new action a∗ (note that the origianl transitions are not changed in the augmentation process); b) the augmented MDP displayed as a trajectory through time (grey lines indicate all transitions, while black lines indicate a given trajectory.",
                "Objective Function (sum of expected rewards over all agents) max X m X s X a rm(s, a) X t xm(s, a, t) (5) Meaning Implication Linear Constraints Tie x to θ.",
                "Agent is only active when occupation measure is nonzero in original MDP states. θm(τ) = 0 =⇒ xm(s, a, τ −τa m+1) = 0 ∀s /∈ {sb , sf }, a ∈ A X s/∈{sb,sf } X a xm(s, a, t) ≤ θm(τa m + t − 1) ∀m ∈ M, ∀t ∈ [1, Tm] (6) Agent can only be active in τ ∈ (τa m, τd m) θm(τ) = 0 ∀m ∈ M, τ /∈ (τa m, τd m) (7) Cannot use resources when not active θm(τ) = 0 =⇒ Δm(τ, ω) = 0 ∀τ ∈ [0, bτ], ω ∈ Ω Δm(τ, ω) ≤ θm(τ) ∀m ∈ M, τ ∈ [0, bτ], ω ∈ Ω (8) Tie x to Δ (nonzero x forces corresponding Δ to be nonzero.)",
                "Δm(τ, ω) = 0, ϕm(a, ω) = 1 =⇒ xm(s, a, τ − τa m + 1) = 0 ∀s /∈ {sb , sf } 1/|A| X a ϕm(a, ω) X s/∈{sb,sf } xm(s, a, t) ≤ Δm(t + τa m − 1, ω) ∀m ∈ M, ω ∈ Ω, t ∈ [1, Tm] (9) Resource bounds X m Δm(τ, ω) ≤ bϕ(ω) ∀ω ∈ Ω, τ ∈ [0, bτ] (10) Agent cannot change resources while active.",
                "Only enabled for scheduling with static assignments. θm(τ) = 1 and θm(τ + 1) = 1 =⇒ Δm(τ, ω) = Δm(τ + 1, ω) Δm(τ, ω) − Z(1 − θm(τ + 1)) ≤ Δm(τ + 1, ω) + Z(1 − θm(τ)) Δm(τ, ω) + Z(1 − θm(τ + 1)) ≥ Δm(τ + 1, ω) − Z(1 − θm(τ)) ∀m ∈ M, ω ∈ Ω, τ ∈ [0, bτ] (11) Table 1: MILP for globally optimal resource scheduling.",
                "Tm = τd m − τa m + 1 is the time horizon for the agents MDP.",
                "Using this LP as a basis, we augment it with constraints that ensure that the resource usage implied by the agents occupation measures {xm} does not violate the global resource requirements bϕ at any time step τ ∈ [0, bτ].",
                "To formulate these resource constraints, we use the following binary variables: • Δm(τ, ω) = {0, 1}, ∀m ∈ M, τ ∈ [0, bτ], ω ∈ Ω, which serve as indicator variables that define whether agent m possesses resource ω at time τ.",
                "These are analogous to the static indicator variables used in the one-shot static resource-allocation problem in [6]. • θm = {0, 1}, ∀m ∈ M, τ ∈ [0, bτ] are indicator variables that specify whether agent m is active (i.e., executing its MDP) at time τ.",
                "The meaning of resource-usage variables Δ is illustrated in Figure 1: Δm(τ, ω) = 1 only if resource ω is allocated to agent m at time τ.",
                "The meaning of the activity indicators θ is illustrated in Figure 2b: when agent m is in either the start state sb or the finish state sf , the corresponding θm = 0, but once the agent becomes active and enters one of the other states, we set θm = 1 .",
                "This meaning of θ can be enforced with a linear constraint that synchronizes the values of the agents occupation measures xm and the activity 1224 The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) indicators θ, as shown in (6) in Table 1.",
                "Another constraint we have to add-because the activity indicators θ are defined on the global timeline τ-is to enforce the fact that the agent is inactive outside of its arrivaldeparture window.",
                "This is accomplished by constraint (7) in Table 1.",
                "Furthermore, agents should not be using resources while they are inactive.",
                "This constraint can also be enforced via a linear inequality on θ and Δ, as shown in (8).",
                "Constraint (6) sets the value of θ to match the policy defined by the occupation measure xm.",
                "In a similar fashion, we have to make sure that the resource-usage variables Δ are also synchronized with the occupation measure xm.",
                "This is done via constraint (9) in Table 1, which is nearly identical to the analogous constraint from [6].",
                "After implementing the above constraint, which enforces the meaning of Δ, we add a constraint that ensures that the agents resource usage never exceeds the amounts of available resources.",
                "This condition is also trivially expressed as a linear inequality (10) in Table 1.",
                "Finally, for the problem formulation where resource assignments are static during a lifetime of an agent, we add a constraint that ensures that the resource-usage variables Δ do not change their value while the agent is active (θ = 1).",
                "This is accomplished via the linear constraint (11), where Z ≥ 2 is a constant that is used to turn off the constraints when θm(τ) = 0 or θm(τ + 1) = 0.",
                "This constraint is not used for the dynamic problem formulation, where resources can be reallocated between agents at every time step.",
                "To summarize, Table 1 together with the conservationof-flow constraints from (12) defines the MILP that simultaneously computes an optimal resource assignment for all agents across time as well as optimal finite-horizon MDP policies that are valid under that resource assignment.",
                "As a rough measure of the complexity of this MILP, let us consider the number of optimization variables and constraints.",
                "Let TM = P Tm = P m(τa m − τd m + 1) be the sum of the lengths of the arrival-departure windows across all agents.",
                "Then, the number of optimization variables is: TM + bτ|M||Ω| + bτ|M|, TM of which are continuous (xm), and bτ|M||Ω| + bτ|M| are binary (Δ and θ).",
                "However, notice that all but TM|M| of the θ are set to zero by constraint (7), which also immediately forces all but TM|M||Ω| of the Δ to be zero via the constraints (8).",
                "The number of constraints (not including the degenerate constraints in (7)) in the MILP is: TM + TM|Ω| + bτ|Ω| + bτ|M||Ω|.",
                "Despite the fact that the complexity of the MILP is, in the worst case, exponential1 in the number of binary variables, the complexity of this MILP is significantly (exponentially) lower than that of the MILP with flat utility functions, described in Section 2.2.",
                "This result echos the efficiency gains reported in [6] for single-shot resource-allocation problems, but is much more pronounced, because of the explosion of the flat utility representation due to the temporal aspect of the problem (recall the prohibitive complexity of the combinatorial optimization in Section 2.2).",
                "We empirically analyze the performance of this method in Section 5. 1 Strictly speaking, solving MILPs to optimality is NPcomplete in the number of integer variables. 5.",
                "EXPERIMENTAL RESULTS Although the complexity of solving MILPs is in the worst case exponential in the number of integer variables, there are many efficient methods for solving MILPs that allow our algorithm to scale well for parameters common to resource allocation and scheduling problems.",
                "In particular, this section introduces a problem domain-the repairshop problem-used to empirically evaluate our algorithms scalability in terms of the number of agents |M|, the number of shared resources |Ω|, and the varied lengths of global time bτ during which agents may enter and exit the system.",
                "The repairshop problem is a simple parameterized MDP adopting the metaphor of a vehicular repair shop.",
                "Agents in the repair shop are mechanics with a number of independent tasks that yield reward only when completed.",
                "In our MDP model of this system, actions taken to advance through the state space are only allowed if the agent holds certain resources that are publicly available to the shop.",
                "These resources are in finite supply, and optimal policies for the shop will determine when each agent may hold the limited resources to take actions and earn individual rewards.",
                "Each task to be completed is associated with a single action, although the agent is required to repeat the action numerous times before completing the task and earning a reward.",
                "This model was parameterized in terms of the number of agents in the system, the number of different types of resources that could be linked to necessary actions, a global time during which agents are allowed to arrive and depart, and a maximum length for the number of time steps an agent may remain in the system.",
                "All datapoints in our experiments were obtained with 20 evaluations using CPLEX to solve the MILPs on a Pentium4 computer with 2Gb of RAM.",
                "Trials were conducted on both the static and the dynamic version of the resourcescheduling problem, as defined earlier.",
                "Figure 3 shows the runtime and policy value for independent modifications to the parameter set.",
                "The top row shows how the solution time for the MILP scales as we increase the number of agents |M|, the global time horizon bτ, and the number of resources |Ω|.",
                "Increasing the number of agents leads to exponential complexity scaling, which is to be expected for an NP-complete problem.",
                "However, increasing the global time limit bτ or the total number of resource types |Ω|-while holding the number of agents constantdoes not lead to decreased performance.",
                "This occurs because the problems get easier as they become under-constrained, which is also a common phenomenon for NP-complete problems.",
                "We also observe that the solution to the dynamic version of the problem can often be computed much faster than the static version.",
                "The bottom row of Figure 3 shows the joint policy value of the policies that correspond to the computed optimal resource-allocation schedules.",
                "We can observe that the dynamic version yields higher reward (as expected, since the reward for the dynamic version is always no less than the reward of the static version).",
                "We should point out that these graphs should not be viewed as a measure of performance of two different algorithms (both algorithms produce optimal solutions but to different problems), but rather as observations about how the quality of optimal solutions change as more flexibility is allowed in the reallocation of resources.",
                "Figure 4 shows runtime and policy value for trials in which common input variables are scaled together.",
                "This allows The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) 1225 2 4 6 8 10 10 −3 10 −2 10 −1 10 0 10 1 10 2 10 3 10 4 Number of Agents |M| CPUTime,sec |Ω| = 5, τ = 50 static dynamic 50 100 150 200 10 −2 10 −1 10 0 10 1 10 2 10 3 Global Time Boundary τ CPUTime,sec |M| = 5, |Ω| = 5 static dynamic 10 20 30 40 50 10 −2 10 −1 10 0 10 1 10 2 Number of Resources |Ω| CPUTime,sec |M| = 5, τ = 50 static dynamic 2 4 6 8 10 200 400 600 800 1000 1200 1400 1600 Number of Agents |M| Value |Ω| = 5, τ = 50 static dynamic 50 100 150 200 400 500 600 700 800 900 1000 1100 1200 1300 1400 Global Time Boundary τ Value |M| = 5, |Ω| = 5 static dynamic 10 20 30 40 50 500 600 700 800 900 1000 1100 1200 1300 1400 Number of Resources |Ω| Value |M| = 5, τ = 50 static dynamic Figure 3: Evaluation of our MILP for variable numbers of agents (column 1), lengths of global-time window (column 2), and numbers of resource types (column 3).",
                "Top row shows CPU time, and bottom row shows the joint reward of agents MDP policies.",
                "Error bars show the 1st and 3rd quartiles (25% and 75%). 2 4 6 8 10 10 −3 10 −2 10 −1 10 0 10 1 10 2 10 3 Number of Agents |M| CPUTime,sec τ = 10|M| static dynamic 2 4 6 8 10 10 −3 10 −2 10 −1 10 0 10 1 10 2 10 3 10 4 Number of Agents |M| CPUTime,sec |Ω| = 2|M| static dynamic 2 4 6 8 10 10 −3 10 −2 10 −1 10 0 10 1 10 2 10 3 10 4 Number of Agents |M| CPUTime,sec |Ω| = 5|M| static dynamic 2 4 6 8 10 200 400 600 800 1000 1200 1400 1600 1800 2000 2200 Number of Agents |M| Value τ = 10|M| static dynamic 2 4 6 8 10 200 400 600 800 1000 1200 1400 1600 1800 2000 Number of Agents |M| Value |Ω| = 2|M| static dynamic 2 4 6 8 10 0 500 1000 1500 2000 2500 Number of Agents |M| Value |Ω| = 5|M| static dynamic Figure 4: Evaluation of our MILP using correlated input variables.",
                "The left column tracks the performance and CPU time as the number of agents and global-time window increase together (bτ = 10|M|).",
                "The middle and the right column track the performance and CPU time as the number of resources and the number of agents increase together as |Ω| = 2|M| and |Ω| = 5|M|, respectively.",
                "Error bars show the 1st and 3rd quartiles (25% and 75%). 1226 The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) us to explore domains where the total number of agents scales proportionally to the total number of resource types or the global time horizon, while keeping constant the average agent density (per unit of global time) or the average number of resources per agent (which commonly occurs in real-life applications).",
                "Overall, we believe that these experimental results indicate that our MILP formulation can be used to effectively solve resource-scheduling problems of nontrivial size. 6.",
                "DISCUSSION AND CONCLUSIONS Throughout the paper, we have made a number of assumptions in our model and solution algorithm; we discuss their implications below. • Continual execution.",
                "We assume that once an agent stops executing its MDP (transitions into state sf ), it exits the system and cannot return.",
                "It is easy to relax this assumption for domains where agents MDPs can be paused and restarted.",
                "All that is required is to include an additional pause action which transitions from a given state back to itself, and has zero reward. • Indifference to start time.",
                "We used a reward model where agents rewards depend only on the time horizon of their MDPs and not the global start time.",
                "This is a consequence of our MDP-augmentation procedure from Section 4.1.",
                "It is easy to extend the model so that the agents incur an explicit penalty for idling by assigning a non-zero negative reward to the start state sb . • Binary resource requirements.",
                "For simplicity, we have assumed that resource costs are binary: ϕm(a, ω) = {0, 1}, but our results generalize in a straightforward manner to non-binary resource mappings, analogously to the procedure used in [5]. • Cooperative agents.",
                "The optimization procedure discussed in this paper was developed in the context of cooperative agents, but it can also be used to design a mechanism for scheduling resources among self-interested agents.",
                "This optimization procedure can be embedded in a VickreyClarke-Groves auction, completely analogously to the way it was done in [7].",
                "In fact, all the results of [7] about the properties of the auction and information privacy directly carry over to the scheduling domain discussed in this paper, requiring only slight modifications to deal with finitehorizon MDPs. • Known, deterministic arrival and departure times.",
                "Finally, we have assumed that agents arrival and departure times (τa m and τd m) are deterministic and known a priori.",
                "This assumption is fundamental to our solution method.",
                "While there are many domains where this assumption is valid, in many cases agents arrive and depart dynamically and their arrival and departure times can only be predicted probabilistically, leading to online resource-allocation problems.",
                "In particular, in the case of self-interested agents, this becomes an interesting version of an online-mechanism-design problem [11, 12].",
                "In summary, we have presented an MILP formulation for the combinatorial resource-scheduling problem where agents values for possible resource assignments are defined by finitehorizon MDPs.",
                "This result extends previous work ([6, 7]) on static one-shot resource allocation under MDP-induced preferences to resource-scheduling problems with a temporal aspect.",
                "As such, this work takes a step in the direction of designing an online mechanism for agents with combinatorial resource preferences induced by stochastic planning problems.",
                "Relaxing the assumption about deterministic arrival and departure times of the agents is a focus of our future work.",
                "We would like to thank the anonymous reviewers for their insightful comments and suggestions. 7.",
                "REFERENCES [1] E. Altman and A. Shwartz.",
                "Adaptive control of constrained Markov chains: Criteria and policies.",
                "Annals of Operations Research, special issue on Markov Decision Processes, 28:101-134, 1991. [2] R. Bellman.",
                "Dynamic Programming.",
                "Princeton University Press, 1957. [3] C. Boutilier.",
                "Solving concisely expressed combinatorial auction problems.",
                "In Proc. of AAAI-02, pages 359-366, 2002. [4] C. Boutilier and H. H. Hoos.",
                "Bidding languages for combinatorial auctions.",
                "In Proc. of IJCAI-01, pages 1211-1217, 2001. [5] D. Dolgov.",
                "Integrated Resource Allocation and Planning in Stochastic Multiagent Environments.",
                "PhD thesis, Computer Science Department, University of Michigan, February 2006. [6] D. A. Dolgov and E. H. Durfee.",
                "Optimal resource allocation and policy formulation in loosely-coupled Markov decision processes.",
                "In Proc. of ICAPS-04, pages 315-324, June 2004. [7] D. A. Dolgov and E. H. Durfee.",
                "Computationally efficient combinatorial auctions for resource allocation in weakly-coupled MDPs.",
                "In Proc. of AAMAS-05, New York, NY, USA, 2005.",
                "ACM Press. [8] D. A. Dolgov and E. H. Durfee.",
                "Resource allocation among agents with preferences induced by factored MDPs.",
                "In Proc. of AAMAS-06, 2006. [9] K. Larson and T. Sandholm.",
                "Mechanism design and deliberative agents.",
                "In Proc. of AAMAS-05, pages 650-656, New York, NY, USA, 2005.",
                "ACM Press. [10] N. Nisan.",
                "Bidding and allocation in combinatorial auctions.",
                "In Electronic Commerce, 2000. [11] D. C. Parkes and S. Singh.",
                "An MDP-based approach to Online Mechanism Design.",
                "In Proc. of the Seventeenths Annual Conference on Neural Information Processing Systems (NIPS-03), 2003. [12] D. C. Parkes, S. Singh, and D. Yanovsky.",
                "Approximately efficient online mechanism design.",
                "In Proc. of the Eighteenths Annual Conference on Neural Information Processing Systems (NIPS-04), 2004. [13] M. L. Puterman.",
                "Markov Decision Processes.",
                "John Wiley & Sons, New York, 1994. [14] M. H. Rothkopf, A. Pekec, and R. M. Harstad.",
                "Computationally manageable combinational auctions.",
                "Management Science, 44(8):1131-1147, 1998. [15] T. Sandholm.",
                "An algorithm for optimal winner determination in combinatorial auctions.",
                "In Proc. of IJCAI-99, pages 542-547, San Francisco, CA, USA, 1999.",
                "Morgan Kaufmann Publishers Inc.",
                "The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) 1227"
            ],
            "original_annotated_samples": [],
            "translated_annotated_samples": [
                "Programación de recursos combinatorios para MDP múltiples Dmitri A. Dolgov, Michael R. James y Michael E. Investigación técnica de AI y Robotics Group, Toyota Technical Center USA {Ddolgov, Michael.R.James, Michael.samples}@gmail.comLa programación óptima de los recursos en el \"sistema multiagente\" es una tarea computacionalmente desafiante, particularmente cuando los valores de los recursos no son aditivos.sistema multiagente",
                "Introducción Las tareas de asignación y programación de recursos óptimos son ubicuas en los \"sistemas multiagentes\", pero resolver tales problemas de optimización puede ser computacionalmente difícil, debido a una serie de factores.sistema multiagente"
            ],
            "translated_text": "",
            "candidates": [],
            "error": []
        },
        "resource": {
            "translated_key": "",
            "is_in_text": true,
            "original_annotated_sentences": [
                "Combinatorial <br>resource</br> Scheduling for Multiagent MDPs Dmitri A. Dolgov, Michael R. James, and Michael E. Samples AI and Robotics Group Technical Research, Toyota Technical Center USA {ddolgov, michael.r.james, michael.samples}@gmail.com ABSTRACT Optimal <br>resource</br> scheduling in multiagent systems is a computationally challenging task, particularly when the values of resources are not additive.",
                "We consider the combinatorial problem of scheduling the usage of multiple resources among agents that operate in stochastic environments, modeled as Markov decision processes (MDPs).",
                "In recent years, efficient <br>resource</br>-allocation algorithms have been developed for agents with <br>resource</br> values induced by MDPs.",
                "However, this prior work has focused on static <br>resource</br>-allocation problems where resources are distributed once and then utilized in infinite-horizon MDPs.",
                "We extend those existing models to the problem of combinatorial <br>resource</br> scheduling, where agents persist only for finite periods between their (predefined) arrival and departure times, requiring resources only for those time periods.",
                "We provide a computationally efficient procedure for computing globally optimal <br>resource</br> assignments to agents over time.",
                "We illustrate and empirically analyze the method in the context of a stochastic jobscheduling domain.",
                "Categories and Subject Descriptors I.2.8 [Artificial Intelligence]: Problem Solving, Control Methods, and Search; I.2.11 [Artificial Intelligence]: Distributed Artificial Intelligence-Multiagent systems General Terms Algorithms, Performance, Design 1.",
                "INTRODUCTION The tasks of optimal <br>resource</br> allocation and scheduling are ubiquitous in multiagent systems, but solving such optimization problems can be computationally difficult, due to a number of factors.",
                "In particular, when the value of a set of resources to an agent is not additive (as is often the case with resources that are substitutes or complements), the utility function might have to be defined on an exponentially large space of <br>resource</br> bundles, which very quickly becomes computationally intractable.",
                "Further, even when each agent has a utility function that is nonzero only on a small subset of the possible <br>resource</br> bundles, obtaining optimal allocation is still computationally prohibitive, as the problem becomes NP-complete [14].",
                "Such computational issues have recently spawned several threads of work in using compact models of agents preferences.",
                "One idea is to use any structure present in utility functions to represent them compactly, via, for example, logical formulas [15, 10, 4, 3].",
                "An alternative is to directly model the mechanisms that define the agents utility functions and perform <br>resource</br> allocation directly with these models [9].",
                "A way of accomplishing this is to model the processes by which an agent might utilize the resources and define the utility function as the payoff of these processes.",
                "In particular, if an agent uses resources to act in a stochastic environment, its utility function can be naturally modeled with a Markov decision process, whose action set is parameterized by the available resources.",
                "This representation can then be used to construct very efficient <br>resource</br>-allocation algorithms that lead to an exponential speedup over a straightforward optimization problem with flat representations of combinatorial preferences [6, 7, 8].",
                "However, this existing work on <br>resource</br> allocation with preferences induced by <br>resource</br>-parameterized MDPs makes an assumption that the resources are only allocated once and are then utilized by the agents independently within their infinite-horizon MDPs.",
                "This assumption that no reallocation of resources is possible can be limiting in domains where agents arrive and depart dynamically.",
                "In this paper, we extend the work on <br>resource</br> allocation under MDP-induced preferences to discrete-time scheduling problems, where agents are present in the system for finite time intervals and can only use resources within these intervals.",
                "In particular, agents arrive and depart at arbitrary (predefined) times and within these intervals use resources to execute tasks in finite-horizon MDPs.",
                "We address the problem of globally optimal <br>resource</br> scheduling, where the objective is to find an allocation of resources to the agents across time that maximizes the sum of the expected rewards that they obtain.",
                "In this context, our main contribution is a mixed-integerprogramming formulation of the scheduling problem that chooses globally optimal <br>resource</br> assignments, starting times, and execution horizons for all agents (within their arrival1220 978-81-904262-7-5 (RPS) c 2007 IFAAMAS departure intervals).",
                "We analyze and empirically compare two flavors of the scheduling problem: one, where agents have static <br>resource</br> assignments within their finite-horizon MDPs, and another, where resources can be dynamically reallocated between agents at every time step.",
                "In the rest of the paper, we first lay down the necessary groundwork in Section 2 and then introduce our model and formal problem statement in Section 3.",
                "In Section 4.2, we describe our main result, the optimization program for globally optimal <br>resource</br> scheduling.",
                "Following the discussion of our experimental results on a job-scheduling problem in Section 5, we conclude in Section 6 with a discussion of possible extensions and generalizations of our method. 2.",
                "BACKGROUND Similarly to the model used in previous work on resourceallocation with MDP-induced preferences [6, 7], we define the value of a set of resources to an agent as the value of the best MDP policy that is realizable, given those resources.",
                "However, since the focus of our work is on scheduling problems, and a large part of the optimization problem is to decide how resources are allocated in time among agents with finite arrival and departure times, we model the agents planning problems as finite-horizon MDPs, in contrast to previous work that used infinite-horizon discounted MDPs.",
                "In the rest of this section, we first introduce some necessary background on finite-horizon MDPs and present a linear-programming formulation that serves as the basis for our solution algorithm developed in Section 4.",
                "We also outline the standard methods for combinatorial <br>resource</br> scheduling with flat <br>resource</br> values, which serve as a comparison benchmark for the new model developed here. 2.1 Markov Decision Processes A stationary, finite-domain, discrete-time MDP (see, for example, [13] for a thorough and detailed development) can be described as S, A, p, r , where: S is a finite set of system states; A is a finite set of actions that are available to the agent; p is a stationary stochastic transition function, where p(σ|s, a) is the probability of transitioning to state σ upon executing action a in state s; r is a stationary reward function, where r(s, a) specifies the reward obtained upon executing action a in state s. Given such an MDP, a decision problem under a finite horizon T is to choose an optimal action at every time step to maximize the expected value of the total reward accrued during the agents (finite) lifetime.",
                "The agents optimal policy is then a function of current state s and the time until the horizon.",
                "An optimal policy for such a problem is to act greedily with respect to the optimal value function, defined recursively by the following system of finite-time Bellman equations [2]: v(s, t) = max a r(s, a) + X σ p(σ|s, a)v(σ, t + 1), ∀s ∈ S, t ∈ [1, T − 1]; v(s, T) = 0, ∀s ∈ S; where v(s, t) is the optimal value of being in state s at time t ∈ [1, T].",
                "This optimal value function can be easily computed using dynamic programming, leading to the following optimal policy π, where π(s, a, t) is the probability of executing action a in state s at time t: π(s, a, t) = ( 1, a = argmaxa r(s, a) + P σ p(σ|s, a)v(σ, t + 1), 0, otherwise.",
                "The above is the most common way of computing the optimal value function (and therefore an optimal policy) for a finite-horizon MDP.",
                "However, we can also formulate the problem as the following linear program (similarly to the dual LP for infinite-horizon discounted MDPs [13, 6, 7]): max X s X a r(s, a) X t x(s, a, t) subject to: X a x(σ, a, t + 1) = X s,a p(σ|s, a)x(s, a, t) ∀σ, t ∈ [1, T − 1]; X a x(s, a, 1) = α(s), ∀s ∈ S; (1) where α(s) is the initial distribution over the state space, and x is the (non-stationary) occupation measure (x(s, a, t) ∈ [0, 1] is the total expected number of times action a is executed in state s at time t).",
                "An optimal (non-stationary) policy is obtained from the occupation measure as follows: π(s, a, t) = x(s, a, t)/ X a x(s, a, t) ∀s ∈ S, t ∈ [1, T]. (2) Note that the standard unconstrained finite-horizon MDP, as described above, always has a uniformly-optimal solution (optimal for any initial distribution α(s)).",
                "Therefore, an optimal policy can be obtained by using an arbitrary constant α(s) > 0 (in particular, α(s) = 1 will result in x(s, a, t) = π(s, a, t)).",
                "However, for MDPs with <br>resource</br> constraints (as defined below in Section 3), uniformly-optimal policies do not in general exist.",
                "In such cases, α becomes a part of the problem input, and a resulting policy is only optimal for that particular α.",
                "This result is well known for infinite-horizon MDPs with various types of constraints [1, 6], and it also holds for our finite-horizon model, which can be easily established via a line of reasoning completely analogous to the arguments in [6]. 2.2 Combinatorial <br>resource</br> Scheduling A straightforward approach to <br>resource</br> scheduling for a set of agents M, whose values for the resources are induced by stochastic planning problems (in our case, finite-horizon MDPs) would be to have each agent enumerate all possible resource assignments over time and, for each one, compute its value by solving the corresponding MDP.",
                "Then, each agent would provide valuations for each possible <br>resource</br> bundle over time to a centralized coordinator, who would compute the optimal <br>resource</br> assignments across time based on these valuations.",
                "When resources can be allocated at different times to different agents, each agent must submit valuations for every combination of possible time horizons.",
                "Let each agent m ∈ M execute its MDP within the arrival-departure time interval τ ∈ [τa m, τd m].",
                "Hence, agent m will execute an MDP with time horizon no greater than Tm = τd m−τa m+1.",
                "Let bτ be the global time horizon for the problem, before which all of the agents MDPs must finish.",
                "We assume τd m < bτ, ∀m ∈ M. The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) 1221 For the scheduling problem where agents have static <br>resource</br> requirements within their finite-horizon MDPs, the agents provide a valuation for each <br>resource</br> bundle for each possible time horizon (from [1, Tm]) that they may use.",
                "Let Ω be the set of resources to be allocated among the agents.",
                "An agent will get at most one <br>resource</br> bundle for one of the time horizons.",
                "Let the variable ψ ∈ Ψm enumerate all possible pairs of <br>resource</br> bundles and time horizons for agent m, so there are 2|Ω| × Tm values for ψ (the space of bundles is exponential in the number of <br>resource</br> types |Ω|).",
                "The agent m must provide a value vψ m for each ψ, and the coordinator will allocate at most one ψ (<br>resource</br>, time horizon) pair to each agent.",
                "This allocation is expressed as an indicator variable zψ m ∈ {0, 1} that shows whether ψ is assigned to agent m. For time τ and <br>resource</br> ω, the function nm(ψ, τ, ω) ∈ {0, 1} indicates whether the bundle in ψ uses <br>resource</br> ω at time τ (we make the assumption that agents have binary resource requirements).",
                "This allocation problem is NP-complete, even when considering only a single time step, and its difficulty increases significantly with multiple time steps because of the increasing number of values of ψ.",
                "The problem of finding an optimal allocation that satisfies the global constraint that the amount of each <br>resource</br> ω allocated to all agents does not exceed the available amount bϕ(ω) can be expressed as the following integer program: max X m∈M X ψ∈Ψm zψ mvψ m subject to: X ψ∈Ψm zψ m ≤ 1, ∀m ∈ M; X m∈M X ψ∈Ψm zψ mnm(ψ, τ, ω) ≤ bϕ(ω), ∀τ ∈ [1, bτ], ∀ω ∈ Ω; (3) The first constraint in equation 3 says that no agent can receive more than one bundle, and the second constraint ensures that the total assignment of <br>resource</br> ω does not, at any time, exceed the resource bound.",
                "For the scheduling problem where the agents are able to dynamically reallocate resources, each agent must specify a value for every combination of bundles and time steps within its time horizon.",
                "Let the variable ψ ∈ Ψm in this case enumerate all possible <br>resource</br> bundles for which at most one bundle may be assigned to agent m at each time step.",
                "Therefore, in this case there are P t∈[1,Tm](2|Ω| )t ∼ 2|Ω|Tm possibilities of <br>resource</br> bundles assigned to different time slots, for the Tm different time horizons.",
                "The same set of equations (3) can be used to solve this dynamic scheduling problem, but the integer program is different because of the difference in how ψ is defined.",
                "In this case, the number of ψ values is exponential in each agents planning horizon Tm, resulting in a much larger program.",
                "This straightforward approach to solving both of these scheduling problems requires an enumeration and solution of either 2|Ω| Tm (static allocation) or P t∈[1,Tm] 2|Ω|t (dynamic reallocation) MDPs for each agent, which very quickly becomes intractable with the growth of the number of resources |Ω| or the time horizon Tm. 3.",
                "MODEL AND PROBLEM STATEMENT We now formally introduce our model of the resourcescheduling problem.",
                "The problem input consists of the following components: • M, Ω, bϕ, τa m, τd m, bτ are as defined above in Section 2.2. • {Θm} = {S, A, pm, rm, αm} are the MDPs of all agents m ∈ M. Without loss of generality, we assume that state and action spaces of all agents are the same, but each has its own transition function pm, reward function rm, and initial conditions αm. • ϕm : A×Ω → {0, 1} is the mapping of actions to resources for agent m. ϕm(a, ω) indicates whether action a of agent m needs <br>resource</br> ω.",
                "An agent m that receives a set of resources that does not include <br>resource</br> ω cannot execute in its MDP policy any action a for which ϕm(a, ω) = 0.",
                "We assume all <br>resource</br> requirements are binary; as discussed below in Section 6, this assumption is not limiting.",
                "Given the above input, the optimization problem we consider is to find the globally optimal-maximizing the sum of expected rewards-mapping of resources to agents for all time steps: Δ : τ × M × Ω → {0, 1}.",
                "A solution is feasible if the corresponding assignment of resources to the agents does not violate the global <br>resource</br> constraint: X m Δm(τ, ω) ≤ bϕ(ω), ∀ω ∈ Ω, τ ∈ [1, bτ]. (4) We consider two flavors of the <br>resource</br>-scheduling problem.",
                "The first formulation restricts <br>resource</br> assignments to the space where the allocation of resources to each agent is static during the agents lifetime.",
                "The second formulation allows reassignment of resources between agents at every time step within their lifetimes.",
                "Figure 1 depicts a <br>resource</br>-scheduling problem with three agents M = {m1, m2, m3}, three resources Ω = {ω1, ω2, ω3}, and a global problem horizon of bτ = 11.",
                "The agents arrival and departure times are shown as gray boxes and are {1, 6}, {3, 7}, and {2, 11}, respectively.",
                "A solution to this problem is shown via horizontal bars within each agents box, where the bars correspond to the allocation of the three <br>resource</br> types.",
                "Figure 1a shows a solution to a static scheduling problem.",
                "According to the shown solution, agent m1 begins the execution of its MDP at time τ = 1 and has a lock on all three resources until it finishes execution at time τ = 3.",
                "Note that agent m1 relinquishes its hold on the resources before its announced departure time of τd m1 = 6, ostensibly because other agents can utilize the resources more effectively.",
                "Thus, at time τ = 4, resources ω1 and ω3 are allocated to agent m2, who then uses them to execute its MDP (using only actions supported by resources ω1 and ω3) until time τ = 7.",
                "Agent m3 holds <br>resource</br> ω3 during the interval τ ∈ [4, 10].",
                "Figure 1b shows a possible solution to the dynamic version of the same problem.",
                "There, resources can be reallocated between agents at every time step.",
                "For example, agent m1 gives up its use of <br>resource</br> ω2 at time τ = 2, although it continues the execution of its MDP until time τ = 6.",
                "Notice that an agent is not allowed to stop and restart its MDP, so agent m1 is only able to continue executing in the interval τ ∈ [3, 4] if it has actions that do not require any resources (ϕm(a, ω) = 0).",
                "Clearly, the model and problem statement described above make a number of assumptions about the problem and the desired solution properties.",
                "We discuss some of those assumptions and their implications in Section 6. 1222 The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) (a) (b) Figure 1: Illustration of a solution to a <br>resource</br>-scheduling problem with three agents and three resources: a) static <br>resource</br> assignments (resource assignments are constant within agents lifetimes; b) dynamic assignment (resource assignments are allowed to change at every time step). 4.",
                "<br>resource</br> SCHEDULING Our <br>resource</br>-scheduling algorithm proceeds in two stages.",
                "First, we perform a preprocessing step that augments the agent MDPs; this process is described in Section 4.1.",
                "Second, using these augmented MDPs we construct a global optimization problem, which is described in Section 4.2. 4.1 Augmenting Agents MDPs In the model described in the previous section, we assume that if an agent does not possess the necessary resources to perform actions in its MDP, its execution is halted and the agent leaves the system.",
                "In other words, the MDPs cannot be paused and resumed.",
                "For example, in the problem shown in Figure 1a, agent m1 releases all resources after time τ = 3, at which point the execution of its MDP is halted.",
                "Similarly, agents m2 and m3 only execute their MDPs in the intervals τ ∈ [4, 6] and τ ∈ [4, 10], respectively.",
                "Therefore, an important part of the global decision-making problem is to decide the window of time during which each of the agents is active (i.e., executing its MDP).",
                "To accomplish this, we augment each agents MDP with two new states (start and finish states sb , sf , respectively) and a new start/stop action a∗ , as illustrated in Figure 2.",
                "The idea is that an agent stays in the start state sb until it is ready to execute its MDP, at which point it performs the start/stop action a∗ and transitions into the state space of the original MDP with the transition probability that corresponds to the original initial distribution α(s).",
                "For example, in Figure 1a, for agent m2 this would happen at time τ = 4.",
                "Once the agent gets to the end of its activity window (time τ = 6 for agent m2 in Figure 1a), it performs the start/stop action, which takes it into the sink finish state sf at time τ = 7.",
                "More precisely, given an MDP S, A, pm, rm, αm , we define an augmented MDP S , A , pm, rm, αm as follows: S = S ∪ sb ∪ sf ; A = A ∪ a∗ ; p (s|sb , a∗ ) = α(s), ∀s ∈ S; p (sb |sb , a) = 1.0, ∀a ∈ A; p (sf |s, a∗ ) = 1.0, ∀s ∈ S; p (σ|s, a) = p(σ|s, a), ∀s, σ ∈ S, a ∈ A; r (sb , a) = r (sf , a) = 0, ∀a ∈ A ; r (s, a) = r(s, a), ∀s ∈ S, a ∈ A; α (sb ) = 1; α (s) = 0, ∀s ∈ S; where all non-specified transition probabilities are assumed to be zero.",
                "Further, in order to account for the new starting state, we begin the MDP one time-step earlier, setting τa m ← τa m − 1.",
                "This will not affect the <br>resource</br> allocation due to the <br>resource</br> constraints only being enforced for the original MDP states, as will be discussed in the next section.",
                "For example, the augmented MDPs shown in Figure 2b (which starts in state sb at time τ = 2) would be constructed from an MDP with original arrival time τ = 3.",
                "Figure 2b also shows a sample trajectory through the state space: the agent starts in state sb , transitions into the state space S of the original MDP, and finally exists into the sink state sf .",
                "Note that if we wanted to model a problem where agents could pause their MDPs at arbitrary time steps (which might be useful for domains where dynamic reallocation is possible), we could easily accomplish this by including an extra action that transitions from each state to itself with zero reward. 4.2 MILP for <br>resource</br> Scheduling Given a set of augmented MDPs, as defined above, the goal of this section is to formulate a global optimization program that solves the <br>resource</br>-scheduling problem.",
                "In this section and below, all MDPs are assumed to be the augmented MDPs as defined in Section 4.1.",
                "Our approach is similar to the idea used in [6]: we begin with the linear-program formulation of agents MDPs (1) and augment it with constraints that ensure that the corresponding <br>resource</br> allocation across agents and time is valid.",
                "The resulting optimization problem then simultaneously solves the agents MDPs and <br>resource</br>-scheduling problems.",
                "In the rest of this section, we incrementally develop a mixed integer program (MILP) that achieves this.",
                "In the absence of <br>resource</br> constraints, the agents finitehorizon MDPs are completely independent, and the globally optimal solution can be trivially obtained via the following LP, which is simply an aggregation of single-agent finitehorizon LPs: max X m X s X a rm(s, a) X t xm(s, a, t) subject to: X a xm(σ, a, t + 1) = X s,a pm(σ|s, a)xm(s, a, t), ∀m ∈ M, σ ∈ S, t ∈ [1, Tm − 1]; X a xm(s, a, 1) = αm(s), ∀m ∈ M, s ∈ S; (12) where xm(s, a, t) is the occupation measure of agent m, and The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) 1223 (a) (b) Figure 2: Illustration of augmenting an MDP to allow for variable starting and stopping times: a) (left) the original two-state MDP with a single action; (right) the augmented MDP with new states sb and sf and the new action a∗ (note that the origianl transitions are not changed in the augmentation process); b) the augmented MDP displayed as a trajectory through time (grey lines indicate all transitions, while black lines indicate a given trajectory.",
                "Objective Function (sum of expected rewards over all agents) max X m X s X a rm(s, a) X t xm(s, a, t) (5) Meaning Implication Linear Constraints Tie x to θ.",
                "Agent is only active when occupation measure is nonzero in original MDP states. θm(τ) = 0 =⇒ xm(s, a, τ −τa m+1) = 0 ∀s /∈ {sb , sf }, a ∈ A X s/∈{sb,sf } X a xm(s, a, t) ≤ θm(τa m + t − 1) ∀m ∈ M, ∀t ∈ [1, Tm] (6) Agent can only be active in τ ∈ (τa m, τd m) θm(τ) = 0 ∀m ∈ M, τ /∈ (τa m, τd m) (7) Cannot use resources when not active θm(τ) = 0 =⇒ Δm(τ, ω) = 0 ∀τ ∈ [0, bτ], ω ∈ Ω Δm(τ, ω) ≤ θm(τ) ∀m ∈ M, τ ∈ [0, bτ], ω ∈ Ω (8) Tie x to Δ (nonzero x forces corresponding Δ to be nonzero.)",
                "Δm(τ, ω) = 0, ϕm(a, ω) = 1 =⇒ xm(s, a, τ − τa m + 1) = 0 ∀s /∈ {sb , sf } 1/|A| X a ϕm(a, ω) X s/∈{sb,sf } xm(s, a, t) ≤ Δm(t + τa m − 1, ω) ∀m ∈ M, ω ∈ Ω, t ∈ [1, Tm] (9) <br>resource</br> bounds X m Δm(τ, ω) ≤ bϕ(ω) ∀ω ∈ Ω, τ ∈ [0, bτ] (10) Agent cannot change resources while active.",
                "Only enabled for scheduling with static assignments. θm(τ) = 1 and θm(τ + 1) = 1 =⇒ Δm(τ, ω) = Δm(τ + 1, ω) Δm(τ, ω) − Z(1 − θm(τ + 1)) ≤ Δm(τ + 1, ω) + Z(1 − θm(τ)) Δm(τ, ω) + Z(1 − θm(τ + 1)) ≥ Δm(τ + 1, ω) − Z(1 − θm(τ)) ∀m ∈ M, ω ∈ Ω, τ ∈ [0, bτ] (11) Table 1: MILP for globally optimal <br>resource</br> scheduling.",
                "Tm = τd m − τa m + 1 is the time horizon for the agents MDP.",
                "Using this LP as a basis, we augment it with constraints that ensure that the <br>resource</br> usage implied by the agents occupation measures {xm} does not violate the global <br>resource</br> requirements bϕ at any time step τ ∈ [0, bτ].",
                "To formulate these <br>resource</br> constraints, we use the following binary variables: • Δm(τ, ω) = {0, 1}, ∀m ∈ M, τ ∈ [0, bτ], ω ∈ Ω, which serve as indicator variables that define whether agent m possesses <br>resource</br> ω at time τ.",
                "These are analogous to the static indicator variables used in the one-shot static <br>resource</br>-allocation problem in [6]. • θm = {0, 1}, ∀m ∈ M, τ ∈ [0, bτ] are indicator variables that specify whether agent m is active (i.e., executing its MDP) at time τ.",
                "The meaning of <br>resource</br>-usage variables Δ is illustrated in Figure 1: Δm(τ, ω) = 1 only if <br>resource</br> ω is allocated to agent m at time τ.",
                "The meaning of the activity indicators θ is illustrated in Figure 2b: when agent m is in either the start state sb or the finish state sf , the corresponding θm = 0, but once the agent becomes active and enters one of the other states, we set θm = 1 .",
                "This meaning of θ can be enforced with a linear constraint that synchronizes the values of the agents occupation measures xm and the activity 1224 The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) indicators θ, as shown in (6) in Table 1.",
                "Another constraint we have to add-because the activity indicators θ are defined on the global timeline τ-is to enforce the fact that the agent is inactive outside of its arrivaldeparture window.",
                "This is accomplished by constraint (7) in Table 1.",
                "Furthermore, agents should not be using resources while they are inactive.",
                "This constraint can also be enforced via a linear inequality on θ and Δ, as shown in (8).",
                "Constraint (6) sets the value of θ to match the policy defined by the occupation measure xm.",
                "In a similar fashion, we have to make sure that the <br>resource</br>-usage variables Δ are also synchronized with the occupation measure xm.",
                "This is done via constraint (9) in Table 1, which is nearly identical to the analogous constraint from [6].",
                "After implementing the above constraint, which enforces the meaning of Δ, we add a constraint that ensures that the agents <br>resource</br> usage never exceeds the amounts of available resources.",
                "This condition is also trivially expressed as a linear inequality (10) in Table 1.",
                "Finally, for the problem formulation where <br>resource</br> assignments are static during a lifetime of an agent, we add a constraint that ensures that the <br>resource</br>-usage variables Δ do not change their value while the agent is active (θ = 1).",
                "This is accomplished via the linear constraint (11), where Z ≥ 2 is a constant that is used to turn off the constraints when θm(τ) = 0 or θm(τ + 1) = 0.",
                "This constraint is not used for the dynamic problem formulation, where resources can be reallocated between agents at every time step.",
                "To summarize, Table 1 together with the conservationof-flow constraints from (12) defines the MILP that simultaneously computes an optimal <br>resource</br> assignment for all agents across time as well as optimal finite-horizon MDP policies that are valid under that <br>resource</br> assignment.",
                "As a rough measure of the complexity of this MILP, let us consider the number of optimization variables and constraints.",
                "Let TM = P Tm = P m(τa m − τd m + 1) be the sum of the lengths of the arrival-departure windows across all agents.",
                "Then, the number of optimization variables is: TM + bτ|M||Ω| + bτ|M|, TM of which are continuous (xm), and bτ|M||Ω| + bτ|M| are binary (Δ and θ).",
                "However, notice that all but TM|M| of the θ are set to zero by constraint (7), which also immediately forces all but TM|M||Ω| of the Δ to be zero via the constraints (8).",
                "The number of constraints (not including the degenerate constraints in (7)) in the MILP is: TM + TM|Ω| + bτ|Ω| + bτ|M||Ω|.",
                "Despite the fact that the complexity of the MILP is, in the worst case, exponential1 in the number of binary variables, the complexity of this MILP is significantly (exponentially) lower than that of the MILP with flat utility functions, described in Section 2.2.",
                "This result echos the efficiency gains reported in [6] for single-shot <br>resource</br>-allocation problems, but is much more pronounced, because of the explosion of the flat utility representation due to the temporal aspect of the problem (recall the prohibitive complexity of the combinatorial optimization in Section 2.2).",
                "We empirically analyze the performance of this method in Section 5. 1 Strictly speaking, solving MILPs to optimality is NPcomplete in the number of integer variables. 5.",
                "EXPERIMENTAL RESULTS Although the complexity of solving MILPs is in the worst case exponential in the number of integer variables, there are many efficient methods for solving MILPs that allow our algorithm to scale well for parameters common to <br>resource</br> allocation and scheduling problems.",
                "In particular, this section introduces a problem domain-the repairshop problem-used to empirically evaluate our algorithms scalability in terms of the number of agents |M|, the number of shared resources |Ω|, and the varied lengths of global time bτ during which agents may enter and exit the system.",
                "The repairshop problem is a simple parameterized MDP adopting the metaphor of a vehicular repair shop.",
                "Agents in the repair shop are mechanics with a number of independent tasks that yield reward only when completed.",
                "In our MDP model of this system, actions taken to advance through the state space are only allowed if the agent holds certain resources that are publicly available to the shop.",
                "These resources are in finite supply, and optimal policies for the shop will determine when each agent may hold the limited resources to take actions and earn individual rewards.",
                "Each task to be completed is associated with a single action, although the agent is required to repeat the action numerous times before completing the task and earning a reward.",
                "This model was parameterized in terms of the number of agents in the system, the number of different types of resources that could be linked to necessary actions, a global time during which agents are allowed to arrive and depart, and a maximum length for the number of time steps an agent may remain in the system.",
                "All datapoints in our experiments were obtained with 20 evaluations using CPLEX to solve the MILPs on a Pentium4 computer with 2Gb of RAM.",
                "Trials were conducted on both the static and the dynamic version of the resourcescheduling problem, as defined earlier.",
                "Figure 3 shows the runtime and policy value for independent modifications to the parameter set.",
                "The top row shows how the solution time for the MILP scales as we increase the number of agents |M|, the global time horizon bτ, and the number of resources |Ω|.",
                "Increasing the number of agents leads to exponential complexity scaling, which is to be expected for an NP-complete problem.",
                "However, increasing the global time limit bτ or the total number of <br>resource</br> types |Ω|-while holding the number of agents constantdoes not lead to decreased performance.",
                "This occurs because the problems get easier as they become under-constrained, which is also a common phenomenon for NP-complete problems.",
                "We also observe that the solution to the dynamic version of the problem can often be computed much faster than the static version.",
                "The bottom row of Figure 3 shows the joint policy value of the policies that correspond to the computed optimal <br>resource</br>-allocation schedules.",
                "We can observe that the dynamic version yields higher reward (as expected, since the reward for the dynamic version is always no less than the reward of the static version).",
                "We should point out that these graphs should not be viewed as a measure of performance of two different algorithms (both algorithms produce optimal solutions but to different problems), but rather as observations about how the quality of optimal solutions change as more flexibility is allowed in the reallocation of resources.",
                "Figure 4 shows runtime and policy value for trials in which common input variables are scaled together.",
                "This allows The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) 1225 2 4 6 8 10 10 −3 10 −2 10 −1 10 0 10 1 10 2 10 3 10 4 Number of Agents |M| CPUTime,sec |Ω| = 5, τ = 50 static dynamic 50 100 150 200 10 −2 10 −1 10 0 10 1 10 2 10 3 Global Time Boundary τ CPUTime,sec |M| = 5, |Ω| = 5 static dynamic 10 20 30 40 50 10 −2 10 −1 10 0 10 1 10 2 Number of Resources |Ω| CPUTime,sec |M| = 5, τ = 50 static dynamic 2 4 6 8 10 200 400 600 800 1000 1200 1400 1600 Number of Agents |M| Value |Ω| = 5, τ = 50 static dynamic 50 100 150 200 400 500 600 700 800 900 1000 1100 1200 1300 1400 Global Time Boundary τ Value |M| = 5, |Ω| = 5 static dynamic 10 20 30 40 50 500 600 700 800 900 1000 1100 1200 1300 1400 Number of Resources |Ω| Value |M| = 5, τ = 50 static dynamic Figure 3: Evaluation of our MILP for variable numbers of agents (column 1), lengths of global-time window (column 2), and numbers of <br>resource</br> types (column 3).",
                "Top row shows CPU time, and bottom row shows the joint reward of agents MDP policies.",
                "Error bars show the 1st and 3rd quartiles (25% and 75%). 2 4 6 8 10 10 −3 10 −2 10 −1 10 0 10 1 10 2 10 3 Number of Agents |M| CPUTime,sec τ = 10|M| static dynamic 2 4 6 8 10 10 −3 10 −2 10 −1 10 0 10 1 10 2 10 3 10 4 Number of Agents |M| CPUTime,sec |Ω| = 2|M| static dynamic 2 4 6 8 10 10 −3 10 −2 10 −1 10 0 10 1 10 2 10 3 10 4 Number of Agents |M| CPUTime,sec |Ω| = 5|M| static dynamic 2 4 6 8 10 200 400 600 800 1000 1200 1400 1600 1800 2000 2200 Number of Agents |M| Value τ = 10|M| static dynamic 2 4 6 8 10 200 400 600 800 1000 1200 1400 1600 1800 2000 Number of Agents |M| Value |Ω| = 2|M| static dynamic 2 4 6 8 10 0 500 1000 1500 2000 2500 Number of Agents |M| Value |Ω| = 5|M| static dynamic Figure 4: Evaluation of our MILP using correlated input variables.",
                "The left column tracks the performance and CPU time as the number of agents and global-time window increase together (bτ = 10|M|).",
                "The middle and the right column track the performance and CPU time as the number of resources and the number of agents increase together as |Ω| = 2|M| and |Ω| = 5|M|, respectively.",
                "Error bars show the 1st and 3rd quartiles (25% and 75%). 1226 The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) us to explore domains where the total number of agents scales proportionally to the total number of <br>resource</br> types or the global time horizon, while keeping constant the average agent density (per unit of global time) or the average number of resources per agent (which commonly occurs in real-life applications).",
                "Overall, we believe that these experimental results indicate that our MILP formulation can be used to effectively solve <br>resource</br>-scheduling problems of nontrivial size. 6.",
                "DISCUSSION AND CONCLUSIONS Throughout the paper, we have made a number of assumptions in our model and solution algorithm; we discuss their implications below. • Continual execution.",
                "We assume that once an agent stops executing its MDP (transitions into state sf ), it exits the system and cannot return.",
                "It is easy to relax this assumption for domains where agents MDPs can be paused and restarted.",
                "All that is required is to include an additional pause action which transitions from a given state back to itself, and has zero reward. • Indifference to start time.",
                "We used a reward model where agents rewards depend only on the time horizon of their MDPs and not the global start time.",
                "This is a consequence of our MDP-augmentation procedure from Section 4.1.",
                "It is easy to extend the model so that the agents incur an explicit penalty for idling by assigning a non-zero negative reward to the start state sb . • Binary <br>resource</br> requirements.",
                "For simplicity, we have assumed that <br>resource</br> costs are binary: ϕm(a, ω) = {0, 1}, but our results generalize in a straightforward manner to non-binary <br>resource</br> mappings, analogously to the procedure used in [5]. • Cooperative agents.",
                "The optimization procedure discussed in this paper was developed in the context of cooperative agents, but it can also be used to design a mechanism for scheduling resources among self-interested agents.",
                "This optimization procedure can be embedded in a VickreyClarke-Groves auction, completely analogously to the way it was done in [7].",
                "In fact, all the results of [7] about the properties of the auction and information privacy directly carry over to the scheduling domain discussed in this paper, requiring only slight modifications to deal with finitehorizon MDPs. • Known, deterministic arrival and departure times.",
                "Finally, we have assumed that agents arrival and departure times (τa m and τd m) are deterministic and known a priori.",
                "This assumption is fundamental to our solution method.",
                "While there are many domains where this assumption is valid, in many cases agents arrive and depart dynamically and their arrival and departure times can only be predicted probabilistically, leading to online <br>resource</br>-allocation problems.",
                "In particular, in the case of self-interested agents, this becomes an interesting version of an online-mechanism-design problem [11, 12].",
                "In summary, we have presented an MILP formulation for the combinatorial <br>resource</br>-scheduling problem where agents values for possible <br>resource</br> assignments are defined by finitehorizon MDPs.",
                "This result extends previous work ([6, 7]) on static one-shot <br>resource</br> allocation under MDP-induced preferences to <br>resource</br>-scheduling problems with a temporal aspect.",
                "As such, this work takes a step in the direction of designing an online mechanism for agents with combinatorial <br>resource</br> preferences induced by stochastic planning problems.",
                "Relaxing the assumption about deterministic arrival and departure times of the agents is a focus of our future work.",
                "We would like to thank the anonymous reviewers for their insightful comments and suggestions. 7.",
                "REFERENCES [1] E. Altman and A. Shwartz.",
                "Adaptive control of constrained Markov chains: Criteria and policies.",
                "Annals of Operations Research, special issue on Markov Decision Processes, 28:101-134, 1991. [2] R. Bellman.",
                "Dynamic Programming.",
                "Princeton University Press, 1957. [3] C. Boutilier.",
                "Solving concisely expressed combinatorial auction problems.",
                "In Proc. of AAAI-02, pages 359-366, 2002. [4] C. Boutilier and H. H. Hoos.",
                "Bidding languages for combinatorial auctions.",
                "In Proc. of IJCAI-01, pages 1211-1217, 2001. [5] D. Dolgov.",
                "Integrated <br>resource</br> Allocation and Planning in Stochastic Multiagent Environments.",
                "PhD thesis, Computer Science Department, University of Michigan, February 2006. [6] D. A. Dolgov and E. H. Durfee.",
                "Optimal <br>resource</br> allocation and policy formulation in loosely-coupled Markov decision processes.",
                "In Proc. of ICAPS-04, pages 315-324, June 2004. [7] D. A. Dolgov and E. H. Durfee.",
                "Computationally efficient combinatorial auctions for <br>resource</br> allocation in weakly-coupled MDPs.",
                "In Proc. of AAMAS-05, New York, NY, USA, 2005.",
                "ACM Press. [8] D. A. Dolgov and E. H. Durfee.",
                "<br>resource</br> allocation among agents with preferences induced by factored MDPs.",
                "In Proc. of AAMAS-06, 2006. [9] K. Larson and T. Sandholm.",
                "Mechanism design and deliberative agents.",
                "In Proc. of AAMAS-05, pages 650-656, New York, NY, USA, 2005.",
                "ACM Press. [10] N. Nisan.",
                "Bidding and allocation in combinatorial auctions.",
                "In Electronic Commerce, 2000. [11] D. C. Parkes and S. Singh.",
                "An MDP-based approach to Online Mechanism Design.",
                "In Proc. of the Seventeenths Annual Conference on Neural Information Processing Systems (NIPS-03), 2003. [12] D. C. Parkes, S. Singh, and D. Yanovsky.",
                "Approximately efficient online mechanism design.",
                "In Proc. of the Eighteenths Annual Conference on Neural Information Processing Systems (NIPS-04), 2004. [13] M. L. Puterman.",
                "Markov Decision Processes.",
                "John Wiley & Sons, New York, 1994. [14] M. H. Rothkopf, A. Pekec, and R. M. Harstad.",
                "Computationally manageable combinational auctions.",
                "Management Science, 44(8):1131-1147, 1998. [15] T. Sandholm.",
                "An algorithm for optimal winner determination in combinatorial auctions.",
                "In Proc. of IJCAI-99, pages 542-547, San Francisco, CA, USA, 1999.",
                "Morgan Kaufmann Publishers Inc.",
                "The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) 1227"
            ],
            "original_annotated_samples": [],
            "translated_annotated_samples": [
                "Programación de \"recursos\" combinatorios para MDP múltiples Dmitri A. Dolgov, Michael R. James y Michael E. Research Technical Investigación técnica del Grupo de Robotics, Centro Técnico Toyota USA {Ddolgov, Michael.R.James, Michael.samples}@gmail.La programación óptima de \"recursos\" de resumen en sistemas múltiples es una tarea computacionalmente desafiante, particularmente cuando los valores de los recursos no son aditivos.recurso",
                "En los últimos años, se han desarrollado algoritmos eficientes de asignación de \"recursos\" para agentes con valores de \"recursos\" inducidos por los MDP.recurso",
                "Sin embargo, este trabajo anterior se ha centrado en los problemas de asignación de \"recursos\" estáticos donde los recursos se distribuyen una vez y luego se utilizan en MDP de Horizon Infinite.recurso",
                "Extendemos esos modelos existentes al problema de la programación combinatoria de \"recursos\", donde los agentes persisten solo por períodos finitos entre sus tiempos de llegada y salida (predefinidas), que requieren recursos solo para esos períodos de tiempo.recurso",
                "Proporcionamos un procedimiento computacionalmente eficiente para calcular las tareas de \"recursos\" óptimas a nivel mundial a los agentes a lo largo del tiempo.recurso",
                "Introducción Las tareas de asignación y programación óptimas de \"recursos\" son ubicuas en sistemas multiagentes, pero resolver tales problemas de optimización puede ser computacionalmente difícil, debido a una serie de factores.recurso",
                "En particular, cuando el valor de un conjunto de recursos a un agente no es aditivo (como suele ser el caso con los recursos que son sustitutos o complementos), la función de utilidad podría deberse definirse en un espacio exponencialmente grande de \"recursos\"., que rápidamente se vuelve computacionalmente intratable.recurso",
                "Además, incluso cuando cada agente tiene una función de utilidad que no es cero solo en un pequeño subconjunto de los posibles paquetes de \"recursos\", obtener una asignación óptima aún es computacionalmente prohibitiva, a medida que el problema se convierte en NP-complete [14].recurso",
                "Una alternativa es modelar directamente los mecanismos que definen las funciones de utilidad de los agentes y realizar la asignación de \"recursos\" directamente con estos modelos [9].recurso",
                "Esta representación se puede usar para construir algoritmos de asignación de \"recursos\" muy eficientes que conducen a una aceleración exponencial sobre un problema de optimización sencilla con representaciones planas de preferencias combinatorias [6, 7, 8].recurso",
                "Sin embargo, este trabajo existente sobre la asignación de \"recursos\" con preferencias inducidas por los MDP parametrizados por \"recursos\" supone que los recursos solo se asignan una vez y luego son utilizados por los agentes de forma independiente dentro de sus MDP de Horizon Infinite.recurso",
                "En este documento, ampliamos el trabajo sobre la asignación de \"recursos\" bajo las preferencias inducidas por MDP a problemas de programación de tiempo discreto, donde los agentes están presentes en el sistema durante intervalos de tiempo finitos y solo pueden usar recursos dentro de estos intervalos.recurso",
                "Abordamos el problema de la programación de \"recursos\" globalmente óptimo, donde el objetivo es encontrar una asignación de recursos a los agentes a lo largo del tiempo que maximiza la suma de las recompensas esperadas que obtienen.recurso",
                "En este contexto, nuestra contribución principal es una formulación de programación de información mixta del problema de programación que elige tareas de \"recursos\" óptimas globalmente óptimas, tiempos de partida y horizontes de ejecución para todos) c 2007 Intervalos de salida de Ifaamas).recurso",
                "Analizamos y comparamos empíricamente dos sabores del problema de programación: uno, donde los agentes tienen asignaciones estáticas de \"recursos\" dentro de sus MDP de horizones finitos, y otro, donde los recursos pueden reasignarse dinámicamente entre los agentes en cada paso de tiempo.recurso",
                "En la Sección 4.2, describimos nuestro resultado principal, el programa de optimización para la programación globalmente óptima de \"recursos\".recurso",
                "También describimos los métodos estándar para la programación combinatoria de \"recursos\" con valores planos de \"recursos\", que sirven como un punto de referencia de comparación para el nuevo modelo desarrollado aquí.2.1 Procesos de decisión de Markov Un MDP estacionario, de dominio finito y en tiempo discreto (ver, por ejemplo, [13] para un desarrollo exhaustivo y detallado) puede describirse como S, A, P, R, donde: S es un conjunto finitode estados del sistema;A es un conjunto finito de acciones que están disponibles para el agente;P es una función de transición estocástica estacionaria, donde p (σ | s, a) es la probabilidad de transición al estado σ al ejecutar la acción A en el estado s;r es una función de recompensa estacionaria, donde R (s, a) especifica la recompensa obtenida al ejecutar la acción A en el estado s.Dado dicho MDP, un problema de decisión bajo un horizonte finito T es elegir una acción óptima en cada paso de tiempo para maximizar el valor esperado de la recompensa total acumulada durante la vida de los agentes (finitos).recurso",
                "Sin embargo, para los MDP con restricciones de \"recursos\" (como se define a continuación en la Sección 3), las políticas uniformemente óptimas no existen en general.recurso",
                "Este resultado es bien conocido por los MDP de horizonte infinito con varios tipos de restricciones [1, 6], y también es válida para nuestro modelo de horizonte finito, que puede establecerse fácilmente a través de una línea de razonamiento completamente análoga a los argumentos en [6]2.2 \"Recursos\" combinatorios La programación de un enfoque directo de la programación de \"recursos\" para un conjunto de agentes M, cuyos valores para los recursos son inducidos por problemas de planificación estocástica (en nuestro caso, MDP de hornosposibles asignaciones de recursos a lo largo del tiempo y, para cada uno, calcule su valor resolviendo el MDP correspondiente.recurso",
                "Luego, cada agente proporcionaría valoraciones para cada posible conjunto de \"recursos\" con el tiempo a un coordinador centralizado, que calcularía las tareas óptimas de \"recursos\" a lo largo del tiempo en función de estas valoraciones.recurso",
                "Conf.Sobre los agentes autónomos y los sistemas de múltiples agentes (AAMAS 07) 1221 Para el problema de programación donde los agentes tienen requisitos estáticos de \"recursos\" dentro de sus MDP finitos de orificio, los agentes proporcionan una valoración para cada paquete de \"recursos\" para cada horizonte de tiempo posible (desde[1, tm]) que pueden usar.recurso",
                "Un agente obtendrá como máximo un paquete de \"recursos\" para uno de los horizontes temporales.recurso",
                "Deje que la variable ψ ∈ ψm enumere todos los pares posibles de paquetes de \"recursos\" y horizontes temporales para el agente M, por lo que hay 2 | Ω |× valores TM para ψ (el espacio de los paquetes es exponencial en el número de tipos de \"recursos\" | ω |).recurso",
                "El agente m debe proporcionar un valor vψ m para cada ψ, y el coordinador asignará como máximo un par ψ (\"recurso\", horizonte de tiempo) a cada agente.recurso",
                "Esta asignación se expresa como una variable indicadora zψ m ∈ {0, 1} que muestra si ψ se asigna al agente m.Para el tiempo τ y el \"recurso\" ω, la función nm (ψ, τ, ω) ∈ {0, 1} indica si el paquete en ψ usa \"recursos\" Ω en el tiempo τ (hacemos que los agentes tienen requisitos de recursos binarios).recurso",
                "El problema de encontrar una asignación óptima que satisfaga la restricción global de que la cantidad de cada \"recurso\" Ω asignado a todos los agentes no excede la cantidad disponible Bϕ (Ω) puede expresarse como el siguiente programa entero: max x m∈M x ψ∈ψm zψ mvψ m sujeto a: x ψ∈ no Zψ m ≤ 1, ∀m ∈ M;X m∈M x ψ∈ no Zψ mnm (ψ, τ, Ω) ≤ bϕ (ω), ∀τ ∈ [1, bτ], ∀Ω ∈ ω;(3) La primera restricción en la ecuación 3 dice que ningún agente puede recibir más de un paquete, y la segunda restricción asegura que la asignación total de \"recurso\" ω no exceda, en ningún momento, excede los recursos.recurso",
                "Deje que la variable ψ ∈ ψm en este caso enumere todos los paquetes de \"recursos\" posibles para los cuales se puede asignar un paquete al agente M en cada paso de tiempo.recurso",
                "Por lo tanto, en este caso hay p t∈ [1, tm] (2 | ω |) t ∼ 2 | ω | tm posibilidades de paquetes de \"recursos\" asignados a diferentes ranuras de tiempo, para los tm diferentes horizontes de tiempo.recurso",
                "La entrada del problema consiste en los siguientes componentes: • M, ω, Bϕ, τa m, τd m, bτ son como se definió anteriormente en la Sección 2.2.• {θm} = {s, a, pm, rm, αm} son los MDP de todos los agentes m ∈ M. Sin pérdida de generalidad, suponemos que los espacios de estado y de acción de todos los agentes son los mismos, pero cada uno tiene su propiofunción de transición PM, función de recompensa RM y condiciones iniciales αm.• ϕM: A × Ω → {0, 1} es la asignación de acciones a los recursos para el agente m.ϕm (a, ω) indica si la acción A del agente M necesita \"recurso\" Ω.recurso",
                "Un agente M que recibe un conjunto de recursos que no incluye \"recurso\" Ω no puede ejecutar en su política MDP cualquier acción A para la cual ϕm (a, ω) = 0. recursos",
                "Asumimos que todos los requisitos de \"recursos\" son binarios;Como se discute a continuación en la Sección 6, esta suposición no es limitante.recurso",
                "Es factible una solución si la asignación correspondiente de recursos a los agentes no viola la restricción global de \"recurso\": x m Δm (τ, ω) ≤ bϕ (ω), ∀Ω ∈ ω, τ ∈ [1, bτ].(4) Consideramos dos sabores del problema de reducción de \"recursos\".recurso",
                "La primera formulación restringe las asignaciones de \"recursos\" al espacio donde la asignación de recursos a cada agente es estática durante la vida de los agentes.recurso",
                "La Figura 1 muestra un problema de reducción de \"recursos\" con tres agentes M = {M1, M2, M3}, tres recursos ω = {ω1, ω2, ω3} y un horizonte de problemas global de Bτ = 11. Resource",
                "Se muestra una solución a este problema a través de barras horizontales dentro de cada caja de agentes, donde las barras corresponden a la asignación de los tres tipos de \"recursos\".recurso",
                "El agente M3 contiene \"recurso\" ω3 durante el intervalo τ ∈ [4, 10].recurso",
                "Por ejemplo, el agente M1 da el uso de \"recurso\" ω2 en el momento τ = 2, aunque continúa la ejecución de su MDP hasta el tiempo τ = 6. recurso",
                "Conf.en agentes autónomos y sistemas de múltiples agentes (AAMAS 07) (a) (b) Figura 1: Ilustración de una solución a un problema de reducción de \"recursos\" con tres agentes y tres recursos: A) asignaciones estáticas de \"recursos\" (tareas de recursos (asignaciones de recursosson constantes dentro de la vida de los agentes; b) asignación dinámica (las asignaciones de recursos pueden cambiar en cada paso de tiempo).4. Recurso",
                "La programación de \"recursos\" de nuestro algoritmo de scheduling \"recursos\" se realiza en dos etapas.recurso",
                "Esto no afectará la asignación de \"recursos\" debido a que las limitaciones de \"recursos\" solo se aplican para los estados originales de MDP, como se discutirá en la siguiente sección.recurso",
                "Tenga en cuenta que si quisiéramos modelar un problema en el que los agentes pudieran detener sus MDP en pasos de tiempo arbitrarios (lo que podría ser útil para dominios donde es posible la reasignación dinámica), podríamos lograrlo fácilmente al incluir una acción adicional que pasa de cada estado a sí mismo a sí mismo.con cero recompensa.4.2 MILP Para la programación de \"recursos\" dado un conjunto de MDP aumentados, como se definió anteriormente, el objetivo de esta sección es formular un programa de optimización global que resuelva el problema de reducción de \"recursos\".recurso",
                "Nuestro enfoque es similar a la idea utilizada en [6]: comenzamos con la formulación del programa lineal de los MDP de los agentes (1) y la aumentamos con limitaciones que aseguran que la asignación de \"recursos\" correspondiente entre los agentes y el tiempo sea válida.recurso",
                "El problema de optimización resultante luego resuelve simultáneamente los MDP de los agentes y los problemas de reducción de \"recursos\".recurso",
                "En ausencia de limitaciones de \"recursos\", los agentes Finitehorizon MDPS son completamente independientes, y la solución globalmente óptima se puede obtener de manera trivial a través del siguiente LP, que es simplemente una agregación de LPS de Horizon de agente único: Max X M x S x A.rm (s, a) x t xm (s, a, t) sujeto a: x a xm (σ, a, t + 1) = x s, a pm (σ | s, a) xm (s, a,t), ∀m ∈ M, σ ∈ S, t ∈ [1, tm - 1];X a xm (s, a, 1) = αm (s), ∀m ∈ M, s ∈ S;(12) donde xm (s, a, t) es la medida de ocupación del agente m, y el sexto intl.recurso",
                "Δm (τ, ω) = 0, ϕm (a, ω) = 1 = ⇒ xm (s, a, τ - τa m + 1) = 0 ∀s /∈ {sb, sf} 1 /| a |X a ϕm (a, ω) x s/∈ {sb, sf} xm (s, a, t) ≤ Δm (t + τa m - 1, ω) ∀m ∈ M, ω ∈ ω, t ∈ [1, Tm] (9) límites de \"recurso\" x m Δm (τ, ω) ≤ bϕ (ω) ∀ω ∈ ω, τ ∈ [0, bτ] (10) El agente no puede cambiar los recursos mientras está activo.recurso",
                "Solo habilitado para programar con tareas estáticas.θm (τ) = 1 y θm (τ + 1) = 1 = ⇒ ΔM (τ, ω) = ΔM (τ + 1, ω) ΔM (τ, ω) - z (1 - θm (τ + 1))≤ Δm (τ + 1, Ω) + z (1 - θm (τ)) Δm (τ, ω) + z (1 - θm (τ + 1)) ≥ ΔM (τ + 1, ω) - z (1- θm (τ)) ∀m ∈ M, Ω ∈ ω, τ ∈ [0, Bτ] (11) Tabla 1: MILP para la programación de \"recursos\" óptimos globalmente óptimos.recurso",
                "Utilizando este LP como base, lo aumentamos con restricciones que aseguran que el uso de \"recursos\" implícito por las medidas de ocupación de los agentes {xm} no viole los requisitos globales de \"recursos\" Bϕ en ningún paso de tiempo τ ∈ [0, bτ].recurso",
                "Para formular estas restricciones \"recursos\", usamos las siguientes variables binarias: • ΔM (τ, ω) = {0, 1}, ∀m ∈ M, τ ∈ [0, bτ], ω ∈ ω, que sirve como indicadorVariables que definen si el agente M posee \"recursos\" Ω en el momento τ.recurso",
                "Estas son análogas a las variables indicadoras estáticas utilizadas en el problema de asignación de \"recurso\" estático de una sola vez en [6].• θm = {0, 1}, ∀m ∈ M, τ ∈ [0, bτ] son variables indicadoras que especifican si el agente m está activo (es decir, ejecutar su MDP) en el momento τ.recurso",
                "El significado de las variables de uso de \"recurso\" δ se ilustra en la Figura 1: Δm (τ, ω) = 1 Solo si el \"recurso\" ω se asigna al agente M en el tiempo τ.recurso",
                "De manera similar, debemos asegurarnos de que las variables de uso de \"recurso\" δ también se sincronizen con la medida de ocupación XM.recurso",
                "Después de implementar la restricción anterior, que hace cumplir el significado de δ, agregamos una restricción que garantiza que el uso de \"recursos\" de los agentes nunca exceda las cantidades de recursos disponibles.recurso",
                "Finalmente, para la formulación del problema donde las asignaciones de \"recursos\" son estáticas durante la vida útil de un agente, agregamos una restricción que asegura que las variables de uso de \"recurso\" δ no cambien su valor mientras el agente está activo (θ = 1).recurso",
                "Para resumir, la Tabla 1 junto con la conservación de las restricciones de flujo de (12) define el MILP que calcula simultáneamente una asignación óptima de \"recursos\" para todos los agentes a lo largo del tiempo, así como las políticas MDP de Horizon finitas óptimas que son válidas bajo ese \"recurso\"asignación.recurso",
                "Este resultado hace eco de las ganancias de eficiencia informadas en [6] para problemas de \"recursos\" de disparo único, pero es mucho más pronunciado, debido a la explosión de la representación de utilidad plana debido al aspecto temporal del problema (recuerde la complejidad prohibitivade la optimización combinatoria en la Sección 2.2).recurso",
                "Resultados experimentales Aunque la complejidad de resolver MILP es en el peor de los casos exponencial en el número de variables enteras, existen muchos métodos eficientes para resolver MILP que permiten que nuestro algoritmo escala bien para los parámetros comunes a los problemas de asignación y programación de \"recursos\".recurso",
                "Sin embargo, aumentando el límite de tiempo global Bτ o el número total de tipos de \"recursos\" | ω |-mientras mantiene el número de agentes constantes no conducen a una disminución del rendimiento.recurso",
                "La fila inferior de la Figura 3 muestra el valor de la política conjunta de las políticas que corresponden a los horarios de asignación óptimos calculados calculados.recurso",
                "Conf.En agentes autónomos y sistemas de múltiples agentes (AAMAS 07) 1225 2 4 6 8 10 10 −3 10 −2 10 −1 10 0 10 10 1 10 2 10 3 10 4 Número de agentes | M |Cputime, sec | ω |= 5, τ = 50 dinámica estática 50 100 150 200 10 −2 10 −1 10 0 10 10 1 10 2 10 3 Límite de tiempo global τ CPutime, Sec | M |= 5, | Ω |= 5 dinámica estática 10 20 30 40 50 10 −2 10 −1 10 0 10 1 10 2 Número de recursos | Ω |Cputime, sec | m |= 5, τ = 50 dinámica estática 2 4 6 8 10 200 400 600 800 1000 1200 1400 1600 Número de agentes | M |Valor | Ω |= 5, τ = 50 dinámica estática 50 100 150 200 400 500 600 700 800 900 1000 1100 1200 1300 1400 Tiempo global Límite τ Valor | M |= 5, | Ω |= 5 dinámica estática 10 20 30 40 50 500 600 700 800 900 1000 1100 1200 1300 1400 Número de recursos | Ω |Valor | M |= 5, τ = 50 dinámica estática Figura 3: Evaluación de nuestro MILP para números variables de agentes (columna 1), longitudes de la ventana de tiempo global (columna 2) y números de tipos de \"recursos\" (columna 3).recurso",
                "Conf.En agentes autónomos y sistemas de múltiples agentes (AAMAS 07) para explorar dominios donde el número total de agentes escala proporcionalmente al número total de tipos de \"recursos\" o el horizonte de tiempo global, mientras mantiene constante la densidad de agente promedio (por unidad deTiempo global) o el número promedio de recursos por agente (que se produce comúnmente en aplicaciones de la vida real).recurso",
                "En general, creemos que estos resultados experimentales indican que nuestra formulación MILP puede usarse para resolver efectivamente los problemas de tamaño no trivial de \"recursos\".6. Recurso",
                "Es fácil extender el modelo para que los agentes incurran en una penalización explícita por ralentí al asignar una recompensa negativa no cero al estado de inicio SB.• Requisitos binarios de \"recursos\".recurso",
                "Para simplificar, hemos asumido que los costos de \"recursos\" son binarios: ϕm (a, ω) = {0, 1}, pero nuestros resultados se generalizan de manera directa a las asignaciones de \"recursos\" no binarios, análoga al procedimiento utilizado en[5].• Agentes cooperativos.recurso",
                "Si bien hay muchos dominios en los que esta suposición es válida, en muchos casos los agentes llegan y salen dinámicamente y sus tiempos de llegada y salida solo pueden predecirse probabilísticamente, lo que lleva a problemas de asignación de \"recursos\" en línea.recurso",
                "En resumen, hemos presentado una formulación de MILP para el problema combinatorio de \"recursos\", donde los valores de los agentes para las posibles asignaciones de \"recursos\" están definidos por los MDP de Horizon de Finitehorizon.recurso",
                "Este resultado extiende el trabajo anterior ([6, 7]) sobre la asignación estática de \"recursos\" de un solo disparo bajo las preferencias inducidas por MDP a los problemas de reducción de \"recursos\" con un aspecto temporal.recurso",
                "Como tal, este trabajo da un paso en la dirección de diseñar un mecanismo en línea para agentes con preferencias combinatorias de \"recursos\" inducidas por problemas de planificación estocástica.recurso",
                "Asignación y planificación integradas de \"recursos\" en entornos multiagentes estocásticos.recurso",
                "Asignación óptima de \"recursos\" y formulación de políticas en procesos de decisión de Markov libremente acoplados.recurso",
                "Subastas combinatorias computacionalmente eficientes para la asignación de \"recursos\" en MDP débilmente acoplados.recurso",
                "Asignación de \"recursos\" entre los agentes con preferencias inducidas por los MDP factorizados.recurso"
            ],
            "translated_text": "",
            "candidates": [],
            "error": []
        },
        "markov decision process": {
            "translated_key": "",
            "is_in_text": true,
            "original_annotated_sentences": [
                "Combinatorial Resource Scheduling for Multiagent MDPs Dmitri A. Dolgov, Michael R. James, and Michael E. Samples AI and Robotics Group Technical Research, Toyota Technical Center USA {ddolgov, michael.r.james, michael.samples}@gmail.com ABSTRACT Optimal resource scheduling in multiagent systems is a computationally challenging task, particularly when the values of resources are not additive.",
                "We consider the combinatorial problem of scheduling the usage of multiple resources among agents that operate in stochastic environments, modeled as Markov decision processes (MDPs).",
                "In recent years, efficient resource-allocation algorithms have been developed for agents with resource values induced by MDPs.",
                "However, this prior work has focused on static resource-allocation problems where resources are distributed once and then utilized in infinite-horizon MDPs.",
                "We extend those existing models to the problem of combinatorial resource scheduling, where agents persist only for finite periods between their (predefined) arrival and departure times, requiring resources only for those time periods.",
                "We provide a computationally efficient procedure for computing globally optimal resource assignments to agents over time.",
                "We illustrate and empirically analyze the method in the context of a stochastic jobscheduling domain.",
                "Categories and Subject Descriptors I.2.8 [Artificial Intelligence]: Problem Solving, Control Methods, and Search; I.2.11 [Artificial Intelligence]: Distributed Artificial Intelligence-Multiagent systems General Terms Algorithms, Performance, Design 1.",
                "INTRODUCTION The tasks of optimal resource allocation and scheduling are ubiquitous in multiagent systems, but solving such optimization problems can be computationally difficult, due to a number of factors.",
                "In particular, when the value of a set of resources to an agent is not additive (as is often the case with resources that are substitutes or complements), the utility function might have to be defined on an exponentially large space of resource bundles, which very quickly becomes computationally intractable.",
                "Further, even when each agent has a utility function that is nonzero only on a small subset of the possible resource bundles, obtaining optimal allocation is still computationally prohibitive, as the problem becomes NP-complete [14].",
                "Such computational issues have recently spawned several threads of work in using compact models of agents preferences.",
                "One idea is to use any structure present in utility functions to represent them compactly, via, for example, logical formulas [15, 10, 4, 3].",
                "An alternative is to directly model the mechanisms that define the agents utility functions and perform resource allocation directly with these models [9].",
                "A way of accomplishing this is to model the processes by which an agent might utilize the resources and define the utility function as the payoff of these processes.",
                "In particular, if an agent uses resources to act in a stochastic environment, its utility function can be naturally modeled with a <br>markov decision process</br>, whose action set is parameterized by the available resources.",
                "This representation can then be used to construct very efficient resource-allocation algorithms that lead to an exponential speedup over a straightforward optimization problem with flat representations of combinatorial preferences [6, 7, 8].",
                "However, this existing work on resource allocation with preferences induced by resource-parameterized MDPs makes an assumption that the resources are only allocated once and are then utilized by the agents independently within their infinite-horizon MDPs.",
                "This assumption that no reallocation of resources is possible can be limiting in domains where agents arrive and depart dynamically.",
                "In this paper, we extend the work on resource allocation under MDP-induced preferences to discrete-time scheduling problems, where agents are present in the system for finite time intervals and can only use resources within these intervals.",
                "In particular, agents arrive and depart at arbitrary (predefined) times and within these intervals use resources to execute tasks in finite-horizon MDPs.",
                "We address the problem of globally optimal resource scheduling, where the objective is to find an allocation of resources to the agents across time that maximizes the sum of the expected rewards that they obtain.",
                "In this context, our main contribution is a mixed-integerprogramming formulation of the scheduling problem that chooses globally optimal resource assignments, starting times, and execution horizons for all agents (within their arrival1220 978-81-904262-7-5 (RPS) c 2007 IFAAMAS departure intervals).",
                "We analyze and empirically compare two flavors of the scheduling problem: one, where agents have static resource assignments within their finite-horizon MDPs, and another, where resources can be dynamically reallocated between agents at every time step.",
                "In the rest of the paper, we first lay down the necessary groundwork in Section 2 and then introduce our model and formal problem statement in Section 3.",
                "In Section 4.2, we describe our main result, the optimization program for globally optimal resource scheduling.",
                "Following the discussion of our experimental results on a job-scheduling problem in Section 5, we conclude in Section 6 with a discussion of possible extensions and generalizations of our method. 2.",
                "BACKGROUND Similarly to the model used in previous work on resourceallocation with MDP-induced preferences [6, 7], we define the value of a set of resources to an agent as the value of the best MDP policy that is realizable, given those resources.",
                "However, since the focus of our work is on scheduling problems, and a large part of the optimization problem is to decide how resources are allocated in time among agents with finite arrival and departure times, we model the agents planning problems as finite-horizon MDPs, in contrast to previous work that used infinite-horizon discounted MDPs.",
                "In the rest of this section, we first introduce some necessary background on finite-horizon MDPs and present a linear-programming formulation that serves as the basis for our solution algorithm developed in Section 4.",
                "We also outline the standard methods for combinatorial resource scheduling with flat resource values, which serve as a comparison benchmark for the new model developed here. 2.1 Markov Decision Processes A stationary, finite-domain, discrete-time MDP (see, for example, [13] for a thorough and detailed development) can be described as S, A, p, r , where: S is a finite set of system states; A is a finite set of actions that are available to the agent; p is a stationary stochastic transition function, where p(σ|s, a) is the probability of transitioning to state σ upon executing action a in state s; r is a stationary reward function, where r(s, a) specifies the reward obtained upon executing action a in state s. Given such an MDP, a decision problem under a finite horizon T is to choose an optimal action at every time step to maximize the expected value of the total reward accrued during the agents (finite) lifetime.",
                "The agents optimal policy is then a function of current state s and the time until the horizon.",
                "An optimal policy for such a problem is to act greedily with respect to the optimal value function, defined recursively by the following system of finite-time Bellman equations [2]: v(s, t) = max a r(s, a) + X σ p(σ|s, a)v(σ, t + 1), ∀s ∈ S, t ∈ [1, T − 1]; v(s, T) = 0, ∀s ∈ S; where v(s, t) is the optimal value of being in state s at time t ∈ [1, T].",
                "This optimal value function can be easily computed using dynamic programming, leading to the following optimal policy π, where π(s, a, t) is the probability of executing action a in state s at time t: π(s, a, t) = ( 1, a = argmaxa r(s, a) + P σ p(σ|s, a)v(σ, t + 1), 0, otherwise.",
                "The above is the most common way of computing the optimal value function (and therefore an optimal policy) for a finite-horizon MDP.",
                "However, we can also formulate the problem as the following linear program (similarly to the dual LP for infinite-horizon discounted MDPs [13, 6, 7]): max X s X a r(s, a) X t x(s, a, t) subject to: X a x(σ, a, t + 1) = X s,a p(σ|s, a)x(s, a, t) ∀σ, t ∈ [1, T − 1]; X a x(s, a, 1) = α(s), ∀s ∈ S; (1) where α(s) is the initial distribution over the state space, and x is the (non-stationary) occupation measure (x(s, a, t) ∈ [0, 1] is the total expected number of times action a is executed in state s at time t).",
                "An optimal (non-stationary) policy is obtained from the occupation measure as follows: π(s, a, t) = x(s, a, t)/ X a x(s, a, t) ∀s ∈ S, t ∈ [1, T]. (2) Note that the standard unconstrained finite-horizon MDP, as described above, always has a uniformly-optimal solution (optimal for any initial distribution α(s)).",
                "Therefore, an optimal policy can be obtained by using an arbitrary constant α(s) > 0 (in particular, α(s) = 1 will result in x(s, a, t) = π(s, a, t)).",
                "However, for MDPs with resource constraints (as defined below in Section 3), uniformly-optimal policies do not in general exist.",
                "In such cases, α becomes a part of the problem input, and a resulting policy is only optimal for that particular α.",
                "This result is well known for infinite-horizon MDPs with various types of constraints [1, 6], and it also holds for our finite-horizon model, which can be easily established via a line of reasoning completely analogous to the arguments in [6]. 2.2 Combinatorial Resource Scheduling A straightforward approach to resource scheduling for a set of agents M, whose values for the resources are induced by stochastic planning problems (in our case, finite-horizon MDPs) would be to have each agent enumerate all possible resource assignments over time and, for each one, compute its value by solving the corresponding MDP.",
                "Then, each agent would provide valuations for each possible resource bundle over time to a centralized coordinator, who would compute the optimal resource assignments across time based on these valuations.",
                "When resources can be allocated at different times to different agents, each agent must submit valuations for every combination of possible time horizons.",
                "Let each agent m ∈ M execute its MDP within the arrival-departure time interval τ ∈ [τa m, τd m].",
                "Hence, agent m will execute an MDP with time horizon no greater than Tm = τd m−τa m+1.",
                "Let bτ be the global time horizon for the problem, before which all of the agents MDPs must finish.",
                "We assume τd m < bτ, ∀m ∈ M. The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) 1221 For the scheduling problem where agents have static resource requirements within their finite-horizon MDPs, the agents provide a valuation for each resource bundle for each possible time horizon (from [1, Tm]) that they may use.",
                "Let Ω be the set of resources to be allocated among the agents.",
                "An agent will get at most one resource bundle for one of the time horizons.",
                "Let the variable ψ ∈ Ψm enumerate all possible pairs of resource bundles and time horizons for agent m, so there are 2|Ω| × Tm values for ψ (the space of bundles is exponential in the number of resource types |Ω|).",
                "The agent m must provide a value vψ m for each ψ, and the coordinator will allocate at most one ψ (resource, time horizon) pair to each agent.",
                "This allocation is expressed as an indicator variable zψ m ∈ {0, 1} that shows whether ψ is assigned to agent m. For time τ and resource ω, the function nm(ψ, τ, ω) ∈ {0, 1} indicates whether the bundle in ψ uses resource ω at time τ (we make the assumption that agents have binary resource requirements).",
                "This allocation problem is NP-complete, even when considering only a single time step, and its difficulty increases significantly with multiple time steps because of the increasing number of values of ψ.",
                "The problem of finding an optimal allocation that satisfies the global constraint that the amount of each resource ω allocated to all agents does not exceed the available amount bϕ(ω) can be expressed as the following integer program: max X m∈M X ψ∈Ψm zψ mvψ m subject to: X ψ∈Ψm zψ m ≤ 1, ∀m ∈ M; X m∈M X ψ∈Ψm zψ mnm(ψ, τ, ω) ≤ bϕ(ω), ∀τ ∈ [1, bτ], ∀ω ∈ Ω; (3) The first constraint in equation 3 says that no agent can receive more than one bundle, and the second constraint ensures that the total assignment of resource ω does not, at any time, exceed the resource bound.",
                "For the scheduling problem where the agents are able to dynamically reallocate resources, each agent must specify a value for every combination of bundles and time steps within its time horizon.",
                "Let the variable ψ ∈ Ψm in this case enumerate all possible resource bundles for which at most one bundle may be assigned to agent m at each time step.",
                "Therefore, in this case there are P t∈[1,Tm](2|Ω| )t ∼ 2|Ω|Tm possibilities of resource bundles assigned to different time slots, for the Tm different time horizons.",
                "The same set of equations (3) can be used to solve this dynamic scheduling problem, but the integer program is different because of the difference in how ψ is defined.",
                "In this case, the number of ψ values is exponential in each agents planning horizon Tm, resulting in a much larger program.",
                "This straightforward approach to solving both of these scheduling problems requires an enumeration and solution of either 2|Ω| Tm (static allocation) or P t∈[1,Tm] 2|Ω|t (dynamic reallocation) MDPs for each agent, which very quickly becomes intractable with the growth of the number of resources |Ω| or the time horizon Tm. 3.",
                "MODEL AND PROBLEM STATEMENT We now formally introduce our model of the resourcescheduling problem.",
                "The problem input consists of the following components: • M, Ω, bϕ, τa m, τd m, bτ are as defined above in Section 2.2. • {Θm} = {S, A, pm, rm, αm} are the MDPs of all agents m ∈ M. Without loss of generality, we assume that state and action spaces of all agents are the same, but each has its own transition function pm, reward function rm, and initial conditions αm. • ϕm : A×Ω → {0, 1} is the mapping of actions to resources for agent m. ϕm(a, ω) indicates whether action a of agent m needs resource ω.",
                "An agent m that receives a set of resources that does not include resource ω cannot execute in its MDP policy any action a for which ϕm(a, ω) = 0.",
                "We assume all resource requirements are binary; as discussed below in Section 6, this assumption is not limiting.",
                "Given the above input, the optimization problem we consider is to find the globally optimal-maximizing the sum of expected rewards-mapping of resources to agents for all time steps: Δ : τ × M × Ω → {0, 1}.",
                "A solution is feasible if the corresponding assignment of resources to the agents does not violate the global resource constraint: X m Δm(τ, ω) ≤ bϕ(ω), ∀ω ∈ Ω, τ ∈ [1, bτ]. (4) We consider two flavors of the resource-scheduling problem.",
                "The first formulation restricts resource assignments to the space where the allocation of resources to each agent is static during the agents lifetime.",
                "The second formulation allows reassignment of resources between agents at every time step within their lifetimes.",
                "Figure 1 depicts a resource-scheduling problem with three agents M = {m1, m2, m3}, three resources Ω = {ω1, ω2, ω3}, and a global problem horizon of bτ = 11.",
                "The agents arrival and departure times are shown as gray boxes and are {1, 6}, {3, 7}, and {2, 11}, respectively.",
                "A solution to this problem is shown via horizontal bars within each agents box, where the bars correspond to the allocation of the three resource types.",
                "Figure 1a shows a solution to a static scheduling problem.",
                "According to the shown solution, agent m1 begins the execution of its MDP at time τ = 1 and has a lock on all three resources until it finishes execution at time τ = 3.",
                "Note that agent m1 relinquishes its hold on the resources before its announced departure time of τd m1 = 6, ostensibly because other agents can utilize the resources more effectively.",
                "Thus, at time τ = 4, resources ω1 and ω3 are allocated to agent m2, who then uses them to execute its MDP (using only actions supported by resources ω1 and ω3) until time τ = 7.",
                "Agent m3 holds resource ω3 during the interval τ ∈ [4, 10].",
                "Figure 1b shows a possible solution to the dynamic version of the same problem.",
                "There, resources can be reallocated between agents at every time step.",
                "For example, agent m1 gives up its use of resource ω2 at time τ = 2, although it continues the execution of its MDP until time τ = 6.",
                "Notice that an agent is not allowed to stop and restart its MDP, so agent m1 is only able to continue executing in the interval τ ∈ [3, 4] if it has actions that do not require any resources (ϕm(a, ω) = 0).",
                "Clearly, the model and problem statement described above make a number of assumptions about the problem and the desired solution properties.",
                "We discuss some of those assumptions and their implications in Section 6. 1222 The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) (a) (b) Figure 1: Illustration of a solution to a resource-scheduling problem with three agents and three resources: a) static resource assignments (resource assignments are constant within agents lifetimes; b) dynamic assignment (resource assignments are allowed to change at every time step). 4.",
                "RESOURCE SCHEDULING Our resource-scheduling algorithm proceeds in two stages.",
                "First, we perform a preprocessing step that augments the agent MDPs; this process is described in Section 4.1.",
                "Second, using these augmented MDPs we construct a global optimization problem, which is described in Section 4.2. 4.1 Augmenting Agents MDPs In the model described in the previous section, we assume that if an agent does not possess the necessary resources to perform actions in its MDP, its execution is halted and the agent leaves the system.",
                "In other words, the MDPs cannot be paused and resumed.",
                "For example, in the problem shown in Figure 1a, agent m1 releases all resources after time τ = 3, at which point the execution of its MDP is halted.",
                "Similarly, agents m2 and m3 only execute their MDPs in the intervals τ ∈ [4, 6] and τ ∈ [4, 10], respectively.",
                "Therefore, an important part of the global decision-making problem is to decide the window of time during which each of the agents is active (i.e., executing its MDP).",
                "To accomplish this, we augment each agents MDP with two new states (start and finish states sb , sf , respectively) and a new start/stop action a∗ , as illustrated in Figure 2.",
                "The idea is that an agent stays in the start state sb until it is ready to execute its MDP, at which point it performs the start/stop action a∗ and transitions into the state space of the original MDP with the transition probability that corresponds to the original initial distribution α(s).",
                "For example, in Figure 1a, for agent m2 this would happen at time τ = 4.",
                "Once the agent gets to the end of its activity window (time τ = 6 for agent m2 in Figure 1a), it performs the start/stop action, which takes it into the sink finish state sf at time τ = 7.",
                "More precisely, given an MDP S, A, pm, rm, αm , we define an augmented MDP S , A , pm, rm, αm as follows: S = S ∪ sb ∪ sf ; A = A ∪ a∗ ; p (s|sb , a∗ ) = α(s), ∀s ∈ S; p (sb |sb , a) = 1.0, ∀a ∈ A; p (sf |s, a∗ ) = 1.0, ∀s ∈ S; p (σ|s, a) = p(σ|s, a), ∀s, σ ∈ S, a ∈ A; r (sb , a) = r (sf , a) = 0, ∀a ∈ A ; r (s, a) = r(s, a), ∀s ∈ S, a ∈ A; α (sb ) = 1; α (s) = 0, ∀s ∈ S; where all non-specified transition probabilities are assumed to be zero.",
                "Further, in order to account for the new starting state, we begin the MDP one time-step earlier, setting τa m ← τa m − 1.",
                "This will not affect the resource allocation due to the resource constraints only being enforced for the original MDP states, as will be discussed in the next section.",
                "For example, the augmented MDPs shown in Figure 2b (which starts in state sb at time τ = 2) would be constructed from an MDP with original arrival time τ = 3.",
                "Figure 2b also shows a sample trajectory through the state space: the agent starts in state sb , transitions into the state space S of the original MDP, and finally exists into the sink state sf .",
                "Note that if we wanted to model a problem where agents could pause their MDPs at arbitrary time steps (which might be useful for domains where dynamic reallocation is possible), we could easily accomplish this by including an extra action that transitions from each state to itself with zero reward. 4.2 MILP for Resource Scheduling Given a set of augmented MDPs, as defined above, the goal of this section is to formulate a global optimization program that solves the resource-scheduling problem.",
                "In this section and below, all MDPs are assumed to be the augmented MDPs as defined in Section 4.1.",
                "Our approach is similar to the idea used in [6]: we begin with the linear-program formulation of agents MDPs (1) and augment it with constraints that ensure that the corresponding resource allocation across agents and time is valid.",
                "The resulting optimization problem then simultaneously solves the agents MDPs and resource-scheduling problems.",
                "In the rest of this section, we incrementally develop a mixed integer program (MILP) that achieves this.",
                "In the absence of resource constraints, the agents finitehorizon MDPs are completely independent, and the globally optimal solution can be trivially obtained via the following LP, which is simply an aggregation of single-agent finitehorizon LPs: max X m X s X a rm(s, a) X t xm(s, a, t) subject to: X a xm(σ, a, t + 1) = X s,a pm(σ|s, a)xm(s, a, t), ∀m ∈ M, σ ∈ S, t ∈ [1, Tm − 1]; X a xm(s, a, 1) = αm(s), ∀m ∈ M, s ∈ S; (12) where xm(s, a, t) is the occupation measure of agent m, and The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) 1223 (a) (b) Figure 2: Illustration of augmenting an MDP to allow for variable starting and stopping times: a) (left) the original two-state MDP with a single action; (right) the augmented MDP with new states sb and sf and the new action a∗ (note that the origianl transitions are not changed in the augmentation process); b) the augmented MDP displayed as a trajectory through time (grey lines indicate all transitions, while black lines indicate a given trajectory.",
                "Objective Function (sum of expected rewards over all agents) max X m X s X a rm(s, a) X t xm(s, a, t) (5) Meaning Implication Linear Constraints Tie x to θ.",
                "Agent is only active when occupation measure is nonzero in original MDP states. θm(τ) = 0 =⇒ xm(s, a, τ −τa m+1) = 0 ∀s /∈ {sb , sf }, a ∈ A X s/∈{sb,sf } X a xm(s, a, t) ≤ θm(τa m + t − 1) ∀m ∈ M, ∀t ∈ [1, Tm] (6) Agent can only be active in τ ∈ (τa m, τd m) θm(τ) = 0 ∀m ∈ M, τ /∈ (τa m, τd m) (7) Cannot use resources when not active θm(τ) = 0 =⇒ Δm(τ, ω) = 0 ∀τ ∈ [0, bτ], ω ∈ Ω Δm(τ, ω) ≤ θm(τ) ∀m ∈ M, τ ∈ [0, bτ], ω ∈ Ω (8) Tie x to Δ (nonzero x forces corresponding Δ to be nonzero.)",
                "Δm(τ, ω) = 0, ϕm(a, ω) = 1 =⇒ xm(s, a, τ − τa m + 1) = 0 ∀s /∈ {sb , sf } 1/|A| X a ϕm(a, ω) X s/∈{sb,sf } xm(s, a, t) ≤ Δm(t + τa m − 1, ω) ∀m ∈ M, ω ∈ Ω, t ∈ [1, Tm] (9) Resource bounds X m Δm(τ, ω) ≤ bϕ(ω) ∀ω ∈ Ω, τ ∈ [0, bτ] (10) Agent cannot change resources while active.",
                "Only enabled for scheduling with static assignments. θm(τ) = 1 and θm(τ + 1) = 1 =⇒ Δm(τ, ω) = Δm(τ + 1, ω) Δm(τ, ω) − Z(1 − θm(τ + 1)) ≤ Δm(τ + 1, ω) + Z(1 − θm(τ)) Δm(τ, ω) + Z(1 − θm(τ + 1)) ≥ Δm(τ + 1, ω) − Z(1 − θm(τ)) ∀m ∈ M, ω ∈ Ω, τ ∈ [0, bτ] (11) Table 1: MILP for globally optimal resource scheduling.",
                "Tm = τd m − τa m + 1 is the time horizon for the agents MDP.",
                "Using this LP as a basis, we augment it with constraints that ensure that the resource usage implied by the agents occupation measures {xm} does not violate the global resource requirements bϕ at any time step τ ∈ [0, bτ].",
                "To formulate these resource constraints, we use the following binary variables: • Δm(τ, ω) = {0, 1}, ∀m ∈ M, τ ∈ [0, bτ], ω ∈ Ω, which serve as indicator variables that define whether agent m possesses resource ω at time τ.",
                "These are analogous to the static indicator variables used in the one-shot static resource-allocation problem in [6]. • θm = {0, 1}, ∀m ∈ M, τ ∈ [0, bτ] are indicator variables that specify whether agent m is active (i.e., executing its MDP) at time τ.",
                "The meaning of resource-usage variables Δ is illustrated in Figure 1: Δm(τ, ω) = 1 only if resource ω is allocated to agent m at time τ.",
                "The meaning of the activity indicators θ is illustrated in Figure 2b: when agent m is in either the start state sb or the finish state sf , the corresponding θm = 0, but once the agent becomes active and enters one of the other states, we set θm = 1 .",
                "This meaning of θ can be enforced with a linear constraint that synchronizes the values of the agents occupation measures xm and the activity 1224 The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) indicators θ, as shown in (6) in Table 1.",
                "Another constraint we have to add-because the activity indicators θ are defined on the global timeline τ-is to enforce the fact that the agent is inactive outside of its arrivaldeparture window.",
                "This is accomplished by constraint (7) in Table 1.",
                "Furthermore, agents should not be using resources while they are inactive.",
                "This constraint can also be enforced via a linear inequality on θ and Δ, as shown in (8).",
                "Constraint (6) sets the value of θ to match the policy defined by the occupation measure xm.",
                "In a similar fashion, we have to make sure that the resource-usage variables Δ are also synchronized with the occupation measure xm.",
                "This is done via constraint (9) in Table 1, which is nearly identical to the analogous constraint from [6].",
                "After implementing the above constraint, which enforces the meaning of Δ, we add a constraint that ensures that the agents resource usage never exceeds the amounts of available resources.",
                "This condition is also trivially expressed as a linear inequality (10) in Table 1.",
                "Finally, for the problem formulation where resource assignments are static during a lifetime of an agent, we add a constraint that ensures that the resource-usage variables Δ do not change their value while the agent is active (θ = 1).",
                "This is accomplished via the linear constraint (11), where Z ≥ 2 is a constant that is used to turn off the constraints when θm(τ) = 0 or θm(τ + 1) = 0.",
                "This constraint is not used for the dynamic problem formulation, where resources can be reallocated between agents at every time step.",
                "To summarize, Table 1 together with the conservationof-flow constraints from (12) defines the MILP that simultaneously computes an optimal resource assignment for all agents across time as well as optimal finite-horizon MDP policies that are valid under that resource assignment.",
                "As a rough measure of the complexity of this MILP, let us consider the number of optimization variables and constraints.",
                "Let TM = P Tm = P m(τa m − τd m + 1) be the sum of the lengths of the arrival-departure windows across all agents.",
                "Then, the number of optimization variables is: TM + bτ|M||Ω| + bτ|M|, TM of which are continuous (xm), and bτ|M||Ω| + bτ|M| are binary (Δ and θ).",
                "However, notice that all but TM|M| of the θ are set to zero by constraint (7), which also immediately forces all but TM|M||Ω| of the Δ to be zero via the constraints (8).",
                "The number of constraints (not including the degenerate constraints in (7)) in the MILP is: TM + TM|Ω| + bτ|Ω| + bτ|M||Ω|.",
                "Despite the fact that the complexity of the MILP is, in the worst case, exponential1 in the number of binary variables, the complexity of this MILP is significantly (exponentially) lower than that of the MILP with flat utility functions, described in Section 2.2.",
                "This result echos the efficiency gains reported in [6] for single-shot resource-allocation problems, but is much more pronounced, because of the explosion of the flat utility representation due to the temporal aspect of the problem (recall the prohibitive complexity of the combinatorial optimization in Section 2.2).",
                "We empirically analyze the performance of this method in Section 5. 1 Strictly speaking, solving MILPs to optimality is NPcomplete in the number of integer variables. 5.",
                "EXPERIMENTAL RESULTS Although the complexity of solving MILPs is in the worst case exponential in the number of integer variables, there are many efficient methods for solving MILPs that allow our algorithm to scale well for parameters common to resource allocation and scheduling problems.",
                "In particular, this section introduces a problem domain-the repairshop problem-used to empirically evaluate our algorithms scalability in terms of the number of agents |M|, the number of shared resources |Ω|, and the varied lengths of global time bτ during which agents may enter and exit the system.",
                "The repairshop problem is a simple parameterized MDP adopting the metaphor of a vehicular repair shop.",
                "Agents in the repair shop are mechanics with a number of independent tasks that yield reward only when completed.",
                "In our MDP model of this system, actions taken to advance through the state space are only allowed if the agent holds certain resources that are publicly available to the shop.",
                "These resources are in finite supply, and optimal policies for the shop will determine when each agent may hold the limited resources to take actions and earn individual rewards.",
                "Each task to be completed is associated with a single action, although the agent is required to repeat the action numerous times before completing the task and earning a reward.",
                "This model was parameterized in terms of the number of agents in the system, the number of different types of resources that could be linked to necessary actions, a global time during which agents are allowed to arrive and depart, and a maximum length for the number of time steps an agent may remain in the system.",
                "All datapoints in our experiments were obtained with 20 evaluations using CPLEX to solve the MILPs on a Pentium4 computer with 2Gb of RAM.",
                "Trials were conducted on both the static and the dynamic version of the resourcescheduling problem, as defined earlier.",
                "Figure 3 shows the runtime and policy value for independent modifications to the parameter set.",
                "The top row shows how the solution time for the MILP scales as we increase the number of agents |M|, the global time horizon bτ, and the number of resources |Ω|.",
                "Increasing the number of agents leads to exponential complexity scaling, which is to be expected for an NP-complete problem.",
                "However, increasing the global time limit bτ or the total number of resource types |Ω|-while holding the number of agents constantdoes not lead to decreased performance.",
                "This occurs because the problems get easier as they become under-constrained, which is also a common phenomenon for NP-complete problems.",
                "We also observe that the solution to the dynamic version of the problem can often be computed much faster than the static version.",
                "The bottom row of Figure 3 shows the joint policy value of the policies that correspond to the computed optimal resource-allocation schedules.",
                "We can observe that the dynamic version yields higher reward (as expected, since the reward for the dynamic version is always no less than the reward of the static version).",
                "We should point out that these graphs should not be viewed as a measure of performance of two different algorithms (both algorithms produce optimal solutions but to different problems), but rather as observations about how the quality of optimal solutions change as more flexibility is allowed in the reallocation of resources.",
                "Figure 4 shows runtime and policy value for trials in which common input variables are scaled together.",
                "This allows The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) 1225 2 4 6 8 10 10 −3 10 −2 10 −1 10 0 10 1 10 2 10 3 10 4 Number of Agents |M| CPUTime,sec |Ω| = 5, τ = 50 static dynamic 50 100 150 200 10 −2 10 −1 10 0 10 1 10 2 10 3 Global Time Boundary τ CPUTime,sec |M| = 5, |Ω| = 5 static dynamic 10 20 30 40 50 10 −2 10 −1 10 0 10 1 10 2 Number of Resources |Ω| CPUTime,sec |M| = 5, τ = 50 static dynamic 2 4 6 8 10 200 400 600 800 1000 1200 1400 1600 Number of Agents |M| Value |Ω| = 5, τ = 50 static dynamic 50 100 150 200 400 500 600 700 800 900 1000 1100 1200 1300 1400 Global Time Boundary τ Value |M| = 5, |Ω| = 5 static dynamic 10 20 30 40 50 500 600 700 800 900 1000 1100 1200 1300 1400 Number of Resources |Ω| Value |M| = 5, τ = 50 static dynamic Figure 3: Evaluation of our MILP for variable numbers of agents (column 1), lengths of global-time window (column 2), and numbers of resource types (column 3).",
                "Top row shows CPU time, and bottom row shows the joint reward of agents MDP policies.",
                "Error bars show the 1st and 3rd quartiles (25% and 75%). 2 4 6 8 10 10 −3 10 −2 10 −1 10 0 10 1 10 2 10 3 Number of Agents |M| CPUTime,sec τ = 10|M| static dynamic 2 4 6 8 10 10 −3 10 −2 10 −1 10 0 10 1 10 2 10 3 10 4 Number of Agents |M| CPUTime,sec |Ω| = 2|M| static dynamic 2 4 6 8 10 10 −3 10 −2 10 −1 10 0 10 1 10 2 10 3 10 4 Number of Agents |M| CPUTime,sec |Ω| = 5|M| static dynamic 2 4 6 8 10 200 400 600 800 1000 1200 1400 1600 1800 2000 2200 Number of Agents |M| Value τ = 10|M| static dynamic 2 4 6 8 10 200 400 600 800 1000 1200 1400 1600 1800 2000 Number of Agents |M| Value |Ω| = 2|M| static dynamic 2 4 6 8 10 0 500 1000 1500 2000 2500 Number of Agents |M| Value |Ω| = 5|M| static dynamic Figure 4: Evaluation of our MILP using correlated input variables.",
                "The left column tracks the performance and CPU time as the number of agents and global-time window increase together (bτ = 10|M|).",
                "The middle and the right column track the performance and CPU time as the number of resources and the number of agents increase together as |Ω| = 2|M| and |Ω| = 5|M|, respectively.",
                "Error bars show the 1st and 3rd quartiles (25% and 75%). 1226 The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) us to explore domains where the total number of agents scales proportionally to the total number of resource types or the global time horizon, while keeping constant the average agent density (per unit of global time) or the average number of resources per agent (which commonly occurs in real-life applications).",
                "Overall, we believe that these experimental results indicate that our MILP formulation can be used to effectively solve resource-scheduling problems of nontrivial size. 6.",
                "DISCUSSION AND CONCLUSIONS Throughout the paper, we have made a number of assumptions in our model and solution algorithm; we discuss their implications below. • Continual execution.",
                "We assume that once an agent stops executing its MDP (transitions into state sf ), it exits the system and cannot return.",
                "It is easy to relax this assumption for domains where agents MDPs can be paused and restarted.",
                "All that is required is to include an additional pause action which transitions from a given state back to itself, and has zero reward. • Indifference to start time.",
                "We used a reward model where agents rewards depend only on the time horizon of their MDPs and not the global start time.",
                "This is a consequence of our MDP-augmentation procedure from Section 4.1.",
                "It is easy to extend the model so that the agents incur an explicit penalty for idling by assigning a non-zero negative reward to the start state sb . • Binary resource requirements.",
                "For simplicity, we have assumed that resource costs are binary: ϕm(a, ω) = {0, 1}, but our results generalize in a straightforward manner to non-binary resource mappings, analogously to the procedure used in [5]. • Cooperative agents.",
                "The optimization procedure discussed in this paper was developed in the context of cooperative agents, but it can also be used to design a mechanism for scheduling resources among self-interested agents.",
                "This optimization procedure can be embedded in a VickreyClarke-Groves auction, completely analogously to the way it was done in [7].",
                "In fact, all the results of [7] about the properties of the auction and information privacy directly carry over to the scheduling domain discussed in this paper, requiring only slight modifications to deal with finitehorizon MDPs. • Known, deterministic arrival and departure times.",
                "Finally, we have assumed that agents arrival and departure times (τa m and τd m) are deterministic and known a priori.",
                "This assumption is fundamental to our solution method.",
                "While there are many domains where this assumption is valid, in many cases agents arrive and depart dynamically and their arrival and departure times can only be predicted probabilistically, leading to online resource-allocation problems.",
                "In particular, in the case of self-interested agents, this becomes an interesting version of an online-mechanism-design problem [11, 12].",
                "In summary, we have presented an MILP formulation for the combinatorial resource-scheduling problem where agents values for possible resource assignments are defined by finitehorizon MDPs.",
                "This result extends previous work ([6, 7]) on static one-shot resource allocation under MDP-induced preferences to resource-scheduling problems with a temporal aspect.",
                "As such, this work takes a step in the direction of designing an online mechanism for agents with combinatorial resource preferences induced by stochastic planning problems.",
                "Relaxing the assumption about deterministic arrival and departure times of the agents is a focus of our future work.",
                "We would like to thank the anonymous reviewers for their insightful comments and suggestions. 7.",
                "REFERENCES [1] E. Altman and A. Shwartz.",
                "Adaptive control of constrained Markov chains: Criteria and policies.",
                "Annals of Operations Research, special issue on Markov Decision Processes, 28:101-134, 1991. [2] R. Bellman.",
                "Dynamic Programming.",
                "Princeton University Press, 1957. [3] C. Boutilier.",
                "Solving concisely expressed combinatorial auction problems.",
                "In Proc. of AAAI-02, pages 359-366, 2002. [4] C. Boutilier and H. H. Hoos.",
                "Bidding languages for combinatorial auctions.",
                "In Proc. of IJCAI-01, pages 1211-1217, 2001. [5] D. Dolgov.",
                "Integrated Resource Allocation and Planning in Stochastic Multiagent Environments.",
                "PhD thesis, Computer Science Department, University of Michigan, February 2006. [6] D. A. Dolgov and E. H. Durfee.",
                "Optimal resource allocation and policy formulation in loosely-coupled Markov decision processes.",
                "In Proc. of ICAPS-04, pages 315-324, June 2004. [7] D. A. Dolgov and E. H. Durfee.",
                "Computationally efficient combinatorial auctions for resource allocation in weakly-coupled MDPs.",
                "In Proc. of AAMAS-05, New York, NY, USA, 2005.",
                "ACM Press. [8] D. A. Dolgov and E. H. Durfee.",
                "Resource allocation among agents with preferences induced by factored MDPs.",
                "In Proc. of AAMAS-06, 2006. [9] K. Larson and T. Sandholm.",
                "Mechanism design and deliberative agents.",
                "In Proc. of AAMAS-05, pages 650-656, New York, NY, USA, 2005.",
                "ACM Press. [10] N. Nisan.",
                "Bidding and allocation in combinatorial auctions.",
                "In Electronic Commerce, 2000. [11] D. C. Parkes and S. Singh.",
                "An MDP-based approach to Online Mechanism Design.",
                "In Proc. of the Seventeenths Annual Conference on Neural Information Processing Systems (NIPS-03), 2003. [12] D. C. Parkes, S. Singh, and D. Yanovsky.",
                "Approximately efficient online mechanism design.",
                "In Proc. of the Eighteenths Annual Conference on Neural Information Processing Systems (NIPS-04), 2004. [13] M. L. Puterman.",
                "Markov Decision Processes.",
                "John Wiley & Sons, New York, 1994. [14] M. H. Rothkopf, A. Pekec, and R. M. Harstad.",
                "Computationally manageable combinational auctions.",
                "Management Science, 44(8):1131-1147, 1998. [15] T. Sandholm.",
                "An algorithm for optimal winner determination in combinatorial auctions.",
                "In Proc. of IJCAI-99, pages 542-547, San Francisco, CA, USA, 1999.",
                "Morgan Kaufmann Publishers Inc.",
                "The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) 1227"
            ],
            "original_annotated_samples": [],
            "translated_annotated_samples": [
                "En particular, si un agente usa recursos para actuar en un entorno estocástico, su función de utilidad puede modelarse naturalmente con un \"proceso de decisión de Markov\", cuyo conjunto de acciones es parametrizado por los recursos disponibles.Proceso de decisión de Markov"
            ],
            "translated_text": "",
            "candidates": [],
            "error": []
        },
        "resource allocation": {
            "translated_key": "",
            "is_in_text": true,
            "original_annotated_sentences": [
                "Combinatorial Resource Scheduling for Multiagent MDPs Dmitri A. Dolgov, Michael R. James, and Michael E. Samples AI and Robotics Group Technical Research, Toyota Technical Center USA {ddolgov, michael.r.james, michael.samples}@gmail.com ABSTRACT Optimal resource scheduling in multiagent systems is a computationally challenging task, particularly when the values of resources are not additive.",
                "We consider the combinatorial problem of scheduling the usage of multiple resources among agents that operate in stochastic environments, modeled as Markov decision processes (MDPs).",
                "In recent years, efficient resource-allocation algorithms have been developed for agents with resource values induced by MDPs.",
                "However, this prior work has focused on static resource-allocation problems where resources are distributed once and then utilized in infinite-horizon MDPs.",
                "We extend those existing models to the problem of combinatorial resource scheduling, where agents persist only for finite periods between their (predefined) arrival and departure times, requiring resources only for those time periods.",
                "We provide a computationally efficient procedure for computing globally optimal resource assignments to agents over time.",
                "We illustrate and empirically analyze the method in the context of a stochastic jobscheduling domain.",
                "Categories and Subject Descriptors I.2.8 [Artificial Intelligence]: Problem Solving, Control Methods, and Search; I.2.11 [Artificial Intelligence]: Distributed Artificial Intelligence-Multiagent systems General Terms Algorithms, Performance, Design 1.",
                "INTRODUCTION The tasks of optimal <br>resource allocation</br> and scheduling are ubiquitous in multiagent systems, but solving such optimization problems can be computationally difficult, due to a number of factors.",
                "In particular, when the value of a set of resources to an agent is not additive (as is often the case with resources that are substitutes or complements), the utility function might have to be defined on an exponentially large space of resource bundles, which very quickly becomes computationally intractable.",
                "Further, even when each agent has a utility function that is nonzero only on a small subset of the possible resource bundles, obtaining optimal allocation is still computationally prohibitive, as the problem becomes NP-complete [14].",
                "Such computational issues have recently spawned several threads of work in using compact models of agents preferences.",
                "One idea is to use any structure present in utility functions to represent them compactly, via, for example, logical formulas [15, 10, 4, 3].",
                "An alternative is to directly model the mechanisms that define the agents utility functions and perform <br>resource allocation</br> directly with these models [9].",
                "A way of accomplishing this is to model the processes by which an agent might utilize the resources and define the utility function as the payoff of these processes.",
                "In particular, if an agent uses resources to act in a stochastic environment, its utility function can be naturally modeled with a Markov decision process, whose action set is parameterized by the available resources.",
                "This representation can then be used to construct very efficient resource-allocation algorithms that lead to an exponential speedup over a straightforward optimization problem with flat representations of combinatorial preferences [6, 7, 8].",
                "However, this existing work on <br>resource allocation</br> with preferences induced by resource-parameterized MDPs makes an assumption that the resources are only allocated once and are then utilized by the agents independently within their infinite-horizon MDPs.",
                "This assumption that no reallocation of resources is possible can be limiting in domains where agents arrive and depart dynamically.",
                "In this paper, we extend the work on <br>resource allocation</br> under MDP-induced preferences to discrete-time scheduling problems, where agents are present in the system for finite time intervals and can only use resources within these intervals.",
                "In particular, agents arrive and depart at arbitrary (predefined) times and within these intervals use resources to execute tasks in finite-horizon MDPs.",
                "We address the problem of globally optimal resource scheduling, where the objective is to find an allocation of resources to the agents across time that maximizes the sum of the expected rewards that they obtain.",
                "In this context, our main contribution is a mixed-integerprogramming formulation of the scheduling problem that chooses globally optimal resource assignments, starting times, and execution horizons for all agents (within their arrival1220 978-81-904262-7-5 (RPS) c 2007 IFAAMAS departure intervals).",
                "We analyze and empirically compare two flavors of the scheduling problem: one, where agents have static resource assignments within their finite-horizon MDPs, and another, where resources can be dynamically reallocated between agents at every time step.",
                "In the rest of the paper, we first lay down the necessary groundwork in Section 2 and then introduce our model and formal problem statement in Section 3.",
                "In Section 4.2, we describe our main result, the optimization program for globally optimal resource scheduling.",
                "Following the discussion of our experimental results on a job-scheduling problem in Section 5, we conclude in Section 6 with a discussion of possible extensions and generalizations of our method. 2.",
                "BACKGROUND Similarly to the model used in previous work on resourceallocation with MDP-induced preferences [6, 7], we define the value of a set of resources to an agent as the value of the best MDP policy that is realizable, given those resources.",
                "However, since the focus of our work is on scheduling problems, and a large part of the optimization problem is to decide how resources are allocated in time among agents with finite arrival and departure times, we model the agents planning problems as finite-horizon MDPs, in contrast to previous work that used infinite-horizon discounted MDPs.",
                "In the rest of this section, we first introduce some necessary background on finite-horizon MDPs and present a linear-programming formulation that serves as the basis for our solution algorithm developed in Section 4.",
                "We also outline the standard methods for combinatorial resource scheduling with flat resource values, which serve as a comparison benchmark for the new model developed here. 2.1 Markov Decision Processes A stationary, finite-domain, discrete-time MDP (see, for example, [13] for a thorough and detailed development) can be described as S, A, p, r , where: S is a finite set of system states; A is a finite set of actions that are available to the agent; p is a stationary stochastic transition function, where p(σ|s, a) is the probability of transitioning to state σ upon executing action a in state s; r is a stationary reward function, where r(s, a) specifies the reward obtained upon executing action a in state s. Given such an MDP, a decision problem under a finite horizon T is to choose an optimal action at every time step to maximize the expected value of the total reward accrued during the agents (finite) lifetime.",
                "The agents optimal policy is then a function of current state s and the time until the horizon.",
                "An optimal policy for such a problem is to act greedily with respect to the optimal value function, defined recursively by the following system of finite-time Bellman equations [2]: v(s, t) = max a r(s, a) + X σ p(σ|s, a)v(σ, t + 1), ∀s ∈ S, t ∈ [1, T − 1]; v(s, T) = 0, ∀s ∈ S; where v(s, t) is the optimal value of being in state s at time t ∈ [1, T].",
                "This optimal value function can be easily computed using dynamic programming, leading to the following optimal policy π, where π(s, a, t) is the probability of executing action a in state s at time t: π(s, a, t) = ( 1, a = argmaxa r(s, a) + P σ p(σ|s, a)v(σ, t + 1), 0, otherwise.",
                "The above is the most common way of computing the optimal value function (and therefore an optimal policy) for a finite-horizon MDP.",
                "However, we can also formulate the problem as the following linear program (similarly to the dual LP for infinite-horizon discounted MDPs [13, 6, 7]): max X s X a r(s, a) X t x(s, a, t) subject to: X a x(σ, a, t + 1) = X s,a p(σ|s, a)x(s, a, t) ∀σ, t ∈ [1, T − 1]; X a x(s, a, 1) = α(s), ∀s ∈ S; (1) where α(s) is the initial distribution over the state space, and x is the (non-stationary) occupation measure (x(s, a, t) ∈ [0, 1] is the total expected number of times action a is executed in state s at time t).",
                "An optimal (non-stationary) policy is obtained from the occupation measure as follows: π(s, a, t) = x(s, a, t)/ X a x(s, a, t) ∀s ∈ S, t ∈ [1, T]. (2) Note that the standard unconstrained finite-horizon MDP, as described above, always has a uniformly-optimal solution (optimal for any initial distribution α(s)).",
                "Therefore, an optimal policy can be obtained by using an arbitrary constant α(s) > 0 (in particular, α(s) = 1 will result in x(s, a, t) = π(s, a, t)).",
                "However, for MDPs with resource constraints (as defined below in Section 3), uniformly-optimal policies do not in general exist.",
                "In such cases, α becomes a part of the problem input, and a resulting policy is only optimal for that particular α.",
                "This result is well known for infinite-horizon MDPs with various types of constraints [1, 6], and it also holds for our finite-horizon model, which can be easily established via a line of reasoning completely analogous to the arguments in [6]. 2.2 Combinatorial Resource Scheduling A straightforward approach to resource scheduling for a set of agents M, whose values for the resources are induced by stochastic planning problems (in our case, finite-horizon MDPs) would be to have each agent enumerate all possible resource assignments over time and, for each one, compute its value by solving the corresponding MDP.",
                "Then, each agent would provide valuations for each possible resource bundle over time to a centralized coordinator, who would compute the optimal resource assignments across time based on these valuations.",
                "When resources can be allocated at different times to different agents, each agent must submit valuations for every combination of possible time horizons.",
                "Let each agent m ∈ M execute its MDP within the arrival-departure time interval τ ∈ [τa m, τd m].",
                "Hence, agent m will execute an MDP with time horizon no greater than Tm = τd m−τa m+1.",
                "Let bτ be the global time horizon for the problem, before which all of the agents MDPs must finish.",
                "We assume τd m < bτ, ∀m ∈ M. The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) 1221 For the scheduling problem where agents have static resource requirements within their finite-horizon MDPs, the agents provide a valuation for each resource bundle for each possible time horizon (from [1, Tm]) that they may use.",
                "Let Ω be the set of resources to be allocated among the agents.",
                "An agent will get at most one resource bundle for one of the time horizons.",
                "Let the variable ψ ∈ Ψm enumerate all possible pairs of resource bundles and time horizons for agent m, so there are 2|Ω| × Tm values for ψ (the space of bundles is exponential in the number of resource types |Ω|).",
                "The agent m must provide a value vψ m for each ψ, and the coordinator will allocate at most one ψ (resource, time horizon) pair to each agent.",
                "This allocation is expressed as an indicator variable zψ m ∈ {0, 1} that shows whether ψ is assigned to agent m. For time τ and resource ω, the function nm(ψ, τ, ω) ∈ {0, 1} indicates whether the bundle in ψ uses resource ω at time τ (we make the assumption that agents have binary resource requirements).",
                "This allocation problem is NP-complete, even when considering only a single time step, and its difficulty increases significantly with multiple time steps because of the increasing number of values of ψ.",
                "The problem of finding an optimal allocation that satisfies the global constraint that the amount of each resource ω allocated to all agents does not exceed the available amount bϕ(ω) can be expressed as the following integer program: max X m∈M X ψ∈Ψm zψ mvψ m subject to: X ψ∈Ψm zψ m ≤ 1, ∀m ∈ M; X m∈M X ψ∈Ψm zψ mnm(ψ, τ, ω) ≤ bϕ(ω), ∀τ ∈ [1, bτ], ∀ω ∈ Ω; (3) The first constraint in equation 3 says that no agent can receive more than one bundle, and the second constraint ensures that the total assignment of resource ω does not, at any time, exceed the resource bound.",
                "For the scheduling problem where the agents are able to dynamically reallocate resources, each agent must specify a value for every combination of bundles and time steps within its time horizon.",
                "Let the variable ψ ∈ Ψm in this case enumerate all possible resource bundles for which at most one bundle may be assigned to agent m at each time step.",
                "Therefore, in this case there are P t∈[1,Tm](2|Ω| )t ∼ 2|Ω|Tm possibilities of resource bundles assigned to different time slots, for the Tm different time horizons.",
                "The same set of equations (3) can be used to solve this dynamic scheduling problem, but the integer program is different because of the difference in how ψ is defined.",
                "In this case, the number of ψ values is exponential in each agents planning horizon Tm, resulting in a much larger program.",
                "This straightforward approach to solving both of these scheduling problems requires an enumeration and solution of either 2|Ω| Tm (static allocation) or P t∈[1,Tm] 2|Ω|t (dynamic reallocation) MDPs for each agent, which very quickly becomes intractable with the growth of the number of resources |Ω| or the time horizon Tm. 3.",
                "MODEL AND PROBLEM STATEMENT We now formally introduce our model of the resourcescheduling problem.",
                "The problem input consists of the following components: • M, Ω, bϕ, τa m, τd m, bτ are as defined above in Section 2.2. • {Θm} = {S, A, pm, rm, αm} are the MDPs of all agents m ∈ M. Without loss of generality, we assume that state and action spaces of all agents are the same, but each has its own transition function pm, reward function rm, and initial conditions αm. • ϕm : A×Ω → {0, 1} is the mapping of actions to resources for agent m. ϕm(a, ω) indicates whether action a of agent m needs resource ω.",
                "An agent m that receives a set of resources that does not include resource ω cannot execute in its MDP policy any action a for which ϕm(a, ω) = 0.",
                "We assume all resource requirements are binary; as discussed below in Section 6, this assumption is not limiting.",
                "Given the above input, the optimization problem we consider is to find the globally optimal-maximizing the sum of expected rewards-mapping of resources to agents for all time steps: Δ : τ × M × Ω → {0, 1}.",
                "A solution is feasible if the corresponding assignment of resources to the agents does not violate the global resource constraint: X m Δm(τ, ω) ≤ bϕ(ω), ∀ω ∈ Ω, τ ∈ [1, bτ]. (4) We consider two flavors of the resource-scheduling problem.",
                "The first formulation restricts resource assignments to the space where the allocation of resources to each agent is static during the agents lifetime.",
                "The second formulation allows reassignment of resources between agents at every time step within their lifetimes.",
                "Figure 1 depicts a resource-scheduling problem with three agents M = {m1, m2, m3}, three resources Ω = {ω1, ω2, ω3}, and a global problem horizon of bτ = 11.",
                "The agents arrival and departure times are shown as gray boxes and are {1, 6}, {3, 7}, and {2, 11}, respectively.",
                "A solution to this problem is shown via horizontal bars within each agents box, where the bars correspond to the allocation of the three resource types.",
                "Figure 1a shows a solution to a static scheduling problem.",
                "According to the shown solution, agent m1 begins the execution of its MDP at time τ = 1 and has a lock on all three resources until it finishes execution at time τ = 3.",
                "Note that agent m1 relinquishes its hold on the resources before its announced departure time of τd m1 = 6, ostensibly because other agents can utilize the resources more effectively.",
                "Thus, at time τ = 4, resources ω1 and ω3 are allocated to agent m2, who then uses them to execute its MDP (using only actions supported by resources ω1 and ω3) until time τ = 7.",
                "Agent m3 holds resource ω3 during the interval τ ∈ [4, 10].",
                "Figure 1b shows a possible solution to the dynamic version of the same problem.",
                "There, resources can be reallocated between agents at every time step.",
                "For example, agent m1 gives up its use of resource ω2 at time τ = 2, although it continues the execution of its MDP until time τ = 6.",
                "Notice that an agent is not allowed to stop and restart its MDP, so agent m1 is only able to continue executing in the interval τ ∈ [3, 4] if it has actions that do not require any resources (ϕm(a, ω) = 0).",
                "Clearly, the model and problem statement described above make a number of assumptions about the problem and the desired solution properties.",
                "We discuss some of those assumptions and their implications in Section 6. 1222 The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) (a) (b) Figure 1: Illustration of a solution to a resource-scheduling problem with three agents and three resources: a) static resource assignments (resource assignments are constant within agents lifetimes; b) dynamic assignment (resource assignments are allowed to change at every time step). 4.",
                "RESOURCE SCHEDULING Our resource-scheduling algorithm proceeds in two stages.",
                "First, we perform a preprocessing step that augments the agent MDPs; this process is described in Section 4.1.",
                "Second, using these augmented MDPs we construct a global optimization problem, which is described in Section 4.2. 4.1 Augmenting Agents MDPs In the model described in the previous section, we assume that if an agent does not possess the necessary resources to perform actions in its MDP, its execution is halted and the agent leaves the system.",
                "In other words, the MDPs cannot be paused and resumed.",
                "For example, in the problem shown in Figure 1a, agent m1 releases all resources after time τ = 3, at which point the execution of its MDP is halted.",
                "Similarly, agents m2 and m3 only execute their MDPs in the intervals τ ∈ [4, 6] and τ ∈ [4, 10], respectively.",
                "Therefore, an important part of the global decision-making problem is to decide the window of time during which each of the agents is active (i.e., executing its MDP).",
                "To accomplish this, we augment each agents MDP with two new states (start and finish states sb , sf , respectively) and a new start/stop action a∗ , as illustrated in Figure 2.",
                "The idea is that an agent stays in the start state sb until it is ready to execute its MDP, at which point it performs the start/stop action a∗ and transitions into the state space of the original MDP with the transition probability that corresponds to the original initial distribution α(s).",
                "For example, in Figure 1a, for agent m2 this would happen at time τ = 4.",
                "Once the agent gets to the end of its activity window (time τ = 6 for agent m2 in Figure 1a), it performs the start/stop action, which takes it into the sink finish state sf at time τ = 7.",
                "More precisely, given an MDP S, A, pm, rm, αm , we define an augmented MDP S , A , pm, rm, αm as follows: S = S ∪ sb ∪ sf ; A = A ∪ a∗ ; p (s|sb , a∗ ) = α(s), ∀s ∈ S; p (sb |sb , a) = 1.0, ∀a ∈ A; p (sf |s, a∗ ) = 1.0, ∀s ∈ S; p (σ|s, a) = p(σ|s, a), ∀s, σ ∈ S, a ∈ A; r (sb , a) = r (sf , a) = 0, ∀a ∈ A ; r (s, a) = r(s, a), ∀s ∈ S, a ∈ A; α (sb ) = 1; α (s) = 0, ∀s ∈ S; where all non-specified transition probabilities are assumed to be zero.",
                "Further, in order to account for the new starting state, we begin the MDP one time-step earlier, setting τa m ← τa m − 1.",
                "This will not affect the <br>resource allocation</br> due to the resource constraints only being enforced for the original MDP states, as will be discussed in the next section.",
                "For example, the augmented MDPs shown in Figure 2b (which starts in state sb at time τ = 2) would be constructed from an MDP with original arrival time τ = 3.",
                "Figure 2b also shows a sample trajectory through the state space: the agent starts in state sb , transitions into the state space S of the original MDP, and finally exists into the sink state sf .",
                "Note that if we wanted to model a problem where agents could pause their MDPs at arbitrary time steps (which might be useful for domains where dynamic reallocation is possible), we could easily accomplish this by including an extra action that transitions from each state to itself with zero reward. 4.2 MILP for Resource Scheduling Given a set of augmented MDPs, as defined above, the goal of this section is to formulate a global optimization program that solves the resource-scheduling problem.",
                "In this section and below, all MDPs are assumed to be the augmented MDPs as defined in Section 4.1.",
                "Our approach is similar to the idea used in [6]: we begin with the linear-program formulation of agents MDPs (1) and augment it with constraints that ensure that the corresponding <br>resource allocation</br> across agents and time is valid.",
                "The resulting optimization problem then simultaneously solves the agents MDPs and resource-scheduling problems.",
                "In the rest of this section, we incrementally develop a mixed integer program (MILP) that achieves this.",
                "In the absence of resource constraints, the agents finitehorizon MDPs are completely independent, and the globally optimal solution can be trivially obtained via the following LP, which is simply an aggregation of single-agent finitehorizon LPs: max X m X s X a rm(s, a) X t xm(s, a, t) subject to: X a xm(σ, a, t + 1) = X s,a pm(σ|s, a)xm(s, a, t), ∀m ∈ M, σ ∈ S, t ∈ [1, Tm − 1]; X a xm(s, a, 1) = αm(s), ∀m ∈ M, s ∈ S; (12) where xm(s, a, t) is the occupation measure of agent m, and The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) 1223 (a) (b) Figure 2: Illustration of augmenting an MDP to allow for variable starting and stopping times: a) (left) the original two-state MDP with a single action; (right) the augmented MDP with new states sb and sf and the new action a∗ (note that the origianl transitions are not changed in the augmentation process); b) the augmented MDP displayed as a trajectory through time (grey lines indicate all transitions, while black lines indicate a given trajectory.",
                "Objective Function (sum of expected rewards over all agents) max X m X s X a rm(s, a) X t xm(s, a, t) (5) Meaning Implication Linear Constraints Tie x to θ.",
                "Agent is only active when occupation measure is nonzero in original MDP states. θm(τ) = 0 =⇒ xm(s, a, τ −τa m+1) = 0 ∀s /∈ {sb , sf }, a ∈ A X s/∈{sb,sf } X a xm(s, a, t) ≤ θm(τa m + t − 1) ∀m ∈ M, ∀t ∈ [1, Tm] (6) Agent can only be active in τ ∈ (τa m, τd m) θm(τ) = 0 ∀m ∈ M, τ /∈ (τa m, τd m) (7) Cannot use resources when not active θm(τ) = 0 =⇒ Δm(τ, ω) = 0 ∀τ ∈ [0, bτ], ω ∈ Ω Δm(τ, ω) ≤ θm(τ) ∀m ∈ M, τ ∈ [0, bτ], ω ∈ Ω (8) Tie x to Δ (nonzero x forces corresponding Δ to be nonzero.)",
                "Δm(τ, ω) = 0, ϕm(a, ω) = 1 =⇒ xm(s, a, τ − τa m + 1) = 0 ∀s /∈ {sb , sf } 1/|A| X a ϕm(a, ω) X s/∈{sb,sf } xm(s, a, t) ≤ Δm(t + τa m − 1, ω) ∀m ∈ M, ω ∈ Ω, t ∈ [1, Tm] (9) Resource bounds X m Δm(τ, ω) ≤ bϕ(ω) ∀ω ∈ Ω, τ ∈ [0, bτ] (10) Agent cannot change resources while active.",
                "Only enabled for scheduling with static assignments. θm(τ) = 1 and θm(τ + 1) = 1 =⇒ Δm(τ, ω) = Δm(τ + 1, ω) Δm(τ, ω) − Z(1 − θm(τ + 1)) ≤ Δm(τ + 1, ω) + Z(1 − θm(τ)) Δm(τ, ω) + Z(1 − θm(τ + 1)) ≥ Δm(τ + 1, ω) − Z(1 − θm(τ)) ∀m ∈ M, ω ∈ Ω, τ ∈ [0, bτ] (11) Table 1: MILP for globally optimal resource scheduling.",
                "Tm = τd m − τa m + 1 is the time horizon for the agents MDP.",
                "Using this LP as a basis, we augment it with constraints that ensure that the resource usage implied by the agents occupation measures {xm} does not violate the global resource requirements bϕ at any time step τ ∈ [0, bτ].",
                "To formulate these resource constraints, we use the following binary variables: • Δm(τ, ω) = {0, 1}, ∀m ∈ M, τ ∈ [0, bτ], ω ∈ Ω, which serve as indicator variables that define whether agent m possesses resource ω at time τ.",
                "These are analogous to the static indicator variables used in the one-shot static resource-allocation problem in [6]. • θm = {0, 1}, ∀m ∈ M, τ ∈ [0, bτ] are indicator variables that specify whether agent m is active (i.e., executing its MDP) at time τ.",
                "The meaning of resource-usage variables Δ is illustrated in Figure 1: Δm(τ, ω) = 1 only if resource ω is allocated to agent m at time τ.",
                "The meaning of the activity indicators θ is illustrated in Figure 2b: when agent m is in either the start state sb or the finish state sf , the corresponding θm = 0, but once the agent becomes active and enters one of the other states, we set θm = 1 .",
                "This meaning of θ can be enforced with a linear constraint that synchronizes the values of the agents occupation measures xm and the activity 1224 The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) indicators θ, as shown in (6) in Table 1.",
                "Another constraint we have to add-because the activity indicators θ are defined on the global timeline τ-is to enforce the fact that the agent is inactive outside of its arrivaldeparture window.",
                "This is accomplished by constraint (7) in Table 1.",
                "Furthermore, agents should not be using resources while they are inactive.",
                "This constraint can also be enforced via a linear inequality on θ and Δ, as shown in (8).",
                "Constraint (6) sets the value of θ to match the policy defined by the occupation measure xm.",
                "In a similar fashion, we have to make sure that the resource-usage variables Δ are also synchronized with the occupation measure xm.",
                "This is done via constraint (9) in Table 1, which is nearly identical to the analogous constraint from [6].",
                "After implementing the above constraint, which enforces the meaning of Δ, we add a constraint that ensures that the agents resource usage never exceeds the amounts of available resources.",
                "This condition is also trivially expressed as a linear inequality (10) in Table 1.",
                "Finally, for the problem formulation where resource assignments are static during a lifetime of an agent, we add a constraint that ensures that the resource-usage variables Δ do not change their value while the agent is active (θ = 1).",
                "This is accomplished via the linear constraint (11), where Z ≥ 2 is a constant that is used to turn off the constraints when θm(τ) = 0 or θm(τ + 1) = 0.",
                "This constraint is not used for the dynamic problem formulation, where resources can be reallocated between agents at every time step.",
                "To summarize, Table 1 together with the conservationof-flow constraints from (12) defines the MILP that simultaneously computes an optimal resource assignment for all agents across time as well as optimal finite-horizon MDP policies that are valid under that resource assignment.",
                "As a rough measure of the complexity of this MILP, let us consider the number of optimization variables and constraints.",
                "Let TM = P Tm = P m(τa m − τd m + 1) be the sum of the lengths of the arrival-departure windows across all agents.",
                "Then, the number of optimization variables is: TM + bτ|M||Ω| + bτ|M|, TM of which are continuous (xm), and bτ|M||Ω| + bτ|M| are binary (Δ and θ).",
                "However, notice that all but TM|M| of the θ are set to zero by constraint (7), which also immediately forces all but TM|M||Ω| of the Δ to be zero via the constraints (8).",
                "The number of constraints (not including the degenerate constraints in (7)) in the MILP is: TM + TM|Ω| + bτ|Ω| + bτ|M||Ω|.",
                "Despite the fact that the complexity of the MILP is, in the worst case, exponential1 in the number of binary variables, the complexity of this MILP is significantly (exponentially) lower than that of the MILP with flat utility functions, described in Section 2.2.",
                "This result echos the efficiency gains reported in [6] for single-shot resource-allocation problems, but is much more pronounced, because of the explosion of the flat utility representation due to the temporal aspect of the problem (recall the prohibitive complexity of the combinatorial optimization in Section 2.2).",
                "We empirically analyze the performance of this method in Section 5. 1 Strictly speaking, solving MILPs to optimality is NPcomplete in the number of integer variables. 5.",
                "EXPERIMENTAL RESULTS Although the complexity of solving MILPs is in the worst case exponential in the number of integer variables, there are many efficient methods for solving MILPs that allow our algorithm to scale well for parameters common to <br>resource allocation</br> and scheduling problems.",
                "In particular, this section introduces a problem domain-the repairshop problem-used to empirically evaluate our algorithms scalability in terms of the number of agents |M|, the number of shared resources |Ω|, and the varied lengths of global time bτ during which agents may enter and exit the system.",
                "The repairshop problem is a simple parameterized MDP adopting the metaphor of a vehicular repair shop.",
                "Agents in the repair shop are mechanics with a number of independent tasks that yield reward only when completed.",
                "In our MDP model of this system, actions taken to advance through the state space are only allowed if the agent holds certain resources that are publicly available to the shop.",
                "These resources are in finite supply, and optimal policies for the shop will determine when each agent may hold the limited resources to take actions and earn individual rewards.",
                "Each task to be completed is associated with a single action, although the agent is required to repeat the action numerous times before completing the task and earning a reward.",
                "This model was parameterized in terms of the number of agents in the system, the number of different types of resources that could be linked to necessary actions, a global time during which agents are allowed to arrive and depart, and a maximum length for the number of time steps an agent may remain in the system.",
                "All datapoints in our experiments were obtained with 20 evaluations using CPLEX to solve the MILPs on a Pentium4 computer with 2Gb of RAM.",
                "Trials were conducted on both the static and the dynamic version of the resourcescheduling problem, as defined earlier.",
                "Figure 3 shows the runtime and policy value for independent modifications to the parameter set.",
                "The top row shows how the solution time for the MILP scales as we increase the number of agents |M|, the global time horizon bτ, and the number of resources |Ω|.",
                "Increasing the number of agents leads to exponential complexity scaling, which is to be expected for an NP-complete problem.",
                "However, increasing the global time limit bτ or the total number of resource types |Ω|-while holding the number of agents constantdoes not lead to decreased performance.",
                "This occurs because the problems get easier as they become under-constrained, which is also a common phenomenon for NP-complete problems.",
                "We also observe that the solution to the dynamic version of the problem can often be computed much faster than the static version.",
                "The bottom row of Figure 3 shows the joint policy value of the policies that correspond to the computed optimal resource-allocation schedules.",
                "We can observe that the dynamic version yields higher reward (as expected, since the reward for the dynamic version is always no less than the reward of the static version).",
                "We should point out that these graphs should not be viewed as a measure of performance of two different algorithms (both algorithms produce optimal solutions but to different problems), but rather as observations about how the quality of optimal solutions change as more flexibility is allowed in the reallocation of resources.",
                "Figure 4 shows runtime and policy value for trials in which common input variables are scaled together.",
                "This allows The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) 1225 2 4 6 8 10 10 −3 10 −2 10 −1 10 0 10 1 10 2 10 3 10 4 Number of Agents |M| CPUTime,sec |Ω| = 5, τ = 50 static dynamic 50 100 150 200 10 −2 10 −1 10 0 10 1 10 2 10 3 Global Time Boundary τ CPUTime,sec |M| = 5, |Ω| = 5 static dynamic 10 20 30 40 50 10 −2 10 −1 10 0 10 1 10 2 Number of Resources |Ω| CPUTime,sec |M| = 5, τ = 50 static dynamic 2 4 6 8 10 200 400 600 800 1000 1200 1400 1600 Number of Agents |M| Value |Ω| = 5, τ = 50 static dynamic 50 100 150 200 400 500 600 700 800 900 1000 1100 1200 1300 1400 Global Time Boundary τ Value |M| = 5, |Ω| = 5 static dynamic 10 20 30 40 50 500 600 700 800 900 1000 1100 1200 1300 1400 Number of Resources |Ω| Value |M| = 5, τ = 50 static dynamic Figure 3: Evaluation of our MILP for variable numbers of agents (column 1), lengths of global-time window (column 2), and numbers of resource types (column 3).",
                "Top row shows CPU time, and bottom row shows the joint reward of agents MDP policies.",
                "Error bars show the 1st and 3rd quartiles (25% and 75%). 2 4 6 8 10 10 −3 10 −2 10 −1 10 0 10 1 10 2 10 3 Number of Agents |M| CPUTime,sec τ = 10|M| static dynamic 2 4 6 8 10 10 −3 10 −2 10 −1 10 0 10 1 10 2 10 3 10 4 Number of Agents |M| CPUTime,sec |Ω| = 2|M| static dynamic 2 4 6 8 10 10 −3 10 −2 10 −1 10 0 10 1 10 2 10 3 10 4 Number of Agents |M| CPUTime,sec |Ω| = 5|M| static dynamic 2 4 6 8 10 200 400 600 800 1000 1200 1400 1600 1800 2000 2200 Number of Agents |M| Value τ = 10|M| static dynamic 2 4 6 8 10 200 400 600 800 1000 1200 1400 1600 1800 2000 Number of Agents |M| Value |Ω| = 2|M| static dynamic 2 4 6 8 10 0 500 1000 1500 2000 2500 Number of Agents |M| Value |Ω| = 5|M| static dynamic Figure 4: Evaluation of our MILP using correlated input variables.",
                "The left column tracks the performance and CPU time as the number of agents and global-time window increase together (bτ = 10|M|).",
                "The middle and the right column track the performance and CPU time as the number of resources and the number of agents increase together as |Ω| = 2|M| and |Ω| = 5|M|, respectively.",
                "Error bars show the 1st and 3rd quartiles (25% and 75%). 1226 The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) us to explore domains where the total number of agents scales proportionally to the total number of resource types or the global time horizon, while keeping constant the average agent density (per unit of global time) or the average number of resources per agent (which commonly occurs in real-life applications).",
                "Overall, we believe that these experimental results indicate that our MILP formulation can be used to effectively solve resource-scheduling problems of nontrivial size. 6.",
                "DISCUSSION AND CONCLUSIONS Throughout the paper, we have made a number of assumptions in our model and solution algorithm; we discuss their implications below. • Continual execution.",
                "We assume that once an agent stops executing its MDP (transitions into state sf ), it exits the system and cannot return.",
                "It is easy to relax this assumption for domains where agents MDPs can be paused and restarted.",
                "All that is required is to include an additional pause action which transitions from a given state back to itself, and has zero reward. • Indifference to start time.",
                "We used a reward model where agents rewards depend only on the time horizon of their MDPs and not the global start time.",
                "This is a consequence of our MDP-augmentation procedure from Section 4.1.",
                "It is easy to extend the model so that the agents incur an explicit penalty for idling by assigning a non-zero negative reward to the start state sb . • Binary resource requirements.",
                "For simplicity, we have assumed that resource costs are binary: ϕm(a, ω) = {0, 1}, but our results generalize in a straightforward manner to non-binary resource mappings, analogously to the procedure used in [5]. • Cooperative agents.",
                "The optimization procedure discussed in this paper was developed in the context of cooperative agents, but it can also be used to design a mechanism for scheduling resources among self-interested agents.",
                "This optimization procedure can be embedded in a VickreyClarke-Groves auction, completely analogously to the way it was done in [7].",
                "In fact, all the results of [7] about the properties of the auction and information privacy directly carry over to the scheduling domain discussed in this paper, requiring only slight modifications to deal with finitehorizon MDPs. • Known, deterministic arrival and departure times.",
                "Finally, we have assumed that agents arrival and departure times (τa m and τd m) are deterministic and known a priori.",
                "This assumption is fundamental to our solution method.",
                "While there are many domains where this assumption is valid, in many cases agents arrive and depart dynamically and their arrival and departure times can only be predicted probabilistically, leading to online resource-allocation problems.",
                "In particular, in the case of self-interested agents, this becomes an interesting version of an online-mechanism-design problem [11, 12].",
                "In summary, we have presented an MILP formulation for the combinatorial resource-scheduling problem where agents values for possible resource assignments are defined by finitehorizon MDPs.",
                "This result extends previous work ([6, 7]) on static one-shot <br>resource allocation</br> under MDP-induced preferences to resource-scheduling problems with a temporal aspect.",
                "As such, this work takes a step in the direction of designing an online mechanism for agents with combinatorial resource preferences induced by stochastic planning problems.",
                "Relaxing the assumption about deterministic arrival and departure times of the agents is a focus of our future work.",
                "We would like to thank the anonymous reviewers for their insightful comments and suggestions. 7.",
                "REFERENCES [1] E. Altman and A. Shwartz.",
                "Adaptive control of constrained Markov chains: Criteria and policies.",
                "Annals of Operations Research, special issue on Markov Decision Processes, 28:101-134, 1991. [2] R. Bellman.",
                "Dynamic Programming.",
                "Princeton University Press, 1957. [3] C. Boutilier.",
                "Solving concisely expressed combinatorial auction problems.",
                "In Proc. of AAAI-02, pages 359-366, 2002. [4] C. Boutilier and H. H. Hoos.",
                "Bidding languages for combinatorial auctions.",
                "In Proc. of IJCAI-01, pages 1211-1217, 2001. [5] D. Dolgov.",
                "Integrated <br>resource allocation</br> and Planning in Stochastic Multiagent Environments.",
                "PhD thesis, Computer Science Department, University of Michigan, February 2006. [6] D. A. Dolgov and E. H. Durfee.",
                "Optimal <br>resource allocation</br> and policy formulation in loosely-coupled Markov decision processes.",
                "In Proc. of ICAPS-04, pages 315-324, June 2004. [7] D. A. Dolgov and E. H. Durfee.",
                "Computationally efficient combinatorial auctions for <br>resource allocation</br> in weakly-coupled MDPs.",
                "In Proc. of AAMAS-05, New York, NY, USA, 2005.",
                "ACM Press. [8] D. A. Dolgov and E. H. Durfee.",
                "<br>resource allocation</br> among agents with preferences induced by factored MDPs.",
                "In Proc. of AAMAS-06, 2006. [9] K. Larson and T. Sandholm.",
                "Mechanism design and deliberative agents.",
                "In Proc. of AAMAS-05, pages 650-656, New York, NY, USA, 2005.",
                "ACM Press. [10] N. Nisan.",
                "Bidding and allocation in combinatorial auctions.",
                "In Electronic Commerce, 2000. [11] D. C. Parkes and S. Singh.",
                "An MDP-based approach to Online Mechanism Design.",
                "In Proc. of the Seventeenths Annual Conference on Neural Information Processing Systems (NIPS-03), 2003. [12] D. C. Parkes, S. Singh, and D. Yanovsky.",
                "Approximately efficient online mechanism design.",
                "In Proc. of the Eighteenths Annual Conference on Neural Information Processing Systems (NIPS-04), 2004. [13] M. L. Puterman.",
                "Markov Decision Processes.",
                "John Wiley & Sons, New York, 1994. [14] M. H. Rothkopf, A. Pekec, and R. M. Harstad.",
                "Computationally manageable combinational auctions.",
                "Management Science, 44(8):1131-1147, 1998. [15] T. Sandholm.",
                "An algorithm for optimal winner determination in combinatorial auctions.",
                "In Proc. of IJCAI-99, pages 542-547, San Francisco, CA, USA, 1999.",
                "Morgan Kaufmann Publishers Inc.",
                "The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) 1227"
            ],
            "original_annotated_samples": [],
            "translated_annotated_samples": [
                "INTRODUCCIÓN Las tareas de \"asignación de recursos\" óptimas son ubicuas en sistemas multiagentes, pero resolver tales problemas de optimización puede ser computacionalmente difícil, debido a una serie de factores.Asignación de recursos",
                "Una alternativa es modelar directamente los mecanismos que definen las funciones de utilidad de los agentes y realizar la \"asignación de recursos\" directamente con estos modelos [9].Asignación de recursos",
                "Sin embargo, este trabajo existente sobre \"asignación de recursos\" con preferencias inducidas por los MDP parametrizados por recursos supone que los recursos solo se asignan una vez y luego son utilizados por los agentes de forma independiente dentro de sus MDP infinitos.Asignación de recursos",
                "En este documento, ampliamos el trabajo en la \"asignación de recursos\" bajo las preferencias inducidas por MDP a problemas de programación de tiempo discreto, donde los agentes están presentes en el sistema para intervalos de tiempo finitos y solo pueden usar recursos dentro de estos intervalos.Asignación de recursos",
                "Esto no afectará la \"asignación de recursos\" debido a que las limitaciones de recursos solo se aplican para los estados originales de MDP, como se discutirá en la siguiente sección.Asignación de recursos",
                "Nuestro enfoque es similar a la idea utilizada en [6]: comenzamos con la formulación lineal del programa de agentes MDP (1) y la aumentamos con limitaciones que aseguran que la \"asignación de recursos\" correspondiente entre los agentes y el tiempo sea válida.Asignación de recursos",
                "Resultados experimentales Aunque la complejidad de resolver MILP es en el peor de los casos exponencial en el número de variables enteras, existen muchos métodos eficientes para resolver MILP que permiten que nuestro algoritmo escala bien para los parámetros comunes a la \"asignación de recursos\" y los problemas de programación.Asignación de recursos",
                "Este resultado extiende el trabajo anterior ([6, 7]) en la \"asignación de recursos\" estática de un solo disparo bajo las preferencias inducidas por MDP a problemas de programación de recursos con un aspecto temporal.Asignación de recursos",
                "\"Asignación de recursos\" integrada en entornos estocásticos multiagentes.Asignación de recursos",
                "La \"asignación de recursos\" óptima y la formulación de políticas en los procesos de decisión de Markov libremente acoplados.Asignación de recursos",
                "Subastas combinatorias computacionalmente eficientes para la \"asignación de recursos\" en MDP débilmente acoplados.Asignación de recursos",
                "\"Asignación de recursos\" entre los agentes con preferencias inducidas por los MDP factorizados.Asignación de recursos"
            ],
            "translated_text": "",
            "candidates": [],
            "error": []
        },
        "scheduling": {
            "translated_key": "",
            "is_in_text": true,
            "original_annotated_sentences": [
                "Combinatorial Resource <br>scheduling</br> for Multiagent MDPs Dmitri A. Dolgov, Michael R. James, and Michael E. Samples AI and Robotics Group Technical Research, Toyota Technical Center USA {ddolgov, michael.r.james, michael.samples}@gmail.com ABSTRACT Optimal resource <br>scheduling</br> in multiagent systems is a computationally challenging task, particularly when the values of resources are not additive.",
                "We consider the combinatorial problem of <br>scheduling</br> the usage of multiple resources among agents that operate in stochastic environments, modeled as Markov decision processes (MDPs).",
                "In recent years, efficient resource-allocation algorithms have been developed for agents with resource values induced by MDPs.",
                "However, this prior work has focused on static resource-allocation problems where resources are distributed once and then utilized in infinite-horizon MDPs.",
                "We extend those existing models to the problem of combinatorial resource <br>scheduling</br>, where agents persist only for finite periods between their (predefined) arrival and departure times, requiring resources only for those time periods.",
                "We provide a computationally efficient procedure for computing globally optimal resource assignments to agents over time.",
                "We illustrate and empirically analyze the method in the context of a stochastic jobscheduling domain.",
                "Categories and Subject Descriptors I.2.8 [Artificial Intelligence]: Problem Solving, Control Methods, and Search; I.2.11 [Artificial Intelligence]: Distributed Artificial Intelligence-Multiagent systems General Terms Algorithms, Performance, Design 1.",
                "INTRODUCTION The tasks of optimal resource allocation and <br>scheduling</br> are ubiquitous in multiagent systems, but solving such optimization problems can be computationally difficult, due to a number of factors.",
                "In particular, when the value of a set of resources to an agent is not additive (as is often the case with resources that are substitutes or complements), the utility function might have to be defined on an exponentially large space of resource bundles, which very quickly becomes computationally intractable.",
                "Further, even when each agent has a utility function that is nonzero only on a small subset of the possible resource bundles, obtaining optimal allocation is still computationally prohibitive, as the problem becomes NP-complete [14].",
                "Such computational issues have recently spawned several threads of work in using compact models of agents preferences.",
                "One idea is to use any structure present in utility functions to represent them compactly, via, for example, logical formulas [15, 10, 4, 3].",
                "An alternative is to directly model the mechanisms that define the agents utility functions and perform resource allocation directly with these models [9].",
                "A way of accomplishing this is to model the processes by which an agent might utilize the resources and define the utility function as the payoff of these processes.",
                "In particular, if an agent uses resources to act in a stochastic environment, its utility function can be naturally modeled with a Markov decision process, whose action set is parameterized by the available resources.",
                "This representation can then be used to construct very efficient resource-allocation algorithms that lead to an exponential speedup over a straightforward optimization problem with flat representations of combinatorial preferences [6, 7, 8].",
                "However, this existing work on resource allocation with preferences induced by resource-parameterized MDPs makes an assumption that the resources are only allocated once and are then utilized by the agents independently within their infinite-horizon MDPs.",
                "This assumption that no reallocation of resources is possible can be limiting in domains where agents arrive and depart dynamically.",
                "In this paper, we extend the work on resource allocation under MDP-induced preferences to discrete-time <br>scheduling</br> problems, where agents are present in the system for finite time intervals and can only use resources within these intervals.",
                "In particular, agents arrive and depart at arbitrary (predefined) times and within these intervals use resources to execute tasks in finite-horizon MDPs.",
                "We address the problem of globally optimal resource <br>scheduling</br>, where the objective is to find an allocation of resources to the agents across time that maximizes the sum of the expected rewards that they obtain.",
                "In this context, our main contribution is a mixed-integerprogramming formulation of the <br>scheduling</br> problem that chooses globally optimal resource assignments, starting times, and execution horizons for all agents (within their arrival1220 978-81-904262-7-5 (RPS) c 2007 IFAAMAS departure intervals).",
                "We analyze and empirically compare two flavors of the <br>scheduling</br> problem: one, where agents have static resource assignments within their finite-horizon MDPs, and another, where resources can be dynamically reallocated between agents at every time step.",
                "In the rest of the paper, we first lay down the necessary groundwork in Section 2 and then introduce our model and formal problem statement in Section 3.",
                "In Section 4.2, we describe our main result, the optimization program for globally optimal resource <br>scheduling</br>.",
                "Following the discussion of our experimental results on a job-<br>scheduling</br> problem in Section 5, we conclude in Section 6 with a discussion of possible extensions and generalizations of our method. 2.",
                "BACKGROUND Similarly to the model used in previous work on resourceallocation with MDP-induced preferences [6, 7], we define the value of a set of resources to an agent as the value of the best MDP policy that is realizable, given those resources.",
                "However, since the focus of our work is on <br>scheduling</br> problems, and a large part of the optimization problem is to decide how resources are allocated in time among agents with finite arrival and departure times, we model the agents planning problems as finite-horizon MDPs, in contrast to previous work that used infinite-horizon discounted MDPs.",
                "In the rest of this section, we first introduce some necessary background on finite-horizon MDPs and present a linear-programming formulation that serves as the basis for our solution algorithm developed in Section 4.",
                "We also outline the standard methods for combinatorial resource <br>scheduling</br> with flat resource values, which serve as a comparison benchmark for the new model developed here. 2.1 Markov Decision Processes A stationary, finite-domain, discrete-time MDP (see, for example, [13] for a thorough and detailed development) can be described as S, A, p, r , where: S is a finite set of system states; A is a finite set of actions that are available to the agent; p is a stationary stochastic transition function, where p(σ|s, a) is the probability of transitioning to state σ upon executing action a in state s; r is a stationary reward function, where r(s, a) specifies the reward obtained upon executing action a in state s. Given such an MDP, a decision problem under a finite horizon T is to choose an optimal action at every time step to maximize the expected value of the total reward accrued during the agents (finite) lifetime.",
                "The agents optimal policy is then a function of current state s and the time until the horizon.",
                "An optimal policy for such a problem is to act greedily with respect to the optimal value function, defined recursively by the following system of finite-time Bellman equations [2]: v(s, t) = max a r(s, a) + X σ p(σ|s, a)v(σ, t + 1), ∀s ∈ S, t ∈ [1, T − 1]; v(s, T) = 0, ∀s ∈ S; where v(s, t) is the optimal value of being in state s at time t ∈ [1, T].",
                "This optimal value function can be easily computed using dynamic programming, leading to the following optimal policy π, where π(s, a, t) is the probability of executing action a in state s at time t: π(s, a, t) = ( 1, a = argmaxa r(s, a) + P σ p(σ|s, a)v(σ, t + 1), 0, otherwise.",
                "The above is the most common way of computing the optimal value function (and therefore an optimal policy) for a finite-horizon MDP.",
                "However, we can also formulate the problem as the following linear program (similarly to the dual LP for infinite-horizon discounted MDPs [13, 6, 7]): max X s X a r(s, a) X t x(s, a, t) subject to: X a x(σ, a, t + 1) = X s,a p(σ|s, a)x(s, a, t) ∀σ, t ∈ [1, T − 1]; X a x(s, a, 1) = α(s), ∀s ∈ S; (1) where α(s) is the initial distribution over the state space, and x is the (non-stationary) occupation measure (x(s, a, t) ∈ [0, 1] is the total expected number of times action a is executed in state s at time t).",
                "An optimal (non-stationary) policy is obtained from the occupation measure as follows: π(s, a, t) = x(s, a, t)/ X a x(s, a, t) ∀s ∈ S, t ∈ [1, T]. (2) Note that the standard unconstrained finite-horizon MDP, as described above, always has a uniformly-optimal solution (optimal for any initial distribution α(s)).",
                "Therefore, an optimal policy can be obtained by using an arbitrary constant α(s) > 0 (in particular, α(s) = 1 will result in x(s, a, t) = π(s, a, t)).",
                "However, for MDPs with resource constraints (as defined below in Section 3), uniformly-optimal policies do not in general exist.",
                "In such cases, α becomes a part of the problem input, and a resulting policy is only optimal for that particular α.",
                "This result is well known for infinite-horizon MDPs with various types of constraints [1, 6], and it also holds for our finite-horizon model, which can be easily established via a line of reasoning completely analogous to the arguments in [6]. 2.2 Combinatorial Resource <br>scheduling</br> A straightforward approach to resource <br>scheduling</br> for a set of agents M, whose values for the resources are induced by stochastic planning problems (in our case, finite-horizon MDPs) would be to have each agent enumerate all possible resource assignments over time and, for each one, compute its value by solving the corresponding MDP.",
                "Then, each agent would provide valuations for each possible resource bundle over time to a centralized coordinator, who would compute the optimal resource assignments across time based on these valuations.",
                "When resources can be allocated at different times to different agents, each agent must submit valuations for every combination of possible time horizons.",
                "Let each agent m ∈ M execute its MDP within the arrival-departure time interval τ ∈ [τa m, τd m].",
                "Hence, agent m will execute an MDP with time horizon no greater than Tm = τd m−τa m+1.",
                "Let bτ be the global time horizon for the problem, before which all of the agents MDPs must finish.",
                "We assume τd m < bτ, ∀m ∈ M. The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) 1221 For the <br>scheduling</br> problem where agents have static resource requirements within their finite-horizon MDPs, the agents provide a valuation for each resource bundle for each possible time horizon (from [1, Tm]) that they may use.",
                "Let Ω be the set of resources to be allocated among the agents.",
                "An agent will get at most one resource bundle for one of the time horizons.",
                "Let the variable ψ ∈ Ψm enumerate all possible pairs of resource bundles and time horizons for agent m, so there are 2|Ω| × Tm values for ψ (the space of bundles is exponential in the number of resource types |Ω|).",
                "The agent m must provide a value vψ m for each ψ, and the coordinator will allocate at most one ψ (resource, time horizon) pair to each agent.",
                "This allocation is expressed as an indicator variable zψ m ∈ {0, 1} that shows whether ψ is assigned to agent m. For time τ and resource ω, the function nm(ψ, τ, ω) ∈ {0, 1} indicates whether the bundle in ψ uses resource ω at time τ (we make the assumption that agents have binary resource requirements).",
                "This allocation problem is NP-complete, even when considering only a single time step, and its difficulty increases significantly with multiple time steps because of the increasing number of values of ψ.",
                "The problem of finding an optimal allocation that satisfies the global constraint that the amount of each resource ω allocated to all agents does not exceed the available amount bϕ(ω) can be expressed as the following integer program: max X m∈M X ψ∈Ψm zψ mvψ m subject to: X ψ∈Ψm zψ m ≤ 1, ∀m ∈ M; X m∈M X ψ∈Ψm zψ mnm(ψ, τ, ω) ≤ bϕ(ω), ∀τ ∈ [1, bτ], ∀ω ∈ Ω; (3) The first constraint in equation 3 says that no agent can receive more than one bundle, and the second constraint ensures that the total assignment of resource ω does not, at any time, exceed the resource bound.",
                "For the <br>scheduling</br> problem where the agents are able to dynamically reallocate resources, each agent must specify a value for every combination of bundles and time steps within its time horizon.",
                "Let the variable ψ ∈ Ψm in this case enumerate all possible resource bundles for which at most one bundle may be assigned to agent m at each time step.",
                "Therefore, in this case there are P t∈[1,Tm](2|Ω| )t ∼ 2|Ω|Tm possibilities of resource bundles assigned to different time slots, for the Tm different time horizons.",
                "The same set of equations (3) can be used to solve this dynamic <br>scheduling</br> problem, but the integer program is different because of the difference in how ψ is defined.",
                "In this case, the number of ψ values is exponential in each agents planning horizon Tm, resulting in a much larger program.",
                "This straightforward approach to solving both of these <br>scheduling</br> problems requires an enumeration and solution of either 2|Ω| Tm (static allocation) or P t∈[1,Tm] 2|Ω|t (dynamic reallocation) MDPs for each agent, which very quickly becomes intractable with the growth of the number of resources |Ω| or the time horizon Tm. 3.",
                "MODEL AND PROBLEM STATEMENT We now formally introduce our model of the resourcescheduling problem.",
                "The problem input consists of the following components: • M, Ω, bϕ, τa m, τd m, bτ are as defined above in Section 2.2. • {Θm} = {S, A, pm, rm, αm} are the MDPs of all agents m ∈ M. Without loss of generality, we assume that state and action spaces of all agents are the same, but each has its own transition function pm, reward function rm, and initial conditions αm. • ϕm : A×Ω → {0, 1} is the mapping of actions to resources for agent m. ϕm(a, ω) indicates whether action a of agent m needs resource ω.",
                "An agent m that receives a set of resources that does not include resource ω cannot execute in its MDP policy any action a for which ϕm(a, ω) = 0.",
                "We assume all resource requirements are binary; as discussed below in Section 6, this assumption is not limiting.",
                "Given the above input, the optimization problem we consider is to find the globally optimal-maximizing the sum of expected rewards-mapping of resources to agents for all time steps: Δ : τ × M × Ω → {0, 1}.",
                "A solution is feasible if the corresponding assignment of resources to the agents does not violate the global resource constraint: X m Δm(τ, ω) ≤ bϕ(ω), ∀ω ∈ Ω, τ ∈ [1, bτ]. (4) We consider two flavors of the resource-<br>scheduling</br> problem.",
                "The first formulation restricts resource assignments to the space where the allocation of resources to each agent is static during the agents lifetime.",
                "The second formulation allows reassignment of resources between agents at every time step within their lifetimes.",
                "Figure 1 depicts a resource-<br>scheduling</br> problem with three agents M = {m1, m2, m3}, three resources Ω = {ω1, ω2, ω3}, and a global problem horizon of bτ = 11.",
                "The agents arrival and departure times are shown as gray boxes and are {1, 6}, {3, 7}, and {2, 11}, respectively.",
                "A solution to this problem is shown via horizontal bars within each agents box, where the bars correspond to the allocation of the three resource types.",
                "Figure 1a shows a solution to a static <br>scheduling</br> problem.",
                "According to the shown solution, agent m1 begins the execution of its MDP at time τ = 1 and has a lock on all three resources until it finishes execution at time τ = 3.",
                "Note that agent m1 relinquishes its hold on the resources before its announced departure time of τd m1 = 6, ostensibly because other agents can utilize the resources more effectively.",
                "Thus, at time τ = 4, resources ω1 and ω3 are allocated to agent m2, who then uses them to execute its MDP (using only actions supported by resources ω1 and ω3) until time τ = 7.",
                "Agent m3 holds resource ω3 during the interval τ ∈ [4, 10].",
                "Figure 1b shows a possible solution to the dynamic version of the same problem.",
                "There, resources can be reallocated between agents at every time step.",
                "For example, agent m1 gives up its use of resource ω2 at time τ = 2, although it continues the execution of its MDP until time τ = 6.",
                "Notice that an agent is not allowed to stop and restart its MDP, so agent m1 is only able to continue executing in the interval τ ∈ [3, 4] if it has actions that do not require any resources (ϕm(a, ω) = 0).",
                "Clearly, the model and problem statement described above make a number of assumptions about the problem and the desired solution properties.",
                "We discuss some of those assumptions and their implications in Section 6. 1222 The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) (a) (b) Figure 1: Illustration of a solution to a resource-<br>scheduling</br> problem with three agents and three resources: a) static resource assignments (resource assignments are constant within agents lifetimes; b) dynamic assignment (resource assignments are allowed to change at every time step). 4.",
                "RESOURCE <br>scheduling</br> Our resource-<br>scheduling</br> algorithm proceeds in two stages.",
                "First, we perform a preprocessing step that augments the agent MDPs; this process is described in Section 4.1.",
                "Second, using these augmented MDPs we construct a global optimization problem, which is described in Section 4.2. 4.1 Augmenting Agents MDPs In the model described in the previous section, we assume that if an agent does not possess the necessary resources to perform actions in its MDP, its execution is halted and the agent leaves the system.",
                "In other words, the MDPs cannot be paused and resumed.",
                "For example, in the problem shown in Figure 1a, agent m1 releases all resources after time τ = 3, at which point the execution of its MDP is halted.",
                "Similarly, agents m2 and m3 only execute their MDPs in the intervals τ ∈ [4, 6] and τ ∈ [4, 10], respectively.",
                "Therefore, an important part of the global decision-making problem is to decide the window of time during which each of the agents is active (i.e., executing its MDP).",
                "To accomplish this, we augment each agents MDP with two new states (start and finish states sb , sf , respectively) and a new start/stop action a∗ , as illustrated in Figure 2.",
                "The idea is that an agent stays in the start state sb until it is ready to execute its MDP, at which point it performs the start/stop action a∗ and transitions into the state space of the original MDP with the transition probability that corresponds to the original initial distribution α(s).",
                "For example, in Figure 1a, for agent m2 this would happen at time τ = 4.",
                "Once the agent gets to the end of its activity window (time τ = 6 for agent m2 in Figure 1a), it performs the start/stop action, which takes it into the sink finish state sf at time τ = 7.",
                "More precisely, given an MDP S, A, pm, rm, αm , we define an augmented MDP S , A , pm, rm, αm as follows: S = S ∪ sb ∪ sf ; A = A ∪ a∗ ; p (s|sb , a∗ ) = α(s), ∀s ∈ S; p (sb |sb , a) = 1.0, ∀a ∈ A; p (sf |s, a∗ ) = 1.0, ∀s ∈ S; p (σ|s, a) = p(σ|s, a), ∀s, σ ∈ S, a ∈ A; r (sb , a) = r (sf , a) = 0, ∀a ∈ A ; r (s, a) = r(s, a), ∀s ∈ S, a ∈ A; α (sb ) = 1; α (s) = 0, ∀s ∈ S; where all non-specified transition probabilities are assumed to be zero.",
                "Further, in order to account for the new starting state, we begin the MDP one time-step earlier, setting τa m ← τa m − 1.",
                "This will not affect the resource allocation due to the resource constraints only being enforced for the original MDP states, as will be discussed in the next section.",
                "For example, the augmented MDPs shown in Figure 2b (which starts in state sb at time τ = 2) would be constructed from an MDP with original arrival time τ = 3.",
                "Figure 2b also shows a sample trajectory through the state space: the agent starts in state sb , transitions into the state space S of the original MDP, and finally exists into the sink state sf .",
                "Note that if we wanted to model a problem where agents could pause their MDPs at arbitrary time steps (which might be useful for domains where dynamic reallocation is possible), we could easily accomplish this by including an extra action that transitions from each state to itself with zero reward. 4.2 MILP for Resource <br>scheduling</br> Given a set of augmented MDPs, as defined above, the goal of this section is to formulate a global optimization program that solves the resource-<br>scheduling</br> problem.",
                "In this section and below, all MDPs are assumed to be the augmented MDPs as defined in Section 4.1.",
                "Our approach is similar to the idea used in [6]: we begin with the linear-program formulation of agents MDPs (1) and augment it with constraints that ensure that the corresponding resource allocation across agents and time is valid.",
                "The resulting optimization problem then simultaneously solves the agents MDPs and resource-<br>scheduling</br> problems.",
                "In the rest of this section, we incrementally develop a mixed integer program (MILP) that achieves this.",
                "In the absence of resource constraints, the agents finitehorizon MDPs are completely independent, and the globally optimal solution can be trivially obtained via the following LP, which is simply an aggregation of single-agent finitehorizon LPs: max X m X s X a rm(s, a) X t xm(s, a, t) subject to: X a xm(σ, a, t + 1) = X s,a pm(σ|s, a)xm(s, a, t), ∀m ∈ M, σ ∈ S, t ∈ [1, Tm − 1]; X a xm(s, a, 1) = αm(s), ∀m ∈ M, s ∈ S; (12) where xm(s, a, t) is the occupation measure of agent m, and The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) 1223 (a) (b) Figure 2: Illustration of augmenting an MDP to allow for variable starting and stopping times: a) (left) the original two-state MDP with a single action; (right) the augmented MDP with new states sb and sf and the new action a∗ (note that the origianl transitions are not changed in the augmentation process); b) the augmented MDP displayed as a trajectory through time (grey lines indicate all transitions, while black lines indicate a given trajectory.",
                "Objective Function (sum of expected rewards over all agents) max X m X s X a rm(s, a) X t xm(s, a, t) (5) Meaning Implication Linear Constraints Tie x to θ.",
                "Agent is only active when occupation measure is nonzero in original MDP states. θm(τ) = 0 =⇒ xm(s, a, τ −τa m+1) = 0 ∀s /∈ {sb , sf }, a ∈ A X s/∈{sb,sf } X a xm(s, a, t) ≤ θm(τa m + t − 1) ∀m ∈ M, ∀t ∈ [1, Tm] (6) Agent can only be active in τ ∈ (τa m, τd m) θm(τ) = 0 ∀m ∈ M, τ /∈ (τa m, τd m) (7) Cannot use resources when not active θm(τ) = 0 =⇒ Δm(τ, ω) = 0 ∀τ ∈ [0, bτ], ω ∈ Ω Δm(τ, ω) ≤ θm(τ) ∀m ∈ M, τ ∈ [0, bτ], ω ∈ Ω (8) Tie x to Δ (nonzero x forces corresponding Δ to be nonzero.)",
                "Δm(τ, ω) = 0, ϕm(a, ω) = 1 =⇒ xm(s, a, τ − τa m + 1) = 0 ∀s /∈ {sb , sf } 1/|A| X a ϕm(a, ω) X s/∈{sb,sf } xm(s, a, t) ≤ Δm(t + τa m − 1, ω) ∀m ∈ M, ω ∈ Ω, t ∈ [1, Tm] (9) Resource bounds X m Δm(τ, ω) ≤ bϕ(ω) ∀ω ∈ Ω, τ ∈ [0, bτ] (10) Agent cannot change resources while active.",
                "Only enabled for <br>scheduling</br> with static assignments. θm(τ) = 1 and θm(τ + 1) = 1 =⇒ Δm(τ, ω) = Δm(τ + 1, ω) Δm(τ, ω) − Z(1 − θm(τ + 1)) ≤ Δm(τ + 1, ω) + Z(1 − θm(τ)) Δm(τ, ω) + Z(1 − θm(τ + 1)) ≥ Δm(τ + 1, ω) − Z(1 − θm(τ)) ∀m ∈ M, ω ∈ Ω, τ ∈ [0, bτ] (11) Table 1: MILP for globally optimal resource <br>scheduling</br>.",
                "Tm = τd m − τa m + 1 is the time horizon for the agents MDP.",
                "Using this LP as a basis, we augment it with constraints that ensure that the resource usage implied by the agents occupation measures {xm} does not violate the global resource requirements bϕ at any time step τ ∈ [0, bτ].",
                "To formulate these resource constraints, we use the following binary variables: • Δm(τ, ω) = {0, 1}, ∀m ∈ M, τ ∈ [0, bτ], ω ∈ Ω, which serve as indicator variables that define whether agent m possesses resource ω at time τ.",
                "These are analogous to the static indicator variables used in the one-shot static resource-allocation problem in [6]. • θm = {0, 1}, ∀m ∈ M, τ ∈ [0, bτ] are indicator variables that specify whether agent m is active (i.e., executing its MDP) at time τ.",
                "The meaning of resource-usage variables Δ is illustrated in Figure 1: Δm(τ, ω) = 1 only if resource ω is allocated to agent m at time τ.",
                "The meaning of the activity indicators θ is illustrated in Figure 2b: when agent m is in either the start state sb or the finish state sf , the corresponding θm = 0, but once the agent becomes active and enters one of the other states, we set θm = 1 .",
                "This meaning of θ can be enforced with a linear constraint that synchronizes the values of the agents occupation measures xm and the activity 1224 The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) indicators θ, as shown in (6) in Table 1.",
                "Another constraint we have to add-because the activity indicators θ are defined on the global timeline τ-is to enforce the fact that the agent is inactive outside of its arrivaldeparture window.",
                "This is accomplished by constraint (7) in Table 1.",
                "Furthermore, agents should not be using resources while they are inactive.",
                "This constraint can also be enforced via a linear inequality on θ and Δ, as shown in (8).",
                "Constraint (6) sets the value of θ to match the policy defined by the occupation measure xm.",
                "In a similar fashion, we have to make sure that the resource-usage variables Δ are also synchronized with the occupation measure xm.",
                "This is done via constraint (9) in Table 1, which is nearly identical to the analogous constraint from [6].",
                "After implementing the above constraint, which enforces the meaning of Δ, we add a constraint that ensures that the agents resource usage never exceeds the amounts of available resources.",
                "This condition is also trivially expressed as a linear inequality (10) in Table 1.",
                "Finally, for the problem formulation where resource assignments are static during a lifetime of an agent, we add a constraint that ensures that the resource-usage variables Δ do not change their value while the agent is active (θ = 1).",
                "This is accomplished via the linear constraint (11), where Z ≥ 2 is a constant that is used to turn off the constraints when θm(τ) = 0 or θm(τ + 1) = 0.",
                "This constraint is not used for the dynamic problem formulation, where resources can be reallocated between agents at every time step.",
                "To summarize, Table 1 together with the conservationof-flow constraints from (12) defines the MILP that simultaneously computes an optimal resource assignment for all agents across time as well as optimal finite-horizon MDP policies that are valid under that resource assignment.",
                "As a rough measure of the complexity of this MILP, let us consider the number of optimization variables and constraints.",
                "Let TM = P Tm = P m(τa m − τd m + 1) be the sum of the lengths of the arrival-departure windows across all agents.",
                "Then, the number of optimization variables is: TM + bτ|M||Ω| + bτ|M|, TM of which are continuous (xm), and bτ|M||Ω| + bτ|M| are binary (Δ and θ).",
                "However, notice that all but TM|M| of the θ are set to zero by constraint (7), which also immediately forces all but TM|M||Ω| of the Δ to be zero via the constraints (8).",
                "The number of constraints (not including the degenerate constraints in (7)) in the MILP is: TM + TM|Ω| + bτ|Ω| + bτ|M||Ω|.",
                "Despite the fact that the complexity of the MILP is, in the worst case, exponential1 in the number of binary variables, the complexity of this MILP is significantly (exponentially) lower than that of the MILP with flat utility functions, described in Section 2.2.",
                "This result echos the efficiency gains reported in [6] for single-shot resource-allocation problems, but is much more pronounced, because of the explosion of the flat utility representation due to the temporal aspect of the problem (recall the prohibitive complexity of the combinatorial optimization in Section 2.2).",
                "We empirically analyze the performance of this method in Section 5. 1 Strictly speaking, solving MILPs to optimality is NPcomplete in the number of integer variables. 5.",
                "EXPERIMENTAL RESULTS Although the complexity of solving MILPs is in the worst case exponential in the number of integer variables, there are many efficient methods for solving MILPs that allow our algorithm to scale well for parameters common to resource allocation and <br>scheduling</br> problems.",
                "In particular, this section introduces a problem domain-the repairshop problem-used to empirically evaluate our algorithms scalability in terms of the number of agents |M|, the number of shared resources |Ω|, and the varied lengths of global time bτ during which agents may enter and exit the system.",
                "The repairshop problem is a simple parameterized MDP adopting the metaphor of a vehicular repair shop.",
                "Agents in the repair shop are mechanics with a number of independent tasks that yield reward only when completed.",
                "In our MDP model of this system, actions taken to advance through the state space are only allowed if the agent holds certain resources that are publicly available to the shop.",
                "These resources are in finite supply, and optimal policies for the shop will determine when each agent may hold the limited resources to take actions and earn individual rewards.",
                "Each task to be completed is associated with a single action, although the agent is required to repeat the action numerous times before completing the task and earning a reward.",
                "This model was parameterized in terms of the number of agents in the system, the number of different types of resources that could be linked to necessary actions, a global time during which agents are allowed to arrive and depart, and a maximum length for the number of time steps an agent may remain in the system.",
                "All datapoints in our experiments were obtained with 20 evaluations using CPLEX to solve the MILPs on a Pentium4 computer with 2Gb of RAM.",
                "Trials were conducted on both the static and the dynamic version of the resourcescheduling problem, as defined earlier.",
                "Figure 3 shows the runtime and policy value for independent modifications to the parameter set.",
                "The top row shows how the solution time for the MILP scales as we increase the number of agents |M|, the global time horizon bτ, and the number of resources |Ω|.",
                "Increasing the number of agents leads to exponential complexity scaling, which is to be expected for an NP-complete problem.",
                "However, increasing the global time limit bτ or the total number of resource types |Ω|-while holding the number of agents constantdoes not lead to decreased performance.",
                "This occurs because the problems get easier as they become under-constrained, which is also a common phenomenon for NP-complete problems.",
                "We also observe that the solution to the dynamic version of the problem can often be computed much faster than the static version.",
                "The bottom row of Figure 3 shows the joint policy value of the policies that correspond to the computed optimal resource-allocation schedules.",
                "We can observe that the dynamic version yields higher reward (as expected, since the reward for the dynamic version is always no less than the reward of the static version).",
                "We should point out that these graphs should not be viewed as a measure of performance of two different algorithms (both algorithms produce optimal solutions but to different problems), but rather as observations about how the quality of optimal solutions change as more flexibility is allowed in the reallocation of resources.",
                "Figure 4 shows runtime and policy value for trials in which common input variables are scaled together.",
                "This allows The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) 1225 2 4 6 8 10 10 −3 10 −2 10 −1 10 0 10 1 10 2 10 3 10 4 Number of Agents |M| CPUTime,sec |Ω| = 5, τ = 50 static dynamic 50 100 150 200 10 −2 10 −1 10 0 10 1 10 2 10 3 Global Time Boundary τ CPUTime,sec |M| = 5, |Ω| = 5 static dynamic 10 20 30 40 50 10 −2 10 −1 10 0 10 1 10 2 Number of Resources |Ω| CPUTime,sec |M| = 5, τ = 50 static dynamic 2 4 6 8 10 200 400 600 800 1000 1200 1400 1600 Number of Agents |M| Value |Ω| = 5, τ = 50 static dynamic 50 100 150 200 400 500 600 700 800 900 1000 1100 1200 1300 1400 Global Time Boundary τ Value |M| = 5, |Ω| = 5 static dynamic 10 20 30 40 50 500 600 700 800 900 1000 1100 1200 1300 1400 Number of Resources |Ω| Value |M| = 5, τ = 50 static dynamic Figure 3: Evaluation of our MILP for variable numbers of agents (column 1), lengths of global-time window (column 2), and numbers of resource types (column 3).",
                "Top row shows CPU time, and bottom row shows the joint reward of agents MDP policies.",
                "Error bars show the 1st and 3rd quartiles (25% and 75%). 2 4 6 8 10 10 −3 10 −2 10 −1 10 0 10 1 10 2 10 3 Number of Agents |M| CPUTime,sec τ = 10|M| static dynamic 2 4 6 8 10 10 −3 10 −2 10 −1 10 0 10 1 10 2 10 3 10 4 Number of Agents |M| CPUTime,sec |Ω| = 2|M| static dynamic 2 4 6 8 10 10 −3 10 −2 10 −1 10 0 10 1 10 2 10 3 10 4 Number of Agents |M| CPUTime,sec |Ω| = 5|M| static dynamic 2 4 6 8 10 200 400 600 800 1000 1200 1400 1600 1800 2000 2200 Number of Agents |M| Value τ = 10|M| static dynamic 2 4 6 8 10 200 400 600 800 1000 1200 1400 1600 1800 2000 Number of Agents |M| Value |Ω| = 2|M| static dynamic 2 4 6 8 10 0 500 1000 1500 2000 2500 Number of Agents |M| Value |Ω| = 5|M| static dynamic Figure 4: Evaluation of our MILP using correlated input variables.",
                "The left column tracks the performance and CPU time as the number of agents and global-time window increase together (bτ = 10|M|).",
                "The middle and the right column track the performance and CPU time as the number of resources and the number of agents increase together as |Ω| = 2|M| and |Ω| = 5|M|, respectively.",
                "Error bars show the 1st and 3rd quartiles (25% and 75%). 1226 The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) us to explore domains where the total number of agents scales proportionally to the total number of resource types or the global time horizon, while keeping constant the average agent density (per unit of global time) or the average number of resources per agent (which commonly occurs in real-life applications).",
                "Overall, we believe that these experimental results indicate that our MILP formulation can be used to effectively solve resource-<br>scheduling</br> problems of nontrivial size. 6.",
                "DISCUSSION AND CONCLUSIONS Throughout the paper, we have made a number of assumptions in our model and solution algorithm; we discuss their implications below. • Continual execution.",
                "We assume that once an agent stops executing its MDP (transitions into state sf ), it exits the system and cannot return.",
                "It is easy to relax this assumption for domains where agents MDPs can be paused and restarted.",
                "All that is required is to include an additional pause action which transitions from a given state back to itself, and has zero reward. • Indifference to start time.",
                "We used a reward model where agents rewards depend only on the time horizon of their MDPs and not the global start time.",
                "This is a consequence of our MDP-augmentation procedure from Section 4.1.",
                "It is easy to extend the model so that the agents incur an explicit penalty for idling by assigning a non-zero negative reward to the start state sb . • Binary resource requirements.",
                "For simplicity, we have assumed that resource costs are binary: ϕm(a, ω) = {0, 1}, but our results generalize in a straightforward manner to non-binary resource mappings, analogously to the procedure used in [5]. • Cooperative agents.",
                "The optimization procedure discussed in this paper was developed in the context of cooperative agents, but it can also be used to design a mechanism for <br>scheduling</br> resources among self-interested agents.",
                "This optimization procedure can be embedded in a VickreyClarke-Groves auction, completely analogously to the way it was done in [7].",
                "In fact, all the results of [7] about the properties of the auction and information privacy directly carry over to the <br>scheduling</br> domain discussed in this paper, requiring only slight modifications to deal with finitehorizon MDPs. • Known, deterministic arrival and departure times.",
                "Finally, we have assumed that agents arrival and departure times (τa m and τd m) are deterministic and known a priori.",
                "This assumption is fundamental to our solution method.",
                "While there are many domains where this assumption is valid, in many cases agents arrive and depart dynamically and their arrival and departure times can only be predicted probabilistically, leading to online resource-allocation problems.",
                "In particular, in the case of self-interested agents, this becomes an interesting version of an online-mechanism-design problem [11, 12].",
                "In summary, we have presented an MILP formulation for the combinatorial resource-<br>scheduling</br> problem where agents values for possible resource assignments are defined by finitehorizon MDPs.",
                "This result extends previous work ([6, 7]) on static one-shot resource allocation under MDP-induced preferences to resource-<br>scheduling</br> problems with a temporal aspect.",
                "As such, this work takes a step in the direction of designing an online mechanism for agents with combinatorial resource preferences induced by stochastic planning problems.",
                "Relaxing the assumption about deterministic arrival and departure times of the agents is a focus of our future work.",
                "We would like to thank the anonymous reviewers for their insightful comments and suggestions. 7.",
                "REFERENCES [1] E. Altman and A. Shwartz.",
                "Adaptive control of constrained Markov chains: Criteria and policies.",
                "Annals of Operations Research, special issue on Markov Decision Processes, 28:101-134, 1991. [2] R. Bellman.",
                "Dynamic Programming.",
                "Princeton University Press, 1957. [3] C. Boutilier.",
                "Solving concisely expressed combinatorial auction problems.",
                "In Proc. of AAAI-02, pages 359-366, 2002. [4] C. Boutilier and H. H. Hoos.",
                "Bidding languages for combinatorial auctions.",
                "In Proc. of IJCAI-01, pages 1211-1217, 2001. [5] D. Dolgov.",
                "Integrated Resource Allocation and Planning in Stochastic Multiagent Environments.",
                "PhD thesis, Computer Science Department, University of Michigan, February 2006. [6] D. A. Dolgov and E. H. Durfee.",
                "Optimal resource allocation and policy formulation in loosely-coupled Markov decision processes.",
                "In Proc. of ICAPS-04, pages 315-324, June 2004. [7] D. A. Dolgov and E. H. Durfee.",
                "Computationally efficient combinatorial auctions for resource allocation in weakly-coupled MDPs.",
                "In Proc. of AAMAS-05, New York, NY, USA, 2005.",
                "ACM Press. [8] D. A. Dolgov and E. H. Durfee.",
                "Resource allocation among agents with preferences induced by factored MDPs.",
                "In Proc. of AAMAS-06, 2006. [9] K. Larson and T. Sandholm.",
                "Mechanism design and deliberative agents.",
                "In Proc. of AAMAS-05, pages 650-656, New York, NY, USA, 2005.",
                "ACM Press. [10] N. Nisan.",
                "Bidding and allocation in combinatorial auctions.",
                "In Electronic Commerce, 2000. [11] D. C. Parkes and S. Singh.",
                "An MDP-based approach to Online Mechanism Design.",
                "In Proc. of the Seventeenths Annual Conference on Neural Information Processing Systems (NIPS-03), 2003. [12] D. C. Parkes, S. Singh, and D. Yanovsky.",
                "Approximately efficient online mechanism design.",
                "In Proc. of the Eighteenths Annual Conference on Neural Information Processing Systems (NIPS-04), 2004. [13] M. L. Puterman.",
                "Markov Decision Processes.",
                "John Wiley & Sons, New York, 1994. [14] M. H. Rothkopf, A. Pekec, and R. M. Harstad.",
                "Computationally manageable combinational auctions.",
                "Management Science, 44(8):1131-1147, 1998. [15] T. Sandholm.",
                "An algorithm for optimal winner determination in combinatorial auctions.",
                "In Proc. of IJCAI-99, pages 542-547, San Francisco, CA, USA, 1999.",
                "Morgan Kaufmann Publishers Inc.",
                "The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) 1227"
            ],
            "original_annotated_samples": [],
            "translated_annotated_samples": [
                "\"Programación\" de recursos combinatorios para MDP múltiples Dmitri A. Dolgov, Michael R. James y Michael E. Investigación técnica de AI y Robotics Group, Toyota Technical Center USA {Ddolgov, Michael.R.James, Michael.samples}@gmail.COM La \"programación\" de recursos óptimos en los sistemas múltiples es una tarea computacionalmente desafiante, particularmente cuando los valores de los recursos no son aditivos.Planificación",
                "Consideramos el problema combinatorio de \"programar\" el uso de múltiples recursos entre los agentes que operan en entornos estocásticos, modelados como procesos de decisión de Markov (MDP).Planificación",
                "Extendemos esos modelos existentes al problema de la \"programación\" de los recursos combinatorios, donde los agentes persisten solo por períodos finitos entre sus tiempos de llegada y salida (predefinidas), que requieren recursos solo para esos períodos de tiempo.Planificación",
                "Introducción Las tareas de asignación óptima de recursos y \"programación\" son ubicuas en sistemas multiagentes, pero resolver tales problemas de optimización puede ser computacionalmente difícil, debido a una serie de factores.Planificación",
                "En este documento, ampliamos el trabajo sobre la asignación de recursos bajo las preferencias inducidas por MDP a problemas de \"programación\" de tiempo discreto, donde los agentes están presentes en el sistema para intervalos de tiempo finitos y solo pueden usar recursos dentro de estos intervalos.Planificación",
                "Abordamos el problema de la \"programación\" de recursos óptimos globalmente, donde el objetivo es encontrar una asignación de recursos a los agentes a lo largo del tiempo que maximiza la suma de las recompensas esperadas que obtienen.Planificación",
                "En este contexto, nuestra contribución principal es una formulación de programación de intensidad mixta del problema de \"programación\" que elige tareas de recursos óptimos globalmente óptimos, tiempos de partida y horizontes de ejecución para todos los agentes (dentro de su llegada1220 978-81-904262-7-5 (RPS (RPS) c 2007 Intervalos de salida de Ifaamas).Planificación",
                "Analizamos y comparamos empíricamente dos sabores del problema de \"programación\": uno, donde los agentes tienen asignaciones de recursos estáticos dentro de sus MDP de horizones finitos, y otro, donde los recursos pueden reasignarse dinámicamente entre los agentes en cada paso de tiempo.Planificación",
                "En la Sección 4.2, describimos nuestro resultado principal, el programa de optimización para la \"programación\" de recursos óptimos a nivel mundial.Planificación",
                "Tras la discusión de nuestros resultados experimentales sobre un problema de \"programación\" en la Sección 5, concluimos en la Sección 6 con una discusión de posibles extensiones y generalizaciones de nuestro método.2. Programación",
                "Sin embargo, dado que el enfoque de nuestro trabajo está en los problemas de \"programación\", y una gran parte del problema de optimización es decidir cómo se asignan los recursos a tiempo entre los agentes con tiempos de llegada y salida finitos, modelamos los agentes que planean los problemas como finitos.MDPS Horizon, en contraste con trabajos anteriores que utilizaron MDP con descuento de Horizon Infinite.Planificación",
                "También describimos los métodos estándar para la \"programación\" de recursos combinatorios con valores de recursos planos, que sirven como un punto de referencia de comparación para el nuevo modelo desarrollado aquí.2.1 Procesos de decisión de Markov Un MDP estacionario, de dominio finito y en tiempo discreto (ver, por ejemplo, [13] para un desarrollo exhaustivo y detallado) puede describirse como S, A, P, R, donde: S es un conjunto finitode estados del sistema;A es un conjunto finito de acciones que están disponibles para el agente;P es una función de transición estocástica estacionaria, donde p (σ | s, a) es la probabilidad de transición al estado σ al ejecutar la acción A en el estado s;r es una función de recompensa estacionaria, donde R (s, a) especifica la recompensa obtenida al ejecutar la acción A en el estado s.Dado dicho MDP, un problema de decisión bajo un horizonte finito T es elegir una acción óptima en cada paso de tiempo para maximizar el valor esperado de la recompensa total acumulada durante la vida de los agentes (finitos).Planificación",
                "Este resultado es bien conocido por los MDP de horizonte infinito con varios tipos de restricciones [1, 6], y también es válida para nuestro modelo de horizonte finito, que puede establecerse fácilmente a través de una línea de razonamiento completamente análoga a los argumentos en [6]2.2 Los recursos combinatorios \"programación\" un enfoque directo para la \"programación\" de los recursos para un conjunto de agentes M, cuyos valores para los recursos son inducidos por problemas de planificación estocástica (en nuestro caso, MDP de Horizon Finito) sería hacer que cada agente enumere todosposibles asignaciones de recursos a lo largo del tiempo y, para cada uno, calcule su valor resolviendo el MDP correspondiente.Planificación",
                "Conf.Sobre los agentes autónomos y los sistemas de múltiples agentes (AAMAS 07) 1221 Para el problema de \"programación\" donde los agentes tienen requisitos de recursos estáticos dentro de sus MDP finitos de horarios, los agentes proporcionan una valoración para cada paquete de recursos para cada posible horizonte de tiempo (1, Tm]) que pueden usar.Planificación",
                "Para el problema de \"programación\" donde los agentes pueden reasignar dinámicamente los recursos, cada agente debe especificar un valor para cada combinación de paquetes y pasos de tiempo dentro de su horizonte temporal.Planificación",
                "El mismo conjunto de ecuaciones (3) se puede usar para resolver este problema dinámico de \"programación\", pero el programa entero es diferente debido a la diferencia en cómo se define ψ.Planificación",
                "Este enfoque directo para resolver estos dos problemas de \"programación\" requiere una enumeración y solución de 2 | Ω |Tm (asignación estática) o p t∈ [1, tm] 2 | ω | t (reasignación dinámica) MDP para cada agente, que rápidamente se vuelve intratable con el crecimiento del número de recursos | ω |o el horizonte de tiempo tm.3. Programación",
                "Es factible una solución si la asignación correspondiente de recursos a los agentes no viola la restricción global de recursos: x m Δm (τ, Ω) ≤ bϕ (ω), ∀ω ∈ ω, τ ∈ [1, bτ].(4) Consideramos dos sabores del problema de \"programación\" de recursos.Planificación",
                "La Figura 1 muestra un problema de recursos: \"programación\" con tres agentes M = {M1, M2, M3}, tres recursos ω = {ω1, ω2, ω3}, y un horizonte de problemas global de Bτ = 11. Programación",
                "La Figura 1A muestra una solución a un problema estático de \"programación\".Planificación",
                "Conf.en agentes autónomos y sistemas de múltiples agentes (AAMAS 07) (a) (b) Figura 1: Ilustración de una solución a un problema de recursos: \"Programación\" con tres agentes y tres recursos: a) Asignaciones de recursos estáticos (las tareas de recursos son constantesDentro de la vida de los agentes; b) asignación dinámica (las asignaciones de recursos pueden cambiar en cada paso de tiempo).4. Programación",
                "La \"programación de recursos\" nuestro algoritmo de recursos- \"Programación\" se realiza en dos etapas.Planificación",
                "Tenga en cuenta que si quisiéramos modelar un problema en el que los agentes pudieran detener sus MDP en pasos de tiempo arbitrarios (lo que podría ser útil para dominios donde es posible la reasignación dinámica), podríamos lograrlo fácilmente al incluir una acción adicional que pasa de cada estado a sí mismo a sí mismo.con cero recompensa.4.2 milp para la \"programación\" de recursos dado un conjunto de MDP aumentados, como se definió anteriormente, el objetivo de esta sección es formular un programa de optimización global que resuelva el problema de la \"programación\" de recursos.Planificación",
                "El problema de optimización resultante luego resuelve simultáneamente los MDP de los agentes y los problemas de \"programación\" de recursos.Planificación",
                "Solo habilitado para \"programar\" con tareas estáticas.θm (τ) = 1 y θm (τ + 1) = 1 = ⇒ ΔM (τ, ω) = ΔM (τ + 1, ω) ΔM (τ, ω) - z (1 - θm (τ + 1))≤ Δm (τ + 1, Ω) + z (1 - θm (τ)) Δm (τ, ω) + z (1 - θm (τ + 1)) ≥ ΔM (τ + 1, ω) - z (1- θm (τ)) ∀m ∈ M, ω ∈ ω, τ ∈ [0, Bτ] (11) Tabla 1: MILP para la \"programación\" de recursos óptimos globalmente óptimos.Planificación",
                "Resultados experimentales Aunque la complejidad de resolver MILP es en el peor de los casos exponencial en el número de variables enteras, existen muchos métodos eficientes para resolver MILP que permiten que nuestro algoritmo escala bien para los parámetros comunes a los problemas de asignación de recursos y \"programación\".Planificación",
                "En general, creemos que estos resultados experimentales indican que nuestra formulación MILP puede usarse para resolver efectivamente los problemas de \"programación\" del tamaño no trivial.6. Programación",
                "El procedimiento de optimización discutido en este documento se desarrolló en el contexto de los agentes cooperativos, pero también se puede utilizar para diseñar un mecanismo para \"programar\" los recursos entre los agentes egoístas.Planificación",
                "De hecho, todos los resultados de [7] sobre las propiedades de la subasta y la privacidad de la información se trasladan directamente al dominio de \"programación\" discutido en este documento, lo que requiere solo pequeñas modificaciones para tratar con los MDP de Horizon Finite.• Los tiempos de llegada y salida deterministas conocidas.Planificación",
                "En resumen, hemos presentado una formulación MILP para el problema de recursos combinatorios: \"Programación\" del problema donde los valores de los agentes para posibles asignaciones de recursos están definidos por los MDP de Horizon de FiniteS.Planificación",
                "Este resultado extiende el trabajo anterior ([6, 7]) en la asignación estática de recursos de un solo disparo bajo las preferencias inducidas por MDP a los problemas de \"programación\" de recursos con un aspecto temporal.Planificación"
            ],
            "translated_text": "",
            "candidates": [],
            "error": []
        },
        "optimization problem": {
            "translated_key": "",
            "is_in_text": true,
            "original_annotated_sentences": [
                "Combinatorial Resource Scheduling for Multiagent MDPs Dmitri A. Dolgov, Michael R. James, and Michael E. Samples AI and Robotics Group Technical Research, Toyota Technical Center USA {ddolgov, michael.r.james, michael.samples}@gmail.com ABSTRACT Optimal resource scheduling in multiagent systems is a computationally challenging task, particularly when the values of resources are not additive.",
                "We consider the combinatorial problem of scheduling the usage of multiple resources among agents that operate in stochastic environments, modeled as Markov decision processes (MDPs).",
                "In recent years, efficient resource-allocation algorithms have been developed for agents with resource values induced by MDPs.",
                "However, this prior work has focused on static resource-allocation problems where resources are distributed once and then utilized in infinite-horizon MDPs.",
                "We extend those existing models to the problem of combinatorial resource scheduling, where agents persist only for finite periods between their (predefined) arrival and departure times, requiring resources only for those time periods.",
                "We provide a computationally efficient procedure for computing globally optimal resource assignments to agents over time.",
                "We illustrate and empirically analyze the method in the context of a stochastic jobscheduling domain.",
                "Categories and Subject Descriptors I.2.8 [Artificial Intelligence]: Problem Solving, Control Methods, and Search; I.2.11 [Artificial Intelligence]: Distributed Artificial Intelligence-Multiagent systems General Terms Algorithms, Performance, Design 1.",
                "INTRODUCTION The tasks of optimal resource allocation and scheduling are ubiquitous in multiagent systems, but solving such optimization problems can be computationally difficult, due to a number of factors.",
                "In particular, when the value of a set of resources to an agent is not additive (as is often the case with resources that are substitutes or complements), the utility function might have to be defined on an exponentially large space of resource bundles, which very quickly becomes computationally intractable.",
                "Further, even when each agent has a utility function that is nonzero only on a small subset of the possible resource bundles, obtaining optimal allocation is still computationally prohibitive, as the problem becomes NP-complete [14].",
                "Such computational issues have recently spawned several threads of work in using compact models of agents preferences.",
                "One idea is to use any structure present in utility functions to represent them compactly, via, for example, logical formulas [15, 10, 4, 3].",
                "An alternative is to directly model the mechanisms that define the agents utility functions and perform resource allocation directly with these models [9].",
                "A way of accomplishing this is to model the processes by which an agent might utilize the resources and define the utility function as the payoff of these processes.",
                "In particular, if an agent uses resources to act in a stochastic environment, its utility function can be naturally modeled with a Markov decision process, whose action set is parameterized by the available resources.",
                "This representation can then be used to construct very efficient resource-allocation algorithms that lead to an exponential speedup over a straightforward <br>optimization problem</br> with flat representations of combinatorial preferences [6, 7, 8].",
                "However, this existing work on resource allocation with preferences induced by resource-parameterized MDPs makes an assumption that the resources are only allocated once and are then utilized by the agents independently within their infinite-horizon MDPs.",
                "This assumption that no reallocation of resources is possible can be limiting in domains where agents arrive and depart dynamically.",
                "In this paper, we extend the work on resource allocation under MDP-induced preferences to discrete-time scheduling problems, where agents are present in the system for finite time intervals and can only use resources within these intervals.",
                "In particular, agents arrive and depart at arbitrary (predefined) times and within these intervals use resources to execute tasks in finite-horizon MDPs.",
                "We address the problem of globally optimal resource scheduling, where the objective is to find an allocation of resources to the agents across time that maximizes the sum of the expected rewards that they obtain.",
                "In this context, our main contribution is a mixed-integerprogramming formulation of the scheduling problem that chooses globally optimal resource assignments, starting times, and execution horizons for all agents (within their arrival1220 978-81-904262-7-5 (RPS) c 2007 IFAAMAS departure intervals).",
                "We analyze and empirically compare two flavors of the scheduling problem: one, where agents have static resource assignments within their finite-horizon MDPs, and another, where resources can be dynamically reallocated between agents at every time step.",
                "In the rest of the paper, we first lay down the necessary groundwork in Section 2 and then introduce our model and formal problem statement in Section 3.",
                "In Section 4.2, we describe our main result, the optimization program for globally optimal resource scheduling.",
                "Following the discussion of our experimental results on a job-scheduling problem in Section 5, we conclude in Section 6 with a discussion of possible extensions and generalizations of our method. 2.",
                "BACKGROUND Similarly to the model used in previous work on resourceallocation with MDP-induced preferences [6, 7], we define the value of a set of resources to an agent as the value of the best MDP policy that is realizable, given those resources.",
                "However, since the focus of our work is on scheduling problems, and a large part of the <br>optimization problem</br> is to decide how resources are allocated in time among agents with finite arrival and departure times, we model the agents planning problems as finite-horizon MDPs, in contrast to previous work that used infinite-horizon discounted MDPs.",
                "In the rest of this section, we first introduce some necessary background on finite-horizon MDPs and present a linear-programming formulation that serves as the basis for our solution algorithm developed in Section 4.",
                "We also outline the standard methods for combinatorial resource scheduling with flat resource values, which serve as a comparison benchmark for the new model developed here. 2.1 Markov Decision Processes A stationary, finite-domain, discrete-time MDP (see, for example, [13] for a thorough and detailed development) can be described as S, A, p, r , where: S is a finite set of system states; A is a finite set of actions that are available to the agent; p is a stationary stochastic transition function, where p(σ|s, a) is the probability of transitioning to state σ upon executing action a in state s; r is a stationary reward function, where r(s, a) specifies the reward obtained upon executing action a in state s. Given such an MDP, a decision problem under a finite horizon T is to choose an optimal action at every time step to maximize the expected value of the total reward accrued during the agents (finite) lifetime.",
                "The agents optimal policy is then a function of current state s and the time until the horizon.",
                "An optimal policy for such a problem is to act greedily with respect to the optimal value function, defined recursively by the following system of finite-time Bellman equations [2]: v(s, t) = max a r(s, a) + X σ p(σ|s, a)v(σ, t + 1), ∀s ∈ S, t ∈ [1, T − 1]; v(s, T) = 0, ∀s ∈ S; where v(s, t) is the optimal value of being in state s at time t ∈ [1, T].",
                "This optimal value function can be easily computed using dynamic programming, leading to the following optimal policy π, where π(s, a, t) is the probability of executing action a in state s at time t: π(s, a, t) = ( 1, a = argmaxa r(s, a) + P σ p(σ|s, a)v(σ, t + 1), 0, otherwise.",
                "The above is the most common way of computing the optimal value function (and therefore an optimal policy) for a finite-horizon MDP.",
                "However, we can also formulate the problem as the following linear program (similarly to the dual LP for infinite-horizon discounted MDPs [13, 6, 7]): max X s X a r(s, a) X t x(s, a, t) subject to: X a x(σ, a, t + 1) = X s,a p(σ|s, a)x(s, a, t) ∀σ, t ∈ [1, T − 1]; X a x(s, a, 1) = α(s), ∀s ∈ S; (1) where α(s) is the initial distribution over the state space, and x is the (non-stationary) occupation measure (x(s, a, t) ∈ [0, 1] is the total expected number of times action a is executed in state s at time t).",
                "An optimal (non-stationary) policy is obtained from the occupation measure as follows: π(s, a, t) = x(s, a, t)/ X a x(s, a, t) ∀s ∈ S, t ∈ [1, T]. (2) Note that the standard unconstrained finite-horizon MDP, as described above, always has a uniformly-optimal solution (optimal for any initial distribution α(s)).",
                "Therefore, an optimal policy can be obtained by using an arbitrary constant α(s) > 0 (in particular, α(s) = 1 will result in x(s, a, t) = π(s, a, t)).",
                "However, for MDPs with resource constraints (as defined below in Section 3), uniformly-optimal policies do not in general exist.",
                "In such cases, α becomes a part of the problem input, and a resulting policy is only optimal for that particular α.",
                "This result is well known for infinite-horizon MDPs with various types of constraints [1, 6], and it also holds for our finite-horizon model, which can be easily established via a line of reasoning completely analogous to the arguments in [6]. 2.2 Combinatorial Resource Scheduling A straightforward approach to resource scheduling for a set of agents M, whose values for the resources are induced by stochastic planning problems (in our case, finite-horizon MDPs) would be to have each agent enumerate all possible resource assignments over time and, for each one, compute its value by solving the corresponding MDP.",
                "Then, each agent would provide valuations for each possible resource bundle over time to a centralized coordinator, who would compute the optimal resource assignments across time based on these valuations.",
                "When resources can be allocated at different times to different agents, each agent must submit valuations for every combination of possible time horizons.",
                "Let each agent m ∈ M execute its MDP within the arrival-departure time interval τ ∈ [τa m, τd m].",
                "Hence, agent m will execute an MDP with time horizon no greater than Tm = τd m−τa m+1.",
                "Let bτ be the global time horizon for the problem, before which all of the agents MDPs must finish.",
                "We assume τd m < bτ, ∀m ∈ M. The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) 1221 For the scheduling problem where agents have static resource requirements within their finite-horizon MDPs, the agents provide a valuation for each resource bundle for each possible time horizon (from [1, Tm]) that they may use.",
                "Let Ω be the set of resources to be allocated among the agents.",
                "An agent will get at most one resource bundle for one of the time horizons.",
                "Let the variable ψ ∈ Ψm enumerate all possible pairs of resource bundles and time horizons for agent m, so there are 2|Ω| × Tm values for ψ (the space of bundles is exponential in the number of resource types |Ω|).",
                "The agent m must provide a value vψ m for each ψ, and the coordinator will allocate at most one ψ (resource, time horizon) pair to each agent.",
                "This allocation is expressed as an indicator variable zψ m ∈ {0, 1} that shows whether ψ is assigned to agent m. For time τ and resource ω, the function nm(ψ, τ, ω) ∈ {0, 1} indicates whether the bundle in ψ uses resource ω at time τ (we make the assumption that agents have binary resource requirements).",
                "This allocation problem is NP-complete, even when considering only a single time step, and its difficulty increases significantly with multiple time steps because of the increasing number of values of ψ.",
                "The problem of finding an optimal allocation that satisfies the global constraint that the amount of each resource ω allocated to all agents does not exceed the available amount bϕ(ω) can be expressed as the following integer program: max X m∈M X ψ∈Ψm zψ mvψ m subject to: X ψ∈Ψm zψ m ≤ 1, ∀m ∈ M; X m∈M X ψ∈Ψm zψ mnm(ψ, τ, ω) ≤ bϕ(ω), ∀τ ∈ [1, bτ], ∀ω ∈ Ω; (3) The first constraint in equation 3 says that no agent can receive more than one bundle, and the second constraint ensures that the total assignment of resource ω does not, at any time, exceed the resource bound.",
                "For the scheduling problem where the agents are able to dynamically reallocate resources, each agent must specify a value for every combination of bundles and time steps within its time horizon.",
                "Let the variable ψ ∈ Ψm in this case enumerate all possible resource bundles for which at most one bundle may be assigned to agent m at each time step.",
                "Therefore, in this case there are P t∈[1,Tm](2|Ω| )t ∼ 2|Ω|Tm possibilities of resource bundles assigned to different time slots, for the Tm different time horizons.",
                "The same set of equations (3) can be used to solve this dynamic scheduling problem, but the integer program is different because of the difference in how ψ is defined.",
                "In this case, the number of ψ values is exponential in each agents planning horizon Tm, resulting in a much larger program.",
                "This straightforward approach to solving both of these scheduling problems requires an enumeration and solution of either 2|Ω| Tm (static allocation) or P t∈[1,Tm] 2|Ω|t (dynamic reallocation) MDPs for each agent, which very quickly becomes intractable with the growth of the number of resources |Ω| or the time horizon Tm. 3.",
                "MODEL AND PROBLEM STATEMENT We now formally introduce our model of the resourcescheduling problem.",
                "The problem input consists of the following components: • M, Ω, bϕ, τa m, τd m, bτ are as defined above in Section 2.2. • {Θm} = {S, A, pm, rm, αm} are the MDPs of all agents m ∈ M. Without loss of generality, we assume that state and action spaces of all agents are the same, but each has its own transition function pm, reward function rm, and initial conditions αm. • ϕm : A×Ω → {0, 1} is the mapping of actions to resources for agent m. ϕm(a, ω) indicates whether action a of agent m needs resource ω.",
                "An agent m that receives a set of resources that does not include resource ω cannot execute in its MDP policy any action a for which ϕm(a, ω) = 0.",
                "We assume all resource requirements are binary; as discussed below in Section 6, this assumption is not limiting.",
                "Given the above input, the <br>optimization problem</br> we consider is to find the globally optimal-maximizing the sum of expected rewards-mapping of resources to agents for all time steps: Δ : τ × M × Ω → {0, 1}.",
                "A solution is feasible if the corresponding assignment of resources to the agents does not violate the global resource constraint: X m Δm(τ, ω) ≤ bϕ(ω), ∀ω ∈ Ω, τ ∈ [1, bτ]. (4) We consider two flavors of the resource-scheduling problem.",
                "The first formulation restricts resource assignments to the space where the allocation of resources to each agent is static during the agents lifetime.",
                "The second formulation allows reassignment of resources between agents at every time step within their lifetimes.",
                "Figure 1 depicts a resource-scheduling problem with three agents M = {m1, m2, m3}, three resources Ω = {ω1, ω2, ω3}, and a global problem horizon of bτ = 11.",
                "The agents arrival and departure times are shown as gray boxes and are {1, 6}, {3, 7}, and {2, 11}, respectively.",
                "A solution to this problem is shown via horizontal bars within each agents box, where the bars correspond to the allocation of the three resource types.",
                "Figure 1a shows a solution to a static scheduling problem.",
                "According to the shown solution, agent m1 begins the execution of its MDP at time τ = 1 and has a lock on all three resources until it finishes execution at time τ = 3.",
                "Note that agent m1 relinquishes its hold on the resources before its announced departure time of τd m1 = 6, ostensibly because other agents can utilize the resources more effectively.",
                "Thus, at time τ = 4, resources ω1 and ω3 are allocated to agent m2, who then uses them to execute its MDP (using only actions supported by resources ω1 and ω3) until time τ = 7.",
                "Agent m3 holds resource ω3 during the interval τ ∈ [4, 10].",
                "Figure 1b shows a possible solution to the dynamic version of the same problem.",
                "There, resources can be reallocated between agents at every time step.",
                "For example, agent m1 gives up its use of resource ω2 at time τ = 2, although it continues the execution of its MDP until time τ = 6.",
                "Notice that an agent is not allowed to stop and restart its MDP, so agent m1 is only able to continue executing in the interval τ ∈ [3, 4] if it has actions that do not require any resources (ϕm(a, ω) = 0).",
                "Clearly, the model and problem statement described above make a number of assumptions about the problem and the desired solution properties.",
                "We discuss some of those assumptions and their implications in Section 6. 1222 The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) (a) (b) Figure 1: Illustration of a solution to a resource-scheduling problem with three agents and three resources: a) static resource assignments (resource assignments are constant within agents lifetimes; b) dynamic assignment (resource assignments are allowed to change at every time step). 4.",
                "RESOURCE SCHEDULING Our resource-scheduling algorithm proceeds in two stages.",
                "First, we perform a preprocessing step that augments the agent MDPs; this process is described in Section 4.1.",
                "Second, using these augmented MDPs we construct a global <br>optimization problem</br>, which is described in Section 4.2. 4.1 Augmenting Agents MDPs In the model described in the previous section, we assume that if an agent does not possess the necessary resources to perform actions in its MDP, its execution is halted and the agent leaves the system.",
                "In other words, the MDPs cannot be paused and resumed.",
                "For example, in the problem shown in Figure 1a, agent m1 releases all resources after time τ = 3, at which point the execution of its MDP is halted.",
                "Similarly, agents m2 and m3 only execute their MDPs in the intervals τ ∈ [4, 6] and τ ∈ [4, 10], respectively.",
                "Therefore, an important part of the global decision-making problem is to decide the window of time during which each of the agents is active (i.e., executing its MDP).",
                "To accomplish this, we augment each agents MDP with two new states (start and finish states sb , sf , respectively) and a new start/stop action a∗ , as illustrated in Figure 2.",
                "The idea is that an agent stays in the start state sb until it is ready to execute its MDP, at which point it performs the start/stop action a∗ and transitions into the state space of the original MDP with the transition probability that corresponds to the original initial distribution α(s).",
                "For example, in Figure 1a, for agent m2 this would happen at time τ = 4.",
                "Once the agent gets to the end of its activity window (time τ = 6 for agent m2 in Figure 1a), it performs the start/stop action, which takes it into the sink finish state sf at time τ = 7.",
                "More precisely, given an MDP S, A, pm, rm, αm , we define an augmented MDP S , A , pm, rm, αm as follows: S = S ∪ sb ∪ sf ; A = A ∪ a∗ ; p (s|sb , a∗ ) = α(s), ∀s ∈ S; p (sb |sb , a) = 1.0, ∀a ∈ A; p (sf |s, a∗ ) = 1.0, ∀s ∈ S; p (σ|s, a) = p(σ|s, a), ∀s, σ ∈ S, a ∈ A; r (sb , a) = r (sf , a) = 0, ∀a ∈ A ; r (s, a) = r(s, a), ∀s ∈ S, a ∈ A; α (sb ) = 1; α (s) = 0, ∀s ∈ S; where all non-specified transition probabilities are assumed to be zero.",
                "Further, in order to account for the new starting state, we begin the MDP one time-step earlier, setting τa m ← τa m − 1.",
                "This will not affect the resource allocation due to the resource constraints only being enforced for the original MDP states, as will be discussed in the next section.",
                "For example, the augmented MDPs shown in Figure 2b (which starts in state sb at time τ = 2) would be constructed from an MDP with original arrival time τ = 3.",
                "Figure 2b also shows a sample trajectory through the state space: the agent starts in state sb , transitions into the state space S of the original MDP, and finally exists into the sink state sf .",
                "Note that if we wanted to model a problem where agents could pause their MDPs at arbitrary time steps (which might be useful for domains where dynamic reallocation is possible), we could easily accomplish this by including an extra action that transitions from each state to itself with zero reward. 4.2 MILP for Resource Scheduling Given a set of augmented MDPs, as defined above, the goal of this section is to formulate a global optimization program that solves the resource-scheduling problem.",
                "In this section and below, all MDPs are assumed to be the augmented MDPs as defined in Section 4.1.",
                "Our approach is similar to the idea used in [6]: we begin with the linear-program formulation of agents MDPs (1) and augment it with constraints that ensure that the corresponding resource allocation across agents and time is valid.",
                "The resulting <br>optimization problem</br> then simultaneously solves the agents MDPs and resource-scheduling problems.",
                "In the rest of this section, we incrementally develop a mixed integer program (MILP) that achieves this.",
                "In the absence of resource constraints, the agents finitehorizon MDPs are completely independent, and the globally optimal solution can be trivially obtained via the following LP, which is simply an aggregation of single-agent finitehorizon LPs: max X m X s X a rm(s, a) X t xm(s, a, t) subject to: X a xm(σ, a, t + 1) = X s,a pm(σ|s, a)xm(s, a, t), ∀m ∈ M, σ ∈ S, t ∈ [1, Tm − 1]; X a xm(s, a, 1) = αm(s), ∀m ∈ M, s ∈ S; (12) where xm(s, a, t) is the occupation measure of agent m, and The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) 1223 (a) (b) Figure 2: Illustration of augmenting an MDP to allow for variable starting and stopping times: a) (left) the original two-state MDP with a single action; (right) the augmented MDP with new states sb and sf and the new action a∗ (note that the origianl transitions are not changed in the augmentation process); b) the augmented MDP displayed as a trajectory through time (grey lines indicate all transitions, while black lines indicate a given trajectory.",
                "Objective Function (sum of expected rewards over all agents) max X m X s X a rm(s, a) X t xm(s, a, t) (5) Meaning Implication Linear Constraints Tie x to θ.",
                "Agent is only active when occupation measure is nonzero in original MDP states. θm(τ) = 0 =⇒ xm(s, a, τ −τa m+1) = 0 ∀s /∈ {sb , sf }, a ∈ A X s/∈{sb,sf } X a xm(s, a, t) ≤ θm(τa m + t − 1) ∀m ∈ M, ∀t ∈ [1, Tm] (6) Agent can only be active in τ ∈ (τa m, τd m) θm(τ) = 0 ∀m ∈ M, τ /∈ (τa m, τd m) (7) Cannot use resources when not active θm(τ) = 0 =⇒ Δm(τ, ω) = 0 ∀τ ∈ [0, bτ], ω ∈ Ω Δm(τ, ω) ≤ θm(τ) ∀m ∈ M, τ ∈ [0, bτ], ω ∈ Ω (8) Tie x to Δ (nonzero x forces corresponding Δ to be nonzero.)",
                "Δm(τ, ω) = 0, ϕm(a, ω) = 1 =⇒ xm(s, a, τ − τa m + 1) = 0 ∀s /∈ {sb , sf } 1/|A| X a ϕm(a, ω) X s/∈{sb,sf } xm(s, a, t) ≤ Δm(t + τa m − 1, ω) ∀m ∈ M, ω ∈ Ω, t ∈ [1, Tm] (9) Resource bounds X m Δm(τ, ω) ≤ bϕ(ω) ∀ω ∈ Ω, τ ∈ [0, bτ] (10) Agent cannot change resources while active.",
                "Only enabled for scheduling with static assignments. θm(τ) = 1 and θm(τ + 1) = 1 =⇒ Δm(τ, ω) = Δm(τ + 1, ω) Δm(τ, ω) − Z(1 − θm(τ + 1)) ≤ Δm(τ + 1, ω) + Z(1 − θm(τ)) Δm(τ, ω) + Z(1 − θm(τ + 1)) ≥ Δm(τ + 1, ω) − Z(1 − θm(τ)) ∀m ∈ M, ω ∈ Ω, τ ∈ [0, bτ] (11) Table 1: MILP for globally optimal resource scheduling.",
                "Tm = τd m − τa m + 1 is the time horizon for the agents MDP.",
                "Using this LP as a basis, we augment it with constraints that ensure that the resource usage implied by the agents occupation measures {xm} does not violate the global resource requirements bϕ at any time step τ ∈ [0, bτ].",
                "To formulate these resource constraints, we use the following binary variables: • Δm(τ, ω) = {0, 1}, ∀m ∈ M, τ ∈ [0, bτ], ω ∈ Ω, which serve as indicator variables that define whether agent m possesses resource ω at time τ.",
                "These are analogous to the static indicator variables used in the one-shot static resource-allocation problem in [6]. • θm = {0, 1}, ∀m ∈ M, τ ∈ [0, bτ] are indicator variables that specify whether agent m is active (i.e., executing its MDP) at time τ.",
                "The meaning of resource-usage variables Δ is illustrated in Figure 1: Δm(τ, ω) = 1 only if resource ω is allocated to agent m at time τ.",
                "The meaning of the activity indicators θ is illustrated in Figure 2b: when agent m is in either the start state sb or the finish state sf , the corresponding θm = 0, but once the agent becomes active and enters one of the other states, we set θm = 1 .",
                "This meaning of θ can be enforced with a linear constraint that synchronizes the values of the agents occupation measures xm and the activity 1224 The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) indicators θ, as shown in (6) in Table 1.",
                "Another constraint we have to add-because the activity indicators θ are defined on the global timeline τ-is to enforce the fact that the agent is inactive outside of its arrivaldeparture window.",
                "This is accomplished by constraint (7) in Table 1.",
                "Furthermore, agents should not be using resources while they are inactive.",
                "This constraint can also be enforced via a linear inequality on θ and Δ, as shown in (8).",
                "Constraint (6) sets the value of θ to match the policy defined by the occupation measure xm.",
                "In a similar fashion, we have to make sure that the resource-usage variables Δ are also synchronized with the occupation measure xm.",
                "This is done via constraint (9) in Table 1, which is nearly identical to the analogous constraint from [6].",
                "After implementing the above constraint, which enforces the meaning of Δ, we add a constraint that ensures that the agents resource usage never exceeds the amounts of available resources.",
                "This condition is also trivially expressed as a linear inequality (10) in Table 1.",
                "Finally, for the problem formulation where resource assignments are static during a lifetime of an agent, we add a constraint that ensures that the resource-usage variables Δ do not change their value while the agent is active (θ = 1).",
                "This is accomplished via the linear constraint (11), where Z ≥ 2 is a constant that is used to turn off the constraints when θm(τ) = 0 or θm(τ + 1) = 0.",
                "This constraint is not used for the dynamic problem formulation, where resources can be reallocated between agents at every time step.",
                "To summarize, Table 1 together with the conservationof-flow constraints from (12) defines the MILP that simultaneously computes an optimal resource assignment for all agents across time as well as optimal finite-horizon MDP policies that are valid under that resource assignment.",
                "As a rough measure of the complexity of this MILP, let us consider the number of optimization variables and constraints.",
                "Let TM = P Tm = P m(τa m − τd m + 1) be the sum of the lengths of the arrival-departure windows across all agents.",
                "Then, the number of optimization variables is: TM + bτ|M||Ω| + bτ|M|, TM of which are continuous (xm), and bτ|M||Ω| + bτ|M| are binary (Δ and θ).",
                "However, notice that all but TM|M| of the θ are set to zero by constraint (7), which also immediately forces all but TM|M||Ω| of the Δ to be zero via the constraints (8).",
                "The number of constraints (not including the degenerate constraints in (7)) in the MILP is: TM + TM|Ω| + bτ|Ω| + bτ|M||Ω|.",
                "Despite the fact that the complexity of the MILP is, in the worst case, exponential1 in the number of binary variables, the complexity of this MILP is significantly (exponentially) lower than that of the MILP with flat utility functions, described in Section 2.2.",
                "This result echos the efficiency gains reported in [6] for single-shot resource-allocation problems, but is much more pronounced, because of the explosion of the flat utility representation due to the temporal aspect of the problem (recall the prohibitive complexity of the combinatorial optimization in Section 2.2).",
                "We empirically analyze the performance of this method in Section 5. 1 Strictly speaking, solving MILPs to optimality is NPcomplete in the number of integer variables. 5.",
                "EXPERIMENTAL RESULTS Although the complexity of solving MILPs is in the worst case exponential in the number of integer variables, there are many efficient methods for solving MILPs that allow our algorithm to scale well for parameters common to resource allocation and scheduling problems.",
                "In particular, this section introduces a problem domain-the repairshop problem-used to empirically evaluate our algorithms scalability in terms of the number of agents |M|, the number of shared resources |Ω|, and the varied lengths of global time bτ during which agents may enter and exit the system.",
                "The repairshop problem is a simple parameterized MDP adopting the metaphor of a vehicular repair shop.",
                "Agents in the repair shop are mechanics with a number of independent tasks that yield reward only when completed.",
                "In our MDP model of this system, actions taken to advance through the state space are only allowed if the agent holds certain resources that are publicly available to the shop.",
                "These resources are in finite supply, and optimal policies for the shop will determine when each agent may hold the limited resources to take actions and earn individual rewards.",
                "Each task to be completed is associated with a single action, although the agent is required to repeat the action numerous times before completing the task and earning a reward.",
                "This model was parameterized in terms of the number of agents in the system, the number of different types of resources that could be linked to necessary actions, a global time during which agents are allowed to arrive and depart, and a maximum length for the number of time steps an agent may remain in the system.",
                "All datapoints in our experiments were obtained with 20 evaluations using CPLEX to solve the MILPs on a Pentium4 computer with 2Gb of RAM.",
                "Trials were conducted on both the static and the dynamic version of the resourcescheduling problem, as defined earlier.",
                "Figure 3 shows the runtime and policy value for independent modifications to the parameter set.",
                "The top row shows how the solution time for the MILP scales as we increase the number of agents |M|, the global time horizon bτ, and the number of resources |Ω|.",
                "Increasing the number of agents leads to exponential complexity scaling, which is to be expected for an NP-complete problem.",
                "However, increasing the global time limit bτ or the total number of resource types |Ω|-while holding the number of agents constantdoes not lead to decreased performance.",
                "This occurs because the problems get easier as they become under-constrained, which is also a common phenomenon for NP-complete problems.",
                "We also observe that the solution to the dynamic version of the problem can often be computed much faster than the static version.",
                "The bottom row of Figure 3 shows the joint policy value of the policies that correspond to the computed optimal resource-allocation schedules.",
                "We can observe that the dynamic version yields higher reward (as expected, since the reward for the dynamic version is always no less than the reward of the static version).",
                "We should point out that these graphs should not be viewed as a measure of performance of two different algorithms (both algorithms produce optimal solutions but to different problems), but rather as observations about how the quality of optimal solutions change as more flexibility is allowed in the reallocation of resources.",
                "Figure 4 shows runtime and policy value for trials in which common input variables are scaled together.",
                "This allows The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) 1225 2 4 6 8 10 10 −3 10 −2 10 −1 10 0 10 1 10 2 10 3 10 4 Number of Agents |M| CPUTime,sec |Ω| = 5, τ = 50 static dynamic 50 100 150 200 10 −2 10 −1 10 0 10 1 10 2 10 3 Global Time Boundary τ CPUTime,sec |M| = 5, |Ω| = 5 static dynamic 10 20 30 40 50 10 −2 10 −1 10 0 10 1 10 2 Number of Resources |Ω| CPUTime,sec |M| = 5, τ = 50 static dynamic 2 4 6 8 10 200 400 600 800 1000 1200 1400 1600 Number of Agents |M| Value |Ω| = 5, τ = 50 static dynamic 50 100 150 200 400 500 600 700 800 900 1000 1100 1200 1300 1400 Global Time Boundary τ Value |M| = 5, |Ω| = 5 static dynamic 10 20 30 40 50 500 600 700 800 900 1000 1100 1200 1300 1400 Number of Resources |Ω| Value |M| = 5, τ = 50 static dynamic Figure 3: Evaluation of our MILP for variable numbers of agents (column 1), lengths of global-time window (column 2), and numbers of resource types (column 3).",
                "Top row shows CPU time, and bottom row shows the joint reward of agents MDP policies.",
                "Error bars show the 1st and 3rd quartiles (25% and 75%). 2 4 6 8 10 10 −3 10 −2 10 −1 10 0 10 1 10 2 10 3 Number of Agents |M| CPUTime,sec τ = 10|M| static dynamic 2 4 6 8 10 10 −3 10 −2 10 −1 10 0 10 1 10 2 10 3 10 4 Number of Agents |M| CPUTime,sec |Ω| = 2|M| static dynamic 2 4 6 8 10 10 −3 10 −2 10 −1 10 0 10 1 10 2 10 3 10 4 Number of Agents |M| CPUTime,sec |Ω| = 5|M| static dynamic 2 4 6 8 10 200 400 600 800 1000 1200 1400 1600 1800 2000 2200 Number of Agents |M| Value τ = 10|M| static dynamic 2 4 6 8 10 200 400 600 800 1000 1200 1400 1600 1800 2000 Number of Agents |M| Value |Ω| = 2|M| static dynamic 2 4 6 8 10 0 500 1000 1500 2000 2500 Number of Agents |M| Value |Ω| = 5|M| static dynamic Figure 4: Evaluation of our MILP using correlated input variables.",
                "The left column tracks the performance and CPU time as the number of agents and global-time window increase together (bτ = 10|M|).",
                "The middle and the right column track the performance and CPU time as the number of resources and the number of agents increase together as |Ω| = 2|M| and |Ω| = 5|M|, respectively.",
                "Error bars show the 1st and 3rd quartiles (25% and 75%). 1226 The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) us to explore domains where the total number of agents scales proportionally to the total number of resource types or the global time horizon, while keeping constant the average agent density (per unit of global time) or the average number of resources per agent (which commonly occurs in real-life applications).",
                "Overall, we believe that these experimental results indicate that our MILP formulation can be used to effectively solve resource-scheduling problems of nontrivial size. 6.",
                "DISCUSSION AND CONCLUSIONS Throughout the paper, we have made a number of assumptions in our model and solution algorithm; we discuss their implications below. • Continual execution.",
                "We assume that once an agent stops executing its MDP (transitions into state sf ), it exits the system and cannot return.",
                "It is easy to relax this assumption for domains where agents MDPs can be paused and restarted.",
                "All that is required is to include an additional pause action which transitions from a given state back to itself, and has zero reward. • Indifference to start time.",
                "We used a reward model where agents rewards depend only on the time horizon of their MDPs and not the global start time.",
                "This is a consequence of our MDP-augmentation procedure from Section 4.1.",
                "It is easy to extend the model so that the agents incur an explicit penalty for idling by assigning a non-zero negative reward to the start state sb . • Binary resource requirements.",
                "For simplicity, we have assumed that resource costs are binary: ϕm(a, ω) = {0, 1}, but our results generalize in a straightforward manner to non-binary resource mappings, analogously to the procedure used in [5]. • Cooperative agents.",
                "The optimization procedure discussed in this paper was developed in the context of cooperative agents, but it can also be used to design a mechanism for scheduling resources among self-interested agents.",
                "This optimization procedure can be embedded in a VickreyClarke-Groves auction, completely analogously to the way it was done in [7].",
                "In fact, all the results of [7] about the properties of the auction and information privacy directly carry over to the scheduling domain discussed in this paper, requiring only slight modifications to deal with finitehorizon MDPs. • Known, deterministic arrival and departure times.",
                "Finally, we have assumed that agents arrival and departure times (τa m and τd m) are deterministic and known a priori.",
                "This assumption is fundamental to our solution method.",
                "While there are many domains where this assumption is valid, in many cases agents arrive and depart dynamically and their arrival and departure times can only be predicted probabilistically, leading to online resource-allocation problems.",
                "In particular, in the case of self-interested agents, this becomes an interesting version of an online-mechanism-design problem [11, 12].",
                "In summary, we have presented an MILP formulation for the combinatorial resource-scheduling problem where agents values for possible resource assignments are defined by finitehorizon MDPs.",
                "This result extends previous work ([6, 7]) on static one-shot resource allocation under MDP-induced preferences to resource-scheduling problems with a temporal aspect.",
                "As such, this work takes a step in the direction of designing an online mechanism for agents with combinatorial resource preferences induced by stochastic planning problems.",
                "Relaxing the assumption about deterministic arrival and departure times of the agents is a focus of our future work.",
                "We would like to thank the anonymous reviewers for their insightful comments and suggestions. 7.",
                "REFERENCES [1] E. Altman and A. Shwartz.",
                "Adaptive control of constrained Markov chains: Criteria and policies.",
                "Annals of Operations Research, special issue on Markov Decision Processes, 28:101-134, 1991. [2] R. Bellman.",
                "Dynamic Programming.",
                "Princeton University Press, 1957. [3] C. Boutilier.",
                "Solving concisely expressed combinatorial auction problems.",
                "In Proc. of AAAI-02, pages 359-366, 2002. [4] C. Boutilier and H. H. Hoos.",
                "Bidding languages for combinatorial auctions.",
                "In Proc. of IJCAI-01, pages 1211-1217, 2001. [5] D. Dolgov.",
                "Integrated Resource Allocation and Planning in Stochastic Multiagent Environments.",
                "PhD thesis, Computer Science Department, University of Michigan, February 2006. [6] D. A. Dolgov and E. H. Durfee.",
                "Optimal resource allocation and policy formulation in loosely-coupled Markov decision processes.",
                "In Proc. of ICAPS-04, pages 315-324, June 2004. [7] D. A. Dolgov and E. H. Durfee.",
                "Computationally efficient combinatorial auctions for resource allocation in weakly-coupled MDPs.",
                "In Proc. of AAMAS-05, New York, NY, USA, 2005.",
                "ACM Press. [8] D. A. Dolgov and E. H. Durfee.",
                "Resource allocation among agents with preferences induced by factored MDPs.",
                "In Proc. of AAMAS-06, 2006. [9] K. Larson and T. Sandholm.",
                "Mechanism design and deliberative agents.",
                "In Proc. of AAMAS-05, pages 650-656, New York, NY, USA, 2005.",
                "ACM Press. [10] N. Nisan.",
                "Bidding and allocation in combinatorial auctions.",
                "In Electronic Commerce, 2000. [11] D. C. Parkes and S. Singh.",
                "An MDP-based approach to Online Mechanism Design.",
                "In Proc. of the Seventeenths Annual Conference on Neural Information Processing Systems (NIPS-03), 2003. [12] D. C. Parkes, S. Singh, and D. Yanovsky.",
                "Approximately efficient online mechanism design.",
                "In Proc. of the Eighteenths Annual Conference on Neural Information Processing Systems (NIPS-04), 2004. [13] M. L. Puterman.",
                "Markov Decision Processes.",
                "John Wiley & Sons, New York, 1994. [14] M. H. Rothkopf, A. Pekec, and R. M. Harstad.",
                "Computationally manageable combinational auctions.",
                "Management Science, 44(8):1131-1147, 1998. [15] T. Sandholm.",
                "An algorithm for optimal winner determination in combinatorial auctions.",
                "In Proc. of IJCAI-99, pages 542-547, San Francisco, CA, USA, 1999.",
                "Morgan Kaufmann Publishers Inc.",
                "The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) 1227"
            ],
            "original_annotated_samples": [],
            "translated_annotated_samples": [
                "Esta representación se puede utilizar para construir algoritmos de asignación de recursos muy eficientes que conducen a una aceleración exponencial sobre un \"problema de optimización\" directo con representaciones planas de preferencias combinatorias [6, 7, 8].problema de optimizacion",
                "Sin embargo, dado que el enfoque de nuestro trabajo está en los problemas de programación, y una gran parte del \"problema de optimización\" es decidir cómo se asignan los recursos a tiempo entre los agentes con tiempos de llegada y salida finitos, modelamos los agentes que planean los problemas como finitos.MDPS Horizon, en contraste con trabajos anteriores que utilizaron MDP con descuento de Horizon Infinite.problema de optimizacion",
                "Dada la entrada anterior, el \"problema de optimización\" que consideramos es encontrar la maximización óptima globalmente óptima, la suma del mapeo de recompensas esperados de los recursos a los agentes para los pasos de tiempo: δ: τ × m × ω → {0, 1}.problema de optimizacion",
                "En segundo lugar, utilizando estos MDP aumentados, construimos un \"problema de optimización\" global, que se describe en la Sección 4.2.4.1 Agentes de aumento MDP en el modelo descrito en la sección anterior, suponemos que si un agente no posee los recursos necesarios para realizar acciones en su MDP, su ejecución se detiene y el agente deja el sistema.problema de optimizacion",
                "El \"problema de optimización\" resultante resuelve simultáneamente los MDP de los agentes y los problemas de programación de recursos.problema de optimizacion"
            ],
            "translated_text": "",
            "candidates": [],
            "error": []
        },
        "utility function": {
            "translated_key": "",
            "is_in_text": true,
            "original_annotated_sentences": [
                "Combinatorial Resource Scheduling for Multiagent MDPs Dmitri A. Dolgov, Michael R. James, and Michael E. Samples AI and Robotics Group Technical Research, Toyota Technical Center USA {ddolgov, michael.r.james, michael.samples}@gmail.com ABSTRACT Optimal resource scheduling in multiagent systems is a computationally challenging task, particularly when the values of resources are not additive.",
                "We consider the combinatorial problem of scheduling the usage of multiple resources among agents that operate in stochastic environments, modeled as Markov decision processes (MDPs).",
                "In recent years, efficient resource-allocation algorithms have been developed for agents with resource values induced by MDPs.",
                "However, this prior work has focused on static resource-allocation problems where resources are distributed once and then utilized in infinite-horizon MDPs.",
                "We extend those existing models to the problem of combinatorial resource scheduling, where agents persist only for finite periods between their (predefined) arrival and departure times, requiring resources only for those time periods.",
                "We provide a computationally efficient procedure for computing globally optimal resource assignments to agents over time.",
                "We illustrate and empirically analyze the method in the context of a stochastic jobscheduling domain.",
                "Categories and Subject Descriptors I.2.8 [Artificial Intelligence]: Problem Solving, Control Methods, and Search; I.2.11 [Artificial Intelligence]: Distributed Artificial Intelligence-Multiagent systems General Terms Algorithms, Performance, Design 1.",
                "INTRODUCTION The tasks of optimal resource allocation and scheduling are ubiquitous in multiagent systems, but solving such optimization problems can be computationally difficult, due to a number of factors.",
                "In particular, when the value of a set of resources to an agent is not additive (as is often the case with resources that are substitutes or complements), the <br>utility function</br> might have to be defined on an exponentially large space of resource bundles, which very quickly becomes computationally intractable.",
                "Further, even when each agent has a <br>utility function</br> that is nonzero only on a small subset of the possible resource bundles, obtaining optimal allocation is still computationally prohibitive, as the problem becomes NP-complete [14].",
                "Such computational issues have recently spawned several threads of work in using compact models of agents preferences.",
                "One idea is to use any structure present in utility functions to represent them compactly, via, for example, logical formulas [15, 10, 4, 3].",
                "An alternative is to directly model the mechanisms that define the agents utility functions and perform resource allocation directly with these models [9].",
                "A way of accomplishing this is to model the processes by which an agent might utilize the resources and define the <br>utility function</br> as the payoff of these processes.",
                "In particular, if an agent uses resources to act in a stochastic environment, its <br>utility function</br> can be naturally modeled with a Markov decision process, whose action set is parameterized by the available resources.",
                "This representation can then be used to construct very efficient resource-allocation algorithms that lead to an exponential speedup over a straightforward optimization problem with flat representations of combinatorial preferences [6, 7, 8].",
                "However, this existing work on resource allocation with preferences induced by resource-parameterized MDPs makes an assumption that the resources are only allocated once and are then utilized by the agents independently within their infinite-horizon MDPs.",
                "This assumption that no reallocation of resources is possible can be limiting in domains where agents arrive and depart dynamically.",
                "In this paper, we extend the work on resource allocation under MDP-induced preferences to discrete-time scheduling problems, where agents are present in the system for finite time intervals and can only use resources within these intervals.",
                "In particular, agents arrive and depart at arbitrary (predefined) times and within these intervals use resources to execute tasks in finite-horizon MDPs.",
                "We address the problem of globally optimal resource scheduling, where the objective is to find an allocation of resources to the agents across time that maximizes the sum of the expected rewards that they obtain.",
                "In this context, our main contribution is a mixed-integerprogramming formulation of the scheduling problem that chooses globally optimal resource assignments, starting times, and execution horizons for all agents (within their arrival1220 978-81-904262-7-5 (RPS) c 2007 IFAAMAS departure intervals).",
                "We analyze and empirically compare two flavors of the scheduling problem: one, where agents have static resource assignments within their finite-horizon MDPs, and another, where resources can be dynamically reallocated between agents at every time step.",
                "In the rest of the paper, we first lay down the necessary groundwork in Section 2 and then introduce our model and formal problem statement in Section 3.",
                "In Section 4.2, we describe our main result, the optimization program for globally optimal resource scheduling.",
                "Following the discussion of our experimental results on a job-scheduling problem in Section 5, we conclude in Section 6 with a discussion of possible extensions and generalizations of our method. 2.",
                "BACKGROUND Similarly to the model used in previous work on resourceallocation with MDP-induced preferences [6, 7], we define the value of a set of resources to an agent as the value of the best MDP policy that is realizable, given those resources.",
                "However, since the focus of our work is on scheduling problems, and a large part of the optimization problem is to decide how resources are allocated in time among agents with finite arrival and departure times, we model the agents planning problems as finite-horizon MDPs, in contrast to previous work that used infinite-horizon discounted MDPs.",
                "In the rest of this section, we first introduce some necessary background on finite-horizon MDPs and present a linear-programming formulation that serves as the basis for our solution algorithm developed in Section 4.",
                "We also outline the standard methods for combinatorial resource scheduling with flat resource values, which serve as a comparison benchmark for the new model developed here. 2.1 Markov Decision Processes A stationary, finite-domain, discrete-time MDP (see, for example, [13] for a thorough and detailed development) can be described as S, A, p, r , where: S is a finite set of system states; A is a finite set of actions that are available to the agent; p is a stationary stochastic transition function, where p(σ|s, a) is the probability of transitioning to state σ upon executing action a in state s; r is a stationary reward function, where r(s, a) specifies the reward obtained upon executing action a in state s. Given such an MDP, a decision problem under a finite horizon T is to choose an optimal action at every time step to maximize the expected value of the total reward accrued during the agents (finite) lifetime.",
                "The agents optimal policy is then a function of current state s and the time until the horizon.",
                "An optimal policy for such a problem is to act greedily with respect to the optimal value function, defined recursively by the following system of finite-time Bellman equations [2]: v(s, t) = max a r(s, a) + X σ p(σ|s, a)v(σ, t + 1), ∀s ∈ S, t ∈ [1, T − 1]; v(s, T) = 0, ∀s ∈ S; where v(s, t) is the optimal value of being in state s at time t ∈ [1, T].",
                "This optimal value function can be easily computed using dynamic programming, leading to the following optimal policy π, where π(s, a, t) is the probability of executing action a in state s at time t: π(s, a, t) = ( 1, a = argmaxa r(s, a) + P σ p(σ|s, a)v(σ, t + 1), 0, otherwise.",
                "The above is the most common way of computing the optimal value function (and therefore an optimal policy) for a finite-horizon MDP.",
                "However, we can also formulate the problem as the following linear program (similarly to the dual LP for infinite-horizon discounted MDPs [13, 6, 7]): max X s X a r(s, a) X t x(s, a, t) subject to: X a x(σ, a, t + 1) = X s,a p(σ|s, a)x(s, a, t) ∀σ, t ∈ [1, T − 1]; X a x(s, a, 1) = α(s), ∀s ∈ S; (1) where α(s) is the initial distribution over the state space, and x is the (non-stationary) occupation measure (x(s, a, t) ∈ [0, 1] is the total expected number of times action a is executed in state s at time t).",
                "An optimal (non-stationary) policy is obtained from the occupation measure as follows: π(s, a, t) = x(s, a, t)/ X a x(s, a, t) ∀s ∈ S, t ∈ [1, T]. (2) Note that the standard unconstrained finite-horizon MDP, as described above, always has a uniformly-optimal solution (optimal for any initial distribution α(s)).",
                "Therefore, an optimal policy can be obtained by using an arbitrary constant α(s) > 0 (in particular, α(s) = 1 will result in x(s, a, t) = π(s, a, t)).",
                "However, for MDPs with resource constraints (as defined below in Section 3), uniformly-optimal policies do not in general exist.",
                "In such cases, α becomes a part of the problem input, and a resulting policy is only optimal for that particular α.",
                "This result is well known for infinite-horizon MDPs with various types of constraints [1, 6], and it also holds for our finite-horizon model, which can be easily established via a line of reasoning completely analogous to the arguments in [6]. 2.2 Combinatorial Resource Scheduling A straightforward approach to resource scheduling for a set of agents M, whose values for the resources are induced by stochastic planning problems (in our case, finite-horizon MDPs) would be to have each agent enumerate all possible resource assignments over time and, for each one, compute its value by solving the corresponding MDP.",
                "Then, each agent would provide valuations for each possible resource bundle over time to a centralized coordinator, who would compute the optimal resource assignments across time based on these valuations.",
                "When resources can be allocated at different times to different agents, each agent must submit valuations for every combination of possible time horizons.",
                "Let each agent m ∈ M execute its MDP within the arrival-departure time interval τ ∈ [τa m, τd m].",
                "Hence, agent m will execute an MDP with time horizon no greater than Tm = τd m−τa m+1.",
                "Let bτ be the global time horizon for the problem, before which all of the agents MDPs must finish.",
                "We assume τd m < bτ, ∀m ∈ M. The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) 1221 For the scheduling problem where agents have static resource requirements within their finite-horizon MDPs, the agents provide a valuation for each resource bundle for each possible time horizon (from [1, Tm]) that they may use.",
                "Let Ω be the set of resources to be allocated among the agents.",
                "An agent will get at most one resource bundle for one of the time horizons.",
                "Let the variable ψ ∈ Ψm enumerate all possible pairs of resource bundles and time horizons for agent m, so there are 2|Ω| × Tm values for ψ (the space of bundles is exponential in the number of resource types |Ω|).",
                "The agent m must provide a value vψ m for each ψ, and the coordinator will allocate at most one ψ (resource, time horizon) pair to each agent.",
                "This allocation is expressed as an indicator variable zψ m ∈ {0, 1} that shows whether ψ is assigned to agent m. For time τ and resource ω, the function nm(ψ, τ, ω) ∈ {0, 1} indicates whether the bundle in ψ uses resource ω at time τ (we make the assumption that agents have binary resource requirements).",
                "This allocation problem is NP-complete, even when considering only a single time step, and its difficulty increases significantly with multiple time steps because of the increasing number of values of ψ.",
                "The problem of finding an optimal allocation that satisfies the global constraint that the amount of each resource ω allocated to all agents does not exceed the available amount bϕ(ω) can be expressed as the following integer program: max X m∈M X ψ∈Ψm zψ mvψ m subject to: X ψ∈Ψm zψ m ≤ 1, ∀m ∈ M; X m∈M X ψ∈Ψm zψ mnm(ψ, τ, ω) ≤ bϕ(ω), ∀τ ∈ [1, bτ], ∀ω ∈ Ω; (3) The first constraint in equation 3 says that no agent can receive more than one bundle, and the second constraint ensures that the total assignment of resource ω does not, at any time, exceed the resource bound.",
                "For the scheduling problem where the agents are able to dynamically reallocate resources, each agent must specify a value for every combination of bundles and time steps within its time horizon.",
                "Let the variable ψ ∈ Ψm in this case enumerate all possible resource bundles for which at most one bundle may be assigned to agent m at each time step.",
                "Therefore, in this case there are P t∈[1,Tm](2|Ω| )t ∼ 2|Ω|Tm possibilities of resource bundles assigned to different time slots, for the Tm different time horizons.",
                "The same set of equations (3) can be used to solve this dynamic scheduling problem, but the integer program is different because of the difference in how ψ is defined.",
                "In this case, the number of ψ values is exponential in each agents planning horizon Tm, resulting in a much larger program.",
                "This straightforward approach to solving both of these scheduling problems requires an enumeration and solution of either 2|Ω| Tm (static allocation) or P t∈[1,Tm] 2|Ω|t (dynamic reallocation) MDPs for each agent, which very quickly becomes intractable with the growth of the number of resources |Ω| or the time horizon Tm. 3.",
                "MODEL AND PROBLEM STATEMENT We now formally introduce our model of the resourcescheduling problem.",
                "The problem input consists of the following components: • M, Ω, bϕ, τa m, τd m, bτ are as defined above in Section 2.2. • {Θm} = {S, A, pm, rm, αm} are the MDPs of all agents m ∈ M. Without loss of generality, we assume that state and action spaces of all agents are the same, but each has its own transition function pm, reward function rm, and initial conditions αm. • ϕm : A×Ω → {0, 1} is the mapping of actions to resources for agent m. ϕm(a, ω) indicates whether action a of agent m needs resource ω.",
                "An agent m that receives a set of resources that does not include resource ω cannot execute in its MDP policy any action a for which ϕm(a, ω) = 0.",
                "We assume all resource requirements are binary; as discussed below in Section 6, this assumption is not limiting.",
                "Given the above input, the optimization problem we consider is to find the globally optimal-maximizing the sum of expected rewards-mapping of resources to agents for all time steps: Δ : τ × M × Ω → {0, 1}.",
                "A solution is feasible if the corresponding assignment of resources to the agents does not violate the global resource constraint: X m Δm(τ, ω) ≤ bϕ(ω), ∀ω ∈ Ω, τ ∈ [1, bτ]. (4) We consider two flavors of the resource-scheduling problem.",
                "The first formulation restricts resource assignments to the space where the allocation of resources to each agent is static during the agents lifetime.",
                "The second formulation allows reassignment of resources between agents at every time step within their lifetimes.",
                "Figure 1 depicts a resource-scheduling problem with three agents M = {m1, m2, m3}, three resources Ω = {ω1, ω2, ω3}, and a global problem horizon of bτ = 11.",
                "The agents arrival and departure times are shown as gray boxes and are {1, 6}, {3, 7}, and {2, 11}, respectively.",
                "A solution to this problem is shown via horizontal bars within each agents box, where the bars correspond to the allocation of the three resource types.",
                "Figure 1a shows a solution to a static scheduling problem.",
                "According to the shown solution, agent m1 begins the execution of its MDP at time τ = 1 and has a lock on all three resources until it finishes execution at time τ = 3.",
                "Note that agent m1 relinquishes its hold on the resources before its announced departure time of τd m1 = 6, ostensibly because other agents can utilize the resources more effectively.",
                "Thus, at time τ = 4, resources ω1 and ω3 are allocated to agent m2, who then uses them to execute its MDP (using only actions supported by resources ω1 and ω3) until time τ = 7.",
                "Agent m3 holds resource ω3 during the interval τ ∈ [4, 10].",
                "Figure 1b shows a possible solution to the dynamic version of the same problem.",
                "There, resources can be reallocated between agents at every time step.",
                "For example, agent m1 gives up its use of resource ω2 at time τ = 2, although it continues the execution of its MDP until time τ = 6.",
                "Notice that an agent is not allowed to stop and restart its MDP, so agent m1 is only able to continue executing in the interval τ ∈ [3, 4] if it has actions that do not require any resources (ϕm(a, ω) = 0).",
                "Clearly, the model and problem statement described above make a number of assumptions about the problem and the desired solution properties.",
                "We discuss some of those assumptions and their implications in Section 6. 1222 The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) (a) (b) Figure 1: Illustration of a solution to a resource-scheduling problem with three agents and three resources: a) static resource assignments (resource assignments are constant within agents lifetimes; b) dynamic assignment (resource assignments are allowed to change at every time step). 4.",
                "RESOURCE SCHEDULING Our resource-scheduling algorithm proceeds in two stages.",
                "First, we perform a preprocessing step that augments the agent MDPs; this process is described in Section 4.1.",
                "Second, using these augmented MDPs we construct a global optimization problem, which is described in Section 4.2. 4.1 Augmenting Agents MDPs In the model described in the previous section, we assume that if an agent does not possess the necessary resources to perform actions in its MDP, its execution is halted and the agent leaves the system.",
                "In other words, the MDPs cannot be paused and resumed.",
                "For example, in the problem shown in Figure 1a, agent m1 releases all resources after time τ = 3, at which point the execution of its MDP is halted.",
                "Similarly, agents m2 and m3 only execute their MDPs in the intervals τ ∈ [4, 6] and τ ∈ [4, 10], respectively.",
                "Therefore, an important part of the global decision-making problem is to decide the window of time during which each of the agents is active (i.e., executing its MDP).",
                "To accomplish this, we augment each agents MDP with two new states (start and finish states sb , sf , respectively) and a new start/stop action a∗ , as illustrated in Figure 2.",
                "The idea is that an agent stays in the start state sb until it is ready to execute its MDP, at which point it performs the start/stop action a∗ and transitions into the state space of the original MDP with the transition probability that corresponds to the original initial distribution α(s).",
                "For example, in Figure 1a, for agent m2 this would happen at time τ = 4.",
                "Once the agent gets to the end of its activity window (time τ = 6 for agent m2 in Figure 1a), it performs the start/stop action, which takes it into the sink finish state sf at time τ = 7.",
                "More precisely, given an MDP S, A, pm, rm, αm , we define an augmented MDP S , A , pm, rm, αm as follows: S = S ∪ sb ∪ sf ; A = A ∪ a∗ ; p (s|sb , a∗ ) = α(s), ∀s ∈ S; p (sb |sb , a) = 1.0, ∀a ∈ A; p (sf |s, a∗ ) = 1.0, ∀s ∈ S; p (σ|s, a) = p(σ|s, a), ∀s, σ ∈ S, a ∈ A; r (sb , a) = r (sf , a) = 0, ∀a ∈ A ; r (s, a) = r(s, a), ∀s ∈ S, a ∈ A; α (sb ) = 1; α (s) = 0, ∀s ∈ S; where all non-specified transition probabilities are assumed to be zero.",
                "Further, in order to account for the new starting state, we begin the MDP one time-step earlier, setting τa m ← τa m − 1.",
                "This will not affect the resource allocation due to the resource constraints only being enforced for the original MDP states, as will be discussed in the next section.",
                "For example, the augmented MDPs shown in Figure 2b (which starts in state sb at time τ = 2) would be constructed from an MDP with original arrival time τ = 3.",
                "Figure 2b also shows a sample trajectory through the state space: the agent starts in state sb , transitions into the state space S of the original MDP, and finally exists into the sink state sf .",
                "Note that if we wanted to model a problem where agents could pause their MDPs at arbitrary time steps (which might be useful for domains where dynamic reallocation is possible), we could easily accomplish this by including an extra action that transitions from each state to itself with zero reward. 4.2 MILP for Resource Scheduling Given a set of augmented MDPs, as defined above, the goal of this section is to formulate a global optimization program that solves the resource-scheduling problem.",
                "In this section and below, all MDPs are assumed to be the augmented MDPs as defined in Section 4.1.",
                "Our approach is similar to the idea used in [6]: we begin with the linear-program formulation of agents MDPs (1) and augment it with constraints that ensure that the corresponding resource allocation across agents and time is valid.",
                "The resulting optimization problem then simultaneously solves the agents MDPs and resource-scheduling problems.",
                "In the rest of this section, we incrementally develop a mixed integer program (MILP) that achieves this.",
                "In the absence of resource constraints, the agents finitehorizon MDPs are completely independent, and the globally optimal solution can be trivially obtained via the following LP, which is simply an aggregation of single-agent finitehorizon LPs: max X m X s X a rm(s, a) X t xm(s, a, t) subject to: X a xm(σ, a, t + 1) = X s,a pm(σ|s, a)xm(s, a, t), ∀m ∈ M, σ ∈ S, t ∈ [1, Tm − 1]; X a xm(s, a, 1) = αm(s), ∀m ∈ M, s ∈ S; (12) where xm(s, a, t) is the occupation measure of agent m, and The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) 1223 (a) (b) Figure 2: Illustration of augmenting an MDP to allow for variable starting and stopping times: a) (left) the original two-state MDP with a single action; (right) the augmented MDP with new states sb and sf and the new action a∗ (note that the origianl transitions are not changed in the augmentation process); b) the augmented MDP displayed as a trajectory through time (grey lines indicate all transitions, while black lines indicate a given trajectory.",
                "Objective Function (sum of expected rewards over all agents) max X m X s X a rm(s, a) X t xm(s, a, t) (5) Meaning Implication Linear Constraints Tie x to θ.",
                "Agent is only active when occupation measure is nonzero in original MDP states. θm(τ) = 0 =⇒ xm(s, a, τ −τa m+1) = 0 ∀s /∈ {sb , sf }, a ∈ A X s/∈{sb,sf } X a xm(s, a, t) ≤ θm(τa m + t − 1) ∀m ∈ M, ∀t ∈ [1, Tm] (6) Agent can only be active in τ ∈ (τa m, τd m) θm(τ) = 0 ∀m ∈ M, τ /∈ (τa m, τd m) (7) Cannot use resources when not active θm(τ) = 0 =⇒ Δm(τ, ω) = 0 ∀τ ∈ [0, bτ], ω ∈ Ω Δm(τ, ω) ≤ θm(τ) ∀m ∈ M, τ ∈ [0, bτ], ω ∈ Ω (8) Tie x to Δ (nonzero x forces corresponding Δ to be nonzero.)",
                "Δm(τ, ω) = 0, ϕm(a, ω) = 1 =⇒ xm(s, a, τ − τa m + 1) = 0 ∀s /∈ {sb , sf } 1/|A| X a ϕm(a, ω) X s/∈{sb,sf } xm(s, a, t) ≤ Δm(t + τa m − 1, ω) ∀m ∈ M, ω ∈ Ω, t ∈ [1, Tm] (9) Resource bounds X m Δm(τ, ω) ≤ bϕ(ω) ∀ω ∈ Ω, τ ∈ [0, bτ] (10) Agent cannot change resources while active.",
                "Only enabled for scheduling with static assignments. θm(τ) = 1 and θm(τ + 1) = 1 =⇒ Δm(τ, ω) = Δm(τ + 1, ω) Δm(τ, ω) − Z(1 − θm(τ + 1)) ≤ Δm(τ + 1, ω) + Z(1 − θm(τ)) Δm(τ, ω) + Z(1 − θm(τ + 1)) ≥ Δm(τ + 1, ω) − Z(1 − θm(τ)) ∀m ∈ M, ω ∈ Ω, τ ∈ [0, bτ] (11) Table 1: MILP for globally optimal resource scheduling.",
                "Tm = τd m − τa m + 1 is the time horizon for the agents MDP.",
                "Using this LP as a basis, we augment it with constraints that ensure that the resource usage implied by the agents occupation measures {xm} does not violate the global resource requirements bϕ at any time step τ ∈ [0, bτ].",
                "To formulate these resource constraints, we use the following binary variables: • Δm(τ, ω) = {0, 1}, ∀m ∈ M, τ ∈ [0, bτ], ω ∈ Ω, which serve as indicator variables that define whether agent m possesses resource ω at time τ.",
                "These are analogous to the static indicator variables used in the one-shot static resource-allocation problem in [6]. • θm = {0, 1}, ∀m ∈ M, τ ∈ [0, bτ] are indicator variables that specify whether agent m is active (i.e., executing its MDP) at time τ.",
                "The meaning of resource-usage variables Δ is illustrated in Figure 1: Δm(τ, ω) = 1 only if resource ω is allocated to agent m at time τ.",
                "The meaning of the activity indicators θ is illustrated in Figure 2b: when agent m is in either the start state sb or the finish state sf , the corresponding θm = 0, but once the agent becomes active and enters one of the other states, we set θm = 1 .",
                "This meaning of θ can be enforced with a linear constraint that synchronizes the values of the agents occupation measures xm and the activity 1224 The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) indicators θ, as shown in (6) in Table 1.",
                "Another constraint we have to add-because the activity indicators θ are defined on the global timeline τ-is to enforce the fact that the agent is inactive outside of its arrivaldeparture window.",
                "This is accomplished by constraint (7) in Table 1.",
                "Furthermore, agents should not be using resources while they are inactive.",
                "This constraint can also be enforced via a linear inequality on θ and Δ, as shown in (8).",
                "Constraint (6) sets the value of θ to match the policy defined by the occupation measure xm.",
                "In a similar fashion, we have to make sure that the resource-usage variables Δ are also synchronized with the occupation measure xm.",
                "This is done via constraint (9) in Table 1, which is nearly identical to the analogous constraint from [6].",
                "After implementing the above constraint, which enforces the meaning of Δ, we add a constraint that ensures that the agents resource usage never exceeds the amounts of available resources.",
                "This condition is also trivially expressed as a linear inequality (10) in Table 1.",
                "Finally, for the problem formulation where resource assignments are static during a lifetime of an agent, we add a constraint that ensures that the resource-usage variables Δ do not change their value while the agent is active (θ = 1).",
                "This is accomplished via the linear constraint (11), where Z ≥ 2 is a constant that is used to turn off the constraints when θm(τ) = 0 or θm(τ + 1) = 0.",
                "This constraint is not used for the dynamic problem formulation, where resources can be reallocated between agents at every time step.",
                "To summarize, Table 1 together with the conservationof-flow constraints from (12) defines the MILP that simultaneously computes an optimal resource assignment for all agents across time as well as optimal finite-horizon MDP policies that are valid under that resource assignment.",
                "As a rough measure of the complexity of this MILP, let us consider the number of optimization variables and constraints.",
                "Let TM = P Tm = P m(τa m − τd m + 1) be the sum of the lengths of the arrival-departure windows across all agents.",
                "Then, the number of optimization variables is: TM + bτ|M||Ω| + bτ|M|, TM of which are continuous (xm), and bτ|M||Ω| + bτ|M| are binary (Δ and θ).",
                "However, notice that all but TM|M| of the θ are set to zero by constraint (7), which also immediately forces all but TM|M||Ω| of the Δ to be zero via the constraints (8).",
                "The number of constraints (not including the degenerate constraints in (7)) in the MILP is: TM + TM|Ω| + bτ|Ω| + bτ|M||Ω|.",
                "Despite the fact that the complexity of the MILP is, in the worst case, exponential1 in the number of binary variables, the complexity of this MILP is significantly (exponentially) lower than that of the MILP with flat utility functions, described in Section 2.2.",
                "This result echos the efficiency gains reported in [6] for single-shot resource-allocation problems, but is much more pronounced, because of the explosion of the flat utility representation due to the temporal aspect of the problem (recall the prohibitive complexity of the combinatorial optimization in Section 2.2).",
                "We empirically analyze the performance of this method in Section 5. 1 Strictly speaking, solving MILPs to optimality is NPcomplete in the number of integer variables. 5.",
                "EXPERIMENTAL RESULTS Although the complexity of solving MILPs is in the worst case exponential in the number of integer variables, there are many efficient methods for solving MILPs that allow our algorithm to scale well for parameters common to resource allocation and scheduling problems.",
                "In particular, this section introduces a problem domain-the repairshop problem-used to empirically evaluate our algorithms scalability in terms of the number of agents |M|, the number of shared resources |Ω|, and the varied lengths of global time bτ during which agents may enter and exit the system.",
                "The repairshop problem is a simple parameterized MDP adopting the metaphor of a vehicular repair shop.",
                "Agents in the repair shop are mechanics with a number of independent tasks that yield reward only when completed.",
                "In our MDP model of this system, actions taken to advance through the state space are only allowed if the agent holds certain resources that are publicly available to the shop.",
                "These resources are in finite supply, and optimal policies for the shop will determine when each agent may hold the limited resources to take actions and earn individual rewards.",
                "Each task to be completed is associated with a single action, although the agent is required to repeat the action numerous times before completing the task and earning a reward.",
                "This model was parameterized in terms of the number of agents in the system, the number of different types of resources that could be linked to necessary actions, a global time during which agents are allowed to arrive and depart, and a maximum length for the number of time steps an agent may remain in the system.",
                "All datapoints in our experiments were obtained with 20 evaluations using CPLEX to solve the MILPs on a Pentium4 computer with 2Gb of RAM.",
                "Trials were conducted on both the static and the dynamic version of the resourcescheduling problem, as defined earlier.",
                "Figure 3 shows the runtime and policy value for independent modifications to the parameter set.",
                "The top row shows how the solution time for the MILP scales as we increase the number of agents |M|, the global time horizon bτ, and the number of resources |Ω|.",
                "Increasing the number of agents leads to exponential complexity scaling, which is to be expected for an NP-complete problem.",
                "However, increasing the global time limit bτ or the total number of resource types |Ω|-while holding the number of agents constantdoes not lead to decreased performance.",
                "This occurs because the problems get easier as they become under-constrained, which is also a common phenomenon for NP-complete problems.",
                "We also observe that the solution to the dynamic version of the problem can often be computed much faster than the static version.",
                "The bottom row of Figure 3 shows the joint policy value of the policies that correspond to the computed optimal resource-allocation schedules.",
                "We can observe that the dynamic version yields higher reward (as expected, since the reward for the dynamic version is always no less than the reward of the static version).",
                "We should point out that these graphs should not be viewed as a measure of performance of two different algorithms (both algorithms produce optimal solutions but to different problems), but rather as observations about how the quality of optimal solutions change as more flexibility is allowed in the reallocation of resources.",
                "Figure 4 shows runtime and policy value for trials in which common input variables are scaled together.",
                "This allows The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) 1225 2 4 6 8 10 10 −3 10 −2 10 −1 10 0 10 1 10 2 10 3 10 4 Number of Agents |M| CPUTime,sec |Ω| = 5, τ = 50 static dynamic 50 100 150 200 10 −2 10 −1 10 0 10 1 10 2 10 3 Global Time Boundary τ CPUTime,sec |M| = 5, |Ω| = 5 static dynamic 10 20 30 40 50 10 −2 10 −1 10 0 10 1 10 2 Number of Resources |Ω| CPUTime,sec |M| = 5, τ = 50 static dynamic 2 4 6 8 10 200 400 600 800 1000 1200 1400 1600 Number of Agents |M| Value |Ω| = 5, τ = 50 static dynamic 50 100 150 200 400 500 600 700 800 900 1000 1100 1200 1300 1400 Global Time Boundary τ Value |M| = 5, |Ω| = 5 static dynamic 10 20 30 40 50 500 600 700 800 900 1000 1100 1200 1300 1400 Number of Resources |Ω| Value |M| = 5, τ = 50 static dynamic Figure 3: Evaluation of our MILP for variable numbers of agents (column 1), lengths of global-time window (column 2), and numbers of resource types (column 3).",
                "Top row shows CPU time, and bottom row shows the joint reward of agents MDP policies.",
                "Error bars show the 1st and 3rd quartiles (25% and 75%). 2 4 6 8 10 10 −3 10 −2 10 −1 10 0 10 1 10 2 10 3 Number of Agents |M| CPUTime,sec τ = 10|M| static dynamic 2 4 6 8 10 10 −3 10 −2 10 −1 10 0 10 1 10 2 10 3 10 4 Number of Agents |M| CPUTime,sec |Ω| = 2|M| static dynamic 2 4 6 8 10 10 −3 10 −2 10 −1 10 0 10 1 10 2 10 3 10 4 Number of Agents |M| CPUTime,sec |Ω| = 5|M| static dynamic 2 4 6 8 10 200 400 600 800 1000 1200 1400 1600 1800 2000 2200 Number of Agents |M| Value τ = 10|M| static dynamic 2 4 6 8 10 200 400 600 800 1000 1200 1400 1600 1800 2000 Number of Agents |M| Value |Ω| = 2|M| static dynamic 2 4 6 8 10 0 500 1000 1500 2000 2500 Number of Agents |M| Value |Ω| = 5|M| static dynamic Figure 4: Evaluation of our MILP using correlated input variables.",
                "The left column tracks the performance and CPU time as the number of agents and global-time window increase together (bτ = 10|M|).",
                "The middle and the right column track the performance and CPU time as the number of resources and the number of agents increase together as |Ω| = 2|M| and |Ω| = 5|M|, respectively.",
                "Error bars show the 1st and 3rd quartiles (25% and 75%). 1226 The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) us to explore domains where the total number of agents scales proportionally to the total number of resource types or the global time horizon, while keeping constant the average agent density (per unit of global time) or the average number of resources per agent (which commonly occurs in real-life applications).",
                "Overall, we believe that these experimental results indicate that our MILP formulation can be used to effectively solve resource-scheduling problems of nontrivial size. 6.",
                "DISCUSSION AND CONCLUSIONS Throughout the paper, we have made a number of assumptions in our model and solution algorithm; we discuss their implications below. • Continual execution.",
                "We assume that once an agent stops executing its MDP (transitions into state sf ), it exits the system and cannot return.",
                "It is easy to relax this assumption for domains where agents MDPs can be paused and restarted.",
                "All that is required is to include an additional pause action which transitions from a given state back to itself, and has zero reward. • Indifference to start time.",
                "We used a reward model where agents rewards depend only on the time horizon of their MDPs and not the global start time.",
                "This is a consequence of our MDP-augmentation procedure from Section 4.1.",
                "It is easy to extend the model so that the agents incur an explicit penalty for idling by assigning a non-zero negative reward to the start state sb . • Binary resource requirements.",
                "For simplicity, we have assumed that resource costs are binary: ϕm(a, ω) = {0, 1}, but our results generalize in a straightforward manner to non-binary resource mappings, analogously to the procedure used in [5]. • Cooperative agents.",
                "The optimization procedure discussed in this paper was developed in the context of cooperative agents, but it can also be used to design a mechanism for scheduling resources among self-interested agents.",
                "This optimization procedure can be embedded in a VickreyClarke-Groves auction, completely analogously to the way it was done in [7].",
                "In fact, all the results of [7] about the properties of the auction and information privacy directly carry over to the scheduling domain discussed in this paper, requiring only slight modifications to deal with finitehorizon MDPs. • Known, deterministic arrival and departure times.",
                "Finally, we have assumed that agents arrival and departure times (τa m and τd m) are deterministic and known a priori.",
                "This assumption is fundamental to our solution method.",
                "While there are many domains where this assumption is valid, in many cases agents arrive and depart dynamically and their arrival and departure times can only be predicted probabilistically, leading to online resource-allocation problems.",
                "In particular, in the case of self-interested agents, this becomes an interesting version of an online-mechanism-design problem [11, 12].",
                "In summary, we have presented an MILP formulation for the combinatorial resource-scheduling problem where agents values for possible resource assignments are defined by finitehorizon MDPs.",
                "This result extends previous work ([6, 7]) on static one-shot resource allocation under MDP-induced preferences to resource-scheduling problems with a temporal aspect.",
                "As such, this work takes a step in the direction of designing an online mechanism for agents with combinatorial resource preferences induced by stochastic planning problems.",
                "Relaxing the assumption about deterministic arrival and departure times of the agents is a focus of our future work.",
                "We would like to thank the anonymous reviewers for their insightful comments and suggestions. 7.",
                "REFERENCES [1] E. Altman and A. Shwartz.",
                "Adaptive control of constrained Markov chains: Criteria and policies.",
                "Annals of Operations Research, special issue on Markov Decision Processes, 28:101-134, 1991. [2] R. Bellman.",
                "Dynamic Programming.",
                "Princeton University Press, 1957. [3] C. Boutilier.",
                "Solving concisely expressed combinatorial auction problems.",
                "In Proc. of AAAI-02, pages 359-366, 2002. [4] C. Boutilier and H. H. Hoos.",
                "Bidding languages for combinatorial auctions.",
                "In Proc. of IJCAI-01, pages 1211-1217, 2001. [5] D. Dolgov.",
                "Integrated Resource Allocation and Planning in Stochastic Multiagent Environments.",
                "PhD thesis, Computer Science Department, University of Michigan, February 2006. [6] D. A. Dolgov and E. H. Durfee.",
                "Optimal resource allocation and policy formulation in loosely-coupled Markov decision processes.",
                "In Proc. of ICAPS-04, pages 315-324, June 2004. [7] D. A. Dolgov and E. H. Durfee.",
                "Computationally efficient combinatorial auctions for resource allocation in weakly-coupled MDPs.",
                "In Proc. of AAMAS-05, New York, NY, USA, 2005.",
                "ACM Press. [8] D. A. Dolgov and E. H. Durfee.",
                "Resource allocation among agents with preferences induced by factored MDPs.",
                "In Proc. of AAMAS-06, 2006. [9] K. Larson and T. Sandholm.",
                "Mechanism design and deliberative agents.",
                "In Proc. of AAMAS-05, pages 650-656, New York, NY, USA, 2005.",
                "ACM Press. [10] N. Nisan.",
                "Bidding and allocation in combinatorial auctions.",
                "In Electronic Commerce, 2000. [11] D. C. Parkes and S. Singh.",
                "An MDP-based approach to Online Mechanism Design.",
                "In Proc. of the Seventeenths Annual Conference on Neural Information Processing Systems (NIPS-03), 2003. [12] D. C. Parkes, S. Singh, and D. Yanovsky.",
                "Approximately efficient online mechanism design.",
                "In Proc. of the Eighteenths Annual Conference on Neural Information Processing Systems (NIPS-04), 2004. [13] M. L. Puterman.",
                "Markov Decision Processes.",
                "John Wiley & Sons, New York, 1994. [14] M. H. Rothkopf, A. Pekec, and R. M. Harstad.",
                "Computationally manageable combinational auctions.",
                "Management Science, 44(8):1131-1147, 1998. [15] T. Sandholm.",
                "An algorithm for optimal winner determination in combinatorial auctions.",
                "In Proc. of IJCAI-99, pages 542-547, San Francisco, CA, USA, 1999.",
                "Morgan Kaufmann Publishers Inc.",
                "The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) 1227"
            ],
            "original_annotated_samples": [],
            "translated_annotated_samples": [
                "En particular, cuando el valor de un conjunto de recursos a un agente no es aditivo (como suele ser el caso con los recursos que son sustitutos o complementos), la \"función de utilidad\" podría tener que definirse en un espacio exponencialmente grande de paquetes de recursos, que rápidamente se vuelve computacionalmente intratable.función de utilidad",
                "Además, incluso cuando cada agente tiene una \"función de utilidad\" que no es cero solo en un pequeño subconjunto de los posibles paquetes de recursos, obtener una asignación óptima aún es computacionalmente prohibitiva, ya que el problema se convierte en NP-complete [14].función de utilidad",
                "Una forma de lograr esto es modelar los procesos por los cuales un agente podría utilizar los recursos y definir la \"función de utilidad\" como el recompensa de estos procesos.función de utilidad",
                "En particular, si un agente usa recursos para actuar en un entorno estocástico, su \"función de utilidad\" puede modelarse naturalmente con un proceso de decisión de Markov, cuyo conjunto de acciones es parametrizado por los recursos disponibles.función de utilidad"
            ],
            "translated_text": "",
            "candidates": [],
            "error": []
        },
        "optimal allocation": {
            "translated_key": "",
            "is_in_text": true,
            "original_annotated_sentences": [
                "Combinatorial Resource Scheduling for Multiagent MDPs Dmitri A. Dolgov, Michael R. James, and Michael E. Samples AI and Robotics Group Technical Research, Toyota Technical Center USA {ddolgov, michael.r.james, michael.samples}@gmail.com ABSTRACT Optimal resource scheduling in multiagent systems is a computationally challenging task, particularly when the values of resources are not additive.",
                "We consider the combinatorial problem of scheduling the usage of multiple resources among agents that operate in stochastic environments, modeled as Markov decision processes (MDPs).",
                "In recent years, efficient resource-allocation algorithms have been developed for agents with resource values induced by MDPs.",
                "However, this prior work has focused on static resource-allocation problems where resources are distributed once and then utilized in infinite-horizon MDPs.",
                "We extend those existing models to the problem of combinatorial resource scheduling, where agents persist only for finite periods between their (predefined) arrival and departure times, requiring resources only for those time periods.",
                "We provide a computationally efficient procedure for computing globally optimal resource assignments to agents over time.",
                "We illustrate and empirically analyze the method in the context of a stochastic jobscheduling domain.",
                "Categories and Subject Descriptors I.2.8 [Artificial Intelligence]: Problem Solving, Control Methods, and Search; I.2.11 [Artificial Intelligence]: Distributed Artificial Intelligence-Multiagent systems General Terms Algorithms, Performance, Design 1.",
                "INTRODUCTION The tasks of optimal resource allocation and scheduling are ubiquitous in multiagent systems, but solving such optimization problems can be computationally difficult, due to a number of factors.",
                "In particular, when the value of a set of resources to an agent is not additive (as is often the case with resources that are substitutes or complements), the utility function might have to be defined on an exponentially large space of resource bundles, which very quickly becomes computationally intractable.",
                "Further, even when each agent has a utility function that is nonzero only on a small subset of the possible resource bundles, obtaining <br>optimal allocation</br> is still computationally prohibitive, as the problem becomes NP-complete [14].",
                "Such computational issues have recently spawned several threads of work in using compact models of agents preferences.",
                "One idea is to use any structure present in utility functions to represent them compactly, via, for example, logical formulas [15, 10, 4, 3].",
                "An alternative is to directly model the mechanisms that define the agents utility functions and perform resource allocation directly with these models [9].",
                "A way of accomplishing this is to model the processes by which an agent might utilize the resources and define the utility function as the payoff of these processes.",
                "In particular, if an agent uses resources to act in a stochastic environment, its utility function can be naturally modeled with a Markov decision process, whose action set is parameterized by the available resources.",
                "This representation can then be used to construct very efficient resource-allocation algorithms that lead to an exponential speedup over a straightforward optimization problem with flat representations of combinatorial preferences [6, 7, 8].",
                "However, this existing work on resource allocation with preferences induced by resource-parameterized MDPs makes an assumption that the resources are only allocated once and are then utilized by the agents independently within their infinite-horizon MDPs.",
                "This assumption that no reallocation of resources is possible can be limiting in domains where agents arrive and depart dynamically.",
                "In this paper, we extend the work on resource allocation under MDP-induced preferences to discrete-time scheduling problems, where agents are present in the system for finite time intervals and can only use resources within these intervals.",
                "In particular, agents arrive and depart at arbitrary (predefined) times and within these intervals use resources to execute tasks in finite-horizon MDPs.",
                "We address the problem of globally optimal resource scheduling, where the objective is to find an allocation of resources to the agents across time that maximizes the sum of the expected rewards that they obtain.",
                "In this context, our main contribution is a mixed-integerprogramming formulation of the scheduling problem that chooses globally optimal resource assignments, starting times, and execution horizons for all agents (within their arrival1220 978-81-904262-7-5 (RPS) c 2007 IFAAMAS departure intervals).",
                "We analyze and empirically compare two flavors of the scheduling problem: one, where agents have static resource assignments within their finite-horizon MDPs, and another, where resources can be dynamically reallocated between agents at every time step.",
                "In the rest of the paper, we first lay down the necessary groundwork in Section 2 and then introduce our model and formal problem statement in Section 3.",
                "In Section 4.2, we describe our main result, the optimization program for globally optimal resource scheduling.",
                "Following the discussion of our experimental results on a job-scheduling problem in Section 5, we conclude in Section 6 with a discussion of possible extensions and generalizations of our method. 2.",
                "BACKGROUND Similarly to the model used in previous work on resourceallocation with MDP-induced preferences [6, 7], we define the value of a set of resources to an agent as the value of the best MDP policy that is realizable, given those resources.",
                "However, since the focus of our work is on scheduling problems, and a large part of the optimization problem is to decide how resources are allocated in time among agents with finite arrival and departure times, we model the agents planning problems as finite-horizon MDPs, in contrast to previous work that used infinite-horizon discounted MDPs.",
                "In the rest of this section, we first introduce some necessary background on finite-horizon MDPs and present a linear-programming formulation that serves as the basis for our solution algorithm developed in Section 4.",
                "We also outline the standard methods for combinatorial resource scheduling with flat resource values, which serve as a comparison benchmark for the new model developed here. 2.1 Markov Decision Processes A stationary, finite-domain, discrete-time MDP (see, for example, [13] for a thorough and detailed development) can be described as S, A, p, r , where: S is a finite set of system states; A is a finite set of actions that are available to the agent; p is a stationary stochastic transition function, where p(σ|s, a) is the probability of transitioning to state σ upon executing action a in state s; r is a stationary reward function, where r(s, a) specifies the reward obtained upon executing action a in state s. Given such an MDP, a decision problem under a finite horizon T is to choose an optimal action at every time step to maximize the expected value of the total reward accrued during the agents (finite) lifetime.",
                "The agents optimal policy is then a function of current state s and the time until the horizon.",
                "An optimal policy for such a problem is to act greedily with respect to the optimal value function, defined recursively by the following system of finite-time Bellman equations [2]: v(s, t) = max a r(s, a) + X σ p(σ|s, a)v(σ, t + 1), ∀s ∈ S, t ∈ [1, T − 1]; v(s, T) = 0, ∀s ∈ S; where v(s, t) is the optimal value of being in state s at time t ∈ [1, T].",
                "This optimal value function can be easily computed using dynamic programming, leading to the following optimal policy π, where π(s, a, t) is the probability of executing action a in state s at time t: π(s, a, t) = ( 1, a = argmaxa r(s, a) + P σ p(σ|s, a)v(σ, t + 1), 0, otherwise.",
                "The above is the most common way of computing the optimal value function (and therefore an optimal policy) for a finite-horizon MDP.",
                "However, we can also formulate the problem as the following linear program (similarly to the dual LP for infinite-horizon discounted MDPs [13, 6, 7]): max X s X a r(s, a) X t x(s, a, t) subject to: X a x(σ, a, t + 1) = X s,a p(σ|s, a)x(s, a, t) ∀σ, t ∈ [1, T − 1]; X a x(s, a, 1) = α(s), ∀s ∈ S; (1) where α(s) is the initial distribution over the state space, and x is the (non-stationary) occupation measure (x(s, a, t) ∈ [0, 1] is the total expected number of times action a is executed in state s at time t).",
                "An optimal (non-stationary) policy is obtained from the occupation measure as follows: π(s, a, t) = x(s, a, t)/ X a x(s, a, t) ∀s ∈ S, t ∈ [1, T]. (2) Note that the standard unconstrained finite-horizon MDP, as described above, always has a uniformly-optimal solution (optimal for any initial distribution α(s)).",
                "Therefore, an optimal policy can be obtained by using an arbitrary constant α(s) > 0 (in particular, α(s) = 1 will result in x(s, a, t) = π(s, a, t)).",
                "However, for MDPs with resource constraints (as defined below in Section 3), uniformly-optimal policies do not in general exist.",
                "In such cases, α becomes a part of the problem input, and a resulting policy is only optimal for that particular α.",
                "This result is well known for infinite-horizon MDPs with various types of constraints [1, 6], and it also holds for our finite-horizon model, which can be easily established via a line of reasoning completely analogous to the arguments in [6]. 2.2 Combinatorial Resource Scheduling A straightforward approach to resource scheduling for a set of agents M, whose values for the resources are induced by stochastic planning problems (in our case, finite-horizon MDPs) would be to have each agent enumerate all possible resource assignments over time and, for each one, compute its value by solving the corresponding MDP.",
                "Then, each agent would provide valuations for each possible resource bundle over time to a centralized coordinator, who would compute the optimal resource assignments across time based on these valuations.",
                "When resources can be allocated at different times to different agents, each agent must submit valuations for every combination of possible time horizons.",
                "Let each agent m ∈ M execute its MDP within the arrival-departure time interval τ ∈ [τa m, τd m].",
                "Hence, agent m will execute an MDP with time horizon no greater than Tm = τd m−τa m+1.",
                "Let bτ be the global time horizon for the problem, before which all of the agents MDPs must finish.",
                "We assume τd m < bτ, ∀m ∈ M. The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) 1221 For the scheduling problem where agents have static resource requirements within their finite-horizon MDPs, the agents provide a valuation for each resource bundle for each possible time horizon (from [1, Tm]) that they may use.",
                "Let Ω be the set of resources to be allocated among the agents.",
                "An agent will get at most one resource bundle for one of the time horizons.",
                "Let the variable ψ ∈ Ψm enumerate all possible pairs of resource bundles and time horizons for agent m, so there are 2|Ω| × Tm values for ψ (the space of bundles is exponential in the number of resource types |Ω|).",
                "The agent m must provide a value vψ m for each ψ, and the coordinator will allocate at most one ψ (resource, time horizon) pair to each agent.",
                "This allocation is expressed as an indicator variable zψ m ∈ {0, 1} that shows whether ψ is assigned to agent m. For time τ and resource ω, the function nm(ψ, τ, ω) ∈ {0, 1} indicates whether the bundle in ψ uses resource ω at time τ (we make the assumption that agents have binary resource requirements).",
                "This allocation problem is NP-complete, even when considering only a single time step, and its difficulty increases significantly with multiple time steps because of the increasing number of values of ψ.",
                "The problem of finding an <br>optimal allocation</br> that satisfies the global constraint that the amount of each resource ω allocated to all agents does not exceed the available amount bϕ(ω) can be expressed as the following integer program: max X m∈M X ψ∈Ψm zψ mvψ m subject to: X ψ∈Ψm zψ m ≤ 1, ∀m ∈ M; X m∈M X ψ∈Ψm zψ mnm(ψ, τ, ω) ≤ bϕ(ω), ∀τ ∈ [1, bτ], ∀ω ∈ Ω; (3) The first constraint in equation 3 says that no agent can receive more than one bundle, and the second constraint ensures that the total assignment of resource ω does not, at any time, exceed the resource bound.",
                "For the scheduling problem where the agents are able to dynamically reallocate resources, each agent must specify a value for every combination of bundles and time steps within its time horizon.",
                "Let the variable ψ ∈ Ψm in this case enumerate all possible resource bundles for which at most one bundle may be assigned to agent m at each time step.",
                "Therefore, in this case there are P t∈[1,Tm](2|Ω| )t ∼ 2|Ω|Tm possibilities of resource bundles assigned to different time slots, for the Tm different time horizons.",
                "The same set of equations (3) can be used to solve this dynamic scheduling problem, but the integer program is different because of the difference in how ψ is defined.",
                "In this case, the number of ψ values is exponential in each agents planning horizon Tm, resulting in a much larger program.",
                "This straightforward approach to solving both of these scheduling problems requires an enumeration and solution of either 2|Ω| Tm (static allocation) or P t∈[1,Tm] 2|Ω|t (dynamic reallocation) MDPs for each agent, which very quickly becomes intractable with the growth of the number of resources |Ω| or the time horizon Tm. 3.",
                "MODEL AND PROBLEM STATEMENT We now formally introduce our model of the resourcescheduling problem.",
                "The problem input consists of the following components: • M, Ω, bϕ, τa m, τd m, bτ are as defined above in Section 2.2. • {Θm} = {S, A, pm, rm, αm} are the MDPs of all agents m ∈ M. Without loss of generality, we assume that state and action spaces of all agents are the same, but each has its own transition function pm, reward function rm, and initial conditions αm. • ϕm : A×Ω → {0, 1} is the mapping of actions to resources for agent m. ϕm(a, ω) indicates whether action a of agent m needs resource ω.",
                "An agent m that receives a set of resources that does not include resource ω cannot execute in its MDP policy any action a for which ϕm(a, ω) = 0.",
                "We assume all resource requirements are binary; as discussed below in Section 6, this assumption is not limiting.",
                "Given the above input, the optimization problem we consider is to find the globally optimal-maximizing the sum of expected rewards-mapping of resources to agents for all time steps: Δ : τ × M × Ω → {0, 1}.",
                "A solution is feasible if the corresponding assignment of resources to the agents does not violate the global resource constraint: X m Δm(τ, ω) ≤ bϕ(ω), ∀ω ∈ Ω, τ ∈ [1, bτ]. (4) We consider two flavors of the resource-scheduling problem.",
                "The first formulation restricts resource assignments to the space where the allocation of resources to each agent is static during the agents lifetime.",
                "The second formulation allows reassignment of resources between agents at every time step within their lifetimes.",
                "Figure 1 depicts a resource-scheduling problem with three agents M = {m1, m2, m3}, three resources Ω = {ω1, ω2, ω3}, and a global problem horizon of bτ = 11.",
                "The agents arrival and departure times are shown as gray boxes and are {1, 6}, {3, 7}, and {2, 11}, respectively.",
                "A solution to this problem is shown via horizontal bars within each agents box, where the bars correspond to the allocation of the three resource types.",
                "Figure 1a shows a solution to a static scheduling problem.",
                "According to the shown solution, agent m1 begins the execution of its MDP at time τ = 1 and has a lock on all three resources until it finishes execution at time τ = 3.",
                "Note that agent m1 relinquishes its hold on the resources before its announced departure time of τd m1 = 6, ostensibly because other agents can utilize the resources more effectively.",
                "Thus, at time τ = 4, resources ω1 and ω3 are allocated to agent m2, who then uses them to execute its MDP (using only actions supported by resources ω1 and ω3) until time τ = 7.",
                "Agent m3 holds resource ω3 during the interval τ ∈ [4, 10].",
                "Figure 1b shows a possible solution to the dynamic version of the same problem.",
                "There, resources can be reallocated between agents at every time step.",
                "For example, agent m1 gives up its use of resource ω2 at time τ = 2, although it continues the execution of its MDP until time τ = 6.",
                "Notice that an agent is not allowed to stop and restart its MDP, so agent m1 is only able to continue executing in the interval τ ∈ [3, 4] if it has actions that do not require any resources (ϕm(a, ω) = 0).",
                "Clearly, the model and problem statement described above make a number of assumptions about the problem and the desired solution properties.",
                "We discuss some of those assumptions and their implications in Section 6. 1222 The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) (a) (b) Figure 1: Illustration of a solution to a resource-scheduling problem with three agents and three resources: a) static resource assignments (resource assignments are constant within agents lifetimes; b) dynamic assignment (resource assignments are allowed to change at every time step). 4.",
                "RESOURCE SCHEDULING Our resource-scheduling algorithm proceeds in two stages.",
                "First, we perform a preprocessing step that augments the agent MDPs; this process is described in Section 4.1.",
                "Second, using these augmented MDPs we construct a global optimization problem, which is described in Section 4.2. 4.1 Augmenting Agents MDPs In the model described in the previous section, we assume that if an agent does not possess the necessary resources to perform actions in its MDP, its execution is halted and the agent leaves the system.",
                "In other words, the MDPs cannot be paused and resumed.",
                "For example, in the problem shown in Figure 1a, agent m1 releases all resources after time τ = 3, at which point the execution of its MDP is halted.",
                "Similarly, agents m2 and m3 only execute their MDPs in the intervals τ ∈ [4, 6] and τ ∈ [4, 10], respectively.",
                "Therefore, an important part of the global decision-making problem is to decide the window of time during which each of the agents is active (i.e., executing its MDP).",
                "To accomplish this, we augment each agents MDP with two new states (start and finish states sb , sf , respectively) and a new start/stop action a∗ , as illustrated in Figure 2.",
                "The idea is that an agent stays in the start state sb until it is ready to execute its MDP, at which point it performs the start/stop action a∗ and transitions into the state space of the original MDP with the transition probability that corresponds to the original initial distribution α(s).",
                "For example, in Figure 1a, for agent m2 this would happen at time τ = 4.",
                "Once the agent gets to the end of its activity window (time τ = 6 for agent m2 in Figure 1a), it performs the start/stop action, which takes it into the sink finish state sf at time τ = 7.",
                "More precisely, given an MDP S, A, pm, rm, αm , we define an augmented MDP S , A , pm, rm, αm as follows: S = S ∪ sb ∪ sf ; A = A ∪ a∗ ; p (s|sb , a∗ ) = α(s), ∀s ∈ S; p (sb |sb , a) = 1.0, ∀a ∈ A; p (sf |s, a∗ ) = 1.0, ∀s ∈ S; p (σ|s, a) = p(σ|s, a), ∀s, σ ∈ S, a ∈ A; r (sb , a) = r (sf , a) = 0, ∀a ∈ A ; r (s, a) = r(s, a), ∀s ∈ S, a ∈ A; α (sb ) = 1; α (s) = 0, ∀s ∈ S; where all non-specified transition probabilities are assumed to be zero.",
                "Further, in order to account for the new starting state, we begin the MDP one time-step earlier, setting τa m ← τa m − 1.",
                "This will not affect the resource allocation due to the resource constraints only being enforced for the original MDP states, as will be discussed in the next section.",
                "For example, the augmented MDPs shown in Figure 2b (which starts in state sb at time τ = 2) would be constructed from an MDP with original arrival time τ = 3.",
                "Figure 2b also shows a sample trajectory through the state space: the agent starts in state sb , transitions into the state space S of the original MDP, and finally exists into the sink state sf .",
                "Note that if we wanted to model a problem where agents could pause their MDPs at arbitrary time steps (which might be useful for domains where dynamic reallocation is possible), we could easily accomplish this by including an extra action that transitions from each state to itself with zero reward. 4.2 MILP for Resource Scheduling Given a set of augmented MDPs, as defined above, the goal of this section is to formulate a global optimization program that solves the resource-scheduling problem.",
                "In this section and below, all MDPs are assumed to be the augmented MDPs as defined in Section 4.1.",
                "Our approach is similar to the idea used in [6]: we begin with the linear-program formulation of agents MDPs (1) and augment it with constraints that ensure that the corresponding resource allocation across agents and time is valid.",
                "The resulting optimization problem then simultaneously solves the agents MDPs and resource-scheduling problems.",
                "In the rest of this section, we incrementally develop a mixed integer program (MILP) that achieves this.",
                "In the absence of resource constraints, the agents finitehorizon MDPs are completely independent, and the globally optimal solution can be trivially obtained via the following LP, which is simply an aggregation of single-agent finitehorizon LPs: max X m X s X a rm(s, a) X t xm(s, a, t) subject to: X a xm(σ, a, t + 1) = X s,a pm(σ|s, a)xm(s, a, t), ∀m ∈ M, σ ∈ S, t ∈ [1, Tm − 1]; X a xm(s, a, 1) = αm(s), ∀m ∈ M, s ∈ S; (12) where xm(s, a, t) is the occupation measure of agent m, and The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) 1223 (a) (b) Figure 2: Illustration of augmenting an MDP to allow for variable starting and stopping times: a) (left) the original two-state MDP with a single action; (right) the augmented MDP with new states sb and sf and the new action a∗ (note that the origianl transitions are not changed in the augmentation process); b) the augmented MDP displayed as a trajectory through time (grey lines indicate all transitions, while black lines indicate a given trajectory.",
                "Objective Function (sum of expected rewards over all agents) max X m X s X a rm(s, a) X t xm(s, a, t) (5) Meaning Implication Linear Constraints Tie x to θ.",
                "Agent is only active when occupation measure is nonzero in original MDP states. θm(τ) = 0 =⇒ xm(s, a, τ −τa m+1) = 0 ∀s /∈ {sb , sf }, a ∈ A X s/∈{sb,sf } X a xm(s, a, t) ≤ θm(τa m + t − 1) ∀m ∈ M, ∀t ∈ [1, Tm] (6) Agent can only be active in τ ∈ (τa m, τd m) θm(τ) = 0 ∀m ∈ M, τ /∈ (τa m, τd m) (7) Cannot use resources when not active θm(τ) = 0 =⇒ Δm(τ, ω) = 0 ∀τ ∈ [0, bτ], ω ∈ Ω Δm(τ, ω) ≤ θm(τ) ∀m ∈ M, τ ∈ [0, bτ], ω ∈ Ω (8) Tie x to Δ (nonzero x forces corresponding Δ to be nonzero.)",
                "Δm(τ, ω) = 0, ϕm(a, ω) = 1 =⇒ xm(s, a, τ − τa m + 1) = 0 ∀s /∈ {sb , sf } 1/|A| X a ϕm(a, ω) X s/∈{sb,sf } xm(s, a, t) ≤ Δm(t + τa m − 1, ω) ∀m ∈ M, ω ∈ Ω, t ∈ [1, Tm] (9) Resource bounds X m Δm(τ, ω) ≤ bϕ(ω) ∀ω ∈ Ω, τ ∈ [0, bτ] (10) Agent cannot change resources while active.",
                "Only enabled for scheduling with static assignments. θm(τ) = 1 and θm(τ + 1) = 1 =⇒ Δm(τ, ω) = Δm(τ + 1, ω) Δm(τ, ω) − Z(1 − θm(τ + 1)) ≤ Δm(τ + 1, ω) + Z(1 − θm(τ)) Δm(τ, ω) + Z(1 − θm(τ + 1)) ≥ Δm(τ + 1, ω) − Z(1 − θm(τ)) ∀m ∈ M, ω ∈ Ω, τ ∈ [0, bτ] (11) Table 1: MILP for globally optimal resource scheduling.",
                "Tm = τd m − τa m + 1 is the time horizon for the agents MDP.",
                "Using this LP as a basis, we augment it with constraints that ensure that the resource usage implied by the agents occupation measures {xm} does not violate the global resource requirements bϕ at any time step τ ∈ [0, bτ].",
                "To formulate these resource constraints, we use the following binary variables: • Δm(τ, ω) = {0, 1}, ∀m ∈ M, τ ∈ [0, bτ], ω ∈ Ω, which serve as indicator variables that define whether agent m possesses resource ω at time τ.",
                "These are analogous to the static indicator variables used in the one-shot static resource-allocation problem in [6]. • θm = {0, 1}, ∀m ∈ M, τ ∈ [0, bτ] are indicator variables that specify whether agent m is active (i.e., executing its MDP) at time τ.",
                "The meaning of resource-usage variables Δ is illustrated in Figure 1: Δm(τ, ω) = 1 only if resource ω is allocated to agent m at time τ.",
                "The meaning of the activity indicators θ is illustrated in Figure 2b: when agent m is in either the start state sb or the finish state sf , the corresponding θm = 0, but once the agent becomes active and enters one of the other states, we set θm = 1 .",
                "This meaning of θ can be enforced with a linear constraint that synchronizes the values of the agents occupation measures xm and the activity 1224 The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) indicators θ, as shown in (6) in Table 1.",
                "Another constraint we have to add-because the activity indicators θ are defined on the global timeline τ-is to enforce the fact that the agent is inactive outside of its arrivaldeparture window.",
                "This is accomplished by constraint (7) in Table 1.",
                "Furthermore, agents should not be using resources while they are inactive.",
                "This constraint can also be enforced via a linear inequality on θ and Δ, as shown in (8).",
                "Constraint (6) sets the value of θ to match the policy defined by the occupation measure xm.",
                "In a similar fashion, we have to make sure that the resource-usage variables Δ are also synchronized with the occupation measure xm.",
                "This is done via constraint (9) in Table 1, which is nearly identical to the analogous constraint from [6].",
                "After implementing the above constraint, which enforces the meaning of Δ, we add a constraint that ensures that the agents resource usage never exceeds the amounts of available resources.",
                "This condition is also trivially expressed as a linear inequality (10) in Table 1.",
                "Finally, for the problem formulation where resource assignments are static during a lifetime of an agent, we add a constraint that ensures that the resource-usage variables Δ do not change their value while the agent is active (θ = 1).",
                "This is accomplished via the linear constraint (11), where Z ≥ 2 is a constant that is used to turn off the constraints when θm(τ) = 0 or θm(τ + 1) = 0.",
                "This constraint is not used for the dynamic problem formulation, where resources can be reallocated between agents at every time step.",
                "To summarize, Table 1 together with the conservationof-flow constraints from (12) defines the MILP that simultaneously computes an optimal resource assignment for all agents across time as well as optimal finite-horizon MDP policies that are valid under that resource assignment.",
                "As a rough measure of the complexity of this MILP, let us consider the number of optimization variables and constraints.",
                "Let TM = P Tm = P m(τa m − τd m + 1) be the sum of the lengths of the arrival-departure windows across all agents.",
                "Then, the number of optimization variables is: TM + bτ|M||Ω| + bτ|M|, TM of which are continuous (xm), and bτ|M||Ω| + bτ|M| are binary (Δ and θ).",
                "However, notice that all but TM|M| of the θ are set to zero by constraint (7), which also immediately forces all but TM|M||Ω| of the Δ to be zero via the constraints (8).",
                "The number of constraints (not including the degenerate constraints in (7)) in the MILP is: TM + TM|Ω| + bτ|Ω| + bτ|M||Ω|.",
                "Despite the fact that the complexity of the MILP is, in the worst case, exponential1 in the number of binary variables, the complexity of this MILP is significantly (exponentially) lower than that of the MILP with flat utility functions, described in Section 2.2.",
                "This result echos the efficiency gains reported in [6] for single-shot resource-allocation problems, but is much more pronounced, because of the explosion of the flat utility representation due to the temporal aspect of the problem (recall the prohibitive complexity of the combinatorial optimization in Section 2.2).",
                "We empirically analyze the performance of this method in Section 5. 1 Strictly speaking, solving MILPs to optimality is NPcomplete in the number of integer variables. 5.",
                "EXPERIMENTAL RESULTS Although the complexity of solving MILPs is in the worst case exponential in the number of integer variables, there are many efficient methods for solving MILPs that allow our algorithm to scale well for parameters common to resource allocation and scheduling problems.",
                "In particular, this section introduces a problem domain-the repairshop problem-used to empirically evaluate our algorithms scalability in terms of the number of agents |M|, the number of shared resources |Ω|, and the varied lengths of global time bτ during which agents may enter and exit the system.",
                "The repairshop problem is a simple parameterized MDP adopting the metaphor of a vehicular repair shop.",
                "Agents in the repair shop are mechanics with a number of independent tasks that yield reward only when completed.",
                "In our MDP model of this system, actions taken to advance through the state space are only allowed if the agent holds certain resources that are publicly available to the shop.",
                "These resources are in finite supply, and optimal policies for the shop will determine when each agent may hold the limited resources to take actions and earn individual rewards.",
                "Each task to be completed is associated with a single action, although the agent is required to repeat the action numerous times before completing the task and earning a reward.",
                "This model was parameterized in terms of the number of agents in the system, the number of different types of resources that could be linked to necessary actions, a global time during which agents are allowed to arrive and depart, and a maximum length for the number of time steps an agent may remain in the system.",
                "All datapoints in our experiments were obtained with 20 evaluations using CPLEX to solve the MILPs on a Pentium4 computer with 2Gb of RAM.",
                "Trials were conducted on both the static and the dynamic version of the resourcescheduling problem, as defined earlier.",
                "Figure 3 shows the runtime and policy value for independent modifications to the parameter set.",
                "The top row shows how the solution time for the MILP scales as we increase the number of agents |M|, the global time horizon bτ, and the number of resources |Ω|.",
                "Increasing the number of agents leads to exponential complexity scaling, which is to be expected for an NP-complete problem.",
                "However, increasing the global time limit bτ or the total number of resource types |Ω|-while holding the number of agents constantdoes not lead to decreased performance.",
                "This occurs because the problems get easier as they become under-constrained, which is also a common phenomenon for NP-complete problems.",
                "We also observe that the solution to the dynamic version of the problem can often be computed much faster than the static version.",
                "The bottom row of Figure 3 shows the joint policy value of the policies that correspond to the computed optimal resource-allocation schedules.",
                "We can observe that the dynamic version yields higher reward (as expected, since the reward for the dynamic version is always no less than the reward of the static version).",
                "We should point out that these graphs should not be viewed as a measure of performance of two different algorithms (both algorithms produce optimal solutions but to different problems), but rather as observations about how the quality of optimal solutions change as more flexibility is allowed in the reallocation of resources.",
                "Figure 4 shows runtime and policy value for trials in which common input variables are scaled together.",
                "This allows The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) 1225 2 4 6 8 10 10 −3 10 −2 10 −1 10 0 10 1 10 2 10 3 10 4 Number of Agents |M| CPUTime,sec |Ω| = 5, τ = 50 static dynamic 50 100 150 200 10 −2 10 −1 10 0 10 1 10 2 10 3 Global Time Boundary τ CPUTime,sec |M| = 5, |Ω| = 5 static dynamic 10 20 30 40 50 10 −2 10 −1 10 0 10 1 10 2 Number of Resources |Ω| CPUTime,sec |M| = 5, τ = 50 static dynamic 2 4 6 8 10 200 400 600 800 1000 1200 1400 1600 Number of Agents |M| Value |Ω| = 5, τ = 50 static dynamic 50 100 150 200 400 500 600 700 800 900 1000 1100 1200 1300 1400 Global Time Boundary τ Value |M| = 5, |Ω| = 5 static dynamic 10 20 30 40 50 500 600 700 800 900 1000 1100 1200 1300 1400 Number of Resources |Ω| Value |M| = 5, τ = 50 static dynamic Figure 3: Evaluation of our MILP for variable numbers of agents (column 1), lengths of global-time window (column 2), and numbers of resource types (column 3).",
                "Top row shows CPU time, and bottom row shows the joint reward of agents MDP policies.",
                "Error bars show the 1st and 3rd quartiles (25% and 75%). 2 4 6 8 10 10 −3 10 −2 10 −1 10 0 10 1 10 2 10 3 Number of Agents |M| CPUTime,sec τ = 10|M| static dynamic 2 4 6 8 10 10 −3 10 −2 10 −1 10 0 10 1 10 2 10 3 10 4 Number of Agents |M| CPUTime,sec |Ω| = 2|M| static dynamic 2 4 6 8 10 10 −3 10 −2 10 −1 10 0 10 1 10 2 10 3 10 4 Number of Agents |M| CPUTime,sec |Ω| = 5|M| static dynamic 2 4 6 8 10 200 400 600 800 1000 1200 1400 1600 1800 2000 2200 Number of Agents |M| Value τ = 10|M| static dynamic 2 4 6 8 10 200 400 600 800 1000 1200 1400 1600 1800 2000 Number of Agents |M| Value |Ω| = 2|M| static dynamic 2 4 6 8 10 0 500 1000 1500 2000 2500 Number of Agents |M| Value |Ω| = 5|M| static dynamic Figure 4: Evaluation of our MILP using correlated input variables.",
                "The left column tracks the performance and CPU time as the number of agents and global-time window increase together (bτ = 10|M|).",
                "The middle and the right column track the performance and CPU time as the number of resources and the number of agents increase together as |Ω| = 2|M| and |Ω| = 5|M|, respectively.",
                "Error bars show the 1st and 3rd quartiles (25% and 75%). 1226 The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) us to explore domains where the total number of agents scales proportionally to the total number of resource types or the global time horizon, while keeping constant the average agent density (per unit of global time) or the average number of resources per agent (which commonly occurs in real-life applications).",
                "Overall, we believe that these experimental results indicate that our MILP formulation can be used to effectively solve resource-scheduling problems of nontrivial size. 6.",
                "DISCUSSION AND CONCLUSIONS Throughout the paper, we have made a number of assumptions in our model and solution algorithm; we discuss their implications below. • Continual execution.",
                "We assume that once an agent stops executing its MDP (transitions into state sf ), it exits the system and cannot return.",
                "It is easy to relax this assumption for domains where agents MDPs can be paused and restarted.",
                "All that is required is to include an additional pause action which transitions from a given state back to itself, and has zero reward. • Indifference to start time.",
                "We used a reward model where agents rewards depend only on the time horizon of their MDPs and not the global start time.",
                "This is a consequence of our MDP-augmentation procedure from Section 4.1.",
                "It is easy to extend the model so that the agents incur an explicit penalty for idling by assigning a non-zero negative reward to the start state sb . • Binary resource requirements.",
                "For simplicity, we have assumed that resource costs are binary: ϕm(a, ω) = {0, 1}, but our results generalize in a straightforward manner to non-binary resource mappings, analogously to the procedure used in [5]. • Cooperative agents.",
                "The optimization procedure discussed in this paper was developed in the context of cooperative agents, but it can also be used to design a mechanism for scheduling resources among self-interested agents.",
                "This optimization procedure can be embedded in a VickreyClarke-Groves auction, completely analogously to the way it was done in [7].",
                "In fact, all the results of [7] about the properties of the auction and information privacy directly carry over to the scheduling domain discussed in this paper, requiring only slight modifications to deal with finitehorizon MDPs. • Known, deterministic arrival and departure times.",
                "Finally, we have assumed that agents arrival and departure times (τa m and τd m) are deterministic and known a priori.",
                "This assumption is fundamental to our solution method.",
                "While there are many domains where this assumption is valid, in many cases agents arrive and depart dynamically and their arrival and departure times can only be predicted probabilistically, leading to online resource-allocation problems.",
                "In particular, in the case of self-interested agents, this becomes an interesting version of an online-mechanism-design problem [11, 12].",
                "In summary, we have presented an MILP formulation for the combinatorial resource-scheduling problem where agents values for possible resource assignments are defined by finitehorizon MDPs.",
                "This result extends previous work ([6, 7]) on static one-shot resource allocation under MDP-induced preferences to resource-scheduling problems with a temporal aspect.",
                "As such, this work takes a step in the direction of designing an online mechanism for agents with combinatorial resource preferences induced by stochastic planning problems.",
                "Relaxing the assumption about deterministic arrival and departure times of the agents is a focus of our future work.",
                "We would like to thank the anonymous reviewers for their insightful comments and suggestions. 7.",
                "REFERENCES [1] E. Altman and A. Shwartz.",
                "Adaptive control of constrained Markov chains: Criteria and policies.",
                "Annals of Operations Research, special issue on Markov Decision Processes, 28:101-134, 1991. [2] R. Bellman.",
                "Dynamic Programming.",
                "Princeton University Press, 1957. [3] C. Boutilier.",
                "Solving concisely expressed combinatorial auction problems.",
                "In Proc. of AAAI-02, pages 359-366, 2002. [4] C. Boutilier and H. H. Hoos.",
                "Bidding languages for combinatorial auctions.",
                "In Proc. of IJCAI-01, pages 1211-1217, 2001. [5] D. Dolgov.",
                "Integrated Resource Allocation and Planning in Stochastic Multiagent Environments.",
                "PhD thesis, Computer Science Department, University of Michigan, February 2006. [6] D. A. Dolgov and E. H. Durfee.",
                "Optimal resource allocation and policy formulation in loosely-coupled Markov decision processes.",
                "In Proc. of ICAPS-04, pages 315-324, June 2004. [7] D. A. Dolgov and E. H. Durfee.",
                "Computationally efficient combinatorial auctions for resource allocation in weakly-coupled MDPs.",
                "In Proc. of AAMAS-05, New York, NY, USA, 2005.",
                "ACM Press. [8] D. A. Dolgov and E. H. Durfee.",
                "Resource allocation among agents with preferences induced by factored MDPs.",
                "In Proc. of AAMAS-06, 2006. [9] K. Larson and T. Sandholm.",
                "Mechanism design and deliberative agents.",
                "In Proc. of AAMAS-05, pages 650-656, New York, NY, USA, 2005.",
                "ACM Press. [10] N. Nisan.",
                "Bidding and allocation in combinatorial auctions.",
                "In Electronic Commerce, 2000. [11] D. C. Parkes and S. Singh.",
                "An MDP-based approach to Online Mechanism Design.",
                "In Proc. of the Seventeenths Annual Conference on Neural Information Processing Systems (NIPS-03), 2003. [12] D. C. Parkes, S. Singh, and D. Yanovsky.",
                "Approximately efficient online mechanism design.",
                "In Proc. of the Eighteenths Annual Conference on Neural Information Processing Systems (NIPS-04), 2004. [13] M. L. Puterman.",
                "Markov Decision Processes.",
                "John Wiley & Sons, New York, 1994. [14] M. H. Rothkopf, A. Pekec, and R. M. Harstad.",
                "Computationally manageable combinational auctions.",
                "Management Science, 44(8):1131-1147, 1998. [15] T. Sandholm.",
                "An algorithm for optimal winner determination in combinatorial auctions.",
                "In Proc. of IJCAI-99, pages 542-547, San Francisco, CA, USA, 1999.",
                "Morgan Kaufmann Publishers Inc.",
                "The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) 1227"
            ],
            "original_annotated_samples": [],
            "translated_annotated_samples": [],
            "translated_text": "",
            "candidates": [],
            "error": []
        },
        "discrete-time scheduling problem": {
            "translated_key": "",
            "is_in_text": true,
            "original_annotated_sentences": [
                "Combinatorial Resource Scheduling for Multiagent MDPs Dmitri A. Dolgov, Michael R. James, and Michael E. Samples AI and Robotics Group Technical Research, Toyota Technical Center USA {ddolgov, michael.r.james, michael.samples}@gmail.com ABSTRACT Optimal resource scheduling in multiagent systems is a computationally challenging task, particularly when the values of resources are not additive.",
                "We consider the combinatorial problem of scheduling the usage of multiple resources among agents that operate in stochastic environments, modeled as Markov decision processes (MDPs).",
                "In recent years, efficient resource-allocation algorithms have been developed for agents with resource values induced by MDPs.",
                "However, this prior work has focused on static resource-allocation problems where resources are distributed once and then utilized in infinite-horizon MDPs.",
                "We extend those existing models to the problem of combinatorial resource scheduling, where agents persist only for finite periods between their (predefined) arrival and departure times, requiring resources only for those time periods.",
                "We provide a computationally efficient procedure for computing globally optimal resource assignments to agents over time.",
                "We illustrate and empirically analyze the method in the context of a stochastic jobscheduling domain.",
                "Categories and Subject Descriptors I.2.8 [Artificial Intelligence]: Problem Solving, Control Methods, and Search; I.2.11 [Artificial Intelligence]: Distributed Artificial Intelligence-Multiagent systems General Terms Algorithms, Performance, Design 1.",
                "INTRODUCTION The tasks of optimal resource allocation and scheduling are ubiquitous in multiagent systems, but solving such optimization problems can be computationally difficult, due to a number of factors.",
                "In particular, when the value of a set of resources to an agent is not additive (as is often the case with resources that are substitutes or complements), the utility function might have to be defined on an exponentially large space of resource bundles, which very quickly becomes computationally intractable.",
                "Further, even when each agent has a utility function that is nonzero only on a small subset of the possible resource bundles, obtaining optimal allocation is still computationally prohibitive, as the problem becomes NP-complete [14].",
                "Such computational issues have recently spawned several threads of work in using compact models of agents preferences.",
                "One idea is to use any structure present in utility functions to represent them compactly, via, for example, logical formulas [15, 10, 4, 3].",
                "An alternative is to directly model the mechanisms that define the agents utility functions and perform resource allocation directly with these models [9].",
                "A way of accomplishing this is to model the processes by which an agent might utilize the resources and define the utility function as the payoff of these processes.",
                "In particular, if an agent uses resources to act in a stochastic environment, its utility function can be naturally modeled with a Markov decision process, whose action set is parameterized by the available resources.",
                "This representation can then be used to construct very efficient resource-allocation algorithms that lead to an exponential speedup over a straightforward optimization problem with flat representations of combinatorial preferences [6, 7, 8].",
                "However, this existing work on resource allocation with preferences induced by resource-parameterized MDPs makes an assumption that the resources are only allocated once and are then utilized by the agents independently within their infinite-horizon MDPs.",
                "This assumption that no reallocation of resources is possible can be limiting in domains where agents arrive and depart dynamically.",
                "In this paper, we extend the work on resource allocation under MDP-induced preferences to <br>discrete-time scheduling problem</br>s, where agents are present in the system for finite time intervals and can only use resources within these intervals.",
                "In particular, agents arrive and depart at arbitrary (predefined) times and within these intervals use resources to execute tasks in finite-horizon MDPs.",
                "We address the problem of globally optimal resource scheduling, where the objective is to find an allocation of resources to the agents across time that maximizes the sum of the expected rewards that they obtain.",
                "In this context, our main contribution is a mixed-integerprogramming formulation of the scheduling problem that chooses globally optimal resource assignments, starting times, and execution horizons for all agents (within their arrival1220 978-81-904262-7-5 (RPS) c 2007 IFAAMAS departure intervals).",
                "We analyze and empirically compare two flavors of the scheduling problem: one, where agents have static resource assignments within their finite-horizon MDPs, and another, where resources can be dynamically reallocated between agents at every time step.",
                "In the rest of the paper, we first lay down the necessary groundwork in Section 2 and then introduce our model and formal problem statement in Section 3.",
                "In Section 4.2, we describe our main result, the optimization program for globally optimal resource scheduling.",
                "Following the discussion of our experimental results on a job-scheduling problem in Section 5, we conclude in Section 6 with a discussion of possible extensions and generalizations of our method. 2.",
                "BACKGROUND Similarly to the model used in previous work on resourceallocation with MDP-induced preferences [6, 7], we define the value of a set of resources to an agent as the value of the best MDP policy that is realizable, given those resources.",
                "However, since the focus of our work is on scheduling problems, and a large part of the optimization problem is to decide how resources are allocated in time among agents with finite arrival and departure times, we model the agents planning problems as finite-horizon MDPs, in contrast to previous work that used infinite-horizon discounted MDPs.",
                "In the rest of this section, we first introduce some necessary background on finite-horizon MDPs and present a linear-programming formulation that serves as the basis for our solution algorithm developed in Section 4.",
                "We also outline the standard methods for combinatorial resource scheduling with flat resource values, which serve as a comparison benchmark for the new model developed here. 2.1 Markov Decision Processes A stationary, finite-domain, discrete-time MDP (see, for example, [13] for a thorough and detailed development) can be described as S, A, p, r , where: S is a finite set of system states; A is a finite set of actions that are available to the agent; p is a stationary stochastic transition function, where p(σ|s, a) is the probability of transitioning to state σ upon executing action a in state s; r is a stationary reward function, where r(s, a) specifies the reward obtained upon executing action a in state s. Given such an MDP, a decision problem under a finite horizon T is to choose an optimal action at every time step to maximize the expected value of the total reward accrued during the agents (finite) lifetime.",
                "The agents optimal policy is then a function of current state s and the time until the horizon.",
                "An optimal policy for such a problem is to act greedily with respect to the optimal value function, defined recursively by the following system of finite-time Bellman equations [2]: v(s, t) = max a r(s, a) + X σ p(σ|s, a)v(σ, t + 1), ∀s ∈ S, t ∈ [1, T − 1]; v(s, T) = 0, ∀s ∈ S; where v(s, t) is the optimal value of being in state s at time t ∈ [1, T].",
                "This optimal value function can be easily computed using dynamic programming, leading to the following optimal policy π, where π(s, a, t) is the probability of executing action a in state s at time t: π(s, a, t) = ( 1, a = argmaxa r(s, a) + P σ p(σ|s, a)v(σ, t + 1), 0, otherwise.",
                "The above is the most common way of computing the optimal value function (and therefore an optimal policy) for a finite-horizon MDP.",
                "However, we can also formulate the problem as the following linear program (similarly to the dual LP for infinite-horizon discounted MDPs [13, 6, 7]): max X s X a r(s, a) X t x(s, a, t) subject to: X a x(σ, a, t + 1) = X s,a p(σ|s, a)x(s, a, t) ∀σ, t ∈ [1, T − 1]; X a x(s, a, 1) = α(s), ∀s ∈ S; (1) where α(s) is the initial distribution over the state space, and x is the (non-stationary) occupation measure (x(s, a, t) ∈ [0, 1] is the total expected number of times action a is executed in state s at time t).",
                "An optimal (non-stationary) policy is obtained from the occupation measure as follows: π(s, a, t) = x(s, a, t)/ X a x(s, a, t) ∀s ∈ S, t ∈ [1, T]. (2) Note that the standard unconstrained finite-horizon MDP, as described above, always has a uniformly-optimal solution (optimal for any initial distribution α(s)).",
                "Therefore, an optimal policy can be obtained by using an arbitrary constant α(s) > 0 (in particular, α(s) = 1 will result in x(s, a, t) = π(s, a, t)).",
                "However, for MDPs with resource constraints (as defined below in Section 3), uniformly-optimal policies do not in general exist.",
                "In such cases, α becomes a part of the problem input, and a resulting policy is only optimal for that particular α.",
                "This result is well known for infinite-horizon MDPs with various types of constraints [1, 6], and it also holds for our finite-horizon model, which can be easily established via a line of reasoning completely analogous to the arguments in [6]. 2.2 Combinatorial Resource Scheduling A straightforward approach to resource scheduling for a set of agents M, whose values for the resources are induced by stochastic planning problems (in our case, finite-horizon MDPs) would be to have each agent enumerate all possible resource assignments over time and, for each one, compute its value by solving the corresponding MDP.",
                "Then, each agent would provide valuations for each possible resource bundle over time to a centralized coordinator, who would compute the optimal resource assignments across time based on these valuations.",
                "When resources can be allocated at different times to different agents, each agent must submit valuations for every combination of possible time horizons.",
                "Let each agent m ∈ M execute its MDP within the arrival-departure time interval τ ∈ [τa m, τd m].",
                "Hence, agent m will execute an MDP with time horizon no greater than Tm = τd m−τa m+1.",
                "Let bτ be the global time horizon for the problem, before which all of the agents MDPs must finish.",
                "We assume τd m < bτ, ∀m ∈ M. The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) 1221 For the scheduling problem where agents have static resource requirements within their finite-horizon MDPs, the agents provide a valuation for each resource bundle for each possible time horizon (from [1, Tm]) that they may use.",
                "Let Ω be the set of resources to be allocated among the agents.",
                "An agent will get at most one resource bundle for one of the time horizons.",
                "Let the variable ψ ∈ Ψm enumerate all possible pairs of resource bundles and time horizons for agent m, so there are 2|Ω| × Tm values for ψ (the space of bundles is exponential in the number of resource types |Ω|).",
                "The agent m must provide a value vψ m for each ψ, and the coordinator will allocate at most one ψ (resource, time horizon) pair to each agent.",
                "This allocation is expressed as an indicator variable zψ m ∈ {0, 1} that shows whether ψ is assigned to agent m. For time τ and resource ω, the function nm(ψ, τ, ω) ∈ {0, 1} indicates whether the bundle in ψ uses resource ω at time τ (we make the assumption that agents have binary resource requirements).",
                "This allocation problem is NP-complete, even when considering only a single time step, and its difficulty increases significantly with multiple time steps because of the increasing number of values of ψ.",
                "The problem of finding an optimal allocation that satisfies the global constraint that the amount of each resource ω allocated to all agents does not exceed the available amount bϕ(ω) can be expressed as the following integer program: max X m∈M X ψ∈Ψm zψ mvψ m subject to: X ψ∈Ψm zψ m ≤ 1, ∀m ∈ M; X m∈M X ψ∈Ψm zψ mnm(ψ, τ, ω) ≤ bϕ(ω), ∀τ ∈ [1, bτ], ∀ω ∈ Ω; (3) The first constraint in equation 3 says that no agent can receive more than one bundle, and the second constraint ensures that the total assignment of resource ω does not, at any time, exceed the resource bound.",
                "For the scheduling problem where the agents are able to dynamically reallocate resources, each agent must specify a value for every combination of bundles and time steps within its time horizon.",
                "Let the variable ψ ∈ Ψm in this case enumerate all possible resource bundles for which at most one bundle may be assigned to agent m at each time step.",
                "Therefore, in this case there are P t∈[1,Tm](2|Ω| )t ∼ 2|Ω|Tm possibilities of resource bundles assigned to different time slots, for the Tm different time horizons.",
                "The same set of equations (3) can be used to solve this dynamic scheduling problem, but the integer program is different because of the difference in how ψ is defined.",
                "In this case, the number of ψ values is exponential in each agents planning horizon Tm, resulting in a much larger program.",
                "This straightforward approach to solving both of these scheduling problems requires an enumeration and solution of either 2|Ω| Tm (static allocation) or P t∈[1,Tm] 2|Ω|t (dynamic reallocation) MDPs for each agent, which very quickly becomes intractable with the growth of the number of resources |Ω| or the time horizon Tm. 3.",
                "MODEL AND PROBLEM STATEMENT We now formally introduce our model of the resourcescheduling problem.",
                "The problem input consists of the following components: • M, Ω, bϕ, τa m, τd m, bτ are as defined above in Section 2.2. • {Θm} = {S, A, pm, rm, αm} are the MDPs of all agents m ∈ M. Without loss of generality, we assume that state and action spaces of all agents are the same, but each has its own transition function pm, reward function rm, and initial conditions αm. • ϕm : A×Ω → {0, 1} is the mapping of actions to resources for agent m. ϕm(a, ω) indicates whether action a of agent m needs resource ω.",
                "An agent m that receives a set of resources that does not include resource ω cannot execute in its MDP policy any action a for which ϕm(a, ω) = 0.",
                "We assume all resource requirements are binary; as discussed below in Section 6, this assumption is not limiting.",
                "Given the above input, the optimization problem we consider is to find the globally optimal-maximizing the sum of expected rewards-mapping of resources to agents for all time steps: Δ : τ × M × Ω → {0, 1}.",
                "A solution is feasible if the corresponding assignment of resources to the agents does not violate the global resource constraint: X m Δm(τ, ω) ≤ bϕ(ω), ∀ω ∈ Ω, τ ∈ [1, bτ]. (4) We consider two flavors of the resource-scheduling problem.",
                "The first formulation restricts resource assignments to the space where the allocation of resources to each agent is static during the agents lifetime.",
                "The second formulation allows reassignment of resources between agents at every time step within their lifetimes.",
                "Figure 1 depicts a resource-scheduling problem with three agents M = {m1, m2, m3}, three resources Ω = {ω1, ω2, ω3}, and a global problem horizon of bτ = 11.",
                "The agents arrival and departure times are shown as gray boxes and are {1, 6}, {3, 7}, and {2, 11}, respectively.",
                "A solution to this problem is shown via horizontal bars within each agents box, where the bars correspond to the allocation of the three resource types.",
                "Figure 1a shows a solution to a static scheduling problem.",
                "According to the shown solution, agent m1 begins the execution of its MDP at time τ = 1 and has a lock on all three resources until it finishes execution at time τ = 3.",
                "Note that agent m1 relinquishes its hold on the resources before its announced departure time of τd m1 = 6, ostensibly because other agents can utilize the resources more effectively.",
                "Thus, at time τ = 4, resources ω1 and ω3 are allocated to agent m2, who then uses them to execute its MDP (using only actions supported by resources ω1 and ω3) until time τ = 7.",
                "Agent m3 holds resource ω3 during the interval τ ∈ [4, 10].",
                "Figure 1b shows a possible solution to the dynamic version of the same problem.",
                "There, resources can be reallocated between agents at every time step.",
                "For example, agent m1 gives up its use of resource ω2 at time τ = 2, although it continues the execution of its MDP until time τ = 6.",
                "Notice that an agent is not allowed to stop and restart its MDP, so agent m1 is only able to continue executing in the interval τ ∈ [3, 4] if it has actions that do not require any resources (ϕm(a, ω) = 0).",
                "Clearly, the model and problem statement described above make a number of assumptions about the problem and the desired solution properties.",
                "We discuss some of those assumptions and their implications in Section 6. 1222 The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) (a) (b) Figure 1: Illustration of a solution to a resource-scheduling problem with three agents and three resources: a) static resource assignments (resource assignments are constant within agents lifetimes; b) dynamic assignment (resource assignments are allowed to change at every time step). 4.",
                "RESOURCE SCHEDULING Our resource-scheduling algorithm proceeds in two stages.",
                "First, we perform a preprocessing step that augments the agent MDPs; this process is described in Section 4.1.",
                "Second, using these augmented MDPs we construct a global optimization problem, which is described in Section 4.2. 4.1 Augmenting Agents MDPs In the model described in the previous section, we assume that if an agent does not possess the necessary resources to perform actions in its MDP, its execution is halted and the agent leaves the system.",
                "In other words, the MDPs cannot be paused and resumed.",
                "For example, in the problem shown in Figure 1a, agent m1 releases all resources after time τ = 3, at which point the execution of its MDP is halted.",
                "Similarly, agents m2 and m3 only execute their MDPs in the intervals τ ∈ [4, 6] and τ ∈ [4, 10], respectively.",
                "Therefore, an important part of the global decision-making problem is to decide the window of time during which each of the agents is active (i.e., executing its MDP).",
                "To accomplish this, we augment each agents MDP with two new states (start and finish states sb , sf , respectively) and a new start/stop action a∗ , as illustrated in Figure 2.",
                "The idea is that an agent stays in the start state sb until it is ready to execute its MDP, at which point it performs the start/stop action a∗ and transitions into the state space of the original MDP with the transition probability that corresponds to the original initial distribution α(s).",
                "For example, in Figure 1a, for agent m2 this would happen at time τ = 4.",
                "Once the agent gets to the end of its activity window (time τ = 6 for agent m2 in Figure 1a), it performs the start/stop action, which takes it into the sink finish state sf at time τ = 7.",
                "More precisely, given an MDP S, A, pm, rm, αm , we define an augmented MDP S , A , pm, rm, αm as follows: S = S ∪ sb ∪ sf ; A = A ∪ a∗ ; p (s|sb , a∗ ) = α(s), ∀s ∈ S; p (sb |sb , a) = 1.0, ∀a ∈ A; p (sf |s, a∗ ) = 1.0, ∀s ∈ S; p (σ|s, a) = p(σ|s, a), ∀s, σ ∈ S, a ∈ A; r (sb , a) = r (sf , a) = 0, ∀a ∈ A ; r (s, a) = r(s, a), ∀s ∈ S, a ∈ A; α (sb ) = 1; α (s) = 0, ∀s ∈ S; where all non-specified transition probabilities are assumed to be zero.",
                "Further, in order to account for the new starting state, we begin the MDP one time-step earlier, setting τa m ← τa m − 1.",
                "This will not affect the resource allocation due to the resource constraints only being enforced for the original MDP states, as will be discussed in the next section.",
                "For example, the augmented MDPs shown in Figure 2b (which starts in state sb at time τ = 2) would be constructed from an MDP with original arrival time τ = 3.",
                "Figure 2b also shows a sample trajectory through the state space: the agent starts in state sb , transitions into the state space S of the original MDP, and finally exists into the sink state sf .",
                "Note that if we wanted to model a problem where agents could pause their MDPs at arbitrary time steps (which might be useful for domains where dynamic reallocation is possible), we could easily accomplish this by including an extra action that transitions from each state to itself with zero reward. 4.2 MILP for Resource Scheduling Given a set of augmented MDPs, as defined above, the goal of this section is to formulate a global optimization program that solves the resource-scheduling problem.",
                "In this section and below, all MDPs are assumed to be the augmented MDPs as defined in Section 4.1.",
                "Our approach is similar to the idea used in [6]: we begin with the linear-program formulation of agents MDPs (1) and augment it with constraints that ensure that the corresponding resource allocation across agents and time is valid.",
                "The resulting optimization problem then simultaneously solves the agents MDPs and resource-scheduling problems.",
                "In the rest of this section, we incrementally develop a mixed integer program (MILP) that achieves this.",
                "In the absence of resource constraints, the agents finitehorizon MDPs are completely independent, and the globally optimal solution can be trivially obtained via the following LP, which is simply an aggregation of single-agent finitehorizon LPs: max X m X s X a rm(s, a) X t xm(s, a, t) subject to: X a xm(σ, a, t + 1) = X s,a pm(σ|s, a)xm(s, a, t), ∀m ∈ M, σ ∈ S, t ∈ [1, Tm − 1]; X a xm(s, a, 1) = αm(s), ∀m ∈ M, s ∈ S; (12) where xm(s, a, t) is the occupation measure of agent m, and The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) 1223 (a) (b) Figure 2: Illustration of augmenting an MDP to allow for variable starting and stopping times: a) (left) the original two-state MDP with a single action; (right) the augmented MDP with new states sb and sf and the new action a∗ (note that the origianl transitions are not changed in the augmentation process); b) the augmented MDP displayed as a trajectory through time (grey lines indicate all transitions, while black lines indicate a given trajectory.",
                "Objective Function (sum of expected rewards over all agents) max X m X s X a rm(s, a) X t xm(s, a, t) (5) Meaning Implication Linear Constraints Tie x to θ.",
                "Agent is only active when occupation measure is nonzero in original MDP states. θm(τ) = 0 =⇒ xm(s, a, τ −τa m+1) = 0 ∀s /∈ {sb , sf }, a ∈ A X s/∈{sb,sf } X a xm(s, a, t) ≤ θm(τa m + t − 1) ∀m ∈ M, ∀t ∈ [1, Tm] (6) Agent can only be active in τ ∈ (τa m, τd m) θm(τ) = 0 ∀m ∈ M, τ /∈ (τa m, τd m) (7) Cannot use resources when not active θm(τ) = 0 =⇒ Δm(τ, ω) = 0 ∀τ ∈ [0, bτ], ω ∈ Ω Δm(τ, ω) ≤ θm(τ) ∀m ∈ M, τ ∈ [0, bτ], ω ∈ Ω (8) Tie x to Δ (nonzero x forces corresponding Δ to be nonzero.)",
                "Δm(τ, ω) = 0, ϕm(a, ω) = 1 =⇒ xm(s, a, τ − τa m + 1) = 0 ∀s /∈ {sb , sf } 1/|A| X a ϕm(a, ω) X s/∈{sb,sf } xm(s, a, t) ≤ Δm(t + τa m − 1, ω) ∀m ∈ M, ω ∈ Ω, t ∈ [1, Tm] (9) Resource bounds X m Δm(τ, ω) ≤ bϕ(ω) ∀ω ∈ Ω, τ ∈ [0, bτ] (10) Agent cannot change resources while active.",
                "Only enabled for scheduling with static assignments. θm(τ) = 1 and θm(τ + 1) = 1 =⇒ Δm(τ, ω) = Δm(τ + 1, ω) Δm(τ, ω) − Z(1 − θm(τ + 1)) ≤ Δm(τ + 1, ω) + Z(1 − θm(τ)) Δm(τ, ω) + Z(1 − θm(τ + 1)) ≥ Δm(τ + 1, ω) − Z(1 − θm(τ)) ∀m ∈ M, ω ∈ Ω, τ ∈ [0, bτ] (11) Table 1: MILP for globally optimal resource scheduling.",
                "Tm = τd m − τa m + 1 is the time horizon for the agents MDP.",
                "Using this LP as a basis, we augment it with constraints that ensure that the resource usage implied by the agents occupation measures {xm} does not violate the global resource requirements bϕ at any time step τ ∈ [0, bτ].",
                "To formulate these resource constraints, we use the following binary variables: • Δm(τ, ω) = {0, 1}, ∀m ∈ M, τ ∈ [0, bτ], ω ∈ Ω, which serve as indicator variables that define whether agent m possesses resource ω at time τ.",
                "These are analogous to the static indicator variables used in the one-shot static resource-allocation problem in [6]. • θm = {0, 1}, ∀m ∈ M, τ ∈ [0, bτ] are indicator variables that specify whether agent m is active (i.e., executing its MDP) at time τ.",
                "The meaning of resource-usage variables Δ is illustrated in Figure 1: Δm(τ, ω) = 1 only if resource ω is allocated to agent m at time τ.",
                "The meaning of the activity indicators θ is illustrated in Figure 2b: when agent m is in either the start state sb or the finish state sf , the corresponding θm = 0, but once the agent becomes active and enters one of the other states, we set θm = 1 .",
                "This meaning of θ can be enforced with a linear constraint that synchronizes the values of the agents occupation measures xm and the activity 1224 The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) indicators θ, as shown in (6) in Table 1.",
                "Another constraint we have to add-because the activity indicators θ are defined on the global timeline τ-is to enforce the fact that the agent is inactive outside of its arrivaldeparture window.",
                "This is accomplished by constraint (7) in Table 1.",
                "Furthermore, agents should not be using resources while they are inactive.",
                "This constraint can also be enforced via a linear inequality on θ and Δ, as shown in (8).",
                "Constraint (6) sets the value of θ to match the policy defined by the occupation measure xm.",
                "In a similar fashion, we have to make sure that the resource-usage variables Δ are also synchronized with the occupation measure xm.",
                "This is done via constraint (9) in Table 1, which is nearly identical to the analogous constraint from [6].",
                "After implementing the above constraint, which enforces the meaning of Δ, we add a constraint that ensures that the agents resource usage never exceeds the amounts of available resources.",
                "This condition is also trivially expressed as a linear inequality (10) in Table 1.",
                "Finally, for the problem formulation where resource assignments are static during a lifetime of an agent, we add a constraint that ensures that the resource-usage variables Δ do not change their value while the agent is active (θ = 1).",
                "This is accomplished via the linear constraint (11), where Z ≥ 2 is a constant that is used to turn off the constraints when θm(τ) = 0 or θm(τ + 1) = 0.",
                "This constraint is not used for the dynamic problem formulation, where resources can be reallocated between agents at every time step.",
                "To summarize, Table 1 together with the conservationof-flow constraints from (12) defines the MILP that simultaneously computes an optimal resource assignment for all agents across time as well as optimal finite-horizon MDP policies that are valid under that resource assignment.",
                "As a rough measure of the complexity of this MILP, let us consider the number of optimization variables and constraints.",
                "Let TM = P Tm = P m(τa m − τd m + 1) be the sum of the lengths of the arrival-departure windows across all agents.",
                "Then, the number of optimization variables is: TM + bτ|M||Ω| + bτ|M|, TM of which are continuous (xm), and bτ|M||Ω| + bτ|M| are binary (Δ and θ).",
                "However, notice that all but TM|M| of the θ are set to zero by constraint (7), which also immediately forces all but TM|M||Ω| of the Δ to be zero via the constraints (8).",
                "The number of constraints (not including the degenerate constraints in (7)) in the MILP is: TM + TM|Ω| + bτ|Ω| + bτ|M||Ω|.",
                "Despite the fact that the complexity of the MILP is, in the worst case, exponential1 in the number of binary variables, the complexity of this MILP is significantly (exponentially) lower than that of the MILP with flat utility functions, described in Section 2.2.",
                "This result echos the efficiency gains reported in [6] for single-shot resource-allocation problems, but is much more pronounced, because of the explosion of the flat utility representation due to the temporal aspect of the problem (recall the prohibitive complexity of the combinatorial optimization in Section 2.2).",
                "We empirically analyze the performance of this method in Section 5. 1 Strictly speaking, solving MILPs to optimality is NPcomplete in the number of integer variables. 5.",
                "EXPERIMENTAL RESULTS Although the complexity of solving MILPs is in the worst case exponential in the number of integer variables, there are many efficient methods for solving MILPs that allow our algorithm to scale well for parameters common to resource allocation and scheduling problems.",
                "In particular, this section introduces a problem domain-the repairshop problem-used to empirically evaluate our algorithms scalability in terms of the number of agents |M|, the number of shared resources |Ω|, and the varied lengths of global time bτ during which agents may enter and exit the system.",
                "The repairshop problem is a simple parameterized MDP adopting the metaphor of a vehicular repair shop.",
                "Agents in the repair shop are mechanics with a number of independent tasks that yield reward only when completed.",
                "In our MDP model of this system, actions taken to advance through the state space are only allowed if the agent holds certain resources that are publicly available to the shop.",
                "These resources are in finite supply, and optimal policies for the shop will determine when each agent may hold the limited resources to take actions and earn individual rewards.",
                "Each task to be completed is associated with a single action, although the agent is required to repeat the action numerous times before completing the task and earning a reward.",
                "This model was parameterized in terms of the number of agents in the system, the number of different types of resources that could be linked to necessary actions, a global time during which agents are allowed to arrive and depart, and a maximum length for the number of time steps an agent may remain in the system.",
                "All datapoints in our experiments were obtained with 20 evaluations using CPLEX to solve the MILPs on a Pentium4 computer with 2Gb of RAM.",
                "Trials were conducted on both the static and the dynamic version of the resourcescheduling problem, as defined earlier.",
                "Figure 3 shows the runtime and policy value for independent modifications to the parameter set.",
                "The top row shows how the solution time for the MILP scales as we increase the number of agents |M|, the global time horizon bτ, and the number of resources |Ω|.",
                "Increasing the number of agents leads to exponential complexity scaling, which is to be expected for an NP-complete problem.",
                "However, increasing the global time limit bτ or the total number of resource types |Ω|-while holding the number of agents constantdoes not lead to decreased performance.",
                "This occurs because the problems get easier as they become under-constrained, which is also a common phenomenon for NP-complete problems.",
                "We also observe that the solution to the dynamic version of the problem can often be computed much faster than the static version.",
                "The bottom row of Figure 3 shows the joint policy value of the policies that correspond to the computed optimal resource-allocation schedules.",
                "We can observe that the dynamic version yields higher reward (as expected, since the reward for the dynamic version is always no less than the reward of the static version).",
                "We should point out that these graphs should not be viewed as a measure of performance of two different algorithms (both algorithms produce optimal solutions but to different problems), but rather as observations about how the quality of optimal solutions change as more flexibility is allowed in the reallocation of resources.",
                "Figure 4 shows runtime and policy value for trials in which common input variables are scaled together.",
                "This allows The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) 1225 2 4 6 8 10 10 −3 10 −2 10 −1 10 0 10 1 10 2 10 3 10 4 Number of Agents |M| CPUTime,sec |Ω| = 5, τ = 50 static dynamic 50 100 150 200 10 −2 10 −1 10 0 10 1 10 2 10 3 Global Time Boundary τ CPUTime,sec |M| = 5, |Ω| = 5 static dynamic 10 20 30 40 50 10 −2 10 −1 10 0 10 1 10 2 Number of Resources |Ω| CPUTime,sec |M| = 5, τ = 50 static dynamic 2 4 6 8 10 200 400 600 800 1000 1200 1400 1600 Number of Agents |M| Value |Ω| = 5, τ = 50 static dynamic 50 100 150 200 400 500 600 700 800 900 1000 1100 1200 1300 1400 Global Time Boundary τ Value |M| = 5, |Ω| = 5 static dynamic 10 20 30 40 50 500 600 700 800 900 1000 1100 1200 1300 1400 Number of Resources |Ω| Value |M| = 5, τ = 50 static dynamic Figure 3: Evaluation of our MILP for variable numbers of agents (column 1), lengths of global-time window (column 2), and numbers of resource types (column 3).",
                "Top row shows CPU time, and bottom row shows the joint reward of agents MDP policies.",
                "Error bars show the 1st and 3rd quartiles (25% and 75%). 2 4 6 8 10 10 −3 10 −2 10 −1 10 0 10 1 10 2 10 3 Number of Agents |M| CPUTime,sec τ = 10|M| static dynamic 2 4 6 8 10 10 −3 10 −2 10 −1 10 0 10 1 10 2 10 3 10 4 Number of Agents |M| CPUTime,sec |Ω| = 2|M| static dynamic 2 4 6 8 10 10 −3 10 −2 10 −1 10 0 10 1 10 2 10 3 10 4 Number of Agents |M| CPUTime,sec |Ω| = 5|M| static dynamic 2 4 6 8 10 200 400 600 800 1000 1200 1400 1600 1800 2000 2200 Number of Agents |M| Value τ = 10|M| static dynamic 2 4 6 8 10 200 400 600 800 1000 1200 1400 1600 1800 2000 Number of Agents |M| Value |Ω| = 2|M| static dynamic 2 4 6 8 10 0 500 1000 1500 2000 2500 Number of Agents |M| Value |Ω| = 5|M| static dynamic Figure 4: Evaluation of our MILP using correlated input variables.",
                "The left column tracks the performance and CPU time as the number of agents and global-time window increase together (bτ = 10|M|).",
                "The middle and the right column track the performance and CPU time as the number of resources and the number of agents increase together as |Ω| = 2|M| and |Ω| = 5|M|, respectively.",
                "Error bars show the 1st and 3rd quartiles (25% and 75%). 1226 The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) us to explore domains where the total number of agents scales proportionally to the total number of resource types or the global time horizon, while keeping constant the average agent density (per unit of global time) or the average number of resources per agent (which commonly occurs in real-life applications).",
                "Overall, we believe that these experimental results indicate that our MILP formulation can be used to effectively solve resource-scheduling problems of nontrivial size. 6.",
                "DISCUSSION AND CONCLUSIONS Throughout the paper, we have made a number of assumptions in our model and solution algorithm; we discuss their implications below. • Continual execution.",
                "We assume that once an agent stops executing its MDP (transitions into state sf ), it exits the system and cannot return.",
                "It is easy to relax this assumption for domains where agents MDPs can be paused and restarted.",
                "All that is required is to include an additional pause action which transitions from a given state back to itself, and has zero reward. • Indifference to start time.",
                "We used a reward model where agents rewards depend only on the time horizon of their MDPs and not the global start time.",
                "This is a consequence of our MDP-augmentation procedure from Section 4.1.",
                "It is easy to extend the model so that the agents incur an explicit penalty for idling by assigning a non-zero negative reward to the start state sb . • Binary resource requirements.",
                "For simplicity, we have assumed that resource costs are binary: ϕm(a, ω) = {0, 1}, but our results generalize in a straightforward manner to non-binary resource mappings, analogously to the procedure used in [5]. • Cooperative agents.",
                "The optimization procedure discussed in this paper was developed in the context of cooperative agents, but it can also be used to design a mechanism for scheduling resources among self-interested agents.",
                "This optimization procedure can be embedded in a VickreyClarke-Groves auction, completely analogously to the way it was done in [7].",
                "In fact, all the results of [7] about the properties of the auction and information privacy directly carry over to the scheduling domain discussed in this paper, requiring only slight modifications to deal with finitehorizon MDPs. • Known, deterministic arrival and departure times.",
                "Finally, we have assumed that agents arrival and departure times (τa m and τd m) are deterministic and known a priori.",
                "This assumption is fundamental to our solution method.",
                "While there are many domains where this assumption is valid, in many cases agents arrive and depart dynamically and their arrival and departure times can only be predicted probabilistically, leading to online resource-allocation problems.",
                "In particular, in the case of self-interested agents, this becomes an interesting version of an online-mechanism-design problem [11, 12].",
                "In summary, we have presented an MILP formulation for the combinatorial resource-scheduling problem where agents values for possible resource assignments are defined by finitehorizon MDPs.",
                "This result extends previous work ([6, 7]) on static one-shot resource allocation under MDP-induced preferences to resource-scheduling problems with a temporal aspect.",
                "As such, this work takes a step in the direction of designing an online mechanism for agents with combinatorial resource preferences induced by stochastic planning problems.",
                "Relaxing the assumption about deterministic arrival and departure times of the agents is a focus of our future work.",
                "We would like to thank the anonymous reviewers for their insightful comments and suggestions. 7.",
                "REFERENCES [1] E. Altman and A. Shwartz.",
                "Adaptive control of constrained Markov chains: Criteria and policies.",
                "Annals of Operations Research, special issue on Markov Decision Processes, 28:101-134, 1991. [2] R. Bellman.",
                "Dynamic Programming.",
                "Princeton University Press, 1957. [3] C. Boutilier.",
                "Solving concisely expressed combinatorial auction problems.",
                "In Proc. of AAAI-02, pages 359-366, 2002. [4] C. Boutilier and H. H. Hoos.",
                "Bidding languages for combinatorial auctions.",
                "In Proc. of IJCAI-01, pages 1211-1217, 2001. [5] D. Dolgov.",
                "Integrated Resource Allocation and Planning in Stochastic Multiagent Environments.",
                "PhD thesis, Computer Science Department, University of Michigan, February 2006. [6] D. A. Dolgov and E. H. Durfee.",
                "Optimal resource allocation and policy formulation in loosely-coupled Markov decision processes.",
                "In Proc. of ICAPS-04, pages 315-324, June 2004. [7] D. A. Dolgov and E. H. Durfee.",
                "Computationally efficient combinatorial auctions for resource allocation in weakly-coupled MDPs.",
                "In Proc. of AAMAS-05, New York, NY, USA, 2005.",
                "ACM Press. [8] D. A. Dolgov and E. H. Durfee.",
                "Resource allocation among agents with preferences induced by factored MDPs.",
                "In Proc. of AAMAS-06, 2006. [9] K. Larson and T. Sandholm.",
                "Mechanism design and deliberative agents.",
                "In Proc. of AAMAS-05, pages 650-656, New York, NY, USA, 2005.",
                "ACM Press. [10] N. Nisan.",
                "Bidding and allocation in combinatorial auctions.",
                "In Electronic Commerce, 2000. [11] D. C. Parkes and S. Singh.",
                "An MDP-based approach to Online Mechanism Design.",
                "In Proc. of the Seventeenths Annual Conference on Neural Information Processing Systems (NIPS-03), 2003. [12] D. C. Parkes, S. Singh, and D. Yanovsky.",
                "Approximately efficient online mechanism design.",
                "In Proc. of the Eighteenths Annual Conference on Neural Information Processing Systems (NIPS-04), 2004. [13] M. L. Puterman.",
                "Markov Decision Processes.",
                "John Wiley & Sons, New York, 1994. [14] M. H. Rothkopf, A. Pekec, and R. M. Harstad.",
                "Computationally manageable combinational auctions.",
                "Management Science, 44(8):1131-1147, 1998. [15] T. Sandholm.",
                "An algorithm for optimal winner determination in combinatorial auctions.",
                "In Proc. of IJCAI-99, pages 542-547, San Francisco, CA, USA, 1999.",
                "Morgan Kaufmann Publishers Inc.",
                "The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) 1227"
            ],
            "original_annotated_samples": [],
            "translated_annotated_samples": [],
            "translated_text": "",
            "candidates": [],
            "error": []
        },
        "resource-scheduling algorithm": {
            "translated_key": "",
            "is_in_text": true,
            "original_annotated_sentences": [
                "Combinatorial Resource Scheduling for Multiagent MDPs Dmitri A. Dolgov, Michael R. James, and Michael E. Samples AI and Robotics Group Technical Research, Toyota Technical Center USA {ddolgov, michael.r.james, michael.samples}@gmail.com ABSTRACT Optimal resource scheduling in multiagent systems is a computationally challenging task, particularly when the values of resources are not additive.",
                "We consider the combinatorial problem of scheduling the usage of multiple resources among agents that operate in stochastic environments, modeled as Markov decision processes (MDPs).",
                "In recent years, efficient resource-allocation algorithms have been developed for agents with resource values induced by MDPs.",
                "However, this prior work has focused on static resource-allocation problems where resources are distributed once and then utilized in infinite-horizon MDPs.",
                "We extend those existing models to the problem of combinatorial resource scheduling, where agents persist only for finite periods between their (predefined) arrival and departure times, requiring resources only for those time periods.",
                "We provide a computationally efficient procedure for computing globally optimal resource assignments to agents over time.",
                "We illustrate and empirically analyze the method in the context of a stochastic jobscheduling domain.",
                "Categories and Subject Descriptors I.2.8 [Artificial Intelligence]: Problem Solving, Control Methods, and Search; I.2.11 [Artificial Intelligence]: Distributed Artificial Intelligence-Multiagent systems General Terms Algorithms, Performance, Design 1.",
                "INTRODUCTION The tasks of optimal resource allocation and scheduling are ubiquitous in multiagent systems, but solving such optimization problems can be computationally difficult, due to a number of factors.",
                "In particular, when the value of a set of resources to an agent is not additive (as is often the case with resources that are substitutes or complements), the utility function might have to be defined on an exponentially large space of resource bundles, which very quickly becomes computationally intractable.",
                "Further, even when each agent has a utility function that is nonzero only on a small subset of the possible resource bundles, obtaining optimal allocation is still computationally prohibitive, as the problem becomes NP-complete [14].",
                "Such computational issues have recently spawned several threads of work in using compact models of agents preferences.",
                "One idea is to use any structure present in utility functions to represent them compactly, via, for example, logical formulas [15, 10, 4, 3].",
                "An alternative is to directly model the mechanisms that define the agents utility functions and perform resource allocation directly with these models [9].",
                "A way of accomplishing this is to model the processes by which an agent might utilize the resources and define the utility function as the payoff of these processes.",
                "In particular, if an agent uses resources to act in a stochastic environment, its utility function can be naturally modeled with a Markov decision process, whose action set is parameterized by the available resources.",
                "This representation can then be used to construct very efficient resource-allocation algorithms that lead to an exponential speedup over a straightforward optimization problem with flat representations of combinatorial preferences [6, 7, 8].",
                "However, this existing work on resource allocation with preferences induced by resource-parameterized MDPs makes an assumption that the resources are only allocated once and are then utilized by the agents independently within their infinite-horizon MDPs.",
                "This assumption that no reallocation of resources is possible can be limiting in domains where agents arrive and depart dynamically.",
                "In this paper, we extend the work on resource allocation under MDP-induced preferences to discrete-time scheduling problems, where agents are present in the system for finite time intervals and can only use resources within these intervals.",
                "In particular, agents arrive and depart at arbitrary (predefined) times and within these intervals use resources to execute tasks in finite-horizon MDPs.",
                "We address the problem of globally optimal resource scheduling, where the objective is to find an allocation of resources to the agents across time that maximizes the sum of the expected rewards that they obtain.",
                "In this context, our main contribution is a mixed-integerprogramming formulation of the scheduling problem that chooses globally optimal resource assignments, starting times, and execution horizons for all agents (within their arrival1220 978-81-904262-7-5 (RPS) c 2007 IFAAMAS departure intervals).",
                "We analyze and empirically compare two flavors of the scheduling problem: one, where agents have static resource assignments within their finite-horizon MDPs, and another, where resources can be dynamically reallocated between agents at every time step.",
                "In the rest of the paper, we first lay down the necessary groundwork in Section 2 and then introduce our model and formal problem statement in Section 3.",
                "In Section 4.2, we describe our main result, the optimization program for globally optimal resource scheduling.",
                "Following the discussion of our experimental results on a job-scheduling problem in Section 5, we conclude in Section 6 with a discussion of possible extensions and generalizations of our method. 2.",
                "BACKGROUND Similarly to the model used in previous work on resourceallocation with MDP-induced preferences [6, 7], we define the value of a set of resources to an agent as the value of the best MDP policy that is realizable, given those resources.",
                "However, since the focus of our work is on scheduling problems, and a large part of the optimization problem is to decide how resources are allocated in time among agents with finite arrival and departure times, we model the agents planning problems as finite-horizon MDPs, in contrast to previous work that used infinite-horizon discounted MDPs.",
                "In the rest of this section, we first introduce some necessary background on finite-horizon MDPs and present a linear-programming formulation that serves as the basis for our solution algorithm developed in Section 4.",
                "We also outline the standard methods for combinatorial resource scheduling with flat resource values, which serve as a comparison benchmark for the new model developed here. 2.1 Markov Decision Processes A stationary, finite-domain, discrete-time MDP (see, for example, [13] for a thorough and detailed development) can be described as S, A, p, r , where: S is a finite set of system states; A is a finite set of actions that are available to the agent; p is a stationary stochastic transition function, where p(σ|s, a) is the probability of transitioning to state σ upon executing action a in state s; r is a stationary reward function, where r(s, a) specifies the reward obtained upon executing action a in state s. Given such an MDP, a decision problem under a finite horizon T is to choose an optimal action at every time step to maximize the expected value of the total reward accrued during the agents (finite) lifetime.",
                "The agents optimal policy is then a function of current state s and the time until the horizon.",
                "An optimal policy for such a problem is to act greedily with respect to the optimal value function, defined recursively by the following system of finite-time Bellman equations [2]: v(s, t) = max a r(s, a) + X σ p(σ|s, a)v(σ, t + 1), ∀s ∈ S, t ∈ [1, T − 1]; v(s, T) = 0, ∀s ∈ S; where v(s, t) is the optimal value of being in state s at time t ∈ [1, T].",
                "This optimal value function can be easily computed using dynamic programming, leading to the following optimal policy π, where π(s, a, t) is the probability of executing action a in state s at time t: π(s, a, t) = ( 1, a = argmaxa r(s, a) + P σ p(σ|s, a)v(σ, t + 1), 0, otherwise.",
                "The above is the most common way of computing the optimal value function (and therefore an optimal policy) for a finite-horizon MDP.",
                "However, we can also formulate the problem as the following linear program (similarly to the dual LP for infinite-horizon discounted MDPs [13, 6, 7]): max X s X a r(s, a) X t x(s, a, t) subject to: X a x(σ, a, t + 1) = X s,a p(σ|s, a)x(s, a, t) ∀σ, t ∈ [1, T − 1]; X a x(s, a, 1) = α(s), ∀s ∈ S; (1) where α(s) is the initial distribution over the state space, and x is the (non-stationary) occupation measure (x(s, a, t) ∈ [0, 1] is the total expected number of times action a is executed in state s at time t).",
                "An optimal (non-stationary) policy is obtained from the occupation measure as follows: π(s, a, t) = x(s, a, t)/ X a x(s, a, t) ∀s ∈ S, t ∈ [1, T]. (2) Note that the standard unconstrained finite-horizon MDP, as described above, always has a uniformly-optimal solution (optimal for any initial distribution α(s)).",
                "Therefore, an optimal policy can be obtained by using an arbitrary constant α(s) > 0 (in particular, α(s) = 1 will result in x(s, a, t) = π(s, a, t)).",
                "However, for MDPs with resource constraints (as defined below in Section 3), uniformly-optimal policies do not in general exist.",
                "In such cases, α becomes a part of the problem input, and a resulting policy is only optimal for that particular α.",
                "This result is well known for infinite-horizon MDPs with various types of constraints [1, 6], and it also holds for our finite-horizon model, which can be easily established via a line of reasoning completely analogous to the arguments in [6]. 2.2 Combinatorial Resource Scheduling A straightforward approach to resource scheduling for a set of agents M, whose values for the resources are induced by stochastic planning problems (in our case, finite-horizon MDPs) would be to have each agent enumerate all possible resource assignments over time and, for each one, compute its value by solving the corresponding MDP.",
                "Then, each agent would provide valuations for each possible resource bundle over time to a centralized coordinator, who would compute the optimal resource assignments across time based on these valuations.",
                "When resources can be allocated at different times to different agents, each agent must submit valuations for every combination of possible time horizons.",
                "Let each agent m ∈ M execute its MDP within the arrival-departure time interval τ ∈ [τa m, τd m].",
                "Hence, agent m will execute an MDP with time horizon no greater than Tm = τd m−τa m+1.",
                "Let bτ be the global time horizon for the problem, before which all of the agents MDPs must finish.",
                "We assume τd m < bτ, ∀m ∈ M. The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) 1221 For the scheduling problem where agents have static resource requirements within their finite-horizon MDPs, the agents provide a valuation for each resource bundle for each possible time horizon (from [1, Tm]) that they may use.",
                "Let Ω be the set of resources to be allocated among the agents.",
                "An agent will get at most one resource bundle for one of the time horizons.",
                "Let the variable ψ ∈ Ψm enumerate all possible pairs of resource bundles and time horizons for agent m, so there are 2|Ω| × Tm values for ψ (the space of bundles is exponential in the number of resource types |Ω|).",
                "The agent m must provide a value vψ m for each ψ, and the coordinator will allocate at most one ψ (resource, time horizon) pair to each agent.",
                "This allocation is expressed as an indicator variable zψ m ∈ {0, 1} that shows whether ψ is assigned to agent m. For time τ and resource ω, the function nm(ψ, τ, ω) ∈ {0, 1} indicates whether the bundle in ψ uses resource ω at time τ (we make the assumption that agents have binary resource requirements).",
                "This allocation problem is NP-complete, even when considering only a single time step, and its difficulty increases significantly with multiple time steps because of the increasing number of values of ψ.",
                "The problem of finding an optimal allocation that satisfies the global constraint that the amount of each resource ω allocated to all agents does not exceed the available amount bϕ(ω) can be expressed as the following integer program: max X m∈M X ψ∈Ψm zψ mvψ m subject to: X ψ∈Ψm zψ m ≤ 1, ∀m ∈ M; X m∈M X ψ∈Ψm zψ mnm(ψ, τ, ω) ≤ bϕ(ω), ∀τ ∈ [1, bτ], ∀ω ∈ Ω; (3) The first constraint in equation 3 says that no agent can receive more than one bundle, and the second constraint ensures that the total assignment of resource ω does not, at any time, exceed the resource bound.",
                "For the scheduling problem where the agents are able to dynamically reallocate resources, each agent must specify a value for every combination of bundles and time steps within its time horizon.",
                "Let the variable ψ ∈ Ψm in this case enumerate all possible resource bundles for which at most one bundle may be assigned to agent m at each time step.",
                "Therefore, in this case there are P t∈[1,Tm](2|Ω| )t ∼ 2|Ω|Tm possibilities of resource bundles assigned to different time slots, for the Tm different time horizons.",
                "The same set of equations (3) can be used to solve this dynamic scheduling problem, but the integer program is different because of the difference in how ψ is defined.",
                "In this case, the number of ψ values is exponential in each agents planning horizon Tm, resulting in a much larger program.",
                "This straightforward approach to solving both of these scheduling problems requires an enumeration and solution of either 2|Ω| Tm (static allocation) or P t∈[1,Tm] 2|Ω|t (dynamic reallocation) MDPs for each agent, which very quickly becomes intractable with the growth of the number of resources |Ω| or the time horizon Tm. 3.",
                "MODEL AND PROBLEM STATEMENT We now formally introduce our model of the resourcescheduling problem.",
                "The problem input consists of the following components: • M, Ω, bϕ, τa m, τd m, bτ are as defined above in Section 2.2. • {Θm} = {S, A, pm, rm, αm} are the MDPs of all agents m ∈ M. Without loss of generality, we assume that state and action spaces of all agents are the same, but each has its own transition function pm, reward function rm, and initial conditions αm. • ϕm : A×Ω → {0, 1} is the mapping of actions to resources for agent m. ϕm(a, ω) indicates whether action a of agent m needs resource ω.",
                "An agent m that receives a set of resources that does not include resource ω cannot execute in its MDP policy any action a for which ϕm(a, ω) = 0.",
                "We assume all resource requirements are binary; as discussed below in Section 6, this assumption is not limiting.",
                "Given the above input, the optimization problem we consider is to find the globally optimal-maximizing the sum of expected rewards-mapping of resources to agents for all time steps: Δ : τ × M × Ω → {0, 1}.",
                "A solution is feasible if the corresponding assignment of resources to the agents does not violate the global resource constraint: X m Δm(τ, ω) ≤ bϕ(ω), ∀ω ∈ Ω, τ ∈ [1, bτ]. (4) We consider two flavors of the resource-scheduling problem.",
                "The first formulation restricts resource assignments to the space where the allocation of resources to each agent is static during the agents lifetime.",
                "The second formulation allows reassignment of resources between agents at every time step within their lifetimes.",
                "Figure 1 depicts a resource-scheduling problem with three agents M = {m1, m2, m3}, three resources Ω = {ω1, ω2, ω3}, and a global problem horizon of bτ = 11.",
                "The agents arrival and departure times are shown as gray boxes and are {1, 6}, {3, 7}, and {2, 11}, respectively.",
                "A solution to this problem is shown via horizontal bars within each agents box, where the bars correspond to the allocation of the three resource types.",
                "Figure 1a shows a solution to a static scheduling problem.",
                "According to the shown solution, agent m1 begins the execution of its MDP at time τ = 1 and has a lock on all three resources until it finishes execution at time τ = 3.",
                "Note that agent m1 relinquishes its hold on the resources before its announced departure time of τd m1 = 6, ostensibly because other agents can utilize the resources more effectively.",
                "Thus, at time τ = 4, resources ω1 and ω3 are allocated to agent m2, who then uses them to execute its MDP (using only actions supported by resources ω1 and ω3) until time τ = 7.",
                "Agent m3 holds resource ω3 during the interval τ ∈ [4, 10].",
                "Figure 1b shows a possible solution to the dynamic version of the same problem.",
                "There, resources can be reallocated between agents at every time step.",
                "For example, agent m1 gives up its use of resource ω2 at time τ = 2, although it continues the execution of its MDP until time τ = 6.",
                "Notice that an agent is not allowed to stop and restart its MDP, so agent m1 is only able to continue executing in the interval τ ∈ [3, 4] if it has actions that do not require any resources (ϕm(a, ω) = 0).",
                "Clearly, the model and problem statement described above make a number of assumptions about the problem and the desired solution properties.",
                "We discuss some of those assumptions and their implications in Section 6. 1222 The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) (a) (b) Figure 1: Illustration of a solution to a resource-scheduling problem with three agents and three resources: a) static resource assignments (resource assignments are constant within agents lifetimes; b) dynamic assignment (resource assignments are allowed to change at every time step). 4.",
                "RESOURCE SCHEDULING Our <br>resource-scheduling algorithm</br> proceeds in two stages.",
                "First, we perform a preprocessing step that augments the agent MDPs; this process is described in Section 4.1.",
                "Second, using these augmented MDPs we construct a global optimization problem, which is described in Section 4.2. 4.1 Augmenting Agents MDPs In the model described in the previous section, we assume that if an agent does not possess the necessary resources to perform actions in its MDP, its execution is halted and the agent leaves the system.",
                "In other words, the MDPs cannot be paused and resumed.",
                "For example, in the problem shown in Figure 1a, agent m1 releases all resources after time τ = 3, at which point the execution of its MDP is halted.",
                "Similarly, agents m2 and m3 only execute their MDPs in the intervals τ ∈ [4, 6] and τ ∈ [4, 10], respectively.",
                "Therefore, an important part of the global decision-making problem is to decide the window of time during which each of the agents is active (i.e., executing its MDP).",
                "To accomplish this, we augment each agents MDP with two new states (start and finish states sb , sf , respectively) and a new start/stop action a∗ , as illustrated in Figure 2.",
                "The idea is that an agent stays in the start state sb until it is ready to execute its MDP, at which point it performs the start/stop action a∗ and transitions into the state space of the original MDP with the transition probability that corresponds to the original initial distribution α(s).",
                "For example, in Figure 1a, for agent m2 this would happen at time τ = 4.",
                "Once the agent gets to the end of its activity window (time τ = 6 for agent m2 in Figure 1a), it performs the start/stop action, which takes it into the sink finish state sf at time τ = 7.",
                "More precisely, given an MDP S, A, pm, rm, αm , we define an augmented MDP S , A , pm, rm, αm as follows: S = S ∪ sb ∪ sf ; A = A ∪ a∗ ; p (s|sb , a∗ ) = α(s), ∀s ∈ S; p (sb |sb , a) = 1.0, ∀a ∈ A; p (sf |s, a∗ ) = 1.0, ∀s ∈ S; p (σ|s, a) = p(σ|s, a), ∀s, σ ∈ S, a ∈ A; r (sb , a) = r (sf , a) = 0, ∀a ∈ A ; r (s, a) = r(s, a), ∀s ∈ S, a ∈ A; α (sb ) = 1; α (s) = 0, ∀s ∈ S; where all non-specified transition probabilities are assumed to be zero.",
                "Further, in order to account for the new starting state, we begin the MDP one time-step earlier, setting τa m ← τa m − 1.",
                "This will not affect the resource allocation due to the resource constraints only being enforced for the original MDP states, as will be discussed in the next section.",
                "For example, the augmented MDPs shown in Figure 2b (which starts in state sb at time τ = 2) would be constructed from an MDP with original arrival time τ = 3.",
                "Figure 2b also shows a sample trajectory through the state space: the agent starts in state sb , transitions into the state space S of the original MDP, and finally exists into the sink state sf .",
                "Note that if we wanted to model a problem where agents could pause their MDPs at arbitrary time steps (which might be useful for domains where dynamic reallocation is possible), we could easily accomplish this by including an extra action that transitions from each state to itself with zero reward. 4.2 MILP for Resource Scheduling Given a set of augmented MDPs, as defined above, the goal of this section is to formulate a global optimization program that solves the resource-scheduling problem.",
                "In this section and below, all MDPs are assumed to be the augmented MDPs as defined in Section 4.1.",
                "Our approach is similar to the idea used in [6]: we begin with the linear-program formulation of agents MDPs (1) and augment it with constraints that ensure that the corresponding resource allocation across agents and time is valid.",
                "The resulting optimization problem then simultaneously solves the agents MDPs and resource-scheduling problems.",
                "In the rest of this section, we incrementally develop a mixed integer program (MILP) that achieves this.",
                "In the absence of resource constraints, the agents finitehorizon MDPs are completely independent, and the globally optimal solution can be trivially obtained via the following LP, which is simply an aggregation of single-agent finitehorizon LPs: max X m X s X a rm(s, a) X t xm(s, a, t) subject to: X a xm(σ, a, t + 1) = X s,a pm(σ|s, a)xm(s, a, t), ∀m ∈ M, σ ∈ S, t ∈ [1, Tm − 1]; X a xm(s, a, 1) = αm(s), ∀m ∈ M, s ∈ S; (12) where xm(s, a, t) is the occupation measure of agent m, and The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) 1223 (a) (b) Figure 2: Illustration of augmenting an MDP to allow for variable starting and stopping times: a) (left) the original two-state MDP with a single action; (right) the augmented MDP with new states sb and sf and the new action a∗ (note that the origianl transitions are not changed in the augmentation process); b) the augmented MDP displayed as a trajectory through time (grey lines indicate all transitions, while black lines indicate a given trajectory.",
                "Objective Function (sum of expected rewards over all agents) max X m X s X a rm(s, a) X t xm(s, a, t) (5) Meaning Implication Linear Constraints Tie x to θ.",
                "Agent is only active when occupation measure is nonzero in original MDP states. θm(τ) = 0 =⇒ xm(s, a, τ −τa m+1) = 0 ∀s /∈ {sb , sf }, a ∈ A X s/∈{sb,sf } X a xm(s, a, t) ≤ θm(τa m + t − 1) ∀m ∈ M, ∀t ∈ [1, Tm] (6) Agent can only be active in τ ∈ (τa m, τd m) θm(τ) = 0 ∀m ∈ M, τ /∈ (τa m, τd m) (7) Cannot use resources when not active θm(τ) = 0 =⇒ Δm(τ, ω) = 0 ∀τ ∈ [0, bτ], ω ∈ Ω Δm(τ, ω) ≤ θm(τ) ∀m ∈ M, τ ∈ [0, bτ], ω ∈ Ω (8) Tie x to Δ (nonzero x forces corresponding Δ to be nonzero.)",
                "Δm(τ, ω) = 0, ϕm(a, ω) = 1 =⇒ xm(s, a, τ − τa m + 1) = 0 ∀s /∈ {sb , sf } 1/|A| X a ϕm(a, ω) X s/∈{sb,sf } xm(s, a, t) ≤ Δm(t + τa m − 1, ω) ∀m ∈ M, ω ∈ Ω, t ∈ [1, Tm] (9) Resource bounds X m Δm(τ, ω) ≤ bϕ(ω) ∀ω ∈ Ω, τ ∈ [0, bτ] (10) Agent cannot change resources while active.",
                "Only enabled for scheduling with static assignments. θm(τ) = 1 and θm(τ + 1) = 1 =⇒ Δm(τ, ω) = Δm(τ + 1, ω) Δm(τ, ω) − Z(1 − θm(τ + 1)) ≤ Δm(τ + 1, ω) + Z(1 − θm(τ)) Δm(τ, ω) + Z(1 − θm(τ + 1)) ≥ Δm(τ + 1, ω) − Z(1 − θm(τ)) ∀m ∈ M, ω ∈ Ω, τ ∈ [0, bτ] (11) Table 1: MILP for globally optimal resource scheduling.",
                "Tm = τd m − τa m + 1 is the time horizon for the agents MDP.",
                "Using this LP as a basis, we augment it with constraints that ensure that the resource usage implied by the agents occupation measures {xm} does not violate the global resource requirements bϕ at any time step τ ∈ [0, bτ].",
                "To formulate these resource constraints, we use the following binary variables: • Δm(τ, ω) = {0, 1}, ∀m ∈ M, τ ∈ [0, bτ], ω ∈ Ω, which serve as indicator variables that define whether agent m possesses resource ω at time τ.",
                "These are analogous to the static indicator variables used in the one-shot static resource-allocation problem in [6]. • θm = {0, 1}, ∀m ∈ M, τ ∈ [0, bτ] are indicator variables that specify whether agent m is active (i.e., executing its MDP) at time τ.",
                "The meaning of resource-usage variables Δ is illustrated in Figure 1: Δm(τ, ω) = 1 only if resource ω is allocated to agent m at time τ.",
                "The meaning of the activity indicators θ is illustrated in Figure 2b: when agent m is in either the start state sb or the finish state sf , the corresponding θm = 0, but once the agent becomes active and enters one of the other states, we set θm = 1 .",
                "This meaning of θ can be enforced with a linear constraint that synchronizes the values of the agents occupation measures xm and the activity 1224 The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) indicators θ, as shown in (6) in Table 1.",
                "Another constraint we have to add-because the activity indicators θ are defined on the global timeline τ-is to enforce the fact that the agent is inactive outside of its arrivaldeparture window.",
                "This is accomplished by constraint (7) in Table 1.",
                "Furthermore, agents should not be using resources while they are inactive.",
                "This constraint can also be enforced via a linear inequality on θ and Δ, as shown in (8).",
                "Constraint (6) sets the value of θ to match the policy defined by the occupation measure xm.",
                "In a similar fashion, we have to make sure that the resource-usage variables Δ are also synchronized with the occupation measure xm.",
                "This is done via constraint (9) in Table 1, which is nearly identical to the analogous constraint from [6].",
                "After implementing the above constraint, which enforces the meaning of Δ, we add a constraint that ensures that the agents resource usage never exceeds the amounts of available resources.",
                "This condition is also trivially expressed as a linear inequality (10) in Table 1.",
                "Finally, for the problem formulation where resource assignments are static during a lifetime of an agent, we add a constraint that ensures that the resource-usage variables Δ do not change their value while the agent is active (θ = 1).",
                "This is accomplished via the linear constraint (11), where Z ≥ 2 is a constant that is used to turn off the constraints when θm(τ) = 0 or θm(τ + 1) = 0.",
                "This constraint is not used for the dynamic problem formulation, where resources can be reallocated between agents at every time step.",
                "To summarize, Table 1 together with the conservationof-flow constraints from (12) defines the MILP that simultaneously computes an optimal resource assignment for all agents across time as well as optimal finite-horizon MDP policies that are valid under that resource assignment.",
                "As a rough measure of the complexity of this MILP, let us consider the number of optimization variables and constraints.",
                "Let TM = P Tm = P m(τa m − τd m + 1) be the sum of the lengths of the arrival-departure windows across all agents.",
                "Then, the number of optimization variables is: TM + bτ|M||Ω| + bτ|M|, TM of which are continuous (xm), and bτ|M||Ω| + bτ|M| are binary (Δ and θ).",
                "However, notice that all but TM|M| of the θ are set to zero by constraint (7), which also immediately forces all but TM|M||Ω| of the Δ to be zero via the constraints (8).",
                "The number of constraints (not including the degenerate constraints in (7)) in the MILP is: TM + TM|Ω| + bτ|Ω| + bτ|M||Ω|.",
                "Despite the fact that the complexity of the MILP is, in the worst case, exponential1 in the number of binary variables, the complexity of this MILP is significantly (exponentially) lower than that of the MILP with flat utility functions, described in Section 2.2.",
                "This result echos the efficiency gains reported in [6] for single-shot resource-allocation problems, but is much more pronounced, because of the explosion of the flat utility representation due to the temporal aspect of the problem (recall the prohibitive complexity of the combinatorial optimization in Section 2.2).",
                "We empirically analyze the performance of this method in Section 5. 1 Strictly speaking, solving MILPs to optimality is NPcomplete in the number of integer variables. 5.",
                "EXPERIMENTAL RESULTS Although the complexity of solving MILPs is in the worst case exponential in the number of integer variables, there are many efficient methods for solving MILPs that allow our algorithm to scale well for parameters common to resource allocation and scheduling problems.",
                "In particular, this section introduces a problem domain-the repairshop problem-used to empirically evaluate our algorithms scalability in terms of the number of agents |M|, the number of shared resources |Ω|, and the varied lengths of global time bτ during which agents may enter and exit the system.",
                "The repairshop problem is a simple parameterized MDP adopting the metaphor of a vehicular repair shop.",
                "Agents in the repair shop are mechanics with a number of independent tasks that yield reward only when completed.",
                "In our MDP model of this system, actions taken to advance through the state space are only allowed if the agent holds certain resources that are publicly available to the shop.",
                "These resources are in finite supply, and optimal policies for the shop will determine when each agent may hold the limited resources to take actions and earn individual rewards.",
                "Each task to be completed is associated with a single action, although the agent is required to repeat the action numerous times before completing the task and earning a reward.",
                "This model was parameterized in terms of the number of agents in the system, the number of different types of resources that could be linked to necessary actions, a global time during which agents are allowed to arrive and depart, and a maximum length for the number of time steps an agent may remain in the system.",
                "All datapoints in our experiments were obtained with 20 evaluations using CPLEX to solve the MILPs on a Pentium4 computer with 2Gb of RAM.",
                "Trials were conducted on both the static and the dynamic version of the resourcescheduling problem, as defined earlier.",
                "Figure 3 shows the runtime and policy value for independent modifications to the parameter set.",
                "The top row shows how the solution time for the MILP scales as we increase the number of agents |M|, the global time horizon bτ, and the number of resources |Ω|.",
                "Increasing the number of agents leads to exponential complexity scaling, which is to be expected for an NP-complete problem.",
                "However, increasing the global time limit bτ or the total number of resource types |Ω|-while holding the number of agents constantdoes not lead to decreased performance.",
                "This occurs because the problems get easier as they become under-constrained, which is also a common phenomenon for NP-complete problems.",
                "We also observe that the solution to the dynamic version of the problem can often be computed much faster than the static version.",
                "The bottom row of Figure 3 shows the joint policy value of the policies that correspond to the computed optimal resource-allocation schedules.",
                "We can observe that the dynamic version yields higher reward (as expected, since the reward for the dynamic version is always no less than the reward of the static version).",
                "We should point out that these graphs should not be viewed as a measure of performance of two different algorithms (both algorithms produce optimal solutions but to different problems), but rather as observations about how the quality of optimal solutions change as more flexibility is allowed in the reallocation of resources.",
                "Figure 4 shows runtime and policy value for trials in which common input variables are scaled together.",
                "This allows The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) 1225 2 4 6 8 10 10 −3 10 −2 10 −1 10 0 10 1 10 2 10 3 10 4 Number of Agents |M| CPUTime,sec |Ω| = 5, τ = 50 static dynamic 50 100 150 200 10 −2 10 −1 10 0 10 1 10 2 10 3 Global Time Boundary τ CPUTime,sec |M| = 5, |Ω| = 5 static dynamic 10 20 30 40 50 10 −2 10 −1 10 0 10 1 10 2 Number of Resources |Ω| CPUTime,sec |M| = 5, τ = 50 static dynamic 2 4 6 8 10 200 400 600 800 1000 1200 1400 1600 Number of Agents |M| Value |Ω| = 5, τ = 50 static dynamic 50 100 150 200 400 500 600 700 800 900 1000 1100 1200 1300 1400 Global Time Boundary τ Value |M| = 5, |Ω| = 5 static dynamic 10 20 30 40 50 500 600 700 800 900 1000 1100 1200 1300 1400 Number of Resources |Ω| Value |M| = 5, τ = 50 static dynamic Figure 3: Evaluation of our MILP for variable numbers of agents (column 1), lengths of global-time window (column 2), and numbers of resource types (column 3).",
                "Top row shows CPU time, and bottom row shows the joint reward of agents MDP policies.",
                "Error bars show the 1st and 3rd quartiles (25% and 75%). 2 4 6 8 10 10 −3 10 −2 10 −1 10 0 10 1 10 2 10 3 Number of Agents |M| CPUTime,sec τ = 10|M| static dynamic 2 4 6 8 10 10 −3 10 −2 10 −1 10 0 10 1 10 2 10 3 10 4 Number of Agents |M| CPUTime,sec |Ω| = 2|M| static dynamic 2 4 6 8 10 10 −3 10 −2 10 −1 10 0 10 1 10 2 10 3 10 4 Number of Agents |M| CPUTime,sec |Ω| = 5|M| static dynamic 2 4 6 8 10 200 400 600 800 1000 1200 1400 1600 1800 2000 2200 Number of Agents |M| Value τ = 10|M| static dynamic 2 4 6 8 10 200 400 600 800 1000 1200 1400 1600 1800 2000 Number of Agents |M| Value |Ω| = 2|M| static dynamic 2 4 6 8 10 0 500 1000 1500 2000 2500 Number of Agents |M| Value |Ω| = 5|M| static dynamic Figure 4: Evaluation of our MILP using correlated input variables.",
                "The left column tracks the performance and CPU time as the number of agents and global-time window increase together (bτ = 10|M|).",
                "The middle and the right column track the performance and CPU time as the number of resources and the number of agents increase together as |Ω| = 2|M| and |Ω| = 5|M|, respectively.",
                "Error bars show the 1st and 3rd quartiles (25% and 75%). 1226 The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) us to explore domains where the total number of agents scales proportionally to the total number of resource types or the global time horizon, while keeping constant the average agent density (per unit of global time) or the average number of resources per agent (which commonly occurs in real-life applications).",
                "Overall, we believe that these experimental results indicate that our MILP formulation can be used to effectively solve resource-scheduling problems of nontrivial size. 6.",
                "DISCUSSION AND CONCLUSIONS Throughout the paper, we have made a number of assumptions in our model and solution algorithm; we discuss their implications below. • Continual execution.",
                "We assume that once an agent stops executing its MDP (transitions into state sf ), it exits the system and cannot return.",
                "It is easy to relax this assumption for domains where agents MDPs can be paused and restarted.",
                "All that is required is to include an additional pause action which transitions from a given state back to itself, and has zero reward. • Indifference to start time.",
                "We used a reward model where agents rewards depend only on the time horizon of their MDPs and not the global start time.",
                "This is a consequence of our MDP-augmentation procedure from Section 4.1.",
                "It is easy to extend the model so that the agents incur an explicit penalty for idling by assigning a non-zero negative reward to the start state sb . • Binary resource requirements.",
                "For simplicity, we have assumed that resource costs are binary: ϕm(a, ω) = {0, 1}, but our results generalize in a straightforward manner to non-binary resource mappings, analogously to the procedure used in [5]. • Cooperative agents.",
                "The optimization procedure discussed in this paper was developed in the context of cooperative agents, but it can also be used to design a mechanism for scheduling resources among self-interested agents.",
                "This optimization procedure can be embedded in a VickreyClarke-Groves auction, completely analogously to the way it was done in [7].",
                "In fact, all the results of [7] about the properties of the auction and information privacy directly carry over to the scheduling domain discussed in this paper, requiring only slight modifications to deal with finitehorizon MDPs. • Known, deterministic arrival and departure times.",
                "Finally, we have assumed that agents arrival and departure times (τa m and τd m) are deterministic and known a priori.",
                "This assumption is fundamental to our solution method.",
                "While there are many domains where this assumption is valid, in many cases agents arrive and depart dynamically and their arrival and departure times can only be predicted probabilistically, leading to online resource-allocation problems.",
                "In particular, in the case of self-interested agents, this becomes an interesting version of an online-mechanism-design problem [11, 12].",
                "In summary, we have presented an MILP formulation for the combinatorial resource-scheduling problem where agents values for possible resource assignments are defined by finitehorizon MDPs.",
                "This result extends previous work ([6, 7]) on static one-shot resource allocation under MDP-induced preferences to resource-scheduling problems with a temporal aspect.",
                "As such, this work takes a step in the direction of designing an online mechanism for agents with combinatorial resource preferences induced by stochastic planning problems.",
                "Relaxing the assumption about deterministic arrival and departure times of the agents is a focus of our future work.",
                "We would like to thank the anonymous reviewers for their insightful comments and suggestions. 7.",
                "REFERENCES [1] E. Altman and A. Shwartz.",
                "Adaptive control of constrained Markov chains: Criteria and policies.",
                "Annals of Operations Research, special issue on Markov Decision Processes, 28:101-134, 1991. [2] R. Bellman.",
                "Dynamic Programming.",
                "Princeton University Press, 1957. [3] C. Boutilier.",
                "Solving concisely expressed combinatorial auction problems.",
                "In Proc. of AAAI-02, pages 359-366, 2002. [4] C. Boutilier and H. H. Hoos.",
                "Bidding languages for combinatorial auctions.",
                "In Proc. of IJCAI-01, pages 1211-1217, 2001. [5] D. Dolgov.",
                "Integrated Resource Allocation and Planning in Stochastic Multiagent Environments.",
                "PhD thesis, Computer Science Department, University of Michigan, February 2006. [6] D. A. Dolgov and E. H. Durfee.",
                "Optimal resource allocation and policy formulation in loosely-coupled Markov decision processes.",
                "In Proc. of ICAPS-04, pages 315-324, June 2004. [7] D. A. Dolgov and E. H. Durfee.",
                "Computationally efficient combinatorial auctions for resource allocation in weakly-coupled MDPs.",
                "In Proc. of AAMAS-05, New York, NY, USA, 2005.",
                "ACM Press. [8] D. A. Dolgov and E. H. Durfee.",
                "Resource allocation among agents with preferences induced by factored MDPs.",
                "In Proc. of AAMAS-06, 2006. [9] K. Larson and T. Sandholm.",
                "Mechanism design and deliberative agents.",
                "In Proc. of AAMAS-05, pages 650-656, New York, NY, USA, 2005.",
                "ACM Press. [10] N. Nisan.",
                "Bidding and allocation in combinatorial auctions.",
                "In Electronic Commerce, 2000. [11] D. C. Parkes and S. Singh.",
                "An MDP-based approach to Online Mechanism Design.",
                "In Proc. of the Seventeenths Annual Conference on Neural Information Processing Systems (NIPS-03), 2003. [12] D. C. Parkes, S. Singh, and D. Yanovsky.",
                "Approximately efficient online mechanism design.",
                "In Proc. of the Eighteenths Annual Conference on Neural Information Processing Systems (NIPS-04), 2004. [13] M. L. Puterman.",
                "Markov Decision Processes.",
                "John Wiley & Sons, New York, 1994. [14] M. H. Rothkopf, A. Pekec, and R. M. Harstad.",
                "Computationally manageable combinational auctions.",
                "Management Science, 44(8):1131-1147, 1998. [15] T. Sandholm.",
                "An algorithm for optimal winner determination in combinatorial auctions.",
                "In Proc. of IJCAI-99, pages 542-547, San Francisco, CA, USA, 1999.",
                "Morgan Kaufmann Publishers Inc.",
                "The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) 1227"
            ],
            "original_annotated_samples": [],
            "translated_annotated_samples": [],
            "translated_text": "",
            "candidates": [],
            "error": []
        },
        "resource-scheduling": {
            "translated_key": "",
            "is_in_text": true,
            "original_annotated_sentences": [
                "Combinatorial Resource Scheduling for Multiagent MDPs Dmitri A. Dolgov, Michael R. James, and Michael E. Samples AI and Robotics Group Technical Research, Toyota Technical Center USA {ddolgov, michael.r.james, michael.samples}@gmail.com ABSTRACT Optimal resource scheduling in multiagent systems is a computationally challenging task, particularly when the values of resources are not additive.",
                "We consider the combinatorial problem of scheduling the usage of multiple resources among agents that operate in stochastic environments, modeled as Markov decision processes (MDPs).",
                "In recent years, efficient resource-allocation algorithms have been developed for agents with resource values induced by MDPs.",
                "However, this prior work has focused on static resource-allocation problems where resources are distributed once and then utilized in infinite-horizon MDPs.",
                "We extend those existing models to the problem of combinatorial resource scheduling, where agents persist only for finite periods between their (predefined) arrival and departure times, requiring resources only for those time periods.",
                "We provide a computationally efficient procedure for computing globally optimal resource assignments to agents over time.",
                "We illustrate and empirically analyze the method in the context of a stochastic jobscheduling domain.",
                "Categories and Subject Descriptors I.2.8 [Artificial Intelligence]: Problem Solving, Control Methods, and Search; I.2.11 [Artificial Intelligence]: Distributed Artificial Intelligence-Multiagent systems General Terms Algorithms, Performance, Design 1.",
                "INTRODUCTION The tasks of optimal resource allocation and scheduling are ubiquitous in multiagent systems, but solving such optimization problems can be computationally difficult, due to a number of factors.",
                "In particular, when the value of a set of resources to an agent is not additive (as is often the case with resources that are substitutes or complements), the utility function might have to be defined on an exponentially large space of resource bundles, which very quickly becomes computationally intractable.",
                "Further, even when each agent has a utility function that is nonzero only on a small subset of the possible resource bundles, obtaining optimal allocation is still computationally prohibitive, as the problem becomes NP-complete [14].",
                "Such computational issues have recently spawned several threads of work in using compact models of agents preferences.",
                "One idea is to use any structure present in utility functions to represent them compactly, via, for example, logical formulas [15, 10, 4, 3].",
                "An alternative is to directly model the mechanisms that define the agents utility functions and perform resource allocation directly with these models [9].",
                "A way of accomplishing this is to model the processes by which an agent might utilize the resources and define the utility function as the payoff of these processes.",
                "In particular, if an agent uses resources to act in a stochastic environment, its utility function can be naturally modeled with a Markov decision process, whose action set is parameterized by the available resources.",
                "This representation can then be used to construct very efficient resource-allocation algorithms that lead to an exponential speedup over a straightforward optimization problem with flat representations of combinatorial preferences [6, 7, 8].",
                "However, this existing work on resource allocation with preferences induced by resource-parameterized MDPs makes an assumption that the resources are only allocated once and are then utilized by the agents independently within their infinite-horizon MDPs.",
                "This assumption that no reallocation of resources is possible can be limiting in domains where agents arrive and depart dynamically.",
                "In this paper, we extend the work on resource allocation under MDP-induced preferences to discrete-time scheduling problems, where agents are present in the system for finite time intervals and can only use resources within these intervals.",
                "In particular, agents arrive and depart at arbitrary (predefined) times and within these intervals use resources to execute tasks in finite-horizon MDPs.",
                "We address the problem of globally optimal resource scheduling, where the objective is to find an allocation of resources to the agents across time that maximizes the sum of the expected rewards that they obtain.",
                "In this context, our main contribution is a mixed-integerprogramming formulation of the scheduling problem that chooses globally optimal resource assignments, starting times, and execution horizons for all agents (within their arrival1220 978-81-904262-7-5 (RPS) c 2007 IFAAMAS departure intervals).",
                "We analyze and empirically compare two flavors of the scheduling problem: one, where agents have static resource assignments within their finite-horizon MDPs, and another, where resources can be dynamically reallocated between agents at every time step.",
                "In the rest of the paper, we first lay down the necessary groundwork in Section 2 and then introduce our model and formal problem statement in Section 3.",
                "In Section 4.2, we describe our main result, the optimization program for globally optimal resource scheduling.",
                "Following the discussion of our experimental results on a job-scheduling problem in Section 5, we conclude in Section 6 with a discussion of possible extensions and generalizations of our method. 2.",
                "BACKGROUND Similarly to the model used in previous work on resourceallocation with MDP-induced preferences [6, 7], we define the value of a set of resources to an agent as the value of the best MDP policy that is realizable, given those resources.",
                "However, since the focus of our work is on scheduling problems, and a large part of the optimization problem is to decide how resources are allocated in time among agents with finite arrival and departure times, we model the agents planning problems as finite-horizon MDPs, in contrast to previous work that used infinite-horizon discounted MDPs.",
                "In the rest of this section, we first introduce some necessary background on finite-horizon MDPs and present a linear-programming formulation that serves as the basis for our solution algorithm developed in Section 4.",
                "We also outline the standard methods for combinatorial resource scheduling with flat resource values, which serve as a comparison benchmark for the new model developed here. 2.1 Markov Decision Processes A stationary, finite-domain, discrete-time MDP (see, for example, [13] for a thorough and detailed development) can be described as S, A, p, r , where: S is a finite set of system states; A is a finite set of actions that are available to the agent; p is a stationary stochastic transition function, where p(σ|s, a) is the probability of transitioning to state σ upon executing action a in state s; r is a stationary reward function, where r(s, a) specifies the reward obtained upon executing action a in state s. Given such an MDP, a decision problem under a finite horizon T is to choose an optimal action at every time step to maximize the expected value of the total reward accrued during the agents (finite) lifetime.",
                "The agents optimal policy is then a function of current state s and the time until the horizon.",
                "An optimal policy for such a problem is to act greedily with respect to the optimal value function, defined recursively by the following system of finite-time Bellman equations [2]: v(s, t) = max a r(s, a) + X σ p(σ|s, a)v(σ, t + 1), ∀s ∈ S, t ∈ [1, T − 1]; v(s, T) = 0, ∀s ∈ S; where v(s, t) is the optimal value of being in state s at time t ∈ [1, T].",
                "This optimal value function can be easily computed using dynamic programming, leading to the following optimal policy π, where π(s, a, t) is the probability of executing action a in state s at time t: π(s, a, t) = ( 1, a = argmaxa r(s, a) + P σ p(σ|s, a)v(σ, t + 1), 0, otherwise.",
                "The above is the most common way of computing the optimal value function (and therefore an optimal policy) for a finite-horizon MDP.",
                "However, we can also formulate the problem as the following linear program (similarly to the dual LP for infinite-horizon discounted MDPs [13, 6, 7]): max X s X a r(s, a) X t x(s, a, t) subject to: X a x(σ, a, t + 1) = X s,a p(σ|s, a)x(s, a, t) ∀σ, t ∈ [1, T − 1]; X a x(s, a, 1) = α(s), ∀s ∈ S; (1) where α(s) is the initial distribution over the state space, and x is the (non-stationary) occupation measure (x(s, a, t) ∈ [0, 1] is the total expected number of times action a is executed in state s at time t).",
                "An optimal (non-stationary) policy is obtained from the occupation measure as follows: π(s, a, t) = x(s, a, t)/ X a x(s, a, t) ∀s ∈ S, t ∈ [1, T]. (2) Note that the standard unconstrained finite-horizon MDP, as described above, always has a uniformly-optimal solution (optimal for any initial distribution α(s)).",
                "Therefore, an optimal policy can be obtained by using an arbitrary constant α(s) > 0 (in particular, α(s) = 1 will result in x(s, a, t) = π(s, a, t)).",
                "However, for MDPs with resource constraints (as defined below in Section 3), uniformly-optimal policies do not in general exist.",
                "In such cases, α becomes a part of the problem input, and a resulting policy is only optimal for that particular α.",
                "This result is well known for infinite-horizon MDPs with various types of constraints [1, 6], and it also holds for our finite-horizon model, which can be easily established via a line of reasoning completely analogous to the arguments in [6]. 2.2 Combinatorial Resource Scheduling A straightforward approach to resource scheduling for a set of agents M, whose values for the resources are induced by stochastic planning problems (in our case, finite-horizon MDPs) would be to have each agent enumerate all possible resource assignments over time and, for each one, compute its value by solving the corresponding MDP.",
                "Then, each agent would provide valuations for each possible resource bundle over time to a centralized coordinator, who would compute the optimal resource assignments across time based on these valuations.",
                "When resources can be allocated at different times to different agents, each agent must submit valuations for every combination of possible time horizons.",
                "Let each agent m ∈ M execute its MDP within the arrival-departure time interval τ ∈ [τa m, τd m].",
                "Hence, agent m will execute an MDP with time horizon no greater than Tm = τd m−τa m+1.",
                "Let bτ be the global time horizon for the problem, before which all of the agents MDPs must finish.",
                "We assume τd m < bτ, ∀m ∈ M. The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) 1221 For the scheduling problem where agents have static resource requirements within their finite-horizon MDPs, the agents provide a valuation for each resource bundle for each possible time horizon (from [1, Tm]) that they may use.",
                "Let Ω be the set of resources to be allocated among the agents.",
                "An agent will get at most one resource bundle for one of the time horizons.",
                "Let the variable ψ ∈ Ψm enumerate all possible pairs of resource bundles and time horizons for agent m, so there are 2|Ω| × Tm values for ψ (the space of bundles is exponential in the number of resource types |Ω|).",
                "The agent m must provide a value vψ m for each ψ, and the coordinator will allocate at most one ψ (resource, time horizon) pair to each agent.",
                "This allocation is expressed as an indicator variable zψ m ∈ {0, 1} that shows whether ψ is assigned to agent m. For time τ and resource ω, the function nm(ψ, τ, ω) ∈ {0, 1} indicates whether the bundle in ψ uses resource ω at time τ (we make the assumption that agents have binary resource requirements).",
                "This allocation problem is NP-complete, even when considering only a single time step, and its difficulty increases significantly with multiple time steps because of the increasing number of values of ψ.",
                "The problem of finding an optimal allocation that satisfies the global constraint that the amount of each resource ω allocated to all agents does not exceed the available amount bϕ(ω) can be expressed as the following integer program: max X m∈M X ψ∈Ψm zψ mvψ m subject to: X ψ∈Ψm zψ m ≤ 1, ∀m ∈ M; X m∈M X ψ∈Ψm zψ mnm(ψ, τ, ω) ≤ bϕ(ω), ∀τ ∈ [1, bτ], ∀ω ∈ Ω; (3) The first constraint in equation 3 says that no agent can receive more than one bundle, and the second constraint ensures that the total assignment of resource ω does not, at any time, exceed the resource bound.",
                "For the scheduling problem where the agents are able to dynamically reallocate resources, each agent must specify a value for every combination of bundles and time steps within its time horizon.",
                "Let the variable ψ ∈ Ψm in this case enumerate all possible resource bundles for which at most one bundle may be assigned to agent m at each time step.",
                "Therefore, in this case there are P t∈[1,Tm](2|Ω| )t ∼ 2|Ω|Tm possibilities of resource bundles assigned to different time slots, for the Tm different time horizons.",
                "The same set of equations (3) can be used to solve this dynamic scheduling problem, but the integer program is different because of the difference in how ψ is defined.",
                "In this case, the number of ψ values is exponential in each agents planning horizon Tm, resulting in a much larger program.",
                "This straightforward approach to solving both of these scheduling problems requires an enumeration and solution of either 2|Ω| Tm (static allocation) or P t∈[1,Tm] 2|Ω|t (dynamic reallocation) MDPs for each agent, which very quickly becomes intractable with the growth of the number of resources |Ω| or the time horizon Tm. 3.",
                "MODEL AND PROBLEM STATEMENT We now formally introduce our model of the resourcescheduling problem.",
                "The problem input consists of the following components: • M, Ω, bϕ, τa m, τd m, bτ are as defined above in Section 2.2. • {Θm} = {S, A, pm, rm, αm} are the MDPs of all agents m ∈ M. Without loss of generality, we assume that state and action spaces of all agents are the same, but each has its own transition function pm, reward function rm, and initial conditions αm. • ϕm : A×Ω → {0, 1} is the mapping of actions to resources for agent m. ϕm(a, ω) indicates whether action a of agent m needs resource ω.",
                "An agent m that receives a set of resources that does not include resource ω cannot execute in its MDP policy any action a for which ϕm(a, ω) = 0.",
                "We assume all resource requirements are binary; as discussed below in Section 6, this assumption is not limiting.",
                "Given the above input, the optimization problem we consider is to find the globally optimal-maximizing the sum of expected rewards-mapping of resources to agents for all time steps: Δ : τ × M × Ω → {0, 1}.",
                "A solution is feasible if the corresponding assignment of resources to the agents does not violate the global resource constraint: X m Δm(τ, ω) ≤ bϕ(ω), ∀ω ∈ Ω, τ ∈ [1, bτ]. (4) We consider two flavors of the <br>resource-scheduling</br> problem.",
                "The first formulation restricts resource assignments to the space where the allocation of resources to each agent is static during the agents lifetime.",
                "The second formulation allows reassignment of resources between agents at every time step within their lifetimes.",
                "Figure 1 depicts a <br>resource-scheduling</br> problem with three agents M = {m1, m2, m3}, three resources Ω = {ω1, ω2, ω3}, and a global problem horizon of bτ = 11.",
                "The agents arrival and departure times are shown as gray boxes and are {1, 6}, {3, 7}, and {2, 11}, respectively.",
                "A solution to this problem is shown via horizontal bars within each agents box, where the bars correspond to the allocation of the three resource types.",
                "Figure 1a shows a solution to a static scheduling problem.",
                "According to the shown solution, agent m1 begins the execution of its MDP at time τ = 1 and has a lock on all three resources until it finishes execution at time τ = 3.",
                "Note that agent m1 relinquishes its hold on the resources before its announced departure time of τd m1 = 6, ostensibly because other agents can utilize the resources more effectively.",
                "Thus, at time τ = 4, resources ω1 and ω3 are allocated to agent m2, who then uses them to execute its MDP (using only actions supported by resources ω1 and ω3) until time τ = 7.",
                "Agent m3 holds resource ω3 during the interval τ ∈ [4, 10].",
                "Figure 1b shows a possible solution to the dynamic version of the same problem.",
                "There, resources can be reallocated between agents at every time step.",
                "For example, agent m1 gives up its use of resource ω2 at time τ = 2, although it continues the execution of its MDP until time τ = 6.",
                "Notice that an agent is not allowed to stop and restart its MDP, so agent m1 is only able to continue executing in the interval τ ∈ [3, 4] if it has actions that do not require any resources (ϕm(a, ω) = 0).",
                "Clearly, the model and problem statement described above make a number of assumptions about the problem and the desired solution properties.",
                "We discuss some of those assumptions and their implications in Section 6. 1222 The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) (a) (b) Figure 1: Illustration of a solution to a <br>resource-scheduling</br> problem with three agents and three resources: a) static resource assignments (resource assignments are constant within agents lifetimes; b) dynamic assignment (resource assignments are allowed to change at every time step). 4.",
                "RESOURCE SCHEDULING Our <br>resource-scheduling</br> algorithm proceeds in two stages.",
                "First, we perform a preprocessing step that augments the agent MDPs; this process is described in Section 4.1.",
                "Second, using these augmented MDPs we construct a global optimization problem, which is described in Section 4.2. 4.1 Augmenting Agents MDPs In the model described in the previous section, we assume that if an agent does not possess the necessary resources to perform actions in its MDP, its execution is halted and the agent leaves the system.",
                "In other words, the MDPs cannot be paused and resumed.",
                "For example, in the problem shown in Figure 1a, agent m1 releases all resources after time τ = 3, at which point the execution of its MDP is halted.",
                "Similarly, agents m2 and m3 only execute their MDPs in the intervals τ ∈ [4, 6] and τ ∈ [4, 10], respectively.",
                "Therefore, an important part of the global decision-making problem is to decide the window of time during which each of the agents is active (i.e., executing its MDP).",
                "To accomplish this, we augment each agents MDP with two new states (start and finish states sb , sf , respectively) and a new start/stop action a∗ , as illustrated in Figure 2.",
                "The idea is that an agent stays in the start state sb until it is ready to execute its MDP, at which point it performs the start/stop action a∗ and transitions into the state space of the original MDP with the transition probability that corresponds to the original initial distribution α(s).",
                "For example, in Figure 1a, for agent m2 this would happen at time τ = 4.",
                "Once the agent gets to the end of its activity window (time τ = 6 for agent m2 in Figure 1a), it performs the start/stop action, which takes it into the sink finish state sf at time τ = 7.",
                "More precisely, given an MDP S, A, pm, rm, αm , we define an augmented MDP S , A , pm, rm, αm as follows: S = S ∪ sb ∪ sf ; A = A ∪ a∗ ; p (s|sb , a∗ ) = α(s), ∀s ∈ S; p (sb |sb , a) = 1.0, ∀a ∈ A; p (sf |s, a∗ ) = 1.0, ∀s ∈ S; p (σ|s, a) = p(σ|s, a), ∀s, σ ∈ S, a ∈ A; r (sb , a) = r (sf , a) = 0, ∀a ∈ A ; r (s, a) = r(s, a), ∀s ∈ S, a ∈ A; α (sb ) = 1; α (s) = 0, ∀s ∈ S; where all non-specified transition probabilities are assumed to be zero.",
                "Further, in order to account for the new starting state, we begin the MDP one time-step earlier, setting τa m ← τa m − 1.",
                "This will not affect the resource allocation due to the resource constraints only being enforced for the original MDP states, as will be discussed in the next section.",
                "For example, the augmented MDPs shown in Figure 2b (which starts in state sb at time τ = 2) would be constructed from an MDP with original arrival time τ = 3.",
                "Figure 2b also shows a sample trajectory through the state space: the agent starts in state sb , transitions into the state space S of the original MDP, and finally exists into the sink state sf .",
                "Note that if we wanted to model a problem where agents could pause their MDPs at arbitrary time steps (which might be useful for domains where dynamic reallocation is possible), we could easily accomplish this by including an extra action that transitions from each state to itself with zero reward. 4.2 MILP for Resource Scheduling Given a set of augmented MDPs, as defined above, the goal of this section is to formulate a global optimization program that solves the <br>resource-scheduling</br> problem.",
                "In this section and below, all MDPs are assumed to be the augmented MDPs as defined in Section 4.1.",
                "Our approach is similar to the idea used in [6]: we begin with the linear-program formulation of agents MDPs (1) and augment it with constraints that ensure that the corresponding resource allocation across agents and time is valid.",
                "The resulting optimization problem then simultaneously solves the agents MDPs and <br>resource-scheduling</br> problems.",
                "In the rest of this section, we incrementally develop a mixed integer program (MILP) that achieves this.",
                "In the absence of resource constraints, the agents finitehorizon MDPs are completely independent, and the globally optimal solution can be trivially obtained via the following LP, which is simply an aggregation of single-agent finitehorizon LPs: max X m X s X a rm(s, a) X t xm(s, a, t) subject to: X a xm(σ, a, t + 1) = X s,a pm(σ|s, a)xm(s, a, t), ∀m ∈ M, σ ∈ S, t ∈ [1, Tm − 1]; X a xm(s, a, 1) = αm(s), ∀m ∈ M, s ∈ S; (12) where xm(s, a, t) is the occupation measure of agent m, and The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) 1223 (a) (b) Figure 2: Illustration of augmenting an MDP to allow for variable starting and stopping times: a) (left) the original two-state MDP with a single action; (right) the augmented MDP with new states sb and sf and the new action a∗ (note that the origianl transitions are not changed in the augmentation process); b) the augmented MDP displayed as a trajectory through time (grey lines indicate all transitions, while black lines indicate a given trajectory.",
                "Objective Function (sum of expected rewards over all agents) max X m X s X a rm(s, a) X t xm(s, a, t) (5) Meaning Implication Linear Constraints Tie x to θ.",
                "Agent is only active when occupation measure is nonzero in original MDP states. θm(τ) = 0 =⇒ xm(s, a, τ −τa m+1) = 0 ∀s /∈ {sb , sf }, a ∈ A X s/∈{sb,sf } X a xm(s, a, t) ≤ θm(τa m + t − 1) ∀m ∈ M, ∀t ∈ [1, Tm] (6) Agent can only be active in τ ∈ (τa m, τd m) θm(τ) = 0 ∀m ∈ M, τ /∈ (τa m, τd m) (7) Cannot use resources when not active θm(τ) = 0 =⇒ Δm(τ, ω) = 0 ∀τ ∈ [0, bτ], ω ∈ Ω Δm(τ, ω) ≤ θm(τ) ∀m ∈ M, τ ∈ [0, bτ], ω ∈ Ω (8) Tie x to Δ (nonzero x forces corresponding Δ to be nonzero.)",
                "Δm(τ, ω) = 0, ϕm(a, ω) = 1 =⇒ xm(s, a, τ − τa m + 1) = 0 ∀s /∈ {sb , sf } 1/|A| X a ϕm(a, ω) X s/∈{sb,sf } xm(s, a, t) ≤ Δm(t + τa m − 1, ω) ∀m ∈ M, ω ∈ Ω, t ∈ [1, Tm] (9) Resource bounds X m Δm(τ, ω) ≤ bϕ(ω) ∀ω ∈ Ω, τ ∈ [0, bτ] (10) Agent cannot change resources while active.",
                "Only enabled for scheduling with static assignments. θm(τ) = 1 and θm(τ + 1) = 1 =⇒ Δm(τ, ω) = Δm(τ + 1, ω) Δm(τ, ω) − Z(1 − θm(τ + 1)) ≤ Δm(τ + 1, ω) + Z(1 − θm(τ)) Δm(τ, ω) + Z(1 − θm(τ + 1)) ≥ Δm(τ + 1, ω) − Z(1 − θm(τ)) ∀m ∈ M, ω ∈ Ω, τ ∈ [0, bτ] (11) Table 1: MILP for globally optimal resource scheduling.",
                "Tm = τd m − τa m + 1 is the time horizon for the agents MDP.",
                "Using this LP as a basis, we augment it with constraints that ensure that the resource usage implied by the agents occupation measures {xm} does not violate the global resource requirements bϕ at any time step τ ∈ [0, bτ].",
                "To formulate these resource constraints, we use the following binary variables: • Δm(τ, ω) = {0, 1}, ∀m ∈ M, τ ∈ [0, bτ], ω ∈ Ω, which serve as indicator variables that define whether agent m possesses resource ω at time τ.",
                "These are analogous to the static indicator variables used in the one-shot static resource-allocation problem in [6]. • θm = {0, 1}, ∀m ∈ M, τ ∈ [0, bτ] are indicator variables that specify whether agent m is active (i.e., executing its MDP) at time τ.",
                "The meaning of resource-usage variables Δ is illustrated in Figure 1: Δm(τ, ω) = 1 only if resource ω is allocated to agent m at time τ.",
                "The meaning of the activity indicators θ is illustrated in Figure 2b: when agent m is in either the start state sb or the finish state sf , the corresponding θm = 0, but once the agent becomes active and enters one of the other states, we set θm = 1 .",
                "This meaning of θ can be enforced with a linear constraint that synchronizes the values of the agents occupation measures xm and the activity 1224 The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) indicators θ, as shown in (6) in Table 1.",
                "Another constraint we have to add-because the activity indicators θ are defined on the global timeline τ-is to enforce the fact that the agent is inactive outside of its arrivaldeparture window.",
                "This is accomplished by constraint (7) in Table 1.",
                "Furthermore, agents should not be using resources while they are inactive.",
                "This constraint can also be enforced via a linear inequality on θ and Δ, as shown in (8).",
                "Constraint (6) sets the value of θ to match the policy defined by the occupation measure xm.",
                "In a similar fashion, we have to make sure that the resource-usage variables Δ are also synchronized with the occupation measure xm.",
                "This is done via constraint (9) in Table 1, which is nearly identical to the analogous constraint from [6].",
                "After implementing the above constraint, which enforces the meaning of Δ, we add a constraint that ensures that the agents resource usage never exceeds the amounts of available resources.",
                "This condition is also trivially expressed as a linear inequality (10) in Table 1.",
                "Finally, for the problem formulation where resource assignments are static during a lifetime of an agent, we add a constraint that ensures that the resource-usage variables Δ do not change their value while the agent is active (θ = 1).",
                "This is accomplished via the linear constraint (11), where Z ≥ 2 is a constant that is used to turn off the constraints when θm(τ) = 0 or θm(τ + 1) = 0.",
                "This constraint is not used for the dynamic problem formulation, where resources can be reallocated between agents at every time step.",
                "To summarize, Table 1 together with the conservationof-flow constraints from (12) defines the MILP that simultaneously computes an optimal resource assignment for all agents across time as well as optimal finite-horizon MDP policies that are valid under that resource assignment.",
                "As a rough measure of the complexity of this MILP, let us consider the number of optimization variables and constraints.",
                "Let TM = P Tm = P m(τa m − τd m + 1) be the sum of the lengths of the arrival-departure windows across all agents.",
                "Then, the number of optimization variables is: TM + bτ|M||Ω| + bτ|M|, TM of which are continuous (xm), and bτ|M||Ω| + bτ|M| are binary (Δ and θ).",
                "However, notice that all but TM|M| of the θ are set to zero by constraint (7), which also immediately forces all but TM|M||Ω| of the Δ to be zero via the constraints (8).",
                "The number of constraints (not including the degenerate constraints in (7)) in the MILP is: TM + TM|Ω| + bτ|Ω| + bτ|M||Ω|.",
                "Despite the fact that the complexity of the MILP is, in the worst case, exponential1 in the number of binary variables, the complexity of this MILP is significantly (exponentially) lower than that of the MILP with flat utility functions, described in Section 2.2.",
                "This result echos the efficiency gains reported in [6] for single-shot resource-allocation problems, but is much more pronounced, because of the explosion of the flat utility representation due to the temporal aspect of the problem (recall the prohibitive complexity of the combinatorial optimization in Section 2.2).",
                "We empirically analyze the performance of this method in Section 5. 1 Strictly speaking, solving MILPs to optimality is NPcomplete in the number of integer variables. 5.",
                "EXPERIMENTAL RESULTS Although the complexity of solving MILPs is in the worst case exponential in the number of integer variables, there are many efficient methods for solving MILPs that allow our algorithm to scale well for parameters common to resource allocation and scheduling problems.",
                "In particular, this section introduces a problem domain-the repairshop problem-used to empirically evaluate our algorithms scalability in terms of the number of agents |M|, the number of shared resources |Ω|, and the varied lengths of global time bτ during which agents may enter and exit the system.",
                "The repairshop problem is a simple parameterized MDP adopting the metaphor of a vehicular repair shop.",
                "Agents in the repair shop are mechanics with a number of independent tasks that yield reward only when completed.",
                "In our MDP model of this system, actions taken to advance through the state space are only allowed if the agent holds certain resources that are publicly available to the shop.",
                "These resources are in finite supply, and optimal policies for the shop will determine when each agent may hold the limited resources to take actions and earn individual rewards.",
                "Each task to be completed is associated with a single action, although the agent is required to repeat the action numerous times before completing the task and earning a reward.",
                "This model was parameterized in terms of the number of agents in the system, the number of different types of resources that could be linked to necessary actions, a global time during which agents are allowed to arrive and depart, and a maximum length for the number of time steps an agent may remain in the system.",
                "All datapoints in our experiments were obtained with 20 evaluations using CPLEX to solve the MILPs on a Pentium4 computer with 2Gb of RAM.",
                "Trials were conducted on both the static and the dynamic version of the resourcescheduling problem, as defined earlier.",
                "Figure 3 shows the runtime and policy value for independent modifications to the parameter set.",
                "The top row shows how the solution time for the MILP scales as we increase the number of agents |M|, the global time horizon bτ, and the number of resources |Ω|.",
                "Increasing the number of agents leads to exponential complexity scaling, which is to be expected for an NP-complete problem.",
                "However, increasing the global time limit bτ or the total number of resource types |Ω|-while holding the number of agents constantdoes not lead to decreased performance.",
                "This occurs because the problems get easier as they become under-constrained, which is also a common phenomenon for NP-complete problems.",
                "We also observe that the solution to the dynamic version of the problem can often be computed much faster than the static version.",
                "The bottom row of Figure 3 shows the joint policy value of the policies that correspond to the computed optimal resource-allocation schedules.",
                "We can observe that the dynamic version yields higher reward (as expected, since the reward for the dynamic version is always no less than the reward of the static version).",
                "We should point out that these graphs should not be viewed as a measure of performance of two different algorithms (both algorithms produce optimal solutions but to different problems), but rather as observations about how the quality of optimal solutions change as more flexibility is allowed in the reallocation of resources.",
                "Figure 4 shows runtime and policy value for trials in which common input variables are scaled together.",
                "This allows The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) 1225 2 4 6 8 10 10 −3 10 −2 10 −1 10 0 10 1 10 2 10 3 10 4 Number of Agents |M| CPUTime,sec |Ω| = 5, τ = 50 static dynamic 50 100 150 200 10 −2 10 −1 10 0 10 1 10 2 10 3 Global Time Boundary τ CPUTime,sec |M| = 5, |Ω| = 5 static dynamic 10 20 30 40 50 10 −2 10 −1 10 0 10 1 10 2 Number of Resources |Ω| CPUTime,sec |M| = 5, τ = 50 static dynamic 2 4 6 8 10 200 400 600 800 1000 1200 1400 1600 Number of Agents |M| Value |Ω| = 5, τ = 50 static dynamic 50 100 150 200 400 500 600 700 800 900 1000 1100 1200 1300 1400 Global Time Boundary τ Value |M| = 5, |Ω| = 5 static dynamic 10 20 30 40 50 500 600 700 800 900 1000 1100 1200 1300 1400 Number of Resources |Ω| Value |M| = 5, τ = 50 static dynamic Figure 3: Evaluation of our MILP for variable numbers of agents (column 1), lengths of global-time window (column 2), and numbers of resource types (column 3).",
                "Top row shows CPU time, and bottom row shows the joint reward of agents MDP policies.",
                "Error bars show the 1st and 3rd quartiles (25% and 75%). 2 4 6 8 10 10 −3 10 −2 10 −1 10 0 10 1 10 2 10 3 Number of Agents |M| CPUTime,sec τ = 10|M| static dynamic 2 4 6 8 10 10 −3 10 −2 10 −1 10 0 10 1 10 2 10 3 10 4 Number of Agents |M| CPUTime,sec |Ω| = 2|M| static dynamic 2 4 6 8 10 10 −3 10 −2 10 −1 10 0 10 1 10 2 10 3 10 4 Number of Agents |M| CPUTime,sec |Ω| = 5|M| static dynamic 2 4 6 8 10 200 400 600 800 1000 1200 1400 1600 1800 2000 2200 Number of Agents |M| Value τ = 10|M| static dynamic 2 4 6 8 10 200 400 600 800 1000 1200 1400 1600 1800 2000 Number of Agents |M| Value |Ω| = 2|M| static dynamic 2 4 6 8 10 0 500 1000 1500 2000 2500 Number of Agents |M| Value |Ω| = 5|M| static dynamic Figure 4: Evaluation of our MILP using correlated input variables.",
                "The left column tracks the performance and CPU time as the number of agents and global-time window increase together (bτ = 10|M|).",
                "The middle and the right column track the performance and CPU time as the number of resources and the number of agents increase together as |Ω| = 2|M| and |Ω| = 5|M|, respectively.",
                "Error bars show the 1st and 3rd quartiles (25% and 75%). 1226 The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) us to explore domains where the total number of agents scales proportionally to the total number of resource types or the global time horizon, while keeping constant the average agent density (per unit of global time) or the average number of resources per agent (which commonly occurs in real-life applications).",
                "Overall, we believe that these experimental results indicate that our MILP formulation can be used to effectively solve <br>resource-scheduling</br> problems of nontrivial size. 6.",
                "DISCUSSION AND CONCLUSIONS Throughout the paper, we have made a number of assumptions in our model and solution algorithm; we discuss their implications below. • Continual execution.",
                "We assume that once an agent stops executing its MDP (transitions into state sf ), it exits the system and cannot return.",
                "It is easy to relax this assumption for domains where agents MDPs can be paused and restarted.",
                "All that is required is to include an additional pause action which transitions from a given state back to itself, and has zero reward. • Indifference to start time.",
                "We used a reward model where agents rewards depend only on the time horizon of their MDPs and not the global start time.",
                "This is a consequence of our MDP-augmentation procedure from Section 4.1.",
                "It is easy to extend the model so that the agents incur an explicit penalty for idling by assigning a non-zero negative reward to the start state sb . • Binary resource requirements.",
                "For simplicity, we have assumed that resource costs are binary: ϕm(a, ω) = {0, 1}, but our results generalize in a straightforward manner to non-binary resource mappings, analogously to the procedure used in [5]. • Cooperative agents.",
                "The optimization procedure discussed in this paper was developed in the context of cooperative agents, but it can also be used to design a mechanism for scheduling resources among self-interested agents.",
                "This optimization procedure can be embedded in a VickreyClarke-Groves auction, completely analogously to the way it was done in [7].",
                "In fact, all the results of [7] about the properties of the auction and information privacy directly carry over to the scheduling domain discussed in this paper, requiring only slight modifications to deal with finitehorizon MDPs. • Known, deterministic arrival and departure times.",
                "Finally, we have assumed that agents arrival and departure times (τa m and τd m) are deterministic and known a priori.",
                "This assumption is fundamental to our solution method.",
                "While there are many domains where this assumption is valid, in many cases agents arrive and depart dynamically and their arrival and departure times can only be predicted probabilistically, leading to online resource-allocation problems.",
                "In particular, in the case of self-interested agents, this becomes an interesting version of an online-mechanism-design problem [11, 12].",
                "In summary, we have presented an MILP formulation for the combinatorial <br>resource-scheduling</br> problem where agents values for possible resource assignments are defined by finitehorizon MDPs.",
                "This result extends previous work ([6, 7]) on static one-shot resource allocation under MDP-induced preferences to <br>resource-scheduling</br> problems with a temporal aspect.",
                "As such, this work takes a step in the direction of designing an online mechanism for agents with combinatorial resource preferences induced by stochastic planning problems.",
                "Relaxing the assumption about deterministic arrival and departure times of the agents is a focus of our future work.",
                "We would like to thank the anonymous reviewers for their insightful comments and suggestions. 7.",
                "REFERENCES [1] E. Altman and A. Shwartz.",
                "Adaptive control of constrained Markov chains: Criteria and policies.",
                "Annals of Operations Research, special issue on Markov Decision Processes, 28:101-134, 1991. [2] R. Bellman.",
                "Dynamic Programming.",
                "Princeton University Press, 1957. [3] C. Boutilier.",
                "Solving concisely expressed combinatorial auction problems.",
                "In Proc. of AAAI-02, pages 359-366, 2002. [4] C. Boutilier and H. H. Hoos.",
                "Bidding languages for combinatorial auctions.",
                "In Proc. of IJCAI-01, pages 1211-1217, 2001. [5] D. Dolgov.",
                "Integrated Resource Allocation and Planning in Stochastic Multiagent Environments.",
                "PhD thesis, Computer Science Department, University of Michigan, February 2006. [6] D. A. Dolgov and E. H. Durfee.",
                "Optimal resource allocation and policy formulation in loosely-coupled Markov decision processes.",
                "In Proc. of ICAPS-04, pages 315-324, June 2004. [7] D. A. Dolgov and E. H. Durfee.",
                "Computationally efficient combinatorial auctions for resource allocation in weakly-coupled MDPs.",
                "In Proc. of AAMAS-05, New York, NY, USA, 2005.",
                "ACM Press. [8] D. A. Dolgov and E. H. Durfee.",
                "Resource allocation among agents with preferences induced by factored MDPs.",
                "In Proc. of AAMAS-06, 2006. [9] K. Larson and T. Sandholm.",
                "Mechanism design and deliberative agents.",
                "In Proc. of AAMAS-05, pages 650-656, New York, NY, USA, 2005.",
                "ACM Press. [10] N. Nisan.",
                "Bidding and allocation in combinatorial auctions.",
                "In Electronic Commerce, 2000. [11] D. C. Parkes and S. Singh.",
                "An MDP-based approach to Online Mechanism Design.",
                "In Proc. of the Seventeenths Annual Conference on Neural Information Processing Systems (NIPS-03), 2003. [12] D. C. Parkes, S. Singh, and D. Yanovsky.",
                "Approximately efficient online mechanism design.",
                "In Proc. of the Eighteenths Annual Conference on Neural Information Processing Systems (NIPS-04), 2004. [13] M. L. Puterman.",
                "Markov Decision Processes.",
                "John Wiley & Sons, New York, 1994. [14] M. H. Rothkopf, A. Pekec, and R. M. Harstad.",
                "Computationally manageable combinational auctions.",
                "Management Science, 44(8):1131-1147, 1998. [15] T. Sandholm.",
                "An algorithm for optimal winner determination in combinatorial auctions.",
                "In Proc. of IJCAI-99, pages 542-547, San Francisco, CA, USA, 1999.",
                "Morgan Kaufmann Publishers Inc.",
                "The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) 1227"
            ],
            "original_annotated_samples": [],
            "translated_annotated_samples": [],
            "translated_text": "",
            "candidates": [],
            "error": []
        },
        "task and resource allocation in agent system": {
            "translated_key": "",
            "is_in_text": false,
            "original_annotated_sentences": [
                "Combinatorial Resource Scheduling for Multiagent MDPs Dmitri A. Dolgov, Michael R. James, and Michael E. Samples AI and Robotics Group Technical Research, Toyota Technical Center USA {ddolgov, michael.r.james, michael.samples}@gmail.com ABSTRACT Optimal resource scheduling in multiagent systems is a computationally challenging task, particularly when the values of resources are not additive.",
                "We consider the combinatorial problem of scheduling the usage of multiple resources among agents that operate in stochastic environments, modeled as Markov decision processes (MDPs).",
                "In recent years, efficient resource-allocation algorithms have been developed for agents with resource values induced by MDPs.",
                "However, this prior work has focused on static resource-allocation problems where resources are distributed once and then utilized in infinite-horizon MDPs.",
                "We extend those existing models to the problem of combinatorial resource scheduling, where agents persist only for finite periods between their (predefined) arrival and departure times, requiring resources only for those time periods.",
                "We provide a computationally efficient procedure for computing globally optimal resource assignments to agents over time.",
                "We illustrate and empirically analyze the method in the context of a stochastic jobscheduling domain.",
                "Categories and Subject Descriptors I.2.8 [Artificial Intelligence]: Problem Solving, Control Methods, and Search; I.2.11 [Artificial Intelligence]: Distributed Artificial Intelligence-Multiagent systems General Terms Algorithms, Performance, Design 1.",
                "INTRODUCTION The tasks of optimal resource allocation and scheduling are ubiquitous in multiagent systems, but solving such optimization problems can be computationally difficult, due to a number of factors.",
                "In particular, when the value of a set of resources to an agent is not additive (as is often the case with resources that are substitutes or complements), the utility function might have to be defined on an exponentially large space of resource bundles, which very quickly becomes computationally intractable.",
                "Further, even when each agent has a utility function that is nonzero only on a small subset of the possible resource bundles, obtaining optimal allocation is still computationally prohibitive, as the problem becomes NP-complete [14].",
                "Such computational issues have recently spawned several threads of work in using compact models of agents preferences.",
                "One idea is to use any structure present in utility functions to represent them compactly, via, for example, logical formulas [15, 10, 4, 3].",
                "An alternative is to directly model the mechanisms that define the agents utility functions and perform resource allocation directly with these models [9].",
                "A way of accomplishing this is to model the processes by which an agent might utilize the resources and define the utility function as the payoff of these processes.",
                "In particular, if an agent uses resources to act in a stochastic environment, its utility function can be naturally modeled with a Markov decision process, whose action set is parameterized by the available resources.",
                "This representation can then be used to construct very efficient resource-allocation algorithms that lead to an exponential speedup over a straightforward optimization problem with flat representations of combinatorial preferences [6, 7, 8].",
                "However, this existing work on resource allocation with preferences induced by resource-parameterized MDPs makes an assumption that the resources are only allocated once and are then utilized by the agents independently within their infinite-horizon MDPs.",
                "This assumption that no reallocation of resources is possible can be limiting in domains where agents arrive and depart dynamically.",
                "In this paper, we extend the work on resource allocation under MDP-induced preferences to discrete-time scheduling problems, where agents are present in the system for finite time intervals and can only use resources within these intervals.",
                "In particular, agents arrive and depart at arbitrary (predefined) times and within these intervals use resources to execute tasks in finite-horizon MDPs.",
                "We address the problem of globally optimal resource scheduling, where the objective is to find an allocation of resources to the agents across time that maximizes the sum of the expected rewards that they obtain.",
                "In this context, our main contribution is a mixed-integerprogramming formulation of the scheduling problem that chooses globally optimal resource assignments, starting times, and execution horizons for all agents (within their arrival1220 978-81-904262-7-5 (RPS) c 2007 IFAAMAS departure intervals).",
                "We analyze and empirically compare two flavors of the scheduling problem: one, where agents have static resource assignments within their finite-horizon MDPs, and another, where resources can be dynamically reallocated between agents at every time step.",
                "In the rest of the paper, we first lay down the necessary groundwork in Section 2 and then introduce our model and formal problem statement in Section 3.",
                "In Section 4.2, we describe our main result, the optimization program for globally optimal resource scheduling.",
                "Following the discussion of our experimental results on a job-scheduling problem in Section 5, we conclude in Section 6 with a discussion of possible extensions and generalizations of our method. 2.",
                "BACKGROUND Similarly to the model used in previous work on resourceallocation with MDP-induced preferences [6, 7], we define the value of a set of resources to an agent as the value of the best MDP policy that is realizable, given those resources.",
                "However, since the focus of our work is on scheduling problems, and a large part of the optimization problem is to decide how resources are allocated in time among agents with finite arrival and departure times, we model the agents planning problems as finite-horizon MDPs, in contrast to previous work that used infinite-horizon discounted MDPs.",
                "In the rest of this section, we first introduce some necessary background on finite-horizon MDPs and present a linear-programming formulation that serves as the basis for our solution algorithm developed in Section 4.",
                "We also outline the standard methods for combinatorial resource scheduling with flat resource values, which serve as a comparison benchmark for the new model developed here. 2.1 Markov Decision Processes A stationary, finite-domain, discrete-time MDP (see, for example, [13] for a thorough and detailed development) can be described as S, A, p, r , where: S is a finite set of system states; A is a finite set of actions that are available to the agent; p is a stationary stochastic transition function, where p(σ|s, a) is the probability of transitioning to state σ upon executing action a in state s; r is a stationary reward function, where r(s, a) specifies the reward obtained upon executing action a in state s. Given such an MDP, a decision problem under a finite horizon T is to choose an optimal action at every time step to maximize the expected value of the total reward accrued during the agents (finite) lifetime.",
                "The agents optimal policy is then a function of current state s and the time until the horizon.",
                "An optimal policy for such a problem is to act greedily with respect to the optimal value function, defined recursively by the following system of finite-time Bellman equations [2]: v(s, t) = max a r(s, a) + X σ p(σ|s, a)v(σ, t + 1), ∀s ∈ S, t ∈ [1, T − 1]; v(s, T) = 0, ∀s ∈ S; where v(s, t) is the optimal value of being in state s at time t ∈ [1, T].",
                "This optimal value function can be easily computed using dynamic programming, leading to the following optimal policy π, where π(s, a, t) is the probability of executing action a in state s at time t: π(s, a, t) = ( 1, a = argmaxa r(s, a) + P σ p(σ|s, a)v(σ, t + 1), 0, otherwise.",
                "The above is the most common way of computing the optimal value function (and therefore an optimal policy) for a finite-horizon MDP.",
                "However, we can also formulate the problem as the following linear program (similarly to the dual LP for infinite-horizon discounted MDPs [13, 6, 7]): max X s X a r(s, a) X t x(s, a, t) subject to: X a x(σ, a, t + 1) = X s,a p(σ|s, a)x(s, a, t) ∀σ, t ∈ [1, T − 1]; X a x(s, a, 1) = α(s), ∀s ∈ S; (1) where α(s) is the initial distribution over the state space, and x is the (non-stationary) occupation measure (x(s, a, t) ∈ [0, 1] is the total expected number of times action a is executed in state s at time t).",
                "An optimal (non-stationary) policy is obtained from the occupation measure as follows: π(s, a, t) = x(s, a, t)/ X a x(s, a, t) ∀s ∈ S, t ∈ [1, T]. (2) Note that the standard unconstrained finite-horizon MDP, as described above, always has a uniformly-optimal solution (optimal for any initial distribution α(s)).",
                "Therefore, an optimal policy can be obtained by using an arbitrary constant α(s) > 0 (in particular, α(s) = 1 will result in x(s, a, t) = π(s, a, t)).",
                "However, for MDPs with resource constraints (as defined below in Section 3), uniformly-optimal policies do not in general exist.",
                "In such cases, α becomes a part of the problem input, and a resulting policy is only optimal for that particular α.",
                "This result is well known for infinite-horizon MDPs with various types of constraints [1, 6], and it also holds for our finite-horizon model, which can be easily established via a line of reasoning completely analogous to the arguments in [6]. 2.2 Combinatorial Resource Scheduling A straightforward approach to resource scheduling for a set of agents M, whose values for the resources are induced by stochastic planning problems (in our case, finite-horizon MDPs) would be to have each agent enumerate all possible resource assignments over time and, for each one, compute its value by solving the corresponding MDP.",
                "Then, each agent would provide valuations for each possible resource bundle over time to a centralized coordinator, who would compute the optimal resource assignments across time based on these valuations.",
                "When resources can be allocated at different times to different agents, each agent must submit valuations for every combination of possible time horizons.",
                "Let each agent m ∈ M execute its MDP within the arrival-departure time interval τ ∈ [τa m, τd m].",
                "Hence, agent m will execute an MDP with time horizon no greater than Tm = τd m−τa m+1.",
                "Let bτ be the global time horizon for the problem, before which all of the agents MDPs must finish.",
                "We assume τd m < bτ, ∀m ∈ M. The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) 1221 For the scheduling problem where agents have static resource requirements within their finite-horizon MDPs, the agents provide a valuation for each resource bundle for each possible time horizon (from [1, Tm]) that they may use.",
                "Let Ω be the set of resources to be allocated among the agents.",
                "An agent will get at most one resource bundle for one of the time horizons.",
                "Let the variable ψ ∈ Ψm enumerate all possible pairs of resource bundles and time horizons for agent m, so there are 2|Ω| × Tm values for ψ (the space of bundles is exponential in the number of resource types |Ω|).",
                "The agent m must provide a value vψ m for each ψ, and the coordinator will allocate at most one ψ (resource, time horizon) pair to each agent.",
                "This allocation is expressed as an indicator variable zψ m ∈ {0, 1} that shows whether ψ is assigned to agent m. For time τ and resource ω, the function nm(ψ, τ, ω) ∈ {0, 1} indicates whether the bundle in ψ uses resource ω at time τ (we make the assumption that agents have binary resource requirements).",
                "This allocation problem is NP-complete, even when considering only a single time step, and its difficulty increases significantly with multiple time steps because of the increasing number of values of ψ.",
                "The problem of finding an optimal allocation that satisfies the global constraint that the amount of each resource ω allocated to all agents does not exceed the available amount bϕ(ω) can be expressed as the following integer program: max X m∈M X ψ∈Ψm zψ mvψ m subject to: X ψ∈Ψm zψ m ≤ 1, ∀m ∈ M; X m∈M X ψ∈Ψm zψ mnm(ψ, τ, ω) ≤ bϕ(ω), ∀τ ∈ [1, bτ], ∀ω ∈ Ω; (3) The first constraint in equation 3 says that no agent can receive more than one bundle, and the second constraint ensures that the total assignment of resource ω does not, at any time, exceed the resource bound.",
                "For the scheduling problem where the agents are able to dynamically reallocate resources, each agent must specify a value for every combination of bundles and time steps within its time horizon.",
                "Let the variable ψ ∈ Ψm in this case enumerate all possible resource bundles for which at most one bundle may be assigned to agent m at each time step.",
                "Therefore, in this case there are P t∈[1,Tm](2|Ω| )t ∼ 2|Ω|Tm possibilities of resource bundles assigned to different time slots, for the Tm different time horizons.",
                "The same set of equations (3) can be used to solve this dynamic scheduling problem, but the integer program is different because of the difference in how ψ is defined.",
                "In this case, the number of ψ values is exponential in each agents planning horizon Tm, resulting in a much larger program.",
                "This straightforward approach to solving both of these scheduling problems requires an enumeration and solution of either 2|Ω| Tm (static allocation) or P t∈[1,Tm] 2|Ω|t (dynamic reallocation) MDPs for each agent, which very quickly becomes intractable with the growth of the number of resources |Ω| or the time horizon Tm. 3.",
                "MODEL AND PROBLEM STATEMENT We now formally introduce our model of the resourcescheduling problem.",
                "The problem input consists of the following components: • M, Ω, bϕ, τa m, τd m, bτ are as defined above in Section 2.2. • {Θm} = {S, A, pm, rm, αm} are the MDPs of all agents m ∈ M. Without loss of generality, we assume that state and action spaces of all agents are the same, but each has its own transition function pm, reward function rm, and initial conditions αm. • ϕm : A×Ω → {0, 1} is the mapping of actions to resources for agent m. ϕm(a, ω) indicates whether action a of agent m needs resource ω.",
                "An agent m that receives a set of resources that does not include resource ω cannot execute in its MDP policy any action a for which ϕm(a, ω) = 0.",
                "We assume all resource requirements are binary; as discussed below in Section 6, this assumption is not limiting.",
                "Given the above input, the optimization problem we consider is to find the globally optimal-maximizing the sum of expected rewards-mapping of resources to agents for all time steps: Δ : τ × M × Ω → {0, 1}.",
                "A solution is feasible if the corresponding assignment of resources to the agents does not violate the global resource constraint: X m Δm(τ, ω) ≤ bϕ(ω), ∀ω ∈ Ω, τ ∈ [1, bτ]. (4) We consider two flavors of the resource-scheduling problem.",
                "The first formulation restricts resource assignments to the space where the allocation of resources to each agent is static during the agents lifetime.",
                "The second formulation allows reassignment of resources between agents at every time step within their lifetimes.",
                "Figure 1 depicts a resource-scheduling problem with three agents M = {m1, m2, m3}, three resources Ω = {ω1, ω2, ω3}, and a global problem horizon of bτ = 11.",
                "The agents arrival and departure times are shown as gray boxes and are {1, 6}, {3, 7}, and {2, 11}, respectively.",
                "A solution to this problem is shown via horizontal bars within each agents box, where the bars correspond to the allocation of the three resource types.",
                "Figure 1a shows a solution to a static scheduling problem.",
                "According to the shown solution, agent m1 begins the execution of its MDP at time τ = 1 and has a lock on all three resources until it finishes execution at time τ = 3.",
                "Note that agent m1 relinquishes its hold on the resources before its announced departure time of τd m1 = 6, ostensibly because other agents can utilize the resources more effectively.",
                "Thus, at time τ = 4, resources ω1 and ω3 are allocated to agent m2, who then uses them to execute its MDP (using only actions supported by resources ω1 and ω3) until time τ = 7.",
                "Agent m3 holds resource ω3 during the interval τ ∈ [4, 10].",
                "Figure 1b shows a possible solution to the dynamic version of the same problem.",
                "There, resources can be reallocated between agents at every time step.",
                "For example, agent m1 gives up its use of resource ω2 at time τ = 2, although it continues the execution of its MDP until time τ = 6.",
                "Notice that an agent is not allowed to stop and restart its MDP, so agent m1 is only able to continue executing in the interval τ ∈ [3, 4] if it has actions that do not require any resources (ϕm(a, ω) = 0).",
                "Clearly, the model and problem statement described above make a number of assumptions about the problem and the desired solution properties.",
                "We discuss some of those assumptions and their implications in Section 6. 1222 The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) (a) (b) Figure 1: Illustration of a solution to a resource-scheduling problem with three agents and three resources: a) static resource assignments (resource assignments are constant within agents lifetimes; b) dynamic assignment (resource assignments are allowed to change at every time step). 4.",
                "RESOURCE SCHEDULING Our resource-scheduling algorithm proceeds in two stages.",
                "First, we perform a preprocessing step that augments the agent MDPs; this process is described in Section 4.1.",
                "Second, using these augmented MDPs we construct a global optimization problem, which is described in Section 4.2. 4.1 Augmenting Agents MDPs In the model described in the previous section, we assume that if an agent does not possess the necessary resources to perform actions in its MDP, its execution is halted and the agent leaves the system.",
                "In other words, the MDPs cannot be paused and resumed.",
                "For example, in the problem shown in Figure 1a, agent m1 releases all resources after time τ = 3, at which point the execution of its MDP is halted.",
                "Similarly, agents m2 and m3 only execute their MDPs in the intervals τ ∈ [4, 6] and τ ∈ [4, 10], respectively.",
                "Therefore, an important part of the global decision-making problem is to decide the window of time during which each of the agents is active (i.e., executing its MDP).",
                "To accomplish this, we augment each agents MDP with two new states (start and finish states sb , sf , respectively) and a new start/stop action a∗ , as illustrated in Figure 2.",
                "The idea is that an agent stays in the start state sb until it is ready to execute its MDP, at which point it performs the start/stop action a∗ and transitions into the state space of the original MDP with the transition probability that corresponds to the original initial distribution α(s).",
                "For example, in Figure 1a, for agent m2 this would happen at time τ = 4.",
                "Once the agent gets to the end of its activity window (time τ = 6 for agent m2 in Figure 1a), it performs the start/stop action, which takes it into the sink finish state sf at time τ = 7.",
                "More precisely, given an MDP S, A, pm, rm, αm , we define an augmented MDP S , A , pm, rm, αm as follows: S = S ∪ sb ∪ sf ; A = A ∪ a∗ ; p (s|sb , a∗ ) = α(s), ∀s ∈ S; p (sb |sb , a) = 1.0, ∀a ∈ A; p (sf |s, a∗ ) = 1.0, ∀s ∈ S; p (σ|s, a) = p(σ|s, a), ∀s, σ ∈ S, a ∈ A; r (sb , a) = r (sf , a) = 0, ∀a ∈ A ; r (s, a) = r(s, a), ∀s ∈ S, a ∈ A; α (sb ) = 1; α (s) = 0, ∀s ∈ S; where all non-specified transition probabilities are assumed to be zero.",
                "Further, in order to account for the new starting state, we begin the MDP one time-step earlier, setting τa m ← τa m − 1.",
                "This will not affect the resource allocation due to the resource constraints only being enforced for the original MDP states, as will be discussed in the next section.",
                "For example, the augmented MDPs shown in Figure 2b (which starts in state sb at time τ = 2) would be constructed from an MDP with original arrival time τ = 3.",
                "Figure 2b also shows a sample trajectory through the state space: the agent starts in state sb , transitions into the state space S of the original MDP, and finally exists into the sink state sf .",
                "Note that if we wanted to model a problem where agents could pause their MDPs at arbitrary time steps (which might be useful for domains where dynamic reallocation is possible), we could easily accomplish this by including an extra action that transitions from each state to itself with zero reward. 4.2 MILP for Resource Scheduling Given a set of augmented MDPs, as defined above, the goal of this section is to formulate a global optimization program that solves the resource-scheduling problem.",
                "In this section and below, all MDPs are assumed to be the augmented MDPs as defined in Section 4.1.",
                "Our approach is similar to the idea used in [6]: we begin with the linear-program formulation of agents MDPs (1) and augment it with constraints that ensure that the corresponding resource allocation across agents and time is valid.",
                "The resulting optimization problem then simultaneously solves the agents MDPs and resource-scheduling problems.",
                "In the rest of this section, we incrementally develop a mixed integer program (MILP) that achieves this.",
                "In the absence of resource constraints, the agents finitehorizon MDPs are completely independent, and the globally optimal solution can be trivially obtained via the following LP, which is simply an aggregation of single-agent finitehorizon LPs: max X m X s X a rm(s, a) X t xm(s, a, t) subject to: X a xm(σ, a, t + 1) = X s,a pm(σ|s, a)xm(s, a, t), ∀m ∈ M, σ ∈ S, t ∈ [1, Tm − 1]; X a xm(s, a, 1) = αm(s), ∀m ∈ M, s ∈ S; (12) where xm(s, a, t) is the occupation measure of agent m, and The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) 1223 (a) (b) Figure 2: Illustration of augmenting an MDP to allow for variable starting and stopping times: a) (left) the original two-state MDP with a single action; (right) the augmented MDP with new states sb and sf and the new action a∗ (note that the origianl transitions are not changed in the augmentation process); b) the augmented MDP displayed as a trajectory through time (grey lines indicate all transitions, while black lines indicate a given trajectory.",
                "Objective Function (sum of expected rewards over all agents) max X m X s X a rm(s, a) X t xm(s, a, t) (5) Meaning Implication Linear Constraints Tie x to θ.",
                "Agent is only active when occupation measure is nonzero in original MDP states. θm(τ) = 0 =⇒ xm(s, a, τ −τa m+1) = 0 ∀s /∈ {sb , sf }, a ∈ A X s/∈{sb,sf } X a xm(s, a, t) ≤ θm(τa m + t − 1) ∀m ∈ M, ∀t ∈ [1, Tm] (6) Agent can only be active in τ ∈ (τa m, τd m) θm(τ) = 0 ∀m ∈ M, τ /∈ (τa m, τd m) (7) Cannot use resources when not active θm(τ) = 0 =⇒ Δm(τ, ω) = 0 ∀τ ∈ [0, bτ], ω ∈ Ω Δm(τ, ω) ≤ θm(τ) ∀m ∈ M, τ ∈ [0, bτ], ω ∈ Ω (8) Tie x to Δ (nonzero x forces corresponding Δ to be nonzero.)",
                "Δm(τ, ω) = 0, ϕm(a, ω) = 1 =⇒ xm(s, a, τ − τa m + 1) = 0 ∀s /∈ {sb , sf } 1/|A| X a ϕm(a, ω) X s/∈{sb,sf } xm(s, a, t) ≤ Δm(t + τa m − 1, ω) ∀m ∈ M, ω ∈ Ω, t ∈ [1, Tm] (9) Resource bounds X m Δm(τ, ω) ≤ bϕ(ω) ∀ω ∈ Ω, τ ∈ [0, bτ] (10) Agent cannot change resources while active.",
                "Only enabled for scheduling with static assignments. θm(τ) = 1 and θm(τ + 1) = 1 =⇒ Δm(τ, ω) = Δm(τ + 1, ω) Δm(τ, ω) − Z(1 − θm(τ + 1)) ≤ Δm(τ + 1, ω) + Z(1 − θm(τ)) Δm(τ, ω) + Z(1 − θm(τ + 1)) ≥ Δm(τ + 1, ω) − Z(1 − θm(τ)) ∀m ∈ M, ω ∈ Ω, τ ∈ [0, bτ] (11) Table 1: MILP for globally optimal resource scheduling.",
                "Tm = τd m − τa m + 1 is the time horizon for the agents MDP.",
                "Using this LP as a basis, we augment it with constraints that ensure that the resource usage implied by the agents occupation measures {xm} does not violate the global resource requirements bϕ at any time step τ ∈ [0, bτ].",
                "To formulate these resource constraints, we use the following binary variables: • Δm(τ, ω) = {0, 1}, ∀m ∈ M, τ ∈ [0, bτ], ω ∈ Ω, which serve as indicator variables that define whether agent m possesses resource ω at time τ.",
                "These are analogous to the static indicator variables used in the one-shot static resource-allocation problem in [6]. • θm = {0, 1}, ∀m ∈ M, τ ∈ [0, bτ] are indicator variables that specify whether agent m is active (i.e., executing its MDP) at time τ.",
                "The meaning of resource-usage variables Δ is illustrated in Figure 1: Δm(τ, ω) = 1 only if resource ω is allocated to agent m at time τ.",
                "The meaning of the activity indicators θ is illustrated in Figure 2b: when agent m is in either the start state sb or the finish state sf , the corresponding θm = 0, but once the agent becomes active and enters one of the other states, we set θm = 1 .",
                "This meaning of θ can be enforced with a linear constraint that synchronizes the values of the agents occupation measures xm and the activity 1224 The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) indicators θ, as shown in (6) in Table 1.",
                "Another constraint we have to add-because the activity indicators θ are defined on the global timeline τ-is to enforce the fact that the agent is inactive outside of its arrivaldeparture window.",
                "This is accomplished by constraint (7) in Table 1.",
                "Furthermore, agents should not be using resources while they are inactive.",
                "This constraint can also be enforced via a linear inequality on θ and Δ, as shown in (8).",
                "Constraint (6) sets the value of θ to match the policy defined by the occupation measure xm.",
                "In a similar fashion, we have to make sure that the resource-usage variables Δ are also synchronized with the occupation measure xm.",
                "This is done via constraint (9) in Table 1, which is nearly identical to the analogous constraint from [6].",
                "After implementing the above constraint, which enforces the meaning of Δ, we add a constraint that ensures that the agents resource usage never exceeds the amounts of available resources.",
                "This condition is also trivially expressed as a linear inequality (10) in Table 1.",
                "Finally, for the problem formulation where resource assignments are static during a lifetime of an agent, we add a constraint that ensures that the resource-usage variables Δ do not change their value while the agent is active (θ = 1).",
                "This is accomplished via the linear constraint (11), where Z ≥ 2 is a constant that is used to turn off the constraints when θm(τ) = 0 or θm(τ + 1) = 0.",
                "This constraint is not used for the dynamic problem formulation, where resources can be reallocated between agents at every time step.",
                "To summarize, Table 1 together with the conservationof-flow constraints from (12) defines the MILP that simultaneously computes an optimal resource assignment for all agents across time as well as optimal finite-horizon MDP policies that are valid under that resource assignment.",
                "As a rough measure of the complexity of this MILP, let us consider the number of optimization variables and constraints.",
                "Let TM = P Tm = P m(τa m − τd m + 1) be the sum of the lengths of the arrival-departure windows across all agents.",
                "Then, the number of optimization variables is: TM + bτ|M||Ω| + bτ|M|, TM of which are continuous (xm), and bτ|M||Ω| + bτ|M| are binary (Δ and θ).",
                "However, notice that all but TM|M| of the θ are set to zero by constraint (7), which also immediately forces all but TM|M||Ω| of the Δ to be zero via the constraints (8).",
                "The number of constraints (not including the degenerate constraints in (7)) in the MILP is: TM + TM|Ω| + bτ|Ω| + bτ|M||Ω|.",
                "Despite the fact that the complexity of the MILP is, in the worst case, exponential1 in the number of binary variables, the complexity of this MILP is significantly (exponentially) lower than that of the MILP with flat utility functions, described in Section 2.2.",
                "This result echos the efficiency gains reported in [6] for single-shot resource-allocation problems, but is much more pronounced, because of the explosion of the flat utility representation due to the temporal aspect of the problem (recall the prohibitive complexity of the combinatorial optimization in Section 2.2).",
                "We empirically analyze the performance of this method in Section 5. 1 Strictly speaking, solving MILPs to optimality is NPcomplete in the number of integer variables. 5.",
                "EXPERIMENTAL RESULTS Although the complexity of solving MILPs is in the worst case exponential in the number of integer variables, there are many efficient methods for solving MILPs that allow our algorithm to scale well for parameters common to resource allocation and scheduling problems.",
                "In particular, this section introduces a problem domain-the repairshop problem-used to empirically evaluate our algorithms scalability in terms of the number of agents |M|, the number of shared resources |Ω|, and the varied lengths of global time bτ during which agents may enter and exit the system.",
                "The repairshop problem is a simple parameterized MDP adopting the metaphor of a vehicular repair shop.",
                "Agents in the repair shop are mechanics with a number of independent tasks that yield reward only when completed.",
                "In our MDP model of this system, actions taken to advance through the state space are only allowed if the agent holds certain resources that are publicly available to the shop.",
                "These resources are in finite supply, and optimal policies for the shop will determine when each agent may hold the limited resources to take actions and earn individual rewards.",
                "Each task to be completed is associated with a single action, although the agent is required to repeat the action numerous times before completing the task and earning a reward.",
                "This model was parameterized in terms of the number of agents in the system, the number of different types of resources that could be linked to necessary actions, a global time during which agents are allowed to arrive and depart, and a maximum length for the number of time steps an agent may remain in the system.",
                "All datapoints in our experiments were obtained with 20 evaluations using CPLEX to solve the MILPs on a Pentium4 computer with 2Gb of RAM.",
                "Trials were conducted on both the static and the dynamic version of the resourcescheduling problem, as defined earlier.",
                "Figure 3 shows the runtime and policy value for independent modifications to the parameter set.",
                "The top row shows how the solution time for the MILP scales as we increase the number of agents |M|, the global time horizon bτ, and the number of resources |Ω|.",
                "Increasing the number of agents leads to exponential complexity scaling, which is to be expected for an NP-complete problem.",
                "However, increasing the global time limit bτ or the total number of resource types |Ω|-while holding the number of agents constantdoes not lead to decreased performance.",
                "This occurs because the problems get easier as they become under-constrained, which is also a common phenomenon for NP-complete problems.",
                "We also observe that the solution to the dynamic version of the problem can often be computed much faster than the static version.",
                "The bottom row of Figure 3 shows the joint policy value of the policies that correspond to the computed optimal resource-allocation schedules.",
                "We can observe that the dynamic version yields higher reward (as expected, since the reward for the dynamic version is always no less than the reward of the static version).",
                "We should point out that these graphs should not be viewed as a measure of performance of two different algorithms (both algorithms produce optimal solutions but to different problems), but rather as observations about how the quality of optimal solutions change as more flexibility is allowed in the reallocation of resources.",
                "Figure 4 shows runtime and policy value for trials in which common input variables are scaled together.",
                "This allows The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) 1225 2 4 6 8 10 10 −3 10 −2 10 −1 10 0 10 1 10 2 10 3 10 4 Number of Agents |M| CPUTime,sec |Ω| = 5, τ = 50 static dynamic 50 100 150 200 10 −2 10 −1 10 0 10 1 10 2 10 3 Global Time Boundary τ CPUTime,sec |M| = 5, |Ω| = 5 static dynamic 10 20 30 40 50 10 −2 10 −1 10 0 10 1 10 2 Number of Resources |Ω| CPUTime,sec |M| = 5, τ = 50 static dynamic 2 4 6 8 10 200 400 600 800 1000 1200 1400 1600 Number of Agents |M| Value |Ω| = 5, τ = 50 static dynamic 50 100 150 200 400 500 600 700 800 900 1000 1100 1200 1300 1400 Global Time Boundary τ Value |M| = 5, |Ω| = 5 static dynamic 10 20 30 40 50 500 600 700 800 900 1000 1100 1200 1300 1400 Number of Resources |Ω| Value |M| = 5, τ = 50 static dynamic Figure 3: Evaluation of our MILP for variable numbers of agents (column 1), lengths of global-time window (column 2), and numbers of resource types (column 3).",
                "Top row shows CPU time, and bottom row shows the joint reward of agents MDP policies.",
                "Error bars show the 1st and 3rd quartiles (25% and 75%). 2 4 6 8 10 10 −3 10 −2 10 −1 10 0 10 1 10 2 10 3 Number of Agents |M| CPUTime,sec τ = 10|M| static dynamic 2 4 6 8 10 10 −3 10 −2 10 −1 10 0 10 1 10 2 10 3 10 4 Number of Agents |M| CPUTime,sec |Ω| = 2|M| static dynamic 2 4 6 8 10 10 −3 10 −2 10 −1 10 0 10 1 10 2 10 3 10 4 Number of Agents |M| CPUTime,sec |Ω| = 5|M| static dynamic 2 4 6 8 10 200 400 600 800 1000 1200 1400 1600 1800 2000 2200 Number of Agents |M| Value τ = 10|M| static dynamic 2 4 6 8 10 200 400 600 800 1000 1200 1400 1600 1800 2000 Number of Agents |M| Value |Ω| = 2|M| static dynamic 2 4 6 8 10 0 500 1000 1500 2000 2500 Number of Agents |M| Value |Ω| = 5|M| static dynamic Figure 4: Evaluation of our MILP using correlated input variables.",
                "The left column tracks the performance and CPU time as the number of agents and global-time window increase together (bτ = 10|M|).",
                "The middle and the right column track the performance and CPU time as the number of resources and the number of agents increase together as |Ω| = 2|M| and |Ω| = 5|M|, respectively.",
                "Error bars show the 1st and 3rd quartiles (25% and 75%). 1226 The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) us to explore domains where the total number of agents scales proportionally to the total number of resource types or the global time horizon, while keeping constant the average agent density (per unit of global time) or the average number of resources per agent (which commonly occurs in real-life applications).",
                "Overall, we believe that these experimental results indicate that our MILP formulation can be used to effectively solve resource-scheduling problems of nontrivial size. 6.",
                "DISCUSSION AND CONCLUSIONS Throughout the paper, we have made a number of assumptions in our model and solution algorithm; we discuss their implications below. • Continual execution.",
                "We assume that once an agent stops executing its MDP (transitions into state sf ), it exits the system and cannot return.",
                "It is easy to relax this assumption for domains where agents MDPs can be paused and restarted.",
                "All that is required is to include an additional pause action which transitions from a given state back to itself, and has zero reward. • Indifference to start time.",
                "We used a reward model where agents rewards depend only on the time horizon of their MDPs and not the global start time.",
                "This is a consequence of our MDP-augmentation procedure from Section 4.1.",
                "It is easy to extend the model so that the agents incur an explicit penalty for idling by assigning a non-zero negative reward to the start state sb . • Binary resource requirements.",
                "For simplicity, we have assumed that resource costs are binary: ϕm(a, ω) = {0, 1}, but our results generalize in a straightforward manner to non-binary resource mappings, analogously to the procedure used in [5]. • Cooperative agents.",
                "The optimization procedure discussed in this paper was developed in the context of cooperative agents, but it can also be used to design a mechanism for scheduling resources among self-interested agents.",
                "This optimization procedure can be embedded in a VickreyClarke-Groves auction, completely analogously to the way it was done in [7].",
                "In fact, all the results of [7] about the properties of the auction and information privacy directly carry over to the scheduling domain discussed in this paper, requiring only slight modifications to deal with finitehorizon MDPs. • Known, deterministic arrival and departure times.",
                "Finally, we have assumed that agents arrival and departure times (τa m and τd m) are deterministic and known a priori.",
                "This assumption is fundamental to our solution method.",
                "While there are many domains where this assumption is valid, in many cases agents arrive and depart dynamically and their arrival and departure times can only be predicted probabilistically, leading to online resource-allocation problems.",
                "In particular, in the case of self-interested agents, this becomes an interesting version of an online-mechanism-design problem [11, 12].",
                "In summary, we have presented an MILP formulation for the combinatorial resource-scheduling problem where agents values for possible resource assignments are defined by finitehorizon MDPs.",
                "This result extends previous work ([6, 7]) on static one-shot resource allocation under MDP-induced preferences to resource-scheduling problems with a temporal aspect.",
                "As such, this work takes a step in the direction of designing an online mechanism for agents with combinatorial resource preferences induced by stochastic planning problems.",
                "Relaxing the assumption about deterministic arrival and departure times of the agents is a focus of our future work.",
                "We would like to thank the anonymous reviewers for their insightful comments and suggestions. 7.",
                "REFERENCES [1] E. Altman and A. Shwartz.",
                "Adaptive control of constrained Markov chains: Criteria and policies.",
                "Annals of Operations Research, special issue on Markov Decision Processes, 28:101-134, 1991. [2] R. Bellman.",
                "Dynamic Programming.",
                "Princeton University Press, 1957. [3] C. Boutilier.",
                "Solving concisely expressed combinatorial auction problems.",
                "In Proc. of AAAI-02, pages 359-366, 2002. [4] C. Boutilier and H. H. Hoos.",
                "Bidding languages for combinatorial auctions.",
                "In Proc. of IJCAI-01, pages 1211-1217, 2001. [5] D. Dolgov.",
                "Integrated Resource Allocation and Planning in Stochastic Multiagent Environments.",
                "PhD thesis, Computer Science Department, University of Michigan, February 2006. [6] D. A. Dolgov and E. H. Durfee.",
                "Optimal resource allocation and policy formulation in loosely-coupled Markov decision processes.",
                "In Proc. of ICAPS-04, pages 315-324, June 2004. [7] D. A. Dolgov and E. H. Durfee.",
                "Computationally efficient combinatorial auctions for resource allocation in weakly-coupled MDPs.",
                "In Proc. of AAMAS-05, New York, NY, USA, 2005.",
                "ACM Press. [8] D. A. Dolgov and E. H. Durfee.",
                "Resource allocation among agents with preferences induced by factored MDPs.",
                "In Proc. of AAMAS-06, 2006. [9] K. Larson and T. Sandholm.",
                "Mechanism design and deliberative agents.",
                "In Proc. of AAMAS-05, pages 650-656, New York, NY, USA, 2005.",
                "ACM Press. [10] N. Nisan.",
                "Bidding and allocation in combinatorial auctions.",
                "In Electronic Commerce, 2000. [11] D. C. Parkes and S. Singh.",
                "An MDP-based approach to Online Mechanism Design.",
                "In Proc. of the Seventeenths Annual Conference on Neural Information Processing Systems (NIPS-03), 2003. [12] D. C. Parkes, S. Singh, and D. Yanovsky.",
                "Approximately efficient online mechanism design.",
                "In Proc. of the Eighteenths Annual Conference on Neural Information Processing Systems (NIPS-04), 2004. [13] M. L. Puterman.",
                "Markov Decision Processes.",
                "John Wiley & Sons, New York, 1994. [14] M. H. Rothkopf, A. Pekec, and R. M. Harstad.",
                "Computationally manageable combinational auctions.",
                "Management Science, 44(8):1131-1147, 1998. [15] T. Sandholm.",
                "An algorithm for optimal winner determination in combinatorial auctions.",
                "In Proc. of IJCAI-99, pages 542-547, San Francisco, CA, USA, 1999.",
                "Morgan Kaufmann Publishers Inc.",
                "The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) 1227"
            ],
            "original_annotated_samples": [],
            "translated_annotated_samples": [],
            "translated_text": "",
            "candidates": [],
            "error": []
        },
        "multiagent plan": {
            "translated_key": "",
            "is_in_text": false,
            "original_annotated_sentences": [
                "Combinatorial Resource Scheduling for Multiagent MDPs Dmitri A. Dolgov, Michael R. James, and Michael E. Samples AI and Robotics Group Technical Research, Toyota Technical Center USA {ddolgov, michael.r.james, michael.samples}@gmail.com ABSTRACT Optimal resource scheduling in multiagent systems is a computationally challenging task, particularly when the values of resources are not additive.",
                "We consider the combinatorial problem of scheduling the usage of multiple resources among agents that operate in stochastic environments, modeled as Markov decision processes (MDPs).",
                "In recent years, efficient resource-allocation algorithms have been developed for agents with resource values induced by MDPs.",
                "However, this prior work has focused on static resource-allocation problems where resources are distributed once and then utilized in infinite-horizon MDPs.",
                "We extend those existing models to the problem of combinatorial resource scheduling, where agents persist only for finite periods between their (predefined) arrival and departure times, requiring resources only for those time periods.",
                "We provide a computationally efficient procedure for computing globally optimal resource assignments to agents over time.",
                "We illustrate and empirically analyze the method in the context of a stochastic jobscheduling domain.",
                "Categories and Subject Descriptors I.2.8 [Artificial Intelligence]: Problem Solving, Control Methods, and Search; I.2.11 [Artificial Intelligence]: Distributed Artificial Intelligence-Multiagent systems General Terms Algorithms, Performance, Design 1.",
                "INTRODUCTION The tasks of optimal resource allocation and scheduling are ubiquitous in multiagent systems, but solving such optimization problems can be computationally difficult, due to a number of factors.",
                "In particular, when the value of a set of resources to an agent is not additive (as is often the case with resources that are substitutes or complements), the utility function might have to be defined on an exponentially large space of resource bundles, which very quickly becomes computationally intractable.",
                "Further, even when each agent has a utility function that is nonzero only on a small subset of the possible resource bundles, obtaining optimal allocation is still computationally prohibitive, as the problem becomes NP-complete [14].",
                "Such computational issues have recently spawned several threads of work in using compact models of agents preferences.",
                "One idea is to use any structure present in utility functions to represent them compactly, via, for example, logical formulas [15, 10, 4, 3].",
                "An alternative is to directly model the mechanisms that define the agents utility functions and perform resource allocation directly with these models [9].",
                "A way of accomplishing this is to model the processes by which an agent might utilize the resources and define the utility function as the payoff of these processes.",
                "In particular, if an agent uses resources to act in a stochastic environment, its utility function can be naturally modeled with a Markov decision process, whose action set is parameterized by the available resources.",
                "This representation can then be used to construct very efficient resource-allocation algorithms that lead to an exponential speedup over a straightforward optimization problem with flat representations of combinatorial preferences [6, 7, 8].",
                "However, this existing work on resource allocation with preferences induced by resource-parameterized MDPs makes an assumption that the resources are only allocated once and are then utilized by the agents independently within their infinite-horizon MDPs.",
                "This assumption that no reallocation of resources is possible can be limiting in domains where agents arrive and depart dynamically.",
                "In this paper, we extend the work on resource allocation under MDP-induced preferences to discrete-time scheduling problems, where agents are present in the system for finite time intervals and can only use resources within these intervals.",
                "In particular, agents arrive and depart at arbitrary (predefined) times and within these intervals use resources to execute tasks in finite-horizon MDPs.",
                "We address the problem of globally optimal resource scheduling, where the objective is to find an allocation of resources to the agents across time that maximizes the sum of the expected rewards that they obtain.",
                "In this context, our main contribution is a mixed-integerprogramming formulation of the scheduling problem that chooses globally optimal resource assignments, starting times, and execution horizons for all agents (within their arrival1220 978-81-904262-7-5 (RPS) c 2007 IFAAMAS departure intervals).",
                "We analyze and empirically compare two flavors of the scheduling problem: one, where agents have static resource assignments within their finite-horizon MDPs, and another, where resources can be dynamically reallocated between agents at every time step.",
                "In the rest of the paper, we first lay down the necessary groundwork in Section 2 and then introduce our model and formal problem statement in Section 3.",
                "In Section 4.2, we describe our main result, the optimization program for globally optimal resource scheduling.",
                "Following the discussion of our experimental results on a job-scheduling problem in Section 5, we conclude in Section 6 with a discussion of possible extensions and generalizations of our method. 2.",
                "BACKGROUND Similarly to the model used in previous work on resourceallocation with MDP-induced preferences [6, 7], we define the value of a set of resources to an agent as the value of the best MDP policy that is realizable, given those resources.",
                "However, since the focus of our work is on scheduling problems, and a large part of the optimization problem is to decide how resources are allocated in time among agents with finite arrival and departure times, we model the agents planning problems as finite-horizon MDPs, in contrast to previous work that used infinite-horizon discounted MDPs.",
                "In the rest of this section, we first introduce some necessary background on finite-horizon MDPs and present a linear-programming formulation that serves as the basis for our solution algorithm developed in Section 4.",
                "We also outline the standard methods for combinatorial resource scheduling with flat resource values, which serve as a comparison benchmark for the new model developed here. 2.1 Markov Decision Processes A stationary, finite-domain, discrete-time MDP (see, for example, [13] for a thorough and detailed development) can be described as S, A, p, r , where: S is a finite set of system states; A is a finite set of actions that are available to the agent; p is a stationary stochastic transition function, where p(σ|s, a) is the probability of transitioning to state σ upon executing action a in state s; r is a stationary reward function, where r(s, a) specifies the reward obtained upon executing action a in state s. Given such an MDP, a decision problem under a finite horizon T is to choose an optimal action at every time step to maximize the expected value of the total reward accrued during the agents (finite) lifetime.",
                "The agents optimal policy is then a function of current state s and the time until the horizon.",
                "An optimal policy for such a problem is to act greedily with respect to the optimal value function, defined recursively by the following system of finite-time Bellman equations [2]: v(s, t) = max a r(s, a) + X σ p(σ|s, a)v(σ, t + 1), ∀s ∈ S, t ∈ [1, T − 1]; v(s, T) = 0, ∀s ∈ S; where v(s, t) is the optimal value of being in state s at time t ∈ [1, T].",
                "This optimal value function can be easily computed using dynamic programming, leading to the following optimal policy π, where π(s, a, t) is the probability of executing action a in state s at time t: π(s, a, t) = ( 1, a = argmaxa r(s, a) + P σ p(σ|s, a)v(σ, t + 1), 0, otherwise.",
                "The above is the most common way of computing the optimal value function (and therefore an optimal policy) for a finite-horizon MDP.",
                "However, we can also formulate the problem as the following linear program (similarly to the dual LP for infinite-horizon discounted MDPs [13, 6, 7]): max X s X a r(s, a) X t x(s, a, t) subject to: X a x(σ, a, t + 1) = X s,a p(σ|s, a)x(s, a, t) ∀σ, t ∈ [1, T − 1]; X a x(s, a, 1) = α(s), ∀s ∈ S; (1) where α(s) is the initial distribution over the state space, and x is the (non-stationary) occupation measure (x(s, a, t) ∈ [0, 1] is the total expected number of times action a is executed in state s at time t).",
                "An optimal (non-stationary) policy is obtained from the occupation measure as follows: π(s, a, t) = x(s, a, t)/ X a x(s, a, t) ∀s ∈ S, t ∈ [1, T]. (2) Note that the standard unconstrained finite-horizon MDP, as described above, always has a uniformly-optimal solution (optimal for any initial distribution α(s)).",
                "Therefore, an optimal policy can be obtained by using an arbitrary constant α(s) > 0 (in particular, α(s) = 1 will result in x(s, a, t) = π(s, a, t)).",
                "However, for MDPs with resource constraints (as defined below in Section 3), uniformly-optimal policies do not in general exist.",
                "In such cases, α becomes a part of the problem input, and a resulting policy is only optimal for that particular α.",
                "This result is well known for infinite-horizon MDPs with various types of constraints [1, 6], and it also holds for our finite-horizon model, which can be easily established via a line of reasoning completely analogous to the arguments in [6]. 2.2 Combinatorial Resource Scheduling A straightforward approach to resource scheduling for a set of agents M, whose values for the resources are induced by stochastic planning problems (in our case, finite-horizon MDPs) would be to have each agent enumerate all possible resource assignments over time and, for each one, compute its value by solving the corresponding MDP.",
                "Then, each agent would provide valuations for each possible resource bundle over time to a centralized coordinator, who would compute the optimal resource assignments across time based on these valuations.",
                "When resources can be allocated at different times to different agents, each agent must submit valuations for every combination of possible time horizons.",
                "Let each agent m ∈ M execute its MDP within the arrival-departure time interval τ ∈ [τa m, τd m].",
                "Hence, agent m will execute an MDP with time horizon no greater than Tm = τd m−τa m+1.",
                "Let bτ be the global time horizon for the problem, before which all of the agents MDPs must finish.",
                "We assume τd m < bτ, ∀m ∈ M. The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) 1221 For the scheduling problem where agents have static resource requirements within their finite-horizon MDPs, the agents provide a valuation for each resource bundle for each possible time horizon (from [1, Tm]) that they may use.",
                "Let Ω be the set of resources to be allocated among the agents.",
                "An agent will get at most one resource bundle for one of the time horizons.",
                "Let the variable ψ ∈ Ψm enumerate all possible pairs of resource bundles and time horizons for agent m, so there are 2|Ω| × Tm values for ψ (the space of bundles is exponential in the number of resource types |Ω|).",
                "The agent m must provide a value vψ m for each ψ, and the coordinator will allocate at most one ψ (resource, time horizon) pair to each agent.",
                "This allocation is expressed as an indicator variable zψ m ∈ {0, 1} that shows whether ψ is assigned to agent m. For time τ and resource ω, the function nm(ψ, τ, ω) ∈ {0, 1} indicates whether the bundle in ψ uses resource ω at time τ (we make the assumption that agents have binary resource requirements).",
                "This allocation problem is NP-complete, even when considering only a single time step, and its difficulty increases significantly with multiple time steps because of the increasing number of values of ψ.",
                "The problem of finding an optimal allocation that satisfies the global constraint that the amount of each resource ω allocated to all agents does not exceed the available amount bϕ(ω) can be expressed as the following integer program: max X m∈M X ψ∈Ψm zψ mvψ m subject to: X ψ∈Ψm zψ m ≤ 1, ∀m ∈ M; X m∈M X ψ∈Ψm zψ mnm(ψ, τ, ω) ≤ bϕ(ω), ∀τ ∈ [1, bτ], ∀ω ∈ Ω; (3) The first constraint in equation 3 says that no agent can receive more than one bundle, and the second constraint ensures that the total assignment of resource ω does not, at any time, exceed the resource bound.",
                "For the scheduling problem where the agents are able to dynamically reallocate resources, each agent must specify a value for every combination of bundles and time steps within its time horizon.",
                "Let the variable ψ ∈ Ψm in this case enumerate all possible resource bundles for which at most one bundle may be assigned to agent m at each time step.",
                "Therefore, in this case there are P t∈[1,Tm](2|Ω| )t ∼ 2|Ω|Tm possibilities of resource bundles assigned to different time slots, for the Tm different time horizons.",
                "The same set of equations (3) can be used to solve this dynamic scheduling problem, but the integer program is different because of the difference in how ψ is defined.",
                "In this case, the number of ψ values is exponential in each agents planning horizon Tm, resulting in a much larger program.",
                "This straightforward approach to solving both of these scheduling problems requires an enumeration and solution of either 2|Ω| Tm (static allocation) or P t∈[1,Tm] 2|Ω|t (dynamic reallocation) MDPs for each agent, which very quickly becomes intractable with the growth of the number of resources |Ω| or the time horizon Tm. 3.",
                "MODEL AND PROBLEM STATEMENT We now formally introduce our model of the resourcescheduling problem.",
                "The problem input consists of the following components: • M, Ω, bϕ, τa m, τd m, bτ are as defined above in Section 2.2. • {Θm} = {S, A, pm, rm, αm} are the MDPs of all agents m ∈ M. Without loss of generality, we assume that state and action spaces of all agents are the same, but each has its own transition function pm, reward function rm, and initial conditions αm. • ϕm : A×Ω → {0, 1} is the mapping of actions to resources for agent m. ϕm(a, ω) indicates whether action a of agent m needs resource ω.",
                "An agent m that receives a set of resources that does not include resource ω cannot execute in its MDP policy any action a for which ϕm(a, ω) = 0.",
                "We assume all resource requirements are binary; as discussed below in Section 6, this assumption is not limiting.",
                "Given the above input, the optimization problem we consider is to find the globally optimal-maximizing the sum of expected rewards-mapping of resources to agents for all time steps: Δ : τ × M × Ω → {0, 1}.",
                "A solution is feasible if the corresponding assignment of resources to the agents does not violate the global resource constraint: X m Δm(τ, ω) ≤ bϕ(ω), ∀ω ∈ Ω, τ ∈ [1, bτ]. (4) We consider two flavors of the resource-scheduling problem.",
                "The first formulation restricts resource assignments to the space where the allocation of resources to each agent is static during the agents lifetime.",
                "The second formulation allows reassignment of resources between agents at every time step within their lifetimes.",
                "Figure 1 depicts a resource-scheduling problem with three agents M = {m1, m2, m3}, three resources Ω = {ω1, ω2, ω3}, and a global problem horizon of bτ = 11.",
                "The agents arrival and departure times are shown as gray boxes and are {1, 6}, {3, 7}, and {2, 11}, respectively.",
                "A solution to this problem is shown via horizontal bars within each agents box, where the bars correspond to the allocation of the three resource types.",
                "Figure 1a shows a solution to a static scheduling problem.",
                "According to the shown solution, agent m1 begins the execution of its MDP at time τ = 1 and has a lock on all three resources until it finishes execution at time τ = 3.",
                "Note that agent m1 relinquishes its hold on the resources before its announced departure time of τd m1 = 6, ostensibly because other agents can utilize the resources more effectively.",
                "Thus, at time τ = 4, resources ω1 and ω3 are allocated to agent m2, who then uses them to execute its MDP (using only actions supported by resources ω1 and ω3) until time τ = 7.",
                "Agent m3 holds resource ω3 during the interval τ ∈ [4, 10].",
                "Figure 1b shows a possible solution to the dynamic version of the same problem.",
                "There, resources can be reallocated between agents at every time step.",
                "For example, agent m1 gives up its use of resource ω2 at time τ = 2, although it continues the execution of its MDP until time τ = 6.",
                "Notice that an agent is not allowed to stop and restart its MDP, so agent m1 is only able to continue executing in the interval τ ∈ [3, 4] if it has actions that do not require any resources (ϕm(a, ω) = 0).",
                "Clearly, the model and problem statement described above make a number of assumptions about the problem and the desired solution properties.",
                "We discuss some of those assumptions and their implications in Section 6. 1222 The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) (a) (b) Figure 1: Illustration of a solution to a resource-scheduling problem with three agents and three resources: a) static resource assignments (resource assignments are constant within agents lifetimes; b) dynamic assignment (resource assignments are allowed to change at every time step). 4.",
                "RESOURCE SCHEDULING Our resource-scheduling algorithm proceeds in two stages.",
                "First, we perform a preprocessing step that augments the agent MDPs; this process is described in Section 4.1.",
                "Second, using these augmented MDPs we construct a global optimization problem, which is described in Section 4.2. 4.1 Augmenting Agents MDPs In the model described in the previous section, we assume that if an agent does not possess the necessary resources to perform actions in its MDP, its execution is halted and the agent leaves the system.",
                "In other words, the MDPs cannot be paused and resumed.",
                "For example, in the problem shown in Figure 1a, agent m1 releases all resources after time τ = 3, at which point the execution of its MDP is halted.",
                "Similarly, agents m2 and m3 only execute their MDPs in the intervals τ ∈ [4, 6] and τ ∈ [4, 10], respectively.",
                "Therefore, an important part of the global decision-making problem is to decide the window of time during which each of the agents is active (i.e., executing its MDP).",
                "To accomplish this, we augment each agents MDP with two new states (start and finish states sb , sf , respectively) and a new start/stop action a∗ , as illustrated in Figure 2.",
                "The idea is that an agent stays in the start state sb until it is ready to execute its MDP, at which point it performs the start/stop action a∗ and transitions into the state space of the original MDP with the transition probability that corresponds to the original initial distribution α(s).",
                "For example, in Figure 1a, for agent m2 this would happen at time τ = 4.",
                "Once the agent gets to the end of its activity window (time τ = 6 for agent m2 in Figure 1a), it performs the start/stop action, which takes it into the sink finish state sf at time τ = 7.",
                "More precisely, given an MDP S, A, pm, rm, αm , we define an augmented MDP S , A , pm, rm, αm as follows: S = S ∪ sb ∪ sf ; A = A ∪ a∗ ; p (s|sb , a∗ ) = α(s), ∀s ∈ S; p (sb |sb , a) = 1.0, ∀a ∈ A; p (sf |s, a∗ ) = 1.0, ∀s ∈ S; p (σ|s, a) = p(σ|s, a), ∀s, σ ∈ S, a ∈ A; r (sb , a) = r (sf , a) = 0, ∀a ∈ A ; r (s, a) = r(s, a), ∀s ∈ S, a ∈ A; α (sb ) = 1; α (s) = 0, ∀s ∈ S; where all non-specified transition probabilities are assumed to be zero.",
                "Further, in order to account for the new starting state, we begin the MDP one time-step earlier, setting τa m ← τa m − 1.",
                "This will not affect the resource allocation due to the resource constraints only being enforced for the original MDP states, as will be discussed in the next section.",
                "For example, the augmented MDPs shown in Figure 2b (which starts in state sb at time τ = 2) would be constructed from an MDP with original arrival time τ = 3.",
                "Figure 2b also shows a sample trajectory through the state space: the agent starts in state sb , transitions into the state space S of the original MDP, and finally exists into the sink state sf .",
                "Note that if we wanted to model a problem where agents could pause their MDPs at arbitrary time steps (which might be useful for domains where dynamic reallocation is possible), we could easily accomplish this by including an extra action that transitions from each state to itself with zero reward. 4.2 MILP for Resource Scheduling Given a set of augmented MDPs, as defined above, the goal of this section is to formulate a global optimization program that solves the resource-scheduling problem.",
                "In this section and below, all MDPs are assumed to be the augmented MDPs as defined in Section 4.1.",
                "Our approach is similar to the idea used in [6]: we begin with the linear-program formulation of agents MDPs (1) and augment it with constraints that ensure that the corresponding resource allocation across agents and time is valid.",
                "The resulting optimization problem then simultaneously solves the agents MDPs and resource-scheduling problems.",
                "In the rest of this section, we incrementally develop a mixed integer program (MILP) that achieves this.",
                "In the absence of resource constraints, the agents finitehorizon MDPs are completely independent, and the globally optimal solution can be trivially obtained via the following LP, which is simply an aggregation of single-agent finitehorizon LPs: max X m X s X a rm(s, a) X t xm(s, a, t) subject to: X a xm(σ, a, t + 1) = X s,a pm(σ|s, a)xm(s, a, t), ∀m ∈ M, σ ∈ S, t ∈ [1, Tm − 1]; X a xm(s, a, 1) = αm(s), ∀m ∈ M, s ∈ S; (12) where xm(s, a, t) is the occupation measure of agent m, and The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) 1223 (a) (b) Figure 2: Illustration of augmenting an MDP to allow for variable starting and stopping times: a) (left) the original two-state MDP with a single action; (right) the augmented MDP with new states sb and sf and the new action a∗ (note that the origianl transitions are not changed in the augmentation process); b) the augmented MDP displayed as a trajectory through time (grey lines indicate all transitions, while black lines indicate a given trajectory.",
                "Objective Function (sum of expected rewards over all agents) max X m X s X a rm(s, a) X t xm(s, a, t) (5) Meaning Implication Linear Constraints Tie x to θ.",
                "Agent is only active when occupation measure is nonzero in original MDP states. θm(τ) = 0 =⇒ xm(s, a, τ −τa m+1) = 0 ∀s /∈ {sb , sf }, a ∈ A X s/∈{sb,sf } X a xm(s, a, t) ≤ θm(τa m + t − 1) ∀m ∈ M, ∀t ∈ [1, Tm] (6) Agent can only be active in τ ∈ (τa m, τd m) θm(τ) = 0 ∀m ∈ M, τ /∈ (τa m, τd m) (7) Cannot use resources when not active θm(τ) = 0 =⇒ Δm(τ, ω) = 0 ∀τ ∈ [0, bτ], ω ∈ Ω Δm(τ, ω) ≤ θm(τ) ∀m ∈ M, τ ∈ [0, bτ], ω ∈ Ω (8) Tie x to Δ (nonzero x forces corresponding Δ to be nonzero.)",
                "Δm(τ, ω) = 0, ϕm(a, ω) = 1 =⇒ xm(s, a, τ − τa m + 1) = 0 ∀s /∈ {sb , sf } 1/|A| X a ϕm(a, ω) X s/∈{sb,sf } xm(s, a, t) ≤ Δm(t + τa m − 1, ω) ∀m ∈ M, ω ∈ Ω, t ∈ [1, Tm] (9) Resource bounds X m Δm(τ, ω) ≤ bϕ(ω) ∀ω ∈ Ω, τ ∈ [0, bτ] (10) Agent cannot change resources while active.",
                "Only enabled for scheduling with static assignments. θm(τ) = 1 and θm(τ + 1) = 1 =⇒ Δm(τ, ω) = Δm(τ + 1, ω) Δm(τ, ω) − Z(1 − θm(τ + 1)) ≤ Δm(τ + 1, ω) + Z(1 − θm(τ)) Δm(τ, ω) + Z(1 − θm(τ + 1)) ≥ Δm(τ + 1, ω) − Z(1 − θm(τ)) ∀m ∈ M, ω ∈ Ω, τ ∈ [0, bτ] (11) Table 1: MILP for globally optimal resource scheduling.",
                "Tm = τd m − τa m + 1 is the time horizon for the agents MDP.",
                "Using this LP as a basis, we augment it with constraints that ensure that the resource usage implied by the agents occupation measures {xm} does not violate the global resource requirements bϕ at any time step τ ∈ [0, bτ].",
                "To formulate these resource constraints, we use the following binary variables: • Δm(τ, ω) = {0, 1}, ∀m ∈ M, τ ∈ [0, bτ], ω ∈ Ω, which serve as indicator variables that define whether agent m possesses resource ω at time τ.",
                "These are analogous to the static indicator variables used in the one-shot static resource-allocation problem in [6]. • θm = {0, 1}, ∀m ∈ M, τ ∈ [0, bτ] are indicator variables that specify whether agent m is active (i.e., executing its MDP) at time τ.",
                "The meaning of resource-usage variables Δ is illustrated in Figure 1: Δm(τ, ω) = 1 only if resource ω is allocated to agent m at time τ.",
                "The meaning of the activity indicators θ is illustrated in Figure 2b: when agent m is in either the start state sb or the finish state sf , the corresponding θm = 0, but once the agent becomes active and enters one of the other states, we set θm = 1 .",
                "This meaning of θ can be enforced with a linear constraint that synchronizes the values of the agents occupation measures xm and the activity 1224 The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) indicators θ, as shown in (6) in Table 1.",
                "Another constraint we have to add-because the activity indicators θ are defined on the global timeline τ-is to enforce the fact that the agent is inactive outside of its arrivaldeparture window.",
                "This is accomplished by constraint (7) in Table 1.",
                "Furthermore, agents should not be using resources while they are inactive.",
                "This constraint can also be enforced via a linear inequality on θ and Δ, as shown in (8).",
                "Constraint (6) sets the value of θ to match the policy defined by the occupation measure xm.",
                "In a similar fashion, we have to make sure that the resource-usage variables Δ are also synchronized with the occupation measure xm.",
                "This is done via constraint (9) in Table 1, which is nearly identical to the analogous constraint from [6].",
                "After implementing the above constraint, which enforces the meaning of Δ, we add a constraint that ensures that the agents resource usage never exceeds the amounts of available resources.",
                "This condition is also trivially expressed as a linear inequality (10) in Table 1.",
                "Finally, for the problem formulation where resource assignments are static during a lifetime of an agent, we add a constraint that ensures that the resource-usage variables Δ do not change their value while the agent is active (θ = 1).",
                "This is accomplished via the linear constraint (11), where Z ≥ 2 is a constant that is used to turn off the constraints when θm(τ) = 0 or θm(τ + 1) = 0.",
                "This constraint is not used for the dynamic problem formulation, where resources can be reallocated between agents at every time step.",
                "To summarize, Table 1 together with the conservationof-flow constraints from (12) defines the MILP that simultaneously computes an optimal resource assignment for all agents across time as well as optimal finite-horizon MDP policies that are valid under that resource assignment.",
                "As a rough measure of the complexity of this MILP, let us consider the number of optimization variables and constraints.",
                "Let TM = P Tm = P m(τa m − τd m + 1) be the sum of the lengths of the arrival-departure windows across all agents.",
                "Then, the number of optimization variables is: TM + bτ|M||Ω| + bτ|M|, TM of which are continuous (xm), and bτ|M||Ω| + bτ|M| are binary (Δ and θ).",
                "However, notice that all but TM|M| of the θ are set to zero by constraint (7), which also immediately forces all but TM|M||Ω| of the Δ to be zero via the constraints (8).",
                "The number of constraints (not including the degenerate constraints in (7)) in the MILP is: TM + TM|Ω| + bτ|Ω| + bτ|M||Ω|.",
                "Despite the fact that the complexity of the MILP is, in the worst case, exponential1 in the number of binary variables, the complexity of this MILP is significantly (exponentially) lower than that of the MILP with flat utility functions, described in Section 2.2.",
                "This result echos the efficiency gains reported in [6] for single-shot resource-allocation problems, but is much more pronounced, because of the explosion of the flat utility representation due to the temporal aspect of the problem (recall the prohibitive complexity of the combinatorial optimization in Section 2.2).",
                "We empirically analyze the performance of this method in Section 5. 1 Strictly speaking, solving MILPs to optimality is NPcomplete in the number of integer variables. 5.",
                "EXPERIMENTAL RESULTS Although the complexity of solving MILPs is in the worst case exponential in the number of integer variables, there are many efficient methods for solving MILPs that allow our algorithm to scale well for parameters common to resource allocation and scheduling problems.",
                "In particular, this section introduces a problem domain-the repairshop problem-used to empirically evaluate our algorithms scalability in terms of the number of agents |M|, the number of shared resources |Ω|, and the varied lengths of global time bτ during which agents may enter and exit the system.",
                "The repairshop problem is a simple parameterized MDP adopting the metaphor of a vehicular repair shop.",
                "Agents in the repair shop are mechanics with a number of independent tasks that yield reward only when completed.",
                "In our MDP model of this system, actions taken to advance through the state space are only allowed if the agent holds certain resources that are publicly available to the shop.",
                "These resources are in finite supply, and optimal policies for the shop will determine when each agent may hold the limited resources to take actions and earn individual rewards.",
                "Each task to be completed is associated with a single action, although the agent is required to repeat the action numerous times before completing the task and earning a reward.",
                "This model was parameterized in terms of the number of agents in the system, the number of different types of resources that could be linked to necessary actions, a global time during which agents are allowed to arrive and depart, and a maximum length for the number of time steps an agent may remain in the system.",
                "All datapoints in our experiments were obtained with 20 evaluations using CPLEX to solve the MILPs on a Pentium4 computer with 2Gb of RAM.",
                "Trials were conducted on both the static and the dynamic version of the resourcescheduling problem, as defined earlier.",
                "Figure 3 shows the runtime and policy value for independent modifications to the parameter set.",
                "The top row shows how the solution time for the MILP scales as we increase the number of agents |M|, the global time horizon bτ, and the number of resources |Ω|.",
                "Increasing the number of agents leads to exponential complexity scaling, which is to be expected for an NP-complete problem.",
                "However, increasing the global time limit bτ or the total number of resource types |Ω|-while holding the number of agents constantdoes not lead to decreased performance.",
                "This occurs because the problems get easier as they become under-constrained, which is also a common phenomenon for NP-complete problems.",
                "We also observe that the solution to the dynamic version of the problem can often be computed much faster than the static version.",
                "The bottom row of Figure 3 shows the joint policy value of the policies that correspond to the computed optimal resource-allocation schedules.",
                "We can observe that the dynamic version yields higher reward (as expected, since the reward for the dynamic version is always no less than the reward of the static version).",
                "We should point out that these graphs should not be viewed as a measure of performance of two different algorithms (both algorithms produce optimal solutions but to different problems), but rather as observations about how the quality of optimal solutions change as more flexibility is allowed in the reallocation of resources.",
                "Figure 4 shows runtime and policy value for trials in which common input variables are scaled together.",
                "This allows The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) 1225 2 4 6 8 10 10 −3 10 −2 10 −1 10 0 10 1 10 2 10 3 10 4 Number of Agents |M| CPUTime,sec |Ω| = 5, τ = 50 static dynamic 50 100 150 200 10 −2 10 −1 10 0 10 1 10 2 10 3 Global Time Boundary τ CPUTime,sec |M| = 5, |Ω| = 5 static dynamic 10 20 30 40 50 10 −2 10 −1 10 0 10 1 10 2 Number of Resources |Ω| CPUTime,sec |M| = 5, τ = 50 static dynamic 2 4 6 8 10 200 400 600 800 1000 1200 1400 1600 Number of Agents |M| Value |Ω| = 5, τ = 50 static dynamic 50 100 150 200 400 500 600 700 800 900 1000 1100 1200 1300 1400 Global Time Boundary τ Value |M| = 5, |Ω| = 5 static dynamic 10 20 30 40 50 500 600 700 800 900 1000 1100 1200 1300 1400 Number of Resources |Ω| Value |M| = 5, τ = 50 static dynamic Figure 3: Evaluation of our MILP for variable numbers of agents (column 1), lengths of global-time window (column 2), and numbers of resource types (column 3).",
                "Top row shows CPU time, and bottom row shows the joint reward of agents MDP policies.",
                "Error bars show the 1st and 3rd quartiles (25% and 75%). 2 4 6 8 10 10 −3 10 −2 10 −1 10 0 10 1 10 2 10 3 Number of Agents |M| CPUTime,sec τ = 10|M| static dynamic 2 4 6 8 10 10 −3 10 −2 10 −1 10 0 10 1 10 2 10 3 10 4 Number of Agents |M| CPUTime,sec |Ω| = 2|M| static dynamic 2 4 6 8 10 10 −3 10 −2 10 −1 10 0 10 1 10 2 10 3 10 4 Number of Agents |M| CPUTime,sec |Ω| = 5|M| static dynamic 2 4 6 8 10 200 400 600 800 1000 1200 1400 1600 1800 2000 2200 Number of Agents |M| Value τ = 10|M| static dynamic 2 4 6 8 10 200 400 600 800 1000 1200 1400 1600 1800 2000 Number of Agents |M| Value |Ω| = 2|M| static dynamic 2 4 6 8 10 0 500 1000 1500 2000 2500 Number of Agents |M| Value |Ω| = 5|M| static dynamic Figure 4: Evaluation of our MILP using correlated input variables.",
                "The left column tracks the performance and CPU time as the number of agents and global-time window increase together (bτ = 10|M|).",
                "The middle and the right column track the performance and CPU time as the number of resources and the number of agents increase together as |Ω| = 2|M| and |Ω| = 5|M|, respectively.",
                "Error bars show the 1st and 3rd quartiles (25% and 75%). 1226 The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) us to explore domains where the total number of agents scales proportionally to the total number of resource types or the global time horizon, while keeping constant the average agent density (per unit of global time) or the average number of resources per agent (which commonly occurs in real-life applications).",
                "Overall, we believe that these experimental results indicate that our MILP formulation can be used to effectively solve resource-scheduling problems of nontrivial size. 6.",
                "DISCUSSION AND CONCLUSIONS Throughout the paper, we have made a number of assumptions in our model and solution algorithm; we discuss their implications below. • Continual execution.",
                "We assume that once an agent stops executing its MDP (transitions into state sf ), it exits the system and cannot return.",
                "It is easy to relax this assumption for domains where agents MDPs can be paused and restarted.",
                "All that is required is to include an additional pause action which transitions from a given state back to itself, and has zero reward. • Indifference to start time.",
                "We used a reward model where agents rewards depend only on the time horizon of their MDPs and not the global start time.",
                "This is a consequence of our MDP-augmentation procedure from Section 4.1.",
                "It is easy to extend the model so that the agents incur an explicit penalty for idling by assigning a non-zero negative reward to the start state sb . • Binary resource requirements.",
                "For simplicity, we have assumed that resource costs are binary: ϕm(a, ω) = {0, 1}, but our results generalize in a straightforward manner to non-binary resource mappings, analogously to the procedure used in [5]. • Cooperative agents.",
                "The optimization procedure discussed in this paper was developed in the context of cooperative agents, but it can also be used to design a mechanism for scheduling resources among self-interested agents.",
                "This optimization procedure can be embedded in a VickreyClarke-Groves auction, completely analogously to the way it was done in [7].",
                "In fact, all the results of [7] about the properties of the auction and information privacy directly carry over to the scheduling domain discussed in this paper, requiring only slight modifications to deal with finitehorizon MDPs. • Known, deterministic arrival and departure times.",
                "Finally, we have assumed that agents arrival and departure times (τa m and τd m) are deterministic and known a priori.",
                "This assumption is fundamental to our solution method.",
                "While there are many domains where this assumption is valid, in many cases agents arrive and depart dynamically and their arrival and departure times can only be predicted probabilistically, leading to online resource-allocation problems.",
                "In particular, in the case of self-interested agents, this becomes an interesting version of an online-mechanism-design problem [11, 12].",
                "In summary, we have presented an MILP formulation for the combinatorial resource-scheduling problem where agents values for possible resource assignments are defined by finitehorizon MDPs.",
                "This result extends previous work ([6, 7]) on static one-shot resource allocation under MDP-induced preferences to resource-scheduling problems with a temporal aspect.",
                "As such, this work takes a step in the direction of designing an online mechanism for agents with combinatorial resource preferences induced by stochastic planning problems.",
                "Relaxing the assumption about deterministic arrival and departure times of the agents is a focus of our future work.",
                "We would like to thank the anonymous reviewers for their insightful comments and suggestions. 7.",
                "REFERENCES [1] E. Altman and A. Shwartz.",
                "Adaptive control of constrained Markov chains: Criteria and policies.",
                "Annals of Operations Research, special issue on Markov Decision Processes, 28:101-134, 1991. [2] R. Bellman.",
                "Dynamic Programming.",
                "Princeton University Press, 1957. [3] C. Boutilier.",
                "Solving concisely expressed combinatorial auction problems.",
                "In Proc. of AAAI-02, pages 359-366, 2002. [4] C. Boutilier and H. H. Hoos.",
                "Bidding languages for combinatorial auctions.",
                "In Proc. of IJCAI-01, pages 1211-1217, 2001. [5] D. Dolgov.",
                "Integrated Resource Allocation and Planning in Stochastic Multiagent Environments.",
                "PhD thesis, Computer Science Department, University of Michigan, February 2006. [6] D. A. Dolgov and E. H. Durfee.",
                "Optimal resource allocation and policy formulation in loosely-coupled Markov decision processes.",
                "In Proc. of ICAPS-04, pages 315-324, June 2004. [7] D. A. Dolgov and E. H. Durfee.",
                "Computationally efficient combinatorial auctions for resource allocation in weakly-coupled MDPs.",
                "In Proc. of AAMAS-05, New York, NY, USA, 2005.",
                "ACM Press. [8] D. A. Dolgov and E. H. Durfee.",
                "Resource allocation among agents with preferences induced by factored MDPs.",
                "In Proc. of AAMAS-06, 2006. [9] K. Larson and T. Sandholm.",
                "Mechanism design and deliberative agents.",
                "In Proc. of AAMAS-05, pages 650-656, New York, NY, USA, 2005.",
                "ACM Press. [10] N. Nisan.",
                "Bidding and allocation in combinatorial auctions.",
                "In Electronic Commerce, 2000. [11] D. C. Parkes and S. Singh.",
                "An MDP-based approach to Online Mechanism Design.",
                "In Proc. of the Seventeenths Annual Conference on Neural Information Processing Systems (NIPS-03), 2003. [12] D. C. Parkes, S. Singh, and D. Yanovsky.",
                "Approximately efficient online mechanism design.",
                "In Proc. of the Eighteenths Annual Conference on Neural Information Processing Systems (NIPS-04), 2004. [13] M. L. Puterman.",
                "Markov Decision Processes.",
                "John Wiley & Sons, New York, 1994. [14] M. H. Rothkopf, A. Pekec, and R. M. Harstad.",
                "Computationally manageable combinational auctions.",
                "Management Science, 44(8):1131-1147, 1998. [15] T. Sandholm.",
                "An algorithm for optimal winner determination in combinatorial auctions.",
                "In Proc. of IJCAI-99, pages 542-547, San Francisco, CA, USA, 1999.",
                "Morgan Kaufmann Publishers Inc.",
                "The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) 1227"
            ],
            "original_annotated_samples": [],
            "translated_annotated_samples": [],
            "translated_text": "",
            "candidates": [],
            "error": []
        }
    }
}