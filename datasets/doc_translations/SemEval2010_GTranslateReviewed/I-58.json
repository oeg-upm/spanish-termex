{
    "id": "I-58",
    "original_text": "An Efficient Heuristic Approach for Security Against Multiple Adversaries Praveen Paruchuri, Jonathan P. Pearce, Milind Tambe, Fernando Ordonez University of Southern California Los Angeles, CA 90089 {paruchur, jppearce, tambe, fordon}@usc.edu Sarit Kraus Bar-Ilan University Ramat-Gan 52900, Israel sarit@cs.biu.ac.il ABSTRACT In adversarial multiagent domains, security, commonly defined as the ability to deal with intentional threats from other agents, is a critical issue. This paper focuses on domains where these threats come from unknown adversaries. These domains can be modeled as Bayesian games; much work has been done on finding equilibria for such games. However, it is often the case in multiagent security domains that one agent can commit to a mixed strategy which its adversaries observe before choosing their own strategies. In this case, the agent can maximize reward by finding an optimal strategy, without requiring equilibrium. Previous work has shown this problem of optimal strategy selection to be NP-hard. Therefore, we present a heuristic called ASAP, with three key advantages to address the problem. First, ASAP searches for the highest-reward strategy, rather than a Bayes-Nash equilibrium, allowing it to find feasible strategies that exploit the natural first-mover advantage of the game. Second, it provides strategies which are simple to understand, represent, and implement. Third, it operates directly on the compact, Bayesian game representation, without requiring conversion to normal form. We provide an efficient Mixed Integer Linear Program (MILP) implementation for ASAP, along with experimental results illustrating significant speedups and higher rewards over other approaches. Categories and Subject Descriptors I.2.11 [Computing Methodologies]: Artificial Intelligence: Distributed Artificial Intelligence - Intelligent Agents General Terms Security, Design, Theory 1. INTRODUCTION In many multiagent domains, agents must act in order to provide security against attacks by adversaries. A common issue that agents face in such security domains is uncertainty about the adversaries they may be facing. For example, a security robot may need to make a choice about which areas to patrol, and how often [16]. However, it will not know in advance exactly where a robber will choose to strike. A team of unmanned aerial vehicles (UAVs) [1] monitoring a region undergoing a humanitarian crisis may also need to choose a patrolling policy. They must make this decision without knowing in advance whether terrorists or other adversaries may be waiting to disrupt the mission at a given location. It may indeed be possible to model the motivations of types of adversaries the agent or agent team is likely to face in order to target these adversaries more closely. However, in both cases, the security robot or UAV team will not know exactly which kinds of adversaries may be active on any given day. A common approach for choosing a policy for agents in such scenarios is to model the scenarios as Bayesian games. A Bayesian game is a game in which agents may belong to one or more types; the type of an agent determines its possible actions and payoffs. The distribution of adversary types that an agent will face may be known or inferred from historical data. Usually, these games are analyzed according to the solution concept of a Bayes-Nash equilibrium, an extension of the Nash equilibrium for Bayesian games. However, in many settings, a Nash or Bayes-Nash equilibrium is not an appropriate solution concept, since it assumes that the agents strategies are chosen simultaneously [5]. In some settings, one player can (or must) commit to a strategy before the other players choose their strategies. These scenarios are known as Stackelberg games [6]. In a Stackelberg game, a leader commits to a strategy first, and then a follower (or group of followers) selfishly optimize their own rewards, considering the action chosen by the leader. For example, the security agent (leader) must first commit to a strategy for patrolling various areas. This strategy could be a mixed strategy in order to be unpredictable to the robbers (followers). The robbers, after observing the pattern of patrols over time, can then choose their strategy (which location to rob). Often, the leader in a Stackelberg game can attain a higher reward than if the strategies were chosen simultaneously. To see the advantage of being the leader in a Stackelberg game, consider a simple game with the payoff table as shown in Table 1. The leader is the row player and the follower is the column player. Here, the leaders payoff is listed first. 1 2 3 1 5,5 0,0 3,10 2 0,0 2,2 5,0 Table 1: Payoff table for example normal form game. The only Nash equilibrium for this game is when the leader plays 2 and the follower plays 2 which gives the leader a payoff of 2. 311 978-81-904262-7-5 (RPS) c 2007 IFAAMAS However, if the leader commits to a uniform mixed strategy of playing 1 and 2 with equal (0.5) probability, the followers best response is to play 3 to get an expected payoff of 5 (10 and 0 with equal probability). The leaders payoff would then be 4 (3 and 5 with equal probability). In this case, the leader now has an incentive to deviate and choose a pure strategy of 2 (to get a payoff of 5). However, this would cause the follower to deviate to strategy 2 as well, resulting in the Nash equilibrium. Thus, by committing to a strategy that is observed by the follower, and by avoiding the temptation to deviate, the leader manages to obtain a reward higher than that of the best Nash equilibrium. The problem of choosing an optimal strategy for the leader to commit to in a Stackelberg game is analyzed in [5] and found to be NP-hard in the case of a Bayesian game with multiple types of followers. Thus, efficient heuristic techniques for choosing highreward strategies in these games is an important open issue. Methods for finding optimal leader strategies for non-Bayesian games [5] can be applied to this problem by converting the Bayesian game into a normal-form game by the Harsanyi transformation [8]. If, on the other hand, we wish to compute the highest-reward Nash equilibrium, new methods using mixed-integer linear programs (MILPs) [17] may be used, since the highest-reward Bayes-Nash equilibrium is equivalent to the corresponding Nash equilibrium in the transformed game. However, by transforming the game, the compact structure of the Bayesian game is lost. In addition, since the Nash equilibrium assumes a simultaneous choice of strategies, the advantages of being the leader are not considered. This paper introduces an efficient heuristic method for approximating the optimal leader strategy for security domains, known as ASAP (Agent Security via Approximate Policies). This method has three key advantages. First, it directly searches for an optimal strategy, rather than a Nash (or Bayes-Nash) equilibrium, thus allowing it to find high-reward non-equilibrium strategies like the one in the above example. Second, it generates policies with a support which can be expressed as a uniform distribution over a multiset of fixed size as proposed in [12]. This allows for policies that are simple to understand and represent [12], as well as a tunable parameter (the size of the multiset) that controls the simplicity of the policy. Third, the method allows for a Bayes-Nash game to be expressed compactly without conversion to a normal-form game, allowing for large speedups over existing Nash methods such as [17] and [11]. The rest of the paper is organized as follows. In Section 2 we fully describe the patrolling domain and its properties. Section 3 introduces the Bayesian game, the Harsanyi transformation, and existing methods for finding an optimal leaders strategy in a Stackelberg game. Then, in Section 4 the ASAP algorithm is presented for normal-form games, and in Section 5 we show how it can be adapted to the structure of Bayesian games with uncertain adversaries. Experimental results showing higher reward and faster policy computation over existing Nash methods are shown in Section 6, and we conclude with a discussion of related work in Section 7. 2. THE PATROLLING DOMAIN In most security patrolling domains, the security agents (like UAVs [1] or security robots [16]) cannot feasibly patrol all areas all the time. Instead, they must choose a policy by which they patrol various routes at different times, taking into account factors such as the likelihood of crime in different areas, possible targets for crime, and the security agents own resources (number of security agents, amount of available time, fuel, etc.). It is usually beneficial for this policy to be nondeterministic so that robbers cannot safely rob certain locations, knowing that they will be safe from the security agents [14]. To demonstrate the utility of our algorithm, we use a simplified version of such a domain, expressed as a game. The most basic version of our game consists of two players: the security agent (the leader) and the robber (the follower) in a world consisting of m houses, 1 . . . m. The security agents set of pure strategies consists of possible routes of d houses to patrol (in an order). The security agent can choose a mixed strategy so that the robber will be unsure of exactly where the security agent may patrol, but the robber will know the mixed strategy the security agent has chosen. For example, the robber can observe over time how often the security agent patrols each area. With this knowledge, the robber must choose a single house to rob. We assume that the robber generally takes a long time to rob a house. If the house chosen by the robber is not on the security agents route, then the robber successfully robs it. Otherwise, if it is on the security agents route, then the earlier the house is on the route, the easier it is for the security agent to catch the robber before he finishes robbing it. We model the payoffs for this game with the following variables: • vl,x: value of the goods in house l to the security agent. • vl,q: value of the goods in house l to the robber. • cx: reward to the security agent of catching the robber. • cq: cost to the robber of getting caught. • pl: probability that the security agent can catch the robber at the lth house in the patrol (pl < pl ⇐⇒ l < l). The security agents set of possible pure strategies (patrol routes) is denoted by X and includes all d-tuples i =< w1, w2, ..., wd > with w1 . . . wd = 1 . . . m where no two elements are equal (the agent is not allowed to return to the same house). The robbers set of possible pure strategies (houses to rob) is denoted by Q and includes all integers j = 1 . . . m. The payoffs (security agent, robber) for pure strategies i, j are: • −vl,x, vl,q, for j = l /∈ i. • plcx +(1−pl)(−vl,x), −plcq +(1−pl)(vl,q), for j = l ∈ i. With this structure it is possible to model many different types of robbers who have differing motivations; for example, one robber may have a lower cost of getting caught than another, or may value the goods in the various houses differently. If the distribution of different robber types is known or inferred from historical data, then the game can be modeled as a Bayesian game [6]. 3. BAYESIAN GAMES A Bayesian game contains a set of N agents, and each agent n must be one of a given set of types θn. For our patrolling domain, we have two agents, the security agent and the robber. θ1 is the set of security agent types and θ2 is the set of robber types. Since there is only one type of security agent, θ1 contains only one element. During the game, the robber knows its type but the security agent does not know the robbers type. For each agent (the security agent or the robber) n, there is a set of strategies σn and a utility function un : θ1 × θ2 × σ1 × σ2 → . A Bayesian game can be transformed into a normal-form game using the Harsanyi transformation [8]. Once this is done, new, linear-program (LP)-based methods for finding high-reward strategies for normal-form games [5] can be used to find a strategy in the transformed game; this strategy can then be used for the Bayesian game. While methods exist for finding Bayes-Nash equilibria directly, without the Harsanyi transformation [10], they find only a 312 The Sixth Intl. Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) single equilibrium in the general case, which may not be of high reward. Recent work [17] has led to efficient mixed-integer linear program techniques to find the best Nash equilibrium for a given agent. However, these techniques do require a normal-form game, and so to compare the policies given by ASAP against the optimal policy, as well as against the highest-reward Nash equilibrium, we must apply these techniques to the Harsanyi-transformed matrix. The next two subsections elaborate on how this is done. 3.1 Harsanyi Transformation The first step in solving Bayesian games is to apply the Harsanyi transformation [8] that converts the Bayesian game into a normal form game. Given that the Harsanyi transformation is a standard concept in game theory, we explain it briefly through a simple example in our patrolling domain without introducing the mathematical formulations. Let us assume there are two robber types a and b in the Bayesian game. Robber a will be active with probability α, and robber b will be active with probability 1 − α. The rules described in Section 2 allow us to construct simple payoff tables. Assume that there are two houses in the world (1 and 2) and hence there are two patrol routes (pure strategies) for the agent: {1,2} and {2,1}. The robber can rob either house 1 or house 2 and hence he has two strategies (denoted as 1l, 2l for robber type l). Since there are two types assumed (denoted as a and b), we construct two payoff tables (shown in Table 2) corresponding to the security agent playing a separate game with each of the two robber types with probabilities α and 1 − α. First, consider robber type a. Borrowing the notation from the domain section, we assign the following values to the variables: v1,x = v1,q = 3/4, v2,x = v2,q = 1/4, cx = 1/2, cq = 1, p1 = 1, p2 = 1/2. Using these values we construct a base payoff table as the payoff for the game against robber type a. For example, if the security agent chooses route {1,2} when robber a is active, and robber a chooses house 1, the robber receives a reward of -1 (for being caught) and the agent receives a reward of 0.5 for catching the robber. The payoffs for the game against robber type b are constructed using different values. Security agent: {1,2} {2,1} Robber a 1a -1, .5 -.375, .125 2a -.125, -.125 -1, .5 Robber b 1b -.9, .6 -.275, .225 2b -.025, -.025 -.9, .6 Table 2: Payoff tables: Security Agent vs Robbers a and b Using the Harsanyi technique involves introducing a chance node, that determines the robbers type, thus transforming the security agents incomplete information regarding the robber into imperfect information [3]. The Bayesian equilibrium of the game is then precisely the Nash equilibrium of the imperfect information game. The transformed, normal-form game is shown in Table 3. In the transformed game, the security agent is the column player, and the set of all robber types together is the row player. Suppose that robber type a robs house 1 and robber type b robs house 2, while the security agent chooses patrol {1,2}. Then, the security agent and the robber receive an expected payoff corresponding to their payoffs from the agent encountering robber a at house 1 with probability α and robber b at house 2 with probability 1 − α. 3.2 Finding an Optimal Strategy Although a Nash equilibrium is the standard solution concept for games in which agents choose strategies simultaneously, in our security domain, the security agent (the leader) can gain an advantage by committing to a mixed strategy in advance. Since the followers (the robbers) will know the leaders strategy, the optimal response for the followers will be a pure strategy. Given the common assumption, taken in [5], in the case where followers are indifferent, they will choose the strategy that benefits the leader, there must exist a guaranteed optimal strategy for the leader [5]. From the Bayesian game in Table 2, we constructed the Harsanyi transformed bimatrix in Table 3. The strategies for each player (security agent or robber) in the transformed game correspond to all combinations of possible strategies taken by each of that players types. Therefore, we denote X = σθ1 1 = σ1 and Q = σθ2 2 as the index sets of the security agent and robbers pure strategies respectively, with R and C as the corresponding payoff matrices. Rij is the reward of the security agent and Cij is the reward of the robbers when the security agent takes pure strategy i and the robbers take pure strategy j. A mixed strategy for the security agent is a probability distribution over its set of pure strategies and will be represented by a vector x = (px1, px2, . . . , px|X|), where pxi ≥ 0 and P pxi = 1. Here, pxi is the probability that the security agent will choose its ith pure strategy. The optimal mixed strategy for the security agent can be found in time polynomial in the number of rows in the normal form game using the following linear program formulation from [5]. For every possible pure strategy j by the follower (the set of all robber types), max P i∈X pxiRij s.t. ∀j ∈ Q, P i∈σ1 pxiCij ≥ P i∈σ1 pxiCij P i∈X pxi = 1 ∀i∈X , pxi >= 0 (1) Then, for all feasible follower strategies j, choose the one that maximizes P i∈X pxiRij, the reward for the security agent (leader). The pxi variables give the optimal strategy for the security agent. Note that while this method is polynomial in the number of rows in the transformed, normal-form game, the number of rows increases exponentially with the number of robber types. Using this method for a Bayesian game thus requires running |σ2||θ2| separate linear programs. This is no surprise, since finding the leaders optimal strategy in a Bayesian Stackelberg game is NP-hard [5]. 4. HEURISTIC APPROACHES Given that finding the optimal strategy for the leader is NP-hard, we provide a heuristic approach. In this heuristic we limit the possible mixed strategies of the leader to select actions with probabilities that are integer multiples of 1/k for a predetermined integer k. Previous work [14] has shown that strategies with high entropy are beneficial for security applications when opponents utilities are completely unknown. In our domain, if utilities are not considered, this method will result in uniform-distribution strategies. One advantage of such strategies is that they are compact to represent (as fractions) and simple to understand; therefore they can be efficiently implemented by real organizations. We aim to maintain the advantage provided by simple strategies for our security application problem, incorporating the effect of the robbers rewards on the security agents rewards. Thus, the ASAP heuristic will produce strategies which are k-uniform. A mixed strategy is denoted k-uniform if it is a uniform distribution on a multiset S of pure strategies with |S| = k. A multiset is a set whose elements may be repeated multiple times; thus, for example, the mixed strategy corresponding to the multiset {1, 1, 2} would take strategy 1 The Sixth Intl. Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) 313 {1,2} {2,1} {1a, 1b} −1α − .9(1 − α), .5α + .6(1 − α) −.375α − .275(1 − α), .125α + .225(1 − α) {1a, 2b} −1α − .025(1 − α), .5α − .025(1 − α) −.375α − .9(1 − α), .125α + .6(1 − α) {2a, 1b} −.125α − .9(1 − α), −.125α + .6(1 − α) −1α − .275(1 − α), .5α + .225(1 − α) {2a, 2b} −.125α − .025(1 − α), −.125α − .025(1 − α) −1α − .9(1 − α), .5α + .6(1 − α) Table 3: Harsanyi Transformed Payoff Table with probability 2/3 and strategy 2 with probability 1/3. ASAP allows the size of the multiset to be chosen in order to balance the complexity of the strategy reached with the goal that the identified strategy will yield a high reward. Another advantage of the ASAP heuristic is that it operates directly on the compact Bayesian representation, without requiring the Harsanyi transformation. This is because the different follower (robber) types are independent of each other. Hence, evaluating the leader strategy against a Harsanyi-transformed game matrix is equivalent to evaluating against each of the game matrices for the individual follower types. This independence property is exploited in ASAP to yield a decomposition scheme. Note that the LP method introduced by [5] to compute optimal Stackelberg policies is unlikely to be decomposable into a small number of games as it was shown to be NP-hard for Bayes-Nash problems. Finally, note that ASAP requires the solution of only one optimization problem, rather than solving a series of problems as in the LP method of [5]. For a single follower type, the algorithm works the following way. Given a particular k, for each possible mixed strategy x for the leader that corresponds to a multiset of size k, evaluate the leaders payoff from x when the follower plays a reward-maximizing pure strategy. We then take the mixed strategy with the highest payoff. We need only to consider the reward-maximizing pure strategies of the followers (robbers), since for a given fixed strategy x of the security agent, each robber type faces a problem with fixed linear rewards. If a mixed strategy is optimal for the robber, then so are all the pure strategies in the support of that mixed strategy. Note also that because we limit the leaders strategies to take on discrete values, the assumption from Section 3.2 that the followers will break ties in the leaders favor is not significant, since ties will be unlikely to arise. This is because, in domains where rewards are drawn from any random distribution, the probability of a follower having more than one pure optimal response to a given leader strategy approaches zero, and the leader will have only a finite number of possible mixed strategies. Our approach to characterize the optimal strategy for the security agent makes use of properties of linear programming. We briefly outline these results here for completeness, for detailed discussion and proofs see one of many references on the topic, such as [2]. Every linear programming problem, such as: max cT x | Ax = b, x ≥ 0, has an associated dual linear program, in this case: min bT y | AT y ≥ c. These primal/dual pairs of problems satisfy weak duality: For any x and y primal and dual feasible solutions respectively, cT x ≤ bT y. Thus a pair of feasible solutions is optimal if cT x = bT y, and the problems are said to satisfy strong duality. In fact if a linear program is feasible and has a bounded optimal solution, then the dual is also feasible and there is a pair x∗ , y∗ that satisfies cT x∗ = bT y∗ . These optimal solutions are characterized with the following optimality conditions (as defined in [2]): • primal feasibility: Ax = b, x ≥ 0 • dual feasibility: AT y ≥ c • complementary slackness: xi(AT y − c)i = 0 for all i. Note that this last condition implies that cT x = xT AT y = bT y, which proves optimality for primal dual feasible solutions x and y. In the following subsections, we first define the problem in its most intuititive form as a mixed-integer quadratic program (MIQP), and then show how this problem can be converted into a mixedinteger linear program (MILP). 4.1 Mixed-Integer Quadratic Program We begin with the case of a single type of follower. Let the leader be the row player and the follower the column player. We denote by x the vector of strategies of the leader and q the vector of strategies of the follower. We also denote X and Q the index sets of the leader and followers pure strategies, respectively. The payoff matrices R and C correspond to: Rij is the reward of the leader and Cij is the reward of the follower when the leader takes pure strategy i and the follower takes pure strategy j. Let k be the size of the multiset. We first fix the policy of the leader to some k-uniform policy x. The value xi is the number of times pure strategy i is used in the k-uniform policy, which is selected with probability xi/k. We formulate the optimization problem the follower solves to find its optimal response to x as the following linear program: max X j∈Q X i∈X 1 k Cijxi qj s.t. P j∈Q qj = 1 q ≥ 0. (2) The objective function maximizes the followers expected reward given x, while the constraints make feasible any mixed strategy q for the follower. The dual to this linear programming problem is the following: min a s.t. a ≥ X i∈X 1 k Cijxi j ∈ Q. (3) From strong duality and complementary slackness we obtain that the followers maximum reward value, a, is the value of every pure strategy with qj > 0, that is in the support of the optimal mixed strategy. Therefore each of these pure strategies is optimal. Optimal solutions to the followers problem are characterized by linear programming optimality conditions: primal feasibility constraints in (2), dual feasibility constraints in (3), and complementary slackness qj a − X i∈X 1 k Cijxi ! = 0 j ∈ Q. These conditions must be included in the problem solved by the leader in order to consider only best responses by the follower to the k-uniform policy x. 314 The Sixth Intl. Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) The leader seeks the k-uniform solution x that maximizes its own payoff, given that the follower uses an optimal response q(x). Therefore the leader solves the following integer problem: max X i∈X X j∈Q 1 k Rijq(x)j xi s.t. P i∈X xi = k xi ∈ {0, 1, . . . , k}. (4) Problem (4) maximizes the leaders reward with the followers best response (qj for fixed leaders policy x and hence denoted q(x)j) by selecting a uniform policy from a multiset of constant size k. We complete this problem by including the characterization of q(x) through linear programming optimality conditions. To simplify writing the complementary slackness conditions, we will constrain q(x) to be only optimal pure strategies by just considering integer solutions of q(x). The leaders problem becomes: maxx,q X i∈X X j∈Q 1 k Rijxiqj s.t. P i xi = kP j∈Q qj = 1 0 ≤ (a − P i∈X 1 k Cijxi) ≤ (1 − qj)M xi ∈ {0, 1, ...., k} qj ∈ {0, 1}. (5) Here, the constant M is some large number. The first and fourth constraints enforce a k-uniform policy for the leader, and the second and fifth constraints enforce a feasible pure strategy for the follower. The third constraint enforces dual feasibility of the followers problem (leftmost inequality) and the complementary slackness constraint for an optimal pure strategy q for the follower (rightmost inequality). In fact, since only one pure strategy can be selected by the follower, say qh = 1, this last constraint enforces that a = P i∈X 1 k Cihxi imposing no additional constraint for all other pure strategies which have qj = 0. We conclude this subsection noting that Problem (5) is an integer program with a non-convex quadratic objective in general, as the matrix R need not be positive-semi-definite. Efficient solution methods for non-linear, non-convex integer problems remains a challenging research question. In the next section we show a reformulation of this problem as a linear integer programming problem, for which a number of efficient commercial solvers exist. 4.2 Mixed-Integer Linear Program We can linearize the quadratic program of Problem 5 through the change of variables zij = xiqj, obtaining the following problem maxq,z P i∈X P j∈Q 1 k Rijzij s.t. P i∈X P j∈Q zij = k P j∈Q zij ≤ k kqj ≤ P i∈X zij ≤ k P j∈Q qj = 1 0 ≤ (a − P i∈X 1 k Cij( P h∈Q zih)) ≤ (1 − qj)M zij ∈ {0, 1, ...., k} qj ∈ {0, 1} (6) PROPOSITION 1. Problems (5) and (6) are equivalent. Proof: Consider x, q a feasible solution of (5). We will show that q, zij = xiqj is a feasible solution of (6) of same objective function value. The equivalence of the objective functions, and constraints 4, 6 and 7 of (6) are satisfied by construction. The fact that P j∈Q zij = xi as P j∈Q qj = 1 explains constraints 1, 2, and 5 of (6). Constraint 3 of (6) is satisfied because P i∈X zij = kqj. Let us now consider q, z feasible for (6). We will show that q and xi = P j∈Q zij are feasible for (5) with the same objective value. In fact all constraints of (5) are readily satisfied by construction. To see that the objectives match, notice that if qh = 1 then the third constraint in (6) implies that P i∈X zih = k, which means that zij = 0 for all i ∈ X and all j = h. Therefore, xiqj = X l∈Q zilqj = zihqj = zij. This last equality is because both are 0 when j = h. This shows that the transformation preserves the objective function value, completing the proof. Given this transformation to a mixed-integer linear program (MILP), we now show how we can apply our decomposition technique on the MILP to obtain significant speedups for Bayesian games with multiple follower types. 5. DECOMPOSITION FOR MULTIPLE ADVERSARIES The MILP developed in the previous section handles only one follower. Since our security scenario contains multiple follower (robber) types, we change the response function for the follower from a pure strategy into a weighted combination over various pure follower strategies where the weights are probabilities of occurrence of each of the follower types. 5.1 Decomposed MIQP To admit multiple adversaries in our framework, we modify the notation defined in the previous section to reason about multiple follower types. We denote by x the vector of strategies of the leader and ql the vector of strategies of follower l, with L denoting the index set of follower types. We also denote by X and Q the index sets of leader and follower ls pure strategies, respectively. We also index the payoff matrices on each follower l, considering the matrices Rl and Cl . Using this modified notation, we characterize the optimal solution of follower ls problem given the leaders k-uniform policy x, with the following optimality conditions: X j∈Q ql j = 1 al − X i∈X 1 k Cl ijxi ≥ 0 ql j(al − X i∈X 1 k Cl ijxi) = 0 ql j ≥ 0 Again, considering only optimal pure strategies for follower ls problem we can linearize the complementarity constraint above. We incorporate these constraints on the leaders problem that selects the optimal k-uniform policy. Therefore, given a priori probabilities pl , with l ∈ L of facing each follower, the leader solves the following problem: The Sixth Intl. Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) 315 maxx,q X i∈X X l∈L X j∈Q pl k Rl ijxiql j s.t. P i xi = kP j∈Q ql j = 1 0 ≤ (al − P i∈X 1 k Cl ijxi) ≤ (1 − ql j)M xi ∈ {0, 1, ...., k} ql j ∈ {0, 1}. (7) Problem (7) for a Bayesian game with multiple follower types is indeed equivalent to Problem (5) on the payoff matrix obtained from the Harsanyi transformation of the game. In fact, every pure strategy j in Problem (5) corresponds to a sequence of pure strategies jl, one for each follower l ∈ L. This means that qj = 1 if and only if ql jl = 1 for all l ∈ L. In addition, given the a priori probabilities pl of facing player l, the reward in the Harsanyi transformation payoff table is Rij = P l∈L pl Rl ijl . The same relation holds between C and Cl . These relations between a pure strategy in the equivalent normal form game and pure strategies in the individual games with each followers are key in showing these problems are equivalent. 5.2 Decomposed MILP We can linearize the quadratic programming problem 7 through the change of variables zl ij = xiql j, obtaining the following problem maxq,z P i∈X P l∈L P j∈Q pl k Rl ijzl ij s.t. P i∈X P j∈Q zl ij = k P j∈Q zl ij ≤ k kql j ≤ P i∈X zl ij ≤ k P j∈Q ql j = 1 0 ≤ (al − P i∈X 1 k Cl ij( P h∈Q zl ih)) ≤ (1 − ql j)M P j∈Q zl ij = P j∈Q z1 ij zl ij ∈ {0, 1, ...., k} ql j ∈ {0, 1} (8) PROPOSITION 2. Problems (7) and (8) are equivalent. Proof: Consider x, ql , al with l ∈ L a feasible solution of (7). We will show that ql , al , zl ij = xiql j is a feasible solution of (8) of same objective function value. The equivalence of the objective functions, and constraints 4, 7 and 8 of (8) are satisfied by construction. The fact that P j∈Q zl ij = xi as P j∈Q ql j = 1 explains constraints 1, 2, 5 and 6 of (8). Constraint 3 of (8) is satisfied because P i∈X zl ij = kql j. Lets now consider ql , zl , al feasible for (8). We will show that ql , al and xi = P j∈Q z1 ij are feasible for (7) with the same objective value. In fact all constraints of (7) are readily satisfied by construction. To see that the objectives match, notice for each l one ql j must equal 1 and the rest equal 0. Let us say that ql jl = 1, then the third constraint in (8) implies that P i∈X zl ijl = k, which means that zl ij = 0 for all i ∈ X and all j = jl. In particular this implies that xi = X j∈Q z1 ij = z1 ij1 = zl ijl , the last equality from constraint 6 of (8). Therefore xiql j = zl ijl ql j = zl ij. This last equality is because both are 0 when j = jl. Effectively, constraint 6 ensures that all the adversaries are calculating their best responses against a particular fixed policy of the agent. This shows that the transformation preserves the objective function value, completing the proof. We can therefore solve this equivalent linear integer program with efficient integer programming packages which can handle problems with thousands of integer variables. We implemented the decomposed MILP and the results are shown in the following section. 6. EXPERIMENTAL RESULTS The patrolling domain and the payoffs for the associated game are detailed in Sections 2 and 3. We performed experiments for this game in worlds of three and four houses with patrols consisting of two houses. The description given in Section 2 is used to generate a base case for both the security agent and robber payoff functions. The payoff tables for additional robber types are constructed and added to the game by adding a random distribution of varying size to the payoffs in the base case. All games are normalized so that, for each robber type, the minimum and maximum payoffs to the security agent and robber are 0 and 1, respectively. Using the data generated, we performed the experiments using four methods for generating the security agents strategy: • uniform randomization • ASAP • the multiple linear programs method from [5] (to find the true optimal strategy) • the highest reward Bayes-Nash equilibrium, found using the MIP-Nash algorithm [17] The last three methods were applied using CPLEX 8.1. Because the last two methods are designed for normal-form games rather than Bayesian games, the games were first converted using the Harsanyi transformation [8]. The uniform randomization method is simply choosing a uniform random policy over all possible patrol routes. We use this method as a simple baseline to measure the performance of our heuristics. We anticipated that the uniform policy would perform reasonably well since maximum-entropy policies have been shown to be effective in multiagent security domains [14]. The highest-reward Bayes-Nash equilibria were used in order to demonstrate the higher reward gained by looking for an optimal policy rather than an equilibria in Stackelberg games such as our security domain. Based on our experiments we present three sets of graphs to demonstrate (1) the runtime of ASAP compared to other common methods for finding a strategy, (2) the reward guaranteed by ASAP compared to other methods, and (3) the effect of varying the parameter k, the size of the multiset, on the performance of ASAP. In the first two sets of graphs, ASAP is run using a multiset of 80 elements; in the third set this number is varied. The first set of graphs, shown in Figure 1 shows the runtime graphs for three-house (left column) and four-house (right column) domains. Each of the three rows of graphs corresponds to a different randomly-generated scenario. The x-axis shows the number of robber types the security agent faces and the y-axis of the graph shows the runtime in seconds. All experiments that were not concluded in 30 minutes (1800 seconds) were cut off. The runtime for the uniform policy is always negligible irrespective of the number of adversaries and hence is not shown. The ASAP algorithm clearly outperforms the optimal, multipleLP method as well as the MIP-Nash algorithm for finding the highestreward Bayes-Nash equilibrium with respect to runtime. For a 316 The Sixth Intl. Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) Figure 1: Runtimes for various algorithms on problems of 3 and 4 houses. domain of three houses, the optimal method cannot reach a solution for more than seven robber types, and for four houses it cannot solve for more than six types within the cutoff time in any of the three scenarios. MIP-Nash solves for even fewer robber types within the cutoff time. On the other hand, ASAP runs much faster, and is able to solve for at least 20 adversaries for the three-house scenarios and for at least 12 adversaries in the four-house scenarios within the cutoff time. The runtime of ASAP does not increase strictly with the number of robber types for each scenario, but in general, the addition of more types increases the runtime required. The second set of graphs, Figure 2, shows the reward to the patrol agent given by each method for three scenarios in the three-house (left column) and four-house (right column) domains. This reward is the utility received by the security agent in the patrolling game, and not as a percentage of the optimal reward, since it was not possible to obtain the optimal reward as the number of robber types increased. The uniform policy consistently provides the lowest reward in both domains; while the optimal method of course produces the optimal reward. The ASAP method remains consistently close to the optimal, even as the number of robber types increases. The highest-reward Bayes-Nash equilibria, provided by the MIPNash method, produced rewards higher than the uniform method, but lower than ASAP. This difference clearly illustrates the gains in the patrolling domain from committing to a strategy as the leader in a Stackelberg game, rather than playing a standard Bayes-Nash strategy. The third set of graphs, shown in Figure 3 shows the effect of the multiset size on runtime in seconds (left column) and reward (right column), again expressed as the reward received by the security agent in the patrolling game, and not a percentage of the optimal Figure 2: Reward for various algorithms on problems of 3 and 4 houses. reward. Results here are for the three-house domain. The trend is that as as the multiset size is increased, the runtime and reward level both increase. Not surprisingly, the reward increases monotonically as the multiset size increases, but what is interesting is that there is relatively little benefit to using a large multiset in this domain. In all cases, the reward given by a multiset of 10 elements was within at least 96% of the reward given by an 80-element multiset. The runtime does not always increase strictly with the multiset size; indeed in one example (scenario 2 with 20 robber types), using a multiset of 10 elements took 1228 seconds, while using 80 elements only took 617 seconds. In general, runtime should increase since a larger multiset means a larger domain for the variables in the MILP, and thus a larger search space. However, an increase in the number of variables can sometimes allow for a policy to be constructed more quickly due to more flexibility in the problem. 7. SUMMARY AND RELATED WORK This paper focuses on security for agents patrolling in hostile environments. In these environments, intentional threats are caused by adversaries about whom the security patrolling agents have incomplete information. Specifically, we deal with situations where the adversaries actions and payoffs are known but the exact adversary type is unknown to the security agent. Agents acting in the real world quite frequently have such incomplete information about other agents. Bayesian games have been a popular choice to model such incomplete information games [3]. The Gala toolkit is one method for defining such games [9] without requiring the game to be represented in normal form via the Harsanyi transformation [8]; Galas guarantees are focused on fully competitive games. Much work has been done on finding optimal Bayes-Nash equilbria for The Sixth Intl. Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) 317 Figure 3: Reward for ASAP using multisets of 10, 30, and 80 elements subclasses of Bayesian games, finding single Bayes-Nash equilibria for general Bayesian games [10] or approximate Bayes-Nash equilibria [18]. Less attention has been paid to finding the optimal strategy to commit to in a Bayesian game (the Stackelberg scenario [15]). However, the complexity of this problem was shown to be NP-hard in the general case [5], which also provides algorithms for this problem in the non-Bayesian case. Therefore, we present a heuristic called ASAP, with three key advantages towards addressing this problem. First, ASAP searches for the highest reward strategy, rather than a Bayes-Nash equilibrium, allowing it to find feasible strategies that exploit the natural first-mover advantage of the game. Second, it provides strategies which are simple to understand, represent, and implement. Third, it operates directly on the compact, Bayesian game representation, without requiring conversion to normal form. We provide an efficient Mixed Integer Linear Program (MILP) implementation for ASAP, along with experimental results illustrating significant speedups and higher rewards over other approaches. Our k-uniform strategies are similar to the k-uniform strategies of [12]. While that work provides epsilon error-bounds based on the k-uniform strategies, their solution concept is still that of a Nash equilibrium, and they do not provide efficient algorithms for obtaining such k-uniform strategies. This contrasts with ASAP, where our emphasis is on a highly efficient heuristic approach that is not focused on equilibrium solutions. Finally the patrolling problem which motivated our work has recently received growing attention from the multiagent community due to its wide range of applications [4, 13]. However most of this work is focused on either limiting energy consumption involved in patrolling [7] or optimizing on criteria like the length of the path traveled [4, 13], without reasoning about any explicit model of an adversary[14]. Acknowledgments : This research is supported by the United States Department of Homeland Security through Center for Risk and Economic Analysis of Terrorism Events (CREATE). It is also supported by the Defense Advanced Research Projects Agency (DARPA), through the Department of the Interior, NBC, Acquisition Services Division, under Contract No. NBCHD030010. Sarit Kraus is also affiliated with UMIACS. 8. REFERENCES [1] R. W. Beard and T. McLain. Multiple UAV cooperative search under collision avoidance and limited range communication constraints. In IEEE CDC, 2003. [2] D. Bertsimas and J. Tsitsiklis. Introduction to Linear Optimization. Athena Scientific, 1997. [3] J. Brynielsson and S. Arnborg. Bayesian games for threat prediction and situation analysis. In FUSION, 2004. [4] Y. Chevaleyre. Theoretical analysis of multi-agent patrolling problem. In AAMAS, 2004. [5] V. Conitzer and T. Sandholm. Choosing the best strategy to commit to. In ACM Conference on Electronic Commerce, 2006. [6] D. Fudenberg and J. Tirole. Game Theory. MIT Press, 1991. [7] C. Gui and P. Mohapatra. Virtual patrol: A new power conservation design for surveillance using sensor networks. In IPSN, 2005. [8] J. C. Harsanyi and R. Selten. A generalized Nash solution for two-person bargaining games with incomplete information. Management Science, 18(5):80-106, 1972. [9] D. Koller and A. Pfeffer. Generating and solving imperfect information games. In IJCAI, pages 1185-1193, 1995. [10] D. Koller and A. Pfeffer. Representations and solutions for game-theoretic problems. Artificial Intelligence, 94(1):167-215, 1997. [11] C. Lemke and J. Howson. Equilibrium points of bimatrix games. Journal of the Society for Industrial and Applied Mathematics, 12:413-423, 1964. [12] R. J. Lipton, E. Markakis, and A. Mehta. Playing large games using simple strategies. In ACM Conference on Electronic Commerce, 2003. [13] A. Machado, G. Ramalho, J. D. Zucker, and A. Drougoul. Multi-agent patrolling: an empirical analysis on alternative architectures. In MABS, 2002. [14] P. Paruchuri, M. Tambe, F. Ordonez, and S. Kraus. Security in multiagent systems by policy randomization. In AAMAS, 2006. [15] T. Roughgarden. Stackelberg scheduling strategies. In ACM Symposium on TOC, 2001. [16] S. Ruan, C. Meirina, F. Yu, K. R. Pattipati, and R. L. Popp. Patrolling in a stochastic environment. In 10th Intl. Command and Control Research Symp., 2005. [17] T. Sandholm, A. Gilpin, and V. Conitzer. Mixed-integer programming methods for finding nash equilibria. In AAAI, 2005. [18] S. Singh, V. Soni, and M. Wellman. Computing approximate Bayes-Nash equilibria with tree-games of incomplete information. In ACM Conference on Electronic Commerce, 2004. 318 The Sixth Intl. Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07)",
    "original_translation": "Un enfoque heurístico eficiente para la seguridad contra los múltiples adversarios Praveen Paruchuri, Jonathan P. Pearce, Milind Tambe, Fernando Ordonez Universidad del Sur de California en Los Ángeles, CA 90089 {Paruchur, Jppearce, Tambe, Fordon}@usc.edu Sarit Kraus Bar-ilan University UniversityRamat-Gan 52900, Israel sarit@cs.biu.ac.il Resumen en dominios multiagentes adversos, seguridad, comúnmente definida como la capacidad de lidiar con las amenazas intencionales de otros agentes, es un problema crítico. Este documento se centra en dominios donde estas amenazas provienen de adversarios desconocidos. Estos dominios se pueden modelar como juegos bayesianos;Se ha trabajado mucho para encontrar equilibrios para tales juegos. Sin embargo, a menudo es el caso en los dominios de seguridad múltiples que un agente puede comprometerse con una estrategia mixta que sus adversarios observan antes de elegir sus propias estrategias. En este caso, el agente puede maximizar la recompensa al encontrar una estrategia óptima, sin requerir equilibrio. El trabajo anterior ha demostrado que este problema de la selección de estrategia óptima es NP-HARD. Por lo tanto, presentamos una heurística llamada ASAP, con tres ventajas clave para abordar el problema. Primero, ASAP busca la estrategia de mayor recompensa, en lugar de un equilibrio de Bayes-Nash, lo que le permite encontrar estrategias factibles que exploten la ventaja natural del primer movimiento del juego. En segundo lugar, proporciona estrategias que son fáciles de entender, representar e implementar. En tercer lugar, opera directamente en la representación compacta del juego bayesiano, sin requerir la conversión a la forma normal. Proporcionamos una implementación eficiente del Programa Lineal Integer Integer (MILP) para ASAP, junto con resultados experimentales que ilustran aceleraciones significativas y mayores recompensas sobre otros enfoques. Categorías y descriptores de sujetos I.2.11 [Metodologías de computación]: Inteligencia artificial: inteligencia artificial distribuida - Agentes inteligentes Términos generales Seguridad, Diseño, Teoría 1. Introducción En muchos dominios multiagentes, los agentes deben actuar para proporcionar seguridad contra los ataques por parte de los adversarios. Un problema común que enfrentan los agentes en tales dominios de seguridad es la incertidumbre sobre los adversarios que pueden enfrentar. Por ejemplo, un robot de seguridad puede necesitar tomar una decisión sobre qué áreas patrullar y con qué frecuencia [16]. Sin embargo, no sabrá de antemano exactamente dónde elegirá un ladrón para atacar. Un equipo de vehículos aéreos no tripulados (UAV) [1] que monitorea una región sometida a una crisis humanitaria también puede necesitar elegir una política de patrulla. Deben tomar esta decisión sin saber de antemano si los terroristas u otros adversarios pueden estar esperando para interrumpir la misión en un lugar determinado. De hecho, puede ser posible modelar las motivaciones de los tipos de adversarios que el agente o el equipo de agente enfrentarán para atacar a estos adversarios más de cerca. Sin embargo, en ambos casos, el robot de seguridad o el equipo de UAV no sabrán exactamente qué tipos de adversarios pueden estar activos en un día determinado. Un enfoque común para elegir una política para agentes en tales escenarios es modelar los escenarios como juegos bayesianos. Un juego bayesiano es un juego en el que los agentes pueden pertenecer a uno o más tipos;El tipo de agente determina sus posibles acciones y pagos. La distribución de tipos adversarios que enfrentará un agente puede ser conocido o inferido de datos históricos. Por lo general, estos juegos se analizan de acuerdo con el concepto de solución de un equilibrio de Bayes-Nash, una extensión del equilibrio de Nash para los Juegos Bayesianos. Sin embargo, en muchos entornos, un equilibrio de Nash o Bayes-Nash no es un concepto de solución apropiado, ya que supone que las estrategias de los agentes se eligen simultáneamente [5]. En algunos entornos, un jugador puede (o debe) comprometerse con una estrategia antes de que los otros jugadores elijan sus estrategias. Estos escenarios se conocen como Stackelberg Games [6]. En un juego de Stackelberg, un líder se compromete a una estrategia primero, y luego un seguidor (o grupo de seguidores) optimiza egoístamente sus propias recompensas, considerando la acción elegida por el líder. Por ejemplo, el agente de seguridad (líder) primero debe comprometerse con una estrategia para patrullar varias áreas. Esta estrategia podría ser una estrategia mixta para ser impredecible para los ladrones (seguidores). Los ladrones, después de observar el patrón de patrullas con el tiempo, pueden elegir su estrategia (qué ubicación robar). A menudo, el líder en un juego de Stackelberg puede lograr una recompensa más alta que si las estrategias se eligieran simultáneamente. Para ver la ventaja de ser el líder en un juego de Stackelberg, considere un juego simple con la tabla de pago como se muestra en la Tabla 1. El líder es el jugador de filas y el seguidor es el reproductor de columna. Aquí, el recompensa de los líderes se enumera primero.1 2 3 1 5,5 0,0 3,10 2 0,0 2,2 5,0 Tabla 1: Tabla de pago, por ejemplo, el juego de formulario normal. El único equilibrio de Nash para este juego es cuando el líder juega 2 y el seguidor juega 2, lo que le da al líder una recompensa de 2. 311 978-81-904262-7-5 (RPS) C 2007 Ifaamas, sin embargo, si el líder se compromete aUna estrategia mixta uniforme de jugar 1 y 2 con igual probabilidad (0.5), la mejor respuesta de los seguidores es jugar 3 para obtener un pago esperado de 5 (10 y 0 con igual probabilidad). El pago de los líderes sería 4 (3 y 5 con igual probabilidad). En este caso, el líder ahora tiene un incentivo para desviarse y elegir una estrategia pura de 2 (para obtener una recompensa de 5). Sin embargo, esto también haría que el seguidor se desvíe a la Estrategia 2, lo que resulta en el equilibrio de Nash. Por lo tanto, al comprometerse con una estrategia que observa el seguidor, y evitando la tentación de desviarse, el líder logra obtener una recompensa más alta que la del mejor equilibrio de Nash. El problema de elegir una estrategia óptima para el líder para comprometerse en un juego de Stackelberg se analiza en [5] y se encuentra que es NP-Hard en el caso de un juego bayesiano con múltiples tipos de seguidores. Por lo tanto, las técnicas heurísticas eficientes para elegir estrategias altas en estos juegos son un importante problema abierto. Los métodos para encontrar estrategias óptimas del líder para juegos no bayesianos [5] se pueden aplicar a este problema al convertir el juego bayesiano en un juego de forma normal por la transformación Harsanyi [8]. Si, por otro lado, deseamos calcular el equilibrio NASH de más alta recompensa, se pueden usar nuevos métodos que utilizan programas lineales de intemperie mixta (MILP) [17], ya que el equilibrio de Nash-Bayes-Nash es equivalente a los correspondientesEquilibrio de Nash en el juego transformado. Sin embargo, al transformar el juego, se pierde la estructura compacta del juego bayesiano. Además, dado que el equilibrio de Nash asume una elección simultánea de estrategias, no se consideran las ventajas de ser el líder. Este documento introduce un método heurístico eficiente para aproximar la estrategia óptima del líder para los dominios de seguridad, conocida lo antes posible (seguridad del agente a través de políticas aproximadas). Este método tiene tres ventajas clave. Primero, busca directamente una estrategia óptima, en lugar de un equilibrio de Nash (o Nash Bayes-Nash), lo que le permite encontrar estrategias sin equilibrio de alta recompensa como la del ejemplo anterior. En segundo lugar, genera políticas con un soporte que puede expresarse como una distribución uniforme sobre un multiset de tamaño fijo como se propuso en [12]. Esto permite políticas que son fáciles de comprender y representan [12], así como un parámetro sintonizable (el tamaño del Multiset) que controla la simplicidad de la política. En tercer lugar, el método permite que un juego de bayes-nash se exprese de manera compacta sin conversión a un juego de forma normal, lo que permite acelerar grandes sobre los métodos NASH existentes como [17] y [11]. El resto del documento está organizado de la siguiente manera. En la Sección 2 describimos completamente el dominio de patrulla y sus propiedades. La Sección 3 presenta el juego bayesiano, la transformación Harsanyi y los métodos existentes para encontrar una estrategia de líderes óptimos en un juego de Stackelberg. Luego, en la Sección 4, el algoritmo ASAP se presenta para juegos de forma normal, y en la Sección 5 mostramos cómo se puede adaptar a la estructura de los juegos bayesianos con adversarios inciertos. Los resultados experimentales que muestran una recompensa más alta y un cálculo político más rápido sobre los métodos NASH existentes se muestran en la Sección 6, y concluimos con una discusión sobre el trabajo relacionado en la Sección 7. 2. El dominio de patrulla en la mayoría de los dominios de patrullaje de seguridad, los agentes de seguridad (como los UAV [1] o los robots de seguridad [16]) no pueden patrullar todas las áreas todo el tiempo. En cambio, deben elegir una política por la cual patrulan varias rutas en diferentes momentos, teniendo en cuenta factores como la probabilidad de delito en diferentes áreas, posibles objetivos para el delito, y los agentes de seguridad poseen recursos (número de agentes de seguridad, cantidad detiempo disponible, combustible, etc.). Por lo general, es beneficioso que esta política sea no determinista para que los ladrones no puedan robar de manera segura ciertas ubicaciones, sabiendo que estarán a salvo de los agentes de seguridad [14]. Para demostrar la utilidad de nuestro algoritmo, utilizamos una versión simplificada de dicho dominio, expresada como un juego. La versión más básica de nuestro juego consta de dos jugadores: el agente de seguridad (el líder) y el ladrón (el seguidor) en un mundo que consta de M casas, 1...metro.El conjunto de agentes de seguridad de estrategias puras consiste en posibles rutas de las casas D para patrullar (en un orden). El agente de seguridad puede elegir una estrategia mixta para que el ladrón no esté seguro de dónde puede patrullar el agente de seguridad, pero el ladrón sabrá la estrategia mixta que el agente de seguridad ha elegido. Por ejemplo, el ladrón puede observar con el tiempo con qué frecuencia el agente de seguridad patrulla cada área. Con este conocimiento, el ladrón debe elegir una sola casa para robar. Suponemos que el ladrón generalmente lleva mucho tiempo robar una casa. Si la casa elegida por el ladrón no está en la ruta de los agentes de seguridad, entonces el ladrón lo roba con éxito. De lo contrario, si está en la ruta de los agentes de seguridad, entonces cuanto antes esté en la ruta, más fácil es para el agente de seguridad atrapar al ladrón antes de terminar de robarla. Modelamos los pagos para este juego con las siguientes variables: • VL, X: Valor de los bienes en casa L al agente de seguridad.• VL, P: Valor de los bienes en casa l al ladrón.• CX: recompensa al agente de seguridad de atrapar al ladrón.• CQ: costo para el ladrón de ser atrapados.• PL: Probabilidad de que el agente de seguridad pueda atrapar al ladrón en la casa LTH en la Patrulla (PL <PL ⇐⇒ L <L). El conjunto de agentes de seguridad de posibles estrategias puras (rutas de patrulla) se denota por X e incluye todas las dugumes D i = <w1, w2, ..., wd> con w1...WD = 1...m donde no hay dos elementos iguales (el agente no puede regresar a la misma casa). El conjunto de ladrones de posibles estrategias puras (casas para robar) se denota por Q e incluye todos los enteros J = 1...metro.Los pagos (agente de seguridad, ladrón) para estrategias puras I, j son: • −vl, x, vl, q, para j = l /∈ I.• PLCX +(1 - PL) ( - VL, X), −PLCQ +(1 - PL) (VL, Q), para J = L ∈ I. Con esta estructura es posible modelar muchos tipos diferentes de ladrones que tienen motivaciones diferentes;Por ejemplo, un ladrón puede tener un costo más bajo de ser atrapado que otro, o puede valorar los bienes en las diversas casas de manera diferente. Si la distribución de diferentes tipos de ladrones se conoce o infiere de los datos históricos, entonces el juego se puede modelar como un juego bayesiano [6].3. Juegos bayesianos Un juego bayesiano contiene un conjunto de n agentes, y cada agente N debe ser uno de un conjunto dado de tipos θn. Para nuestro dominio de patrulla, tenemos dos agentes, el agente de seguridad y el ladrón.θ1 es el conjunto de tipos de agentes de seguridad y θ2 es el conjunto de tipos de ladrones. Dado que solo hay un tipo de agente de seguridad, θ1 contiene solo un elemento. Durante el juego, el ladrón conoce su tipo, pero el agente de seguridad no conoce el tipo de ladrones. Para cada agente (el agente de seguridad o el ladrón) n, hay un conjunto de estrategias σn y una función de utilidad un: θ1 × θ2 × σ1 × σ2 →. Un juego bayesiano se puede transformar en un juego de forma normal utilizando la transformación Harsanyi [8]. Una vez hecho esto, se pueden utilizar nuevos métodos basados en programas lineales (LP) para encontrar estrategias de alta recompensa para juegos de forma normal [5] para encontrar una estrategia en el juego transformado;Esta estrategia se puede usar para el juego bayesiano. Si bien existen métodos para encontrar equilibrios de bayes-nash directamente, sin la transformación de Harsanyi [10], encuentran solo un 312 el sexto intl. Conf.en agentes autónomos y sistemas de múltiples agentes (AAMAS 07) equilibrio único en el caso general, que puede no ser de alta recompensa. El trabajo reciente [17] ha llevado a técnicas eficientes del programa lineal de introducción mixta para encontrar el mejor equilibrio de Nash para un agente determinado. Sin embargo, estas técnicas requieren un juego de forma normal y, por lo tanto, para comparar las políticas dadas por ASAP contra la política óptima, así como contra el equilibrio de NASH de mayor recompensa, debemos aplicar estas técnicas a la matriz transformada de Harsanyi. Las siguientes dos subsecciones elaboran cómo se hace esto.3.1 Transformación de Harsanyi El primer paso para resolver juegos bayesianos es aplicar la transformación Harsanyi [8] que convierte el juego bayesiano en un juego de forma normal. Dado que la transformación de Harsanyi es un concepto estándar en la teoría del juego, lo explicamos brevemente a través de un ejemplo simple en nuestro dominio de patrulla sin introducir las formulaciones matemáticas. Supongamos que hay dos tipos de ladrones A y B en el juego bayesiano. El ladrón A estará activo con probabilidad α, y el ladrón B estará activo con probabilidad 1 - α. Las reglas descritas en la Sección 2 nos permiten construir tablas de pago simples. Suponga que hay dos casas en el mundo (1 y 2) y, por lo tanto, hay dos rutas de patrulla (estrategias puras) para el agente: {1,2} y {2,1}. El ladrón puede robar la casa 1 o la casa 2 y, por lo tanto, tiene dos estrategias (denotadas como 1L, 2L para el tipo de ladrón L). Dado que hay dos tipos asumidos (denotados como A y B), construimos dos tablas de pago (que se muestran en la Tabla 2) correspondientes al agente de seguridad que juega un juego separado con cada uno de los dos tipos de ladrones con probabilidades α y 1 - α. Primero, considere Robber Type A. Tomando prestado la notación de la sección Dominio, asignamos los siguientes valores a las variables: V1, X = V1, Q = 3/4, V2, X = V2, Q = 1/4, CX = 1/2, CQ = 1, p1 = 1, p2 = 1/2. Usando estos valores, construimos una tabla de pago base como el pago para el juego contra Robber Tipo A. Por ejemplo, si el agente de seguridad elige la ruta {1,2} cuando el ladrón A está activo, y el robador A elige la Casa 1, el ladrón recibe una recompensa de -1 (por ser atrapado) y el agente recibe una recompensa de 0.5 por atraparel ladrón. Los pagos para el juego contra Robber Tipo B se construyen utilizando diferentes valores. Agente de seguridad: {1,2} {2,1} robador A 1a -1, .5 -.375, .125 2a -.125, -.125 -1, .5 robador b 1b -.9, .6 -.275, .225 2b -.025, -.025 -.9, .6 Tabla 2: Tablas de pago: Agente de seguridad vs ladrones A y B utilizando la técnica Harsanyi implica introducir un nodo casual, que determina el tipo de ladrones, transformando asíLos agentes de seguridad incompletan información sobre el ladrón en información imperfecta [3]. El equilibrio bayesiano del juego es precisamente el equilibrio de Nash del juego de información imperfecta. El juego transformado de forma normal se muestra en la Tabla 3. En el juego transformado, el agente de seguridad es el reproductor de columnas, y el conjunto de todos los tipos de ladrones juntos es el reproductor de filas. Supongamos que el ladrón tipo A Robs House 1 y Robber Type B Robs House 2, mientras que el agente de seguridad elige patrulla {1,2}. Luego, el agente de seguridad y el ladrón reciben una recompensa esperada correspondiente a sus pagos del agente que se encuentra con el ladrón A en la casa 1 con probabilidad α y ladrón B en la casa 2 con probabilidad 1 - α.3.2 Encontrar una estrategia óptima Aunque un equilibrio de Nash es el concepto de solución estándar para los juegos en los que los agentes eligen estrategias simultáneamente, en nuestro dominio de seguridad, el agente de seguridad (el líder) puede obtener una ventaja al comprometerse con una estrategia mixta de antemano. Dado que los seguidores (los ladrones) conocerán la estrategia de los líderes, la respuesta óptima para los seguidores será una estrategia pura. Dada la suposición común, tomada en [5], en el caso de que los seguidores son indiferentes, elegirán la estrategia que beneficia al líder, debe existir una estrategia óptima garantizada para el líder [5]. Desde el juego bayesiano en la Tabla 2, construimos la bimatriz transformada Harsanyi en la Tabla 3. Las estrategias para cada jugador (agente de seguridad o ladrón) en el juego transformado corresponden a todas las combinaciones de posibles estrategias tomadas por cada uno de los tipos de jugadores. Por lo tanto, denotamos x = σθ1 1 = σ1 y q = σθ2 2 como los conjuntos de índice del agente de seguridad y los ladrones de estrategias puras respectivamente, con R y C como las matrices de pago correspondientes. RIJ es la recompensa del agente de seguridad y CIJ es la recompensa de los ladrones cuando el agente de seguridad toma la estrategia pura I y los ladrones toman pura estrategia j. Una estrategia mixta para el agente de seguridad es una distribución de probabilidad sobre su conjunto de estrategias puras y estará representada por un vector x = (px1, px2, ..., px | x |), donde pxi ≥ 0 y p pxi = 1. Aquí, PXI es la probabilidad de que el agente de seguridad elija su estrategia pura. La estrategia mixta óptima para el agente de seguridad se puede encontrar en el polinomio de tiempo en el número de filas en el juego de forma normal utilizando la siguiente formulación del programa lineal de [5]. Para cada estrategia pura posible j por el seguidor (el conjunto de todos los tipos de ladrones), max p i∈X pxirij s.t.∀j ∈ Q, p i∈σ1 pxiciJ ≥ p i∈σ1 pxicij p i∈X pxi = 1 ∀i∈X, pxi> = 0 (1) Entonces, para todasi∈X pxirij, la recompensa para el agente de seguridad (líder). Las variables PXI dan la estrategia óptima para el agente de seguridad. Tenga en cuenta que si bien este método es polinomio en el número de filas en el juego transformado de forma normal, el número de filas aumenta exponencialmente con el número de tipos de ladrones. Usar este método para un juego bayesiano, por lo tanto, requiere ejecutar | σ2 || θ2 |programas lineales separados. Esto no es sorprendente, ya que encontrar la estrategia óptima de los líderes en un juego bayesiano de Stackelberg es NP-Hard [5].4. Enfoques heurísticos Dados que encontrar la estrategia óptima para el líder es NP-Hard, proporcionamos un enfoque heurístico. En esta heurística limitamos las posibles estrategias mixtas del líder para seleccionar acciones con probabilidades que sean múltiplos enteros de 1/K para un entero predeterminado k.El trabajo anterior [14] ha demostrado que las estrategias con alta entropía son beneficiosas para las aplicaciones de seguridad cuando los servicios públicos de oponentes son completamente desconocidas. En nuestro dominio, si no se consideran los servicios públicos, este método dará como resultado estrategias de distribución uniforme. Una ventaja de tales estrategias es que son compactos para representar (como fracciones) y fáciles de entender;Por lo tanto, pueden ser implementados de manera eficiente por organizaciones reales. Nuestro objetivo es mantener la ventaja proporcionada por estrategias simples para nuestro problema de aplicación de seguridad, incorporando el efecto de las recompensas de los ladrones en las recompensas de los agentes de seguridad. Por lo tanto, la heurística ASAP producirá estrategias que son K-uniformes. Una estrategia mixta se denota k-uniforme si se trata de una distribución uniforme en una multisets de estrategias puras con | S |= k.Un Multiset es un conjunto cuyos elementos pueden repetirse varias veces;Así, por ejemplo, la estrategia mixta correspondiente a la multiset {1, 1, 2} tomaría la estrategia 1 la sexta intl. Conf.en agentes autónomos y sistemas de múltiples agentes (aamas 07) 313 {1,2} {2,1} {1a, 1b} −1α-.9 (1-α), .5α + .6 (1-α)--.375α - .275 (1 - α), .125α + .225 (1 - α) {1a, 2b} −1α - .025 (1 - α), .5α - .025 (1 - α) −.375α °- .9 (1 - α), .125α + .6 (1 - α) {2a, 1b} −.125α - .9 (1 - α), −.125α + .6 (1 - α) −1α −.275 (1 - α), .5α + .225 (1 - α) {2a, 2b} −.125α - .025 (1 - α), −.125α - .025 (1 - α) −1α -.9 (1 - α), .5α + .6 (1 - α) Tabla 3: Tabla de pago transformada de Harsanyi con probabilidad 2/3 y estrategia 2 con probabilidad 1/3. ASAP permite que se elija el tamaño del Multiset para equilibrar la complejidad de la estrategia alcanzada con el objetivo de que la estrategia identificada generará una alta recompensa. Otra ventaja de la heurística ASAP es que opera directamente en la representación bayesiana compacta, sin requerir la transformación de Harsanyi. Esto se debe a que los diferentes tipos de seguidores (ladrones) son independientes entre sí. Por lo tanto, evaluar la estrategia del líder contra una matriz de juego transformada en Harsanyi es equivalente a evaluar contra cada una de las matrices de juego para los tipos de seguidores individuales. Esta propiedad de independencia se explota lo antes posible para producir un esquema de descomposición. Tenga en cuenta que el método LP introducido por [5] para calcular políticas óptimas de Stackelberg es poco probable que se descomponga en un pequeño número de juegos, ya que se demostró que era NP-Hard para los problemas de Bayes-Nash. Finalmente, tenga en cuenta que ASAP requiere la solución de un solo problema de optimización, en lugar de resolver una serie de problemas como en el método LP de [5]. Para un solo tipo de seguidor, el algoritmo funciona de la siguiente manera. Dada una K en particular, para cada posible estrategia mixta X para el líder que corresponde a un multiset de tamaño K, evalúa el recompensa de los líderes de X cuando el seguidor juega una estrategia pura que maximiza la recompensa. Luego tomamos la estrategia mixta con la mayor recompensa. Solo necesitamos considerar las estrategias puras de la maximización de la recompensa de los seguidores (ladrones), ya que para una estrategia fija dada X del agente de seguridad, cada tipo de ladrón enfrenta un problema con recompensas lineales fijas. Si una estrategia mixta es óptima para el ladrón, entonces todas las estrategias puras en el apoyo de esa estrategia mixta. Tenga en cuenta también que debido a que limitamos las estrategias de los líderes para asumir valores discretos, la suposición de la Sección 3.2 de que los seguidores romperán los lazos a favor de los líderes no es significativo, ya que es poco probable que surjan los lazos. Esto se debe a que, en los dominios donde las recompensas se extraen de cualquier distribución aleatoria, la probabilidad de que un seguidor tenga más de una respuesta óptima pura a una estrategia de líder dada se acerca a cero, y el líder solo tendrá un número finito de estrategias mixtas posibles. Nuestro enfoque para caracterizar la estrategia óptima para el agente de seguridad hace uso de las propiedades de la programación lineal. Brevemente describimos estos resultados aquí para completar, para una discusión detallada y pruebas, consulte una de las muchas referencias sobre el tema, como [2]. Cada problema de programación lineal, como: Max CT X |Ax = b, x ≥ 0, tiene un programa lineal dual asociado, en este caso: Min Bt y |En y ≥ c.Estos pares de problemas primarios/duales satisfacen la dualidad débil: para cualquier soluciones de X e Y Primal y Dual, respectivamente, CT x ≤ Bt y. Por lo tanto, un par de soluciones factibles es óptima si CT x = bt y, y se dice que los problemas satisfacen una fuerte dualidad. De hecho, si un programa lineal es factible y tiene una solución óptima limitada, entonces el dual también es factible y hay un par X ∗, y ∗ que satisface CT x ∗ = bt y ∗. Estas soluciones óptimas se caracterizan con las siguientes condiciones de optimización (como se define en [2]): • Facibilidad primaria: ax = B, x ≥ 0 • Dual Feasibilidad: a y ≥ C • Flackness complementaria: xi (a y - c) I= 0 para todo i. Tenga en cuenta que esta última condición implica que CT x = xt en y = bt y, lo que demuestra optimización para las soluciones primarias de doble factible x e y. En las siguientes subsecciones, primero definimos el problema en su forma más intuitiva como un programa cuadrático de entero mixto (MIQP), y luego mostramos cómo este problema se puede convertir en un programa lineal de Instigua mixed (MILP).4.1 Programa cuadrático de entero mixto Comenzamos con el caso de un solo tipo de seguidor. Deje que el líder sea el jugador de fila y el seguidor del reproductor de columna. Denotamos por x el vector de estrategias del líder y Q el vector de estrategias del seguidor. También denotamos X y Q los conjuntos de índices del líder y seguidores estrategias puras, respectivamente. Las matrices de pago R y C corresponden a: RIJ es la recompensa del líder y CIJ es la recompensa del seguidor cuando el líder toma pura estrategia I y el seguidor toma la estrategia pura j. Deje que K sea del tamaño del Multiset. Primero solucionamos la política del líder a una política uniforme K. El valor XI es el número de veces que la estrategia I pura se usa en la política uniforme K, que se selecciona con probabilidad xi/k. Formulamos el problema de optimización que el seguidor resuelve para encontrar su respuesta óptima a X como el siguiente programa lineal: Max x j∈Q x i∈X 1 K CIJXI QJ S.T. P j∈Q QJ = 1 Q ≥ 0. (2) La función objetivo maximiza la recompensa esperada de los seguidores dada x, mientras que las restricciones hacen que sea factible cualquier estrategia mixta Q para el seguidor. El dual de este problema de programación lineal es el siguiente: min a s.t.a ≥ x i∈X 1 k cijxi j ∈ Q. (3) De una fuerte dualidad y flojedad complementaria obtenemos que el valor de recompensa máxima de los seguidores, a, es el valor de cada estrategia pura con qj> 0, es decirde la estrategia mixta óptima. Por lo tanto, cada una de estas estrategias puras es óptima. Las soluciones óptimas al problema de los seguidores se caracterizan por condiciones de optimización de programación lineal: restricciones de viabilidad primaria en (2), limitaciones de factibilidad dual en (3) y flojura complementaria QJ A - x i∈X 1 K CIJXI!= 0 j ∈ Q. Estas condiciones deben incluirse en el problema resuelto por el líder para considerar solo las mejores respuestas por parte del seguidor de la Política Uniforme K.314 El sexto intl. Conf.En agentes autónomos y sistemas de múltiples agentes (AAMAS 07), el líder busca la solución uniforme K que maximiza su propio pago, dado que el seguidor utiliza una respuesta óptima Q (x). Por lo tanto, el líder resuelve el siguiente problema entero: max x i∈X x j∈Q 1 k rijq (x) j xi s.t. P i∈X xi = k xi ∈ {0, 1 ,..., k}.(4) El problema (4) maximiza la recompensa de los líderes con la mejor respuesta de los seguidores (QJ para la política de líderes fijos x y, por lo tanto, denotó q (x) j) seleccionando una política uniforme de un multiset de tamaño constante k.Completamos este problema al incluir la caracterización de Q (x) a través de condiciones de optimización de programación lineal. Para simplificar la redacción de las condiciones de holgura complementaria, limitaremos que Q (x) sean solo estrategias puras óptimas solo considerando soluciones enteras de Q (x). El problema de los líderes se convierte en: maxx, q x i∈X x j∈Q 1 k rijxiqj s.t. P i xi = kp j∈Q qj = 1 0 ≤ (a - p i∈X 1 k cijxi) ≤ (1 - qj) m xi ∈ {0, 1, ...., k} qj ∈ {0,1}.(5) Aquí, la constante M es un gran número. La primera y la cuarta limitaciones imponen una política uniforme de K para el líder, y las restricciones segunda y quinta imponen una estrategia pura factible para el seguidor. La tercera restricción impone una doble viabilidad del problema de los seguidores (desigualdad más izquierda) y la restricción de flojedad complementaria para una estrategia pura Q óptima Q para el seguidor (desigualdad más derecha). De hecho, dado que el seguidor solo puede seleccionar una estrategia pura, digamos Qh = 1, esta última restricción aplica que a = P i∈X 1 k cihxi imponiendo ninguna restricción adicional para todas las demás estrategias puras que tienen qj = 0. Llegamos a la conclusión de esta subsección que señala que el problema (5) es un programa entero con un objetivo cuadrático no convexo en general, ya que la matriz R no necesita ser positiva-semi-definitiva. Métodos de solución eficientes para problemas enteros no lineales y no convexos sigue siendo una pregunta de investigación desafiante. En la siguiente sección mostramos una reformulación de este problema como un problema de programación de enteros lineales, para el cual existen varios solucionadores comerciales eficientes.4.2 Programa lineal de introducción mixta Podemos linealizar el programa cuadrático del problema 5 a través del cambio de variables Zij = xiqj, obteniendo el siguiente problema maxq, z p i∈X p j∈Q 1 k rijzij s.t. P i∈X p j∈Q zij = k p j∈Q zij ≤ k kqj ≤ p i∈X zij ≤ k p j∈Q qj = 1 0 ≤ (a - p i∈X 1 k cij (p h∈Qzih)) ≤ (1 - qj) m zij ∈ {0, 1, ...., k} qj ∈ {0, 1} (6) Proposición 1. Los problemas (5) y (6) son equivalentes. Prueba: Considere X, Q Una solución factible de (5). Mostraremos que q, zij = xiqj es una solución factible de (6) del mismo valor de función objetivo. La construcción satisface la equivalencia de las funciones objetivas y las limitaciones 4, 6 y 7 de (6). El hecho de que p j∈Q zij = xi como p j∈Q qj = 1 explica las restricciones 1, 2 y 5 de (6). La restricción 3 de (6) se satisface porque p i∈X zij = kqj. Consideremos ahora Q, Z factible para (6). Mostraremos que q y xi = p j∈Q zij son factibles para (5) con el mismo valor objetivo. De hecho, todas las limitaciones de (5) se satisfacen fácilmente por la construcción. Para ver que los objetivos coincidan, observe que si qh = 1 entonces la tercera restricción en (6) implica que p i∈X zih = k, lo que significa que zij = 0 para todo i ∈ X y todos j = h.Por lo tanto, xiqj = x l∈Q zilqj = zihqj = zij. Esta última igualdad se debe a que ambos son 0 cuando j = h.Esto muestra que la transformación conserva el valor de la función objetivo, completando la prueba. Dada esta transformación en un programa lineal de introducción mixta (MILP), ahora mostramos cómo podemos aplicar nuestra técnica de descomposición en el MILP para obtener aceleraciones significativas para juegos bayesianos con múltiples tipos de seguidores.5. Descomposición para múltiples adversarios El MILP desarrollado en la sección anterior maneja solo un seguidor. Dado que nuestro escenario de seguridad contiene múltiples tipos de seguidores (ladrones), cambiamos la función de respuesta para el seguidor de una estrategia pura a una combinación ponderada sobre varias estrategias de seguidor puro donde los pesos son probabilidades de ocurrencia de cada uno de los tipos de seguidores.5.1 MIQP descompuesto Para admitir múltiples adversarios en nuestro marco, modificamos la notación definida en la sección anterior para razonar sobre múltiples tipos de seguidores. Denotamos por x el vector de estrategias del líder y QL el vector de estrategias del seguidor L, con L denotando el conjunto de índice de tipos de seguidores. También denotamos por X y Q los conjuntos de índices del líder y el seguidor de estrategias puras, respectivamente. También indexamos las matrices de pago en cada seguidor L, considerando las matrices RL y CL. Usando esta notación modificada, caracterizamos la solución óptima del problema del seguidor LS dada la política uniforme de los líderes K, con las siguientes condiciones de optimización: x j∈Q ql j = 1 al-x i∈X 1 k cl ijxi ≥ 0 qlJ (al - x i∈X 1 k cl ijxi) = 0 ql j ≥ 0 nuevamente, considerando solo estrategias puras óptimas para el problema del seguidor LS podemos linealizar la restricción de complementariedad anterior. Incorporamos estas restricciones en el problema de los líderes que selecciona la política óptima de K-uniforme. Por lo tanto, dadas las probabilidades a priori PL, con l ∈ L de enfrentar cada seguidor, el líder resuelve el siguiente problema: el sexto intl. Conf.en agentes autónomos y sistemas de múltiples agentes (AAMAS 07) 315 Maxx, Q x i∈X x l∈L x j∈Q pl K rl ijxiql j s.t. P i xi = kp j∈Q ql j = 1 0 ≤ (al - p i∈X 1 k cl ijxi) ≤ (1 - ql j) m xi ∈ {0, 1, ...., k} ql j∈ {0, 1}.(7) El problema (7) para un juego bayesiano con múltiples tipos de seguidores es realmente equivalente al problema (5) en la matriz de pago obtenida de la transformación Harsanyi del juego. De hecho, cada estrategia pura j en el problema (5) corresponde a una secuencia de estrategias puras jl, una para cada seguidor l ∈ L. Esto significa que qj = 1 si y solo si ql jl = 1 para todos l ∈ L. enAdemás, dadas las probabilidades a priori PL de Jugador Facing L, la recompensa en la tabla de pago de transformación Harsanyi es rij = p l∈L pl rl ijl. La misma relación se mantiene entre C y Cl. Estas relaciones entre una estrategia pura en el juego de forma normal equivalente y las estrategias puras en los juegos individuales con cada seguidor son clave para mostrar estos problemas son equivalentes.5.2 MILP descompuesto Podemos linealizar el problema de programación cuadrática 7 a través del cambio de variables Zl ij = xiql j, obteniendo el siguiente problema maxq, z p i∈X p l∈L p j∈Q pl k rl ijzl ij s.t. P i∈X p j∈Q zl ij = k p j∈Q zl ij ≤ k kql j ≤ p i∈x cansil zl ij ≤ k p j∈Q ql j = 1 0 ≤ (al - p i∈X 1 k clij (p h∈Q zl ih)) ≤ (1 - ql j) m p j∈Q zl ij = p j), 1} (8) Proposición 2. Los problemas (7) y (8) son equivalentes. Prueba: Considere X, Ql, Al con L ∈ L una solución factible de (7). Mostraremos que Ql, Al, Zl ij = xiql J es una solución factible de (8) del mismo valor de función objetivo. La construcción satisface la equivalencia de las funciones objetivas y las limitaciones 4, 7 y 8 de (8). El hecho de que p j∈Q zl ij = xi as p j∈Q ql j = 1 explica las restricciones 1, 2, 5 y 6 de (8). La restricción 3 de (8) se satisface porque p i∈X zl ij = kql j. Consideremos ahora QL, ZL, Al factible para (8). Mostraremos que QL, Al y Xi = P j∈Q Z1 IJ son factibles para (7) con el mismo valor objetivo. De hecho, todas las limitaciones de (7) se satisfacen fácilmente por la construcción. Para ver que los objetivos coincidan, la notificación para cada L, un Ql J debe igual 1 y el resto igual 0. Digamos que ql jl = 1, entonces la tercera restricción en (8) implica que p i∈X zl ijl = k, lo que significa que zl ij = 0 para todos los i ∈ X y todos j = jl. En particular, esto implica que xi = x j∈Q z1 ij = z1 ij1 = zl ijl, la última igualdad de la restricción 6 de (8). Por lo tanto, xiql j = zl ijl ql j = zl ij. Esta última igualdad se debe a que ambos son 0 cuando J = JL. Efectivamente, la restricción 6 asegura que todos los adversarios calculen sus mejores respuestas contra una política fija particular del agente. Esto muestra que la transformación conserva el valor de la función objetivo, completando la prueba. Por lo tanto, podemos resolver este programa entero lineal equivalente con paquetes de programación enteros eficientes que pueden manejar problemas con miles de variables enteras. Implementamos el MILP descompuesto y los resultados se muestran en la siguiente sección.6. Resultados experimentales El dominio de patrulla y los pagos para el juego asociado se detallan en las Secciones 2 y 3. Realizamos experimentos para este juego en mundos de tres y cuatro casas con patrullas que consisten en dos casas. La descripción dada en la Sección 2 se utiliza para generar un caso base tanto para el agente de seguridad como para las funciones de pago de ladrones. Las tablas de pago para tipos de ladrones adicionales se construyen y se agregan al juego agregando una distribución aleatoria del tamaño variable a los pagos en el caso base. Todos los juegos se normalizan de modo que, para cada tipo de ladrón, los pagos mínimos y máximos para el agente de seguridad y el ladrón sean 0 y 1, respectivamente. Usando los datos generados, realizamos los experimentos utilizando cuatro métodos para generar la estrategia de agentes de seguridad: • Aleatorización uniforme • ASAP • El método de programas lineales múltiples de [5] (para encontrar la verdadera estrategia óptima) • El equilibrio de narración de bayes más alta de recompensa, encontrado utilizando el algoritmo MIP-Nash [17], los últimos tres métodos se aplicaron usando CPlex 8.1. Debido a que los dos últimos métodos están diseñados para juegos de forma normal en lugar de juegos bayesianos, los juegos se convirtieron primero utilizando la transformación Harsanyi [8]. El método de aleatorización uniforme es simplemente elegir una política aleatoria uniforme sobre todas las rutas de patrulla posibles. Utilizamos este método como una simple línea de base para medir el rendimiento de nuestras heurísticas. Anticipamos que la política uniforme funcionaría razonablemente bien ya que las políticas de entropía máxima han demostrado ser efectivas en los dominios de seguridad multiagente [14]. Se utilizaron los equilibrios de Nash de Bayes de mayor recompensa para demostrar la mayor recompensa obtenida al buscar una política óptima en lugar de un equilibrio en los juegos de Stackelberg como nuestro dominio de seguridad. En base a nuestros experimentos, presentamos tres conjuntos de gráficos para demostrar (1) el tiempo de ejecución de ASAP en comparación con otros métodos comunes para encontrar una estrategia, (2) la recompensa garantizada por ASAP en comparación con otros métodos y (3) el efecto de variar variandoEl parámetro K, el tamaño del Multiset, en el rendimiento de ASAP. En los dos primeros conjuntos de gráficos, ASAP se ejecuta utilizando una multiset de 80 elementos;En el tercer conjunto, este número es variado. El primer conjunto de gráficos, que se muestran en la Figura 1, muestra los gráficos de tiempo de ejecución para dominios de tres casas (columna izquierda) y de cuatro casas (columna derecha). Cada una de las tres filas de gráficos corresponde a un escenario diferente generado al azar. El eje X muestra el número de tipos de ladrones que enfrenta el agente de seguridad y el eje Y del gráfico muestra el tiempo de ejecución en segundos. Todos los experimentos que no se concluyeron en 30 minutos (1800 segundos) fueron cortados. El tiempo de ejecución de la política uniforme siempre es insignificante, independientemente del número de adversarios y, por lo tanto, no se muestra. El algoritmo ASAP claramente supera el método óptimo de multiplica, así como el algoritmo MIP-Nash para encontrar el equilibrio de Bayes-Nash Highingward con respecto al tiempo de ejecución. Para un 316 el sexto intl. Conf.en agentes autónomos y sistemas de múltiples agentes (AAMAS 07) Figura 1: tiempos de ejecución para varios algoritmos sobre problemas de 3 y 4 casas.Dominio de tres casas, el método óptimo no puede alcanzar una solución para más de siete tipos de ladrones, y para cuatro casas no puede resolver durante más de seis tipos dentro del tiempo de corte en cualquiera de los tres escenarios. MIP-Nash resuelve incluso menos tipos de ladrones dentro del tiempo de corte. Por otro lado, ASAP se ejecuta mucho más rápido, y puede resolver al menos 20 adversarios para los escenarios de tres casas y para al menos 12 adversarios en los escenarios de cuatro casas dentro del tiempo de corte. El tiempo de ejecución de ASAP no aumenta estrictamente con el número de tipos de ladrones para cada escenario, pero en general, la adición de más tipos aumenta el tiempo de ejecución requerido. El segundo conjunto de gráficos, Figura 2, muestra la recompensa al agente de patrulla dada por cada método para tres escenarios en los dominios de tres casas (columna izquierda) y de cuatro casas (columna derecha). Esta recompensa es la utilidad recibida por el agente de seguridad en el juego de patrulla, y no como un porcentaje de la recompensa óptima, ya que no fue posible obtener la recompensa óptima a medida que aumentó el número de tipos de ladrones. La política uniforme proporciona constantemente la recompensa más baja en ambos dominios;mientras que el método óptimo, por supuesto, produce la recompensa óptima. El método ASAP permanece constantemente cerca de lo óptimo, incluso a medida que aumenta el número de tipos de ladrones. Los equilibrios de Nash de Bayes más altos recompensas, proporcionados por el método Mipnash, produjeron recompensas más altas que el método uniforme, pero más bajo que ASAP. Esta diferencia ilustra claramente las ganancias en el dominio de patrulla que se comprometen a una estrategia como líder en un juego de Stackelberg, en lugar de jugar una estrategia estándar de Bayes-Nash. El tercer conjunto de gráficos, que se muestran en la Figura 3, muestra el efecto del tamaño multiset en el tiempo de ejecución en segundos (columna izquierda) y recompensa (columna derecha), nuevamente expresada como la recompensa recibida por el agente de seguridad en el juego de patrulla, y no unPorcentaje de la Figura óptima 2: Recompensa por varios algoritmos en problemas de 3 y 4 casas.premio. Los resultados aquí son para el dominio de tres casas. La tendencia es que como se aumenta el tamaño de múltiples unidades, el tiempo de ejecución y el nivel de recompensa aumentan. No es sorprendente que la recompensa aumente monotónicamente a medida que aumenta el tamaño de la multisetea, pero lo interesante es que hay relativamente poco beneficio al usar un gran conjunto de múltiples en este dominio. En todos los casos, la recompensa dada por un Multiset de 10 elementos fue al menos el 96% de la recompensa dada por un Multiset de 80 elementos. El tiempo de ejecución no siempre aumenta estrictamente con el tamaño multisetúrgico;De hecho, en un ejemplo (el escenario 2 con 20 tipos de ladrones), el uso de una multiset de 10 elementos tomó 1228 segundos, mientras que usar 80 elementos solo tomó 617 segundos. En general, el tiempo de ejecución debería aumentar ya que un Multiset más grande significa un dominio más grande para las variables en el MILP y, por lo tanto, un espacio de búsqueda más grande. Sin embargo, un aumento en el número de variables a veces puede permitir que una política se construya más rápidamente debido a una mayor flexibilidad en el problema.7. Resumen y trabajo relacionado Este documento se centra en la seguridad para los agentes que patrullan en entornos hostiles. En estos entornos, las amenazas intencionales son causadas por adversarios sobre los cuales los agentes de patrullaje de seguridad tienen información incompleta. Específicamente, tratamos con situaciones en las que se conocen las acciones y los pagos adversarios, pero el tipo de adversario exacto es desconocido para el agente de seguridad. Los agentes que actúan en el mundo real con bastante frecuencia tienen información tan incompleta sobre otros agentes. Los juegos bayesianos han sido una opción popular para modelar tales juegos de información incompletos [3]. El kit de herramientas de gala es un método para definir tales juegos [9] sin requerir que el juego se represente en forma normal a través de la transformación Harsanyi [8];Las garantías de Galas se centran en juegos completamente competitivos. Se ha realizado mucho trabajo para encontrar el óptimo Bayes-Nash Equilbria para el sexto INTL. Conf.Sobre agentes autónomos y sistemas de múltiples agentes (AAMAS 07) 317 Figura 3: Recompensa por lo antes posible usando multisets de 10, 30 y 80 elementos subclases de juegos bayesianos, encontrando equilibrios de bayes-nash para juegos bayesianos generales [10] o bayes aproximados-Enlash Equilibrios [18]. Se ha prestado menos atención a encontrar la estrategia óptima para comprometerse en un juego bayesiano (el escenario de Stackelberg [15]). Sin embargo, se demostró que la complejidad de este problema es NP-Hard en el caso general [5], lo que también proporciona algoritmos para este problema en el caso no bayesiano. Por lo tanto, presentamos una heurística llamada ASAP, con tres ventajas clave para abordar este problema. Primero, ASAP busca la estrategia de recompensa más alta, en lugar de un equilibrio de Bayes-Nash, lo que le permite encontrar estrategias factibles que exploten la ventaja natural del primer movimiento del juego. En segundo lugar, proporciona estrategias que son fáciles de entender, representar e implementar. En tercer lugar, opera directamente en la representación compacta del juego bayesiano, sin requerir la conversión a la forma normal. Proporcionamos una implementación eficiente del Programa Lineal Integer Integer (MILP) para ASAP, junto con resultados experimentales que ilustran aceleraciones significativas y mayores recompensas sobre otros enfoques. Nuestras estrategias uniformes K son similares a las estrategias uniformes de K de [12]. Si bien ese trabajo proporciona límites de error Epsilon basados en las estrategias uniformes K, su concepto de solución sigue siendo el de un equilibrio de Nash, y no proporcionan algoritmos eficientes para obtener tales estrategias uniformes K. Esto contrasta con lo antes posible, donde nuestro énfasis está en un enfoque heurístico altamente eficiente que no se centra en las soluciones de equilibrio. Finalmente, el problema de patrulla que motivó nuestro trabajo recientemente ha recibido una creciente atención de la comunidad multiagente debido a su amplia gama de aplicaciones [4, 13]. Sin embargo, la mayor parte de este trabajo se centra en limitar el consumo de energía involucrado en la patrulla [7] u optimizar en criterios como la longitud de la ruta recorrida [4, 13], sin razonar sobre ningún modelo explícito de un adversario [14]. Agradecimientos: Esta investigación es apoyada por el Departamento de Seguridad Nacional de los Estados Unidos a través del Centro de Riesgo y Análisis Económico de Eventos de Terrorismo (Creación). También cuenta con el apoyo de la Agencia de Proyectos de Investigación Avanzada de Defensa (DARPA), a través del Departamento del Interior, NBC, División de Servicios de Adquisición, bajo el No. NBCHD030010. Sarit Kraus también está afiliado a Umiacs.8. Referencias [1] R. W. Beard y T. McLain. Múltiple búsqueda cooperativa de UAV bajo evitación de colisión y restricciones de comunicación de rango limitado. En IEEE CDC, 2003. [2] D. Bertsimas y J. Tsitsiklis. Introducción a la optimización lineal. Athena Scientific, 1997. [3] J. Brynielsson y S. Arnborg. Juegos bayesianos para predicción de amenazas y análisis de situación. En Fusion, 2004. [4] Y. Chevaleyre. Análisis teórico del problema de patrulla de múltiples agentes. En Aamas, 2004. [5] V. Conitzer y T. Sandholm. Elegir la mejor estrategia para comprometerse. En la Conferencia ACM sobre Comercio Electrónico, 2006. [6] D. Fudenberg y J. Tirole. Teoría de juego. MIT Press, 1991. [7] C. Gui y P. Mohapatra. Patrulla virtual: un nuevo diseño de conservación de potencia para la vigilancia utilizando redes de sensores. En IPSN, 2005. [8] J. C. Harsanyi y R. Selten. Una solución NASH generalizada para juegos de negociación de dos personas con información incompleta. Management Science, 18 (5): 80-106, 1972. [9] D. Koller y A. Pfeffer. Generar y resolver juegos de información imperfectos. En IJCAI, páginas 1185-1193, 1995. [10] D. Koller y A. Pfeffer. Representaciones y soluciones para problemas teóricos del juego. Inteligencia Artificial, 94 (1): 167-215, 1997. [11] C. Lemke y J. Howson. Puntos de equilibrio de los juegos de bimatriz. Journal of the Society for Industrial and Applied Mathematics, 12: 413-423, 1964. [12] R. J. Lipton, E. Markakis y A. Mehta. Jugando juegos grandes usando estrategias simples. En la Conferencia ACM sobre Comercio Electrónico, 2003. [13] A. Machado, G. Ramalho, J. D. Zucker y A. Drougul. Patrulla de múltiples agentes: un análisis empírico sobre arquitecturas alternativas. En Mabs, 2002. [14] P. Paruchuri, M. Tambe, F. Ordonez y S. Kraus. Seguridad en sistemas multiagentes por aleatorización de políticas. En Aamas, 2006. [15] T. Roughgarden. Estrategias de programación de Stackelberg. En el Simposio ACM sobre TOC, 2001. [16] S. Ruan, C. Meirina, F. Yu, K. R. Pattipati y R. L. Popp. Patrullar en un entorno estocástico. En décimo intl. Comando y control de la investigación del Symp., 2005. [17] T. Sandholm, A. Gilpin y V. Conitzer. Métodos de programación de enteros mixtos para encontrar equilibrios de Nash. En AAAI, 2005. [18] S. Singh, V. Soni y M. Wellman. Calculación de equilibrio de bayes de bayes aproximado con juegos de árboles de información incompleta. En la Conferencia ACM sobre Comercio Electrónico, 2004. 318 El sexto intl. Conf.en agentes autónomos y sistemas de múltiples agentes (AAMAS 07)",
    "original_sentences": [
        "An Efficient Heuristic Approach for Security Against Multiple Adversaries Praveen Paruchuri, Jonathan P. Pearce, Milind Tambe, Fernando Ordonez University of Southern California Los Angeles, CA 90089 {paruchur, jppearce, tambe, fordon}@usc.edu Sarit Kraus Bar-Ilan University Ramat-Gan 52900, Israel sarit@cs.biu.ac.il ABSTRACT In adversarial multiagent domains, security, commonly defined as the ability to deal with intentional threats from other agents, is a critical issue.",
        "This paper focuses on domains where these threats come from unknown adversaries.",
        "These domains can be modeled as Bayesian games; much work has been done on finding equilibria for such games.",
        "However, it is often the case in multiagent security domains that one agent can commit to a mixed strategy which its adversaries observe before choosing their own strategies.",
        "In this case, the agent can maximize reward by finding an optimal strategy, without requiring equilibrium.",
        "Previous work has shown this problem of optimal strategy selection to be NP-hard.",
        "Therefore, we present a heuristic called ASAP, with three key advantages to address the problem.",
        "First, ASAP searches for the highest-reward strategy, rather than a Bayes-Nash equilibrium, allowing it to find feasible strategies that exploit the natural first-mover advantage of the game.",
        "Second, it provides strategies which are simple to understand, represent, and implement.",
        "Third, it operates directly on the compact, Bayesian game representation, without requiring conversion to normal form.",
        "We provide an efficient Mixed Integer Linear Program (MILP) implementation for ASAP, along with experimental results illustrating significant speedups and higher rewards over other approaches.",
        "Categories and Subject Descriptors I.2.11 [Computing Methodologies]: Artificial Intelligence: Distributed Artificial Intelligence - Intelligent Agents General Terms Security, Design, Theory 1.",
        "INTRODUCTION In many multiagent domains, agents must act in order to provide security against attacks by adversaries.",
        "A common issue that agents face in such security domains is uncertainty about the adversaries they may be facing.",
        "For example, a security robot may need to make a choice about which areas to patrol, and how often [16].",
        "However, it will not know in advance exactly where a robber will choose to strike.",
        "A team of unmanned aerial vehicles (UAVs) [1] monitoring a region undergoing a humanitarian crisis may also need to choose a patrolling policy.",
        "They must make this decision without knowing in advance whether terrorists or other adversaries may be waiting to disrupt the mission at a given location.",
        "It may indeed be possible to model the motivations of types of adversaries the agent or agent team is likely to face in order to target these adversaries more closely.",
        "However, in both cases, the security robot or UAV team will not know exactly which kinds of adversaries may be active on any given day.",
        "A common approach for choosing a policy for agents in such scenarios is to model the scenarios as Bayesian games.",
        "A Bayesian game is a game in which agents may belong to one or more types; the type of an agent determines its possible actions and payoffs.",
        "The distribution of adversary types that an agent will face may be known or inferred from historical data.",
        "Usually, these games are analyzed according to the solution concept of a Bayes-Nash equilibrium, an extension of the Nash equilibrium for Bayesian games.",
        "However, in many settings, a Nash or Bayes-Nash equilibrium is not an appropriate solution concept, since it assumes that the agents strategies are chosen simultaneously [5].",
        "In some settings, one player can (or must) commit to a strategy before the other players choose their strategies.",
        "These scenarios are known as Stackelberg games [6].",
        "In a Stackelberg game, a leader commits to a strategy first, and then a follower (or group of followers) selfishly optimize their own rewards, considering the action chosen by the leader.",
        "For example, the security agent (leader) must first commit to a strategy for patrolling various areas.",
        "This strategy could be a mixed strategy in order to be unpredictable to the robbers (followers).",
        "The robbers, after observing the pattern of patrols over time, can then choose their strategy (which location to rob).",
        "Often, the leader in a Stackelberg game can attain a higher reward than if the strategies were chosen simultaneously.",
        "To see the advantage of being the leader in a Stackelberg game, consider a simple game with the payoff table as shown in Table 1.",
        "The leader is the row player and the follower is the column player.",
        "Here, the leaders payoff is listed first. 1 2 3 1 5,5 0,0 3,10 2 0,0 2,2 5,0 Table 1: Payoff table for example normal form game.",
        "The only Nash equilibrium for this game is when the leader plays 2 and the follower plays 2 which gives the leader a payoff of 2. 311 978-81-904262-7-5 (RPS) c 2007 IFAAMAS However, if the leader commits to a uniform mixed strategy of playing 1 and 2 with equal (0.5) probability, the followers best response is to play 3 to get an expected payoff of 5 (10 and 0 with equal probability).",
        "The leaders payoff would then be 4 (3 and 5 with equal probability).",
        "In this case, the leader now has an incentive to deviate and choose a pure strategy of 2 (to get a payoff of 5).",
        "However, this would cause the follower to deviate to strategy 2 as well, resulting in the Nash equilibrium.",
        "Thus, by committing to a strategy that is observed by the follower, and by avoiding the temptation to deviate, the leader manages to obtain a reward higher than that of the best Nash equilibrium.",
        "The problem of choosing an optimal strategy for the leader to commit to in a Stackelberg game is analyzed in [5] and found to be NP-hard in the case of a Bayesian game with multiple types of followers.",
        "Thus, efficient heuristic techniques for choosing highreward strategies in these games is an important open issue.",
        "Methods for finding optimal leader strategies for non-Bayesian games [5] can be applied to this problem by converting the Bayesian game into a normal-form game by the Harsanyi transformation [8].",
        "If, on the other hand, we wish to compute the highest-reward Nash equilibrium, new methods using mixed-integer linear programs (MILPs) [17] may be used, since the highest-reward Bayes-Nash equilibrium is equivalent to the corresponding Nash equilibrium in the transformed game.",
        "However, by transforming the game, the compact structure of the Bayesian game is lost.",
        "In addition, since the Nash equilibrium assumes a simultaneous choice of strategies, the advantages of being the leader are not considered.",
        "This paper introduces an efficient heuristic method for approximating the optimal leader strategy for security domains, known as ASAP (Agent Security via Approximate Policies).",
        "This method has three key advantages.",
        "First, it directly searches for an optimal strategy, rather than a Nash (or Bayes-Nash) equilibrium, thus allowing it to find high-reward non-equilibrium strategies like the one in the above example.",
        "Second, it generates policies with a support which can be expressed as a uniform distribution over a multiset of fixed size as proposed in [12].",
        "This allows for policies that are simple to understand and represent [12], as well as a tunable parameter (the size of the multiset) that controls the simplicity of the policy.",
        "Third, the method allows for a Bayes-Nash game to be expressed compactly without conversion to a normal-form game, allowing for large speedups over existing Nash methods such as [17] and [11].",
        "The rest of the paper is organized as follows.",
        "In Section 2 we fully describe the patrolling domain and its properties.",
        "Section 3 introduces the Bayesian game, the Harsanyi transformation, and existing methods for finding an optimal leaders strategy in a Stackelberg game.",
        "Then, in Section 4 the ASAP algorithm is presented for normal-form games, and in Section 5 we show how it can be adapted to the structure of Bayesian games with uncertain adversaries.",
        "Experimental results showing higher reward and faster policy computation over existing Nash methods are shown in Section 6, and we conclude with a discussion of related work in Section 7. 2.",
        "THE PATROLLING DOMAIN In most security patrolling domains, the security agents (like UAVs [1] or security robots [16]) cannot feasibly patrol all areas all the time.",
        "Instead, they must choose a policy by which they patrol various routes at different times, taking into account factors such as the likelihood of crime in different areas, possible targets for crime, and the security agents own resources (number of security agents, amount of available time, fuel, etc.).",
        "It is usually beneficial for this policy to be nondeterministic so that robbers cannot safely rob certain locations, knowing that they will be safe from the security agents [14].",
        "To demonstrate the utility of our algorithm, we use a simplified version of such a domain, expressed as a game.",
        "The most basic version of our game consists of two players: the security agent (the leader) and the robber (the follower) in a world consisting of m houses, 1 . . . m. The security agents set of pure strategies consists of possible routes of d houses to patrol (in an order).",
        "The security agent can choose a mixed strategy so that the robber will be unsure of exactly where the security agent may patrol, but the robber will know the mixed strategy the security agent has chosen.",
        "For example, the robber can observe over time how often the security agent patrols each area.",
        "With this knowledge, the robber must choose a single house to rob.",
        "We assume that the robber generally takes a long time to rob a house.",
        "If the house chosen by the robber is not on the security agents route, then the robber successfully robs it.",
        "Otherwise, if it is on the security agents route, then the earlier the house is on the route, the easier it is for the security agent to catch the robber before he finishes robbing it.",
        "We model the payoffs for this game with the following variables: • vl,x: value of the goods in house l to the security agent. • vl,q: value of the goods in house l to the robber. • cx: reward to the security agent of catching the robber. • cq: cost to the robber of getting caught. • pl: probability that the security agent can catch the robber at the lth house in the patrol (pl < pl ⇐⇒ l < l).",
        "The security agents set of possible pure strategies (patrol routes) is denoted by X and includes all d-tuples i =< w1, w2, ..., wd > with w1 . . . wd = 1 . . . m where no two elements are equal (the agent is not allowed to return to the same house).",
        "The robbers set of possible pure strategies (houses to rob) is denoted by Q and includes all integers j = 1 . . . m. The payoffs (security agent, robber) for pure strategies i, j are: • −vl,x, vl,q, for j = l /∈ i. • plcx +(1−pl)(−vl,x), −plcq +(1−pl)(vl,q), for j = l ∈ i.",
        "With this structure it is possible to model many different types of robbers who have differing motivations; for example, one robber may have a lower cost of getting caught than another, or may value the goods in the various houses differently.",
        "If the distribution of different robber types is known or inferred from historical data, then the game can be modeled as a Bayesian game [6]. 3.",
        "BAYESIAN GAMES A Bayesian game contains a set of N agents, and each agent n must be one of a given set of types θn.",
        "For our patrolling domain, we have two agents, the security agent and the robber. θ1 is the set of security agent types and θ2 is the set of robber types.",
        "Since there is only one type of security agent, θ1 contains only one element.",
        "During the game, the robber knows its type but the security agent does not know the robbers type.",
        "For each agent (the security agent or the robber) n, there is a set of strategies σn and a utility function un : θ1 × θ2 × σ1 × σ2 → .",
        "A Bayesian game can be transformed into a normal-form game using the Harsanyi transformation [8].",
        "Once this is done, new, linear-program (LP)-based methods for finding high-reward strategies for normal-form games [5] can be used to find a strategy in the transformed game; this strategy can then be used for the Bayesian game.",
        "While methods exist for finding Bayes-Nash equilibria directly, without the Harsanyi transformation [10], they find only a 312 The Sixth Intl.",
        "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) single equilibrium in the general case, which may not be of high reward.",
        "Recent work [17] has led to efficient mixed-integer linear program techniques to find the best Nash equilibrium for a given agent.",
        "However, these techniques do require a normal-form game, and so to compare the policies given by ASAP against the optimal policy, as well as against the highest-reward Nash equilibrium, we must apply these techniques to the Harsanyi-transformed matrix.",
        "The next two subsections elaborate on how this is done. 3.1 Harsanyi Transformation The first step in solving Bayesian games is to apply the Harsanyi transformation [8] that converts the Bayesian game into a normal form game.",
        "Given that the Harsanyi transformation is a standard concept in game theory, we explain it briefly through a simple example in our patrolling domain without introducing the mathematical formulations.",
        "Let us assume there are two robber types a and b in the Bayesian game.",
        "Robber a will be active with probability α, and robber b will be active with probability 1 − α.",
        "The rules described in Section 2 allow us to construct simple payoff tables.",
        "Assume that there are two houses in the world (1 and 2) and hence there are two patrol routes (pure strategies) for the agent: {1,2} and {2,1}.",
        "The robber can rob either house 1 or house 2 and hence he has two strategies (denoted as 1l, 2l for robber type l).",
        "Since there are two types assumed (denoted as a and b), we construct two payoff tables (shown in Table 2) corresponding to the security agent playing a separate game with each of the two robber types with probabilities α and 1 − α.",
        "First, consider robber type a.",
        "Borrowing the notation from the domain section, we assign the following values to the variables: v1,x = v1,q = 3/4, v2,x = v2,q = 1/4, cx = 1/2, cq = 1, p1 = 1, p2 = 1/2.",
        "Using these values we construct a base payoff table as the payoff for the game against robber type a.",
        "For example, if the security agent chooses route {1,2} when robber a is active, and robber a chooses house 1, the robber receives a reward of -1 (for being caught) and the agent receives a reward of 0.5 for catching the robber.",
        "The payoffs for the game against robber type b are constructed using different values.",
        "Security agent: {1,2} {2,1} Robber a 1a -1, .5 -.375, .125 2a -.125, -.125 -1, .5 Robber b 1b -.9, .6 -.275, .225 2b -.025, -.025 -.9, .6 Table 2: Payoff tables: Security Agent vs Robbers a and b Using the Harsanyi technique involves introducing a chance node, that determines the robbers type, thus transforming the security agents incomplete information regarding the robber into imperfect information [3].",
        "The Bayesian equilibrium of the game is then precisely the Nash equilibrium of the imperfect information game.",
        "The transformed, normal-form game is shown in Table 3.",
        "In the transformed game, the security agent is the column player, and the set of all robber types together is the row player.",
        "Suppose that robber type a robs house 1 and robber type b robs house 2, while the security agent chooses patrol {1,2}.",
        "Then, the security agent and the robber receive an expected payoff corresponding to their payoffs from the agent encountering robber a at house 1 with probability α and robber b at house 2 with probability 1 − α. 3.2 Finding an Optimal Strategy Although a Nash equilibrium is the standard solution concept for games in which agents choose strategies simultaneously, in our security domain, the security agent (the leader) can gain an advantage by committing to a mixed strategy in advance.",
        "Since the followers (the robbers) will know the leaders strategy, the optimal response for the followers will be a pure strategy.",
        "Given the common assumption, taken in [5], in the case where followers are indifferent, they will choose the strategy that benefits the leader, there must exist a guaranteed optimal strategy for the leader [5].",
        "From the Bayesian game in Table 2, we constructed the Harsanyi transformed bimatrix in Table 3.",
        "The strategies for each player (security agent or robber) in the transformed game correspond to all combinations of possible strategies taken by each of that players types.",
        "Therefore, we denote X = σθ1 1 = σ1 and Q = σθ2 2 as the index sets of the security agent and robbers pure strategies respectively, with R and C as the corresponding payoff matrices.",
        "Rij is the reward of the security agent and Cij is the reward of the robbers when the security agent takes pure strategy i and the robbers take pure strategy j.",
        "A mixed strategy for the security agent is a probability distribution over its set of pure strategies and will be represented by a vector x = (px1, px2, . . . , px|X|), where pxi ≥ 0 and P pxi = 1.",
        "Here, pxi is the probability that the security agent will choose its ith pure strategy.",
        "The optimal mixed strategy for the security agent can be found in time polynomial in the number of rows in the normal form game using the following linear program formulation from [5].",
        "For every possible pure strategy j by the follower (the set of all robber types), max P i∈X pxiRij s.t. ∀j ∈ Q, P i∈σ1 pxiCij ≥ P i∈σ1 pxiCij P i∈X pxi = 1 ∀i∈X , pxi >= 0 (1) Then, for all feasible follower strategies j, choose the one that maximizes P i∈X pxiRij, the reward for the security agent (leader).",
        "The pxi variables give the optimal strategy for the security agent.",
        "Note that while this method is polynomial in the number of rows in the transformed, normal-form game, the number of rows increases exponentially with the number of robber types.",
        "Using this method for a Bayesian game thus requires running |σ2||θ2| separate linear programs.",
        "This is no surprise, since finding the leaders optimal strategy in a Bayesian Stackelberg game is NP-hard [5]. 4.",
        "HEURISTIC APPROACHES Given that finding the optimal strategy for the leader is NP-hard, we provide a heuristic approach.",
        "In this heuristic we limit the possible mixed strategies of the leader to select actions with probabilities that are integer multiples of 1/k for a predetermined integer k. Previous work [14] has shown that strategies with high entropy are beneficial for security applications when opponents utilities are completely unknown.",
        "In our domain, if utilities are not considered, this method will result in uniform-distribution strategies.",
        "One advantage of such strategies is that they are compact to represent (as fractions) and simple to understand; therefore they can be efficiently implemented by real organizations.",
        "We aim to maintain the advantage provided by simple strategies for our security application problem, incorporating the effect of the robbers rewards on the security agents rewards.",
        "Thus, the ASAP heuristic will produce strategies which are k-uniform.",
        "A mixed strategy is denoted k-uniform if it is a uniform distribution on a multiset S of pure strategies with |S| = k. A multiset is a set whose elements may be repeated multiple times; thus, for example, the mixed strategy corresponding to the multiset {1, 1, 2} would take strategy 1 The Sixth Intl.",
        "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) 313 {1,2} {2,1} {1a, 1b} −1α − .9(1 − α), .5α + .6(1 − α) −.375α − .275(1 − α), .125α + .225(1 − α) {1a, 2b} −1α − .025(1 − α), .5α − .025(1 − α) −.375α − .9(1 − α), .125α + .6(1 − α) {2a, 1b} −.125α − .9(1 − α), −.125α + .6(1 − α) −1α − .275(1 − α), .5α + .225(1 − α) {2a, 2b} −.125α − .025(1 − α), −.125α − .025(1 − α) −1α − .9(1 − α), .5α + .6(1 − α) Table 3: Harsanyi Transformed Payoff Table with probability 2/3 and strategy 2 with probability 1/3.",
        "ASAP allows the size of the multiset to be chosen in order to balance the complexity of the strategy reached with the goal that the identified strategy will yield a high reward.",
        "Another advantage of the ASAP heuristic is that it operates directly on the compact Bayesian representation, without requiring the Harsanyi transformation.",
        "This is because the different follower (robber) types are independent of each other.",
        "Hence, evaluating the leader strategy against a Harsanyi-transformed game matrix is equivalent to evaluating against each of the game matrices for the individual follower types.",
        "This independence property is exploited in ASAP to yield a decomposition scheme.",
        "Note that the LP method introduced by [5] to compute optimal Stackelberg policies is unlikely to be decomposable into a small number of games as it was shown to be NP-hard for Bayes-Nash problems.",
        "Finally, note that ASAP requires the solution of only one optimization problem, rather than solving a series of problems as in the LP method of [5].",
        "For a single follower type, the algorithm works the following way.",
        "Given a particular k, for each possible mixed strategy x for the leader that corresponds to a multiset of size k, evaluate the leaders payoff from x when the follower plays a reward-maximizing pure strategy.",
        "We then take the mixed strategy with the highest payoff.",
        "We need only to consider the reward-maximizing pure strategies of the followers (robbers), since for a given fixed strategy x of the security agent, each robber type faces a problem with fixed linear rewards.",
        "If a mixed strategy is optimal for the robber, then so are all the pure strategies in the support of that mixed strategy.",
        "Note also that because we limit the leaders strategies to take on discrete values, the assumption from Section 3.2 that the followers will break ties in the leaders favor is not significant, since ties will be unlikely to arise.",
        "This is because, in domains where rewards are drawn from any random distribution, the probability of a follower having more than one pure optimal response to a given leader strategy approaches zero, and the leader will have only a finite number of possible mixed strategies.",
        "Our approach to characterize the optimal strategy for the security agent makes use of properties of linear programming.",
        "We briefly outline these results here for completeness, for detailed discussion and proofs see one of many references on the topic, such as [2].",
        "Every linear programming problem, such as: max cT x | Ax = b, x ≥ 0, has an associated dual linear program, in this case: min bT y | AT y ≥ c. These primal/dual pairs of problems satisfy weak duality: For any x and y primal and dual feasible solutions respectively, cT x ≤ bT y.",
        "Thus a pair of feasible solutions is optimal if cT x = bT y, and the problems are said to satisfy strong duality.",
        "In fact if a linear program is feasible and has a bounded optimal solution, then the dual is also feasible and there is a pair x∗ , y∗ that satisfies cT x∗ = bT y∗ .",
        "These optimal solutions are characterized with the following optimality conditions (as defined in [2]): • primal feasibility: Ax = b, x ≥ 0 • dual feasibility: AT y ≥ c • complementary slackness: xi(AT y − c)i = 0 for all i.",
        "Note that this last condition implies that cT x = xT AT y = bT y, which proves optimality for primal dual feasible solutions x and y.",
        "In the following subsections, we first define the problem in its most intuititive form as a mixed-integer quadratic program (MIQP), and then show how this problem can be converted into a mixedinteger linear program (MILP). 4.1 Mixed-Integer Quadratic Program We begin with the case of a single type of follower.",
        "Let the leader be the row player and the follower the column player.",
        "We denote by x the vector of strategies of the leader and q the vector of strategies of the follower.",
        "We also denote X and Q the index sets of the leader and followers pure strategies, respectively.",
        "The payoff matrices R and C correspond to: Rij is the reward of the leader and Cij is the reward of the follower when the leader takes pure strategy i and the follower takes pure strategy j.",
        "Let k be the size of the multiset.",
        "We first fix the policy of the leader to some k-uniform policy x.",
        "The value xi is the number of times pure strategy i is used in the k-uniform policy, which is selected with probability xi/k.",
        "We formulate the optimization problem the follower solves to find its optimal response to x as the following linear program: max X j∈Q X i∈X 1 k Cijxi qj s.t.",
        "P j∈Q qj = 1 q ≥ 0. (2) The objective function maximizes the followers expected reward given x, while the constraints make feasible any mixed strategy q for the follower.",
        "The dual to this linear programming problem is the following: min a s.t. a ≥ X i∈X 1 k Cijxi j ∈ Q. (3) From strong duality and complementary slackness we obtain that the followers maximum reward value, a, is the value of every pure strategy with qj > 0, that is in the support of the optimal mixed strategy.",
        "Therefore each of these pure strategies is optimal.",
        "Optimal solutions to the followers problem are characterized by linear programming optimality conditions: primal feasibility constraints in (2), dual feasibility constraints in (3), and complementary slackness qj a − X i∈X 1 k Cijxi ! = 0 j ∈ Q.",
        "These conditions must be included in the problem solved by the leader in order to consider only best responses by the follower to the k-uniform policy x. 314 The Sixth Intl.",
        "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) The leader seeks the k-uniform solution x that maximizes its own payoff, given that the follower uses an optimal response q(x).",
        "Therefore the leader solves the following integer problem: max X i∈X X j∈Q 1 k Rijq(x)j xi s.t.",
        "P i∈X xi = k xi ∈ {0, 1, . . . , k}. (4) Problem (4) maximizes the leaders reward with the followers best response (qj for fixed leaders policy x and hence denoted q(x)j) by selecting a uniform policy from a multiset of constant size k. We complete this problem by including the characterization of q(x) through linear programming optimality conditions.",
        "To simplify writing the complementary slackness conditions, we will constrain q(x) to be only optimal pure strategies by just considering integer solutions of q(x).",
        "The leaders problem becomes: maxx,q X i∈X X j∈Q 1 k Rijxiqj s.t.",
        "P i xi = kP j∈Q qj = 1 0 ≤ (a − P i∈X 1 k Cijxi) ≤ (1 − qj)M xi ∈ {0, 1, ...., k} qj ∈ {0, 1}. (5) Here, the constant M is some large number.",
        "The first and fourth constraints enforce a k-uniform policy for the leader, and the second and fifth constraints enforce a feasible pure strategy for the follower.",
        "The third constraint enforces dual feasibility of the followers problem (leftmost inequality) and the complementary slackness constraint for an optimal pure strategy q for the follower (rightmost inequality).",
        "In fact, since only one pure strategy can be selected by the follower, say qh = 1, this last constraint enforces that a = P i∈X 1 k Cihxi imposing no additional constraint for all other pure strategies which have qj = 0.",
        "We conclude this subsection noting that Problem (5) is an integer program with a non-convex quadratic objective in general, as the matrix R need not be positive-semi-definite.",
        "Efficient solution methods for non-linear, non-convex integer problems remains a challenging research question.",
        "In the next section we show a reformulation of this problem as a linear integer programming problem, for which a number of efficient commercial solvers exist. 4.2 Mixed-Integer Linear Program We can linearize the quadratic program of Problem 5 through the change of variables zij = xiqj, obtaining the following problem maxq,z P i∈X P j∈Q 1 k Rijzij s.t.",
        "P i∈X P j∈Q zij = k P j∈Q zij ≤ k kqj ≤ P i∈X zij ≤ k P j∈Q qj = 1 0 ≤ (a − P i∈X 1 k Cij( P h∈Q zih)) ≤ (1 − qj)M zij ∈ {0, 1, ...., k} qj ∈ {0, 1} (6) PROPOSITION 1.",
        "Problems (5) and (6) are equivalent.",
        "Proof: Consider x, q a feasible solution of (5).",
        "We will show that q, zij = xiqj is a feasible solution of (6) of same objective function value.",
        "The equivalence of the objective functions, and constraints 4, 6 and 7 of (6) are satisfied by construction.",
        "The fact that P j∈Q zij = xi as P j∈Q qj = 1 explains constraints 1, 2, and 5 of (6).",
        "Constraint 3 of (6) is satisfied because P i∈X zij = kqj.",
        "Let us now consider q, z feasible for (6).",
        "We will show that q and xi = P j∈Q zij are feasible for (5) with the same objective value.",
        "In fact all constraints of (5) are readily satisfied by construction.",
        "To see that the objectives match, notice that if qh = 1 then the third constraint in (6) implies that P i∈X zih = k, which means that zij = 0 for all i ∈ X and all j = h. Therefore, xiqj = X l∈Q zilqj = zihqj = zij.",
        "This last equality is because both are 0 when j = h. This shows that the transformation preserves the objective function value, completing the proof.",
        "Given this transformation to a mixed-integer linear program (MILP), we now show how we can apply our decomposition technique on the MILP to obtain significant speedups for Bayesian games with multiple follower types. 5.",
        "DECOMPOSITION FOR MULTIPLE ADVERSARIES The MILP developed in the previous section handles only one follower.",
        "Since our security scenario contains multiple follower (robber) types, we change the response function for the follower from a pure strategy into a weighted combination over various pure follower strategies where the weights are probabilities of occurrence of each of the follower types. 5.1 Decomposed MIQP To admit multiple adversaries in our framework, we modify the notation defined in the previous section to reason about multiple follower types.",
        "We denote by x the vector of strategies of the leader and ql the vector of strategies of follower l, with L denoting the index set of follower types.",
        "We also denote by X and Q the index sets of leader and follower ls pure strategies, respectively.",
        "We also index the payoff matrices on each follower l, considering the matrices Rl and Cl .",
        "Using this modified notation, we characterize the optimal solution of follower ls problem given the leaders k-uniform policy x, with the following optimality conditions: X j∈Q ql j = 1 al − X i∈X 1 k Cl ijxi ≥ 0 ql j(al − X i∈X 1 k Cl ijxi) = 0 ql j ≥ 0 Again, considering only optimal pure strategies for follower ls problem we can linearize the complementarity constraint above.",
        "We incorporate these constraints on the leaders problem that selects the optimal k-uniform policy.",
        "Therefore, given a priori probabilities pl , with l ∈ L of facing each follower, the leader solves the following problem: The Sixth Intl.",
        "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) 315 maxx,q X i∈X X l∈L X j∈Q pl k Rl ijxiql j s.t.",
        "P i xi = kP j∈Q ql j = 1 0 ≤ (al − P i∈X 1 k Cl ijxi) ≤ (1 − ql j)M xi ∈ {0, 1, ...., k} ql j ∈ {0, 1}. (7) Problem (7) for a Bayesian game with multiple follower types is indeed equivalent to Problem (5) on the payoff matrix obtained from the Harsanyi transformation of the game.",
        "In fact, every pure strategy j in Problem (5) corresponds to a sequence of pure strategies jl, one for each follower l ∈ L. This means that qj = 1 if and only if ql jl = 1 for all l ∈ L. In addition, given the a priori probabilities pl of facing player l, the reward in the Harsanyi transformation payoff table is Rij = P l∈L pl Rl ijl .",
        "The same relation holds between C and Cl .",
        "These relations between a pure strategy in the equivalent normal form game and pure strategies in the individual games with each followers are key in showing these problems are equivalent. 5.2 Decomposed MILP We can linearize the quadratic programming problem 7 through the change of variables zl ij = xiql j, obtaining the following problem maxq,z P i∈X P l∈L P j∈Q pl k Rl ijzl ij s.t.",
        "P i∈X P j∈Q zl ij = k P j∈Q zl ij ≤ k kql j ≤ P i∈X zl ij ≤ k P j∈Q ql j = 1 0 ≤ (al − P i∈X 1 k Cl ij( P h∈Q zl ih)) ≤ (1 − ql j)M P j∈Q zl ij = P j∈Q z1 ij zl ij ∈ {0, 1, ...., k} ql j ∈ {0, 1} (8) PROPOSITION 2.",
        "Problems (7) and (8) are equivalent.",
        "Proof: Consider x, ql , al with l ∈ L a feasible solution of (7).",
        "We will show that ql , al , zl ij = xiql j is a feasible solution of (8) of same objective function value.",
        "The equivalence of the objective functions, and constraints 4, 7 and 8 of (8) are satisfied by construction.",
        "The fact that P j∈Q zl ij = xi as P j∈Q ql j = 1 explains constraints 1, 2, 5 and 6 of (8).",
        "Constraint 3 of (8) is satisfied because P i∈X zl ij = kql j.",
        "Lets now consider ql , zl , al feasible for (8).",
        "We will show that ql , al and xi = P j∈Q z1 ij are feasible for (7) with the same objective value.",
        "In fact all constraints of (7) are readily satisfied by construction.",
        "To see that the objectives match, notice for each l one ql j must equal 1 and the rest equal 0.",
        "Let us say that ql jl = 1, then the third constraint in (8) implies that P i∈X zl ijl = k, which means that zl ij = 0 for all i ∈ X and all j = jl.",
        "In particular this implies that xi = X j∈Q z1 ij = z1 ij1 = zl ijl , the last equality from constraint 6 of (8).",
        "Therefore xiql j = zl ijl ql j = zl ij.",
        "This last equality is because both are 0 when j = jl.",
        "Effectively, constraint 6 ensures that all the adversaries are calculating their best responses against a particular fixed policy of the agent.",
        "This shows that the transformation preserves the objective function value, completing the proof.",
        "We can therefore solve this equivalent linear integer program with efficient integer programming packages which can handle problems with thousands of integer variables.",
        "We implemented the decomposed MILP and the results are shown in the following section. 6.",
        "EXPERIMENTAL RESULTS The patrolling domain and the payoffs for the associated game are detailed in Sections 2 and 3.",
        "We performed experiments for this game in worlds of three and four houses with patrols consisting of two houses.",
        "The description given in Section 2 is used to generate a base case for both the security agent and robber payoff functions.",
        "The payoff tables for additional robber types are constructed and added to the game by adding a random distribution of varying size to the payoffs in the base case.",
        "All games are normalized so that, for each robber type, the minimum and maximum payoffs to the security agent and robber are 0 and 1, respectively.",
        "Using the data generated, we performed the experiments using four methods for generating the security agents strategy: • uniform randomization • ASAP • the multiple linear programs method from [5] (to find the true optimal strategy) • the highest reward Bayes-Nash equilibrium, found using the MIP-Nash algorithm [17] The last three methods were applied using CPLEX 8.1.",
        "Because the last two methods are designed for normal-form games rather than Bayesian games, the games were first converted using the Harsanyi transformation [8].",
        "The uniform randomization method is simply choosing a uniform random policy over all possible patrol routes.",
        "We use this method as a simple baseline to measure the performance of our heuristics.",
        "We anticipated that the uniform policy would perform reasonably well since maximum-entropy policies have been shown to be effective in multiagent security domains [14].",
        "The highest-reward Bayes-Nash equilibria were used in order to demonstrate the higher reward gained by looking for an optimal policy rather than an equilibria in Stackelberg games such as our security domain.",
        "Based on our experiments we present three sets of graphs to demonstrate (1) the runtime of ASAP compared to other common methods for finding a strategy, (2) the reward guaranteed by ASAP compared to other methods, and (3) the effect of varying the parameter k, the size of the multiset, on the performance of ASAP.",
        "In the first two sets of graphs, ASAP is run using a multiset of 80 elements; in the third set this number is varied.",
        "The first set of graphs, shown in Figure 1 shows the runtime graphs for three-house (left column) and four-house (right column) domains.",
        "Each of the three rows of graphs corresponds to a different randomly-generated scenario.",
        "The x-axis shows the number of robber types the security agent faces and the y-axis of the graph shows the runtime in seconds.",
        "All experiments that were not concluded in 30 minutes (1800 seconds) were cut off.",
        "The runtime for the uniform policy is always negligible irrespective of the number of adversaries and hence is not shown.",
        "The ASAP algorithm clearly outperforms the optimal, multipleLP method as well as the MIP-Nash algorithm for finding the highestreward Bayes-Nash equilibrium with respect to runtime.",
        "For a 316 The Sixth Intl.",
        "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) Figure 1: Runtimes for various algorithms on problems of 3 and 4 houses. domain of three houses, the optimal method cannot reach a solution for more than seven robber types, and for four houses it cannot solve for more than six types within the cutoff time in any of the three scenarios.",
        "MIP-Nash solves for even fewer robber types within the cutoff time.",
        "On the other hand, ASAP runs much faster, and is able to solve for at least 20 adversaries for the three-house scenarios and for at least 12 adversaries in the four-house scenarios within the cutoff time.",
        "The runtime of ASAP does not increase strictly with the number of robber types for each scenario, but in general, the addition of more types increases the runtime required.",
        "The second set of graphs, Figure 2, shows the reward to the patrol agent given by each method for three scenarios in the three-house (left column) and four-house (right column) domains.",
        "This reward is the utility received by the security agent in the patrolling game, and not as a percentage of the optimal reward, since it was not possible to obtain the optimal reward as the number of robber types increased.",
        "The uniform policy consistently provides the lowest reward in both domains; while the optimal method of course produces the optimal reward.",
        "The ASAP method remains consistently close to the optimal, even as the number of robber types increases.",
        "The highest-reward Bayes-Nash equilibria, provided by the MIPNash method, produced rewards higher than the uniform method, but lower than ASAP.",
        "This difference clearly illustrates the gains in the patrolling domain from committing to a strategy as the leader in a Stackelberg game, rather than playing a standard Bayes-Nash strategy.",
        "The third set of graphs, shown in Figure 3 shows the effect of the multiset size on runtime in seconds (left column) and reward (right column), again expressed as the reward received by the security agent in the patrolling game, and not a percentage of the optimal Figure 2: Reward for various algorithms on problems of 3 and 4 houses. reward.",
        "Results here are for the three-house domain.",
        "The trend is that as as the multiset size is increased, the runtime and reward level both increase.",
        "Not surprisingly, the reward increases monotonically as the multiset size increases, but what is interesting is that there is relatively little benefit to using a large multiset in this domain.",
        "In all cases, the reward given by a multiset of 10 elements was within at least 96% of the reward given by an 80-element multiset.",
        "The runtime does not always increase strictly with the multiset size; indeed in one example (scenario 2 with 20 robber types), using a multiset of 10 elements took 1228 seconds, while using 80 elements only took 617 seconds.",
        "In general, runtime should increase since a larger multiset means a larger domain for the variables in the MILP, and thus a larger search space.",
        "However, an increase in the number of variables can sometimes allow for a policy to be constructed more quickly due to more flexibility in the problem. 7.",
        "SUMMARY AND RELATED WORK This paper focuses on security for agents patrolling in hostile environments.",
        "In these environments, intentional threats are caused by adversaries about whom the security patrolling agents have incomplete information.",
        "Specifically, we deal with situations where the adversaries actions and payoffs are known but the exact adversary type is unknown to the security agent.",
        "Agents acting in the real world quite frequently have such incomplete information about other agents.",
        "Bayesian games have been a popular choice to model such incomplete information games [3].",
        "The Gala toolkit is one method for defining such games [9] without requiring the game to be represented in normal form via the Harsanyi transformation [8]; Galas guarantees are focused on fully competitive games.",
        "Much work has been done on finding optimal Bayes-Nash equilbria for The Sixth Intl.",
        "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) 317 Figure 3: Reward for ASAP using multisets of 10, 30, and 80 elements subclasses of Bayesian games, finding single Bayes-Nash equilibria for general Bayesian games [10] or approximate Bayes-Nash equilibria [18].",
        "Less attention has been paid to finding the optimal strategy to commit to in a Bayesian game (the Stackelberg scenario [15]).",
        "However, the complexity of this problem was shown to be NP-hard in the general case [5], which also provides algorithms for this problem in the non-Bayesian case.",
        "Therefore, we present a heuristic called ASAP, with three key advantages towards addressing this problem.",
        "First, ASAP searches for the highest reward strategy, rather than a Bayes-Nash equilibrium, allowing it to find feasible strategies that exploit the natural first-mover advantage of the game.",
        "Second, it provides strategies which are simple to understand, represent, and implement.",
        "Third, it operates directly on the compact, Bayesian game representation, without requiring conversion to normal form.",
        "We provide an efficient Mixed Integer Linear Program (MILP) implementation for ASAP, along with experimental results illustrating significant speedups and higher rewards over other approaches.",
        "Our k-uniform strategies are similar to the k-uniform strategies of [12].",
        "While that work provides epsilon error-bounds based on the k-uniform strategies, their solution concept is still that of a Nash equilibrium, and they do not provide efficient algorithms for obtaining such k-uniform strategies.",
        "This contrasts with ASAP, where our emphasis is on a highly efficient heuristic approach that is not focused on equilibrium solutions.",
        "Finally the patrolling problem which motivated our work has recently received growing attention from the multiagent community due to its wide range of applications [4, 13].",
        "However most of this work is focused on either limiting energy consumption involved in patrolling [7] or optimizing on criteria like the length of the path traveled [4, 13], without reasoning about any explicit model of an adversary[14].",
        "Acknowledgments : This research is supported by the United States Department of Homeland Security through Center for Risk and Economic Analysis of Terrorism Events (CREATE).",
        "It is also supported by the Defense Advanced Research Projects Agency (DARPA), through the Department of the Interior, NBC, Acquisition Services Division, under Contract No.",
        "NBCHD030010.",
        "Sarit Kraus is also affiliated with UMIACS. 8.",
        "REFERENCES [1] R. W. Beard and T. McLain.",
        "Multiple UAV cooperative search under collision avoidance and limited range communication constraints.",
        "In IEEE CDC, 2003. [2] D. Bertsimas and J. Tsitsiklis.",
        "Introduction to Linear Optimization.",
        "Athena Scientific, 1997. [3] J. Brynielsson and S. Arnborg.",
        "Bayesian games for threat prediction and situation analysis.",
        "In FUSION, 2004. [4] Y. Chevaleyre.",
        "Theoretical analysis of multi-agent patrolling problem.",
        "In AAMAS, 2004. [5] V. Conitzer and T. Sandholm.",
        "Choosing the best strategy to commit to.",
        "In ACM Conference on Electronic Commerce, 2006. [6] D. Fudenberg and J. Tirole.",
        "Game Theory.",
        "MIT Press, 1991. [7] C. Gui and P. Mohapatra.",
        "Virtual patrol: A new power conservation design for surveillance using sensor networks.",
        "In IPSN, 2005. [8] J. C. Harsanyi and R. Selten.",
        "A generalized Nash solution for two-person bargaining games with incomplete information.",
        "Management Science, 18(5):80-106, 1972. [9] D. Koller and A. Pfeffer.",
        "Generating and solving imperfect information games.",
        "In IJCAI, pages 1185-1193, 1995. [10] D. Koller and A. Pfeffer.",
        "Representations and solutions for game-theoretic problems.",
        "Artificial Intelligence, 94(1):167-215, 1997. [11] C. Lemke and J. Howson.",
        "Equilibrium points of bimatrix games.",
        "Journal of the Society for Industrial and Applied Mathematics, 12:413-423, 1964. [12] R. J. Lipton, E. Markakis, and A. Mehta.",
        "Playing large games using simple strategies.",
        "In ACM Conference on Electronic Commerce, 2003. [13] A. Machado, G. Ramalho, J. D. Zucker, and A. Drougoul.",
        "Multi-agent patrolling: an empirical analysis on alternative architectures.",
        "In MABS, 2002. [14] P. Paruchuri, M. Tambe, F. Ordonez, and S. Kraus.",
        "Security in multiagent systems by policy randomization.",
        "In AAMAS, 2006. [15] T. Roughgarden.",
        "Stackelberg scheduling strategies.",
        "In ACM Symposium on TOC, 2001. [16] S. Ruan, C. Meirina, F. Yu, K. R. Pattipati, and R. L. Popp.",
        "Patrolling in a stochastic environment.",
        "In 10th Intl.",
        "Command and Control Research Symp., 2005. [17] T. Sandholm, A. Gilpin, and V. Conitzer.",
        "Mixed-integer programming methods for finding nash equilibria.",
        "In AAAI, 2005. [18] S. Singh, V. Soni, and M. Wellman.",
        "Computing approximate Bayes-Nash equilibria with tree-games of incomplete information.",
        "In ACM Conference on Electronic Commerce, 2004. 318 The Sixth Intl.",
        "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07)"
    ],
    "error_count": 0,
    "keys": {
        "adversarial multiagent domain": {
            "translated_key": "dominio multiagente adversario",
            "is_in_text": true,
            "original_annotated_sentences": [
                "An Efficient Heuristic Approach for Security Against Multiple Adversaries Praveen Paruchuri, Jonathan P. Pearce, Milind Tambe, Fernando Ordonez University of Southern California Los Angeles, CA 90089 {paruchur, jppearce, tambe, fordon}@usc.edu Sarit Kraus Bar-Ilan University Ramat-Gan 52900, Israel sarit@cs.biu.ac.il ABSTRACT In <br>adversarial multiagent domain</br>s, security, commonly defined as the ability to deal with intentional threats from other agents, is a critical issue.",
                "This paper focuses on domains where these threats come from unknown adversaries.",
                "These domains can be modeled as Bayesian games; much work has been done on finding equilibria for such games.",
                "However, it is often the case in multiagent security domains that one agent can commit to a mixed strategy which its adversaries observe before choosing their own strategies.",
                "In this case, the agent can maximize reward by finding an optimal strategy, without requiring equilibrium.",
                "Previous work has shown this problem of optimal strategy selection to be NP-hard.",
                "Therefore, we present a heuristic called ASAP, with three key advantages to address the problem.",
                "First, ASAP searches for the highest-reward strategy, rather than a Bayes-Nash equilibrium, allowing it to find feasible strategies that exploit the natural first-mover advantage of the game.",
                "Second, it provides strategies which are simple to understand, represent, and implement.",
                "Third, it operates directly on the compact, Bayesian game representation, without requiring conversion to normal form.",
                "We provide an efficient Mixed Integer Linear Program (MILP) implementation for ASAP, along with experimental results illustrating significant speedups and higher rewards over other approaches.",
                "Categories and Subject Descriptors I.2.11 [Computing Methodologies]: Artificial Intelligence: Distributed Artificial Intelligence - Intelligent Agents General Terms Security, Design, Theory 1.",
                "INTRODUCTION In many multiagent domains, agents must act in order to provide security against attacks by adversaries.",
                "A common issue that agents face in such security domains is uncertainty about the adversaries they may be facing.",
                "For example, a security robot may need to make a choice about which areas to patrol, and how often [16].",
                "However, it will not know in advance exactly where a robber will choose to strike.",
                "A team of unmanned aerial vehicles (UAVs) [1] monitoring a region undergoing a humanitarian crisis may also need to choose a patrolling policy.",
                "They must make this decision without knowing in advance whether terrorists or other adversaries may be waiting to disrupt the mission at a given location.",
                "It may indeed be possible to model the motivations of types of adversaries the agent or agent team is likely to face in order to target these adversaries more closely.",
                "However, in both cases, the security robot or UAV team will not know exactly which kinds of adversaries may be active on any given day.",
                "A common approach for choosing a policy for agents in such scenarios is to model the scenarios as Bayesian games.",
                "A Bayesian game is a game in which agents may belong to one or more types; the type of an agent determines its possible actions and payoffs.",
                "The distribution of adversary types that an agent will face may be known or inferred from historical data.",
                "Usually, these games are analyzed according to the solution concept of a Bayes-Nash equilibrium, an extension of the Nash equilibrium for Bayesian games.",
                "However, in many settings, a Nash or Bayes-Nash equilibrium is not an appropriate solution concept, since it assumes that the agents strategies are chosen simultaneously [5].",
                "In some settings, one player can (or must) commit to a strategy before the other players choose their strategies.",
                "These scenarios are known as Stackelberg games [6].",
                "In a Stackelberg game, a leader commits to a strategy first, and then a follower (or group of followers) selfishly optimize their own rewards, considering the action chosen by the leader.",
                "For example, the security agent (leader) must first commit to a strategy for patrolling various areas.",
                "This strategy could be a mixed strategy in order to be unpredictable to the robbers (followers).",
                "The robbers, after observing the pattern of patrols over time, can then choose their strategy (which location to rob).",
                "Often, the leader in a Stackelberg game can attain a higher reward than if the strategies were chosen simultaneously.",
                "To see the advantage of being the leader in a Stackelberg game, consider a simple game with the payoff table as shown in Table 1.",
                "The leader is the row player and the follower is the column player.",
                "Here, the leaders payoff is listed first. 1 2 3 1 5,5 0,0 3,10 2 0,0 2,2 5,0 Table 1: Payoff table for example normal form game.",
                "The only Nash equilibrium for this game is when the leader plays 2 and the follower plays 2 which gives the leader a payoff of 2. 311 978-81-904262-7-5 (RPS) c 2007 IFAAMAS However, if the leader commits to a uniform mixed strategy of playing 1 and 2 with equal (0.5) probability, the followers best response is to play 3 to get an expected payoff of 5 (10 and 0 with equal probability).",
                "The leaders payoff would then be 4 (3 and 5 with equal probability).",
                "In this case, the leader now has an incentive to deviate and choose a pure strategy of 2 (to get a payoff of 5).",
                "However, this would cause the follower to deviate to strategy 2 as well, resulting in the Nash equilibrium.",
                "Thus, by committing to a strategy that is observed by the follower, and by avoiding the temptation to deviate, the leader manages to obtain a reward higher than that of the best Nash equilibrium.",
                "The problem of choosing an optimal strategy for the leader to commit to in a Stackelberg game is analyzed in [5] and found to be NP-hard in the case of a Bayesian game with multiple types of followers.",
                "Thus, efficient heuristic techniques for choosing highreward strategies in these games is an important open issue.",
                "Methods for finding optimal leader strategies for non-Bayesian games [5] can be applied to this problem by converting the Bayesian game into a normal-form game by the Harsanyi transformation [8].",
                "If, on the other hand, we wish to compute the highest-reward Nash equilibrium, new methods using mixed-integer linear programs (MILPs) [17] may be used, since the highest-reward Bayes-Nash equilibrium is equivalent to the corresponding Nash equilibrium in the transformed game.",
                "However, by transforming the game, the compact structure of the Bayesian game is lost.",
                "In addition, since the Nash equilibrium assumes a simultaneous choice of strategies, the advantages of being the leader are not considered.",
                "This paper introduces an efficient heuristic method for approximating the optimal leader strategy for security domains, known as ASAP (Agent Security via Approximate Policies).",
                "This method has three key advantages.",
                "First, it directly searches for an optimal strategy, rather than a Nash (or Bayes-Nash) equilibrium, thus allowing it to find high-reward non-equilibrium strategies like the one in the above example.",
                "Second, it generates policies with a support which can be expressed as a uniform distribution over a multiset of fixed size as proposed in [12].",
                "This allows for policies that are simple to understand and represent [12], as well as a tunable parameter (the size of the multiset) that controls the simplicity of the policy.",
                "Third, the method allows for a Bayes-Nash game to be expressed compactly without conversion to a normal-form game, allowing for large speedups over existing Nash methods such as [17] and [11].",
                "The rest of the paper is organized as follows.",
                "In Section 2 we fully describe the patrolling domain and its properties.",
                "Section 3 introduces the Bayesian game, the Harsanyi transformation, and existing methods for finding an optimal leaders strategy in a Stackelberg game.",
                "Then, in Section 4 the ASAP algorithm is presented for normal-form games, and in Section 5 we show how it can be adapted to the structure of Bayesian games with uncertain adversaries.",
                "Experimental results showing higher reward and faster policy computation over existing Nash methods are shown in Section 6, and we conclude with a discussion of related work in Section 7. 2.",
                "THE PATROLLING DOMAIN In most security patrolling domains, the security agents (like UAVs [1] or security robots [16]) cannot feasibly patrol all areas all the time.",
                "Instead, they must choose a policy by which they patrol various routes at different times, taking into account factors such as the likelihood of crime in different areas, possible targets for crime, and the security agents own resources (number of security agents, amount of available time, fuel, etc.).",
                "It is usually beneficial for this policy to be nondeterministic so that robbers cannot safely rob certain locations, knowing that they will be safe from the security agents [14].",
                "To demonstrate the utility of our algorithm, we use a simplified version of such a domain, expressed as a game.",
                "The most basic version of our game consists of two players: the security agent (the leader) and the robber (the follower) in a world consisting of m houses, 1 . . . m. The security agents set of pure strategies consists of possible routes of d houses to patrol (in an order).",
                "The security agent can choose a mixed strategy so that the robber will be unsure of exactly where the security agent may patrol, but the robber will know the mixed strategy the security agent has chosen.",
                "For example, the robber can observe over time how often the security agent patrols each area.",
                "With this knowledge, the robber must choose a single house to rob.",
                "We assume that the robber generally takes a long time to rob a house.",
                "If the house chosen by the robber is not on the security agents route, then the robber successfully robs it.",
                "Otherwise, if it is on the security agents route, then the earlier the house is on the route, the easier it is for the security agent to catch the robber before he finishes robbing it.",
                "We model the payoffs for this game with the following variables: • vl,x: value of the goods in house l to the security agent. • vl,q: value of the goods in house l to the robber. • cx: reward to the security agent of catching the robber. • cq: cost to the robber of getting caught. • pl: probability that the security agent can catch the robber at the lth house in the patrol (pl < pl ⇐⇒ l < l).",
                "The security agents set of possible pure strategies (patrol routes) is denoted by X and includes all d-tuples i =< w1, w2, ..., wd > with w1 . . . wd = 1 . . . m where no two elements are equal (the agent is not allowed to return to the same house).",
                "The robbers set of possible pure strategies (houses to rob) is denoted by Q and includes all integers j = 1 . . . m. The payoffs (security agent, robber) for pure strategies i, j are: • −vl,x, vl,q, for j = l /∈ i. • plcx +(1−pl)(−vl,x), −plcq +(1−pl)(vl,q), for j = l ∈ i.",
                "With this structure it is possible to model many different types of robbers who have differing motivations; for example, one robber may have a lower cost of getting caught than another, or may value the goods in the various houses differently.",
                "If the distribution of different robber types is known or inferred from historical data, then the game can be modeled as a Bayesian game [6]. 3.",
                "BAYESIAN GAMES A Bayesian game contains a set of N agents, and each agent n must be one of a given set of types θn.",
                "For our patrolling domain, we have two agents, the security agent and the robber. θ1 is the set of security agent types and θ2 is the set of robber types.",
                "Since there is only one type of security agent, θ1 contains only one element.",
                "During the game, the robber knows its type but the security agent does not know the robbers type.",
                "For each agent (the security agent or the robber) n, there is a set of strategies σn and a utility function un : θ1 × θ2 × σ1 × σ2 → .",
                "A Bayesian game can be transformed into a normal-form game using the Harsanyi transformation [8].",
                "Once this is done, new, linear-program (LP)-based methods for finding high-reward strategies for normal-form games [5] can be used to find a strategy in the transformed game; this strategy can then be used for the Bayesian game.",
                "While methods exist for finding Bayes-Nash equilibria directly, without the Harsanyi transformation [10], they find only a 312 The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) single equilibrium in the general case, which may not be of high reward.",
                "Recent work [17] has led to efficient mixed-integer linear program techniques to find the best Nash equilibrium for a given agent.",
                "However, these techniques do require a normal-form game, and so to compare the policies given by ASAP against the optimal policy, as well as against the highest-reward Nash equilibrium, we must apply these techniques to the Harsanyi-transformed matrix.",
                "The next two subsections elaborate on how this is done. 3.1 Harsanyi Transformation The first step in solving Bayesian games is to apply the Harsanyi transformation [8] that converts the Bayesian game into a normal form game.",
                "Given that the Harsanyi transformation is a standard concept in game theory, we explain it briefly through a simple example in our patrolling domain without introducing the mathematical formulations.",
                "Let us assume there are two robber types a and b in the Bayesian game.",
                "Robber a will be active with probability α, and robber b will be active with probability 1 − α.",
                "The rules described in Section 2 allow us to construct simple payoff tables.",
                "Assume that there are two houses in the world (1 and 2) and hence there are two patrol routes (pure strategies) for the agent: {1,2} and {2,1}.",
                "The robber can rob either house 1 or house 2 and hence he has two strategies (denoted as 1l, 2l for robber type l).",
                "Since there are two types assumed (denoted as a and b), we construct two payoff tables (shown in Table 2) corresponding to the security agent playing a separate game with each of the two robber types with probabilities α and 1 − α.",
                "First, consider robber type a.",
                "Borrowing the notation from the domain section, we assign the following values to the variables: v1,x = v1,q = 3/4, v2,x = v2,q = 1/4, cx = 1/2, cq = 1, p1 = 1, p2 = 1/2.",
                "Using these values we construct a base payoff table as the payoff for the game against robber type a.",
                "For example, if the security agent chooses route {1,2} when robber a is active, and robber a chooses house 1, the robber receives a reward of -1 (for being caught) and the agent receives a reward of 0.5 for catching the robber.",
                "The payoffs for the game against robber type b are constructed using different values.",
                "Security agent: {1,2} {2,1} Robber a 1a -1, .5 -.375, .125 2a -.125, -.125 -1, .5 Robber b 1b -.9, .6 -.275, .225 2b -.025, -.025 -.9, .6 Table 2: Payoff tables: Security Agent vs Robbers a and b Using the Harsanyi technique involves introducing a chance node, that determines the robbers type, thus transforming the security agents incomplete information regarding the robber into imperfect information [3].",
                "The Bayesian equilibrium of the game is then precisely the Nash equilibrium of the imperfect information game.",
                "The transformed, normal-form game is shown in Table 3.",
                "In the transformed game, the security agent is the column player, and the set of all robber types together is the row player.",
                "Suppose that robber type a robs house 1 and robber type b robs house 2, while the security agent chooses patrol {1,2}.",
                "Then, the security agent and the robber receive an expected payoff corresponding to their payoffs from the agent encountering robber a at house 1 with probability α and robber b at house 2 with probability 1 − α. 3.2 Finding an Optimal Strategy Although a Nash equilibrium is the standard solution concept for games in which agents choose strategies simultaneously, in our security domain, the security agent (the leader) can gain an advantage by committing to a mixed strategy in advance.",
                "Since the followers (the robbers) will know the leaders strategy, the optimal response for the followers will be a pure strategy.",
                "Given the common assumption, taken in [5], in the case where followers are indifferent, they will choose the strategy that benefits the leader, there must exist a guaranteed optimal strategy for the leader [5].",
                "From the Bayesian game in Table 2, we constructed the Harsanyi transformed bimatrix in Table 3.",
                "The strategies for each player (security agent or robber) in the transformed game correspond to all combinations of possible strategies taken by each of that players types.",
                "Therefore, we denote X = σθ1 1 = σ1 and Q = σθ2 2 as the index sets of the security agent and robbers pure strategies respectively, with R and C as the corresponding payoff matrices.",
                "Rij is the reward of the security agent and Cij is the reward of the robbers when the security agent takes pure strategy i and the robbers take pure strategy j.",
                "A mixed strategy for the security agent is a probability distribution over its set of pure strategies and will be represented by a vector x = (px1, px2, . . . , px|X|), where pxi ≥ 0 and P pxi = 1.",
                "Here, pxi is the probability that the security agent will choose its ith pure strategy.",
                "The optimal mixed strategy for the security agent can be found in time polynomial in the number of rows in the normal form game using the following linear program formulation from [5].",
                "For every possible pure strategy j by the follower (the set of all robber types), max P i∈X pxiRij s.t. ∀j ∈ Q, P i∈σ1 pxiCij ≥ P i∈σ1 pxiCij P i∈X pxi = 1 ∀i∈X , pxi >= 0 (1) Then, for all feasible follower strategies j, choose the one that maximizes P i∈X pxiRij, the reward for the security agent (leader).",
                "The pxi variables give the optimal strategy for the security agent.",
                "Note that while this method is polynomial in the number of rows in the transformed, normal-form game, the number of rows increases exponentially with the number of robber types.",
                "Using this method for a Bayesian game thus requires running |σ2||θ2| separate linear programs.",
                "This is no surprise, since finding the leaders optimal strategy in a Bayesian Stackelberg game is NP-hard [5]. 4.",
                "HEURISTIC APPROACHES Given that finding the optimal strategy for the leader is NP-hard, we provide a heuristic approach.",
                "In this heuristic we limit the possible mixed strategies of the leader to select actions with probabilities that are integer multiples of 1/k for a predetermined integer k. Previous work [14] has shown that strategies with high entropy are beneficial for security applications when opponents utilities are completely unknown.",
                "In our domain, if utilities are not considered, this method will result in uniform-distribution strategies.",
                "One advantage of such strategies is that they are compact to represent (as fractions) and simple to understand; therefore they can be efficiently implemented by real organizations.",
                "We aim to maintain the advantage provided by simple strategies for our security application problem, incorporating the effect of the robbers rewards on the security agents rewards.",
                "Thus, the ASAP heuristic will produce strategies which are k-uniform.",
                "A mixed strategy is denoted k-uniform if it is a uniform distribution on a multiset S of pure strategies with |S| = k. A multiset is a set whose elements may be repeated multiple times; thus, for example, the mixed strategy corresponding to the multiset {1, 1, 2} would take strategy 1 The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) 313 {1,2} {2,1} {1a, 1b} −1α − .9(1 − α), .5α + .6(1 − α) −.375α − .275(1 − α), .125α + .225(1 − α) {1a, 2b} −1α − .025(1 − α), .5α − .025(1 − α) −.375α − .9(1 − α), .125α + .6(1 − α) {2a, 1b} −.125α − .9(1 − α), −.125α + .6(1 − α) −1α − .275(1 − α), .5α + .225(1 − α) {2a, 2b} −.125α − .025(1 − α), −.125α − .025(1 − α) −1α − .9(1 − α), .5α + .6(1 − α) Table 3: Harsanyi Transformed Payoff Table with probability 2/3 and strategy 2 with probability 1/3.",
                "ASAP allows the size of the multiset to be chosen in order to balance the complexity of the strategy reached with the goal that the identified strategy will yield a high reward.",
                "Another advantage of the ASAP heuristic is that it operates directly on the compact Bayesian representation, without requiring the Harsanyi transformation.",
                "This is because the different follower (robber) types are independent of each other.",
                "Hence, evaluating the leader strategy against a Harsanyi-transformed game matrix is equivalent to evaluating against each of the game matrices for the individual follower types.",
                "This independence property is exploited in ASAP to yield a decomposition scheme.",
                "Note that the LP method introduced by [5] to compute optimal Stackelberg policies is unlikely to be decomposable into a small number of games as it was shown to be NP-hard for Bayes-Nash problems.",
                "Finally, note that ASAP requires the solution of only one optimization problem, rather than solving a series of problems as in the LP method of [5].",
                "For a single follower type, the algorithm works the following way.",
                "Given a particular k, for each possible mixed strategy x for the leader that corresponds to a multiset of size k, evaluate the leaders payoff from x when the follower plays a reward-maximizing pure strategy.",
                "We then take the mixed strategy with the highest payoff.",
                "We need only to consider the reward-maximizing pure strategies of the followers (robbers), since for a given fixed strategy x of the security agent, each robber type faces a problem with fixed linear rewards.",
                "If a mixed strategy is optimal for the robber, then so are all the pure strategies in the support of that mixed strategy.",
                "Note also that because we limit the leaders strategies to take on discrete values, the assumption from Section 3.2 that the followers will break ties in the leaders favor is not significant, since ties will be unlikely to arise.",
                "This is because, in domains where rewards are drawn from any random distribution, the probability of a follower having more than one pure optimal response to a given leader strategy approaches zero, and the leader will have only a finite number of possible mixed strategies.",
                "Our approach to characterize the optimal strategy for the security agent makes use of properties of linear programming.",
                "We briefly outline these results here for completeness, for detailed discussion and proofs see one of many references on the topic, such as [2].",
                "Every linear programming problem, such as: max cT x | Ax = b, x ≥ 0, has an associated dual linear program, in this case: min bT y | AT y ≥ c. These primal/dual pairs of problems satisfy weak duality: For any x and y primal and dual feasible solutions respectively, cT x ≤ bT y.",
                "Thus a pair of feasible solutions is optimal if cT x = bT y, and the problems are said to satisfy strong duality.",
                "In fact if a linear program is feasible and has a bounded optimal solution, then the dual is also feasible and there is a pair x∗ , y∗ that satisfies cT x∗ = bT y∗ .",
                "These optimal solutions are characterized with the following optimality conditions (as defined in [2]): • primal feasibility: Ax = b, x ≥ 0 • dual feasibility: AT y ≥ c • complementary slackness: xi(AT y − c)i = 0 for all i.",
                "Note that this last condition implies that cT x = xT AT y = bT y, which proves optimality for primal dual feasible solutions x and y.",
                "In the following subsections, we first define the problem in its most intuititive form as a mixed-integer quadratic program (MIQP), and then show how this problem can be converted into a mixedinteger linear program (MILP). 4.1 Mixed-Integer Quadratic Program We begin with the case of a single type of follower.",
                "Let the leader be the row player and the follower the column player.",
                "We denote by x the vector of strategies of the leader and q the vector of strategies of the follower.",
                "We also denote X and Q the index sets of the leader and followers pure strategies, respectively.",
                "The payoff matrices R and C correspond to: Rij is the reward of the leader and Cij is the reward of the follower when the leader takes pure strategy i and the follower takes pure strategy j.",
                "Let k be the size of the multiset.",
                "We first fix the policy of the leader to some k-uniform policy x.",
                "The value xi is the number of times pure strategy i is used in the k-uniform policy, which is selected with probability xi/k.",
                "We formulate the optimization problem the follower solves to find its optimal response to x as the following linear program: max X j∈Q X i∈X 1 k Cijxi qj s.t.",
                "P j∈Q qj = 1 q ≥ 0. (2) The objective function maximizes the followers expected reward given x, while the constraints make feasible any mixed strategy q for the follower.",
                "The dual to this linear programming problem is the following: min a s.t. a ≥ X i∈X 1 k Cijxi j ∈ Q. (3) From strong duality and complementary slackness we obtain that the followers maximum reward value, a, is the value of every pure strategy with qj > 0, that is in the support of the optimal mixed strategy.",
                "Therefore each of these pure strategies is optimal.",
                "Optimal solutions to the followers problem are characterized by linear programming optimality conditions: primal feasibility constraints in (2), dual feasibility constraints in (3), and complementary slackness qj a − X i∈X 1 k Cijxi ! = 0 j ∈ Q.",
                "These conditions must be included in the problem solved by the leader in order to consider only best responses by the follower to the k-uniform policy x. 314 The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) The leader seeks the k-uniform solution x that maximizes its own payoff, given that the follower uses an optimal response q(x).",
                "Therefore the leader solves the following integer problem: max X i∈X X j∈Q 1 k Rijq(x)j xi s.t.",
                "P i∈X xi = k xi ∈ {0, 1, . . . , k}. (4) Problem (4) maximizes the leaders reward with the followers best response (qj for fixed leaders policy x and hence denoted q(x)j) by selecting a uniform policy from a multiset of constant size k. We complete this problem by including the characterization of q(x) through linear programming optimality conditions.",
                "To simplify writing the complementary slackness conditions, we will constrain q(x) to be only optimal pure strategies by just considering integer solutions of q(x).",
                "The leaders problem becomes: maxx,q X i∈X X j∈Q 1 k Rijxiqj s.t.",
                "P i xi = kP j∈Q qj = 1 0 ≤ (a − P i∈X 1 k Cijxi) ≤ (1 − qj)M xi ∈ {0, 1, ...., k} qj ∈ {0, 1}. (5) Here, the constant M is some large number.",
                "The first and fourth constraints enforce a k-uniform policy for the leader, and the second and fifth constraints enforce a feasible pure strategy for the follower.",
                "The third constraint enforces dual feasibility of the followers problem (leftmost inequality) and the complementary slackness constraint for an optimal pure strategy q for the follower (rightmost inequality).",
                "In fact, since only one pure strategy can be selected by the follower, say qh = 1, this last constraint enforces that a = P i∈X 1 k Cihxi imposing no additional constraint for all other pure strategies which have qj = 0.",
                "We conclude this subsection noting that Problem (5) is an integer program with a non-convex quadratic objective in general, as the matrix R need not be positive-semi-definite.",
                "Efficient solution methods for non-linear, non-convex integer problems remains a challenging research question.",
                "In the next section we show a reformulation of this problem as a linear integer programming problem, for which a number of efficient commercial solvers exist. 4.2 Mixed-Integer Linear Program We can linearize the quadratic program of Problem 5 through the change of variables zij = xiqj, obtaining the following problem maxq,z P i∈X P j∈Q 1 k Rijzij s.t.",
                "P i∈X P j∈Q zij = k P j∈Q zij ≤ k kqj ≤ P i∈X zij ≤ k P j∈Q qj = 1 0 ≤ (a − P i∈X 1 k Cij( P h∈Q zih)) ≤ (1 − qj)M zij ∈ {0, 1, ...., k} qj ∈ {0, 1} (6) PROPOSITION 1.",
                "Problems (5) and (6) are equivalent.",
                "Proof: Consider x, q a feasible solution of (5).",
                "We will show that q, zij = xiqj is a feasible solution of (6) of same objective function value.",
                "The equivalence of the objective functions, and constraints 4, 6 and 7 of (6) are satisfied by construction.",
                "The fact that P j∈Q zij = xi as P j∈Q qj = 1 explains constraints 1, 2, and 5 of (6).",
                "Constraint 3 of (6) is satisfied because P i∈X zij = kqj.",
                "Let us now consider q, z feasible for (6).",
                "We will show that q and xi = P j∈Q zij are feasible for (5) with the same objective value.",
                "In fact all constraints of (5) are readily satisfied by construction.",
                "To see that the objectives match, notice that if qh = 1 then the third constraint in (6) implies that P i∈X zih = k, which means that zij = 0 for all i ∈ X and all j = h. Therefore, xiqj = X l∈Q zilqj = zihqj = zij.",
                "This last equality is because both are 0 when j = h. This shows that the transformation preserves the objective function value, completing the proof.",
                "Given this transformation to a mixed-integer linear program (MILP), we now show how we can apply our decomposition technique on the MILP to obtain significant speedups for Bayesian games with multiple follower types. 5.",
                "DECOMPOSITION FOR MULTIPLE ADVERSARIES The MILP developed in the previous section handles only one follower.",
                "Since our security scenario contains multiple follower (robber) types, we change the response function for the follower from a pure strategy into a weighted combination over various pure follower strategies where the weights are probabilities of occurrence of each of the follower types. 5.1 Decomposed MIQP To admit multiple adversaries in our framework, we modify the notation defined in the previous section to reason about multiple follower types.",
                "We denote by x the vector of strategies of the leader and ql the vector of strategies of follower l, with L denoting the index set of follower types.",
                "We also denote by X and Q the index sets of leader and follower ls pure strategies, respectively.",
                "We also index the payoff matrices on each follower l, considering the matrices Rl and Cl .",
                "Using this modified notation, we characterize the optimal solution of follower ls problem given the leaders k-uniform policy x, with the following optimality conditions: X j∈Q ql j = 1 al − X i∈X 1 k Cl ijxi ≥ 0 ql j(al − X i∈X 1 k Cl ijxi) = 0 ql j ≥ 0 Again, considering only optimal pure strategies for follower ls problem we can linearize the complementarity constraint above.",
                "We incorporate these constraints on the leaders problem that selects the optimal k-uniform policy.",
                "Therefore, given a priori probabilities pl , with l ∈ L of facing each follower, the leader solves the following problem: The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) 315 maxx,q X i∈X X l∈L X j∈Q pl k Rl ijxiql j s.t.",
                "P i xi = kP j∈Q ql j = 1 0 ≤ (al − P i∈X 1 k Cl ijxi) ≤ (1 − ql j)M xi ∈ {0, 1, ...., k} ql j ∈ {0, 1}. (7) Problem (7) for a Bayesian game with multiple follower types is indeed equivalent to Problem (5) on the payoff matrix obtained from the Harsanyi transformation of the game.",
                "In fact, every pure strategy j in Problem (5) corresponds to a sequence of pure strategies jl, one for each follower l ∈ L. This means that qj = 1 if and only if ql jl = 1 for all l ∈ L. In addition, given the a priori probabilities pl of facing player l, the reward in the Harsanyi transformation payoff table is Rij = P l∈L pl Rl ijl .",
                "The same relation holds between C and Cl .",
                "These relations between a pure strategy in the equivalent normal form game and pure strategies in the individual games with each followers are key in showing these problems are equivalent. 5.2 Decomposed MILP We can linearize the quadratic programming problem 7 through the change of variables zl ij = xiql j, obtaining the following problem maxq,z P i∈X P l∈L P j∈Q pl k Rl ijzl ij s.t.",
                "P i∈X P j∈Q zl ij = k P j∈Q zl ij ≤ k kql j ≤ P i∈X zl ij ≤ k P j∈Q ql j = 1 0 ≤ (al − P i∈X 1 k Cl ij( P h∈Q zl ih)) ≤ (1 − ql j)M P j∈Q zl ij = P j∈Q z1 ij zl ij ∈ {0, 1, ...., k} ql j ∈ {0, 1} (8) PROPOSITION 2.",
                "Problems (7) and (8) are equivalent.",
                "Proof: Consider x, ql , al with l ∈ L a feasible solution of (7).",
                "We will show that ql , al , zl ij = xiql j is a feasible solution of (8) of same objective function value.",
                "The equivalence of the objective functions, and constraints 4, 7 and 8 of (8) are satisfied by construction.",
                "The fact that P j∈Q zl ij = xi as P j∈Q ql j = 1 explains constraints 1, 2, 5 and 6 of (8).",
                "Constraint 3 of (8) is satisfied because P i∈X zl ij = kql j.",
                "Lets now consider ql , zl , al feasible for (8).",
                "We will show that ql , al and xi = P j∈Q z1 ij are feasible for (7) with the same objective value.",
                "In fact all constraints of (7) are readily satisfied by construction.",
                "To see that the objectives match, notice for each l one ql j must equal 1 and the rest equal 0.",
                "Let us say that ql jl = 1, then the third constraint in (8) implies that P i∈X zl ijl = k, which means that zl ij = 0 for all i ∈ X and all j = jl.",
                "In particular this implies that xi = X j∈Q z1 ij = z1 ij1 = zl ijl , the last equality from constraint 6 of (8).",
                "Therefore xiql j = zl ijl ql j = zl ij.",
                "This last equality is because both are 0 when j = jl.",
                "Effectively, constraint 6 ensures that all the adversaries are calculating their best responses against a particular fixed policy of the agent.",
                "This shows that the transformation preserves the objective function value, completing the proof.",
                "We can therefore solve this equivalent linear integer program with efficient integer programming packages which can handle problems with thousands of integer variables.",
                "We implemented the decomposed MILP and the results are shown in the following section. 6.",
                "EXPERIMENTAL RESULTS The patrolling domain and the payoffs for the associated game are detailed in Sections 2 and 3.",
                "We performed experiments for this game in worlds of three and four houses with patrols consisting of two houses.",
                "The description given in Section 2 is used to generate a base case for both the security agent and robber payoff functions.",
                "The payoff tables for additional robber types are constructed and added to the game by adding a random distribution of varying size to the payoffs in the base case.",
                "All games are normalized so that, for each robber type, the minimum and maximum payoffs to the security agent and robber are 0 and 1, respectively.",
                "Using the data generated, we performed the experiments using four methods for generating the security agents strategy: • uniform randomization • ASAP • the multiple linear programs method from [5] (to find the true optimal strategy) • the highest reward Bayes-Nash equilibrium, found using the MIP-Nash algorithm [17] The last three methods were applied using CPLEX 8.1.",
                "Because the last two methods are designed for normal-form games rather than Bayesian games, the games were first converted using the Harsanyi transformation [8].",
                "The uniform randomization method is simply choosing a uniform random policy over all possible patrol routes.",
                "We use this method as a simple baseline to measure the performance of our heuristics.",
                "We anticipated that the uniform policy would perform reasonably well since maximum-entropy policies have been shown to be effective in multiagent security domains [14].",
                "The highest-reward Bayes-Nash equilibria were used in order to demonstrate the higher reward gained by looking for an optimal policy rather than an equilibria in Stackelberg games such as our security domain.",
                "Based on our experiments we present three sets of graphs to demonstrate (1) the runtime of ASAP compared to other common methods for finding a strategy, (2) the reward guaranteed by ASAP compared to other methods, and (3) the effect of varying the parameter k, the size of the multiset, on the performance of ASAP.",
                "In the first two sets of graphs, ASAP is run using a multiset of 80 elements; in the third set this number is varied.",
                "The first set of graphs, shown in Figure 1 shows the runtime graphs for three-house (left column) and four-house (right column) domains.",
                "Each of the three rows of graphs corresponds to a different randomly-generated scenario.",
                "The x-axis shows the number of robber types the security agent faces and the y-axis of the graph shows the runtime in seconds.",
                "All experiments that were not concluded in 30 minutes (1800 seconds) were cut off.",
                "The runtime for the uniform policy is always negligible irrespective of the number of adversaries and hence is not shown.",
                "The ASAP algorithm clearly outperforms the optimal, multipleLP method as well as the MIP-Nash algorithm for finding the highestreward Bayes-Nash equilibrium with respect to runtime.",
                "For a 316 The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) Figure 1: Runtimes for various algorithms on problems of 3 and 4 houses. domain of three houses, the optimal method cannot reach a solution for more than seven robber types, and for four houses it cannot solve for more than six types within the cutoff time in any of the three scenarios.",
                "MIP-Nash solves for even fewer robber types within the cutoff time.",
                "On the other hand, ASAP runs much faster, and is able to solve for at least 20 adversaries for the three-house scenarios and for at least 12 adversaries in the four-house scenarios within the cutoff time.",
                "The runtime of ASAP does not increase strictly with the number of robber types for each scenario, but in general, the addition of more types increases the runtime required.",
                "The second set of graphs, Figure 2, shows the reward to the patrol agent given by each method for three scenarios in the three-house (left column) and four-house (right column) domains.",
                "This reward is the utility received by the security agent in the patrolling game, and not as a percentage of the optimal reward, since it was not possible to obtain the optimal reward as the number of robber types increased.",
                "The uniform policy consistently provides the lowest reward in both domains; while the optimal method of course produces the optimal reward.",
                "The ASAP method remains consistently close to the optimal, even as the number of robber types increases.",
                "The highest-reward Bayes-Nash equilibria, provided by the MIPNash method, produced rewards higher than the uniform method, but lower than ASAP.",
                "This difference clearly illustrates the gains in the patrolling domain from committing to a strategy as the leader in a Stackelberg game, rather than playing a standard Bayes-Nash strategy.",
                "The third set of graphs, shown in Figure 3 shows the effect of the multiset size on runtime in seconds (left column) and reward (right column), again expressed as the reward received by the security agent in the patrolling game, and not a percentage of the optimal Figure 2: Reward for various algorithms on problems of 3 and 4 houses. reward.",
                "Results here are for the three-house domain.",
                "The trend is that as as the multiset size is increased, the runtime and reward level both increase.",
                "Not surprisingly, the reward increases monotonically as the multiset size increases, but what is interesting is that there is relatively little benefit to using a large multiset in this domain.",
                "In all cases, the reward given by a multiset of 10 elements was within at least 96% of the reward given by an 80-element multiset.",
                "The runtime does not always increase strictly with the multiset size; indeed in one example (scenario 2 with 20 robber types), using a multiset of 10 elements took 1228 seconds, while using 80 elements only took 617 seconds.",
                "In general, runtime should increase since a larger multiset means a larger domain for the variables in the MILP, and thus a larger search space.",
                "However, an increase in the number of variables can sometimes allow for a policy to be constructed more quickly due to more flexibility in the problem. 7.",
                "SUMMARY AND RELATED WORK This paper focuses on security for agents patrolling in hostile environments.",
                "In these environments, intentional threats are caused by adversaries about whom the security patrolling agents have incomplete information.",
                "Specifically, we deal with situations where the adversaries actions and payoffs are known but the exact adversary type is unknown to the security agent.",
                "Agents acting in the real world quite frequently have such incomplete information about other agents.",
                "Bayesian games have been a popular choice to model such incomplete information games [3].",
                "The Gala toolkit is one method for defining such games [9] without requiring the game to be represented in normal form via the Harsanyi transformation [8]; Galas guarantees are focused on fully competitive games.",
                "Much work has been done on finding optimal Bayes-Nash equilbria for The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) 317 Figure 3: Reward for ASAP using multisets of 10, 30, and 80 elements subclasses of Bayesian games, finding single Bayes-Nash equilibria for general Bayesian games [10] or approximate Bayes-Nash equilibria [18].",
                "Less attention has been paid to finding the optimal strategy to commit to in a Bayesian game (the Stackelberg scenario [15]).",
                "However, the complexity of this problem was shown to be NP-hard in the general case [5], which also provides algorithms for this problem in the non-Bayesian case.",
                "Therefore, we present a heuristic called ASAP, with three key advantages towards addressing this problem.",
                "First, ASAP searches for the highest reward strategy, rather than a Bayes-Nash equilibrium, allowing it to find feasible strategies that exploit the natural first-mover advantage of the game.",
                "Second, it provides strategies which are simple to understand, represent, and implement.",
                "Third, it operates directly on the compact, Bayesian game representation, without requiring conversion to normal form.",
                "We provide an efficient Mixed Integer Linear Program (MILP) implementation for ASAP, along with experimental results illustrating significant speedups and higher rewards over other approaches.",
                "Our k-uniform strategies are similar to the k-uniform strategies of [12].",
                "While that work provides epsilon error-bounds based on the k-uniform strategies, their solution concept is still that of a Nash equilibrium, and they do not provide efficient algorithms for obtaining such k-uniform strategies.",
                "This contrasts with ASAP, where our emphasis is on a highly efficient heuristic approach that is not focused on equilibrium solutions.",
                "Finally the patrolling problem which motivated our work has recently received growing attention from the multiagent community due to its wide range of applications [4, 13].",
                "However most of this work is focused on either limiting energy consumption involved in patrolling [7] or optimizing on criteria like the length of the path traveled [4, 13], without reasoning about any explicit model of an adversary[14].",
                "Acknowledgments : This research is supported by the United States Department of Homeland Security through Center for Risk and Economic Analysis of Terrorism Events (CREATE).",
                "It is also supported by the Defense Advanced Research Projects Agency (DARPA), through the Department of the Interior, NBC, Acquisition Services Division, under Contract No.",
                "NBCHD030010.",
                "Sarit Kraus is also affiliated with UMIACS. 8.",
                "REFERENCES [1] R. W. Beard and T. McLain.",
                "Multiple UAV cooperative search under collision avoidance and limited range communication constraints.",
                "In IEEE CDC, 2003. [2] D. Bertsimas and J. Tsitsiklis.",
                "Introduction to Linear Optimization.",
                "Athena Scientific, 1997. [3] J. Brynielsson and S. Arnborg.",
                "Bayesian games for threat prediction and situation analysis.",
                "In FUSION, 2004. [4] Y. Chevaleyre.",
                "Theoretical analysis of multi-agent patrolling problem.",
                "In AAMAS, 2004. [5] V. Conitzer and T. Sandholm.",
                "Choosing the best strategy to commit to.",
                "In ACM Conference on Electronic Commerce, 2006. [6] D. Fudenberg and J. Tirole.",
                "Game Theory.",
                "MIT Press, 1991. [7] C. Gui and P. Mohapatra.",
                "Virtual patrol: A new power conservation design for surveillance using sensor networks.",
                "In IPSN, 2005. [8] J. C. Harsanyi and R. Selten.",
                "A generalized Nash solution for two-person bargaining games with incomplete information.",
                "Management Science, 18(5):80-106, 1972. [9] D. Koller and A. Pfeffer.",
                "Generating and solving imperfect information games.",
                "In IJCAI, pages 1185-1193, 1995. [10] D. Koller and A. Pfeffer.",
                "Representations and solutions for game-theoretic problems.",
                "Artificial Intelligence, 94(1):167-215, 1997. [11] C. Lemke and J. Howson.",
                "Equilibrium points of bimatrix games.",
                "Journal of the Society for Industrial and Applied Mathematics, 12:413-423, 1964. [12] R. J. Lipton, E. Markakis, and A. Mehta.",
                "Playing large games using simple strategies.",
                "In ACM Conference on Electronic Commerce, 2003. [13] A. Machado, G. Ramalho, J. D. Zucker, and A. Drougoul.",
                "Multi-agent patrolling: an empirical analysis on alternative architectures.",
                "In MABS, 2002. [14] P. Paruchuri, M. Tambe, F. Ordonez, and S. Kraus.",
                "Security in multiagent systems by policy randomization.",
                "In AAMAS, 2006. [15] T. Roughgarden.",
                "Stackelberg scheduling strategies.",
                "In ACM Symposium on TOC, 2001. [16] S. Ruan, C. Meirina, F. Yu, K. R. Pattipati, and R. L. Popp.",
                "Patrolling in a stochastic environment.",
                "In 10th Intl.",
                "Command and Control Research Symp., 2005. [17] T. Sandholm, A. Gilpin, and V. Conitzer.",
                "Mixed-integer programming methods for finding nash equilibria.",
                "In AAAI, 2005. [18] S. Singh, V. Soni, and M. Wellman.",
                "Computing approximate Bayes-Nash equilibria with tree-games of incomplete information.",
                "In ACM Conference on Electronic Commerce, 2004. 318 The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07)"
            ],
            "original_annotated_samples": [],
            "translated_annotated_samples": [
                "Un enfoque heurístico eficiente para la seguridad contra los múltiples adversarios Praveen Paruchuri, Jonathan P. Pearce, Milind Tambe, Fernando Ordonez Universidad del Sur de California en Los Ángeles, CA 90089 {Paruchur, Jppearce, Tambe, Fordon}@usc.edu Sarit Kraus Bar-ilan University UniversityRamat-Gan 52900, Israel sarit@cs.biu.ac.il Resumen en \"dominio multiagente adversario\" S, seguridad, comúnmente definida como la capacidad de lidiar con las amenazas intencionales de otros agentes, es un tema crítico."
            ],
            "translated_text": "",
            "candidates": [
                "dominio multiagente adversario",
                "dominio multiagente adversario"
            ],
            "error": []
        },
        "agent security via approximate policy": {
            "translated_key": "Seguridad del agente a través de una política aproximada",
            "is_in_text": false,
            "original_annotated_sentences": [
                "An Efficient Heuristic Approach for Security Against Multiple Adversaries Praveen Paruchuri, Jonathan P. Pearce, Milind Tambe, Fernando Ordonez University of Southern California Los Angeles, CA 90089 {paruchur, jppearce, tambe, fordon}@usc.edu Sarit Kraus Bar-Ilan University Ramat-Gan 52900, Israel sarit@cs.biu.ac.il ABSTRACT In adversarial multiagent domains, security, commonly defined as the ability to deal with intentional threats from other agents, is a critical issue.",
                "This paper focuses on domains where these threats come from unknown adversaries.",
                "These domains can be modeled as Bayesian games; much work has been done on finding equilibria for such games.",
                "However, it is often the case in multiagent security domains that one agent can commit to a mixed strategy which its adversaries observe before choosing their own strategies.",
                "In this case, the agent can maximize reward by finding an optimal strategy, without requiring equilibrium.",
                "Previous work has shown this problem of optimal strategy selection to be NP-hard.",
                "Therefore, we present a heuristic called ASAP, with three key advantages to address the problem.",
                "First, ASAP searches for the highest-reward strategy, rather than a Bayes-Nash equilibrium, allowing it to find feasible strategies that exploit the natural first-mover advantage of the game.",
                "Second, it provides strategies which are simple to understand, represent, and implement.",
                "Third, it operates directly on the compact, Bayesian game representation, without requiring conversion to normal form.",
                "We provide an efficient Mixed Integer Linear Program (MILP) implementation for ASAP, along with experimental results illustrating significant speedups and higher rewards over other approaches.",
                "Categories and Subject Descriptors I.2.11 [Computing Methodologies]: Artificial Intelligence: Distributed Artificial Intelligence - Intelligent Agents General Terms Security, Design, Theory 1.",
                "INTRODUCTION In many multiagent domains, agents must act in order to provide security against attacks by adversaries.",
                "A common issue that agents face in such security domains is uncertainty about the adversaries they may be facing.",
                "For example, a security robot may need to make a choice about which areas to patrol, and how often [16].",
                "However, it will not know in advance exactly where a robber will choose to strike.",
                "A team of unmanned aerial vehicles (UAVs) [1] monitoring a region undergoing a humanitarian crisis may also need to choose a patrolling policy.",
                "They must make this decision without knowing in advance whether terrorists or other adversaries may be waiting to disrupt the mission at a given location.",
                "It may indeed be possible to model the motivations of types of adversaries the agent or agent team is likely to face in order to target these adversaries more closely.",
                "However, in both cases, the security robot or UAV team will not know exactly which kinds of adversaries may be active on any given day.",
                "A common approach for choosing a policy for agents in such scenarios is to model the scenarios as Bayesian games.",
                "A Bayesian game is a game in which agents may belong to one or more types; the type of an agent determines its possible actions and payoffs.",
                "The distribution of adversary types that an agent will face may be known or inferred from historical data.",
                "Usually, these games are analyzed according to the solution concept of a Bayes-Nash equilibrium, an extension of the Nash equilibrium for Bayesian games.",
                "However, in many settings, a Nash or Bayes-Nash equilibrium is not an appropriate solution concept, since it assumes that the agents strategies are chosen simultaneously [5].",
                "In some settings, one player can (or must) commit to a strategy before the other players choose their strategies.",
                "These scenarios are known as Stackelberg games [6].",
                "In a Stackelberg game, a leader commits to a strategy first, and then a follower (or group of followers) selfishly optimize their own rewards, considering the action chosen by the leader.",
                "For example, the security agent (leader) must first commit to a strategy for patrolling various areas.",
                "This strategy could be a mixed strategy in order to be unpredictable to the robbers (followers).",
                "The robbers, after observing the pattern of patrols over time, can then choose their strategy (which location to rob).",
                "Often, the leader in a Stackelberg game can attain a higher reward than if the strategies were chosen simultaneously.",
                "To see the advantage of being the leader in a Stackelberg game, consider a simple game with the payoff table as shown in Table 1.",
                "The leader is the row player and the follower is the column player.",
                "Here, the leaders payoff is listed first. 1 2 3 1 5,5 0,0 3,10 2 0,0 2,2 5,0 Table 1: Payoff table for example normal form game.",
                "The only Nash equilibrium for this game is when the leader plays 2 and the follower plays 2 which gives the leader a payoff of 2. 311 978-81-904262-7-5 (RPS) c 2007 IFAAMAS However, if the leader commits to a uniform mixed strategy of playing 1 and 2 with equal (0.5) probability, the followers best response is to play 3 to get an expected payoff of 5 (10 and 0 with equal probability).",
                "The leaders payoff would then be 4 (3 and 5 with equal probability).",
                "In this case, the leader now has an incentive to deviate and choose a pure strategy of 2 (to get a payoff of 5).",
                "However, this would cause the follower to deviate to strategy 2 as well, resulting in the Nash equilibrium.",
                "Thus, by committing to a strategy that is observed by the follower, and by avoiding the temptation to deviate, the leader manages to obtain a reward higher than that of the best Nash equilibrium.",
                "The problem of choosing an optimal strategy for the leader to commit to in a Stackelberg game is analyzed in [5] and found to be NP-hard in the case of a Bayesian game with multiple types of followers.",
                "Thus, efficient heuristic techniques for choosing highreward strategies in these games is an important open issue.",
                "Methods for finding optimal leader strategies for non-Bayesian games [5] can be applied to this problem by converting the Bayesian game into a normal-form game by the Harsanyi transformation [8].",
                "If, on the other hand, we wish to compute the highest-reward Nash equilibrium, new methods using mixed-integer linear programs (MILPs) [17] may be used, since the highest-reward Bayes-Nash equilibrium is equivalent to the corresponding Nash equilibrium in the transformed game.",
                "However, by transforming the game, the compact structure of the Bayesian game is lost.",
                "In addition, since the Nash equilibrium assumes a simultaneous choice of strategies, the advantages of being the leader are not considered.",
                "This paper introduces an efficient heuristic method for approximating the optimal leader strategy for security domains, known as ASAP (Agent Security via Approximate Policies).",
                "This method has three key advantages.",
                "First, it directly searches for an optimal strategy, rather than a Nash (or Bayes-Nash) equilibrium, thus allowing it to find high-reward non-equilibrium strategies like the one in the above example.",
                "Second, it generates policies with a support which can be expressed as a uniform distribution over a multiset of fixed size as proposed in [12].",
                "This allows for policies that are simple to understand and represent [12], as well as a tunable parameter (the size of the multiset) that controls the simplicity of the policy.",
                "Third, the method allows for a Bayes-Nash game to be expressed compactly without conversion to a normal-form game, allowing for large speedups over existing Nash methods such as [17] and [11].",
                "The rest of the paper is organized as follows.",
                "In Section 2 we fully describe the patrolling domain and its properties.",
                "Section 3 introduces the Bayesian game, the Harsanyi transformation, and existing methods for finding an optimal leaders strategy in a Stackelberg game.",
                "Then, in Section 4 the ASAP algorithm is presented for normal-form games, and in Section 5 we show how it can be adapted to the structure of Bayesian games with uncertain adversaries.",
                "Experimental results showing higher reward and faster policy computation over existing Nash methods are shown in Section 6, and we conclude with a discussion of related work in Section 7. 2.",
                "THE PATROLLING DOMAIN In most security patrolling domains, the security agents (like UAVs [1] or security robots [16]) cannot feasibly patrol all areas all the time.",
                "Instead, they must choose a policy by which they patrol various routes at different times, taking into account factors such as the likelihood of crime in different areas, possible targets for crime, and the security agents own resources (number of security agents, amount of available time, fuel, etc.).",
                "It is usually beneficial for this policy to be nondeterministic so that robbers cannot safely rob certain locations, knowing that they will be safe from the security agents [14].",
                "To demonstrate the utility of our algorithm, we use a simplified version of such a domain, expressed as a game.",
                "The most basic version of our game consists of two players: the security agent (the leader) and the robber (the follower) in a world consisting of m houses, 1 . . . m. The security agents set of pure strategies consists of possible routes of d houses to patrol (in an order).",
                "The security agent can choose a mixed strategy so that the robber will be unsure of exactly where the security agent may patrol, but the robber will know the mixed strategy the security agent has chosen.",
                "For example, the robber can observe over time how often the security agent patrols each area.",
                "With this knowledge, the robber must choose a single house to rob.",
                "We assume that the robber generally takes a long time to rob a house.",
                "If the house chosen by the robber is not on the security agents route, then the robber successfully robs it.",
                "Otherwise, if it is on the security agents route, then the earlier the house is on the route, the easier it is for the security agent to catch the robber before he finishes robbing it.",
                "We model the payoffs for this game with the following variables: • vl,x: value of the goods in house l to the security agent. • vl,q: value of the goods in house l to the robber. • cx: reward to the security agent of catching the robber. • cq: cost to the robber of getting caught. • pl: probability that the security agent can catch the robber at the lth house in the patrol (pl < pl ⇐⇒ l < l).",
                "The security agents set of possible pure strategies (patrol routes) is denoted by X and includes all d-tuples i =< w1, w2, ..., wd > with w1 . . . wd = 1 . . . m where no two elements are equal (the agent is not allowed to return to the same house).",
                "The robbers set of possible pure strategies (houses to rob) is denoted by Q and includes all integers j = 1 . . . m. The payoffs (security agent, robber) for pure strategies i, j are: • −vl,x, vl,q, for j = l /∈ i. • plcx +(1−pl)(−vl,x), −plcq +(1−pl)(vl,q), for j = l ∈ i.",
                "With this structure it is possible to model many different types of robbers who have differing motivations; for example, one robber may have a lower cost of getting caught than another, or may value the goods in the various houses differently.",
                "If the distribution of different robber types is known or inferred from historical data, then the game can be modeled as a Bayesian game [6]. 3.",
                "BAYESIAN GAMES A Bayesian game contains a set of N agents, and each agent n must be one of a given set of types θn.",
                "For our patrolling domain, we have two agents, the security agent and the robber. θ1 is the set of security agent types and θ2 is the set of robber types.",
                "Since there is only one type of security agent, θ1 contains only one element.",
                "During the game, the robber knows its type but the security agent does not know the robbers type.",
                "For each agent (the security agent or the robber) n, there is a set of strategies σn and a utility function un : θ1 × θ2 × σ1 × σ2 → .",
                "A Bayesian game can be transformed into a normal-form game using the Harsanyi transformation [8].",
                "Once this is done, new, linear-program (LP)-based methods for finding high-reward strategies for normal-form games [5] can be used to find a strategy in the transformed game; this strategy can then be used for the Bayesian game.",
                "While methods exist for finding Bayes-Nash equilibria directly, without the Harsanyi transformation [10], they find only a 312 The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) single equilibrium in the general case, which may not be of high reward.",
                "Recent work [17] has led to efficient mixed-integer linear program techniques to find the best Nash equilibrium for a given agent.",
                "However, these techniques do require a normal-form game, and so to compare the policies given by ASAP against the optimal policy, as well as against the highest-reward Nash equilibrium, we must apply these techniques to the Harsanyi-transformed matrix.",
                "The next two subsections elaborate on how this is done. 3.1 Harsanyi Transformation The first step in solving Bayesian games is to apply the Harsanyi transformation [8] that converts the Bayesian game into a normal form game.",
                "Given that the Harsanyi transformation is a standard concept in game theory, we explain it briefly through a simple example in our patrolling domain without introducing the mathematical formulations.",
                "Let us assume there are two robber types a and b in the Bayesian game.",
                "Robber a will be active with probability α, and robber b will be active with probability 1 − α.",
                "The rules described in Section 2 allow us to construct simple payoff tables.",
                "Assume that there are two houses in the world (1 and 2) and hence there are two patrol routes (pure strategies) for the agent: {1,2} and {2,1}.",
                "The robber can rob either house 1 or house 2 and hence he has two strategies (denoted as 1l, 2l for robber type l).",
                "Since there are two types assumed (denoted as a and b), we construct two payoff tables (shown in Table 2) corresponding to the security agent playing a separate game with each of the two robber types with probabilities α and 1 − α.",
                "First, consider robber type a.",
                "Borrowing the notation from the domain section, we assign the following values to the variables: v1,x = v1,q = 3/4, v2,x = v2,q = 1/4, cx = 1/2, cq = 1, p1 = 1, p2 = 1/2.",
                "Using these values we construct a base payoff table as the payoff for the game against robber type a.",
                "For example, if the security agent chooses route {1,2} when robber a is active, and robber a chooses house 1, the robber receives a reward of -1 (for being caught) and the agent receives a reward of 0.5 for catching the robber.",
                "The payoffs for the game against robber type b are constructed using different values.",
                "Security agent: {1,2} {2,1} Robber a 1a -1, .5 -.375, .125 2a -.125, -.125 -1, .5 Robber b 1b -.9, .6 -.275, .225 2b -.025, -.025 -.9, .6 Table 2: Payoff tables: Security Agent vs Robbers a and b Using the Harsanyi technique involves introducing a chance node, that determines the robbers type, thus transforming the security agents incomplete information regarding the robber into imperfect information [3].",
                "The Bayesian equilibrium of the game is then precisely the Nash equilibrium of the imperfect information game.",
                "The transformed, normal-form game is shown in Table 3.",
                "In the transformed game, the security agent is the column player, and the set of all robber types together is the row player.",
                "Suppose that robber type a robs house 1 and robber type b robs house 2, while the security agent chooses patrol {1,2}.",
                "Then, the security agent and the robber receive an expected payoff corresponding to their payoffs from the agent encountering robber a at house 1 with probability α and robber b at house 2 with probability 1 − α. 3.2 Finding an Optimal Strategy Although a Nash equilibrium is the standard solution concept for games in which agents choose strategies simultaneously, in our security domain, the security agent (the leader) can gain an advantage by committing to a mixed strategy in advance.",
                "Since the followers (the robbers) will know the leaders strategy, the optimal response for the followers will be a pure strategy.",
                "Given the common assumption, taken in [5], in the case where followers are indifferent, they will choose the strategy that benefits the leader, there must exist a guaranteed optimal strategy for the leader [5].",
                "From the Bayesian game in Table 2, we constructed the Harsanyi transformed bimatrix in Table 3.",
                "The strategies for each player (security agent or robber) in the transformed game correspond to all combinations of possible strategies taken by each of that players types.",
                "Therefore, we denote X = σθ1 1 = σ1 and Q = σθ2 2 as the index sets of the security agent and robbers pure strategies respectively, with R and C as the corresponding payoff matrices.",
                "Rij is the reward of the security agent and Cij is the reward of the robbers when the security agent takes pure strategy i and the robbers take pure strategy j.",
                "A mixed strategy for the security agent is a probability distribution over its set of pure strategies and will be represented by a vector x = (px1, px2, . . . , px|X|), where pxi ≥ 0 and P pxi = 1.",
                "Here, pxi is the probability that the security agent will choose its ith pure strategy.",
                "The optimal mixed strategy for the security agent can be found in time polynomial in the number of rows in the normal form game using the following linear program formulation from [5].",
                "For every possible pure strategy j by the follower (the set of all robber types), max P i∈X pxiRij s.t. ∀j ∈ Q, P i∈σ1 pxiCij ≥ P i∈σ1 pxiCij P i∈X pxi = 1 ∀i∈X , pxi >= 0 (1) Then, for all feasible follower strategies j, choose the one that maximizes P i∈X pxiRij, the reward for the security agent (leader).",
                "The pxi variables give the optimal strategy for the security agent.",
                "Note that while this method is polynomial in the number of rows in the transformed, normal-form game, the number of rows increases exponentially with the number of robber types.",
                "Using this method for a Bayesian game thus requires running |σ2||θ2| separate linear programs.",
                "This is no surprise, since finding the leaders optimal strategy in a Bayesian Stackelberg game is NP-hard [5]. 4.",
                "HEURISTIC APPROACHES Given that finding the optimal strategy for the leader is NP-hard, we provide a heuristic approach.",
                "In this heuristic we limit the possible mixed strategies of the leader to select actions with probabilities that are integer multiples of 1/k for a predetermined integer k. Previous work [14] has shown that strategies with high entropy are beneficial for security applications when opponents utilities are completely unknown.",
                "In our domain, if utilities are not considered, this method will result in uniform-distribution strategies.",
                "One advantage of such strategies is that they are compact to represent (as fractions) and simple to understand; therefore they can be efficiently implemented by real organizations.",
                "We aim to maintain the advantage provided by simple strategies for our security application problem, incorporating the effect of the robbers rewards on the security agents rewards.",
                "Thus, the ASAP heuristic will produce strategies which are k-uniform.",
                "A mixed strategy is denoted k-uniform if it is a uniform distribution on a multiset S of pure strategies with |S| = k. A multiset is a set whose elements may be repeated multiple times; thus, for example, the mixed strategy corresponding to the multiset {1, 1, 2} would take strategy 1 The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) 313 {1,2} {2,1} {1a, 1b} −1α − .9(1 − α), .5α + .6(1 − α) −.375α − .275(1 − α), .125α + .225(1 − α) {1a, 2b} −1α − .025(1 − α), .5α − .025(1 − α) −.375α − .9(1 − α), .125α + .6(1 − α) {2a, 1b} −.125α − .9(1 − α), −.125α + .6(1 − α) −1α − .275(1 − α), .5α + .225(1 − α) {2a, 2b} −.125α − .025(1 − α), −.125α − .025(1 − α) −1α − .9(1 − α), .5α + .6(1 − α) Table 3: Harsanyi Transformed Payoff Table with probability 2/3 and strategy 2 with probability 1/3.",
                "ASAP allows the size of the multiset to be chosen in order to balance the complexity of the strategy reached with the goal that the identified strategy will yield a high reward.",
                "Another advantage of the ASAP heuristic is that it operates directly on the compact Bayesian representation, without requiring the Harsanyi transformation.",
                "This is because the different follower (robber) types are independent of each other.",
                "Hence, evaluating the leader strategy against a Harsanyi-transformed game matrix is equivalent to evaluating against each of the game matrices for the individual follower types.",
                "This independence property is exploited in ASAP to yield a decomposition scheme.",
                "Note that the LP method introduced by [5] to compute optimal Stackelberg policies is unlikely to be decomposable into a small number of games as it was shown to be NP-hard for Bayes-Nash problems.",
                "Finally, note that ASAP requires the solution of only one optimization problem, rather than solving a series of problems as in the LP method of [5].",
                "For a single follower type, the algorithm works the following way.",
                "Given a particular k, for each possible mixed strategy x for the leader that corresponds to a multiset of size k, evaluate the leaders payoff from x when the follower plays a reward-maximizing pure strategy.",
                "We then take the mixed strategy with the highest payoff.",
                "We need only to consider the reward-maximizing pure strategies of the followers (robbers), since for a given fixed strategy x of the security agent, each robber type faces a problem with fixed linear rewards.",
                "If a mixed strategy is optimal for the robber, then so are all the pure strategies in the support of that mixed strategy.",
                "Note also that because we limit the leaders strategies to take on discrete values, the assumption from Section 3.2 that the followers will break ties in the leaders favor is not significant, since ties will be unlikely to arise.",
                "This is because, in domains where rewards are drawn from any random distribution, the probability of a follower having more than one pure optimal response to a given leader strategy approaches zero, and the leader will have only a finite number of possible mixed strategies.",
                "Our approach to characterize the optimal strategy for the security agent makes use of properties of linear programming.",
                "We briefly outline these results here for completeness, for detailed discussion and proofs see one of many references on the topic, such as [2].",
                "Every linear programming problem, such as: max cT x | Ax = b, x ≥ 0, has an associated dual linear program, in this case: min bT y | AT y ≥ c. These primal/dual pairs of problems satisfy weak duality: For any x and y primal and dual feasible solutions respectively, cT x ≤ bT y.",
                "Thus a pair of feasible solutions is optimal if cT x = bT y, and the problems are said to satisfy strong duality.",
                "In fact if a linear program is feasible and has a bounded optimal solution, then the dual is also feasible and there is a pair x∗ , y∗ that satisfies cT x∗ = bT y∗ .",
                "These optimal solutions are characterized with the following optimality conditions (as defined in [2]): • primal feasibility: Ax = b, x ≥ 0 • dual feasibility: AT y ≥ c • complementary slackness: xi(AT y − c)i = 0 for all i.",
                "Note that this last condition implies that cT x = xT AT y = bT y, which proves optimality for primal dual feasible solutions x and y.",
                "In the following subsections, we first define the problem in its most intuititive form as a mixed-integer quadratic program (MIQP), and then show how this problem can be converted into a mixedinteger linear program (MILP). 4.1 Mixed-Integer Quadratic Program We begin with the case of a single type of follower.",
                "Let the leader be the row player and the follower the column player.",
                "We denote by x the vector of strategies of the leader and q the vector of strategies of the follower.",
                "We also denote X and Q the index sets of the leader and followers pure strategies, respectively.",
                "The payoff matrices R and C correspond to: Rij is the reward of the leader and Cij is the reward of the follower when the leader takes pure strategy i and the follower takes pure strategy j.",
                "Let k be the size of the multiset.",
                "We first fix the policy of the leader to some k-uniform policy x.",
                "The value xi is the number of times pure strategy i is used in the k-uniform policy, which is selected with probability xi/k.",
                "We formulate the optimization problem the follower solves to find its optimal response to x as the following linear program: max X j∈Q X i∈X 1 k Cijxi qj s.t.",
                "P j∈Q qj = 1 q ≥ 0. (2) The objective function maximizes the followers expected reward given x, while the constraints make feasible any mixed strategy q for the follower.",
                "The dual to this linear programming problem is the following: min a s.t. a ≥ X i∈X 1 k Cijxi j ∈ Q. (3) From strong duality and complementary slackness we obtain that the followers maximum reward value, a, is the value of every pure strategy with qj > 0, that is in the support of the optimal mixed strategy.",
                "Therefore each of these pure strategies is optimal.",
                "Optimal solutions to the followers problem are characterized by linear programming optimality conditions: primal feasibility constraints in (2), dual feasibility constraints in (3), and complementary slackness qj a − X i∈X 1 k Cijxi ! = 0 j ∈ Q.",
                "These conditions must be included in the problem solved by the leader in order to consider only best responses by the follower to the k-uniform policy x. 314 The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) The leader seeks the k-uniform solution x that maximizes its own payoff, given that the follower uses an optimal response q(x).",
                "Therefore the leader solves the following integer problem: max X i∈X X j∈Q 1 k Rijq(x)j xi s.t.",
                "P i∈X xi = k xi ∈ {0, 1, . . . , k}. (4) Problem (4) maximizes the leaders reward with the followers best response (qj for fixed leaders policy x and hence denoted q(x)j) by selecting a uniform policy from a multiset of constant size k. We complete this problem by including the characterization of q(x) through linear programming optimality conditions.",
                "To simplify writing the complementary slackness conditions, we will constrain q(x) to be only optimal pure strategies by just considering integer solutions of q(x).",
                "The leaders problem becomes: maxx,q X i∈X X j∈Q 1 k Rijxiqj s.t.",
                "P i xi = kP j∈Q qj = 1 0 ≤ (a − P i∈X 1 k Cijxi) ≤ (1 − qj)M xi ∈ {0, 1, ...., k} qj ∈ {0, 1}. (5) Here, the constant M is some large number.",
                "The first and fourth constraints enforce a k-uniform policy for the leader, and the second and fifth constraints enforce a feasible pure strategy for the follower.",
                "The third constraint enforces dual feasibility of the followers problem (leftmost inequality) and the complementary slackness constraint for an optimal pure strategy q for the follower (rightmost inequality).",
                "In fact, since only one pure strategy can be selected by the follower, say qh = 1, this last constraint enforces that a = P i∈X 1 k Cihxi imposing no additional constraint for all other pure strategies which have qj = 0.",
                "We conclude this subsection noting that Problem (5) is an integer program with a non-convex quadratic objective in general, as the matrix R need not be positive-semi-definite.",
                "Efficient solution methods for non-linear, non-convex integer problems remains a challenging research question.",
                "In the next section we show a reformulation of this problem as a linear integer programming problem, for which a number of efficient commercial solvers exist. 4.2 Mixed-Integer Linear Program We can linearize the quadratic program of Problem 5 through the change of variables zij = xiqj, obtaining the following problem maxq,z P i∈X P j∈Q 1 k Rijzij s.t.",
                "P i∈X P j∈Q zij = k P j∈Q zij ≤ k kqj ≤ P i∈X zij ≤ k P j∈Q qj = 1 0 ≤ (a − P i∈X 1 k Cij( P h∈Q zih)) ≤ (1 − qj)M zij ∈ {0, 1, ...., k} qj ∈ {0, 1} (6) PROPOSITION 1.",
                "Problems (5) and (6) are equivalent.",
                "Proof: Consider x, q a feasible solution of (5).",
                "We will show that q, zij = xiqj is a feasible solution of (6) of same objective function value.",
                "The equivalence of the objective functions, and constraints 4, 6 and 7 of (6) are satisfied by construction.",
                "The fact that P j∈Q zij = xi as P j∈Q qj = 1 explains constraints 1, 2, and 5 of (6).",
                "Constraint 3 of (6) is satisfied because P i∈X zij = kqj.",
                "Let us now consider q, z feasible for (6).",
                "We will show that q and xi = P j∈Q zij are feasible for (5) with the same objective value.",
                "In fact all constraints of (5) are readily satisfied by construction.",
                "To see that the objectives match, notice that if qh = 1 then the third constraint in (6) implies that P i∈X zih = k, which means that zij = 0 for all i ∈ X and all j = h. Therefore, xiqj = X l∈Q zilqj = zihqj = zij.",
                "This last equality is because both are 0 when j = h. This shows that the transformation preserves the objective function value, completing the proof.",
                "Given this transformation to a mixed-integer linear program (MILP), we now show how we can apply our decomposition technique on the MILP to obtain significant speedups for Bayesian games with multiple follower types. 5.",
                "DECOMPOSITION FOR MULTIPLE ADVERSARIES The MILP developed in the previous section handles only one follower.",
                "Since our security scenario contains multiple follower (robber) types, we change the response function for the follower from a pure strategy into a weighted combination over various pure follower strategies where the weights are probabilities of occurrence of each of the follower types. 5.1 Decomposed MIQP To admit multiple adversaries in our framework, we modify the notation defined in the previous section to reason about multiple follower types.",
                "We denote by x the vector of strategies of the leader and ql the vector of strategies of follower l, with L denoting the index set of follower types.",
                "We also denote by X and Q the index sets of leader and follower ls pure strategies, respectively.",
                "We also index the payoff matrices on each follower l, considering the matrices Rl and Cl .",
                "Using this modified notation, we characterize the optimal solution of follower ls problem given the leaders k-uniform policy x, with the following optimality conditions: X j∈Q ql j = 1 al − X i∈X 1 k Cl ijxi ≥ 0 ql j(al − X i∈X 1 k Cl ijxi) = 0 ql j ≥ 0 Again, considering only optimal pure strategies for follower ls problem we can linearize the complementarity constraint above.",
                "We incorporate these constraints on the leaders problem that selects the optimal k-uniform policy.",
                "Therefore, given a priori probabilities pl , with l ∈ L of facing each follower, the leader solves the following problem: The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) 315 maxx,q X i∈X X l∈L X j∈Q pl k Rl ijxiql j s.t.",
                "P i xi = kP j∈Q ql j = 1 0 ≤ (al − P i∈X 1 k Cl ijxi) ≤ (1 − ql j)M xi ∈ {0, 1, ...., k} ql j ∈ {0, 1}. (7) Problem (7) for a Bayesian game with multiple follower types is indeed equivalent to Problem (5) on the payoff matrix obtained from the Harsanyi transformation of the game.",
                "In fact, every pure strategy j in Problem (5) corresponds to a sequence of pure strategies jl, one for each follower l ∈ L. This means that qj = 1 if and only if ql jl = 1 for all l ∈ L. In addition, given the a priori probabilities pl of facing player l, the reward in the Harsanyi transformation payoff table is Rij = P l∈L pl Rl ijl .",
                "The same relation holds between C and Cl .",
                "These relations between a pure strategy in the equivalent normal form game and pure strategies in the individual games with each followers are key in showing these problems are equivalent. 5.2 Decomposed MILP We can linearize the quadratic programming problem 7 through the change of variables zl ij = xiql j, obtaining the following problem maxq,z P i∈X P l∈L P j∈Q pl k Rl ijzl ij s.t.",
                "P i∈X P j∈Q zl ij = k P j∈Q zl ij ≤ k kql j ≤ P i∈X zl ij ≤ k P j∈Q ql j = 1 0 ≤ (al − P i∈X 1 k Cl ij( P h∈Q zl ih)) ≤ (1 − ql j)M P j∈Q zl ij = P j∈Q z1 ij zl ij ∈ {0, 1, ...., k} ql j ∈ {0, 1} (8) PROPOSITION 2.",
                "Problems (7) and (8) are equivalent.",
                "Proof: Consider x, ql , al with l ∈ L a feasible solution of (7).",
                "We will show that ql , al , zl ij = xiql j is a feasible solution of (8) of same objective function value.",
                "The equivalence of the objective functions, and constraints 4, 7 and 8 of (8) are satisfied by construction.",
                "The fact that P j∈Q zl ij = xi as P j∈Q ql j = 1 explains constraints 1, 2, 5 and 6 of (8).",
                "Constraint 3 of (8) is satisfied because P i∈X zl ij = kql j.",
                "Lets now consider ql , zl , al feasible for (8).",
                "We will show that ql , al and xi = P j∈Q z1 ij are feasible for (7) with the same objective value.",
                "In fact all constraints of (7) are readily satisfied by construction.",
                "To see that the objectives match, notice for each l one ql j must equal 1 and the rest equal 0.",
                "Let us say that ql jl = 1, then the third constraint in (8) implies that P i∈X zl ijl = k, which means that zl ij = 0 for all i ∈ X and all j = jl.",
                "In particular this implies that xi = X j∈Q z1 ij = z1 ij1 = zl ijl , the last equality from constraint 6 of (8).",
                "Therefore xiql j = zl ijl ql j = zl ij.",
                "This last equality is because both are 0 when j = jl.",
                "Effectively, constraint 6 ensures that all the adversaries are calculating their best responses against a particular fixed policy of the agent.",
                "This shows that the transformation preserves the objective function value, completing the proof.",
                "We can therefore solve this equivalent linear integer program with efficient integer programming packages which can handle problems with thousands of integer variables.",
                "We implemented the decomposed MILP and the results are shown in the following section. 6.",
                "EXPERIMENTAL RESULTS The patrolling domain and the payoffs for the associated game are detailed in Sections 2 and 3.",
                "We performed experiments for this game in worlds of three and four houses with patrols consisting of two houses.",
                "The description given in Section 2 is used to generate a base case for both the security agent and robber payoff functions.",
                "The payoff tables for additional robber types are constructed and added to the game by adding a random distribution of varying size to the payoffs in the base case.",
                "All games are normalized so that, for each robber type, the minimum and maximum payoffs to the security agent and robber are 0 and 1, respectively.",
                "Using the data generated, we performed the experiments using four methods for generating the security agents strategy: • uniform randomization • ASAP • the multiple linear programs method from [5] (to find the true optimal strategy) • the highest reward Bayes-Nash equilibrium, found using the MIP-Nash algorithm [17] The last three methods were applied using CPLEX 8.1.",
                "Because the last two methods are designed for normal-form games rather than Bayesian games, the games were first converted using the Harsanyi transformation [8].",
                "The uniform randomization method is simply choosing a uniform random policy over all possible patrol routes.",
                "We use this method as a simple baseline to measure the performance of our heuristics.",
                "We anticipated that the uniform policy would perform reasonably well since maximum-entropy policies have been shown to be effective in multiagent security domains [14].",
                "The highest-reward Bayes-Nash equilibria were used in order to demonstrate the higher reward gained by looking for an optimal policy rather than an equilibria in Stackelberg games such as our security domain.",
                "Based on our experiments we present three sets of graphs to demonstrate (1) the runtime of ASAP compared to other common methods for finding a strategy, (2) the reward guaranteed by ASAP compared to other methods, and (3) the effect of varying the parameter k, the size of the multiset, on the performance of ASAP.",
                "In the first two sets of graphs, ASAP is run using a multiset of 80 elements; in the third set this number is varied.",
                "The first set of graphs, shown in Figure 1 shows the runtime graphs for three-house (left column) and four-house (right column) domains.",
                "Each of the three rows of graphs corresponds to a different randomly-generated scenario.",
                "The x-axis shows the number of robber types the security agent faces and the y-axis of the graph shows the runtime in seconds.",
                "All experiments that were not concluded in 30 minutes (1800 seconds) were cut off.",
                "The runtime for the uniform policy is always negligible irrespective of the number of adversaries and hence is not shown.",
                "The ASAP algorithm clearly outperforms the optimal, multipleLP method as well as the MIP-Nash algorithm for finding the highestreward Bayes-Nash equilibrium with respect to runtime.",
                "For a 316 The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) Figure 1: Runtimes for various algorithms on problems of 3 and 4 houses. domain of three houses, the optimal method cannot reach a solution for more than seven robber types, and for four houses it cannot solve for more than six types within the cutoff time in any of the three scenarios.",
                "MIP-Nash solves for even fewer robber types within the cutoff time.",
                "On the other hand, ASAP runs much faster, and is able to solve for at least 20 adversaries for the three-house scenarios and for at least 12 adversaries in the four-house scenarios within the cutoff time.",
                "The runtime of ASAP does not increase strictly with the number of robber types for each scenario, but in general, the addition of more types increases the runtime required.",
                "The second set of graphs, Figure 2, shows the reward to the patrol agent given by each method for three scenarios in the three-house (left column) and four-house (right column) domains.",
                "This reward is the utility received by the security agent in the patrolling game, and not as a percentage of the optimal reward, since it was not possible to obtain the optimal reward as the number of robber types increased.",
                "The uniform policy consistently provides the lowest reward in both domains; while the optimal method of course produces the optimal reward.",
                "The ASAP method remains consistently close to the optimal, even as the number of robber types increases.",
                "The highest-reward Bayes-Nash equilibria, provided by the MIPNash method, produced rewards higher than the uniform method, but lower than ASAP.",
                "This difference clearly illustrates the gains in the patrolling domain from committing to a strategy as the leader in a Stackelberg game, rather than playing a standard Bayes-Nash strategy.",
                "The third set of graphs, shown in Figure 3 shows the effect of the multiset size on runtime in seconds (left column) and reward (right column), again expressed as the reward received by the security agent in the patrolling game, and not a percentage of the optimal Figure 2: Reward for various algorithms on problems of 3 and 4 houses. reward.",
                "Results here are for the three-house domain.",
                "The trend is that as as the multiset size is increased, the runtime and reward level both increase.",
                "Not surprisingly, the reward increases monotonically as the multiset size increases, but what is interesting is that there is relatively little benefit to using a large multiset in this domain.",
                "In all cases, the reward given by a multiset of 10 elements was within at least 96% of the reward given by an 80-element multiset.",
                "The runtime does not always increase strictly with the multiset size; indeed in one example (scenario 2 with 20 robber types), using a multiset of 10 elements took 1228 seconds, while using 80 elements only took 617 seconds.",
                "In general, runtime should increase since a larger multiset means a larger domain for the variables in the MILP, and thus a larger search space.",
                "However, an increase in the number of variables can sometimes allow for a policy to be constructed more quickly due to more flexibility in the problem. 7.",
                "SUMMARY AND RELATED WORK This paper focuses on security for agents patrolling in hostile environments.",
                "In these environments, intentional threats are caused by adversaries about whom the security patrolling agents have incomplete information.",
                "Specifically, we deal with situations where the adversaries actions and payoffs are known but the exact adversary type is unknown to the security agent.",
                "Agents acting in the real world quite frequently have such incomplete information about other agents.",
                "Bayesian games have been a popular choice to model such incomplete information games [3].",
                "The Gala toolkit is one method for defining such games [9] without requiring the game to be represented in normal form via the Harsanyi transformation [8]; Galas guarantees are focused on fully competitive games.",
                "Much work has been done on finding optimal Bayes-Nash equilbria for The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) 317 Figure 3: Reward for ASAP using multisets of 10, 30, and 80 elements subclasses of Bayesian games, finding single Bayes-Nash equilibria for general Bayesian games [10] or approximate Bayes-Nash equilibria [18].",
                "Less attention has been paid to finding the optimal strategy to commit to in a Bayesian game (the Stackelberg scenario [15]).",
                "However, the complexity of this problem was shown to be NP-hard in the general case [5], which also provides algorithms for this problem in the non-Bayesian case.",
                "Therefore, we present a heuristic called ASAP, with three key advantages towards addressing this problem.",
                "First, ASAP searches for the highest reward strategy, rather than a Bayes-Nash equilibrium, allowing it to find feasible strategies that exploit the natural first-mover advantage of the game.",
                "Second, it provides strategies which are simple to understand, represent, and implement.",
                "Third, it operates directly on the compact, Bayesian game representation, without requiring conversion to normal form.",
                "We provide an efficient Mixed Integer Linear Program (MILP) implementation for ASAP, along with experimental results illustrating significant speedups and higher rewards over other approaches.",
                "Our k-uniform strategies are similar to the k-uniform strategies of [12].",
                "While that work provides epsilon error-bounds based on the k-uniform strategies, their solution concept is still that of a Nash equilibrium, and they do not provide efficient algorithms for obtaining such k-uniform strategies.",
                "This contrasts with ASAP, where our emphasis is on a highly efficient heuristic approach that is not focused on equilibrium solutions.",
                "Finally the patrolling problem which motivated our work has recently received growing attention from the multiagent community due to its wide range of applications [4, 13].",
                "However most of this work is focused on either limiting energy consumption involved in patrolling [7] or optimizing on criteria like the length of the path traveled [4, 13], without reasoning about any explicit model of an adversary[14].",
                "Acknowledgments : This research is supported by the United States Department of Homeland Security through Center for Risk and Economic Analysis of Terrorism Events (CREATE).",
                "It is also supported by the Defense Advanced Research Projects Agency (DARPA), through the Department of the Interior, NBC, Acquisition Services Division, under Contract No.",
                "NBCHD030010.",
                "Sarit Kraus is also affiliated with UMIACS. 8.",
                "REFERENCES [1] R. W. Beard and T. McLain.",
                "Multiple UAV cooperative search under collision avoidance and limited range communication constraints.",
                "In IEEE CDC, 2003. [2] D. Bertsimas and J. Tsitsiklis.",
                "Introduction to Linear Optimization.",
                "Athena Scientific, 1997. [3] J. Brynielsson and S. Arnborg.",
                "Bayesian games for threat prediction and situation analysis.",
                "In FUSION, 2004. [4] Y. Chevaleyre.",
                "Theoretical analysis of multi-agent patrolling problem.",
                "In AAMAS, 2004. [5] V. Conitzer and T. Sandholm.",
                "Choosing the best strategy to commit to.",
                "In ACM Conference on Electronic Commerce, 2006. [6] D. Fudenberg and J. Tirole.",
                "Game Theory.",
                "MIT Press, 1991. [7] C. Gui and P. Mohapatra.",
                "Virtual patrol: A new power conservation design for surveillance using sensor networks.",
                "In IPSN, 2005. [8] J. C. Harsanyi and R. Selten.",
                "A generalized Nash solution for two-person bargaining games with incomplete information.",
                "Management Science, 18(5):80-106, 1972. [9] D. Koller and A. Pfeffer.",
                "Generating and solving imperfect information games.",
                "In IJCAI, pages 1185-1193, 1995. [10] D. Koller and A. Pfeffer.",
                "Representations and solutions for game-theoretic problems.",
                "Artificial Intelligence, 94(1):167-215, 1997. [11] C. Lemke and J. Howson.",
                "Equilibrium points of bimatrix games.",
                "Journal of the Society for Industrial and Applied Mathematics, 12:413-423, 1964. [12] R. J. Lipton, E. Markakis, and A. Mehta.",
                "Playing large games using simple strategies.",
                "In ACM Conference on Electronic Commerce, 2003. [13] A. Machado, G. Ramalho, J. D. Zucker, and A. Drougoul.",
                "Multi-agent patrolling: an empirical analysis on alternative architectures.",
                "In MABS, 2002. [14] P. Paruchuri, M. Tambe, F. Ordonez, and S. Kraus.",
                "Security in multiagent systems by policy randomization.",
                "In AAMAS, 2006. [15] T. Roughgarden.",
                "Stackelberg scheduling strategies.",
                "In ACM Symposium on TOC, 2001. [16] S. Ruan, C. Meirina, F. Yu, K. R. Pattipati, and R. L. Popp.",
                "Patrolling in a stochastic environment.",
                "In 10th Intl.",
                "Command and Control Research Symp., 2005. [17] T. Sandholm, A. Gilpin, and V. Conitzer.",
                "Mixed-integer programming methods for finding nash equilibria.",
                "In AAAI, 2005. [18] S. Singh, V. Soni, and M. Wellman.",
                "Computing approximate Bayes-Nash equilibria with tree-games of incomplete information.",
                "In ACM Conference on Electronic Commerce, 2004. 318 The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07)"
            ],
            "original_annotated_samples": [],
            "translated_annotated_samples": [],
            "translated_text": "",
            "candidates": [],
            "error": []
        },
        "security of agent system": {
            "translated_key": "Seguridad del sistema de agentes",
            "is_in_text": false,
            "original_annotated_sentences": [
                "An Efficient Heuristic Approach for Security Against Multiple Adversaries Praveen Paruchuri, Jonathan P. Pearce, Milind Tambe, Fernando Ordonez University of Southern California Los Angeles, CA 90089 {paruchur, jppearce, tambe, fordon}@usc.edu Sarit Kraus Bar-Ilan University Ramat-Gan 52900, Israel sarit@cs.biu.ac.il ABSTRACT In adversarial multiagent domains, security, commonly defined as the ability to deal with intentional threats from other agents, is a critical issue.",
                "This paper focuses on domains where these threats come from unknown adversaries.",
                "These domains can be modeled as Bayesian games; much work has been done on finding equilibria for such games.",
                "However, it is often the case in multiagent security domains that one agent can commit to a mixed strategy which its adversaries observe before choosing their own strategies.",
                "In this case, the agent can maximize reward by finding an optimal strategy, without requiring equilibrium.",
                "Previous work has shown this problem of optimal strategy selection to be NP-hard.",
                "Therefore, we present a heuristic called ASAP, with three key advantages to address the problem.",
                "First, ASAP searches for the highest-reward strategy, rather than a Bayes-Nash equilibrium, allowing it to find feasible strategies that exploit the natural first-mover advantage of the game.",
                "Second, it provides strategies which are simple to understand, represent, and implement.",
                "Third, it operates directly on the compact, Bayesian game representation, without requiring conversion to normal form.",
                "We provide an efficient Mixed Integer Linear Program (MILP) implementation for ASAP, along with experimental results illustrating significant speedups and higher rewards over other approaches.",
                "Categories and Subject Descriptors I.2.11 [Computing Methodologies]: Artificial Intelligence: Distributed Artificial Intelligence - Intelligent Agents General Terms Security, Design, Theory 1.",
                "INTRODUCTION In many multiagent domains, agents must act in order to provide security against attacks by adversaries.",
                "A common issue that agents face in such security domains is uncertainty about the adversaries they may be facing.",
                "For example, a security robot may need to make a choice about which areas to patrol, and how often [16].",
                "However, it will not know in advance exactly where a robber will choose to strike.",
                "A team of unmanned aerial vehicles (UAVs) [1] monitoring a region undergoing a humanitarian crisis may also need to choose a patrolling policy.",
                "They must make this decision without knowing in advance whether terrorists or other adversaries may be waiting to disrupt the mission at a given location.",
                "It may indeed be possible to model the motivations of types of adversaries the agent or agent team is likely to face in order to target these adversaries more closely.",
                "However, in both cases, the security robot or UAV team will not know exactly which kinds of adversaries may be active on any given day.",
                "A common approach for choosing a policy for agents in such scenarios is to model the scenarios as Bayesian games.",
                "A Bayesian game is a game in which agents may belong to one or more types; the type of an agent determines its possible actions and payoffs.",
                "The distribution of adversary types that an agent will face may be known or inferred from historical data.",
                "Usually, these games are analyzed according to the solution concept of a Bayes-Nash equilibrium, an extension of the Nash equilibrium for Bayesian games.",
                "However, in many settings, a Nash or Bayes-Nash equilibrium is not an appropriate solution concept, since it assumes that the agents strategies are chosen simultaneously [5].",
                "In some settings, one player can (or must) commit to a strategy before the other players choose their strategies.",
                "These scenarios are known as Stackelberg games [6].",
                "In a Stackelberg game, a leader commits to a strategy first, and then a follower (or group of followers) selfishly optimize their own rewards, considering the action chosen by the leader.",
                "For example, the security agent (leader) must first commit to a strategy for patrolling various areas.",
                "This strategy could be a mixed strategy in order to be unpredictable to the robbers (followers).",
                "The robbers, after observing the pattern of patrols over time, can then choose their strategy (which location to rob).",
                "Often, the leader in a Stackelberg game can attain a higher reward than if the strategies were chosen simultaneously.",
                "To see the advantage of being the leader in a Stackelberg game, consider a simple game with the payoff table as shown in Table 1.",
                "The leader is the row player and the follower is the column player.",
                "Here, the leaders payoff is listed first. 1 2 3 1 5,5 0,0 3,10 2 0,0 2,2 5,0 Table 1: Payoff table for example normal form game.",
                "The only Nash equilibrium for this game is when the leader plays 2 and the follower plays 2 which gives the leader a payoff of 2. 311 978-81-904262-7-5 (RPS) c 2007 IFAAMAS However, if the leader commits to a uniform mixed strategy of playing 1 and 2 with equal (0.5) probability, the followers best response is to play 3 to get an expected payoff of 5 (10 and 0 with equal probability).",
                "The leaders payoff would then be 4 (3 and 5 with equal probability).",
                "In this case, the leader now has an incentive to deviate and choose a pure strategy of 2 (to get a payoff of 5).",
                "However, this would cause the follower to deviate to strategy 2 as well, resulting in the Nash equilibrium.",
                "Thus, by committing to a strategy that is observed by the follower, and by avoiding the temptation to deviate, the leader manages to obtain a reward higher than that of the best Nash equilibrium.",
                "The problem of choosing an optimal strategy for the leader to commit to in a Stackelberg game is analyzed in [5] and found to be NP-hard in the case of a Bayesian game with multiple types of followers.",
                "Thus, efficient heuristic techniques for choosing highreward strategies in these games is an important open issue.",
                "Methods for finding optimal leader strategies for non-Bayesian games [5] can be applied to this problem by converting the Bayesian game into a normal-form game by the Harsanyi transformation [8].",
                "If, on the other hand, we wish to compute the highest-reward Nash equilibrium, new methods using mixed-integer linear programs (MILPs) [17] may be used, since the highest-reward Bayes-Nash equilibrium is equivalent to the corresponding Nash equilibrium in the transformed game.",
                "However, by transforming the game, the compact structure of the Bayesian game is lost.",
                "In addition, since the Nash equilibrium assumes a simultaneous choice of strategies, the advantages of being the leader are not considered.",
                "This paper introduces an efficient heuristic method for approximating the optimal leader strategy for security domains, known as ASAP (Agent Security via Approximate Policies).",
                "This method has three key advantages.",
                "First, it directly searches for an optimal strategy, rather than a Nash (or Bayes-Nash) equilibrium, thus allowing it to find high-reward non-equilibrium strategies like the one in the above example.",
                "Second, it generates policies with a support which can be expressed as a uniform distribution over a multiset of fixed size as proposed in [12].",
                "This allows for policies that are simple to understand and represent [12], as well as a tunable parameter (the size of the multiset) that controls the simplicity of the policy.",
                "Third, the method allows for a Bayes-Nash game to be expressed compactly without conversion to a normal-form game, allowing for large speedups over existing Nash methods such as [17] and [11].",
                "The rest of the paper is organized as follows.",
                "In Section 2 we fully describe the patrolling domain and its properties.",
                "Section 3 introduces the Bayesian game, the Harsanyi transformation, and existing methods for finding an optimal leaders strategy in a Stackelberg game.",
                "Then, in Section 4 the ASAP algorithm is presented for normal-form games, and in Section 5 we show how it can be adapted to the structure of Bayesian games with uncertain adversaries.",
                "Experimental results showing higher reward and faster policy computation over existing Nash methods are shown in Section 6, and we conclude with a discussion of related work in Section 7. 2.",
                "THE PATROLLING DOMAIN In most security patrolling domains, the security agents (like UAVs [1] or security robots [16]) cannot feasibly patrol all areas all the time.",
                "Instead, they must choose a policy by which they patrol various routes at different times, taking into account factors such as the likelihood of crime in different areas, possible targets for crime, and the security agents own resources (number of security agents, amount of available time, fuel, etc.).",
                "It is usually beneficial for this policy to be nondeterministic so that robbers cannot safely rob certain locations, knowing that they will be safe from the security agents [14].",
                "To demonstrate the utility of our algorithm, we use a simplified version of such a domain, expressed as a game.",
                "The most basic version of our game consists of two players: the security agent (the leader) and the robber (the follower) in a world consisting of m houses, 1 . . . m. The security agents set of pure strategies consists of possible routes of d houses to patrol (in an order).",
                "The security agent can choose a mixed strategy so that the robber will be unsure of exactly where the security agent may patrol, but the robber will know the mixed strategy the security agent has chosen.",
                "For example, the robber can observe over time how often the security agent patrols each area.",
                "With this knowledge, the robber must choose a single house to rob.",
                "We assume that the robber generally takes a long time to rob a house.",
                "If the house chosen by the robber is not on the security agents route, then the robber successfully robs it.",
                "Otherwise, if it is on the security agents route, then the earlier the house is on the route, the easier it is for the security agent to catch the robber before he finishes robbing it.",
                "We model the payoffs for this game with the following variables: • vl,x: value of the goods in house l to the security agent. • vl,q: value of the goods in house l to the robber. • cx: reward to the security agent of catching the robber. • cq: cost to the robber of getting caught. • pl: probability that the security agent can catch the robber at the lth house in the patrol (pl < pl ⇐⇒ l < l).",
                "The security agents set of possible pure strategies (patrol routes) is denoted by X and includes all d-tuples i =< w1, w2, ..., wd > with w1 . . . wd = 1 . . . m where no two elements are equal (the agent is not allowed to return to the same house).",
                "The robbers set of possible pure strategies (houses to rob) is denoted by Q and includes all integers j = 1 . . . m. The payoffs (security agent, robber) for pure strategies i, j are: • −vl,x, vl,q, for j = l /∈ i. • plcx +(1−pl)(−vl,x), −plcq +(1−pl)(vl,q), for j = l ∈ i.",
                "With this structure it is possible to model many different types of robbers who have differing motivations; for example, one robber may have a lower cost of getting caught than another, or may value the goods in the various houses differently.",
                "If the distribution of different robber types is known or inferred from historical data, then the game can be modeled as a Bayesian game [6]. 3.",
                "BAYESIAN GAMES A Bayesian game contains a set of N agents, and each agent n must be one of a given set of types θn.",
                "For our patrolling domain, we have two agents, the security agent and the robber. θ1 is the set of security agent types and θ2 is the set of robber types.",
                "Since there is only one type of security agent, θ1 contains only one element.",
                "During the game, the robber knows its type but the security agent does not know the robbers type.",
                "For each agent (the security agent or the robber) n, there is a set of strategies σn and a utility function un : θ1 × θ2 × σ1 × σ2 → .",
                "A Bayesian game can be transformed into a normal-form game using the Harsanyi transformation [8].",
                "Once this is done, new, linear-program (LP)-based methods for finding high-reward strategies for normal-form games [5] can be used to find a strategy in the transformed game; this strategy can then be used for the Bayesian game.",
                "While methods exist for finding Bayes-Nash equilibria directly, without the Harsanyi transformation [10], they find only a 312 The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) single equilibrium in the general case, which may not be of high reward.",
                "Recent work [17] has led to efficient mixed-integer linear program techniques to find the best Nash equilibrium for a given agent.",
                "However, these techniques do require a normal-form game, and so to compare the policies given by ASAP against the optimal policy, as well as against the highest-reward Nash equilibrium, we must apply these techniques to the Harsanyi-transformed matrix.",
                "The next two subsections elaborate on how this is done. 3.1 Harsanyi Transformation The first step in solving Bayesian games is to apply the Harsanyi transformation [8] that converts the Bayesian game into a normal form game.",
                "Given that the Harsanyi transformation is a standard concept in game theory, we explain it briefly through a simple example in our patrolling domain without introducing the mathematical formulations.",
                "Let us assume there are two robber types a and b in the Bayesian game.",
                "Robber a will be active with probability α, and robber b will be active with probability 1 − α.",
                "The rules described in Section 2 allow us to construct simple payoff tables.",
                "Assume that there are two houses in the world (1 and 2) and hence there are two patrol routes (pure strategies) for the agent: {1,2} and {2,1}.",
                "The robber can rob either house 1 or house 2 and hence he has two strategies (denoted as 1l, 2l for robber type l).",
                "Since there are two types assumed (denoted as a and b), we construct two payoff tables (shown in Table 2) corresponding to the security agent playing a separate game with each of the two robber types with probabilities α and 1 − α.",
                "First, consider robber type a.",
                "Borrowing the notation from the domain section, we assign the following values to the variables: v1,x = v1,q = 3/4, v2,x = v2,q = 1/4, cx = 1/2, cq = 1, p1 = 1, p2 = 1/2.",
                "Using these values we construct a base payoff table as the payoff for the game against robber type a.",
                "For example, if the security agent chooses route {1,2} when robber a is active, and robber a chooses house 1, the robber receives a reward of -1 (for being caught) and the agent receives a reward of 0.5 for catching the robber.",
                "The payoffs for the game against robber type b are constructed using different values.",
                "Security agent: {1,2} {2,1} Robber a 1a -1, .5 -.375, .125 2a -.125, -.125 -1, .5 Robber b 1b -.9, .6 -.275, .225 2b -.025, -.025 -.9, .6 Table 2: Payoff tables: Security Agent vs Robbers a and b Using the Harsanyi technique involves introducing a chance node, that determines the robbers type, thus transforming the security agents incomplete information regarding the robber into imperfect information [3].",
                "The Bayesian equilibrium of the game is then precisely the Nash equilibrium of the imperfect information game.",
                "The transformed, normal-form game is shown in Table 3.",
                "In the transformed game, the security agent is the column player, and the set of all robber types together is the row player.",
                "Suppose that robber type a robs house 1 and robber type b robs house 2, while the security agent chooses patrol {1,2}.",
                "Then, the security agent and the robber receive an expected payoff corresponding to their payoffs from the agent encountering robber a at house 1 with probability α and robber b at house 2 with probability 1 − α. 3.2 Finding an Optimal Strategy Although a Nash equilibrium is the standard solution concept for games in which agents choose strategies simultaneously, in our security domain, the security agent (the leader) can gain an advantage by committing to a mixed strategy in advance.",
                "Since the followers (the robbers) will know the leaders strategy, the optimal response for the followers will be a pure strategy.",
                "Given the common assumption, taken in [5], in the case where followers are indifferent, they will choose the strategy that benefits the leader, there must exist a guaranteed optimal strategy for the leader [5].",
                "From the Bayesian game in Table 2, we constructed the Harsanyi transformed bimatrix in Table 3.",
                "The strategies for each player (security agent or robber) in the transformed game correspond to all combinations of possible strategies taken by each of that players types.",
                "Therefore, we denote X = σθ1 1 = σ1 and Q = σθ2 2 as the index sets of the security agent and robbers pure strategies respectively, with R and C as the corresponding payoff matrices.",
                "Rij is the reward of the security agent and Cij is the reward of the robbers when the security agent takes pure strategy i and the robbers take pure strategy j.",
                "A mixed strategy for the security agent is a probability distribution over its set of pure strategies and will be represented by a vector x = (px1, px2, . . . , px|X|), where pxi ≥ 0 and P pxi = 1.",
                "Here, pxi is the probability that the security agent will choose its ith pure strategy.",
                "The optimal mixed strategy for the security agent can be found in time polynomial in the number of rows in the normal form game using the following linear program formulation from [5].",
                "For every possible pure strategy j by the follower (the set of all robber types), max P i∈X pxiRij s.t. ∀j ∈ Q, P i∈σ1 pxiCij ≥ P i∈σ1 pxiCij P i∈X pxi = 1 ∀i∈X , pxi >= 0 (1) Then, for all feasible follower strategies j, choose the one that maximizes P i∈X pxiRij, the reward for the security agent (leader).",
                "The pxi variables give the optimal strategy for the security agent.",
                "Note that while this method is polynomial in the number of rows in the transformed, normal-form game, the number of rows increases exponentially with the number of robber types.",
                "Using this method for a Bayesian game thus requires running |σ2||θ2| separate linear programs.",
                "This is no surprise, since finding the leaders optimal strategy in a Bayesian Stackelberg game is NP-hard [5]. 4.",
                "HEURISTIC APPROACHES Given that finding the optimal strategy for the leader is NP-hard, we provide a heuristic approach.",
                "In this heuristic we limit the possible mixed strategies of the leader to select actions with probabilities that are integer multiples of 1/k for a predetermined integer k. Previous work [14] has shown that strategies with high entropy are beneficial for security applications when opponents utilities are completely unknown.",
                "In our domain, if utilities are not considered, this method will result in uniform-distribution strategies.",
                "One advantage of such strategies is that they are compact to represent (as fractions) and simple to understand; therefore they can be efficiently implemented by real organizations.",
                "We aim to maintain the advantage provided by simple strategies for our security application problem, incorporating the effect of the robbers rewards on the security agents rewards.",
                "Thus, the ASAP heuristic will produce strategies which are k-uniform.",
                "A mixed strategy is denoted k-uniform if it is a uniform distribution on a multiset S of pure strategies with |S| = k. A multiset is a set whose elements may be repeated multiple times; thus, for example, the mixed strategy corresponding to the multiset {1, 1, 2} would take strategy 1 The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) 313 {1,2} {2,1} {1a, 1b} −1α − .9(1 − α), .5α + .6(1 − α) −.375α − .275(1 − α), .125α + .225(1 − α) {1a, 2b} −1α − .025(1 − α), .5α − .025(1 − α) −.375α − .9(1 − α), .125α + .6(1 − α) {2a, 1b} −.125α − .9(1 − α), −.125α + .6(1 − α) −1α − .275(1 − α), .5α + .225(1 − α) {2a, 2b} −.125α − .025(1 − α), −.125α − .025(1 − α) −1α − .9(1 − α), .5α + .6(1 − α) Table 3: Harsanyi Transformed Payoff Table with probability 2/3 and strategy 2 with probability 1/3.",
                "ASAP allows the size of the multiset to be chosen in order to balance the complexity of the strategy reached with the goal that the identified strategy will yield a high reward.",
                "Another advantage of the ASAP heuristic is that it operates directly on the compact Bayesian representation, without requiring the Harsanyi transformation.",
                "This is because the different follower (robber) types are independent of each other.",
                "Hence, evaluating the leader strategy against a Harsanyi-transformed game matrix is equivalent to evaluating against each of the game matrices for the individual follower types.",
                "This independence property is exploited in ASAP to yield a decomposition scheme.",
                "Note that the LP method introduced by [5] to compute optimal Stackelberg policies is unlikely to be decomposable into a small number of games as it was shown to be NP-hard for Bayes-Nash problems.",
                "Finally, note that ASAP requires the solution of only one optimization problem, rather than solving a series of problems as in the LP method of [5].",
                "For a single follower type, the algorithm works the following way.",
                "Given a particular k, for each possible mixed strategy x for the leader that corresponds to a multiset of size k, evaluate the leaders payoff from x when the follower plays a reward-maximizing pure strategy.",
                "We then take the mixed strategy with the highest payoff.",
                "We need only to consider the reward-maximizing pure strategies of the followers (robbers), since for a given fixed strategy x of the security agent, each robber type faces a problem with fixed linear rewards.",
                "If a mixed strategy is optimal for the robber, then so are all the pure strategies in the support of that mixed strategy.",
                "Note also that because we limit the leaders strategies to take on discrete values, the assumption from Section 3.2 that the followers will break ties in the leaders favor is not significant, since ties will be unlikely to arise.",
                "This is because, in domains where rewards are drawn from any random distribution, the probability of a follower having more than one pure optimal response to a given leader strategy approaches zero, and the leader will have only a finite number of possible mixed strategies.",
                "Our approach to characterize the optimal strategy for the security agent makes use of properties of linear programming.",
                "We briefly outline these results here for completeness, for detailed discussion and proofs see one of many references on the topic, such as [2].",
                "Every linear programming problem, such as: max cT x | Ax = b, x ≥ 0, has an associated dual linear program, in this case: min bT y | AT y ≥ c. These primal/dual pairs of problems satisfy weak duality: For any x and y primal and dual feasible solutions respectively, cT x ≤ bT y.",
                "Thus a pair of feasible solutions is optimal if cT x = bT y, and the problems are said to satisfy strong duality.",
                "In fact if a linear program is feasible and has a bounded optimal solution, then the dual is also feasible and there is a pair x∗ , y∗ that satisfies cT x∗ = bT y∗ .",
                "These optimal solutions are characterized with the following optimality conditions (as defined in [2]): • primal feasibility: Ax = b, x ≥ 0 • dual feasibility: AT y ≥ c • complementary slackness: xi(AT y − c)i = 0 for all i.",
                "Note that this last condition implies that cT x = xT AT y = bT y, which proves optimality for primal dual feasible solutions x and y.",
                "In the following subsections, we first define the problem in its most intuititive form as a mixed-integer quadratic program (MIQP), and then show how this problem can be converted into a mixedinteger linear program (MILP). 4.1 Mixed-Integer Quadratic Program We begin with the case of a single type of follower.",
                "Let the leader be the row player and the follower the column player.",
                "We denote by x the vector of strategies of the leader and q the vector of strategies of the follower.",
                "We also denote X and Q the index sets of the leader and followers pure strategies, respectively.",
                "The payoff matrices R and C correspond to: Rij is the reward of the leader and Cij is the reward of the follower when the leader takes pure strategy i and the follower takes pure strategy j.",
                "Let k be the size of the multiset.",
                "We first fix the policy of the leader to some k-uniform policy x.",
                "The value xi is the number of times pure strategy i is used in the k-uniform policy, which is selected with probability xi/k.",
                "We formulate the optimization problem the follower solves to find its optimal response to x as the following linear program: max X j∈Q X i∈X 1 k Cijxi qj s.t.",
                "P j∈Q qj = 1 q ≥ 0. (2) The objective function maximizes the followers expected reward given x, while the constraints make feasible any mixed strategy q for the follower.",
                "The dual to this linear programming problem is the following: min a s.t. a ≥ X i∈X 1 k Cijxi j ∈ Q. (3) From strong duality and complementary slackness we obtain that the followers maximum reward value, a, is the value of every pure strategy with qj > 0, that is in the support of the optimal mixed strategy.",
                "Therefore each of these pure strategies is optimal.",
                "Optimal solutions to the followers problem are characterized by linear programming optimality conditions: primal feasibility constraints in (2), dual feasibility constraints in (3), and complementary slackness qj a − X i∈X 1 k Cijxi ! = 0 j ∈ Q.",
                "These conditions must be included in the problem solved by the leader in order to consider only best responses by the follower to the k-uniform policy x. 314 The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) The leader seeks the k-uniform solution x that maximizes its own payoff, given that the follower uses an optimal response q(x).",
                "Therefore the leader solves the following integer problem: max X i∈X X j∈Q 1 k Rijq(x)j xi s.t.",
                "P i∈X xi = k xi ∈ {0, 1, . . . , k}. (4) Problem (4) maximizes the leaders reward with the followers best response (qj for fixed leaders policy x and hence denoted q(x)j) by selecting a uniform policy from a multiset of constant size k. We complete this problem by including the characterization of q(x) through linear programming optimality conditions.",
                "To simplify writing the complementary slackness conditions, we will constrain q(x) to be only optimal pure strategies by just considering integer solutions of q(x).",
                "The leaders problem becomes: maxx,q X i∈X X j∈Q 1 k Rijxiqj s.t.",
                "P i xi = kP j∈Q qj = 1 0 ≤ (a − P i∈X 1 k Cijxi) ≤ (1 − qj)M xi ∈ {0, 1, ...., k} qj ∈ {0, 1}. (5) Here, the constant M is some large number.",
                "The first and fourth constraints enforce a k-uniform policy for the leader, and the second and fifth constraints enforce a feasible pure strategy for the follower.",
                "The third constraint enforces dual feasibility of the followers problem (leftmost inequality) and the complementary slackness constraint for an optimal pure strategy q for the follower (rightmost inequality).",
                "In fact, since only one pure strategy can be selected by the follower, say qh = 1, this last constraint enforces that a = P i∈X 1 k Cihxi imposing no additional constraint for all other pure strategies which have qj = 0.",
                "We conclude this subsection noting that Problem (5) is an integer program with a non-convex quadratic objective in general, as the matrix R need not be positive-semi-definite.",
                "Efficient solution methods for non-linear, non-convex integer problems remains a challenging research question.",
                "In the next section we show a reformulation of this problem as a linear integer programming problem, for which a number of efficient commercial solvers exist. 4.2 Mixed-Integer Linear Program We can linearize the quadratic program of Problem 5 through the change of variables zij = xiqj, obtaining the following problem maxq,z P i∈X P j∈Q 1 k Rijzij s.t.",
                "P i∈X P j∈Q zij = k P j∈Q zij ≤ k kqj ≤ P i∈X zij ≤ k P j∈Q qj = 1 0 ≤ (a − P i∈X 1 k Cij( P h∈Q zih)) ≤ (1 − qj)M zij ∈ {0, 1, ...., k} qj ∈ {0, 1} (6) PROPOSITION 1.",
                "Problems (5) and (6) are equivalent.",
                "Proof: Consider x, q a feasible solution of (5).",
                "We will show that q, zij = xiqj is a feasible solution of (6) of same objective function value.",
                "The equivalence of the objective functions, and constraints 4, 6 and 7 of (6) are satisfied by construction.",
                "The fact that P j∈Q zij = xi as P j∈Q qj = 1 explains constraints 1, 2, and 5 of (6).",
                "Constraint 3 of (6) is satisfied because P i∈X zij = kqj.",
                "Let us now consider q, z feasible for (6).",
                "We will show that q and xi = P j∈Q zij are feasible for (5) with the same objective value.",
                "In fact all constraints of (5) are readily satisfied by construction.",
                "To see that the objectives match, notice that if qh = 1 then the third constraint in (6) implies that P i∈X zih = k, which means that zij = 0 for all i ∈ X and all j = h. Therefore, xiqj = X l∈Q zilqj = zihqj = zij.",
                "This last equality is because both are 0 when j = h. This shows that the transformation preserves the objective function value, completing the proof.",
                "Given this transformation to a mixed-integer linear program (MILP), we now show how we can apply our decomposition technique on the MILP to obtain significant speedups for Bayesian games with multiple follower types. 5.",
                "DECOMPOSITION FOR MULTIPLE ADVERSARIES The MILP developed in the previous section handles only one follower.",
                "Since our security scenario contains multiple follower (robber) types, we change the response function for the follower from a pure strategy into a weighted combination over various pure follower strategies where the weights are probabilities of occurrence of each of the follower types. 5.1 Decomposed MIQP To admit multiple adversaries in our framework, we modify the notation defined in the previous section to reason about multiple follower types.",
                "We denote by x the vector of strategies of the leader and ql the vector of strategies of follower l, with L denoting the index set of follower types.",
                "We also denote by X and Q the index sets of leader and follower ls pure strategies, respectively.",
                "We also index the payoff matrices on each follower l, considering the matrices Rl and Cl .",
                "Using this modified notation, we characterize the optimal solution of follower ls problem given the leaders k-uniform policy x, with the following optimality conditions: X j∈Q ql j = 1 al − X i∈X 1 k Cl ijxi ≥ 0 ql j(al − X i∈X 1 k Cl ijxi) = 0 ql j ≥ 0 Again, considering only optimal pure strategies for follower ls problem we can linearize the complementarity constraint above.",
                "We incorporate these constraints on the leaders problem that selects the optimal k-uniform policy.",
                "Therefore, given a priori probabilities pl , with l ∈ L of facing each follower, the leader solves the following problem: The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) 315 maxx,q X i∈X X l∈L X j∈Q pl k Rl ijxiql j s.t.",
                "P i xi = kP j∈Q ql j = 1 0 ≤ (al − P i∈X 1 k Cl ijxi) ≤ (1 − ql j)M xi ∈ {0, 1, ...., k} ql j ∈ {0, 1}. (7) Problem (7) for a Bayesian game with multiple follower types is indeed equivalent to Problem (5) on the payoff matrix obtained from the Harsanyi transformation of the game.",
                "In fact, every pure strategy j in Problem (5) corresponds to a sequence of pure strategies jl, one for each follower l ∈ L. This means that qj = 1 if and only if ql jl = 1 for all l ∈ L. In addition, given the a priori probabilities pl of facing player l, the reward in the Harsanyi transformation payoff table is Rij = P l∈L pl Rl ijl .",
                "The same relation holds between C and Cl .",
                "These relations between a pure strategy in the equivalent normal form game and pure strategies in the individual games with each followers are key in showing these problems are equivalent. 5.2 Decomposed MILP We can linearize the quadratic programming problem 7 through the change of variables zl ij = xiql j, obtaining the following problem maxq,z P i∈X P l∈L P j∈Q pl k Rl ijzl ij s.t.",
                "P i∈X P j∈Q zl ij = k P j∈Q zl ij ≤ k kql j ≤ P i∈X zl ij ≤ k P j∈Q ql j = 1 0 ≤ (al − P i∈X 1 k Cl ij( P h∈Q zl ih)) ≤ (1 − ql j)M P j∈Q zl ij = P j∈Q z1 ij zl ij ∈ {0, 1, ...., k} ql j ∈ {0, 1} (8) PROPOSITION 2.",
                "Problems (7) and (8) are equivalent.",
                "Proof: Consider x, ql , al with l ∈ L a feasible solution of (7).",
                "We will show that ql , al , zl ij = xiql j is a feasible solution of (8) of same objective function value.",
                "The equivalence of the objective functions, and constraints 4, 7 and 8 of (8) are satisfied by construction.",
                "The fact that P j∈Q zl ij = xi as P j∈Q ql j = 1 explains constraints 1, 2, 5 and 6 of (8).",
                "Constraint 3 of (8) is satisfied because P i∈X zl ij = kql j.",
                "Lets now consider ql , zl , al feasible for (8).",
                "We will show that ql , al and xi = P j∈Q z1 ij are feasible for (7) with the same objective value.",
                "In fact all constraints of (7) are readily satisfied by construction.",
                "To see that the objectives match, notice for each l one ql j must equal 1 and the rest equal 0.",
                "Let us say that ql jl = 1, then the third constraint in (8) implies that P i∈X zl ijl = k, which means that zl ij = 0 for all i ∈ X and all j = jl.",
                "In particular this implies that xi = X j∈Q z1 ij = z1 ij1 = zl ijl , the last equality from constraint 6 of (8).",
                "Therefore xiql j = zl ijl ql j = zl ij.",
                "This last equality is because both are 0 when j = jl.",
                "Effectively, constraint 6 ensures that all the adversaries are calculating their best responses against a particular fixed policy of the agent.",
                "This shows that the transformation preserves the objective function value, completing the proof.",
                "We can therefore solve this equivalent linear integer program with efficient integer programming packages which can handle problems with thousands of integer variables.",
                "We implemented the decomposed MILP and the results are shown in the following section. 6.",
                "EXPERIMENTAL RESULTS The patrolling domain and the payoffs for the associated game are detailed in Sections 2 and 3.",
                "We performed experiments for this game in worlds of three and four houses with patrols consisting of two houses.",
                "The description given in Section 2 is used to generate a base case for both the security agent and robber payoff functions.",
                "The payoff tables for additional robber types are constructed and added to the game by adding a random distribution of varying size to the payoffs in the base case.",
                "All games are normalized so that, for each robber type, the minimum and maximum payoffs to the security agent and robber are 0 and 1, respectively.",
                "Using the data generated, we performed the experiments using four methods for generating the security agents strategy: • uniform randomization • ASAP • the multiple linear programs method from [5] (to find the true optimal strategy) • the highest reward Bayes-Nash equilibrium, found using the MIP-Nash algorithm [17] The last three methods were applied using CPLEX 8.1.",
                "Because the last two methods are designed for normal-form games rather than Bayesian games, the games were first converted using the Harsanyi transformation [8].",
                "The uniform randomization method is simply choosing a uniform random policy over all possible patrol routes.",
                "We use this method as a simple baseline to measure the performance of our heuristics.",
                "We anticipated that the uniform policy would perform reasonably well since maximum-entropy policies have been shown to be effective in multiagent security domains [14].",
                "The highest-reward Bayes-Nash equilibria were used in order to demonstrate the higher reward gained by looking for an optimal policy rather than an equilibria in Stackelberg games such as our security domain.",
                "Based on our experiments we present three sets of graphs to demonstrate (1) the runtime of ASAP compared to other common methods for finding a strategy, (2) the reward guaranteed by ASAP compared to other methods, and (3) the effect of varying the parameter k, the size of the multiset, on the performance of ASAP.",
                "In the first two sets of graphs, ASAP is run using a multiset of 80 elements; in the third set this number is varied.",
                "The first set of graphs, shown in Figure 1 shows the runtime graphs for three-house (left column) and four-house (right column) domains.",
                "Each of the three rows of graphs corresponds to a different randomly-generated scenario.",
                "The x-axis shows the number of robber types the security agent faces and the y-axis of the graph shows the runtime in seconds.",
                "All experiments that were not concluded in 30 minutes (1800 seconds) were cut off.",
                "The runtime for the uniform policy is always negligible irrespective of the number of adversaries and hence is not shown.",
                "The ASAP algorithm clearly outperforms the optimal, multipleLP method as well as the MIP-Nash algorithm for finding the highestreward Bayes-Nash equilibrium with respect to runtime.",
                "For a 316 The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) Figure 1: Runtimes for various algorithms on problems of 3 and 4 houses. domain of three houses, the optimal method cannot reach a solution for more than seven robber types, and for four houses it cannot solve for more than six types within the cutoff time in any of the three scenarios.",
                "MIP-Nash solves for even fewer robber types within the cutoff time.",
                "On the other hand, ASAP runs much faster, and is able to solve for at least 20 adversaries for the three-house scenarios and for at least 12 adversaries in the four-house scenarios within the cutoff time.",
                "The runtime of ASAP does not increase strictly with the number of robber types for each scenario, but in general, the addition of more types increases the runtime required.",
                "The second set of graphs, Figure 2, shows the reward to the patrol agent given by each method for three scenarios in the three-house (left column) and four-house (right column) domains.",
                "This reward is the utility received by the security agent in the patrolling game, and not as a percentage of the optimal reward, since it was not possible to obtain the optimal reward as the number of robber types increased.",
                "The uniform policy consistently provides the lowest reward in both domains; while the optimal method of course produces the optimal reward.",
                "The ASAP method remains consistently close to the optimal, even as the number of robber types increases.",
                "The highest-reward Bayes-Nash equilibria, provided by the MIPNash method, produced rewards higher than the uniform method, but lower than ASAP.",
                "This difference clearly illustrates the gains in the patrolling domain from committing to a strategy as the leader in a Stackelberg game, rather than playing a standard Bayes-Nash strategy.",
                "The third set of graphs, shown in Figure 3 shows the effect of the multiset size on runtime in seconds (left column) and reward (right column), again expressed as the reward received by the security agent in the patrolling game, and not a percentage of the optimal Figure 2: Reward for various algorithms on problems of 3 and 4 houses. reward.",
                "Results here are for the three-house domain.",
                "The trend is that as as the multiset size is increased, the runtime and reward level both increase.",
                "Not surprisingly, the reward increases monotonically as the multiset size increases, but what is interesting is that there is relatively little benefit to using a large multiset in this domain.",
                "In all cases, the reward given by a multiset of 10 elements was within at least 96% of the reward given by an 80-element multiset.",
                "The runtime does not always increase strictly with the multiset size; indeed in one example (scenario 2 with 20 robber types), using a multiset of 10 elements took 1228 seconds, while using 80 elements only took 617 seconds.",
                "In general, runtime should increase since a larger multiset means a larger domain for the variables in the MILP, and thus a larger search space.",
                "However, an increase in the number of variables can sometimes allow for a policy to be constructed more quickly due to more flexibility in the problem. 7.",
                "SUMMARY AND RELATED WORK This paper focuses on security for agents patrolling in hostile environments.",
                "In these environments, intentional threats are caused by adversaries about whom the security patrolling agents have incomplete information.",
                "Specifically, we deal with situations where the adversaries actions and payoffs are known but the exact adversary type is unknown to the security agent.",
                "Agents acting in the real world quite frequently have such incomplete information about other agents.",
                "Bayesian games have been a popular choice to model such incomplete information games [3].",
                "The Gala toolkit is one method for defining such games [9] without requiring the game to be represented in normal form via the Harsanyi transformation [8]; Galas guarantees are focused on fully competitive games.",
                "Much work has been done on finding optimal Bayes-Nash equilbria for The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) 317 Figure 3: Reward for ASAP using multisets of 10, 30, and 80 elements subclasses of Bayesian games, finding single Bayes-Nash equilibria for general Bayesian games [10] or approximate Bayes-Nash equilibria [18].",
                "Less attention has been paid to finding the optimal strategy to commit to in a Bayesian game (the Stackelberg scenario [15]).",
                "However, the complexity of this problem was shown to be NP-hard in the general case [5], which also provides algorithms for this problem in the non-Bayesian case.",
                "Therefore, we present a heuristic called ASAP, with three key advantages towards addressing this problem.",
                "First, ASAP searches for the highest reward strategy, rather than a Bayes-Nash equilibrium, allowing it to find feasible strategies that exploit the natural first-mover advantage of the game.",
                "Second, it provides strategies which are simple to understand, represent, and implement.",
                "Third, it operates directly on the compact, Bayesian game representation, without requiring conversion to normal form.",
                "We provide an efficient Mixed Integer Linear Program (MILP) implementation for ASAP, along with experimental results illustrating significant speedups and higher rewards over other approaches.",
                "Our k-uniform strategies are similar to the k-uniform strategies of [12].",
                "While that work provides epsilon error-bounds based on the k-uniform strategies, their solution concept is still that of a Nash equilibrium, and they do not provide efficient algorithms for obtaining such k-uniform strategies.",
                "This contrasts with ASAP, where our emphasis is on a highly efficient heuristic approach that is not focused on equilibrium solutions.",
                "Finally the patrolling problem which motivated our work has recently received growing attention from the multiagent community due to its wide range of applications [4, 13].",
                "However most of this work is focused on either limiting energy consumption involved in patrolling [7] or optimizing on criteria like the length of the path traveled [4, 13], without reasoning about any explicit model of an adversary[14].",
                "Acknowledgments : This research is supported by the United States Department of Homeland Security through Center for Risk and Economic Analysis of Terrorism Events (CREATE).",
                "It is also supported by the Defense Advanced Research Projects Agency (DARPA), through the Department of the Interior, NBC, Acquisition Services Division, under Contract No.",
                "NBCHD030010.",
                "Sarit Kraus is also affiliated with UMIACS. 8.",
                "REFERENCES [1] R. W. Beard and T. McLain.",
                "Multiple UAV cooperative search under collision avoidance and limited range communication constraints.",
                "In IEEE CDC, 2003. [2] D. Bertsimas and J. Tsitsiklis.",
                "Introduction to Linear Optimization.",
                "Athena Scientific, 1997. [3] J. Brynielsson and S. Arnborg.",
                "Bayesian games for threat prediction and situation analysis.",
                "In FUSION, 2004. [4] Y. Chevaleyre.",
                "Theoretical analysis of multi-agent patrolling problem.",
                "In AAMAS, 2004. [5] V. Conitzer and T. Sandholm.",
                "Choosing the best strategy to commit to.",
                "In ACM Conference on Electronic Commerce, 2006. [6] D. Fudenberg and J. Tirole.",
                "Game Theory.",
                "MIT Press, 1991. [7] C. Gui and P. Mohapatra.",
                "Virtual patrol: A new power conservation design for surveillance using sensor networks.",
                "In IPSN, 2005. [8] J. C. Harsanyi and R. Selten.",
                "A generalized Nash solution for two-person bargaining games with incomplete information.",
                "Management Science, 18(5):80-106, 1972. [9] D. Koller and A. Pfeffer.",
                "Generating and solving imperfect information games.",
                "In IJCAI, pages 1185-1193, 1995. [10] D. Koller and A. Pfeffer.",
                "Representations and solutions for game-theoretic problems.",
                "Artificial Intelligence, 94(1):167-215, 1997. [11] C. Lemke and J. Howson.",
                "Equilibrium points of bimatrix games.",
                "Journal of the Society for Industrial and Applied Mathematics, 12:413-423, 1964. [12] R. J. Lipton, E. Markakis, and A. Mehta.",
                "Playing large games using simple strategies.",
                "In ACM Conference on Electronic Commerce, 2003. [13] A. Machado, G. Ramalho, J. D. Zucker, and A. Drougoul.",
                "Multi-agent patrolling: an empirical analysis on alternative architectures.",
                "In MABS, 2002. [14] P. Paruchuri, M. Tambe, F. Ordonez, and S. Kraus.",
                "Security in multiagent systems by policy randomization.",
                "In AAMAS, 2006. [15] T. Roughgarden.",
                "Stackelberg scheduling strategies.",
                "In ACM Symposium on TOC, 2001. [16] S. Ruan, C. Meirina, F. Yu, K. R. Pattipati, and R. L. Popp.",
                "Patrolling in a stochastic environment.",
                "In 10th Intl.",
                "Command and Control Research Symp., 2005. [17] T. Sandholm, A. Gilpin, and V. Conitzer.",
                "Mixed-integer programming methods for finding nash equilibria.",
                "In AAAI, 2005. [18] S. Singh, V. Soni, and M. Wellman.",
                "Computing approximate Bayes-Nash equilibria with tree-games of incomplete information.",
                "In ACM Conference on Electronic Commerce, 2004. 318 The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07)"
            ],
            "original_annotated_samples": [],
            "translated_annotated_samples": [],
            "translated_text": "",
            "candidates": [],
            "error": []
        },
        "agent system security": {
            "translated_key": "Seguridad del sistema de agentes",
            "is_in_text": false,
            "original_annotated_sentences": [
                "An Efficient Heuristic Approach for Security Against Multiple Adversaries Praveen Paruchuri, Jonathan P. Pearce, Milind Tambe, Fernando Ordonez University of Southern California Los Angeles, CA 90089 {paruchur, jppearce, tambe, fordon}@usc.edu Sarit Kraus Bar-Ilan University Ramat-Gan 52900, Israel sarit@cs.biu.ac.il ABSTRACT In adversarial multiagent domains, security, commonly defined as the ability to deal with intentional threats from other agents, is a critical issue.",
                "This paper focuses on domains where these threats come from unknown adversaries.",
                "These domains can be modeled as Bayesian games; much work has been done on finding equilibria for such games.",
                "However, it is often the case in multiagent security domains that one agent can commit to a mixed strategy which its adversaries observe before choosing their own strategies.",
                "In this case, the agent can maximize reward by finding an optimal strategy, without requiring equilibrium.",
                "Previous work has shown this problem of optimal strategy selection to be NP-hard.",
                "Therefore, we present a heuristic called ASAP, with three key advantages to address the problem.",
                "First, ASAP searches for the highest-reward strategy, rather than a Bayes-Nash equilibrium, allowing it to find feasible strategies that exploit the natural first-mover advantage of the game.",
                "Second, it provides strategies which are simple to understand, represent, and implement.",
                "Third, it operates directly on the compact, Bayesian game representation, without requiring conversion to normal form.",
                "We provide an efficient Mixed Integer Linear Program (MILP) implementation for ASAP, along with experimental results illustrating significant speedups and higher rewards over other approaches.",
                "Categories and Subject Descriptors I.2.11 [Computing Methodologies]: Artificial Intelligence: Distributed Artificial Intelligence - Intelligent Agents General Terms Security, Design, Theory 1.",
                "INTRODUCTION In many multiagent domains, agents must act in order to provide security against attacks by adversaries.",
                "A common issue that agents face in such security domains is uncertainty about the adversaries they may be facing.",
                "For example, a security robot may need to make a choice about which areas to patrol, and how often [16].",
                "However, it will not know in advance exactly where a robber will choose to strike.",
                "A team of unmanned aerial vehicles (UAVs) [1] monitoring a region undergoing a humanitarian crisis may also need to choose a patrolling policy.",
                "They must make this decision without knowing in advance whether terrorists or other adversaries may be waiting to disrupt the mission at a given location.",
                "It may indeed be possible to model the motivations of types of adversaries the agent or agent team is likely to face in order to target these adversaries more closely.",
                "However, in both cases, the security robot or UAV team will not know exactly which kinds of adversaries may be active on any given day.",
                "A common approach for choosing a policy for agents in such scenarios is to model the scenarios as Bayesian games.",
                "A Bayesian game is a game in which agents may belong to one or more types; the type of an agent determines its possible actions and payoffs.",
                "The distribution of adversary types that an agent will face may be known or inferred from historical data.",
                "Usually, these games are analyzed according to the solution concept of a Bayes-Nash equilibrium, an extension of the Nash equilibrium for Bayesian games.",
                "However, in many settings, a Nash or Bayes-Nash equilibrium is not an appropriate solution concept, since it assumes that the agents strategies are chosen simultaneously [5].",
                "In some settings, one player can (or must) commit to a strategy before the other players choose their strategies.",
                "These scenarios are known as Stackelberg games [6].",
                "In a Stackelberg game, a leader commits to a strategy first, and then a follower (or group of followers) selfishly optimize their own rewards, considering the action chosen by the leader.",
                "For example, the security agent (leader) must first commit to a strategy for patrolling various areas.",
                "This strategy could be a mixed strategy in order to be unpredictable to the robbers (followers).",
                "The robbers, after observing the pattern of patrols over time, can then choose their strategy (which location to rob).",
                "Often, the leader in a Stackelberg game can attain a higher reward than if the strategies were chosen simultaneously.",
                "To see the advantage of being the leader in a Stackelberg game, consider a simple game with the payoff table as shown in Table 1.",
                "The leader is the row player and the follower is the column player.",
                "Here, the leaders payoff is listed first. 1 2 3 1 5,5 0,0 3,10 2 0,0 2,2 5,0 Table 1: Payoff table for example normal form game.",
                "The only Nash equilibrium for this game is when the leader plays 2 and the follower plays 2 which gives the leader a payoff of 2. 311 978-81-904262-7-5 (RPS) c 2007 IFAAMAS However, if the leader commits to a uniform mixed strategy of playing 1 and 2 with equal (0.5) probability, the followers best response is to play 3 to get an expected payoff of 5 (10 and 0 with equal probability).",
                "The leaders payoff would then be 4 (3 and 5 with equal probability).",
                "In this case, the leader now has an incentive to deviate and choose a pure strategy of 2 (to get a payoff of 5).",
                "However, this would cause the follower to deviate to strategy 2 as well, resulting in the Nash equilibrium.",
                "Thus, by committing to a strategy that is observed by the follower, and by avoiding the temptation to deviate, the leader manages to obtain a reward higher than that of the best Nash equilibrium.",
                "The problem of choosing an optimal strategy for the leader to commit to in a Stackelberg game is analyzed in [5] and found to be NP-hard in the case of a Bayesian game with multiple types of followers.",
                "Thus, efficient heuristic techniques for choosing highreward strategies in these games is an important open issue.",
                "Methods for finding optimal leader strategies for non-Bayesian games [5] can be applied to this problem by converting the Bayesian game into a normal-form game by the Harsanyi transformation [8].",
                "If, on the other hand, we wish to compute the highest-reward Nash equilibrium, new methods using mixed-integer linear programs (MILPs) [17] may be used, since the highest-reward Bayes-Nash equilibrium is equivalent to the corresponding Nash equilibrium in the transformed game.",
                "However, by transforming the game, the compact structure of the Bayesian game is lost.",
                "In addition, since the Nash equilibrium assumes a simultaneous choice of strategies, the advantages of being the leader are not considered.",
                "This paper introduces an efficient heuristic method for approximating the optimal leader strategy for security domains, known as ASAP (Agent Security via Approximate Policies).",
                "This method has three key advantages.",
                "First, it directly searches for an optimal strategy, rather than a Nash (or Bayes-Nash) equilibrium, thus allowing it to find high-reward non-equilibrium strategies like the one in the above example.",
                "Second, it generates policies with a support which can be expressed as a uniform distribution over a multiset of fixed size as proposed in [12].",
                "This allows for policies that are simple to understand and represent [12], as well as a tunable parameter (the size of the multiset) that controls the simplicity of the policy.",
                "Third, the method allows for a Bayes-Nash game to be expressed compactly without conversion to a normal-form game, allowing for large speedups over existing Nash methods such as [17] and [11].",
                "The rest of the paper is organized as follows.",
                "In Section 2 we fully describe the patrolling domain and its properties.",
                "Section 3 introduces the Bayesian game, the Harsanyi transformation, and existing methods for finding an optimal leaders strategy in a Stackelberg game.",
                "Then, in Section 4 the ASAP algorithm is presented for normal-form games, and in Section 5 we show how it can be adapted to the structure of Bayesian games with uncertain adversaries.",
                "Experimental results showing higher reward and faster policy computation over existing Nash methods are shown in Section 6, and we conclude with a discussion of related work in Section 7. 2.",
                "THE PATROLLING DOMAIN In most security patrolling domains, the security agents (like UAVs [1] or security robots [16]) cannot feasibly patrol all areas all the time.",
                "Instead, they must choose a policy by which they patrol various routes at different times, taking into account factors such as the likelihood of crime in different areas, possible targets for crime, and the security agents own resources (number of security agents, amount of available time, fuel, etc.).",
                "It is usually beneficial for this policy to be nondeterministic so that robbers cannot safely rob certain locations, knowing that they will be safe from the security agents [14].",
                "To demonstrate the utility of our algorithm, we use a simplified version of such a domain, expressed as a game.",
                "The most basic version of our game consists of two players: the security agent (the leader) and the robber (the follower) in a world consisting of m houses, 1 . . . m. The security agents set of pure strategies consists of possible routes of d houses to patrol (in an order).",
                "The security agent can choose a mixed strategy so that the robber will be unsure of exactly where the security agent may patrol, but the robber will know the mixed strategy the security agent has chosen.",
                "For example, the robber can observe over time how often the security agent patrols each area.",
                "With this knowledge, the robber must choose a single house to rob.",
                "We assume that the robber generally takes a long time to rob a house.",
                "If the house chosen by the robber is not on the security agents route, then the robber successfully robs it.",
                "Otherwise, if it is on the security agents route, then the earlier the house is on the route, the easier it is for the security agent to catch the robber before he finishes robbing it.",
                "We model the payoffs for this game with the following variables: • vl,x: value of the goods in house l to the security agent. • vl,q: value of the goods in house l to the robber. • cx: reward to the security agent of catching the robber. • cq: cost to the robber of getting caught. • pl: probability that the security agent can catch the robber at the lth house in the patrol (pl < pl ⇐⇒ l < l).",
                "The security agents set of possible pure strategies (patrol routes) is denoted by X and includes all d-tuples i =< w1, w2, ..., wd > with w1 . . . wd = 1 . . . m where no two elements are equal (the agent is not allowed to return to the same house).",
                "The robbers set of possible pure strategies (houses to rob) is denoted by Q and includes all integers j = 1 . . . m. The payoffs (security agent, robber) for pure strategies i, j are: • −vl,x, vl,q, for j = l /∈ i. • plcx +(1−pl)(−vl,x), −plcq +(1−pl)(vl,q), for j = l ∈ i.",
                "With this structure it is possible to model many different types of robbers who have differing motivations; for example, one robber may have a lower cost of getting caught than another, or may value the goods in the various houses differently.",
                "If the distribution of different robber types is known or inferred from historical data, then the game can be modeled as a Bayesian game [6]. 3.",
                "BAYESIAN GAMES A Bayesian game contains a set of N agents, and each agent n must be one of a given set of types θn.",
                "For our patrolling domain, we have two agents, the security agent and the robber. θ1 is the set of security agent types and θ2 is the set of robber types.",
                "Since there is only one type of security agent, θ1 contains only one element.",
                "During the game, the robber knows its type but the security agent does not know the robbers type.",
                "For each agent (the security agent or the robber) n, there is a set of strategies σn and a utility function un : θ1 × θ2 × σ1 × σ2 → .",
                "A Bayesian game can be transformed into a normal-form game using the Harsanyi transformation [8].",
                "Once this is done, new, linear-program (LP)-based methods for finding high-reward strategies for normal-form games [5] can be used to find a strategy in the transformed game; this strategy can then be used for the Bayesian game.",
                "While methods exist for finding Bayes-Nash equilibria directly, without the Harsanyi transformation [10], they find only a 312 The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) single equilibrium in the general case, which may not be of high reward.",
                "Recent work [17] has led to efficient mixed-integer linear program techniques to find the best Nash equilibrium for a given agent.",
                "However, these techniques do require a normal-form game, and so to compare the policies given by ASAP against the optimal policy, as well as against the highest-reward Nash equilibrium, we must apply these techniques to the Harsanyi-transformed matrix.",
                "The next two subsections elaborate on how this is done. 3.1 Harsanyi Transformation The first step in solving Bayesian games is to apply the Harsanyi transformation [8] that converts the Bayesian game into a normal form game.",
                "Given that the Harsanyi transformation is a standard concept in game theory, we explain it briefly through a simple example in our patrolling domain without introducing the mathematical formulations.",
                "Let us assume there are two robber types a and b in the Bayesian game.",
                "Robber a will be active with probability α, and robber b will be active with probability 1 − α.",
                "The rules described in Section 2 allow us to construct simple payoff tables.",
                "Assume that there are two houses in the world (1 and 2) and hence there are two patrol routes (pure strategies) for the agent: {1,2} and {2,1}.",
                "The robber can rob either house 1 or house 2 and hence he has two strategies (denoted as 1l, 2l for robber type l).",
                "Since there are two types assumed (denoted as a and b), we construct two payoff tables (shown in Table 2) corresponding to the security agent playing a separate game with each of the two robber types with probabilities α and 1 − α.",
                "First, consider robber type a.",
                "Borrowing the notation from the domain section, we assign the following values to the variables: v1,x = v1,q = 3/4, v2,x = v2,q = 1/4, cx = 1/2, cq = 1, p1 = 1, p2 = 1/2.",
                "Using these values we construct a base payoff table as the payoff for the game against robber type a.",
                "For example, if the security agent chooses route {1,2} when robber a is active, and robber a chooses house 1, the robber receives a reward of -1 (for being caught) and the agent receives a reward of 0.5 for catching the robber.",
                "The payoffs for the game against robber type b are constructed using different values.",
                "Security agent: {1,2} {2,1} Robber a 1a -1, .5 -.375, .125 2a -.125, -.125 -1, .5 Robber b 1b -.9, .6 -.275, .225 2b -.025, -.025 -.9, .6 Table 2: Payoff tables: Security Agent vs Robbers a and b Using the Harsanyi technique involves introducing a chance node, that determines the robbers type, thus transforming the security agents incomplete information regarding the robber into imperfect information [3].",
                "The Bayesian equilibrium of the game is then precisely the Nash equilibrium of the imperfect information game.",
                "The transformed, normal-form game is shown in Table 3.",
                "In the transformed game, the security agent is the column player, and the set of all robber types together is the row player.",
                "Suppose that robber type a robs house 1 and robber type b robs house 2, while the security agent chooses patrol {1,2}.",
                "Then, the security agent and the robber receive an expected payoff corresponding to their payoffs from the agent encountering robber a at house 1 with probability α and robber b at house 2 with probability 1 − α. 3.2 Finding an Optimal Strategy Although a Nash equilibrium is the standard solution concept for games in which agents choose strategies simultaneously, in our security domain, the security agent (the leader) can gain an advantage by committing to a mixed strategy in advance.",
                "Since the followers (the robbers) will know the leaders strategy, the optimal response for the followers will be a pure strategy.",
                "Given the common assumption, taken in [5], in the case where followers are indifferent, they will choose the strategy that benefits the leader, there must exist a guaranteed optimal strategy for the leader [5].",
                "From the Bayesian game in Table 2, we constructed the Harsanyi transformed bimatrix in Table 3.",
                "The strategies for each player (security agent or robber) in the transformed game correspond to all combinations of possible strategies taken by each of that players types.",
                "Therefore, we denote X = σθ1 1 = σ1 and Q = σθ2 2 as the index sets of the security agent and robbers pure strategies respectively, with R and C as the corresponding payoff matrices.",
                "Rij is the reward of the security agent and Cij is the reward of the robbers when the security agent takes pure strategy i and the robbers take pure strategy j.",
                "A mixed strategy for the security agent is a probability distribution over its set of pure strategies and will be represented by a vector x = (px1, px2, . . . , px|X|), where pxi ≥ 0 and P pxi = 1.",
                "Here, pxi is the probability that the security agent will choose its ith pure strategy.",
                "The optimal mixed strategy for the security agent can be found in time polynomial in the number of rows in the normal form game using the following linear program formulation from [5].",
                "For every possible pure strategy j by the follower (the set of all robber types), max P i∈X pxiRij s.t. ∀j ∈ Q, P i∈σ1 pxiCij ≥ P i∈σ1 pxiCij P i∈X pxi = 1 ∀i∈X , pxi >= 0 (1) Then, for all feasible follower strategies j, choose the one that maximizes P i∈X pxiRij, the reward for the security agent (leader).",
                "The pxi variables give the optimal strategy for the security agent.",
                "Note that while this method is polynomial in the number of rows in the transformed, normal-form game, the number of rows increases exponentially with the number of robber types.",
                "Using this method for a Bayesian game thus requires running |σ2||θ2| separate linear programs.",
                "This is no surprise, since finding the leaders optimal strategy in a Bayesian Stackelberg game is NP-hard [5]. 4.",
                "HEURISTIC APPROACHES Given that finding the optimal strategy for the leader is NP-hard, we provide a heuristic approach.",
                "In this heuristic we limit the possible mixed strategies of the leader to select actions with probabilities that are integer multiples of 1/k for a predetermined integer k. Previous work [14] has shown that strategies with high entropy are beneficial for security applications when opponents utilities are completely unknown.",
                "In our domain, if utilities are not considered, this method will result in uniform-distribution strategies.",
                "One advantage of such strategies is that they are compact to represent (as fractions) and simple to understand; therefore they can be efficiently implemented by real organizations.",
                "We aim to maintain the advantage provided by simple strategies for our security application problem, incorporating the effect of the robbers rewards on the security agents rewards.",
                "Thus, the ASAP heuristic will produce strategies which are k-uniform.",
                "A mixed strategy is denoted k-uniform if it is a uniform distribution on a multiset S of pure strategies with |S| = k. A multiset is a set whose elements may be repeated multiple times; thus, for example, the mixed strategy corresponding to the multiset {1, 1, 2} would take strategy 1 The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) 313 {1,2} {2,1} {1a, 1b} −1α − .9(1 − α), .5α + .6(1 − α) −.375α − .275(1 − α), .125α + .225(1 − α) {1a, 2b} −1α − .025(1 − α), .5α − .025(1 − α) −.375α − .9(1 − α), .125α + .6(1 − α) {2a, 1b} −.125α − .9(1 − α), −.125α + .6(1 − α) −1α − .275(1 − α), .5α + .225(1 − α) {2a, 2b} −.125α − .025(1 − α), −.125α − .025(1 − α) −1α − .9(1 − α), .5α + .6(1 − α) Table 3: Harsanyi Transformed Payoff Table with probability 2/3 and strategy 2 with probability 1/3.",
                "ASAP allows the size of the multiset to be chosen in order to balance the complexity of the strategy reached with the goal that the identified strategy will yield a high reward.",
                "Another advantage of the ASAP heuristic is that it operates directly on the compact Bayesian representation, without requiring the Harsanyi transformation.",
                "This is because the different follower (robber) types are independent of each other.",
                "Hence, evaluating the leader strategy against a Harsanyi-transformed game matrix is equivalent to evaluating against each of the game matrices for the individual follower types.",
                "This independence property is exploited in ASAP to yield a decomposition scheme.",
                "Note that the LP method introduced by [5] to compute optimal Stackelberg policies is unlikely to be decomposable into a small number of games as it was shown to be NP-hard for Bayes-Nash problems.",
                "Finally, note that ASAP requires the solution of only one optimization problem, rather than solving a series of problems as in the LP method of [5].",
                "For a single follower type, the algorithm works the following way.",
                "Given a particular k, for each possible mixed strategy x for the leader that corresponds to a multiset of size k, evaluate the leaders payoff from x when the follower plays a reward-maximizing pure strategy.",
                "We then take the mixed strategy with the highest payoff.",
                "We need only to consider the reward-maximizing pure strategies of the followers (robbers), since for a given fixed strategy x of the security agent, each robber type faces a problem with fixed linear rewards.",
                "If a mixed strategy is optimal for the robber, then so are all the pure strategies in the support of that mixed strategy.",
                "Note also that because we limit the leaders strategies to take on discrete values, the assumption from Section 3.2 that the followers will break ties in the leaders favor is not significant, since ties will be unlikely to arise.",
                "This is because, in domains where rewards are drawn from any random distribution, the probability of a follower having more than one pure optimal response to a given leader strategy approaches zero, and the leader will have only a finite number of possible mixed strategies.",
                "Our approach to characterize the optimal strategy for the security agent makes use of properties of linear programming.",
                "We briefly outline these results here for completeness, for detailed discussion and proofs see one of many references on the topic, such as [2].",
                "Every linear programming problem, such as: max cT x | Ax = b, x ≥ 0, has an associated dual linear program, in this case: min bT y | AT y ≥ c. These primal/dual pairs of problems satisfy weak duality: For any x and y primal and dual feasible solutions respectively, cT x ≤ bT y.",
                "Thus a pair of feasible solutions is optimal if cT x = bT y, and the problems are said to satisfy strong duality.",
                "In fact if a linear program is feasible and has a bounded optimal solution, then the dual is also feasible and there is a pair x∗ , y∗ that satisfies cT x∗ = bT y∗ .",
                "These optimal solutions are characterized with the following optimality conditions (as defined in [2]): • primal feasibility: Ax = b, x ≥ 0 • dual feasibility: AT y ≥ c • complementary slackness: xi(AT y − c)i = 0 for all i.",
                "Note that this last condition implies that cT x = xT AT y = bT y, which proves optimality for primal dual feasible solutions x and y.",
                "In the following subsections, we first define the problem in its most intuititive form as a mixed-integer quadratic program (MIQP), and then show how this problem can be converted into a mixedinteger linear program (MILP). 4.1 Mixed-Integer Quadratic Program We begin with the case of a single type of follower.",
                "Let the leader be the row player and the follower the column player.",
                "We denote by x the vector of strategies of the leader and q the vector of strategies of the follower.",
                "We also denote X and Q the index sets of the leader and followers pure strategies, respectively.",
                "The payoff matrices R and C correspond to: Rij is the reward of the leader and Cij is the reward of the follower when the leader takes pure strategy i and the follower takes pure strategy j.",
                "Let k be the size of the multiset.",
                "We first fix the policy of the leader to some k-uniform policy x.",
                "The value xi is the number of times pure strategy i is used in the k-uniform policy, which is selected with probability xi/k.",
                "We formulate the optimization problem the follower solves to find its optimal response to x as the following linear program: max X j∈Q X i∈X 1 k Cijxi qj s.t.",
                "P j∈Q qj = 1 q ≥ 0. (2) The objective function maximizes the followers expected reward given x, while the constraints make feasible any mixed strategy q for the follower.",
                "The dual to this linear programming problem is the following: min a s.t. a ≥ X i∈X 1 k Cijxi j ∈ Q. (3) From strong duality and complementary slackness we obtain that the followers maximum reward value, a, is the value of every pure strategy with qj > 0, that is in the support of the optimal mixed strategy.",
                "Therefore each of these pure strategies is optimal.",
                "Optimal solutions to the followers problem are characterized by linear programming optimality conditions: primal feasibility constraints in (2), dual feasibility constraints in (3), and complementary slackness qj a − X i∈X 1 k Cijxi ! = 0 j ∈ Q.",
                "These conditions must be included in the problem solved by the leader in order to consider only best responses by the follower to the k-uniform policy x. 314 The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) The leader seeks the k-uniform solution x that maximizes its own payoff, given that the follower uses an optimal response q(x).",
                "Therefore the leader solves the following integer problem: max X i∈X X j∈Q 1 k Rijq(x)j xi s.t.",
                "P i∈X xi = k xi ∈ {0, 1, . . . , k}. (4) Problem (4) maximizes the leaders reward with the followers best response (qj for fixed leaders policy x and hence denoted q(x)j) by selecting a uniform policy from a multiset of constant size k. We complete this problem by including the characterization of q(x) through linear programming optimality conditions.",
                "To simplify writing the complementary slackness conditions, we will constrain q(x) to be only optimal pure strategies by just considering integer solutions of q(x).",
                "The leaders problem becomes: maxx,q X i∈X X j∈Q 1 k Rijxiqj s.t.",
                "P i xi = kP j∈Q qj = 1 0 ≤ (a − P i∈X 1 k Cijxi) ≤ (1 − qj)M xi ∈ {0, 1, ...., k} qj ∈ {0, 1}. (5) Here, the constant M is some large number.",
                "The first and fourth constraints enforce a k-uniform policy for the leader, and the second and fifth constraints enforce a feasible pure strategy for the follower.",
                "The third constraint enforces dual feasibility of the followers problem (leftmost inequality) and the complementary slackness constraint for an optimal pure strategy q for the follower (rightmost inequality).",
                "In fact, since only one pure strategy can be selected by the follower, say qh = 1, this last constraint enforces that a = P i∈X 1 k Cihxi imposing no additional constraint for all other pure strategies which have qj = 0.",
                "We conclude this subsection noting that Problem (5) is an integer program with a non-convex quadratic objective in general, as the matrix R need not be positive-semi-definite.",
                "Efficient solution methods for non-linear, non-convex integer problems remains a challenging research question.",
                "In the next section we show a reformulation of this problem as a linear integer programming problem, for which a number of efficient commercial solvers exist. 4.2 Mixed-Integer Linear Program We can linearize the quadratic program of Problem 5 through the change of variables zij = xiqj, obtaining the following problem maxq,z P i∈X P j∈Q 1 k Rijzij s.t.",
                "P i∈X P j∈Q zij = k P j∈Q zij ≤ k kqj ≤ P i∈X zij ≤ k P j∈Q qj = 1 0 ≤ (a − P i∈X 1 k Cij( P h∈Q zih)) ≤ (1 − qj)M zij ∈ {0, 1, ...., k} qj ∈ {0, 1} (6) PROPOSITION 1.",
                "Problems (5) and (6) are equivalent.",
                "Proof: Consider x, q a feasible solution of (5).",
                "We will show that q, zij = xiqj is a feasible solution of (6) of same objective function value.",
                "The equivalence of the objective functions, and constraints 4, 6 and 7 of (6) are satisfied by construction.",
                "The fact that P j∈Q zij = xi as P j∈Q qj = 1 explains constraints 1, 2, and 5 of (6).",
                "Constraint 3 of (6) is satisfied because P i∈X zij = kqj.",
                "Let us now consider q, z feasible for (6).",
                "We will show that q and xi = P j∈Q zij are feasible for (5) with the same objective value.",
                "In fact all constraints of (5) are readily satisfied by construction.",
                "To see that the objectives match, notice that if qh = 1 then the third constraint in (6) implies that P i∈X zih = k, which means that zij = 0 for all i ∈ X and all j = h. Therefore, xiqj = X l∈Q zilqj = zihqj = zij.",
                "This last equality is because both are 0 when j = h. This shows that the transformation preserves the objective function value, completing the proof.",
                "Given this transformation to a mixed-integer linear program (MILP), we now show how we can apply our decomposition technique on the MILP to obtain significant speedups for Bayesian games with multiple follower types. 5.",
                "DECOMPOSITION FOR MULTIPLE ADVERSARIES The MILP developed in the previous section handles only one follower.",
                "Since our security scenario contains multiple follower (robber) types, we change the response function for the follower from a pure strategy into a weighted combination over various pure follower strategies where the weights are probabilities of occurrence of each of the follower types. 5.1 Decomposed MIQP To admit multiple adversaries in our framework, we modify the notation defined in the previous section to reason about multiple follower types.",
                "We denote by x the vector of strategies of the leader and ql the vector of strategies of follower l, with L denoting the index set of follower types.",
                "We also denote by X and Q the index sets of leader and follower ls pure strategies, respectively.",
                "We also index the payoff matrices on each follower l, considering the matrices Rl and Cl .",
                "Using this modified notation, we characterize the optimal solution of follower ls problem given the leaders k-uniform policy x, with the following optimality conditions: X j∈Q ql j = 1 al − X i∈X 1 k Cl ijxi ≥ 0 ql j(al − X i∈X 1 k Cl ijxi) = 0 ql j ≥ 0 Again, considering only optimal pure strategies for follower ls problem we can linearize the complementarity constraint above.",
                "We incorporate these constraints on the leaders problem that selects the optimal k-uniform policy.",
                "Therefore, given a priori probabilities pl , with l ∈ L of facing each follower, the leader solves the following problem: The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) 315 maxx,q X i∈X X l∈L X j∈Q pl k Rl ijxiql j s.t.",
                "P i xi = kP j∈Q ql j = 1 0 ≤ (al − P i∈X 1 k Cl ijxi) ≤ (1 − ql j)M xi ∈ {0, 1, ...., k} ql j ∈ {0, 1}. (7) Problem (7) for a Bayesian game with multiple follower types is indeed equivalent to Problem (5) on the payoff matrix obtained from the Harsanyi transformation of the game.",
                "In fact, every pure strategy j in Problem (5) corresponds to a sequence of pure strategies jl, one for each follower l ∈ L. This means that qj = 1 if and only if ql jl = 1 for all l ∈ L. In addition, given the a priori probabilities pl of facing player l, the reward in the Harsanyi transformation payoff table is Rij = P l∈L pl Rl ijl .",
                "The same relation holds between C and Cl .",
                "These relations between a pure strategy in the equivalent normal form game and pure strategies in the individual games with each followers are key in showing these problems are equivalent. 5.2 Decomposed MILP We can linearize the quadratic programming problem 7 through the change of variables zl ij = xiql j, obtaining the following problem maxq,z P i∈X P l∈L P j∈Q pl k Rl ijzl ij s.t.",
                "P i∈X P j∈Q zl ij = k P j∈Q zl ij ≤ k kql j ≤ P i∈X zl ij ≤ k P j∈Q ql j = 1 0 ≤ (al − P i∈X 1 k Cl ij( P h∈Q zl ih)) ≤ (1 − ql j)M P j∈Q zl ij = P j∈Q z1 ij zl ij ∈ {0, 1, ...., k} ql j ∈ {0, 1} (8) PROPOSITION 2.",
                "Problems (7) and (8) are equivalent.",
                "Proof: Consider x, ql , al with l ∈ L a feasible solution of (7).",
                "We will show that ql , al , zl ij = xiql j is a feasible solution of (8) of same objective function value.",
                "The equivalence of the objective functions, and constraints 4, 7 and 8 of (8) are satisfied by construction.",
                "The fact that P j∈Q zl ij = xi as P j∈Q ql j = 1 explains constraints 1, 2, 5 and 6 of (8).",
                "Constraint 3 of (8) is satisfied because P i∈X zl ij = kql j.",
                "Lets now consider ql , zl , al feasible for (8).",
                "We will show that ql , al and xi = P j∈Q z1 ij are feasible for (7) with the same objective value.",
                "In fact all constraints of (7) are readily satisfied by construction.",
                "To see that the objectives match, notice for each l one ql j must equal 1 and the rest equal 0.",
                "Let us say that ql jl = 1, then the third constraint in (8) implies that P i∈X zl ijl = k, which means that zl ij = 0 for all i ∈ X and all j = jl.",
                "In particular this implies that xi = X j∈Q z1 ij = z1 ij1 = zl ijl , the last equality from constraint 6 of (8).",
                "Therefore xiql j = zl ijl ql j = zl ij.",
                "This last equality is because both are 0 when j = jl.",
                "Effectively, constraint 6 ensures that all the adversaries are calculating their best responses against a particular fixed policy of the agent.",
                "This shows that the transformation preserves the objective function value, completing the proof.",
                "We can therefore solve this equivalent linear integer program with efficient integer programming packages which can handle problems with thousands of integer variables.",
                "We implemented the decomposed MILP and the results are shown in the following section. 6.",
                "EXPERIMENTAL RESULTS The patrolling domain and the payoffs for the associated game are detailed in Sections 2 and 3.",
                "We performed experiments for this game in worlds of three and four houses with patrols consisting of two houses.",
                "The description given in Section 2 is used to generate a base case for both the security agent and robber payoff functions.",
                "The payoff tables for additional robber types are constructed and added to the game by adding a random distribution of varying size to the payoffs in the base case.",
                "All games are normalized so that, for each robber type, the minimum and maximum payoffs to the security agent and robber are 0 and 1, respectively.",
                "Using the data generated, we performed the experiments using four methods for generating the security agents strategy: • uniform randomization • ASAP • the multiple linear programs method from [5] (to find the true optimal strategy) • the highest reward Bayes-Nash equilibrium, found using the MIP-Nash algorithm [17] The last three methods were applied using CPLEX 8.1.",
                "Because the last two methods are designed for normal-form games rather than Bayesian games, the games were first converted using the Harsanyi transformation [8].",
                "The uniform randomization method is simply choosing a uniform random policy over all possible patrol routes.",
                "We use this method as a simple baseline to measure the performance of our heuristics.",
                "We anticipated that the uniform policy would perform reasonably well since maximum-entropy policies have been shown to be effective in multiagent security domains [14].",
                "The highest-reward Bayes-Nash equilibria were used in order to demonstrate the higher reward gained by looking for an optimal policy rather than an equilibria in Stackelberg games such as our security domain.",
                "Based on our experiments we present three sets of graphs to demonstrate (1) the runtime of ASAP compared to other common methods for finding a strategy, (2) the reward guaranteed by ASAP compared to other methods, and (3) the effect of varying the parameter k, the size of the multiset, on the performance of ASAP.",
                "In the first two sets of graphs, ASAP is run using a multiset of 80 elements; in the third set this number is varied.",
                "The first set of graphs, shown in Figure 1 shows the runtime graphs for three-house (left column) and four-house (right column) domains.",
                "Each of the three rows of graphs corresponds to a different randomly-generated scenario.",
                "The x-axis shows the number of robber types the security agent faces and the y-axis of the graph shows the runtime in seconds.",
                "All experiments that were not concluded in 30 minutes (1800 seconds) were cut off.",
                "The runtime for the uniform policy is always negligible irrespective of the number of adversaries and hence is not shown.",
                "The ASAP algorithm clearly outperforms the optimal, multipleLP method as well as the MIP-Nash algorithm for finding the highestreward Bayes-Nash equilibrium with respect to runtime.",
                "For a 316 The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) Figure 1: Runtimes for various algorithms on problems of 3 and 4 houses. domain of three houses, the optimal method cannot reach a solution for more than seven robber types, and for four houses it cannot solve for more than six types within the cutoff time in any of the three scenarios.",
                "MIP-Nash solves for even fewer robber types within the cutoff time.",
                "On the other hand, ASAP runs much faster, and is able to solve for at least 20 adversaries for the three-house scenarios and for at least 12 adversaries in the four-house scenarios within the cutoff time.",
                "The runtime of ASAP does not increase strictly with the number of robber types for each scenario, but in general, the addition of more types increases the runtime required.",
                "The second set of graphs, Figure 2, shows the reward to the patrol agent given by each method for three scenarios in the three-house (left column) and four-house (right column) domains.",
                "This reward is the utility received by the security agent in the patrolling game, and not as a percentage of the optimal reward, since it was not possible to obtain the optimal reward as the number of robber types increased.",
                "The uniform policy consistently provides the lowest reward in both domains; while the optimal method of course produces the optimal reward.",
                "The ASAP method remains consistently close to the optimal, even as the number of robber types increases.",
                "The highest-reward Bayes-Nash equilibria, provided by the MIPNash method, produced rewards higher than the uniform method, but lower than ASAP.",
                "This difference clearly illustrates the gains in the patrolling domain from committing to a strategy as the leader in a Stackelberg game, rather than playing a standard Bayes-Nash strategy.",
                "The third set of graphs, shown in Figure 3 shows the effect of the multiset size on runtime in seconds (left column) and reward (right column), again expressed as the reward received by the security agent in the patrolling game, and not a percentage of the optimal Figure 2: Reward for various algorithms on problems of 3 and 4 houses. reward.",
                "Results here are for the three-house domain.",
                "The trend is that as as the multiset size is increased, the runtime and reward level both increase.",
                "Not surprisingly, the reward increases monotonically as the multiset size increases, but what is interesting is that there is relatively little benefit to using a large multiset in this domain.",
                "In all cases, the reward given by a multiset of 10 elements was within at least 96% of the reward given by an 80-element multiset.",
                "The runtime does not always increase strictly with the multiset size; indeed in one example (scenario 2 with 20 robber types), using a multiset of 10 elements took 1228 seconds, while using 80 elements only took 617 seconds.",
                "In general, runtime should increase since a larger multiset means a larger domain for the variables in the MILP, and thus a larger search space.",
                "However, an increase in the number of variables can sometimes allow for a policy to be constructed more quickly due to more flexibility in the problem. 7.",
                "SUMMARY AND RELATED WORK This paper focuses on security for agents patrolling in hostile environments.",
                "In these environments, intentional threats are caused by adversaries about whom the security patrolling agents have incomplete information.",
                "Specifically, we deal with situations where the adversaries actions and payoffs are known but the exact adversary type is unknown to the security agent.",
                "Agents acting in the real world quite frequently have such incomplete information about other agents.",
                "Bayesian games have been a popular choice to model such incomplete information games [3].",
                "The Gala toolkit is one method for defining such games [9] without requiring the game to be represented in normal form via the Harsanyi transformation [8]; Galas guarantees are focused on fully competitive games.",
                "Much work has been done on finding optimal Bayes-Nash equilbria for The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) 317 Figure 3: Reward for ASAP using multisets of 10, 30, and 80 elements subclasses of Bayesian games, finding single Bayes-Nash equilibria for general Bayesian games [10] or approximate Bayes-Nash equilibria [18].",
                "Less attention has been paid to finding the optimal strategy to commit to in a Bayesian game (the Stackelberg scenario [15]).",
                "However, the complexity of this problem was shown to be NP-hard in the general case [5], which also provides algorithms for this problem in the non-Bayesian case.",
                "Therefore, we present a heuristic called ASAP, with three key advantages towards addressing this problem.",
                "First, ASAP searches for the highest reward strategy, rather than a Bayes-Nash equilibrium, allowing it to find feasible strategies that exploit the natural first-mover advantage of the game.",
                "Second, it provides strategies which are simple to understand, represent, and implement.",
                "Third, it operates directly on the compact, Bayesian game representation, without requiring conversion to normal form.",
                "We provide an efficient Mixed Integer Linear Program (MILP) implementation for ASAP, along with experimental results illustrating significant speedups and higher rewards over other approaches.",
                "Our k-uniform strategies are similar to the k-uniform strategies of [12].",
                "While that work provides epsilon error-bounds based on the k-uniform strategies, their solution concept is still that of a Nash equilibrium, and they do not provide efficient algorithms for obtaining such k-uniform strategies.",
                "This contrasts with ASAP, where our emphasis is on a highly efficient heuristic approach that is not focused on equilibrium solutions.",
                "Finally the patrolling problem which motivated our work has recently received growing attention from the multiagent community due to its wide range of applications [4, 13].",
                "However most of this work is focused on either limiting energy consumption involved in patrolling [7] or optimizing on criteria like the length of the path traveled [4, 13], without reasoning about any explicit model of an adversary[14].",
                "Acknowledgments : This research is supported by the United States Department of Homeland Security through Center for Risk and Economic Analysis of Terrorism Events (CREATE).",
                "It is also supported by the Defense Advanced Research Projects Agency (DARPA), through the Department of the Interior, NBC, Acquisition Services Division, under Contract No.",
                "NBCHD030010.",
                "Sarit Kraus is also affiliated with UMIACS. 8.",
                "REFERENCES [1] R. W. Beard and T. McLain.",
                "Multiple UAV cooperative search under collision avoidance and limited range communication constraints.",
                "In IEEE CDC, 2003. [2] D. Bertsimas and J. Tsitsiklis.",
                "Introduction to Linear Optimization.",
                "Athena Scientific, 1997. [3] J. Brynielsson and S. Arnborg.",
                "Bayesian games for threat prediction and situation analysis.",
                "In FUSION, 2004. [4] Y. Chevaleyre.",
                "Theoretical analysis of multi-agent patrolling problem.",
                "In AAMAS, 2004. [5] V. Conitzer and T. Sandholm.",
                "Choosing the best strategy to commit to.",
                "In ACM Conference on Electronic Commerce, 2006. [6] D. Fudenberg and J. Tirole.",
                "Game Theory.",
                "MIT Press, 1991. [7] C. Gui and P. Mohapatra.",
                "Virtual patrol: A new power conservation design for surveillance using sensor networks.",
                "In IPSN, 2005. [8] J. C. Harsanyi and R. Selten.",
                "A generalized Nash solution for two-person bargaining games with incomplete information.",
                "Management Science, 18(5):80-106, 1972. [9] D. Koller and A. Pfeffer.",
                "Generating and solving imperfect information games.",
                "In IJCAI, pages 1185-1193, 1995. [10] D. Koller and A. Pfeffer.",
                "Representations and solutions for game-theoretic problems.",
                "Artificial Intelligence, 94(1):167-215, 1997. [11] C. Lemke and J. Howson.",
                "Equilibrium points of bimatrix games.",
                "Journal of the Society for Industrial and Applied Mathematics, 12:413-423, 1964. [12] R. J. Lipton, E. Markakis, and A. Mehta.",
                "Playing large games using simple strategies.",
                "In ACM Conference on Electronic Commerce, 2003. [13] A. Machado, G. Ramalho, J. D. Zucker, and A. Drougoul.",
                "Multi-agent patrolling: an empirical analysis on alternative architectures.",
                "In MABS, 2002. [14] P. Paruchuri, M. Tambe, F. Ordonez, and S. Kraus.",
                "Security in multiagent systems by policy randomization.",
                "In AAMAS, 2006. [15] T. Roughgarden.",
                "Stackelberg scheduling strategies.",
                "In ACM Symposium on TOC, 2001. [16] S. Ruan, C. Meirina, F. Yu, K. R. Pattipati, and R. L. Popp.",
                "Patrolling in a stochastic environment.",
                "In 10th Intl.",
                "Command and Control Research Symp., 2005. [17] T. Sandholm, A. Gilpin, and V. Conitzer.",
                "Mixed-integer programming methods for finding nash equilibria.",
                "In AAAI, 2005. [18] S. Singh, V. Soni, and M. Wellman.",
                "Computing approximate Bayes-Nash equilibria with tree-games of incomplete information.",
                "In ACM Conference on Electronic Commerce, 2004. 318 The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07)"
            ],
            "original_annotated_samples": [],
            "translated_annotated_samples": [],
            "translated_text": "",
            "candidates": [],
            "error": []
        },
        "bayesian game": {
            "translated_key": "juego bayesiano",
            "is_in_text": true,
            "original_annotated_sentences": [
                "An Efficient Heuristic Approach for Security Against Multiple Adversaries Praveen Paruchuri, Jonathan P. Pearce, Milind Tambe, Fernando Ordonez University of Southern California Los Angeles, CA 90089 {paruchur, jppearce, tambe, fordon}@usc.edu Sarit Kraus Bar-Ilan University Ramat-Gan 52900, Israel sarit@cs.biu.ac.il ABSTRACT In adversarial multiagent domains, security, commonly defined as the ability to deal with intentional threats from other agents, is a critical issue.",
                "This paper focuses on domains where these threats come from unknown adversaries.",
                "These domains can be modeled as Bayesian games; much work has been done on finding equilibria for such games.",
                "However, it is often the case in multiagent security domains that one agent can commit to a mixed strategy which its adversaries observe before choosing their own strategies.",
                "In this case, the agent can maximize reward by finding an optimal strategy, without requiring equilibrium.",
                "Previous work has shown this problem of optimal strategy selection to be NP-hard.",
                "Therefore, we present a heuristic called ASAP, with three key advantages to address the problem.",
                "First, ASAP searches for the highest-reward strategy, rather than a Bayes-Nash equilibrium, allowing it to find feasible strategies that exploit the natural first-mover advantage of the game.",
                "Second, it provides strategies which are simple to understand, represent, and implement.",
                "Third, it operates directly on the compact, <br>bayesian game</br> representation, without requiring conversion to normal form.",
                "We provide an efficient Mixed Integer Linear Program (MILP) implementation for ASAP, along with experimental results illustrating significant speedups and higher rewards over other approaches.",
                "Categories and Subject Descriptors I.2.11 [Computing Methodologies]: Artificial Intelligence: Distributed Artificial Intelligence - Intelligent Agents General Terms Security, Design, Theory 1.",
                "INTRODUCTION In many multiagent domains, agents must act in order to provide security against attacks by adversaries.",
                "A common issue that agents face in such security domains is uncertainty about the adversaries they may be facing.",
                "For example, a security robot may need to make a choice about which areas to patrol, and how often [16].",
                "However, it will not know in advance exactly where a robber will choose to strike.",
                "A team of unmanned aerial vehicles (UAVs) [1] monitoring a region undergoing a humanitarian crisis may also need to choose a patrolling policy.",
                "They must make this decision without knowing in advance whether terrorists or other adversaries may be waiting to disrupt the mission at a given location.",
                "It may indeed be possible to model the motivations of types of adversaries the agent or agent team is likely to face in order to target these adversaries more closely.",
                "However, in both cases, the security robot or UAV team will not know exactly which kinds of adversaries may be active on any given day.",
                "A common approach for choosing a policy for agents in such scenarios is to model the scenarios as Bayesian games.",
                "A <br>bayesian game</br> is a game in which agents may belong to one or more types; the type of an agent determines its possible actions and payoffs.",
                "The distribution of adversary types that an agent will face may be known or inferred from historical data.",
                "Usually, these games are analyzed according to the solution concept of a Bayes-Nash equilibrium, an extension of the Nash equilibrium for Bayesian games.",
                "However, in many settings, a Nash or Bayes-Nash equilibrium is not an appropriate solution concept, since it assumes that the agents strategies are chosen simultaneously [5].",
                "In some settings, one player can (or must) commit to a strategy before the other players choose their strategies.",
                "These scenarios are known as Stackelberg games [6].",
                "In a Stackelberg game, a leader commits to a strategy first, and then a follower (or group of followers) selfishly optimize their own rewards, considering the action chosen by the leader.",
                "For example, the security agent (leader) must first commit to a strategy for patrolling various areas.",
                "This strategy could be a mixed strategy in order to be unpredictable to the robbers (followers).",
                "The robbers, after observing the pattern of patrols over time, can then choose their strategy (which location to rob).",
                "Often, the leader in a Stackelberg game can attain a higher reward than if the strategies were chosen simultaneously.",
                "To see the advantage of being the leader in a Stackelberg game, consider a simple game with the payoff table as shown in Table 1.",
                "The leader is the row player and the follower is the column player.",
                "Here, the leaders payoff is listed first. 1 2 3 1 5,5 0,0 3,10 2 0,0 2,2 5,0 Table 1: Payoff table for example normal form game.",
                "The only Nash equilibrium for this game is when the leader plays 2 and the follower plays 2 which gives the leader a payoff of 2. 311 978-81-904262-7-5 (RPS) c 2007 IFAAMAS However, if the leader commits to a uniform mixed strategy of playing 1 and 2 with equal (0.5) probability, the followers best response is to play 3 to get an expected payoff of 5 (10 and 0 with equal probability).",
                "The leaders payoff would then be 4 (3 and 5 with equal probability).",
                "In this case, the leader now has an incentive to deviate and choose a pure strategy of 2 (to get a payoff of 5).",
                "However, this would cause the follower to deviate to strategy 2 as well, resulting in the Nash equilibrium.",
                "Thus, by committing to a strategy that is observed by the follower, and by avoiding the temptation to deviate, the leader manages to obtain a reward higher than that of the best Nash equilibrium.",
                "The problem of choosing an optimal strategy for the leader to commit to in a Stackelberg game is analyzed in [5] and found to be NP-hard in the case of a <br>bayesian game</br> with multiple types of followers.",
                "Thus, efficient heuristic techniques for choosing highreward strategies in these games is an important open issue.",
                "Methods for finding optimal leader strategies for non-Bayesian games [5] can be applied to this problem by converting the <br>bayesian game</br> into a normal-form game by the Harsanyi transformation [8].",
                "If, on the other hand, we wish to compute the highest-reward Nash equilibrium, new methods using mixed-integer linear programs (MILPs) [17] may be used, since the highest-reward Bayes-Nash equilibrium is equivalent to the corresponding Nash equilibrium in the transformed game.",
                "However, by transforming the game, the compact structure of the <br>bayesian game</br> is lost.",
                "In addition, since the Nash equilibrium assumes a simultaneous choice of strategies, the advantages of being the leader are not considered.",
                "This paper introduces an efficient heuristic method for approximating the optimal leader strategy for security domains, known as ASAP (Agent Security via Approximate Policies).",
                "This method has three key advantages.",
                "First, it directly searches for an optimal strategy, rather than a Nash (or Bayes-Nash) equilibrium, thus allowing it to find high-reward non-equilibrium strategies like the one in the above example.",
                "Second, it generates policies with a support which can be expressed as a uniform distribution over a multiset of fixed size as proposed in [12].",
                "This allows for policies that are simple to understand and represent [12], as well as a tunable parameter (the size of the multiset) that controls the simplicity of the policy.",
                "Third, the method allows for a Bayes-Nash game to be expressed compactly without conversion to a normal-form game, allowing for large speedups over existing Nash methods such as [17] and [11].",
                "The rest of the paper is organized as follows.",
                "In Section 2 we fully describe the patrolling domain and its properties.",
                "Section 3 introduces the <br>bayesian game</br>, the Harsanyi transformation, and existing methods for finding an optimal leaders strategy in a Stackelberg game.",
                "Then, in Section 4 the ASAP algorithm is presented for normal-form games, and in Section 5 we show how it can be adapted to the structure of Bayesian games with uncertain adversaries.",
                "Experimental results showing higher reward and faster policy computation over existing Nash methods are shown in Section 6, and we conclude with a discussion of related work in Section 7. 2.",
                "THE PATROLLING DOMAIN In most security patrolling domains, the security agents (like UAVs [1] or security robots [16]) cannot feasibly patrol all areas all the time.",
                "Instead, they must choose a policy by which they patrol various routes at different times, taking into account factors such as the likelihood of crime in different areas, possible targets for crime, and the security agents own resources (number of security agents, amount of available time, fuel, etc.).",
                "It is usually beneficial for this policy to be nondeterministic so that robbers cannot safely rob certain locations, knowing that they will be safe from the security agents [14].",
                "To demonstrate the utility of our algorithm, we use a simplified version of such a domain, expressed as a game.",
                "The most basic version of our game consists of two players: the security agent (the leader) and the robber (the follower) in a world consisting of m houses, 1 . . . m. The security agents set of pure strategies consists of possible routes of d houses to patrol (in an order).",
                "The security agent can choose a mixed strategy so that the robber will be unsure of exactly where the security agent may patrol, but the robber will know the mixed strategy the security agent has chosen.",
                "For example, the robber can observe over time how often the security agent patrols each area.",
                "With this knowledge, the robber must choose a single house to rob.",
                "We assume that the robber generally takes a long time to rob a house.",
                "If the house chosen by the robber is not on the security agents route, then the robber successfully robs it.",
                "Otherwise, if it is on the security agents route, then the earlier the house is on the route, the easier it is for the security agent to catch the robber before he finishes robbing it.",
                "We model the payoffs for this game with the following variables: • vl,x: value of the goods in house l to the security agent. • vl,q: value of the goods in house l to the robber. • cx: reward to the security agent of catching the robber. • cq: cost to the robber of getting caught. • pl: probability that the security agent can catch the robber at the lth house in the patrol (pl < pl ⇐⇒ l < l).",
                "The security agents set of possible pure strategies (patrol routes) is denoted by X and includes all d-tuples i =< w1, w2, ..., wd > with w1 . . . wd = 1 . . . m where no two elements are equal (the agent is not allowed to return to the same house).",
                "The robbers set of possible pure strategies (houses to rob) is denoted by Q and includes all integers j = 1 . . . m. The payoffs (security agent, robber) for pure strategies i, j are: • −vl,x, vl,q, for j = l /∈ i. • plcx +(1−pl)(−vl,x), −plcq +(1−pl)(vl,q), for j = l ∈ i.",
                "With this structure it is possible to model many different types of robbers who have differing motivations; for example, one robber may have a lower cost of getting caught than another, or may value the goods in the various houses differently.",
                "If the distribution of different robber types is known or inferred from historical data, then the game can be modeled as a <br>bayesian game</br> [6]. 3.",
                "BAYESIAN GAMES A <br>bayesian game</br> contains a set of N agents, and each agent n must be one of a given set of types θn.",
                "For our patrolling domain, we have two agents, the security agent and the robber. θ1 is the set of security agent types and θ2 is the set of robber types.",
                "Since there is only one type of security agent, θ1 contains only one element.",
                "During the game, the robber knows its type but the security agent does not know the robbers type.",
                "For each agent (the security agent or the robber) n, there is a set of strategies σn and a utility function un : θ1 × θ2 × σ1 × σ2 → .",
                "A <br>bayesian game</br> can be transformed into a normal-form game using the Harsanyi transformation [8].",
                "Once this is done, new, linear-program (LP)-based methods for finding high-reward strategies for normal-form games [5] can be used to find a strategy in the transformed game; this strategy can then be used for the <br>bayesian game</br>.",
                "While methods exist for finding Bayes-Nash equilibria directly, without the Harsanyi transformation [10], they find only a 312 The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) single equilibrium in the general case, which may not be of high reward.",
                "Recent work [17] has led to efficient mixed-integer linear program techniques to find the best Nash equilibrium for a given agent.",
                "However, these techniques do require a normal-form game, and so to compare the policies given by ASAP against the optimal policy, as well as against the highest-reward Nash equilibrium, we must apply these techniques to the Harsanyi-transformed matrix.",
                "The next two subsections elaborate on how this is done. 3.1 Harsanyi Transformation The first step in solving Bayesian games is to apply the Harsanyi transformation [8] that converts the <br>bayesian game</br> into a normal form game.",
                "Given that the Harsanyi transformation is a standard concept in game theory, we explain it briefly through a simple example in our patrolling domain without introducing the mathematical formulations.",
                "Let us assume there are two robber types a and b in the <br>bayesian game</br>.",
                "Robber a will be active with probability α, and robber b will be active with probability 1 − α.",
                "The rules described in Section 2 allow us to construct simple payoff tables.",
                "Assume that there are two houses in the world (1 and 2) and hence there are two patrol routes (pure strategies) for the agent: {1,2} and {2,1}.",
                "The robber can rob either house 1 or house 2 and hence he has two strategies (denoted as 1l, 2l for robber type l).",
                "Since there are two types assumed (denoted as a and b), we construct two payoff tables (shown in Table 2) corresponding to the security agent playing a separate game with each of the two robber types with probabilities α and 1 − α.",
                "First, consider robber type a.",
                "Borrowing the notation from the domain section, we assign the following values to the variables: v1,x = v1,q = 3/4, v2,x = v2,q = 1/4, cx = 1/2, cq = 1, p1 = 1, p2 = 1/2.",
                "Using these values we construct a base payoff table as the payoff for the game against robber type a.",
                "For example, if the security agent chooses route {1,2} when robber a is active, and robber a chooses house 1, the robber receives a reward of -1 (for being caught) and the agent receives a reward of 0.5 for catching the robber.",
                "The payoffs for the game against robber type b are constructed using different values.",
                "Security agent: {1,2} {2,1} Robber a 1a -1, .5 -.375, .125 2a -.125, -.125 -1, .5 Robber b 1b -.9, .6 -.275, .225 2b -.025, -.025 -.9, .6 Table 2: Payoff tables: Security Agent vs Robbers a and b Using the Harsanyi technique involves introducing a chance node, that determines the robbers type, thus transforming the security agents incomplete information regarding the robber into imperfect information [3].",
                "The Bayesian equilibrium of the game is then precisely the Nash equilibrium of the imperfect information game.",
                "The transformed, normal-form game is shown in Table 3.",
                "In the transformed game, the security agent is the column player, and the set of all robber types together is the row player.",
                "Suppose that robber type a robs house 1 and robber type b robs house 2, while the security agent chooses patrol {1,2}.",
                "Then, the security agent and the robber receive an expected payoff corresponding to their payoffs from the agent encountering robber a at house 1 with probability α and robber b at house 2 with probability 1 − α. 3.2 Finding an Optimal Strategy Although a Nash equilibrium is the standard solution concept for games in which agents choose strategies simultaneously, in our security domain, the security agent (the leader) can gain an advantage by committing to a mixed strategy in advance.",
                "Since the followers (the robbers) will know the leaders strategy, the optimal response for the followers will be a pure strategy.",
                "Given the common assumption, taken in [5], in the case where followers are indifferent, they will choose the strategy that benefits the leader, there must exist a guaranteed optimal strategy for the leader [5].",
                "From the <br>bayesian game</br> in Table 2, we constructed the Harsanyi transformed bimatrix in Table 3.",
                "The strategies for each player (security agent or robber) in the transformed game correspond to all combinations of possible strategies taken by each of that players types.",
                "Therefore, we denote X = σθ1 1 = σ1 and Q = σθ2 2 as the index sets of the security agent and robbers pure strategies respectively, with R and C as the corresponding payoff matrices.",
                "Rij is the reward of the security agent and Cij is the reward of the robbers when the security agent takes pure strategy i and the robbers take pure strategy j.",
                "A mixed strategy for the security agent is a probability distribution over its set of pure strategies and will be represented by a vector x = (px1, px2, . . . , px|X|), where pxi ≥ 0 and P pxi = 1.",
                "Here, pxi is the probability that the security agent will choose its ith pure strategy.",
                "The optimal mixed strategy for the security agent can be found in time polynomial in the number of rows in the normal form game using the following linear program formulation from [5].",
                "For every possible pure strategy j by the follower (the set of all robber types), max P i∈X pxiRij s.t. ∀j ∈ Q, P i∈σ1 pxiCij ≥ P i∈σ1 pxiCij P i∈X pxi = 1 ∀i∈X , pxi >= 0 (1) Then, for all feasible follower strategies j, choose the one that maximizes P i∈X pxiRij, the reward for the security agent (leader).",
                "The pxi variables give the optimal strategy for the security agent.",
                "Note that while this method is polynomial in the number of rows in the transformed, normal-form game, the number of rows increases exponentially with the number of robber types.",
                "Using this method for a <br>bayesian game</br> thus requires running |σ2||θ2| separate linear programs.",
                "This is no surprise, since finding the leaders optimal strategy in a Bayesian Stackelberg game is NP-hard [5]. 4.",
                "HEURISTIC APPROACHES Given that finding the optimal strategy for the leader is NP-hard, we provide a heuristic approach.",
                "In this heuristic we limit the possible mixed strategies of the leader to select actions with probabilities that are integer multiples of 1/k for a predetermined integer k. Previous work [14] has shown that strategies with high entropy are beneficial for security applications when opponents utilities are completely unknown.",
                "In our domain, if utilities are not considered, this method will result in uniform-distribution strategies.",
                "One advantage of such strategies is that they are compact to represent (as fractions) and simple to understand; therefore they can be efficiently implemented by real organizations.",
                "We aim to maintain the advantage provided by simple strategies for our security application problem, incorporating the effect of the robbers rewards on the security agents rewards.",
                "Thus, the ASAP heuristic will produce strategies which are k-uniform.",
                "A mixed strategy is denoted k-uniform if it is a uniform distribution on a multiset S of pure strategies with |S| = k. A multiset is a set whose elements may be repeated multiple times; thus, for example, the mixed strategy corresponding to the multiset {1, 1, 2} would take strategy 1 The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) 313 {1,2} {2,1} {1a, 1b} −1α − .9(1 − α), .5α + .6(1 − α) −.375α − .275(1 − α), .125α + .225(1 − α) {1a, 2b} −1α − .025(1 − α), .5α − .025(1 − α) −.375α − .9(1 − α), .125α + .6(1 − α) {2a, 1b} −.125α − .9(1 − α), −.125α + .6(1 − α) −1α − .275(1 − α), .5α + .225(1 − α) {2a, 2b} −.125α − .025(1 − α), −.125α − .025(1 − α) −1α − .9(1 − α), .5α + .6(1 − α) Table 3: Harsanyi Transformed Payoff Table with probability 2/3 and strategy 2 with probability 1/3.",
                "ASAP allows the size of the multiset to be chosen in order to balance the complexity of the strategy reached with the goal that the identified strategy will yield a high reward.",
                "Another advantage of the ASAP heuristic is that it operates directly on the compact Bayesian representation, without requiring the Harsanyi transformation.",
                "This is because the different follower (robber) types are independent of each other.",
                "Hence, evaluating the leader strategy against a Harsanyi-transformed game matrix is equivalent to evaluating against each of the game matrices for the individual follower types.",
                "This independence property is exploited in ASAP to yield a decomposition scheme.",
                "Note that the LP method introduced by [5] to compute optimal Stackelberg policies is unlikely to be decomposable into a small number of games as it was shown to be NP-hard for Bayes-Nash problems.",
                "Finally, note that ASAP requires the solution of only one optimization problem, rather than solving a series of problems as in the LP method of [5].",
                "For a single follower type, the algorithm works the following way.",
                "Given a particular k, for each possible mixed strategy x for the leader that corresponds to a multiset of size k, evaluate the leaders payoff from x when the follower plays a reward-maximizing pure strategy.",
                "We then take the mixed strategy with the highest payoff.",
                "We need only to consider the reward-maximizing pure strategies of the followers (robbers), since for a given fixed strategy x of the security agent, each robber type faces a problem with fixed linear rewards.",
                "If a mixed strategy is optimal for the robber, then so are all the pure strategies in the support of that mixed strategy.",
                "Note also that because we limit the leaders strategies to take on discrete values, the assumption from Section 3.2 that the followers will break ties in the leaders favor is not significant, since ties will be unlikely to arise.",
                "This is because, in domains where rewards are drawn from any random distribution, the probability of a follower having more than one pure optimal response to a given leader strategy approaches zero, and the leader will have only a finite number of possible mixed strategies.",
                "Our approach to characterize the optimal strategy for the security agent makes use of properties of linear programming.",
                "We briefly outline these results here for completeness, for detailed discussion and proofs see one of many references on the topic, such as [2].",
                "Every linear programming problem, such as: max cT x | Ax = b, x ≥ 0, has an associated dual linear program, in this case: min bT y | AT y ≥ c. These primal/dual pairs of problems satisfy weak duality: For any x and y primal and dual feasible solutions respectively, cT x ≤ bT y.",
                "Thus a pair of feasible solutions is optimal if cT x = bT y, and the problems are said to satisfy strong duality.",
                "In fact if a linear program is feasible and has a bounded optimal solution, then the dual is also feasible and there is a pair x∗ , y∗ that satisfies cT x∗ = bT y∗ .",
                "These optimal solutions are characterized with the following optimality conditions (as defined in [2]): • primal feasibility: Ax = b, x ≥ 0 • dual feasibility: AT y ≥ c • complementary slackness: xi(AT y − c)i = 0 for all i.",
                "Note that this last condition implies that cT x = xT AT y = bT y, which proves optimality for primal dual feasible solutions x and y.",
                "In the following subsections, we first define the problem in its most intuititive form as a mixed-integer quadratic program (MIQP), and then show how this problem can be converted into a mixedinteger linear program (MILP). 4.1 Mixed-Integer Quadratic Program We begin with the case of a single type of follower.",
                "Let the leader be the row player and the follower the column player.",
                "We denote by x the vector of strategies of the leader and q the vector of strategies of the follower.",
                "We also denote X and Q the index sets of the leader and followers pure strategies, respectively.",
                "The payoff matrices R and C correspond to: Rij is the reward of the leader and Cij is the reward of the follower when the leader takes pure strategy i and the follower takes pure strategy j.",
                "Let k be the size of the multiset.",
                "We first fix the policy of the leader to some k-uniform policy x.",
                "The value xi is the number of times pure strategy i is used in the k-uniform policy, which is selected with probability xi/k.",
                "We formulate the optimization problem the follower solves to find its optimal response to x as the following linear program: max X j∈Q X i∈X 1 k Cijxi qj s.t.",
                "P j∈Q qj = 1 q ≥ 0. (2) The objective function maximizes the followers expected reward given x, while the constraints make feasible any mixed strategy q for the follower.",
                "The dual to this linear programming problem is the following: min a s.t. a ≥ X i∈X 1 k Cijxi j ∈ Q. (3) From strong duality and complementary slackness we obtain that the followers maximum reward value, a, is the value of every pure strategy with qj > 0, that is in the support of the optimal mixed strategy.",
                "Therefore each of these pure strategies is optimal.",
                "Optimal solutions to the followers problem are characterized by linear programming optimality conditions: primal feasibility constraints in (2), dual feasibility constraints in (3), and complementary slackness qj a − X i∈X 1 k Cijxi ! = 0 j ∈ Q.",
                "These conditions must be included in the problem solved by the leader in order to consider only best responses by the follower to the k-uniform policy x. 314 The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) The leader seeks the k-uniform solution x that maximizes its own payoff, given that the follower uses an optimal response q(x).",
                "Therefore the leader solves the following integer problem: max X i∈X X j∈Q 1 k Rijq(x)j xi s.t.",
                "P i∈X xi = k xi ∈ {0, 1, . . . , k}. (4) Problem (4) maximizes the leaders reward with the followers best response (qj for fixed leaders policy x and hence denoted q(x)j) by selecting a uniform policy from a multiset of constant size k. We complete this problem by including the characterization of q(x) through linear programming optimality conditions.",
                "To simplify writing the complementary slackness conditions, we will constrain q(x) to be only optimal pure strategies by just considering integer solutions of q(x).",
                "The leaders problem becomes: maxx,q X i∈X X j∈Q 1 k Rijxiqj s.t.",
                "P i xi = kP j∈Q qj = 1 0 ≤ (a − P i∈X 1 k Cijxi) ≤ (1 − qj)M xi ∈ {0, 1, ...., k} qj ∈ {0, 1}. (5) Here, the constant M is some large number.",
                "The first and fourth constraints enforce a k-uniform policy for the leader, and the second and fifth constraints enforce a feasible pure strategy for the follower.",
                "The third constraint enforces dual feasibility of the followers problem (leftmost inequality) and the complementary slackness constraint for an optimal pure strategy q for the follower (rightmost inequality).",
                "In fact, since only one pure strategy can be selected by the follower, say qh = 1, this last constraint enforces that a = P i∈X 1 k Cihxi imposing no additional constraint for all other pure strategies which have qj = 0.",
                "We conclude this subsection noting that Problem (5) is an integer program with a non-convex quadratic objective in general, as the matrix R need not be positive-semi-definite.",
                "Efficient solution methods for non-linear, non-convex integer problems remains a challenging research question.",
                "In the next section we show a reformulation of this problem as a linear integer programming problem, for which a number of efficient commercial solvers exist. 4.2 Mixed-Integer Linear Program We can linearize the quadratic program of Problem 5 through the change of variables zij = xiqj, obtaining the following problem maxq,z P i∈X P j∈Q 1 k Rijzij s.t.",
                "P i∈X P j∈Q zij = k P j∈Q zij ≤ k kqj ≤ P i∈X zij ≤ k P j∈Q qj = 1 0 ≤ (a − P i∈X 1 k Cij( P h∈Q zih)) ≤ (1 − qj)M zij ∈ {0, 1, ...., k} qj ∈ {0, 1} (6) PROPOSITION 1.",
                "Problems (5) and (6) are equivalent.",
                "Proof: Consider x, q a feasible solution of (5).",
                "We will show that q, zij = xiqj is a feasible solution of (6) of same objective function value.",
                "The equivalence of the objective functions, and constraints 4, 6 and 7 of (6) are satisfied by construction.",
                "The fact that P j∈Q zij = xi as P j∈Q qj = 1 explains constraints 1, 2, and 5 of (6).",
                "Constraint 3 of (6) is satisfied because P i∈X zij = kqj.",
                "Let us now consider q, z feasible for (6).",
                "We will show that q and xi = P j∈Q zij are feasible for (5) with the same objective value.",
                "In fact all constraints of (5) are readily satisfied by construction.",
                "To see that the objectives match, notice that if qh = 1 then the third constraint in (6) implies that P i∈X zih = k, which means that zij = 0 for all i ∈ X and all j = h. Therefore, xiqj = X l∈Q zilqj = zihqj = zij.",
                "This last equality is because both are 0 when j = h. This shows that the transformation preserves the objective function value, completing the proof.",
                "Given this transformation to a mixed-integer linear program (MILP), we now show how we can apply our decomposition technique on the MILP to obtain significant speedups for Bayesian games with multiple follower types. 5.",
                "DECOMPOSITION FOR MULTIPLE ADVERSARIES The MILP developed in the previous section handles only one follower.",
                "Since our security scenario contains multiple follower (robber) types, we change the response function for the follower from a pure strategy into a weighted combination over various pure follower strategies where the weights are probabilities of occurrence of each of the follower types. 5.1 Decomposed MIQP To admit multiple adversaries in our framework, we modify the notation defined in the previous section to reason about multiple follower types.",
                "We denote by x the vector of strategies of the leader and ql the vector of strategies of follower l, with L denoting the index set of follower types.",
                "We also denote by X and Q the index sets of leader and follower ls pure strategies, respectively.",
                "We also index the payoff matrices on each follower l, considering the matrices Rl and Cl .",
                "Using this modified notation, we characterize the optimal solution of follower ls problem given the leaders k-uniform policy x, with the following optimality conditions: X j∈Q ql j = 1 al − X i∈X 1 k Cl ijxi ≥ 0 ql j(al − X i∈X 1 k Cl ijxi) = 0 ql j ≥ 0 Again, considering only optimal pure strategies for follower ls problem we can linearize the complementarity constraint above.",
                "We incorporate these constraints on the leaders problem that selects the optimal k-uniform policy.",
                "Therefore, given a priori probabilities pl , with l ∈ L of facing each follower, the leader solves the following problem: The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) 315 maxx,q X i∈X X l∈L X j∈Q pl k Rl ijxiql j s.t.",
                "P i xi = kP j∈Q ql j = 1 0 ≤ (al − P i∈X 1 k Cl ijxi) ≤ (1 − ql j)M xi ∈ {0, 1, ...., k} ql j ∈ {0, 1}. (7) Problem (7) for a <br>bayesian game</br> with multiple follower types is indeed equivalent to Problem (5) on the payoff matrix obtained from the Harsanyi transformation of the game.",
                "In fact, every pure strategy j in Problem (5) corresponds to a sequence of pure strategies jl, one for each follower l ∈ L. This means that qj = 1 if and only if ql jl = 1 for all l ∈ L. In addition, given the a priori probabilities pl of facing player l, the reward in the Harsanyi transformation payoff table is Rij = P l∈L pl Rl ijl .",
                "The same relation holds between C and Cl .",
                "These relations between a pure strategy in the equivalent normal form game and pure strategies in the individual games with each followers are key in showing these problems are equivalent. 5.2 Decomposed MILP We can linearize the quadratic programming problem 7 through the change of variables zl ij = xiql j, obtaining the following problem maxq,z P i∈X P l∈L P j∈Q pl k Rl ijzl ij s.t.",
                "P i∈X P j∈Q zl ij = k P j∈Q zl ij ≤ k kql j ≤ P i∈X zl ij ≤ k P j∈Q ql j = 1 0 ≤ (al − P i∈X 1 k Cl ij( P h∈Q zl ih)) ≤ (1 − ql j)M P j∈Q zl ij = P j∈Q z1 ij zl ij ∈ {0, 1, ...., k} ql j ∈ {0, 1} (8) PROPOSITION 2.",
                "Problems (7) and (8) are equivalent.",
                "Proof: Consider x, ql , al with l ∈ L a feasible solution of (7).",
                "We will show that ql , al , zl ij = xiql j is a feasible solution of (8) of same objective function value.",
                "The equivalence of the objective functions, and constraints 4, 7 and 8 of (8) are satisfied by construction.",
                "The fact that P j∈Q zl ij = xi as P j∈Q ql j = 1 explains constraints 1, 2, 5 and 6 of (8).",
                "Constraint 3 of (8) is satisfied because P i∈X zl ij = kql j.",
                "Lets now consider ql , zl , al feasible for (8).",
                "We will show that ql , al and xi = P j∈Q z1 ij are feasible for (7) with the same objective value.",
                "In fact all constraints of (7) are readily satisfied by construction.",
                "To see that the objectives match, notice for each l one ql j must equal 1 and the rest equal 0.",
                "Let us say that ql jl = 1, then the third constraint in (8) implies that P i∈X zl ijl = k, which means that zl ij = 0 for all i ∈ X and all j = jl.",
                "In particular this implies that xi = X j∈Q z1 ij = z1 ij1 = zl ijl , the last equality from constraint 6 of (8).",
                "Therefore xiql j = zl ijl ql j = zl ij.",
                "This last equality is because both are 0 when j = jl.",
                "Effectively, constraint 6 ensures that all the adversaries are calculating their best responses against a particular fixed policy of the agent.",
                "This shows that the transformation preserves the objective function value, completing the proof.",
                "We can therefore solve this equivalent linear integer program with efficient integer programming packages which can handle problems with thousands of integer variables.",
                "We implemented the decomposed MILP and the results are shown in the following section. 6.",
                "EXPERIMENTAL RESULTS The patrolling domain and the payoffs for the associated game are detailed in Sections 2 and 3.",
                "We performed experiments for this game in worlds of three and four houses with patrols consisting of two houses.",
                "The description given in Section 2 is used to generate a base case for both the security agent and robber payoff functions.",
                "The payoff tables for additional robber types are constructed and added to the game by adding a random distribution of varying size to the payoffs in the base case.",
                "All games are normalized so that, for each robber type, the minimum and maximum payoffs to the security agent and robber are 0 and 1, respectively.",
                "Using the data generated, we performed the experiments using four methods for generating the security agents strategy: • uniform randomization • ASAP • the multiple linear programs method from [5] (to find the true optimal strategy) • the highest reward Bayes-Nash equilibrium, found using the MIP-Nash algorithm [17] The last three methods were applied using CPLEX 8.1.",
                "Because the last two methods are designed for normal-form games rather than Bayesian games, the games were first converted using the Harsanyi transformation [8].",
                "The uniform randomization method is simply choosing a uniform random policy over all possible patrol routes.",
                "We use this method as a simple baseline to measure the performance of our heuristics.",
                "We anticipated that the uniform policy would perform reasonably well since maximum-entropy policies have been shown to be effective in multiagent security domains [14].",
                "The highest-reward Bayes-Nash equilibria were used in order to demonstrate the higher reward gained by looking for an optimal policy rather than an equilibria in Stackelberg games such as our security domain.",
                "Based on our experiments we present three sets of graphs to demonstrate (1) the runtime of ASAP compared to other common methods for finding a strategy, (2) the reward guaranteed by ASAP compared to other methods, and (3) the effect of varying the parameter k, the size of the multiset, on the performance of ASAP.",
                "In the first two sets of graphs, ASAP is run using a multiset of 80 elements; in the third set this number is varied.",
                "The first set of graphs, shown in Figure 1 shows the runtime graphs for three-house (left column) and four-house (right column) domains.",
                "Each of the three rows of graphs corresponds to a different randomly-generated scenario.",
                "The x-axis shows the number of robber types the security agent faces and the y-axis of the graph shows the runtime in seconds.",
                "All experiments that were not concluded in 30 minutes (1800 seconds) were cut off.",
                "The runtime for the uniform policy is always negligible irrespective of the number of adversaries and hence is not shown.",
                "The ASAP algorithm clearly outperforms the optimal, multipleLP method as well as the MIP-Nash algorithm for finding the highestreward Bayes-Nash equilibrium with respect to runtime.",
                "For a 316 The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) Figure 1: Runtimes for various algorithms on problems of 3 and 4 houses. domain of three houses, the optimal method cannot reach a solution for more than seven robber types, and for four houses it cannot solve for more than six types within the cutoff time in any of the three scenarios.",
                "MIP-Nash solves for even fewer robber types within the cutoff time.",
                "On the other hand, ASAP runs much faster, and is able to solve for at least 20 adversaries for the three-house scenarios and for at least 12 adversaries in the four-house scenarios within the cutoff time.",
                "The runtime of ASAP does not increase strictly with the number of robber types for each scenario, but in general, the addition of more types increases the runtime required.",
                "The second set of graphs, Figure 2, shows the reward to the patrol agent given by each method for three scenarios in the three-house (left column) and four-house (right column) domains.",
                "This reward is the utility received by the security agent in the patrolling game, and not as a percentage of the optimal reward, since it was not possible to obtain the optimal reward as the number of robber types increased.",
                "The uniform policy consistently provides the lowest reward in both domains; while the optimal method of course produces the optimal reward.",
                "The ASAP method remains consistently close to the optimal, even as the number of robber types increases.",
                "The highest-reward Bayes-Nash equilibria, provided by the MIPNash method, produced rewards higher than the uniform method, but lower than ASAP.",
                "This difference clearly illustrates the gains in the patrolling domain from committing to a strategy as the leader in a Stackelberg game, rather than playing a standard Bayes-Nash strategy.",
                "The third set of graphs, shown in Figure 3 shows the effect of the multiset size on runtime in seconds (left column) and reward (right column), again expressed as the reward received by the security agent in the patrolling game, and not a percentage of the optimal Figure 2: Reward for various algorithms on problems of 3 and 4 houses. reward.",
                "Results here are for the three-house domain.",
                "The trend is that as as the multiset size is increased, the runtime and reward level both increase.",
                "Not surprisingly, the reward increases monotonically as the multiset size increases, but what is interesting is that there is relatively little benefit to using a large multiset in this domain.",
                "In all cases, the reward given by a multiset of 10 elements was within at least 96% of the reward given by an 80-element multiset.",
                "The runtime does not always increase strictly with the multiset size; indeed in one example (scenario 2 with 20 robber types), using a multiset of 10 elements took 1228 seconds, while using 80 elements only took 617 seconds.",
                "In general, runtime should increase since a larger multiset means a larger domain for the variables in the MILP, and thus a larger search space.",
                "However, an increase in the number of variables can sometimes allow for a policy to be constructed more quickly due to more flexibility in the problem. 7.",
                "SUMMARY AND RELATED WORK This paper focuses on security for agents patrolling in hostile environments.",
                "In these environments, intentional threats are caused by adversaries about whom the security patrolling agents have incomplete information.",
                "Specifically, we deal with situations where the adversaries actions and payoffs are known but the exact adversary type is unknown to the security agent.",
                "Agents acting in the real world quite frequently have such incomplete information about other agents.",
                "Bayesian games have been a popular choice to model such incomplete information games [3].",
                "The Gala toolkit is one method for defining such games [9] without requiring the game to be represented in normal form via the Harsanyi transformation [8]; Galas guarantees are focused on fully competitive games.",
                "Much work has been done on finding optimal Bayes-Nash equilbria for The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) 317 Figure 3: Reward for ASAP using multisets of 10, 30, and 80 elements subclasses of Bayesian games, finding single Bayes-Nash equilibria for general Bayesian games [10] or approximate Bayes-Nash equilibria [18].",
                "Less attention has been paid to finding the optimal strategy to commit to in a <br>bayesian game</br> (the Stackelberg scenario [15]).",
                "However, the complexity of this problem was shown to be NP-hard in the general case [5], which also provides algorithms for this problem in the non-Bayesian case.",
                "Therefore, we present a heuristic called ASAP, with three key advantages towards addressing this problem.",
                "First, ASAP searches for the highest reward strategy, rather than a Bayes-Nash equilibrium, allowing it to find feasible strategies that exploit the natural first-mover advantage of the game.",
                "Second, it provides strategies which are simple to understand, represent, and implement.",
                "Third, it operates directly on the compact, <br>bayesian game</br> representation, without requiring conversion to normal form.",
                "We provide an efficient Mixed Integer Linear Program (MILP) implementation for ASAP, along with experimental results illustrating significant speedups and higher rewards over other approaches.",
                "Our k-uniform strategies are similar to the k-uniform strategies of [12].",
                "While that work provides epsilon error-bounds based on the k-uniform strategies, their solution concept is still that of a Nash equilibrium, and they do not provide efficient algorithms for obtaining such k-uniform strategies.",
                "This contrasts with ASAP, where our emphasis is on a highly efficient heuristic approach that is not focused on equilibrium solutions.",
                "Finally the patrolling problem which motivated our work has recently received growing attention from the multiagent community due to its wide range of applications [4, 13].",
                "However most of this work is focused on either limiting energy consumption involved in patrolling [7] or optimizing on criteria like the length of the path traveled [4, 13], without reasoning about any explicit model of an adversary[14].",
                "Acknowledgments : This research is supported by the United States Department of Homeland Security through Center for Risk and Economic Analysis of Terrorism Events (CREATE).",
                "It is also supported by the Defense Advanced Research Projects Agency (DARPA), through the Department of the Interior, NBC, Acquisition Services Division, under Contract No.",
                "NBCHD030010.",
                "Sarit Kraus is also affiliated with UMIACS. 8.",
                "REFERENCES [1] R. W. Beard and T. McLain.",
                "Multiple UAV cooperative search under collision avoidance and limited range communication constraints.",
                "In IEEE CDC, 2003. [2] D. Bertsimas and J. Tsitsiklis.",
                "Introduction to Linear Optimization.",
                "Athena Scientific, 1997. [3] J. Brynielsson and S. Arnborg.",
                "Bayesian games for threat prediction and situation analysis.",
                "In FUSION, 2004. [4] Y. Chevaleyre.",
                "Theoretical analysis of multi-agent patrolling problem.",
                "In AAMAS, 2004. [5] V. Conitzer and T. Sandholm.",
                "Choosing the best strategy to commit to.",
                "In ACM Conference on Electronic Commerce, 2006. [6] D. Fudenberg and J. Tirole.",
                "Game Theory.",
                "MIT Press, 1991. [7] C. Gui and P. Mohapatra.",
                "Virtual patrol: A new power conservation design for surveillance using sensor networks.",
                "In IPSN, 2005. [8] J. C. Harsanyi and R. Selten.",
                "A generalized Nash solution for two-person bargaining games with incomplete information.",
                "Management Science, 18(5):80-106, 1972. [9] D. Koller and A. Pfeffer.",
                "Generating and solving imperfect information games.",
                "In IJCAI, pages 1185-1193, 1995. [10] D. Koller and A. Pfeffer.",
                "Representations and solutions for game-theoretic problems.",
                "Artificial Intelligence, 94(1):167-215, 1997. [11] C. Lemke and J. Howson.",
                "Equilibrium points of bimatrix games.",
                "Journal of the Society for Industrial and Applied Mathematics, 12:413-423, 1964. [12] R. J. Lipton, E. Markakis, and A. Mehta.",
                "Playing large games using simple strategies.",
                "In ACM Conference on Electronic Commerce, 2003. [13] A. Machado, G. Ramalho, J. D. Zucker, and A. Drougoul.",
                "Multi-agent patrolling: an empirical analysis on alternative architectures.",
                "In MABS, 2002. [14] P. Paruchuri, M. Tambe, F. Ordonez, and S. Kraus.",
                "Security in multiagent systems by policy randomization.",
                "In AAMAS, 2006. [15] T. Roughgarden.",
                "Stackelberg scheduling strategies.",
                "In ACM Symposium on TOC, 2001. [16] S. Ruan, C. Meirina, F. Yu, K. R. Pattipati, and R. L. Popp.",
                "Patrolling in a stochastic environment.",
                "In 10th Intl.",
                "Command and Control Research Symp., 2005. [17] T. Sandholm, A. Gilpin, and V. Conitzer.",
                "Mixed-integer programming methods for finding nash equilibria.",
                "In AAAI, 2005. [18] S. Singh, V. Soni, and M. Wellman.",
                "Computing approximate Bayes-Nash equilibria with tree-games of incomplete information.",
                "In ACM Conference on Electronic Commerce, 2004. 318 The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07)"
            ],
            "original_annotated_samples": [],
            "translated_annotated_samples": [
                "Tercero, opera directamente en la representación compacta, \"Juego bayesiano\", sin requerir la conversión a la forma normal.",
                "Un \"juego bayesiano\" es un juego en el que los agentes pueden pertenecer a uno o más tipos;El tipo de agente determina sus posibles acciones y pagos.",
                "El problema de elegir una estrategia óptima para que el líder se comprometa en un juego de Stackelberg se analiza en [5] y se encuentra que es NP-Hard en el caso de un \"juego bayesiano\" con múltiples tipos de seguidores.",
                "Los métodos para encontrar estrategias de líderes óptimas para juegos no bayesianos [5] se pueden aplicar a este problema al convertir el \"juego bayesiano\" en un juego de forma normal por la transformación de Harsanyi [8].",
                "Sin embargo, al transformar el juego, se pierde la estructura compacta del \"juego bayesiano\".",
                "La Sección 3 presenta el \"Juego Bayesiano\", la transformación Harsanyi y los métodos existentes para encontrar una estrategia de líderes óptimos en un juego de Stackelberg.",
                "Si la distribución de diferentes tipos de ladrones se conoce o infiere de los datos históricos, entonces el juego se puede modelar como un \"juego bayesiano\" [6].3.",
                "Juegos bayesianos Un \"juego bayesiano\" contiene un conjunto de n agentes, y cada agente N debe ser uno de un conjunto dado de tipos θn.",
                "Un \"juego bayesiano\" puede transformarse en un juego de forma normal utilizando la transformación Harsanyi [8].",
                "Una vez hecho esto, se pueden utilizar nuevos métodos basados en programas lineales (LP) para encontrar estrategias de alta recompensa para juegos de forma normal [5] para encontrar una estrategia en el juego transformado;Esta estrategia se puede usar para el \"juego bayesiano\".",
                "Las siguientes dos subsecciones elaboran cómo se hace esto.3.1 Transformación de Harsanyi El primer paso para resolver juegos bayesianos es aplicar la transformación Harsanyi [8] que convierte el \"juego bayesiano\" en un juego de forma normal.",
                "Supongamos que hay dos tipos de ladrones A y B en el \"juego bayesiano\".",
                "Del \"Juego bayesiano\" en la Tabla 2, construimos la Bimatriz transformada Harsanyi en la Tabla 3.",
                "El uso de este método para un \"juego bayesiano\" requiere correr | σ2 || θ2 |programas lineales separados.",
                "P i xi = kp j∈Q ql j = 1 0 ≤ (al - p i∈X 1 k cl ijxi) ≤ (1 - ql j) m xi ∈ {0, 1, ...., k} ql j∈ {0, 1}.(7) El problema (7) para un \"juego bayesiano\" con múltiples tipos de seguidores es realmente equivalente al problema (5) en la matriz de pago obtenida de la transformación de Harsanyi del juego.",
                "Se ha prestado menos atención a encontrar la estrategia óptima para comprometerse en un \"juego bayesiano\" (el escenario de Stackelberg [15]).",
                "Tercero, opera directamente en la representación compacta, \"Juego bayesiano\", sin requerir la conversión a la forma normal."
            ],
            "translated_text": "",
            "candidates": [
                "juego bayesiano",
                "Juego bayesiano",
                "juego bayesiano",
                "juego bayesiano",
                "juego bayesiano",
                "juego bayesiano",
                "juego bayesiano",
                "juego bayesiano",
                "juego bayesiano",
                "juego bayesiano",
                "juego bayesiano",
                "Juego Bayesiano",
                "Juego bayesiano",
                "juego bayesiano",
                "juego bayesiano",
                "juego bayesiano",
                "juego bayesiano",
                "juego bayesiano",
                "juego bayesiano",
                "juego bayesiano",
                "juego bayesiano",
                "juego bayesiano",
                "juego bayesiano",
                "juego bayesiano",
                "Juego bayesiano",
                "Juego bayesiano",
                "juego bayesiano",
                "juego bayesiano",
                "juego bayesiano",
                "juego bayesiano",
                "juego bayesiano",
                "juego bayesiano",
                "juego bayesiano",
                "Juego bayesiano"
            ],
            "error": []
        },
        "bayes-nash equilibrium": {
            "translated_key": "Equilibrio de Nash Bayes",
            "is_in_text": true,
            "original_annotated_sentences": [
                "An Efficient Heuristic Approach for Security Against Multiple Adversaries Praveen Paruchuri, Jonathan P. Pearce, Milind Tambe, Fernando Ordonez University of Southern California Los Angeles, CA 90089 {paruchur, jppearce, tambe, fordon}@usc.edu Sarit Kraus Bar-Ilan University Ramat-Gan 52900, Israel sarit@cs.biu.ac.il ABSTRACT In adversarial multiagent domains, security, commonly defined as the ability to deal with intentional threats from other agents, is a critical issue.",
                "This paper focuses on domains where these threats come from unknown adversaries.",
                "These domains can be modeled as Bayesian games; much work has been done on finding equilibria for such games.",
                "However, it is often the case in multiagent security domains that one agent can commit to a mixed strategy which its adversaries observe before choosing their own strategies.",
                "In this case, the agent can maximize reward by finding an optimal strategy, without requiring equilibrium.",
                "Previous work has shown this problem of optimal strategy selection to be NP-hard.",
                "Therefore, we present a heuristic called ASAP, with three key advantages to address the problem.",
                "First, ASAP searches for the highest-reward strategy, rather than a <br>bayes-nash equilibrium</br>, allowing it to find feasible strategies that exploit the natural first-mover advantage of the game.",
                "Second, it provides strategies which are simple to understand, represent, and implement.",
                "Third, it operates directly on the compact, Bayesian game representation, without requiring conversion to normal form.",
                "We provide an efficient Mixed Integer Linear Program (MILP) implementation for ASAP, along with experimental results illustrating significant speedups and higher rewards over other approaches.",
                "Categories and Subject Descriptors I.2.11 [Computing Methodologies]: Artificial Intelligence: Distributed Artificial Intelligence - Intelligent Agents General Terms Security, Design, Theory 1.",
                "INTRODUCTION In many multiagent domains, agents must act in order to provide security against attacks by adversaries.",
                "A common issue that agents face in such security domains is uncertainty about the adversaries they may be facing.",
                "For example, a security robot may need to make a choice about which areas to patrol, and how often [16].",
                "However, it will not know in advance exactly where a robber will choose to strike.",
                "A team of unmanned aerial vehicles (UAVs) [1] monitoring a region undergoing a humanitarian crisis may also need to choose a patrolling policy.",
                "They must make this decision without knowing in advance whether terrorists or other adversaries may be waiting to disrupt the mission at a given location.",
                "It may indeed be possible to model the motivations of types of adversaries the agent or agent team is likely to face in order to target these adversaries more closely.",
                "However, in both cases, the security robot or UAV team will not know exactly which kinds of adversaries may be active on any given day.",
                "A common approach for choosing a policy for agents in such scenarios is to model the scenarios as Bayesian games.",
                "A Bayesian game is a game in which agents may belong to one or more types; the type of an agent determines its possible actions and payoffs.",
                "The distribution of adversary types that an agent will face may be known or inferred from historical data.",
                "Usually, these games are analyzed according to the solution concept of a <br>bayes-nash equilibrium</br>, an extension of the Nash equilibrium for Bayesian games.",
                "However, in many settings, a Nash or <br>bayes-nash equilibrium</br> is not an appropriate solution concept, since it assumes that the agents strategies are chosen simultaneously [5].",
                "In some settings, one player can (or must) commit to a strategy before the other players choose their strategies.",
                "These scenarios are known as Stackelberg games [6].",
                "In a Stackelberg game, a leader commits to a strategy first, and then a follower (or group of followers) selfishly optimize their own rewards, considering the action chosen by the leader.",
                "For example, the security agent (leader) must first commit to a strategy for patrolling various areas.",
                "This strategy could be a mixed strategy in order to be unpredictable to the robbers (followers).",
                "The robbers, after observing the pattern of patrols over time, can then choose their strategy (which location to rob).",
                "Often, the leader in a Stackelberg game can attain a higher reward than if the strategies were chosen simultaneously.",
                "To see the advantage of being the leader in a Stackelberg game, consider a simple game with the payoff table as shown in Table 1.",
                "The leader is the row player and the follower is the column player.",
                "Here, the leaders payoff is listed first. 1 2 3 1 5,5 0,0 3,10 2 0,0 2,2 5,0 Table 1: Payoff table for example normal form game.",
                "The only Nash equilibrium for this game is when the leader plays 2 and the follower plays 2 which gives the leader a payoff of 2. 311 978-81-904262-7-5 (RPS) c 2007 IFAAMAS However, if the leader commits to a uniform mixed strategy of playing 1 and 2 with equal (0.5) probability, the followers best response is to play 3 to get an expected payoff of 5 (10 and 0 with equal probability).",
                "The leaders payoff would then be 4 (3 and 5 with equal probability).",
                "In this case, the leader now has an incentive to deviate and choose a pure strategy of 2 (to get a payoff of 5).",
                "However, this would cause the follower to deviate to strategy 2 as well, resulting in the Nash equilibrium.",
                "Thus, by committing to a strategy that is observed by the follower, and by avoiding the temptation to deviate, the leader manages to obtain a reward higher than that of the best Nash equilibrium.",
                "The problem of choosing an optimal strategy for the leader to commit to in a Stackelberg game is analyzed in [5] and found to be NP-hard in the case of a Bayesian game with multiple types of followers.",
                "Thus, efficient heuristic techniques for choosing highreward strategies in these games is an important open issue.",
                "Methods for finding optimal leader strategies for non-Bayesian games [5] can be applied to this problem by converting the Bayesian game into a normal-form game by the Harsanyi transformation [8].",
                "If, on the other hand, we wish to compute the highest-reward Nash equilibrium, new methods using mixed-integer linear programs (MILPs) [17] may be used, since the highest-reward <br>bayes-nash equilibrium</br> is equivalent to the corresponding Nash equilibrium in the transformed game.",
                "However, by transforming the game, the compact structure of the Bayesian game is lost.",
                "In addition, since the Nash equilibrium assumes a simultaneous choice of strategies, the advantages of being the leader are not considered.",
                "This paper introduces an efficient heuristic method for approximating the optimal leader strategy for security domains, known as ASAP (Agent Security via Approximate Policies).",
                "This method has three key advantages.",
                "First, it directly searches for an optimal strategy, rather than a Nash (or Bayes-Nash) equilibrium, thus allowing it to find high-reward non-equilibrium strategies like the one in the above example.",
                "Second, it generates policies with a support which can be expressed as a uniform distribution over a multiset of fixed size as proposed in [12].",
                "This allows for policies that are simple to understand and represent [12], as well as a tunable parameter (the size of the multiset) that controls the simplicity of the policy.",
                "Third, the method allows for a Bayes-Nash game to be expressed compactly without conversion to a normal-form game, allowing for large speedups over existing Nash methods such as [17] and [11].",
                "The rest of the paper is organized as follows.",
                "In Section 2 we fully describe the patrolling domain and its properties.",
                "Section 3 introduces the Bayesian game, the Harsanyi transformation, and existing methods for finding an optimal leaders strategy in a Stackelberg game.",
                "Then, in Section 4 the ASAP algorithm is presented for normal-form games, and in Section 5 we show how it can be adapted to the structure of Bayesian games with uncertain adversaries.",
                "Experimental results showing higher reward and faster policy computation over existing Nash methods are shown in Section 6, and we conclude with a discussion of related work in Section 7. 2.",
                "THE PATROLLING DOMAIN In most security patrolling domains, the security agents (like UAVs [1] or security robots [16]) cannot feasibly patrol all areas all the time.",
                "Instead, they must choose a policy by which they patrol various routes at different times, taking into account factors such as the likelihood of crime in different areas, possible targets for crime, and the security agents own resources (number of security agents, amount of available time, fuel, etc.).",
                "It is usually beneficial for this policy to be nondeterministic so that robbers cannot safely rob certain locations, knowing that they will be safe from the security agents [14].",
                "To demonstrate the utility of our algorithm, we use a simplified version of such a domain, expressed as a game.",
                "The most basic version of our game consists of two players: the security agent (the leader) and the robber (the follower) in a world consisting of m houses, 1 . . . m. The security agents set of pure strategies consists of possible routes of d houses to patrol (in an order).",
                "The security agent can choose a mixed strategy so that the robber will be unsure of exactly where the security agent may patrol, but the robber will know the mixed strategy the security agent has chosen.",
                "For example, the robber can observe over time how often the security agent patrols each area.",
                "With this knowledge, the robber must choose a single house to rob.",
                "We assume that the robber generally takes a long time to rob a house.",
                "If the house chosen by the robber is not on the security agents route, then the robber successfully robs it.",
                "Otherwise, if it is on the security agents route, then the earlier the house is on the route, the easier it is for the security agent to catch the robber before he finishes robbing it.",
                "We model the payoffs for this game with the following variables: • vl,x: value of the goods in house l to the security agent. • vl,q: value of the goods in house l to the robber. • cx: reward to the security agent of catching the robber. • cq: cost to the robber of getting caught. • pl: probability that the security agent can catch the robber at the lth house in the patrol (pl < pl ⇐⇒ l < l).",
                "The security agents set of possible pure strategies (patrol routes) is denoted by X and includes all d-tuples i =< w1, w2, ..., wd > with w1 . . . wd = 1 . . . m where no two elements are equal (the agent is not allowed to return to the same house).",
                "The robbers set of possible pure strategies (houses to rob) is denoted by Q and includes all integers j = 1 . . . m. The payoffs (security agent, robber) for pure strategies i, j are: • −vl,x, vl,q, for j = l /∈ i. • plcx +(1−pl)(−vl,x), −plcq +(1−pl)(vl,q), for j = l ∈ i.",
                "With this structure it is possible to model many different types of robbers who have differing motivations; for example, one robber may have a lower cost of getting caught than another, or may value the goods in the various houses differently.",
                "If the distribution of different robber types is known or inferred from historical data, then the game can be modeled as a Bayesian game [6]. 3.",
                "BAYESIAN GAMES A Bayesian game contains a set of N agents, and each agent n must be one of a given set of types θn.",
                "For our patrolling domain, we have two agents, the security agent and the robber. θ1 is the set of security agent types and θ2 is the set of robber types.",
                "Since there is only one type of security agent, θ1 contains only one element.",
                "During the game, the robber knows its type but the security agent does not know the robbers type.",
                "For each agent (the security agent or the robber) n, there is a set of strategies σn and a utility function un : θ1 × θ2 × σ1 × σ2 → .",
                "A Bayesian game can be transformed into a normal-form game using the Harsanyi transformation [8].",
                "Once this is done, new, linear-program (LP)-based methods for finding high-reward strategies for normal-form games [5] can be used to find a strategy in the transformed game; this strategy can then be used for the Bayesian game.",
                "While methods exist for finding Bayes-Nash equilibria directly, without the Harsanyi transformation [10], they find only a 312 The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) single equilibrium in the general case, which may not be of high reward.",
                "Recent work [17] has led to efficient mixed-integer linear program techniques to find the best Nash equilibrium for a given agent.",
                "However, these techniques do require a normal-form game, and so to compare the policies given by ASAP against the optimal policy, as well as against the highest-reward Nash equilibrium, we must apply these techniques to the Harsanyi-transformed matrix.",
                "The next two subsections elaborate on how this is done. 3.1 Harsanyi Transformation The first step in solving Bayesian games is to apply the Harsanyi transformation [8] that converts the Bayesian game into a normal form game.",
                "Given that the Harsanyi transformation is a standard concept in game theory, we explain it briefly through a simple example in our patrolling domain without introducing the mathematical formulations.",
                "Let us assume there are two robber types a and b in the Bayesian game.",
                "Robber a will be active with probability α, and robber b will be active with probability 1 − α.",
                "The rules described in Section 2 allow us to construct simple payoff tables.",
                "Assume that there are two houses in the world (1 and 2) and hence there are two patrol routes (pure strategies) for the agent: {1,2} and {2,1}.",
                "The robber can rob either house 1 or house 2 and hence he has two strategies (denoted as 1l, 2l for robber type l).",
                "Since there are two types assumed (denoted as a and b), we construct two payoff tables (shown in Table 2) corresponding to the security agent playing a separate game with each of the two robber types with probabilities α and 1 − α.",
                "First, consider robber type a.",
                "Borrowing the notation from the domain section, we assign the following values to the variables: v1,x = v1,q = 3/4, v2,x = v2,q = 1/4, cx = 1/2, cq = 1, p1 = 1, p2 = 1/2.",
                "Using these values we construct a base payoff table as the payoff for the game against robber type a.",
                "For example, if the security agent chooses route {1,2} when robber a is active, and robber a chooses house 1, the robber receives a reward of -1 (for being caught) and the agent receives a reward of 0.5 for catching the robber.",
                "The payoffs for the game against robber type b are constructed using different values.",
                "Security agent: {1,2} {2,1} Robber a 1a -1, .5 -.375, .125 2a -.125, -.125 -1, .5 Robber b 1b -.9, .6 -.275, .225 2b -.025, -.025 -.9, .6 Table 2: Payoff tables: Security Agent vs Robbers a and b Using the Harsanyi technique involves introducing a chance node, that determines the robbers type, thus transforming the security agents incomplete information regarding the robber into imperfect information [3].",
                "The Bayesian equilibrium of the game is then precisely the Nash equilibrium of the imperfect information game.",
                "The transformed, normal-form game is shown in Table 3.",
                "In the transformed game, the security agent is the column player, and the set of all robber types together is the row player.",
                "Suppose that robber type a robs house 1 and robber type b robs house 2, while the security agent chooses patrol {1,2}.",
                "Then, the security agent and the robber receive an expected payoff corresponding to their payoffs from the agent encountering robber a at house 1 with probability α and robber b at house 2 with probability 1 − α. 3.2 Finding an Optimal Strategy Although a Nash equilibrium is the standard solution concept for games in which agents choose strategies simultaneously, in our security domain, the security agent (the leader) can gain an advantage by committing to a mixed strategy in advance.",
                "Since the followers (the robbers) will know the leaders strategy, the optimal response for the followers will be a pure strategy.",
                "Given the common assumption, taken in [5], in the case where followers are indifferent, they will choose the strategy that benefits the leader, there must exist a guaranteed optimal strategy for the leader [5].",
                "From the Bayesian game in Table 2, we constructed the Harsanyi transformed bimatrix in Table 3.",
                "The strategies for each player (security agent or robber) in the transformed game correspond to all combinations of possible strategies taken by each of that players types.",
                "Therefore, we denote X = σθ1 1 = σ1 and Q = σθ2 2 as the index sets of the security agent and robbers pure strategies respectively, with R and C as the corresponding payoff matrices.",
                "Rij is the reward of the security agent and Cij is the reward of the robbers when the security agent takes pure strategy i and the robbers take pure strategy j.",
                "A mixed strategy for the security agent is a probability distribution over its set of pure strategies and will be represented by a vector x = (px1, px2, . . . , px|X|), where pxi ≥ 0 and P pxi = 1.",
                "Here, pxi is the probability that the security agent will choose its ith pure strategy.",
                "The optimal mixed strategy for the security agent can be found in time polynomial in the number of rows in the normal form game using the following linear program formulation from [5].",
                "For every possible pure strategy j by the follower (the set of all robber types), max P i∈X pxiRij s.t. ∀j ∈ Q, P i∈σ1 pxiCij ≥ P i∈σ1 pxiCij P i∈X pxi = 1 ∀i∈X , pxi >= 0 (1) Then, for all feasible follower strategies j, choose the one that maximizes P i∈X pxiRij, the reward for the security agent (leader).",
                "The pxi variables give the optimal strategy for the security agent.",
                "Note that while this method is polynomial in the number of rows in the transformed, normal-form game, the number of rows increases exponentially with the number of robber types.",
                "Using this method for a Bayesian game thus requires running |σ2||θ2| separate linear programs.",
                "This is no surprise, since finding the leaders optimal strategy in a Bayesian Stackelberg game is NP-hard [5]. 4.",
                "HEURISTIC APPROACHES Given that finding the optimal strategy for the leader is NP-hard, we provide a heuristic approach.",
                "In this heuristic we limit the possible mixed strategies of the leader to select actions with probabilities that are integer multiples of 1/k for a predetermined integer k. Previous work [14] has shown that strategies with high entropy are beneficial for security applications when opponents utilities are completely unknown.",
                "In our domain, if utilities are not considered, this method will result in uniform-distribution strategies.",
                "One advantage of such strategies is that they are compact to represent (as fractions) and simple to understand; therefore they can be efficiently implemented by real organizations.",
                "We aim to maintain the advantage provided by simple strategies for our security application problem, incorporating the effect of the robbers rewards on the security agents rewards.",
                "Thus, the ASAP heuristic will produce strategies which are k-uniform.",
                "A mixed strategy is denoted k-uniform if it is a uniform distribution on a multiset S of pure strategies with |S| = k. A multiset is a set whose elements may be repeated multiple times; thus, for example, the mixed strategy corresponding to the multiset {1, 1, 2} would take strategy 1 The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) 313 {1,2} {2,1} {1a, 1b} −1α − .9(1 − α), .5α + .6(1 − α) −.375α − .275(1 − α), .125α + .225(1 − α) {1a, 2b} −1α − .025(1 − α), .5α − .025(1 − α) −.375α − .9(1 − α), .125α + .6(1 − α) {2a, 1b} −.125α − .9(1 − α), −.125α + .6(1 − α) −1α − .275(1 − α), .5α + .225(1 − α) {2a, 2b} −.125α − .025(1 − α), −.125α − .025(1 − α) −1α − .9(1 − α), .5α + .6(1 − α) Table 3: Harsanyi Transformed Payoff Table with probability 2/3 and strategy 2 with probability 1/3.",
                "ASAP allows the size of the multiset to be chosen in order to balance the complexity of the strategy reached with the goal that the identified strategy will yield a high reward.",
                "Another advantage of the ASAP heuristic is that it operates directly on the compact Bayesian representation, without requiring the Harsanyi transformation.",
                "This is because the different follower (robber) types are independent of each other.",
                "Hence, evaluating the leader strategy against a Harsanyi-transformed game matrix is equivalent to evaluating against each of the game matrices for the individual follower types.",
                "This independence property is exploited in ASAP to yield a decomposition scheme.",
                "Note that the LP method introduced by [5] to compute optimal Stackelberg policies is unlikely to be decomposable into a small number of games as it was shown to be NP-hard for Bayes-Nash problems.",
                "Finally, note that ASAP requires the solution of only one optimization problem, rather than solving a series of problems as in the LP method of [5].",
                "For a single follower type, the algorithm works the following way.",
                "Given a particular k, for each possible mixed strategy x for the leader that corresponds to a multiset of size k, evaluate the leaders payoff from x when the follower plays a reward-maximizing pure strategy.",
                "We then take the mixed strategy with the highest payoff.",
                "We need only to consider the reward-maximizing pure strategies of the followers (robbers), since for a given fixed strategy x of the security agent, each robber type faces a problem with fixed linear rewards.",
                "If a mixed strategy is optimal for the robber, then so are all the pure strategies in the support of that mixed strategy.",
                "Note also that because we limit the leaders strategies to take on discrete values, the assumption from Section 3.2 that the followers will break ties in the leaders favor is not significant, since ties will be unlikely to arise.",
                "This is because, in domains where rewards are drawn from any random distribution, the probability of a follower having more than one pure optimal response to a given leader strategy approaches zero, and the leader will have only a finite number of possible mixed strategies.",
                "Our approach to characterize the optimal strategy for the security agent makes use of properties of linear programming.",
                "We briefly outline these results here for completeness, for detailed discussion and proofs see one of many references on the topic, such as [2].",
                "Every linear programming problem, such as: max cT x | Ax = b, x ≥ 0, has an associated dual linear program, in this case: min bT y | AT y ≥ c. These primal/dual pairs of problems satisfy weak duality: For any x and y primal and dual feasible solutions respectively, cT x ≤ bT y.",
                "Thus a pair of feasible solutions is optimal if cT x = bT y, and the problems are said to satisfy strong duality.",
                "In fact if a linear program is feasible and has a bounded optimal solution, then the dual is also feasible and there is a pair x∗ , y∗ that satisfies cT x∗ = bT y∗ .",
                "These optimal solutions are characterized with the following optimality conditions (as defined in [2]): • primal feasibility: Ax = b, x ≥ 0 • dual feasibility: AT y ≥ c • complementary slackness: xi(AT y − c)i = 0 for all i.",
                "Note that this last condition implies that cT x = xT AT y = bT y, which proves optimality for primal dual feasible solutions x and y.",
                "In the following subsections, we first define the problem in its most intuititive form as a mixed-integer quadratic program (MIQP), and then show how this problem can be converted into a mixedinteger linear program (MILP). 4.1 Mixed-Integer Quadratic Program We begin with the case of a single type of follower.",
                "Let the leader be the row player and the follower the column player.",
                "We denote by x the vector of strategies of the leader and q the vector of strategies of the follower.",
                "We also denote X and Q the index sets of the leader and followers pure strategies, respectively.",
                "The payoff matrices R and C correspond to: Rij is the reward of the leader and Cij is the reward of the follower when the leader takes pure strategy i and the follower takes pure strategy j.",
                "Let k be the size of the multiset.",
                "We first fix the policy of the leader to some k-uniform policy x.",
                "The value xi is the number of times pure strategy i is used in the k-uniform policy, which is selected with probability xi/k.",
                "We formulate the optimization problem the follower solves to find its optimal response to x as the following linear program: max X j∈Q X i∈X 1 k Cijxi qj s.t.",
                "P j∈Q qj = 1 q ≥ 0. (2) The objective function maximizes the followers expected reward given x, while the constraints make feasible any mixed strategy q for the follower.",
                "The dual to this linear programming problem is the following: min a s.t. a ≥ X i∈X 1 k Cijxi j ∈ Q. (3) From strong duality and complementary slackness we obtain that the followers maximum reward value, a, is the value of every pure strategy with qj > 0, that is in the support of the optimal mixed strategy.",
                "Therefore each of these pure strategies is optimal.",
                "Optimal solutions to the followers problem are characterized by linear programming optimality conditions: primal feasibility constraints in (2), dual feasibility constraints in (3), and complementary slackness qj a − X i∈X 1 k Cijxi ! = 0 j ∈ Q.",
                "These conditions must be included in the problem solved by the leader in order to consider only best responses by the follower to the k-uniform policy x. 314 The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) The leader seeks the k-uniform solution x that maximizes its own payoff, given that the follower uses an optimal response q(x).",
                "Therefore the leader solves the following integer problem: max X i∈X X j∈Q 1 k Rijq(x)j xi s.t.",
                "P i∈X xi = k xi ∈ {0, 1, . . . , k}. (4) Problem (4) maximizes the leaders reward with the followers best response (qj for fixed leaders policy x and hence denoted q(x)j) by selecting a uniform policy from a multiset of constant size k. We complete this problem by including the characterization of q(x) through linear programming optimality conditions.",
                "To simplify writing the complementary slackness conditions, we will constrain q(x) to be only optimal pure strategies by just considering integer solutions of q(x).",
                "The leaders problem becomes: maxx,q X i∈X X j∈Q 1 k Rijxiqj s.t.",
                "P i xi = kP j∈Q qj = 1 0 ≤ (a − P i∈X 1 k Cijxi) ≤ (1 − qj)M xi ∈ {0, 1, ...., k} qj ∈ {0, 1}. (5) Here, the constant M is some large number.",
                "The first and fourth constraints enforce a k-uniform policy for the leader, and the second and fifth constraints enforce a feasible pure strategy for the follower.",
                "The third constraint enforces dual feasibility of the followers problem (leftmost inequality) and the complementary slackness constraint for an optimal pure strategy q for the follower (rightmost inequality).",
                "In fact, since only one pure strategy can be selected by the follower, say qh = 1, this last constraint enforces that a = P i∈X 1 k Cihxi imposing no additional constraint for all other pure strategies which have qj = 0.",
                "We conclude this subsection noting that Problem (5) is an integer program with a non-convex quadratic objective in general, as the matrix R need not be positive-semi-definite.",
                "Efficient solution methods for non-linear, non-convex integer problems remains a challenging research question.",
                "In the next section we show a reformulation of this problem as a linear integer programming problem, for which a number of efficient commercial solvers exist. 4.2 Mixed-Integer Linear Program We can linearize the quadratic program of Problem 5 through the change of variables zij = xiqj, obtaining the following problem maxq,z P i∈X P j∈Q 1 k Rijzij s.t.",
                "P i∈X P j∈Q zij = k P j∈Q zij ≤ k kqj ≤ P i∈X zij ≤ k P j∈Q qj = 1 0 ≤ (a − P i∈X 1 k Cij( P h∈Q zih)) ≤ (1 − qj)M zij ∈ {0, 1, ...., k} qj ∈ {0, 1} (6) PROPOSITION 1.",
                "Problems (5) and (6) are equivalent.",
                "Proof: Consider x, q a feasible solution of (5).",
                "We will show that q, zij = xiqj is a feasible solution of (6) of same objective function value.",
                "The equivalence of the objective functions, and constraints 4, 6 and 7 of (6) are satisfied by construction.",
                "The fact that P j∈Q zij = xi as P j∈Q qj = 1 explains constraints 1, 2, and 5 of (6).",
                "Constraint 3 of (6) is satisfied because P i∈X zij = kqj.",
                "Let us now consider q, z feasible for (6).",
                "We will show that q and xi = P j∈Q zij are feasible for (5) with the same objective value.",
                "In fact all constraints of (5) are readily satisfied by construction.",
                "To see that the objectives match, notice that if qh = 1 then the third constraint in (6) implies that P i∈X zih = k, which means that zij = 0 for all i ∈ X and all j = h. Therefore, xiqj = X l∈Q zilqj = zihqj = zij.",
                "This last equality is because both are 0 when j = h. This shows that the transformation preserves the objective function value, completing the proof.",
                "Given this transformation to a mixed-integer linear program (MILP), we now show how we can apply our decomposition technique on the MILP to obtain significant speedups for Bayesian games with multiple follower types. 5.",
                "DECOMPOSITION FOR MULTIPLE ADVERSARIES The MILP developed in the previous section handles only one follower.",
                "Since our security scenario contains multiple follower (robber) types, we change the response function for the follower from a pure strategy into a weighted combination over various pure follower strategies where the weights are probabilities of occurrence of each of the follower types. 5.1 Decomposed MIQP To admit multiple adversaries in our framework, we modify the notation defined in the previous section to reason about multiple follower types.",
                "We denote by x the vector of strategies of the leader and ql the vector of strategies of follower l, with L denoting the index set of follower types.",
                "We also denote by X and Q the index sets of leader and follower ls pure strategies, respectively.",
                "We also index the payoff matrices on each follower l, considering the matrices Rl and Cl .",
                "Using this modified notation, we characterize the optimal solution of follower ls problem given the leaders k-uniform policy x, with the following optimality conditions: X j∈Q ql j = 1 al − X i∈X 1 k Cl ijxi ≥ 0 ql j(al − X i∈X 1 k Cl ijxi) = 0 ql j ≥ 0 Again, considering only optimal pure strategies for follower ls problem we can linearize the complementarity constraint above.",
                "We incorporate these constraints on the leaders problem that selects the optimal k-uniform policy.",
                "Therefore, given a priori probabilities pl , with l ∈ L of facing each follower, the leader solves the following problem: The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) 315 maxx,q X i∈X X l∈L X j∈Q pl k Rl ijxiql j s.t.",
                "P i xi = kP j∈Q ql j = 1 0 ≤ (al − P i∈X 1 k Cl ijxi) ≤ (1 − ql j)M xi ∈ {0, 1, ...., k} ql j ∈ {0, 1}. (7) Problem (7) for a Bayesian game with multiple follower types is indeed equivalent to Problem (5) on the payoff matrix obtained from the Harsanyi transformation of the game.",
                "In fact, every pure strategy j in Problem (5) corresponds to a sequence of pure strategies jl, one for each follower l ∈ L. This means that qj = 1 if and only if ql jl = 1 for all l ∈ L. In addition, given the a priori probabilities pl of facing player l, the reward in the Harsanyi transformation payoff table is Rij = P l∈L pl Rl ijl .",
                "The same relation holds between C and Cl .",
                "These relations between a pure strategy in the equivalent normal form game and pure strategies in the individual games with each followers are key in showing these problems are equivalent. 5.2 Decomposed MILP We can linearize the quadratic programming problem 7 through the change of variables zl ij = xiql j, obtaining the following problem maxq,z P i∈X P l∈L P j∈Q pl k Rl ijzl ij s.t.",
                "P i∈X P j∈Q zl ij = k P j∈Q zl ij ≤ k kql j ≤ P i∈X zl ij ≤ k P j∈Q ql j = 1 0 ≤ (al − P i∈X 1 k Cl ij( P h∈Q zl ih)) ≤ (1 − ql j)M P j∈Q zl ij = P j∈Q z1 ij zl ij ∈ {0, 1, ...., k} ql j ∈ {0, 1} (8) PROPOSITION 2.",
                "Problems (7) and (8) are equivalent.",
                "Proof: Consider x, ql , al with l ∈ L a feasible solution of (7).",
                "We will show that ql , al , zl ij = xiql j is a feasible solution of (8) of same objective function value.",
                "The equivalence of the objective functions, and constraints 4, 7 and 8 of (8) are satisfied by construction.",
                "The fact that P j∈Q zl ij = xi as P j∈Q ql j = 1 explains constraints 1, 2, 5 and 6 of (8).",
                "Constraint 3 of (8) is satisfied because P i∈X zl ij = kql j.",
                "Lets now consider ql , zl , al feasible for (8).",
                "We will show that ql , al and xi = P j∈Q z1 ij are feasible for (7) with the same objective value.",
                "In fact all constraints of (7) are readily satisfied by construction.",
                "To see that the objectives match, notice for each l one ql j must equal 1 and the rest equal 0.",
                "Let us say that ql jl = 1, then the third constraint in (8) implies that P i∈X zl ijl = k, which means that zl ij = 0 for all i ∈ X and all j = jl.",
                "In particular this implies that xi = X j∈Q z1 ij = z1 ij1 = zl ijl , the last equality from constraint 6 of (8).",
                "Therefore xiql j = zl ijl ql j = zl ij.",
                "This last equality is because both are 0 when j = jl.",
                "Effectively, constraint 6 ensures that all the adversaries are calculating their best responses against a particular fixed policy of the agent.",
                "This shows that the transformation preserves the objective function value, completing the proof.",
                "We can therefore solve this equivalent linear integer program with efficient integer programming packages which can handle problems with thousands of integer variables.",
                "We implemented the decomposed MILP and the results are shown in the following section. 6.",
                "EXPERIMENTAL RESULTS The patrolling domain and the payoffs for the associated game are detailed in Sections 2 and 3.",
                "We performed experiments for this game in worlds of three and four houses with patrols consisting of two houses.",
                "The description given in Section 2 is used to generate a base case for both the security agent and robber payoff functions.",
                "The payoff tables for additional robber types are constructed and added to the game by adding a random distribution of varying size to the payoffs in the base case.",
                "All games are normalized so that, for each robber type, the minimum and maximum payoffs to the security agent and robber are 0 and 1, respectively.",
                "Using the data generated, we performed the experiments using four methods for generating the security agents strategy: • uniform randomization • ASAP • the multiple linear programs method from [5] (to find the true optimal strategy) • the highest reward <br>bayes-nash equilibrium</br>, found using the MIP-Nash algorithm [17] The last three methods were applied using CPLEX 8.1.",
                "Because the last two methods are designed for normal-form games rather than Bayesian games, the games were first converted using the Harsanyi transformation [8].",
                "The uniform randomization method is simply choosing a uniform random policy over all possible patrol routes.",
                "We use this method as a simple baseline to measure the performance of our heuristics.",
                "We anticipated that the uniform policy would perform reasonably well since maximum-entropy policies have been shown to be effective in multiagent security domains [14].",
                "The highest-reward Bayes-Nash equilibria were used in order to demonstrate the higher reward gained by looking for an optimal policy rather than an equilibria in Stackelberg games such as our security domain.",
                "Based on our experiments we present three sets of graphs to demonstrate (1) the runtime of ASAP compared to other common methods for finding a strategy, (2) the reward guaranteed by ASAP compared to other methods, and (3) the effect of varying the parameter k, the size of the multiset, on the performance of ASAP.",
                "In the first two sets of graphs, ASAP is run using a multiset of 80 elements; in the third set this number is varied.",
                "The first set of graphs, shown in Figure 1 shows the runtime graphs for three-house (left column) and four-house (right column) domains.",
                "Each of the three rows of graphs corresponds to a different randomly-generated scenario.",
                "The x-axis shows the number of robber types the security agent faces and the y-axis of the graph shows the runtime in seconds.",
                "All experiments that were not concluded in 30 minutes (1800 seconds) were cut off.",
                "The runtime for the uniform policy is always negligible irrespective of the number of adversaries and hence is not shown.",
                "The ASAP algorithm clearly outperforms the optimal, multipleLP method as well as the MIP-Nash algorithm for finding the highestreward <br>bayes-nash equilibrium</br> with respect to runtime.",
                "For a 316 The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) Figure 1: Runtimes for various algorithms on problems of 3 and 4 houses. domain of three houses, the optimal method cannot reach a solution for more than seven robber types, and for four houses it cannot solve for more than six types within the cutoff time in any of the three scenarios.",
                "MIP-Nash solves for even fewer robber types within the cutoff time.",
                "On the other hand, ASAP runs much faster, and is able to solve for at least 20 adversaries for the three-house scenarios and for at least 12 adversaries in the four-house scenarios within the cutoff time.",
                "The runtime of ASAP does not increase strictly with the number of robber types for each scenario, but in general, the addition of more types increases the runtime required.",
                "The second set of graphs, Figure 2, shows the reward to the patrol agent given by each method for three scenarios in the three-house (left column) and four-house (right column) domains.",
                "This reward is the utility received by the security agent in the patrolling game, and not as a percentage of the optimal reward, since it was not possible to obtain the optimal reward as the number of robber types increased.",
                "The uniform policy consistently provides the lowest reward in both domains; while the optimal method of course produces the optimal reward.",
                "The ASAP method remains consistently close to the optimal, even as the number of robber types increases.",
                "The highest-reward Bayes-Nash equilibria, provided by the MIPNash method, produced rewards higher than the uniform method, but lower than ASAP.",
                "This difference clearly illustrates the gains in the patrolling domain from committing to a strategy as the leader in a Stackelberg game, rather than playing a standard Bayes-Nash strategy.",
                "The third set of graphs, shown in Figure 3 shows the effect of the multiset size on runtime in seconds (left column) and reward (right column), again expressed as the reward received by the security agent in the patrolling game, and not a percentage of the optimal Figure 2: Reward for various algorithms on problems of 3 and 4 houses. reward.",
                "Results here are for the three-house domain.",
                "The trend is that as as the multiset size is increased, the runtime and reward level both increase.",
                "Not surprisingly, the reward increases monotonically as the multiset size increases, but what is interesting is that there is relatively little benefit to using a large multiset in this domain.",
                "In all cases, the reward given by a multiset of 10 elements was within at least 96% of the reward given by an 80-element multiset.",
                "The runtime does not always increase strictly with the multiset size; indeed in one example (scenario 2 with 20 robber types), using a multiset of 10 elements took 1228 seconds, while using 80 elements only took 617 seconds.",
                "In general, runtime should increase since a larger multiset means a larger domain for the variables in the MILP, and thus a larger search space.",
                "However, an increase in the number of variables can sometimes allow for a policy to be constructed more quickly due to more flexibility in the problem. 7.",
                "SUMMARY AND RELATED WORK This paper focuses on security for agents patrolling in hostile environments.",
                "In these environments, intentional threats are caused by adversaries about whom the security patrolling agents have incomplete information.",
                "Specifically, we deal with situations where the adversaries actions and payoffs are known but the exact adversary type is unknown to the security agent.",
                "Agents acting in the real world quite frequently have such incomplete information about other agents.",
                "Bayesian games have been a popular choice to model such incomplete information games [3].",
                "The Gala toolkit is one method for defining such games [9] without requiring the game to be represented in normal form via the Harsanyi transformation [8]; Galas guarantees are focused on fully competitive games.",
                "Much work has been done on finding optimal Bayes-Nash equilbria for The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) 317 Figure 3: Reward for ASAP using multisets of 10, 30, and 80 elements subclasses of Bayesian games, finding single Bayes-Nash equilibria for general Bayesian games [10] or approximate Bayes-Nash equilibria [18].",
                "Less attention has been paid to finding the optimal strategy to commit to in a Bayesian game (the Stackelberg scenario [15]).",
                "However, the complexity of this problem was shown to be NP-hard in the general case [5], which also provides algorithms for this problem in the non-Bayesian case.",
                "Therefore, we present a heuristic called ASAP, with three key advantages towards addressing this problem.",
                "First, ASAP searches for the highest reward strategy, rather than a <br>bayes-nash equilibrium</br>, allowing it to find feasible strategies that exploit the natural first-mover advantage of the game.",
                "Second, it provides strategies which are simple to understand, represent, and implement.",
                "Third, it operates directly on the compact, Bayesian game representation, without requiring conversion to normal form.",
                "We provide an efficient Mixed Integer Linear Program (MILP) implementation for ASAP, along with experimental results illustrating significant speedups and higher rewards over other approaches.",
                "Our k-uniform strategies are similar to the k-uniform strategies of [12].",
                "While that work provides epsilon error-bounds based on the k-uniform strategies, their solution concept is still that of a Nash equilibrium, and they do not provide efficient algorithms for obtaining such k-uniform strategies.",
                "This contrasts with ASAP, where our emphasis is on a highly efficient heuristic approach that is not focused on equilibrium solutions.",
                "Finally the patrolling problem which motivated our work has recently received growing attention from the multiagent community due to its wide range of applications [4, 13].",
                "However most of this work is focused on either limiting energy consumption involved in patrolling [7] or optimizing on criteria like the length of the path traveled [4, 13], without reasoning about any explicit model of an adversary[14].",
                "Acknowledgments : This research is supported by the United States Department of Homeland Security through Center for Risk and Economic Analysis of Terrorism Events (CREATE).",
                "It is also supported by the Defense Advanced Research Projects Agency (DARPA), through the Department of the Interior, NBC, Acquisition Services Division, under Contract No.",
                "NBCHD030010.",
                "Sarit Kraus is also affiliated with UMIACS. 8.",
                "REFERENCES [1] R. W. Beard and T. McLain.",
                "Multiple UAV cooperative search under collision avoidance and limited range communication constraints.",
                "In IEEE CDC, 2003. [2] D. Bertsimas and J. Tsitsiklis.",
                "Introduction to Linear Optimization.",
                "Athena Scientific, 1997. [3] J. Brynielsson and S. Arnborg.",
                "Bayesian games for threat prediction and situation analysis.",
                "In FUSION, 2004. [4] Y. Chevaleyre.",
                "Theoretical analysis of multi-agent patrolling problem.",
                "In AAMAS, 2004. [5] V. Conitzer and T. Sandholm.",
                "Choosing the best strategy to commit to.",
                "In ACM Conference on Electronic Commerce, 2006. [6] D. Fudenberg and J. Tirole.",
                "Game Theory.",
                "MIT Press, 1991. [7] C. Gui and P. Mohapatra.",
                "Virtual patrol: A new power conservation design for surveillance using sensor networks.",
                "In IPSN, 2005. [8] J. C. Harsanyi and R. Selten.",
                "A generalized Nash solution for two-person bargaining games with incomplete information.",
                "Management Science, 18(5):80-106, 1972. [9] D. Koller and A. Pfeffer.",
                "Generating and solving imperfect information games.",
                "In IJCAI, pages 1185-1193, 1995. [10] D. Koller and A. Pfeffer.",
                "Representations and solutions for game-theoretic problems.",
                "Artificial Intelligence, 94(1):167-215, 1997. [11] C. Lemke and J. Howson.",
                "Equilibrium points of bimatrix games.",
                "Journal of the Society for Industrial and Applied Mathematics, 12:413-423, 1964. [12] R. J. Lipton, E. Markakis, and A. Mehta.",
                "Playing large games using simple strategies.",
                "In ACM Conference on Electronic Commerce, 2003. [13] A. Machado, G. Ramalho, J. D. Zucker, and A. Drougoul.",
                "Multi-agent patrolling: an empirical analysis on alternative architectures.",
                "In MABS, 2002. [14] P. Paruchuri, M. Tambe, F. Ordonez, and S. Kraus.",
                "Security in multiagent systems by policy randomization.",
                "In AAMAS, 2006. [15] T. Roughgarden.",
                "Stackelberg scheduling strategies.",
                "In ACM Symposium on TOC, 2001. [16] S. Ruan, C. Meirina, F. Yu, K. R. Pattipati, and R. L. Popp.",
                "Patrolling in a stochastic environment.",
                "In 10th Intl.",
                "Command and Control Research Symp., 2005. [17] T. Sandholm, A. Gilpin, and V. Conitzer.",
                "Mixed-integer programming methods for finding nash equilibria.",
                "In AAAI, 2005. [18] S. Singh, V. Soni, and M. Wellman.",
                "Computing approximate Bayes-Nash equilibria with tree-games of incomplete information.",
                "In ACM Conference on Electronic Commerce, 2004. 318 The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07)"
            ],
            "original_annotated_samples": [],
            "translated_annotated_samples": [
                "Primero, lo antes posible busca la estrategia de mayor recompensa, en lugar de un \"equilibrio de bayes-nash\", lo que le permite encontrar estrategias factibles que exploten la ventaja natural del primer movimiento del juego.",
                "Por lo general, estos juegos se analizan de acuerdo con el concepto de solución de un \"equilibrio de bayes-nash\", una extensión del equilibrio de Nash para los juegos bayesianos.",
                "Sin embargo, en muchos entornos, un Nash o \"equilibrio de la Nash Bayes\" no es un concepto de solución apropiado, ya que supone que las estrategias de los agentes se eligen simultáneamente [5].",
                "Si, por otro lado, deseamos calcular el equilibrio NASH de más recompensa, se pueden usar nuevos métodos que utilizan programas lineales de intemperie mixta (MILP) [17], ya que el \"equilibrio de bayes-nash\" de más recompensa es equivalente aEl equilibrio NASH correspondiente en el juego transformado.",
                "Utilizando los datos generados, realizamos los experimentos utilizando cuatro métodos para generar la estrategia de agentes de seguridad: • Aleatización uniforme • Lo antes posible • El método de programas lineales múltiples de [5] (para encontrar la verdadera estrategia óptima) • La más alta recompensa \"Bayes-NashEquilibrio \", encontrado utilizando el algoritmo MIP-Nash [17] Los últimos tres métodos se aplicaron usando CPlex 8.1.",
                "El algoritmo ASAP claramente supera el método óptimo de multiplica, así como el algoritmo MIP-Nash para encontrar el \"equilibrio de Nash Bayes\" más alto con respecto al tiempo de ejecución.",
                "Primero, ASAP busca la estrategia de recompensa más alta, en lugar de un \"equilibrio de bayes-nash\", lo que le permite encontrar estrategias factibles que exploten la ventaja natural del primer movimiento del juego."
            ],
            "translated_text": "",
            "candidates": [
                "Equilibrio de Nash Bayes",
                "equilibrio de bayes-nash",
                "Equilibrio de Nash Bayes",
                "equilibrio de bayes-nash",
                "Equilibrio de Nash Bayes",
                "equilibrio de la Nash Bayes",
                "Equilibrio de Nash Bayes",
                "equilibrio de bayes-nash",
                "Equilibrio de Nash Bayes",
                "Bayes-NashEquilibrio ",
                "Equilibrio de Nash Bayes",
                "equilibrio de Nash Bayes",
                "Equilibrio de Nash Bayes",
                "equilibrio de bayes-nash"
            ],
            "error": []
        },
        "bayesian and stackelberg game": {
            "translated_key": "Juego Bayesian y Stackelberg",
            "is_in_text": false,
            "original_annotated_sentences": [
                "An Efficient Heuristic Approach for Security Against Multiple Adversaries Praveen Paruchuri, Jonathan P. Pearce, Milind Tambe, Fernando Ordonez University of Southern California Los Angeles, CA 90089 {paruchur, jppearce, tambe, fordon}@usc.edu Sarit Kraus Bar-Ilan University Ramat-Gan 52900, Israel sarit@cs.biu.ac.il ABSTRACT In adversarial multiagent domains, security, commonly defined as the ability to deal with intentional threats from other agents, is a critical issue.",
                "This paper focuses on domains where these threats come from unknown adversaries.",
                "These domains can be modeled as Bayesian games; much work has been done on finding equilibria for such games.",
                "However, it is often the case in multiagent security domains that one agent can commit to a mixed strategy which its adversaries observe before choosing their own strategies.",
                "In this case, the agent can maximize reward by finding an optimal strategy, without requiring equilibrium.",
                "Previous work has shown this problem of optimal strategy selection to be NP-hard.",
                "Therefore, we present a heuristic called ASAP, with three key advantages to address the problem.",
                "First, ASAP searches for the highest-reward strategy, rather than a Bayes-Nash equilibrium, allowing it to find feasible strategies that exploit the natural first-mover advantage of the game.",
                "Second, it provides strategies which are simple to understand, represent, and implement.",
                "Third, it operates directly on the compact, Bayesian game representation, without requiring conversion to normal form.",
                "We provide an efficient Mixed Integer Linear Program (MILP) implementation for ASAP, along with experimental results illustrating significant speedups and higher rewards over other approaches.",
                "Categories and Subject Descriptors I.2.11 [Computing Methodologies]: Artificial Intelligence: Distributed Artificial Intelligence - Intelligent Agents General Terms Security, Design, Theory 1.",
                "INTRODUCTION In many multiagent domains, agents must act in order to provide security against attacks by adversaries.",
                "A common issue that agents face in such security domains is uncertainty about the adversaries they may be facing.",
                "For example, a security robot may need to make a choice about which areas to patrol, and how often [16].",
                "However, it will not know in advance exactly where a robber will choose to strike.",
                "A team of unmanned aerial vehicles (UAVs) [1] monitoring a region undergoing a humanitarian crisis may also need to choose a patrolling policy.",
                "They must make this decision without knowing in advance whether terrorists or other adversaries may be waiting to disrupt the mission at a given location.",
                "It may indeed be possible to model the motivations of types of adversaries the agent or agent team is likely to face in order to target these adversaries more closely.",
                "However, in both cases, the security robot or UAV team will not know exactly which kinds of adversaries may be active on any given day.",
                "A common approach for choosing a policy for agents in such scenarios is to model the scenarios as Bayesian games.",
                "A Bayesian game is a game in which agents may belong to one or more types; the type of an agent determines its possible actions and payoffs.",
                "The distribution of adversary types that an agent will face may be known or inferred from historical data.",
                "Usually, these games are analyzed according to the solution concept of a Bayes-Nash equilibrium, an extension of the Nash equilibrium for Bayesian games.",
                "However, in many settings, a Nash or Bayes-Nash equilibrium is not an appropriate solution concept, since it assumes that the agents strategies are chosen simultaneously [5].",
                "In some settings, one player can (or must) commit to a strategy before the other players choose their strategies.",
                "These scenarios are known as Stackelberg games [6].",
                "In a Stackelberg game, a leader commits to a strategy first, and then a follower (or group of followers) selfishly optimize their own rewards, considering the action chosen by the leader.",
                "For example, the security agent (leader) must first commit to a strategy for patrolling various areas.",
                "This strategy could be a mixed strategy in order to be unpredictable to the robbers (followers).",
                "The robbers, after observing the pattern of patrols over time, can then choose their strategy (which location to rob).",
                "Often, the leader in a Stackelberg game can attain a higher reward than if the strategies were chosen simultaneously.",
                "To see the advantage of being the leader in a Stackelberg game, consider a simple game with the payoff table as shown in Table 1.",
                "The leader is the row player and the follower is the column player.",
                "Here, the leaders payoff is listed first. 1 2 3 1 5,5 0,0 3,10 2 0,0 2,2 5,0 Table 1: Payoff table for example normal form game.",
                "The only Nash equilibrium for this game is when the leader plays 2 and the follower plays 2 which gives the leader a payoff of 2. 311 978-81-904262-7-5 (RPS) c 2007 IFAAMAS However, if the leader commits to a uniform mixed strategy of playing 1 and 2 with equal (0.5) probability, the followers best response is to play 3 to get an expected payoff of 5 (10 and 0 with equal probability).",
                "The leaders payoff would then be 4 (3 and 5 with equal probability).",
                "In this case, the leader now has an incentive to deviate and choose a pure strategy of 2 (to get a payoff of 5).",
                "However, this would cause the follower to deviate to strategy 2 as well, resulting in the Nash equilibrium.",
                "Thus, by committing to a strategy that is observed by the follower, and by avoiding the temptation to deviate, the leader manages to obtain a reward higher than that of the best Nash equilibrium.",
                "The problem of choosing an optimal strategy for the leader to commit to in a Stackelberg game is analyzed in [5] and found to be NP-hard in the case of a Bayesian game with multiple types of followers.",
                "Thus, efficient heuristic techniques for choosing highreward strategies in these games is an important open issue.",
                "Methods for finding optimal leader strategies for non-Bayesian games [5] can be applied to this problem by converting the Bayesian game into a normal-form game by the Harsanyi transformation [8].",
                "If, on the other hand, we wish to compute the highest-reward Nash equilibrium, new methods using mixed-integer linear programs (MILPs) [17] may be used, since the highest-reward Bayes-Nash equilibrium is equivalent to the corresponding Nash equilibrium in the transformed game.",
                "However, by transforming the game, the compact structure of the Bayesian game is lost.",
                "In addition, since the Nash equilibrium assumes a simultaneous choice of strategies, the advantages of being the leader are not considered.",
                "This paper introduces an efficient heuristic method for approximating the optimal leader strategy for security domains, known as ASAP (Agent Security via Approximate Policies).",
                "This method has three key advantages.",
                "First, it directly searches for an optimal strategy, rather than a Nash (or Bayes-Nash) equilibrium, thus allowing it to find high-reward non-equilibrium strategies like the one in the above example.",
                "Second, it generates policies with a support which can be expressed as a uniform distribution over a multiset of fixed size as proposed in [12].",
                "This allows for policies that are simple to understand and represent [12], as well as a tunable parameter (the size of the multiset) that controls the simplicity of the policy.",
                "Third, the method allows for a Bayes-Nash game to be expressed compactly without conversion to a normal-form game, allowing for large speedups over existing Nash methods such as [17] and [11].",
                "The rest of the paper is organized as follows.",
                "In Section 2 we fully describe the patrolling domain and its properties.",
                "Section 3 introduces the Bayesian game, the Harsanyi transformation, and existing methods for finding an optimal leaders strategy in a Stackelberg game.",
                "Then, in Section 4 the ASAP algorithm is presented for normal-form games, and in Section 5 we show how it can be adapted to the structure of Bayesian games with uncertain adversaries.",
                "Experimental results showing higher reward and faster policy computation over existing Nash methods are shown in Section 6, and we conclude with a discussion of related work in Section 7. 2.",
                "THE PATROLLING DOMAIN In most security patrolling domains, the security agents (like UAVs [1] or security robots [16]) cannot feasibly patrol all areas all the time.",
                "Instead, they must choose a policy by which they patrol various routes at different times, taking into account factors such as the likelihood of crime in different areas, possible targets for crime, and the security agents own resources (number of security agents, amount of available time, fuel, etc.).",
                "It is usually beneficial for this policy to be nondeterministic so that robbers cannot safely rob certain locations, knowing that they will be safe from the security agents [14].",
                "To demonstrate the utility of our algorithm, we use a simplified version of such a domain, expressed as a game.",
                "The most basic version of our game consists of two players: the security agent (the leader) and the robber (the follower) in a world consisting of m houses, 1 . . . m. The security agents set of pure strategies consists of possible routes of d houses to patrol (in an order).",
                "The security agent can choose a mixed strategy so that the robber will be unsure of exactly where the security agent may patrol, but the robber will know the mixed strategy the security agent has chosen.",
                "For example, the robber can observe over time how often the security agent patrols each area.",
                "With this knowledge, the robber must choose a single house to rob.",
                "We assume that the robber generally takes a long time to rob a house.",
                "If the house chosen by the robber is not on the security agents route, then the robber successfully robs it.",
                "Otherwise, if it is on the security agents route, then the earlier the house is on the route, the easier it is for the security agent to catch the robber before he finishes robbing it.",
                "We model the payoffs for this game with the following variables: • vl,x: value of the goods in house l to the security agent. • vl,q: value of the goods in house l to the robber. • cx: reward to the security agent of catching the robber. • cq: cost to the robber of getting caught. • pl: probability that the security agent can catch the robber at the lth house in the patrol (pl < pl ⇐⇒ l < l).",
                "The security agents set of possible pure strategies (patrol routes) is denoted by X and includes all d-tuples i =< w1, w2, ..., wd > with w1 . . . wd = 1 . . . m where no two elements are equal (the agent is not allowed to return to the same house).",
                "The robbers set of possible pure strategies (houses to rob) is denoted by Q and includes all integers j = 1 . . . m. The payoffs (security agent, robber) for pure strategies i, j are: • −vl,x, vl,q, for j = l /∈ i. • plcx +(1−pl)(−vl,x), −plcq +(1−pl)(vl,q), for j = l ∈ i.",
                "With this structure it is possible to model many different types of robbers who have differing motivations; for example, one robber may have a lower cost of getting caught than another, or may value the goods in the various houses differently.",
                "If the distribution of different robber types is known or inferred from historical data, then the game can be modeled as a Bayesian game [6]. 3.",
                "BAYESIAN GAMES A Bayesian game contains a set of N agents, and each agent n must be one of a given set of types θn.",
                "For our patrolling domain, we have two agents, the security agent and the robber. θ1 is the set of security agent types and θ2 is the set of robber types.",
                "Since there is only one type of security agent, θ1 contains only one element.",
                "During the game, the robber knows its type but the security agent does not know the robbers type.",
                "For each agent (the security agent or the robber) n, there is a set of strategies σn and a utility function un : θ1 × θ2 × σ1 × σ2 → .",
                "A Bayesian game can be transformed into a normal-form game using the Harsanyi transformation [8].",
                "Once this is done, new, linear-program (LP)-based methods for finding high-reward strategies for normal-form games [5] can be used to find a strategy in the transformed game; this strategy can then be used for the Bayesian game.",
                "While methods exist for finding Bayes-Nash equilibria directly, without the Harsanyi transformation [10], they find only a 312 The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) single equilibrium in the general case, which may not be of high reward.",
                "Recent work [17] has led to efficient mixed-integer linear program techniques to find the best Nash equilibrium for a given agent.",
                "However, these techniques do require a normal-form game, and so to compare the policies given by ASAP against the optimal policy, as well as against the highest-reward Nash equilibrium, we must apply these techniques to the Harsanyi-transformed matrix.",
                "The next two subsections elaborate on how this is done. 3.1 Harsanyi Transformation The first step in solving Bayesian games is to apply the Harsanyi transformation [8] that converts the Bayesian game into a normal form game.",
                "Given that the Harsanyi transformation is a standard concept in game theory, we explain it briefly through a simple example in our patrolling domain without introducing the mathematical formulations.",
                "Let us assume there are two robber types a and b in the Bayesian game.",
                "Robber a will be active with probability α, and robber b will be active with probability 1 − α.",
                "The rules described in Section 2 allow us to construct simple payoff tables.",
                "Assume that there are two houses in the world (1 and 2) and hence there are two patrol routes (pure strategies) for the agent: {1,2} and {2,1}.",
                "The robber can rob either house 1 or house 2 and hence he has two strategies (denoted as 1l, 2l for robber type l).",
                "Since there are two types assumed (denoted as a and b), we construct two payoff tables (shown in Table 2) corresponding to the security agent playing a separate game with each of the two robber types with probabilities α and 1 − α.",
                "First, consider robber type a.",
                "Borrowing the notation from the domain section, we assign the following values to the variables: v1,x = v1,q = 3/4, v2,x = v2,q = 1/4, cx = 1/2, cq = 1, p1 = 1, p2 = 1/2.",
                "Using these values we construct a base payoff table as the payoff for the game against robber type a.",
                "For example, if the security agent chooses route {1,2} when robber a is active, and robber a chooses house 1, the robber receives a reward of -1 (for being caught) and the agent receives a reward of 0.5 for catching the robber.",
                "The payoffs for the game against robber type b are constructed using different values.",
                "Security agent: {1,2} {2,1} Robber a 1a -1, .5 -.375, .125 2a -.125, -.125 -1, .5 Robber b 1b -.9, .6 -.275, .225 2b -.025, -.025 -.9, .6 Table 2: Payoff tables: Security Agent vs Robbers a and b Using the Harsanyi technique involves introducing a chance node, that determines the robbers type, thus transforming the security agents incomplete information regarding the robber into imperfect information [3].",
                "The Bayesian equilibrium of the game is then precisely the Nash equilibrium of the imperfect information game.",
                "The transformed, normal-form game is shown in Table 3.",
                "In the transformed game, the security agent is the column player, and the set of all robber types together is the row player.",
                "Suppose that robber type a robs house 1 and robber type b robs house 2, while the security agent chooses patrol {1,2}.",
                "Then, the security agent and the robber receive an expected payoff corresponding to their payoffs from the agent encountering robber a at house 1 with probability α and robber b at house 2 with probability 1 − α. 3.2 Finding an Optimal Strategy Although a Nash equilibrium is the standard solution concept for games in which agents choose strategies simultaneously, in our security domain, the security agent (the leader) can gain an advantage by committing to a mixed strategy in advance.",
                "Since the followers (the robbers) will know the leaders strategy, the optimal response for the followers will be a pure strategy.",
                "Given the common assumption, taken in [5], in the case where followers are indifferent, they will choose the strategy that benefits the leader, there must exist a guaranteed optimal strategy for the leader [5].",
                "From the Bayesian game in Table 2, we constructed the Harsanyi transformed bimatrix in Table 3.",
                "The strategies for each player (security agent or robber) in the transformed game correspond to all combinations of possible strategies taken by each of that players types.",
                "Therefore, we denote X = σθ1 1 = σ1 and Q = σθ2 2 as the index sets of the security agent and robbers pure strategies respectively, with R and C as the corresponding payoff matrices.",
                "Rij is the reward of the security agent and Cij is the reward of the robbers when the security agent takes pure strategy i and the robbers take pure strategy j.",
                "A mixed strategy for the security agent is a probability distribution over its set of pure strategies and will be represented by a vector x = (px1, px2, . . . , px|X|), where pxi ≥ 0 and P pxi = 1.",
                "Here, pxi is the probability that the security agent will choose its ith pure strategy.",
                "The optimal mixed strategy for the security agent can be found in time polynomial in the number of rows in the normal form game using the following linear program formulation from [5].",
                "For every possible pure strategy j by the follower (the set of all robber types), max P i∈X pxiRij s.t. ∀j ∈ Q, P i∈σ1 pxiCij ≥ P i∈σ1 pxiCij P i∈X pxi = 1 ∀i∈X , pxi >= 0 (1) Then, for all feasible follower strategies j, choose the one that maximizes P i∈X pxiRij, the reward for the security agent (leader).",
                "The pxi variables give the optimal strategy for the security agent.",
                "Note that while this method is polynomial in the number of rows in the transformed, normal-form game, the number of rows increases exponentially with the number of robber types.",
                "Using this method for a Bayesian game thus requires running |σ2||θ2| separate linear programs.",
                "This is no surprise, since finding the leaders optimal strategy in a Bayesian Stackelberg game is NP-hard [5]. 4.",
                "HEURISTIC APPROACHES Given that finding the optimal strategy for the leader is NP-hard, we provide a heuristic approach.",
                "In this heuristic we limit the possible mixed strategies of the leader to select actions with probabilities that are integer multiples of 1/k for a predetermined integer k. Previous work [14] has shown that strategies with high entropy are beneficial for security applications when opponents utilities are completely unknown.",
                "In our domain, if utilities are not considered, this method will result in uniform-distribution strategies.",
                "One advantage of such strategies is that they are compact to represent (as fractions) and simple to understand; therefore they can be efficiently implemented by real organizations.",
                "We aim to maintain the advantage provided by simple strategies for our security application problem, incorporating the effect of the robbers rewards on the security agents rewards.",
                "Thus, the ASAP heuristic will produce strategies which are k-uniform.",
                "A mixed strategy is denoted k-uniform if it is a uniform distribution on a multiset S of pure strategies with |S| = k. A multiset is a set whose elements may be repeated multiple times; thus, for example, the mixed strategy corresponding to the multiset {1, 1, 2} would take strategy 1 The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) 313 {1,2} {2,1} {1a, 1b} −1α − .9(1 − α), .5α + .6(1 − α) −.375α − .275(1 − α), .125α + .225(1 − α) {1a, 2b} −1α − .025(1 − α), .5α − .025(1 − α) −.375α − .9(1 − α), .125α + .6(1 − α) {2a, 1b} −.125α − .9(1 − α), −.125α + .6(1 − α) −1α − .275(1 − α), .5α + .225(1 − α) {2a, 2b} −.125α − .025(1 − α), −.125α − .025(1 − α) −1α − .9(1 − α), .5α + .6(1 − α) Table 3: Harsanyi Transformed Payoff Table with probability 2/3 and strategy 2 with probability 1/3.",
                "ASAP allows the size of the multiset to be chosen in order to balance the complexity of the strategy reached with the goal that the identified strategy will yield a high reward.",
                "Another advantage of the ASAP heuristic is that it operates directly on the compact Bayesian representation, without requiring the Harsanyi transformation.",
                "This is because the different follower (robber) types are independent of each other.",
                "Hence, evaluating the leader strategy against a Harsanyi-transformed game matrix is equivalent to evaluating against each of the game matrices for the individual follower types.",
                "This independence property is exploited in ASAP to yield a decomposition scheme.",
                "Note that the LP method introduced by [5] to compute optimal Stackelberg policies is unlikely to be decomposable into a small number of games as it was shown to be NP-hard for Bayes-Nash problems.",
                "Finally, note that ASAP requires the solution of only one optimization problem, rather than solving a series of problems as in the LP method of [5].",
                "For a single follower type, the algorithm works the following way.",
                "Given a particular k, for each possible mixed strategy x for the leader that corresponds to a multiset of size k, evaluate the leaders payoff from x when the follower plays a reward-maximizing pure strategy.",
                "We then take the mixed strategy with the highest payoff.",
                "We need only to consider the reward-maximizing pure strategies of the followers (robbers), since for a given fixed strategy x of the security agent, each robber type faces a problem with fixed linear rewards.",
                "If a mixed strategy is optimal for the robber, then so are all the pure strategies in the support of that mixed strategy.",
                "Note also that because we limit the leaders strategies to take on discrete values, the assumption from Section 3.2 that the followers will break ties in the leaders favor is not significant, since ties will be unlikely to arise.",
                "This is because, in domains where rewards are drawn from any random distribution, the probability of a follower having more than one pure optimal response to a given leader strategy approaches zero, and the leader will have only a finite number of possible mixed strategies.",
                "Our approach to characterize the optimal strategy for the security agent makes use of properties of linear programming.",
                "We briefly outline these results here for completeness, for detailed discussion and proofs see one of many references on the topic, such as [2].",
                "Every linear programming problem, such as: max cT x | Ax = b, x ≥ 0, has an associated dual linear program, in this case: min bT y | AT y ≥ c. These primal/dual pairs of problems satisfy weak duality: For any x and y primal and dual feasible solutions respectively, cT x ≤ bT y.",
                "Thus a pair of feasible solutions is optimal if cT x = bT y, and the problems are said to satisfy strong duality.",
                "In fact if a linear program is feasible and has a bounded optimal solution, then the dual is also feasible and there is a pair x∗ , y∗ that satisfies cT x∗ = bT y∗ .",
                "These optimal solutions are characterized with the following optimality conditions (as defined in [2]): • primal feasibility: Ax = b, x ≥ 0 • dual feasibility: AT y ≥ c • complementary slackness: xi(AT y − c)i = 0 for all i.",
                "Note that this last condition implies that cT x = xT AT y = bT y, which proves optimality for primal dual feasible solutions x and y.",
                "In the following subsections, we first define the problem in its most intuititive form as a mixed-integer quadratic program (MIQP), and then show how this problem can be converted into a mixedinteger linear program (MILP). 4.1 Mixed-Integer Quadratic Program We begin with the case of a single type of follower.",
                "Let the leader be the row player and the follower the column player.",
                "We denote by x the vector of strategies of the leader and q the vector of strategies of the follower.",
                "We also denote X and Q the index sets of the leader and followers pure strategies, respectively.",
                "The payoff matrices R and C correspond to: Rij is the reward of the leader and Cij is the reward of the follower when the leader takes pure strategy i and the follower takes pure strategy j.",
                "Let k be the size of the multiset.",
                "We first fix the policy of the leader to some k-uniform policy x.",
                "The value xi is the number of times pure strategy i is used in the k-uniform policy, which is selected with probability xi/k.",
                "We formulate the optimization problem the follower solves to find its optimal response to x as the following linear program: max X j∈Q X i∈X 1 k Cijxi qj s.t.",
                "P j∈Q qj = 1 q ≥ 0. (2) The objective function maximizes the followers expected reward given x, while the constraints make feasible any mixed strategy q for the follower.",
                "The dual to this linear programming problem is the following: min a s.t. a ≥ X i∈X 1 k Cijxi j ∈ Q. (3) From strong duality and complementary slackness we obtain that the followers maximum reward value, a, is the value of every pure strategy with qj > 0, that is in the support of the optimal mixed strategy.",
                "Therefore each of these pure strategies is optimal.",
                "Optimal solutions to the followers problem are characterized by linear programming optimality conditions: primal feasibility constraints in (2), dual feasibility constraints in (3), and complementary slackness qj a − X i∈X 1 k Cijxi ! = 0 j ∈ Q.",
                "These conditions must be included in the problem solved by the leader in order to consider only best responses by the follower to the k-uniform policy x. 314 The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) The leader seeks the k-uniform solution x that maximizes its own payoff, given that the follower uses an optimal response q(x).",
                "Therefore the leader solves the following integer problem: max X i∈X X j∈Q 1 k Rijq(x)j xi s.t.",
                "P i∈X xi = k xi ∈ {0, 1, . . . , k}. (4) Problem (4) maximizes the leaders reward with the followers best response (qj for fixed leaders policy x and hence denoted q(x)j) by selecting a uniform policy from a multiset of constant size k. We complete this problem by including the characterization of q(x) through linear programming optimality conditions.",
                "To simplify writing the complementary slackness conditions, we will constrain q(x) to be only optimal pure strategies by just considering integer solutions of q(x).",
                "The leaders problem becomes: maxx,q X i∈X X j∈Q 1 k Rijxiqj s.t.",
                "P i xi = kP j∈Q qj = 1 0 ≤ (a − P i∈X 1 k Cijxi) ≤ (1 − qj)M xi ∈ {0, 1, ...., k} qj ∈ {0, 1}. (5) Here, the constant M is some large number.",
                "The first and fourth constraints enforce a k-uniform policy for the leader, and the second and fifth constraints enforce a feasible pure strategy for the follower.",
                "The third constraint enforces dual feasibility of the followers problem (leftmost inequality) and the complementary slackness constraint for an optimal pure strategy q for the follower (rightmost inequality).",
                "In fact, since only one pure strategy can be selected by the follower, say qh = 1, this last constraint enforces that a = P i∈X 1 k Cihxi imposing no additional constraint for all other pure strategies which have qj = 0.",
                "We conclude this subsection noting that Problem (5) is an integer program with a non-convex quadratic objective in general, as the matrix R need not be positive-semi-definite.",
                "Efficient solution methods for non-linear, non-convex integer problems remains a challenging research question.",
                "In the next section we show a reformulation of this problem as a linear integer programming problem, for which a number of efficient commercial solvers exist. 4.2 Mixed-Integer Linear Program We can linearize the quadratic program of Problem 5 through the change of variables zij = xiqj, obtaining the following problem maxq,z P i∈X P j∈Q 1 k Rijzij s.t.",
                "P i∈X P j∈Q zij = k P j∈Q zij ≤ k kqj ≤ P i∈X zij ≤ k P j∈Q qj = 1 0 ≤ (a − P i∈X 1 k Cij( P h∈Q zih)) ≤ (1 − qj)M zij ∈ {0, 1, ...., k} qj ∈ {0, 1} (6) PROPOSITION 1.",
                "Problems (5) and (6) are equivalent.",
                "Proof: Consider x, q a feasible solution of (5).",
                "We will show that q, zij = xiqj is a feasible solution of (6) of same objective function value.",
                "The equivalence of the objective functions, and constraints 4, 6 and 7 of (6) are satisfied by construction.",
                "The fact that P j∈Q zij = xi as P j∈Q qj = 1 explains constraints 1, 2, and 5 of (6).",
                "Constraint 3 of (6) is satisfied because P i∈X zij = kqj.",
                "Let us now consider q, z feasible for (6).",
                "We will show that q and xi = P j∈Q zij are feasible for (5) with the same objective value.",
                "In fact all constraints of (5) are readily satisfied by construction.",
                "To see that the objectives match, notice that if qh = 1 then the third constraint in (6) implies that P i∈X zih = k, which means that zij = 0 for all i ∈ X and all j = h. Therefore, xiqj = X l∈Q zilqj = zihqj = zij.",
                "This last equality is because both are 0 when j = h. This shows that the transformation preserves the objective function value, completing the proof.",
                "Given this transformation to a mixed-integer linear program (MILP), we now show how we can apply our decomposition technique on the MILP to obtain significant speedups for Bayesian games with multiple follower types. 5.",
                "DECOMPOSITION FOR MULTIPLE ADVERSARIES The MILP developed in the previous section handles only one follower.",
                "Since our security scenario contains multiple follower (robber) types, we change the response function for the follower from a pure strategy into a weighted combination over various pure follower strategies where the weights are probabilities of occurrence of each of the follower types. 5.1 Decomposed MIQP To admit multiple adversaries in our framework, we modify the notation defined in the previous section to reason about multiple follower types.",
                "We denote by x the vector of strategies of the leader and ql the vector of strategies of follower l, with L denoting the index set of follower types.",
                "We also denote by X and Q the index sets of leader and follower ls pure strategies, respectively.",
                "We also index the payoff matrices on each follower l, considering the matrices Rl and Cl .",
                "Using this modified notation, we characterize the optimal solution of follower ls problem given the leaders k-uniform policy x, with the following optimality conditions: X j∈Q ql j = 1 al − X i∈X 1 k Cl ijxi ≥ 0 ql j(al − X i∈X 1 k Cl ijxi) = 0 ql j ≥ 0 Again, considering only optimal pure strategies for follower ls problem we can linearize the complementarity constraint above.",
                "We incorporate these constraints on the leaders problem that selects the optimal k-uniform policy.",
                "Therefore, given a priori probabilities pl , with l ∈ L of facing each follower, the leader solves the following problem: The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) 315 maxx,q X i∈X X l∈L X j∈Q pl k Rl ijxiql j s.t.",
                "P i xi = kP j∈Q ql j = 1 0 ≤ (al − P i∈X 1 k Cl ijxi) ≤ (1 − ql j)M xi ∈ {0, 1, ...., k} ql j ∈ {0, 1}. (7) Problem (7) for a Bayesian game with multiple follower types is indeed equivalent to Problem (5) on the payoff matrix obtained from the Harsanyi transformation of the game.",
                "In fact, every pure strategy j in Problem (5) corresponds to a sequence of pure strategies jl, one for each follower l ∈ L. This means that qj = 1 if and only if ql jl = 1 for all l ∈ L. In addition, given the a priori probabilities pl of facing player l, the reward in the Harsanyi transformation payoff table is Rij = P l∈L pl Rl ijl .",
                "The same relation holds between C and Cl .",
                "These relations between a pure strategy in the equivalent normal form game and pure strategies in the individual games with each followers are key in showing these problems are equivalent. 5.2 Decomposed MILP We can linearize the quadratic programming problem 7 through the change of variables zl ij = xiql j, obtaining the following problem maxq,z P i∈X P l∈L P j∈Q pl k Rl ijzl ij s.t.",
                "P i∈X P j∈Q zl ij = k P j∈Q zl ij ≤ k kql j ≤ P i∈X zl ij ≤ k P j∈Q ql j = 1 0 ≤ (al − P i∈X 1 k Cl ij( P h∈Q zl ih)) ≤ (1 − ql j)M P j∈Q zl ij = P j∈Q z1 ij zl ij ∈ {0, 1, ...., k} ql j ∈ {0, 1} (8) PROPOSITION 2.",
                "Problems (7) and (8) are equivalent.",
                "Proof: Consider x, ql , al with l ∈ L a feasible solution of (7).",
                "We will show that ql , al , zl ij = xiql j is a feasible solution of (8) of same objective function value.",
                "The equivalence of the objective functions, and constraints 4, 7 and 8 of (8) are satisfied by construction.",
                "The fact that P j∈Q zl ij = xi as P j∈Q ql j = 1 explains constraints 1, 2, 5 and 6 of (8).",
                "Constraint 3 of (8) is satisfied because P i∈X zl ij = kql j.",
                "Lets now consider ql , zl , al feasible for (8).",
                "We will show that ql , al and xi = P j∈Q z1 ij are feasible for (7) with the same objective value.",
                "In fact all constraints of (7) are readily satisfied by construction.",
                "To see that the objectives match, notice for each l one ql j must equal 1 and the rest equal 0.",
                "Let us say that ql jl = 1, then the third constraint in (8) implies that P i∈X zl ijl = k, which means that zl ij = 0 for all i ∈ X and all j = jl.",
                "In particular this implies that xi = X j∈Q z1 ij = z1 ij1 = zl ijl , the last equality from constraint 6 of (8).",
                "Therefore xiql j = zl ijl ql j = zl ij.",
                "This last equality is because both are 0 when j = jl.",
                "Effectively, constraint 6 ensures that all the adversaries are calculating their best responses against a particular fixed policy of the agent.",
                "This shows that the transformation preserves the objective function value, completing the proof.",
                "We can therefore solve this equivalent linear integer program with efficient integer programming packages which can handle problems with thousands of integer variables.",
                "We implemented the decomposed MILP and the results are shown in the following section. 6.",
                "EXPERIMENTAL RESULTS The patrolling domain and the payoffs for the associated game are detailed in Sections 2 and 3.",
                "We performed experiments for this game in worlds of three and four houses with patrols consisting of two houses.",
                "The description given in Section 2 is used to generate a base case for both the security agent and robber payoff functions.",
                "The payoff tables for additional robber types are constructed and added to the game by adding a random distribution of varying size to the payoffs in the base case.",
                "All games are normalized so that, for each robber type, the minimum and maximum payoffs to the security agent and robber are 0 and 1, respectively.",
                "Using the data generated, we performed the experiments using four methods for generating the security agents strategy: • uniform randomization • ASAP • the multiple linear programs method from [5] (to find the true optimal strategy) • the highest reward Bayes-Nash equilibrium, found using the MIP-Nash algorithm [17] The last three methods were applied using CPLEX 8.1.",
                "Because the last two methods are designed for normal-form games rather than Bayesian games, the games were first converted using the Harsanyi transformation [8].",
                "The uniform randomization method is simply choosing a uniform random policy over all possible patrol routes.",
                "We use this method as a simple baseline to measure the performance of our heuristics.",
                "We anticipated that the uniform policy would perform reasonably well since maximum-entropy policies have been shown to be effective in multiagent security domains [14].",
                "The highest-reward Bayes-Nash equilibria were used in order to demonstrate the higher reward gained by looking for an optimal policy rather than an equilibria in Stackelberg games such as our security domain.",
                "Based on our experiments we present three sets of graphs to demonstrate (1) the runtime of ASAP compared to other common methods for finding a strategy, (2) the reward guaranteed by ASAP compared to other methods, and (3) the effect of varying the parameter k, the size of the multiset, on the performance of ASAP.",
                "In the first two sets of graphs, ASAP is run using a multiset of 80 elements; in the third set this number is varied.",
                "The first set of graphs, shown in Figure 1 shows the runtime graphs for three-house (left column) and four-house (right column) domains.",
                "Each of the three rows of graphs corresponds to a different randomly-generated scenario.",
                "The x-axis shows the number of robber types the security agent faces and the y-axis of the graph shows the runtime in seconds.",
                "All experiments that were not concluded in 30 minutes (1800 seconds) were cut off.",
                "The runtime for the uniform policy is always negligible irrespective of the number of adversaries and hence is not shown.",
                "The ASAP algorithm clearly outperforms the optimal, multipleLP method as well as the MIP-Nash algorithm for finding the highestreward Bayes-Nash equilibrium with respect to runtime.",
                "For a 316 The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) Figure 1: Runtimes for various algorithms on problems of 3 and 4 houses. domain of three houses, the optimal method cannot reach a solution for more than seven robber types, and for four houses it cannot solve for more than six types within the cutoff time in any of the three scenarios.",
                "MIP-Nash solves for even fewer robber types within the cutoff time.",
                "On the other hand, ASAP runs much faster, and is able to solve for at least 20 adversaries for the three-house scenarios and for at least 12 adversaries in the four-house scenarios within the cutoff time.",
                "The runtime of ASAP does not increase strictly with the number of robber types for each scenario, but in general, the addition of more types increases the runtime required.",
                "The second set of graphs, Figure 2, shows the reward to the patrol agent given by each method for three scenarios in the three-house (left column) and four-house (right column) domains.",
                "This reward is the utility received by the security agent in the patrolling game, and not as a percentage of the optimal reward, since it was not possible to obtain the optimal reward as the number of robber types increased.",
                "The uniform policy consistently provides the lowest reward in both domains; while the optimal method of course produces the optimal reward.",
                "The ASAP method remains consistently close to the optimal, even as the number of robber types increases.",
                "The highest-reward Bayes-Nash equilibria, provided by the MIPNash method, produced rewards higher than the uniform method, but lower than ASAP.",
                "This difference clearly illustrates the gains in the patrolling domain from committing to a strategy as the leader in a Stackelberg game, rather than playing a standard Bayes-Nash strategy.",
                "The third set of graphs, shown in Figure 3 shows the effect of the multiset size on runtime in seconds (left column) and reward (right column), again expressed as the reward received by the security agent in the patrolling game, and not a percentage of the optimal Figure 2: Reward for various algorithms on problems of 3 and 4 houses. reward.",
                "Results here are for the three-house domain.",
                "The trend is that as as the multiset size is increased, the runtime and reward level both increase.",
                "Not surprisingly, the reward increases monotonically as the multiset size increases, but what is interesting is that there is relatively little benefit to using a large multiset in this domain.",
                "In all cases, the reward given by a multiset of 10 elements was within at least 96% of the reward given by an 80-element multiset.",
                "The runtime does not always increase strictly with the multiset size; indeed in one example (scenario 2 with 20 robber types), using a multiset of 10 elements took 1228 seconds, while using 80 elements only took 617 seconds.",
                "In general, runtime should increase since a larger multiset means a larger domain for the variables in the MILP, and thus a larger search space.",
                "However, an increase in the number of variables can sometimes allow for a policy to be constructed more quickly due to more flexibility in the problem. 7.",
                "SUMMARY AND RELATED WORK This paper focuses on security for agents patrolling in hostile environments.",
                "In these environments, intentional threats are caused by adversaries about whom the security patrolling agents have incomplete information.",
                "Specifically, we deal with situations where the adversaries actions and payoffs are known but the exact adversary type is unknown to the security agent.",
                "Agents acting in the real world quite frequently have such incomplete information about other agents.",
                "Bayesian games have been a popular choice to model such incomplete information games [3].",
                "The Gala toolkit is one method for defining such games [9] without requiring the game to be represented in normal form via the Harsanyi transformation [8]; Galas guarantees are focused on fully competitive games.",
                "Much work has been done on finding optimal Bayes-Nash equilbria for The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) 317 Figure 3: Reward for ASAP using multisets of 10, 30, and 80 elements subclasses of Bayesian games, finding single Bayes-Nash equilibria for general Bayesian games [10] or approximate Bayes-Nash equilibria [18].",
                "Less attention has been paid to finding the optimal strategy to commit to in a Bayesian game (the Stackelberg scenario [15]).",
                "However, the complexity of this problem was shown to be NP-hard in the general case [5], which also provides algorithms for this problem in the non-Bayesian case.",
                "Therefore, we present a heuristic called ASAP, with three key advantages towards addressing this problem.",
                "First, ASAP searches for the highest reward strategy, rather than a Bayes-Nash equilibrium, allowing it to find feasible strategies that exploit the natural first-mover advantage of the game.",
                "Second, it provides strategies which are simple to understand, represent, and implement.",
                "Third, it operates directly on the compact, Bayesian game representation, without requiring conversion to normal form.",
                "We provide an efficient Mixed Integer Linear Program (MILP) implementation for ASAP, along with experimental results illustrating significant speedups and higher rewards over other approaches.",
                "Our k-uniform strategies are similar to the k-uniform strategies of [12].",
                "While that work provides epsilon error-bounds based on the k-uniform strategies, their solution concept is still that of a Nash equilibrium, and they do not provide efficient algorithms for obtaining such k-uniform strategies.",
                "This contrasts with ASAP, where our emphasis is on a highly efficient heuristic approach that is not focused on equilibrium solutions.",
                "Finally the patrolling problem which motivated our work has recently received growing attention from the multiagent community due to its wide range of applications [4, 13].",
                "However most of this work is focused on either limiting energy consumption involved in patrolling [7] or optimizing on criteria like the length of the path traveled [4, 13], without reasoning about any explicit model of an adversary[14].",
                "Acknowledgments : This research is supported by the United States Department of Homeland Security through Center for Risk and Economic Analysis of Terrorism Events (CREATE).",
                "It is also supported by the Defense Advanced Research Projects Agency (DARPA), through the Department of the Interior, NBC, Acquisition Services Division, under Contract No.",
                "NBCHD030010.",
                "Sarit Kraus is also affiliated with UMIACS. 8.",
                "REFERENCES [1] R. W. Beard and T. McLain.",
                "Multiple UAV cooperative search under collision avoidance and limited range communication constraints.",
                "In IEEE CDC, 2003. [2] D. Bertsimas and J. Tsitsiklis.",
                "Introduction to Linear Optimization.",
                "Athena Scientific, 1997. [3] J. Brynielsson and S. Arnborg.",
                "Bayesian games for threat prediction and situation analysis.",
                "In FUSION, 2004. [4] Y. Chevaleyre.",
                "Theoretical analysis of multi-agent patrolling problem.",
                "In AAMAS, 2004. [5] V. Conitzer and T. Sandholm.",
                "Choosing the best strategy to commit to.",
                "In ACM Conference on Electronic Commerce, 2006. [6] D. Fudenberg and J. Tirole.",
                "Game Theory.",
                "MIT Press, 1991. [7] C. Gui and P. Mohapatra.",
                "Virtual patrol: A new power conservation design for surveillance using sensor networks.",
                "In IPSN, 2005. [8] J. C. Harsanyi and R. Selten.",
                "A generalized Nash solution for two-person bargaining games with incomplete information.",
                "Management Science, 18(5):80-106, 1972. [9] D. Koller and A. Pfeffer.",
                "Generating and solving imperfect information games.",
                "In IJCAI, pages 1185-1193, 1995. [10] D. Koller and A. Pfeffer.",
                "Representations and solutions for game-theoretic problems.",
                "Artificial Intelligence, 94(1):167-215, 1997. [11] C. Lemke and J. Howson.",
                "Equilibrium points of bimatrix games.",
                "Journal of the Society for Industrial and Applied Mathematics, 12:413-423, 1964. [12] R. J. Lipton, E. Markakis, and A. Mehta.",
                "Playing large games using simple strategies.",
                "In ACM Conference on Electronic Commerce, 2003. [13] A. Machado, G. Ramalho, J. D. Zucker, and A. Drougoul.",
                "Multi-agent patrolling: an empirical analysis on alternative architectures.",
                "In MABS, 2002. [14] P. Paruchuri, M. Tambe, F. Ordonez, and S. Kraus.",
                "Security in multiagent systems by policy randomization.",
                "In AAMAS, 2006. [15] T. Roughgarden.",
                "Stackelberg scheduling strategies.",
                "In ACM Symposium on TOC, 2001. [16] S. Ruan, C. Meirina, F. Yu, K. R. Pattipati, and R. L. Popp.",
                "Patrolling in a stochastic environment.",
                "In 10th Intl.",
                "Command and Control Research Symp., 2005. [17] T. Sandholm, A. Gilpin, and V. Conitzer.",
                "Mixed-integer programming methods for finding nash equilibria.",
                "In AAAI, 2005. [18] S. Singh, V. Soni, and M. Wellman.",
                "Computing approximate Bayes-Nash equilibria with tree-games of incomplete information.",
                "In ACM Conference on Electronic Commerce, 2004. 318 The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07)"
            ],
            "original_annotated_samples": [],
            "translated_annotated_samples": [],
            "translated_text": "",
            "candidates": [],
            "error": []
        },
        "np-hard": {
            "translated_key": "np difícil",
            "is_in_text": true,
            "original_annotated_sentences": [
                "An Efficient Heuristic Approach for Security Against Multiple Adversaries Praveen Paruchuri, Jonathan P. Pearce, Milind Tambe, Fernando Ordonez University of Southern California Los Angeles, CA 90089 {paruchur, jppearce, tambe, fordon}@usc.edu Sarit Kraus Bar-Ilan University Ramat-Gan 52900, Israel sarit@cs.biu.ac.il ABSTRACT In adversarial multiagent domains, security, commonly defined as the ability to deal with intentional threats from other agents, is a critical issue.",
                "This paper focuses on domains where these threats come from unknown adversaries.",
                "These domains can be modeled as Bayesian games; much work has been done on finding equilibria for such games.",
                "However, it is often the case in multiagent security domains that one agent can commit to a mixed strategy which its adversaries observe before choosing their own strategies.",
                "In this case, the agent can maximize reward by finding an optimal strategy, without requiring equilibrium.",
                "Previous work has shown this problem of optimal strategy selection to be <br>np-hard</br>.",
                "Therefore, we present a heuristic called ASAP, with three key advantages to address the problem.",
                "First, ASAP searches for the highest-reward strategy, rather than a Bayes-Nash equilibrium, allowing it to find feasible strategies that exploit the natural first-mover advantage of the game.",
                "Second, it provides strategies which are simple to understand, represent, and implement.",
                "Third, it operates directly on the compact, Bayesian game representation, without requiring conversion to normal form.",
                "We provide an efficient Mixed Integer Linear Program (MILP) implementation for ASAP, along with experimental results illustrating significant speedups and higher rewards over other approaches.",
                "Categories and Subject Descriptors I.2.11 [Computing Methodologies]: Artificial Intelligence: Distributed Artificial Intelligence - Intelligent Agents General Terms Security, Design, Theory 1.",
                "INTRODUCTION In many multiagent domains, agents must act in order to provide security against attacks by adversaries.",
                "A common issue that agents face in such security domains is uncertainty about the adversaries they may be facing.",
                "For example, a security robot may need to make a choice about which areas to patrol, and how often [16].",
                "However, it will not know in advance exactly where a robber will choose to strike.",
                "A team of unmanned aerial vehicles (UAVs) [1] monitoring a region undergoing a humanitarian crisis may also need to choose a patrolling policy.",
                "They must make this decision without knowing in advance whether terrorists or other adversaries may be waiting to disrupt the mission at a given location.",
                "It may indeed be possible to model the motivations of types of adversaries the agent or agent team is likely to face in order to target these adversaries more closely.",
                "However, in both cases, the security robot or UAV team will not know exactly which kinds of adversaries may be active on any given day.",
                "A common approach for choosing a policy for agents in such scenarios is to model the scenarios as Bayesian games.",
                "A Bayesian game is a game in which agents may belong to one or more types; the type of an agent determines its possible actions and payoffs.",
                "The distribution of adversary types that an agent will face may be known or inferred from historical data.",
                "Usually, these games are analyzed according to the solution concept of a Bayes-Nash equilibrium, an extension of the Nash equilibrium for Bayesian games.",
                "However, in many settings, a Nash or Bayes-Nash equilibrium is not an appropriate solution concept, since it assumes that the agents strategies are chosen simultaneously [5].",
                "In some settings, one player can (or must) commit to a strategy before the other players choose their strategies.",
                "These scenarios are known as Stackelberg games [6].",
                "In a Stackelberg game, a leader commits to a strategy first, and then a follower (or group of followers) selfishly optimize their own rewards, considering the action chosen by the leader.",
                "For example, the security agent (leader) must first commit to a strategy for patrolling various areas.",
                "This strategy could be a mixed strategy in order to be unpredictable to the robbers (followers).",
                "The robbers, after observing the pattern of patrols over time, can then choose their strategy (which location to rob).",
                "Often, the leader in a Stackelberg game can attain a higher reward than if the strategies were chosen simultaneously.",
                "To see the advantage of being the leader in a Stackelberg game, consider a simple game with the payoff table as shown in Table 1.",
                "The leader is the row player and the follower is the column player.",
                "Here, the leaders payoff is listed first. 1 2 3 1 5,5 0,0 3,10 2 0,0 2,2 5,0 Table 1: Payoff table for example normal form game.",
                "The only Nash equilibrium for this game is when the leader plays 2 and the follower plays 2 which gives the leader a payoff of 2. 311 978-81-904262-7-5 (RPS) c 2007 IFAAMAS However, if the leader commits to a uniform mixed strategy of playing 1 and 2 with equal (0.5) probability, the followers best response is to play 3 to get an expected payoff of 5 (10 and 0 with equal probability).",
                "The leaders payoff would then be 4 (3 and 5 with equal probability).",
                "In this case, the leader now has an incentive to deviate and choose a pure strategy of 2 (to get a payoff of 5).",
                "However, this would cause the follower to deviate to strategy 2 as well, resulting in the Nash equilibrium.",
                "Thus, by committing to a strategy that is observed by the follower, and by avoiding the temptation to deviate, the leader manages to obtain a reward higher than that of the best Nash equilibrium.",
                "The problem of choosing an optimal strategy for the leader to commit to in a Stackelberg game is analyzed in [5] and found to be <br>np-hard</br> in the case of a Bayesian game with multiple types of followers.",
                "Thus, efficient heuristic techniques for choosing highreward strategies in these games is an important open issue.",
                "Methods for finding optimal leader strategies for non-Bayesian games [5] can be applied to this problem by converting the Bayesian game into a normal-form game by the Harsanyi transformation [8].",
                "If, on the other hand, we wish to compute the highest-reward Nash equilibrium, new methods using mixed-integer linear programs (MILPs) [17] may be used, since the highest-reward Bayes-Nash equilibrium is equivalent to the corresponding Nash equilibrium in the transformed game.",
                "However, by transforming the game, the compact structure of the Bayesian game is lost.",
                "In addition, since the Nash equilibrium assumes a simultaneous choice of strategies, the advantages of being the leader are not considered.",
                "This paper introduces an efficient heuristic method for approximating the optimal leader strategy for security domains, known as ASAP (Agent Security via Approximate Policies).",
                "This method has three key advantages.",
                "First, it directly searches for an optimal strategy, rather than a Nash (or Bayes-Nash) equilibrium, thus allowing it to find high-reward non-equilibrium strategies like the one in the above example.",
                "Second, it generates policies with a support which can be expressed as a uniform distribution over a multiset of fixed size as proposed in [12].",
                "This allows for policies that are simple to understand and represent [12], as well as a tunable parameter (the size of the multiset) that controls the simplicity of the policy.",
                "Third, the method allows for a Bayes-Nash game to be expressed compactly without conversion to a normal-form game, allowing for large speedups over existing Nash methods such as [17] and [11].",
                "The rest of the paper is organized as follows.",
                "In Section 2 we fully describe the patrolling domain and its properties.",
                "Section 3 introduces the Bayesian game, the Harsanyi transformation, and existing methods for finding an optimal leaders strategy in a Stackelberg game.",
                "Then, in Section 4 the ASAP algorithm is presented for normal-form games, and in Section 5 we show how it can be adapted to the structure of Bayesian games with uncertain adversaries.",
                "Experimental results showing higher reward and faster policy computation over existing Nash methods are shown in Section 6, and we conclude with a discussion of related work in Section 7. 2.",
                "THE PATROLLING DOMAIN In most security patrolling domains, the security agents (like UAVs [1] or security robots [16]) cannot feasibly patrol all areas all the time.",
                "Instead, they must choose a policy by which they patrol various routes at different times, taking into account factors such as the likelihood of crime in different areas, possible targets for crime, and the security agents own resources (number of security agents, amount of available time, fuel, etc.).",
                "It is usually beneficial for this policy to be nondeterministic so that robbers cannot safely rob certain locations, knowing that they will be safe from the security agents [14].",
                "To demonstrate the utility of our algorithm, we use a simplified version of such a domain, expressed as a game.",
                "The most basic version of our game consists of two players: the security agent (the leader) and the robber (the follower) in a world consisting of m houses, 1 . . . m. The security agents set of pure strategies consists of possible routes of d houses to patrol (in an order).",
                "The security agent can choose a mixed strategy so that the robber will be unsure of exactly where the security agent may patrol, but the robber will know the mixed strategy the security agent has chosen.",
                "For example, the robber can observe over time how often the security agent patrols each area.",
                "With this knowledge, the robber must choose a single house to rob.",
                "We assume that the robber generally takes a long time to rob a house.",
                "If the house chosen by the robber is not on the security agents route, then the robber successfully robs it.",
                "Otherwise, if it is on the security agents route, then the earlier the house is on the route, the easier it is for the security agent to catch the robber before he finishes robbing it.",
                "We model the payoffs for this game with the following variables: • vl,x: value of the goods in house l to the security agent. • vl,q: value of the goods in house l to the robber. • cx: reward to the security agent of catching the robber. • cq: cost to the robber of getting caught. • pl: probability that the security agent can catch the robber at the lth house in the patrol (pl < pl ⇐⇒ l < l).",
                "The security agents set of possible pure strategies (patrol routes) is denoted by X and includes all d-tuples i =< w1, w2, ..., wd > with w1 . . . wd = 1 . . . m where no two elements are equal (the agent is not allowed to return to the same house).",
                "The robbers set of possible pure strategies (houses to rob) is denoted by Q and includes all integers j = 1 . . . m. The payoffs (security agent, robber) for pure strategies i, j are: • −vl,x, vl,q, for j = l /∈ i. • plcx +(1−pl)(−vl,x), −plcq +(1−pl)(vl,q), for j = l ∈ i.",
                "With this structure it is possible to model many different types of robbers who have differing motivations; for example, one robber may have a lower cost of getting caught than another, or may value the goods in the various houses differently.",
                "If the distribution of different robber types is known or inferred from historical data, then the game can be modeled as a Bayesian game [6]. 3.",
                "BAYESIAN GAMES A Bayesian game contains a set of N agents, and each agent n must be one of a given set of types θn.",
                "For our patrolling domain, we have two agents, the security agent and the robber. θ1 is the set of security agent types and θ2 is the set of robber types.",
                "Since there is only one type of security agent, θ1 contains only one element.",
                "During the game, the robber knows its type but the security agent does not know the robbers type.",
                "For each agent (the security agent or the robber) n, there is a set of strategies σn and a utility function un : θ1 × θ2 × σ1 × σ2 → .",
                "A Bayesian game can be transformed into a normal-form game using the Harsanyi transformation [8].",
                "Once this is done, new, linear-program (LP)-based methods for finding high-reward strategies for normal-form games [5] can be used to find a strategy in the transformed game; this strategy can then be used for the Bayesian game.",
                "While methods exist for finding Bayes-Nash equilibria directly, without the Harsanyi transformation [10], they find only a 312 The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) single equilibrium in the general case, which may not be of high reward.",
                "Recent work [17] has led to efficient mixed-integer linear program techniques to find the best Nash equilibrium for a given agent.",
                "However, these techniques do require a normal-form game, and so to compare the policies given by ASAP against the optimal policy, as well as against the highest-reward Nash equilibrium, we must apply these techniques to the Harsanyi-transformed matrix.",
                "The next two subsections elaborate on how this is done. 3.1 Harsanyi Transformation The first step in solving Bayesian games is to apply the Harsanyi transformation [8] that converts the Bayesian game into a normal form game.",
                "Given that the Harsanyi transformation is a standard concept in game theory, we explain it briefly through a simple example in our patrolling domain without introducing the mathematical formulations.",
                "Let us assume there are two robber types a and b in the Bayesian game.",
                "Robber a will be active with probability α, and robber b will be active with probability 1 − α.",
                "The rules described in Section 2 allow us to construct simple payoff tables.",
                "Assume that there are two houses in the world (1 and 2) and hence there are two patrol routes (pure strategies) for the agent: {1,2} and {2,1}.",
                "The robber can rob either house 1 or house 2 and hence he has two strategies (denoted as 1l, 2l for robber type l).",
                "Since there are two types assumed (denoted as a and b), we construct two payoff tables (shown in Table 2) corresponding to the security agent playing a separate game with each of the two robber types with probabilities α and 1 − α.",
                "First, consider robber type a.",
                "Borrowing the notation from the domain section, we assign the following values to the variables: v1,x = v1,q = 3/4, v2,x = v2,q = 1/4, cx = 1/2, cq = 1, p1 = 1, p2 = 1/2.",
                "Using these values we construct a base payoff table as the payoff for the game against robber type a.",
                "For example, if the security agent chooses route {1,2} when robber a is active, and robber a chooses house 1, the robber receives a reward of -1 (for being caught) and the agent receives a reward of 0.5 for catching the robber.",
                "The payoffs for the game against robber type b are constructed using different values.",
                "Security agent: {1,2} {2,1} Robber a 1a -1, .5 -.375, .125 2a -.125, -.125 -1, .5 Robber b 1b -.9, .6 -.275, .225 2b -.025, -.025 -.9, .6 Table 2: Payoff tables: Security Agent vs Robbers a and b Using the Harsanyi technique involves introducing a chance node, that determines the robbers type, thus transforming the security agents incomplete information regarding the robber into imperfect information [3].",
                "The Bayesian equilibrium of the game is then precisely the Nash equilibrium of the imperfect information game.",
                "The transformed, normal-form game is shown in Table 3.",
                "In the transformed game, the security agent is the column player, and the set of all robber types together is the row player.",
                "Suppose that robber type a robs house 1 and robber type b robs house 2, while the security agent chooses patrol {1,2}.",
                "Then, the security agent and the robber receive an expected payoff corresponding to their payoffs from the agent encountering robber a at house 1 with probability α and robber b at house 2 with probability 1 − α. 3.2 Finding an Optimal Strategy Although a Nash equilibrium is the standard solution concept for games in which agents choose strategies simultaneously, in our security domain, the security agent (the leader) can gain an advantage by committing to a mixed strategy in advance.",
                "Since the followers (the robbers) will know the leaders strategy, the optimal response for the followers will be a pure strategy.",
                "Given the common assumption, taken in [5], in the case where followers are indifferent, they will choose the strategy that benefits the leader, there must exist a guaranteed optimal strategy for the leader [5].",
                "From the Bayesian game in Table 2, we constructed the Harsanyi transformed bimatrix in Table 3.",
                "The strategies for each player (security agent or robber) in the transformed game correspond to all combinations of possible strategies taken by each of that players types.",
                "Therefore, we denote X = σθ1 1 = σ1 and Q = σθ2 2 as the index sets of the security agent and robbers pure strategies respectively, with R and C as the corresponding payoff matrices.",
                "Rij is the reward of the security agent and Cij is the reward of the robbers when the security agent takes pure strategy i and the robbers take pure strategy j.",
                "A mixed strategy for the security agent is a probability distribution over its set of pure strategies and will be represented by a vector x = (px1, px2, . . . , px|X|), where pxi ≥ 0 and P pxi = 1.",
                "Here, pxi is the probability that the security agent will choose its ith pure strategy.",
                "The optimal mixed strategy for the security agent can be found in time polynomial in the number of rows in the normal form game using the following linear program formulation from [5].",
                "For every possible pure strategy j by the follower (the set of all robber types), max P i∈X pxiRij s.t. ∀j ∈ Q, P i∈σ1 pxiCij ≥ P i∈σ1 pxiCij P i∈X pxi = 1 ∀i∈X , pxi >= 0 (1) Then, for all feasible follower strategies j, choose the one that maximizes P i∈X pxiRij, the reward for the security agent (leader).",
                "The pxi variables give the optimal strategy for the security agent.",
                "Note that while this method is polynomial in the number of rows in the transformed, normal-form game, the number of rows increases exponentially with the number of robber types.",
                "Using this method for a Bayesian game thus requires running |σ2||θ2| separate linear programs.",
                "This is no surprise, since finding the leaders optimal strategy in a Bayesian Stackelberg game is <br>np-hard</br> [5]. 4.",
                "HEURISTIC APPROACHES Given that finding the optimal strategy for the leader is <br>np-hard</br>, we provide a heuristic approach.",
                "In this heuristic we limit the possible mixed strategies of the leader to select actions with probabilities that are integer multiples of 1/k for a predetermined integer k. Previous work [14] has shown that strategies with high entropy are beneficial for security applications when opponents utilities are completely unknown.",
                "In our domain, if utilities are not considered, this method will result in uniform-distribution strategies.",
                "One advantage of such strategies is that they are compact to represent (as fractions) and simple to understand; therefore they can be efficiently implemented by real organizations.",
                "We aim to maintain the advantage provided by simple strategies for our security application problem, incorporating the effect of the robbers rewards on the security agents rewards.",
                "Thus, the ASAP heuristic will produce strategies which are k-uniform.",
                "A mixed strategy is denoted k-uniform if it is a uniform distribution on a multiset S of pure strategies with |S| = k. A multiset is a set whose elements may be repeated multiple times; thus, for example, the mixed strategy corresponding to the multiset {1, 1, 2} would take strategy 1 The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) 313 {1,2} {2,1} {1a, 1b} −1α − .9(1 − α), .5α + .6(1 − α) −.375α − .275(1 − α), .125α + .225(1 − α) {1a, 2b} −1α − .025(1 − α), .5α − .025(1 − α) −.375α − .9(1 − α), .125α + .6(1 − α) {2a, 1b} −.125α − .9(1 − α), −.125α + .6(1 − α) −1α − .275(1 − α), .5α + .225(1 − α) {2a, 2b} −.125α − .025(1 − α), −.125α − .025(1 − α) −1α − .9(1 − α), .5α + .6(1 − α) Table 3: Harsanyi Transformed Payoff Table with probability 2/3 and strategy 2 with probability 1/3.",
                "ASAP allows the size of the multiset to be chosen in order to balance the complexity of the strategy reached with the goal that the identified strategy will yield a high reward.",
                "Another advantage of the ASAP heuristic is that it operates directly on the compact Bayesian representation, without requiring the Harsanyi transformation.",
                "This is because the different follower (robber) types are independent of each other.",
                "Hence, evaluating the leader strategy against a Harsanyi-transformed game matrix is equivalent to evaluating against each of the game matrices for the individual follower types.",
                "This independence property is exploited in ASAP to yield a decomposition scheme.",
                "Note that the LP method introduced by [5] to compute optimal Stackelberg policies is unlikely to be decomposable into a small number of games as it was shown to be <br>np-hard</br> for Bayes-Nash problems.",
                "Finally, note that ASAP requires the solution of only one optimization problem, rather than solving a series of problems as in the LP method of [5].",
                "For a single follower type, the algorithm works the following way.",
                "Given a particular k, for each possible mixed strategy x for the leader that corresponds to a multiset of size k, evaluate the leaders payoff from x when the follower plays a reward-maximizing pure strategy.",
                "We then take the mixed strategy with the highest payoff.",
                "We need only to consider the reward-maximizing pure strategies of the followers (robbers), since for a given fixed strategy x of the security agent, each robber type faces a problem with fixed linear rewards.",
                "If a mixed strategy is optimal for the robber, then so are all the pure strategies in the support of that mixed strategy.",
                "Note also that because we limit the leaders strategies to take on discrete values, the assumption from Section 3.2 that the followers will break ties in the leaders favor is not significant, since ties will be unlikely to arise.",
                "This is because, in domains where rewards are drawn from any random distribution, the probability of a follower having more than one pure optimal response to a given leader strategy approaches zero, and the leader will have only a finite number of possible mixed strategies.",
                "Our approach to characterize the optimal strategy for the security agent makes use of properties of linear programming.",
                "We briefly outline these results here for completeness, for detailed discussion and proofs see one of many references on the topic, such as [2].",
                "Every linear programming problem, such as: max cT x | Ax = b, x ≥ 0, has an associated dual linear program, in this case: min bT y | AT y ≥ c. These primal/dual pairs of problems satisfy weak duality: For any x and y primal and dual feasible solutions respectively, cT x ≤ bT y.",
                "Thus a pair of feasible solutions is optimal if cT x = bT y, and the problems are said to satisfy strong duality.",
                "In fact if a linear program is feasible and has a bounded optimal solution, then the dual is also feasible and there is a pair x∗ , y∗ that satisfies cT x∗ = bT y∗ .",
                "These optimal solutions are characterized with the following optimality conditions (as defined in [2]): • primal feasibility: Ax = b, x ≥ 0 • dual feasibility: AT y ≥ c • complementary slackness: xi(AT y − c)i = 0 for all i.",
                "Note that this last condition implies that cT x = xT AT y = bT y, which proves optimality for primal dual feasible solutions x and y.",
                "In the following subsections, we first define the problem in its most intuititive form as a mixed-integer quadratic program (MIQP), and then show how this problem can be converted into a mixedinteger linear program (MILP). 4.1 Mixed-Integer Quadratic Program We begin with the case of a single type of follower.",
                "Let the leader be the row player and the follower the column player.",
                "We denote by x the vector of strategies of the leader and q the vector of strategies of the follower.",
                "We also denote X and Q the index sets of the leader and followers pure strategies, respectively.",
                "The payoff matrices R and C correspond to: Rij is the reward of the leader and Cij is the reward of the follower when the leader takes pure strategy i and the follower takes pure strategy j.",
                "Let k be the size of the multiset.",
                "We first fix the policy of the leader to some k-uniform policy x.",
                "The value xi is the number of times pure strategy i is used in the k-uniform policy, which is selected with probability xi/k.",
                "We formulate the optimization problem the follower solves to find its optimal response to x as the following linear program: max X j∈Q X i∈X 1 k Cijxi qj s.t.",
                "P j∈Q qj = 1 q ≥ 0. (2) The objective function maximizes the followers expected reward given x, while the constraints make feasible any mixed strategy q for the follower.",
                "The dual to this linear programming problem is the following: min a s.t. a ≥ X i∈X 1 k Cijxi j ∈ Q. (3) From strong duality and complementary slackness we obtain that the followers maximum reward value, a, is the value of every pure strategy with qj > 0, that is in the support of the optimal mixed strategy.",
                "Therefore each of these pure strategies is optimal.",
                "Optimal solutions to the followers problem are characterized by linear programming optimality conditions: primal feasibility constraints in (2), dual feasibility constraints in (3), and complementary slackness qj a − X i∈X 1 k Cijxi ! = 0 j ∈ Q.",
                "These conditions must be included in the problem solved by the leader in order to consider only best responses by the follower to the k-uniform policy x. 314 The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) The leader seeks the k-uniform solution x that maximizes its own payoff, given that the follower uses an optimal response q(x).",
                "Therefore the leader solves the following integer problem: max X i∈X X j∈Q 1 k Rijq(x)j xi s.t.",
                "P i∈X xi = k xi ∈ {0, 1, . . . , k}. (4) Problem (4) maximizes the leaders reward with the followers best response (qj for fixed leaders policy x and hence denoted q(x)j) by selecting a uniform policy from a multiset of constant size k. We complete this problem by including the characterization of q(x) through linear programming optimality conditions.",
                "To simplify writing the complementary slackness conditions, we will constrain q(x) to be only optimal pure strategies by just considering integer solutions of q(x).",
                "The leaders problem becomes: maxx,q X i∈X X j∈Q 1 k Rijxiqj s.t.",
                "P i xi = kP j∈Q qj = 1 0 ≤ (a − P i∈X 1 k Cijxi) ≤ (1 − qj)M xi ∈ {0, 1, ...., k} qj ∈ {0, 1}. (5) Here, the constant M is some large number.",
                "The first and fourth constraints enforce a k-uniform policy for the leader, and the second and fifth constraints enforce a feasible pure strategy for the follower.",
                "The third constraint enforces dual feasibility of the followers problem (leftmost inequality) and the complementary slackness constraint for an optimal pure strategy q for the follower (rightmost inequality).",
                "In fact, since only one pure strategy can be selected by the follower, say qh = 1, this last constraint enforces that a = P i∈X 1 k Cihxi imposing no additional constraint for all other pure strategies which have qj = 0.",
                "We conclude this subsection noting that Problem (5) is an integer program with a non-convex quadratic objective in general, as the matrix R need not be positive-semi-definite.",
                "Efficient solution methods for non-linear, non-convex integer problems remains a challenging research question.",
                "In the next section we show a reformulation of this problem as a linear integer programming problem, for which a number of efficient commercial solvers exist. 4.2 Mixed-Integer Linear Program We can linearize the quadratic program of Problem 5 through the change of variables zij = xiqj, obtaining the following problem maxq,z P i∈X P j∈Q 1 k Rijzij s.t.",
                "P i∈X P j∈Q zij = k P j∈Q zij ≤ k kqj ≤ P i∈X zij ≤ k P j∈Q qj = 1 0 ≤ (a − P i∈X 1 k Cij( P h∈Q zih)) ≤ (1 − qj)M zij ∈ {0, 1, ...., k} qj ∈ {0, 1} (6) PROPOSITION 1.",
                "Problems (5) and (6) are equivalent.",
                "Proof: Consider x, q a feasible solution of (5).",
                "We will show that q, zij = xiqj is a feasible solution of (6) of same objective function value.",
                "The equivalence of the objective functions, and constraints 4, 6 and 7 of (6) are satisfied by construction.",
                "The fact that P j∈Q zij = xi as P j∈Q qj = 1 explains constraints 1, 2, and 5 of (6).",
                "Constraint 3 of (6) is satisfied because P i∈X zij = kqj.",
                "Let us now consider q, z feasible for (6).",
                "We will show that q and xi = P j∈Q zij are feasible for (5) with the same objective value.",
                "In fact all constraints of (5) are readily satisfied by construction.",
                "To see that the objectives match, notice that if qh = 1 then the third constraint in (6) implies that P i∈X zih = k, which means that zij = 0 for all i ∈ X and all j = h. Therefore, xiqj = X l∈Q zilqj = zihqj = zij.",
                "This last equality is because both are 0 when j = h. This shows that the transformation preserves the objective function value, completing the proof.",
                "Given this transformation to a mixed-integer linear program (MILP), we now show how we can apply our decomposition technique on the MILP to obtain significant speedups for Bayesian games with multiple follower types. 5.",
                "DECOMPOSITION FOR MULTIPLE ADVERSARIES The MILP developed in the previous section handles only one follower.",
                "Since our security scenario contains multiple follower (robber) types, we change the response function for the follower from a pure strategy into a weighted combination over various pure follower strategies where the weights are probabilities of occurrence of each of the follower types. 5.1 Decomposed MIQP To admit multiple adversaries in our framework, we modify the notation defined in the previous section to reason about multiple follower types.",
                "We denote by x the vector of strategies of the leader and ql the vector of strategies of follower l, with L denoting the index set of follower types.",
                "We also denote by X and Q the index sets of leader and follower ls pure strategies, respectively.",
                "We also index the payoff matrices on each follower l, considering the matrices Rl and Cl .",
                "Using this modified notation, we characterize the optimal solution of follower ls problem given the leaders k-uniform policy x, with the following optimality conditions: X j∈Q ql j = 1 al − X i∈X 1 k Cl ijxi ≥ 0 ql j(al − X i∈X 1 k Cl ijxi) = 0 ql j ≥ 0 Again, considering only optimal pure strategies for follower ls problem we can linearize the complementarity constraint above.",
                "We incorporate these constraints on the leaders problem that selects the optimal k-uniform policy.",
                "Therefore, given a priori probabilities pl , with l ∈ L of facing each follower, the leader solves the following problem: The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) 315 maxx,q X i∈X X l∈L X j∈Q pl k Rl ijxiql j s.t.",
                "P i xi = kP j∈Q ql j = 1 0 ≤ (al − P i∈X 1 k Cl ijxi) ≤ (1 − ql j)M xi ∈ {0, 1, ...., k} ql j ∈ {0, 1}. (7) Problem (7) for a Bayesian game with multiple follower types is indeed equivalent to Problem (5) on the payoff matrix obtained from the Harsanyi transformation of the game.",
                "In fact, every pure strategy j in Problem (5) corresponds to a sequence of pure strategies jl, one for each follower l ∈ L. This means that qj = 1 if and only if ql jl = 1 for all l ∈ L. In addition, given the a priori probabilities pl of facing player l, the reward in the Harsanyi transformation payoff table is Rij = P l∈L pl Rl ijl .",
                "The same relation holds between C and Cl .",
                "These relations between a pure strategy in the equivalent normal form game and pure strategies in the individual games with each followers are key in showing these problems are equivalent. 5.2 Decomposed MILP We can linearize the quadratic programming problem 7 through the change of variables zl ij = xiql j, obtaining the following problem maxq,z P i∈X P l∈L P j∈Q pl k Rl ijzl ij s.t.",
                "P i∈X P j∈Q zl ij = k P j∈Q zl ij ≤ k kql j ≤ P i∈X zl ij ≤ k P j∈Q ql j = 1 0 ≤ (al − P i∈X 1 k Cl ij( P h∈Q zl ih)) ≤ (1 − ql j)M P j∈Q zl ij = P j∈Q z1 ij zl ij ∈ {0, 1, ...., k} ql j ∈ {0, 1} (8) PROPOSITION 2.",
                "Problems (7) and (8) are equivalent.",
                "Proof: Consider x, ql , al with l ∈ L a feasible solution of (7).",
                "We will show that ql , al , zl ij = xiql j is a feasible solution of (8) of same objective function value.",
                "The equivalence of the objective functions, and constraints 4, 7 and 8 of (8) are satisfied by construction.",
                "The fact that P j∈Q zl ij = xi as P j∈Q ql j = 1 explains constraints 1, 2, 5 and 6 of (8).",
                "Constraint 3 of (8) is satisfied because P i∈X zl ij = kql j.",
                "Lets now consider ql , zl , al feasible for (8).",
                "We will show that ql , al and xi = P j∈Q z1 ij are feasible for (7) with the same objective value.",
                "In fact all constraints of (7) are readily satisfied by construction.",
                "To see that the objectives match, notice for each l one ql j must equal 1 and the rest equal 0.",
                "Let us say that ql jl = 1, then the third constraint in (8) implies that P i∈X zl ijl = k, which means that zl ij = 0 for all i ∈ X and all j = jl.",
                "In particular this implies that xi = X j∈Q z1 ij = z1 ij1 = zl ijl , the last equality from constraint 6 of (8).",
                "Therefore xiql j = zl ijl ql j = zl ij.",
                "This last equality is because both are 0 when j = jl.",
                "Effectively, constraint 6 ensures that all the adversaries are calculating their best responses against a particular fixed policy of the agent.",
                "This shows that the transformation preserves the objective function value, completing the proof.",
                "We can therefore solve this equivalent linear integer program with efficient integer programming packages which can handle problems with thousands of integer variables.",
                "We implemented the decomposed MILP and the results are shown in the following section. 6.",
                "EXPERIMENTAL RESULTS The patrolling domain and the payoffs for the associated game are detailed in Sections 2 and 3.",
                "We performed experiments for this game in worlds of three and four houses with patrols consisting of two houses.",
                "The description given in Section 2 is used to generate a base case for both the security agent and robber payoff functions.",
                "The payoff tables for additional robber types are constructed and added to the game by adding a random distribution of varying size to the payoffs in the base case.",
                "All games are normalized so that, for each robber type, the minimum and maximum payoffs to the security agent and robber are 0 and 1, respectively.",
                "Using the data generated, we performed the experiments using four methods for generating the security agents strategy: • uniform randomization • ASAP • the multiple linear programs method from [5] (to find the true optimal strategy) • the highest reward Bayes-Nash equilibrium, found using the MIP-Nash algorithm [17] The last three methods were applied using CPLEX 8.1.",
                "Because the last two methods are designed for normal-form games rather than Bayesian games, the games were first converted using the Harsanyi transformation [8].",
                "The uniform randomization method is simply choosing a uniform random policy over all possible patrol routes.",
                "We use this method as a simple baseline to measure the performance of our heuristics.",
                "We anticipated that the uniform policy would perform reasonably well since maximum-entropy policies have been shown to be effective in multiagent security domains [14].",
                "The highest-reward Bayes-Nash equilibria were used in order to demonstrate the higher reward gained by looking for an optimal policy rather than an equilibria in Stackelberg games such as our security domain.",
                "Based on our experiments we present three sets of graphs to demonstrate (1) the runtime of ASAP compared to other common methods for finding a strategy, (2) the reward guaranteed by ASAP compared to other methods, and (3) the effect of varying the parameter k, the size of the multiset, on the performance of ASAP.",
                "In the first two sets of graphs, ASAP is run using a multiset of 80 elements; in the third set this number is varied.",
                "The first set of graphs, shown in Figure 1 shows the runtime graphs for three-house (left column) and four-house (right column) domains.",
                "Each of the three rows of graphs corresponds to a different randomly-generated scenario.",
                "The x-axis shows the number of robber types the security agent faces and the y-axis of the graph shows the runtime in seconds.",
                "All experiments that were not concluded in 30 minutes (1800 seconds) were cut off.",
                "The runtime for the uniform policy is always negligible irrespective of the number of adversaries and hence is not shown.",
                "The ASAP algorithm clearly outperforms the optimal, multipleLP method as well as the MIP-Nash algorithm for finding the highestreward Bayes-Nash equilibrium with respect to runtime.",
                "For a 316 The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) Figure 1: Runtimes for various algorithms on problems of 3 and 4 houses. domain of three houses, the optimal method cannot reach a solution for more than seven robber types, and for four houses it cannot solve for more than six types within the cutoff time in any of the three scenarios.",
                "MIP-Nash solves for even fewer robber types within the cutoff time.",
                "On the other hand, ASAP runs much faster, and is able to solve for at least 20 adversaries for the three-house scenarios and for at least 12 adversaries in the four-house scenarios within the cutoff time.",
                "The runtime of ASAP does not increase strictly with the number of robber types for each scenario, but in general, the addition of more types increases the runtime required.",
                "The second set of graphs, Figure 2, shows the reward to the patrol agent given by each method for three scenarios in the three-house (left column) and four-house (right column) domains.",
                "This reward is the utility received by the security agent in the patrolling game, and not as a percentage of the optimal reward, since it was not possible to obtain the optimal reward as the number of robber types increased.",
                "The uniform policy consistently provides the lowest reward in both domains; while the optimal method of course produces the optimal reward.",
                "The ASAP method remains consistently close to the optimal, even as the number of robber types increases.",
                "The highest-reward Bayes-Nash equilibria, provided by the MIPNash method, produced rewards higher than the uniform method, but lower than ASAP.",
                "This difference clearly illustrates the gains in the patrolling domain from committing to a strategy as the leader in a Stackelberg game, rather than playing a standard Bayes-Nash strategy.",
                "The third set of graphs, shown in Figure 3 shows the effect of the multiset size on runtime in seconds (left column) and reward (right column), again expressed as the reward received by the security agent in the patrolling game, and not a percentage of the optimal Figure 2: Reward for various algorithms on problems of 3 and 4 houses. reward.",
                "Results here are for the three-house domain.",
                "The trend is that as as the multiset size is increased, the runtime and reward level both increase.",
                "Not surprisingly, the reward increases monotonically as the multiset size increases, but what is interesting is that there is relatively little benefit to using a large multiset in this domain.",
                "In all cases, the reward given by a multiset of 10 elements was within at least 96% of the reward given by an 80-element multiset.",
                "The runtime does not always increase strictly with the multiset size; indeed in one example (scenario 2 with 20 robber types), using a multiset of 10 elements took 1228 seconds, while using 80 elements only took 617 seconds.",
                "In general, runtime should increase since a larger multiset means a larger domain for the variables in the MILP, and thus a larger search space.",
                "However, an increase in the number of variables can sometimes allow for a policy to be constructed more quickly due to more flexibility in the problem. 7.",
                "SUMMARY AND RELATED WORK This paper focuses on security for agents patrolling in hostile environments.",
                "In these environments, intentional threats are caused by adversaries about whom the security patrolling agents have incomplete information.",
                "Specifically, we deal with situations where the adversaries actions and payoffs are known but the exact adversary type is unknown to the security agent.",
                "Agents acting in the real world quite frequently have such incomplete information about other agents.",
                "Bayesian games have been a popular choice to model such incomplete information games [3].",
                "The Gala toolkit is one method for defining such games [9] without requiring the game to be represented in normal form via the Harsanyi transformation [8]; Galas guarantees are focused on fully competitive games.",
                "Much work has been done on finding optimal Bayes-Nash equilbria for The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) 317 Figure 3: Reward for ASAP using multisets of 10, 30, and 80 elements subclasses of Bayesian games, finding single Bayes-Nash equilibria for general Bayesian games [10] or approximate Bayes-Nash equilibria [18].",
                "Less attention has been paid to finding the optimal strategy to commit to in a Bayesian game (the Stackelberg scenario [15]).",
                "However, the complexity of this problem was shown to be <br>np-hard</br> in the general case [5], which also provides algorithms for this problem in the non-Bayesian case.",
                "Therefore, we present a heuristic called ASAP, with three key advantages towards addressing this problem.",
                "First, ASAP searches for the highest reward strategy, rather than a Bayes-Nash equilibrium, allowing it to find feasible strategies that exploit the natural first-mover advantage of the game.",
                "Second, it provides strategies which are simple to understand, represent, and implement.",
                "Third, it operates directly on the compact, Bayesian game representation, without requiring conversion to normal form.",
                "We provide an efficient Mixed Integer Linear Program (MILP) implementation for ASAP, along with experimental results illustrating significant speedups and higher rewards over other approaches.",
                "Our k-uniform strategies are similar to the k-uniform strategies of [12].",
                "While that work provides epsilon error-bounds based on the k-uniform strategies, their solution concept is still that of a Nash equilibrium, and they do not provide efficient algorithms for obtaining such k-uniform strategies.",
                "This contrasts with ASAP, where our emphasis is on a highly efficient heuristic approach that is not focused on equilibrium solutions.",
                "Finally the patrolling problem which motivated our work has recently received growing attention from the multiagent community due to its wide range of applications [4, 13].",
                "However most of this work is focused on either limiting energy consumption involved in patrolling [7] or optimizing on criteria like the length of the path traveled [4, 13], without reasoning about any explicit model of an adversary[14].",
                "Acknowledgments : This research is supported by the United States Department of Homeland Security through Center for Risk and Economic Analysis of Terrorism Events (CREATE).",
                "It is also supported by the Defense Advanced Research Projects Agency (DARPA), through the Department of the Interior, NBC, Acquisition Services Division, under Contract No.",
                "NBCHD030010.",
                "Sarit Kraus is also affiliated with UMIACS. 8.",
                "REFERENCES [1] R. W. Beard and T. McLain.",
                "Multiple UAV cooperative search under collision avoidance and limited range communication constraints.",
                "In IEEE CDC, 2003. [2] D. Bertsimas and J. Tsitsiklis.",
                "Introduction to Linear Optimization.",
                "Athena Scientific, 1997. [3] J. Brynielsson and S. Arnborg.",
                "Bayesian games for threat prediction and situation analysis.",
                "In FUSION, 2004. [4] Y. Chevaleyre.",
                "Theoretical analysis of multi-agent patrolling problem.",
                "In AAMAS, 2004. [5] V. Conitzer and T. Sandholm.",
                "Choosing the best strategy to commit to.",
                "In ACM Conference on Electronic Commerce, 2006. [6] D. Fudenberg and J. Tirole.",
                "Game Theory.",
                "MIT Press, 1991. [7] C. Gui and P. Mohapatra.",
                "Virtual patrol: A new power conservation design for surveillance using sensor networks.",
                "In IPSN, 2005. [8] J. C. Harsanyi and R. Selten.",
                "A generalized Nash solution for two-person bargaining games with incomplete information.",
                "Management Science, 18(5):80-106, 1972. [9] D. Koller and A. Pfeffer.",
                "Generating and solving imperfect information games.",
                "In IJCAI, pages 1185-1193, 1995. [10] D. Koller and A. Pfeffer.",
                "Representations and solutions for game-theoretic problems.",
                "Artificial Intelligence, 94(1):167-215, 1997. [11] C. Lemke and J. Howson.",
                "Equilibrium points of bimatrix games.",
                "Journal of the Society for Industrial and Applied Mathematics, 12:413-423, 1964. [12] R. J. Lipton, E. Markakis, and A. Mehta.",
                "Playing large games using simple strategies.",
                "In ACM Conference on Electronic Commerce, 2003. [13] A. Machado, G. Ramalho, J. D. Zucker, and A. Drougoul.",
                "Multi-agent patrolling: an empirical analysis on alternative architectures.",
                "In MABS, 2002. [14] P. Paruchuri, M. Tambe, F. Ordonez, and S. Kraus.",
                "Security in multiagent systems by policy randomization.",
                "In AAMAS, 2006. [15] T. Roughgarden.",
                "Stackelberg scheduling strategies.",
                "In ACM Symposium on TOC, 2001. [16] S. Ruan, C. Meirina, F. Yu, K. R. Pattipati, and R. L. Popp.",
                "Patrolling in a stochastic environment.",
                "In 10th Intl.",
                "Command and Control Research Symp., 2005. [17] T. Sandholm, A. Gilpin, and V. Conitzer.",
                "Mixed-integer programming methods for finding nash equilibria.",
                "In AAAI, 2005. [18] S. Singh, V. Soni, and M. Wellman.",
                "Computing approximate Bayes-Nash equilibria with tree-games of incomplete information.",
                "In ACM Conference on Electronic Commerce, 2004. 318 The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07)"
            ],
            "original_annotated_samples": [],
            "translated_annotated_samples": [
                "El trabajo anterior ha demostrado que este problema de la selección de estrategia óptima es \"NP-HARD\".",
                "El problema de elegir una estrategia óptima para que el líder se comprometa en un juego de Stackelberg se analiza en [5] y se encuentra que es \"NP-Hard\" en el caso de un juego bayesiano con múltiples tipos de seguidores.",
                "Esto no es sorprendente, ya que encontrar la estrategia óptima de los líderes en un juego bayesiano de Stackelberg es \"NP-Hard\" [5].4.",
                "Enfoques heurísticos Dados que encontrar la estrategia óptima para el líder es \"NP-Hard\", proporcionamos un enfoque heurístico.",
                "Tenga en cuenta que el método LP introducido por [5] para calcular políticas óptimas de Stackelberg es poco probable que se descomponga en un pequeño número de juegos, ya que se demostró que era \"NP-Hard\" para los problemas de Bayes-Nash.",
                "Sin embargo, se demostró que la complejidad de este problema es \"NP-Hard\" en el caso general [5], que también proporciona algoritmos para este problema en el caso no bayesiano."
            ],
            "translated_text": "",
            "candidates": [
                "np duro",
                "NP-HARD",
                "np duro",
                "NP-Hard",
                "NP-Hard",
                "NP-Hard",
                "np duro",
                "NP-Hard",
                "np duro",
                "NP-Hard",
                "np duro",
                "NP-Hard"
            ],
            "error": []
        },
        "patrolling domain": {
            "translated_key": "dominio de patrulla",
            "is_in_text": true,
            "original_annotated_sentences": [
                "An Efficient Heuristic Approach for Security Against Multiple Adversaries Praveen Paruchuri, Jonathan P. Pearce, Milind Tambe, Fernando Ordonez University of Southern California Los Angeles, CA 90089 {paruchur, jppearce, tambe, fordon}@usc.edu Sarit Kraus Bar-Ilan University Ramat-Gan 52900, Israel sarit@cs.biu.ac.il ABSTRACT In adversarial multiagent domains, security, commonly defined as the ability to deal with intentional threats from other agents, is a critical issue.",
                "This paper focuses on domains where these threats come from unknown adversaries.",
                "These domains can be modeled as Bayesian games; much work has been done on finding equilibria for such games.",
                "However, it is often the case in multiagent security domains that one agent can commit to a mixed strategy which its adversaries observe before choosing their own strategies.",
                "In this case, the agent can maximize reward by finding an optimal strategy, without requiring equilibrium.",
                "Previous work has shown this problem of optimal strategy selection to be NP-hard.",
                "Therefore, we present a heuristic called ASAP, with three key advantages to address the problem.",
                "First, ASAP searches for the highest-reward strategy, rather than a Bayes-Nash equilibrium, allowing it to find feasible strategies that exploit the natural first-mover advantage of the game.",
                "Second, it provides strategies which are simple to understand, represent, and implement.",
                "Third, it operates directly on the compact, Bayesian game representation, without requiring conversion to normal form.",
                "We provide an efficient Mixed Integer Linear Program (MILP) implementation for ASAP, along with experimental results illustrating significant speedups and higher rewards over other approaches.",
                "Categories and Subject Descriptors I.2.11 [Computing Methodologies]: Artificial Intelligence: Distributed Artificial Intelligence - Intelligent Agents General Terms Security, Design, Theory 1.",
                "INTRODUCTION In many multiagent domains, agents must act in order to provide security against attacks by adversaries.",
                "A common issue that agents face in such security domains is uncertainty about the adversaries they may be facing.",
                "For example, a security robot may need to make a choice about which areas to patrol, and how often [16].",
                "However, it will not know in advance exactly where a robber will choose to strike.",
                "A team of unmanned aerial vehicles (UAVs) [1] monitoring a region undergoing a humanitarian crisis may also need to choose a patrolling policy.",
                "They must make this decision without knowing in advance whether terrorists or other adversaries may be waiting to disrupt the mission at a given location.",
                "It may indeed be possible to model the motivations of types of adversaries the agent or agent team is likely to face in order to target these adversaries more closely.",
                "However, in both cases, the security robot or UAV team will not know exactly which kinds of adversaries may be active on any given day.",
                "A common approach for choosing a policy for agents in such scenarios is to model the scenarios as Bayesian games.",
                "A Bayesian game is a game in which agents may belong to one or more types; the type of an agent determines its possible actions and payoffs.",
                "The distribution of adversary types that an agent will face may be known or inferred from historical data.",
                "Usually, these games are analyzed according to the solution concept of a Bayes-Nash equilibrium, an extension of the Nash equilibrium for Bayesian games.",
                "However, in many settings, a Nash or Bayes-Nash equilibrium is not an appropriate solution concept, since it assumes that the agents strategies are chosen simultaneously [5].",
                "In some settings, one player can (or must) commit to a strategy before the other players choose their strategies.",
                "These scenarios are known as Stackelberg games [6].",
                "In a Stackelberg game, a leader commits to a strategy first, and then a follower (or group of followers) selfishly optimize their own rewards, considering the action chosen by the leader.",
                "For example, the security agent (leader) must first commit to a strategy for patrolling various areas.",
                "This strategy could be a mixed strategy in order to be unpredictable to the robbers (followers).",
                "The robbers, after observing the pattern of patrols over time, can then choose their strategy (which location to rob).",
                "Often, the leader in a Stackelberg game can attain a higher reward than if the strategies were chosen simultaneously.",
                "To see the advantage of being the leader in a Stackelberg game, consider a simple game with the payoff table as shown in Table 1.",
                "The leader is the row player and the follower is the column player.",
                "Here, the leaders payoff is listed first. 1 2 3 1 5,5 0,0 3,10 2 0,0 2,2 5,0 Table 1: Payoff table for example normal form game.",
                "The only Nash equilibrium for this game is when the leader plays 2 and the follower plays 2 which gives the leader a payoff of 2. 311 978-81-904262-7-5 (RPS) c 2007 IFAAMAS However, if the leader commits to a uniform mixed strategy of playing 1 and 2 with equal (0.5) probability, the followers best response is to play 3 to get an expected payoff of 5 (10 and 0 with equal probability).",
                "The leaders payoff would then be 4 (3 and 5 with equal probability).",
                "In this case, the leader now has an incentive to deviate and choose a pure strategy of 2 (to get a payoff of 5).",
                "However, this would cause the follower to deviate to strategy 2 as well, resulting in the Nash equilibrium.",
                "Thus, by committing to a strategy that is observed by the follower, and by avoiding the temptation to deviate, the leader manages to obtain a reward higher than that of the best Nash equilibrium.",
                "The problem of choosing an optimal strategy for the leader to commit to in a Stackelberg game is analyzed in [5] and found to be NP-hard in the case of a Bayesian game with multiple types of followers.",
                "Thus, efficient heuristic techniques for choosing highreward strategies in these games is an important open issue.",
                "Methods for finding optimal leader strategies for non-Bayesian games [5] can be applied to this problem by converting the Bayesian game into a normal-form game by the Harsanyi transformation [8].",
                "If, on the other hand, we wish to compute the highest-reward Nash equilibrium, new methods using mixed-integer linear programs (MILPs) [17] may be used, since the highest-reward Bayes-Nash equilibrium is equivalent to the corresponding Nash equilibrium in the transformed game.",
                "However, by transforming the game, the compact structure of the Bayesian game is lost.",
                "In addition, since the Nash equilibrium assumes a simultaneous choice of strategies, the advantages of being the leader are not considered.",
                "This paper introduces an efficient heuristic method for approximating the optimal leader strategy for security domains, known as ASAP (Agent Security via Approximate Policies).",
                "This method has three key advantages.",
                "First, it directly searches for an optimal strategy, rather than a Nash (or Bayes-Nash) equilibrium, thus allowing it to find high-reward non-equilibrium strategies like the one in the above example.",
                "Second, it generates policies with a support which can be expressed as a uniform distribution over a multiset of fixed size as proposed in [12].",
                "This allows for policies that are simple to understand and represent [12], as well as a tunable parameter (the size of the multiset) that controls the simplicity of the policy.",
                "Third, the method allows for a Bayes-Nash game to be expressed compactly without conversion to a normal-form game, allowing for large speedups over existing Nash methods such as [17] and [11].",
                "The rest of the paper is organized as follows.",
                "In Section 2 we fully describe the <br>patrolling domain</br> and its properties.",
                "Section 3 introduces the Bayesian game, the Harsanyi transformation, and existing methods for finding an optimal leaders strategy in a Stackelberg game.",
                "Then, in Section 4 the ASAP algorithm is presented for normal-form games, and in Section 5 we show how it can be adapted to the structure of Bayesian games with uncertain adversaries.",
                "Experimental results showing higher reward and faster policy computation over existing Nash methods are shown in Section 6, and we conclude with a discussion of related work in Section 7. 2.",
                "THE <br>patrolling domain</br> In most security patrolling domains, the security agents (like UAVs [1] or security robots [16]) cannot feasibly patrol all areas all the time.",
                "Instead, they must choose a policy by which they patrol various routes at different times, taking into account factors such as the likelihood of crime in different areas, possible targets for crime, and the security agents own resources (number of security agents, amount of available time, fuel, etc.).",
                "It is usually beneficial for this policy to be nondeterministic so that robbers cannot safely rob certain locations, knowing that they will be safe from the security agents [14].",
                "To demonstrate the utility of our algorithm, we use a simplified version of such a domain, expressed as a game.",
                "The most basic version of our game consists of two players: the security agent (the leader) and the robber (the follower) in a world consisting of m houses, 1 . . . m. The security agents set of pure strategies consists of possible routes of d houses to patrol (in an order).",
                "The security agent can choose a mixed strategy so that the robber will be unsure of exactly where the security agent may patrol, but the robber will know the mixed strategy the security agent has chosen.",
                "For example, the robber can observe over time how often the security agent patrols each area.",
                "With this knowledge, the robber must choose a single house to rob.",
                "We assume that the robber generally takes a long time to rob a house.",
                "If the house chosen by the robber is not on the security agents route, then the robber successfully robs it.",
                "Otherwise, if it is on the security agents route, then the earlier the house is on the route, the easier it is for the security agent to catch the robber before he finishes robbing it.",
                "We model the payoffs for this game with the following variables: • vl,x: value of the goods in house l to the security agent. • vl,q: value of the goods in house l to the robber. • cx: reward to the security agent of catching the robber. • cq: cost to the robber of getting caught. • pl: probability that the security agent can catch the robber at the lth house in the patrol (pl < pl ⇐⇒ l < l).",
                "The security agents set of possible pure strategies (patrol routes) is denoted by X and includes all d-tuples i =< w1, w2, ..., wd > with w1 . . . wd = 1 . . . m where no two elements are equal (the agent is not allowed to return to the same house).",
                "The robbers set of possible pure strategies (houses to rob) is denoted by Q and includes all integers j = 1 . . . m. The payoffs (security agent, robber) for pure strategies i, j are: • −vl,x, vl,q, for j = l /∈ i. • plcx +(1−pl)(−vl,x), −plcq +(1−pl)(vl,q), for j = l ∈ i.",
                "With this structure it is possible to model many different types of robbers who have differing motivations; for example, one robber may have a lower cost of getting caught than another, or may value the goods in the various houses differently.",
                "If the distribution of different robber types is known or inferred from historical data, then the game can be modeled as a Bayesian game [6]. 3.",
                "BAYESIAN GAMES A Bayesian game contains a set of N agents, and each agent n must be one of a given set of types θn.",
                "For our <br>patrolling domain</br>, we have two agents, the security agent and the robber. θ1 is the set of security agent types and θ2 is the set of robber types.",
                "Since there is only one type of security agent, θ1 contains only one element.",
                "During the game, the robber knows its type but the security agent does not know the robbers type.",
                "For each agent (the security agent or the robber) n, there is a set of strategies σn and a utility function un : θ1 × θ2 × σ1 × σ2 → .",
                "A Bayesian game can be transformed into a normal-form game using the Harsanyi transformation [8].",
                "Once this is done, new, linear-program (LP)-based methods for finding high-reward strategies for normal-form games [5] can be used to find a strategy in the transformed game; this strategy can then be used for the Bayesian game.",
                "While methods exist for finding Bayes-Nash equilibria directly, without the Harsanyi transformation [10], they find only a 312 The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) single equilibrium in the general case, which may not be of high reward.",
                "Recent work [17] has led to efficient mixed-integer linear program techniques to find the best Nash equilibrium for a given agent.",
                "However, these techniques do require a normal-form game, and so to compare the policies given by ASAP against the optimal policy, as well as against the highest-reward Nash equilibrium, we must apply these techniques to the Harsanyi-transformed matrix.",
                "The next two subsections elaborate on how this is done. 3.1 Harsanyi Transformation The first step in solving Bayesian games is to apply the Harsanyi transformation [8] that converts the Bayesian game into a normal form game.",
                "Given that the Harsanyi transformation is a standard concept in game theory, we explain it briefly through a simple example in our <br>patrolling domain</br> without introducing the mathematical formulations.",
                "Let us assume there are two robber types a and b in the Bayesian game.",
                "Robber a will be active with probability α, and robber b will be active with probability 1 − α.",
                "The rules described in Section 2 allow us to construct simple payoff tables.",
                "Assume that there are two houses in the world (1 and 2) and hence there are two patrol routes (pure strategies) for the agent: {1,2} and {2,1}.",
                "The robber can rob either house 1 or house 2 and hence he has two strategies (denoted as 1l, 2l for robber type l).",
                "Since there are two types assumed (denoted as a and b), we construct two payoff tables (shown in Table 2) corresponding to the security agent playing a separate game with each of the two robber types with probabilities α and 1 − α.",
                "First, consider robber type a.",
                "Borrowing the notation from the domain section, we assign the following values to the variables: v1,x = v1,q = 3/4, v2,x = v2,q = 1/4, cx = 1/2, cq = 1, p1 = 1, p2 = 1/2.",
                "Using these values we construct a base payoff table as the payoff for the game against robber type a.",
                "For example, if the security agent chooses route {1,2} when robber a is active, and robber a chooses house 1, the robber receives a reward of -1 (for being caught) and the agent receives a reward of 0.5 for catching the robber.",
                "The payoffs for the game against robber type b are constructed using different values.",
                "Security agent: {1,2} {2,1} Robber a 1a -1, .5 -.375, .125 2a -.125, -.125 -1, .5 Robber b 1b -.9, .6 -.275, .225 2b -.025, -.025 -.9, .6 Table 2: Payoff tables: Security Agent vs Robbers a and b Using the Harsanyi technique involves introducing a chance node, that determines the robbers type, thus transforming the security agents incomplete information regarding the robber into imperfect information [3].",
                "The Bayesian equilibrium of the game is then precisely the Nash equilibrium of the imperfect information game.",
                "The transformed, normal-form game is shown in Table 3.",
                "In the transformed game, the security agent is the column player, and the set of all robber types together is the row player.",
                "Suppose that robber type a robs house 1 and robber type b robs house 2, while the security agent chooses patrol {1,2}.",
                "Then, the security agent and the robber receive an expected payoff corresponding to their payoffs from the agent encountering robber a at house 1 with probability α and robber b at house 2 with probability 1 − α. 3.2 Finding an Optimal Strategy Although a Nash equilibrium is the standard solution concept for games in which agents choose strategies simultaneously, in our security domain, the security agent (the leader) can gain an advantage by committing to a mixed strategy in advance.",
                "Since the followers (the robbers) will know the leaders strategy, the optimal response for the followers will be a pure strategy.",
                "Given the common assumption, taken in [5], in the case where followers are indifferent, they will choose the strategy that benefits the leader, there must exist a guaranteed optimal strategy for the leader [5].",
                "From the Bayesian game in Table 2, we constructed the Harsanyi transformed bimatrix in Table 3.",
                "The strategies for each player (security agent or robber) in the transformed game correspond to all combinations of possible strategies taken by each of that players types.",
                "Therefore, we denote X = σθ1 1 = σ1 and Q = σθ2 2 as the index sets of the security agent and robbers pure strategies respectively, with R and C as the corresponding payoff matrices.",
                "Rij is the reward of the security agent and Cij is the reward of the robbers when the security agent takes pure strategy i and the robbers take pure strategy j.",
                "A mixed strategy for the security agent is a probability distribution over its set of pure strategies and will be represented by a vector x = (px1, px2, . . . , px|X|), where pxi ≥ 0 and P pxi = 1.",
                "Here, pxi is the probability that the security agent will choose its ith pure strategy.",
                "The optimal mixed strategy for the security agent can be found in time polynomial in the number of rows in the normal form game using the following linear program formulation from [5].",
                "For every possible pure strategy j by the follower (the set of all robber types), max P i∈X pxiRij s.t. ∀j ∈ Q, P i∈σ1 pxiCij ≥ P i∈σ1 pxiCij P i∈X pxi = 1 ∀i∈X , pxi >= 0 (1) Then, for all feasible follower strategies j, choose the one that maximizes P i∈X pxiRij, the reward for the security agent (leader).",
                "The pxi variables give the optimal strategy for the security agent.",
                "Note that while this method is polynomial in the number of rows in the transformed, normal-form game, the number of rows increases exponentially with the number of robber types.",
                "Using this method for a Bayesian game thus requires running |σ2||θ2| separate linear programs.",
                "This is no surprise, since finding the leaders optimal strategy in a Bayesian Stackelberg game is NP-hard [5]. 4.",
                "HEURISTIC APPROACHES Given that finding the optimal strategy for the leader is NP-hard, we provide a heuristic approach.",
                "In this heuristic we limit the possible mixed strategies of the leader to select actions with probabilities that are integer multiples of 1/k for a predetermined integer k. Previous work [14] has shown that strategies with high entropy are beneficial for security applications when opponents utilities are completely unknown.",
                "In our domain, if utilities are not considered, this method will result in uniform-distribution strategies.",
                "One advantage of such strategies is that they are compact to represent (as fractions) and simple to understand; therefore they can be efficiently implemented by real organizations.",
                "We aim to maintain the advantage provided by simple strategies for our security application problem, incorporating the effect of the robbers rewards on the security agents rewards.",
                "Thus, the ASAP heuristic will produce strategies which are k-uniform.",
                "A mixed strategy is denoted k-uniform if it is a uniform distribution on a multiset S of pure strategies with |S| = k. A multiset is a set whose elements may be repeated multiple times; thus, for example, the mixed strategy corresponding to the multiset {1, 1, 2} would take strategy 1 The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) 313 {1,2} {2,1} {1a, 1b} −1α − .9(1 − α), .5α + .6(1 − α) −.375α − .275(1 − α), .125α + .225(1 − α) {1a, 2b} −1α − .025(1 − α), .5α − .025(1 − α) −.375α − .9(1 − α), .125α + .6(1 − α) {2a, 1b} −.125α − .9(1 − α), −.125α + .6(1 − α) −1α − .275(1 − α), .5α + .225(1 − α) {2a, 2b} −.125α − .025(1 − α), −.125α − .025(1 − α) −1α − .9(1 − α), .5α + .6(1 − α) Table 3: Harsanyi Transformed Payoff Table with probability 2/3 and strategy 2 with probability 1/3.",
                "ASAP allows the size of the multiset to be chosen in order to balance the complexity of the strategy reached with the goal that the identified strategy will yield a high reward.",
                "Another advantage of the ASAP heuristic is that it operates directly on the compact Bayesian representation, without requiring the Harsanyi transformation.",
                "This is because the different follower (robber) types are independent of each other.",
                "Hence, evaluating the leader strategy against a Harsanyi-transformed game matrix is equivalent to evaluating against each of the game matrices for the individual follower types.",
                "This independence property is exploited in ASAP to yield a decomposition scheme.",
                "Note that the LP method introduced by [5] to compute optimal Stackelberg policies is unlikely to be decomposable into a small number of games as it was shown to be NP-hard for Bayes-Nash problems.",
                "Finally, note that ASAP requires the solution of only one optimization problem, rather than solving a series of problems as in the LP method of [5].",
                "For a single follower type, the algorithm works the following way.",
                "Given a particular k, for each possible mixed strategy x for the leader that corresponds to a multiset of size k, evaluate the leaders payoff from x when the follower plays a reward-maximizing pure strategy.",
                "We then take the mixed strategy with the highest payoff.",
                "We need only to consider the reward-maximizing pure strategies of the followers (robbers), since for a given fixed strategy x of the security agent, each robber type faces a problem with fixed linear rewards.",
                "If a mixed strategy is optimal for the robber, then so are all the pure strategies in the support of that mixed strategy.",
                "Note also that because we limit the leaders strategies to take on discrete values, the assumption from Section 3.2 that the followers will break ties in the leaders favor is not significant, since ties will be unlikely to arise.",
                "This is because, in domains where rewards are drawn from any random distribution, the probability of a follower having more than one pure optimal response to a given leader strategy approaches zero, and the leader will have only a finite number of possible mixed strategies.",
                "Our approach to characterize the optimal strategy for the security agent makes use of properties of linear programming.",
                "We briefly outline these results here for completeness, for detailed discussion and proofs see one of many references on the topic, such as [2].",
                "Every linear programming problem, such as: max cT x | Ax = b, x ≥ 0, has an associated dual linear program, in this case: min bT y | AT y ≥ c. These primal/dual pairs of problems satisfy weak duality: For any x and y primal and dual feasible solutions respectively, cT x ≤ bT y.",
                "Thus a pair of feasible solutions is optimal if cT x = bT y, and the problems are said to satisfy strong duality.",
                "In fact if a linear program is feasible and has a bounded optimal solution, then the dual is also feasible and there is a pair x∗ , y∗ that satisfies cT x∗ = bT y∗ .",
                "These optimal solutions are characterized with the following optimality conditions (as defined in [2]): • primal feasibility: Ax = b, x ≥ 0 • dual feasibility: AT y ≥ c • complementary slackness: xi(AT y − c)i = 0 for all i.",
                "Note that this last condition implies that cT x = xT AT y = bT y, which proves optimality for primal dual feasible solutions x and y.",
                "In the following subsections, we first define the problem in its most intuititive form as a mixed-integer quadratic program (MIQP), and then show how this problem can be converted into a mixedinteger linear program (MILP). 4.1 Mixed-Integer Quadratic Program We begin with the case of a single type of follower.",
                "Let the leader be the row player and the follower the column player.",
                "We denote by x the vector of strategies of the leader and q the vector of strategies of the follower.",
                "We also denote X and Q the index sets of the leader and followers pure strategies, respectively.",
                "The payoff matrices R and C correspond to: Rij is the reward of the leader and Cij is the reward of the follower when the leader takes pure strategy i and the follower takes pure strategy j.",
                "Let k be the size of the multiset.",
                "We first fix the policy of the leader to some k-uniform policy x.",
                "The value xi is the number of times pure strategy i is used in the k-uniform policy, which is selected with probability xi/k.",
                "We formulate the optimization problem the follower solves to find its optimal response to x as the following linear program: max X j∈Q X i∈X 1 k Cijxi qj s.t.",
                "P j∈Q qj = 1 q ≥ 0. (2) The objective function maximizes the followers expected reward given x, while the constraints make feasible any mixed strategy q for the follower.",
                "The dual to this linear programming problem is the following: min a s.t. a ≥ X i∈X 1 k Cijxi j ∈ Q. (3) From strong duality and complementary slackness we obtain that the followers maximum reward value, a, is the value of every pure strategy with qj > 0, that is in the support of the optimal mixed strategy.",
                "Therefore each of these pure strategies is optimal.",
                "Optimal solutions to the followers problem are characterized by linear programming optimality conditions: primal feasibility constraints in (2), dual feasibility constraints in (3), and complementary slackness qj a − X i∈X 1 k Cijxi ! = 0 j ∈ Q.",
                "These conditions must be included in the problem solved by the leader in order to consider only best responses by the follower to the k-uniform policy x. 314 The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) The leader seeks the k-uniform solution x that maximizes its own payoff, given that the follower uses an optimal response q(x).",
                "Therefore the leader solves the following integer problem: max X i∈X X j∈Q 1 k Rijq(x)j xi s.t.",
                "P i∈X xi = k xi ∈ {0, 1, . . . , k}. (4) Problem (4) maximizes the leaders reward with the followers best response (qj for fixed leaders policy x and hence denoted q(x)j) by selecting a uniform policy from a multiset of constant size k. We complete this problem by including the characterization of q(x) through linear programming optimality conditions.",
                "To simplify writing the complementary slackness conditions, we will constrain q(x) to be only optimal pure strategies by just considering integer solutions of q(x).",
                "The leaders problem becomes: maxx,q X i∈X X j∈Q 1 k Rijxiqj s.t.",
                "P i xi = kP j∈Q qj = 1 0 ≤ (a − P i∈X 1 k Cijxi) ≤ (1 − qj)M xi ∈ {0, 1, ...., k} qj ∈ {0, 1}. (5) Here, the constant M is some large number.",
                "The first and fourth constraints enforce a k-uniform policy for the leader, and the second and fifth constraints enforce a feasible pure strategy for the follower.",
                "The third constraint enforces dual feasibility of the followers problem (leftmost inequality) and the complementary slackness constraint for an optimal pure strategy q for the follower (rightmost inequality).",
                "In fact, since only one pure strategy can be selected by the follower, say qh = 1, this last constraint enforces that a = P i∈X 1 k Cihxi imposing no additional constraint for all other pure strategies which have qj = 0.",
                "We conclude this subsection noting that Problem (5) is an integer program with a non-convex quadratic objective in general, as the matrix R need not be positive-semi-definite.",
                "Efficient solution methods for non-linear, non-convex integer problems remains a challenging research question.",
                "In the next section we show a reformulation of this problem as a linear integer programming problem, for which a number of efficient commercial solvers exist. 4.2 Mixed-Integer Linear Program We can linearize the quadratic program of Problem 5 through the change of variables zij = xiqj, obtaining the following problem maxq,z P i∈X P j∈Q 1 k Rijzij s.t.",
                "P i∈X P j∈Q zij = k P j∈Q zij ≤ k kqj ≤ P i∈X zij ≤ k P j∈Q qj = 1 0 ≤ (a − P i∈X 1 k Cij( P h∈Q zih)) ≤ (1 − qj)M zij ∈ {0, 1, ...., k} qj ∈ {0, 1} (6) PROPOSITION 1.",
                "Problems (5) and (6) are equivalent.",
                "Proof: Consider x, q a feasible solution of (5).",
                "We will show that q, zij = xiqj is a feasible solution of (6) of same objective function value.",
                "The equivalence of the objective functions, and constraints 4, 6 and 7 of (6) are satisfied by construction.",
                "The fact that P j∈Q zij = xi as P j∈Q qj = 1 explains constraints 1, 2, and 5 of (6).",
                "Constraint 3 of (6) is satisfied because P i∈X zij = kqj.",
                "Let us now consider q, z feasible for (6).",
                "We will show that q and xi = P j∈Q zij are feasible for (5) with the same objective value.",
                "In fact all constraints of (5) are readily satisfied by construction.",
                "To see that the objectives match, notice that if qh = 1 then the third constraint in (6) implies that P i∈X zih = k, which means that zij = 0 for all i ∈ X and all j = h. Therefore, xiqj = X l∈Q zilqj = zihqj = zij.",
                "This last equality is because both are 0 when j = h. This shows that the transformation preserves the objective function value, completing the proof.",
                "Given this transformation to a mixed-integer linear program (MILP), we now show how we can apply our decomposition technique on the MILP to obtain significant speedups for Bayesian games with multiple follower types. 5.",
                "DECOMPOSITION FOR MULTIPLE ADVERSARIES The MILP developed in the previous section handles only one follower.",
                "Since our security scenario contains multiple follower (robber) types, we change the response function for the follower from a pure strategy into a weighted combination over various pure follower strategies where the weights are probabilities of occurrence of each of the follower types. 5.1 Decomposed MIQP To admit multiple adversaries in our framework, we modify the notation defined in the previous section to reason about multiple follower types.",
                "We denote by x the vector of strategies of the leader and ql the vector of strategies of follower l, with L denoting the index set of follower types.",
                "We also denote by X and Q the index sets of leader and follower ls pure strategies, respectively.",
                "We also index the payoff matrices on each follower l, considering the matrices Rl and Cl .",
                "Using this modified notation, we characterize the optimal solution of follower ls problem given the leaders k-uniform policy x, with the following optimality conditions: X j∈Q ql j = 1 al − X i∈X 1 k Cl ijxi ≥ 0 ql j(al − X i∈X 1 k Cl ijxi) = 0 ql j ≥ 0 Again, considering only optimal pure strategies for follower ls problem we can linearize the complementarity constraint above.",
                "We incorporate these constraints on the leaders problem that selects the optimal k-uniform policy.",
                "Therefore, given a priori probabilities pl , with l ∈ L of facing each follower, the leader solves the following problem: The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) 315 maxx,q X i∈X X l∈L X j∈Q pl k Rl ijxiql j s.t.",
                "P i xi = kP j∈Q ql j = 1 0 ≤ (al − P i∈X 1 k Cl ijxi) ≤ (1 − ql j)M xi ∈ {0, 1, ...., k} ql j ∈ {0, 1}. (7) Problem (7) for a Bayesian game with multiple follower types is indeed equivalent to Problem (5) on the payoff matrix obtained from the Harsanyi transformation of the game.",
                "In fact, every pure strategy j in Problem (5) corresponds to a sequence of pure strategies jl, one for each follower l ∈ L. This means that qj = 1 if and only if ql jl = 1 for all l ∈ L. In addition, given the a priori probabilities pl of facing player l, the reward in the Harsanyi transformation payoff table is Rij = P l∈L pl Rl ijl .",
                "The same relation holds between C and Cl .",
                "These relations between a pure strategy in the equivalent normal form game and pure strategies in the individual games with each followers are key in showing these problems are equivalent. 5.2 Decomposed MILP We can linearize the quadratic programming problem 7 through the change of variables zl ij = xiql j, obtaining the following problem maxq,z P i∈X P l∈L P j∈Q pl k Rl ijzl ij s.t.",
                "P i∈X P j∈Q zl ij = k P j∈Q zl ij ≤ k kql j ≤ P i∈X zl ij ≤ k P j∈Q ql j = 1 0 ≤ (al − P i∈X 1 k Cl ij( P h∈Q zl ih)) ≤ (1 − ql j)M P j∈Q zl ij = P j∈Q z1 ij zl ij ∈ {0, 1, ...., k} ql j ∈ {0, 1} (8) PROPOSITION 2.",
                "Problems (7) and (8) are equivalent.",
                "Proof: Consider x, ql , al with l ∈ L a feasible solution of (7).",
                "We will show that ql , al , zl ij = xiql j is a feasible solution of (8) of same objective function value.",
                "The equivalence of the objective functions, and constraints 4, 7 and 8 of (8) are satisfied by construction.",
                "The fact that P j∈Q zl ij = xi as P j∈Q ql j = 1 explains constraints 1, 2, 5 and 6 of (8).",
                "Constraint 3 of (8) is satisfied because P i∈X zl ij = kql j.",
                "Lets now consider ql , zl , al feasible for (8).",
                "We will show that ql , al and xi = P j∈Q z1 ij are feasible for (7) with the same objective value.",
                "In fact all constraints of (7) are readily satisfied by construction.",
                "To see that the objectives match, notice for each l one ql j must equal 1 and the rest equal 0.",
                "Let us say that ql jl = 1, then the third constraint in (8) implies that P i∈X zl ijl = k, which means that zl ij = 0 for all i ∈ X and all j = jl.",
                "In particular this implies that xi = X j∈Q z1 ij = z1 ij1 = zl ijl , the last equality from constraint 6 of (8).",
                "Therefore xiql j = zl ijl ql j = zl ij.",
                "This last equality is because both are 0 when j = jl.",
                "Effectively, constraint 6 ensures that all the adversaries are calculating their best responses against a particular fixed policy of the agent.",
                "This shows that the transformation preserves the objective function value, completing the proof.",
                "We can therefore solve this equivalent linear integer program with efficient integer programming packages which can handle problems with thousands of integer variables.",
                "We implemented the decomposed MILP and the results are shown in the following section. 6.",
                "EXPERIMENTAL RESULTS The <br>patrolling domain</br> and the payoffs for the associated game are detailed in Sections 2 and 3.",
                "We performed experiments for this game in worlds of three and four houses with patrols consisting of two houses.",
                "The description given in Section 2 is used to generate a base case for both the security agent and robber payoff functions.",
                "The payoff tables for additional robber types are constructed and added to the game by adding a random distribution of varying size to the payoffs in the base case.",
                "All games are normalized so that, for each robber type, the minimum and maximum payoffs to the security agent and robber are 0 and 1, respectively.",
                "Using the data generated, we performed the experiments using four methods for generating the security agents strategy: • uniform randomization • ASAP • the multiple linear programs method from [5] (to find the true optimal strategy) • the highest reward Bayes-Nash equilibrium, found using the MIP-Nash algorithm [17] The last three methods were applied using CPLEX 8.1.",
                "Because the last two methods are designed for normal-form games rather than Bayesian games, the games were first converted using the Harsanyi transformation [8].",
                "The uniform randomization method is simply choosing a uniform random policy over all possible patrol routes.",
                "We use this method as a simple baseline to measure the performance of our heuristics.",
                "We anticipated that the uniform policy would perform reasonably well since maximum-entropy policies have been shown to be effective in multiagent security domains [14].",
                "The highest-reward Bayes-Nash equilibria were used in order to demonstrate the higher reward gained by looking for an optimal policy rather than an equilibria in Stackelberg games such as our security domain.",
                "Based on our experiments we present three sets of graphs to demonstrate (1) the runtime of ASAP compared to other common methods for finding a strategy, (2) the reward guaranteed by ASAP compared to other methods, and (3) the effect of varying the parameter k, the size of the multiset, on the performance of ASAP.",
                "In the first two sets of graphs, ASAP is run using a multiset of 80 elements; in the third set this number is varied.",
                "The first set of graphs, shown in Figure 1 shows the runtime graphs for three-house (left column) and four-house (right column) domains.",
                "Each of the three rows of graphs corresponds to a different randomly-generated scenario.",
                "The x-axis shows the number of robber types the security agent faces and the y-axis of the graph shows the runtime in seconds.",
                "All experiments that were not concluded in 30 minutes (1800 seconds) were cut off.",
                "The runtime for the uniform policy is always negligible irrespective of the number of adversaries and hence is not shown.",
                "The ASAP algorithm clearly outperforms the optimal, multipleLP method as well as the MIP-Nash algorithm for finding the highestreward Bayes-Nash equilibrium with respect to runtime.",
                "For a 316 The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) Figure 1: Runtimes for various algorithms on problems of 3 and 4 houses. domain of three houses, the optimal method cannot reach a solution for more than seven robber types, and for four houses it cannot solve for more than six types within the cutoff time in any of the three scenarios.",
                "MIP-Nash solves for even fewer robber types within the cutoff time.",
                "On the other hand, ASAP runs much faster, and is able to solve for at least 20 adversaries for the three-house scenarios and for at least 12 adversaries in the four-house scenarios within the cutoff time.",
                "The runtime of ASAP does not increase strictly with the number of robber types for each scenario, but in general, the addition of more types increases the runtime required.",
                "The second set of graphs, Figure 2, shows the reward to the patrol agent given by each method for three scenarios in the three-house (left column) and four-house (right column) domains.",
                "This reward is the utility received by the security agent in the patrolling game, and not as a percentage of the optimal reward, since it was not possible to obtain the optimal reward as the number of robber types increased.",
                "The uniform policy consistently provides the lowest reward in both domains; while the optimal method of course produces the optimal reward.",
                "The ASAP method remains consistently close to the optimal, even as the number of robber types increases.",
                "The highest-reward Bayes-Nash equilibria, provided by the MIPNash method, produced rewards higher than the uniform method, but lower than ASAP.",
                "This difference clearly illustrates the gains in the <br>patrolling domain</br> from committing to a strategy as the leader in a Stackelberg game, rather than playing a standard Bayes-Nash strategy.",
                "The third set of graphs, shown in Figure 3 shows the effect of the multiset size on runtime in seconds (left column) and reward (right column), again expressed as the reward received by the security agent in the patrolling game, and not a percentage of the optimal Figure 2: Reward for various algorithms on problems of 3 and 4 houses. reward.",
                "Results here are for the three-house domain.",
                "The trend is that as as the multiset size is increased, the runtime and reward level both increase.",
                "Not surprisingly, the reward increases monotonically as the multiset size increases, but what is interesting is that there is relatively little benefit to using a large multiset in this domain.",
                "In all cases, the reward given by a multiset of 10 elements was within at least 96% of the reward given by an 80-element multiset.",
                "The runtime does not always increase strictly with the multiset size; indeed in one example (scenario 2 with 20 robber types), using a multiset of 10 elements took 1228 seconds, while using 80 elements only took 617 seconds.",
                "In general, runtime should increase since a larger multiset means a larger domain for the variables in the MILP, and thus a larger search space.",
                "However, an increase in the number of variables can sometimes allow for a policy to be constructed more quickly due to more flexibility in the problem. 7.",
                "SUMMARY AND RELATED WORK This paper focuses on security for agents patrolling in hostile environments.",
                "In these environments, intentional threats are caused by adversaries about whom the security patrolling agents have incomplete information.",
                "Specifically, we deal with situations where the adversaries actions and payoffs are known but the exact adversary type is unknown to the security agent.",
                "Agents acting in the real world quite frequently have such incomplete information about other agents.",
                "Bayesian games have been a popular choice to model such incomplete information games [3].",
                "The Gala toolkit is one method for defining such games [9] without requiring the game to be represented in normal form via the Harsanyi transformation [8]; Galas guarantees are focused on fully competitive games.",
                "Much work has been done on finding optimal Bayes-Nash equilbria for The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) 317 Figure 3: Reward for ASAP using multisets of 10, 30, and 80 elements subclasses of Bayesian games, finding single Bayes-Nash equilibria for general Bayesian games [10] or approximate Bayes-Nash equilibria [18].",
                "Less attention has been paid to finding the optimal strategy to commit to in a Bayesian game (the Stackelberg scenario [15]).",
                "However, the complexity of this problem was shown to be NP-hard in the general case [5], which also provides algorithms for this problem in the non-Bayesian case.",
                "Therefore, we present a heuristic called ASAP, with three key advantages towards addressing this problem.",
                "First, ASAP searches for the highest reward strategy, rather than a Bayes-Nash equilibrium, allowing it to find feasible strategies that exploit the natural first-mover advantage of the game.",
                "Second, it provides strategies which are simple to understand, represent, and implement.",
                "Third, it operates directly on the compact, Bayesian game representation, without requiring conversion to normal form.",
                "We provide an efficient Mixed Integer Linear Program (MILP) implementation for ASAP, along with experimental results illustrating significant speedups and higher rewards over other approaches.",
                "Our k-uniform strategies are similar to the k-uniform strategies of [12].",
                "While that work provides epsilon error-bounds based on the k-uniform strategies, their solution concept is still that of a Nash equilibrium, and they do not provide efficient algorithms for obtaining such k-uniform strategies.",
                "This contrasts with ASAP, where our emphasis is on a highly efficient heuristic approach that is not focused on equilibrium solutions.",
                "Finally the patrolling problem which motivated our work has recently received growing attention from the multiagent community due to its wide range of applications [4, 13].",
                "However most of this work is focused on either limiting energy consumption involved in patrolling [7] or optimizing on criteria like the length of the path traveled [4, 13], without reasoning about any explicit model of an adversary[14].",
                "Acknowledgments : This research is supported by the United States Department of Homeland Security through Center for Risk and Economic Analysis of Terrorism Events (CREATE).",
                "It is also supported by the Defense Advanced Research Projects Agency (DARPA), through the Department of the Interior, NBC, Acquisition Services Division, under Contract No.",
                "NBCHD030010.",
                "Sarit Kraus is also affiliated with UMIACS. 8.",
                "REFERENCES [1] R. W. Beard and T. McLain.",
                "Multiple UAV cooperative search under collision avoidance and limited range communication constraints.",
                "In IEEE CDC, 2003. [2] D. Bertsimas and J. Tsitsiklis.",
                "Introduction to Linear Optimization.",
                "Athena Scientific, 1997. [3] J. Brynielsson and S. Arnborg.",
                "Bayesian games for threat prediction and situation analysis.",
                "In FUSION, 2004. [4] Y. Chevaleyre.",
                "Theoretical analysis of multi-agent patrolling problem.",
                "In AAMAS, 2004. [5] V. Conitzer and T. Sandholm.",
                "Choosing the best strategy to commit to.",
                "In ACM Conference on Electronic Commerce, 2006. [6] D. Fudenberg and J. Tirole.",
                "Game Theory.",
                "MIT Press, 1991. [7] C. Gui and P. Mohapatra.",
                "Virtual patrol: A new power conservation design for surveillance using sensor networks.",
                "In IPSN, 2005. [8] J. C. Harsanyi and R. Selten.",
                "A generalized Nash solution for two-person bargaining games with incomplete information.",
                "Management Science, 18(5):80-106, 1972. [9] D. Koller and A. Pfeffer.",
                "Generating and solving imperfect information games.",
                "In IJCAI, pages 1185-1193, 1995. [10] D. Koller and A. Pfeffer.",
                "Representations and solutions for game-theoretic problems.",
                "Artificial Intelligence, 94(1):167-215, 1997. [11] C. Lemke and J. Howson.",
                "Equilibrium points of bimatrix games.",
                "Journal of the Society for Industrial and Applied Mathematics, 12:413-423, 1964. [12] R. J. Lipton, E. Markakis, and A. Mehta.",
                "Playing large games using simple strategies.",
                "In ACM Conference on Electronic Commerce, 2003. [13] A. Machado, G. Ramalho, J. D. Zucker, and A. Drougoul.",
                "Multi-agent patrolling: an empirical analysis on alternative architectures.",
                "In MABS, 2002. [14] P. Paruchuri, M. Tambe, F. Ordonez, and S. Kraus.",
                "Security in multiagent systems by policy randomization.",
                "In AAMAS, 2006. [15] T. Roughgarden.",
                "Stackelberg scheduling strategies.",
                "In ACM Symposium on TOC, 2001. [16] S. Ruan, C. Meirina, F. Yu, K. R. Pattipati, and R. L. Popp.",
                "Patrolling in a stochastic environment.",
                "In 10th Intl.",
                "Command and Control Research Symp., 2005. [17] T. Sandholm, A. Gilpin, and V. Conitzer.",
                "Mixed-integer programming methods for finding nash equilibria.",
                "In AAAI, 2005. [18] S. Singh, V. Soni, and M. Wellman.",
                "Computing approximate Bayes-Nash equilibria with tree-games of incomplete information.",
                "In ACM Conference on Electronic Commerce, 2004. 318 The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07)"
            ],
            "original_annotated_samples": [],
            "translated_annotated_samples": [
                "En la Sección 2 describimos completamente el \"dominio de patrulla\" y sus propiedades.",
                "El \"dominio de patrulla\" en la mayoría de los dominios de patrullaje de seguridad, los agentes de seguridad (como los UAV [1] o los robots de seguridad [16]) no pueden patrullar todas las áreas todo el tiempo.",
                "Para nuestro \"dominio de patrulla\", tenemos dos agentes, el agente de seguridad y el ladrón.θ1 es el conjunto de tipos de agentes de seguridad y θ2 es el conjunto de tipos de ladrones.",
                "Dado que la transformación de Harsanyi es un concepto estándar en la teoría del juego, lo explicamos brevemente a través de un ejemplo simple en nuestro \"dominio de patrulla\" sin introducir las formulaciones matemáticas.",
                "Resultados experimentales El \"dominio de patrulla\" y los pagos para el juego asociado se detallan en las Secciones 2 y 3.",
                "Esta diferencia ilustra claramente las ganancias en el \"dominio de patrulla\" de comprometerse a una estrategia como líder en un juego de Stackelberg, en lugar de jugar una estrategia estándar de Bayes-Nash."
            ],
            "translated_text": "",
            "candidates": [
                "dominio de patrulla",
                "dominio de patrulla",
                "dominio de patrulla",
                "dominio de patrulla",
                "dominio de patrulla",
                "dominio de patrulla",
                "dominio de patrulla",
                "dominio de patrulla",
                "Dominio de patrulla",
                "dominio de patrulla",
                "dominio de patrulla",
                "dominio de patrulla"
            ],
            "error": []
        },
        "heuristic approach": {
            "translated_key": "enfoque heurístico",
            "is_in_text": true,
            "original_annotated_sentences": [
                "An Efficient <br>heuristic approach</br> for Security Against Multiple Adversaries Praveen Paruchuri, Jonathan P. Pearce, Milind Tambe, Fernando Ordonez University of Southern California Los Angeles, CA 90089 {paruchur, jppearce, tambe, fordon}@usc.edu Sarit Kraus Bar-Ilan University Ramat-Gan 52900, Israel sarit@cs.biu.ac.il ABSTRACT In adversarial multiagent domains, security, commonly defined as the ability to deal with intentional threats from other agents, is a critical issue.",
                "This paper focuses on domains where these threats come from unknown adversaries.",
                "These domains can be modeled as Bayesian games; much work has been done on finding equilibria for such games.",
                "However, it is often the case in multiagent security domains that one agent can commit to a mixed strategy which its adversaries observe before choosing their own strategies.",
                "In this case, the agent can maximize reward by finding an optimal strategy, without requiring equilibrium.",
                "Previous work has shown this problem of optimal strategy selection to be NP-hard.",
                "Therefore, we present a heuristic called ASAP, with three key advantages to address the problem.",
                "First, ASAP searches for the highest-reward strategy, rather than a Bayes-Nash equilibrium, allowing it to find feasible strategies that exploit the natural first-mover advantage of the game.",
                "Second, it provides strategies which are simple to understand, represent, and implement.",
                "Third, it operates directly on the compact, Bayesian game representation, without requiring conversion to normal form.",
                "We provide an efficient Mixed Integer Linear Program (MILP) implementation for ASAP, along with experimental results illustrating significant speedups and higher rewards over other approaches.",
                "Categories and Subject Descriptors I.2.11 [Computing Methodologies]: Artificial Intelligence: Distributed Artificial Intelligence - Intelligent Agents General Terms Security, Design, Theory 1.",
                "INTRODUCTION In many multiagent domains, agents must act in order to provide security against attacks by adversaries.",
                "A common issue that agents face in such security domains is uncertainty about the adversaries they may be facing.",
                "For example, a security robot may need to make a choice about which areas to patrol, and how often [16].",
                "However, it will not know in advance exactly where a robber will choose to strike.",
                "A team of unmanned aerial vehicles (UAVs) [1] monitoring a region undergoing a humanitarian crisis may also need to choose a patrolling policy.",
                "They must make this decision without knowing in advance whether terrorists or other adversaries may be waiting to disrupt the mission at a given location.",
                "It may indeed be possible to model the motivations of types of adversaries the agent or agent team is likely to face in order to target these adversaries more closely.",
                "However, in both cases, the security robot or UAV team will not know exactly which kinds of adversaries may be active on any given day.",
                "A common approach for choosing a policy for agents in such scenarios is to model the scenarios as Bayesian games.",
                "A Bayesian game is a game in which agents may belong to one or more types; the type of an agent determines its possible actions and payoffs.",
                "The distribution of adversary types that an agent will face may be known or inferred from historical data.",
                "Usually, these games are analyzed according to the solution concept of a Bayes-Nash equilibrium, an extension of the Nash equilibrium for Bayesian games.",
                "However, in many settings, a Nash or Bayes-Nash equilibrium is not an appropriate solution concept, since it assumes that the agents strategies are chosen simultaneously [5].",
                "In some settings, one player can (or must) commit to a strategy before the other players choose their strategies.",
                "These scenarios are known as Stackelberg games [6].",
                "In a Stackelberg game, a leader commits to a strategy first, and then a follower (or group of followers) selfishly optimize their own rewards, considering the action chosen by the leader.",
                "For example, the security agent (leader) must first commit to a strategy for patrolling various areas.",
                "This strategy could be a mixed strategy in order to be unpredictable to the robbers (followers).",
                "The robbers, after observing the pattern of patrols over time, can then choose their strategy (which location to rob).",
                "Often, the leader in a Stackelberg game can attain a higher reward than if the strategies were chosen simultaneously.",
                "To see the advantage of being the leader in a Stackelberg game, consider a simple game with the payoff table as shown in Table 1.",
                "The leader is the row player and the follower is the column player.",
                "Here, the leaders payoff is listed first. 1 2 3 1 5,5 0,0 3,10 2 0,0 2,2 5,0 Table 1: Payoff table for example normal form game.",
                "The only Nash equilibrium for this game is when the leader plays 2 and the follower plays 2 which gives the leader a payoff of 2. 311 978-81-904262-7-5 (RPS) c 2007 IFAAMAS However, if the leader commits to a uniform mixed strategy of playing 1 and 2 with equal (0.5) probability, the followers best response is to play 3 to get an expected payoff of 5 (10 and 0 with equal probability).",
                "The leaders payoff would then be 4 (3 and 5 with equal probability).",
                "In this case, the leader now has an incentive to deviate and choose a pure strategy of 2 (to get a payoff of 5).",
                "However, this would cause the follower to deviate to strategy 2 as well, resulting in the Nash equilibrium.",
                "Thus, by committing to a strategy that is observed by the follower, and by avoiding the temptation to deviate, the leader manages to obtain a reward higher than that of the best Nash equilibrium.",
                "The problem of choosing an optimal strategy for the leader to commit to in a Stackelberg game is analyzed in [5] and found to be NP-hard in the case of a Bayesian game with multiple types of followers.",
                "Thus, efficient heuristic techniques for choosing highreward strategies in these games is an important open issue.",
                "Methods for finding optimal leader strategies for non-Bayesian games [5] can be applied to this problem by converting the Bayesian game into a normal-form game by the Harsanyi transformation [8].",
                "If, on the other hand, we wish to compute the highest-reward Nash equilibrium, new methods using mixed-integer linear programs (MILPs) [17] may be used, since the highest-reward Bayes-Nash equilibrium is equivalent to the corresponding Nash equilibrium in the transformed game.",
                "However, by transforming the game, the compact structure of the Bayesian game is lost.",
                "In addition, since the Nash equilibrium assumes a simultaneous choice of strategies, the advantages of being the leader are not considered.",
                "This paper introduces an efficient heuristic method for approximating the optimal leader strategy for security domains, known as ASAP (Agent Security via Approximate Policies).",
                "This method has three key advantages.",
                "First, it directly searches for an optimal strategy, rather than a Nash (or Bayes-Nash) equilibrium, thus allowing it to find high-reward non-equilibrium strategies like the one in the above example.",
                "Second, it generates policies with a support which can be expressed as a uniform distribution over a multiset of fixed size as proposed in [12].",
                "This allows for policies that are simple to understand and represent [12], as well as a tunable parameter (the size of the multiset) that controls the simplicity of the policy.",
                "Third, the method allows for a Bayes-Nash game to be expressed compactly without conversion to a normal-form game, allowing for large speedups over existing Nash methods such as [17] and [11].",
                "The rest of the paper is organized as follows.",
                "In Section 2 we fully describe the patrolling domain and its properties.",
                "Section 3 introduces the Bayesian game, the Harsanyi transformation, and existing methods for finding an optimal leaders strategy in a Stackelberg game.",
                "Then, in Section 4 the ASAP algorithm is presented for normal-form games, and in Section 5 we show how it can be adapted to the structure of Bayesian games with uncertain adversaries.",
                "Experimental results showing higher reward and faster policy computation over existing Nash methods are shown in Section 6, and we conclude with a discussion of related work in Section 7. 2.",
                "THE PATROLLING DOMAIN In most security patrolling domains, the security agents (like UAVs [1] or security robots [16]) cannot feasibly patrol all areas all the time.",
                "Instead, they must choose a policy by which they patrol various routes at different times, taking into account factors such as the likelihood of crime in different areas, possible targets for crime, and the security agents own resources (number of security agents, amount of available time, fuel, etc.).",
                "It is usually beneficial for this policy to be nondeterministic so that robbers cannot safely rob certain locations, knowing that they will be safe from the security agents [14].",
                "To demonstrate the utility of our algorithm, we use a simplified version of such a domain, expressed as a game.",
                "The most basic version of our game consists of two players: the security agent (the leader) and the robber (the follower) in a world consisting of m houses, 1 . . . m. The security agents set of pure strategies consists of possible routes of d houses to patrol (in an order).",
                "The security agent can choose a mixed strategy so that the robber will be unsure of exactly where the security agent may patrol, but the robber will know the mixed strategy the security agent has chosen.",
                "For example, the robber can observe over time how often the security agent patrols each area.",
                "With this knowledge, the robber must choose a single house to rob.",
                "We assume that the robber generally takes a long time to rob a house.",
                "If the house chosen by the robber is not on the security agents route, then the robber successfully robs it.",
                "Otherwise, if it is on the security agents route, then the earlier the house is on the route, the easier it is for the security agent to catch the robber before he finishes robbing it.",
                "We model the payoffs for this game with the following variables: • vl,x: value of the goods in house l to the security agent. • vl,q: value of the goods in house l to the robber. • cx: reward to the security agent of catching the robber. • cq: cost to the robber of getting caught. • pl: probability that the security agent can catch the robber at the lth house in the patrol (pl < pl ⇐⇒ l < l).",
                "The security agents set of possible pure strategies (patrol routes) is denoted by X and includes all d-tuples i =< w1, w2, ..., wd > with w1 . . . wd = 1 . . . m where no two elements are equal (the agent is not allowed to return to the same house).",
                "The robbers set of possible pure strategies (houses to rob) is denoted by Q and includes all integers j = 1 . . . m. The payoffs (security agent, robber) for pure strategies i, j are: • −vl,x, vl,q, for j = l /∈ i. • plcx +(1−pl)(−vl,x), −plcq +(1−pl)(vl,q), for j = l ∈ i.",
                "With this structure it is possible to model many different types of robbers who have differing motivations; for example, one robber may have a lower cost of getting caught than another, or may value the goods in the various houses differently.",
                "If the distribution of different robber types is known or inferred from historical data, then the game can be modeled as a Bayesian game [6]. 3.",
                "BAYESIAN GAMES A Bayesian game contains a set of N agents, and each agent n must be one of a given set of types θn.",
                "For our patrolling domain, we have two agents, the security agent and the robber. θ1 is the set of security agent types and θ2 is the set of robber types.",
                "Since there is only one type of security agent, θ1 contains only one element.",
                "During the game, the robber knows its type but the security agent does not know the robbers type.",
                "For each agent (the security agent or the robber) n, there is a set of strategies σn and a utility function un : θ1 × θ2 × σ1 × σ2 → .",
                "A Bayesian game can be transformed into a normal-form game using the Harsanyi transformation [8].",
                "Once this is done, new, linear-program (LP)-based methods for finding high-reward strategies for normal-form games [5] can be used to find a strategy in the transformed game; this strategy can then be used for the Bayesian game.",
                "While methods exist for finding Bayes-Nash equilibria directly, without the Harsanyi transformation [10], they find only a 312 The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) single equilibrium in the general case, which may not be of high reward.",
                "Recent work [17] has led to efficient mixed-integer linear program techniques to find the best Nash equilibrium for a given agent.",
                "However, these techniques do require a normal-form game, and so to compare the policies given by ASAP against the optimal policy, as well as against the highest-reward Nash equilibrium, we must apply these techniques to the Harsanyi-transformed matrix.",
                "The next two subsections elaborate on how this is done. 3.1 Harsanyi Transformation The first step in solving Bayesian games is to apply the Harsanyi transformation [8] that converts the Bayesian game into a normal form game.",
                "Given that the Harsanyi transformation is a standard concept in game theory, we explain it briefly through a simple example in our patrolling domain without introducing the mathematical formulations.",
                "Let us assume there are two robber types a and b in the Bayesian game.",
                "Robber a will be active with probability α, and robber b will be active with probability 1 − α.",
                "The rules described in Section 2 allow us to construct simple payoff tables.",
                "Assume that there are two houses in the world (1 and 2) and hence there are two patrol routes (pure strategies) for the agent: {1,2} and {2,1}.",
                "The robber can rob either house 1 or house 2 and hence he has two strategies (denoted as 1l, 2l for robber type l).",
                "Since there are two types assumed (denoted as a and b), we construct two payoff tables (shown in Table 2) corresponding to the security agent playing a separate game with each of the two robber types with probabilities α and 1 − α.",
                "First, consider robber type a.",
                "Borrowing the notation from the domain section, we assign the following values to the variables: v1,x = v1,q = 3/4, v2,x = v2,q = 1/4, cx = 1/2, cq = 1, p1 = 1, p2 = 1/2.",
                "Using these values we construct a base payoff table as the payoff for the game against robber type a.",
                "For example, if the security agent chooses route {1,2} when robber a is active, and robber a chooses house 1, the robber receives a reward of -1 (for being caught) and the agent receives a reward of 0.5 for catching the robber.",
                "The payoffs for the game against robber type b are constructed using different values.",
                "Security agent: {1,2} {2,1} Robber a 1a -1, .5 -.375, .125 2a -.125, -.125 -1, .5 Robber b 1b -.9, .6 -.275, .225 2b -.025, -.025 -.9, .6 Table 2: Payoff tables: Security Agent vs Robbers a and b Using the Harsanyi technique involves introducing a chance node, that determines the robbers type, thus transforming the security agents incomplete information regarding the robber into imperfect information [3].",
                "The Bayesian equilibrium of the game is then precisely the Nash equilibrium of the imperfect information game.",
                "The transformed, normal-form game is shown in Table 3.",
                "In the transformed game, the security agent is the column player, and the set of all robber types together is the row player.",
                "Suppose that robber type a robs house 1 and robber type b robs house 2, while the security agent chooses patrol {1,2}.",
                "Then, the security agent and the robber receive an expected payoff corresponding to their payoffs from the agent encountering robber a at house 1 with probability α and robber b at house 2 with probability 1 − α. 3.2 Finding an Optimal Strategy Although a Nash equilibrium is the standard solution concept for games in which agents choose strategies simultaneously, in our security domain, the security agent (the leader) can gain an advantage by committing to a mixed strategy in advance.",
                "Since the followers (the robbers) will know the leaders strategy, the optimal response for the followers will be a pure strategy.",
                "Given the common assumption, taken in [5], in the case where followers are indifferent, they will choose the strategy that benefits the leader, there must exist a guaranteed optimal strategy for the leader [5].",
                "From the Bayesian game in Table 2, we constructed the Harsanyi transformed bimatrix in Table 3.",
                "The strategies for each player (security agent or robber) in the transformed game correspond to all combinations of possible strategies taken by each of that players types.",
                "Therefore, we denote X = σθ1 1 = σ1 and Q = σθ2 2 as the index sets of the security agent and robbers pure strategies respectively, with R and C as the corresponding payoff matrices.",
                "Rij is the reward of the security agent and Cij is the reward of the robbers when the security agent takes pure strategy i and the robbers take pure strategy j.",
                "A mixed strategy for the security agent is a probability distribution over its set of pure strategies and will be represented by a vector x = (px1, px2, . . . , px|X|), where pxi ≥ 0 and P pxi = 1.",
                "Here, pxi is the probability that the security agent will choose its ith pure strategy.",
                "The optimal mixed strategy for the security agent can be found in time polynomial in the number of rows in the normal form game using the following linear program formulation from [5].",
                "For every possible pure strategy j by the follower (the set of all robber types), max P i∈X pxiRij s.t. ∀j ∈ Q, P i∈σ1 pxiCij ≥ P i∈σ1 pxiCij P i∈X pxi = 1 ∀i∈X , pxi >= 0 (1) Then, for all feasible follower strategies j, choose the one that maximizes P i∈X pxiRij, the reward for the security agent (leader).",
                "The pxi variables give the optimal strategy for the security agent.",
                "Note that while this method is polynomial in the number of rows in the transformed, normal-form game, the number of rows increases exponentially with the number of robber types.",
                "Using this method for a Bayesian game thus requires running |σ2||θ2| separate linear programs.",
                "This is no surprise, since finding the leaders optimal strategy in a Bayesian Stackelberg game is NP-hard [5]. 4.",
                "HEURISTIC APPROACHES Given that finding the optimal strategy for the leader is NP-hard, we provide a <br>heuristic approach</br>.",
                "In this heuristic we limit the possible mixed strategies of the leader to select actions with probabilities that are integer multiples of 1/k for a predetermined integer k. Previous work [14] has shown that strategies with high entropy are beneficial for security applications when opponents utilities are completely unknown.",
                "In our domain, if utilities are not considered, this method will result in uniform-distribution strategies.",
                "One advantage of such strategies is that they are compact to represent (as fractions) and simple to understand; therefore they can be efficiently implemented by real organizations.",
                "We aim to maintain the advantage provided by simple strategies for our security application problem, incorporating the effect of the robbers rewards on the security agents rewards.",
                "Thus, the ASAP heuristic will produce strategies which are k-uniform.",
                "A mixed strategy is denoted k-uniform if it is a uniform distribution on a multiset S of pure strategies with |S| = k. A multiset is a set whose elements may be repeated multiple times; thus, for example, the mixed strategy corresponding to the multiset {1, 1, 2} would take strategy 1 The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) 313 {1,2} {2,1} {1a, 1b} −1α − .9(1 − α), .5α + .6(1 − α) −.375α − .275(1 − α), .125α + .225(1 − α) {1a, 2b} −1α − .025(1 − α), .5α − .025(1 − α) −.375α − .9(1 − α), .125α + .6(1 − α) {2a, 1b} −.125α − .9(1 − α), −.125α + .6(1 − α) −1α − .275(1 − α), .5α + .225(1 − α) {2a, 2b} −.125α − .025(1 − α), −.125α − .025(1 − α) −1α − .9(1 − α), .5α + .6(1 − α) Table 3: Harsanyi Transformed Payoff Table with probability 2/3 and strategy 2 with probability 1/3.",
                "ASAP allows the size of the multiset to be chosen in order to balance the complexity of the strategy reached with the goal that the identified strategy will yield a high reward.",
                "Another advantage of the ASAP heuristic is that it operates directly on the compact Bayesian representation, without requiring the Harsanyi transformation.",
                "This is because the different follower (robber) types are independent of each other.",
                "Hence, evaluating the leader strategy against a Harsanyi-transformed game matrix is equivalent to evaluating against each of the game matrices for the individual follower types.",
                "This independence property is exploited in ASAP to yield a decomposition scheme.",
                "Note that the LP method introduced by [5] to compute optimal Stackelberg policies is unlikely to be decomposable into a small number of games as it was shown to be NP-hard for Bayes-Nash problems.",
                "Finally, note that ASAP requires the solution of only one optimization problem, rather than solving a series of problems as in the LP method of [5].",
                "For a single follower type, the algorithm works the following way.",
                "Given a particular k, for each possible mixed strategy x for the leader that corresponds to a multiset of size k, evaluate the leaders payoff from x when the follower plays a reward-maximizing pure strategy.",
                "We then take the mixed strategy with the highest payoff.",
                "We need only to consider the reward-maximizing pure strategies of the followers (robbers), since for a given fixed strategy x of the security agent, each robber type faces a problem with fixed linear rewards.",
                "If a mixed strategy is optimal for the robber, then so are all the pure strategies in the support of that mixed strategy.",
                "Note also that because we limit the leaders strategies to take on discrete values, the assumption from Section 3.2 that the followers will break ties in the leaders favor is not significant, since ties will be unlikely to arise.",
                "This is because, in domains where rewards are drawn from any random distribution, the probability of a follower having more than one pure optimal response to a given leader strategy approaches zero, and the leader will have only a finite number of possible mixed strategies.",
                "Our approach to characterize the optimal strategy for the security agent makes use of properties of linear programming.",
                "We briefly outline these results here for completeness, for detailed discussion and proofs see one of many references on the topic, such as [2].",
                "Every linear programming problem, such as: max cT x | Ax = b, x ≥ 0, has an associated dual linear program, in this case: min bT y | AT y ≥ c. These primal/dual pairs of problems satisfy weak duality: For any x and y primal and dual feasible solutions respectively, cT x ≤ bT y.",
                "Thus a pair of feasible solutions is optimal if cT x = bT y, and the problems are said to satisfy strong duality.",
                "In fact if a linear program is feasible and has a bounded optimal solution, then the dual is also feasible and there is a pair x∗ , y∗ that satisfies cT x∗ = bT y∗ .",
                "These optimal solutions are characterized with the following optimality conditions (as defined in [2]): • primal feasibility: Ax = b, x ≥ 0 • dual feasibility: AT y ≥ c • complementary slackness: xi(AT y − c)i = 0 for all i.",
                "Note that this last condition implies that cT x = xT AT y = bT y, which proves optimality for primal dual feasible solutions x and y.",
                "In the following subsections, we first define the problem in its most intuititive form as a mixed-integer quadratic program (MIQP), and then show how this problem can be converted into a mixedinteger linear program (MILP). 4.1 Mixed-Integer Quadratic Program We begin with the case of a single type of follower.",
                "Let the leader be the row player and the follower the column player.",
                "We denote by x the vector of strategies of the leader and q the vector of strategies of the follower.",
                "We also denote X and Q the index sets of the leader and followers pure strategies, respectively.",
                "The payoff matrices R and C correspond to: Rij is the reward of the leader and Cij is the reward of the follower when the leader takes pure strategy i and the follower takes pure strategy j.",
                "Let k be the size of the multiset.",
                "We first fix the policy of the leader to some k-uniform policy x.",
                "The value xi is the number of times pure strategy i is used in the k-uniform policy, which is selected with probability xi/k.",
                "We formulate the optimization problem the follower solves to find its optimal response to x as the following linear program: max X j∈Q X i∈X 1 k Cijxi qj s.t.",
                "P j∈Q qj = 1 q ≥ 0. (2) The objective function maximizes the followers expected reward given x, while the constraints make feasible any mixed strategy q for the follower.",
                "The dual to this linear programming problem is the following: min a s.t. a ≥ X i∈X 1 k Cijxi j ∈ Q. (3) From strong duality and complementary slackness we obtain that the followers maximum reward value, a, is the value of every pure strategy with qj > 0, that is in the support of the optimal mixed strategy.",
                "Therefore each of these pure strategies is optimal.",
                "Optimal solutions to the followers problem are characterized by linear programming optimality conditions: primal feasibility constraints in (2), dual feasibility constraints in (3), and complementary slackness qj a − X i∈X 1 k Cijxi ! = 0 j ∈ Q.",
                "These conditions must be included in the problem solved by the leader in order to consider only best responses by the follower to the k-uniform policy x. 314 The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) The leader seeks the k-uniform solution x that maximizes its own payoff, given that the follower uses an optimal response q(x).",
                "Therefore the leader solves the following integer problem: max X i∈X X j∈Q 1 k Rijq(x)j xi s.t.",
                "P i∈X xi = k xi ∈ {0, 1, . . . , k}. (4) Problem (4) maximizes the leaders reward with the followers best response (qj for fixed leaders policy x and hence denoted q(x)j) by selecting a uniform policy from a multiset of constant size k. We complete this problem by including the characterization of q(x) through linear programming optimality conditions.",
                "To simplify writing the complementary slackness conditions, we will constrain q(x) to be only optimal pure strategies by just considering integer solutions of q(x).",
                "The leaders problem becomes: maxx,q X i∈X X j∈Q 1 k Rijxiqj s.t.",
                "P i xi = kP j∈Q qj = 1 0 ≤ (a − P i∈X 1 k Cijxi) ≤ (1 − qj)M xi ∈ {0, 1, ...., k} qj ∈ {0, 1}. (5) Here, the constant M is some large number.",
                "The first and fourth constraints enforce a k-uniform policy for the leader, and the second and fifth constraints enforce a feasible pure strategy for the follower.",
                "The third constraint enforces dual feasibility of the followers problem (leftmost inequality) and the complementary slackness constraint for an optimal pure strategy q for the follower (rightmost inequality).",
                "In fact, since only one pure strategy can be selected by the follower, say qh = 1, this last constraint enforces that a = P i∈X 1 k Cihxi imposing no additional constraint for all other pure strategies which have qj = 0.",
                "We conclude this subsection noting that Problem (5) is an integer program with a non-convex quadratic objective in general, as the matrix R need not be positive-semi-definite.",
                "Efficient solution methods for non-linear, non-convex integer problems remains a challenging research question.",
                "In the next section we show a reformulation of this problem as a linear integer programming problem, for which a number of efficient commercial solvers exist. 4.2 Mixed-Integer Linear Program We can linearize the quadratic program of Problem 5 through the change of variables zij = xiqj, obtaining the following problem maxq,z P i∈X P j∈Q 1 k Rijzij s.t.",
                "P i∈X P j∈Q zij = k P j∈Q zij ≤ k kqj ≤ P i∈X zij ≤ k P j∈Q qj = 1 0 ≤ (a − P i∈X 1 k Cij( P h∈Q zih)) ≤ (1 − qj)M zij ∈ {0, 1, ...., k} qj ∈ {0, 1} (6) PROPOSITION 1.",
                "Problems (5) and (6) are equivalent.",
                "Proof: Consider x, q a feasible solution of (5).",
                "We will show that q, zij = xiqj is a feasible solution of (6) of same objective function value.",
                "The equivalence of the objective functions, and constraints 4, 6 and 7 of (6) are satisfied by construction.",
                "The fact that P j∈Q zij = xi as P j∈Q qj = 1 explains constraints 1, 2, and 5 of (6).",
                "Constraint 3 of (6) is satisfied because P i∈X zij = kqj.",
                "Let us now consider q, z feasible for (6).",
                "We will show that q and xi = P j∈Q zij are feasible for (5) with the same objective value.",
                "In fact all constraints of (5) are readily satisfied by construction.",
                "To see that the objectives match, notice that if qh = 1 then the third constraint in (6) implies that P i∈X zih = k, which means that zij = 0 for all i ∈ X and all j = h. Therefore, xiqj = X l∈Q zilqj = zihqj = zij.",
                "This last equality is because both are 0 when j = h. This shows that the transformation preserves the objective function value, completing the proof.",
                "Given this transformation to a mixed-integer linear program (MILP), we now show how we can apply our decomposition technique on the MILP to obtain significant speedups for Bayesian games with multiple follower types. 5.",
                "DECOMPOSITION FOR MULTIPLE ADVERSARIES The MILP developed in the previous section handles only one follower.",
                "Since our security scenario contains multiple follower (robber) types, we change the response function for the follower from a pure strategy into a weighted combination over various pure follower strategies where the weights are probabilities of occurrence of each of the follower types. 5.1 Decomposed MIQP To admit multiple adversaries in our framework, we modify the notation defined in the previous section to reason about multiple follower types.",
                "We denote by x the vector of strategies of the leader and ql the vector of strategies of follower l, with L denoting the index set of follower types.",
                "We also denote by X and Q the index sets of leader and follower ls pure strategies, respectively.",
                "We also index the payoff matrices on each follower l, considering the matrices Rl and Cl .",
                "Using this modified notation, we characterize the optimal solution of follower ls problem given the leaders k-uniform policy x, with the following optimality conditions: X j∈Q ql j = 1 al − X i∈X 1 k Cl ijxi ≥ 0 ql j(al − X i∈X 1 k Cl ijxi) = 0 ql j ≥ 0 Again, considering only optimal pure strategies for follower ls problem we can linearize the complementarity constraint above.",
                "We incorporate these constraints on the leaders problem that selects the optimal k-uniform policy.",
                "Therefore, given a priori probabilities pl , with l ∈ L of facing each follower, the leader solves the following problem: The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) 315 maxx,q X i∈X X l∈L X j∈Q pl k Rl ijxiql j s.t.",
                "P i xi = kP j∈Q ql j = 1 0 ≤ (al − P i∈X 1 k Cl ijxi) ≤ (1 − ql j)M xi ∈ {0, 1, ...., k} ql j ∈ {0, 1}. (7) Problem (7) for a Bayesian game with multiple follower types is indeed equivalent to Problem (5) on the payoff matrix obtained from the Harsanyi transformation of the game.",
                "In fact, every pure strategy j in Problem (5) corresponds to a sequence of pure strategies jl, one for each follower l ∈ L. This means that qj = 1 if and only if ql jl = 1 for all l ∈ L. In addition, given the a priori probabilities pl of facing player l, the reward in the Harsanyi transformation payoff table is Rij = P l∈L pl Rl ijl .",
                "The same relation holds between C and Cl .",
                "These relations between a pure strategy in the equivalent normal form game and pure strategies in the individual games with each followers are key in showing these problems are equivalent. 5.2 Decomposed MILP We can linearize the quadratic programming problem 7 through the change of variables zl ij = xiql j, obtaining the following problem maxq,z P i∈X P l∈L P j∈Q pl k Rl ijzl ij s.t.",
                "P i∈X P j∈Q zl ij = k P j∈Q zl ij ≤ k kql j ≤ P i∈X zl ij ≤ k P j∈Q ql j = 1 0 ≤ (al − P i∈X 1 k Cl ij( P h∈Q zl ih)) ≤ (1 − ql j)M P j∈Q zl ij = P j∈Q z1 ij zl ij ∈ {0, 1, ...., k} ql j ∈ {0, 1} (8) PROPOSITION 2.",
                "Problems (7) and (8) are equivalent.",
                "Proof: Consider x, ql , al with l ∈ L a feasible solution of (7).",
                "We will show that ql , al , zl ij = xiql j is a feasible solution of (8) of same objective function value.",
                "The equivalence of the objective functions, and constraints 4, 7 and 8 of (8) are satisfied by construction.",
                "The fact that P j∈Q zl ij = xi as P j∈Q ql j = 1 explains constraints 1, 2, 5 and 6 of (8).",
                "Constraint 3 of (8) is satisfied because P i∈X zl ij = kql j.",
                "Lets now consider ql , zl , al feasible for (8).",
                "We will show that ql , al and xi = P j∈Q z1 ij are feasible for (7) with the same objective value.",
                "In fact all constraints of (7) are readily satisfied by construction.",
                "To see that the objectives match, notice for each l one ql j must equal 1 and the rest equal 0.",
                "Let us say that ql jl = 1, then the third constraint in (8) implies that P i∈X zl ijl = k, which means that zl ij = 0 for all i ∈ X and all j = jl.",
                "In particular this implies that xi = X j∈Q z1 ij = z1 ij1 = zl ijl , the last equality from constraint 6 of (8).",
                "Therefore xiql j = zl ijl ql j = zl ij.",
                "This last equality is because both are 0 when j = jl.",
                "Effectively, constraint 6 ensures that all the adversaries are calculating their best responses against a particular fixed policy of the agent.",
                "This shows that the transformation preserves the objective function value, completing the proof.",
                "We can therefore solve this equivalent linear integer program with efficient integer programming packages which can handle problems with thousands of integer variables.",
                "We implemented the decomposed MILP and the results are shown in the following section. 6.",
                "EXPERIMENTAL RESULTS The patrolling domain and the payoffs for the associated game are detailed in Sections 2 and 3.",
                "We performed experiments for this game in worlds of three and four houses with patrols consisting of two houses.",
                "The description given in Section 2 is used to generate a base case for both the security agent and robber payoff functions.",
                "The payoff tables for additional robber types are constructed and added to the game by adding a random distribution of varying size to the payoffs in the base case.",
                "All games are normalized so that, for each robber type, the minimum and maximum payoffs to the security agent and robber are 0 and 1, respectively.",
                "Using the data generated, we performed the experiments using four methods for generating the security agents strategy: • uniform randomization • ASAP • the multiple linear programs method from [5] (to find the true optimal strategy) • the highest reward Bayes-Nash equilibrium, found using the MIP-Nash algorithm [17] The last three methods were applied using CPLEX 8.1.",
                "Because the last two methods are designed for normal-form games rather than Bayesian games, the games were first converted using the Harsanyi transformation [8].",
                "The uniform randomization method is simply choosing a uniform random policy over all possible patrol routes.",
                "We use this method as a simple baseline to measure the performance of our heuristics.",
                "We anticipated that the uniform policy would perform reasonably well since maximum-entropy policies have been shown to be effective in multiagent security domains [14].",
                "The highest-reward Bayes-Nash equilibria were used in order to demonstrate the higher reward gained by looking for an optimal policy rather than an equilibria in Stackelberg games such as our security domain.",
                "Based on our experiments we present three sets of graphs to demonstrate (1) the runtime of ASAP compared to other common methods for finding a strategy, (2) the reward guaranteed by ASAP compared to other methods, and (3) the effect of varying the parameter k, the size of the multiset, on the performance of ASAP.",
                "In the first two sets of graphs, ASAP is run using a multiset of 80 elements; in the third set this number is varied.",
                "The first set of graphs, shown in Figure 1 shows the runtime graphs for three-house (left column) and four-house (right column) domains.",
                "Each of the three rows of graphs corresponds to a different randomly-generated scenario.",
                "The x-axis shows the number of robber types the security agent faces and the y-axis of the graph shows the runtime in seconds.",
                "All experiments that were not concluded in 30 minutes (1800 seconds) were cut off.",
                "The runtime for the uniform policy is always negligible irrespective of the number of adversaries and hence is not shown.",
                "The ASAP algorithm clearly outperforms the optimal, multipleLP method as well as the MIP-Nash algorithm for finding the highestreward Bayes-Nash equilibrium with respect to runtime.",
                "For a 316 The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) Figure 1: Runtimes for various algorithms on problems of 3 and 4 houses. domain of three houses, the optimal method cannot reach a solution for more than seven robber types, and for four houses it cannot solve for more than six types within the cutoff time in any of the three scenarios.",
                "MIP-Nash solves for even fewer robber types within the cutoff time.",
                "On the other hand, ASAP runs much faster, and is able to solve for at least 20 adversaries for the three-house scenarios and for at least 12 adversaries in the four-house scenarios within the cutoff time.",
                "The runtime of ASAP does not increase strictly with the number of robber types for each scenario, but in general, the addition of more types increases the runtime required.",
                "The second set of graphs, Figure 2, shows the reward to the patrol agent given by each method for three scenarios in the three-house (left column) and four-house (right column) domains.",
                "This reward is the utility received by the security agent in the patrolling game, and not as a percentage of the optimal reward, since it was not possible to obtain the optimal reward as the number of robber types increased.",
                "The uniform policy consistently provides the lowest reward in both domains; while the optimal method of course produces the optimal reward.",
                "The ASAP method remains consistently close to the optimal, even as the number of robber types increases.",
                "The highest-reward Bayes-Nash equilibria, provided by the MIPNash method, produced rewards higher than the uniform method, but lower than ASAP.",
                "This difference clearly illustrates the gains in the patrolling domain from committing to a strategy as the leader in a Stackelberg game, rather than playing a standard Bayes-Nash strategy.",
                "The third set of graphs, shown in Figure 3 shows the effect of the multiset size on runtime in seconds (left column) and reward (right column), again expressed as the reward received by the security agent in the patrolling game, and not a percentage of the optimal Figure 2: Reward for various algorithms on problems of 3 and 4 houses. reward.",
                "Results here are for the three-house domain.",
                "The trend is that as as the multiset size is increased, the runtime and reward level both increase.",
                "Not surprisingly, the reward increases monotonically as the multiset size increases, but what is interesting is that there is relatively little benefit to using a large multiset in this domain.",
                "In all cases, the reward given by a multiset of 10 elements was within at least 96% of the reward given by an 80-element multiset.",
                "The runtime does not always increase strictly with the multiset size; indeed in one example (scenario 2 with 20 robber types), using a multiset of 10 elements took 1228 seconds, while using 80 elements only took 617 seconds.",
                "In general, runtime should increase since a larger multiset means a larger domain for the variables in the MILP, and thus a larger search space.",
                "However, an increase in the number of variables can sometimes allow for a policy to be constructed more quickly due to more flexibility in the problem. 7.",
                "SUMMARY AND RELATED WORK This paper focuses on security for agents patrolling in hostile environments.",
                "In these environments, intentional threats are caused by adversaries about whom the security patrolling agents have incomplete information.",
                "Specifically, we deal with situations where the adversaries actions and payoffs are known but the exact adversary type is unknown to the security agent.",
                "Agents acting in the real world quite frequently have such incomplete information about other agents.",
                "Bayesian games have been a popular choice to model such incomplete information games [3].",
                "The Gala toolkit is one method for defining such games [9] without requiring the game to be represented in normal form via the Harsanyi transformation [8]; Galas guarantees are focused on fully competitive games.",
                "Much work has been done on finding optimal Bayes-Nash equilbria for The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) 317 Figure 3: Reward for ASAP using multisets of 10, 30, and 80 elements subclasses of Bayesian games, finding single Bayes-Nash equilibria for general Bayesian games [10] or approximate Bayes-Nash equilibria [18].",
                "Less attention has been paid to finding the optimal strategy to commit to in a Bayesian game (the Stackelberg scenario [15]).",
                "However, the complexity of this problem was shown to be NP-hard in the general case [5], which also provides algorithms for this problem in the non-Bayesian case.",
                "Therefore, we present a heuristic called ASAP, with three key advantages towards addressing this problem.",
                "First, ASAP searches for the highest reward strategy, rather than a Bayes-Nash equilibrium, allowing it to find feasible strategies that exploit the natural first-mover advantage of the game.",
                "Second, it provides strategies which are simple to understand, represent, and implement.",
                "Third, it operates directly on the compact, Bayesian game representation, without requiring conversion to normal form.",
                "We provide an efficient Mixed Integer Linear Program (MILP) implementation for ASAP, along with experimental results illustrating significant speedups and higher rewards over other approaches.",
                "Our k-uniform strategies are similar to the k-uniform strategies of [12].",
                "While that work provides epsilon error-bounds based on the k-uniform strategies, their solution concept is still that of a Nash equilibrium, and they do not provide efficient algorithms for obtaining such k-uniform strategies.",
                "This contrasts with ASAP, where our emphasis is on a highly efficient <br>heuristic approach</br> that is not focused on equilibrium solutions.",
                "Finally the patrolling problem which motivated our work has recently received growing attention from the multiagent community due to its wide range of applications [4, 13].",
                "However most of this work is focused on either limiting energy consumption involved in patrolling [7] or optimizing on criteria like the length of the path traveled [4, 13], without reasoning about any explicit model of an adversary[14].",
                "Acknowledgments : This research is supported by the United States Department of Homeland Security through Center for Risk and Economic Analysis of Terrorism Events (CREATE).",
                "It is also supported by the Defense Advanced Research Projects Agency (DARPA), through the Department of the Interior, NBC, Acquisition Services Division, under Contract No.",
                "NBCHD030010.",
                "Sarit Kraus is also affiliated with UMIACS. 8.",
                "REFERENCES [1] R. W. Beard and T. McLain.",
                "Multiple UAV cooperative search under collision avoidance and limited range communication constraints.",
                "In IEEE CDC, 2003. [2] D. Bertsimas and J. Tsitsiklis.",
                "Introduction to Linear Optimization.",
                "Athena Scientific, 1997. [3] J. Brynielsson and S. Arnborg.",
                "Bayesian games for threat prediction and situation analysis.",
                "In FUSION, 2004. [4] Y. Chevaleyre.",
                "Theoretical analysis of multi-agent patrolling problem.",
                "In AAMAS, 2004. [5] V. Conitzer and T. Sandholm.",
                "Choosing the best strategy to commit to.",
                "In ACM Conference on Electronic Commerce, 2006. [6] D. Fudenberg and J. Tirole.",
                "Game Theory.",
                "MIT Press, 1991. [7] C. Gui and P. Mohapatra.",
                "Virtual patrol: A new power conservation design for surveillance using sensor networks.",
                "In IPSN, 2005. [8] J. C. Harsanyi and R. Selten.",
                "A generalized Nash solution for two-person bargaining games with incomplete information.",
                "Management Science, 18(5):80-106, 1972. [9] D. Koller and A. Pfeffer.",
                "Generating and solving imperfect information games.",
                "In IJCAI, pages 1185-1193, 1995. [10] D. Koller and A. Pfeffer.",
                "Representations and solutions for game-theoretic problems.",
                "Artificial Intelligence, 94(1):167-215, 1997. [11] C. Lemke and J. Howson.",
                "Equilibrium points of bimatrix games.",
                "Journal of the Society for Industrial and Applied Mathematics, 12:413-423, 1964. [12] R. J. Lipton, E. Markakis, and A. Mehta.",
                "Playing large games using simple strategies.",
                "In ACM Conference on Electronic Commerce, 2003. [13] A. Machado, G. Ramalho, J. D. Zucker, and A. Drougoul.",
                "Multi-agent patrolling: an empirical analysis on alternative architectures.",
                "In MABS, 2002. [14] P. Paruchuri, M. Tambe, F. Ordonez, and S. Kraus.",
                "Security in multiagent systems by policy randomization.",
                "In AAMAS, 2006. [15] T. Roughgarden.",
                "Stackelberg scheduling strategies.",
                "In ACM Symposium on TOC, 2001. [16] S. Ruan, C. Meirina, F. Yu, K. R. Pattipati, and R. L. Popp.",
                "Patrolling in a stochastic environment.",
                "In 10th Intl.",
                "Command and Control Research Symp., 2005. [17] T. Sandholm, A. Gilpin, and V. Conitzer.",
                "Mixed-integer programming methods for finding nash equilibria.",
                "In AAAI, 2005. [18] S. Singh, V. Soni, and M. Wellman.",
                "Computing approximate Bayes-Nash equilibria with tree-games of incomplete information.",
                "In ACM Conference on Electronic Commerce, 2004. 318 The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07)"
            ],
            "original_annotated_samples": [],
            "translated_annotated_samples": [
                "Un \"enfoque heurístico\" eficiente para la seguridad contra múltiples adversarios Praveen Paruchuri, Jonathan P. Pearce, Milind Tambe, Fernando Ordonez Universidad del Sur de California en Los Ángeles, CA 90089 {Paruchur, Jppearce, Tambe, Fordon}@usc.edu Sarit Kraus Bar-Ilan University Ramat-Gan 52900, Israel sarit@cs.biu.ac.il Resumen en dominios multiagentes adversos, seguridad, comúnmente definida como la capacidad de lidiar con las amenazas intencionales de otros agentes, es un tema crítico.",
                "Enfoques heurísticos Dados que encontrar la estrategia óptima para el líder es NP-Hard, proporcionamos un \"enfoque heurístico\".",
                "Esto contrasta con lo antes posible, donde nuestro énfasis está en un \"enfoque heurístico\" altamente eficiente que no se centra en las soluciones de equilibrio."
            ],
            "translated_text": "",
            "candidates": [
                "enfoque heurístico",
                "enfoque heurístico",
                "enfoque heurístico",
                "enfoque heurístico",
                "enfoque heurístico",
                "enfoque heurístico"
            ],
            "error": []
        },
        "decomposition for multiple adversary": {
            "translated_key": "descomposición para múltiples adversarios",
            "is_in_text": false,
            "original_annotated_sentences": [
                "An Efficient Heuristic Approach for Security Against Multiple Adversaries Praveen Paruchuri, Jonathan P. Pearce, Milind Tambe, Fernando Ordonez University of Southern California Los Angeles, CA 90089 {paruchur, jppearce, tambe, fordon}@usc.edu Sarit Kraus Bar-Ilan University Ramat-Gan 52900, Israel sarit@cs.biu.ac.il ABSTRACT In adversarial multiagent domains, security, commonly defined as the ability to deal with intentional threats from other agents, is a critical issue.",
                "This paper focuses on domains where these threats come from unknown adversaries.",
                "These domains can be modeled as Bayesian games; much work has been done on finding equilibria for such games.",
                "However, it is often the case in multiagent security domains that one agent can commit to a mixed strategy which its adversaries observe before choosing their own strategies.",
                "In this case, the agent can maximize reward by finding an optimal strategy, without requiring equilibrium.",
                "Previous work has shown this problem of optimal strategy selection to be NP-hard.",
                "Therefore, we present a heuristic called ASAP, with three key advantages to address the problem.",
                "First, ASAP searches for the highest-reward strategy, rather than a Bayes-Nash equilibrium, allowing it to find feasible strategies that exploit the natural first-mover advantage of the game.",
                "Second, it provides strategies which are simple to understand, represent, and implement.",
                "Third, it operates directly on the compact, Bayesian game representation, without requiring conversion to normal form.",
                "We provide an efficient Mixed Integer Linear Program (MILP) implementation for ASAP, along with experimental results illustrating significant speedups and higher rewards over other approaches.",
                "Categories and Subject Descriptors I.2.11 [Computing Methodologies]: Artificial Intelligence: Distributed Artificial Intelligence - Intelligent Agents General Terms Security, Design, Theory 1.",
                "INTRODUCTION In many multiagent domains, agents must act in order to provide security against attacks by adversaries.",
                "A common issue that agents face in such security domains is uncertainty about the adversaries they may be facing.",
                "For example, a security robot may need to make a choice about which areas to patrol, and how often [16].",
                "However, it will not know in advance exactly where a robber will choose to strike.",
                "A team of unmanned aerial vehicles (UAVs) [1] monitoring a region undergoing a humanitarian crisis may also need to choose a patrolling policy.",
                "They must make this decision without knowing in advance whether terrorists or other adversaries may be waiting to disrupt the mission at a given location.",
                "It may indeed be possible to model the motivations of types of adversaries the agent or agent team is likely to face in order to target these adversaries more closely.",
                "However, in both cases, the security robot or UAV team will not know exactly which kinds of adversaries may be active on any given day.",
                "A common approach for choosing a policy for agents in such scenarios is to model the scenarios as Bayesian games.",
                "A Bayesian game is a game in which agents may belong to one or more types; the type of an agent determines its possible actions and payoffs.",
                "The distribution of adversary types that an agent will face may be known or inferred from historical data.",
                "Usually, these games are analyzed according to the solution concept of a Bayes-Nash equilibrium, an extension of the Nash equilibrium for Bayesian games.",
                "However, in many settings, a Nash or Bayes-Nash equilibrium is not an appropriate solution concept, since it assumes that the agents strategies are chosen simultaneously [5].",
                "In some settings, one player can (or must) commit to a strategy before the other players choose their strategies.",
                "These scenarios are known as Stackelberg games [6].",
                "In a Stackelberg game, a leader commits to a strategy first, and then a follower (or group of followers) selfishly optimize their own rewards, considering the action chosen by the leader.",
                "For example, the security agent (leader) must first commit to a strategy for patrolling various areas.",
                "This strategy could be a mixed strategy in order to be unpredictable to the robbers (followers).",
                "The robbers, after observing the pattern of patrols over time, can then choose their strategy (which location to rob).",
                "Often, the leader in a Stackelberg game can attain a higher reward than if the strategies were chosen simultaneously.",
                "To see the advantage of being the leader in a Stackelberg game, consider a simple game with the payoff table as shown in Table 1.",
                "The leader is the row player and the follower is the column player.",
                "Here, the leaders payoff is listed first. 1 2 3 1 5,5 0,0 3,10 2 0,0 2,2 5,0 Table 1: Payoff table for example normal form game.",
                "The only Nash equilibrium for this game is when the leader plays 2 and the follower plays 2 which gives the leader a payoff of 2. 311 978-81-904262-7-5 (RPS) c 2007 IFAAMAS However, if the leader commits to a uniform mixed strategy of playing 1 and 2 with equal (0.5) probability, the followers best response is to play 3 to get an expected payoff of 5 (10 and 0 with equal probability).",
                "The leaders payoff would then be 4 (3 and 5 with equal probability).",
                "In this case, the leader now has an incentive to deviate and choose a pure strategy of 2 (to get a payoff of 5).",
                "However, this would cause the follower to deviate to strategy 2 as well, resulting in the Nash equilibrium.",
                "Thus, by committing to a strategy that is observed by the follower, and by avoiding the temptation to deviate, the leader manages to obtain a reward higher than that of the best Nash equilibrium.",
                "The problem of choosing an optimal strategy for the leader to commit to in a Stackelberg game is analyzed in [5] and found to be NP-hard in the case of a Bayesian game with multiple types of followers.",
                "Thus, efficient heuristic techniques for choosing highreward strategies in these games is an important open issue.",
                "Methods for finding optimal leader strategies for non-Bayesian games [5] can be applied to this problem by converting the Bayesian game into a normal-form game by the Harsanyi transformation [8].",
                "If, on the other hand, we wish to compute the highest-reward Nash equilibrium, new methods using mixed-integer linear programs (MILPs) [17] may be used, since the highest-reward Bayes-Nash equilibrium is equivalent to the corresponding Nash equilibrium in the transformed game.",
                "However, by transforming the game, the compact structure of the Bayesian game is lost.",
                "In addition, since the Nash equilibrium assumes a simultaneous choice of strategies, the advantages of being the leader are not considered.",
                "This paper introduces an efficient heuristic method for approximating the optimal leader strategy for security domains, known as ASAP (Agent Security via Approximate Policies).",
                "This method has three key advantages.",
                "First, it directly searches for an optimal strategy, rather than a Nash (or Bayes-Nash) equilibrium, thus allowing it to find high-reward non-equilibrium strategies like the one in the above example.",
                "Second, it generates policies with a support which can be expressed as a uniform distribution over a multiset of fixed size as proposed in [12].",
                "This allows for policies that are simple to understand and represent [12], as well as a tunable parameter (the size of the multiset) that controls the simplicity of the policy.",
                "Third, the method allows for a Bayes-Nash game to be expressed compactly without conversion to a normal-form game, allowing for large speedups over existing Nash methods such as [17] and [11].",
                "The rest of the paper is organized as follows.",
                "In Section 2 we fully describe the patrolling domain and its properties.",
                "Section 3 introduces the Bayesian game, the Harsanyi transformation, and existing methods for finding an optimal leaders strategy in a Stackelberg game.",
                "Then, in Section 4 the ASAP algorithm is presented for normal-form games, and in Section 5 we show how it can be adapted to the structure of Bayesian games with uncertain adversaries.",
                "Experimental results showing higher reward and faster policy computation over existing Nash methods are shown in Section 6, and we conclude with a discussion of related work in Section 7. 2.",
                "THE PATROLLING DOMAIN In most security patrolling domains, the security agents (like UAVs [1] or security robots [16]) cannot feasibly patrol all areas all the time.",
                "Instead, they must choose a policy by which they patrol various routes at different times, taking into account factors such as the likelihood of crime in different areas, possible targets for crime, and the security agents own resources (number of security agents, amount of available time, fuel, etc.).",
                "It is usually beneficial for this policy to be nondeterministic so that robbers cannot safely rob certain locations, knowing that they will be safe from the security agents [14].",
                "To demonstrate the utility of our algorithm, we use a simplified version of such a domain, expressed as a game.",
                "The most basic version of our game consists of two players: the security agent (the leader) and the robber (the follower) in a world consisting of m houses, 1 . . . m. The security agents set of pure strategies consists of possible routes of d houses to patrol (in an order).",
                "The security agent can choose a mixed strategy so that the robber will be unsure of exactly where the security agent may patrol, but the robber will know the mixed strategy the security agent has chosen.",
                "For example, the robber can observe over time how often the security agent patrols each area.",
                "With this knowledge, the robber must choose a single house to rob.",
                "We assume that the robber generally takes a long time to rob a house.",
                "If the house chosen by the robber is not on the security agents route, then the robber successfully robs it.",
                "Otherwise, if it is on the security agents route, then the earlier the house is on the route, the easier it is for the security agent to catch the robber before he finishes robbing it.",
                "We model the payoffs for this game with the following variables: • vl,x: value of the goods in house l to the security agent. • vl,q: value of the goods in house l to the robber. • cx: reward to the security agent of catching the robber. • cq: cost to the robber of getting caught. • pl: probability that the security agent can catch the robber at the lth house in the patrol (pl < pl ⇐⇒ l < l).",
                "The security agents set of possible pure strategies (patrol routes) is denoted by X and includes all d-tuples i =< w1, w2, ..., wd > with w1 . . . wd = 1 . . . m where no two elements are equal (the agent is not allowed to return to the same house).",
                "The robbers set of possible pure strategies (houses to rob) is denoted by Q and includes all integers j = 1 . . . m. The payoffs (security agent, robber) for pure strategies i, j are: • −vl,x, vl,q, for j = l /∈ i. • plcx +(1−pl)(−vl,x), −plcq +(1−pl)(vl,q), for j = l ∈ i.",
                "With this structure it is possible to model many different types of robbers who have differing motivations; for example, one robber may have a lower cost of getting caught than another, or may value the goods in the various houses differently.",
                "If the distribution of different robber types is known or inferred from historical data, then the game can be modeled as a Bayesian game [6]. 3.",
                "BAYESIAN GAMES A Bayesian game contains a set of N agents, and each agent n must be one of a given set of types θn.",
                "For our patrolling domain, we have two agents, the security agent and the robber. θ1 is the set of security agent types and θ2 is the set of robber types.",
                "Since there is only one type of security agent, θ1 contains only one element.",
                "During the game, the robber knows its type but the security agent does not know the robbers type.",
                "For each agent (the security agent or the robber) n, there is a set of strategies σn and a utility function un : θ1 × θ2 × σ1 × σ2 → .",
                "A Bayesian game can be transformed into a normal-form game using the Harsanyi transformation [8].",
                "Once this is done, new, linear-program (LP)-based methods for finding high-reward strategies for normal-form games [5] can be used to find a strategy in the transformed game; this strategy can then be used for the Bayesian game.",
                "While methods exist for finding Bayes-Nash equilibria directly, without the Harsanyi transformation [10], they find only a 312 The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) single equilibrium in the general case, which may not be of high reward.",
                "Recent work [17] has led to efficient mixed-integer linear program techniques to find the best Nash equilibrium for a given agent.",
                "However, these techniques do require a normal-form game, and so to compare the policies given by ASAP against the optimal policy, as well as against the highest-reward Nash equilibrium, we must apply these techniques to the Harsanyi-transformed matrix.",
                "The next two subsections elaborate on how this is done. 3.1 Harsanyi Transformation The first step in solving Bayesian games is to apply the Harsanyi transformation [8] that converts the Bayesian game into a normal form game.",
                "Given that the Harsanyi transformation is a standard concept in game theory, we explain it briefly through a simple example in our patrolling domain without introducing the mathematical formulations.",
                "Let us assume there are two robber types a and b in the Bayesian game.",
                "Robber a will be active with probability α, and robber b will be active with probability 1 − α.",
                "The rules described in Section 2 allow us to construct simple payoff tables.",
                "Assume that there are two houses in the world (1 and 2) and hence there are two patrol routes (pure strategies) for the agent: {1,2} and {2,1}.",
                "The robber can rob either house 1 or house 2 and hence he has two strategies (denoted as 1l, 2l for robber type l).",
                "Since there are two types assumed (denoted as a and b), we construct two payoff tables (shown in Table 2) corresponding to the security agent playing a separate game with each of the two robber types with probabilities α and 1 − α.",
                "First, consider robber type a.",
                "Borrowing the notation from the domain section, we assign the following values to the variables: v1,x = v1,q = 3/4, v2,x = v2,q = 1/4, cx = 1/2, cq = 1, p1 = 1, p2 = 1/2.",
                "Using these values we construct a base payoff table as the payoff for the game against robber type a.",
                "For example, if the security agent chooses route {1,2} when robber a is active, and robber a chooses house 1, the robber receives a reward of -1 (for being caught) and the agent receives a reward of 0.5 for catching the robber.",
                "The payoffs for the game against robber type b are constructed using different values.",
                "Security agent: {1,2} {2,1} Robber a 1a -1, .5 -.375, .125 2a -.125, -.125 -1, .5 Robber b 1b -.9, .6 -.275, .225 2b -.025, -.025 -.9, .6 Table 2: Payoff tables: Security Agent vs Robbers a and b Using the Harsanyi technique involves introducing a chance node, that determines the robbers type, thus transforming the security agents incomplete information regarding the robber into imperfect information [3].",
                "The Bayesian equilibrium of the game is then precisely the Nash equilibrium of the imperfect information game.",
                "The transformed, normal-form game is shown in Table 3.",
                "In the transformed game, the security agent is the column player, and the set of all robber types together is the row player.",
                "Suppose that robber type a robs house 1 and robber type b robs house 2, while the security agent chooses patrol {1,2}.",
                "Then, the security agent and the robber receive an expected payoff corresponding to their payoffs from the agent encountering robber a at house 1 with probability α and robber b at house 2 with probability 1 − α. 3.2 Finding an Optimal Strategy Although a Nash equilibrium is the standard solution concept for games in which agents choose strategies simultaneously, in our security domain, the security agent (the leader) can gain an advantage by committing to a mixed strategy in advance.",
                "Since the followers (the robbers) will know the leaders strategy, the optimal response for the followers will be a pure strategy.",
                "Given the common assumption, taken in [5], in the case where followers are indifferent, they will choose the strategy that benefits the leader, there must exist a guaranteed optimal strategy for the leader [5].",
                "From the Bayesian game in Table 2, we constructed the Harsanyi transformed bimatrix in Table 3.",
                "The strategies for each player (security agent or robber) in the transformed game correspond to all combinations of possible strategies taken by each of that players types.",
                "Therefore, we denote X = σθ1 1 = σ1 and Q = σθ2 2 as the index sets of the security agent and robbers pure strategies respectively, with R and C as the corresponding payoff matrices.",
                "Rij is the reward of the security agent and Cij is the reward of the robbers when the security agent takes pure strategy i and the robbers take pure strategy j.",
                "A mixed strategy for the security agent is a probability distribution over its set of pure strategies and will be represented by a vector x = (px1, px2, . . . , px|X|), where pxi ≥ 0 and P pxi = 1.",
                "Here, pxi is the probability that the security agent will choose its ith pure strategy.",
                "The optimal mixed strategy for the security agent can be found in time polynomial in the number of rows in the normal form game using the following linear program formulation from [5].",
                "For every possible pure strategy j by the follower (the set of all robber types), max P i∈X pxiRij s.t. ∀j ∈ Q, P i∈σ1 pxiCij ≥ P i∈σ1 pxiCij P i∈X pxi = 1 ∀i∈X , pxi >= 0 (1) Then, for all feasible follower strategies j, choose the one that maximizes P i∈X pxiRij, the reward for the security agent (leader).",
                "The pxi variables give the optimal strategy for the security agent.",
                "Note that while this method is polynomial in the number of rows in the transformed, normal-form game, the number of rows increases exponentially with the number of robber types.",
                "Using this method for a Bayesian game thus requires running |σ2||θ2| separate linear programs.",
                "This is no surprise, since finding the leaders optimal strategy in a Bayesian Stackelberg game is NP-hard [5]. 4.",
                "HEURISTIC APPROACHES Given that finding the optimal strategy for the leader is NP-hard, we provide a heuristic approach.",
                "In this heuristic we limit the possible mixed strategies of the leader to select actions with probabilities that are integer multiples of 1/k for a predetermined integer k. Previous work [14] has shown that strategies with high entropy are beneficial for security applications when opponents utilities are completely unknown.",
                "In our domain, if utilities are not considered, this method will result in uniform-distribution strategies.",
                "One advantage of such strategies is that they are compact to represent (as fractions) and simple to understand; therefore they can be efficiently implemented by real organizations.",
                "We aim to maintain the advantage provided by simple strategies for our security application problem, incorporating the effect of the robbers rewards on the security agents rewards.",
                "Thus, the ASAP heuristic will produce strategies which are k-uniform.",
                "A mixed strategy is denoted k-uniform if it is a uniform distribution on a multiset S of pure strategies with |S| = k. A multiset is a set whose elements may be repeated multiple times; thus, for example, the mixed strategy corresponding to the multiset {1, 1, 2} would take strategy 1 The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) 313 {1,2} {2,1} {1a, 1b} −1α − .9(1 − α), .5α + .6(1 − α) −.375α − .275(1 − α), .125α + .225(1 − α) {1a, 2b} −1α − .025(1 − α), .5α − .025(1 − α) −.375α − .9(1 − α), .125α + .6(1 − α) {2a, 1b} −.125α − .9(1 − α), −.125α + .6(1 − α) −1α − .275(1 − α), .5α + .225(1 − α) {2a, 2b} −.125α − .025(1 − α), −.125α − .025(1 − α) −1α − .9(1 − α), .5α + .6(1 − α) Table 3: Harsanyi Transformed Payoff Table with probability 2/3 and strategy 2 with probability 1/3.",
                "ASAP allows the size of the multiset to be chosen in order to balance the complexity of the strategy reached with the goal that the identified strategy will yield a high reward.",
                "Another advantage of the ASAP heuristic is that it operates directly on the compact Bayesian representation, without requiring the Harsanyi transformation.",
                "This is because the different follower (robber) types are independent of each other.",
                "Hence, evaluating the leader strategy against a Harsanyi-transformed game matrix is equivalent to evaluating against each of the game matrices for the individual follower types.",
                "This independence property is exploited in ASAP to yield a decomposition scheme.",
                "Note that the LP method introduced by [5] to compute optimal Stackelberg policies is unlikely to be decomposable into a small number of games as it was shown to be NP-hard for Bayes-Nash problems.",
                "Finally, note that ASAP requires the solution of only one optimization problem, rather than solving a series of problems as in the LP method of [5].",
                "For a single follower type, the algorithm works the following way.",
                "Given a particular k, for each possible mixed strategy x for the leader that corresponds to a multiset of size k, evaluate the leaders payoff from x when the follower plays a reward-maximizing pure strategy.",
                "We then take the mixed strategy with the highest payoff.",
                "We need only to consider the reward-maximizing pure strategies of the followers (robbers), since for a given fixed strategy x of the security agent, each robber type faces a problem with fixed linear rewards.",
                "If a mixed strategy is optimal for the robber, then so are all the pure strategies in the support of that mixed strategy.",
                "Note also that because we limit the leaders strategies to take on discrete values, the assumption from Section 3.2 that the followers will break ties in the leaders favor is not significant, since ties will be unlikely to arise.",
                "This is because, in domains where rewards are drawn from any random distribution, the probability of a follower having more than one pure optimal response to a given leader strategy approaches zero, and the leader will have only a finite number of possible mixed strategies.",
                "Our approach to characterize the optimal strategy for the security agent makes use of properties of linear programming.",
                "We briefly outline these results here for completeness, for detailed discussion and proofs see one of many references on the topic, such as [2].",
                "Every linear programming problem, such as: max cT x | Ax = b, x ≥ 0, has an associated dual linear program, in this case: min bT y | AT y ≥ c. These primal/dual pairs of problems satisfy weak duality: For any x and y primal and dual feasible solutions respectively, cT x ≤ bT y.",
                "Thus a pair of feasible solutions is optimal if cT x = bT y, and the problems are said to satisfy strong duality.",
                "In fact if a linear program is feasible and has a bounded optimal solution, then the dual is also feasible and there is a pair x∗ , y∗ that satisfies cT x∗ = bT y∗ .",
                "These optimal solutions are characterized with the following optimality conditions (as defined in [2]): • primal feasibility: Ax = b, x ≥ 0 • dual feasibility: AT y ≥ c • complementary slackness: xi(AT y − c)i = 0 for all i.",
                "Note that this last condition implies that cT x = xT AT y = bT y, which proves optimality for primal dual feasible solutions x and y.",
                "In the following subsections, we first define the problem in its most intuititive form as a mixed-integer quadratic program (MIQP), and then show how this problem can be converted into a mixedinteger linear program (MILP). 4.1 Mixed-Integer Quadratic Program We begin with the case of a single type of follower.",
                "Let the leader be the row player and the follower the column player.",
                "We denote by x the vector of strategies of the leader and q the vector of strategies of the follower.",
                "We also denote X and Q the index sets of the leader and followers pure strategies, respectively.",
                "The payoff matrices R and C correspond to: Rij is the reward of the leader and Cij is the reward of the follower when the leader takes pure strategy i and the follower takes pure strategy j.",
                "Let k be the size of the multiset.",
                "We first fix the policy of the leader to some k-uniform policy x.",
                "The value xi is the number of times pure strategy i is used in the k-uniform policy, which is selected with probability xi/k.",
                "We formulate the optimization problem the follower solves to find its optimal response to x as the following linear program: max X j∈Q X i∈X 1 k Cijxi qj s.t.",
                "P j∈Q qj = 1 q ≥ 0. (2) The objective function maximizes the followers expected reward given x, while the constraints make feasible any mixed strategy q for the follower.",
                "The dual to this linear programming problem is the following: min a s.t. a ≥ X i∈X 1 k Cijxi j ∈ Q. (3) From strong duality and complementary slackness we obtain that the followers maximum reward value, a, is the value of every pure strategy with qj > 0, that is in the support of the optimal mixed strategy.",
                "Therefore each of these pure strategies is optimal.",
                "Optimal solutions to the followers problem are characterized by linear programming optimality conditions: primal feasibility constraints in (2), dual feasibility constraints in (3), and complementary slackness qj a − X i∈X 1 k Cijxi ! = 0 j ∈ Q.",
                "These conditions must be included in the problem solved by the leader in order to consider only best responses by the follower to the k-uniform policy x. 314 The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) The leader seeks the k-uniform solution x that maximizes its own payoff, given that the follower uses an optimal response q(x).",
                "Therefore the leader solves the following integer problem: max X i∈X X j∈Q 1 k Rijq(x)j xi s.t.",
                "P i∈X xi = k xi ∈ {0, 1, . . . , k}. (4) Problem (4) maximizes the leaders reward with the followers best response (qj for fixed leaders policy x and hence denoted q(x)j) by selecting a uniform policy from a multiset of constant size k. We complete this problem by including the characterization of q(x) through linear programming optimality conditions.",
                "To simplify writing the complementary slackness conditions, we will constrain q(x) to be only optimal pure strategies by just considering integer solutions of q(x).",
                "The leaders problem becomes: maxx,q X i∈X X j∈Q 1 k Rijxiqj s.t.",
                "P i xi = kP j∈Q qj = 1 0 ≤ (a − P i∈X 1 k Cijxi) ≤ (1 − qj)M xi ∈ {0, 1, ...., k} qj ∈ {0, 1}. (5) Here, the constant M is some large number.",
                "The first and fourth constraints enforce a k-uniform policy for the leader, and the second and fifth constraints enforce a feasible pure strategy for the follower.",
                "The third constraint enforces dual feasibility of the followers problem (leftmost inequality) and the complementary slackness constraint for an optimal pure strategy q for the follower (rightmost inequality).",
                "In fact, since only one pure strategy can be selected by the follower, say qh = 1, this last constraint enforces that a = P i∈X 1 k Cihxi imposing no additional constraint for all other pure strategies which have qj = 0.",
                "We conclude this subsection noting that Problem (5) is an integer program with a non-convex quadratic objective in general, as the matrix R need not be positive-semi-definite.",
                "Efficient solution methods for non-linear, non-convex integer problems remains a challenging research question.",
                "In the next section we show a reformulation of this problem as a linear integer programming problem, for which a number of efficient commercial solvers exist. 4.2 Mixed-Integer Linear Program We can linearize the quadratic program of Problem 5 through the change of variables zij = xiqj, obtaining the following problem maxq,z P i∈X P j∈Q 1 k Rijzij s.t.",
                "P i∈X P j∈Q zij = k P j∈Q zij ≤ k kqj ≤ P i∈X zij ≤ k P j∈Q qj = 1 0 ≤ (a − P i∈X 1 k Cij( P h∈Q zih)) ≤ (1 − qj)M zij ∈ {0, 1, ...., k} qj ∈ {0, 1} (6) PROPOSITION 1.",
                "Problems (5) and (6) are equivalent.",
                "Proof: Consider x, q a feasible solution of (5).",
                "We will show that q, zij = xiqj is a feasible solution of (6) of same objective function value.",
                "The equivalence of the objective functions, and constraints 4, 6 and 7 of (6) are satisfied by construction.",
                "The fact that P j∈Q zij = xi as P j∈Q qj = 1 explains constraints 1, 2, and 5 of (6).",
                "Constraint 3 of (6) is satisfied because P i∈X zij = kqj.",
                "Let us now consider q, z feasible for (6).",
                "We will show that q and xi = P j∈Q zij are feasible for (5) with the same objective value.",
                "In fact all constraints of (5) are readily satisfied by construction.",
                "To see that the objectives match, notice that if qh = 1 then the third constraint in (6) implies that P i∈X zih = k, which means that zij = 0 for all i ∈ X and all j = h. Therefore, xiqj = X l∈Q zilqj = zihqj = zij.",
                "This last equality is because both are 0 when j = h. This shows that the transformation preserves the objective function value, completing the proof.",
                "Given this transformation to a mixed-integer linear program (MILP), we now show how we can apply our decomposition technique on the MILP to obtain significant speedups for Bayesian games with multiple follower types. 5.",
                "DECOMPOSITION FOR MULTIPLE ADVERSARIES The MILP developed in the previous section handles only one follower.",
                "Since our security scenario contains multiple follower (robber) types, we change the response function for the follower from a pure strategy into a weighted combination over various pure follower strategies where the weights are probabilities of occurrence of each of the follower types. 5.1 Decomposed MIQP To admit multiple adversaries in our framework, we modify the notation defined in the previous section to reason about multiple follower types.",
                "We denote by x the vector of strategies of the leader and ql the vector of strategies of follower l, with L denoting the index set of follower types.",
                "We also denote by X and Q the index sets of leader and follower ls pure strategies, respectively.",
                "We also index the payoff matrices on each follower l, considering the matrices Rl and Cl .",
                "Using this modified notation, we characterize the optimal solution of follower ls problem given the leaders k-uniform policy x, with the following optimality conditions: X j∈Q ql j = 1 al − X i∈X 1 k Cl ijxi ≥ 0 ql j(al − X i∈X 1 k Cl ijxi) = 0 ql j ≥ 0 Again, considering only optimal pure strategies for follower ls problem we can linearize the complementarity constraint above.",
                "We incorporate these constraints on the leaders problem that selects the optimal k-uniform policy.",
                "Therefore, given a priori probabilities pl , with l ∈ L of facing each follower, the leader solves the following problem: The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) 315 maxx,q X i∈X X l∈L X j∈Q pl k Rl ijxiql j s.t.",
                "P i xi = kP j∈Q ql j = 1 0 ≤ (al − P i∈X 1 k Cl ijxi) ≤ (1 − ql j)M xi ∈ {0, 1, ...., k} ql j ∈ {0, 1}. (7) Problem (7) for a Bayesian game with multiple follower types is indeed equivalent to Problem (5) on the payoff matrix obtained from the Harsanyi transformation of the game.",
                "In fact, every pure strategy j in Problem (5) corresponds to a sequence of pure strategies jl, one for each follower l ∈ L. This means that qj = 1 if and only if ql jl = 1 for all l ∈ L. In addition, given the a priori probabilities pl of facing player l, the reward in the Harsanyi transformation payoff table is Rij = P l∈L pl Rl ijl .",
                "The same relation holds between C and Cl .",
                "These relations between a pure strategy in the equivalent normal form game and pure strategies in the individual games with each followers are key in showing these problems are equivalent. 5.2 Decomposed MILP We can linearize the quadratic programming problem 7 through the change of variables zl ij = xiql j, obtaining the following problem maxq,z P i∈X P l∈L P j∈Q pl k Rl ijzl ij s.t.",
                "P i∈X P j∈Q zl ij = k P j∈Q zl ij ≤ k kql j ≤ P i∈X zl ij ≤ k P j∈Q ql j = 1 0 ≤ (al − P i∈X 1 k Cl ij( P h∈Q zl ih)) ≤ (1 − ql j)M P j∈Q zl ij = P j∈Q z1 ij zl ij ∈ {0, 1, ...., k} ql j ∈ {0, 1} (8) PROPOSITION 2.",
                "Problems (7) and (8) are equivalent.",
                "Proof: Consider x, ql , al with l ∈ L a feasible solution of (7).",
                "We will show that ql , al , zl ij = xiql j is a feasible solution of (8) of same objective function value.",
                "The equivalence of the objective functions, and constraints 4, 7 and 8 of (8) are satisfied by construction.",
                "The fact that P j∈Q zl ij = xi as P j∈Q ql j = 1 explains constraints 1, 2, 5 and 6 of (8).",
                "Constraint 3 of (8) is satisfied because P i∈X zl ij = kql j.",
                "Lets now consider ql , zl , al feasible for (8).",
                "We will show that ql , al and xi = P j∈Q z1 ij are feasible for (7) with the same objective value.",
                "In fact all constraints of (7) are readily satisfied by construction.",
                "To see that the objectives match, notice for each l one ql j must equal 1 and the rest equal 0.",
                "Let us say that ql jl = 1, then the third constraint in (8) implies that P i∈X zl ijl = k, which means that zl ij = 0 for all i ∈ X and all j = jl.",
                "In particular this implies that xi = X j∈Q z1 ij = z1 ij1 = zl ijl , the last equality from constraint 6 of (8).",
                "Therefore xiql j = zl ijl ql j = zl ij.",
                "This last equality is because both are 0 when j = jl.",
                "Effectively, constraint 6 ensures that all the adversaries are calculating their best responses against a particular fixed policy of the agent.",
                "This shows that the transformation preserves the objective function value, completing the proof.",
                "We can therefore solve this equivalent linear integer program with efficient integer programming packages which can handle problems with thousands of integer variables.",
                "We implemented the decomposed MILP and the results are shown in the following section. 6.",
                "EXPERIMENTAL RESULTS The patrolling domain and the payoffs for the associated game are detailed in Sections 2 and 3.",
                "We performed experiments for this game in worlds of three and four houses with patrols consisting of two houses.",
                "The description given in Section 2 is used to generate a base case for both the security agent and robber payoff functions.",
                "The payoff tables for additional robber types are constructed and added to the game by adding a random distribution of varying size to the payoffs in the base case.",
                "All games are normalized so that, for each robber type, the minimum and maximum payoffs to the security agent and robber are 0 and 1, respectively.",
                "Using the data generated, we performed the experiments using four methods for generating the security agents strategy: • uniform randomization • ASAP • the multiple linear programs method from [5] (to find the true optimal strategy) • the highest reward Bayes-Nash equilibrium, found using the MIP-Nash algorithm [17] The last three methods were applied using CPLEX 8.1.",
                "Because the last two methods are designed for normal-form games rather than Bayesian games, the games were first converted using the Harsanyi transformation [8].",
                "The uniform randomization method is simply choosing a uniform random policy over all possible patrol routes.",
                "We use this method as a simple baseline to measure the performance of our heuristics.",
                "We anticipated that the uniform policy would perform reasonably well since maximum-entropy policies have been shown to be effective in multiagent security domains [14].",
                "The highest-reward Bayes-Nash equilibria were used in order to demonstrate the higher reward gained by looking for an optimal policy rather than an equilibria in Stackelberg games such as our security domain.",
                "Based on our experiments we present three sets of graphs to demonstrate (1) the runtime of ASAP compared to other common methods for finding a strategy, (2) the reward guaranteed by ASAP compared to other methods, and (3) the effect of varying the parameter k, the size of the multiset, on the performance of ASAP.",
                "In the first two sets of graphs, ASAP is run using a multiset of 80 elements; in the third set this number is varied.",
                "The first set of graphs, shown in Figure 1 shows the runtime graphs for three-house (left column) and four-house (right column) domains.",
                "Each of the three rows of graphs corresponds to a different randomly-generated scenario.",
                "The x-axis shows the number of robber types the security agent faces and the y-axis of the graph shows the runtime in seconds.",
                "All experiments that were not concluded in 30 minutes (1800 seconds) were cut off.",
                "The runtime for the uniform policy is always negligible irrespective of the number of adversaries and hence is not shown.",
                "The ASAP algorithm clearly outperforms the optimal, multipleLP method as well as the MIP-Nash algorithm for finding the highestreward Bayes-Nash equilibrium with respect to runtime.",
                "For a 316 The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) Figure 1: Runtimes for various algorithms on problems of 3 and 4 houses. domain of three houses, the optimal method cannot reach a solution for more than seven robber types, and for four houses it cannot solve for more than six types within the cutoff time in any of the three scenarios.",
                "MIP-Nash solves for even fewer robber types within the cutoff time.",
                "On the other hand, ASAP runs much faster, and is able to solve for at least 20 adversaries for the three-house scenarios and for at least 12 adversaries in the four-house scenarios within the cutoff time.",
                "The runtime of ASAP does not increase strictly with the number of robber types for each scenario, but in general, the addition of more types increases the runtime required.",
                "The second set of graphs, Figure 2, shows the reward to the patrol agent given by each method for three scenarios in the three-house (left column) and four-house (right column) domains.",
                "This reward is the utility received by the security agent in the patrolling game, and not as a percentage of the optimal reward, since it was not possible to obtain the optimal reward as the number of robber types increased.",
                "The uniform policy consistently provides the lowest reward in both domains; while the optimal method of course produces the optimal reward.",
                "The ASAP method remains consistently close to the optimal, even as the number of robber types increases.",
                "The highest-reward Bayes-Nash equilibria, provided by the MIPNash method, produced rewards higher than the uniform method, but lower than ASAP.",
                "This difference clearly illustrates the gains in the patrolling domain from committing to a strategy as the leader in a Stackelberg game, rather than playing a standard Bayes-Nash strategy.",
                "The third set of graphs, shown in Figure 3 shows the effect of the multiset size on runtime in seconds (left column) and reward (right column), again expressed as the reward received by the security agent in the patrolling game, and not a percentage of the optimal Figure 2: Reward for various algorithms on problems of 3 and 4 houses. reward.",
                "Results here are for the three-house domain.",
                "The trend is that as as the multiset size is increased, the runtime and reward level both increase.",
                "Not surprisingly, the reward increases monotonically as the multiset size increases, but what is interesting is that there is relatively little benefit to using a large multiset in this domain.",
                "In all cases, the reward given by a multiset of 10 elements was within at least 96% of the reward given by an 80-element multiset.",
                "The runtime does not always increase strictly with the multiset size; indeed in one example (scenario 2 with 20 robber types), using a multiset of 10 elements took 1228 seconds, while using 80 elements only took 617 seconds.",
                "In general, runtime should increase since a larger multiset means a larger domain for the variables in the MILP, and thus a larger search space.",
                "However, an increase in the number of variables can sometimes allow for a policy to be constructed more quickly due to more flexibility in the problem. 7.",
                "SUMMARY AND RELATED WORK This paper focuses on security for agents patrolling in hostile environments.",
                "In these environments, intentional threats are caused by adversaries about whom the security patrolling agents have incomplete information.",
                "Specifically, we deal with situations where the adversaries actions and payoffs are known but the exact adversary type is unknown to the security agent.",
                "Agents acting in the real world quite frequently have such incomplete information about other agents.",
                "Bayesian games have been a popular choice to model such incomplete information games [3].",
                "The Gala toolkit is one method for defining such games [9] without requiring the game to be represented in normal form via the Harsanyi transformation [8]; Galas guarantees are focused on fully competitive games.",
                "Much work has been done on finding optimal Bayes-Nash equilbria for The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) 317 Figure 3: Reward for ASAP using multisets of 10, 30, and 80 elements subclasses of Bayesian games, finding single Bayes-Nash equilibria for general Bayesian games [10] or approximate Bayes-Nash equilibria [18].",
                "Less attention has been paid to finding the optimal strategy to commit to in a Bayesian game (the Stackelberg scenario [15]).",
                "However, the complexity of this problem was shown to be NP-hard in the general case [5], which also provides algorithms for this problem in the non-Bayesian case.",
                "Therefore, we present a heuristic called ASAP, with three key advantages towards addressing this problem.",
                "First, ASAP searches for the highest reward strategy, rather than a Bayes-Nash equilibrium, allowing it to find feasible strategies that exploit the natural first-mover advantage of the game.",
                "Second, it provides strategies which are simple to understand, represent, and implement.",
                "Third, it operates directly on the compact, Bayesian game representation, without requiring conversion to normal form.",
                "We provide an efficient Mixed Integer Linear Program (MILP) implementation for ASAP, along with experimental results illustrating significant speedups and higher rewards over other approaches.",
                "Our k-uniform strategies are similar to the k-uniform strategies of [12].",
                "While that work provides epsilon error-bounds based on the k-uniform strategies, their solution concept is still that of a Nash equilibrium, and they do not provide efficient algorithms for obtaining such k-uniform strategies.",
                "This contrasts with ASAP, where our emphasis is on a highly efficient heuristic approach that is not focused on equilibrium solutions.",
                "Finally the patrolling problem which motivated our work has recently received growing attention from the multiagent community due to its wide range of applications [4, 13].",
                "However most of this work is focused on either limiting energy consumption involved in patrolling [7] or optimizing on criteria like the length of the path traveled [4, 13], without reasoning about any explicit model of an adversary[14].",
                "Acknowledgments : This research is supported by the United States Department of Homeland Security through Center for Risk and Economic Analysis of Terrorism Events (CREATE).",
                "It is also supported by the Defense Advanced Research Projects Agency (DARPA), through the Department of the Interior, NBC, Acquisition Services Division, under Contract No.",
                "NBCHD030010.",
                "Sarit Kraus is also affiliated with UMIACS. 8.",
                "REFERENCES [1] R. W. Beard and T. McLain.",
                "Multiple UAV cooperative search under collision avoidance and limited range communication constraints.",
                "In IEEE CDC, 2003. [2] D. Bertsimas and J. Tsitsiklis.",
                "Introduction to Linear Optimization.",
                "Athena Scientific, 1997. [3] J. Brynielsson and S. Arnborg.",
                "Bayesian games for threat prediction and situation analysis.",
                "In FUSION, 2004. [4] Y. Chevaleyre.",
                "Theoretical analysis of multi-agent patrolling problem.",
                "In AAMAS, 2004. [5] V. Conitzer and T. Sandholm.",
                "Choosing the best strategy to commit to.",
                "In ACM Conference on Electronic Commerce, 2006. [6] D. Fudenberg and J. Tirole.",
                "Game Theory.",
                "MIT Press, 1991. [7] C. Gui and P. Mohapatra.",
                "Virtual patrol: A new power conservation design for surveillance using sensor networks.",
                "In IPSN, 2005. [8] J. C. Harsanyi and R. Selten.",
                "A generalized Nash solution for two-person bargaining games with incomplete information.",
                "Management Science, 18(5):80-106, 1972. [9] D. Koller and A. Pfeffer.",
                "Generating and solving imperfect information games.",
                "In IJCAI, pages 1185-1193, 1995. [10] D. Koller and A. Pfeffer.",
                "Representations and solutions for game-theoretic problems.",
                "Artificial Intelligence, 94(1):167-215, 1997. [11] C. Lemke and J. Howson.",
                "Equilibrium points of bimatrix games.",
                "Journal of the Society for Industrial and Applied Mathematics, 12:413-423, 1964. [12] R. J. Lipton, E. Markakis, and A. Mehta.",
                "Playing large games using simple strategies.",
                "In ACM Conference on Electronic Commerce, 2003. [13] A. Machado, G. Ramalho, J. D. Zucker, and A. Drougoul.",
                "Multi-agent patrolling: an empirical analysis on alternative architectures.",
                "In MABS, 2002. [14] P. Paruchuri, M. Tambe, F. Ordonez, and S. Kraus.",
                "Security in multiagent systems by policy randomization.",
                "In AAMAS, 2006. [15] T. Roughgarden.",
                "Stackelberg scheduling strategies.",
                "In ACM Symposium on TOC, 2001. [16] S. Ruan, C. Meirina, F. Yu, K. R. Pattipati, and R. L. Popp.",
                "Patrolling in a stochastic environment.",
                "In 10th Intl.",
                "Command and Control Research Symp., 2005. [17] T. Sandholm, A. Gilpin, and V. Conitzer.",
                "Mixed-integer programming methods for finding nash equilibria.",
                "In AAAI, 2005. [18] S. Singh, V. Soni, and M. Wellman.",
                "Computing approximate Bayes-Nash equilibria with tree-games of incomplete information.",
                "In ACM Conference on Electronic Commerce, 2004. 318 The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07)"
            ],
            "original_annotated_samples": [],
            "translated_annotated_samples": [],
            "translated_text": "",
            "candidates": [],
            "error": []
        },
        "mixed-integer linear program": {
            "translated_key": "programa lineal entero mixto",
            "is_in_text": true,
            "original_annotated_sentences": [
                "An Efficient Heuristic Approach for Security Against Multiple Adversaries Praveen Paruchuri, Jonathan P. Pearce, Milind Tambe, Fernando Ordonez University of Southern California Los Angeles, CA 90089 {paruchur, jppearce, tambe, fordon}@usc.edu Sarit Kraus Bar-Ilan University Ramat-Gan 52900, Israel sarit@cs.biu.ac.il ABSTRACT In adversarial multiagent domains, security, commonly defined as the ability to deal with intentional threats from other agents, is a critical issue.",
                "This paper focuses on domains where these threats come from unknown adversaries.",
                "These domains can be modeled as Bayesian games; much work has been done on finding equilibria for such games.",
                "However, it is often the case in multiagent security domains that one agent can commit to a mixed strategy which its adversaries observe before choosing their own strategies.",
                "In this case, the agent can maximize reward by finding an optimal strategy, without requiring equilibrium.",
                "Previous work has shown this problem of optimal strategy selection to be NP-hard.",
                "Therefore, we present a heuristic called ASAP, with three key advantages to address the problem.",
                "First, ASAP searches for the highest-reward strategy, rather than a Bayes-Nash equilibrium, allowing it to find feasible strategies that exploit the natural first-mover advantage of the game.",
                "Second, it provides strategies which are simple to understand, represent, and implement.",
                "Third, it operates directly on the compact, Bayesian game representation, without requiring conversion to normal form.",
                "We provide an efficient Mixed Integer Linear Program (MILP) implementation for ASAP, along with experimental results illustrating significant speedups and higher rewards over other approaches.",
                "Categories and Subject Descriptors I.2.11 [Computing Methodologies]: Artificial Intelligence: Distributed Artificial Intelligence - Intelligent Agents General Terms Security, Design, Theory 1.",
                "INTRODUCTION In many multiagent domains, agents must act in order to provide security against attacks by adversaries.",
                "A common issue that agents face in such security domains is uncertainty about the adversaries they may be facing.",
                "For example, a security robot may need to make a choice about which areas to patrol, and how often [16].",
                "However, it will not know in advance exactly where a robber will choose to strike.",
                "A team of unmanned aerial vehicles (UAVs) [1] monitoring a region undergoing a humanitarian crisis may also need to choose a patrolling policy.",
                "They must make this decision without knowing in advance whether terrorists or other adversaries may be waiting to disrupt the mission at a given location.",
                "It may indeed be possible to model the motivations of types of adversaries the agent or agent team is likely to face in order to target these adversaries more closely.",
                "However, in both cases, the security robot or UAV team will not know exactly which kinds of adversaries may be active on any given day.",
                "A common approach for choosing a policy for agents in such scenarios is to model the scenarios as Bayesian games.",
                "A Bayesian game is a game in which agents may belong to one or more types; the type of an agent determines its possible actions and payoffs.",
                "The distribution of adversary types that an agent will face may be known or inferred from historical data.",
                "Usually, these games are analyzed according to the solution concept of a Bayes-Nash equilibrium, an extension of the Nash equilibrium for Bayesian games.",
                "However, in many settings, a Nash or Bayes-Nash equilibrium is not an appropriate solution concept, since it assumes that the agents strategies are chosen simultaneously [5].",
                "In some settings, one player can (or must) commit to a strategy before the other players choose their strategies.",
                "These scenarios are known as Stackelberg games [6].",
                "In a Stackelberg game, a leader commits to a strategy first, and then a follower (or group of followers) selfishly optimize their own rewards, considering the action chosen by the leader.",
                "For example, the security agent (leader) must first commit to a strategy for patrolling various areas.",
                "This strategy could be a mixed strategy in order to be unpredictable to the robbers (followers).",
                "The robbers, after observing the pattern of patrols over time, can then choose their strategy (which location to rob).",
                "Often, the leader in a Stackelberg game can attain a higher reward than if the strategies were chosen simultaneously.",
                "To see the advantage of being the leader in a Stackelberg game, consider a simple game with the payoff table as shown in Table 1.",
                "The leader is the row player and the follower is the column player.",
                "Here, the leaders payoff is listed first. 1 2 3 1 5,5 0,0 3,10 2 0,0 2,2 5,0 Table 1: Payoff table for example normal form game.",
                "The only Nash equilibrium for this game is when the leader plays 2 and the follower plays 2 which gives the leader a payoff of 2. 311 978-81-904262-7-5 (RPS) c 2007 IFAAMAS However, if the leader commits to a uniform mixed strategy of playing 1 and 2 with equal (0.5) probability, the followers best response is to play 3 to get an expected payoff of 5 (10 and 0 with equal probability).",
                "The leaders payoff would then be 4 (3 and 5 with equal probability).",
                "In this case, the leader now has an incentive to deviate and choose a pure strategy of 2 (to get a payoff of 5).",
                "However, this would cause the follower to deviate to strategy 2 as well, resulting in the Nash equilibrium.",
                "Thus, by committing to a strategy that is observed by the follower, and by avoiding the temptation to deviate, the leader manages to obtain a reward higher than that of the best Nash equilibrium.",
                "The problem of choosing an optimal strategy for the leader to commit to in a Stackelberg game is analyzed in [5] and found to be NP-hard in the case of a Bayesian game with multiple types of followers.",
                "Thus, efficient heuristic techniques for choosing highreward strategies in these games is an important open issue.",
                "Methods for finding optimal leader strategies for non-Bayesian games [5] can be applied to this problem by converting the Bayesian game into a normal-form game by the Harsanyi transformation [8].",
                "If, on the other hand, we wish to compute the highest-reward Nash equilibrium, new methods using mixed-integer linear programs (MILPs) [17] may be used, since the highest-reward Bayes-Nash equilibrium is equivalent to the corresponding Nash equilibrium in the transformed game.",
                "However, by transforming the game, the compact structure of the Bayesian game is lost.",
                "In addition, since the Nash equilibrium assumes a simultaneous choice of strategies, the advantages of being the leader are not considered.",
                "This paper introduces an efficient heuristic method for approximating the optimal leader strategy for security domains, known as ASAP (Agent Security via Approximate Policies).",
                "This method has three key advantages.",
                "First, it directly searches for an optimal strategy, rather than a Nash (or Bayes-Nash) equilibrium, thus allowing it to find high-reward non-equilibrium strategies like the one in the above example.",
                "Second, it generates policies with a support which can be expressed as a uniform distribution over a multiset of fixed size as proposed in [12].",
                "This allows for policies that are simple to understand and represent [12], as well as a tunable parameter (the size of the multiset) that controls the simplicity of the policy.",
                "Third, the method allows for a Bayes-Nash game to be expressed compactly without conversion to a normal-form game, allowing for large speedups over existing Nash methods such as [17] and [11].",
                "The rest of the paper is organized as follows.",
                "In Section 2 we fully describe the patrolling domain and its properties.",
                "Section 3 introduces the Bayesian game, the Harsanyi transformation, and existing methods for finding an optimal leaders strategy in a Stackelberg game.",
                "Then, in Section 4 the ASAP algorithm is presented for normal-form games, and in Section 5 we show how it can be adapted to the structure of Bayesian games with uncertain adversaries.",
                "Experimental results showing higher reward and faster policy computation over existing Nash methods are shown in Section 6, and we conclude with a discussion of related work in Section 7. 2.",
                "THE PATROLLING DOMAIN In most security patrolling domains, the security agents (like UAVs [1] or security robots [16]) cannot feasibly patrol all areas all the time.",
                "Instead, they must choose a policy by which they patrol various routes at different times, taking into account factors such as the likelihood of crime in different areas, possible targets for crime, and the security agents own resources (number of security agents, amount of available time, fuel, etc.).",
                "It is usually beneficial for this policy to be nondeterministic so that robbers cannot safely rob certain locations, knowing that they will be safe from the security agents [14].",
                "To demonstrate the utility of our algorithm, we use a simplified version of such a domain, expressed as a game.",
                "The most basic version of our game consists of two players: the security agent (the leader) and the robber (the follower) in a world consisting of m houses, 1 . . . m. The security agents set of pure strategies consists of possible routes of d houses to patrol (in an order).",
                "The security agent can choose a mixed strategy so that the robber will be unsure of exactly where the security agent may patrol, but the robber will know the mixed strategy the security agent has chosen.",
                "For example, the robber can observe over time how often the security agent patrols each area.",
                "With this knowledge, the robber must choose a single house to rob.",
                "We assume that the robber generally takes a long time to rob a house.",
                "If the house chosen by the robber is not on the security agents route, then the robber successfully robs it.",
                "Otherwise, if it is on the security agents route, then the earlier the house is on the route, the easier it is for the security agent to catch the robber before he finishes robbing it.",
                "We model the payoffs for this game with the following variables: • vl,x: value of the goods in house l to the security agent. • vl,q: value of the goods in house l to the robber. • cx: reward to the security agent of catching the robber. • cq: cost to the robber of getting caught. • pl: probability that the security agent can catch the robber at the lth house in the patrol (pl < pl ⇐⇒ l < l).",
                "The security agents set of possible pure strategies (patrol routes) is denoted by X and includes all d-tuples i =< w1, w2, ..., wd > with w1 . . . wd = 1 . . . m where no two elements are equal (the agent is not allowed to return to the same house).",
                "The robbers set of possible pure strategies (houses to rob) is denoted by Q and includes all integers j = 1 . . . m. The payoffs (security agent, robber) for pure strategies i, j are: • −vl,x, vl,q, for j = l /∈ i. • plcx +(1−pl)(−vl,x), −plcq +(1−pl)(vl,q), for j = l ∈ i.",
                "With this structure it is possible to model many different types of robbers who have differing motivations; for example, one robber may have a lower cost of getting caught than another, or may value the goods in the various houses differently.",
                "If the distribution of different robber types is known or inferred from historical data, then the game can be modeled as a Bayesian game [6]. 3.",
                "BAYESIAN GAMES A Bayesian game contains a set of N agents, and each agent n must be one of a given set of types θn.",
                "For our patrolling domain, we have two agents, the security agent and the robber. θ1 is the set of security agent types and θ2 is the set of robber types.",
                "Since there is only one type of security agent, θ1 contains only one element.",
                "During the game, the robber knows its type but the security agent does not know the robbers type.",
                "For each agent (the security agent or the robber) n, there is a set of strategies σn and a utility function un : θ1 × θ2 × σ1 × σ2 → .",
                "A Bayesian game can be transformed into a normal-form game using the Harsanyi transformation [8].",
                "Once this is done, new, linear-program (LP)-based methods for finding high-reward strategies for normal-form games [5] can be used to find a strategy in the transformed game; this strategy can then be used for the Bayesian game.",
                "While methods exist for finding Bayes-Nash equilibria directly, without the Harsanyi transformation [10], they find only a 312 The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) single equilibrium in the general case, which may not be of high reward.",
                "Recent work [17] has led to efficient <br>mixed-integer linear program</br> techniques to find the best Nash equilibrium for a given agent.",
                "However, these techniques do require a normal-form game, and so to compare the policies given by ASAP against the optimal policy, as well as against the highest-reward Nash equilibrium, we must apply these techniques to the Harsanyi-transformed matrix.",
                "The next two subsections elaborate on how this is done. 3.1 Harsanyi Transformation The first step in solving Bayesian games is to apply the Harsanyi transformation [8] that converts the Bayesian game into a normal form game.",
                "Given that the Harsanyi transformation is a standard concept in game theory, we explain it briefly through a simple example in our patrolling domain without introducing the mathematical formulations.",
                "Let us assume there are two robber types a and b in the Bayesian game.",
                "Robber a will be active with probability α, and robber b will be active with probability 1 − α.",
                "The rules described in Section 2 allow us to construct simple payoff tables.",
                "Assume that there are two houses in the world (1 and 2) and hence there are two patrol routes (pure strategies) for the agent: {1,2} and {2,1}.",
                "The robber can rob either house 1 or house 2 and hence he has two strategies (denoted as 1l, 2l for robber type l).",
                "Since there are two types assumed (denoted as a and b), we construct two payoff tables (shown in Table 2) corresponding to the security agent playing a separate game with each of the two robber types with probabilities α and 1 − α.",
                "First, consider robber type a.",
                "Borrowing the notation from the domain section, we assign the following values to the variables: v1,x = v1,q = 3/4, v2,x = v2,q = 1/4, cx = 1/2, cq = 1, p1 = 1, p2 = 1/2.",
                "Using these values we construct a base payoff table as the payoff for the game against robber type a.",
                "For example, if the security agent chooses route {1,2} when robber a is active, and robber a chooses house 1, the robber receives a reward of -1 (for being caught) and the agent receives a reward of 0.5 for catching the robber.",
                "The payoffs for the game against robber type b are constructed using different values.",
                "Security agent: {1,2} {2,1} Robber a 1a -1, .5 -.375, .125 2a -.125, -.125 -1, .5 Robber b 1b -.9, .6 -.275, .225 2b -.025, -.025 -.9, .6 Table 2: Payoff tables: Security Agent vs Robbers a and b Using the Harsanyi technique involves introducing a chance node, that determines the robbers type, thus transforming the security agents incomplete information regarding the robber into imperfect information [3].",
                "The Bayesian equilibrium of the game is then precisely the Nash equilibrium of the imperfect information game.",
                "The transformed, normal-form game is shown in Table 3.",
                "In the transformed game, the security agent is the column player, and the set of all robber types together is the row player.",
                "Suppose that robber type a robs house 1 and robber type b robs house 2, while the security agent chooses patrol {1,2}.",
                "Then, the security agent and the robber receive an expected payoff corresponding to their payoffs from the agent encountering robber a at house 1 with probability α and robber b at house 2 with probability 1 − α. 3.2 Finding an Optimal Strategy Although a Nash equilibrium is the standard solution concept for games in which agents choose strategies simultaneously, in our security domain, the security agent (the leader) can gain an advantage by committing to a mixed strategy in advance.",
                "Since the followers (the robbers) will know the leaders strategy, the optimal response for the followers will be a pure strategy.",
                "Given the common assumption, taken in [5], in the case where followers are indifferent, they will choose the strategy that benefits the leader, there must exist a guaranteed optimal strategy for the leader [5].",
                "From the Bayesian game in Table 2, we constructed the Harsanyi transformed bimatrix in Table 3.",
                "The strategies for each player (security agent or robber) in the transformed game correspond to all combinations of possible strategies taken by each of that players types.",
                "Therefore, we denote X = σθ1 1 = σ1 and Q = σθ2 2 as the index sets of the security agent and robbers pure strategies respectively, with R and C as the corresponding payoff matrices.",
                "Rij is the reward of the security agent and Cij is the reward of the robbers when the security agent takes pure strategy i and the robbers take pure strategy j.",
                "A mixed strategy for the security agent is a probability distribution over its set of pure strategies and will be represented by a vector x = (px1, px2, . . . , px|X|), where pxi ≥ 0 and P pxi = 1.",
                "Here, pxi is the probability that the security agent will choose its ith pure strategy.",
                "The optimal mixed strategy for the security agent can be found in time polynomial in the number of rows in the normal form game using the following linear program formulation from [5].",
                "For every possible pure strategy j by the follower (the set of all robber types), max P i∈X pxiRij s.t. ∀j ∈ Q, P i∈σ1 pxiCij ≥ P i∈σ1 pxiCij P i∈X pxi = 1 ∀i∈X , pxi >= 0 (1) Then, for all feasible follower strategies j, choose the one that maximizes P i∈X pxiRij, the reward for the security agent (leader).",
                "The pxi variables give the optimal strategy for the security agent.",
                "Note that while this method is polynomial in the number of rows in the transformed, normal-form game, the number of rows increases exponentially with the number of robber types.",
                "Using this method for a Bayesian game thus requires running |σ2||θ2| separate linear programs.",
                "This is no surprise, since finding the leaders optimal strategy in a Bayesian Stackelberg game is NP-hard [5]. 4.",
                "HEURISTIC APPROACHES Given that finding the optimal strategy for the leader is NP-hard, we provide a heuristic approach.",
                "In this heuristic we limit the possible mixed strategies of the leader to select actions with probabilities that are integer multiples of 1/k for a predetermined integer k. Previous work [14] has shown that strategies with high entropy are beneficial for security applications when opponents utilities are completely unknown.",
                "In our domain, if utilities are not considered, this method will result in uniform-distribution strategies.",
                "One advantage of such strategies is that they are compact to represent (as fractions) and simple to understand; therefore they can be efficiently implemented by real organizations.",
                "We aim to maintain the advantage provided by simple strategies for our security application problem, incorporating the effect of the robbers rewards on the security agents rewards.",
                "Thus, the ASAP heuristic will produce strategies which are k-uniform.",
                "A mixed strategy is denoted k-uniform if it is a uniform distribution on a multiset S of pure strategies with |S| = k. A multiset is a set whose elements may be repeated multiple times; thus, for example, the mixed strategy corresponding to the multiset {1, 1, 2} would take strategy 1 The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) 313 {1,2} {2,1} {1a, 1b} −1α − .9(1 − α), .5α + .6(1 − α) −.375α − .275(1 − α), .125α + .225(1 − α) {1a, 2b} −1α − .025(1 − α), .5α − .025(1 − α) −.375α − .9(1 − α), .125α + .6(1 − α) {2a, 1b} −.125α − .9(1 − α), −.125α + .6(1 − α) −1α − .275(1 − α), .5α + .225(1 − α) {2a, 2b} −.125α − .025(1 − α), −.125α − .025(1 − α) −1α − .9(1 − α), .5α + .6(1 − α) Table 3: Harsanyi Transformed Payoff Table with probability 2/3 and strategy 2 with probability 1/3.",
                "ASAP allows the size of the multiset to be chosen in order to balance the complexity of the strategy reached with the goal that the identified strategy will yield a high reward.",
                "Another advantage of the ASAP heuristic is that it operates directly on the compact Bayesian representation, without requiring the Harsanyi transformation.",
                "This is because the different follower (robber) types are independent of each other.",
                "Hence, evaluating the leader strategy against a Harsanyi-transformed game matrix is equivalent to evaluating against each of the game matrices for the individual follower types.",
                "This independence property is exploited in ASAP to yield a decomposition scheme.",
                "Note that the LP method introduced by [5] to compute optimal Stackelberg policies is unlikely to be decomposable into a small number of games as it was shown to be NP-hard for Bayes-Nash problems.",
                "Finally, note that ASAP requires the solution of only one optimization problem, rather than solving a series of problems as in the LP method of [5].",
                "For a single follower type, the algorithm works the following way.",
                "Given a particular k, for each possible mixed strategy x for the leader that corresponds to a multiset of size k, evaluate the leaders payoff from x when the follower plays a reward-maximizing pure strategy.",
                "We then take the mixed strategy with the highest payoff.",
                "We need only to consider the reward-maximizing pure strategies of the followers (robbers), since for a given fixed strategy x of the security agent, each robber type faces a problem with fixed linear rewards.",
                "If a mixed strategy is optimal for the robber, then so are all the pure strategies in the support of that mixed strategy.",
                "Note also that because we limit the leaders strategies to take on discrete values, the assumption from Section 3.2 that the followers will break ties in the leaders favor is not significant, since ties will be unlikely to arise.",
                "This is because, in domains where rewards are drawn from any random distribution, the probability of a follower having more than one pure optimal response to a given leader strategy approaches zero, and the leader will have only a finite number of possible mixed strategies.",
                "Our approach to characterize the optimal strategy for the security agent makes use of properties of linear programming.",
                "We briefly outline these results here for completeness, for detailed discussion and proofs see one of many references on the topic, such as [2].",
                "Every linear programming problem, such as: max cT x | Ax = b, x ≥ 0, has an associated dual linear program, in this case: min bT y | AT y ≥ c. These primal/dual pairs of problems satisfy weak duality: For any x and y primal and dual feasible solutions respectively, cT x ≤ bT y.",
                "Thus a pair of feasible solutions is optimal if cT x = bT y, and the problems are said to satisfy strong duality.",
                "In fact if a linear program is feasible and has a bounded optimal solution, then the dual is also feasible and there is a pair x∗ , y∗ that satisfies cT x∗ = bT y∗ .",
                "These optimal solutions are characterized with the following optimality conditions (as defined in [2]): • primal feasibility: Ax = b, x ≥ 0 • dual feasibility: AT y ≥ c • complementary slackness: xi(AT y − c)i = 0 for all i.",
                "Note that this last condition implies that cT x = xT AT y = bT y, which proves optimality for primal dual feasible solutions x and y.",
                "In the following subsections, we first define the problem in its most intuititive form as a mixed-integer quadratic program (MIQP), and then show how this problem can be converted into a mixedinteger linear program (MILP). 4.1 Mixed-Integer Quadratic Program We begin with the case of a single type of follower.",
                "Let the leader be the row player and the follower the column player.",
                "We denote by x the vector of strategies of the leader and q the vector of strategies of the follower.",
                "We also denote X and Q the index sets of the leader and followers pure strategies, respectively.",
                "The payoff matrices R and C correspond to: Rij is the reward of the leader and Cij is the reward of the follower when the leader takes pure strategy i and the follower takes pure strategy j.",
                "Let k be the size of the multiset.",
                "We first fix the policy of the leader to some k-uniform policy x.",
                "The value xi is the number of times pure strategy i is used in the k-uniform policy, which is selected with probability xi/k.",
                "We formulate the optimization problem the follower solves to find its optimal response to x as the following linear program: max X j∈Q X i∈X 1 k Cijxi qj s.t.",
                "P j∈Q qj = 1 q ≥ 0. (2) The objective function maximizes the followers expected reward given x, while the constraints make feasible any mixed strategy q for the follower.",
                "The dual to this linear programming problem is the following: min a s.t. a ≥ X i∈X 1 k Cijxi j ∈ Q. (3) From strong duality and complementary slackness we obtain that the followers maximum reward value, a, is the value of every pure strategy with qj > 0, that is in the support of the optimal mixed strategy.",
                "Therefore each of these pure strategies is optimal.",
                "Optimal solutions to the followers problem are characterized by linear programming optimality conditions: primal feasibility constraints in (2), dual feasibility constraints in (3), and complementary slackness qj a − X i∈X 1 k Cijxi ! = 0 j ∈ Q.",
                "These conditions must be included in the problem solved by the leader in order to consider only best responses by the follower to the k-uniform policy x. 314 The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) The leader seeks the k-uniform solution x that maximizes its own payoff, given that the follower uses an optimal response q(x).",
                "Therefore the leader solves the following integer problem: max X i∈X X j∈Q 1 k Rijq(x)j xi s.t.",
                "P i∈X xi = k xi ∈ {0, 1, . . . , k}. (4) Problem (4) maximizes the leaders reward with the followers best response (qj for fixed leaders policy x and hence denoted q(x)j) by selecting a uniform policy from a multiset of constant size k. We complete this problem by including the characterization of q(x) through linear programming optimality conditions.",
                "To simplify writing the complementary slackness conditions, we will constrain q(x) to be only optimal pure strategies by just considering integer solutions of q(x).",
                "The leaders problem becomes: maxx,q X i∈X X j∈Q 1 k Rijxiqj s.t.",
                "P i xi = kP j∈Q qj = 1 0 ≤ (a − P i∈X 1 k Cijxi) ≤ (1 − qj)M xi ∈ {0, 1, ...., k} qj ∈ {0, 1}. (5) Here, the constant M is some large number.",
                "The first and fourth constraints enforce a k-uniform policy for the leader, and the second and fifth constraints enforce a feasible pure strategy for the follower.",
                "The third constraint enforces dual feasibility of the followers problem (leftmost inequality) and the complementary slackness constraint for an optimal pure strategy q for the follower (rightmost inequality).",
                "In fact, since only one pure strategy can be selected by the follower, say qh = 1, this last constraint enforces that a = P i∈X 1 k Cihxi imposing no additional constraint for all other pure strategies which have qj = 0.",
                "We conclude this subsection noting that Problem (5) is an integer program with a non-convex quadratic objective in general, as the matrix R need not be positive-semi-definite.",
                "Efficient solution methods for non-linear, non-convex integer problems remains a challenging research question.",
                "In the next section we show a reformulation of this problem as a linear integer programming problem, for which a number of efficient commercial solvers exist. 4.2 <br>mixed-integer linear program</br> We can linearize the quadratic program of Problem 5 through the change of variables zij = xiqj, obtaining the following problem maxq,z P i∈X P j∈Q 1 k Rijzij s.t.",
                "P i∈X P j∈Q zij = k P j∈Q zij ≤ k kqj ≤ P i∈X zij ≤ k P j∈Q qj = 1 0 ≤ (a − P i∈X 1 k Cij( P h∈Q zih)) ≤ (1 − qj)M zij ∈ {0, 1, ...., k} qj ∈ {0, 1} (6) PROPOSITION 1.",
                "Problems (5) and (6) are equivalent.",
                "Proof: Consider x, q a feasible solution of (5).",
                "We will show that q, zij = xiqj is a feasible solution of (6) of same objective function value.",
                "The equivalence of the objective functions, and constraints 4, 6 and 7 of (6) are satisfied by construction.",
                "The fact that P j∈Q zij = xi as P j∈Q qj = 1 explains constraints 1, 2, and 5 of (6).",
                "Constraint 3 of (6) is satisfied because P i∈X zij = kqj.",
                "Let us now consider q, z feasible for (6).",
                "We will show that q and xi = P j∈Q zij are feasible for (5) with the same objective value.",
                "In fact all constraints of (5) are readily satisfied by construction.",
                "To see that the objectives match, notice that if qh = 1 then the third constraint in (6) implies that P i∈X zih = k, which means that zij = 0 for all i ∈ X and all j = h. Therefore, xiqj = X l∈Q zilqj = zihqj = zij.",
                "This last equality is because both are 0 when j = h. This shows that the transformation preserves the objective function value, completing the proof.",
                "Given this transformation to a <br>mixed-integer linear program</br> (MILP), we now show how we can apply our decomposition technique on the MILP to obtain significant speedups for Bayesian games with multiple follower types. 5.",
                "DECOMPOSITION FOR MULTIPLE ADVERSARIES The MILP developed in the previous section handles only one follower.",
                "Since our security scenario contains multiple follower (robber) types, we change the response function for the follower from a pure strategy into a weighted combination over various pure follower strategies where the weights are probabilities of occurrence of each of the follower types. 5.1 Decomposed MIQP To admit multiple adversaries in our framework, we modify the notation defined in the previous section to reason about multiple follower types.",
                "We denote by x the vector of strategies of the leader and ql the vector of strategies of follower l, with L denoting the index set of follower types.",
                "We also denote by X and Q the index sets of leader and follower ls pure strategies, respectively.",
                "We also index the payoff matrices on each follower l, considering the matrices Rl and Cl .",
                "Using this modified notation, we characterize the optimal solution of follower ls problem given the leaders k-uniform policy x, with the following optimality conditions: X j∈Q ql j = 1 al − X i∈X 1 k Cl ijxi ≥ 0 ql j(al − X i∈X 1 k Cl ijxi) = 0 ql j ≥ 0 Again, considering only optimal pure strategies for follower ls problem we can linearize the complementarity constraint above.",
                "We incorporate these constraints on the leaders problem that selects the optimal k-uniform policy.",
                "Therefore, given a priori probabilities pl , with l ∈ L of facing each follower, the leader solves the following problem: The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) 315 maxx,q X i∈X X l∈L X j∈Q pl k Rl ijxiql j s.t.",
                "P i xi = kP j∈Q ql j = 1 0 ≤ (al − P i∈X 1 k Cl ijxi) ≤ (1 − ql j)M xi ∈ {0, 1, ...., k} ql j ∈ {0, 1}. (7) Problem (7) for a Bayesian game with multiple follower types is indeed equivalent to Problem (5) on the payoff matrix obtained from the Harsanyi transformation of the game.",
                "In fact, every pure strategy j in Problem (5) corresponds to a sequence of pure strategies jl, one for each follower l ∈ L. This means that qj = 1 if and only if ql jl = 1 for all l ∈ L. In addition, given the a priori probabilities pl of facing player l, the reward in the Harsanyi transformation payoff table is Rij = P l∈L pl Rl ijl .",
                "The same relation holds between C and Cl .",
                "These relations between a pure strategy in the equivalent normal form game and pure strategies in the individual games with each followers are key in showing these problems are equivalent. 5.2 Decomposed MILP We can linearize the quadratic programming problem 7 through the change of variables zl ij = xiql j, obtaining the following problem maxq,z P i∈X P l∈L P j∈Q pl k Rl ijzl ij s.t.",
                "P i∈X P j∈Q zl ij = k P j∈Q zl ij ≤ k kql j ≤ P i∈X zl ij ≤ k P j∈Q ql j = 1 0 ≤ (al − P i∈X 1 k Cl ij( P h∈Q zl ih)) ≤ (1 − ql j)M P j∈Q zl ij = P j∈Q z1 ij zl ij ∈ {0, 1, ...., k} ql j ∈ {0, 1} (8) PROPOSITION 2.",
                "Problems (7) and (8) are equivalent.",
                "Proof: Consider x, ql , al with l ∈ L a feasible solution of (7).",
                "We will show that ql , al , zl ij = xiql j is a feasible solution of (8) of same objective function value.",
                "The equivalence of the objective functions, and constraints 4, 7 and 8 of (8) are satisfied by construction.",
                "The fact that P j∈Q zl ij = xi as P j∈Q ql j = 1 explains constraints 1, 2, 5 and 6 of (8).",
                "Constraint 3 of (8) is satisfied because P i∈X zl ij = kql j.",
                "Lets now consider ql , zl , al feasible for (8).",
                "We will show that ql , al and xi = P j∈Q z1 ij are feasible for (7) with the same objective value.",
                "In fact all constraints of (7) are readily satisfied by construction.",
                "To see that the objectives match, notice for each l one ql j must equal 1 and the rest equal 0.",
                "Let us say that ql jl = 1, then the third constraint in (8) implies that P i∈X zl ijl = k, which means that zl ij = 0 for all i ∈ X and all j = jl.",
                "In particular this implies that xi = X j∈Q z1 ij = z1 ij1 = zl ijl , the last equality from constraint 6 of (8).",
                "Therefore xiql j = zl ijl ql j = zl ij.",
                "This last equality is because both are 0 when j = jl.",
                "Effectively, constraint 6 ensures that all the adversaries are calculating their best responses against a particular fixed policy of the agent.",
                "This shows that the transformation preserves the objective function value, completing the proof.",
                "We can therefore solve this equivalent linear integer program with efficient integer programming packages which can handle problems with thousands of integer variables.",
                "We implemented the decomposed MILP and the results are shown in the following section. 6.",
                "EXPERIMENTAL RESULTS The patrolling domain and the payoffs for the associated game are detailed in Sections 2 and 3.",
                "We performed experiments for this game in worlds of three and four houses with patrols consisting of two houses.",
                "The description given in Section 2 is used to generate a base case for both the security agent and robber payoff functions.",
                "The payoff tables for additional robber types are constructed and added to the game by adding a random distribution of varying size to the payoffs in the base case.",
                "All games are normalized so that, for each robber type, the minimum and maximum payoffs to the security agent and robber are 0 and 1, respectively.",
                "Using the data generated, we performed the experiments using four methods for generating the security agents strategy: • uniform randomization • ASAP • the multiple linear programs method from [5] (to find the true optimal strategy) • the highest reward Bayes-Nash equilibrium, found using the MIP-Nash algorithm [17] The last three methods were applied using CPLEX 8.1.",
                "Because the last two methods are designed for normal-form games rather than Bayesian games, the games were first converted using the Harsanyi transformation [8].",
                "The uniform randomization method is simply choosing a uniform random policy over all possible patrol routes.",
                "We use this method as a simple baseline to measure the performance of our heuristics.",
                "We anticipated that the uniform policy would perform reasonably well since maximum-entropy policies have been shown to be effective in multiagent security domains [14].",
                "The highest-reward Bayes-Nash equilibria were used in order to demonstrate the higher reward gained by looking for an optimal policy rather than an equilibria in Stackelberg games such as our security domain.",
                "Based on our experiments we present three sets of graphs to demonstrate (1) the runtime of ASAP compared to other common methods for finding a strategy, (2) the reward guaranteed by ASAP compared to other methods, and (3) the effect of varying the parameter k, the size of the multiset, on the performance of ASAP.",
                "In the first two sets of graphs, ASAP is run using a multiset of 80 elements; in the third set this number is varied.",
                "The first set of graphs, shown in Figure 1 shows the runtime graphs for three-house (left column) and four-house (right column) domains.",
                "Each of the three rows of graphs corresponds to a different randomly-generated scenario.",
                "The x-axis shows the number of robber types the security agent faces and the y-axis of the graph shows the runtime in seconds.",
                "All experiments that were not concluded in 30 minutes (1800 seconds) were cut off.",
                "The runtime for the uniform policy is always negligible irrespective of the number of adversaries and hence is not shown.",
                "The ASAP algorithm clearly outperforms the optimal, multipleLP method as well as the MIP-Nash algorithm for finding the highestreward Bayes-Nash equilibrium with respect to runtime.",
                "For a 316 The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) Figure 1: Runtimes for various algorithms on problems of 3 and 4 houses. domain of three houses, the optimal method cannot reach a solution for more than seven robber types, and for four houses it cannot solve for more than six types within the cutoff time in any of the three scenarios.",
                "MIP-Nash solves for even fewer robber types within the cutoff time.",
                "On the other hand, ASAP runs much faster, and is able to solve for at least 20 adversaries for the three-house scenarios and for at least 12 adversaries in the four-house scenarios within the cutoff time.",
                "The runtime of ASAP does not increase strictly with the number of robber types for each scenario, but in general, the addition of more types increases the runtime required.",
                "The second set of graphs, Figure 2, shows the reward to the patrol agent given by each method for three scenarios in the three-house (left column) and four-house (right column) domains.",
                "This reward is the utility received by the security agent in the patrolling game, and not as a percentage of the optimal reward, since it was not possible to obtain the optimal reward as the number of robber types increased.",
                "The uniform policy consistently provides the lowest reward in both domains; while the optimal method of course produces the optimal reward.",
                "The ASAP method remains consistently close to the optimal, even as the number of robber types increases.",
                "The highest-reward Bayes-Nash equilibria, provided by the MIPNash method, produced rewards higher than the uniform method, but lower than ASAP.",
                "This difference clearly illustrates the gains in the patrolling domain from committing to a strategy as the leader in a Stackelberg game, rather than playing a standard Bayes-Nash strategy.",
                "The third set of graphs, shown in Figure 3 shows the effect of the multiset size on runtime in seconds (left column) and reward (right column), again expressed as the reward received by the security agent in the patrolling game, and not a percentage of the optimal Figure 2: Reward for various algorithms on problems of 3 and 4 houses. reward.",
                "Results here are for the three-house domain.",
                "The trend is that as as the multiset size is increased, the runtime and reward level both increase.",
                "Not surprisingly, the reward increases monotonically as the multiset size increases, but what is interesting is that there is relatively little benefit to using a large multiset in this domain.",
                "In all cases, the reward given by a multiset of 10 elements was within at least 96% of the reward given by an 80-element multiset.",
                "The runtime does not always increase strictly with the multiset size; indeed in one example (scenario 2 with 20 robber types), using a multiset of 10 elements took 1228 seconds, while using 80 elements only took 617 seconds.",
                "In general, runtime should increase since a larger multiset means a larger domain for the variables in the MILP, and thus a larger search space.",
                "However, an increase in the number of variables can sometimes allow for a policy to be constructed more quickly due to more flexibility in the problem. 7.",
                "SUMMARY AND RELATED WORK This paper focuses on security for agents patrolling in hostile environments.",
                "In these environments, intentional threats are caused by adversaries about whom the security patrolling agents have incomplete information.",
                "Specifically, we deal with situations where the adversaries actions and payoffs are known but the exact adversary type is unknown to the security agent.",
                "Agents acting in the real world quite frequently have such incomplete information about other agents.",
                "Bayesian games have been a popular choice to model such incomplete information games [3].",
                "The Gala toolkit is one method for defining such games [9] without requiring the game to be represented in normal form via the Harsanyi transformation [8]; Galas guarantees are focused on fully competitive games.",
                "Much work has been done on finding optimal Bayes-Nash equilbria for The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) 317 Figure 3: Reward for ASAP using multisets of 10, 30, and 80 elements subclasses of Bayesian games, finding single Bayes-Nash equilibria for general Bayesian games [10] or approximate Bayes-Nash equilibria [18].",
                "Less attention has been paid to finding the optimal strategy to commit to in a Bayesian game (the Stackelberg scenario [15]).",
                "However, the complexity of this problem was shown to be NP-hard in the general case [5], which also provides algorithms for this problem in the non-Bayesian case.",
                "Therefore, we present a heuristic called ASAP, with three key advantages towards addressing this problem.",
                "First, ASAP searches for the highest reward strategy, rather than a Bayes-Nash equilibrium, allowing it to find feasible strategies that exploit the natural first-mover advantage of the game.",
                "Second, it provides strategies which are simple to understand, represent, and implement.",
                "Third, it operates directly on the compact, Bayesian game representation, without requiring conversion to normal form.",
                "We provide an efficient Mixed Integer Linear Program (MILP) implementation for ASAP, along with experimental results illustrating significant speedups and higher rewards over other approaches.",
                "Our k-uniform strategies are similar to the k-uniform strategies of [12].",
                "While that work provides epsilon error-bounds based on the k-uniform strategies, their solution concept is still that of a Nash equilibrium, and they do not provide efficient algorithms for obtaining such k-uniform strategies.",
                "This contrasts with ASAP, where our emphasis is on a highly efficient heuristic approach that is not focused on equilibrium solutions.",
                "Finally the patrolling problem which motivated our work has recently received growing attention from the multiagent community due to its wide range of applications [4, 13].",
                "However most of this work is focused on either limiting energy consumption involved in patrolling [7] or optimizing on criteria like the length of the path traveled [4, 13], without reasoning about any explicit model of an adversary[14].",
                "Acknowledgments : This research is supported by the United States Department of Homeland Security through Center for Risk and Economic Analysis of Terrorism Events (CREATE).",
                "It is also supported by the Defense Advanced Research Projects Agency (DARPA), through the Department of the Interior, NBC, Acquisition Services Division, under Contract No.",
                "NBCHD030010.",
                "Sarit Kraus is also affiliated with UMIACS. 8.",
                "REFERENCES [1] R. W. Beard and T. McLain.",
                "Multiple UAV cooperative search under collision avoidance and limited range communication constraints.",
                "In IEEE CDC, 2003. [2] D. Bertsimas and J. Tsitsiklis.",
                "Introduction to Linear Optimization.",
                "Athena Scientific, 1997. [3] J. Brynielsson and S. Arnborg.",
                "Bayesian games for threat prediction and situation analysis.",
                "In FUSION, 2004. [4] Y. Chevaleyre.",
                "Theoretical analysis of multi-agent patrolling problem.",
                "In AAMAS, 2004. [5] V. Conitzer and T. Sandholm.",
                "Choosing the best strategy to commit to.",
                "In ACM Conference on Electronic Commerce, 2006. [6] D. Fudenberg and J. Tirole.",
                "Game Theory.",
                "MIT Press, 1991. [7] C. Gui and P. Mohapatra.",
                "Virtual patrol: A new power conservation design for surveillance using sensor networks.",
                "In IPSN, 2005. [8] J. C. Harsanyi and R. Selten.",
                "A generalized Nash solution for two-person bargaining games with incomplete information.",
                "Management Science, 18(5):80-106, 1972. [9] D. Koller and A. Pfeffer.",
                "Generating and solving imperfect information games.",
                "In IJCAI, pages 1185-1193, 1995. [10] D. Koller and A. Pfeffer.",
                "Representations and solutions for game-theoretic problems.",
                "Artificial Intelligence, 94(1):167-215, 1997. [11] C. Lemke and J. Howson.",
                "Equilibrium points of bimatrix games.",
                "Journal of the Society for Industrial and Applied Mathematics, 12:413-423, 1964. [12] R. J. Lipton, E. Markakis, and A. Mehta.",
                "Playing large games using simple strategies.",
                "In ACM Conference on Electronic Commerce, 2003. [13] A. Machado, G. Ramalho, J. D. Zucker, and A. Drougoul.",
                "Multi-agent patrolling: an empirical analysis on alternative architectures.",
                "In MABS, 2002. [14] P. Paruchuri, M. Tambe, F. Ordonez, and S. Kraus.",
                "Security in multiagent systems by policy randomization.",
                "In AAMAS, 2006. [15] T. Roughgarden.",
                "Stackelberg scheduling strategies.",
                "In ACM Symposium on TOC, 2001. [16] S. Ruan, C. Meirina, F. Yu, K. R. Pattipati, and R. L. Popp.",
                "Patrolling in a stochastic environment.",
                "In 10th Intl.",
                "Command and Control Research Symp., 2005. [17] T. Sandholm, A. Gilpin, and V. Conitzer.",
                "Mixed-integer programming methods for finding nash equilibria.",
                "In AAAI, 2005. [18] S. Singh, V. Soni, and M. Wellman.",
                "Computing approximate Bayes-Nash equilibria with tree-games of incomplete information.",
                "In ACM Conference on Electronic Commerce, 2004. 318 The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07)"
            ],
            "original_annotated_samples": [],
            "translated_annotated_samples": [
                "El trabajo reciente [17] ha llevado a técnicas eficientes del \"programa lineal de introducción mixta\" para encontrar el mejor equilibrio de Nash para un agente determinado.",
                "En la siguiente sección mostramos una reformulación de este problema como un problema de programación de enteros lineales, para el cual existen varios solucionadores comerciales eficientes.4.2 \"Programa lineal de introducción mixta\" Podemos linealizar el programa cuadrático del problema 5 a través del cambio de variables zij = xiqj, obteniendo el siguiente problema maxq, z p i∈X p j∈Q 1 k rijzij s.t.",
                "Dada esta transformación en un \"programa lineal de introducción mixta\" (MILP), ahora mostramos cómo podemos aplicar nuestra técnica de descomposición en el MILP para obtener aceleraciones significativas para juegos bayesianos con múltiples tipos de seguidores.5."
            ],
            "translated_text": "",
            "candidates": [
                "Programa lineal de In-Entres mixtos",
                "programa lineal de introducción mixta",
                "Programa lineal de In-Entres mixtos",
                "Programa lineal de introducción mixta",
                "Programa lineal de enteros mixtos",
                "programa lineal de introducción mixta"
            ],
            "error": []
        },
        "game theory": {
            "translated_key": "teoría del juego",
            "is_in_text": true,
            "original_annotated_sentences": [
                "An Efficient Heuristic Approach for Security Against Multiple Adversaries Praveen Paruchuri, Jonathan P. Pearce, Milind Tambe, Fernando Ordonez University of Southern California Los Angeles, CA 90089 {paruchur, jppearce, tambe, fordon}@usc.edu Sarit Kraus Bar-Ilan University Ramat-Gan 52900, Israel sarit@cs.biu.ac.il ABSTRACT In adversarial multiagent domains, security, commonly defined as the ability to deal with intentional threats from other agents, is a critical issue.",
                "This paper focuses on domains where these threats come from unknown adversaries.",
                "These domains can be modeled as Bayesian games; much work has been done on finding equilibria for such games.",
                "However, it is often the case in multiagent security domains that one agent can commit to a mixed strategy which its adversaries observe before choosing their own strategies.",
                "In this case, the agent can maximize reward by finding an optimal strategy, without requiring equilibrium.",
                "Previous work has shown this problem of optimal strategy selection to be NP-hard.",
                "Therefore, we present a heuristic called ASAP, with three key advantages to address the problem.",
                "First, ASAP searches for the highest-reward strategy, rather than a Bayes-Nash equilibrium, allowing it to find feasible strategies that exploit the natural first-mover advantage of the game.",
                "Second, it provides strategies which are simple to understand, represent, and implement.",
                "Third, it operates directly on the compact, Bayesian game representation, without requiring conversion to normal form.",
                "We provide an efficient Mixed Integer Linear Program (MILP) implementation for ASAP, along with experimental results illustrating significant speedups and higher rewards over other approaches.",
                "Categories and Subject Descriptors I.2.11 [Computing Methodologies]: Artificial Intelligence: Distributed Artificial Intelligence - Intelligent Agents General Terms Security, Design, Theory 1.",
                "INTRODUCTION In many multiagent domains, agents must act in order to provide security against attacks by adversaries.",
                "A common issue that agents face in such security domains is uncertainty about the adversaries they may be facing.",
                "For example, a security robot may need to make a choice about which areas to patrol, and how often [16].",
                "However, it will not know in advance exactly where a robber will choose to strike.",
                "A team of unmanned aerial vehicles (UAVs) [1] monitoring a region undergoing a humanitarian crisis may also need to choose a patrolling policy.",
                "They must make this decision without knowing in advance whether terrorists or other adversaries may be waiting to disrupt the mission at a given location.",
                "It may indeed be possible to model the motivations of types of adversaries the agent or agent team is likely to face in order to target these adversaries more closely.",
                "However, in both cases, the security robot or UAV team will not know exactly which kinds of adversaries may be active on any given day.",
                "A common approach for choosing a policy for agents in such scenarios is to model the scenarios as Bayesian games.",
                "A Bayesian game is a game in which agents may belong to one or more types; the type of an agent determines its possible actions and payoffs.",
                "The distribution of adversary types that an agent will face may be known or inferred from historical data.",
                "Usually, these games are analyzed according to the solution concept of a Bayes-Nash equilibrium, an extension of the Nash equilibrium for Bayesian games.",
                "However, in many settings, a Nash or Bayes-Nash equilibrium is not an appropriate solution concept, since it assumes that the agents strategies are chosen simultaneously [5].",
                "In some settings, one player can (or must) commit to a strategy before the other players choose their strategies.",
                "These scenarios are known as Stackelberg games [6].",
                "In a Stackelberg game, a leader commits to a strategy first, and then a follower (or group of followers) selfishly optimize their own rewards, considering the action chosen by the leader.",
                "For example, the security agent (leader) must first commit to a strategy for patrolling various areas.",
                "This strategy could be a mixed strategy in order to be unpredictable to the robbers (followers).",
                "The robbers, after observing the pattern of patrols over time, can then choose their strategy (which location to rob).",
                "Often, the leader in a Stackelberg game can attain a higher reward than if the strategies were chosen simultaneously.",
                "To see the advantage of being the leader in a Stackelberg game, consider a simple game with the payoff table as shown in Table 1.",
                "The leader is the row player and the follower is the column player.",
                "Here, the leaders payoff is listed first. 1 2 3 1 5,5 0,0 3,10 2 0,0 2,2 5,0 Table 1: Payoff table for example normal form game.",
                "The only Nash equilibrium for this game is when the leader plays 2 and the follower plays 2 which gives the leader a payoff of 2. 311 978-81-904262-7-5 (RPS) c 2007 IFAAMAS However, if the leader commits to a uniform mixed strategy of playing 1 and 2 with equal (0.5) probability, the followers best response is to play 3 to get an expected payoff of 5 (10 and 0 with equal probability).",
                "The leaders payoff would then be 4 (3 and 5 with equal probability).",
                "In this case, the leader now has an incentive to deviate and choose a pure strategy of 2 (to get a payoff of 5).",
                "However, this would cause the follower to deviate to strategy 2 as well, resulting in the Nash equilibrium.",
                "Thus, by committing to a strategy that is observed by the follower, and by avoiding the temptation to deviate, the leader manages to obtain a reward higher than that of the best Nash equilibrium.",
                "The problem of choosing an optimal strategy for the leader to commit to in a Stackelberg game is analyzed in [5] and found to be NP-hard in the case of a Bayesian game with multiple types of followers.",
                "Thus, efficient heuristic techniques for choosing highreward strategies in these games is an important open issue.",
                "Methods for finding optimal leader strategies for non-Bayesian games [5] can be applied to this problem by converting the Bayesian game into a normal-form game by the Harsanyi transformation [8].",
                "If, on the other hand, we wish to compute the highest-reward Nash equilibrium, new methods using mixed-integer linear programs (MILPs) [17] may be used, since the highest-reward Bayes-Nash equilibrium is equivalent to the corresponding Nash equilibrium in the transformed game.",
                "However, by transforming the game, the compact structure of the Bayesian game is lost.",
                "In addition, since the Nash equilibrium assumes a simultaneous choice of strategies, the advantages of being the leader are not considered.",
                "This paper introduces an efficient heuristic method for approximating the optimal leader strategy for security domains, known as ASAP (Agent Security via Approximate Policies).",
                "This method has three key advantages.",
                "First, it directly searches for an optimal strategy, rather than a Nash (or Bayes-Nash) equilibrium, thus allowing it to find high-reward non-equilibrium strategies like the one in the above example.",
                "Second, it generates policies with a support which can be expressed as a uniform distribution over a multiset of fixed size as proposed in [12].",
                "This allows for policies that are simple to understand and represent [12], as well as a tunable parameter (the size of the multiset) that controls the simplicity of the policy.",
                "Third, the method allows for a Bayes-Nash game to be expressed compactly without conversion to a normal-form game, allowing for large speedups over existing Nash methods such as [17] and [11].",
                "The rest of the paper is organized as follows.",
                "In Section 2 we fully describe the patrolling domain and its properties.",
                "Section 3 introduces the Bayesian game, the Harsanyi transformation, and existing methods for finding an optimal leaders strategy in a Stackelberg game.",
                "Then, in Section 4 the ASAP algorithm is presented for normal-form games, and in Section 5 we show how it can be adapted to the structure of Bayesian games with uncertain adversaries.",
                "Experimental results showing higher reward and faster policy computation over existing Nash methods are shown in Section 6, and we conclude with a discussion of related work in Section 7. 2.",
                "THE PATROLLING DOMAIN In most security patrolling domains, the security agents (like UAVs [1] or security robots [16]) cannot feasibly patrol all areas all the time.",
                "Instead, they must choose a policy by which they patrol various routes at different times, taking into account factors such as the likelihood of crime in different areas, possible targets for crime, and the security agents own resources (number of security agents, amount of available time, fuel, etc.).",
                "It is usually beneficial for this policy to be nondeterministic so that robbers cannot safely rob certain locations, knowing that they will be safe from the security agents [14].",
                "To demonstrate the utility of our algorithm, we use a simplified version of such a domain, expressed as a game.",
                "The most basic version of our game consists of two players: the security agent (the leader) and the robber (the follower) in a world consisting of m houses, 1 . . . m. The security agents set of pure strategies consists of possible routes of d houses to patrol (in an order).",
                "The security agent can choose a mixed strategy so that the robber will be unsure of exactly where the security agent may patrol, but the robber will know the mixed strategy the security agent has chosen.",
                "For example, the robber can observe over time how often the security agent patrols each area.",
                "With this knowledge, the robber must choose a single house to rob.",
                "We assume that the robber generally takes a long time to rob a house.",
                "If the house chosen by the robber is not on the security agents route, then the robber successfully robs it.",
                "Otherwise, if it is on the security agents route, then the earlier the house is on the route, the easier it is for the security agent to catch the robber before he finishes robbing it.",
                "We model the payoffs for this game with the following variables: • vl,x: value of the goods in house l to the security agent. • vl,q: value of the goods in house l to the robber. • cx: reward to the security agent of catching the robber. • cq: cost to the robber of getting caught. • pl: probability that the security agent can catch the robber at the lth house in the patrol (pl < pl ⇐⇒ l < l).",
                "The security agents set of possible pure strategies (patrol routes) is denoted by X and includes all d-tuples i =< w1, w2, ..., wd > with w1 . . . wd = 1 . . . m where no two elements are equal (the agent is not allowed to return to the same house).",
                "The robbers set of possible pure strategies (houses to rob) is denoted by Q and includes all integers j = 1 . . . m. The payoffs (security agent, robber) for pure strategies i, j are: • −vl,x, vl,q, for j = l /∈ i. • plcx +(1−pl)(−vl,x), −plcq +(1−pl)(vl,q), for j = l ∈ i.",
                "With this structure it is possible to model many different types of robbers who have differing motivations; for example, one robber may have a lower cost of getting caught than another, or may value the goods in the various houses differently.",
                "If the distribution of different robber types is known or inferred from historical data, then the game can be modeled as a Bayesian game [6]. 3.",
                "BAYESIAN GAMES A Bayesian game contains a set of N agents, and each agent n must be one of a given set of types θn.",
                "For our patrolling domain, we have two agents, the security agent and the robber. θ1 is the set of security agent types and θ2 is the set of robber types.",
                "Since there is only one type of security agent, θ1 contains only one element.",
                "During the game, the robber knows its type but the security agent does not know the robbers type.",
                "For each agent (the security agent or the robber) n, there is a set of strategies σn and a utility function un : θ1 × θ2 × σ1 × σ2 → .",
                "A Bayesian game can be transformed into a normal-form game using the Harsanyi transformation [8].",
                "Once this is done, new, linear-program (LP)-based methods for finding high-reward strategies for normal-form games [5] can be used to find a strategy in the transformed game; this strategy can then be used for the Bayesian game.",
                "While methods exist for finding Bayes-Nash equilibria directly, without the Harsanyi transformation [10], they find only a 312 The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) single equilibrium in the general case, which may not be of high reward.",
                "Recent work [17] has led to efficient mixed-integer linear program techniques to find the best Nash equilibrium for a given agent.",
                "However, these techniques do require a normal-form game, and so to compare the policies given by ASAP against the optimal policy, as well as against the highest-reward Nash equilibrium, we must apply these techniques to the Harsanyi-transformed matrix.",
                "The next two subsections elaborate on how this is done. 3.1 Harsanyi Transformation The first step in solving Bayesian games is to apply the Harsanyi transformation [8] that converts the Bayesian game into a normal form game.",
                "Given that the Harsanyi transformation is a standard concept in <br>game theory</br>, we explain it briefly through a simple example in our patrolling domain without introducing the mathematical formulations.",
                "Let us assume there are two robber types a and b in the Bayesian game.",
                "Robber a will be active with probability α, and robber b will be active with probability 1 − α.",
                "The rules described in Section 2 allow us to construct simple payoff tables.",
                "Assume that there are two houses in the world (1 and 2) and hence there are two patrol routes (pure strategies) for the agent: {1,2} and {2,1}.",
                "The robber can rob either house 1 or house 2 and hence he has two strategies (denoted as 1l, 2l for robber type l).",
                "Since there are two types assumed (denoted as a and b), we construct two payoff tables (shown in Table 2) corresponding to the security agent playing a separate game with each of the two robber types with probabilities α and 1 − α.",
                "First, consider robber type a.",
                "Borrowing the notation from the domain section, we assign the following values to the variables: v1,x = v1,q = 3/4, v2,x = v2,q = 1/4, cx = 1/2, cq = 1, p1 = 1, p2 = 1/2.",
                "Using these values we construct a base payoff table as the payoff for the game against robber type a.",
                "For example, if the security agent chooses route {1,2} when robber a is active, and robber a chooses house 1, the robber receives a reward of -1 (for being caught) and the agent receives a reward of 0.5 for catching the robber.",
                "The payoffs for the game against robber type b are constructed using different values.",
                "Security agent: {1,2} {2,1} Robber a 1a -1, .5 -.375, .125 2a -.125, -.125 -1, .5 Robber b 1b -.9, .6 -.275, .225 2b -.025, -.025 -.9, .6 Table 2: Payoff tables: Security Agent vs Robbers a and b Using the Harsanyi technique involves introducing a chance node, that determines the robbers type, thus transforming the security agents incomplete information regarding the robber into imperfect information [3].",
                "The Bayesian equilibrium of the game is then precisely the Nash equilibrium of the imperfect information game.",
                "The transformed, normal-form game is shown in Table 3.",
                "In the transformed game, the security agent is the column player, and the set of all robber types together is the row player.",
                "Suppose that robber type a robs house 1 and robber type b robs house 2, while the security agent chooses patrol {1,2}.",
                "Then, the security agent and the robber receive an expected payoff corresponding to their payoffs from the agent encountering robber a at house 1 with probability α and robber b at house 2 with probability 1 − α. 3.2 Finding an Optimal Strategy Although a Nash equilibrium is the standard solution concept for games in which agents choose strategies simultaneously, in our security domain, the security agent (the leader) can gain an advantage by committing to a mixed strategy in advance.",
                "Since the followers (the robbers) will know the leaders strategy, the optimal response for the followers will be a pure strategy.",
                "Given the common assumption, taken in [5], in the case where followers are indifferent, they will choose the strategy that benefits the leader, there must exist a guaranteed optimal strategy for the leader [5].",
                "From the Bayesian game in Table 2, we constructed the Harsanyi transformed bimatrix in Table 3.",
                "The strategies for each player (security agent or robber) in the transformed game correspond to all combinations of possible strategies taken by each of that players types.",
                "Therefore, we denote X = σθ1 1 = σ1 and Q = σθ2 2 as the index sets of the security agent and robbers pure strategies respectively, with R and C as the corresponding payoff matrices.",
                "Rij is the reward of the security agent and Cij is the reward of the robbers when the security agent takes pure strategy i and the robbers take pure strategy j.",
                "A mixed strategy for the security agent is a probability distribution over its set of pure strategies and will be represented by a vector x = (px1, px2, . . . , px|X|), where pxi ≥ 0 and P pxi = 1.",
                "Here, pxi is the probability that the security agent will choose its ith pure strategy.",
                "The optimal mixed strategy for the security agent can be found in time polynomial in the number of rows in the normal form game using the following linear program formulation from [5].",
                "For every possible pure strategy j by the follower (the set of all robber types), max P i∈X pxiRij s.t. ∀j ∈ Q, P i∈σ1 pxiCij ≥ P i∈σ1 pxiCij P i∈X pxi = 1 ∀i∈X , pxi >= 0 (1) Then, for all feasible follower strategies j, choose the one that maximizes P i∈X pxiRij, the reward for the security agent (leader).",
                "The pxi variables give the optimal strategy for the security agent.",
                "Note that while this method is polynomial in the number of rows in the transformed, normal-form game, the number of rows increases exponentially with the number of robber types.",
                "Using this method for a Bayesian game thus requires running |σ2||θ2| separate linear programs.",
                "This is no surprise, since finding the leaders optimal strategy in a Bayesian Stackelberg game is NP-hard [5]. 4.",
                "HEURISTIC APPROACHES Given that finding the optimal strategy for the leader is NP-hard, we provide a heuristic approach.",
                "In this heuristic we limit the possible mixed strategies of the leader to select actions with probabilities that are integer multiples of 1/k for a predetermined integer k. Previous work [14] has shown that strategies with high entropy are beneficial for security applications when opponents utilities are completely unknown.",
                "In our domain, if utilities are not considered, this method will result in uniform-distribution strategies.",
                "One advantage of such strategies is that they are compact to represent (as fractions) and simple to understand; therefore they can be efficiently implemented by real organizations.",
                "We aim to maintain the advantage provided by simple strategies for our security application problem, incorporating the effect of the robbers rewards on the security agents rewards.",
                "Thus, the ASAP heuristic will produce strategies which are k-uniform.",
                "A mixed strategy is denoted k-uniform if it is a uniform distribution on a multiset S of pure strategies with |S| = k. A multiset is a set whose elements may be repeated multiple times; thus, for example, the mixed strategy corresponding to the multiset {1, 1, 2} would take strategy 1 The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) 313 {1,2} {2,1} {1a, 1b} −1α − .9(1 − α), .5α + .6(1 − α) −.375α − .275(1 − α), .125α + .225(1 − α) {1a, 2b} −1α − .025(1 − α), .5α − .025(1 − α) −.375α − .9(1 − α), .125α + .6(1 − α) {2a, 1b} −.125α − .9(1 − α), −.125α + .6(1 − α) −1α − .275(1 − α), .5α + .225(1 − α) {2a, 2b} −.125α − .025(1 − α), −.125α − .025(1 − α) −1α − .9(1 − α), .5α + .6(1 − α) Table 3: Harsanyi Transformed Payoff Table with probability 2/3 and strategy 2 with probability 1/3.",
                "ASAP allows the size of the multiset to be chosen in order to balance the complexity of the strategy reached with the goal that the identified strategy will yield a high reward.",
                "Another advantage of the ASAP heuristic is that it operates directly on the compact Bayesian representation, without requiring the Harsanyi transformation.",
                "This is because the different follower (robber) types are independent of each other.",
                "Hence, evaluating the leader strategy against a Harsanyi-transformed game matrix is equivalent to evaluating against each of the game matrices for the individual follower types.",
                "This independence property is exploited in ASAP to yield a decomposition scheme.",
                "Note that the LP method introduced by [5] to compute optimal Stackelberg policies is unlikely to be decomposable into a small number of games as it was shown to be NP-hard for Bayes-Nash problems.",
                "Finally, note that ASAP requires the solution of only one optimization problem, rather than solving a series of problems as in the LP method of [5].",
                "For a single follower type, the algorithm works the following way.",
                "Given a particular k, for each possible mixed strategy x for the leader that corresponds to a multiset of size k, evaluate the leaders payoff from x when the follower plays a reward-maximizing pure strategy.",
                "We then take the mixed strategy with the highest payoff.",
                "We need only to consider the reward-maximizing pure strategies of the followers (robbers), since for a given fixed strategy x of the security agent, each robber type faces a problem with fixed linear rewards.",
                "If a mixed strategy is optimal for the robber, then so are all the pure strategies in the support of that mixed strategy.",
                "Note also that because we limit the leaders strategies to take on discrete values, the assumption from Section 3.2 that the followers will break ties in the leaders favor is not significant, since ties will be unlikely to arise.",
                "This is because, in domains where rewards are drawn from any random distribution, the probability of a follower having more than one pure optimal response to a given leader strategy approaches zero, and the leader will have only a finite number of possible mixed strategies.",
                "Our approach to characterize the optimal strategy for the security agent makes use of properties of linear programming.",
                "We briefly outline these results here for completeness, for detailed discussion and proofs see one of many references on the topic, such as [2].",
                "Every linear programming problem, such as: max cT x | Ax = b, x ≥ 0, has an associated dual linear program, in this case: min bT y | AT y ≥ c. These primal/dual pairs of problems satisfy weak duality: For any x and y primal and dual feasible solutions respectively, cT x ≤ bT y.",
                "Thus a pair of feasible solutions is optimal if cT x = bT y, and the problems are said to satisfy strong duality.",
                "In fact if a linear program is feasible and has a bounded optimal solution, then the dual is also feasible and there is a pair x∗ , y∗ that satisfies cT x∗ = bT y∗ .",
                "These optimal solutions are characterized with the following optimality conditions (as defined in [2]): • primal feasibility: Ax = b, x ≥ 0 • dual feasibility: AT y ≥ c • complementary slackness: xi(AT y − c)i = 0 for all i.",
                "Note that this last condition implies that cT x = xT AT y = bT y, which proves optimality for primal dual feasible solutions x and y.",
                "In the following subsections, we first define the problem in its most intuititive form as a mixed-integer quadratic program (MIQP), and then show how this problem can be converted into a mixedinteger linear program (MILP). 4.1 Mixed-Integer Quadratic Program We begin with the case of a single type of follower.",
                "Let the leader be the row player and the follower the column player.",
                "We denote by x the vector of strategies of the leader and q the vector of strategies of the follower.",
                "We also denote X and Q the index sets of the leader and followers pure strategies, respectively.",
                "The payoff matrices R and C correspond to: Rij is the reward of the leader and Cij is the reward of the follower when the leader takes pure strategy i and the follower takes pure strategy j.",
                "Let k be the size of the multiset.",
                "We first fix the policy of the leader to some k-uniform policy x.",
                "The value xi is the number of times pure strategy i is used in the k-uniform policy, which is selected with probability xi/k.",
                "We formulate the optimization problem the follower solves to find its optimal response to x as the following linear program: max X j∈Q X i∈X 1 k Cijxi qj s.t.",
                "P j∈Q qj = 1 q ≥ 0. (2) The objective function maximizes the followers expected reward given x, while the constraints make feasible any mixed strategy q for the follower.",
                "The dual to this linear programming problem is the following: min a s.t. a ≥ X i∈X 1 k Cijxi j ∈ Q. (3) From strong duality and complementary slackness we obtain that the followers maximum reward value, a, is the value of every pure strategy with qj > 0, that is in the support of the optimal mixed strategy.",
                "Therefore each of these pure strategies is optimal.",
                "Optimal solutions to the followers problem are characterized by linear programming optimality conditions: primal feasibility constraints in (2), dual feasibility constraints in (3), and complementary slackness qj a − X i∈X 1 k Cijxi ! = 0 j ∈ Q.",
                "These conditions must be included in the problem solved by the leader in order to consider only best responses by the follower to the k-uniform policy x. 314 The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) The leader seeks the k-uniform solution x that maximizes its own payoff, given that the follower uses an optimal response q(x).",
                "Therefore the leader solves the following integer problem: max X i∈X X j∈Q 1 k Rijq(x)j xi s.t.",
                "P i∈X xi = k xi ∈ {0, 1, . . . , k}. (4) Problem (4) maximizes the leaders reward with the followers best response (qj for fixed leaders policy x and hence denoted q(x)j) by selecting a uniform policy from a multiset of constant size k. We complete this problem by including the characterization of q(x) through linear programming optimality conditions.",
                "To simplify writing the complementary slackness conditions, we will constrain q(x) to be only optimal pure strategies by just considering integer solutions of q(x).",
                "The leaders problem becomes: maxx,q X i∈X X j∈Q 1 k Rijxiqj s.t.",
                "P i xi = kP j∈Q qj = 1 0 ≤ (a − P i∈X 1 k Cijxi) ≤ (1 − qj)M xi ∈ {0, 1, ...., k} qj ∈ {0, 1}. (5) Here, the constant M is some large number.",
                "The first and fourth constraints enforce a k-uniform policy for the leader, and the second and fifth constraints enforce a feasible pure strategy for the follower.",
                "The third constraint enforces dual feasibility of the followers problem (leftmost inequality) and the complementary slackness constraint for an optimal pure strategy q for the follower (rightmost inequality).",
                "In fact, since only one pure strategy can be selected by the follower, say qh = 1, this last constraint enforces that a = P i∈X 1 k Cihxi imposing no additional constraint for all other pure strategies which have qj = 0.",
                "We conclude this subsection noting that Problem (5) is an integer program with a non-convex quadratic objective in general, as the matrix R need not be positive-semi-definite.",
                "Efficient solution methods for non-linear, non-convex integer problems remains a challenging research question.",
                "In the next section we show a reformulation of this problem as a linear integer programming problem, for which a number of efficient commercial solvers exist. 4.2 Mixed-Integer Linear Program We can linearize the quadratic program of Problem 5 through the change of variables zij = xiqj, obtaining the following problem maxq,z P i∈X P j∈Q 1 k Rijzij s.t.",
                "P i∈X P j∈Q zij = k P j∈Q zij ≤ k kqj ≤ P i∈X zij ≤ k P j∈Q qj = 1 0 ≤ (a − P i∈X 1 k Cij( P h∈Q zih)) ≤ (1 − qj)M zij ∈ {0, 1, ...., k} qj ∈ {0, 1} (6) PROPOSITION 1.",
                "Problems (5) and (6) are equivalent.",
                "Proof: Consider x, q a feasible solution of (5).",
                "We will show that q, zij = xiqj is a feasible solution of (6) of same objective function value.",
                "The equivalence of the objective functions, and constraints 4, 6 and 7 of (6) are satisfied by construction.",
                "The fact that P j∈Q zij = xi as P j∈Q qj = 1 explains constraints 1, 2, and 5 of (6).",
                "Constraint 3 of (6) is satisfied because P i∈X zij = kqj.",
                "Let us now consider q, z feasible for (6).",
                "We will show that q and xi = P j∈Q zij are feasible for (5) with the same objective value.",
                "In fact all constraints of (5) are readily satisfied by construction.",
                "To see that the objectives match, notice that if qh = 1 then the third constraint in (6) implies that P i∈X zih = k, which means that zij = 0 for all i ∈ X and all j = h. Therefore, xiqj = X l∈Q zilqj = zihqj = zij.",
                "This last equality is because both are 0 when j = h. This shows that the transformation preserves the objective function value, completing the proof.",
                "Given this transformation to a mixed-integer linear program (MILP), we now show how we can apply our decomposition technique on the MILP to obtain significant speedups for Bayesian games with multiple follower types. 5.",
                "DECOMPOSITION FOR MULTIPLE ADVERSARIES The MILP developed in the previous section handles only one follower.",
                "Since our security scenario contains multiple follower (robber) types, we change the response function for the follower from a pure strategy into a weighted combination over various pure follower strategies where the weights are probabilities of occurrence of each of the follower types. 5.1 Decomposed MIQP To admit multiple adversaries in our framework, we modify the notation defined in the previous section to reason about multiple follower types.",
                "We denote by x the vector of strategies of the leader and ql the vector of strategies of follower l, with L denoting the index set of follower types.",
                "We also denote by X and Q the index sets of leader and follower ls pure strategies, respectively.",
                "We also index the payoff matrices on each follower l, considering the matrices Rl and Cl .",
                "Using this modified notation, we characterize the optimal solution of follower ls problem given the leaders k-uniform policy x, with the following optimality conditions: X j∈Q ql j = 1 al − X i∈X 1 k Cl ijxi ≥ 0 ql j(al − X i∈X 1 k Cl ijxi) = 0 ql j ≥ 0 Again, considering only optimal pure strategies for follower ls problem we can linearize the complementarity constraint above.",
                "We incorporate these constraints on the leaders problem that selects the optimal k-uniform policy.",
                "Therefore, given a priori probabilities pl , with l ∈ L of facing each follower, the leader solves the following problem: The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) 315 maxx,q X i∈X X l∈L X j∈Q pl k Rl ijxiql j s.t.",
                "P i xi = kP j∈Q ql j = 1 0 ≤ (al − P i∈X 1 k Cl ijxi) ≤ (1 − ql j)M xi ∈ {0, 1, ...., k} ql j ∈ {0, 1}. (7) Problem (7) for a Bayesian game with multiple follower types is indeed equivalent to Problem (5) on the payoff matrix obtained from the Harsanyi transformation of the game.",
                "In fact, every pure strategy j in Problem (5) corresponds to a sequence of pure strategies jl, one for each follower l ∈ L. This means that qj = 1 if and only if ql jl = 1 for all l ∈ L. In addition, given the a priori probabilities pl of facing player l, the reward in the Harsanyi transformation payoff table is Rij = P l∈L pl Rl ijl .",
                "The same relation holds between C and Cl .",
                "These relations between a pure strategy in the equivalent normal form game and pure strategies in the individual games with each followers are key in showing these problems are equivalent. 5.2 Decomposed MILP We can linearize the quadratic programming problem 7 through the change of variables zl ij = xiql j, obtaining the following problem maxq,z P i∈X P l∈L P j∈Q pl k Rl ijzl ij s.t.",
                "P i∈X P j∈Q zl ij = k P j∈Q zl ij ≤ k kql j ≤ P i∈X zl ij ≤ k P j∈Q ql j = 1 0 ≤ (al − P i∈X 1 k Cl ij( P h∈Q zl ih)) ≤ (1 − ql j)M P j∈Q zl ij = P j∈Q z1 ij zl ij ∈ {0, 1, ...., k} ql j ∈ {0, 1} (8) PROPOSITION 2.",
                "Problems (7) and (8) are equivalent.",
                "Proof: Consider x, ql , al with l ∈ L a feasible solution of (7).",
                "We will show that ql , al , zl ij = xiql j is a feasible solution of (8) of same objective function value.",
                "The equivalence of the objective functions, and constraints 4, 7 and 8 of (8) are satisfied by construction.",
                "The fact that P j∈Q zl ij = xi as P j∈Q ql j = 1 explains constraints 1, 2, 5 and 6 of (8).",
                "Constraint 3 of (8) is satisfied because P i∈X zl ij = kql j.",
                "Lets now consider ql , zl , al feasible for (8).",
                "We will show that ql , al and xi = P j∈Q z1 ij are feasible for (7) with the same objective value.",
                "In fact all constraints of (7) are readily satisfied by construction.",
                "To see that the objectives match, notice for each l one ql j must equal 1 and the rest equal 0.",
                "Let us say that ql jl = 1, then the third constraint in (8) implies that P i∈X zl ijl = k, which means that zl ij = 0 for all i ∈ X and all j = jl.",
                "In particular this implies that xi = X j∈Q z1 ij = z1 ij1 = zl ijl , the last equality from constraint 6 of (8).",
                "Therefore xiql j = zl ijl ql j = zl ij.",
                "This last equality is because both are 0 when j = jl.",
                "Effectively, constraint 6 ensures that all the adversaries are calculating their best responses against a particular fixed policy of the agent.",
                "This shows that the transformation preserves the objective function value, completing the proof.",
                "We can therefore solve this equivalent linear integer program with efficient integer programming packages which can handle problems with thousands of integer variables.",
                "We implemented the decomposed MILP and the results are shown in the following section. 6.",
                "EXPERIMENTAL RESULTS The patrolling domain and the payoffs for the associated game are detailed in Sections 2 and 3.",
                "We performed experiments for this game in worlds of three and four houses with patrols consisting of two houses.",
                "The description given in Section 2 is used to generate a base case for both the security agent and robber payoff functions.",
                "The payoff tables for additional robber types are constructed and added to the game by adding a random distribution of varying size to the payoffs in the base case.",
                "All games are normalized so that, for each robber type, the minimum and maximum payoffs to the security agent and robber are 0 and 1, respectively.",
                "Using the data generated, we performed the experiments using four methods for generating the security agents strategy: • uniform randomization • ASAP • the multiple linear programs method from [5] (to find the true optimal strategy) • the highest reward Bayes-Nash equilibrium, found using the MIP-Nash algorithm [17] The last three methods were applied using CPLEX 8.1.",
                "Because the last two methods are designed for normal-form games rather than Bayesian games, the games were first converted using the Harsanyi transformation [8].",
                "The uniform randomization method is simply choosing a uniform random policy over all possible patrol routes.",
                "We use this method as a simple baseline to measure the performance of our heuristics.",
                "We anticipated that the uniform policy would perform reasonably well since maximum-entropy policies have been shown to be effective in multiagent security domains [14].",
                "The highest-reward Bayes-Nash equilibria were used in order to demonstrate the higher reward gained by looking for an optimal policy rather than an equilibria in Stackelberg games such as our security domain.",
                "Based on our experiments we present three sets of graphs to demonstrate (1) the runtime of ASAP compared to other common methods for finding a strategy, (2) the reward guaranteed by ASAP compared to other methods, and (3) the effect of varying the parameter k, the size of the multiset, on the performance of ASAP.",
                "In the first two sets of graphs, ASAP is run using a multiset of 80 elements; in the third set this number is varied.",
                "The first set of graphs, shown in Figure 1 shows the runtime graphs for three-house (left column) and four-house (right column) domains.",
                "Each of the three rows of graphs corresponds to a different randomly-generated scenario.",
                "The x-axis shows the number of robber types the security agent faces and the y-axis of the graph shows the runtime in seconds.",
                "All experiments that were not concluded in 30 minutes (1800 seconds) were cut off.",
                "The runtime for the uniform policy is always negligible irrespective of the number of adversaries and hence is not shown.",
                "The ASAP algorithm clearly outperforms the optimal, multipleLP method as well as the MIP-Nash algorithm for finding the highestreward Bayes-Nash equilibrium with respect to runtime.",
                "For a 316 The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) Figure 1: Runtimes for various algorithms on problems of 3 and 4 houses. domain of three houses, the optimal method cannot reach a solution for more than seven robber types, and for four houses it cannot solve for more than six types within the cutoff time in any of the three scenarios.",
                "MIP-Nash solves for even fewer robber types within the cutoff time.",
                "On the other hand, ASAP runs much faster, and is able to solve for at least 20 adversaries for the three-house scenarios and for at least 12 adversaries in the four-house scenarios within the cutoff time.",
                "The runtime of ASAP does not increase strictly with the number of robber types for each scenario, but in general, the addition of more types increases the runtime required.",
                "The second set of graphs, Figure 2, shows the reward to the patrol agent given by each method for three scenarios in the three-house (left column) and four-house (right column) domains.",
                "This reward is the utility received by the security agent in the patrolling game, and not as a percentage of the optimal reward, since it was not possible to obtain the optimal reward as the number of robber types increased.",
                "The uniform policy consistently provides the lowest reward in both domains; while the optimal method of course produces the optimal reward.",
                "The ASAP method remains consistently close to the optimal, even as the number of robber types increases.",
                "The highest-reward Bayes-Nash equilibria, provided by the MIPNash method, produced rewards higher than the uniform method, but lower than ASAP.",
                "This difference clearly illustrates the gains in the patrolling domain from committing to a strategy as the leader in a Stackelberg game, rather than playing a standard Bayes-Nash strategy.",
                "The third set of graphs, shown in Figure 3 shows the effect of the multiset size on runtime in seconds (left column) and reward (right column), again expressed as the reward received by the security agent in the patrolling game, and not a percentage of the optimal Figure 2: Reward for various algorithms on problems of 3 and 4 houses. reward.",
                "Results here are for the three-house domain.",
                "The trend is that as as the multiset size is increased, the runtime and reward level both increase.",
                "Not surprisingly, the reward increases monotonically as the multiset size increases, but what is interesting is that there is relatively little benefit to using a large multiset in this domain.",
                "In all cases, the reward given by a multiset of 10 elements was within at least 96% of the reward given by an 80-element multiset.",
                "The runtime does not always increase strictly with the multiset size; indeed in one example (scenario 2 with 20 robber types), using a multiset of 10 elements took 1228 seconds, while using 80 elements only took 617 seconds.",
                "In general, runtime should increase since a larger multiset means a larger domain for the variables in the MILP, and thus a larger search space.",
                "However, an increase in the number of variables can sometimes allow for a policy to be constructed more quickly due to more flexibility in the problem. 7.",
                "SUMMARY AND RELATED WORK This paper focuses on security for agents patrolling in hostile environments.",
                "In these environments, intentional threats are caused by adversaries about whom the security patrolling agents have incomplete information.",
                "Specifically, we deal with situations where the adversaries actions and payoffs are known but the exact adversary type is unknown to the security agent.",
                "Agents acting in the real world quite frequently have such incomplete information about other agents.",
                "Bayesian games have been a popular choice to model such incomplete information games [3].",
                "The Gala toolkit is one method for defining such games [9] without requiring the game to be represented in normal form via the Harsanyi transformation [8]; Galas guarantees are focused on fully competitive games.",
                "Much work has been done on finding optimal Bayes-Nash equilbria for The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) 317 Figure 3: Reward for ASAP using multisets of 10, 30, and 80 elements subclasses of Bayesian games, finding single Bayes-Nash equilibria for general Bayesian games [10] or approximate Bayes-Nash equilibria [18].",
                "Less attention has been paid to finding the optimal strategy to commit to in a Bayesian game (the Stackelberg scenario [15]).",
                "However, the complexity of this problem was shown to be NP-hard in the general case [5], which also provides algorithms for this problem in the non-Bayesian case.",
                "Therefore, we present a heuristic called ASAP, with three key advantages towards addressing this problem.",
                "First, ASAP searches for the highest reward strategy, rather than a Bayes-Nash equilibrium, allowing it to find feasible strategies that exploit the natural first-mover advantage of the game.",
                "Second, it provides strategies which are simple to understand, represent, and implement.",
                "Third, it operates directly on the compact, Bayesian game representation, without requiring conversion to normal form.",
                "We provide an efficient Mixed Integer Linear Program (MILP) implementation for ASAP, along with experimental results illustrating significant speedups and higher rewards over other approaches.",
                "Our k-uniform strategies are similar to the k-uniform strategies of [12].",
                "While that work provides epsilon error-bounds based on the k-uniform strategies, their solution concept is still that of a Nash equilibrium, and they do not provide efficient algorithms for obtaining such k-uniform strategies.",
                "This contrasts with ASAP, where our emphasis is on a highly efficient heuristic approach that is not focused on equilibrium solutions.",
                "Finally the patrolling problem which motivated our work has recently received growing attention from the multiagent community due to its wide range of applications [4, 13].",
                "However most of this work is focused on either limiting energy consumption involved in patrolling [7] or optimizing on criteria like the length of the path traveled [4, 13], without reasoning about any explicit model of an adversary[14].",
                "Acknowledgments : This research is supported by the United States Department of Homeland Security through Center for Risk and Economic Analysis of Terrorism Events (CREATE).",
                "It is also supported by the Defense Advanced Research Projects Agency (DARPA), through the Department of the Interior, NBC, Acquisition Services Division, under Contract No.",
                "NBCHD030010.",
                "Sarit Kraus is also affiliated with UMIACS. 8.",
                "REFERENCES [1] R. W. Beard and T. McLain.",
                "Multiple UAV cooperative search under collision avoidance and limited range communication constraints.",
                "In IEEE CDC, 2003. [2] D. Bertsimas and J. Tsitsiklis.",
                "Introduction to Linear Optimization.",
                "Athena Scientific, 1997. [3] J. Brynielsson and S. Arnborg.",
                "Bayesian games for threat prediction and situation analysis.",
                "In FUSION, 2004. [4] Y. Chevaleyre.",
                "Theoretical analysis of multi-agent patrolling problem.",
                "In AAMAS, 2004. [5] V. Conitzer and T. Sandholm.",
                "Choosing the best strategy to commit to.",
                "In ACM Conference on Electronic Commerce, 2006. [6] D. Fudenberg and J. Tirole.",
                "<br>game theory</br>.",
                "MIT Press, 1991. [7] C. Gui and P. Mohapatra.",
                "Virtual patrol: A new power conservation design for surveillance using sensor networks.",
                "In IPSN, 2005. [8] J. C. Harsanyi and R. Selten.",
                "A generalized Nash solution for two-person bargaining games with incomplete information.",
                "Management Science, 18(5):80-106, 1972. [9] D. Koller and A. Pfeffer.",
                "Generating and solving imperfect information games.",
                "In IJCAI, pages 1185-1193, 1995. [10] D. Koller and A. Pfeffer.",
                "Representations and solutions for game-theoretic problems.",
                "Artificial Intelligence, 94(1):167-215, 1997. [11] C. Lemke and J. Howson.",
                "Equilibrium points of bimatrix games.",
                "Journal of the Society for Industrial and Applied Mathematics, 12:413-423, 1964. [12] R. J. Lipton, E. Markakis, and A. Mehta.",
                "Playing large games using simple strategies.",
                "In ACM Conference on Electronic Commerce, 2003. [13] A. Machado, G. Ramalho, J. D. Zucker, and A. Drougoul.",
                "Multi-agent patrolling: an empirical analysis on alternative architectures.",
                "In MABS, 2002. [14] P. Paruchuri, M. Tambe, F. Ordonez, and S. Kraus.",
                "Security in multiagent systems by policy randomization.",
                "In AAMAS, 2006. [15] T. Roughgarden.",
                "Stackelberg scheduling strategies.",
                "In ACM Symposium on TOC, 2001. [16] S. Ruan, C. Meirina, F. Yu, K. R. Pattipati, and R. L. Popp.",
                "Patrolling in a stochastic environment.",
                "In 10th Intl.",
                "Command and Control Research Symp., 2005. [17] T. Sandholm, A. Gilpin, and V. Conitzer.",
                "Mixed-integer programming methods for finding nash equilibria.",
                "In AAAI, 2005. [18] S. Singh, V. Soni, and M. Wellman.",
                "Computing approximate Bayes-Nash equilibria with tree-games of incomplete information.",
                "In ACM Conference on Electronic Commerce, 2004. 318 The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07)"
            ],
            "original_annotated_samples": [],
            "translated_annotated_samples": [
                "Dado que la transformación de Harsanyi es un concepto estándar en la \"teoría del juego\", lo explicamos brevemente a través de un ejemplo simple en nuestro dominio de patrullaje sin introducir las formulaciones matemáticas.",
                "\"teoría de juego\"."
            ],
            "translated_text": "",
            "candidates": [
                "teoría de juego",
                "teoría del juego",
                "teoría de juego",
                "teoría de juego"
            ],
            "error": []
        }
    }
}