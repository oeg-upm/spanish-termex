{
    "id": "H-50",
    "original_text": "An Outranking Approach for Rank Aggregation in Information Retrieval Mohamed Farah Lamsade, Paris Dauphine University Place du Mal de Lattre de Tassigny 75775 Paris Cedex 16, France farah@lamsade.dauphine.fr Daniel Vanderpooten Lamsade, Paris Dauphine University Place du Mal de Lattre de Tassigny 75775 Paris Cedex 16, France vdp@lamsade.dauphine.fr ABSTRACT Research in Information Retrieval usually shows performance improvement when many sources of evidence are combined to produce a ranking of documents (e.g., texts, pictures, sounds, etc.). In this paper, we focus on the rank aggregation problem, also called data fusion problem, where rankings of documents, searched into the same collection and provided by multiple methods, are combined in order to produce a new ranking. In this context, we propose a rank aggregation method within a multiple criteria framework using aggregation mechanisms based on decision rules identifying positive and negative reasons for judging whether a document should get a better rank than another. We show that the proposed method deals well with the Information Retrieval distinctive features. Experimental results are reported showing that the suggested method performs better than the well-known CombSUM and CombMNZ operators. Categories and Subject Descriptors: H.3.3 [Information Systems]: Information Search and Retrieval - Retrieval models. General Terms: Algorithms, Measurement, Experimentation, Performance, Theory. 1. INTRODUCTION A wide range of current Information Retrieval (IR) approaches are based on various search models (Boolean, Vector Space, Probabilistic, Language, etc. [2]) in order to retrieve relevant documents in response to a user request. The result lists produced by these approaches depend on the exact definition of the relevance concept. Rank aggregation approaches, also called data fusion approaches, consist in combining these result lists in order to produce a new and hopefully better ranking. Such approaches give rise to metasearch engines in the Web context. We consider, in the following, cases where only ranks are available and no other additional information is provided such as the relevance scores. This corresponds indeed to the reality, where only ordinal information is available. Data fusion is also relevant in other contexts, such as when the user writes several queries of his/her information need (e.g., a boolean query and a natural language query) [4], or when many document surrogates are available [16]. Several studies argued that rank aggregation has the potential of combining effectively all the various sources of evidence considered in various input methods. For instance, experiments carried out in [16], [30], [4] and [19] showed that documents which appear in the lists of the majority of the input methods are more likely to be relevant. Moreover, Lee [19] and Vogt and Cottrell [31] found that various retrieval approaches often return very different irrelevant documents, but many of the same relevant documents. Bartell et al. [3] also found that rank aggregation methods improve the performances w.r.t. those of the input methods, even when some of them have weak individual performances. These methods also tend to smooth out biases of the input methods according to Montague and Aslam [22]. Data fusion has recently been proved to improve performances for both the ad-hoc retrieval and categorization tasks within the TREC genomics track in 2005 [1]. The rank aggregation problem was addressed in various fields such as i) in social choice theory which studies voting algorithms which specify winners of elections or winners of competitions in tournaments [29], ii) in statistics when studying correlation between rankings, iii) in distributed databases when results from different databases must be combined [12], and iv) in collaborative filtering [23]. Most current rank aggregation methods consider each input ranking as a permutation over the same set of items. They also give rigid interpretation to the exact ranking of the items. Both of these assumptions are rather not valid in the IR context, as will be shown in the following sections. The remaining of the paper is organized as follows. We first review current rank aggregation methods in Section 2. Then we outline the specificities of the data fusion problem in the IR context (Section 3). In Section 4, we present a new aggregation method which is proven to best fit the IR context. Experimental results are presented in Section 5 and conclusions are provided in a final section. 2. RELATED WORK As pointed out by Riker [25], we can distinguish two families of rank aggregation methods: positional methods which assign scores to items to be ranked according to the ranks they receive and majoritarian methods which are based on pairwise comparisons of items to be ranked. These two families of methods find their roots in the pioneering works of Borda [5] and Condorcet [7], respectively, in the social choice literature. 2.1 Preliminaries We first introduce some basic notations to present the rank aggregation methods in a uniform way. Let D = {d1, d2, . . . , dnd } be a set of nd documents. A list or a ranking j is an ordering defined on Dj ⊆ D (j = 1, . . . , n). Thus, di j di means di is ranked better than di in j. When Dj = D, j is said to be a full list. Otherwise, it is a partial list. If di belongs to Dj, rj i denotes the rank or position of di in j. We assume that the best answer (document) is assigned the position 1 and the worst one is assigned the position |Dj|. Let D be the set of all permutations on D or all subsets of D. A profile is a n-tuple of rankings PR = ( 1, 2, . . . , n). Restricting PR to the rankings containing document di defines PRi. We also call the number of rankings which contain document di the rank hits of di [19]. The rank aggregation or data fusion problem consists of finding a ranking function or mechanism Ψ (also called a social welfare function in the social choice theory terminology) defined by: Ψ : n D → D PR = ( 1, 2, . . . , n) → σ = Ψ(PR) where σ is called a consensus ranking. 2.2 Positional Methods 2.2.1 Borda Count This method [5] first assigns a score n j=1 rj i to each document di. Documents are then ranked by increasing order of this score, breaking ties, if any, arbitrarily. 2.2.2 Linear Combination Methods This family of methods basically combine scores of documents. When used for the rank aggregation problem, ranks are assumed to be scores or performances to be combined using aggregation operators such as the weighted sum or some variation of it [3, 31, 17, 28]. For instance, Callan et al. [6] used the inference networks model [30] to combine rankings. Fox and Shaw [15] proposed several combination strategies which are CombSUM, CombMIN, CombMAX, CombANZ and CombMNZ. The first three operators correspond to the sum, min and max operators, respectively. CombANZ and CombMNZ respectively divides and multiplies the CombSUM score by the rank hits. It is shown in [19] that the CombSUM and CombMNZ operators perform better than the others. Metasearch engines such as SavvySearch and MetaCrawler use the CombSUM strategy to fuse rankings. 2.2.3 Footrule Optimal Aggregation In this method, a consensus ranking minimizes the Spearman footrule distance from the input rankings [21]. Formally, given two full lists j and j , this distance is given by F( j, j ) = nd i=1 |rj i − rj i |. It extends to several lists as follows. Given a profile PR and a consensus ranking σ, the Spearman footrule distance of σ to PR is given by F(σ, PR) = n j=1 F(σ, j). Cook and Kress [8] proposed a similar method which consists in optimizing the distance D( j, j ) = 1 2 nd i,i =1 |rj i,i − rj i,i |, where rj i,i = rj i −rj i . This formulation has the advantage that it considers the intensity of preferences. 2.2.4 Probabilistic Methods This kind of methods assume that the performance of the input methods on a number of training queries is indicative of their future performance. During the training process, probabilities of relevance are calculated. For subsequent queries, documents are ranked based on these probabilities. For instance, in [20], each input ranking j is divided into a number of segments, and the conditional probability of relevance (R) of each document di depending on the segment k it occurs in, is computed, i.e. prob(R|di, k, j). For subsequent queries, the score of each document di is given by n j=1 prob(R|di,k, j ) k . Le Calve and Savoy [18] suggest using a logistic regression approach for combining scores. Training data is needed to infer the model parameters. 2.3 Majoritarian Methods 2.3.1 Condorcet Procedure The original Condorcet rule [7] specifies that a winner of the election is any item that beats or ties with every other item in a pairwise contest. Formally, let C(diσdi ) = { j∈ PR : di j di } be the coalition of rankings that are concordant with establishing diσdi , i.e. with the proposition di should be ranked better than di in the final ranking σ. di beats or ties with di iff |C(diσdi )| ≥ |C(di σdi)|. The repetitive application of the Condorcet algorithm can produce a ranking of items in a natural way: select the Condorcet winner, remove it from the lists, and repeat the previous two steps until there are no more documents to rank. Since there is not always Condorcet winners, variations of the Condorcet procedure have been developed within the multiple criteria decision aid theory, with methods such as ELECTRE [26]. 2.3.2 Kemeny Optimal Aggregation As in section 2.2.3, a consensus ranking minimizes a geometric distance from the input rankings, where the Kendall tau distance is used instead of the Spearman footrule distance. Formally, given two full lists j and j , the Kendall tau distance is given by K( j, j ) = |{(di, di ) : i < i , rj i < rj i , rj i > rj i }|, i.e. the number of pairwise disagreements between the two lists. It is easy to show that the consensus ranking corresponds to the geometric median of the input rankings and that the Kemeny optimal aggregation problem corresponds to the minimum feedback edge set problem. 2.3.3 Markov Chain Methods Markov chains (MCs) have been used by Dwork et al. [11] as a natural method to obtain a consensus ranking where states correspond to the documents to be ranked and the transition probabilities vary depending on the interpretation of the transition event. In the same reference, the authors proposed four specific MCs and experimental testing had shown that the following MC is the best performing one (see also [24]): • MC4: move from the current state di to the next state di by first choosing a document di uniformly from D. If for the majority of the rankings, we have rj i ≤ rj i , then move to di , else stay in di. The consensus ranking corresponds to the stationary distribution of MC4. 3. SPECIFICITIES OF THE RANK AGGREGATION PROBLEM IN THE IR CONTEXT 3.1 Limited Significance of the Rankings The exact positions of documents in one input ranking have limited significance and should not be overemphasized. For instance, having three relevant documents in the first three positions, any perturbation of these three items will have the same value. Indeed, in the IR context, the complete order provided by an input method may hide ties. In this case, we call such rankings semi orders. This was outlined in [13] as the problem of aggregation with ties. It is therefore important to build the consensus ranking based on robust information: • Documents with near positions in j are more likely to have similar interest or relevance. Thus a slight perturbation of the initial ranking is meaningless. • Assuming that document di is better ranked than document di in a ranking j, di is more likely to be definitively more relevant than di in j when the number of intermediate positions between di and di increases. 3.2 Partial Lists In real world applications, such as metasearch engines, rankings provided by the input methods are often partial lists. This was outlined in [14] as the problem of having to merge top-k results from various input lists. For instance, in the experiments carried out by Dwork et al. [11], authors found that among the top 100 best documents of 7 input search engines, 67% of the documents were present in only one search engine, whereas less than two documents were present in all the search engines. Rank aggregation of partial lists raises four major difficulties which we state hereafter, proposing for each of them various working assumptions: 1. Partial lists can have various lengths, which can favour long lists. We thus consider the following two working hypotheses: H1 k : We only consider the top k best documents from each input ranking. H1 all: We consider all the documents from each input ranking. 2. Since there are different documents in the input rankings, we must decide which documents should be kept in the consensus ranking. Two working hypotheses are therefore considered: H2 k : We only consider documents which are present in at least k input rankings (k > 1). H2 all: We consider all the documents which are ranked in at least one input ranking. Hereafter, we call documents which will be retained in the consensus ranking, candidate documents, and documents that will be excluded from the consensus ranking, excluded documents. We also call a candidate document which is missing in one or more rankings, a missing document. 3. Some candidate documents are missing documents in some input rankings. Main reasons for a missing document are that it was not indexed or it was indexed but deemed irrelevant ; usually this information is not available. We consider the following two working hypotheses: H3 yes: Each missing document in each j is assigned a position. H3 no: No assumption is made, that is each missing document is considered neither better nor worse than any other document. 4. When assumption H2 k holds, each input ranking may contain documents which will not be considered in the consensus ranking. Regarding the positions of the candidate documents, we can consider the following working hypotheses: H4 init: The initial positions of candidate documents are kept in each input ranking. H4 new: Candidate documents receive new positions in each input ranking, after discarding excluded ones. In the IR context, rank aggregation methods need to decide more or less explicitly which assumptions to retain w.r.t. the above-mentioned difficulties. 4. OUTRANKING APPROACH FOR RANK AGGREGATION 4.1 Presentation Positional methods consider implicitly that the positions of the documents in the input rankings are scores giving thus a cardinal meaning to an ordinal information. This constitutes a strong assumption that is questionable, especially when the input rankings have different lengths. Moreover, for positional methods, assumptions H3 and H4 , which are often arbitrary, have a strong impact on the results. For instance, let us consider an input ranking of 500 documents out of 1000 candidate documents. Whether we assign to each of the missing documents the position 1, 501, 750 or 1000 -corresponding to variations of H3 yes- will give rise to very contrasted results, especially regarding the top of the consensus ranking. Majoritarian methods do not suffer from the above-mentioned drawbacks of the positional methods since they build consensus rankings exploiting only ordinal information contained in the input rankings. Nevertheless, they suppose that such rankings are complete orders, ignoring that they may hide ties. Therefore, majoritarian methods base consensus rankings on illusory discriminant information rather than less discriminant but more robust information. Trying to overcome the limits of current rank aggregation methods, we found that outranking approaches, which were initially used for multiple criteria aggregation problems [26], can also be used for the rank aggregation purpose, where each ranking plays the role of a criterion. Therefore, in order to decide whether a document di should be ranked better than di in the consensus ranking σ, the two following conditions should be met: • a concordance condition which ensures that a majority of the input rankings are concordant with diσdi (majority principle). • a discordance condition which ensures that none of the discordant input rankings strongly refutes dσd (respect of minorities principle). Formally, the concordance coalition with diσdi is Csp (diσdi ) = { j∈ PR : rj i ≤ rj i − sp} where sp is a preference threshold which is the variation of document positions -whether it is absolute or relative to the ranking length- which draws the boundaries between an indifference and a preference situation between documents. The discordance coalition with diσdi is Dsv (diσdi ) = { j∈ PR : rj i ≥ rj i + sv} where sv is a veto threshold which is the variation of document positions -whether it is absolute or relative to the ranking length- which draws the boundaries between a weak and a strong opposition to diσdi . Depending on the exact definition of the preceding concordance and discordance coalitions leading to the definition of some decision rules, several outranking relations can be defined. They can be more or less demanding depending on i) the values of the thresholds sp and sv, ii) the importance or minimal size cmin required for the concordance coalition, and iii) the importance or maximum size dmax of the discordance coalition. A generic outranking relation can thus be defined as follows: diS(sp,sv,cmin,dmax)di ⇔ |Csp (diσdi )| ≥ cmin AND |Dsv (diσdi )| ≤ dmax This expression defines a family of nested outranking relations since S(sp,sv,cmin,dmax) ⊆ S(sp,sv,cmin,dmax) when cmin ≥ cmin and/or dmax ≤ dmax and/or sp ≥ sp and/or sv ≤ sv. This expression also generalizes the majority rule which corresponds to the particular relation S(0,∞, n 2 ,n). It also satisfies important properties of rank aggregation methods, called neutrality, Pareto-optimality, Condorcet property and Extended Condorcet property, in the social choice literature [29]. Outranking relations are not necessarily transitive and do not necessarily correspond to rankings since directed cycles may exist. Therefore, we need specific procedures in order to derive a consensus ranking. We propose the following procedure which finds its roots in [27]. It consists in partitioning the set of documents into r ranked classes. Each class Ch contains documents with the same relevance and results from the application of all relations (if possible) to the set of documents remaining after previous classes are computed. Documents within the same equivalence class are ranked arbitrarily. Formally, let • R be the set of candidate documents for a query, • S1 , S2 , . . . be a family of nested outranking relations, • Fk(di, E) = |{di ∈ E : diSk di }| be the number of documents in E(E ⊆ R) that could be considered worse than di according to relation Sk , • fk(di, E) = |{di ∈ E : di Sk di}| be the number of documents in E that could be considered better than di according to Sk , • sk(di, E) = Fk(di, E) − fk(di, E) be the qualification of di in E according to Sk . Each class Ch results from a distillation process. It corresponds to the last distillate of a series of sets E0 ⊇ E1 ⊇ . . . where E0 = R \\ (C1 ∪ . . . ∪ Ch−1) and Ek is a reduced subset of Ek−1 resulting from the application of the following procedure: 1. compute for each di ∈ Ek−1 its qualification according to Sk , i.e. sk(di, Ek−1), 2. define smax = maxdi∈Ek−1 {sk(di, Ek−1)}, then 3. Ek = {di ∈ Ek−1 : sk(di, Ek−1) = smax} When one outranking relation is used, the distillation process stops after the first application of the previous procedure, i.e., Ch corresponds to distillate E1. When different outranking relations are used, the distillation process stops when all the pre-defined outranking relations have been used or when |Ek| = 1. 4.2 Illustrative Example This section illustrates the concepts and procedures of section 4.1. Let us consider a set of candidate documents R = {d1, d2, d3, d4, d5}. The following table gives a profile PR of different rankings of the documents of R: PR = ( 1 , 2, 3, 4). Table 1: Rankings of documents rj i 1 2 3 4 d1 1 3 1 5 d2 2 1 3 3 d3 3 2 2 1 d4 4 4 5 2 d5 5 5 4 4 Let us suppose that the preference and veto thresholds are set to values 1 and 4 respectively, and that the concordance and discordance thresholds are set to values 2 and 1 respectively. The following tables give the concordance, discordance and outranking matrices. Each entry csp (di, di ) (dsv (di, di )) in the concordance (discordance) matrix gives the number of rankings that are concordant (discordant) with diσdi , i.e. csp (di, di ) = |Csp (diσdi )| and dsv (di, di ) = |Dsv (diσdi )|. Table 2: Computation of the outranking relation d1 d2 d3 d4 d5 d1 - 2 2 3 3 d2 2 - 2 3 4 d3 2 2 - 4 4 d4 1 1 0 - 3 d5 1 0 0 1Concordance Matrix d1 d2 d3 d4 d5 d1 - 0 1 0 0 d2 0 - 0 0 0 d3 0 0 - 0 0 d4 1 0 0 - 0 d5 1 1 0 0Discordance Matrix d1 d2 d3 d4 d5 d1 - 1 1 1 1 d2 1 - 1 1 1 d3 1 1 - 1 1 d4 0 0 0 - 1 d5 0 0 0 0Outranking Matrix (S1) For instance, the concordance coalition for the assertion d1σd4 is C1(d1σd4) = { 1, 2, 3} and the discordance coalition for the same assertion is D4(d1σd4) = ∅. Therefore, c1(d1, d4) = 3, d4(d1, d4) = 0 and d1S1 d4 holds. Notice that Fk(di, R) (fk(di, R)) is given by summing the values of the ith row (column) of the outranking matrix. The consensus ranking is obtained as follows: to get the first class C1, we compute the qualifications of all the documents of E0 = R with respect to S1 . They are respectively 2, 2, 2, -2 and -4. Therefore smax equals 2 and C1 = E1 = {d1, d2, d3}. Observe that, if we had used a second outranking relation S2(⊇ S1), these three documents could have been possibly discriminated. At this stage, we remove documents of C1 from the outranking matrix and compute the next class C2: we compute the new qualifications of the documents of E0 = R \\ C1 = {d4, d5}. They are respectively 1 and -1. So C3 = E1 = {d4}. The last document d5 is the only document of the last class C3. Thus, the consensus ranking is {d1, d2, d3} → {d4} → {d5}. 5. EXPERIMENTS AND RESULTS 5.1 Test Setting To facilitate empirical investigation of the proposed methodology, we developed a prototype metasearch engine that implements a version of our outranking approach for rank aggregation. In this paper, we apply our approach to the Topic Distillation (TD) task of TREC-2004 Web track [10]. In this task, there are 75 topics where only a short description of each is given. For each query, we retained the rankings of the 10 best runs of the TD task which are provided by TREC-2004 participating teams. The performances of these runs are reported in table 3. Table 3: Performances of the 10 best runs of the TD task of TREC-2004 Run Id MAP P@10 S@1 S@5 S@10 uogWebCAU150 17.9% 24.9% 50.7% 77.3% 89.3% MSRAmixed1 17.8% 25.1% 38.7% 72.0% 88.0% MSRC04C12 16.5% 23.1% 38.7% 74.7% 80.0% humW04rdpl 16.3% 23.1% 37.3% 78.7% 90.7% THUIRmix042 14.7% 20.5% 21.3% 58.7% 74.7% UAmsT04MWScb 14.6% 20.9% 36.0% 66.7% 76.0% ICT04CIIS1AT 14.1% 20.8% 33.3% 64.0% 78.7% SJTUINCMIX5 12.9% 18.9% 29.3% 57.3% 72.0% MU04web1 11.5% 19.9% 33.3% 64.0% 76.0% MeijiHILw3 11.5% 15.3% 30.7% 54.7% 64.0% Average 14.7% 21.2% 34.9% 66.8% 78.94% For each query, each run provides a ranking of about 1000 documents. The number of documents retrieved by all these runs ranges from 543 to 5769. Their average (median) number is 3340 (3386). It is worth noting that we found similar distributions of the documents among the rankings as in [11]. For evaluation, we used the trec eval standard tool which is used by the TREC community to calculate the standard measures of system effectiveness which are Mean Average Precision (MAP) and Success@n (S@n) for n=1, 5 and 10. Our approach effectiveness is compared against some high performing official results from TREC-2004 as well as against some standard rank aggregation algorithms. In the experiments, significance testing is mainly based on the t-student statistic which is computed on the basis of the MAP values of the compared runs. In the tables of the following section, statistically significant differences are marked with an asterisk. Values between brackets of the first column of each table, indicate the parameter value of the corresponding run. 5.2 Results We carried out several series of runs in order to i) study performance variations of the outranking approach when tuning the parameters and working assumptions, ii) compare performances of the outranking approach vs standard rank aggregation strategies , and iii) check whether rank aggregation performs better than the best input rankings. We set our basic run mcm with the following parameters. We considered that each input ranking is a complete order (sp = 0) and that an input ranking strongly refutes diσdi when the difference of both document positions is large enough (sv = 75%). Preference and veto thresholds are computed proportionally to the number of documents retained in each input ranking. They consequently may vary from one ranking to another. In addition, to accept the assertion diσdi , we supposed that the majority of the rankings must be concordant (cmin = 50%) and that every input ranking can impose its veto (dmax = 0). Concordance and discordance thresholds are computed for each tuple (di, di ) as the percentage of the input rankings of PRi ∩PRi . Thus, our choice of parameters leads to the definition of the outranking relation S(0,75%,50%,0). To test the run mcm, we had chosen the following assumptions. We retained the top 100 best documents from each input ranking (H1 100), only considered documents which are present in at least half of the input rankings (H2 5 ) and assumed H3 no and H4 new. In these conditions, the number of successful documents was about 100 on average, and the computation time per query was less than one second. Obviously, modifying the working assumptions should have deeper impact on the performances than tuning our model parameters. This was validated by preliminary experiments. Thus, we hereafter begin by studying performance variation when different sets of assumptions are considered. Afterwards, we study the impact of tuning parameters. Finally, we compare our model performances w.r.t. the input rankings as well as some standard data fusion algorithms. 5.2.1 Impact of the Working Assumptions Table 4 summarizes the performance variation of the outranking approach under different working hypotheses. In Table 4: Impact of the working assumptions Run Id MAP S@1 S@5 S@10 mcm 18.47% 41.33% 81.33% 86.67% mcm22 (H3 yes) 17.72% (-4.06%) 34.67% 81.33% 86.67% mcm23 (H4 init) 18.26% (-1.14%) 41.33% 81.33% 86.67% mcm24 (H1 all) 20.67% (+11.91%*) 38.66% 80.00% 86.66% mcm25 (H2 all) 21.68% (+17.38%*) 40.00% 78.66% 89.33% this table, we first show that run mcm22, in which missing documents are all put in the same last position of each input ranking, leads to performance drop w.r.t. run mcm. Moreover, S@1 moves from 41.33% to 34.67% (-16.11%). This shows that several relevant documents which were initially put at the first position of the consensus ranking in mcm, lose this first position but remain ranked in the top 5 documents since S@5 did not change. We also conclude that documents which have rather good positions in some input rankings are more likely to be relevant, even though they are missing in some other rankings. Consequently, when they are missing in some rankings, assigning worse ranks to these documents is harmful for performance. Also, from Table 4, we found that the performances of runs mcm and mcm23 are similar. Therefore, the outranking approach is not sensitive to keeping the initial positions of candidate documents or recomputing them by discarding excluded ones. From the same Table 4, performance of the outranking approach increases significantly for runs mcm24 and mcm25. Therefore, whether we consider all the documents which are present in half of the rankings (mcm24) or we consider all the documents which are ranked in the first 100 positions in one or more rankings (mcm25), increases performances. This result was predictable since in both cases we have more detailed information on the relative importance of documents. Tables 5 and 6 confirm this evidence. Table 5, where values between brackets of the first column give the number of documents which are retained from each input ranking, shows that selecting more documents from each input ranking leads to performance increase. It is worth mentioning that selecting more than 600 documents from each input ranking does not improve performance. Table 5: Impact of the number of retained documents Run Id MAP S@1 S@5 S@10 mcm (100) 18.47% 41.33% 81.33% 86.67% mcm24-1 (200) 19.32% (+4.60%) 42.67% 78.67% 88.00% mcm24-2 (400) 19.88% (+7.63%*) 37.33% 80.00% 88.00% mcm24-3 (600) 20.80% (+12.62%*) 40.00% 80.00% 88.00% mcm24-4 (800) 20.66% (+11.86%*) 40.00% 78.67% 86.67% mcm24 (1000) 20.67% (+11.91%*) 38.66% 80.00% 86.66% Table 6 reports runs corresponding to variations of H2 k . Values between brackets are rank hits. For instance, in the run mcm32, only documents which are present in 3 or more input rankings, were considered successful. This table shows that performance is significantly better when rare documents are considered, whereas it decreases significantly when these documents are discarded. Therefore, we conclude that many of the relevant documents are retrieved by a rather small set of IR models. Table 6: Performance considering different rank hits Run Id MAP S@1 S@5 S@10 mcm25 (1) 21.68% (+17.38%*) 40.00% 78.67% 89.33% mcm32 (3) 18.98% (+2.76%) 38.67% 80.00% 85.33% mcm (5) 18.47% 41.33% 81.33% 86.67% mcm33 (7) 15.83% (-14.29%*) 37.33% 78.67% 85.33% mcm34 (9) 10.96% (-40.66%*) 36.11% 66.67% 70.83% mcm35 (10) 7.42% (-59.83%*) 39.22% 62.75% 64.70% For both runs mcm24 and mcm25, the number of successful documents was about 1000 and therefore, the computation time per query increased and became around 5 seconds. 5.2.2 Impact of the Variation of the Parameters Table 7 shows performance variation of the outranking approach when different preference thresholds are considered. We found performance improvement up to threshold values of about 5%, then there is a decrease in the performance which becomes significant for threshold values greater than 10%. Moreover, S@1 improves from 41.33% to 46.67% when preference threshold changes from 0 to 5%. We can thus conclude that the input rankings are semi orders rather than complete orders. Table 8 shows the evolution of the performance measures w.r.t. the concordance threshold. We can conclude that in order to put document di before di in the consensus ranking, Table 7: Impact of the variation of the preference threshold from 0 to 12.5% Run Id MAP S@1 S@5 S@10 mcm (0%) 18.47% 41.33% 81.33% 86.67% mcm1 (1%) 18.57% (+0.54%) 41.33% 81.33% 86.67% mcm2 (2.5%) 18.63% (+0.87%) 42.67% 78.67% 86.67% mcm3 (5%) 18.69% (+1.19%) 46.67% 81.33% 86.67% mcm4 (7.5%) 18.24% (-1.25%) 46.67% 81.33% 86.67% mcm5 (10%) 17.93% (-2.92%) 40.00% 82.67% 86.67% mcm5b (12.5%) 17.51% (-5.20%*) 41.33% 80.00% 86.67% at least half of the input rankings of PRi ∩ PRi should be concordant. Performance drops significantly for very low and very high values of the concordance threshold. In fact, for such values, the concordance condition is either fulfilled rather always by too many document pairs or not fulfilled at all, respectively. Therefore, the outranking relation becomes either too weak or too strong respectively. Table 8: Impact of the variation of cmin Run Id MAP S@1 S@5 S@10 mcm11 (20%) 17.63% (-4.55%*) 41.33% 76.00% 85.33% mcm12 (40%) 18.37% (-0.54%) 42.67% 76.00% 86.67% mcm (50%) 18.47% 41.33% 81.33% 86.67% mcm13 (60%) 18.42% (-0.27%) 40.00% 78.67% 86.67% mcm14 (80%) 17.43% (-5.63%*) 40.00% 78.67% 86.67% mcm15 (100%) 16.12% (-12.72%*) 41.33% 70.67% 85.33% In the experiments, varying the veto threshold as well as the discordance threshold within reasonable intervals does not have significant impact on performance measures. In fact, runs with different veto thresholds (sv ∈ [50%; 100%]) had similar performances even though there is a slight advantage for runs with high threshold values which means that it is better not to allow the input rankings to put their veto easily. Also, tuning the discordance threshold was carried out for values 50% and 75% of the veto threshold. For these runs we did not get any noticeable performance variation, although for low discordance thresholds (dmax < 20%), performance slightly decreased. 5.2.3 Impact of the Variation of the Number of Input Rankings To study performance evolution when different sets of input rankings are considered, we carried three more runs where 2, 4, and 6 of the best performing sets of the input rankings are considered. Results reported in Table 9 are seemingly counter-intuitive and also do not support previous findings regarding rank aggregation research [3]. Nevertheless, this result shows that low performing rankings bring more noise than information to the establishment of the consensus ranking. Therefore, when they are considered, performance decreases. Table 9: Performance considering different best performing sets of input rankings Run Id MAP S@1 S@5 S@10 mcm (10) 18.47% 41.33% 81.33% 86.67% mcm27 (6) 18.60% (+0.70%) 41.33% 80.00% 85.33% mcm28 (4) 19.02% (+2.98%) 40.00% 86.67% 88.00% mcm29 (2) 18.33% (-0.76%) 44.00% 76.00% 88.00% 5.2.4 Comparison of the Performance of Different Rank Aggregation Methods In this set of runs, we compare the outranking approach with some standard rank aggregation methods which were proven to have acceptable performance in previous studies: we considered two positional methods which are the CombSUM and the CombMNZ strategies. We also examined the performance of one majoritarian method which is the Markov chain method (MC4). For the comparisons, we considered a specific outranking relation S∗ = S(5%,50%,50%,30%) which results in good overall performances when tuning all the parameters. The first row of Table 10 gives performances of the rank aggregation methods w.r.t. a basic assumption set A1 = (H1 100, H2 5 , H4 new): we only consider the 100 first documents from each ranking, then retain documents present in 5 or more rankings and update ranks of successful documents. For positional methods, we place missing documents at the queue of the ranking (H3 yes) whereas for our method as well as for MC4, we retained hypothesis H3 no. The three following rows of Table 10 report performances when changing one element from the basic assumption set: the second row corresponds to the assumption set A2 = (H1 1000, H2 5 , H4 new), i.e. changing the number of retained documents from 100 to 1000. The third row corresponds to the assumption set A3 = (H1 100, H2 all, H4 new), i.e. considering the documents present in at least one ranking. The fourth row corresponds to the assumption set A4 = (H1 100, H2 5 , H4 init), i.e. keeping the original ranks of successful documents. The fifth row of Table 10, labeled A5, gives performance when all the 225 queries of the Web track of TREC-2004 are considered. Obviously, performance level cannot be compared with previous lines since the additional queries are different from the TD queries and correspond to other tasks (Home Page and Named Page tasks [10]) of TREC-2004 Web track. This set of runs aims to show whether relative performance of the various methods is task-dependent. The last row of Table 10, labeled A6, reports performance of the various methods considering the TD task of TREC2002 instead of TREC-2004: we fused the results of input rankings of the 10 best official runs for each of the 50 TD queries [9] considering the set of assumptions A1 of the first row. This aims to show whether relative performance of the various methods changes from year to year. Values between brackets of Table 10 are variations of performance of each rank aggregation method w.r.t. performance of the outranking approach. Table 10: Performance (MAP) of different rank aggregation methods under 3 different test collections mcm combSUM combMNZ markov A1 18.79% 17.54% (-6.65%*) 17.08% (-9.10%*) 18.63% (-0.85%) A2 21.36% 19.18% (-10.21%*) 18.61% (-12.87%*) 21.33% (-0.14%) A3 21.92% 21.38% (-2.46%) 20.88% (-4.74%) 19.35% (-11.72%*) A4 18.64% 17.58% (-5.69%*) 17.18% (-7.83%*) 18.63% (-0.05%) A5 55.39% 52.16% (-5.83%*) 49.70% (-10.27%*) 53.30% (-3.77%) A6 16.95% 15.65% (-7.67%*) 14.57% (-14.04%*) 16.39% (-3.30%) From the analysis of table 10 the following can be established: • for all the runs, considering all the documents in each input ranking (A2) significantly improves performance (MAP increases by 11.62% on average). This is predictable since some initially unreported relevant documents would receive better positions in the consensus ranking. • for all the runs, considering documents even those present in only one input ranking (A3) significantly improves performance. For mcm, combSUM and combMNZ, performance improvement is more important (MAP increases by 20.27% on average) than for the markov run (MAP increases by 3.86%). • preserving the initial positions of documents (A4) or recomputing them (A1) does not have a noticeable influence on performance for both positional and majoritarian methods. • considering all the queries of the Web track of TREC2004 (A5) as well as the TD queries of the Web track of TREC-2002 (A6) does not alter the relative performance of the different data fusion methods. • considering the TD queries of the Web track of TREC2002, performances of all the data fusion methods are lower than that of the best performing input ranking for which the MAP value equals 18.58%. This is because most of the fused input rankings have very low performances compared to the best one, which brings more noise to the consensus ranking. • performances of the data fusion methods mcm and markov are significantly better than that of the best input ranking uogWebCAU150. This remains true for runs combSUM and combMNZ only under assumptions H1 all or H2 all. This shows that majoritarian methods are less sensitive to assumptions than positional methods. • outranking approach always performs significantly better than positional methods combSUM and combMNZ. It has also better performances than the Markov chain method, especially under assumption H2 all where difference of performances becomes significant. 6. CONCLUSIONS In this paper, we address the rank aggregation problem where different, but not disjoint, lists of documents are to be fused. We noticed that the input rankings can hide ties, so they should not be considered as complete orders. Only robust information should be used from each input ranking. Current rank aggregation methods, and especially positional methods (e.g. combSUM [15]), are not initially designed to work with such rankings. They should be adapted by considering specific working assumptions. We propose a new outranking method for rank aggregation which is well adapted to the IR context. Indeed, it ranks two documents w.r.t. the intensity of their positions difference in each input ranking and also considering the number of the input rankings that are concordant and discordant in favor of a specific document. There is also no need to make specific assumptions on the positions of the missing documents. This is an important feature since the absence of a document from a ranking should not be necessarily interpreted negatively. Experimental results show that the outranking method significantly out-performs popular classical positional data fusion methods like combSUM and combMNZ strategies. It also out-performs a good performing majoritarian methods which is the Markov chain method. These results are tested against different test collections and queries. From the experiments, we can also conclude that in order to improve the performances, we should fuse result lists of well performing IR models, and that majoritarian data fusion methods perform better than positional methods. The proposed method can have a real impact on Web metasearch performances since only ranks are available from most primary search engines, whereas most of the current approaches need scores to merge result lists into one single list. Further work involves investigating whether the outranking approach performs well in various other contexts, e.g. using the document scores or some combination of document ranks and scores. Acknowledgments The authors would like to thank Jacques Savoy for his valuable comments on a preliminary version of this paper. 7. REFERENCES [1] A. Aronson, D. Demner-Fushman, S. Humphrey, J. Lin, H. Liu, P. Ruch, M. Ruiz, L. Smith, L. Tanabe, and W. Wilbur. Fusion of knowledge-intensive and statistical approaches for retrieving and annotating textual genomics documents. In Proceedings TREC2005. NIST Publication, 2005. [2] R. A. Baeza-Yates and B. A. Ribeiro-Neto. Modern Information Retrieval. ACM Press , 1999. [3] B. T. Bartell, G. W. Cottrell, and R. K. Belew. Automatic combination of multiple ranked retrieval systems. In Proceedings ACM-SIGIR94, pages 173-181. Springer-Verlag, 1994. [4] N. J. Belkin, P. Kantor, E. A. Fox, and J. A. Shaw. Combining evidence of multiple query representations for information retrieval. IPM, 31(3):431-448, 1995. [5] J. Borda. M´emoire sur les ´elections au scrutin. Histoire de lAcad´emie des Sciences, 1781. [6] J. P. Callan, Z. Lu, and W. B. Croft. Searching distributed collections with inference networks. In Proceedings ACM-SIGIR95, pages 21-28, 1995. [7] M. Condorcet. Essai sur lapplication de lanalyse `a la probabilit´e des d´ecisions rendues `a la pluralit´e des voix. Imprimerie Royale, Paris, 1785. [8] W. D. Cook and M. Kress. Ordinal ranking with intensity of preference. Management Science, 31(1):26-32, 1985. [9] N. Craswell and D. Hawking. Overview of the TREC-2002 Web Track. In Proceedings TREC2002. NIST Publication, 2002. [10] N. Craswell and D. Hawking. Overview of the TREC-2004 Web Track. In Proceedings of TREC2004. NIST Publication, 2004. [11] C. Dwork, S. R. Kumar, M. Naor, and D. Sivakumar. Rank aggregation methods for the Web. In Proceedings WWW2001, pages 613-622, 2001. [12] R. Fagin. Combining fuzzy information from multiple systems. JCSS, 58(1):83-99, 1999. [13] R. Fagin, R. Kumar, M. Mahdian, D. Sivakumar, and E. Vee. Comparing and aggregating rankings with ties. In PODS, pages 47-58, 2004. [14] R. Fagin, R. Kumar, and D. Sivakumar. Comparing top k lists. SIAM J. on Discrete Mathematics, 17(1):134-160, 2003. [15] E. A. Fox and J. A. Shaw. Combination of multiple searches. In Proceedings of TREC3. NIST Publication, 1994. [16] J. Katzer, M. McGill, J. Tessier, W. Frakes, and P. DasGupta. A study of the overlap among document representations. Information Technology: Research and Development, 1(4):261-274, 1982. [17] L. S. Larkey, M. E. Connell, and J. Callan. Collection selection and results merging with topically organized U.S. patents and TREC data. In Proceedings ACM-CIKM2000, pages 282-289. ACM Press, 2000. [18] A. Le Calv´e and J. Savoy. Database merging strategy based on logistic regression. IPM, 36(3):341-359, 2000. [19] J. H. Lee. Analyses of multiple evidence combination. In Proceedings ACM-SIGIR97, pages 267-276, 1997. [20] D. Lillis, F. Toolan, R. Collier, and J. Dunnion. Probfuse: a probabilistic approach to data fusion. In Proceedings ACM-SIGIR2006, pages 139-146. ACM Press, 2006. [21] J. I. Marden. Analyzing and Modeling Rank Data. Number 64 in Monographs on Statistics and Applied Probability. Chapman & Hall, 1995. [22] M. Montague and J. A. Aslam. Metasearch consistency. In Proceedings ACM-SIGIR2001, pages 386-387. ACM Press, 2001. [23] D. M. Pennock and E. Horvitz. Analysis of the axiomatic foundations of collaborative filtering. In Workshop on AI for Electronic Commerce at the 16th National Conference on Artificial Intelligence, 1999. [24] M. E. Renda and U. Straccia. Web metasearch: rank vs. score based rank aggregation methods. In Proceedings ACM-SAC2003, pages 841-846. ACM Press, 2003. [25] W. H. Riker. Liberalism against populism. Waveland Press, 1982. [26] B. Roy. The outranking approach and the foundations of ELECTRE methods. Theory and Decision, 31:49-73, 1991. [27] B. Roy and J. Hugonnard. Ranking of suburban line extension projects on the Paris metro system by a multicriteria method. Transportation Research, 16A(4):301-312, 1982. [28] L. Si and J. Callan. Using sampled data and regression to merge search engine results. In Proceedings ACM-SIGIR2002, pages 19-26. ACM Press, 2002. [29] M. Truchon. An extension of the Condorcet criterion and Kemeny orders. Cahier 9813, Centre de Recherche en Economie et Finance Appliqu´ees, Oct. 1998. [30] H. Turtle and W. B. Croft. Inference networks for document retrieval. In Proceedings of ACM-SIGIR90, pages 1-24. ACM Press, 1990. [31] C. C. Vogt and G. W. Cottrell. Fusion via a linear combination of scores. Information Retrieval, 1(3):151-173, 1999.",
    "original_translation": "Un enfoque de supervisión para la agregación de rango en la recuperación de la información Mohamed Farah Lamsade, Paris Dauphine University Place du Mal de Lattre de Tassigny 75775 Paris Cedex 16, France Farah@lamsade.dauphine.fr Daniel Vanderpooten Lamsade, Paris Dauphine University Place du Mal de Lattre de Tassigny75775 Paris Cedex 16, Francia vdp@lamsade.dauphine.fr abstracto La investigación en recuperación de información generalmente muestra una mejora del rendimiento cuando muchas fuentes de evidencia se combinan para producir una clasificación de documentos (por ejemplo, textos, imágenes, sonidos, etc.). En este documento, nos centramos en el problema de agregación de rango, también llamado problema de fusión de datos, donde las clasificaciones de documentos, buscadas en la misma colección y proporcionadas por múltiples métodos, se combinan para producir una nueva clasificación. En este contexto, proponemos un método de agregación de rango dentro de un marco de criterios múltiples utilizando mecanismos de agregación basados en reglas de decisión que identifican razones positivas y negativas para juzgar si un documento debería obtener un mejor rango que otro. Mostramos que el método propuesto trata bien con las características distintivas de recuperación de información. Se informan resultados experimentales que muestran que el método sugerido funciona mejor que los conocidos operadores de calcetines y combustibles. Categorías y descriptores de asignaturas: H.3.3 [Sistemas de información]: Búsqueda y recuperación de información - Modelos de recuperación. Términos generales: algoritmos, medición, experimentación, rendimiento, teoría.1. Introducción Una amplia gama de enfoques de recuperación de información (IR) actuales se basa en varios modelos de búsqueda (booleano, espacio vectorial, probabilístico, lenguaje, etc. [2]) para recuperar documentos relevantes en respuesta a una solicitud de usuario. Las listas de resultados producidas por estos enfoques dependen de la definición exacta del concepto de relevancia. Los enfoques de agregación de rango, también llamados enfoques de fusión de datos, consisten en combinar estas listas de resultados para producir una clasificación nueva y con suerte mejor. Tales enfoques dan lugar a motores de Metasearch en el contexto web. Consideramos, en los siguientes, casos en los que solo hay rangos disponibles y no se proporcionan otra información adicional, como los puntajes de relevancia. Esto corresponde de hecho a la realidad, donde solo la información ordinal está disponible. Data Fusion también es relevante en otros contextos, como cuando el usuario escribe varias consultas de su necesidad de información (por ejemplo, una consulta booleana y una consulta de lenguaje natural) [4], o cuando hay muchos subrogados de documentos disponibles [16]. Varios estudios argumentaron que la agregación de rango tiene el potencial de combinar efectivamente todas las diversas fuentes de evidencia consideradas en varios métodos de entrada. Por ejemplo, los experimentos llevados a cabo en [16], [30], [4] y [19] mostraron que los documentos que aparecen en las listas de la mayoría de los métodos de entrada tienen más probabilidades de ser relevantes. Además, Lee [19] y Vogt y Cottrell [31] encontraron que varios enfoques de recuperación a menudo devuelven documentos irrelevantes muy diferentes, pero muchos de los mismos documentos relevantes. Bartell et al.[3] también encontró que los métodos de agregación de rango mejoran los rendimientos W.R.T.Los de los métodos de entrada, incluso cuando algunos de ellos tienen actuaciones individuales débiles. Estos métodos también tienden a suavizar los sesgos de los métodos de entrada según Montague y Aslam [22]. Recientemente se ha demostrado que Data Fusion mejore el rendimiento tanto para las tareas de recuperación y categorización AD-HOC dentro de la pista de genómica TREC en 2005 [1]. El problema de la agregación de rango se abordó en varios campos como I) en la teoría de la elección social que estudia algoritmos de votación que especifican a los ganadores de elecciones o ganadores de competiciones en torneos [29], ii) en estadísticas al estudiar la correlación entre las clasificaciones, iii) en distribuidos distribuidos.Las bases de datos cuando se deben combinar los resultados de diferentes bases de datos [12] y IV) en el filtrado colaborativo [23]. La mayoría de los métodos de agregación de rango actuales consideran cada clasificación de entrada como una permutación sobre el mismo conjunto de elementos. También dan una interpretación rígida a la clasificación exacta de los elementos. Ambos supuestos no son más bien válidos en el contexto IR, como se mostrará en las siguientes secciones. El resto del documento se organiza de la siguiente manera. Primero revisamos los métodos actuales de agregación de rango en la Sección 2. Luego describimos las especificidades del problema de fusión de datos en el contexto IR (Sección 3). En la Sección 4, presentamos un nuevo método de agregación que se demuestra que mejor se ajusta al contexto IR. Los resultados experimentales se presentan en la Sección 5 y se proporcionan conclusiones en una sección final.2. Trabajo relacionado según lo señalado por Riker [25], podemos distinguir dos familias de métodos de agregación de rango: métodos posicionales que asignan puntajes a los elementos que se clasificarán de acuerdo con las filas que reciben y los métodos mayores que se basan en las comparaciones de elementos por paresclasificado. Estas dos familias de métodos encuentran sus raíces en las obras pioneras de Borda [5] y Condorcet [7], respectivamente, en la literatura de elección social.2.1 Preliminares Primero presentamos algunas anotaciones básicas para presentar los métodos de agregación de rango de manera uniforme. Sea d = {d1, d2 ,..., dnd} ser un conjunto de documentos ND. Una lista o una clasificación J es un orden definido en DJ ⊆ D (J = 1, ..., N). Por lo tanto, di j di significa Di se clasifica mejor que Di en j. Cuando DJ = D, J se dice que es una lista completa. De lo contrario, es una lista parcial. Si Di pertenece a DJ, RJ I denota el rango o posición de DI en j. Suponemos que la mejor respuesta (documento) se le asigna la posición 1 y la peor se le asigna la posición | DJ |. Sea D el conjunto de todas las permutaciones en D o todos los subconjuntos de D. Un perfil es una n-tupla de clasificaciones pr = (1, 2, ...., N). Restringir las relaciones públicas a las clasificaciones que contienen documento DI define PRI. También llamamos al número de clasificaciones que contienen documentos DI los éxitos de rango de DI [19]. El problema de agregación de rango o fusión de datos consiste en encontrar una función o mecanismo de clasificación ψ (también llamado función de bienestar social en la terminología de la teoría de elección social) definida por: ψ: n d → d pr = (1, 2, .., ...n) → σ = ψ (pr) donde σ se llama clasificación de consenso.2.2 Métodos posicionales 2.2.1 Borda Cuenta este método [5] primero asigna una puntuación n j = 1 rj I a cada documento di. Los documentos se clasifican luego mediante el aumento del orden de este puntaje, rompiendo lazos, si los hay, arbitrariamente.2.2.2 Métodos de combinación lineal Esta familia de métodos básicamente combina puntajes de documentos. Cuando se usa para el problema de agregación de rango, se supone que los rangos son puntajes o actuaciones para combinar utilizando operadores de agregación, como la suma ponderada o alguna variación [3, 31, 17, 28]. Por ejemplo, Callan et al.[6] usó el modelo de redes de inferencia [30] para combinar las clasificaciones. Fox y Shaw [15] propusieron varias estrategias combinadas que son Combsum, Combmin, Combmax, Combanz y Combmnz. Los primeros tres operadores corresponden a los operadores SUM, MIN y MAX, respectivamente. Combanz y Combmnz dividen y multiplican respectivamente el puntaje de cocsil por los éxitos de rango. Se muestra en [19] que los operadores CombmNz y Combmnz funcionan mejor que los demás. Los motores de MetaSearch como SavvySearch y Metacrawler usan la estrategia de combinación para fusionar las clasificaciones.2.2.3 Agregación óptima del fútbol En este método, una clasificación de consenso minimiza la distancia del fallrule de Spearman desde la clasificación de entrada [21]. Formalmente, dadas dos listas completas j y j, esta distancia viene dada por f (j, j) = nd i = 1 | rj i - rj i |. Se extiende a varias listas de la siguiente manera. Dado un perfil PR y una clasificación de consenso σ, la distancia del fútbol de Spearman de σ a PR viene dada por F (σ, PR) = n j = 1 f (σ, j). Cook y Kress [8] propusieron un método similar que consiste en optimizar la distancia d (j, j) = 1 2d i, i = 1 | rj i, i - rj i, i |, donde rj i, i = rji −rj i. Esta formulación tiene la ventaja de que considera la intensidad de las preferencias.2.2.4 Métodos probabilísticos Este tipo de métodos supone que el rendimiento de los métodos de entrada en una serie de consultas de entrenamiento es indicativo de su rendimiento futuro. Durante el proceso de capacitación, se calculan las probabilidades de relevancia. Para consultas posteriores, los documentos se clasifican en función de estas probabilidades. Por ejemplo, en [20], cada clasificación de entrada J se divide en una serie de segmentos, y la probabilidad condicional de relevancia (r) de cada documento DI dependiendo del segmento K en el que ocurre, se calcula, es decir, Prob (r |Di, K, J). Para consultas posteriores, la puntuación de cada documento DI viene dada por n j = 1 prob (r | di, k, j) k. Le Calve y Savoy [18] sugieren usar un enfoque de regresión logística para combinar puntajes. Se necesitan datos de capacitación para inferir los parámetros del modelo.2.3 Métodos principales 2.3.1 Procedimiento de Condorcet La regla de Condorcet original [7] Especifica que un ganador de la elección es cualquier elemento que supera o vincule con cualquier otro elemento en un concurso por pares. Formalmente, deje c (diσdi) = {j∈ Pr: di j di} la coalición de clasificaciones que son concordantes con el establecimiento de Diσdi, es decir, con la proposición DI debe clasificarse mejor que DI en la clasificación final σ.DI latidos o lazos con di iff | c (diσdi) |≥ | c (di σdi) |. La aplicación repetitiva del algoritmo de Condorcet puede producir una clasificación de artículos de manera natural: seleccione el ganador del Condorcet, elimínelo de las listas y repita los dos pasos anteriores hasta que no haya más documentos para clasificar. Dado que no siempre hay ganadores de Condorcet, se han desarrollado variaciones del procedimiento de Condorcet dentro de la teoría de la ayuda de decisiones de criterios múltiples, con métodos como Electre [26].2.3.2 Agregación óptima de Kemeny Como en la Sección 2.2.3, una clasificación de consenso minimiza una distancia geométrica desde la clasificación de entrada, donde se usa la distancia de Kendall Tau en lugar de la distancia del torthule de Spearman. Formalmente, dadas dos listas completas J y J, la distancia de Kendall Tau está dada por K (j, j) = | {(di, di): i <i, rj i <rj i, rj i> rj i} |,es decir, el número de desacuerdos por pares entre las dos listas. Es fácil demostrar que la clasificación de consenso corresponde a la mediana geométrica de las clasificaciones de entrada y que el problema de agregación óptima de Kemeny corresponde al problema mínimo del conjunto de borde de retroalimentación.2.3.3 Métodos de la cadena de Markov Las cadenas de Markov (MCS) han sido utilizadas por DWork et al.[11] Como un método natural para obtener una clasificación de consenso donde los estados corresponden a los documentos que se clasificarán y las probabilidades de transición varían según la interpretación del evento de transición. En la misma referencia, los autores propusieron cuatro MC específicos y pruebas experimentales habían demostrado que el siguiente MC es el de mejor rendimiento (ver también [24]): • MC4: Mover del estado actual DI al siguiente estado DI eligiendo primeroUn documento de uniformemente de D. Si para la mayoría de las clasificaciones, tenemos RJ I ≤ Rj I, luego nos mudamos a Di, de lo contrario, permanezca en DI. La clasificación de consenso corresponde a la distribución estacionaria de MC4.3. Especificidades del problema de agregación de rango en el contexto IR 3.1 Importancia limitada de las clasificaciones Las posiciones exactas de los documentos en una clasificación de entrada tienen una importancia limitada y no deben enfatizarse demasiado. Por ejemplo, con tres documentos relevantes en las primeras tres posiciones, cualquier perturbación de estos tres elementos tendrá el mismo valor. De hecho, en el contexto IR, el orden completo proporcionado por un método de entrada puede ocultar lazos. En este caso, llamamos a tales rankings semi órdenes. Esto se describió en [13] como el problema de la agregación con lazos. Por lo tanto, es importante construir la clasificación de consenso en función de la información sólida: • Los documentos con posiciones cercanas en J tienen más probabilidades de tener un interés o relevancia similar. Por lo tanto, una ligera perturbación de la clasificación inicial no tiene sentido.• Suponiendo que el documento DI esté mejor clasificado que el documento DI en una clasificación J, es más probable que sea definitivamente más relevante que DI en J cuando aumenta el número de posiciones intermedias entre DI y DI.3.2 Listas parciales en aplicaciones del mundo real, como motores de metasearch, las clasificaciones proporcionadas por los métodos de entrada a menudo son listas parciales. Esto se describió en [14] como el problema de tener que fusionar los resultados de Top-K de varias listas de entrada. Por ejemplo, en los experimentos realizados por DWork et al.[11], los autores encontraron que entre los 100 mejores documentos de 7 motores de búsqueda de entrada, el 67% de los documentos estaban presentes en un solo motor de búsqueda, mientras que menos de dos documentos estaban presentes en todos los motores de búsqueda. La agregación de rango de listas parciales plantea cuatro dificultades principales que declaramos en adelante, proponiendo para cada uno de ellos varios supuestos de trabajo: 1. Las listas parciales pueden tener varias longitudes, lo que puede favorecer listas largas. Por lo tanto, consideramos las siguientes dos hipótesis de trabajo: H1 K: Solo consideramos los mejores documentos K mejores de cada clasificación de entrada. H1 Todos: Consideramos todos los documentos de cada clasificación de entrada.2. Dado que hay diferentes documentos en la clasificación de entrada, debemos decidir qué documentos deben mantenerse en la clasificación de consenso. Por lo tanto, se consideran dos hipótesis de trabajo: H2 K: solo consideramos documentos que están presentes en al menos K clasificaciones de entrada (k> 1). H2 Todos: Consideramos todos los documentos que se clasifican en al menos una clasificación de entrada. En adelante, llamamos documentos que se conservarán en la clasificación de consenso, los documentos candidatos y los documentos que se excluirán de la clasificación de consenso, excluyeron los documentos. También llamamos a un documento candidato que falta en una o más clasificaciones, un documento que falta.3. Algunos documentos candidatos faltan documentos en algunas clasificaciones de entrada. Las razones principales para un documento faltante son que no fue indexado o fue indexado, pero se consideró irrelevante;Por lo general, esta información no está disponible. Consideramos las siguientes dos hipótesis de trabajo: H3 Sí: A cada documento faltante en cada J se le asigna una posición. H3 No: No se asume, es decir, cada documento faltante no se considera mejor ni peor que cualquier otro documento.4. Cuando la suposición H2 K se mantiene, cada clasificación de entrada puede contener documentos que no se considerarán en la clasificación de consenso. Con respecto a los puestos de los documentos candidatos, podemos considerar las siguientes hipótesis de trabajo: H4 Init: Las posiciones iniciales de los documentos candidatos se mantienen en cada clasificación de entrada. H4 Nuevo: los documentos candidatos reciben nuevos puestos en cada clasificación de entrada, después de descartar los excluidos. En el contexto IR, los métodos de agregación de rango deben decidir más o menos explícitamente qué supuestos retener a W.R.T.Las dificultades mencionadas anteriormente.4. Enfoque de extaniación para la agregación de rango 4.1 Métodos posicionales de presentación Considere implícitamente que las posiciones de los documentos en las clasificaciones de entrada son puntajes dando así un significado cardinal a una información ordinal. Esto constituye una fuerte suposición que es cuestionable, especialmente cuando las clasificaciones de entrada tienen diferentes longitudes. Además, para los métodos posicionales, las suposiciones H3 y H4, que a menudo son arbitrarias, tienen un fuerte impacto en los resultados. Por ejemplo, consideremos una clasificación de entrada de 500 documentos de 1000 documentos candidatos. Ya sea que asignemos a cada uno de los documentos faltantes, la posición 1, 501, 750 o 1000 correspondiera a variaciones de H3 sí, dará lugar a resultados muy contrastados, especialmente con respecto a la parte superior de la clasificación de consenso. Los métodos mayores no sufren los inconvenientes mencionados anteriormente de los métodos posicionales, ya que construyen clasificaciones de consenso que explotan solo la información ordinal contenida en las clasificaciones de entrada. Sin embargo, suponen que tales clasificaciones son órdenes completas, ignorando que pueden ocultar lazos. Por lo tanto, los métodos principales basan la clasificación de consenso en información discriminante ilusoria en lugar de información menos discriminante pero más robusta. Al tratar de superar los límites de los métodos de agregación de rango actuales, encontramos que los enfoques de supervanación, que inicialmente se usaron para múltiples problemas de agregación de criterios [26], también pueden usarse para el propósito de agregación de rango, donde cada clasificación juega el papel de un criterio. Por lo tanto, para decidir si un documento DI debe clasificarse mejor que DI en la clasificación de consenso σ, las dos condiciones siguientes deben cumplirse: • Una condición de concordancia que asegura que la mayoría de las clasificaciones de entrada sean concordantes con Diσdi (Principio mayoritario).• Una condición de discordancia que garantiza que ninguna de las clasificaciones de entrada discordantes refuta fuertemente a DσD (respeto del principio de las minorías). Formalmente, la coalición de concordancia con diσdi es csp (diσdi) = {j∈ Pr: rj i ≤ rj i -sp} donde sp es un umbral de preferencia, que es la variación de las posiciones de documento, ya sea absoluta o relativa a la longitud de clasificación- que dibuja los límites entre una indiferencia y una situación de preferencia entre documentos. La coalición de discordancia con diσdi es dsv (diσdi) = {j∈ Pr: rj I ≥ rj i + sv} donde sv es un umbral de veto que es la variación de las posiciones de documento, ya sea absoluta o relativa a la longitud de clasificación, que, queDibuja los límites entre una oposición débil y fuerte a Diσdi. Dependiendo de la definición exacta de las coaliciones de concordancia y discordancia anteriores que conducen a la definición de algunas reglas de decisión, se pueden definir varias relaciones superiores. Pueden ser más o menos exigentes dependiendo de i) los valores de los umbrales SP y SV, ii) la importancia o el tamaño mínimo de Cmin requerido para la coalición de concordancia, y iii) la importancia o el tamaño máximo dmax de la coalición de discordancia. Por lo tanto, una relación genérica de extaniación se puede definir de la siguiente manera: dis (sp, sv, cmin, dmax) di ⇔ | csp (diσdi) |≥ cmin y | dsv (diσdi) |≤ dmax Esta expresión define una familia de relaciones de extaniación anidada desde S (SP, SV, Cmin, DMAX) ⊆ S (SP, SV, Cmin, DMAX) cuando Cmin ≥ Cmin y/o DMAX ≤ DMAX y/o SP ≥ SP y SP y/o SV ≤ SV. Esta expresión también generaliza la regla mayoritaria que corresponde a la relación particular s (0, ∞, n 2, n). También satisface propiedades importantes de los métodos de agregación de rango, llamados neutralidad, optimidad de pareto, propiedad de condorcet y propiedad de condorcet extendida, en la literatura de elección social [29]. Las relaciones de extaniación no son necesariamente transitivas y no se corresponden necesariamente a las clasificaciones, ya que pueden existir ciclos dirigidos. Por lo tanto, necesitamos procedimientos específicos para obtener una clasificación de consenso. Proponemos el siguiente procedimiento que encuentra sus raíces en [27]. Consiste en dividir el conjunto de documentos en clases clasificadas. Cada clase CH contiene documentos con la misma relevancia y resultados de la aplicación de todas las relaciones (si es posible) al conjunto de documentos restantes después de calcular las clases anteriores. Los documentos dentro de la misma clase de equivalencia se clasifican arbitrariamente. Formalmente, sea el conjunto de documentos candidatos para una consulta, • S1, S2 ,...Sea una familia de relaciones atenuadas anidadas, • fk (di, e) = | {di ∈ E: disco di} |ser el número de documentos en e (e ⊆ r) que podrían considerarse peor que di de acuerdo con la relación sk, • fk (di, e) = | {di ∈ E: di sk di} |ser el número de documentos en E que podrían considerarse mejor que Di de acuerdo con SK, • SK (DI, E) = FK (DI, E) - FK (DI, E) ser la calificación de DI en E según SK. Cada clase CH resulta de un proceso de destilación. Corresponde al último destilado de una serie de conjuntos E0 ⊇ E1 ⊇...donde e0 = r \\ (c1 ∪ ... ∪ ch - 1) y ek es un subconjunto reducido de ek - 1 resultante de la aplicación del siguiente procedimiento: 1. Calcule para cada di ∈ Ek - 1 su calificación de acuerdo con SK, es decir, SK (DI, EK - 1), 2. Define smax = maxdi∈Ek - 1 {sk (di, ek - 1)}, entonces 3. Ek = {di ∈ Ek - 1: SK (DI, EK - 1) = Smax} Cuando se usa una relación superpuesta, el proceso de destilación se detiene después de la primera aplicación del procedimiento anterior, es decir, CH corresponde al destilado E1. Cuando se utilizan diferentes relaciones de extaniación, el proceso de destilación se detiene cuando se han utilizado todas las relaciones de superación predanionantes predefinidas o cuando | Ek |= 1. 4.2 Ejemplo ilustrativo Esta sección ilustra los conceptos y procedimientos de la Sección 4.1. Consideremos un conjunto de documentos candidatos r = {d1, d2, d3, d4, d5}. La siguiente tabla proporciona un perfil PR de diferentes clasificaciones de los documentos de R: Pr = (1, 2, 3, 4). Tabla 1: Rankings of Documents RJ I 1 2 3 4 D1 1 3 1 5 D2 2 1 3 3 D3 3 2 2 1 D4 4 4 4 5 2 D5 5 5 4 4 Supongamos que la preferencia y los umbrales de veto están establecidos en valores1 y 4 respectivamente, y que los umbrales de concordancia y discordancia se establecen en los valores 2 y 1 respectivamente. Las siguientes tablas dan la concordancia, la discordancia y las matrices superpolentes. Cada entrada CSP (DI, DI) (DSV (DI, DI)) en la matriz de concordancia (discordancia) da el número de clasificaciones que son concordantes (discordantes) con diσdi, es decir, CSP (di, di) = | csp (diσdi)|y dsv (di, di) = | dsv (diσdi) |. Tabla 2: Cálculo de la relación superpuesta D1 D2 D3 D4 D5 D1 - 2 2 3 3 3 D2 2 - 2 3 4 D3 2 2 - 4 4 D4 1 1 0 - 3 D5 1 0 0 1 Matriz de concordancia D1 D2 D3 D4 D5 D1 -0 1 0 0 D2 0 - 0 0 0 D3 0 0 - 0 0 0 D4 1 0 0 - 0 D5 1 1 0 0 Discordance Matriz D1 D2 D3 D4 D5 D1 - 1 1 1 1 D2 1 - 1 1 1 D3 1 - 1 1 1 11 d4 0 0 0 - 1 d5 0 0 0 0 0outranking Matrix (S1) Por ejemplo, la coalición de concordancia para la afirmación d1σd4 es C1 (d1σd4) = {1, 2, 3} y la coalición de discordancia para la misma afirmación es d4 ((d1σd4) = ∅. Por lo tanto, C1 (D1, D4) = 3, D4 (D1, D4) = 0 y D1S1 D4 se mantiene. Observe que FK (DI, R) (FK (DI, R)) se da sumando los valores de la fila (columna) de la matriz superpuesta. La clasificación de consenso se obtiene de la siguiente manera: para obtener la primera clase C1, calificamos las calificaciones de todos los documentos de E0 = R con respecto a S1. Son respectivamente 2, 2, 2, -2 y -4. Por lo tanto, Smax es igual a 2 y C1 = E1 = {D1, D2, D3}. Observe que, si hubiéramos usado una segunda relación superpuesta S2 (⊇ S1), estos tres documentos podrían haber sido posiblemente discriminados. En esta etapa, eliminamos los documentos de C1 de la matriz superpuesta y calculamos la siguiente clase C2: Calificamos las nuevas calificaciones de los documentos de e0 = r \\ c1 = {d4, d5}. Son respectivamente 1 y -1. Entonces c3 = e1 = {d4}. El último documento D5 es el único documento de la última clase C3. Por lo tanto, la clasificación de consenso es {d1, d2, d3} → {d4} → {d5}.5. Experimentos y resultados 5.1 Configuración de prueba Para facilitar la investigación empírica de la metodología propuesta, desarrollamos un prototipo de motor MetaSearch que implementa una versión de nuestro enfoque supervisado para la agregación de rango. En este documento, aplicamos nuestro enfoque a la tarea de destilación del tema (TD) de la pista web de TREC-2004 [10]. En esta tarea, hay 75 temas en los que solo se da una breve descripción de cada uno. Para cada consulta, conservamos las clasificaciones de las 10 mejores carreras de la tarea TD que proporcionan los equipos participantes de TREC-2004. Las actuaciones de estas ejecuciones se informan en la Tabla 3. Tabla 3: Actuaciones de las 10 mejores ejecuciones de la tarea TD de TREC-2004 Run ID MAP P@10 S@1 S@5 S@10 Uogwebcau150 17.9% 24.9% 50.7% 77.3% 89.3% Msramixed1 17.8% 25.1% 38.7%72.0% 88.0% MSRC04C12 16.5% 23.1% 38.7% 74.7% 80.0% HUMW04RDPL 16.3% 23.1% 37.3% 78.7% 90.7% thuirmix042 14.7% 20.5% 21.3% 58.7% 74.7% UAMSt04MWb. 0% ICT04CIIS1AT 14.1% 20.8% 33.3% 64.0% 78.7% SJTUINCMIX5 12.9% 18.9% 29.3% 57.3% 72.0% MU04WEB1 11.5% 19.9% 33.3% 64.0% 76.0% Meijihilw3 11.5% 15.3% 30.7% 54.7% 64.0% promedio de 14.7% 2% 2%.% 78.94% Para cada consulta, cada ejecución proporciona una clasificación de aproximadamente 1000 documentos. El número de documentos recuperados por todas estas ejecuciones varía de 543 a 5769. Su número promedio (mediano) es 3340 (3386). Vale la pena señalar que encontramos distribuciones similares de los documentos entre las clasificaciones como en [11]. Para la evaluación, utilizamos la herramienta estándar TREC EVALO que es utilizada por la comunidad TREC para calcular las medidas estándar de efectividad del sistema que son precisión promedio media (map) y éxito@n (s@n) para n = 1, 5 y 10. Nuestra efectividad del enfoque se compara con algunos resultados oficiales de alto rendimiento de TREC-2004, así como contra algunos algoritmos de agregación de rango estándar. En los experimentos, las pruebas de significación se basan principalmente en la estadística del estudio T que se calcula sobre la base de los valores del mapa de las ejecuciones comparadas. En las tablas de la siguiente sección, las diferencias estadísticamente significativas se marcan con un asterisco. Los valores entre los soportes de la primera columna de cada tabla, indiquen el valor del parámetro de la ejecución correspondiente.5.2 Resultados Llevamos a cabo varias series de ejecuciones para i) Estudiar variaciones de rendimiento del enfoque de supervisión al ajustar los parámetros y los supuestos de trabajo, ii) comparar las actuaciones del enfoque de supervisión frente a las estrategias de agregación de rango estándar, y iii) verificar si la agregación de rangofunciona mejor que las mejores clasificaciones de entrada. Establecimos nuestro MCM de ejecución básica con los siguientes parámetros. Consideramos que cada clasificación de entrada es un orden completo (SP = 0) y que una clasificación de entrada refuta fuertemente a Diσdi cuando la diferencia de ambas posiciones de documentos es lo suficientemente grande (SV = 75%). Los umbrales de preferencia y veto se calculan proporcionalmente al número de documentos retenidos en cada clasificación de entrada. En consecuencia, pueden variar de una clasificación a otra. Además, para aceptar la afirmación Diσdi, supusimos que la mayoría de las clasificaciones deben ser concordantes (cmin = 50%) y que cada clasificación de entrada puede imponer su veto (dmax = 0). Los umbrales de concordancia y discordancia se calculan para cada tupla (DI, DI) como el porcentaje de las clasificaciones de entrada de Pri ∩Pri. Por lo tanto, nuestra elección de los parámetros conduce a la definición de las relaciones superanitivas (0,75%, 50%, 0). Para probar el MCM de ejecución, habíamos elegido los siguientes supuestos. Conservamos los 100 mejores documentos principales de cada clasificación de entrada (H1 100), solo consideraron documentos que están presentes en al menos la mitad de las clasificaciones de entrada (H2 5) y asumimos H3 No y H4 nuevos. En estas condiciones, el número de documentos exitosos fue de aproximadamente 100 en promedio, y el tiempo de cálculo por consulta fue inferior a un segundo. Obviamente, modificar los supuestos de trabajo debería tener un impacto más profundo en las actuaciones que ajustar los parámetros de nuestro modelo. Esto fue validado por experimentos preliminares. Por lo tanto, comenzamos en adelante estudiando la variación de rendimiento cuando se consideran diferentes conjuntos de supuestos. Posteriormente, estudiamos el impacto de los parámetros de sintonización. Finalmente, comparamos nuestras actuaciones modelo W.R.T.Las clasificaciones de entrada, así como algunos algoritmos estándar de fusión de datos.5.2.1 Impacto de los supuestos de trabajo La Tabla 4 resume la variación de rendimiento del enfoque de supervisión bajo diferentes hipótesis de trabajo. En la Tabla 4: Impacto de los supuestos de trabajo Ejecutar el mapa de identificación S@1 S@5 S@10 MCM 18.47% 41.33% 81.33% 86.67% MCM22 (H3 Sí) 17.72% (-4.06%) 34.67% 81.33% 86.67% MCM23 (H4 init) 18.26% (-1.14%) 41.33% 81.33% 86.67% MCM24 (H1 All) 20.67% (+11.91%*) 38.66% 80.00% 86.66% MCM25 (H2 All) 21.68% (+17.38%*) 40.00%78.66% 89.33% Esta tabla, primero mostramos que Run MCM22, en el que los documentos faltantes se colocan en la misma última posición de cada clasificación de entrada, conduce a la caída de rendimiento W.R.T.ejecutar mcm. Además, S@1 se mueve de 41.33% a 34.67% (-16.11%). Esto muestra que varios documentos relevantes que inicialmente se colocaron en la primera posición de la clasificación de consenso en MCM, pierden esta primera posición pero permanecen clasificados en los 5 documentos principales ya que S@5 no cambió. También concluimos que los documentos que tienen buenas posiciones en algunas clasificaciones de entrada tienen más probabilidades de ser relevantes, a pesar de que están faltando en otras clasificaciones. En consecuencia, cuando faltan en algunas clasificaciones, asignar rangos peores a estos documentos es perjudicial para el rendimiento. Además, de la Tabla 4, encontramos que las actuaciones de las ejecuciones MCM y MCM23 son similares. Por lo tanto, el enfoque de supervisión no es sensible a mantener las posiciones iniciales de los documentos candidatos o recomputarlos descartando los excluidos. De la misma Tabla 4, el rendimiento del enfoque de supervisión aumenta significativamente para las ejecuciones MCM24 y MCM25. Por lo tanto, si consideramos todos los documentos que están presentes en la mitad de las clasificaciones (MCM24) o consideramos todos los documentos que se clasifican en las primeras 100 posiciones en una o más clasificaciones (MCM25), aumenta el rendimiento. Este resultado fue predecible ya que en ambos casos tenemos información más detallada sobre la importancia relativa de los documentos. Las tablas 5 y 6 confirman esta evidencia. La Tabla 5, donde los valores entre los soportes de la primera columna dan el número de documentos que se conservan de cada clasificación de entrada, muestra que seleccionar más documentos de cada clasificación de entrada conduce al aumento del rendimiento. Vale la pena mencionar que seleccionar más de 600 documentos de cada clasificación de entrada no mejora el rendimiento. Tabla 5: Impacto del número de documentos retenidos MAP de identificación S@1 S@5 S@10 MCM (100) 18.47% 41.33% 81.33% 86.67% MCM24-1 (200) 19.32% (+4.60%) 42.67% 78.67777% 88.00% MCM24-2 (400) 19.88% (+7.63%*) 37.33% 80.00% 88.00% MCM24-3 (600) 20.80% (+12.62%*) 40.00% 80.00% 88.00% MCM24-4 (800) 20.66% (+11.86%*) 40.00% 78.67% 86.67% MCM24 (1000) 20.67% (+11.91%*) 38.66% 80.00% 86.66% Tabla 6 Los informes corren correspondientes a variaciones de H2 K. Los valores entre los soportes son golpes de rango. Por ejemplo, en la ejecución MCM32, solo los documentos que están presentes en 3 o más clasificaciones de entrada, se consideraron exitosos. Esta tabla muestra que el rendimiento es significativamente mejor cuando se consideran documentos raros, mientras que disminuye significativamente cuando estos documentos se descartan. Por lo tanto, concluimos que muchos de los documentos relevantes son recuperados por un conjunto bastante pequeño de modelos IR. Tabla 6: Rendimiento considerando los golpes de rango de diferentes Run Map S@1 S@5 S@10 MCM25 (1) 21.68% (+17.38%*) 40.00% 78.67% 89.33% MCM32 (3) 18.98% (+2.76%) 38.67)% 80.00% 85.33% mcm (5) 18.47% 41.33% 81.33% 86.67% MCM33 (7) 15.83% (-14.29%*) 37.33% 78.67% 85.33% MCM34 (9) 10.96% (-40.66%*) 36.11% 66.6777777777777% 70.83% MCM35 (10) 7.42% (-59.83%*) 39.22% 62.75% 64.70% para ambas ejecuciones MCM24 y MCM25, el número de documentos exitosos fue de aproximadamente 1000 y, por lo tanto, el tiempo de cálculo perseguía aumentó y se convirtió en alrededor de 5 segundos.5.2.2 Impacto de la variación de los parámetros La Tabla 7 muestra la variación de rendimiento del enfoque de supervisión cuando se consideran diferentes umbrales de preferencia. Encontramos la mejora del rendimiento hasta los valores umbral de aproximadamente 5%, entonces hay una disminución en el rendimiento que se vuelve significativa para los valores umbral superiores al 10%. Además, S@1 mejora de 41.33% a 46.67% cuando el umbral de preferencia cambia de 0 a 5%. Por lo tanto, podemos concluir que las clasificaciones de entrada son semi órdenes en lugar de órdenes completas. La Tabla 8 muestra la evolución de las medidas de rendimiento W.R.T.El umbral de concordancia. Podemos concluir que para poner documento DI antes de DI en la clasificación de consenso, Tabla 7: Impacto de la variación del umbral de preferencia de 0 a 12.5% Run ID MAP S@1 S@5 S@10 mcm (0%)18.47% 41.33% 81.33% 86.67% MCM1 (1%) 18.57% (+0.54%) 41.33% 81.33% 86.67% MCM2 (2.5%) 18.63% (+0.87%) 42.67% 78.67% 86.67% MCM3 (5%) 18.69.69.69.69.69.69% (+1.19%) 46.67% 81.33% 86.67% mcm4 (7.5%) 18.24% (-1.25%) 46.67% 81.33% 86.67% mcm5 (10%) 17.93% (-2.92%) 40.00% 82.67% 86.67% mcm5b (12.5%) 17.51% (-5.20%*) 41.33% 80.00% 86.67% Al menos la mitad de las clasificaciones de entrada de PRI ∩ PRI deben ser concordantes. El rendimiento cae significativamente para valores muy bajos y muy altos del umbral de concordancia. De hecho, para tales valores, la condición de concordancia se cumple más bien siempre por demasiados pares de documentos o no se cumple en absoluto, respectivamente. Por lo tanto, la relación superalimentación se vuelve demasiado débil o demasiado fuerte respectivamente. Tabla 8: Impacto de la variación de Cmin Run ID MAP S@1 S@5 S@10 MCM11 (20%) 17.63% (-4.55%*) 41.33% 76.00% 85.33% MCM12 (40%) 18.37% (-0.54%) 42.67% 76.00% 86.67% MCM (50%) 18.47% 41.33% 81.33% 86.67% MCM13 (60%) 18.42% (-0.27%) 40.00% 78.67% 86.67% MCM14 (80%) 17.43% (-5.63%*) 40.00% 78.67% 86.67% MCM15 (100%) 16.12% (-12.72%*) 41.33% 70.67% 85.33% en los experimentos, variando el umbral de veto, así como el tesorial de discordancia, dentro de intervalos razonables no tiene un impacto significativo en elmedidas de desempeño. De hecho, las ejecuciones con diferentes umbrales de veto (sv ∈ [50%; 100%]) tuvieron rendimientos similares a pesar de que hay una ligera ventaja para las ejecuciones con valores de umbral altos, lo que significa que es mejor no permitir que las clasificaciones de entrada colocen susveto fácilmente. Además, la sintonización del umbral de discordancia se llevó a cabo para los valores del 50% y el 75% del umbral del veto. Para estas ejecuciones no obtuvimos ninguna variación de rendimiento notable, aunque para umbrales de baja discordancia (DMAX <20%), el rendimiento disminuyó ligeramente.5.2.3 Impacto de la variación del número de clasificaciones de entrada para estudiar la evolución del rendimiento Cuando se consideran diferentes conjuntos de clasificaciones de entrada, llevamos tres corridas más donde se consideran 2, 4 y 6 de los conjuntos de mejor rendimiento de las clasificaciones de entrada. Los resultados informados en la Tabla 9 son aparentemente contra-intuitivos y tampoco apoyan hallazgos anteriores con respecto a la investigación de agregación de rango [3]. Sin embargo, este resultado muestra que las clasificaciones de bajo rendimiento traen más ruido que información al establecimiento de la clasificación de consenso. Por lo tanto, cuando se consideran, el rendimiento disminuye. Tabla 9: Rendimiento considerando diferentes conjuntos de mejor rendimiento de clasificaciones de entrada Run Map de identificación S@1 S@5 S@10 McM (10) 18.47% 41.33% 81.33% 86.67% MCM27 (6) 18.60% (+0.70%) 41.33% 80.00% 85.33% MCM28 (4) 19.02% (+2.98%) 40.00% 86.67% 88.00% MCM29 (2) 18.33% (-0.76%) 44.00% 76.00% 88.00% 5.2.4 Comparación del rendimiento de diferentes métodos de agregación de rango enEste conjunto de ejecuciones, comparamos el enfoque de supervisión con algunos métodos de agregación de rango estándar que se demostró que tenían un rendimiento aceptable en estudios anteriores: consideramos dos métodos posicionales que son las estrategias Cocsum y Combmnz. También examinamos el rendimiento de un método mayoritario, que es el método de cadena de Markov (MC4). Para las comparaciones, consideramos una relación superpuesta específica S ∗ = S (5%, 50%, 50%, 30%) que resulta en buenos rendimientos generales al ajustar todos los parámetros. La primera fila de la Tabla 10 proporciona actuaciones de los métodos de agregación de rango W.R.T.Una suposición básica establecida A1 = (H1 100, H2 5, H4 NUEVO): Solo consideramos los 100 primeros documentos de cada clasificación, luego retenemos documentos presentes en 5 o más clasificaciones y actualizamos rangos de documentos exitosos. Para los métodos posicionales, colocamos documentos faltantes en la cola de la clasificación (H3 sí) mientras que para nuestro método, así como para MC4, conservamos la hipótesis H3 no. Las tres filas siguientes de la Tabla 10 informan que se cambian un elemento del conjunto de suposición básica: la segunda fila corresponde al conjunto de suposición A2 = (H1 1000, H2 5, H4 nuevo), es decir, cambiar el número de documentos retenidos de 100 a1000. La tercera fila corresponde al conjunto de suposición A3 = (H1 100, H2 All, H4 NUEVO), es decir, considerando los documentos presentes en al menos una clasificación. La cuarta fila corresponde al conjunto de suposición A4 = (H1 100, H2 5, H4 Init), es decir, manteniendo las filas originales de documentos exitosos. La quinta fila de la Tabla 10, etiquetada A5, da rendimiento cuando se consideran las 225 consultas de la pista web de TREC-2004. Obviamente, el nivel de rendimiento no se puede comparar con las líneas anteriores ya que las consultas adicionales son diferentes de las consultas TD y corresponden a otras tareas (tareas de página de inicio y página con nombre [10]) de la pista web de TREC-2004. Este conjunto de ejecuciones tiene como objetivo mostrar si el rendimiento relativo de los diversos métodos depende de la tarea. La última fila de la Tabla 10, etiquetada A6, informa el rendimiento de los diversos métodos considerando la tarea TD de TREC2002 en lugar de TREC-2004: fusionamos los resultados de las clasificaciones de entrada de las 10 mejores ejecuciones oficiales para cada una de las 50 consultas TD [9] considerando el conjunto de supuestos A1 de la primera fila. Esto tiene como objetivo mostrar si el rendimiento relativo de los diversos métodos cambia de año en año. Los valores entre los soportes de la Tabla 10 son variaciones de rendimiento de cada método de agregación de rango W.R.T.Rendimiento del enfoque de supervisión. Tabla 10: rendimiento (mapa) de diferentes métodos de agregación de rango en 3 colecciones de prueba diferentes McM Combmnz Markov A1 18.79% 17.54% (-6.65%*) 17.08% (-9.10%*) 18.63% (-0.85%) A2 21.36%19.18% (-10.21%*) 18.61% (-12.87%*) 21.33% (-0.14%) A3 21.92% 21.38% (-2.46%) 20.88% (-4.74%) 19.35% (-11.72%*) A4 18.64% 17.58% (-5.69%*) 17.18% (-7.83%*) 18.63% (-0.05%) A5 55.39% 52.16% (-5.83%*) 49.70% (-10.27%*) 53.30% (-3.77%)A6 16.95% 15.65% (-7.67%*) 14.57% (-14.04%*) 16.39% (-3.30%) del análisis de la Tabla 10 se puede establecer lo siguiente: • Para todas las ejecuciones, considerando todos los documentos en cada uno en cada unoLa clasificación de entrada (A2) mejora significativamente el rendimiento (el mapa aumenta en un 11,62% en promedio). Esto es predecible ya que algunos documentos relevantes inicialmente no informados recibirían mejores posiciones en la clasificación de consenso.• Para todas las ejecuciones, considerar documentos incluso aquellos presentes en una sola clasificación de entrada (A3) mejora significativamente el rendimiento. Para MCM, Combsum y Combmnz, la mejora del rendimiento es más importante (el mapa aumenta en un 20.27% en promedio) que para la ejecución de Markov (el mapa aumenta en un 3,86%).• Preservar las posiciones iniciales de los documentos (A4) o recomputarlos (A1) no tiene una influencia notable en el rendimiento de los métodos posicionales y mayores.• Teniendo en cuenta todas las consultas de la pista web de TREC2004 (A5), así como las consultas TD de la pista web de TREC-2002 (A6) no altera el rendimiento relativo de los diferentes métodos de fusión de datos.• Teniendo en cuenta las consultas TD de la pista web de TREC2002, las actuaciones de todos los métodos de fusión de datos son más bajas que las de la clasificación de entrada de mejor rendimiento para la cual el valor del mapa es igual al 18.58%. Esto se debe a que la mayoría de las clasificaciones de entrada fusionadas tienen un rendimiento muy bajo en comparación con el mejor, lo que aporta más ruido a la clasificación de consenso.• Los rendimientos de los métodos de fusión de datos McM y Markov son significativamente mejores que los de la mejor clasificación de entrada UOGWEBCAU150. Esto sigue siendo cierto para las ejecuciones Combsum y Combmnz solo bajo supuestos H1 todos o H2 todos. Esto muestra que los métodos mayores son menos sensibles a los supuestos que los métodos posicionales.• El enfoque de supervisión siempre funciona significativamente mejor que los métodos posicionales Combsum y Combmnz. También tiene mejores rendimientos que el método de la cadena de Markov, especialmente bajo la suposición de H2, donde la diferencia de rendimiento se vuelve significativa.6. Conclusiones En este documento, abordamos el problema de agregación de rango donde se deben fusionar las listas de documentos diferentes, pero no desarticulantes. Notamos que las clasificaciones de entrada pueden ocultar lazos, por lo que no deben considerarse como órdenes completas. Solo se debe utilizar información robusta de cada clasificación de entrada. Los métodos de agregación de rango actuales, y especialmente los métodos posicionales (por ejemplo, Cocebro [15]), no están diseñados inicialmente para funcionar con tales clasificaciones. Deben adaptarse considerando suposiciones de trabajo específicas. Proponemos un nuevo método de supervisión para la agregación de rango que está bien adaptado al contexto IR. De hecho, clasifica dos documentos W.R.T.La intensidad de sus posiciones diferencia en cada clasificación de entrada y también considerando el número de clasificaciones de entrada que son concordantes y discordantes a favor de un documento específico. Tampoco es necesario hacer suposiciones específicas en las posiciones de los documentos que faltan. Esta es una característica importante ya que la ausencia de un documento de una clasificación no debe interpretarse necesariamente negativamente. Los resultados experimentales muestran que el método de supervisión supera significativamente los métodos populares de fusión de datos posicionales clásicos como las estrategias Combmnz. También supera los métodos de mayor rendimiento de mayor rendimiento, que es el método de la cadena de Markov. Estos resultados se prueban con diferentes colecciones y consultas de pruebas. A partir de los experimentos, también podemos concluir que para mejorar los rendimientos, debemos fusionar las listas de resultados de modelos IR de bieninga, y que los métodos de fusión de datos mayores funcionan mejor que los métodos posicionales. El método propuesto puede tener un impacto real en las actuaciones de metasearch web, ya que solo los rangos están disponibles en la mayoría de los motores de búsqueda primarios, mientras que la mayoría de los enfoques actuales necesitan puntajes para fusionar las listas de resultados en una sola lista. El trabajo adicional implica investigar si el enfoque de supervisión funciona bien en varios otros contextos, p.Uso de los puntajes de documentos o alguna combinación de rangos y puntajes de documentos. Agradecimientos Los autores desean agradecer a Jacques Savoy por sus valiosos comentarios sobre una versión preliminar de este documento.7. Referencias [1] A. Aronson, D. Demner-Fushman, S. Humphrey, J. Lin, H. Liu, P. Ruch, M. Ruiz, L. Smith, L. Tanabe y W. Wilbur. Fusión de enfoques intensivos y estadísticos de conocimiento para recuperar y anotar documentos de genómica textual. En los procedimientos trec2005. Publicación NIST, 2005. [2] R. A. Baeza-Yates y B. A. Ribeiro-Neto. Recuperación de información moderna. ACM Press, 1999. [3] B. T. Bartell, G. W. Cottrell y R. K. Belew. Combinación automática de sistemas de recuperación de rango múltiple. En los procedimientos ACM-SIGIR94, páginas 173-181. Springer-Verlag, 1994. [4] N. J. Belkin, P. Kantor, E. A. Fox y J. A. Shaw. Combinando evidencia de múltiples representaciones de consultas para la recuperación de información. IPM, 31 (3): 431-448, 1995. [5] J. Borda. Mémoire sur les ´Elections au scrutin. Histoire de Lacad´emie des Sciences, 1781. [6] J. P. Callan, Z. Lu y W. B. Croft. Buscando colecciones distribuidas con redes de inferencia. En los procedimientos ACM-SIGIR95, páginas 21-28, 1995. [7] M. Condorcet. Essai sur l'Aplation de l'Ansalze `a la probabilité des d´recisions rendues` a la pluralit´e des voix. Imprimerie Royale, París, 1785. [8] W. D. Cook y M. Kress. Clasificación ordinal con intensidad de preferencia. Management Science, 31 (1): 26-32, 1985. [9] N. Craswell y D. Hawking. Descripción general de la pista web TREC-2002. En los procedimientos trec2002. Publicación NIST, 2002. [10] N. Craswell y D. Hawking. Descripción general de la pista web TREC-2004. En Actas de TREC2004. Publicación NIST, 2004. [11] C. Dwork, S. R. Kumar, M. Naor y D. Sivakumar. Métodos de agregación de rango para la web. En los procedimientos www2001, páginas 613-622, 2001. [12] R. Fagin. Combinando información difusa de múltiples sistemas. JCSS, 58 (1): 83-99, 1999. [13] R. Fagin, R. Kumar, M. Mahdian, D. Sivakumar y E. Vee. Comparar y agregar clasificaciones con lazos. En Pods, Pages 47-58, 2004. [14] R. Fagin, R. Kumar y D. Sivakumar. Comparación de las listas K superiores. Siam J. sobre Matemáticas Discretas, 17 (1): 134-160, 2003. [15] E. A. Fox y J. A. Shaw. Combinación de múltiples búsquedas. En Actas de TREC3. Publicación NIST, 1994. [16] J. Katzer, M. McGill, J. Tessier, W. Frakes y P. Dasgupta. Un estudio de la superposición entre las representaciones de documentos. Tecnología de la información: Investigación y desarrollo, 1 (4): 261-274, 1982. [17] L. S. Larkey, M. E. Connell y J. Callan. Selección y resultados de recopilación fusionados con patentes de EE. UU. Y datos de TREC organizados tópicamente. En procedimientos ACM-CIKM2000, páginas 282-289. ACM Press, 2000. [18] A. Le Calv´e y J. Savoy. Estrategia de fusión de bases de datos basada en la regresión logística. IPM, 36 (3): 341-359, 2000. [19] J. H. Lee. Análisis de combinación de evidencia múltiple. En Actas ACM-SIGIR97, Páginas 267-276, 1997. [20] D. Lillis, F. Toolan, R. Collier y J. Dunnion. Probfuse: un enfoque probabilístico de la fusión de datos. En procedimientos ACM-SIGIR2006, páginas 139-146. ACM Press, 2006. [21] J. I. Marden. Análisis y modelado de datos de rango. Número 64 en monografías sobre estadísticas y probabilidad aplicada. Chapman y Hall, 1995. [22] M. Montague y J. A. Aslam. Consistencia de metasearch. En procedimientos ACM-SIGIR2001, páginas 386-387. ACM Press, 2001. [23] D. M. Pennock y E. Horvitz. Análisis de los cimientos axiomáticos del filtrado colaborativo. En taller sobre IA para el comercio electrónico en la 16ª Conferencia Nacional sobre Inteligencia Artificial, 1999. [24] M. E. renda y U. Straccia. MetaSearch Web: Métodos de agregación de rango basados en rango vs. puntaje. En procedimientos ACM-SAC2003, páginas 841-846. ACM Press, 2003. [25] W. H. Riker. Liberalismo contra el populismo. Waveland Press, 1982. [26] B. Roy. El enfoque de supervisión y los cimientos de los métodos electas. Teoría y decisión, 31: 49-73, 1991. [27] B. Roy y J. Hugonnard. Clasificación de proyectos de extensión de línea suburbana en el sistema de metro de París mediante un método de multicriterios. Transportation Research, 16a (4): 301-312, 1982. [28] L. Si y J. Callan. Uso de datos y regresión muestreados para fusionar los resultados del motor de búsqueda. En procedimientos ACM-SIGIR2002, páginas 19-26. ACM Press, 2002. [29] M. Truchon. Una extensión del criterio de Condorcet y las órdenes de Kemeny. Cahier 9813, Center de Recherche en Economie et Finance Appliqu´ees, octubre de 1998. [30] H. Turtle y W. B. Croft. Redes de inferencia para la recuperación de documentos. En Actas de ACM-SIGIR90, páginas 1-24. ACM Press, 1990. [31] C. C. Vogt y G. W. Cottrell. Fusión a través de una combinación lineal de puntajes. Recuperación de información, 1 (3): 151-173, 1999.",
    "original_sentences": [
        "An Outranking Approach for Rank Aggregation in Information Retrieval Mohamed Farah Lamsade, Paris Dauphine University Place du Mal de Lattre de Tassigny 75775 Paris Cedex 16, France farah@lamsade.dauphine.fr Daniel Vanderpooten Lamsade, Paris Dauphine University Place du Mal de Lattre de Tassigny 75775 Paris Cedex 16, France vdp@lamsade.dauphine.fr ABSTRACT Research in Information Retrieval usually shows performance improvement when many sources of evidence are combined to produce a ranking of documents (e.g., texts, pictures, sounds, etc.).",
        "In this paper, we focus on the rank aggregation problem, also called data fusion problem, where rankings of documents, searched into the same collection and provided by multiple methods, are combined in order to produce a new ranking.",
        "In this context, we propose a rank aggregation method within a multiple criteria framework using aggregation mechanisms based on decision rules identifying positive and negative reasons for judging whether a document should get a better rank than another.",
        "We show that the proposed method deals well with the Information Retrieval distinctive features.",
        "Experimental results are reported showing that the suggested method performs better than the well-known CombSUM and CombMNZ operators.",
        "Categories and Subject Descriptors: H.3.3 [Information Systems]: Information Search and Retrieval - Retrieval models.",
        "General Terms: Algorithms, Measurement, Experimentation, Performance, Theory. 1.",
        "INTRODUCTION A wide range of current Information Retrieval (IR) approaches are based on various search models (Boolean, Vector Space, Probabilistic, Language, etc. [2]) in order to retrieve relevant documents in response to a user request.",
        "The result lists produced by these approaches depend on the exact definition of the relevance concept.",
        "Rank aggregation approaches, also called data fusion approaches, consist in combining these result lists in order to produce a new and hopefully better ranking.",
        "Such approaches give rise to metasearch engines in the Web context.",
        "We consider, in the following, cases where only ranks are available and no other additional information is provided such as the relevance scores.",
        "This corresponds indeed to the reality, where only ordinal information is available.",
        "Data fusion is also relevant in other contexts, such as when the user writes several queries of his/her information need (e.g., a boolean query and a natural language query) [4], or when many document surrogates are available [16].",
        "Several studies argued that rank aggregation has the potential of combining effectively all the various sources of evidence considered in various input methods.",
        "For instance, experiments carried out in [16], [30], [4] and [19] showed that documents which appear in the lists of the majority of the input methods are more likely to be relevant.",
        "Moreover, Lee [19] and Vogt and Cottrell [31] found that various retrieval approaches often return very different irrelevant documents, but many of the same relevant documents.",
        "Bartell et al. [3] also found that rank aggregation methods improve the performances w.r.t. those of the input methods, even when some of them have weak individual performances.",
        "These methods also tend to smooth out biases of the input methods according to Montague and Aslam [22].",
        "Data fusion has recently been proved to improve performances for both the ad-hoc retrieval and categorization tasks within the TREC genomics track in 2005 [1].",
        "The rank aggregation problem was addressed in various fields such as i) in social choice theory which studies voting algorithms which specify winners of elections or winners of competitions in tournaments [29], ii) in statistics when studying correlation between rankings, iii) in distributed databases when results from different databases must be combined [12], and iv) in collaborative filtering [23].",
        "Most current rank aggregation methods consider each input ranking as a permutation over the same set of items.",
        "They also give rigid interpretation to the exact ranking of the items.",
        "Both of these assumptions are rather not valid in the IR context, as will be shown in the following sections.",
        "The remaining of the paper is organized as follows.",
        "We first review current rank aggregation methods in Section 2.",
        "Then we outline the specificities of the data fusion problem in the IR context (Section 3).",
        "In Section 4, we present a new aggregation method which is proven to best fit the IR context.",
        "Experimental results are presented in Section 5 and conclusions are provided in a final section. 2.",
        "RELATED WORK As pointed out by Riker [25], we can distinguish two families of rank aggregation methods: positional methods which assign scores to items to be ranked according to the ranks they receive and majoritarian methods which are based on pairwise comparisons of items to be ranked.",
        "These two families of methods find their roots in the pioneering works of Borda [5] and Condorcet [7], respectively, in the social choice literature. 2.1 Preliminaries We first introduce some basic notations to present the rank aggregation methods in a uniform way.",
        "Let D = {d1, d2, . . . , dnd } be a set of nd documents.",
        "A list or a ranking j is an ordering defined on Dj ⊆ D (j = 1, . . . , n).",
        "Thus, di j di means di is ranked better than di in j.",
        "When Dj = D, j is said to be a full list.",
        "Otherwise, it is a partial list.",
        "If di belongs to Dj, rj i denotes the rank or position of di in j.",
        "We assume that the best answer (document) is assigned the position 1 and the worst one is assigned the position |Dj|.",
        "Let D be the set of all permutations on D or all subsets of D. A profile is a n-tuple of rankings PR = ( 1, 2, . . . , n).",
        "Restricting PR to the rankings containing document di defines PRi.",
        "We also call the number of rankings which contain document di the rank hits of di [19].",
        "The rank aggregation or data fusion problem consists of finding a ranking function or mechanism Ψ (also called a social welfare function in the social choice theory terminology) defined by: Ψ : n D → D PR = ( 1, 2, . . . , n) → σ = Ψ(PR) where σ is called a consensus ranking. 2.2 Positional Methods 2.2.1 Borda Count This method [5] first assigns a score n j=1 rj i to each document di.",
        "Documents are then ranked by increasing order of this score, breaking ties, if any, arbitrarily. 2.2.2 Linear Combination Methods This family of methods basically combine scores of documents.",
        "When used for the rank aggregation problem, ranks are assumed to be scores or performances to be combined using aggregation operators such as the weighted sum or some variation of it [3, 31, 17, 28].",
        "For instance, Callan et al. [6] used the inference networks model [30] to combine rankings.",
        "Fox and Shaw [15] proposed several combination strategies which are CombSUM, CombMIN, CombMAX, CombANZ and CombMNZ.",
        "The first three operators correspond to the sum, min and max operators, respectively.",
        "CombANZ and CombMNZ respectively divides and multiplies the CombSUM score by the rank hits.",
        "It is shown in [19] that the CombSUM and CombMNZ operators perform better than the others.",
        "Metasearch engines such as SavvySearch and MetaCrawler use the CombSUM strategy to fuse rankings. 2.2.3 Footrule Optimal Aggregation In this method, a consensus ranking minimizes the Spearman footrule distance from the input rankings [21].",
        "Formally, given two full lists j and j , this distance is given by F( j, j ) = nd i=1 |rj i − rj i |.",
        "It extends to several lists as follows.",
        "Given a profile PR and a consensus ranking σ, the Spearman footrule distance of σ to PR is given by F(σ, PR) = n j=1 F(σ, j).",
        "Cook and Kress [8] proposed a similar method which consists in optimizing the distance D( j, j ) = 1 2 nd i,i =1 |rj i,i − rj i,i |, where rj i,i = rj i −rj i .",
        "This formulation has the advantage that it considers the intensity of preferences. 2.2.4 Probabilistic Methods This kind of methods assume that the performance of the input methods on a number of training queries is indicative of their future performance.",
        "During the training process, probabilities of relevance are calculated.",
        "For subsequent queries, documents are ranked based on these probabilities.",
        "For instance, in [20], each input ranking j is divided into a number of segments, and the conditional probability of relevance (R) of each document di depending on the segment k it occurs in, is computed, i.e. prob(R|di, k, j).",
        "For subsequent queries, the score of each document di is given by n j=1 prob(R|di,k, j ) k .",
        "Le Calve and Savoy [18] suggest using a logistic regression approach for combining scores.",
        "Training data is needed to infer the model parameters. 2.3 Majoritarian Methods 2.3.1 Condorcet Procedure The original Condorcet rule [7] specifies that a winner of the election is any item that beats or ties with every other item in a pairwise contest.",
        "Formally, let C(diσdi ) = { j∈ PR : di j di } be the coalition of rankings that are concordant with establishing diσdi , i.e. with the proposition di should be ranked better than di in the final ranking σ. di beats or ties with di iff |C(diσdi )| ≥ |C(di σdi)|.",
        "The repetitive application of the Condorcet algorithm can produce a ranking of items in a natural way: select the Condorcet winner, remove it from the lists, and repeat the previous two steps until there are no more documents to rank.",
        "Since there is not always Condorcet winners, variations of the Condorcet procedure have been developed within the multiple criteria decision aid theory, with methods such as ELECTRE [26]. 2.3.2 Kemeny Optimal Aggregation As in section 2.2.3, a consensus ranking minimizes a geometric distance from the input rankings, where the Kendall tau distance is used instead of the Spearman footrule distance.",
        "Formally, given two full lists j and j , the Kendall tau distance is given by K( j, j ) = |{(di, di ) : i < i , rj i < rj i , rj i > rj i }|, i.e. the number of pairwise disagreements between the two lists.",
        "It is easy to show that the consensus ranking corresponds to the geometric median of the input rankings and that the Kemeny optimal aggregation problem corresponds to the minimum feedback edge set problem. 2.3.3 Markov Chain Methods Markov chains (MCs) have been used by Dwork et al. [11] as a natural method to obtain a consensus ranking where states correspond to the documents to be ranked and the transition probabilities vary depending on the interpretation of the transition event.",
        "In the same reference, the authors proposed four specific MCs and experimental testing had shown that the following MC is the best performing one (see also [24]): • MC4: move from the current state di to the next state di by first choosing a document di uniformly from D. If for the majority of the rankings, we have rj i ≤ rj i , then move to di , else stay in di.",
        "The consensus ranking corresponds to the stationary distribution of MC4. 3.",
        "SPECIFICITIES OF THE RANK AGGREGATION PROBLEM IN THE IR CONTEXT 3.1 Limited Significance of the Rankings The exact positions of documents in one input ranking have limited significance and should not be overemphasized.",
        "For instance, having three relevant documents in the first three positions, any perturbation of these three items will have the same value.",
        "Indeed, in the IR context, the complete order provided by an input method may hide ties.",
        "In this case, we call such rankings semi orders.",
        "This was outlined in [13] as the problem of aggregation with ties.",
        "It is therefore important to build the consensus ranking based on robust information: • Documents with near positions in j are more likely to have similar interest or relevance.",
        "Thus a slight perturbation of the initial ranking is meaningless. • Assuming that document di is better ranked than document di in a ranking j, di is more likely to be definitively more relevant than di in j when the number of intermediate positions between di and di increases. 3.2 Partial Lists In real world applications, such as metasearch engines, rankings provided by the input methods are often partial lists.",
        "This was outlined in [14] as the problem of having to merge top-k results from various input lists.",
        "For instance, in the experiments carried out by Dwork et al. [11], authors found that among the top 100 best documents of 7 input search engines, 67% of the documents were present in only one search engine, whereas less than two documents were present in all the search engines.",
        "Rank aggregation of partial lists raises four major difficulties which we state hereafter, proposing for each of them various working assumptions: 1.",
        "Partial lists can have various lengths, which can favour long lists.",
        "We thus consider the following two working hypotheses: H1 k : We only consider the top k best documents from each input ranking.",
        "H1 all: We consider all the documents from each input ranking. 2.",
        "Since there are different documents in the input rankings, we must decide which documents should be kept in the consensus ranking.",
        "Two working hypotheses are therefore considered: H2 k : We only consider documents which are present in at least k input rankings (k > 1).",
        "H2 all: We consider all the documents which are ranked in at least one input ranking.",
        "Hereafter, we call documents which will be retained in the consensus ranking, candidate documents, and documents that will be excluded from the consensus ranking, excluded documents.",
        "We also call a candidate document which is missing in one or more rankings, a missing document. 3.",
        "Some candidate documents are missing documents in some input rankings.",
        "Main reasons for a missing document are that it was not indexed or it was indexed but deemed irrelevant ; usually this information is not available.",
        "We consider the following two working hypotheses: H3 yes: Each missing document in each j is assigned a position.",
        "H3 no: No assumption is made, that is each missing document is considered neither better nor worse than any other document. 4.",
        "When assumption H2 k holds, each input ranking may contain documents which will not be considered in the consensus ranking.",
        "Regarding the positions of the candidate documents, we can consider the following working hypotheses: H4 init: The initial positions of candidate documents are kept in each input ranking.",
        "H4 new: Candidate documents receive new positions in each input ranking, after discarding excluded ones.",
        "In the IR context, rank aggregation methods need to decide more or less explicitly which assumptions to retain w.r.t. the above-mentioned difficulties. 4.",
        "OUTRANKING APPROACH FOR RANK AGGREGATION 4.1 Presentation Positional methods consider implicitly that the positions of the documents in the input rankings are scores giving thus a cardinal meaning to an ordinal information.",
        "This constitutes a strong assumption that is questionable, especially when the input rankings have different lengths.",
        "Moreover, for positional methods, assumptions H3 and H4 , which are often arbitrary, have a strong impact on the results.",
        "For instance, let us consider an input ranking of 500 documents out of 1000 candidate documents.",
        "Whether we assign to each of the missing documents the position 1, 501, 750 or 1000 -corresponding to variations of H3 yes- will give rise to very contrasted results, especially regarding the top of the consensus ranking.",
        "Majoritarian methods do not suffer from the above-mentioned drawbacks of the positional methods since they build consensus rankings exploiting only ordinal information contained in the input rankings.",
        "Nevertheless, they suppose that such rankings are complete orders, ignoring that they may hide ties.",
        "Therefore, majoritarian methods base consensus rankings on illusory discriminant information rather than less discriminant but more robust information.",
        "Trying to overcome the limits of current rank aggregation methods, we found that outranking approaches, which were initially used for multiple criteria aggregation problems [26], can also be used for the rank aggregation purpose, where each ranking plays the role of a criterion.",
        "Therefore, in order to decide whether a document di should be ranked better than di in the consensus ranking σ, the two following conditions should be met: • a concordance condition which ensures that a majority of the input rankings are concordant with diσdi (majority principle). • a discordance condition which ensures that none of the discordant input rankings strongly refutes dσd (respect of minorities principle).",
        "Formally, the concordance coalition with diσdi is Csp (diσdi ) = { j∈ PR : rj i ≤ rj i − sp} where sp is a preference threshold which is the variation of document positions -whether it is absolute or relative to the ranking length- which draws the boundaries between an indifference and a preference situation between documents.",
        "The discordance coalition with diσdi is Dsv (diσdi ) = { j∈ PR : rj i ≥ rj i + sv} where sv is a veto threshold which is the variation of document positions -whether it is absolute or relative to the ranking length- which draws the boundaries between a weak and a strong opposition to diσdi .",
        "Depending on the exact definition of the preceding concordance and discordance coalitions leading to the definition of some decision rules, several outranking relations can be defined.",
        "They can be more or less demanding depending on i) the values of the thresholds sp and sv, ii) the importance or minimal size cmin required for the concordance coalition, and iii) the importance or maximum size dmax of the discordance coalition.",
        "A generic outranking relation can thus be defined as follows: diS(sp,sv,cmin,dmax)di ⇔ |Csp (diσdi )| ≥ cmin AND |Dsv (diσdi )| ≤ dmax This expression defines a family of nested outranking relations since S(sp,sv,cmin,dmax) ⊆ S(sp,sv,cmin,dmax) when cmin ≥ cmin and/or dmax ≤ dmax and/or sp ≥ sp and/or sv ≤ sv.",
        "This expression also generalizes the majority rule which corresponds to the particular relation S(0,∞, n 2 ,n).",
        "It also satisfies important properties of rank aggregation methods, called neutrality, Pareto-optimality, Condorcet property and Extended Condorcet property, in the social choice literature [29].",
        "Outranking relations are not necessarily transitive and do not necessarily correspond to rankings since directed cycles may exist.",
        "Therefore, we need specific procedures in order to derive a consensus ranking.",
        "We propose the following procedure which finds its roots in [27].",
        "It consists in partitioning the set of documents into r ranked classes.",
        "Each class Ch contains documents with the same relevance and results from the application of all relations (if possible) to the set of documents remaining after previous classes are computed.",
        "Documents within the same equivalence class are ranked arbitrarily.",
        "Formally, let • R be the set of candidate documents for a query, • S1 , S2 , . . . be a family of nested outranking relations, • Fk(di, E) = |{di ∈ E : diSk di }| be the number of documents in E(E ⊆ R) that could be considered worse than di according to relation Sk , • fk(di, E) = |{di ∈ E : di Sk di}| be the number of documents in E that could be considered better than di according to Sk , • sk(di, E) = Fk(di, E) − fk(di, E) be the qualification of di in E according to Sk .",
        "Each class Ch results from a distillation process.",
        "It corresponds to the last distillate of a series of sets E0 ⊇ E1 ⊇ . . . where E0 = R \\ (C1 ∪ . . . ∪ Ch−1) and Ek is a reduced subset of Ek−1 resulting from the application of the following procedure: 1. compute for each di ∈ Ek−1 its qualification according to Sk , i.e. sk(di, Ek−1), 2. define smax = maxdi∈Ek−1 {sk(di, Ek−1)}, then 3.",
        "Ek = {di ∈ Ek−1 : sk(di, Ek−1) = smax} When one outranking relation is used, the distillation process stops after the first application of the previous procedure, i.e., Ch corresponds to distillate E1.",
        "When different outranking relations are used, the distillation process stops when all the pre-defined outranking relations have been used or when |Ek| = 1. 4.2 Illustrative Example This section illustrates the concepts and procedures of section 4.1.",
        "Let us consider a set of candidate documents R = {d1, d2, d3, d4, d5}.",
        "The following table gives a profile PR of different rankings of the documents of R: PR = ( 1 , 2, 3, 4).",
        "Table 1: Rankings of documents rj i 1 2 3 4 d1 1 3 1 5 d2 2 1 3 3 d3 3 2 2 1 d4 4 4 5 2 d5 5 5 4 4 Let us suppose that the preference and veto thresholds are set to values 1 and 4 respectively, and that the concordance and discordance thresholds are set to values 2 and 1 respectively.",
        "The following tables give the concordance, discordance and outranking matrices.",
        "Each entry csp (di, di ) (dsv (di, di )) in the concordance (discordance) matrix gives the number of rankings that are concordant (discordant) with diσdi , i.e. csp (di, di ) = |Csp (diσdi )| and dsv (di, di ) = |Dsv (diσdi )|.",
        "Table 2: Computation of the outranking relation d1 d2 d3 d4 d5 d1 - 2 2 3 3 d2 2 - 2 3 4 d3 2 2 - 4 4 d4 1 1 0 - 3 d5 1 0 0 1Concordance Matrix d1 d2 d3 d4 d5 d1 - 0 1 0 0 d2 0 - 0 0 0 d3 0 0 - 0 0 d4 1 0 0 - 0 d5 1 1 0 0Discordance Matrix d1 d2 d3 d4 d5 d1 - 1 1 1 1 d2 1 - 1 1 1 d3 1 1 - 1 1 d4 0 0 0 - 1 d5 0 0 0 0Outranking Matrix (S1) For instance, the concordance coalition for the assertion d1σd4 is C1(d1σd4) = { 1, 2, 3} and the discordance coalition for the same assertion is D4(d1σd4) = ∅.",
        "Therefore, c1(d1, d4) = 3, d4(d1, d4) = 0 and d1S1 d4 holds.",
        "Notice that Fk(di, R) (fk(di, R)) is given by summing the values of the ith row (column) of the outranking matrix.",
        "The consensus ranking is obtained as follows: to get the first class C1, we compute the qualifications of all the documents of E0 = R with respect to S1 .",
        "They are respectively 2, 2, 2, -2 and -4.",
        "Therefore smax equals 2 and C1 = E1 = {d1, d2, d3}.",
        "Observe that, if we had used a second outranking relation S2(⊇ S1), these three documents could have been possibly discriminated.",
        "At this stage, we remove documents of C1 from the outranking matrix and compute the next class C2: we compute the new qualifications of the documents of E0 = R \\ C1 = {d4, d5}.",
        "They are respectively 1 and -1.",
        "So C3 = E1 = {d4}.",
        "The last document d5 is the only document of the last class C3.",
        "Thus, the consensus ranking is {d1, d2, d3} → {d4} → {d5}. 5.",
        "EXPERIMENTS AND RESULTS 5.1 Test Setting To facilitate empirical investigation of the proposed methodology, we developed a prototype metasearch engine that implements a version of our outranking approach for rank aggregation.",
        "In this paper, we apply our approach to the Topic Distillation (TD) task of TREC-2004 Web track [10].",
        "In this task, there are 75 topics where only a short description of each is given.",
        "For each query, we retained the rankings of the 10 best runs of the TD task which are provided by TREC-2004 participating teams.",
        "The performances of these runs are reported in table 3.",
        "Table 3: Performances of the 10 best runs of the TD task of TREC-2004 Run Id MAP P@10 S@1 S@5 S@10 uogWebCAU150 17.9% 24.9% 50.7% 77.3% 89.3% MSRAmixed1 17.8% 25.1% 38.7% 72.0% 88.0% MSRC04C12 16.5% 23.1% 38.7% 74.7% 80.0% humW04rdpl 16.3% 23.1% 37.3% 78.7% 90.7% THUIRmix042 14.7% 20.5% 21.3% 58.7% 74.7% UAmsT04MWScb 14.6% 20.9% 36.0% 66.7% 76.0% ICT04CIIS1AT 14.1% 20.8% 33.3% 64.0% 78.7% SJTUINCMIX5 12.9% 18.9% 29.3% 57.3% 72.0% MU04web1 11.5% 19.9% 33.3% 64.0% 76.0% MeijiHILw3 11.5% 15.3% 30.7% 54.7% 64.0% Average 14.7% 21.2% 34.9% 66.8% 78.94% For each query, each run provides a ranking of about 1000 documents.",
        "The number of documents retrieved by all these runs ranges from 543 to 5769.",
        "Their average (median) number is 3340 (3386).",
        "It is worth noting that we found similar distributions of the documents among the rankings as in [11].",
        "For evaluation, we used the trec eval standard tool which is used by the TREC community to calculate the standard measures of system effectiveness which are Mean Average Precision (MAP) and Success@n (S@n) for n=1, 5 and 10.",
        "Our approach effectiveness is compared against some high performing official results from TREC-2004 as well as against some standard rank aggregation algorithms.",
        "In the experiments, significance testing is mainly based on the t-student statistic which is computed on the basis of the MAP values of the compared runs.",
        "In the tables of the following section, statistically significant differences are marked with an asterisk.",
        "Values between brackets of the first column of each table, indicate the parameter value of the corresponding run. 5.2 Results We carried out several series of runs in order to i) study performance variations of the outranking approach when tuning the parameters and working assumptions, ii) compare performances of the outranking approach vs standard rank aggregation strategies , and iii) check whether rank aggregation performs better than the best input rankings.",
        "We set our basic run mcm with the following parameters.",
        "We considered that each input ranking is a complete order (sp = 0) and that an input ranking strongly refutes diσdi when the difference of both document positions is large enough (sv = 75%).",
        "Preference and veto thresholds are computed proportionally to the number of documents retained in each input ranking.",
        "They consequently may vary from one ranking to another.",
        "In addition, to accept the assertion diσdi , we supposed that the majority of the rankings must be concordant (cmin = 50%) and that every input ranking can impose its veto (dmax = 0).",
        "Concordance and discordance thresholds are computed for each tuple (di, di ) as the percentage of the input rankings of PRi ∩PRi .",
        "Thus, our choice of parameters leads to the definition of the outranking relation S(0,75%,50%,0).",
        "To test the run mcm, we had chosen the following assumptions.",
        "We retained the top 100 best documents from each input ranking (H1 100), only considered documents which are present in at least half of the input rankings (H2 5 ) and assumed H3 no and H4 new.",
        "In these conditions, the number of successful documents was about 100 on average, and the computation time per query was less than one second.",
        "Obviously, modifying the working assumptions should have deeper impact on the performances than tuning our model parameters.",
        "This was validated by preliminary experiments.",
        "Thus, we hereafter begin by studying performance variation when different sets of assumptions are considered.",
        "Afterwards, we study the impact of tuning parameters.",
        "Finally, we compare our model performances w.r.t. the input rankings as well as some standard data fusion algorithms. 5.2.1 Impact of the Working Assumptions Table 4 summarizes the performance variation of the outranking approach under different working hypotheses.",
        "In Table 4: Impact of the working assumptions Run Id MAP S@1 S@5 S@10 mcm 18.47% 41.33% 81.33% 86.67% mcm22 (H3 yes) 17.72% (-4.06%) 34.67% 81.33% 86.67% mcm23 (H4 init) 18.26% (-1.14%) 41.33% 81.33% 86.67% mcm24 (H1 all) 20.67% (+11.91%*) 38.66% 80.00% 86.66% mcm25 (H2 all) 21.68% (+17.38%*) 40.00% 78.66% 89.33% this table, we first show that run mcm22, in which missing documents are all put in the same last position of each input ranking, leads to performance drop w.r.t. run mcm.",
        "Moreover, S@1 moves from 41.33% to 34.67% (-16.11%).",
        "This shows that several relevant documents which were initially put at the first position of the consensus ranking in mcm, lose this first position but remain ranked in the top 5 documents since S@5 did not change.",
        "We also conclude that documents which have rather good positions in some input rankings are more likely to be relevant, even though they are missing in some other rankings.",
        "Consequently, when they are missing in some rankings, assigning worse ranks to these documents is harmful for performance.",
        "Also, from Table 4, we found that the performances of runs mcm and mcm23 are similar.",
        "Therefore, the outranking approach is not sensitive to keeping the initial positions of candidate documents or recomputing them by discarding excluded ones.",
        "From the same Table 4, performance of the outranking approach increases significantly for runs mcm24 and mcm25.",
        "Therefore, whether we consider all the documents which are present in half of the rankings (mcm24) or we consider all the documents which are ranked in the first 100 positions in one or more rankings (mcm25), increases performances.",
        "This result was predictable since in both cases we have more detailed information on the relative importance of documents.",
        "Tables 5 and 6 confirm this evidence.",
        "Table 5, where values between brackets of the first column give the number of documents which are retained from each input ranking, shows that selecting more documents from each input ranking leads to performance increase.",
        "It is worth mentioning that selecting more than 600 documents from each input ranking does not improve performance.",
        "Table 5: Impact of the number of retained documents Run Id MAP S@1 S@5 S@10 mcm (100) 18.47% 41.33% 81.33% 86.67% mcm24-1 (200) 19.32% (+4.60%) 42.67% 78.67% 88.00% mcm24-2 (400) 19.88% (+7.63%*) 37.33% 80.00% 88.00% mcm24-3 (600) 20.80% (+12.62%*) 40.00% 80.00% 88.00% mcm24-4 (800) 20.66% (+11.86%*) 40.00% 78.67% 86.67% mcm24 (1000) 20.67% (+11.91%*) 38.66% 80.00% 86.66% Table 6 reports runs corresponding to variations of H2 k .",
        "Values between brackets are rank hits.",
        "For instance, in the run mcm32, only documents which are present in 3 or more input rankings, were considered successful.",
        "This table shows that performance is significantly better when rare documents are considered, whereas it decreases significantly when these documents are discarded.",
        "Therefore, we conclude that many of the relevant documents are retrieved by a rather small set of IR models.",
        "Table 6: Performance considering different rank hits Run Id MAP S@1 S@5 S@10 mcm25 (1) 21.68% (+17.38%*) 40.00% 78.67% 89.33% mcm32 (3) 18.98% (+2.76%) 38.67% 80.00% 85.33% mcm (5) 18.47% 41.33% 81.33% 86.67% mcm33 (7) 15.83% (-14.29%*) 37.33% 78.67% 85.33% mcm34 (9) 10.96% (-40.66%*) 36.11% 66.67% 70.83% mcm35 (10) 7.42% (-59.83%*) 39.22% 62.75% 64.70% For both runs mcm24 and mcm25, the number of successful documents was about 1000 and therefore, the computation time per query increased and became around 5 seconds. 5.2.2 Impact of the Variation of the Parameters Table 7 shows performance variation of the outranking approach when different preference thresholds are considered.",
        "We found performance improvement up to threshold values of about 5%, then there is a decrease in the performance which becomes significant for threshold values greater than 10%.",
        "Moreover, S@1 improves from 41.33% to 46.67% when preference threshold changes from 0 to 5%.",
        "We can thus conclude that the input rankings are semi orders rather than complete orders.",
        "Table 8 shows the evolution of the performance measures w.r.t. the concordance threshold.",
        "We can conclude that in order to put document di before di in the consensus ranking, Table 7: Impact of the variation of the preference threshold from 0 to 12.5% Run Id MAP S@1 S@5 S@10 mcm (0%) 18.47% 41.33% 81.33% 86.67% mcm1 (1%) 18.57% (+0.54%) 41.33% 81.33% 86.67% mcm2 (2.5%) 18.63% (+0.87%) 42.67% 78.67% 86.67% mcm3 (5%) 18.69% (+1.19%) 46.67% 81.33% 86.67% mcm4 (7.5%) 18.24% (-1.25%) 46.67% 81.33% 86.67% mcm5 (10%) 17.93% (-2.92%) 40.00% 82.67% 86.67% mcm5b (12.5%) 17.51% (-5.20%*) 41.33% 80.00% 86.67% at least half of the input rankings of PRi ∩ PRi should be concordant.",
        "Performance drops significantly for very low and very high values of the concordance threshold.",
        "In fact, for such values, the concordance condition is either fulfilled rather always by too many document pairs or not fulfilled at all, respectively.",
        "Therefore, the outranking relation becomes either too weak or too strong respectively.",
        "Table 8: Impact of the variation of cmin Run Id MAP S@1 S@5 S@10 mcm11 (20%) 17.63% (-4.55%*) 41.33% 76.00% 85.33% mcm12 (40%) 18.37% (-0.54%) 42.67% 76.00% 86.67% mcm (50%) 18.47% 41.33% 81.33% 86.67% mcm13 (60%) 18.42% (-0.27%) 40.00% 78.67% 86.67% mcm14 (80%) 17.43% (-5.63%*) 40.00% 78.67% 86.67% mcm15 (100%) 16.12% (-12.72%*) 41.33% 70.67% 85.33% In the experiments, varying the veto threshold as well as the discordance threshold within reasonable intervals does not have significant impact on performance measures.",
        "In fact, runs with different veto thresholds (sv ∈ [50%; 100%]) had similar performances even though there is a slight advantage for runs with high threshold values which means that it is better not to allow the input rankings to put their veto easily.",
        "Also, tuning the discordance threshold was carried out for values 50% and 75% of the veto threshold.",
        "For these runs we did not get any noticeable performance variation, although for low discordance thresholds (dmax < 20%), performance slightly decreased. 5.2.3 Impact of the Variation of the Number of Input Rankings To study performance evolution when different sets of input rankings are considered, we carried three more runs where 2, 4, and 6 of the best performing sets of the input rankings are considered.",
        "Results reported in Table 9 are seemingly counter-intuitive and also do not support previous findings regarding rank aggregation research [3].",
        "Nevertheless, this result shows that low performing rankings bring more noise than information to the establishment of the consensus ranking.",
        "Therefore, when they are considered, performance decreases.",
        "Table 9: Performance considering different best performing sets of input rankings Run Id MAP S@1 S@5 S@10 mcm (10) 18.47% 41.33% 81.33% 86.67% mcm27 (6) 18.60% (+0.70%) 41.33% 80.00% 85.33% mcm28 (4) 19.02% (+2.98%) 40.00% 86.67% 88.00% mcm29 (2) 18.33% (-0.76%) 44.00% 76.00% 88.00% 5.2.4 Comparison of the Performance of Different Rank Aggregation Methods In this set of runs, we compare the outranking approach with some standard rank aggregation methods which were proven to have acceptable performance in previous studies: we considered two positional methods which are the CombSUM and the CombMNZ strategies.",
        "We also examined the performance of one majoritarian method which is the Markov chain method (MC4).",
        "For the comparisons, we considered a specific outranking relation S∗ = S(5%,50%,50%,30%) which results in good overall performances when tuning all the parameters.",
        "The first row of Table 10 gives performances of the rank aggregation methods w.r.t. a basic assumption set A1 = (H1 100, H2 5 , H4 new): we only consider the 100 first documents from each ranking, then retain documents present in 5 or more rankings and update ranks of successful documents.",
        "For positional methods, we place missing documents at the queue of the ranking (H3 yes) whereas for our method as well as for MC4, we retained hypothesis H3 no.",
        "The three following rows of Table 10 report performances when changing one element from the basic assumption set: the second row corresponds to the assumption set A2 = (H1 1000, H2 5 , H4 new), i.e. changing the number of retained documents from 100 to 1000.",
        "The third row corresponds to the assumption set A3 = (H1 100, H2 all, H4 new), i.e. considering the documents present in at least one ranking.",
        "The fourth row corresponds to the assumption set A4 = (H1 100, H2 5 , H4 init), i.e. keeping the original ranks of successful documents.",
        "The fifth row of Table 10, labeled A5, gives performance when all the 225 queries of the Web track of TREC-2004 are considered.",
        "Obviously, performance level cannot be compared with previous lines since the additional queries are different from the TD queries and correspond to other tasks (Home Page and Named Page tasks [10]) of TREC-2004 Web track.",
        "This set of runs aims to show whether relative performance of the various methods is task-dependent.",
        "The last row of Table 10, labeled A6, reports performance of the various methods considering the TD task of TREC2002 instead of TREC-2004: we fused the results of input rankings of the 10 best official runs for each of the 50 TD queries [9] considering the set of assumptions A1 of the first row.",
        "This aims to show whether relative performance of the various methods changes from year to year.",
        "Values between brackets of Table 10 are variations of performance of each rank aggregation method w.r.t. performance of the outranking approach.",
        "Table 10: Performance (MAP) of different rank aggregation methods under 3 different test collections mcm combSUM combMNZ markov A1 18.79% 17.54% (-6.65%*) 17.08% (-9.10%*) 18.63% (-0.85%) A2 21.36% 19.18% (-10.21%*) 18.61% (-12.87%*) 21.33% (-0.14%) A3 21.92% 21.38% (-2.46%) 20.88% (-4.74%) 19.35% (-11.72%*) A4 18.64% 17.58% (-5.69%*) 17.18% (-7.83%*) 18.63% (-0.05%) A5 55.39% 52.16% (-5.83%*) 49.70% (-10.27%*) 53.30% (-3.77%) A6 16.95% 15.65% (-7.67%*) 14.57% (-14.04%*) 16.39% (-3.30%) From the analysis of table 10 the following can be established: • for all the runs, considering all the documents in each input ranking (A2) significantly improves performance (MAP increases by 11.62% on average).",
        "This is predictable since some initially unreported relevant documents would receive better positions in the consensus ranking. • for all the runs, considering documents even those present in only one input ranking (A3) significantly improves performance.",
        "For mcm, combSUM and combMNZ, performance improvement is more important (MAP increases by 20.27% on average) than for the markov run (MAP increases by 3.86%). • preserving the initial positions of documents (A4) or recomputing them (A1) does not have a noticeable influence on performance for both positional and majoritarian methods. • considering all the queries of the Web track of TREC2004 (A5) as well as the TD queries of the Web track of TREC-2002 (A6) does not alter the relative performance of the different data fusion methods. • considering the TD queries of the Web track of TREC2002, performances of all the data fusion methods are lower than that of the best performing input ranking for which the MAP value equals 18.58%.",
        "This is because most of the fused input rankings have very low performances compared to the best one, which brings more noise to the consensus ranking. • performances of the data fusion methods mcm and markov are significantly better than that of the best input ranking uogWebCAU150.",
        "This remains true for runs combSUM and combMNZ only under assumptions H1 all or H2 all.",
        "This shows that majoritarian methods are less sensitive to assumptions than positional methods. • outranking approach always performs significantly better than positional methods combSUM and combMNZ.",
        "It has also better performances than the Markov chain method, especially under assumption H2 all where difference of performances becomes significant. 6.",
        "CONCLUSIONS In this paper, we address the rank aggregation problem where different, but not disjoint, lists of documents are to be fused.",
        "We noticed that the input rankings can hide ties, so they should not be considered as complete orders.",
        "Only robust information should be used from each input ranking.",
        "Current rank aggregation methods, and especially positional methods (e.g. combSUM [15]), are not initially designed to work with such rankings.",
        "They should be adapted by considering specific working assumptions.",
        "We propose a new outranking method for rank aggregation which is well adapted to the IR context.",
        "Indeed, it ranks two documents w.r.t. the intensity of their positions difference in each input ranking and also considering the number of the input rankings that are concordant and discordant in favor of a specific document.",
        "There is also no need to make specific assumptions on the positions of the missing documents.",
        "This is an important feature since the absence of a document from a ranking should not be necessarily interpreted negatively.",
        "Experimental results show that the outranking method significantly out-performs popular classical positional data fusion methods like combSUM and combMNZ strategies.",
        "It also out-performs a good performing majoritarian methods which is the Markov chain method.",
        "These results are tested against different test collections and queries.",
        "From the experiments, we can also conclude that in order to improve the performances, we should fuse result lists of well performing IR models, and that majoritarian data fusion methods perform better than positional methods.",
        "The proposed method can have a real impact on Web metasearch performances since only ranks are available from most primary search engines, whereas most of the current approaches need scores to merge result lists into one single list.",
        "Further work involves investigating whether the outranking approach performs well in various other contexts, e.g. using the document scores or some combination of document ranks and scores.",
        "Acknowledgments The authors would like to thank Jacques Savoy for his valuable comments on a preliminary version of this paper. 7.",
        "REFERENCES [1] A. Aronson, D. Demner-Fushman, S. Humphrey, J. Lin, H. Liu, P. Ruch, M. Ruiz, L. Smith, L. Tanabe, and W. Wilbur.",
        "Fusion of knowledge-intensive and statistical approaches for retrieving and annotating textual genomics documents.",
        "In Proceedings TREC2005.",
        "NIST Publication, 2005. [2] R. A. Baeza-Yates and B.",
        "A. Ribeiro-Neto.",
        "Modern Information Retrieval.",
        "ACM Press , 1999. [3] B. T. Bartell, G. W. Cottrell, and R. K. Belew.",
        "Automatic combination of multiple ranked retrieval systems.",
        "In Proceedings ACM-SIGIR94, pages 173-181.",
        "Springer-Verlag, 1994. [4] N. J. Belkin, P. Kantor, E. A.",
        "Fox, and J.",
        "A. Shaw.",
        "Combining evidence of multiple query representations for information retrieval.",
        "IPM, 31(3):431-448, 1995. [5] J. Borda.",
        "M´emoire sur les ´elections au scrutin.",
        "Histoire de lAcad´emie des Sciences, 1781. [6] J. P. Callan, Z. Lu, and W. B. Croft.",
        "Searching distributed collections with inference networks.",
        "In Proceedings ACM-SIGIR95, pages 21-28, 1995. [7] M. Condorcet.",
        "Essai sur lapplication de lanalyse `a la probabilit´e des d´ecisions rendues `a la pluralit´e des voix.",
        "Imprimerie Royale, Paris, 1785. [8] W. D. Cook and M. Kress.",
        "Ordinal ranking with intensity of preference.",
        "Management Science, 31(1):26-32, 1985. [9] N. Craswell and D. Hawking.",
        "Overview of the TREC-2002 Web Track.",
        "In Proceedings TREC2002.",
        "NIST Publication, 2002. [10] N. Craswell and D. Hawking.",
        "Overview of the TREC-2004 Web Track.",
        "In Proceedings of TREC2004.",
        "NIST Publication, 2004. [11] C. Dwork, S. R. Kumar, M. Naor, and D. Sivakumar.",
        "Rank aggregation methods for the Web.",
        "In Proceedings WWW2001, pages 613-622, 2001. [12] R. Fagin.",
        "Combining fuzzy information from multiple systems.",
        "JCSS, 58(1):83-99, 1999. [13] R. Fagin, R. Kumar, M. Mahdian, D. Sivakumar, and E. Vee.",
        "Comparing and aggregating rankings with ties.",
        "In PODS, pages 47-58, 2004. [14] R. Fagin, R. Kumar, and D. Sivakumar.",
        "Comparing top k lists.",
        "SIAM J. on Discrete Mathematics, 17(1):134-160, 2003. [15] E. A.",
        "Fox and J.",
        "A. Shaw.",
        "Combination of multiple searches.",
        "In Proceedings of TREC3.",
        "NIST Publication, 1994. [16] J. Katzer, M. McGill, J. Tessier, W. Frakes, and P. DasGupta.",
        "A study of the overlap among document representations.",
        "Information Technology: Research and Development, 1(4):261-274, 1982. [17] L. S. Larkey, M. E. Connell, and J. Callan.",
        "Collection selection and results merging with topically organized U.S. patents and TREC data.",
        "In Proceedings ACM-CIKM2000, pages 282-289.",
        "ACM Press, 2000. [18] A.",
        "Le Calv´e and J. Savoy.",
        "Database merging strategy based on logistic regression.",
        "IPM, 36(3):341-359, 2000. [19] J. H. Lee.",
        "Analyses of multiple evidence combination.",
        "In Proceedings ACM-SIGIR97, pages 267-276, 1997. [20] D. Lillis, F. Toolan, R. Collier, and J. Dunnion.",
        "Probfuse: a probabilistic approach to data fusion.",
        "In Proceedings ACM-SIGIR2006, pages 139-146.",
        "ACM Press, 2006. [21] J. I. Marden.",
        "Analyzing and Modeling Rank Data.",
        "Number 64 in Monographs on Statistics and Applied Probability.",
        "Chapman & Hall, 1995. [22] M. Montague and J.",
        "A. Aslam.",
        "Metasearch consistency.",
        "In Proceedings ACM-SIGIR2001, pages 386-387.",
        "ACM Press, 2001. [23] D. M. Pennock and E. Horvitz.",
        "Analysis of the axiomatic foundations of collaborative filtering.",
        "In Workshop on AI for Electronic Commerce at the 16th National Conference on Artificial Intelligence, 1999. [24] M. E. Renda and U. Straccia.",
        "Web metasearch: rank vs. score based rank aggregation methods.",
        "In Proceedings ACM-SAC2003, pages 841-846.",
        "ACM Press, 2003. [25] W. H. Riker.",
        "Liberalism against populism.",
        "Waveland Press, 1982. [26] B. Roy.",
        "The outranking approach and the foundations of ELECTRE methods.",
        "Theory and Decision, 31:49-73, 1991. [27] B. Roy and J. Hugonnard.",
        "Ranking of suburban line extension projects on the Paris metro system by a multicriteria method.",
        "Transportation Research, 16A(4):301-312, 1982. [28] L. Si and J. Callan.",
        "Using sampled data and regression to merge search engine results.",
        "In Proceedings ACM-SIGIR2002, pages 19-26.",
        "ACM Press, 2002. [29] M. Truchon.",
        "An extension of the Condorcet criterion and Kemeny orders.",
        "Cahier 9813, Centre de Recherche en Economie et Finance Appliqu´ees, Oct. 1998. [30] H. Turtle and W. B. Croft.",
        "Inference networks for document retrieval.",
        "In Proceedings of ACM-SIGIR90, pages 1-24.",
        "ACM Press, 1990. [31] C. C. Vogt and G. W. Cottrell.",
        "Fusion via a linear combination of scores.",
        "Information Retrieval, 1(3):151-173, 1999."
    ],
    "error_count": 0,
    "keys": {
        "rank aggregation": {
            "translated_key": "agregación de rango",
            "is_in_text": true,
            "original_annotated_sentences": [
                "An Outranking Approach for <br>rank aggregation</br> in Information Retrieval Mohamed Farah Lamsade, Paris Dauphine University Place du Mal de Lattre de Tassigny 75775 Paris Cedex 16, France farah@lamsade.dauphine.fr Daniel Vanderpooten Lamsade, Paris Dauphine University Place du Mal de Lattre de Tassigny 75775 Paris Cedex 16, France vdp@lamsade.dauphine.fr ABSTRACT Research in Information Retrieval usually shows performance improvement when many sources of evidence are combined to produce a ranking of documents (e.g., texts, pictures, sounds, etc.).",
                "In this paper, we focus on the <br>rank aggregation</br> problem, also called data fusion problem, where rankings of documents, searched into the same collection and provided by multiple methods, are combined in order to produce a new ranking.",
                "In this context, we propose a <br>rank aggregation</br> method within a multiple criteria framework using aggregation mechanisms based on decision rules identifying positive and negative reasons for judging whether a document should get a better rank than another.",
                "We show that the proposed method deals well with the Information Retrieval distinctive features.",
                "Experimental results are reported showing that the suggested method performs better than the well-known CombSUM and CombMNZ operators.",
                "Categories and Subject Descriptors: H.3.3 [Information Systems]: Information Search and Retrieval - Retrieval models.",
                "General Terms: Algorithms, Measurement, Experimentation, Performance, Theory. 1.",
                "INTRODUCTION A wide range of current Information Retrieval (IR) approaches are based on various search models (Boolean, Vector Space, Probabilistic, Language, etc. [2]) in order to retrieve relevant documents in response to a user request.",
                "The result lists produced by these approaches depend on the exact definition of the relevance concept.",
                "<br>rank aggregation</br> approaches, also called data fusion approaches, consist in combining these result lists in order to produce a new and hopefully better ranking.",
                "Such approaches give rise to metasearch engines in the Web context.",
                "We consider, in the following, cases where only ranks are available and no other additional information is provided such as the relevance scores.",
                "This corresponds indeed to the reality, where only ordinal information is available.",
                "Data fusion is also relevant in other contexts, such as when the user writes several queries of his/her information need (e.g., a boolean query and a natural language query) [4], or when many document surrogates are available [16].",
                "Several studies argued that <br>rank aggregation</br> has the potential of combining effectively all the various sources of evidence considered in various input methods.",
                "For instance, experiments carried out in [16], [30], [4] and [19] showed that documents which appear in the lists of the majority of the input methods are more likely to be relevant.",
                "Moreover, Lee [19] and Vogt and Cottrell [31] found that various retrieval approaches often return very different irrelevant documents, but many of the same relevant documents.",
                "Bartell et al. [3] also found that <br>rank aggregation</br> methods improve the performances w.r.t. those of the input methods, even when some of them have weak individual performances.",
                "These methods also tend to smooth out biases of the input methods according to Montague and Aslam [22].",
                "Data fusion has recently been proved to improve performances for both the ad-hoc retrieval and categorization tasks within the TREC genomics track in 2005 [1].",
                "The <br>rank aggregation</br> problem was addressed in various fields such as i) in social choice theory which studies voting algorithms which specify winners of elections or winners of competitions in tournaments [29], ii) in statistics when studying correlation between rankings, iii) in distributed databases when results from different databases must be combined [12], and iv) in collaborative filtering [23].",
                "Most current <br>rank aggregation</br> methods consider each input ranking as a permutation over the same set of items.",
                "They also give rigid interpretation to the exact ranking of the items.",
                "Both of these assumptions are rather not valid in the IR context, as will be shown in the following sections.",
                "The remaining of the paper is organized as follows.",
                "We first review current <br>rank aggregation</br> methods in Section 2.",
                "Then we outline the specificities of the data fusion problem in the IR context (Section 3).",
                "In Section 4, we present a new aggregation method which is proven to best fit the IR context.",
                "Experimental results are presented in Section 5 and conclusions are provided in a final section. 2.",
                "RELATED WORK As pointed out by Riker [25], we can distinguish two families of <br>rank aggregation</br> methods: positional methods which assign scores to items to be ranked according to the ranks they receive and majoritarian methods which are based on pairwise comparisons of items to be ranked.",
                "These two families of methods find their roots in the pioneering works of Borda [5] and Condorcet [7], respectively, in the social choice literature. 2.1 Preliminaries We first introduce some basic notations to present the <br>rank aggregation</br> methods in a uniform way.",
                "Let D = {d1, d2, . . . , dnd } be a set of nd documents.",
                "A list or a ranking j is an ordering defined on Dj ⊆ D (j = 1, . . . , n).",
                "Thus, di j di means di is ranked better than di in j.",
                "When Dj = D, j is said to be a full list.",
                "Otherwise, it is a partial list.",
                "If di belongs to Dj, rj i denotes the rank or position of di in j.",
                "We assume that the best answer (document) is assigned the position 1 and the worst one is assigned the position |Dj|.",
                "Let D be the set of all permutations on D or all subsets of D. A profile is a n-tuple of rankings PR = ( 1, 2, . . . , n).",
                "Restricting PR to the rankings containing document di defines PRi.",
                "We also call the number of rankings which contain document di the rank hits of di [19].",
                "The <br>rank aggregation</br> or data fusion problem consists of finding a ranking function or mechanism Ψ (also called a social welfare function in the social choice theory terminology) defined by: Ψ : n D → D PR = ( 1, 2, . . . , n) → σ = Ψ(PR) where σ is called a consensus ranking. 2.2 Positional Methods 2.2.1 Borda Count This method [5] first assigns a score n j=1 rj i to each document di.",
                "Documents are then ranked by increasing order of this score, breaking ties, if any, arbitrarily. 2.2.2 Linear Combination Methods This family of methods basically combine scores of documents.",
                "When used for the <br>rank aggregation</br> problem, ranks are assumed to be scores or performances to be combined using aggregation operators such as the weighted sum or some variation of it [3, 31, 17, 28].",
                "For instance, Callan et al. [6] used the inference networks model [30] to combine rankings.",
                "Fox and Shaw [15] proposed several combination strategies which are CombSUM, CombMIN, CombMAX, CombANZ and CombMNZ.",
                "The first three operators correspond to the sum, min and max operators, respectively.",
                "CombANZ and CombMNZ respectively divides and multiplies the CombSUM score by the rank hits.",
                "It is shown in [19] that the CombSUM and CombMNZ operators perform better than the others.",
                "Metasearch engines such as SavvySearch and MetaCrawler use the CombSUM strategy to fuse rankings. 2.2.3 Footrule Optimal Aggregation In this method, a consensus ranking minimizes the Spearman footrule distance from the input rankings [21].",
                "Formally, given two full lists j and j , this distance is given by F( j, j ) = nd i=1 |rj i − rj i |.",
                "It extends to several lists as follows.",
                "Given a profile PR and a consensus ranking σ, the Spearman footrule distance of σ to PR is given by F(σ, PR) = n j=1 F(σ, j).",
                "Cook and Kress [8] proposed a similar method which consists in optimizing the distance D( j, j ) = 1 2 nd i,i =1 |rj i,i − rj i,i |, where rj i,i = rj i −rj i .",
                "This formulation has the advantage that it considers the intensity of preferences. 2.2.4 Probabilistic Methods This kind of methods assume that the performance of the input methods on a number of training queries is indicative of their future performance.",
                "During the training process, probabilities of relevance are calculated.",
                "For subsequent queries, documents are ranked based on these probabilities.",
                "For instance, in [20], each input ranking j is divided into a number of segments, and the conditional probability of relevance (R) of each document di depending on the segment k it occurs in, is computed, i.e. prob(R|di, k, j).",
                "For subsequent queries, the score of each document di is given by n j=1 prob(R|di,k, j ) k .",
                "Le Calve and Savoy [18] suggest using a logistic regression approach for combining scores.",
                "Training data is needed to infer the model parameters. 2.3 Majoritarian Methods 2.3.1 Condorcet Procedure The original Condorcet rule [7] specifies that a winner of the election is any item that beats or ties with every other item in a pairwise contest.",
                "Formally, let C(diσdi ) = { j∈ PR : di j di } be the coalition of rankings that are concordant with establishing diσdi , i.e. with the proposition di should be ranked better than di in the final ranking σ. di beats or ties with di iff |C(diσdi )| ≥ |C(di σdi)|.",
                "The repetitive application of the Condorcet algorithm can produce a ranking of items in a natural way: select the Condorcet winner, remove it from the lists, and repeat the previous two steps until there are no more documents to rank.",
                "Since there is not always Condorcet winners, variations of the Condorcet procedure have been developed within the multiple criteria decision aid theory, with methods such as ELECTRE [26]. 2.3.2 Kemeny Optimal Aggregation As in section 2.2.3, a consensus ranking minimizes a geometric distance from the input rankings, where the Kendall tau distance is used instead of the Spearman footrule distance.",
                "Formally, given two full lists j and j , the Kendall tau distance is given by K( j, j ) = |{(di, di ) : i < i , rj i < rj i , rj i > rj i }|, i.e. the number of pairwise disagreements between the two lists.",
                "It is easy to show that the consensus ranking corresponds to the geometric median of the input rankings and that the Kemeny optimal aggregation problem corresponds to the minimum feedback edge set problem. 2.3.3 Markov Chain Methods Markov chains (MCs) have been used by Dwork et al. [11] as a natural method to obtain a consensus ranking where states correspond to the documents to be ranked and the transition probabilities vary depending on the interpretation of the transition event.",
                "In the same reference, the authors proposed four specific MCs and experimental testing had shown that the following MC is the best performing one (see also [24]): • MC4: move from the current state di to the next state di by first choosing a document di uniformly from D. If for the majority of the rankings, we have rj i ≤ rj i , then move to di , else stay in di.",
                "The consensus ranking corresponds to the stationary distribution of MC4. 3.",
                "SPECIFICITIES OF THE <br>rank aggregation</br> PROBLEM IN THE IR CONTEXT 3.1 Limited Significance of the Rankings The exact positions of documents in one input ranking have limited significance and should not be overemphasized.",
                "For instance, having three relevant documents in the first three positions, any perturbation of these three items will have the same value.",
                "Indeed, in the IR context, the complete order provided by an input method may hide ties.",
                "In this case, we call such rankings semi orders.",
                "This was outlined in [13] as the problem of aggregation with ties.",
                "It is therefore important to build the consensus ranking based on robust information: • Documents with near positions in j are more likely to have similar interest or relevance.",
                "Thus a slight perturbation of the initial ranking is meaningless. • Assuming that document di is better ranked than document di in a ranking j, di is more likely to be definitively more relevant than di in j when the number of intermediate positions between di and di increases. 3.2 Partial Lists In real world applications, such as metasearch engines, rankings provided by the input methods are often partial lists.",
                "This was outlined in [14] as the problem of having to merge top-k results from various input lists.",
                "For instance, in the experiments carried out by Dwork et al. [11], authors found that among the top 100 best documents of 7 input search engines, 67% of the documents were present in only one search engine, whereas less than two documents were present in all the search engines.",
                "<br>rank aggregation</br> of partial lists raises four major difficulties which we state hereafter, proposing for each of them various working assumptions: 1.",
                "Partial lists can have various lengths, which can favour long lists.",
                "We thus consider the following two working hypotheses: H1 k : We only consider the top k best documents from each input ranking.",
                "H1 all: We consider all the documents from each input ranking. 2.",
                "Since there are different documents in the input rankings, we must decide which documents should be kept in the consensus ranking.",
                "Two working hypotheses are therefore considered: H2 k : We only consider documents which are present in at least k input rankings (k > 1).",
                "H2 all: We consider all the documents which are ranked in at least one input ranking.",
                "Hereafter, we call documents which will be retained in the consensus ranking, candidate documents, and documents that will be excluded from the consensus ranking, excluded documents.",
                "We also call a candidate document which is missing in one or more rankings, a missing document. 3.",
                "Some candidate documents are missing documents in some input rankings.",
                "Main reasons for a missing document are that it was not indexed or it was indexed but deemed irrelevant ; usually this information is not available.",
                "We consider the following two working hypotheses: H3 yes: Each missing document in each j is assigned a position.",
                "H3 no: No assumption is made, that is each missing document is considered neither better nor worse than any other document. 4.",
                "When assumption H2 k holds, each input ranking may contain documents which will not be considered in the consensus ranking.",
                "Regarding the positions of the candidate documents, we can consider the following working hypotheses: H4 init: The initial positions of candidate documents are kept in each input ranking.",
                "H4 new: Candidate documents receive new positions in each input ranking, after discarding excluded ones.",
                "In the IR context, <br>rank aggregation</br> methods need to decide more or less explicitly which assumptions to retain w.r.t. the above-mentioned difficulties. 4.",
                "OUTRANKING APPROACH FOR <br>rank aggregation</br> 4.1 Presentation Positional methods consider implicitly that the positions of the documents in the input rankings are scores giving thus a cardinal meaning to an ordinal information.",
                "This constitutes a strong assumption that is questionable, especially when the input rankings have different lengths.",
                "Moreover, for positional methods, assumptions H3 and H4 , which are often arbitrary, have a strong impact on the results.",
                "For instance, let us consider an input ranking of 500 documents out of 1000 candidate documents.",
                "Whether we assign to each of the missing documents the position 1, 501, 750 or 1000 -corresponding to variations of H3 yes- will give rise to very contrasted results, especially regarding the top of the consensus ranking.",
                "Majoritarian methods do not suffer from the above-mentioned drawbacks of the positional methods since they build consensus rankings exploiting only ordinal information contained in the input rankings.",
                "Nevertheless, they suppose that such rankings are complete orders, ignoring that they may hide ties.",
                "Therefore, majoritarian methods base consensus rankings on illusory discriminant information rather than less discriminant but more robust information.",
                "Trying to overcome the limits of current <br>rank aggregation</br> methods, we found that outranking approaches, which were initially used for multiple criteria aggregation problems [26], can also be used for the <br>rank aggregation</br> purpose, where each ranking plays the role of a criterion.",
                "Therefore, in order to decide whether a document di should be ranked better than di in the consensus ranking σ, the two following conditions should be met: • a concordance condition which ensures that a majority of the input rankings are concordant with diσdi (majority principle). • a discordance condition which ensures that none of the discordant input rankings strongly refutes dσd (respect of minorities principle).",
                "Formally, the concordance coalition with diσdi is Csp (diσdi ) = { j∈ PR : rj i ≤ rj i − sp} where sp is a preference threshold which is the variation of document positions -whether it is absolute or relative to the ranking length- which draws the boundaries between an indifference and a preference situation between documents.",
                "The discordance coalition with diσdi is Dsv (diσdi ) = { j∈ PR : rj i ≥ rj i + sv} where sv is a veto threshold which is the variation of document positions -whether it is absolute or relative to the ranking length- which draws the boundaries between a weak and a strong opposition to diσdi .",
                "Depending on the exact definition of the preceding concordance and discordance coalitions leading to the definition of some decision rules, several outranking relations can be defined.",
                "They can be more or less demanding depending on i) the values of the thresholds sp and sv, ii) the importance or minimal size cmin required for the concordance coalition, and iii) the importance or maximum size dmax of the discordance coalition.",
                "A generic outranking relation can thus be defined as follows: diS(sp,sv,cmin,dmax)di ⇔ |Csp (diσdi )| ≥ cmin AND |Dsv (diσdi )| ≤ dmax This expression defines a family of nested outranking relations since S(sp,sv,cmin,dmax) ⊆ S(sp,sv,cmin,dmax) when cmin ≥ cmin and/or dmax ≤ dmax and/or sp ≥ sp and/or sv ≤ sv.",
                "This expression also generalizes the majority rule which corresponds to the particular relation S(0,∞, n 2 ,n).",
                "It also satisfies important properties of <br>rank aggregation</br> methods, called neutrality, Pareto-optimality, Condorcet property and Extended Condorcet property, in the social choice literature [29].",
                "Outranking relations are not necessarily transitive and do not necessarily correspond to rankings since directed cycles may exist.",
                "Therefore, we need specific procedures in order to derive a consensus ranking.",
                "We propose the following procedure which finds its roots in [27].",
                "It consists in partitioning the set of documents into r ranked classes.",
                "Each class Ch contains documents with the same relevance and results from the application of all relations (if possible) to the set of documents remaining after previous classes are computed.",
                "Documents within the same equivalence class are ranked arbitrarily.",
                "Formally, let • R be the set of candidate documents for a query, • S1 , S2 , . . . be a family of nested outranking relations, • Fk(di, E) = |{di ∈ E : diSk di }| be the number of documents in E(E ⊆ R) that could be considered worse than di according to relation Sk , • fk(di, E) = |{di ∈ E : di Sk di}| be the number of documents in E that could be considered better than di according to Sk , • sk(di, E) = Fk(di, E) − fk(di, E) be the qualification of di in E according to Sk .",
                "Each class Ch results from a distillation process.",
                "It corresponds to the last distillate of a series of sets E0 ⊇ E1 ⊇ . . . where E0 = R \\ (C1 ∪ . . . ∪ Ch−1) and Ek is a reduced subset of Ek−1 resulting from the application of the following procedure: 1. compute for each di ∈ Ek−1 its qualification according to Sk , i.e. sk(di, Ek−1), 2. define smax = maxdi∈Ek−1 {sk(di, Ek−1)}, then 3.",
                "Ek = {di ∈ Ek−1 : sk(di, Ek−1) = smax} When one outranking relation is used, the distillation process stops after the first application of the previous procedure, i.e., Ch corresponds to distillate E1.",
                "When different outranking relations are used, the distillation process stops when all the pre-defined outranking relations have been used or when |Ek| = 1. 4.2 Illustrative Example This section illustrates the concepts and procedures of section 4.1.",
                "Let us consider a set of candidate documents R = {d1, d2, d3, d4, d5}.",
                "The following table gives a profile PR of different rankings of the documents of R: PR = ( 1 , 2, 3, 4).",
                "Table 1: Rankings of documents rj i 1 2 3 4 d1 1 3 1 5 d2 2 1 3 3 d3 3 2 2 1 d4 4 4 5 2 d5 5 5 4 4 Let us suppose that the preference and veto thresholds are set to values 1 and 4 respectively, and that the concordance and discordance thresholds are set to values 2 and 1 respectively.",
                "The following tables give the concordance, discordance and outranking matrices.",
                "Each entry csp (di, di ) (dsv (di, di )) in the concordance (discordance) matrix gives the number of rankings that are concordant (discordant) with diσdi , i.e. csp (di, di ) = |Csp (diσdi )| and dsv (di, di ) = |Dsv (diσdi )|.",
                "Table 2: Computation of the outranking relation d1 d2 d3 d4 d5 d1 - 2 2 3 3 d2 2 - 2 3 4 d3 2 2 - 4 4 d4 1 1 0 - 3 d5 1 0 0 1Concordance Matrix d1 d2 d3 d4 d5 d1 - 0 1 0 0 d2 0 - 0 0 0 d3 0 0 - 0 0 d4 1 0 0 - 0 d5 1 1 0 0Discordance Matrix d1 d2 d3 d4 d5 d1 - 1 1 1 1 d2 1 - 1 1 1 d3 1 1 - 1 1 d4 0 0 0 - 1 d5 0 0 0 0Outranking Matrix (S1) For instance, the concordance coalition for the assertion d1σd4 is C1(d1σd4) = { 1, 2, 3} and the discordance coalition for the same assertion is D4(d1σd4) = ∅.",
                "Therefore, c1(d1, d4) = 3, d4(d1, d4) = 0 and d1S1 d4 holds.",
                "Notice that Fk(di, R) (fk(di, R)) is given by summing the values of the ith row (column) of the outranking matrix.",
                "The consensus ranking is obtained as follows: to get the first class C1, we compute the qualifications of all the documents of E0 = R with respect to S1 .",
                "They are respectively 2, 2, 2, -2 and -4.",
                "Therefore smax equals 2 and C1 = E1 = {d1, d2, d3}.",
                "Observe that, if we had used a second outranking relation S2(⊇ S1), these three documents could have been possibly discriminated.",
                "At this stage, we remove documents of C1 from the outranking matrix and compute the next class C2: we compute the new qualifications of the documents of E0 = R \\ C1 = {d4, d5}.",
                "They are respectively 1 and -1.",
                "So C3 = E1 = {d4}.",
                "The last document d5 is the only document of the last class C3.",
                "Thus, the consensus ranking is {d1, d2, d3} → {d4} → {d5}. 5.",
                "EXPERIMENTS AND RESULTS 5.1 Test Setting To facilitate empirical investigation of the proposed methodology, we developed a prototype metasearch engine that implements a version of our outranking approach for <br>rank aggregation</br>.",
                "In this paper, we apply our approach to the Topic Distillation (TD) task of TREC-2004 Web track [10].",
                "In this task, there are 75 topics where only a short description of each is given.",
                "For each query, we retained the rankings of the 10 best runs of the TD task which are provided by TREC-2004 participating teams.",
                "The performances of these runs are reported in table 3.",
                "Table 3: Performances of the 10 best runs of the TD task of TREC-2004 Run Id MAP P@10 S@1 S@5 S@10 uogWebCAU150 17.9% 24.9% 50.7% 77.3% 89.3% MSRAmixed1 17.8% 25.1% 38.7% 72.0% 88.0% MSRC04C12 16.5% 23.1% 38.7% 74.7% 80.0% humW04rdpl 16.3% 23.1% 37.3% 78.7% 90.7% THUIRmix042 14.7% 20.5% 21.3% 58.7% 74.7% UAmsT04MWScb 14.6% 20.9% 36.0% 66.7% 76.0% ICT04CIIS1AT 14.1% 20.8% 33.3% 64.0% 78.7% SJTUINCMIX5 12.9% 18.9% 29.3% 57.3% 72.0% MU04web1 11.5% 19.9% 33.3% 64.0% 76.0% MeijiHILw3 11.5% 15.3% 30.7% 54.7% 64.0% Average 14.7% 21.2% 34.9% 66.8% 78.94% For each query, each run provides a ranking of about 1000 documents.",
                "The number of documents retrieved by all these runs ranges from 543 to 5769.",
                "Their average (median) number is 3340 (3386).",
                "It is worth noting that we found similar distributions of the documents among the rankings as in [11].",
                "For evaluation, we used the trec eval standard tool which is used by the TREC community to calculate the standard measures of system effectiveness which are Mean Average Precision (MAP) and Success@n (S@n) for n=1, 5 and 10.",
                "Our approach effectiveness is compared against some high performing official results from TREC-2004 as well as against some standard <br>rank aggregation</br> algorithms.",
                "In the experiments, significance testing is mainly based on the t-student statistic which is computed on the basis of the MAP values of the compared runs.",
                "In the tables of the following section, statistically significant differences are marked with an asterisk.",
                "Values between brackets of the first column of each table, indicate the parameter value of the corresponding run. 5.2 Results We carried out several series of runs in order to i) study performance variations of the outranking approach when tuning the parameters and working assumptions, ii) compare performances of the outranking approach vs standard <br>rank aggregation</br> strategies , and iii) check whether <br>rank aggregation</br> performs better than the best input rankings.",
                "We set our basic run mcm with the following parameters.",
                "We considered that each input ranking is a complete order (sp = 0) and that an input ranking strongly refutes diσdi when the difference of both document positions is large enough (sv = 75%).",
                "Preference and veto thresholds are computed proportionally to the number of documents retained in each input ranking.",
                "They consequently may vary from one ranking to another.",
                "In addition, to accept the assertion diσdi , we supposed that the majority of the rankings must be concordant (cmin = 50%) and that every input ranking can impose its veto (dmax = 0).",
                "Concordance and discordance thresholds are computed for each tuple (di, di ) as the percentage of the input rankings of PRi ∩PRi .",
                "Thus, our choice of parameters leads to the definition of the outranking relation S(0,75%,50%,0).",
                "To test the run mcm, we had chosen the following assumptions.",
                "We retained the top 100 best documents from each input ranking (H1 100), only considered documents which are present in at least half of the input rankings (H2 5 ) and assumed H3 no and H4 new.",
                "In these conditions, the number of successful documents was about 100 on average, and the computation time per query was less than one second.",
                "Obviously, modifying the working assumptions should have deeper impact on the performances than tuning our model parameters.",
                "This was validated by preliminary experiments.",
                "Thus, we hereafter begin by studying performance variation when different sets of assumptions are considered.",
                "Afterwards, we study the impact of tuning parameters.",
                "Finally, we compare our model performances w.r.t. the input rankings as well as some standard data fusion algorithms. 5.2.1 Impact of the Working Assumptions Table 4 summarizes the performance variation of the outranking approach under different working hypotheses.",
                "In Table 4: Impact of the working assumptions Run Id MAP S@1 S@5 S@10 mcm 18.47% 41.33% 81.33% 86.67% mcm22 (H3 yes) 17.72% (-4.06%) 34.67% 81.33% 86.67% mcm23 (H4 init) 18.26% (-1.14%) 41.33% 81.33% 86.67% mcm24 (H1 all) 20.67% (+11.91%*) 38.66% 80.00% 86.66% mcm25 (H2 all) 21.68% (+17.38%*) 40.00% 78.66% 89.33% this table, we first show that run mcm22, in which missing documents are all put in the same last position of each input ranking, leads to performance drop w.r.t. run mcm.",
                "Moreover, S@1 moves from 41.33% to 34.67% (-16.11%).",
                "This shows that several relevant documents which were initially put at the first position of the consensus ranking in mcm, lose this first position but remain ranked in the top 5 documents since S@5 did not change.",
                "We also conclude that documents which have rather good positions in some input rankings are more likely to be relevant, even though they are missing in some other rankings.",
                "Consequently, when they are missing in some rankings, assigning worse ranks to these documents is harmful for performance.",
                "Also, from Table 4, we found that the performances of runs mcm and mcm23 are similar.",
                "Therefore, the outranking approach is not sensitive to keeping the initial positions of candidate documents or recomputing them by discarding excluded ones.",
                "From the same Table 4, performance of the outranking approach increases significantly for runs mcm24 and mcm25.",
                "Therefore, whether we consider all the documents which are present in half of the rankings (mcm24) or we consider all the documents which are ranked in the first 100 positions in one or more rankings (mcm25), increases performances.",
                "This result was predictable since in both cases we have more detailed information on the relative importance of documents.",
                "Tables 5 and 6 confirm this evidence.",
                "Table 5, where values between brackets of the first column give the number of documents which are retained from each input ranking, shows that selecting more documents from each input ranking leads to performance increase.",
                "It is worth mentioning that selecting more than 600 documents from each input ranking does not improve performance.",
                "Table 5: Impact of the number of retained documents Run Id MAP S@1 S@5 S@10 mcm (100) 18.47% 41.33% 81.33% 86.67% mcm24-1 (200) 19.32% (+4.60%) 42.67% 78.67% 88.00% mcm24-2 (400) 19.88% (+7.63%*) 37.33% 80.00% 88.00% mcm24-3 (600) 20.80% (+12.62%*) 40.00% 80.00% 88.00% mcm24-4 (800) 20.66% (+11.86%*) 40.00% 78.67% 86.67% mcm24 (1000) 20.67% (+11.91%*) 38.66% 80.00% 86.66% Table 6 reports runs corresponding to variations of H2 k .",
                "Values between brackets are rank hits.",
                "For instance, in the run mcm32, only documents which are present in 3 or more input rankings, were considered successful.",
                "This table shows that performance is significantly better when rare documents are considered, whereas it decreases significantly when these documents are discarded.",
                "Therefore, we conclude that many of the relevant documents are retrieved by a rather small set of IR models.",
                "Table 6: Performance considering different rank hits Run Id MAP S@1 S@5 S@10 mcm25 (1) 21.68% (+17.38%*) 40.00% 78.67% 89.33% mcm32 (3) 18.98% (+2.76%) 38.67% 80.00% 85.33% mcm (5) 18.47% 41.33% 81.33% 86.67% mcm33 (7) 15.83% (-14.29%*) 37.33% 78.67% 85.33% mcm34 (9) 10.96% (-40.66%*) 36.11% 66.67% 70.83% mcm35 (10) 7.42% (-59.83%*) 39.22% 62.75% 64.70% For both runs mcm24 and mcm25, the number of successful documents was about 1000 and therefore, the computation time per query increased and became around 5 seconds. 5.2.2 Impact of the Variation of the Parameters Table 7 shows performance variation of the outranking approach when different preference thresholds are considered.",
                "We found performance improvement up to threshold values of about 5%, then there is a decrease in the performance which becomes significant for threshold values greater than 10%.",
                "Moreover, S@1 improves from 41.33% to 46.67% when preference threshold changes from 0 to 5%.",
                "We can thus conclude that the input rankings are semi orders rather than complete orders.",
                "Table 8 shows the evolution of the performance measures w.r.t. the concordance threshold.",
                "We can conclude that in order to put document di before di in the consensus ranking, Table 7: Impact of the variation of the preference threshold from 0 to 12.5% Run Id MAP S@1 S@5 S@10 mcm (0%) 18.47% 41.33% 81.33% 86.67% mcm1 (1%) 18.57% (+0.54%) 41.33% 81.33% 86.67% mcm2 (2.5%) 18.63% (+0.87%) 42.67% 78.67% 86.67% mcm3 (5%) 18.69% (+1.19%) 46.67% 81.33% 86.67% mcm4 (7.5%) 18.24% (-1.25%) 46.67% 81.33% 86.67% mcm5 (10%) 17.93% (-2.92%) 40.00% 82.67% 86.67% mcm5b (12.5%) 17.51% (-5.20%*) 41.33% 80.00% 86.67% at least half of the input rankings of PRi ∩ PRi should be concordant.",
                "Performance drops significantly for very low and very high values of the concordance threshold.",
                "In fact, for such values, the concordance condition is either fulfilled rather always by too many document pairs or not fulfilled at all, respectively.",
                "Therefore, the outranking relation becomes either too weak or too strong respectively.",
                "Table 8: Impact of the variation of cmin Run Id MAP S@1 S@5 S@10 mcm11 (20%) 17.63% (-4.55%*) 41.33% 76.00% 85.33% mcm12 (40%) 18.37% (-0.54%) 42.67% 76.00% 86.67% mcm (50%) 18.47% 41.33% 81.33% 86.67% mcm13 (60%) 18.42% (-0.27%) 40.00% 78.67% 86.67% mcm14 (80%) 17.43% (-5.63%*) 40.00% 78.67% 86.67% mcm15 (100%) 16.12% (-12.72%*) 41.33% 70.67% 85.33% In the experiments, varying the veto threshold as well as the discordance threshold within reasonable intervals does not have significant impact on performance measures.",
                "In fact, runs with different veto thresholds (sv ∈ [50%; 100%]) had similar performances even though there is a slight advantage for runs with high threshold values which means that it is better not to allow the input rankings to put their veto easily.",
                "Also, tuning the discordance threshold was carried out for values 50% and 75% of the veto threshold.",
                "For these runs we did not get any noticeable performance variation, although for low discordance thresholds (dmax < 20%), performance slightly decreased. 5.2.3 Impact of the Variation of the Number of Input Rankings To study performance evolution when different sets of input rankings are considered, we carried three more runs where 2, 4, and 6 of the best performing sets of the input rankings are considered.",
                "Results reported in Table 9 are seemingly counter-intuitive and also do not support previous findings regarding <br>rank aggregation</br> research [3].",
                "Nevertheless, this result shows that low performing rankings bring more noise than information to the establishment of the consensus ranking.",
                "Therefore, when they are considered, performance decreases.",
                "Table 9: Performance considering different best performing sets of input rankings Run Id MAP S@1 S@5 S@10 mcm (10) 18.47% 41.33% 81.33% 86.67% mcm27 (6) 18.60% (+0.70%) 41.33% 80.00% 85.33% mcm28 (4) 19.02% (+2.98%) 40.00% 86.67% 88.00% mcm29 (2) 18.33% (-0.76%) 44.00% 76.00% 88.00% 5.2.4 Comparison of the Performance of Different <br>rank aggregation</br> Methods In this set of runs, we compare the outranking approach with some standard <br>rank aggregation</br> methods which were proven to have acceptable performance in previous studies: we considered two positional methods which are the CombSUM and the CombMNZ strategies.",
                "We also examined the performance of one majoritarian method which is the Markov chain method (MC4).",
                "For the comparisons, we considered a specific outranking relation S∗ = S(5%,50%,50%,30%) which results in good overall performances when tuning all the parameters.",
                "The first row of Table 10 gives performances of the <br>rank aggregation</br> methods w.r.t. a basic assumption set A1 = (H1 100, H2 5 , H4 new): we only consider the 100 first documents from each ranking, then retain documents present in 5 or more rankings and update ranks of successful documents.",
                "For positional methods, we place missing documents at the queue of the ranking (H3 yes) whereas for our method as well as for MC4, we retained hypothesis H3 no.",
                "The three following rows of Table 10 report performances when changing one element from the basic assumption set: the second row corresponds to the assumption set A2 = (H1 1000, H2 5 , H4 new), i.e. changing the number of retained documents from 100 to 1000.",
                "The third row corresponds to the assumption set A3 = (H1 100, H2 all, H4 new), i.e. considering the documents present in at least one ranking.",
                "The fourth row corresponds to the assumption set A4 = (H1 100, H2 5 , H4 init), i.e. keeping the original ranks of successful documents.",
                "The fifth row of Table 10, labeled A5, gives performance when all the 225 queries of the Web track of TREC-2004 are considered.",
                "Obviously, performance level cannot be compared with previous lines since the additional queries are different from the TD queries and correspond to other tasks (Home Page and Named Page tasks [10]) of TREC-2004 Web track.",
                "This set of runs aims to show whether relative performance of the various methods is task-dependent.",
                "The last row of Table 10, labeled A6, reports performance of the various methods considering the TD task of TREC2002 instead of TREC-2004: we fused the results of input rankings of the 10 best official runs for each of the 50 TD queries [9] considering the set of assumptions A1 of the first row.",
                "This aims to show whether relative performance of the various methods changes from year to year.",
                "Values between brackets of Table 10 are variations of performance of each <br>rank aggregation</br> method w.r.t. performance of the outranking approach.",
                "Table 10: Performance (MAP) of different <br>rank aggregation</br> methods under 3 different test collections mcm combSUM combMNZ markov A1 18.79% 17.54% (-6.65%*) 17.08% (-9.10%*) 18.63% (-0.85%) A2 21.36% 19.18% (-10.21%*) 18.61% (-12.87%*) 21.33% (-0.14%) A3 21.92% 21.38% (-2.46%) 20.88% (-4.74%) 19.35% (-11.72%*) A4 18.64% 17.58% (-5.69%*) 17.18% (-7.83%*) 18.63% (-0.05%) A5 55.39% 52.16% (-5.83%*) 49.70% (-10.27%*) 53.30% (-3.77%) A6 16.95% 15.65% (-7.67%*) 14.57% (-14.04%*) 16.39% (-3.30%) From the analysis of table 10 the following can be established: • for all the runs, considering all the documents in each input ranking (A2) significantly improves performance (MAP increases by 11.62% on average).",
                "This is predictable since some initially unreported relevant documents would receive better positions in the consensus ranking. • for all the runs, considering documents even those present in only one input ranking (A3) significantly improves performance.",
                "For mcm, combSUM and combMNZ, performance improvement is more important (MAP increases by 20.27% on average) than for the markov run (MAP increases by 3.86%). • preserving the initial positions of documents (A4) or recomputing them (A1) does not have a noticeable influence on performance for both positional and majoritarian methods. • considering all the queries of the Web track of TREC2004 (A5) as well as the TD queries of the Web track of TREC-2002 (A6) does not alter the relative performance of the different data fusion methods. • considering the TD queries of the Web track of TREC2002, performances of all the data fusion methods are lower than that of the best performing input ranking for which the MAP value equals 18.58%.",
                "This is because most of the fused input rankings have very low performances compared to the best one, which brings more noise to the consensus ranking. • performances of the data fusion methods mcm and markov are significantly better than that of the best input ranking uogWebCAU150.",
                "This remains true for runs combSUM and combMNZ only under assumptions H1 all or H2 all.",
                "This shows that majoritarian methods are less sensitive to assumptions than positional methods. • outranking approach always performs significantly better than positional methods combSUM and combMNZ.",
                "It has also better performances than the Markov chain method, especially under assumption H2 all where difference of performances becomes significant. 6.",
                "CONCLUSIONS In this paper, we address the <br>rank aggregation</br> problem where different, but not disjoint, lists of documents are to be fused.",
                "We noticed that the input rankings can hide ties, so they should not be considered as complete orders.",
                "Only robust information should be used from each input ranking.",
                "Current <br>rank aggregation</br> methods, and especially positional methods (e.g. combSUM [15]), are not initially designed to work with such rankings.",
                "They should be adapted by considering specific working assumptions.",
                "We propose a new outranking method for <br>rank aggregation</br> which is well adapted to the IR context.",
                "Indeed, it ranks two documents w.r.t. the intensity of their positions difference in each input ranking and also considering the number of the input rankings that are concordant and discordant in favor of a specific document.",
                "There is also no need to make specific assumptions on the positions of the missing documents.",
                "This is an important feature since the absence of a document from a ranking should not be necessarily interpreted negatively.",
                "Experimental results show that the outranking method significantly out-performs popular classical positional data fusion methods like combSUM and combMNZ strategies.",
                "It also out-performs a good performing majoritarian methods which is the Markov chain method.",
                "These results are tested against different test collections and queries.",
                "From the experiments, we can also conclude that in order to improve the performances, we should fuse result lists of well performing IR models, and that majoritarian data fusion methods perform better than positional methods.",
                "The proposed method can have a real impact on Web metasearch performances since only ranks are available from most primary search engines, whereas most of the current approaches need scores to merge result lists into one single list.",
                "Further work involves investigating whether the outranking approach performs well in various other contexts, e.g. using the document scores or some combination of document ranks and scores.",
                "Acknowledgments The authors would like to thank Jacques Savoy for his valuable comments on a preliminary version of this paper. 7.",
                "REFERENCES [1] A. Aronson, D. Demner-Fushman, S. Humphrey, J. Lin, H. Liu, P. Ruch, M. Ruiz, L. Smith, L. Tanabe, and W. Wilbur.",
                "Fusion of knowledge-intensive and statistical approaches for retrieving and annotating textual genomics documents.",
                "In Proceedings TREC2005.",
                "NIST Publication, 2005. [2] R. A. Baeza-Yates and B.",
                "A. Ribeiro-Neto.",
                "Modern Information Retrieval.",
                "ACM Press , 1999. [3] B. T. Bartell, G. W. Cottrell, and R. K. Belew.",
                "Automatic combination of multiple ranked retrieval systems.",
                "In Proceedings ACM-SIGIR94, pages 173-181.",
                "Springer-Verlag, 1994. [4] N. J. Belkin, P. Kantor, E. A.",
                "Fox, and J.",
                "A. Shaw.",
                "Combining evidence of multiple query representations for information retrieval.",
                "IPM, 31(3):431-448, 1995. [5] J. Borda.",
                "M´emoire sur les ´elections au scrutin.",
                "Histoire de lAcad´emie des Sciences, 1781. [6] J. P. Callan, Z. Lu, and W. B. Croft.",
                "Searching distributed collections with inference networks.",
                "In Proceedings ACM-SIGIR95, pages 21-28, 1995. [7] M. Condorcet.",
                "Essai sur lapplication de lanalyse `a la probabilit´e des d´ecisions rendues `a la pluralit´e des voix.",
                "Imprimerie Royale, Paris, 1785. [8] W. D. Cook and M. Kress.",
                "Ordinal ranking with intensity of preference.",
                "Management Science, 31(1):26-32, 1985. [9] N. Craswell and D. Hawking.",
                "Overview of the TREC-2002 Web Track.",
                "In Proceedings TREC2002.",
                "NIST Publication, 2002. [10] N. Craswell and D. Hawking.",
                "Overview of the TREC-2004 Web Track.",
                "In Proceedings of TREC2004.",
                "NIST Publication, 2004. [11] C. Dwork, S. R. Kumar, M. Naor, and D. Sivakumar.",
                "<br>rank aggregation</br> methods for the Web.",
                "In Proceedings WWW2001, pages 613-622, 2001. [12] R. Fagin.",
                "Combining fuzzy information from multiple systems.",
                "JCSS, 58(1):83-99, 1999. [13] R. Fagin, R. Kumar, M. Mahdian, D. Sivakumar, and E. Vee.",
                "Comparing and aggregating rankings with ties.",
                "In PODS, pages 47-58, 2004. [14] R. Fagin, R. Kumar, and D. Sivakumar.",
                "Comparing top k lists.",
                "SIAM J. on Discrete Mathematics, 17(1):134-160, 2003. [15] E. A.",
                "Fox and J.",
                "A. Shaw.",
                "Combination of multiple searches.",
                "In Proceedings of TREC3.",
                "NIST Publication, 1994. [16] J. Katzer, M. McGill, J. Tessier, W. Frakes, and P. DasGupta.",
                "A study of the overlap among document representations.",
                "Information Technology: Research and Development, 1(4):261-274, 1982. [17] L. S. Larkey, M. E. Connell, and J. Callan.",
                "Collection selection and results merging with topically organized U.S. patents and TREC data.",
                "In Proceedings ACM-CIKM2000, pages 282-289.",
                "ACM Press, 2000. [18] A.",
                "Le Calv´e and J. Savoy.",
                "Database merging strategy based on logistic regression.",
                "IPM, 36(3):341-359, 2000. [19] J. H. Lee.",
                "Analyses of multiple evidence combination.",
                "In Proceedings ACM-SIGIR97, pages 267-276, 1997. [20] D. Lillis, F. Toolan, R. Collier, and J. Dunnion.",
                "Probfuse: a probabilistic approach to data fusion.",
                "In Proceedings ACM-SIGIR2006, pages 139-146.",
                "ACM Press, 2006. [21] J. I. Marden.",
                "Analyzing and Modeling Rank Data.",
                "Number 64 in Monographs on Statistics and Applied Probability.",
                "Chapman & Hall, 1995. [22] M. Montague and J.",
                "A. Aslam.",
                "Metasearch consistency.",
                "In Proceedings ACM-SIGIR2001, pages 386-387.",
                "ACM Press, 2001. [23] D. M. Pennock and E. Horvitz.",
                "Analysis of the axiomatic foundations of collaborative filtering.",
                "In Workshop on AI for Electronic Commerce at the 16th National Conference on Artificial Intelligence, 1999. [24] M. E. Renda and U. Straccia.",
                "Web metasearch: rank vs. score based <br>rank aggregation</br> methods.",
                "In Proceedings ACM-SAC2003, pages 841-846.",
                "ACM Press, 2003. [25] W. H. Riker.",
                "Liberalism against populism.",
                "Waveland Press, 1982. [26] B. Roy.",
                "The outranking approach and the foundations of ELECTRE methods.",
                "Theory and Decision, 31:49-73, 1991. [27] B. Roy and J. Hugonnard.",
                "Ranking of suburban line extension projects on the Paris metro system by a multicriteria method.",
                "Transportation Research, 16A(4):301-312, 1982. [28] L. Si and J. Callan.",
                "Using sampled data and regression to merge search engine results.",
                "In Proceedings ACM-SIGIR2002, pages 19-26.",
                "ACM Press, 2002. [29] M. Truchon.",
                "An extension of the Condorcet criterion and Kemeny orders.",
                "Cahier 9813, Centre de Recherche en Economie et Finance Appliqu´ees, Oct. 1998. [30] H. Turtle and W. B. Croft.",
                "Inference networks for document retrieval.",
                "In Proceedings of ACM-SIGIR90, pages 1-24.",
                "ACM Press, 1990. [31] C. C. Vogt and G. W. Cottrell.",
                "Fusion via a linear combination of scores.",
                "Information Retrieval, 1(3):151-173, 1999."
            ],
            "original_annotated_samples": [],
            "translated_annotated_samples": [
                "An Outranking Approach for \"rank aggregation\" in Information Retrieval Mohamed Farah Lamsade, Paris Dauphine University Place du Mal de Lattre de Tassigny 75775 Paris Cedex 16, France farah@lamsade.dauphine.fr Daniel Vanderpooten Lamsade, Paris Dauphine University Place du Mal de LattreDe Tassigny 75775 Paris Cedex 16, Francia vdp@lamsade.dauphine.fr Resumen La investigación en recuperación de información generalmente muestra una mejora del rendimiento cuando muchas fuentes de evidencia se combinan para producir una clasificación de documentos (por ejemplo, textos, imágenes, sonidos, etc.).",
                "En este documento, nos centramos en el problema de \"agregación de rango\", también llamado problema de fusión de datos, donde las clasificaciones de documentos, buscadas en la misma colección y proporcionadas por múltiples métodos, se combinan para producir una nueva clasificación.",
                "En este contexto, proponemos un método de \"agregación de rango\" dentro de un marco de criterios múltiples utilizando mecanismos de agregación basados en reglas de decisión que identifican razones positivas y negativas para juzgar si un documento debería obtener un rango mejor que otro.",
                "Los enfoques de \"agregación de rango\", también llamados enfoques de fusión de datos, consisten en combinar estas listas de resultados para producir una clasificación nueva y con suerte mejor.",
                "Varios estudios argumentaron que la \"agregación de rango\" tiene el potencial de combinar efectivamente todas las diversas fuentes de evidencia consideradas en varios métodos de entrada.",
                "Bartell et al.[3] también encontró que los métodos de \"agregación de rango\" mejoran los rendimientos W.R.T.Los de los métodos de entrada, incluso cuando algunos de ellos tienen actuaciones individuales débiles.",
                "El problema de \"agregación de rango\" se abordó en varios campos como I) en la teoría de la elección social que estudia algoritmos de votación que especifican a los ganadores de elecciones o ganadores de competiciones en torneos [29], ii) en estadísticas al estudiar la correlación entre las clasificaciones, III)en bases de datos distribuidas cuando los resultados de diferentes bases de datos deben combinarse [12] e iv) en el filtrado colaborativo [23].",
                "La mayoría de los métodos actuales de \"agregación de rango\" consideran cada clasificación de entrada como una permutación sobre el mismo conjunto de elementos.",
                "Primero revisamos los métodos actuales de \"agregación de rango\" en la Sección 2.",
                "Trabajo relacionado según lo señalado por Riker [25], podemos distinguir dos familias de métodos de \"agregación de rango\": métodos posicionales que asignan puntajes a los elementos que se clasificarán de acuerdo con las filas que reciben y los métodos mayores que se basan en las comparaciones por pares de elementosser clasificado.",
                "Estas dos familias de métodos encuentran sus raíces en las obras pioneras de Borda [5] y Condorcet [7], respectivamente, en la literatura de elección social.2.1 Preliminares Primero presentamos algunas anotaciones básicas para presentar los métodos de \"agregación de rango\" de manera uniforme.",
                "El problema de \"agregación de rango\" o fusión de datos consiste en encontrar una función o mecanismo de clasificación ψ (también llamado función de bienestar social en la terminología de la teoría de la elección social) definida por: ψ: n d → d pr = (1, 2, .. ..., n) → σ = ψ (pr) donde σ se llama clasificación de consenso.2.2 Métodos posicionales 2.2.1 Borda Cuenta este método [5] primero asigna una puntuación n j = 1 rj I a cada documento di.",
                "Cuando se usa para el problema de \"agregación de rango\", se supone que los rangos son puntajes o rendimientos para combinar utilizando operadores de agregación, como la suma ponderada o alguna variación de ella [3, 31, 17, 28].",
                "Especificidades del problema de \"agregación de rango\" en el contexto IR 3.1 Importancia limitada de las clasificaciones Las posiciones exactas de los documentos en una clasificación de entrada tienen un significado limitado y no deben enfatizarse demasiado.",
                "La \"agregación de rango\" de las listas parciales plantea cuatro dificultades principales que declaramos en adelante, proponiendo para cada uno de ellos varios supuestos de trabajo: 1.",
                "En el contexto IR, los métodos de \"agregación de rango\" deben decidir más o menos explícitamente qué supuestos retener a W.R.T.Las dificultades mencionadas anteriormente.4.",
                "Enfoque de supervisión para la \"agregación de rango\" 4.1 Métodos posicionales de presentación consideran implícitamente que las posiciones de los documentos en las clasificaciones de entrada son puntajes que otorgan un significado cardinal a una información ordinal.",
                "Al tratar de superar los límites de los métodos actuales de \"agregación de rango\", encontramos que los enfoques de supervisión, que se usaron inicialmente para los problemas de agregación de criterios múltiples [26], también pueden usarse para el propósito de \"agregación de rango\", donde cada clasificación juega el papelde un criterio.",
                "También satisface propiedades importantes de los métodos de \"agregación de rango\", llamados neutralidad, optimidad de pareto, propiedad de condorcet y propiedad de condorcet extendido, en la literatura de elección social [29].",
                "Experimentos y resultados 5.1 Configuración de prueba Para facilitar la investigación empírica de la metodología propuesta, desarrollamos un prototipo de motor MetaSearch que implementa una versión de nuestro enfoque supervisado para la \"agregación de rango\".",
                "Nuestra efectividad del enfoque se compara con algunos resultados oficiales de alto rendimiento de TREC-2004, así como contra algunos algoritmos estándar de \"agregación de rango\".",
                "Los valores entre los soportes de la primera columna de cada tabla, indiquen el valor del parámetro de la ejecución correspondiente.5.2 Resultados Llevamos a cabo varias series de ejecuciones para i) estudiar variaciones de rendimiento del enfoque superpuesto al ajustar los parámetros y los supuestos de trabajo, ii) comparar las actuaciones del enfoque superpuesto frente a las estrategias estándar de \"agregación de rango\", y iii).La \"agregación de rango\" funciona mejor que las mejores clasificaciones de entrada.",
                "Los resultados informados en la Tabla 9 son aparentemente contra-intuitivos y tampoco apoyan los hallazgos anteriores con respecto a la investigación de \"agregación de rango\" [3].",
                "Tabla 9: Rendimiento considerando diferentes conjuntos de mejor rendimiento de clasificaciones de entrada Run Map de identificación S@1 S@5 S@10 McM (10) 18.47% 41.33% 81.33% 86.67% MCM27 (6) 18.60% (+0.70%) 41.33% 80.00% 85.33% MCM28 (4) 19.02% (+2.98%) 40.00% 86.67% 88.00% MCM29 (2) 18.33% (-0.76%) 44.00% 76.00% 88.00% 5.2.4 Comparación del rendimiento de la diferente \"agregación de rango\" \"Métodos En este conjunto de ejecuciones, comparamos el enfoque de supervisión con algunos métodos estándar de \"agregación de rango\" que se demostró que tenían un rendimiento aceptable en estudios anteriores: consideramos dos métodos posicionales que son las estrategias Cocsum y Combmnz.",
                "La primera fila de la Tabla 10 ofrece actuaciones de los métodos de \"agregación de rango\" W.R.T.Una suposición básica establecida A1 = (H1 100, H2 5, H4 NUEVO): Solo consideramos los 100 primeros documentos de cada clasificación, luego retenemos documentos presentes en 5 o más clasificaciones y actualizamos rangos de documentos exitosos.",
                "Los valores entre los soportes de la Tabla 10 son variaciones de rendimiento de cada método de \"agregación de rango\" W.R.T.Rendimiento del enfoque de supervisión.",
                "Tabla 10: rendimiento (mapa) de diferentes métodos de \"agregación de rango\" en 3 colecciones de prueba diferentes MCM Combmnz Markov A1 18.79% 17.54% (-6.65%*) 17.08% (-9.10%*) 18.63% (-0.85%) A221.36% 19.18% (-10.21%*) 18.61% (-12.87%*) 21.33% (-0.14%) A3 21.92% 21.38% (-2.46%) 20.88% (-4.74%) 19.35% (-11.72%*)A4 A4 18.64% 17.58% (-5.69%*) 17.18% (-7.83%*) 18.63% (-0.05%) A5 55.39% 52.16% (-5.83%*) 49.70% (-10.27%*) 53.30% (-3.7777%) A6 16.95% 15.65% (-7.67%*) 14.57% (-14.04%*) 16.39% (-3.30%) del análisis de la Tabla 10 se puede establecer lo siguiente: • Para todas las ejecuciones, considerando todos los documentosEn cada clasificación de entrada (A2) mejora significativamente el rendimiento (el mapa aumenta en un 11,62% en promedio).",
                "Conclusiones En este documento, abordamos el problema de \"agregación de rango\" donde se pueden fusionar las listas de documentos diferentes, pero no desarticulantes.",
                "Los métodos actuales de \"agregación de rango\", y especialmente los métodos posicionales (por ejemplo, Cocebro [15]), no están diseñados inicialmente para funcionar con tales clasificaciones.",
                "Proponemos un nuevo método de supervisión para \"agregación de rango\" que está bien adaptado al contexto IR.",
                "Métodos de \"agregación de rango\" para la web.",
                "MetaSearch web: métodos de \"agregación de rango\" basados en rango."
            ],
            "translated_text": "",
            "candidates": [
                "agregación de rango",
                "rank aggregation",
                "agregación de rango",
                "agregación de rango",
                "agregación de rango",
                "agregación de rango",
                "agregación de rango",
                "agregación de rango",
                "agregación de rango",
                "agregación de rango",
                "agregación de rango",
                "agregación de rango",
                "agregación de rango",
                "agregación de rango",
                "agregación de rango",
                "agregación de rango",
                "Agregación de rango",
                "agregación de rango",
                "agregación de rango",
                "agregación de rango",
                "agregación de rango",
                "agregación de rango",
                "agregación de rango",
                "agregación de rango",
                "agregación de rango",
                "agregación de rango",
                "agregación de rango",
                "agregación de rango",
                "Agregación de rango",
                "agregación de rango",
                "Agregación de rango",
                "agregación de rango",
                "agregación de rango",
                "agregación de rango",
                "agregación de rango",
                "agregación de rango",
                "agregación de rango",
                "agregación de rango",
                "agregación de rango",
                "agregación de rango",
                "agregación de rango",
                "agregación de rango",
                "agregación de rango",
                "agregación de rango",
                "agregación de rango",
                "agregación de rango",
                "agregación de rango",
                "agregación de rango",
                "agregación de rango",
                "agregación de rango",
                "Métodos En este conjunto de ejecuciones, comparamos el enfoque de supervisión con algunos métodos estándar de ",
                "agregación de rango",
                "agregación de rango",
                "agregación de rango",
                "agregación de rango",
                "agregación de rango",
                "agregación de rango",
                "agregación de rango",
                "agregación de rango",
                "agregación de rango",
                "agregación de rango",
                "agregación de rango",
                "agregación de rango",
                "agregación de rango",
                "agregación de rango",
                "agregación de rango",
                "agregación de rango"
            ],
            "error": []
        },
        "information retrieval": {
            "translated_key": "recuperación de información",
            "is_in_text": true,
            "original_annotated_sentences": [
                "An Outranking Approach for Rank Aggregation in <br>information retrieval</br> Mohamed Farah Lamsade, Paris Dauphine University Place du Mal de Lattre de Tassigny 75775 Paris Cedex 16, France farah@lamsade.dauphine.fr Daniel Vanderpooten Lamsade, Paris Dauphine University Place du Mal de Lattre de Tassigny 75775 Paris Cedex 16, France vdp@lamsade.dauphine.fr ABSTRACT Research in <br>information retrieval</br> usually shows performance improvement when many sources of evidence are combined to produce a ranking of documents (e.g., texts, pictures, sounds, etc.).",
                "In this paper, we focus on the rank aggregation problem, also called data fusion problem, where rankings of documents, searched into the same collection and provided by multiple methods, are combined in order to produce a new ranking.",
                "In this context, we propose a rank aggregation method within a multiple criteria framework using aggregation mechanisms based on decision rules identifying positive and negative reasons for judging whether a document should get a better rank than another.",
                "We show that the proposed method deals well with the <br>information retrieval</br> distinctive features.",
                "Experimental results are reported showing that the suggested method performs better than the well-known CombSUM and CombMNZ operators.",
                "Categories and Subject Descriptors: H.3.3 [Information Systems]: Information Search and Retrieval - Retrieval models.",
                "General Terms: Algorithms, Measurement, Experimentation, Performance, Theory. 1.",
                "INTRODUCTION A wide range of current <br>information retrieval</br> (IR) approaches are based on various search models (Boolean, Vector Space, Probabilistic, Language, etc. [2]) in order to retrieve relevant documents in response to a user request.",
                "The result lists produced by these approaches depend on the exact definition of the relevance concept.",
                "Rank aggregation approaches, also called data fusion approaches, consist in combining these result lists in order to produce a new and hopefully better ranking.",
                "Such approaches give rise to metasearch engines in the Web context.",
                "We consider, in the following, cases where only ranks are available and no other additional information is provided such as the relevance scores.",
                "This corresponds indeed to the reality, where only ordinal information is available.",
                "Data fusion is also relevant in other contexts, such as when the user writes several queries of his/her information need (e.g., a boolean query and a natural language query) [4], or when many document surrogates are available [16].",
                "Several studies argued that rank aggregation has the potential of combining effectively all the various sources of evidence considered in various input methods.",
                "For instance, experiments carried out in [16], [30], [4] and [19] showed that documents which appear in the lists of the majority of the input methods are more likely to be relevant.",
                "Moreover, Lee [19] and Vogt and Cottrell [31] found that various retrieval approaches often return very different irrelevant documents, but many of the same relevant documents.",
                "Bartell et al. [3] also found that rank aggregation methods improve the performances w.r.t. those of the input methods, even when some of them have weak individual performances.",
                "These methods also tend to smooth out biases of the input methods according to Montague and Aslam [22].",
                "Data fusion has recently been proved to improve performances for both the ad-hoc retrieval and categorization tasks within the TREC genomics track in 2005 [1].",
                "The rank aggregation problem was addressed in various fields such as i) in social choice theory which studies voting algorithms which specify winners of elections or winners of competitions in tournaments [29], ii) in statistics when studying correlation between rankings, iii) in distributed databases when results from different databases must be combined [12], and iv) in collaborative filtering [23].",
                "Most current rank aggregation methods consider each input ranking as a permutation over the same set of items.",
                "They also give rigid interpretation to the exact ranking of the items.",
                "Both of these assumptions are rather not valid in the IR context, as will be shown in the following sections.",
                "The remaining of the paper is organized as follows.",
                "We first review current rank aggregation methods in Section 2.",
                "Then we outline the specificities of the data fusion problem in the IR context (Section 3).",
                "In Section 4, we present a new aggregation method which is proven to best fit the IR context.",
                "Experimental results are presented in Section 5 and conclusions are provided in a final section. 2.",
                "RELATED WORK As pointed out by Riker [25], we can distinguish two families of rank aggregation methods: positional methods which assign scores to items to be ranked according to the ranks they receive and majoritarian methods which are based on pairwise comparisons of items to be ranked.",
                "These two families of methods find their roots in the pioneering works of Borda [5] and Condorcet [7], respectively, in the social choice literature. 2.1 Preliminaries We first introduce some basic notations to present the rank aggregation methods in a uniform way.",
                "Let D = {d1, d2, . . . , dnd } be a set of nd documents.",
                "A list or a ranking j is an ordering defined on Dj ⊆ D (j = 1, . . . , n).",
                "Thus, di j di means di is ranked better than di in j.",
                "When Dj = D, j is said to be a full list.",
                "Otherwise, it is a partial list.",
                "If di belongs to Dj, rj i denotes the rank or position of di in j.",
                "We assume that the best answer (document) is assigned the position 1 and the worst one is assigned the position |Dj|.",
                "Let D be the set of all permutations on D or all subsets of D. A profile is a n-tuple of rankings PR = ( 1, 2, . . . , n).",
                "Restricting PR to the rankings containing document di defines PRi.",
                "We also call the number of rankings which contain document di the rank hits of di [19].",
                "The rank aggregation or data fusion problem consists of finding a ranking function or mechanism Ψ (also called a social welfare function in the social choice theory terminology) defined by: Ψ : n D → D PR = ( 1, 2, . . . , n) → σ = Ψ(PR) where σ is called a consensus ranking. 2.2 Positional Methods 2.2.1 Borda Count This method [5] first assigns a score n j=1 rj i to each document di.",
                "Documents are then ranked by increasing order of this score, breaking ties, if any, arbitrarily. 2.2.2 Linear Combination Methods This family of methods basically combine scores of documents.",
                "When used for the rank aggregation problem, ranks are assumed to be scores or performances to be combined using aggregation operators such as the weighted sum or some variation of it [3, 31, 17, 28].",
                "For instance, Callan et al. [6] used the inference networks model [30] to combine rankings.",
                "Fox and Shaw [15] proposed several combination strategies which are CombSUM, CombMIN, CombMAX, CombANZ and CombMNZ.",
                "The first three operators correspond to the sum, min and max operators, respectively.",
                "CombANZ and CombMNZ respectively divides and multiplies the CombSUM score by the rank hits.",
                "It is shown in [19] that the CombSUM and CombMNZ operators perform better than the others.",
                "Metasearch engines such as SavvySearch and MetaCrawler use the CombSUM strategy to fuse rankings. 2.2.3 Footrule Optimal Aggregation In this method, a consensus ranking minimizes the Spearman footrule distance from the input rankings [21].",
                "Formally, given two full lists j and j , this distance is given by F( j, j ) = nd i=1 |rj i − rj i |.",
                "It extends to several lists as follows.",
                "Given a profile PR and a consensus ranking σ, the Spearman footrule distance of σ to PR is given by F(σ, PR) = n j=1 F(σ, j).",
                "Cook and Kress [8] proposed a similar method which consists in optimizing the distance D( j, j ) = 1 2 nd i,i =1 |rj i,i − rj i,i |, where rj i,i = rj i −rj i .",
                "This formulation has the advantage that it considers the intensity of preferences. 2.2.4 Probabilistic Methods This kind of methods assume that the performance of the input methods on a number of training queries is indicative of their future performance.",
                "During the training process, probabilities of relevance are calculated.",
                "For subsequent queries, documents are ranked based on these probabilities.",
                "For instance, in [20], each input ranking j is divided into a number of segments, and the conditional probability of relevance (R) of each document di depending on the segment k it occurs in, is computed, i.e. prob(R|di, k, j).",
                "For subsequent queries, the score of each document di is given by n j=1 prob(R|di,k, j ) k .",
                "Le Calve and Savoy [18] suggest using a logistic regression approach for combining scores.",
                "Training data is needed to infer the model parameters. 2.3 Majoritarian Methods 2.3.1 Condorcet Procedure The original Condorcet rule [7] specifies that a winner of the election is any item that beats or ties with every other item in a pairwise contest.",
                "Formally, let C(diσdi ) = { j∈ PR : di j di } be the coalition of rankings that are concordant with establishing diσdi , i.e. with the proposition di should be ranked better than di in the final ranking σ. di beats or ties with di iff |C(diσdi )| ≥ |C(di σdi)|.",
                "The repetitive application of the Condorcet algorithm can produce a ranking of items in a natural way: select the Condorcet winner, remove it from the lists, and repeat the previous two steps until there are no more documents to rank.",
                "Since there is not always Condorcet winners, variations of the Condorcet procedure have been developed within the multiple criteria decision aid theory, with methods such as ELECTRE [26]. 2.3.2 Kemeny Optimal Aggregation As in section 2.2.3, a consensus ranking minimizes a geometric distance from the input rankings, where the Kendall tau distance is used instead of the Spearman footrule distance.",
                "Formally, given two full lists j and j , the Kendall tau distance is given by K( j, j ) = |{(di, di ) : i < i , rj i < rj i , rj i > rj i }|, i.e. the number of pairwise disagreements between the two lists.",
                "It is easy to show that the consensus ranking corresponds to the geometric median of the input rankings and that the Kemeny optimal aggregation problem corresponds to the minimum feedback edge set problem. 2.3.3 Markov Chain Methods Markov chains (MCs) have been used by Dwork et al. [11] as a natural method to obtain a consensus ranking where states correspond to the documents to be ranked and the transition probabilities vary depending on the interpretation of the transition event.",
                "In the same reference, the authors proposed four specific MCs and experimental testing had shown that the following MC is the best performing one (see also [24]): • MC4: move from the current state di to the next state di by first choosing a document di uniformly from D. If for the majority of the rankings, we have rj i ≤ rj i , then move to di , else stay in di.",
                "The consensus ranking corresponds to the stationary distribution of MC4. 3.",
                "SPECIFICITIES OF THE RANK AGGREGATION PROBLEM IN THE IR CONTEXT 3.1 Limited Significance of the Rankings The exact positions of documents in one input ranking have limited significance and should not be overemphasized.",
                "For instance, having three relevant documents in the first three positions, any perturbation of these three items will have the same value.",
                "Indeed, in the IR context, the complete order provided by an input method may hide ties.",
                "In this case, we call such rankings semi orders.",
                "This was outlined in [13] as the problem of aggregation with ties.",
                "It is therefore important to build the consensus ranking based on robust information: • Documents with near positions in j are more likely to have similar interest or relevance.",
                "Thus a slight perturbation of the initial ranking is meaningless. • Assuming that document di is better ranked than document di in a ranking j, di is more likely to be definitively more relevant than di in j when the number of intermediate positions between di and di increases. 3.2 Partial Lists In real world applications, such as metasearch engines, rankings provided by the input methods are often partial lists.",
                "This was outlined in [14] as the problem of having to merge top-k results from various input lists.",
                "For instance, in the experiments carried out by Dwork et al. [11], authors found that among the top 100 best documents of 7 input search engines, 67% of the documents were present in only one search engine, whereas less than two documents were present in all the search engines.",
                "Rank aggregation of partial lists raises four major difficulties which we state hereafter, proposing for each of them various working assumptions: 1.",
                "Partial lists can have various lengths, which can favour long lists.",
                "We thus consider the following two working hypotheses: H1 k : We only consider the top k best documents from each input ranking.",
                "H1 all: We consider all the documents from each input ranking. 2.",
                "Since there are different documents in the input rankings, we must decide which documents should be kept in the consensus ranking.",
                "Two working hypotheses are therefore considered: H2 k : We only consider documents which are present in at least k input rankings (k > 1).",
                "H2 all: We consider all the documents which are ranked in at least one input ranking.",
                "Hereafter, we call documents which will be retained in the consensus ranking, candidate documents, and documents that will be excluded from the consensus ranking, excluded documents.",
                "We also call a candidate document which is missing in one or more rankings, a missing document. 3.",
                "Some candidate documents are missing documents in some input rankings.",
                "Main reasons for a missing document are that it was not indexed or it was indexed but deemed irrelevant ; usually this information is not available.",
                "We consider the following two working hypotheses: H3 yes: Each missing document in each j is assigned a position.",
                "H3 no: No assumption is made, that is each missing document is considered neither better nor worse than any other document. 4.",
                "When assumption H2 k holds, each input ranking may contain documents which will not be considered in the consensus ranking.",
                "Regarding the positions of the candidate documents, we can consider the following working hypotheses: H4 init: The initial positions of candidate documents are kept in each input ranking.",
                "H4 new: Candidate documents receive new positions in each input ranking, after discarding excluded ones.",
                "In the IR context, rank aggregation methods need to decide more or less explicitly which assumptions to retain w.r.t. the above-mentioned difficulties. 4.",
                "OUTRANKING APPROACH FOR RANK AGGREGATION 4.1 Presentation Positional methods consider implicitly that the positions of the documents in the input rankings are scores giving thus a cardinal meaning to an ordinal information.",
                "This constitutes a strong assumption that is questionable, especially when the input rankings have different lengths.",
                "Moreover, for positional methods, assumptions H3 and H4 , which are often arbitrary, have a strong impact on the results.",
                "For instance, let us consider an input ranking of 500 documents out of 1000 candidate documents.",
                "Whether we assign to each of the missing documents the position 1, 501, 750 or 1000 -corresponding to variations of H3 yes- will give rise to very contrasted results, especially regarding the top of the consensus ranking.",
                "Majoritarian methods do not suffer from the above-mentioned drawbacks of the positional methods since they build consensus rankings exploiting only ordinal information contained in the input rankings.",
                "Nevertheless, they suppose that such rankings are complete orders, ignoring that they may hide ties.",
                "Therefore, majoritarian methods base consensus rankings on illusory discriminant information rather than less discriminant but more robust information.",
                "Trying to overcome the limits of current rank aggregation methods, we found that outranking approaches, which were initially used for multiple criteria aggregation problems [26], can also be used for the rank aggregation purpose, where each ranking plays the role of a criterion.",
                "Therefore, in order to decide whether a document di should be ranked better than di in the consensus ranking σ, the two following conditions should be met: • a concordance condition which ensures that a majority of the input rankings are concordant with diσdi (majority principle). • a discordance condition which ensures that none of the discordant input rankings strongly refutes dσd (respect of minorities principle).",
                "Formally, the concordance coalition with diσdi is Csp (diσdi ) = { j∈ PR : rj i ≤ rj i − sp} where sp is a preference threshold which is the variation of document positions -whether it is absolute or relative to the ranking length- which draws the boundaries between an indifference and a preference situation between documents.",
                "The discordance coalition with diσdi is Dsv (diσdi ) = { j∈ PR : rj i ≥ rj i + sv} where sv is a veto threshold which is the variation of document positions -whether it is absolute or relative to the ranking length- which draws the boundaries between a weak and a strong opposition to diσdi .",
                "Depending on the exact definition of the preceding concordance and discordance coalitions leading to the definition of some decision rules, several outranking relations can be defined.",
                "They can be more or less demanding depending on i) the values of the thresholds sp and sv, ii) the importance or minimal size cmin required for the concordance coalition, and iii) the importance or maximum size dmax of the discordance coalition.",
                "A generic outranking relation can thus be defined as follows: diS(sp,sv,cmin,dmax)di ⇔ |Csp (diσdi )| ≥ cmin AND |Dsv (diσdi )| ≤ dmax This expression defines a family of nested outranking relations since S(sp,sv,cmin,dmax) ⊆ S(sp,sv,cmin,dmax) when cmin ≥ cmin and/or dmax ≤ dmax and/or sp ≥ sp and/or sv ≤ sv.",
                "This expression also generalizes the majority rule which corresponds to the particular relation S(0,∞, n 2 ,n).",
                "It also satisfies important properties of rank aggregation methods, called neutrality, Pareto-optimality, Condorcet property and Extended Condorcet property, in the social choice literature [29].",
                "Outranking relations are not necessarily transitive and do not necessarily correspond to rankings since directed cycles may exist.",
                "Therefore, we need specific procedures in order to derive a consensus ranking.",
                "We propose the following procedure which finds its roots in [27].",
                "It consists in partitioning the set of documents into r ranked classes.",
                "Each class Ch contains documents with the same relevance and results from the application of all relations (if possible) to the set of documents remaining after previous classes are computed.",
                "Documents within the same equivalence class are ranked arbitrarily.",
                "Formally, let • R be the set of candidate documents for a query, • S1 , S2 , . . . be a family of nested outranking relations, • Fk(di, E) = |{di ∈ E : diSk di }| be the number of documents in E(E ⊆ R) that could be considered worse than di according to relation Sk , • fk(di, E) = |{di ∈ E : di Sk di}| be the number of documents in E that could be considered better than di according to Sk , • sk(di, E) = Fk(di, E) − fk(di, E) be the qualification of di in E according to Sk .",
                "Each class Ch results from a distillation process.",
                "It corresponds to the last distillate of a series of sets E0 ⊇ E1 ⊇ . . . where E0 = R \\ (C1 ∪ . . . ∪ Ch−1) and Ek is a reduced subset of Ek−1 resulting from the application of the following procedure: 1. compute for each di ∈ Ek−1 its qualification according to Sk , i.e. sk(di, Ek−1), 2. define smax = maxdi∈Ek−1 {sk(di, Ek−1)}, then 3.",
                "Ek = {di ∈ Ek−1 : sk(di, Ek−1) = smax} When one outranking relation is used, the distillation process stops after the first application of the previous procedure, i.e., Ch corresponds to distillate E1.",
                "When different outranking relations are used, the distillation process stops when all the pre-defined outranking relations have been used or when |Ek| = 1. 4.2 Illustrative Example This section illustrates the concepts and procedures of section 4.1.",
                "Let us consider a set of candidate documents R = {d1, d2, d3, d4, d5}.",
                "The following table gives a profile PR of different rankings of the documents of R: PR = ( 1 , 2, 3, 4).",
                "Table 1: Rankings of documents rj i 1 2 3 4 d1 1 3 1 5 d2 2 1 3 3 d3 3 2 2 1 d4 4 4 5 2 d5 5 5 4 4 Let us suppose that the preference and veto thresholds are set to values 1 and 4 respectively, and that the concordance and discordance thresholds are set to values 2 and 1 respectively.",
                "The following tables give the concordance, discordance and outranking matrices.",
                "Each entry csp (di, di ) (dsv (di, di )) in the concordance (discordance) matrix gives the number of rankings that are concordant (discordant) with diσdi , i.e. csp (di, di ) = |Csp (diσdi )| and dsv (di, di ) = |Dsv (diσdi )|.",
                "Table 2: Computation of the outranking relation d1 d2 d3 d4 d5 d1 - 2 2 3 3 d2 2 - 2 3 4 d3 2 2 - 4 4 d4 1 1 0 - 3 d5 1 0 0 1Concordance Matrix d1 d2 d3 d4 d5 d1 - 0 1 0 0 d2 0 - 0 0 0 d3 0 0 - 0 0 d4 1 0 0 - 0 d5 1 1 0 0Discordance Matrix d1 d2 d3 d4 d5 d1 - 1 1 1 1 d2 1 - 1 1 1 d3 1 1 - 1 1 d4 0 0 0 - 1 d5 0 0 0 0Outranking Matrix (S1) For instance, the concordance coalition for the assertion d1σd4 is C1(d1σd4) = { 1, 2, 3} and the discordance coalition for the same assertion is D4(d1σd4) = ∅.",
                "Therefore, c1(d1, d4) = 3, d4(d1, d4) = 0 and d1S1 d4 holds.",
                "Notice that Fk(di, R) (fk(di, R)) is given by summing the values of the ith row (column) of the outranking matrix.",
                "The consensus ranking is obtained as follows: to get the first class C1, we compute the qualifications of all the documents of E0 = R with respect to S1 .",
                "They are respectively 2, 2, 2, -2 and -4.",
                "Therefore smax equals 2 and C1 = E1 = {d1, d2, d3}.",
                "Observe that, if we had used a second outranking relation S2(⊇ S1), these three documents could have been possibly discriminated.",
                "At this stage, we remove documents of C1 from the outranking matrix and compute the next class C2: we compute the new qualifications of the documents of E0 = R \\ C1 = {d4, d5}.",
                "They are respectively 1 and -1.",
                "So C3 = E1 = {d4}.",
                "The last document d5 is the only document of the last class C3.",
                "Thus, the consensus ranking is {d1, d2, d3} → {d4} → {d5}. 5.",
                "EXPERIMENTS AND RESULTS 5.1 Test Setting To facilitate empirical investigation of the proposed methodology, we developed a prototype metasearch engine that implements a version of our outranking approach for rank aggregation.",
                "In this paper, we apply our approach to the Topic Distillation (TD) task of TREC-2004 Web track [10].",
                "In this task, there are 75 topics where only a short description of each is given.",
                "For each query, we retained the rankings of the 10 best runs of the TD task which are provided by TREC-2004 participating teams.",
                "The performances of these runs are reported in table 3.",
                "Table 3: Performances of the 10 best runs of the TD task of TREC-2004 Run Id MAP P@10 S@1 S@5 S@10 uogWebCAU150 17.9% 24.9% 50.7% 77.3% 89.3% MSRAmixed1 17.8% 25.1% 38.7% 72.0% 88.0% MSRC04C12 16.5% 23.1% 38.7% 74.7% 80.0% humW04rdpl 16.3% 23.1% 37.3% 78.7% 90.7% THUIRmix042 14.7% 20.5% 21.3% 58.7% 74.7% UAmsT04MWScb 14.6% 20.9% 36.0% 66.7% 76.0% ICT04CIIS1AT 14.1% 20.8% 33.3% 64.0% 78.7% SJTUINCMIX5 12.9% 18.9% 29.3% 57.3% 72.0% MU04web1 11.5% 19.9% 33.3% 64.0% 76.0% MeijiHILw3 11.5% 15.3% 30.7% 54.7% 64.0% Average 14.7% 21.2% 34.9% 66.8% 78.94% For each query, each run provides a ranking of about 1000 documents.",
                "The number of documents retrieved by all these runs ranges from 543 to 5769.",
                "Their average (median) number is 3340 (3386).",
                "It is worth noting that we found similar distributions of the documents among the rankings as in [11].",
                "For evaluation, we used the trec eval standard tool which is used by the TREC community to calculate the standard measures of system effectiveness which are Mean Average Precision (MAP) and Success@n (S@n) for n=1, 5 and 10.",
                "Our approach effectiveness is compared against some high performing official results from TREC-2004 as well as against some standard rank aggregation algorithms.",
                "In the experiments, significance testing is mainly based on the t-student statistic which is computed on the basis of the MAP values of the compared runs.",
                "In the tables of the following section, statistically significant differences are marked with an asterisk.",
                "Values between brackets of the first column of each table, indicate the parameter value of the corresponding run. 5.2 Results We carried out several series of runs in order to i) study performance variations of the outranking approach when tuning the parameters and working assumptions, ii) compare performances of the outranking approach vs standard rank aggregation strategies , and iii) check whether rank aggregation performs better than the best input rankings.",
                "We set our basic run mcm with the following parameters.",
                "We considered that each input ranking is a complete order (sp = 0) and that an input ranking strongly refutes diσdi when the difference of both document positions is large enough (sv = 75%).",
                "Preference and veto thresholds are computed proportionally to the number of documents retained in each input ranking.",
                "They consequently may vary from one ranking to another.",
                "In addition, to accept the assertion diσdi , we supposed that the majority of the rankings must be concordant (cmin = 50%) and that every input ranking can impose its veto (dmax = 0).",
                "Concordance and discordance thresholds are computed for each tuple (di, di ) as the percentage of the input rankings of PRi ∩PRi .",
                "Thus, our choice of parameters leads to the definition of the outranking relation S(0,75%,50%,0).",
                "To test the run mcm, we had chosen the following assumptions.",
                "We retained the top 100 best documents from each input ranking (H1 100), only considered documents which are present in at least half of the input rankings (H2 5 ) and assumed H3 no and H4 new.",
                "In these conditions, the number of successful documents was about 100 on average, and the computation time per query was less than one second.",
                "Obviously, modifying the working assumptions should have deeper impact on the performances than tuning our model parameters.",
                "This was validated by preliminary experiments.",
                "Thus, we hereafter begin by studying performance variation when different sets of assumptions are considered.",
                "Afterwards, we study the impact of tuning parameters.",
                "Finally, we compare our model performances w.r.t. the input rankings as well as some standard data fusion algorithms. 5.2.1 Impact of the Working Assumptions Table 4 summarizes the performance variation of the outranking approach under different working hypotheses.",
                "In Table 4: Impact of the working assumptions Run Id MAP S@1 S@5 S@10 mcm 18.47% 41.33% 81.33% 86.67% mcm22 (H3 yes) 17.72% (-4.06%) 34.67% 81.33% 86.67% mcm23 (H4 init) 18.26% (-1.14%) 41.33% 81.33% 86.67% mcm24 (H1 all) 20.67% (+11.91%*) 38.66% 80.00% 86.66% mcm25 (H2 all) 21.68% (+17.38%*) 40.00% 78.66% 89.33% this table, we first show that run mcm22, in which missing documents are all put in the same last position of each input ranking, leads to performance drop w.r.t. run mcm.",
                "Moreover, S@1 moves from 41.33% to 34.67% (-16.11%).",
                "This shows that several relevant documents which were initially put at the first position of the consensus ranking in mcm, lose this first position but remain ranked in the top 5 documents since S@5 did not change.",
                "We also conclude that documents which have rather good positions in some input rankings are more likely to be relevant, even though they are missing in some other rankings.",
                "Consequently, when they are missing in some rankings, assigning worse ranks to these documents is harmful for performance.",
                "Also, from Table 4, we found that the performances of runs mcm and mcm23 are similar.",
                "Therefore, the outranking approach is not sensitive to keeping the initial positions of candidate documents or recomputing them by discarding excluded ones.",
                "From the same Table 4, performance of the outranking approach increases significantly for runs mcm24 and mcm25.",
                "Therefore, whether we consider all the documents which are present in half of the rankings (mcm24) or we consider all the documents which are ranked in the first 100 positions in one or more rankings (mcm25), increases performances.",
                "This result was predictable since in both cases we have more detailed information on the relative importance of documents.",
                "Tables 5 and 6 confirm this evidence.",
                "Table 5, where values between brackets of the first column give the number of documents which are retained from each input ranking, shows that selecting more documents from each input ranking leads to performance increase.",
                "It is worth mentioning that selecting more than 600 documents from each input ranking does not improve performance.",
                "Table 5: Impact of the number of retained documents Run Id MAP S@1 S@5 S@10 mcm (100) 18.47% 41.33% 81.33% 86.67% mcm24-1 (200) 19.32% (+4.60%) 42.67% 78.67% 88.00% mcm24-2 (400) 19.88% (+7.63%*) 37.33% 80.00% 88.00% mcm24-3 (600) 20.80% (+12.62%*) 40.00% 80.00% 88.00% mcm24-4 (800) 20.66% (+11.86%*) 40.00% 78.67% 86.67% mcm24 (1000) 20.67% (+11.91%*) 38.66% 80.00% 86.66% Table 6 reports runs corresponding to variations of H2 k .",
                "Values between brackets are rank hits.",
                "For instance, in the run mcm32, only documents which are present in 3 or more input rankings, were considered successful.",
                "This table shows that performance is significantly better when rare documents are considered, whereas it decreases significantly when these documents are discarded.",
                "Therefore, we conclude that many of the relevant documents are retrieved by a rather small set of IR models.",
                "Table 6: Performance considering different rank hits Run Id MAP S@1 S@5 S@10 mcm25 (1) 21.68% (+17.38%*) 40.00% 78.67% 89.33% mcm32 (3) 18.98% (+2.76%) 38.67% 80.00% 85.33% mcm (5) 18.47% 41.33% 81.33% 86.67% mcm33 (7) 15.83% (-14.29%*) 37.33% 78.67% 85.33% mcm34 (9) 10.96% (-40.66%*) 36.11% 66.67% 70.83% mcm35 (10) 7.42% (-59.83%*) 39.22% 62.75% 64.70% For both runs mcm24 and mcm25, the number of successful documents was about 1000 and therefore, the computation time per query increased and became around 5 seconds. 5.2.2 Impact of the Variation of the Parameters Table 7 shows performance variation of the outranking approach when different preference thresholds are considered.",
                "We found performance improvement up to threshold values of about 5%, then there is a decrease in the performance which becomes significant for threshold values greater than 10%.",
                "Moreover, S@1 improves from 41.33% to 46.67% when preference threshold changes from 0 to 5%.",
                "We can thus conclude that the input rankings are semi orders rather than complete orders.",
                "Table 8 shows the evolution of the performance measures w.r.t. the concordance threshold.",
                "We can conclude that in order to put document di before di in the consensus ranking, Table 7: Impact of the variation of the preference threshold from 0 to 12.5% Run Id MAP S@1 S@5 S@10 mcm (0%) 18.47% 41.33% 81.33% 86.67% mcm1 (1%) 18.57% (+0.54%) 41.33% 81.33% 86.67% mcm2 (2.5%) 18.63% (+0.87%) 42.67% 78.67% 86.67% mcm3 (5%) 18.69% (+1.19%) 46.67% 81.33% 86.67% mcm4 (7.5%) 18.24% (-1.25%) 46.67% 81.33% 86.67% mcm5 (10%) 17.93% (-2.92%) 40.00% 82.67% 86.67% mcm5b (12.5%) 17.51% (-5.20%*) 41.33% 80.00% 86.67% at least half of the input rankings of PRi ∩ PRi should be concordant.",
                "Performance drops significantly for very low and very high values of the concordance threshold.",
                "In fact, for such values, the concordance condition is either fulfilled rather always by too many document pairs or not fulfilled at all, respectively.",
                "Therefore, the outranking relation becomes either too weak or too strong respectively.",
                "Table 8: Impact of the variation of cmin Run Id MAP S@1 S@5 S@10 mcm11 (20%) 17.63% (-4.55%*) 41.33% 76.00% 85.33% mcm12 (40%) 18.37% (-0.54%) 42.67% 76.00% 86.67% mcm (50%) 18.47% 41.33% 81.33% 86.67% mcm13 (60%) 18.42% (-0.27%) 40.00% 78.67% 86.67% mcm14 (80%) 17.43% (-5.63%*) 40.00% 78.67% 86.67% mcm15 (100%) 16.12% (-12.72%*) 41.33% 70.67% 85.33% In the experiments, varying the veto threshold as well as the discordance threshold within reasonable intervals does not have significant impact on performance measures.",
                "In fact, runs with different veto thresholds (sv ∈ [50%; 100%]) had similar performances even though there is a slight advantage for runs with high threshold values which means that it is better not to allow the input rankings to put their veto easily.",
                "Also, tuning the discordance threshold was carried out for values 50% and 75% of the veto threshold.",
                "For these runs we did not get any noticeable performance variation, although for low discordance thresholds (dmax < 20%), performance slightly decreased. 5.2.3 Impact of the Variation of the Number of Input Rankings To study performance evolution when different sets of input rankings are considered, we carried three more runs where 2, 4, and 6 of the best performing sets of the input rankings are considered.",
                "Results reported in Table 9 are seemingly counter-intuitive and also do not support previous findings regarding rank aggregation research [3].",
                "Nevertheless, this result shows that low performing rankings bring more noise than information to the establishment of the consensus ranking.",
                "Therefore, when they are considered, performance decreases.",
                "Table 9: Performance considering different best performing sets of input rankings Run Id MAP S@1 S@5 S@10 mcm (10) 18.47% 41.33% 81.33% 86.67% mcm27 (6) 18.60% (+0.70%) 41.33% 80.00% 85.33% mcm28 (4) 19.02% (+2.98%) 40.00% 86.67% 88.00% mcm29 (2) 18.33% (-0.76%) 44.00% 76.00% 88.00% 5.2.4 Comparison of the Performance of Different Rank Aggregation Methods In this set of runs, we compare the outranking approach with some standard rank aggregation methods which were proven to have acceptable performance in previous studies: we considered two positional methods which are the CombSUM and the CombMNZ strategies.",
                "We also examined the performance of one majoritarian method which is the Markov chain method (MC4).",
                "For the comparisons, we considered a specific outranking relation S∗ = S(5%,50%,50%,30%) which results in good overall performances when tuning all the parameters.",
                "The first row of Table 10 gives performances of the rank aggregation methods w.r.t. a basic assumption set A1 = (H1 100, H2 5 , H4 new): we only consider the 100 first documents from each ranking, then retain documents present in 5 or more rankings and update ranks of successful documents.",
                "For positional methods, we place missing documents at the queue of the ranking (H3 yes) whereas for our method as well as for MC4, we retained hypothesis H3 no.",
                "The three following rows of Table 10 report performances when changing one element from the basic assumption set: the second row corresponds to the assumption set A2 = (H1 1000, H2 5 , H4 new), i.e. changing the number of retained documents from 100 to 1000.",
                "The third row corresponds to the assumption set A3 = (H1 100, H2 all, H4 new), i.e. considering the documents present in at least one ranking.",
                "The fourth row corresponds to the assumption set A4 = (H1 100, H2 5 , H4 init), i.e. keeping the original ranks of successful documents.",
                "The fifth row of Table 10, labeled A5, gives performance when all the 225 queries of the Web track of TREC-2004 are considered.",
                "Obviously, performance level cannot be compared with previous lines since the additional queries are different from the TD queries and correspond to other tasks (Home Page and Named Page tasks [10]) of TREC-2004 Web track.",
                "This set of runs aims to show whether relative performance of the various methods is task-dependent.",
                "The last row of Table 10, labeled A6, reports performance of the various methods considering the TD task of TREC2002 instead of TREC-2004: we fused the results of input rankings of the 10 best official runs for each of the 50 TD queries [9] considering the set of assumptions A1 of the first row.",
                "This aims to show whether relative performance of the various methods changes from year to year.",
                "Values between brackets of Table 10 are variations of performance of each rank aggregation method w.r.t. performance of the outranking approach.",
                "Table 10: Performance (MAP) of different rank aggregation methods under 3 different test collections mcm combSUM combMNZ markov A1 18.79% 17.54% (-6.65%*) 17.08% (-9.10%*) 18.63% (-0.85%) A2 21.36% 19.18% (-10.21%*) 18.61% (-12.87%*) 21.33% (-0.14%) A3 21.92% 21.38% (-2.46%) 20.88% (-4.74%) 19.35% (-11.72%*) A4 18.64% 17.58% (-5.69%*) 17.18% (-7.83%*) 18.63% (-0.05%) A5 55.39% 52.16% (-5.83%*) 49.70% (-10.27%*) 53.30% (-3.77%) A6 16.95% 15.65% (-7.67%*) 14.57% (-14.04%*) 16.39% (-3.30%) From the analysis of table 10 the following can be established: • for all the runs, considering all the documents in each input ranking (A2) significantly improves performance (MAP increases by 11.62% on average).",
                "This is predictable since some initially unreported relevant documents would receive better positions in the consensus ranking. • for all the runs, considering documents even those present in only one input ranking (A3) significantly improves performance.",
                "For mcm, combSUM and combMNZ, performance improvement is more important (MAP increases by 20.27% on average) than for the markov run (MAP increases by 3.86%). • preserving the initial positions of documents (A4) or recomputing them (A1) does not have a noticeable influence on performance for both positional and majoritarian methods. • considering all the queries of the Web track of TREC2004 (A5) as well as the TD queries of the Web track of TREC-2002 (A6) does not alter the relative performance of the different data fusion methods. • considering the TD queries of the Web track of TREC2002, performances of all the data fusion methods are lower than that of the best performing input ranking for which the MAP value equals 18.58%.",
                "This is because most of the fused input rankings have very low performances compared to the best one, which brings more noise to the consensus ranking. • performances of the data fusion methods mcm and markov are significantly better than that of the best input ranking uogWebCAU150.",
                "This remains true for runs combSUM and combMNZ only under assumptions H1 all or H2 all.",
                "This shows that majoritarian methods are less sensitive to assumptions than positional methods. • outranking approach always performs significantly better than positional methods combSUM and combMNZ.",
                "It has also better performances than the Markov chain method, especially under assumption H2 all where difference of performances becomes significant. 6.",
                "CONCLUSIONS In this paper, we address the rank aggregation problem where different, but not disjoint, lists of documents are to be fused.",
                "We noticed that the input rankings can hide ties, so they should not be considered as complete orders.",
                "Only robust information should be used from each input ranking.",
                "Current rank aggregation methods, and especially positional methods (e.g. combSUM [15]), are not initially designed to work with such rankings.",
                "They should be adapted by considering specific working assumptions.",
                "We propose a new outranking method for rank aggregation which is well adapted to the IR context.",
                "Indeed, it ranks two documents w.r.t. the intensity of their positions difference in each input ranking and also considering the number of the input rankings that are concordant and discordant in favor of a specific document.",
                "There is also no need to make specific assumptions on the positions of the missing documents.",
                "This is an important feature since the absence of a document from a ranking should not be necessarily interpreted negatively.",
                "Experimental results show that the outranking method significantly out-performs popular classical positional data fusion methods like combSUM and combMNZ strategies.",
                "It also out-performs a good performing majoritarian methods which is the Markov chain method.",
                "These results are tested against different test collections and queries.",
                "From the experiments, we can also conclude that in order to improve the performances, we should fuse result lists of well performing IR models, and that majoritarian data fusion methods perform better than positional methods.",
                "The proposed method can have a real impact on Web metasearch performances since only ranks are available from most primary search engines, whereas most of the current approaches need scores to merge result lists into one single list.",
                "Further work involves investigating whether the outranking approach performs well in various other contexts, e.g. using the document scores or some combination of document ranks and scores.",
                "Acknowledgments The authors would like to thank Jacques Savoy for his valuable comments on a preliminary version of this paper. 7.",
                "REFERENCES [1] A. Aronson, D. Demner-Fushman, S. Humphrey, J. Lin, H. Liu, P. Ruch, M. Ruiz, L. Smith, L. Tanabe, and W. Wilbur.",
                "Fusion of knowledge-intensive and statistical approaches for retrieving and annotating textual genomics documents.",
                "In Proceedings TREC2005.",
                "NIST Publication, 2005. [2] R. A. Baeza-Yates and B.",
                "A. Ribeiro-Neto.",
                "Modern <br>information retrieval</br>.",
                "ACM Press , 1999. [3] B. T. Bartell, G. W. Cottrell, and R. K. Belew.",
                "Automatic combination of multiple ranked retrieval systems.",
                "In Proceedings ACM-SIGIR94, pages 173-181.",
                "Springer-Verlag, 1994. [4] N. J. Belkin, P. Kantor, E. A.",
                "Fox, and J.",
                "A. Shaw.",
                "Combining evidence of multiple query representations for <br>information retrieval</br>.",
                "IPM, 31(3):431-448, 1995. [5] J. Borda.",
                "M´emoire sur les ´elections au scrutin.",
                "Histoire de lAcad´emie des Sciences, 1781. [6] J. P. Callan, Z. Lu, and W. B. Croft.",
                "Searching distributed collections with inference networks.",
                "In Proceedings ACM-SIGIR95, pages 21-28, 1995. [7] M. Condorcet.",
                "Essai sur lapplication de lanalyse `a la probabilit´e des d´ecisions rendues `a la pluralit´e des voix.",
                "Imprimerie Royale, Paris, 1785. [8] W. D. Cook and M. Kress.",
                "Ordinal ranking with intensity of preference.",
                "Management Science, 31(1):26-32, 1985. [9] N. Craswell and D. Hawking.",
                "Overview of the TREC-2002 Web Track.",
                "In Proceedings TREC2002.",
                "NIST Publication, 2002. [10] N. Craswell and D. Hawking.",
                "Overview of the TREC-2004 Web Track.",
                "In Proceedings of TREC2004.",
                "NIST Publication, 2004. [11] C. Dwork, S. R. Kumar, M. Naor, and D. Sivakumar.",
                "Rank aggregation methods for the Web.",
                "In Proceedings WWW2001, pages 613-622, 2001. [12] R. Fagin.",
                "Combining fuzzy information from multiple systems.",
                "JCSS, 58(1):83-99, 1999. [13] R. Fagin, R. Kumar, M. Mahdian, D. Sivakumar, and E. Vee.",
                "Comparing and aggregating rankings with ties.",
                "In PODS, pages 47-58, 2004. [14] R. Fagin, R. Kumar, and D. Sivakumar.",
                "Comparing top k lists.",
                "SIAM J. on Discrete Mathematics, 17(1):134-160, 2003. [15] E. A.",
                "Fox and J.",
                "A. Shaw.",
                "Combination of multiple searches.",
                "In Proceedings of TREC3.",
                "NIST Publication, 1994. [16] J. Katzer, M. McGill, J. Tessier, W. Frakes, and P. DasGupta.",
                "A study of the overlap among document representations.",
                "Information Technology: Research and Development, 1(4):261-274, 1982. [17] L. S. Larkey, M. E. Connell, and J. Callan.",
                "Collection selection and results merging with topically organized U.S. patents and TREC data.",
                "In Proceedings ACM-CIKM2000, pages 282-289.",
                "ACM Press, 2000. [18] A.",
                "Le Calv´e and J. Savoy.",
                "Database merging strategy based on logistic regression.",
                "IPM, 36(3):341-359, 2000. [19] J. H. Lee.",
                "Analyses of multiple evidence combination.",
                "In Proceedings ACM-SIGIR97, pages 267-276, 1997. [20] D. Lillis, F. Toolan, R. Collier, and J. Dunnion.",
                "Probfuse: a probabilistic approach to data fusion.",
                "In Proceedings ACM-SIGIR2006, pages 139-146.",
                "ACM Press, 2006. [21] J. I. Marden.",
                "Analyzing and Modeling Rank Data.",
                "Number 64 in Monographs on Statistics and Applied Probability.",
                "Chapman & Hall, 1995. [22] M. Montague and J.",
                "A. Aslam.",
                "Metasearch consistency.",
                "In Proceedings ACM-SIGIR2001, pages 386-387.",
                "ACM Press, 2001. [23] D. M. Pennock and E. Horvitz.",
                "Analysis of the axiomatic foundations of collaborative filtering.",
                "In Workshop on AI for Electronic Commerce at the 16th National Conference on Artificial Intelligence, 1999. [24] M. E. Renda and U. Straccia.",
                "Web metasearch: rank vs. score based rank aggregation methods.",
                "In Proceedings ACM-SAC2003, pages 841-846.",
                "ACM Press, 2003. [25] W. H. Riker.",
                "Liberalism against populism.",
                "Waveland Press, 1982. [26] B. Roy.",
                "The outranking approach and the foundations of ELECTRE methods.",
                "Theory and Decision, 31:49-73, 1991. [27] B. Roy and J. Hugonnard.",
                "Ranking of suburban line extension projects on the Paris metro system by a multicriteria method.",
                "Transportation Research, 16A(4):301-312, 1982. [28] L. Si and J. Callan.",
                "Using sampled data and regression to merge search engine results.",
                "In Proceedings ACM-SIGIR2002, pages 19-26.",
                "ACM Press, 2002. [29] M. Truchon.",
                "An extension of the Condorcet criterion and Kemeny orders.",
                "Cahier 9813, Centre de Recherche en Economie et Finance Appliqu´ees, Oct. 1998. [30] H. Turtle and W. B. Croft.",
                "Inference networks for document retrieval.",
                "In Proceedings of ACM-SIGIR90, pages 1-24.",
                "ACM Press, 1990. [31] C. C. Vogt and G. W. Cottrell.",
                "Fusion via a linear combination of scores.",
                "<br>information retrieval</br>, 1(3):151-173, 1999."
            ],
            "original_annotated_samples": [],
            "translated_annotated_samples": [
                "An Outranking Approach for Rank Aggregation in \"information retrieval\" Mohamed Farah Lamsade, Paris Dauphine University Place du Mal de Lattre de Tassigny 75775 Paris Cedex 16, France farah@lamsade.dauphine.fr Daniel Vanderpooten Lamsade, Paris Dauphine University Place du Mal de LattreDe Tassigny 75775 Paris Cedex 16, Francia vdp@lamsade.dauphine.fr investigación abstracta en \"recuperación de información\" generalmente muestra una mejora del rendimiento cuando se combinan muchas fuentes de evidencia para producir una clasificación de documentos (por ejemplo, textos, imágenes, sonidos, etc.).",
                "Mostramos que el método propuesto trata bien con las características distintivas de \"recuperación de información\".",
                "Introducción Una amplia gama de enfoques actuales de \"recuperación de información\" (IR) se basa en varios modelos de búsqueda (booleano, espacio vectorial, probabilístico, lenguaje, etc. [2]) para recuperar documentos relevantes en respuesta a una solicitud de usuario.",
                "\"Recuperación de información\" moderna.",
                "Combinando evidencia de múltiples representaciones de consultas para la \"recuperación de información\".",
                "\"Recuperación de información\", 1 (3): 151-173, 1999."
            ],
            "translated_text": "",
            "candidates": [
                "recuperación de información",
                "information retrieval",
                "recuperación de información",
                "recuperación de información",
                "recuperación de información",
                "recuperación de información",
                "recuperación de información",
                "recuperación de información",
                "Recuperación de información",
                "recuperación de información",
                "recuperación de información",
                "Recuperación de información",
                "Recuperación de información"
            ],
            "error": []
        },
        "datum fusion problem": {
            "translated_key": "Problema de fusión de dato",
            "is_in_text": false,
            "original_annotated_sentences": [
                "An Outranking Approach for Rank Aggregation in Information Retrieval Mohamed Farah Lamsade, Paris Dauphine University Place du Mal de Lattre de Tassigny 75775 Paris Cedex 16, France farah@lamsade.dauphine.fr Daniel Vanderpooten Lamsade, Paris Dauphine University Place du Mal de Lattre de Tassigny 75775 Paris Cedex 16, France vdp@lamsade.dauphine.fr ABSTRACT Research in Information Retrieval usually shows performance improvement when many sources of evidence are combined to produce a ranking of documents (e.g., texts, pictures, sounds, etc.).",
                "In this paper, we focus on the rank aggregation problem, also called data fusion problem, where rankings of documents, searched into the same collection and provided by multiple methods, are combined in order to produce a new ranking.",
                "In this context, we propose a rank aggregation method within a multiple criteria framework using aggregation mechanisms based on decision rules identifying positive and negative reasons for judging whether a document should get a better rank than another.",
                "We show that the proposed method deals well with the Information Retrieval distinctive features.",
                "Experimental results are reported showing that the suggested method performs better than the well-known CombSUM and CombMNZ operators.",
                "Categories and Subject Descriptors: H.3.3 [Information Systems]: Information Search and Retrieval - Retrieval models.",
                "General Terms: Algorithms, Measurement, Experimentation, Performance, Theory. 1.",
                "INTRODUCTION A wide range of current Information Retrieval (IR) approaches are based on various search models (Boolean, Vector Space, Probabilistic, Language, etc. [2]) in order to retrieve relevant documents in response to a user request.",
                "The result lists produced by these approaches depend on the exact definition of the relevance concept.",
                "Rank aggregation approaches, also called data fusion approaches, consist in combining these result lists in order to produce a new and hopefully better ranking.",
                "Such approaches give rise to metasearch engines in the Web context.",
                "We consider, in the following, cases where only ranks are available and no other additional information is provided such as the relevance scores.",
                "This corresponds indeed to the reality, where only ordinal information is available.",
                "Data fusion is also relevant in other contexts, such as when the user writes several queries of his/her information need (e.g., a boolean query and a natural language query) [4], or when many document surrogates are available [16].",
                "Several studies argued that rank aggregation has the potential of combining effectively all the various sources of evidence considered in various input methods.",
                "For instance, experiments carried out in [16], [30], [4] and [19] showed that documents which appear in the lists of the majority of the input methods are more likely to be relevant.",
                "Moreover, Lee [19] and Vogt and Cottrell [31] found that various retrieval approaches often return very different irrelevant documents, but many of the same relevant documents.",
                "Bartell et al. [3] also found that rank aggregation methods improve the performances w.r.t. those of the input methods, even when some of them have weak individual performances.",
                "These methods also tend to smooth out biases of the input methods according to Montague and Aslam [22].",
                "Data fusion has recently been proved to improve performances for both the ad-hoc retrieval and categorization tasks within the TREC genomics track in 2005 [1].",
                "The rank aggregation problem was addressed in various fields such as i) in social choice theory which studies voting algorithms which specify winners of elections or winners of competitions in tournaments [29], ii) in statistics when studying correlation between rankings, iii) in distributed databases when results from different databases must be combined [12], and iv) in collaborative filtering [23].",
                "Most current rank aggregation methods consider each input ranking as a permutation over the same set of items.",
                "They also give rigid interpretation to the exact ranking of the items.",
                "Both of these assumptions are rather not valid in the IR context, as will be shown in the following sections.",
                "The remaining of the paper is organized as follows.",
                "We first review current rank aggregation methods in Section 2.",
                "Then we outline the specificities of the data fusion problem in the IR context (Section 3).",
                "In Section 4, we present a new aggregation method which is proven to best fit the IR context.",
                "Experimental results are presented in Section 5 and conclusions are provided in a final section. 2.",
                "RELATED WORK As pointed out by Riker [25], we can distinguish two families of rank aggregation methods: positional methods which assign scores to items to be ranked according to the ranks they receive and majoritarian methods which are based on pairwise comparisons of items to be ranked.",
                "These two families of methods find their roots in the pioneering works of Borda [5] and Condorcet [7], respectively, in the social choice literature. 2.1 Preliminaries We first introduce some basic notations to present the rank aggregation methods in a uniform way.",
                "Let D = {d1, d2, . . . , dnd } be a set of nd documents.",
                "A list or a ranking j is an ordering defined on Dj ⊆ D (j = 1, . . . , n).",
                "Thus, di j di means di is ranked better than di in j.",
                "When Dj = D, j is said to be a full list.",
                "Otherwise, it is a partial list.",
                "If di belongs to Dj, rj i denotes the rank or position of di in j.",
                "We assume that the best answer (document) is assigned the position 1 and the worst one is assigned the position |Dj|.",
                "Let D be the set of all permutations on D or all subsets of D. A profile is a n-tuple of rankings PR = ( 1, 2, . . . , n).",
                "Restricting PR to the rankings containing document di defines PRi.",
                "We also call the number of rankings which contain document di the rank hits of di [19].",
                "The rank aggregation or data fusion problem consists of finding a ranking function or mechanism Ψ (also called a social welfare function in the social choice theory terminology) defined by: Ψ : n D → D PR = ( 1, 2, . . . , n) → σ = Ψ(PR) where σ is called a consensus ranking. 2.2 Positional Methods 2.2.1 Borda Count This method [5] first assigns a score n j=1 rj i to each document di.",
                "Documents are then ranked by increasing order of this score, breaking ties, if any, arbitrarily. 2.2.2 Linear Combination Methods This family of methods basically combine scores of documents.",
                "When used for the rank aggregation problem, ranks are assumed to be scores or performances to be combined using aggregation operators such as the weighted sum or some variation of it [3, 31, 17, 28].",
                "For instance, Callan et al. [6] used the inference networks model [30] to combine rankings.",
                "Fox and Shaw [15] proposed several combination strategies which are CombSUM, CombMIN, CombMAX, CombANZ and CombMNZ.",
                "The first three operators correspond to the sum, min and max operators, respectively.",
                "CombANZ and CombMNZ respectively divides and multiplies the CombSUM score by the rank hits.",
                "It is shown in [19] that the CombSUM and CombMNZ operators perform better than the others.",
                "Metasearch engines such as SavvySearch and MetaCrawler use the CombSUM strategy to fuse rankings. 2.2.3 Footrule Optimal Aggregation In this method, a consensus ranking minimizes the Spearman footrule distance from the input rankings [21].",
                "Formally, given two full lists j and j , this distance is given by F( j, j ) = nd i=1 |rj i − rj i |.",
                "It extends to several lists as follows.",
                "Given a profile PR and a consensus ranking σ, the Spearman footrule distance of σ to PR is given by F(σ, PR) = n j=1 F(σ, j).",
                "Cook and Kress [8] proposed a similar method which consists in optimizing the distance D( j, j ) = 1 2 nd i,i =1 |rj i,i − rj i,i |, where rj i,i = rj i −rj i .",
                "This formulation has the advantage that it considers the intensity of preferences. 2.2.4 Probabilistic Methods This kind of methods assume that the performance of the input methods on a number of training queries is indicative of their future performance.",
                "During the training process, probabilities of relevance are calculated.",
                "For subsequent queries, documents are ranked based on these probabilities.",
                "For instance, in [20], each input ranking j is divided into a number of segments, and the conditional probability of relevance (R) of each document di depending on the segment k it occurs in, is computed, i.e. prob(R|di, k, j).",
                "For subsequent queries, the score of each document di is given by n j=1 prob(R|di,k, j ) k .",
                "Le Calve and Savoy [18] suggest using a logistic regression approach for combining scores.",
                "Training data is needed to infer the model parameters. 2.3 Majoritarian Methods 2.3.1 Condorcet Procedure The original Condorcet rule [7] specifies that a winner of the election is any item that beats or ties with every other item in a pairwise contest.",
                "Formally, let C(diσdi ) = { j∈ PR : di j di } be the coalition of rankings that are concordant with establishing diσdi , i.e. with the proposition di should be ranked better than di in the final ranking σ. di beats or ties with di iff |C(diσdi )| ≥ |C(di σdi)|.",
                "The repetitive application of the Condorcet algorithm can produce a ranking of items in a natural way: select the Condorcet winner, remove it from the lists, and repeat the previous two steps until there are no more documents to rank.",
                "Since there is not always Condorcet winners, variations of the Condorcet procedure have been developed within the multiple criteria decision aid theory, with methods such as ELECTRE [26]. 2.3.2 Kemeny Optimal Aggregation As in section 2.2.3, a consensus ranking minimizes a geometric distance from the input rankings, where the Kendall tau distance is used instead of the Spearman footrule distance.",
                "Formally, given two full lists j and j , the Kendall tau distance is given by K( j, j ) = |{(di, di ) : i < i , rj i < rj i , rj i > rj i }|, i.e. the number of pairwise disagreements between the two lists.",
                "It is easy to show that the consensus ranking corresponds to the geometric median of the input rankings and that the Kemeny optimal aggregation problem corresponds to the minimum feedback edge set problem. 2.3.3 Markov Chain Methods Markov chains (MCs) have been used by Dwork et al. [11] as a natural method to obtain a consensus ranking where states correspond to the documents to be ranked and the transition probabilities vary depending on the interpretation of the transition event.",
                "In the same reference, the authors proposed four specific MCs and experimental testing had shown that the following MC is the best performing one (see also [24]): • MC4: move from the current state di to the next state di by first choosing a document di uniformly from D. If for the majority of the rankings, we have rj i ≤ rj i , then move to di , else stay in di.",
                "The consensus ranking corresponds to the stationary distribution of MC4. 3.",
                "SPECIFICITIES OF THE RANK AGGREGATION PROBLEM IN THE IR CONTEXT 3.1 Limited Significance of the Rankings The exact positions of documents in one input ranking have limited significance and should not be overemphasized.",
                "For instance, having three relevant documents in the first three positions, any perturbation of these three items will have the same value.",
                "Indeed, in the IR context, the complete order provided by an input method may hide ties.",
                "In this case, we call such rankings semi orders.",
                "This was outlined in [13] as the problem of aggregation with ties.",
                "It is therefore important to build the consensus ranking based on robust information: • Documents with near positions in j are more likely to have similar interest or relevance.",
                "Thus a slight perturbation of the initial ranking is meaningless. • Assuming that document di is better ranked than document di in a ranking j, di is more likely to be definitively more relevant than di in j when the number of intermediate positions between di and di increases. 3.2 Partial Lists In real world applications, such as metasearch engines, rankings provided by the input methods are often partial lists.",
                "This was outlined in [14] as the problem of having to merge top-k results from various input lists.",
                "For instance, in the experiments carried out by Dwork et al. [11], authors found that among the top 100 best documents of 7 input search engines, 67% of the documents were present in only one search engine, whereas less than two documents were present in all the search engines.",
                "Rank aggregation of partial lists raises four major difficulties which we state hereafter, proposing for each of them various working assumptions: 1.",
                "Partial lists can have various lengths, which can favour long lists.",
                "We thus consider the following two working hypotheses: H1 k : We only consider the top k best documents from each input ranking.",
                "H1 all: We consider all the documents from each input ranking. 2.",
                "Since there are different documents in the input rankings, we must decide which documents should be kept in the consensus ranking.",
                "Two working hypotheses are therefore considered: H2 k : We only consider documents which are present in at least k input rankings (k > 1).",
                "H2 all: We consider all the documents which are ranked in at least one input ranking.",
                "Hereafter, we call documents which will be retained in the consensus ranking, candidate documents, and documents that will be excluded from the consensus ranking, excluded documents.",
                "We also call a candidate document which is missing in one or more rankings, a missing document. 3.",
                "Some candidate documents are missing documents in some input rankings.",
                "Main reasons for a missing document are that it was not indexed or it was indexed but deemed irrelevant ; usually this information is not available.",
                "We consider the following two working hypotheses: H3 yes: Each missing document in each j is assigned a position.",
                "H3 no: No assumption is made, that is each missing document is considered neither better nor worse than any other document. 4.",
                "When assumption H2 k holds, each input ranking may contain documents which will not be considered in the consensus ranking.",
                "Regarding the positions of the candidate documents, we can consider the following working hypotheses: H4 init: The initial positions of candidate documents are kept in each input ranking.",
                "H4 new: Candidate documents receive new positions in each input ranking, after discarding excluded ones.",
                "In the IR context, rank aggregation methods need to decide more or less explicitly which assumptions to retain w.r.t. the above-mentioned difficulties. 4.",
                "OUTRANKING APPROACH FOR RANK AGGREGATION 4.1 Presentation Positional methods consider implicitly that the positions of the documents in the input rankings are scores giving thus a cardinal meaning to an ordinal information.",
                "This constitutes a strong assumption that is questionable, especially when the input rankings have different lengths.",
                "Moreover, for positional methods, assumptions H3 and H4 , which are often arbitrary, have a strong impact on the results.",
                "For instance, let us consider an input ranking of 500 documents out of 1000 candidate documents.",
                "Whether we assign to each of the missing documents the position 1, 501, 750 or 1000 -corresponding to variations of H3 yes- will give rise to very contrasted results, especially regarding the top of the consensus ranking.",
                "Majoritarian methods do not suffer from the above-mentioned drawbacks of the positional methods since they build consensus rankings exploiting only ordinal information contained in the input rankings.",
                "Nevertheless, they suppose that such rankings are complete orders, ignoring that they may hide ties.",
                "Therefore, majoritarian methods base consensus rankings on illusory discriminant information rather than less discriminant but more robust information.",
                "Trying to overcome the limits of current rank aggregation methods, we found that outranking approaches, which were initially used for multiple criteria aggregation problems [26], can also be used for the rank aggregation purpose, where each ranking plays the role of a criterion.",
                "Therefore, in order to decide whether a document di should be ranked better than di in the consensus ranking σ, the two following conditions should be met: • a concordance condition which ensures that a majority of the input rankings are concordant with diσdi (majority principle). • a discordance condition which ensures that none of the discordant input rankings strongly refutes dσd (respect of minorities principle).",
                "Formally, the concordance coalition with diσdi is Csp (diσdi ) = { j∈ PR : rj i ≤ rj i − sp} where sp is a preference threshold which is the variation of document positions -whether it is absolute or relative to the ranking length- which draws the boundaries between an indifference and a preference situation between documents.",
                "The discordance coalition with diσdi is Dsv (diσdi ) = { j∈ PR : rj i ≥ rj i + sv} where sv is a veto threshold which is the variation of document positions -whether it is absolute or relative to the ranking length- which draws the boundaries between a weak and a strong opposition to diσdi .",
                "Depending on the exact definition of the preceding concordance and discordance coalitions leading to the definition of some decision rules, several outranking relations can be defined.",
                "They can be more or less demanding depending on i) the values of the thresholds sp and sv, ii) the importance or minimal size cmin required for the concordance coalition, and iii) the importance or maximum size dmax of the discordance coalition.",
                "A generic outranking relation can thus be defined as follows: diS(sp,sv,cmin,dmax)di ⇔ |Csp (diσdi )| ≥ cmin AND |Dsv (diσdi )| ≤ dmax This expression defines a family of nested outranking relations since S(sp,sv,cmin,dmax) ⊆ S(sp,sv,cmin,dmax) when cmin ≥ cmin and/or dmax ≤ dmax and/or sp ≥ sp and/or sv ≤ sv.",
                "This expression also generalizes the majority rule which corresponds to the particular relation S(0,∞, n 2 ,n).",
                "It also satisfies important properties of rank aggregation methods, called neutrality, Pareto-optimality, Condorcet property and Extended Condorcet property, in the social choice literature [29].",
                "Outranking relations are not necessarily transitive and do not necessarily correspond to rankings since directed cycles may exist.",
                "Therefore, we need specific procedures in order to derive a consensus ranking.",
                "We propose the following procedure which finds its roots in [27].",
                "It consists in partitioning the set of documents into r ranked classes.",
                "Each class Ch contains documents with the same relevance and results from the application of all relations (if possible) to the set of documents remaining after previous classes are computed.",
                "Documents within the same equivalence class are ranked arbitrarily.",
                "Formally, let • R be the set of candidate documents for a query, • S1 , S2 , . . . be a family of nested outranking relations, • Fk(di, E) = |{di ∈ E : diSk di }| be the number of documents in E(E ⊆ R) that could be considered worse than di according to relation Sk , • fk(di, E) = |{di ∈ E : di Sk di}| be the number of documents in E that could be considered better than di according to Sk , • sk(di, E) = Fk(di, E) − fk(di, E) be the qualification of di in E according to Sk .",
                "Each class Ch results from a distillation process.",
                "It corresponds to the last distillate of a series of sets E0 ⊇ E1 ⊇ . . . where E0 = R \\ (C1 ∪ . . . ∪ Ch−1) and Ek is a reduced subset of Ek−1 resulting from the application of the following procedure: 1. compute for each di ∈ Ek−1 its qualification according to Sk , i.e. sk(di, Ek−1), 2. define smax = maxdi∈Ek−1 {sk(di, Ek−1)}, then 3.",
                "Ek = {di ∈ Ek−1 : sk(di, Ek−1) = smax} When one outranking relation is used, the distillation process stops after the first application of the previous procedure, i.e., Ch corresponds to distillate E1.",
                "When different outranking relations are used, the distillation process stops when all the pre-defined outranking relations have been used or when |Ek| = 1. 4.2 Illustrative Example This section illustrates the concepts and procedures of section 4.1.",
                "Let us consider a set of candidate documents R = {d1, d2, d3, d4, d5}.",
                "The following table gives a profile PR of different rankings of the documents of R: PR = ( 1 , 2, 3, 4).",
                "Table 1: Rankings of documents rj i 1 2 3 4 d1 1 3 1 5 d2 2 1 3 3 d3 3 2 2 1 d4 4 4 5 2 d5 5 5 4 4 Let us suppose that the preference and veto thresholds are set to values 1 and 4 respectively, and that the concordance and discordance thresholds are set to values 2 and 1 respectively.",
                "The following tables give the concordance, discordance and outranking matrices.",
                "Each entry csp (di, di ) (dsv (di, di )) in the concordance (discordance) matrix gives the number of rankings that are concordant (discordant) with diσdi , i.e. csp (di, di ) = |Csp (diσdi )| and dsv (di, di ) = |Dsv (diσdi )|.",
                "Table 2: Computation of the outranking relation d1 d2 d3 d4 d5 d1 - 2 2 3 3 d2 2 - 2 3 4 d3 2 2 - 4 4 d4 1 1 0 - 3 d5 1 0 0 1Concordance Matrix d1 d2 d3 d4 d5 d1 - 0 1 0 0 d2 0 - 0 0 0 d3 0 0 - 0 0 d4 1 0 0 - 0 d5 1 1 0 0Discordance Matrix d1 d2 d3 d4 d5 d1 - 1 1 1 1 d2 1 - 1 1 1 d3 1 1 - 1 1 d4 0 0 0 - 1 d5 0 0 0 0Outranking Matrix (S1) For instance, the concordance coalition for the assertion d1σd4 is C1(d1σd4) = { 1, 2, 3} and the discordance coalition for the same assertion is D4(d1σd4) = ∅.",
                "Therefore, c1(d1, d4) = 3, d4(d1, d4) = 0 and d1S1 d4 holds.",
                "Notice that Fk(di, R) (fk(di, R)) is given by summing the values of the ith row (column) of the outranking matrix.",
                "The consensus ranking is obtained as follows: to get the first class C1, we compute the qualifications of all the documents of E0 = R with respect to S1 .",
                "They are respectively 2, 2, 2, -2 and -4.",
                "Therefore smax equals 2 and C1 = E1 = {d1, d2, d3}.",
                "Observe that, if we had used a second outranking relation S2(⊇ S1), these three documents could have been possibly discriminated.",
                "At this stage, we remove documents of C1 from the outranking matrix and compute the next class C2: we compute the new qualifications of the documents of E0 = R \\ C1 = {d4, d5}.",
                "They are respectively 1 and -1.",
                "So C3 = E1 = {d4}.",
                "The last document d5 is the only document of the last class C3.",
                "Thus, the consensus ranking is {d1, d2, d3} → {d4} → {d5}. 5.",
                "EXPERIMENTS AND RESULTS 5.1 Test Setting To facilitate empirical investigation of the proposed methodology, we developed a prototype metasearch engine that implements a version of our outranking approach for rank aggregation.",
                "In this paper, we apply our approach to the Topic Distillation (TD) task of TREC-2004 Web track [10].",
                "In this task, there are 75 topics where only a short description of each is given.",
                "For each query, we retained the rankings of the 10 best runs of the TD task which are provided by TREC-2004 participating teams.",
                "The performances of these runs are reported in table 3.",
                "Table 3: Performances of the 10 best runs of the TD task of TREC-2004 Run Id MAP P@10 S@1 S@5 S@10 uogWebCAU150 17.9% 24.9% 50.7% 77.3% 89.3% MSRAmixed1 17.8% 25.1% 38.7% 72.0% 88.0% MSRC04C12 16.5% 23.1% 38.7% 74.7% 80.0% humW04rdpl 16.3% 23.1% 37.3% 78.7% 90.7% THUIRmix042 14.7% 20.5% 21.3% 58.7% 74.7% UAmsT04MWScb 14.6% 20.9% 36.0% 66.7% 76.0% ICT04CIIS1AT 14.1% 20.8% 33.3% 64.0% 78.7% SJTUINCMIX5 12.9% 18.9% 29.3% 57.3% 72.0% MU04web1 11.5% 19.9% 33.3% 64.0% 76.0% MeijiHILw3 11.5% 15.3% 30.7% 54.7% 64.0% Average 14.7% 21.2% 34.9% 66.8% 78.94% For each query, each run provides a ranking of about 1000 documents.",
                "The number of documents retrieved by all these runs ranges from 543 to 5769.",
                "Their average (median) number is 3340 (3386).",
                "It is worth noting that we found similar distributions of the documents among the rankings as in [11].",
                "For evaluation, we used the trec eval standard tool which is used by the TREC community to calculate the standard measures of system effectiveness which are Mean Average Precision (MAP) and Success@n (S@n) for n=1, 5 and 10.",
                "Our approach effectiveness is compared against some high performing official results from TREC-2004 as well as against some standard rank aggregation algorithms.",
                "In the experiments, significance testing is mainly based on the t-student statistic which is computed on the basis of the MAP values of the compared runs.",
                "In the tables of the following section, statistically significant differences are marked with an asterisk.",
                "Values between brackets of the first column of each table, indicate the parameter value of the corresponding run. 5.2 Results We carried out several series of runs in order to i) study performance variations of the outranking approach when tuning the parameters and working assumptions, ii) compare performances of the outranking approach vs standard rank aggregation strategies , and iii) check whether rank aggregation performs better than the best input rankings.",
                "We set our basic run mcm with the following parameters.",
                "We considered that each input ranking is a complete order (sp = 0) and that an input ranking strongly refutes diσdi when the difference of both document positions is large enough (sv = 75%).",
                "Preference and veto thresholds are computed proportionally to the number of documents retained in each input ranking.",
                "They consequently may vary from one ranking to another.",
                "In addition, to accept the assertion diσdi , we supposed that the majority of the rankings must be concordant (cmin = 50%) and that every input ranking can impose its veto (dmax = 0).",
                "Concordance and discordance thresholds are computed for each tuple (di, di ) as the percentage of the input rankings of PRi ∩PRi .",
                "Thus, our choice of parameters leads to the definition of the outranking relation S(0,75%,50%,0).",
                "To test the run mcm, we had chosen the following assumptions.",
                "We retained the top 100 best documents from each input ranking (H1 100), only considered documents which are present in at least half of the input rankings (H2 5 ) and assumed H3 no and H4 new.",
                "In these conditions, the number of successful documents was about 100 on average, and the computation time per query was less than one second.",
                "Obviously, modifying the working assumptions should have deeper impact on the performances than tuning our model parameters.",
                "This was validated by preliminary experiments.",
                "Thus, we hereafter begin by studying performance variation when different sets of assumptions are considered.",
                "Afterwards, we study the impact of tuning parameters.",
                "Finally, we compare our model performances w.r.t. the input rankings as well as some standard data fusion algorithms. 5.2.1 Impact of the Working Assumptions Table 4 summarizes the performance variation of the outranking approach under different working hypotheses.",
                "In Table 4: Impact of the working assumptions Run Id MAP S@1 S@5 S@10 mcm 18.47% 41.33% 81.33% 86.67% mcm22 (H3 yes) 17.72% (-4.06%) 34.67% 81.33% 86.67% mcm23 (H4 init) 18.26% (-1.14%) 41.33% 81.33% 86.67% mcm24 (H1 all) 20.67% (+11.91%*) 38.66% 80.00% 86.66% mcm25 (H2 all) 21.68% (+17.38%*) 40.00% 78.66% 89.33% this table, we first show that run mcm22, in which missing documents are all put in the same last position of each input ranking, leads to performance drop w.r.t. run mcm.",
                "Moreover, S@1 moves from 41.33% to 34.67% (-16.11%).",
                "This shows that several relevant documents which were initially put at the first position of the consensus ranking in mcm, lose this first position but remain ranked in the top 5 documents since S@5 did not change.",
                "We also conclude that documents which have rather good positions in some input rankings are more likely to be relevant, even though they are missing in some other rankings.",
                "Consequently, when they are missing in some rankings, assigning worse ranks to these documents is harmful for performance.",
                "Also, from Table 4, we found that the performances of runs mcm and mcm23 are similar.",
                "Therefore, the outranking approach is not sensitive to keeping the initial positions of candidate documents or recomputing them by discarding excluded ones.",
                "From the same Table 4, performance of the outranking approach increases significantly for runs mcm24 and mcm25.",
                "Therefore, whether we consider all the documents which are present in half of the rankings (mcm24) or we consider all the documents which are ranked in the first 100 positions in one or more rankings (mcm25), increases performances.",
                "This result was predictable since in both cases we have more detailed information on the relative importance of documents.",
                "Tables 5 and 6 confirm this evidence.",
                "Table 5, where values between brackets of the first column give the number of documents which are retained from each input ranking, shows that selecting more documents from each input ranking leads to performance increase.",
                "It is worth mentioning that selecting more than 600 documents from each input ranking does not improve performance.",
                "Table 5: Impact of the number of retained documents Run Id MAP S@1 S@5 S@10 mcm (100) 18.47% 41.33% 81.33% 86.67% mcm24-1 (200) 19.32% (+4.60%) 42.67% 78.67% 88.00% mcm24-2 (400) 19.88% (+7.63%*) 37.33% 80.00% 88.00% mcm24-3 (600) 20.80% (+12.62%*) 40.00% 80.00% 88.00% mcm24-4 (800) 20.66% (+11.86%*) 40.00% 78.67% 86.67% mcm24 (1000) 20.67% (+11.91%*) 38.66% 80.00% 86.66% Table 6 reports runs corresponding to variations of H2 k .",
                "Values between brackets are rank hits.",
                "For instance, in the run mcm32, only documents which are present in 3 or more input rankings, were considered successful.",
                "This table shows that performance is significantly better when rare documents are considered, whereas it decreases significantly when these documents are discarded.",
                "Therefore, we conclude that many of the relevant documents are retrieved by a rather small set of IR models.",
                "Table 6: Performance considering different rank hits Run Id MAP S@1 S@5 S@10 mcm25 (1) 21.68% (+17.38%*) 40.00% 78.67% 89.33% mcm32 (3) 18.98% (+2.76%) 38.67% 80.00% 85.33% mcm (5) 18.47% 41.33% 81.33% 86.67% mcm33 (7) 15.83% (-14.29%*) 37.33% 78.67% 85.33% mcm34 (9) 10.96% (-40.66%*) 36.11% 66.67% 70.83% mcm35 (10) 7.42% (-59.83%*) 39.22% 62.75% 64.70% For both runs mcm24 and mcm25, the number of successful documents was about 1000 and therefore, the computation time per query increased and became around 5 seconds. 5.2.2 Impact of the Variation of the Parameters Table 7 shows performance variation of the outranking approach when different preference thresholds are considered.",
                "We found performance improvement up to threshold values of about 5%, then there is a decrease in the performance which becomes significant for threshold values greater than 10%.",
                "Moreover, S@1 improves from 41.33% to 46.67% when preference threshold changes from 0 to 5%.",
                "We can thus conclude that the input rankings are semi orders rather than complete orders.",
                "Table 8 shows the evolution of the performance measures w.r.t. the concordance threshold.",
                "We can conclude that in order to put document di before di in the consensus ranking, Table 7: Impact of the variation of the preference threshold from 0 to 12.5% Run Id MAP S@1 S@5 S@10 mcm (0%) 18.47% 41.33% 81.33% 86.67% mcm1 (1%) 18.57% (+0.54%) 41.33% 81.33% 86.67% mcm2 (2.5%) 18.63% (+0.87%) 42.67% 78.67% 86.67% mcm3 (5%) 18.69% (+1.19%) 46.67% 81.33% 86.67% mcm4 (7.5%) 18.24% (-1.25%) 46.67% 81.33% 86.67% mcm5 (10%) 17.93% (-2.92%) 40.00% 82.67% 86.67% mcm5b (12.5%) 17.51% (-5.20%*) 41.33% 80.00% 86.67% at least half of the input rankings of PRi ∩ PRi should be concordant.",
                "Performance drops significantly for very low and very high values of the concordance threshold.",
                "In fact, for such values, the concordance condition is either fulfilled rather always by too many document pairs or not fulfilled at all, respectively.",
                "Therefore, the outranking relation becomes either too weak or too strong respectively.",
                "Table 8: Impact of the variation of cmin Run Id MAP S@1 S@5 S@10 mcm11 (20%) 17.63% (-4.55%*) 41.33% 76.00% 85.33% mcm12 (40%) 18.37% (-0.54%) 42.67% 76.00% 86.67% mcm (50%) 18.47% 41.33% 81.33% 86.67% mcm13 (60%) 18.42% (-0.27%) 40.00% 78.67% 86.67% mcm14 (80%) 17.43% (-5.63%*) 40.00% 78.67% 86.67% mcm15 (100%) 16.12% (-12.72%*) 41.33% 70.67% 85.33% In the experiments, varying the veto threshold as well as the discordance threshold within reasonable intervals does not have significant impact on performance measures.",
                "In fact, runs with different veto thresholds (sv ∈ [50%; 100%]) had similar performances even though there is a slight advantage for runs with high threshold values which means that it is better not to allow the input rankings to put their veto easily.",
                "Also, tuning the discordance threshold was carried out for values 50% and 75% of the veto threshold.",
                "For these runs we did not get any noticeable performance variation, although for low discordance thresholds (dmax < 20%), performance slightly decreased. 5.2.3 Impact of the Variation of the Number of Input Rankings To study performance evolution when different sets of input rankings are considered, we carried three more runs where 2, 4, and 6 of the best performing sets of the input rankings are considered.",
                "Results reported in Table 9 are seemingly counter-intuitive and also do not support previous findings regarding rank aggregation research [3].",
                "Nevertheless, this result shows that low performing rankings bring more noise than information to the establishment of the consensus ranking.",
                "Therefore, when they are considered, performance decreases.",
                "Table 9: Performance considering different best performing sets of input rankings Run Id MAP S@1 S@5 S@10 mcm (10) 18.47% 41.33% 81.33% 86.67% mcm27 (6) 18.60% (+0.70%) 41.33% 80.00% 85.33% mcm28 (4) 19.02% (+2.98%) 40.00% 86.67% 88.00% mcm29 (2) 18.33% (-0.76%) 44.00% 76.00% 88.00% 5.2.4 Comparison of the Performance of Different Rank Aggregation Methods In this set of runs, we compare the outranking approach with some standard rank aggregation methods which were proven to have acceptable performance in previous studies: we considered two positional methods which are the CombSUM and the CombMNZ strategies.",
                "We also examined the performance of one majoritarian method which is the Markov chain method (MC4).",
                "For the comparisons, we considered a specific outranking relation S∗ = S(5%,50%,50%,30%) which results in good overall performances when tuning all the parameters.",
                "The first row of Table 10 gives performances of the rank aggregation methods w.r.t. a basic assumption set A1 = (H1 100, H2 5 , H4 new): we only consider the 100 first documents from each ranking, then retain documents present in 5 or more rankings and update ranks of successful documents.",
                "For positional methods, we place missing documents at the queue of the ranking (H3 yes) whereas for our method as well as for MC4, we retained hypothesis H3 no.",
                "The three following rows of Table 10 report performances when changing one element from the basic assumption set: the second row corresponds to the assumption set A2 = (H1 1000, H2 5 , H4 new), i.e. changing the number of retained documents from 100 to 1000.",
                "The third row corresponds to the assumption set A3 = (H1 100, H2 all, H4 new), i.e. considering the documents present in at least one ranking.",
                "The fourth row corresponds to the assumption set A4 = (H1 100, H2 5 , H4 init), i.e. keeping the original ranks of successful documents.",
                "The fifth row of Table 10, labeled A5, gives performance when all the 225 queries of the Web track of TREC-2004 are considered.",
                "Obviously, performance level cannot be compared with previous lines since the additional queries are different from the TD queries and correspond to other tasks (Home Page and Named Page tasks [10]) of TREC-2004 Web track.",
                "This set of runs aims to show whether relative performance of the various methods is task-dependent.",
                "The last row of Table 10, labeled A6, reports performance of the various methods considering the TD task of TREC2002 instead of TREC-2004: we fused the results of input rankings of the 10 best official runs for each of the 50 TD queries [9] considering the set of assumptions A1 of the first row.",
                "This aims to show whether relative performance of the various methods changes from year to year.",
                "Values between brackets of Table 10 are variations of performance of each rank aggregation method w.r.t. performance of the outranking approach.",
                "Table 10: Performance (MAP) of different rank aggregation methods under 3 different test collections mcm combSUM combMNZ markov A1 18.79% 17.54% (-6.65%*) 17.08% (-9.10%*) 18.63% (-0.85%) A2 21.36% 19.18% (-10.21%*) 18.61% (-12.87%*) 21.33% (-0.14%) A3 21.92% 21.38% (-2.46%) 20.88% (-4.74%) 19.35% (-11.72%*) A4 18.64% 17.58% (-5.69%*) 17.18% (-7.83%*) 18.63% (-0.05%) A5 55.39% 52.16% (-5.83%*) 49.70% (-10.27%*) 53.30% (-3.77%) A6 16.95% 15.65% (-7.67%*) 14.57% (-14.04%*) 16.39% (-3.30%) From the analysis of table 10 the following can be established: • for all the runs, considering all the documents in each input ranking (A2) significantly improves performance (MAP increases by 11.62% on average).",
                "This is predictable since some initially unreported relevant documents would receive better positions in the consensus ranking. • for all the runs, considering documents even those present in only one input ranking (A3) significantly improves performance.",
                "For mcm, combSUM and combMNZ, performance improvement is more important (MAP increases by 20.27% on average) than for the markov run (MAP increases by 3.86%). • preserving the initial positions of documents (A4) or recomputing them (A1) does not have a noticeable influence on performance for both positional and majoritarian methods. • considering all the queries of the Web track of TREC2004 (A5) as well as the TD queries of the Web track of TREC-2002 (A6) does not alter the relative performance of the different data fusion methods. • considering the TD queries of the Web track of TREC2002, performances of all the data fusion methods are lower than that of the best performing input ranking for which the MAP value equals 18.58%.",
                "This is because most of the fused input rankings have very low performances compared to the best one, which brings more noise to the consensus ranking. • performances of the data fusion methods mcm and markov are significantly better than that of the best input ranking uogWebCAU150.",
                "This remains true for runs combSUM and combMNZ only under assumptions H1 all or H2 all.",
                "This shows that majoritarian methods are less sensitive to assumptions than positional methods. • outranking approach always performs significantly better than positional methods combSUM and combMNZ.",
                "It has also better performances than the Markov chain method, especially under assumption H2 all where difference of performances becomes significant. 6.",
                "CONCLUSIONS In this paper, we address the rank aggregation problem where different, but not disjoint, lists of documents are to be fused.",
                "We noticed that the input rankings can hide ties, so they should not be considered as complete orders.",
                "Only robust information should be used from each input ranking.",
                "Current rank aggregation methods, and especially positional methods (e.g. combSUM [15]), are not initially designed to work with such rankings.",
                "They should be adapted by considering specific working assumptions.",
                "We propose a new outranking method for rank aggregation which is well adapted to the IR context.",
                "Indeed, it ranks two documents w.r.t. the intensity of their positions difference in each input ranking and also considering the number of the input rankings that are concordant and discordant in favor of a specific document.",
                "There is also no need to make specific assumptions on the positions of the missing documents.",
                "This is an important feature since the absence of a document from a ranking should not be necessarily interpreted negatively.",
                "Experimental results show that the outranking method significantly out-performs popular classical positional data fusion methods like combSUM and combMNZ strategies.",
                "It also out-performs a good performing majoritarian methods which is the Markov chain method.",
                "These results are tested against different test collections and queries.",
                "From the experiments, we can also conclude that in order to improve the performances, we should fuse result lists of well performing IR models, and that majoritarian data fusion methods perform better than positional methods.",
                "The proposed method can have a real impact on Web metasearch performances since only ranks are available from most primary search engines, whereas most of the current approaches need scores to merge result lists into one single list.",
                "Further work involves investigating whether the outranking approach performs well in various other contexts, e.g. using the document scores or some combination of document ranks and scores.",
                "Acknowledgments The authors would like to thank Jacques Savoy for his valuable comments on a preliminary version of this paper. 7.",
                "REFERENCES [1] A. Aronson, D. Demner-Fushman, S. Humphrey, J. Lin, H. Liu, P. Ruch, M. Ruiz, L. Smith, L. Tanabe, and W. Wilbur.",
                "Fusion of knowledge-intensive and statistical approaches for retrieving and annotating textual genomics documents.",
                "In Proceedings TREC2005.",
                "NIST Publication, 2005. [2] R. A. Baeza-Yates and B.",
                "A. Ribeiro-Neto.",
                "Modern Information Retrieval.",
                "ACM Press , 1999. [3] B. T. Bartell, G. W. Cottrell, and R. K. Belew.",
                "Automatic combination of multiple ranked retrieval systems.",
                "In Proceedings ACM-SIGIR94, pages 173-181.",
                "Springer-Verlag, 1994. [4] N. J. Belkin, P. Kantor, E. A.",
                "Fox, and J.",
                "A. Shaw.",
                "Combining evidence of multiple query representations for information retrieval.",
                "IPM, 31(3):431-448, 1995. [5] J. Borda.",
                "M´emoire sur les ´elections au scrutin.",
                "Histoire de lAcad´emie des Sciences, 1781. [6] J. P. Callan, Z. Lu, and W. B. Croft.",
                "Searching distributed collections with inference networks.",
                "In Proceedings ACM-SIGIR95, pages 21-28, 1995. [7] M. Condorcet.",
                "Essai sur lapplication de lanalyse `a la probabilit´e des d´ecisions rendues `a la pluralit´e des voix.",
                "Imprimerie Royale, Paris, 1785. [8] W. D. Cook and M. Kress.",
                "Ordinal ranking with intensity of preference.",
                "Management Science, 31(1):26-32, 1985. [9] N. Craswell and D. Hawking.",
                "Overview of the TREC-2002 Web Track.",
                "In Proceedings TREC2002.",
                "NIST Publication, 2002. [10] N. Craswell and D. Hawking.",
                "Overview of the TREC-2004 Web Track.",
                "In Proceedings of TREC2004.",
                "NIST Publication, 2004. [11] C. Dwork, S. R. Kumar, M. Naor, and D. Sivakumar.",
                "Rank aggregation methods for the Web.",
                "In Proceedings WWW2001, pages 613-622, 2001. [12] R. Fagin.",
                "Combining fuzzy information from multiple systems.",
                "JCSS, 58(1):83-99, 1999. [13] R. Fagin, R. Kumar, M. Mahdian, D. Sivakumar, and E. Vee.",
                "Comparing and aggregating rankings with ties.",
                "In PODS, pages 47-58, 2004. [14] R. Fagin, R. Kumar, and D. Sivakumar.",
                "Comparing top k lists.",
                "SIAM J. on Discrete Mathematics, 17(1):134-160, 2003. [15] E. A.",
                "Fox and J.",
                "A. Shaw.",
                "Combination of multiple searches.",
                "In Proceedings of TREC3.",
                "NIST Publication, 1994. [16] J. Katzer, M. McGill, J. Tessier, W. Frakes, and P. DasGupta.",
                "A study of the overlap among document representations.",
                "Information Technology: Research and Development, 1(4):261-274, 1982. [17] L. S. Larkey, M. E. Connell, and J. Callan.",
                "Collection selection and results merging with topically organized U.S. patents and TREC data.",
                "In Proceedings ACM-CIKM2000, pages 282-289.",
                "ACM Press, 2000. [18] A.",
                "Le Calv´e and J. Savoy.",
                "Database merging strategy based on logistic regression.",
                "IPM, 36(3):341-359, 2000. [19] J. H. Lee.",
                "Analyses of multiple evidence combination.",
                "In Proceedings ACM-SIGIR97, pages 267-276, 1997. [20] D. Lillis, F. Toolan, R. Collier, and J. Dunnion.",
                "Probfuse: a probabilistic approach to data fusion.",
                "In Proceedings ACM-SIGIR2006, pages 139-146.",
                "ACM Press, 2006. [21] J. I. Marden.",
                "Analyzing and Modeling Rank Data.",
                "Number 64 in Monographs on Statistics and Applied Probability.",
                "Chapman & Hall, 1995. [22] M. Montague and J.",
                "A. Aslam.",
                "Metasearch consistency.",
                "In Proceedings ACM-SIGIR2001, pages 386-387.",
                "ACM Press, 2001. [23] D. M. Pennock and E. Horvitz.",
                "Analysis of the axiomatic foundations of collaborative filtering.",
                "In Workshop on AI for Electronic Commerce at the 16th National Conference on Artificial Intelligence, 1999. [24] M. E. Renda and U. Straccia.",
                "Web metasearch: rank vs. score based rank aggregation methods.",
                "In Proceedings ACM-SAC2003, pages 841-846.",
                "ACM Press, 2003. [25] W. H. Riker.",
                "Liberalism against populism.",
                "Waveland Press, 1982. [26] B. Roy.",
                "The outranking approach and the foundations of ELECTRE methods.",
                "Theory and Decision, 31:49-73, 1991. [27] B. Roy and J. Hugonnard.",
                "Ranking of suburban line extension projects on the Paris metro system by a multicriteria method.",
                "Transportation Research, 16A(4):301-312, 1982. [28] L. Si and J. Callan.",
                "Using sampled data and regression to merge search engine results.",
                "In Proceedings ACM-SIGIR2002, pages 19-26.",
                "ACM Press, 2002. [29] M. Truchon.",
                "An extension of the Condorcet criterion and Kemeny orders.",
                "Cahier 9813, Centre de Recherche en Economie et Finance Appliqu´ees, Oct. 1998. [30] H. Turtle and W. B. Croft.",
                "Inference networks for document retrieval.",
                "In Proceedings of ACM-SIGIR90, pages 1-24.",
                "ACM Press, 1990. [31] C. C. Vogt and G. W. Cottrell.",
                "Fusion via a linear combination of scores.",
                "Information Retrieval, 1(3):151-173, 1999."
            ],
            "original_annotated_samples": [],
            "translated_annotated_samples": [],
            "translated_text": "",
            "candidates": [],
            "error": []
        },
        "multiple criterion framework": {
            "translated_key": "Marco de criterios múltiples",
            "is_in_text": false,
            "original_annotated_sentences": [
                "An Outranking Approach for Rank Aggregation in Information Retrieval Mohamed Farah Lamsade, Paris Dauphine University Place du Mal de Lattre de Tassigny 75775 Paris Cedex 16, France farah@lamsade.dauphine.fr Daniel Vanderpooten Lamsade, Paris Dauphine University Place du Mal de Lattre de Tassigny 75775 Paris Cedex 16, France vdp@lamsade.dauphine.fr ABSTRACT Research in Information Retrieval usually shows performance improvement when many sources of evidence are combined to produce a ranking of documents (e.g., texts, pictures, sounds, etc.).",
                "In this paper, we focus on the rank aggregation problem, also called data fusion problem, where rankings of documents, searched into the same collection and provided by multiple methods, are combined in order to produce a new ranking.",
                "In this context, we propose a rank aggregation method within a multiple criteria framework using aggregation mechanisms based on decision rules identifying positive and negative reasons for judging whether a document should get a better rank than another.",
                "We show that the proposed method deals well with the Information Retrieval distinctive features.",
                "Experimental results are reported showing that the suggested method performs better than the well-known CombSUM and CombMNZ operators.",
                "Categories and Subject Descriptors: H.3.3 [Information Systems]: Information Search and Retrieval - Retrieval models.",
                "General Terms: Algorithms, Measurement, Experimentation, Performance, Theory. 1.",
                "INTRODUCTION A wide range of current Information Retrieval (IR) approaches are based on various search models (Boolean, Vector Space, Probabilistic, Language, etc. [2]) in order to retrieve relevant documents in response to a user request.",
                "The result lists produced by these approaches depend on the exact definition of the relevance concept.",
                "Rank aggregation approaches, also called data fusion approaches, consist in combining these result lists in order to produce a new and hopefully better ranking.",
                "Such approaches give rise to metasearch engines in the Web context.",
                "We consider, in the following, cases where only ranks are available and no other additional information is provided such as the relevance scores.",
                "This corresponds indeed to the reality, where only ordinal information is available.",
                "Data fusion is also relevant in other contexts, such as when the user writes several queries of his/her information need (e.g., a boolean query and a natural language query) [4], or when many document surrogates are available [16].",
                "Several studies argued that rank aggregation has the potential of combining effectively all the various sources of evidence considered in various input methods.",
                "For instance, experiments carried out in [16], [30], [4] and [19] showed that documents which appear in the lists of the majority of the input methods are more likely to be relevant.",
                "Moreover, Lee [19] and Vogt and Cottrell [31] found that various retrieval approaches often return very different irrelevant documents, but many of the same relevant documents.",
                "Bartell et al. [3] also found that rank aggregation methods improve the performances w.r.t. those of the input methods, even when some of them have weak individual performances.",
                "These methods also tend to smooth out biases of the input methods according to Montague and Aslam [22].",
                "Data fusion has recently been proved to improve performances for both the ad-hoc retrieval and categorization tasks within the TREC genomics track in 2005 [1].",
                "The rank aggregation problem was addressed in various fields such as i) in social choice theory which studies voting algorithms which specify winners of elections or winners of competitions in tournaments [29], ii) in statistics when studying correlation between rankings, iii) in distributed databases when results from different databases must be combined [12], and iv) in collaborative filtering [23].",
                "Most current rank aggregation methods consider each input ranking as a permutation over the same set of items.",
                "They also give rigid interpretation to the exact ranking of the items.",
                "Both of these assumptions are rather not valid in the IR context, as will be shown in the following sections.",
                "The remaining of the paper is organized as follows.",
                "We first review current rank aggregation methods in Section 2.",
                "Then we outline the specificities of the data fusion problem in the IR context (Section 3).",
                "In Section 4, we present a new aggregation method which is proven to best fit the IR context.",
                "Experimental results are presented in Section 5 and conclusions are provided in a final section. 2.",
                "RELATED WORK As pointed out by Riker [25], we can distinguish two families of rank aggregation methods: positional methods which assign scores to items to be ranked according to the ranks they receive and majoritarian methods which are based on pairwise comparisons of items to be ranked.",
                "These two families of methods find their roots in the pioneering works of Borda [5] and Condorcet [7], respectively, in the social choice literature. 2.1 Preliminaries We first introduce some basic notations to present the rank aggregation methods in a uniform way.",
                "Let D = {d1, d2, . . . , dnd } be a set of nd documents.",
                "A list or a ranking j is an ordering defined on Dj ⊆ D (j = 1, . . . , n).",
                "Thus, di j di means di is ranked better than di in j.",
                "When Dj = D, j is said to be a full list.",
                "Otherwise, it is a partial list.",
                "If di belongs to Dj, rj i denotes the rank or position of di in j.",
                "We assume that the best answer (document) is assigned the position 1 and the worst one is assigned the position |Dj|.",
                "Let D be the set of all permutations on D or all subsets of D. A profile is a n-tuple of rankings PR = ( 1, 2, . . . , n).",
                "Restricting PR to the rankings containing document di defines PRi.",
                "We also call the number of rankings which contain document di the rank hits of di [19].",
                "The rank aggregation or data fusion problem consists of finding a ranking function or mechanism Ψ (also called a social welfare function in the social choice theory terminology) defined by: Ψ : n D → D PR = ( 1, 2, . . . , n) → σ = Ψ(PR) where σ is called a consensus ranking. 2.2 Positional Methods 2.2.1 Borda Count This method [5] first assigns a score n j=1 rj i to each document di.",
                "Documents are then ranked by increasing order of this score, breaking ties, if any, arbitrarily. 2.2.2 Linear Combination Methods This family of methods basically combine scores of documents.",
                "When used for the rank aggregation problem, ranks are assumed to be scores or performances to be combined using aggregation operators such as the weighted sum or some variation of it [3, 31, 17, 28].",
                "For instance, Callan et al. [6] used the inference networks model [30] to combine rankings.",
                "Fox and Shaw [15] proposed several combination strategies which are CombSUM, CombMIN, CombMAX, CombANZ and CombMNZ.",
                "The first three operators correspond to the sum, min and max operators, respectively.",
                "CombANZ and CombMNZ respectively divides and multiplies the CombSUM score by the rank hits.",
                "It is shown in [19] that the CombSUM and CombMNZ operators perform better than the others.",
                "Metasearch engines such as SavvySearch and MetaCrawler use the CombSUM strategy to fuse rankings. 2.2.3 Footrule Optimal Aggregation In this method, a consensus ranking minimizes the Spearman footrule distance from the input rankings [21].",
                "Formally, given two full lists j and j , this distance is given by F( j, j ) = nd i=1 |rj i − rj i |.",
                "It extends to several lists as follows.",
                "Given a profile PR and a consensus ranking σ, the Spearman footrule distance of σ to PR is given by F(σ, PR) = n j=1 F(σ, j).",
                "Cook and Kress [8] proposed a similar method which consists in optimizing the distance D( j, j ) = 1 2 nd i,i =1 |rj i,i − rj i,i |, where rj i,i = rj i −rj i .",
                "This formulation has the advantage that it considers the intensity of preferences. 2.2.4 Probabilistic Methods This kind of methods assume that the performance of the input methods on a number of training queries is indicative of their future performance.",
                "During the training process, probabilities of relevance are calculated.",
                "For subsequent queries, documents are ranked based on these probabilities.",
                "For instance, in [20], each input ranking j is divided into a number of segments, and the conditional probability of relevance (R) of each document di depending on the segment k it occurs in, is computed, i.e. prob(R|di, k, j).",
                "For subsequent queries, the score of each document di is given by n j=1 prob(R|di,k, j ) k .",
                "Le Calve and Savoy [18] suggest using a logistic regression approach for combining scores.",
                "Training data is needed to infer the model parameters. 2.3 Majoritarian Methods 2.3.1 Condorcet Procedure The original Condorcet rule [7] specifies that a winner of the election is any item that beats or ties with every other item in a pairwise contest.",
                "Formally, let C(diσdi ) = { j∈ PR : di j di } be the coalition of rankings that are concordant with establishing diσdi , i.e. with the proposition di should be ranked better than di in the final ranking σ. di beats or ties with di iff |C(diσdi )| ≥ |C(di σdi)|.",
                "The repetitive application of the Condorcet algorithm can produce a ranking of items in a natural way: select the Condorcet winner, remove it from the lists, and repeat the previous two steps until there are no more documents to rank.",
                "Since there is not always Condorcet winners, variations of the Condorcet procedure have been developed within the multiple criteria decision aid theory, with methods such as ELECTRE [26]. 2.3.2 Kemeny Optimal Aggregation As in section 2.2.3, a consensus ranking minimizes a geometric distance from the input rankings, where the Kendall tau distance is used instead of the Spearman footrule distance.",
                "Formally, given two full lists j and j , the Kendall tau distance is given by K( j, j ) = |{(di, di ) : i < i , rj i < rj i , rj i > rj i }|, i.e. the number of pairwise disagreements between the two lists.",
                "It is easy to show that the consensus ranking corresponds to the geometric median of the input rankings and that the Kemeny optimal aggregation problem corresponds to the minimum feedback edge set problem. 2.3.3 Markov Chain Methods Markov chains (MCs) have been used by Dwork et al. [11] as a natural method to obtain a consensus ranking where states correspond to the documents to be ranked and the transition probabilities vary depending on the interpretation of the transition event.",
                "In the same reference, the authors proposed four specific MCs and experimental testing had shown that the following MC is the best performing one (see also [24]): • MC4: move from the current state di to the next state di by first choosing a document di uniformly from D. If for the majority of the rankings, we have rj i ≤ rj i , then move to di , else stay in di.",
                "The consensus ranking corresponds to the stationary distribution of MC4. 3.",
                "SPECIFICITIES OF THE RANK AGGREGATION PROBLEM IN THE IR CONTEXT 3.1 Limited Significance of the Rankings The exact positions of documents in one input ranking have limited significance and should not be overemphasized.",
                "For instance, having three relevant documents in the first three positions, any perturbation of these three items will have the same value.",
                "Indeed, in the IR context, the complete order provided by an input method may hide ties.",
                "In this case, we call such rankings semi orders.",
                "This was outlined in [13] as the problem of aggregation with ties.",
                "It is therefore important to build the consensus ranking based on robust information: • Documents with near positions in j are more likely to have similar interest or relevance.",
                "Thus a slight perturbation of the initial ranking is meaningless. • Assuming that document di is better ranked than document di in a ranking j, di is more likely to be definitively more relevant than di in j when the number of intermediate positions between di and di increases. 3.2 Partial Lists In real world applications, such as metasearch engines, rankings provided by the input methods are often partial lists.",
                "This was outlined in [14] as the problem of having to merge top-k results from various input lists.",
                "For instance, in the experiments carried out by Dwork et al. [11], authors found that among the top 100 best documents of 7 input search engines, 67% of the documents were present in only one search engine, whereas less than two documents were present in all the search engines.",
                "Rank aggregation of partial lists raises four major difficulties which we state hereafter, proposing for each of them various working assumptions: 1.",
                "Partial lists can have various lengths, which can favour long lists.",
                "We thus consider the following two working hypotheses: H1 k : We only consider the top k best documents from each input ranking.",
                "H1 all: We consider all the documents from each input ranking. 2.",
                "Since there are different documents in the input rankings, we must decide which documents should be kept in the consensus ranking.",
                "Two working hypotheses are therefore considered: H2 k : We only consider documents which are present in at least k input rankings (k > 1).",
                "H2 all: We consider all the documents which are ranked in at least one input ranking.",
                "Hereafter, we call documents which will be retained in the consensus ranking, candidate documents, and documents that will be excluded from the consensus ranking, excluded documents.",
                "We also call a candidate document which is missing in one or more rankings, a missing document. 3.",
                "Some candidate documents are missing documents in some input rankings.",
                "Main reasons for a missing document are that it was not indexed or it was indexed but deemed irrelevant ; usually this information is not available.",
                "We consider the following two working hypotheses: H3 yes: Each missing document in each j is assigned a position.",
                "H3 no: No assumption is made, that is each missing document is considered neither better nor worse than any other document. 4.",
                "When assumption H2 k holds, each input ranking may contain documents which will not be considered in the consensus ranking.",
                "Regarding the positions of the candidate documents, we can consider the following working hypotheses: H4 init: The initial positions of candidate documents are kept in each input ranking.",
                "H4 new: Candidate documents receive new positions in each input ranking, after discarding excluded ones.",
                "In the IR context, rank aggregation methods need to decide more or less explicitly which assumptions to retain w.r.t. the above-mentioned difficulties. 4.",
                "OUTRANKING APPROACH FOR RANK AGGREGATION 4.1 Presentation Positional methods consider implicitly that the positions of the documents in the input rankings are scores giving thus a cardinal meaning to an ordinal information.",
                "This constitutes a strong assumption that is questionable, especially when the input rankings have different lengths.",
                "Moreover, for positional methods, assumptions H3 and H4 , which are often arbitrary, have a strong impact on the results.",
                "For instance, let us consider an input ranking of 500 documents out of 1000 candidate documents.",
                "Whether we assign to each of the missing documents the position 1, 501, 750 or 1000 -corresponding to variations of H3 yes- will give rise to very contrasted results, especially regarding the top of the consensus ranking.",
                "Majoritarian methods do not suffer from the above-mentioned drawbacks of the positional methods since they build consensus rankings exploiting only ordinal information contained in the input rankings.",
                "Nevertheless, they suppose that such rankings are complete orders, ignoring that they may hide ties.",
                "Therefore, majoritarian methods base consensus rankings on illusory discriminant information rather than less discriminant but more robust information.",
                "Trying to overcome the limits of current rank aggregation methods, we found that outranking approaches, which were initially used for multiple criteria aggregation problems [26], can also be used for the rank aggregation purpose, where each ranking plays the role of a criterion.",
                "Therefore, in order to decide whether a document di should be ranked better than di in the consensus ranking σ, the two following conditions should be met: • a concordance condition which ensures that a majority of the input rankings are concordant with diσdi (majority principle). • a discordance condition which ensures that none of the discordant input rankings strongly refutes dσd (respect of minorities principle).",
                "Formally, the concordance coalition with diσdi is Csp (diσdi ) = { j∈ PR : rj i ≤ rj i − sp} where sp is a preference threshold which is the variation of document positions -whether it is absolute or relative to the ranking length- which draws the boundaries between an indifference and a preference situation between documents.",
                "The discordance coalition with diσdi is Dsv (diσdi ) = { j∈ PR : rj i ≥ rj i + sv} where sv is a veto threshold which is the variation of document positions -whether it is absolute or relative to the ranking length- which draws the boundaries between a weak and a strong opposition to diσdi .",
                "Depending on the exact definition of the preceding concordance and discordance coalitions leading to the definition of some decision rules, several outranking relations can be defined.",
                "They can be more or less demanding depending on i) the values of the thresholds sp and sv, ii) the importance or minimal size cmin required for the concordance coalition, and iii) the importance or maximum size dmax of the discordance coalition.",
                "A generic outranking relation can thus be defined as follows: diS(sp,sv,cmin,dmax)di ⇔ |Csp (diσdi )| ≥ cmin AND |Dsv (diσdi )| ≤ dmax This expression defines a family of nested outranking relations since S(sp,sv,cmin,dmax) ⊆ S(sp,sv,cmin,dmax) when cmin ≥ cmin and/or dmax ≤ dmax and/or sp ≥ sp and/or sv ≤ sv.",
                "This expression also generalizes the majority rule which corresponds to the particular relation S(0,∞, n 2 ,n).",
                "It also satisfies important properties of rank aggregation methods, called neutrality, Pareto-optimality, Condorcet property and Extended Condorcet property, in the social choice literature [29].",
                "Outranking relations are not necessarily transitive and do not necessarily correspond to rankings since directed cycles may exist.",
                "Therefore, we need specific procedures in order to derive a consensus ranking.",
                "We propose the following procedure which finds its roots in [27].",
                "It consists in partitioning the set of documents into r ranked classes.",
                "Each class Ch contains documents with the same relevance and results from the application of all relations (if possible) to the set of documents remaining after previous classes are computed.",
                "Documents within the same equivalence class are ranked arbitrarily.",
                "Formally, let • R be the set of candidate documents for a query, • S1 , S2 , . . . be a family of nested outranking relations, • Fk(di, E) = |{di ∈ E : diSk di }| be the number of documents in E(E ⊆ R) that could be considered worse than di according to relation Sk , • fk(di, E) = |{di ∈ E : di Sk di}| be the number of documents in E that could be considered better than di according to Sk , • sk(di, E) = Fk(di, E) − fk(di, E) be the qualification of di in E according to Sk .",
                "Each class Ch results from a distillation process.",
                "It corresponds to the last distillate of a series of sets E0 ⊇ E1 ⊇ . . . where E0 = R \\ (C1 ∪ . . . ∪ Ch−1) and Ek is a reduced subset of Ek−1 resulting from the application of the following procedure: 1. compute for each di ∈ Ek−1 its qualification according to Sk , i.e. sk(di, Ek−1), 2. define smax = maxdi∈Ek−1 {sk(di, Ek−1)}, then 3.",
                "Ek = {di ∈ Ek−1 : sk(di, Ek−1) = smax} When one outranking relation is used, the distillation process stops after the first application of the previous procedure, i.e., Ch corresponds to distillate E1.",
                "When different outranking relations are used, the distillation process stops when all the pre-defined outranking relations have been used or when |Ek| = 1. 4.2 Illustrative Example This section illustrates the concepts and procedures of section 4.1.",
                "Let us consider a set of candidate documents R = {d1, d2, d3, d4, d5}.",
                "The following table gives a profile PR of different rankings of the documents of R: PR = ( 1 , 2, 3, 4).",
                "Table 1: Rankings of documents rj i 1 2 3 4 d1 1 3 1 5 d2 2 1 3 3 d3 3 2 2 1 d4 4 4 5 2 d5 5 5 4 4 Let us suppose that the preference and veto thresholds are set to values 1 and 4 respectively, and that the concordance and discordance thresholds are set to values 2 and 1 respectively.",
                "The following tables give the concordance, discordance and outranking matrices.",
                "Each entry csp (di, di ) (dsv (di, di )) in the concordance (discordance) matrix gives the number of rankings that are concordant (discordant) with diσdi , i.e. csp (di, di ) = |Csp (diσdi )| and dsv (di, di ) = |Dsv (diσdi )|.",
                "Table 2: Computation of the outranking relation d1 d2 d3 d4 d5 d1 - 2 2 3 3 d2 2 - 2 3 4 d3 2 2 - 4 4 d4 1 1 0 - 3 d5 1 0 0 1Concordance Matrix d1 d2 d3 d4 d5 d1 - 0 1 0 0 d2 0 - 0 0 0 d3 0 0 - 0 0 d4 1 0 0 - 0 d5 1 1 0 0Discordance Matrix d1 d2 d3 d4 d5 d1 - 1 1 1 1 d2 1 - 1 1 1 d3 1 1 - 1 1 d4 0 0 0 - 1 d5 0 0 0 0Outranking Matrix (S1) For instance, the concordance coalition for the assertion d1σd4 is C1(d1σd4) = { 1, 2, 3} and the discordance coalition for the same assertion is D4(d1σd4) = ∅.",
                "Therefore, c1(d1, d4) = 3, d4(d1, d4) = 0 and d1S1 d4 holds.",
                "Notice that Fk(di, R) (fk(di, R)) is given by summing the values of the ith row (column) of the outranking matrix.",
                "The consensus ranking is obtained as follows: to get the first class C1, we compute the qualifications of all the documents of E0 = R with respect to S1 .",
                "They are respectively 2, 2, 2, -2 and -4.",
                "Therefore smax equals 2 and C1 = E1 = {d1, d2, d3}.",
                "Observe that, if we had used a second outranking relation S2(⊇ S1), these three documents could have been possibly discriminated.",
                "At this stage, we remove documents of C1 from the outranking matrix and compute the next class C2: we compute the new qualifications of the documents of E0 = R \\ C1 = {d4, d5}.",
                "They are respectively 1 and -1.",
                "So C3 = E1 = {d4}.",
                "The last document d5 is the only document of the last class C3.",
                "Thus, the consensus ranking is {d1, d2, d3} → {d4} → {d5}. 5.",
                "EXPERIMENTS AND RESULTS 5.1 Test Setting To facilitate empirical investigation of the proposed methodology, we developed a prototype metasearch engine that implements a version of our outranking approach for rank aggregation.",
                "In this paper, we apply our approach to the Topic Distillation (TD) task of TREC-2004 Web track [10].",
                "In this task, there are 75 topics where only a short description of each is given.",
                "For each query, we retained the rankings of the 10 best runs of the TD task which are provided by TREC-2004 participating teams.",
                "The performances of these runs are reported in table 3.",
                "Table 3: Performances of the 10 best runs of the TD task of TREC-2004 Run Id MAP P@10 S@1 S@5 S@10 uogWebCAU150 17.9% 24.9% 50.7% 77.3% 89.3% MSRAmixed1 17.8% 25.1% 38.7% 72.0% 88.0% MSRC04C12 16.5% 23.1% 38.7% 74.7% 80.0% humW04rdpl 16.3% 23.1% 37.3% 78.7% 90.7% THUIRmix042 14.7% 20.5% 21.3% 58.7% 74.7% UAmsT04MWScb 14.6% 20.9% 36.0% 66.7% 76.0% ICT04CIIS1AT 14.1% 20.8% 33.3% 64.0% 78.7% SJTUINCMIX5 12.9% 18.9% 29.3% 57.3% 72.0% MU04web1 11.5% 19.9% 33.3% 64.0% 76.0% MeijiHILw3 11.5% 15.3% 30.7% 54.7% 64.0% Average 14.7% 21.2% 34.9% 66.8% 78.94% For each query, each run provides a ranking of about 1000 documents.",
                "The number of documents retrieved by all these runs ranges from 543 to 5769.",
                "Their average (median) number is 3340 (3386).",
                "It is worth noting that we found similar distributions of the documents among the rankings as in [11].",
                "For evaluation, we used the trec eval standard tool which is used by the TREC community to calculate the standard measures of system effectiveness which are Mean Average Precision (MAP) and Success@n (S@n) for n=1, 5 and 10.",
                "Our approach effectiveness is compared against some high performing official results from TREC-2004 as well as against some standard rank aggregation algorithms.",
                "In the experiments, significance testing is mainly based on the t-student statistic which is computed on the basis of the MAP values of the compared runs.",
                "In the tables of the following section, statistically significant differences are marked with an asterisk.",
                "Values between brackets of the first column of each table, indicate the parameter value of the corresponding run. 5.2 Results We carried out several series of runs in order to i) study performance variations of the outranking approach when tuning the parameters and working assumptions, ii) compare performances of the outranking approach vs standard rank aggregation strategies , and iii) check whether rank aggregation performs better than the best input rankings.",
                "We set our basic run mcm with the following parameters.",
                "We considered that each input ranking is a complete order (sp = 0) and that an input ranking strongly refutes diσdi when the difference of both document positions is large enough (sv = 75%).",
                "Preference and veto thresholds are computed proportionally to the number of documents retained in each input ranking.",
                "They consequently may vary from one ranking to another.",
                "In addition, to accept the assertion diσdi , we supposed that the majority of the rankings must be concordant (cmin = 50%) and that every input ranking can impose its veto (dmax = 0).",
                "Concordance and discordance thresholds are computed for each tuple (di, di ) as the percentage of the input rankings of PRi ∩PRi .",
                "Thus, our choice of parameters leads to the definition of the outranking relation S(0,75%,50%,0).",
                "To test the run mcm, we had chosen the following assumptions.",
                "We retained the top 100 best documents from each input ranking (H1 100), only considered documents which are present in at least half of the input rankings (H2 5 ) and assumed H3 no and H4 new.",
                "In these conditions, the number of successful documents was about 100 on average, and the computation time per query was less than one second.",
                "Obviously, modifying the working assumptions should have deeper impact on the performances than tuning our model parameters.",
                "This was validated by preliminary experiments.",
                "Thus, we hereafter begin by studying performance variation when different sets of assumptions are considered.",
                "Afterwards, we study the impact of tuning parameters.",
                "Finally, we compare our model performances w.r.t. the input rankings as well as some standard data fusion algorithms. 5.2.1 Impact of the Working Assumptions Table 4 summarizes the performance variation of the outranking approach under different working hypotheses.",
                "In Table 4: Impact of the working assumptions Run Id MAP S@1 S@5 S@10 mcm 18.47% 41.33% 81.33% 86.67% mcm22 (H3 yes) 17.72% (-4.06%) 34.67% 81.33% 86.67% mcm23 (H4 init) 18.26% (-1.14%) 41.33% 81.33% 86.67% mcm24 (H1 all) 20.67% (+11.91%*) 38.66% 80.00% 86.66% mcm25 (H2 all) 21.68% (+17.38%*) 40.00% 78.66% 89.33% this table, we first show that run mcm22, in which missing documents are all put in the same last position of each input ranking, leads to performance drop w.r.t. run mcm.",
                "Moreover, S@1 moves from 41.33% to 34.67% (-16.11%).",
                "This shows that several relevant documents which were initially put at the first position of the consensus ranking in mcm, lose this first position but remain ranked in the top 5 documents since S@5 did not change.",
                "We also conclude that documents which have rather good positions in some input rankings are more likely to be relevant, even though they are missing in some other rankings.",
                "Consequently, when they are missing in some rankings, assigning worse ranks to these documents is harmful for performance.",
                "Also, from Table 4, we found that the performances of runs mcm and mcm23 are similar.",
                "Therefore, the outranking approach is not sensitive to keeping the initial positions of candidate documents or recomputing them by discarding excluded ones.",
                "From the same Table 4, performance of the outranking approach increases significantly for runs mcm24 and mcm25.",
                "Therefore, whether we consider all the documents which are present in half of the rankings (mcm24) or we consider all the documents which are ranked in the first 100 positions in one or more rankings (mcm25), increases performances.",
                "This result was predictable since in both cases we have more detailed information on the relative importance of documents.",
                "Tables 5 and 6 confirm this evidence.",
                "Table 5, where values between brackets of the first column give the number of documents which are retained from each input ranking, shows that selecting more documents from each input ranking leads to performance increase.",
                "It is worth mentioning that selecting more than 600 documents from each input ranking does not improve performance.",
                "Table 5: Impact of the number of retained documents Run Id MAP S@1 S@5 S@10 mcm (100) 18.47% 41.33% 81.33% 86.67% mcm24-1 (200) 19.32% (+4.60%) 42.67% 78.67% 88.00% mcm24-2 (400) 19.88% (+7.63%*) 37.33% 80.00% 88.00% mcm24-3 (600) 20.80% (+12.62%*) 40.00% 80.00% 88.00% mcm24-4 (800) 20.66% (+11.86%*) 40.00% 78.67% 86.67% mcm24 (1000) 20.67% (+11.91%*) 38.66% 80.00% 86.66% Table 6 reports runs corresponding to variations of H2 k .",
                "Values between brackets are rank hits.",
                "For instance, in the run mcm32, only documents which are present in 3 or more input rankings, were considered successful.",
                "This table shows that performance is significantly better when rare documents are considered, whereas it decreases significantly when these documents are discarded.",
                "Therefore, we conclude that many of the relevant documents are retrieved by a rather small set of IR models.",
                "Table 6: Performance considering different rank hits Run Id MAP S@1 S@5 S@10 mcm25 (1) 21.68% (+17.38%*) 40.00% 78.67% 89.33% mcm32 (3) 18.98% (+2.76%) 38.67% 80.00% 85.33% mcm (5) 18.47% 41.33% 81.33% 86.67% mcm33 (7) 15.83% (-14.29%*) 37.33% 78.67% 85.33% mcm34 (9) 10.96% (-40.66%*) 36.11% 66.67% 70.83% mcm35 (10) 7.42% (-59.83%*) 39.22% 62.75% 64.70% For both runs mcm24 and mcm25, the number of successful documents was about 1000 and therefore, the computation time per query increased and became around 5 seconds. 5.2.2 Impact of the Variation of the Parameters Table 7 shows performance variation of the outranking approach when different preference thresholds are considered.",
                "We found performance improvement up to threshold values of about 5%, then there is a decrease in the performance which becomes significant for threshold values greater than 10%.",
                "Moreover, S@1 improves from 41.33% to 46.67% when preference threshold changes from 0 to 5%.",
                "We can thus conclude that the input rankings are semi orders rather than complete orders.",
                "Table 8 shows the evolution of the performance measures w.r.t. the concordance threshold.",
                "We can conclude that in order to put document di before di in the consensus ranking, Table 7: Impact of the variation of the preference threshold from 0 to 12.5% Run Id MAP S@1 S@5 S@10 mcm (0%) 18.47% 41.33% 81.33% 86.67% mcm1 (1%) 18.57% (+0.54%) 41.33% 81.33% 86.67% mcm2 (2.5%) 18.63% (+0.87%) 42.67% 78.67% 86.67% mcm3 (5%) 18.69% (+1.19%) 46.67% 81.33% 86.67% mcm4 (7.5%) 18.24% (-1.25%) 46.67% 81.33% 86.67% mcm5 (10%) 17.93% (-2.92%) 40.00% 82.67% 86.67% mcm5b (12.5%) 17.51% (-5.20%*) 41.33% 80.00% 86.67% at least half of the input rankings of PRi ∩ PRi should be concordant.",
                "Performance drops significantly for very low and very high values of the concordance threshold.",
                "In fact, for such values, the concordance condition is either fulfilled rather always by too many document pairs or not fulfilled at all, respectively.",
                "Therefore, the outranking relation becomes either too weak or too strong respectively.",
                "Table 8: Impact of the variation of cmin Run Id MAP S@1 S@5 S@10 mcm11 (20%) 17.63% (-4.55%*) 41.33% 76.00% 85.33% mcm12 (40%) 18.37% (-0.54%) 42.67% 76.00% 86.67% mcm (50%) 18.47% 41.33% 81.33% 86.67% mcm13 (60%) 18.42% (-0.27%) 40.00% 78.67% 86.67% mcm14 (80%) 17.43% (-5.63%*) 40.00% 78.67% 86.67% mcm15 (100%) 16.12% (-12.72%*) 41.33% 70.67% 85.33% In the experiments, varying the veto threshold as well as the discordance threshold within reasonable intervals does not have significant impact on performance measures.",
                "In fact, runs with different veto thresholds (sv ∈ [50%; 100%]) had similar performances even though there is a slight advantage for runs with high threshold values which means that it is better not to allow the input rankings to put their veto easily.",
                "Also, tuning the discordance threshold was carried out for values 50% and 75% of the veto threshold.",
                "For these runs we did not get any noticeable performance variation, although for low discordance thresholds (dmax < 20%), performance slightly decreased. 5.2.3 Impact of the Variation of the Number of Input Rankings To study performance evolution when different sets of input rankings are considered, we carried three more runs where 2, 4, and 6 of the best performing sets of the input rankings are considered.",
                "Results reported in Table 9 are seemingly counter-intuitive and also do not support previous findings regarding rank aggregation research [3].",
                "Nevertheless, this result shows that low performing rankings bring more noise than information to the establishment of the consensus ranking.",
                "Therefore, when they are considered, performance decreases.",
                "Table 9: Performance considering different best performing sets of input rankings Run Id MAP S@1 S@5 S@10 mcm (10) 18.47% 41.33% 81.33% 86.67% mcm27 (6) 18.60% (+0.70%) 41.33% 80.00% 85.33% mcm28 (4) 19.02% (+2.98%) 40.00% 86.67% 88.00% mcm29 (2) 18.33% (-0.76%) 44.00% 76.00% 88.00% 5.2.4 Comparison of the Performance of Different Rank Aggregation Methods In this set of runs, we compare the outranking approach with some standard rank aggregation methods which were proven to have acceptable performance in previous studies: we considered two positional methods which are the CombSUM and the CombMNZ strategies.",
                "We also examined the performance of one majoritarian method which is the Markov chain method (MC4).",
                "For the comparisons, we considered a specific outranking relation S∗ = S(5%,50%,50%,30%) which results in good overall performances when tuning all the parameters.",
                "The first row of Table 10 gives performances of the rank aggregation methods w.r.t. a basic assumption set A1 = (H1 100, H2 5 , H4 new): we only consider the 100 first documents from each ranking, then retain documents present in 5 or more rankings and update ranks of successful documents.",
                "For positional methods, we place missing documents at the queue of the ranking (H3 yes) whereas for our method as well as for MC4, we retained hypothesis H3 no.",
                "The three following rows of Table 10 report performances when changing one element from the basic assumption set: the second row corresponds to the assumption set A2 = (H1 1000, H2 5 , H4 new), i.e. changing the number of retained documents from 100 to 1000.",
                "The third row corresponds to the assumption set A3 = (H1 100, H2 all, H4 new), i.e. considering the documents present in at least one ranking.",
                "The fourth row corresponds to the assumption set A4 = (H1 100, H2 5 , H4 init), i.e. keeping the original ranks of successful documents.",
                "The fifth row of Table 10, labeled A5, gives performance when all the 225 queries of the Web track of TREC-2004 are considered.",
                "Obviously, performance level cannot be compared with previous lines since the additional queries are different from the TD queries and correspond to other tasks (Home Page and Named Page tasks [10]) of TREC-2004 Web track.",
                "This set of runs aims to show whether relative performance of the various methods is task-dependent.",
                "The last row of Table 10, labeled A6, reports performance of the various methods considering the TD task of TREC2002 instead of TREC-2004: we fused the results of input rankings of the 10 best official runs for each of the 50 TD queries [9] considering the set of assumptions A1 of the first row.",
                "This aims to show whether relative performance of the various methods changes from year to year.",
                "Values between brackets of Table 10 are variations of performance of each rank aggregation method w.r.t. performance of the outranking approach.",
                "Table 10: Performance (MAP) of different rank aggregation methods under 3 different test collections mcm combSUM combMNZ markov A1 18.79% 17.54% (-6.65%*) 17.08% (-9.10%*) 18.63% (-0.85%) A2 21.36% 19.18% (-10.21%*) 18.61% (-12.87%*) 21.33% (-0.14%) A3 21.92% 21.38% (-2.46%) 20.88% (-4.74%) 19.35% (-11.72%*) A4 18.64% 17.58% (-5.69%*) 17.18% (-7.83%*) 18.63% (-0.05%) A5 55.39% 52.16% (-5.83%*) 49.70% (-10.27%*) 53.30% (-3.77%) A6 16.95% 15.65% (-7.67%*) 14.57% (-14.04%*) 16.39% (-3.30%) From the analysis of table 10 the following can be established: • for all the runs, considering all the documents in each input ranking (A2) significantly improves performance (MAP increases by 11.62% on average).",
                "This is predictable since some initially unreported relevant documents would receive better positions in the consensus ranking. • for all the runs, considering documents even those present in only one input ranking (A3) significantly improves performance.",
                "For mcm, combSUM and combMNZ, performance improvement is more important (MAP increases by 20.27% on average) than for the markov run (MAP increases by 3.86%). • preserving the initial positions of documents (A4) or recomputing them (A1) does not have a noticeable influence on performance for both positional and majoritarian methods. • considering all the queries of the Web track of TREC2004 (A5) as well as the TD queries of the Web track of TREC-2002 (A6) does not alter the relative performance of the different data fusion methods. • considering the TD queries of the Web track of TREC2002, performances of all the data fusion methods are lower than that of the best performing input ranking for which the MAP value equals 18.58%.",
                "This is because most of the fused input rankings have very low performances compared to the best one, which brings more noise to the consensus ranking. • performances of the data fusion methods mcm and markov are significantly better than that of the best input ranking uogWebCAU150.",
                "This remains true for runs combSUM and combMNZ only under assumptions H1 all or H2 all.",
                "This shows that majoritarian methods are less sensitive to assumptions than positional methods. • outranking approach always performs significantly better than positional methods combSUM and combMNZ.",
                "It has also better performances than the Markov chain method, especially under assumption H2 all where difference of performances becomes significant. 6.",
                "CONCLUSIONS In this paper, we address the rank aggregation problem where different, but not disjoint, lists of documents are to be fused.",
                "We noticed that the input rankings can hide ties, so they should not be considered as complete orders.",
                "Only robust information should be used from each input ranking.",
                "Current rank aggregation methods, and especially positional methods (e.g. combSUM [15]), are not initially designed to work with such rankings.",
                "They should be adapted by considering specific working assumptions.",
                "We propose a new outranking method for rank aggregation which is well adapted to the IR context.",
                "Indeed, it ranks two documents w.r.t. the intensity of their positions difference in each input ranking and also considering the number of the input rankings that are concordant and discordant in favor of a specific document.",
                "There is also no need to make specific assumptions on the positions of the missing documents.",
                "This is an important feature since the absence of a document from a ranking should not be necessarily interpreted negatively.",
                "Experimental results show that the outranking method significantly out-performs popular classical positional data fusion methods like combSUM and combMNZ strategies.",
                "It also out-performs a good performing majoritarian methods which is the Markov chain method.",
                "These results are tested against different test collections and queries.",
                "From the experiments, we can also conclude that in order to improve the performances, we should fuse result lists of well performing IR models, and that majoritarian data fusion methods perform better than positional methods.",
                "The proposed method can have a real impact on Web metasearch performances since only ranks are available from most primary search engines, whereas most of the current approaches need scores to merge result lists into one single list.",
                "Further work involves investigating whether the outranking approach performs well in various other contexts, e.g. using the document scores or some combination of document ranks and scores.",
                "Acknowledgments The authors would like to thank Jacques Savoy for his valuable comments on a preliminary version of this paper. 7.",
                "REFERENCES [1] A. Aronson, D. Demner-Fushman, S. Humphrey, J. Lin, H. Liu, P. Ruch, M. Ruiz, L. Smith, L. Tanabe, and W. Wilbur.",
                "Fusion of knowledge-intensive and statistical approaches for retrieving and annotating textual genomics documents.",
                "In Proceedings TREC2005.",
                "NIST Publication, 2005. [2] R. A. Baeza-Yates and B.",
                "A. Ribeiro-Neto.",
                "Modern Information Retrieval.",
                "ACM Press , 1999. [3] B. T. Bartell, G. W. Cottrell, and R. K. Belew.",
                "Automatic combination of multiple ranked retrieval systems.",
                "In Proceedings ACM-SIGIR94, pages 173-181.",
                "Springer-Verlag, 1994. [4] N. J. Belkin, P. Kantor, E. A.",
                "Fox, and J.",
                "A. Shaw.",
                "Combining evidence of multiple query representations for information retrieval.",
                "IPM, 31(3):431-448, 1995. [5] J. Borda.",
                "M´emoire sur les ´elections au scrutin.",
                "Histoire de lAcad´emie des Sciences, 1781. [6] J. P. Callan, Z. Lu, and W. B. Croft.",
                "Searching distributed collections with inference networks.",
                "In Proceedings ACM-SIGIR95, pages 21-28, 1995. [7] M. Condorcet.",
                "Essai sur lapplication de lanalyse `a la probabilit´e des d´ecisions rendues `a la pluralit´e des voix.",
                "Imprimerie Royale, Paris, 1785. [8] W. D. Cook and M. Kress.",
                "Ordinal ranking with intensity of preference.",
                "Management Science, 31(1):26-32, 1985. [9] N. Craswell and D. Hawking.",
                "Overview of the TREC-2002 Web Track.",
                "In Proceedings TREC2002.",
                "NIST Publication, 2002. [10] N. Craswell and D. Hawking.",
                "Overview of the TREC-2004 Web Track.",
                "In Proceedings of TREC2004.",
                "NIST Publication, 2004. [11] C. Dwork, S. R. Kumar, M. Naor, and D. Sivakumar.",
                "Rank aggregation methods for the Web.",
                "In Proceedings WWW2001, pages 613-622, 2001. [12] R. Fagin.",
                "Combining fuzzy information from multiple systems.",
                "JCSS, 58(1):83-99, 1999. [13] R. Fagin, R. Kumar, M. Mahdian, D. Sivakumar, and E. Vee.",
                "Comparing and aggregating rankings with ties.",
                "In PODS, pages 47-58, 2004. [14] R. Fagin, R. Kumar, and D. Sivakumar.",
                "Comparing top k lists.",
                "SIAM J. on Discrete Mathematics, 17(1):134-160, 2003. [15] E. A.",
                "Fox and J.",
                "A. Shaw.",
                "Combination of multiple searches.",
                "In Proceedings of TREC3.",
                "NIST Publication, 1994. [16] J. Katzer, M. McGill, J. Tessier, W. Frakes, and P. DasGupta.",
                "A study of the overlap among document representations.",
                "Information Technology: Research and Development, 1(4):261-274, 1982. [17] L. S. Larkey, M. E. Connell, and J. Callan.",
                "Collection selection and results merging with topically organized U.S. patents and TREC data.",
                "In Proceedings ACM-CIKM2000, pages 282-289.",
                "ACM Press, 2000. [18] A.",
                "Le Calv´e and J. Savoy.",
                "Database merging strategy based on logistic regression.",
                "IPM, 36(3):341-359, 2000. [19] J. H. Lee.",
                "Analyses of multiple evidence combination.",
                "In Proceedings ACM-SIGIR97, pages 267-276, 1997. [20] D. Lillis, F. Toolan, R. Collier, and J. Dunnion.",
                "Probfuse: a probabilistic approach to data fusion.",
                "In Proceedings ACM-SIGIR2006, pages 139-146.",
                "ACM Press, 2006. [21] J. I. Marden.",
                "Analyzing and Modeling Rank Data.",
                "Number 64 in Monographs on Statistics and Applied Probability.",
                "Chapman & Hall, 1995. [22] M. Montague and J.",
                "A. Aslam.",
                "Metasearch consistency.",
                "In Proceedings ACM-SIGIR2001, pages 386-387.",
                "ACM Press, 2001. [23] D. M. Pennock and E. Horvitz.",
                "Analysis of the axiomatic foundations of collaborative filtering.",
                "In Workshop on AI for Electronic Commerce at the 16th National Conference on Artificial Intelligence, 1999. [24] M. E. Renda and U. Straccia.",
                "Web metasearch: rank vs. score based rank aggregation methods.",
                "In Proceedings ACM-SAC2003, pages 841-846.",
                "ACM Press, 2003. [25] W. H. Riker.",
                "Liberalism against populism.",
                "Waveland Press, 1982. [26] B. Roy.",
                "The outranking approach and the foundations of ELECTRE methods.",
                "Theory and Decision, 31:49-73, 1991. [27] B. Roy and J. Hugonnard.",
                "Ranking of suburban line extension projects on the Paris metro system by a multicriteria method.",
                "Transportation Research, 16A(4):301-312, 1982. [28] L. Si and J. Callan.",
                "Using sampled data and regression to merge search engine results.",
                "In Proceedings ACM-SIGIR2002, pages 19-26.",
                "ACM Press, 2002. [29] M. Truchon.",
                "An extension of the Condorcet criterion and Kemeny orders.",
                "Cahier 9813, Centre de Recherche en Economie et Finance Appliqu´ees, Oct. 1998. [30] H. Turtle and W. B. Croft.",
                "Inference networks for document retrieval.",
                "In Proceedings of ACM-SIGIR90, pages 1-24.",
                "ACM Press, 1990. [31] C. C. Vogt and G. W. Cottrell.",
                "Fusion via a linear combination of scores.",
                "Information Retrieval, 1(3):151-173, 1999."
            ],
            "original_annotated_samples": [],
            "translated_annotated_samples": [],
            "translated_text": "",
            "candidates": [],
            "error": []
        },
        "decision rule": {
            "translated_key": "regla de decisión",
            "is_in_text": true,
            "original_annotated_sentences": [
                "An Outranking Approach for Rank Aggregation in Information Retrieval Mohamed Farah Lamsade, Paris Dauphine University Place du Mal de Lattre de Tassigny 75775 Paris Cedex 16, France farah@lamsade.dauphine.fr Daniel Vanderpooten Lamsade, Paris Dauphine University Place du Mal de Lattre de Tassigny 75775 Paris Cedex 16, France vdp@lamsade.dauphine.fr ABSTRACT Research in Information Retrieval usually shows performance improvement when many sources of evidence are combined to produce a ranking of documents (e.g., texts, pictures, sounds, etc.).",
                "In this paper, we focus on the rank aggregation problem, also called data fusion problem, where rankings of documents, searched into the same collection and provided by multiple methods, are combined in order to produce a new ranking.",
                "In this context, we propose a rank aggregation method within a multiple criteria framework using aggregation mechanisms based on <br>decision rule</br>s identifying positive and negative reasons for judging whether a document should get a better rank than another.",
                "We show that the proposed method deals well with the Information Retrieval distinctive features.",
                "Experimental results are reported showing that the suggested method performs better than the well-known CombSUM and CombMNZ operators.",
                "Categories and Subject Descriptors: H.3.3 [Information Systems]: Information Search and Retrieval - Retrieval models.",
                "General Terms: Algorithms, Measurement, Experimentation, Performance, Theory. 1.",
                "INTRODUCTION A wide range of current Information Retrieval (IR) approaches are based on various search models (Boolean, Vector Space, Probabilistic, Language, etc. [2]) in order to retrieve relevant documents in response to a user request.",
                "The result lists produced by these approaches depend on the exact definition of the relevance concept.",
                "Rank aggregation approaches, also called data fusion approaches, consist in combining these result lists in order to produce a new and hopefully better ranking.",
                "Such approaches give rise to metasearch engines in the Web context.",
                "We consider, in the following, cases where only ranks are available and no other additional information is provided such as the relevance scores.",
                "This corresponds indeed to the reality, where only ordinal information is available.",
                "Data fusion is also relevant in other contexts, such as when the user writes several queries of his/her information need (e.g., a boolean query and a natural language query) [4], or when many document surrogates are available [16].",
                "Several studies argued that rank aggregation has the potential of combining effectively all the various sources of evidence considered in various input methods.",
                "For instance, experiments carried out in [16], [30], [4] and [19] showed that documents which appear in the lists of the majority of the input methods are more likely to be relevant.",
                "Moreover, Lee [19] and Vogt and Cottrell [31] found that various retrieval approaches often return very different irrelevant documents, but many of the same relevant documents.",
                "Bartell et al. [3] also found that rank aggregation methods improve the performances w.r.t. those of the input methods, even when some of them have weak individual performances.",
                "These methods also tend to smooth out biases of the input methods according to Montague and Aslam [22].",
                "Data fusion has recently been proved to improve performances for both the ad-hoc retrieval and categorization tasks within the TREC genomics track in 2005 [1].",
                "The rank aggregation problem was addressed in various fields such as i) in social choice theory which studies voting algorithms which specify winners of elections or winners of competitions in tournaments [29], ii) in statistics when studying correlation between rankings, iii) in distributed databases when results from different databases must be combined [12], and iv) in collaborative filtering [23].",
                "Most current rank aggregation methods consider each input ranking as a permutation over the same set of items.",
                "They also give rigid interpretation to the exact ranking of the items.",
                "Both of these assumptions are rather not valid in the IR context, as will be shown in the following sections.",
                "The remaining of the paper is organized as follows.",
                "We first review current rank aggregation methods in Section 2.",
                "Then we outline the specificities of the data fusion problem in the IR context (Section 3).",
                "In Section 4, we present a new aggregation method which is proven to best fit the IR context.",
                "Experimental results are presented in Section 5 and conclusions are provided in a final section. 2.",
                "RELATED WORK As pointed out by Riker [25], we can distinguish two families of rank aggregation methods: positional methods which assign scores to items to be ranked according to the ranks they receive and majoritarian methods which are based on pairwise comparisons of items to be ranked.",
                "These two families of methods find their roots in the pioneering works of Borda [5] and Condorcet [7], respectively, in the social choice literature. 2.1 Preliminaries We first introduce some basic notations to present the rank aggregation methods in a uniform way.",
                "Let D = {d1, d2, . . . , dnd } be a set of nd documents.",
                "A list or a ranking j is an ordering defined on Dj ⊆ D (j = 1, . . . , n).",
                "Thus, di j di means di is ranked better than di in j.",
                "When Dj = D, j is said to be a full list.",
                "Otherwise, it is a partial list.",
                "If di belongs to Dj, rj i denotes the rank or position of di in j.",
                "We assume that the best answer (document) is assigned the position 1 and the worst one is assigned the position |Dj|.",
                "Let D be the set of all permutations on D or all subsets of D. A profile is a n-tuple of rankings PR = ( 1, 2, . . . , n).",
                "Restricting PR to the rankings containing document di defines PRi.",
                "We also call the number of rankings which contain document di the rank hits of di [19].",
                "The rank aggregation or data fusion problem consists of finding a ranking function or mechanism Ψ (also called a social welfare function in the social choice theory terminology) defined by: Ψ : n D → D PR = ( 1, 2, . . . , n) → σ = Ψ(PR) where σ is called a consensus ranking. 2.2 Positional Methods 2.2.1 Borda Count This method [5] first assigns a score n j=1 rj i to each document di.",
                "Documents are then ranked by increasing order of this score, breaking ties, if any, arbitrarily. 2.2.2 Linear Combination Methods This family of methods basically combine scores of documents.",
                "When used for the rank aggregation problem, ranks are assumed to be scores or performances to be combined using aggregation operators such as the weighted sum or some variation of it [3, 31, 17, 28].",
                "For instance, Callan et al. [6] used the inference networks model [30] to combine rankings.",
                "Fox and Shaw [15] proposed several combination strategies which are CombSUM, CombMIN, CombMAX, CombANZ and CombMNZ.",
                "The first three operators correspond to the sum, min and max operators, respectively.",
                "CombANZ and CombMNZ respectively divides and multiplies the CombSUM score by the rank hits.",
                "It is shown in [19] that the CombSUM and CombMNZ operators perform better than the others.",
                "Metasearch engines such as SavvySearch and MetaCrawler use the CombSUM strategy to fuse rankings. 2.2.3 Footrule Optimal Aggregation In this method, a consensus ranking minimizes the Spearman footrule distance from the input rankings [21].",
                "Formally, given two full lists j and j , this distance is given by F( j, j ) = nd i=1 |rj i − rj i |.",
                "It extends to several lists as follows.",
                "Given a profile PR and a consensus ranking σ, the Spearman footrule distance of σ to PR is given by F(σ, PR) = n j=1 F(σ, j).",
                "Cook and Kress [8] proposed a similar method which consists in optimizing the distance D( j, j ) = 1 2 nd i,i =1 |rj i,i − rj i,i |, where rj i,i = rj i −rj i .",
                "This formulation has the advantage that it considers the intensity of preferences. 2.2.4 Probabilistic Methods This kind of methods assume that the performance of the input methods on a number of training queries is indicative of their future performance.",
                "During the training process, probabilities of relevance are calculated.",
                "For subsequent queries, documents are ranked based on these probabilities.",
                "For instance, in [20], each input ranking j is divided into a number of segments, and the conditional probability of relevance (R) of each document di depending on the segment k it occurs in, is computed, i.e. prob(R|di, k, j).",
                "For subsequent queries, the score of each document di is given by n j=1 prob(R|di,k, j ) k .",
                "Le Calve and Savoy [18] suggest using a logistic regression approach for combining scores.",
                "Training data is needed to infer the model parameters. 2.3 Majoritarian Methods 2.3.1 Condorcet Procedure The original Condorcet rule [7] specifies that a winner of the election is any item that beats or ties with every other item in a pairwise contest.",
                "Formally, let C(diσdi ) = { j∈ PR : di j di } be the coalition of rankings that are concordant with establishing diσdi , i.e. with the proposition di should be ranked better than di in the final ranking σ. di beats or ties with di iff |C(diσdi )| ≥ |C(di σdi)|.",
                "The repetitive application of the Condorcet algorithm can produce a ranking of items in a natural way: select the Condorcet winner, remove it from the lists, and repeat the previous two steps until there are no more documents to rank.",
                "Since there is not always Condorcet winners, variations of the Condorcet procedure have been developed within the multiple criteria decision aid theory, with methods such as ELECTRE [26]. 2.3.2 Kemeny Optimal Aggregation As in section 2.2.3, a consensus ranking minimizes a geometric distance from the input rankings, where the Kendall tau distance is used instead of the Spearman footrule distance.",
                "Formally, given two full lists j and j , the Kendall tau distance is given by K( j, j ) = |{(di, di ) : i < i , rj i < rj i , rj i > rj i }|, i.e. the number of pairwise disagreements between the two lists.",
                "It is easy to show that the consensus ranking corresponds to the geometric median of the input rankings and that the Kemeny optimal aggregation problem corresponds to the minimum feedback edge set problem. 2.3.3 Markov Chain Methods Markov chains (MCs) have been used by Dwork et al. [11] as a natural method to obtain a consensus ranking where states correspond to the documents to be ranked and the transition probabilities vary depending on the interpretation of the transition event.",
                "In the same reference, the authors proposed four specific MCs and experimental testing had shown that the following MC is the best performing one (see also [24]): • MC4: move from the current state di to the next state di by first choosing a document di uniformly from D. If for the majority of the rankings, we have rj i ≤ rj i , then move to di , else stay in di.",
                "The consensus ranking corresponds to the stationary distribution of MC4. 3.",
                "SPECIFICITIES OF THE RANK AGGREGATION PROBLEM IN THE IR CONTEXT 3.1 Limited Significance of the Rankings The exact positions of documents in one input ranking have limited significance and should not be overemphasized.",
                "For instance, having three relevant documents in the first three positions, any perturbation of these three items will have the same value.",
                "Indeed, in the IR context, the complete order provided by an input method may hide ties.",
                "In this case, we call such rankings semi orders.",
                "This was outlined in [13] as the problem of aggregation with ties.",
                "It is therefore important to build the consensus ranking based on robust information: • Documents with near positions in j are more likely to have similar interest or relevance.",
                "Thus a slight perturbation of the initial ranking is meaningless. • Assuming that document di is better ranked than document di in a ranking j, di is more likely to be definitively more relevant than di in j when the number of intermediate positions between di and di increases. 3.2 Partial Lists In real world applications, such as metasearch engines, rankings provided by the input methods are often partial lists.",
                "This was outlined in [14] as the problem of having to merge top-k results from various input lists.",
                "For instance, in the experiments carried out by Dwork et al. [11], authors found that among the top 100 best documents of 7 input search engines, 67% of the documents were present in only one search engine, whereas less than two documents were present in all the search engines.",
                "Rank aggregation of partial lists raises four major difficulties which we state hereafter, proposing for each of them various working assumptions: 1.",
                "Partial lists can have various lengths, which can favour long lists.",
                "We thus consider the following two working hypotheses: H1 k : We only consider the top k best documents from each input ranking.",
                "H1 all: We consider all the documents from each input ranking. 2.",
                "Since there are different documents in the input rankings, we must decide which documents should be kept in the consensus ranking.",
                "Two working hypotheses are therefore considered: H2 k : We only consider documents which are present in at least k input rankings (k > 1).",
                "H2 all: We consider all the documents which are ranked in at least one input ranking.",
                "Hereafter, we call documents which will be retained in the consensus ranking, candidate documents, and documents that will be excluded from the consensus ranking, excluded documents.",
                "We also call a candidate document which is missing in one or more rankings, a missing document. 3.",
                "Some candidate documents are missing documents in some input rankings.",
                "Main reasons for a missing document are that it was not indexed or it was indexed but deemed irrelevant ; usually this information is not available.",
                "We consider the following two working hypotheses: H3 yes: Each missing document in each j is assigned a position.",
                "H3 no: No assumption is made, that is each missing document is considered neither better nor worse than any other document. 4.",
                "When assumption H2 k holds, each input ranking may contain documents which will not be considered in the consensus ranking.",
                "Regarding the positions of the candidate documents, we can consider the following working hypotheses: H4 init: The initial positions of candidate documents are kept in each input ranking.",
                "H4 new: Candidate documents receive new positions in each input ranking, after discarding excluded ones.",
                "In the IR context, rank aggregation methods need to decide more or less explicitly which assumptions to retain w.r.t. the above-mentioned difficulties. 4.",
                "OUTRANKING APPROACH FOR RANK AGGREGATION 4.1 Presentation Positional methods consider implicitly that the positions of the documents in the input rankings are scores giving thus a cardinal meaning to an ordinal information.",
                "This constitutes a strong assumption that is questionable, especially when the input rankings have different lengths.",
                "Moreover, for positional methods, assumptions H3 and H4 , which are often arbitrary, have a strong impact on the results.",
                "For instance, let us consider an input ranking of 500 documents out of 1000 candidate documents.",
                "Whether we assign to each of the missing documents the position 1, 501, 750 or 1000 -corresponding to variations of H3 yes- will give rise to very contrasted results, especially regarding the top of the consensus ranking.",
                "Majoritarian methods do not suffer from the above-mentioned drawbacks of the positional methods since they build consensus rankings exploiting only ordinal information contained in the input rankings.",
                "Nevertheless, they suppose that such rankings are complete orders, ignoring that they may hide ties.",
                "Therefore, majoritarian methods base consensus rankings on illusory discriminant information rather than less discriminant but more robust information.",
                "Trying to overcome the limits of current rank aggregation methods, we found that outranking approaches, which were initially used for multiple criteria aggregation problems [26], can also be used for the rank aggregation purpose, where each ranking plays the role of a criterion.",
                "Therefore, in order to decide whether a document di should be ranked better than di in the consensus ranking σ, the two following conditions should be met: • a concordance condition which ensures that a majority of the input rankings are concordant with diσdi (majority principle). • a discordance condition which ensures that none of the discordant input rankings strongly refutes dσd (respect of minorities principle).",
                "Formally, the concordance coalition with diσdi is Csp (diσdi ) = { j∈ PR : rj i ≤ rj i − sp} where sp is a preference threshold which is the variation of document positions -whether it is absolute or relative to the ranking length- which draws the boundaries between an indifference and a preference situation between documents.",
                "The discordance coalition with diσdi is Dsv (diσdi ) = { j∈ PR : rj i ≥ rj i + sv} where sv is a veto threshold which is the variation of document positions -whether it is absolute or relative to the ranking length- which draws the boundaries between a weak and a strong opposition to diσdi .",
                "Depending on the exact definition of the preceding concordance and discordance coalitions leading to the definition of some <br>decision rule</br>s, several outranking relations can be defined.",
                "They can be more or less demanding depending on i) the values of the thresholds sp and sv, ii) the importance or minimal size cmin required for the concordance coalition, and iii) the importance or maximum size dmax of the discordance coalition.",
                "A generic outranking relation can thus be defined as follows: diS(sp,sv,cmin,dmax)di ⇔ |Csp (diσdi )| ≥ cmin AND |Dsv (diσdi )| ≤ dmax This expression defines a family of nested outranking relations since S(sp,sv,cmin,dmax) ⊆ S(sp,sv,cmin,dmax) when cmin ≥ cmin and/or dmax ≤ dmax and/or sp ≥ sp and/or sv ≤ sv.",
                "This expression also generalizes the majority rule which corresponds to the particular relation S(0,∞, n 2 ,n).",
                "It also satisfies important properties of rank aggregation methods, called neutrality, Pareto-optimality, Condorcet property and Extended Condorcet property, in the social choice literature [29].",
                "Outranking relations are not necessarily transitive and do not necessarily correspond to rankings since directed cycles may exist.",
                "Therefore, we need specific procedures in order to derive a consensus ranking.",
                "We propose the following procedure which finds its roots in [27].",
                "It consists in partitioning the set of documents into r ranked classes.",
                "Each class Ch contains documents with the same relevance and results from the application of all relations (if possible) to the set of documents remaining after previous classes are computed.",
                "Documents within the same equivalence class are ranked arbitrarily.",
                "Formally, let • R be the set of candidate documents for a query, • S1 , S2 , . . . be a family of nested outranking relations, • Fk(di, E) = |{di ∈ E : diSk di }| be the number of documents in E(E ⊆ R) that could be considered worse than di according to relation Sk , • fk(di, E) = |{di ∈ E : di Sk di}| be the number of documents in E that could be considered better than di according to Sk , • sk(di, E) = Fk(di, E) − fk(di, E) be the qualification of di in E according to Sk .",
                "Each class Ch results from a distillation process.",
                "It corresponds to the last distillate of a series of sets E0 ⊇ E1 ⊇ . . . where E0 = R \\ (C1 ∪ . . . ∪ Ch−1) and Ek is a reduced subset of Ek−1 resulting from the application of the following procedure: 1. compute for each di ∈ Ek−1 its qualification according to Sk , i.e. sk(di, Ek−1), 2. define smax = maxdi∈Ek−1 {sk(di, Ek−1)}, then 3.",
                "Ek = {di ∈ Ek−1 : sk(di, Ek−1) = smax} When one outranking relation is used, the distillation process stops after the first application of the previous procedure, i.e., Ch corresponds to distillate E1.",
                "When different outranking relations are used, the distillation process stops when all the pre-defined outranking relations have been used or when |Ek| = 1. 4.2 Illustrative Example This section illustrates the concepts and procedures of section 4.1.",
                "Let us consider a set of candidate documents R = {d1, d2, d3, d4, d5}.",
                "The following table gives a profile PR of different rankings of the documents of R: PR = ( 1 , 2, 3, 4).",
                "Table 1: Rankings of documents rj i 1 2 3 4 d1 1 3 1 5 d2 2 1 3 3 d3 3 2 2 1 d4 4 4 5 2 d5 5 5 4 4 Let us suppose that the preference and veto thresholds are set to values 1 and 4 respectively, and that the concordance and discordance thresholds are set to values 2 and 1 respectively.",
                "The following tables give the concordance, discordance and outranking matrices.",
                "Each entry csp (di, di ) (dsv (di, di )) in the concordance (discordance) matrix gives the number of rankings that are concordant (discordant) with diσdi , i.e. csp (di, di ) = |Csp (diσdi )| and dsv (di, di ) = |Dsv (diσdi )|.",
                "Table 2: Computation of the outranking relation d1 d2 d3 d4 d5 d1 - 2 2 3 3 d2 2 - 2 3 4 d3 2 2 - 4 4 d4 1 1 0 - 3 d5 1 0 0 1Concordance Matrix d1 d2 d3 d4 d5 d1 - 0 1 0 0 d2 0 - 0 0 0 d3 0 0 - 0 0 d4 1 0 0 - 0 d5 1 1 0 0Discordance Matrix d1 d2 d3 d4 d5 d1 - 1 1 1 1 d2 1 - 1 1 1 d3 1 1 - 1 1 d4 0 0 0 - 1 d5 0 0 0 0Outranking Matrix (S1) For instance, the concordance coalition for the assertion d1σd4 is C1(d1σd4) = { 1, 2, 3} and the discordance coalition for the same assertion is D4(d1σd4) = ∅.",
                "Therefore, c1(d1, d4) = 3, d4(d1, d4) = 0 and d1S1 d4 holds.",
                "Notice that Fk(di, R) (fk(di, R)) is given by summing the values of the ith row (column) of the outranking matrix.",
                "The consensus ranking is obtained as follows: to get the first class C1, we compute the qualifications of all the documents of E0 = R with respect to S1 .",
                "They are respectively 2, 2, 2, -2 and -4.",
                "Therefore smax equals 2 and C1 = E1 = {d1, d2, d3}.",
                "Observe that, if we had used a second outranking relation S2(⊇ S1), these three documents could have been possibly discriminated.",
                "At this stage, we remove documents of C1 from the outranking matrix and compute the next class C2: we compute the new qualifications of the documents of E0 = R \\ C1 = {d4, d5}.",
                "They are respectively 1 and -1.",
                "So C3 = E1 = {d4}.",
                "The last document d5 is the only document of the last class C3.",
                "Thus, the consensus ranking is {d1, d2, d3} → {d4} → {d5}. 5.",
                "EXPERIMENTS AND RESULTS 5.1 Test Setting To facilitate empirical investigation of the proposed methodology, we developed a prototype metasearch engine that implements a version of our outranking approach for rank aggregation.",
                "In this paper, we apply our approach to the Topic Distillation (TD) task of TREC-2004 Web track [10].",
                "In this task, there are 75 topics where only a short description of each is given.",
                "For each query, we retained the rankings of the 10 best runs of the TD task which are provided by TREC-2004 participating teams.",
                "The performances of these runs are reported in table 3.",
                "Table 3: Performances of the 10 best runs of the TD task of TREC-2004 Run Id MAP P@10 S@1 S@5 S@10 uogWebCAU150 17.9% 24.9% 50.7% 77.3% 89.3% MSRAmixed1 17.8% 25.1% 38.7% 72.0% 88.0% MSRC04C12 16.5% 23.1% 38.7% 74.7% 80.0% humW04rdpl 16.3% 23.1% 37.3% 78.7% 90.7% THUIRmix042 14.7% 20.5% 21.3% 58.7% 74.7% UAmsT04MWScb 14.6% 20.9% 36.0% 66.7% 76.0% ICT04CIIS1AT 14.1% 20.8% 33.3% 64.0% 78.7% SJTUINCMIX5 12.9% 18.9% 29.3% 57.3% 72.0% MU04web1 11.5% 19.9% 33.3% 64.0% 76.0% MeijiHILw3 11.5% 15.3% 30.7% 54.7% 64.0% Average 14.7% 21.2% 34.9% 66.8% 78.94% For each query, each run provides a ranking of about 1000 documents.",
                "The number of documents retrieved by all these runs ranges from 543 to 5769.",
                "Their average (median) number is 3340 (3386).",
                "It is worth noting that we found similar distributions of the documents among the rankings as in [11].",
                "For evaluation, we used the trec eval standard tool which is used by the TREC community to calculate the standard measures of system effectiveness which are Mean Average Precision (MAP) and Success@n (S@n) for n=1, 5 and 10.",
                "Our approach effectiveness is compared against some high performing official results from TREC-2004 as well as against some standard rank aggregation algorithms.",
                "In the experiments, significance testing is mainly based on the t-student statistic which is computed on the basis of the MAP values of the compared runs.",
                "In the tables of the following section, statistically significant differences are marked with an asterisk.",
                "Values between brackets of the first column of each table, indicate the parameter value of the corresponding run. 5.2 Results We carried out several series of runs in order to i) study performance variations of the outranking approach when tuning the parameters and working assumptions, ii) compare performances of the outranking approach vs standard rank aggregation strategies , and iii) check whether rank aggregation performs better than the best input rankings.",
                "We set our basic run mcm with the following parameters.",
                "We considered that each input ranking is a complete order (sp = 0) and that an input ranking strongly refutes diσdi when the difference of both document positions is large enough (sv = 75%).",
                "Preference and veto thresholds are computed proportionally to the number of documents retained in each input ranking.",
                "They consequently may vary from one ranking to another.",
                "In addition, to accept the assertion diσdi , we supposed that the majority of the rankings must be concordant (cmin = 50%) and that every input ranking can impose its veto (dmax = 0).",
                "Concordance and discordance thresholds are computed for each tuple (di, di ) as the percentage of the input rankings of PRi ∩PRi .",
                "Thus, our choice of parameters leads to the definition of the outranking relation S(0,75%,50%,0).",
                "To test the run mcm, we had chosen the following assumptions.",
                "We retained the top 100 best documents from each input ranking (H1 100), only considered documents which are present in at least half of the input rankings (H2 5 ) and assumed H3 no and H4 new.",
                "In these conditions, the number of successful documents was about 100 on average, and the computation time per query was less than one second.",
                "Obviously, modifying the working assumptions should have deeper impact on the performances than tuning our model parameters.",
                "This was validated by preliminary experiments.",
                "Thus, we hereafter begin by studying performance variation when different sets of assumptions are considered.",
                "Afterwards, we study the impact of tuning parameters.",
                "Finally, we compare our model performances w.r.t. the input rankings as well as some standard data fusion algorithms. 5.2.1 Impact of the Working Assumptions Table 4 summarizes the performance variation of the outranking approach under different working hypotheses.",
                "In Table 4: Impact of the working assumptions Run Id MAP S@1 S@5 S@10 mcm 18.47% 41.33% 81.33% 86.67% mcm22 (H3 yes) 17.72% (-4.06%) 34.67% 81.33% 86.67% mcm23 (H4 init) 18.26% (-1.14%) 41.33% 81.33% 86.67% mcm24 (H1 all) 20.67% (+11.91%*) 38.66% 80.00% 86.66% mcm25 (H2 all) 21.68% (+17.38%*) 40.00% 78.66% 89.33% this table, we first show that run mcm22, in which missing documents are all put in the same last position of each input ranking, leads to performance drop w.r.t. run mcm.",
                "Moreover, S@1 moves from 41.33% to 34.67% (-16.11%).",
                "This shows that several relevant documents which were initially put at the first position of the consensus ranking in mcm, lose this first position but remain ranked in the top 5 documents since S@5 did not change.",
                "We also conclude that documents which have rather good positions in some input rankings are more likely to be relevant, even though they are missing in some other rankings.",
                "Consequently, when they are missing in some rankings, assigning worse ranks to these documents is harmful for performance.",
                "Also, from Table 4, we found that the performances of runs mcm and mcm23 are similar.",
                "Therefore, the outranking approach is not sensitive to keeping the initial positions of candidate documents or recomputing them by discarding excluded ones.",
                "From the same Table 4, performance of the outranking approach increases significantly for runs mcm24 and mcm25.",
                "Therefore, whether we consider all the documents which are present in half of the rankings (mcm24) or we consider all the documents which are ranked in the first 100 positions in one or more rankings (mcm25), increases performances.",
                "This result was predictable since in both cases we have more detailed information on the relative importance of documents.",
                "Tables 5 and 6 confirm this evidence.",
                "Table 5, where values between brackets of the first column give the number of documents which are retained from each input ranking, shows that selecting more documents from each input ranking leads to performance increase.",
                "It is worth mentioning that selecting more than 600 documents from each input ranking does not improve performance.",
                "Table 5: Impact of the number of retained documents Run Id MAP S@1 S@5 S@10 mcm (100) 18.47% 41.33% 81.33% 86.67% mcm24-1 (200) 19.32% (+4.60%) 42.67% 78.67% 88.00% mcm24-2 (400) 19.88% (+7.63%*) 37.33% 80.00% 88.00% mcm24-3 (600) 20.80% (+12.62%*) 40.00% 80.00% 88.00% mcm24-4 (800) 20.66% (+11.86%*) 40.00% 78.67% 86.67% mcm24 (1000) 20.67% (+11.91%*) 38.66% 80.00% 86.66% Table 6 reports runs corresponding to variations of H2 k .",
                "Values between brackets are rank hits.",
                "For instance, in the run mcm32, only documents which are present in 3 or more input rankings, were considered successful.",
                "This table shows that performance is significantly better when rare documents are considered, whereas it decreases significantly when these documents are discarded.",
                "Therefore, we conclude that many of the relevant documents are retrieved by a rather small set of IR models.",
                "Table 6: Performance considering different rank hits Run Id MAP S@1 S@5 S@10 mcm25 (1) 21.68% (+17.38%*) 40.00% 78.67% 89.33% mcm32 (3) 18.98% (+2.76%) 38.67% 80.00% 85.33% mcm (5) 18.47% 41.33% 81.33% 86.67% mcm33 (7) 15.83% (-14.29%*) 37.33% 78.67% 85.33% mcm34 (9) 10.96% (-40.66%*) 36.11% 66.67% 70.83% mcm35 (10) 7.42% (-59.83%*) 39.22% 62.75% 64.70% For both runs mcm24 and mcm25, the number of successful documents was about 1000 and therefore, the computation time per query increased and became around 5 seconds. 5.2.2 Impact of the Variation of the Parameters Table 7 shows performance variation of the outranking approach when different preference thresholds are considered.",
                "We found performance improvement up to threshold values of about 5%, then there is a decrease in the performance which becomes significant for threshold values greater than 10%.",
                "Moreover, S@1 improves from 41.33% to 46.67% when preference threshold changes from 0 to 5%.",
                "We can thus conclude that the input rankings are semi orders rather than complete orders.",
                "Table 8 shows the evolution of the performance measures w.r.t. the concordance threshold.",
                "We can conclude that in order to put document di before di in the consensus ranking, Table 7: Impact of the variation of the preference threshold from 0 to 12.5% Run Id MAP S@1 S@5 S@10 mcm (0%) 18.47% 41.33% 81.33% 86.67% mcm1 (1%) 18.57% (+0.54%) 41.33% 81.33% 86.67% mcm2 (2.5%) 18.63% (+0.87%) 42.67% 78.67% 86.67% mcm3 (5%) 18.69% (+1.19%) 46.67% 81.33% 86.67% mcm4 (7.5%) 18.24% (-1.25%) 46.67% 81.33% 86.67% mcm5 (10%) 17.93% (-2.92%) 40.00% 82.67% 86.67% mcm5b (12.5%) 17.51% (-5.20%*) 41.33% 80.00% 86.67% at least half of the input rankings of PRi ∩ PRi should be concordant.",
                "Performance drops significantly for very low and very high values of the concordance threshold.",
                "In fact, for such values, the concordance condition is either fulfilled rather always by too many document pairs or not fulfilled at all, respectively.",
                "Therefore, the outranking relation becomes either too weak or too strong respectively.",
                "Table 8: Impact of the variation of cmin Run Id MAP S@1 S@5 S@10 mcm11 (20%) 17.63% (-4.55%*) 41.33% 76.00% 85.33% mcm12 (40%) 18.37% (-0.54%) 42.67% 76.00% 86.67% mcm (50%) 18.47% 41.33% 81.33% 86.67% mcm13 (60%) 18.42% (-0.27%) 40.00% 78.67% 86.67% mcm14 (80%) 17.43% (-5.63%*) 40.00% 78.67% 86.67% mcm15 (100%) 16.12% (-12.72%*) 41.33% 70.67% 85.33% In the experiments, varying the veto threshold as well as the discordance threshold within reasonable intervals does not have significant impact on performance measures.",
                "In fact, runs with different veto thresholds (sv ∈ [50%; 100%]) had similar performances even though there is a slight advantage for runs with high threshold values which means that it is better not to allow the input rankings to put their veto easily.",
                "Also, tuning the discordance threshold was carried out for values 50% and 75% of the veto threshold.",
                "For these runs we did not get any noticeable performance variation, although for low discordance thresholds (dmax < 20%), performance slightly decreased. 5.2.3 Impact of the Variation of the Number of Input Rankings To study performance evolution when different sets of input rankings are considered, we carried three more runs where 2, 4, and 6 of the best performing sets of the input rankings are considered.",
                "Results reported in Table 9 are seemingly counter-intuitive and also do not support previous findings regarding rank aggregation research [3].",
                "Nevertheless, this result shows that low performing rankings bring more noise than information to the establishment of the consensus ranking.",
                "Therefore, when they are considered, performance decreases.",
                "Table 9: Performance considering different best performing sets of input rankings Run Id MAP S@1 S@5 S@10 mcm (10) 18.47% 41.33% 81.33% 86.67% mcm27 (6) 18.60% (+0.70%) 41.33% 80.00% 85.33% mcm28 (4) 19.02% (+2.98%) 40.00% 86.67% 88.00% mcm29 (2) 18.33% (-0.76%) 44.00% 76.00% 88.00% 5.2.4 Comparison of the Performance of Different Rank Aggregation Methods In this set of runs, we compare the outranking approach with some standard rank aggregation methods which were proven to have acceptable performance in previous studies: we considered two positional methods which are the CombSUM and the CombMNZ strategies.",
                "We also examined the performance of one majoritarian method which is the Markov chain method (MC4).",
                "For the comparisons, we considered a specific outranking relation S∗ = S(5%,50%,50%,30%) which results in good overall performances when tuning all the parameters.",
                "The first row of Table 10 gives performances of the rank aggregation methods w.r.t. a basic assumption set A1 = (H1 100, H2 5 , H4 new): we only consider the 100 first documents from each ranking, then retain documents present in 5 or more rankings and update ranks of successful documents.",
                "For positional methods, we place missing documents at the queue of the ranking (H3 yes) whereas for our method as well as for MC4, we retained hypothesis H3 no.",
                "The three following rows of Table 10 report performances when changing one element from the basic assumption set: the second row corresponds to the assumption set A2 = (H1 1000, H2 5 , H4 new), i.e. changing the number of retained documents from 100 to 1000.",
                "The third row corresponds to the assumption set A3 = (H1 100, H2 all, H4 new), i.e. considering the documents present in at least one ranking.",
                "The fourth row corresponds to the assumption set A4 = (H1 100, H2 5 , H4 init), i.e. keeping the original ranks of successful documents.",
                "The fifth row of Table 10, labeled A5, gives performance when all the 225 queries of the Web track of TREC-2004 are considered.",
                "Obviously, performance level cannot be compared with previous lines since the additional queries are different from the TD queries and correspond to other tasks (Home Page and Named Page tasks [10]) of TREC-2004 Web track.",
                "This set of runs aims to show whether relative performance of the various methods is task-dependent.",
                "The last row of Table 10, labeled A6, reports performance of the various methods considering the TD task of TREC2002 instead of TREC-2004: we fused the results of input rankings of the 10 best official runs for each of the 50 TD queries [9] considering the set of assumptions A1 of the first row.",
                "This aims to show whether relative performance of the various methods changes from year to year.",
                "Values between brackets of Table 10 are variations of performance of each rank aggregation method w.r.t. performance of the outranking approach.",
                "Table 10: Performance (MAP) of different rank aggregation methods under 3 different test collections mcm combSUM combMNZ markov A1 18.79% 17.54% (-6.65%*) 17.08% (-9.10%*) 18.63% (-0.85%) A2 21.36% 19.18% (-10.21%*) 18.61% (-12.87%*) 21.33% (-0.14%) A3 21.92% 21.38% (-2.46%) 20.88% (-4.74%) 19.35% (-11.72%*) A4 18.64% 17.58% (-5.69%*) 17.18% (-7.83%*) 18.63% (-0.05%) A5 55.39% 52.16% (-5.83%*) 49.70% (-10.27%*) 53.30% (-3.77%) A6 16.95% 15.65% (-7.67%*) 14.57% (-14.04%*) 16.39% (-3.30%) From the analysis of table 10 the following can be established: • for all the runs, considering all the documents in each input ranking (A2) significantly improves performance (MAP increases by 11.62% on average).",
                "This is predictable since some initially unreported relevant documents would receive better positions in the consensus ranking. • for all the runs, considering documents even those present in only one input ranking (A3) significantly improves performance.",
                "For mcm, combSUM and combMNZ, performance improvement is more important (MAP increases by 20.27% on average) than for the markov run (MAP increases by 3.86%). • preserving the initial positions of documents (A4) or recomputing them (A1) does not have a noticeable influence on performance for both positional and majoritarian methods. • considering all the queries of the Web track of TREC2004 (A5) as well as the TD queries of the Web track of TREC-2002 (A6) does not alter the relative performance of the different data fusion methods. • considering the TD queries of the Web track of TREC2002, performances of all the data fusion methods are lower than that of the best performing input ranking for which the MAP value equals 18.58%.",
                "This is because most of the fused input rankings have very low performances compared to the best one, which brings more noise to the consensus ranking. • performances of the data fusion methods mcm and markov are significantly better than that of the best input ranking uogWebCAU150.",
                "This remains true for runs combSUM and combMNZ only under assumptions H1 all or H2 all.",
                "This shows that majoritarian methods are less sensitive to assumptions than positional methods. • outranking approach always performs significantly better than positional methods combSUM and combMNZ.",
                "It has also better performances than the Markov chain method, especially under assumption H2 all where difference of performances becomes significant. 6.",
                "CONCLUSIONS In this paper, we address the rank aggregation problem where different, but not disjoint, lists of documents are to be fused.",
                "We noticed that the input rankings can hide ties, so they should not be considered as complete orders.",
                "Only robust information should be used from each input ranking.",
                "Current rank aggregation methods, and especially positional methods (e.g. combSUM [15]), are not initially designed to work with such rankings.",
                "They should be adapted by considering specific working assumptions.",
                "We propose a new outranking method for rank aggregation which is well adapted to the IR context.",
                "Indeed, it ranks two documents w.r.t. the intensity of their positions difference in each input ranking and also considering the number of the input rankings that are concordant and discordant in favor of a specific document.",
                "There is also no need to make specific assumptions on the positions of the missing documents.",
                "This is an important feature since the absence of a document from a ranking should not be necessarily interpreted negatively.",
                "Experimental results show that the outranking method significantly out-performs popular classical positional data fusion methods like combSUM and combMNZ strategies.",
                "It also out-performs a good performing majoritarian methods which is the Markov chain method.",
                "These results are tested against different test collections and queries.",
                "From the experiments, we can also conclude that in order to improve the performances, we should fuse result lists of well performing IR models, and that majoritarian data fusion methods perform better than positional methods.",
                "The proposed method can have a real impact on Web metasearch performances since only ranks are available from most primary search engines, whereas most of the current approaches need scores to merge result lists into one single list.",
                "Further work involves investigating whether the outranking approach performs well in various other contexts, e.g. using the document scores or some combination of document ranks and scores.",
                "Acknowledgments The authors would like to thank Jacques Savoy for his valuable comments on a preliminary version of this paper. 7.",
                "REFERENCES [1] A. Aronson, D. Demner-Fushman, S. Humphrey, J. Lin, H. Liu, P. Ruch, M. Ruiz, L. Smith, L. Tanabe, and W. Wilbur.",
                "Fusion of knowledge-intensive and statistical approaches for retrieving and annotating textual genomics documents.",
                "In Proceedings TREC2005.",
                "NIST Publication, 2005. [2] R. A. Baeza-Yates and B.",
                "A. Ribeiro-Neto.",
                "Modern Information Retrieval.",
                "ACM Press , 1999. [3] B. T. Bartell, G. W. Cottrell, and R. K. Belew.",
                "Automatic combination of multiple ranked retrieval systems.",
                "In Proceedings ACM-SIGIR94, pages 173-181.",
                "Springer-Verlag, 1994. [4] N. J. Belkin, P. Kantor, E. A.",
                "Fox, and J.",
                "A. Shaw.",
                "Combining evidence of multiple query representations for information retrieval.",
                "IPM, 31(3):431-448, 1995. [5] J. Borda.",
                "M´emoire sur les ´elections au scrutin.",
                "Histoire de lAcad´emie des Sciences, 1781. [6] J. P. Callan, Z. Lu, and W. B. Croft.",
                "Searching distributed collections with inference networks.",
                "In Proceedings ACM-SIGIR95, pages 21-28, 1995. [7] M. Condorcet.",
                "Essai sur lapplication de lanalyse `a la probabilit´e des d´ecisions rendues `a la pluralit´e des voix.",
                "Imprimerie Royale, Paris, 1785. [8] W. D. Cook and M. Kress.",
                "Ordinal ranking with intensity of preference.",
                "Management Science, 31(1):26-32, 1985. [9] N. Craswell and D. Hawking.",
                "Overview of the TREC-2002 Web Track.",
                "In Proceedings TREC2002.",
                "NIST Publication, 2002. [10] N. Craswell and D. Hawking.",
                "Overview of the TREC-2004 Web Track.",
                "In Proceedings of TREC2004.",
                "NIST Publication, 2004. [11] C. Dwork, S. R. Kumar, M. Naor, and D. Sivakumar.",
                "Rank aggregation methods for the Web.",
                "In Proceedings WWW2001, pages 613-622, 2001. [12] R. Fagin.",
                "Combining fuzzy information from multiple systems.",
                "JCSS, 58(1):83-99, 1999. [13] R. Fagin, R. Kumar, M. Mahdian, D. Sivakumar, and E. Vee.",
                "Comparing and aggregating rankings with ties.",
                "In PODS, pages 47-58, 2004. [14] R. Fagin, R. Kumar, and D. Sivakumar.",
                "Comparing top k lists.",
                "SIAM J. on Discrete Mathematics, 17(1):134-160, 2003. [15] E. A.",
                "Fox and J.",
                "A. Shaw.",
                "Combination of multiple searches.",
                "In Proceedings of TREC3.",
                "NIST Publication, 1994. [16] J. Katzer, M. McGill, J. Tessier, W. Frakes, and P. DasGupta.",
                "A study of the overlap among document representations.",
                "Information Technology: Research and Development, 1(4):261-274, 1982. [17] L. S. Larkey, M. E. Connell, and J. Callan.",
                "Collection selection and results merging with topically organized U.S. patents and TREC data.",
                "In Proceedings ACM-CIKM2000, pages 282-289.",
                "ACM Press, 2000. [18] A.",
                "Le Calv´e and J. Savoy.",
                "Database merging strategy based on logistic regression.",
                "IPM, 36(3):341-359, 2000. [19] J. H. Lee.",
                "Analyses of multiple evidence combination.",
                "In Proceedings ACM-SIGIR97, pages 267-276, 1997. [20] D. Lillis, F. Toolan, R. Collier, and J. Dunnion.",
                "Probfuse: a probabilistic approach to data fusion.",
                "In Proceedings ACM-SIGIR2006, pages 139-146.",
                "ACM Press, 2006. [21] J. I. Marden.",
                "Analyzing and Modeling Rank Data.",
                "Number 64 in Monographs on Statistics and Applied Probability.",
                "Chapman & Hall, 1995. [22] M. Montague and J.",
                "A. Aslam.",
                "Metasearch consistency.",
                "In Proceedings ACM-SIGIR2001, pages 386-387.",
                "ACM Press, 2001. [23] D. M. Pennock and E. Horvitz.",
                "Analysis of the axiomatic foundations of collaborative filtering.",
                "In Workshop on AI for Electronic Commerce at the 16th National Conference on Artificial Intelligence, 1999. [24] M. E. Renda and U. Straccia.",
                "Web metasearch: rank vs. score based rank aggregation methods.",
                "In Proceedings ACM-SAC2003, pages 841-846.",
                "ACM Press, 2003. [25] W. H. Riker.",
                "Liberalism against populism.",
                "Waveland Press, 1982. [26] B. Roy.",
                "The outranking approach and the foundations of ELECTRE methods.",
                "Theory and Decision, 31:49-73, 1991. [27] B. Roy and J. Hugonnard.",
                "Ranking of suburban line extension projects on the Paris metro system by a multicriteria method.",
                "Transportation Research, 16A(4):301-312, 1982. [28] L. Si and J. Callan.",
                "Using sampled data and regression to merge search engine results.",
                "In Proceedings ACM-SIGIR2002, pages 19-26.",
                "ACM Press, 2002. [29] M. Truchon.",
                "An extension of the Condorcet criterion and Kemeny orders.",
                "Cahier 9813, Centre de Recherche en Economie et Finance Appliqu´ees, Oct. 1998. [30] H. Turtle and W. B. Croft.",
                "Inference networks for document retrieval.",
                "In Proceedings of ACM-SIGIR90, pages 1-24.",
                "ACM Press, 1990. [31] C. C. Vogt and G. W. Cottrell.",
                "Fusion via a linear combination of scores.",
                "Information Retrieval, 1(3):151-173, 1999."
            ],
            "original_annotated_samples": [],
            "translated_annotated_samples": [
                "En este contexto, proponemos un método de agregación de rango dentro de un marco de criterios múltiples utilizando mecanismos de agregación basados en la \"regla de decisión\" que identifican razones positivas y negativas para juzgar si un documento debería obtener un mejor rango que otro.",
                "Dependiendo de la definición exacta de las coaliciones de concordancia y discordancia anteriores que conducen a la definición de algunas \"reglas de decisión\", se pueden definir varias relaciones superiores."
            ],
            "translated_text": "",
            "candidates": [
                "regla de decisión",
                "regla de decisión",
                "regla de decisión",
                "reglas de decisión"
            ],
            "error": []
        },
        "outranking approach": {
            "translated_key": "enfoque superior",
            "is_in_text": true,
            "original_annotated_sentences": [
                "An <br>outranking approach</br> for Rank Aggregation in Information Retrieval Mohamed Farah Lamsade, Paris Dauphine University Place du Mal de Lattre de Tassigny 75775 Paris Cedex 16, France farah@lamsade.dauphine.fr Daniel Vanderpooten Lamsade, Paris Dauphine University Place du Mal de Lattre de Tassigny 75775 Paris Cedex 16, France vdp@lamsade.dauphine.fr ABSTRACT Research in Information Retrieval usually shows performance improvement when many sources of evidence are combined to produce a ranking of documents (e.g., texts, pictures, sounds, etc.).",
                "In this paper, we focus on the rank aggregation problem, also called data fusion problem, where rankings of documents, searched into the same collection and provided by multiple methods, are combined in order to produce a new ranking.",
                "In this context, we propose a rank aggregation method within a multiple criteria framework using aggregation mechanisms based on decision rules identifying positive and negative reasons for judging whether a document should get a better rank than another.",
                "We show that the proposed method deals well with the Information Retrieval distinctive features.",
                "Experimental results are reported showing that the suggested method performs better than the well-known CombSUM and CombMNZ operators.",
                "Categories and Subject Descriptors: H.3.3 [Information Systems]: Information Search and Retrieval - Retrieval models.",
                "General Terms: Algorithms, Measurement, Experimentation, Performance, Theory. 1.",
                "INTRODUCTION A wide range of current Information Retrieval (IR) approaches are based on various search models (Boolean, Vector Space, Probabilistic, Language, etc. [2]) in order to retrieve relevant documents in response to a user request.",
                "The result lists produced by these approaches depend on the exact definition of the relevance concept.",
                "Rank aggregation approaches, also called data fusion approaches, consist in combining these result lists in order to produce a new and hopefully better ranking.",
                "Such approaches give rise to metasearch engines in the Web context.",
                "We consider, in the following, cases where only ranks are available and no other additional information is provided such as the relevance scores.",
                "This corresponds indeed to the reality, where only ordinal information is available.",
                "Data fusion is also relevant in other contexts, such as when the user writes several queries of his/her information need (e.g., a boolean query and a natural language query) [4], or when many document surrogates are available [16].",
                "Several studies argued that rank aggregation has the potential of combining effectively all the various sources of evidence considered in various input methods.",
                "For instance, experiments carried out in [16], [30], [4] and [19] showed that documents which appear in the lists of the majority of the input methods are more likely to be relevant.",
                "Moreover, Lee [19] and Vogt and Cottrell [31] found that various retrieval approaches often return very different irrelevant documents, but many of the same relevant documents.",
                "Bartell et al. [3] also found that rank aggregation methods improve the performances w.r.t. those of the input methods, even when some of them have weak individual performances.",
                "These methods also tend to smooth out biases of the input methods according to Montague and Aslam [22].",
                "Data fusion has recently been proved to improve performances for both the ad-hoc retrieval and categorization tasks within the TREC genomics track in 2005 [1].",
                "The rank aggregation problem was addressed in various fields such as i) in social choice theory which studies voting algorithms which specify winners of elections or winners of competitions in tournaments [29], ii) in statistics when studying correlation between rankings, iii) in distributed databases when results from different databases must be combined [12], and iv) in collaborative filtering [23].",
                "Most current rank aggregation methods consider each input ranking as a permutation over the same set of items.",
                "They also give rigid interpretation to the exact ranking of the items.",
                "Both of these assumptions are rather not valid in the IR context, as will be shown in the following sections.",
                "The remaining of the paper is organized as follows.",
                "We first review current rank aggregation methods in Section 2.",
                "Then we outline the specificities of the data fusion problem in the IR context (Section 3).",
                "In Section 4, we present a new aggregation method which is proven to best fit the IR context.",
                "Experimental results are presented in Section 5 and conclusions are provided in a final section. 2.",
                "RELATED WORK As pointed out by Riker [25], we can distinguish two families of rank aggregation methods: positional methods which assign scores to items to be ranked according to the ranks they receive and majoritarian methods which are based on pairwise comparisons of items to be ranked.",
                "These two families of methods find their roots in the pioneering works of Borda [5] and Condorcet [7], respectively, in the social choice literature. 2.1 Preliminaries We first introduce some basic notations to present the rank aggregation methods in a uniform way.",
                "Let D = {d1, d2, . . . , dnd } be a set of nd documents.",
                "A list or a ranking j is an ordering defined on Dj ⊆ D (j = 1, . . . , n).",
                "Thus, di j di means di is ranked better than di in j.",
                "When Dj = D, j is said to be a full list.",
                "Otherwise, it is a partial list.",
                "If di belongs to Dj, rj i denotes the rank or position of di in j.",
                "We assume that the best answer (document) is assigned the position 1 and the worst one is assigned the position |Dj|.",
                "Let D be the set of all permutations on D or all subsets of D. A profile is a n-tuple of rankings PR = ( 1, 2, . . . , n).",
                "Restricting PR to the rankings containing document di defines PRi.",
                "We also call the number of rankings which contain document di the rank hits of di [19].",
                "The rank aggregation or data fusion problem consists of finding a ranking function or mechanism Ψ (also called a social welfare function in the social choice theory terminology) defined by: Ψ : n D → D PR = ( 1, 2, . . . , n) → σ = Ψ(PR) where σ is called a consensus ranking. 2.2 Positional Methods 2.2.1 Borda Count This method [5] first assigns a score n j=1 rj i to each document di.",
                "Documents are then ranked by increasing order of this score, breaking ties, if any, arbitrarily. 2.2.2 Linear Combination Methods This family of methods basically combine scores of documents.",
                "When used for the rank aggregation problem, ranks are assumed to be scores or performances to be combined using aggregation operators such as the weighted sum or some variation of it [3, 31, 17, 28].",
                "For instance, Callan et al. [6] used the inference networks model [30] to combine rankings.",
                "Fox and Shaw [15] proposed several combination strategies which are CombSUM, CombMIN, CombMAX, CombANZ and CombMNZ.",
                "The first three operators correspond to the sum, min and max operators, respectively.",
                "CombANZ and CombMNZ respectively divides and multiplies the CombSUM score by the rank hits.",
                "It is shown in [19] that the CombSUM and CombMNZ operators perform better than the others.",
                "Metasearch engines such as SavvySearch and MetaCrawler use the CombSUM strategy to fuse rankings. 2.2.3 Footrule Optimal Aggregation In this method, a consensus ranking minimizes the Spearman footrule distance from the input rankings [21].",
                "Formally, given two full lists j and j , this distance is given by F( j, j ) = nd i=1 |rj i − rj i |.",
                "It extends to several lists as follows.",
                "Given a profile PR and a consensus ranking σ, the Spearman footrule distance of σ to PR is given by F(σ, PR) = n j=1 F(σ, j).",
                "Cook and Kress [8] proposed a similar method which consists in optimizing the distance D( j, j ) = 1 2 nd i,i =1 |rj i,i − rj i,i |, where rj i,i = rj i −rj i .",
                "This formulation has the advantage that it considers the intensity of preferences. 2.2.4 Probabilistic Methods This kind of methods assume that the performance of the input methods on a number of training queries is indicative of their future performance.",
                "During the training process, probabilities of relevance are calculated.",
                "For subsequent queries, documents are ranked based on these probabilities.",
                "For instance, in [20], each input ranking j is divided into a number of segments, and the conditional probability of relevance (R) of each document di depending on the segment k it occurs in, is computed, i.e. prob(R|di, k, j).",
                "For subsequent queries, the score of each document di is given by n j=1 prob(R|di,k, j ) k .",
                "Le Calve and Savoy [18] suggest using a logistic regression approach for combining scores.",
                "Training data is needed to infer the model parameters. 2.3 Majoritarian Methods 2.3.1 Condorcet Procedure The original Condorcet rule [7] specifies that a winner of the election is any item that beats or ties with every other item in a pairwise contest.",
                "Formally, let C(diσdi ) = { j∈ PR : di j di } be the coalition of rankings that are concordant with establishing diσdi , i.e. with the proposition di should be ranked better than di in the final ranking σ. di beats or ties with di iff |C(diσdi )| ≥ |C(di σdi)|.",
                "The repetitive application of the Condorcet algorithm can produce a ranking of items in a natural way: select the Condorcet winner, remove it from the lists, and repeat the previous two steps until there are no more documents to rank.",
                "Since there is not always Condorcet winners, variations of the Condorcet procedure have been developed within the multiple criteria decision aid theory, with methods such as ELECTRE [26]. 2.3.2 Kemeny Optimal Aggregation As in section 2.2.3, a consensus ranking minimizes a geometric distance from the input rankings, where the Kendall tau distance is used instead of the Spearman footrule distance.",
                "Formally, given two full lists j and j , the Kendall tau distance is given by K( j, j ) = |{(di, di ) : i < i , rj i < rj i , rj i > rj i }|, i.e. the number of pairwise disagreements between the two lists.",
                "It is easy to show that the consensus ranking corresponds to the geometric median of the input rankings and that the Kemeny optimal aggregation problem corresponds to the minimum feedback edge set problem. 2.3.3 Markov Chain Methods Markov chains (MCs) have been used by Dwork et al. [11] as a natural method to obtain a consensus ranking where states correspond to the documents to be ranked and the transition probabilities vary depending on the interpretation of the transition event.",
                "In the same reference, the authors proposed four specific MCs and experimental testing had shown that the following MC is the best performing one (see also [24]): • MC4: move from the current state di to the next state di by first choosing a document di uniformly from D. If for the majority of the rankings, we have rj i ≤ rj i , then move to di , else stay in di.",
                "The consensus ranking corresponds to the stationary distribution of MC4. 3.",
                "SPECIFICITIES OF THE RANK AGGREGATION PROBLEM IN THE IR CONTEXT 3.1 Limited Significance of the Rankings The exact positions of documents in one input ranking have limited significance and should not be overemphasized.",
                "For instance, having three relevant documents in the first three positions, any perturbation of these three items will have the same value.",
                "Indeed, in the IR context, the complete order provided by an input method may hide ties.",
                "In this case, we call such rankings semi orders.",
                "This was outlined in [13] as the problem of aggregation with ties.",
                "It is therefore important to build the consensus ranking based on robust information: • Documents with near positions in j are more likely to have similar interest or relevance.",
                "Thus a slight perturbation of the initial ranking is meaningless. • Assuming that document di is better ranked than document di in a ranking j, di is more likely to be definitively more relevant than di in j when the number of intermediate positions between di and di increases. 3.2 Partial Lists In real world applications, such as metasearch engines, rankings provided by the input methods are often partial lists.",
                "This was outlined in [14] as the problem of having to merge top-k results from various input lists.",
                "For instance, in the experiments carried out by Dwork et al. [11], authors found that among the top 100 best documents of 7 input search engines, 67% of the documents were present in only one search engine, whereas less than two documents were present in all the search engines.",
                "Rank aggregation of partial lists raises four major difficulties which we state hereafter, proposing for each of them various working assumptions: 1.",
                "Partial lists can have various lengths, which can favour long lists.",
                "We thus consider the following two working hypotheses: H1 k : We only consider the top k best documents from each input ranking.",
                "H1 all: We consider all the documents from each input ranking. 2.",
                "Since there are different documents in the input rankings, we must decide which documents should be kept in the consensus ranking.",
                "Two working hypotheses are therefore considered: H2 k : We only consider documents which are present in at least k input rankings (k > 1).",
                "H2 all: We consider all the documents which are ranked in at least one input ranking.",
                "Hereafter, we call documents which will be retained in the consensus ranking, candidate documents, and documents that will be excluded from the consensus ranking, excluded documents.",
                "We also call a candidate document which is missing in one or more rankings, a missing document. 3.",
                "Some candidate documents are missing documents in some input rankings.",
                "Main reasons for a missing document are that it was not indexed or it was indexed but deemed irrelevant ; usually this information is not available.",
                "We consider the following two working hypotheses: H3 yes: Each missing document in each j is assigned a position.",
                "H3 no: No assumption is made, that is each missing document is considered neither better nor worse than any other document. 4.",
                "When assumption H2 k holds, each input ranking may contain documents which will not be considered in the consensus ranking.",
                "Regarding the positions of the candidate documents, we can consider the following working hypotheses: H4 init: The initial positions of candidate documents are kept in each input ranking.",
                "H4 new: Candidate documents receive new positions in each input ranking, after discarding excluded ones.",
                "In the IR context, rank aggregation methods need to decide more or less explicitly which assumptions to retain w.r.t. the above-mentioned difficulties. 4.",
                "<br>outranking approach</br> FOR RANK AGGREGATION 4.1 Presentation Positional methods consider implicitly that the positions of the documents in the input rankings are scores giving thus a cardinal meaning to an ordinal information.",
                "This constitutes a strong assumption that is questionable, especially when the input rankings have different lengths.",
                "Moreover, for positional methods, assumptions H3 and H4 , which are often arbitrary, have a strong impact on the results.",
                "For instance, let us consider an input ranking of 500 documents out of 1000 candidate documents.",
                "Whether we assign to each of the missing documents the position 1, 501, 750 or 1000 -corresponding to variations of H3 yes- will give rise to very contrasted results, especially regarding the top of the consensus ranking.",
                "Majoritarian methods do not suffer from the above-mentioned drawbacks of the positional methods since they build consensus rankings exploiting only ordinal information contained in the input rankings.",
                "Nevertheless, they suppose that such rankings are complete orders, ignoring that they may hide ties.",
                "Therefore, majoritarian methods base consensus rankings on illusory discriminant information rather than less discriminant but more robust information.",
                "Trying to overcome the limits of current rank aggregation methods, we found that outranking approaches, which were initially used for multiple criteria aggregation problems [26], can also be used for the rank aggregation purpose, where each ranking plays the role of a criterion.",
                "Therefore, in order to decide whether a document di should be ranked better than di in the consensus ranking σ, the two following conditions should be met: • a concordance condition which ensures that a majority of the input rankings are concordant with diσdi (majority principle). • a discordance condition which ensures that none of the discordant input rankings strongly refutes dσd (respect of minorities principle).",
                "Formally, the concordance coalition with diσdi is Csp (diσdi ) = { j∈ PR : rj i ≤ rj i − sp} where sp is a preference threshold which is the variation of document positions -whether it is absolute or relative to the ranking length- which draws the boundaries between an indifference and a preference situation between documents.",
                "The discordance coalition with diσdi is Dsv (diσdi ) = { j∈ PR : rj i ≥ rj i + sv} where sv is a veto threshold which is the variation of document positions -whether it is absolute or relative to the ranking length- which draws the boundaries between a weak and a strong opposition to diσdi .",
                "Depending on the exact definition of the preceding concordance and discordance coalitions leading to the definition of some decision rules, several outranking relations can be defined.",
                "They can be more or less demanding depending on i) the values of the thresholds sp and sv, ii) the importance or minimal size cmin required for the concordance coalition, and iii) the importance or maximum size dmax of the discordance coalition.",
                "A generic outranking relation can thus be defined as follows: diS(sp,sv,cmin,dmax)di ⇔ |Csp (diσdi )| ≥ cmin AND |Dsv (diσdi )| ≤ dmax This expression defines a family of nested outranking relations since S(sp,sv,cmin,dmax) ⊆ S(sp,sv,cmin,dmax) when cmin ≥ cmin and/or dmax ≤ dmax and/or sp ≥ sp and/or sv ≤ sv.",
                "This expression also generalizes the majority rule which corresponds to the particular relation S(0,∞, n 2 ,n).",
                "It also satisfies important properties of rank aggregation methods, called neutrality, Pareto-optimality, Condorcet property and Extended Condorcet property, in the social choice literature [29].",
                "Outranking relations are not necessarily transitive and do not necessarily correspond to rankings since directed cycles may exist.",
                "Therefore, we need specific procedures in order to derive a consensus ranking.",
                "We propose the following procedure which finds its roots in [27].",
                "It consists in partitioning the set of documents into r ranked classes.",
                "Each class Ch contains documents with the same relevance and results from the application of all relations (if possible) to the set of documents remaining after previous classes are computed.",
                "Documents within the same equivalence class are ranked arbitrarily.",
                "Formally, let • R be the set of candidate documents for a query, • S1 , S2 , . . . be a family of nested outranking relations, • Fk(di, E) = |{di ∈ E : diSk di }| be the number of documents in E(E ⊆ R) that could be considered worse than di according to relation Sk , • fk(di, E) = |{di ∈ E : di Sk di}| be the number of documents in E that could be considered better than di according to Sk , • sk(di, E) = Fk(di, E) − fk(di, E) be the qualification of di in E according to Sk .",
                "Each class Ch results from a distillation process.",
                "It corresponds to the last distillate of a series of sets E0 ⊇ E1 ⊇ . . . where E0 = R \\ (C1 ∪ . . . ∪ Ch−1) and Ek is a reduced subset of Ek−1 resulting from the application of the following procedure: 1. compute for each di ∈ Ek−1 its qualification according to Sk , i.e. sk(di, Ek−1), 2. define smax = maxdi∈Ek−1 {sk(di, Ek−1)}, then 3.",
                "Ek = {di ∈ Ek−1 : sk(di, Ek−1) = smax} When one outranking relation is used, the distillation process stops after the first application of the previous procedure, i.e., Ch corresponds to distillate E1.",
                "When different outranking relations are used, the distillation process stops when all the pre-defined outranking relations have been used or when |Ek| = 1. 4.2 Illustrative Example This section illustrates the concepts and procedures of section 4.1.",
                "Let us consider a set of candidate documents R = {d1, d2, d3, d4, d5}.",
                "The following table gives a profile PR of different rankings of the documents of R: PR = ( 1 , 2, 3, 4).",
                "Table 1: Rankings of documents rj i 1 2 3 4 d1 1 3 1 5 d2 2 1 3 3 d3 3 2 2 1 d4 4 4 5 2 d5 5 5 4 4 Let us suppose that the preference and veto thresholds are set to values 1 and 4 respectively, and that the concordance and discordance thresholds are set to values 2 and 1 respectively.",
                "The following tables give the concordance, discordance and outranking matrices.",
                "Each entry csp (di, di ) (dsv (di, di )) in the concordance (discordance) matrix gives the number of rankings that are concordant (discordant) with diσdi , i.e. csp (di, di ) = |Csp (diσdi )| and dsv (di, di ) = |Dsv (diσdi )|.",
                "Table 2: Computation of the outranking relation d1 d2 d3 d4 d5 d1 - 2 2 3 3 d2 2 - 2 3 4 d3 2 2 - 4 4 d4 1 1 0 - 3 d5 1 0 0 1Concordance Matrix d1 d2 d3 d4 d5 d1 - 0 1 0 0 d2 0 - 0 0 0 d3 0 0 - 0 0 d4 1 0 0 - 0 d5 1 1 0 0Discordance Matrix d1 d2 d3 d4 d5 d1 - 1 1 1 1 d2 1 - 1 1 1 d3 1 1 - 1 1 d4 0 0 0 - 1 d5 0 0 0 0Outranking Matrix (S1) For instance, the concordance coalition for the assertion d1σd4 is C1(d1σd4) = { 1, 2, 3} and the discordance coalition for the same assertion is D4(d1σd4) = ∅.",
                "Therefore, c1(d1, d4) = 3, d4(d1, d4) = 0 and d1S1 d4 holds.",
                "Notice that Fk(di, R) (fk(di, R)) is given by summing the values of the ith row (column) of the outranking matrix.",
                "The consensus ranking is obtained as follows: to get the first class C1, we compute the qualifications of all the documents of E0 = R with respect to S1 .",
                "They are respectively 2, 2, 2, -2 and -4.",
                "Therefore smax equals 2 and C1 = E1 = {d1, d2, d3}.",
                "Observe that, if we had used a second outranking relation S2(⊇ S1), these three documents could have been possibly discriminated.",
                "At this stage, we remove documents of C1 from the outranking matrix and compute the next class C2: we compute the new qualifications of the documents of E0 = R \\ C1 = {d4, d5}.",
                "They are respectively 1 and -1.",
                "So C3 = E1 = {d4}.",
                "The last document d5 is the only document of the last class C3.",
                "Thus, the consensus ranking is {d1, d2, d3} → {d4} → {d5}. 5.",
                "EXPERIMENTS AND RESULTS 5.1 Test Setting To facilitate empirical investigation of the proposed methodology, we developed a prototype metasearch engine that implements a version of our <br>outranking approach</br> for rank aggregation.",
                "In this paper, we apply our approach to the Topic Distillation (TD) task of TREC-2004 Web track [10].",
                "In this task, there are 75 topics where only a short description of each is given.",
                "For each query, we retained the rankings of the 10 best runs of the TD task which are provided by TREC-2004 participating teams.",
                "The performances of these runs are reported in table 3.",
                "Table 3: Performances of the 10 best runs of the TD task of TREC-2004 Run Id MAP P@10 S@1 S@5 S@10 uogWebCAU150 17.9% 24.9% 50.7% 77.3% 89.3% MSRAmixed1 17.8% 25.1% 38.7% 72.0% 88.0% MSRC04C12 16.5% 23.1% 38.7% 74.7% 80.0% humW04rdpl 16.3% 23.1% 37.3% 78.7% 90.7% THUIRmix042 14.7% 20.5% 21.3% 58.7% 74.7% UAmsT04MWScb 14.6% 20.9% 36.0% 66.7% 76.0% ICT04CIIS1AT 14.1% 20.8% 33.3% 64.0% 78.7% SJTUINCMIX5 12.9% 18.9% 29.3% 57.3% 72.0% MU04web1 11.5% 19.9% 33.3% 64.0% 76.0% MeijiHILw3 11.5% 15.3% 30.7% 54.7% 64.0% Average 14.7% 21.2% 34.9% 66.8% 78.94% For each query, each run provides a ranking of about 1000 documents.",
                "The number of documents retrieved by all these runs ranges from 543 to 5769.",
                "Their average (median) number is 3340 (3386).",
                "It is worth noting that we found similar distributions of the documents among the rankings as in [11].",
                "For evaluation, we used the trec eval standard tool which is used by the TREC community to calculate the standard measures of system effectiveness which are Mean Average Precision (MAP) and Success@n (S@n) for n=1, 5 and 10.",
                "Our approach effectiveness is compared against some high performing official results from TREC-2004 as well as against some standard rank aggregation algorithms.",
                "In the experiments, significance testing is mainly based on the t-student statistic which is computed on the basis of the MAP values of the compared runs.",
                "In the tables of the following section, statistically significant differences are marked with an asterisk.",
                "Values between brackets of the first column of each table, indicate the parameter value of the corresponding run. 5.2 Results We carried out several series of runs in order to i) study performance variations of the <br>outranking approach</br> when tuning the parameters and working assumptions, ii) compare performances of the <br>outranking approach</br> vs standard rank aggregation strategies , and iii) check whether rank aggregation performs better than the best input rankings.",
                "We set our basic run mcm with the following parameters.",
                "We considered that each input ranking is a complete order (sp = 0) and that an input ranking strongly refutes diσdi when the difference of both document positions is large enough (sv = 75%).",
                "Preference and veto thresholds are computed proportionally to the number of documents retained in each input ranking.",
                "They consequently may vary from one ranking to another.",
                "In addition, to accept the assertion diσdi , we supposed that the majority of the rankings must be concordant (cmin = 50%) and that every input ranking can impose its veto (dmax = 0).",
                "Concordance and discordance thresholds are computed for each tuple (di, di ) as the percentage of the input rankings of PRi ∩PRi .",
                "Thus, our choice of parameters leads to the definition of the outranking relation S(0,75%,50%,0).",
                "To test the run mcm, we had chosen the following assumptions.",
                "We retained the top 100 best documents from each input ranking (H1 100), only considered documents which are present in at least half of the input rankings (H2 5 ) and assumed H3 no and H4 new.",
                "In these conditions, the number of successful documents was about 100 on average, and the computation time per query was less than one second.",
                "Obviously, modifying the working assumptions should have deeper impact on the performances than tuning our model parameters.",
                "This was validated by preliminary experiments.",
                "Thus, we hereafter begin by studying performance variation when different sets of assumptions are considered.",
                "Afterwards, we study the impact of tuning parameters.",
                "Finally, we compare our model performances w.r.t. the input rankings as well as some standard data fusion algorithms. 5.2.1 Impact of the Working Assumptions Table 4 summarizes the performance variation of the <br>outranking approach</br> under different working hypotheses.",
                "In Table 4: Impact of the working assumptions Run Id MAP S@1 S@5 S@10 mcm 18.47% 41.33% 81.33% 86.67% mcm22 (H3 yes) 17.72% (-4.06%) 34.67% 81.33% 86.67% mcm23 (H4 init) 18.26% (-1.14%) 41.33% 81.33% 86.67% mcm24 (H1 all) 20.67% (+11.91%*) 38.66% 80.00% 86.66% mcm25 (H2 all) 21.68% (+17.38%*) 40.00% 78.66% 89.33% this table, we first show that run mcm22, in which missing documents are all put in the same last position of each input ranking, leads to performance drop w.r.t. run mcm.",
                "Moreover, S@1 moves from 41.33% to 34.67% (-16.11%).",
                "This shows that several relevant documents which were initially put at the first position of the consensus ranking in mcm, lose this first position but remain ranked in the top 5 documents since S@5 did not change.",
                "We also conclude that documents which have rather good positions in some input rankings are more likely to be relevant, even though they are missing in some other rankings.",
                "Consequently, when they are missing in some rankings, assigning worse ranks to these documents is harmful for performance.",
                "Also, from Table 4, we found that the performances of runs mcm and mcm23 are similar.",
                "Therefore, the <br>outranking approach</br> is not sensitive to keeping the initial positions of candidate documents or recomputing them by discarding excluded ones.",
                "From the same Table 4, performance of the <br>outranking approach</br> increases significantly for runs mcm24 and mcm25.",
                "Therefore, whether we consider all the documents which are present in half of the rankings (mcm24) or we consider all the documents which are ranked in the first 100 positions in one or more rankings (mcm25), increases performances.",
                "This result was predictable since in both cases we have more detailed information on the relative importance of documents.",
                "Tables 5 and 6 confirm this evidence.",
                "Table 5, where values between brackets of the first column give the number of documents which are retained from each input ranking, shows that selecting more documents from each input ranking leads to performance increase.",
                "It is worth mentioning that selecting more than 600 documents from each input ranking does not improve performance.",
                "Table 5: Impact of the number of retained documents Run Id MAP S@1 S@5 S@10 mcm (100) 18.47% 41.33% 81.33% 86.67% mcm24-1 (200) 19.32% (+4.60%) 42.67% 78.67% 88.00% mcm24-2 (400) 19.88% (+7.63%*) 37.33% 80.00% 88.00% mcm24-3 (600) 20.80% (+12.62%*) 40.00% 80.00% 88.00% mcm24-4 (800) 20.66% (+11.86%*) 40.00% 78.67% 86.67% mcm24 (1000) 20.67% (+11.91%*) 38.66% 80.00% 86.66% Table 6 reports runs corresponding to variations of H2 k .",
                "Values between brackets are rank hits.",
                "For instance, in the run mcm32, only documents which are present in 3 or more input rankings, were considered successful.",
                "This table shows that performance is significantly better when rare documents are considered, whereas it decreases significantly when these documents are discarded.",
                "Therefore, we conclude that many of the relevant documents are retrieved by a rather small set of IR models.",
                "Table 6: Performance considering different rank hits Run Id MAP S@1 S@5 S@10 mcm25 (1) 21.68% (+17.38%*) 40.00% 78.67% 89.33% mcm32 (3) 18.98% (+2.76%) 38.67% 80.00% 85.33% mcm (5) 18.47% 41.33% 81.33% 86.67% mcm33 (7) 15.83% (-14.29%*) 37.33% 78.67% 85.33% mcm34 (9) 10.96% (-40.66%*) 36.11% 66.67% 70.83% mcm35 (10) 7.42% (-59.83%*) 39.22% 62.75% 64.70% For both runs mcm24 and mcm25, the number of successful documents was about 1000 and therefore, the computation time per query increased and became around 5 seconds. 5.2.2 Impact of the Variation of the Parameters Table 7 shows performance variation of the <br>outranking approach</br> when different preference thresholds are considered.",
                "We found performance improvement up to threshold values of about 5%, then there is a decrease in the performance which becomes significant for threshold values greater than 10%.",
                "Moreover, S@1 improves from 41.33% to 46.67% when preference threshold changes from 0 to 5%.",
                "We can thus conclude that the input rankings are semi orders rather than complete orders.",
                "Table 8 shows the evolution of the performance measures w.r.t. the concordance threshold.",
                "We can conclude that in order to put document di before di in the consensus ranking, Table 7: Impact of the variation of the preference threshold from 0 to 12.5% Run Id MAP S@1 S@5 S@10 mcm (0%) 18.47% 41.33% 81.33% 86.67% mcm1 (1%) 18.57% (+0.54%) 41.33% 81.33% 86.67% mcm2 (2.5%) 18.63% (+0.87%) 42.67% 78.67% 86.67% mcm3 (5%) 18.69% (+1.19%) 46.67% 81.33% 86.67% mcm4 (7.5%) 18.24% (-1.25%) 46.67% 81.33% 86.67% mcm5 (10%) 17.93% (-2.92%) 40.00% 82.67% 86.67% mcm5b (12.5%) 17.51% (-5.20%*) 41.33% 80.00% 86.67% at least half of the input rankings of PRi ∩ PRi should be concordant.",
                "Performance drops significantly for very low and very high values of the concordance threshold.",
                "In fact, for such values, the concordance condition is either fulfilled rather always by too many document pairs or not fulfilled at all, respectively.",
                "Therefore, the outranking relation becomes either too weak or too strong respectively.",
                "Table 8: Impact of the variation of cmin Run Id MAP S@1 S@5 S@10 mcm11 (20%) 17.63% (-4.55%*) 41.33% 76.00% 85.33% mcm12 (40%) 18.37% (-0.54%) 42.67% 76.00% 86.67% mcm (50%) 18.47% 41.33% 81.33% 86.67% mcm13 (60%) 18.42% (-0.27%) 40.00% 78.67% 86.67% mcm14 (80%) 17.43% (-5.63%*) 40.00% 78.67% 86.67% mcm15 (100%) 16.12% (-12.72%*) 41.33% 70.67% 85.33% In the experiments, varying the veto threshold as well as the discordance threshold within reasonable intervals does not have significant impact on performance measures.",
                "In fact, runs with different veto thresholds (sv ∈ [50%; 100%]) had similar performances even though there is a slight advantage for runs with high threshold values which means that it is better not to allow the input rankings to put their veto easily.",
                "Also, tuning the discordance threshold was carried out for values 50% and 75% of the veto threshold.",
                "For these runs we did not get any noticeable performance variation, although for low discordance thresholds (dmax < 20%), performance slightly decreased. 5.2.3 Impact of the Variation of the Number of Input Rankings To study performance evolution when different sets of input rankings are considered, we carried three more runs where 2, 4, and 6 of the best performing sets of the input rankings are considered.",
                "Results reported in Table 9 are seemingly counter-intuitive and also do not support previous findings regarding rank aggregation research [3].",
                "Nevertheless, this result shows that low performing rankings bring more noise than information to the establishment of the consensus ranking.",
                "Therefore, when they are considered, performance decreases.",
                "Table 9: Performance considering different best performing sets of input rankings Run Id MAP S@1 S@5 S@10 mcm (10) 18.47% 41.33% 81.33% 86.67% mcm27 (6) 18.60% (+0.70%) 41.33% 80.00% 85.33% mcm28 (4) 19.02% (+2.98%) 40.00% 86.67% 88.00% mcm29 (2) 18.33% (-0.76%) 44.00% 76.00% 88.00% 5.2.4 Comparison of the Performance of Different Rank Aggregation Methods In this set of runs, we compare the <br>outranking approach</br> with some standard rank aggregation methods which were proven to have acceptable performance in previous studies: we considered two positional methods which are the CombSUM and the CombMNZ strategies.",
                "We also examined the performance of one majoritarian method which is the Markov chain method (MC4).",
                "For the comparisons, we considered a specific outranking relation S∗ = S(5%,50%,50%,30%) which results in good overall performances when tuning all the parameters.",
                "The first row of Table 10 gives performances of the rank aggregation methods w.r.t. a basic assumption set A1 = (H1 100, H2 5 , H4 new): we only consider the 100 first documents from each ranking, then retain documents present in 5 or more rankings and update ranks of successful documents.",
                "For positional methods, we place missing documents at the queue of the ranking (H3 yes) whereas for our method as well as for MC4, we retained hypothesis H3 no.",
                "The three following rows of Table 10 report performances when changing one element from the basic assumption set: the second row corresponds to the assumption set A2 = (H1 1000, H2 5 , H4 new), i.e. changing the number of retained documents from 100 to 1000.",
                "The third row corresponds to the assumption set A3 = (H1 100, H2 all, H4 new), i.e. considering the documents present in at least one ranking.",
                "The fourth row corresponds to the assumption set A4 = (H1 100, H2 5 , H4 init), i.e. keeping the original ranks of successful documents.",
                "The fifth row of Table 10, labeled A5, gives performance when all the 225 queries of the Web track of TREC-2004 are considered.",
                "Obviously, performance level cannot be compared with previous lines since the additional queries are different from the TD queries and correspond to other tasks (Home Page and Named Page tasks [10]) of TREC-2004 Web track.",
                "This set of runs aims to show whether relative performance of the various methods is task-dependent.",
                "The last row of Table 10, labeled A6, reports performance of the various methods considering the TD task of TREC2002 instead of TREC-2004: we fused the results of input rankings of the 10 best official runs for each of the 50 TD queries [9] considering the set of assumptions A1 of the first row.",
                "This aims to show whether relative performance of the various methods changes from year to year.",
                "Values between brackets of Table 10 are variations of performance of each rank aggregation method w.r.t. performance of the <br>outranking approach</br>.",
                "Table 10: Performance (MAP) of different rank aggregation methods under 3 different test collections mcm combSUM combMNZ markov A1 18.79% 17.54% (-6.65%*) 17.08% (-9.10%*) 18.63% (-0.85%) A2 21.36% 19.18% (-10.21%*) 18.61% (-12.87%*) 21.33% (-0.14%) A3 21.92% 21.38% (-2.46%) 20.88% (-4.74%) 19.35% (-11.72%*) A4 18.64% 17.58% (-5.69%*) 17.18% (-7.83%*) 18.63% (-0.05%) A5 55.39% 52.16% (-5.83%*) 49.70% (-10.27%*) 53.30% (-3.77%) A6 16.95% 15.65% (-7.67%*) 14.57% (-14.04%*) 16.39% (-3.30%) From the analysis of table 10 the following can be established: • for all the runs, considering all the documents in each input ranking (A2) significantly improves performance (MAP increases by 11.62% on average).",
                "This is predictable since some initially unreported relevant documents would receive better positions in the consensus ranking. • for all the runs, considering documents even those present in only one input ranking (A3) significantly improves performance.",
                "For mcm, combSUM and combMNZ, performance improvement is more important (MAP increases by 20.27% on average) than for the markov run (MAP increases by 3.86%). • preserving the initial positions of documents (A4) or recomputing them (A1) does not have a noticeable influence on performance for both positional and majoritarian methods. • considering all the queries of the Web track of TREC2004 (A5) as well as the TD queries of the Web track of TREC-2002 (A6) does not alter the relative performance of the different data fusion methods. • considering the TD queries of the Web track of TREC2002, performances of all the data fusion methods are lower than that of the best performing input ranking for which the MAP value equals 18.58%.",
                "This is because most of the fused input rankings have very low performances compared to the best one, which brings more noise to the consensus ranking. • performances of the data fusion methods mcm and markov are significantly better than that of the best input ranking uogWebCAU150.",
                "This remains true for runs combSUM and combMNZ only under assumptions H1 all or H2 all.",
                "This shows that majoritarian methods are less sensitive to assumptions than positional methods. • <br>outranking approach</br> always performs significantly better than positional methods combSUM and combMNZ.",
                "It has also better performances than the Markov chain method, especially under assumption H2 all where difference of performances becomes significant. 6.",
                "CONCLUSIONS In this paper, we address the rank aggregation problem where different, but not disjoint, lists of documents are to be fused.",
                "We noticed that the input rankings can hide ties, so they should not be considered as complete orders.",
                "Only robust information should be used from each input ranking.",
                "Current rank aggregation methods, and especially positional methods (e.g. combSUM [15]), are not initially designed to work with such rankings.",
                "They should be adapted by considering specific working assumptions.",
                "We propose a new outranking method for rank aggregation which is well adapted to the IR context.",
                "Indeed, it ranks two documents w.r.t. the intensity of their positions difference in each input ranking and also considering the number of the input rankings that are concordant and discordant in favor of a specific document.",
                "There is also no need to make specific assumptions on the positions of the missing documents.",
                "This is an important feature since the absence of a document from a ranking should not be necessarily interpreted negatively.",
                "Experimental results show that the outranking method significantly out-performs popular classical positional data fusion methods like combSUM and combMNZ strategies.",
                "It also out-performs a good performing majoritarian methods which is the Markov chain method.",
                "These results are tested against different test collections and queries.",
                "From the experiments, we can also conclude that in order to improve the performances, we should fuse result lists of well performing IR models, and that majoritarian data fusion methods perform better than positional methods.",
                "The proposed method can have a real impact on Web metasearch performances since only ranks are available from most primary search engines, whereas most of the current approaches need scores to merge result lists into one single list.",
                "Further work involves investigating whether the <br>outranking approach</br> performs well in various other contexts, e.g. using the document scores or some combination of document ranks and scores.",
                "Acknowledgments The authors would like to thank Jacques Savoy for his valuable comments on a preliminary version of this paper. 7.",
                "REFERENCES [1] A. Aronson, D. Demner-Fushman, S. Humphrey, J. Lin, H. Liu, P. Ruch, M. Ruiz, L. Smith, L. Tanabe, and W. Wilbur.",
                "Fusion of knowledge-intensive and statistical approaches for retrieving and annotating textual genomics documents.",
                "In Proceedings TREC2005.",
                "NIST Publication, 2005. [2] R. A. Baeza-Yates and B.",
                "A. Ribeiro-Neto.",
                "Modern Information Retrieval.",
                "ACM Press , 1999. [3] B. T. Bartell, G. W. Cottrell, and R. K. Belew.",
                "Automatic combination of multiple ranked retrieval systems.",
                "In Proceedings ACM-SIGIR94, pages 173-181.",
                "Springer-Verlag, 1994. [4] N. J. Belkin, P. Kantor, E. A.",
                "Fox, and J.",
                "A. Shaw.",
                "Combining evidence of multiple query representations for information retrieval.",
                "IPM, 31(3):431-448, 1995. [5] J. Borda.",
                "M´emoire sur les ´elections au scrutin.",
                "Histoire de lAcad´emie des Sciences, 1781. [6] J. P. Callan, Z. Lu, and W. B. Croft.",
                "Searching distributed collections with inference networks.",
                "In Proceedings ACM-SIGIR95, pages 21-28, 1995. [7] M. Condorcet.",
                "Essai sur lapplication de lanalyse `a la probabilit´e des d´ecisions rendues `a la pluralit´e des voix.",
                "Imprimerie Royale, Paris, 1785. [8] W. D. Cook and M. Kress.",
                "Ordinal ranking with intensity of preference.",
                "Management Science, 31(1):26-32, 1985. [9] N. Craswell and D. Hawking.",
                "Overview of the TREC-2002 Web Track.",
                "In Proceedings TREC2002.",
                "NIST Publication, 2002. [10] N. Craswell and D. Hawking.",
                "Overview of the TREC-2004 Web Track.",
                "In Proceedings of TREC2004.",
                "NIST Publication, 2004. [11] C. Dwork, S. R. Kumar, M. Naor, and D. Sivakumar.",
                "Rank aggregation methods for the Web.",
                "In Proceedings WWW2001, pages 613-622, 2001. [12] R. Fagin.",
                "Combining fuzzy information from multiple systems.",
                "JCSS, 58(1):83-99, 1999. [13] R. Fagin, R. Kumar, M. Mahdian, D. Sivakumar, and E. Vee.",
                "Comparing and aggregating rankings with ties.",
                "In PODS, pages 47-58, 2004. [14] R. Fagin, R. Kumar, and D. Sivakumar.",
                "Comparing top k lists.",
                "SIAM J. on Discrete Mathematics, 17(1):134-160, 2003. [15] E. A.",
                "Fox and J.",
                "A. Shaw.",
                "Combination of multiple searches.",
                "In Proceedings of TREC3.",
                "NIST Publication, 1994. [16] J. Katzer, M. McGill, J. Tessier, W. Frakes, and P. DasGupta.",
                "A study of the overlap among document representations.",
                "Information Technology: Research and Development, 1(4):261-274, 1982. [17] L. S. Larkey, M. E. Connell, and J. Callan.",
                "Collection selection and results merging with topically organized U.S. patents and TREC data.",
                "In Proceedings ACM-CIKM2000, pages 282-289.",
                "ACM Press, 2000. [18] A.",
                "Le Calv´e and J. Savoy.",
                "Database merging strategy based on logistic regression.",
                "IPM, 36(3):341-359, 2000. [19] J. H. Lee.",
                "Analyses of multiple evidence combination.",
                "In Proceedings ACM-SIGIR97, pages 267-276, 1997. [20] D. Lillis, F. Toolan, R. Collier, and J. Dunnion.",
                "Probfuse: a probabilistic approach to data fusion.",
                "In Proceedings ACM-SIGIR2006, pages 139-146.",
                "ACM Press, 2006. [21] J. I. Marden.",
                "Analyzing and Modeling Rank Data.",
                "Number 64 in Monographs on Statistics and Applied Probability.",
                "Chapman & Hall, 1995. [22] M. Montague and J.",
                "A. Aslam.",
                "Metasearch consistency.",
                "In Proceedings ACM-SIGIR2001, pages 386-387.",
                "ACM Press, 2001. [23] D. M. Pennock and E. Horvitz.",
                "Analysis of the axiomatic foundations of collaborative filtering.",
                "In Workshop on AI for Electronic Commerce at the 16th National Conference on Artificial Intelligence, 1999. [24] M. E. Renda and U. Straccia.",
                "Web metasearch: rank vs. score based rank aggregation methods.",
                "In Proceedings ACM-SAC2003, pages 841-846.",
                "ACM Press, 2003. [25] W. H. Riker.",
                "Liberalism against populism.",
                "Waveland Press, 1982. [26] B. Roy.",
                "The <br>outranking approach</br> and the foundations of ELECTRE methods.",
                "Theory and Decision, 31:49-73, 1991. [27] B. Roy and J. Hugonnard.",
                "Ranking of suburban line extension projects on the Paris metro system by a multicriteria method.",
                "Transportation Research, 16A(4):301-312, 1982. [28] L. Si and J. Callan.",
                "Using sampled data and regression to merge search engine results.",
                "In Proceedings ACM-SIGIR2002, pages 19-26.",
                "ACM Press, 2002. [29] M. Truchon.",
                "An extension of the Condorcet criterion and Kemeny orders.",
                "Cahier 9813, Centre de Recherche en Economie et Finance Appliqu´ees, Oct. 1998. [30] H. Turtle and W. B. Croft.",
                "Inference networks for document retrieval.",
                "In Proceedings of ACM-SIGIR90, pages 1-24.",
                "ACM Press, 1990. [31] C. C. Vogt and G. W. Cottrell.",
                "Fusion via a linear combination of scores.",
                "Information Retrieval, 1(3):151-173, 1999."
            ],
            "original_annotated_samples": [],
            "translated_annotated_samples": [
                "An \"outranking approach\" for Rank Aggregation in Information Retrieval Mohamed Farah Lamsade, Paris Dauphine University Place du Mal de Lattre de Tassigny 75775 Paris Cedex 16, France farah@lamsade.dauphine.fr Daniel Vanderpooten Lamsade, Paris Dauphine University Place du Mal de LattreDe Tassigny 75775 Paris Cedex 16, Francia vdp@lamsade.dauphine.fr Resumen La investigación en recuperación de información generalmente muestra una mejora del rendimiento cuando muchas fuentes de evidencia se combinan para producir una clasificación de documentos (por ejemplo, textos, imágenes, sonidos, etc.).",
                "\"Enfoque de extaniación\" para la agregación de rango 4.1 Métodos posicionales de presentación consideran implícitamente que las posiciones de los documentos en las clasificaciones de entrada son puntajes que otorgan un significado cardinal a una información ordinal.",
                "Experimentos y resultados 5.1 Configuración de prueba Para facilitar la investigación empírica de la metodología propuesta, desarrollamos un prototipo de motor MetaSearch que implementa una versión de nuestro \"enfoque superalorizante\" para la agregación de rango.",
                "Los valores entre los soportes de la primera columna de cada tabla, indiquen el valor del parámetro de la ejecución correspondiente.5.2 Resultados Llevamos a cabo varias series de ejecuciones para i) estudiar variaciones de rendimiento del \"enfoque de extaniación\" al ajustar los parámetros y los supuestos de trabajo, ii) comparar las actuaciones del \"enfoque de extaniación\" frente a las estrategias de agregación de rango estándar, y iii)Compruebe si la agregación de rango funciona mejor que las mejores clasificaciones de entrada.",
                "Finalmente, comparamos nuestras actuaciones modelo W.R.T.Las clasificaciones de entrada, así como algunos algoritmos estándar de fusión de datos.5.2.1 Impacto de los supuestos de trabajo La Tabla 4 resume la variación de rendimiento del \"enfoque de supervisión\" bajo diferentes hipótesis de trabajo.",
                "Por lo tanto, el \"enfoque de supervisión\" no es sensible a mantener las posiciones iniciales de los documentos candidatos o recomputarlos descartando las excluidas.",
                "De la misma Tabla 4, el rendimiento del \"enfoque de extaniación\" aumenta significativamente para las ejecuciones MCM24 y MCM25.",
                "Tabla 6: Rendimiento considerando los golpes de rango de diferentes Run Map S@1 S@5 S@10 MCM25 (1) 21.68% (+17.38%*) 40.00% 78.67% 89.33% MCM32 (3) 18.98% (+2.76%) 38.67)% 80.00% 85.33% mcm (5) 18.47% 41.33% 81.33% 86.67% MCM33 (7) 15.83% (-14.29%*) 37.33% 78.67% 85.33% MCM34 (9) 10.96% (-40.66%*) 36.11% 66.6777777777777% 70.83% MCM35 (10) 7.42% (-59.83%*) 39.22% 62.75% 64.70% para ambas ejecuciones MCM24 y MCM25, el número de documentos exitosos fue de aproximadamente 1000 y, por lo tanto, el tiempo de cálculo perseguía aumentó y se convirtió en alrededor de 5 segundos.5.2.2 Impacto de la variación de los parámetros La Tabla 7 muestra la variación de rendimiento del \"enfoque de extaniación\" cuando se consideran diferentes umbrales de preferencia.",
                "Tabla 9: Rendimiento considerando diferentes conjuntos de mejor rendimiento de clasificaciones de entrada Run Map de identificación S@1 S@5 S@10 McM (10) 18.47% 41.33% 81.33% 86.67% MCM27 (6) 18.60% (+0.70%) 41.33% 80.00% 85.33% MCM28 (4) 19.02% (+2.98%) 40.00% 86.67% 88.00% MCM29 (2) 18.33% (-0.76%) 44.00% 76.00% 88.00% 5.2.4 Comparación del rendimiento de diferentes métodos de agregación de rango enEste conjunto de ejecuciones, comparamos el \"enfoque de extaniación\" con algunos métodos de agregación de rango estándar que se demostró que tenían un rendimiento aceptable en estudios anteriores: consideramos dos métodos posicionales que son las estrategias Cocsum y Combmnz.",
                "Los valores entre los soportes de la Tabla 10 son variaciones de rendimiento de cada método de agregación de rango W.R.T.rendimiento del \"enfoque de supervisión\".",
                "Esto muestra que los métodos mayores son menos sensibles a los supuestos que los métodos posicionales.• El \"enfoque de extaniación\" siempre funciona significativamente mejor que los métodos posicionales Combsum y Combmnz.",
                "El trabajo adicional implica investigar si el \"enfoque de supervisión\" funciona bien en varios otros contextos, p.Uso de los puntajes de documentos o alguna combinación de rangos y puntajes de documentos.",
                "El \"enfoque de extaniación\" y los cimientos de los métodos electas."
            ],
            "translated_text": "",
            "candidates": [
                "enfoque de atanancia",
                "outranking approach",
                "enfoque de atanancia",
                "Enfoque de extaniación",
                "enfoque de atanancia",
                "enfoque superalorizante",
                "enfoque de atanancia",
                "enfoque de extaniación",
                "enfoque de extaniación",
                "enfoque de atanancia",
                "enfoque de supervisión",
                "enfoque de atanancia",
                "enfoque de supervisión",
                "enfoque de atanancia",
                "enfoque de extaniación",
                "enfoque de atanancia",
                "enfoque de extaniación",
                "enfoque de atanancia",
                "enfoque de extaniación",
                "enfoque de atanancia",
                "enfoque de supervisión",
                "enfoque de atanancia",
                "enfoque de extaniación",
                "enfoque de atanancia",
                "enfoque de supervisión",
                "enfoque de atanancia",
                "enfoque de extaniación"
            ],
            "error": []
        },
        "metasearch engine": {
            "translated_key": "metabuscador",
            "is_in_text": true,
            "original_annotated_sentences": [
                "An Outranking Approach for Rank Aggregation in Information Retrieval Mohamed Farah Lamsade, Paris Dauphine University Place du Mal de Lattre de Tassigny 75775 Paris Cedex 16, France farah@lamsade.dauphine.fr Daniel Vanderpooten Lamsade, Paris Dauphine University Place du Mal de Lattre de Tassigny 75775 Paris Cedex 16, France vdp@lamsade.dauphine.fr ABSTRACT Research in Information Retrieval usually shows performance improvement when many sources of evidence are combined to produce a ranking of documents (e.g., texts, pictures, sounds, etc.).",
                "In this paper, we focus on the rank aggregation problem, also called data fusion problem, where rankings of documents, searched into the same collection and provided by multiple methods, are combined in order to produce a new ranking.",
                "In this context, we propose a rank aggregation method within a multiple criteria framework using aggregation mechanisms based on decision rules identifying positive and negative reasons for judging whether a document should get a better rank than another.",
                "We show that the proposed method deals well with the Information Retrieval distinctive features.",
                "Experimental results are reported showing that the suggested method performs better than the well-known CombSUM and CombMNZ operators.",
                "Categories and Subject Descriptors: H.3.3 [Information Systems]: Information Search and Retrieval - Retrieval models.",
                "General Terms: Algorithms, Measurement, Experimentation, Performance, Theory. 1.",
                "INTRODUCTION A wide range of current Information Retrieval (IR) approaches are based on various search models (Boolean, Vector Space, Probabilistic, Language, etc. [2]) in order to retrieve relevant documents in response to a user request.",
                "The result lists produced by these approaches depend on the exact definition of the relevance concept.",
                "Rank aggregation approaches, also called data fusion approaches, consist in combining these result lists in order to produce a new and hopefully better ranking.",
                "Such approaches give rise to metasearch engines in the Web context.",
                "We consider, in the following, cases where only ranks are available and no other additional information is provided such as the relevance scores.",
                "This corresponds indeed to the reality, where only ordinal information is available.",
                "Data fusion is also relevant in other contexts, such as when the user writes several queries of his/her information need (e.g., a boolean query and a natural language query) [4], or when many document surrogates are available [16].",
                "Several studies argued that rank aggregation has the potential of combining effectively all the various sources of evidence considered in various input methods.",
                "For instance, experiments carried out in [16], [30], [4] and [19] showed that documents which appear in the lists of the majority of the input methods are more likely to be relevant.",
                "Moreover, Lee [19] and Vogt and Cottrell [31] found that various retrieval approaches often return very different irrelevant documents, but many of the same relevant documents.",
                "Bartell et al. [3] also found that rank aggregation methods improve the performances w.r.t. those of the input methods, even when some of them have weak individual performances.",
                "These methods also tend to smooth out biases of the input methods according to Montague and Aslam [22].",
                "Data fusion has recently been proved to improve performances for both the ad-hoc retrieval and categorization tasks within the TREC genomics track in 2005 [1].",
                "The rank aggregation problem was addressed in various fields such as i) in social choice theory which studies voting algorithms which specify winners of elections or winners of competitions in tournaments [29], ii) in statistics when studying correlation between rankings, iii) in distributed databases when results from different databases must be combined [12], and iv) in collaborative filtering [23].",
                "Most current rank aggregation methods consider each input ranking as a permutation over the same set of items.",
                "They also give rigid interpretation to the exact ranking of the items.",
                "Both of these assumptions are rather not valid in the IR context, as will be shown in the following sections.",
                "The remaining of the paper is organized as follows.",
                "We first review current rank aggregation methods in Section 2.",
                "Then we outline the specificities of the data fusion problem in the IR context (Section 3).",
                "In Section 4, we present a new aggregation method which is proven to best fit the IR context.",
                "Experimental results are presented in Section 5 and conclusions are provided in a final section. 2.",
                "RELATED WORK As pointed out by Riker [25], we can distinguish two families of rank aggregation methods: positional methods which assign scores to items to be ranked according to the ranks they receive and majoritarian methods which are based on pairwise comparisons of items to be ranked.",
                "These two families of methods find their roots in the pioneering works of Borda [5] and Condorcet [7], respectively, in the social choice literature. 2.1 Preliminaries We first introduce some basic notations to present the rank aggregation methods in a uniform way.",
                "Let D = {d1, d2, . . . , dnd } be a set of nd documents.",
                "A list or a ranking j is an ordering defined on Dj ⊆ D (j = 1, . . . , n).",
                "Thus, di j di means di is ranked better than di in j.",
                "When Dj = D, j is said to be a full list.",
                "Otherwise, it is a partial list.",
                "If di belongs to Dj, rj i denotes the rank or position of di in j.",
                "We assume that the best answer (document) is assigned the position 1 and the worst one is assigned the position |Dj|.",
                "Let D be the set of all permutations on D or all subsets of D. A profile is a n-tuple of rankings PR = ( 1, 2, . . . , n).",
                "Restricting PR to the rankings containing document di defines PRi.",
                "We also call the number of rankings which contain document di the rank hits of di [19].",
                "The rank aggregation or data fusion problem consists of finding a ranking function or mechanism Ψ (also called a social welfare function in the social choice theory terminology) defined by: Ψ : n D → D PR = ( 1, 2, . . . , n) → σ = Ψ(PR) where σ is called a consensus ranking. 2.2 Positional Methods 2.2.1 Borda Count This method [5] first assigns a score n j=1 rj i to each document di.",
                "Documents are then ranked by increasing order of this score, breaking ties, if any, arbitrarily. 2.2.2 Linear Combination Methods This family of methods basically combine scores of documents.",
                "When used for the rank aggregation problem, ranks are assumed to be scores or performances to be combined using aggregation operators such as the weighted sum or some variation of it [3, 31, 17, 28].",
                "For instance, Callan et al. [6] used the inference networks model [30] to combine rankings.",
                "Fox and Shaw [15] proposed several combination strategies which are CombSUM, CombMIN, CombMAX, CombANZ and CombMNZ.",
                "The first three operators correspond to the sum, min and max operators, respectively.",
                "CombANZ and CombMNZ respectively divides and multiplies the CombSUM score by the rank hits.",
                "It is shown in [19] that the CombSUM and CombMNZ operators perform better than the others.",
                "Metasearch engines such as SavvySearch and MetaCrawler use the CombSUM strategy to fuse rankings. 2.2.3 Footrule Optimal Aggregation In this method, a consensus ranking minimizes the Spearman footrule distance from the input rankings [21].",
                "Formally, given two full lists j and j , this distance is given by F( j, j ) = nd i=1 |rj i − rj i |.",
                "It extends to several lists as follows.",
                "Given a profile PR and a consensus ranking σ, the Spearman footrule distance of σ to PR is given by F(σ, PR) = n j=1 F(σ, j).",
                "Cook and Kress [8] proposed a similar method which consists in optimizing the distance D( j, j ) = 1 2 nd i,i =1 |rj i,i − rj i,i |, where rj i,i = rj i −rj i .",
                "This formulation has the advantage that it considers the intensity of preferences. 2.2.4 Probabilistic Methods This kind of methods assume that the performance of the input methods on a number of training queries is indicative of their future performance.",
                "During the training process, probabilities of relevance are calculated.",
                "For subsequent queries, documents are ranked based on these probabilities.",
                "For instance, in [20], each input ranking j is divided into a number of segments, and the conditional probability of relevance (R) of each document di depending on the segment k it occurs in, is computed, i.e. prob(R|di, k, j).",
                "For subsequent queries, the score of each document di is given by n j=1 prob(R|di,k, j ) k .",
                "Le Calve and Savoy [18] suggest using a logistic regression approach for combining scores.",
                "Training data is needed to infer the model parameters. 2.3 Majoritarian Methods 2.3.1 Condorcet Procedure The original Condorcet rule [7] specifies that a winner of the election is any item that beats or ties with every other item in a pairwise contest.",
                "Formally, let C(diσdi ) = { j∈ PR : di j di } be the coalition of rankings that are concordant with establishing diσdi , i.e. with the proposition di should be ranked better than di in the final ranking σ. di beats or ties with di iff |C(diσdi )| ≥ |C(di σdi)|.",
                "The repetitive application of the Condorcet algorithm can produce a ranking of items in a natural way: select the Condorcet winner, remove it from the lists, and repeat the previous two steps until there are no more documents to rank.",
                "Since there is not always Condorcet winners, variations of the Condorcet procedure have been developed within the multiple criteria decision aid theory, with methods such as ELECTRE [26]. 2.3.2 Kemeny Optimal Aggregation As in section 2.2.3, a consensus ranking minimizes a geometric distance from the input rankings, where the Kendall tau distance is used instead of the Spearman footrule distance.",
                "Formally, given two full lists j and j , the Kendall tau distance is given by K( j, j ) = |{(di, di ) : i < i , rj i < rj i , rj i > rj i }|, i.e. the number of pairwise disagreements between the two lists.",
                "It is easy to show that the consensus ranking corresponds to the geometric median of the input rankings and that the Kemeny optimal aggregation problem corresponds to the minimum feedback edge set problem. 2.3.3 Markov Chain Methods Markov chains (MCs) have been used by Dwork et al. [11] as a natural method to obtain a consensus ranking where states correspond to the documents to be ranked and the transition probabilities vary depending on the interpretation of the transition event.",
                "In the same reference, the authors proposed four specific MCs and experimental testing had shown that the following MC is the best performing one (see also [24]): • MC4: move from the current state di to the next state di by first choosing a document di uniformly from D. If for the majority of the rankings, we have rj i ≤ rj i , then move to di , else stay in di.",
                "The consensus ranking corresponds to the stationary distribution of MC4. 3.",
                "SPECIFICITIES OF THE RANK AGGREGATION PROBLEM IN THE IR CONTEXT 3.1 Limited Significance of the Rankings The exact positions of documents in one input ranking have limited significance and should not be overemphasized.",
                "For instance, having three relevant documents in the first three positions, any perturbation of these three items will have the same value.",
                "Indeed, in the IR context, the complete order provided by an input method may hide ties.",
                "In this case, we call such rankings semi orders.",
                "This was outlined in [13] as the problem of aggregation with ties.",
                "It is therefore important to build the consensus ranking based on robust information: • Documents with near positions in j are more likely to have similar interest or relevance.",
                "Thus a slight perturbation of the initial ranking is meaningless. • Assuming that document di is better ranked than document di in a ranking j, di is more likely to be definitively more relevant than di in j when the number of intermediate positions between di and di increases. 3.2 Partial Lists In real world applications, such as metasearch engines, rankings provided by the input methods are often partial lists.",
                "This was outlined in [14] as the problem of having to merge top-k results from various input lists.",
                "For instance, in the experiments carried out by Dwork et al. [11], authors found that among the top 100 best documents of 7 input search engines, 67% of the documents were present in only one search engine, whereas less than two documents were present in all the search engines.",
                "Rank aggregation of partial lists raises four major difficulties which we state hereafter, proposing for each of them various working assumptions: 1.",
                "Partial lists can have various lengths, which can favour long lists.",
                "We thus consider the following two working hypotheses: H1 k : We only consider the top k best documents from each input ranking.",
                "H1 all: We consider all the documents from each input ranking. 2.",
                "Since there are different documents in the input rankings, we must decide which documents should be kept in the consensus ranking.",
                "Two working hypotheses are therefore considered: H2 k : We only consider documents which are present in at least k input rankings (k > 1).",
                "H2 all: We consider all the documents which are ranked in at least one input ranking.",
                "Hereafter, we call documents which will be retained in the consensus ranking, candidate documents, and documents that will be excluded from the consensus ranking, excluded documents.",
                "We also call a candidate document which is missing in one or more rankings, a missing document. 3.",
                "Some candidate documents are missing documents in some input rankings.",
                "Main reasons for a missing document are that it was not indexed or it was indexed but deemed irrelevant ; usually this information is not available.",
                "We consider the following two working hypotheses: H3 yes: Each missing document in each j is assigned a position.",
                "H3 no: No assumption is made, that is each missing document is considered neither better nor worse than any other document. 4.",
                "When assumption H2 k holds, each input ranking may contain documents which will not be considered in the consensus ranking.",
                "Regarding the positions of the candidate documents, we can consider the following working hypotheses: H4 init: The initial positions of candidate documents are kept in each input ranking.",
                "H4 new: Candidate documents receive new positions in each input ranking, after discarding excluded ones.",
                "In the IR context, rank aggregation methods need to decide more or less explicitly which assumptions to retain w.r.t. the above-mentioned difficulties. 4.",
                "OUTRANKING APPROACH FOR RANK AGGREGATION 4.1 Presentation Positional methods consider implicitly that the positions of the documents in the input rankings are scores giving thus a cardinal meaning to an ordinal information.",
                "This constitutes a strong assumption that is questionable, especially when the input rankings have different lengths.",
                "Moreover, for positional methods, assumptions H3 and H4 , which are often arbitrary, have a strong impact on the results.",
                "For instance, let us consider an input ranking of 500 documents out of 1000 candidate documents.",
                "Whether we assign to each of the missing documents the position 1, 501, 750 or 1000 -corresponding to variations of H3 yes- will give rise to very contrasted results, especially regarding the top of the consensus ranking.",
                "Majoritarian methods do not suffer from the above-mentioned drawbacks of the positional methods since they build consensus rankings exploiting only ordinal information contained in the input rankings.",
                "Nevertheless, they suppose that such rankings are complete orders, ignoring that they may hide ties.",
                "Therefore, majoritarian methods base consensus rankings on illusory discriminant information rather than less discriminant but more robust information.",
                "Trying to overcome the limits of current rank aggregation methods, we found that outranking approaches, which were initially used for multiple criteria aggregation problems [26], can also be used for the rank aggregation purpose, where each ranking plays the role of a criterion.",
                "Therefore, in order to decide whether a document di should be ranked better than di in the consensus ranking σ, the two following conditions should be met: • a concordance condition which ensures that a majority of the input rankings are concordant with diσdi (majority principle). • a discordance condition which ensures that none of the discordant input rankings strongly refutes dσd (respect of minorities principle).",
                "Formally, the concordance coalition with diσdi is Csp (diσdi ) = { j∈ PR : rj i ≤ rj i − sp} where sp is a preference threshold which is the variation of document positions -whether it is absolute or relative to the ranking length- which draws the boundaries between an indifference and a preference situation between documents.",
                "The discordance coalition with diσdi is Dsv (diσdi ) = { j∈ PR : rj i ≥ rj i + sv} where sv is a veto threshold which is the variation of document positions -whether it is absolute or relative to the ranking length- which draws the boundaries between a weak and a strong opposition to diσdi .",
                "Depending on the exact definition of the preceding concordance and discordance coalitions leading to the definition of some decision rules, several outranking relations can be defined.",
                "They can be more or less demanding depending on i) the values of the thresholds sp and sv, ii) the importance or minimal size cmin required for the concordance coalition, and iii) the importance or maximum size dmax of the discordance coalition.",
                "A generic outranking relation can thus be defined as follows: diS(sp,sv,cmin,dmax)di ⇔ |Csp (diσdi )| ≥ cmin AND |Dsv (diσdi )| ≤ dmax This expression defines a family of nested outranking relations since S(sp,sv,cmin,dmax) ⊆ S(sp,sv,cmin,dmax) when cmin ≥ cmin and/or dmax ≤ dmax and/or sp ≥ sp and/or sv ≤ sv.",
                "This expression also generalizes the majority rule which corresponds to the particular relation S(0,∞, n 2 ,n).",
                "It also satisfies important properties of rank aggregation methods, called neutrality, Pareto-optimality, Condorcet property and Extended Condorcet property, in the social choice literature [29].",
                "Outranking relations are not necessarily transitive and do not necessarily correspond to rankings since directed cycles may exist.",
                "Therefore, we need specific procedures in order to derive a consensus ranking.",
                "We propose the following procedure which finds its roots in [27].",
                "It consists in partitioning the set of documents into r ranked classes.",
                "Each class Ch contains documents with the same relevance and results from the application of all relations (if possible) to the set of documents remaining after previous classes are computed.",
                "Documents within the same equivalence class are ranked arbitrarily.",
                "Formally, let • R be the set of candidate documents for a query, • S1 , S2 , . . . be a family of nested outranking relations, • Fk(di, E) = |{di ∈ E : diSk di }| be the number of documents in E(E ⊆ R) that could be considered worse than di according to relation Sk , • fk(di, E) = |{di ∈ E : di Sk di}| be the number of documents in E that could be considered better than di according to Sk , • sk(di, E) = Fk(di, E) − fk(di, E) be the qualification of di in E according to Sk .",
                "Each class Ch results from a distillation process.",
                "It corresponds to the last distillate of a series of sets E0 ⊇ E1 ⊇ . . . where E0 = R \\ (C1 ∪ . . . ∪ Ch−1) and Ek is a reduced subset of Ek−1 resulting from the application of the following procedure: 1. compute for each di ∈ Ek−1 its qualification according to Sk , i.e. sk(di, Ek−1), 2. define smax = maxdi∈Ek−1 {sk(di, Ek−1)}, then 3.",
                "Ek = {di ∈ Ek−1 : sk(di, Ek−1) = smax} When one outranking relation is used, the distillation process stops after the first application of the previous procedure, i.e., Ch corresponds to distillate E1.",
                "When different outranking relations are used, the distillation process stops when all the pre-defined outranking relations have been used or when |Ek| = 1. 4.2 Illustrative Example This section illustrates the concepts and procedures of section 4.1.",
                "Let us consider a set of candidate documents R = {d1, d2, d3, d4, d5}.",
                "The following table gives a profile PR of different rankings of the documents of R: PR = ( 1 , 2, 3, 4).",
                "Table 1: Rankings of documents rj i 1 2 3 4 d1 1 3 1 5 d2 2 1 3 3 d3 3 2 2 1 d4 4 4 5 2 d5 5 5 4 4 Let us suppose that the preference and veto thresholds are set to values 1 and 4 respectively, and that the concordance and discordance thresholds are set to values 2 and 1 respectively.",
                "The following tables give the concordance, discordance and outranking matrices.",
                "Each entry csp (di, di ) (dsv (di, di )) in the concordance (discordance) matrix gives the number of rankings that are concordant (discordant) with diσdi , i.e. csp (di, di ) = |Csp (diσdi )| and dsv (di, di ) = |Dsv (diσdi )|.",
                "Table 2: Computation of the outranking relation d1 d2 d3 d4 d5 d1 - 2 2 3 3 d2 2 - 2 3 4 d3 2 2 - 4 4 d4 1 1 0 - 3 d5 1 0 0 1Concordance Matrix d1 d2 d3 d4 d5 d1 - 0 1 0 0 d2 0 - 0 0 0 d3 0 0 - 0 0 d4 1 0 0 - 0 d5 1 1 0 0Discordance Matrix d1 d2 d3 d4 d5 d1 - 1 1 1 1 d2 1 - 1 1 1 d3 1 1 - 1 1 d4 0 0 0 - 1 d5 0 0 0 0Outranking Matrix (S1) For instance, the concordance coalition for the assertion d1σd4 is C1(d1σd4) = { 1, 2, 3} and the discordance coalition for the same assertion is D4(d1σd4) = ∅.",
                "Therefore, c1(d1, d4) = 3, d4(d1, d4) = 0 and d1S1 d4 holds.",
                "Notice that Fk(di, R) (fk(di, R)) is given by summing the values of the ith row (column) of the outranking matrix.",
                "The consensus ranking is obtained as follows: to get the first class C1, we compute the qualifications of all the documents of E0 = R with respect to S1 .",
                "They are respectively 2, 2, 2, -2 and -4.",
                "Therefore smax equals 2 and C1 = E1 = {d1, d2, d3}.",
                "Observe that, if we had used a second outranking relation S2(⊇ S1), these three documents could have been possibly discriminated.",
                "At this stage, we remove documents of C1 from the outranking matrix and compute the next class C2: we compute the new qualifications of the documents of E0 = R \\ C1 = {d4, d5}.",
                "They are respectively 1 and -1.",
                "So C3 = E1 = {d4}.",
                "The last document d5 is the only document of the last class C3.",
                "Thus, the consensus ranking is {d1, d2, d3} → {d4} → {d5}. 5.",
                "EXPERIMENTS AND RESULTS 5.1 Test Setting To facilitate empirical investigation of the proposed methodology, we developed a prototype <br>metasearch engine</br> that implements a version of our outranking approach for rank aggregation.",
                "In this paper, we apply our approach to the Topic Distillation (TD) task of TREC-2004 Web track [10].",
                "In this task, there are 75 topics where only a short description of each is given.",
                "For each query, we retained the rankings of the 10 best runs of the TD task which are provided by TREC-2004 participating teams.",
                "The performances of these runs are reported in table 3.",
                "Table 3: Performances of the 10 best runs of the TD task of TREC-2004 Run Id MAP P@10 S@1 S@5 S@10 uogWebCAU150 17.9% 24.9% 50.7% 77.3% 89.3% MSRAmixed1 17.8% 25.1% 38.7% 72.0% 88.0% MSRC04C12 16.5% 23.1% 38.7% 74.7% 80.0% humW04rdpl 16.3% 23.1% 37.3% 78.7% 90.7% THUIRmix042 14.7% 20.5% 21.3% 58.7% 74.7% UAmsT04MWScb 14.6% 20.9% 36.0% 66.7% 76.0% ICT04CIIS1AT 14.1% 20.8% 33.3% 64.0% 78.7% SJTUINCMIX5 12.9% 18.9% 29.3% 57.3% 72.0% MU04web1 11.5% 19.9% 33.3% 64.0% 76.0% MeijiHILw3 11.5% 15.3% 30.7% 54.7% 64.0% Average 14.7% 21.2% 34.9% 66.8% 78.94% For each query, each run provides a ranking of about 1000 documents.",
                "The number of documents retrieved by all these runs ranges from 543 to 5769.",
                "Their average (median) number is 3340 (3386).",
                "It is worth noting that we found similar distributions of the documents among the rankings as in [11].",
                "For evaluation, we used the trec eval standard tool which is used by the TREC community to calculate the standard measures of system effectiveness which are Mean Average Precision (MAP) and Success@n (S@n) for n=1, 5 and 10.",
                "Our approach effectiveness is compared against some high performing official results from TREC-2004 as well as against some standard rank aggregation algorithms.",
                "In the experiments, significance testing is mainly based on the t-student statistic which is computed on the basis of the MAP values of the compared runs.",
                "In the tables of the following section, statistically significant differences are marked with an asterisk.",
                "Values between brackets of the first column of each table, indicate the parameter value of the corresponding run. 5.2 Results We carried out several series of runs in order to i) study performance variations of the outranking approach when tuning the parameters and working assumptions, ii) compare performances of the outranking approach vs standard rank aggregation strategies , and iii) check whether rank aggregation performs better than the best input rankings.",
                "We set our basic run mcm with the following parameters.",
                "We considered that each input ranking is a complete order (sp = 0) and that an input ranking strongly refutes diσdi when the difference of both document positions is large enough (sv = 75%).",
                "Preference and veto thresholds are computed proportionally to the number of documents retained in each input ranking.",
                "They consequently may vary from one ranking to another.",
                "In addition, to accept the assertion diσdi , we supposed that the majority of the rankings must be concordant (cmin = 50%) and that every input ranking can impose its veto (dmax = 0).",
                "Concordance and discordance thresholds are computed for each tuple (di, di ) as the percentage of the input rankings of PRi ∩PRi .",
                "Thus, our choice of parameters leads to the definition of the outranking relation S(0,75%,50%,0).",
                "To test the run mcm, we had chosen the following assumptions.",
                "We retained the top 100 best documents from each input ranking (H1 100), only considered documents which are present in at least half of the input rankings (H2 5 ) and assumed H3 no and H4 new.",
                "In these conditions, the number of successful documents was about 100 on average, and the computation time per query was less than one second.",
                "Obviously, modifying the working assumptions should have deeper impact on the performances than tuning our model parameters.",
                "This was validated by preliminary experiments.",
                "Thus, we hereafter begin by studying performance variation when different sets of assumptions are considered.",
                "Afterwards, we study the impact of tuning parameters.",
                "Finally, we compare our model performances w.r.t. the input rankings as well as some standard data fusion algorithms. 5.2.1 Impact of the Working Assumptions Table 4 summarizes the performance variation of the outranking approach under different working hypotheses.",
                "In Table 4: Impact of the working assumptions Run Id MAP S@1 S@5 S@10 mcm 18.47% 41.33% 81.33% 86.67% mcm22 (H3 yes) 17.72% (-4.06%) 34.67% 81.33% 86.67% mcm23 (H4 init) 18.26% (-1.14%) 41.33% 81.33% 86.67% mcm24 (H1 all) 20.67% (+11.91%*) 38.66% 80.00% 86.66% mcm25 (H2 all) 21.68% (+17.38%*) 40.00% 78.66% 89.33% this table, we first show that run mcm22, in which missing documents are all put in the same last position of each input ranking, leads to performance drop w.r.t. run mcm.",
                "Moreover, S@1 moves from 41.33% to 34.67% (-16.11%).",
                "This shows that several relevant documents which were initially put at the first position of the consensus ranking in mcm, lose this first position but remain ranked in the top 5 documents since S@5 did not change.",
                "We also conclude that documents which have rather good positions in some input rankings are more likely to be relevant, even though they are missing in some other rankings.",
                "Consequently, when they are missing in some rankings, assigning worse ranks to these documents is harmful for performance.",
                "Also, from Table 4, we found that the performances of runs mcm and mcm23 are similar.",
                "Therefore, the outranking approach is not sensitive to keeping the initial positions of candidate documents or recomputing them by discarding excluded ones.",
                "From the same Table 4, performance of the outranking approach increases significantly for runs mcm24 and mcm25.",
                "Therefore, whether we consider all the documents which are present in half of the rankings (mcm24) or we consider all the documents which are ranked in the first 100 positions in one or more rankings (mcm25), increases performances.",
                "This result was predictable since in both cases we have more detailed information on the relative importance of documents.",
                "Tables 5 and 6 confirm this evidence.",
                "Table 5, where values between brackets of the first column give the number of documents which are retained from each input ranking, shows that selecting more documents from each input ranking leads to performance increase.",
                "It is worth mentioning that selecting more than 600 documents from each input ranking does not improve performance.",
                "Table 5: Impact of the number of retained documents Run Id MAP S@1 S@5 S@10 mcm (100) 18.47% 41.33% 81.33% 86.67% mcm24-1 (200) 19.32% (+4.60%) 42.67% 78.67% 88.00% mcm24-2 (400) 19.88% (+7.63%*) 37.33% 80.00% 88.00% mcm24-3 (600) 20.80% (+12.62%*) 40.00% 80.00% 88.00% mcm24-4 (800) 20.66% (+11.86%*) 40.00% 78.67% 86.67% mcm24 (1000) 20.67% (+11.91%*) 38.66% 80.00% 86.66% Table 6 reports runs corresponding to variations of H2 k .",
                "Values between brackets are rank hits.",
                "For instance, in the run mcm32, only documents which are present in 3 or more input rankings, were considered successful.",
                "This table shows that performance is significantly better when rare documents are considered, whereas it decreases significantly when these documents are discarded.",
                "Therefore, we conclude that many of the relevant documents are retrieved by a rather small set of IR models.",
                "Table 6: Performance considering different rank hits Run Id MAP S@1 S@5 S@10 mcm25 (1) 21.68% (+17.38%*) 40.00% 78.67% 89.33% mcm32 (3) 18.98% (+2.76%) 38.67% 80.00% 85.33% mcm (5) 18.47% 41.33% 81.33% 86.67% mcm33 (7) 15.83% (-14.29%*) 37.33% 78.67% 85.33% mcm34 (9) 10.96% (-40.66%*) 36.11% 66.67% 70.83% mcm35 (10) 7.42% (-59.83%*) 39.22% 62.75% 64.70% For both runs mcm24 and mcm25, the number of successful documents was about 1000 and therefore, the computation time per query increased and became around 5 seconds. 5.2.2 Impact of the Variation of the Parameters Table 7 shows performance variation of the outranking approach when different preference thresholds are considered.",
                "We found performance improvement up to threshold values of about 5%, then there is a decrease in the performance which becomes significant for threshold values greater than 10%.",
                "Moreover, S@1 improves from 41.33% to 46.67% when preference threshold changes from 0 to 5%.",
                "We can thus conclude that the input rankings are semi orders rather than complete orders.",
                "Table 8 shows the evolution of the performance measures w.r.t. the concordance threshold.",
                "We can conclude that in order to put document di before di in the consensus ranking, Table 7: Impact of the variation of the preference threshold from 0 to 12.5% Run Id MAP S@1 S@5 S@10 mcm (0%) 18.47% 41.33% 81.33% 86.67% mcm1 (1%) 18.57% (+0.54%) 41.33% 81.33% 86.67% mcm2 (2.5%) 18.63% (+0.87%) 42.67% 78.67% 86.67% mcm3 (5%) 18.69% (+1.19%) 46.67% 81.33% 86.67% mcm4 (7.5%) 18.24% (-1.25%) 46.67% 81.33% 86.67% mcm5 (10%) 17.93% (-2.92%) 40.00% 82.67% 86.67% mcm5b (12.5%) 17.51% (-5.20%*) 41.33% 80.00% 86.67% at least half of the input rankings of PRi ∩ PRi should be concordant.",
                "Performance drops significantly for very low and very high values of the concordance threshold.",
                "In fact, for such values, the concordance condition is either fulfilled rather always by too many document pairs or not fulfilled at all, respectively.",
                "Therefore, the outranking relation becomes either too weak or too strong respectively.",
                "Table 8: Impact of the variation of cmin Run Id MAP S@1 S@5 S@10 mcm11 (20%) 17.63% (-4.55%*) 41.33% 76.00% 85.33% mcm12 (40%) 18.37% (-0.54%) 42.67% 76.00% 86.67% mcm (50%) 18.47% 41.33% 81.33% 86.67% mcm13 (60%) 18.42% (-0.27%) 40.00% 78.67% 86.67% mcm14 (80%) 17.43% (-5.63%*) 40.00% 78.67% 86.67% mcm15 (100%) 16.12% (-12.72%*) 41.33% 70.67% 85.33% In the experiments, varying the veto threshold as well as the discordance threshold within reasonable intervals does not have significant impact on performance measures.",
                "In fact, runs with different veto thresholds (sv ∈ [50%; 100%]) had similar performances even though there is a slight advantage for runs with high threshold values which means that it is better not to allow the input rankings to put their veto easily.",
                "Also, tuning the discordance threshold was carried out for values 50% and 75% of the veto threshold.",
                "For these runs we did not get any noticeable performance variation, although for low discordance thresholds (dmax < 20%), performance slightly decreased. 5.2.3 Impact of the Variation of the Number of Input Rankings To study performance evolution when different sets of input rankings are considered, we carried three more runs where 2, 4, and 6 of the best performing sets of the input rankings are considered.",
                "Results reported in Table 9 are seemingly counter-intuitive and also do not support previous findings regarding rank aggregation research [3].",
                "Nevertheless, this result shows that low performing rankings bring more noise than information to the establishment of the consensus ranking.",
                "Therefore, when they are considered, performance decreases.",
                "Table 9: Performance considering different best performing sets of input rankings Run Id MAP S@1 S@5 S@10 mcm (10) 18.47% 41.33% 81.33% 86.67% mcm27 (6) 18.60% (+0.70%) 41.33% 80.00% 85.33% mcm28 (4) 19.02% (+2.98%) 40.00% 86.67% 88.00% mcm29 (2) 18.33% (-0.76%) 44.00% 76.00% 88.00% 5.2.4 Comparison of the Performance of Different Rank Aggregation Methods In this set of runs, we compare the outranking approach with some standard rank aggregation methods which were proven to have acceptable performance in previous studies: we considered two positional methods which are the CombSUM and the CombMNZ strategies.",
                "We also examined the performance of one majoritarian method which is the Markov chain method (MC4).",
                "For the comparisons, we considered a specific outranking relation S∗ = S(5%,50%,50%,30%) which results in good overall performances when tuning all the parameters.",
                "The first row of Table 10 gives performances of the rank aggregation methods w.r.t. a basic assumption set A1 = (H1 100, H2 5 , H4 new): we only consider the 100 first documents from each ranking, then retain documents present in 5 or more rankings and update ranks of successful documents.",
                "For positional methods, we place missing documents at the queue of the ranking (H3 yes) whereas for our method as well as for MC4, we retained hypothesis H3 no.",
                "The three following rows of Table 10 report performances when changing one element from the basic assumption set: the second row corresponds to the assumption set A2 = (H1 1000, H2 5 , H4 new), i.e. changing the number of retained documents from 100 to 1000.",
                "The third row corresponds to the assumption set A3 = (H1 100, H2 all, H4 new), i.e. considering the documents present in at least one ranking.",
                "The fourth row corresponds to the assumption set A4 = (H1 100, H2 5 , H4 init), i.e. keeping the original ranks of successful documents.",
                "The fifth row of Table 10, labeled A5, gives performance when all the 225 queries of the Web track of TREC-2004 are considered.",
                "Obviously, performance level cannot be compared with previous lines since the additional queries are different from the TD queries and correspond to other tasks (Home Page and Named Page tasks [10]) of TREC-2004 Web track.",
                "This set of runs aims to show whether relative performance of the various methods is task-dependent.",
                "The last row of Table 10, labeled A6, reports performance of the various methods considering the TD task of TREC2002 instead of TREC-2004: we fused the results of input rankings of the 10 best official runs for each of the 50 TD queries [9] considering the set of assumptions A1 of the first row.",
                "This aims to show whether relative performance of the various methods changes from year to year.",
                "Values between brackets of Table 10 are variations of performance of each rank aggregation method w.r.t. performance of the outranking approach.",
                "Table 10: Performance (MAP) of different rank aggregation methods under 3 different test collections mcm combSUM combMNZ markov A1 18.79% 17.54% (-6.65%*) 17.08% (-9.10%*) 18.63% (-0.85%) A2 21.36% 19.18% (-10.21%*) 18.61% (-12.87%*) 21.33% (-0.14%) A3 21.92% 21.38% (-2.46%) 20.88% (-4.74%) 19.35% (-11.72%*) A4 18.64% 17.58% (-5.69%*) 17.18% (-7.83%*) 18.63% (-0.05%) A5 55.39% 52.16% (-5.83%*) 49.70% (-10.27%*) 53.30% (-3.77%) A6 16.95% 15.65% (-7.67%*) 14.57% (-14.04%*) 16.39% (-3.30%) From the analysis of table 10 the following can be established: • for all the runs, considering all the documents in each input ranking (A2) significantly improves performance (MAP increases by 11.62% on average).",
                "This is predictable since some initially unreported relevant documents would receive better positions in the consensus ranking. • for all the runs, considering documents even those present in only one input ranking (A3) significantly improves performance.",
                "For mcm, combSUM and combMNZ, performance improvement is more important (MAP increases by 20.27% on average) than for the markov run (MAP increases by 3.86%). • preserving the initial positions of documents (A4) or recomputing them (A1) does not have a noticeable influence on performance for both positional and majoritarian methods. • considering all the queries of the Web track of TREC2004 (A5) as well as the TD queries of the Web track of TREC-2002 (A6) does not alter the relative performance of the different data fusion methods. • considering the TD queries of the Web track of TREC2002, performances of all the data fusion methods are lower than that of the best performing input ranking for which the MAP value equals 18.58%.",
                "This is because most of the fused input rankings have very low performances compared to the best one, which brings more noise to the consensus ranking. • performances of the data fusion methods mcm and markov are significantly better than that of the best input ranking uogWebCAU150.",
                "This remains true for runs combSUM and combMNZ only under assumptions H1 all or H2 all.",
                "This shows that majoritarian methods are less sensitive to assumptions than positional methods. • outranking approach always performs significantly better than positional methods combSUM and combMNZ.",
                "It has also better performances than the Markov chain method, especially under assumption H2 all where difference of performances becomes significant. 6.",
                "CONCLUSIONS In this paper, we address the rank aggregation problem where different, but not disjoint, lists of documents are to be fused.",
                "We noticed that the input rankings can hide ties, so they should not be considered as complete orders.",
                "Only robust information should be used from each input ranking.",
                "Current rank aggregation methods, and especially positional methods (e.g. combSUM [15]), are not initially designed to work with such rankings.",
                "They should be adapted by considering specific working assumptions.",
                "We propose a new outranking method for rank aggregation which is well adapted to the IR context.",
                "Indeed, it ranks two documents w.r.t. the intensity of their positions difference in each input ranking and also considering the number of the input rankings that are concordant and discordant in favor of a specific document.",
                "There is also no need to make specific assumptions on the positions of the missing documents.",
                "This is an important feature since the absence of a document from a ranking should not be necessarily interpreted negatively.",
                "Experimental results show that the outranking method significantly out-performs popular classical positional data fusion methods like combSUM and combMNZ strategies.",
                "It also out-performs a good performing majoritarian methods which is the Markov chain method.",
                "These results are tested against different test collections and queries.",
                "From the experiments, we can also conclude that in order to improve the performances, we should fuse result lists of well performing IR models, and that majoritarian data fusion methods perform better than positional methods.",
                "The proposed method can have a real impact on Web metasearch performances since only ranks are available from most primary search engines, whereas most of the current approaches need scores to merge result lists into one single list.",
                "Further work involves investigating whether the outranking approach performs well in various other contexts, e.g. using the document scores or some combination of document ranks and scores.",
                "Acknowledgments The authors would like to thank Jacques Savoy for his valuable comments on a preliminary version of this paper. 7.",
                "REFERENCES [1] A. Aronson, D. Demner-Fushman, S. Humphrey, J. Lin, H. Liu, P. Ruch, M. Ruiz, L. Smith, L. Tanabe, and W. Wilbur.",
                "Fusion of knowledge-intensive and statistical approaches for retrieving and annotating textual genomics documents.",
                "In Proceedings TREC2005.",
                "NIST Publication, 2005. [2] R. A. Baeza-Yates and B.",
                "A. Ribeiro-Neto.",
                "Modern Information Retrieval.",
                "ACM Press , 1999. [3] B. T. Bartell, G. W. Cottrell, and R. K. Belew.",
                "Automatic combination of multiple ranked retrieval systems.",
                "In Proceedings ACM-SIGIR94, pages 173-181.",
                "Springer-Verlag, 1994. [4] N. J. Belkin, P. Kantor, E. A.",
                "Fox, and J.",
                "A. Shaw.",
                "Combining evidence of multiple query representations for information retrieval.",
                "IPM, 31(3):431-448, 1995. [5] J. Borda.",
                "M´emoire sur les ´elections au scrutin.",
                "Histoire de lAcad´emie des Sciences, 1781. [6] J. P. Callan, Z. Lu, and W. B. Croft.",
                "Searching distributed collections with inference networks.",
                "In Proceedings ACM-SIGIR95, pages 21-28, 1995. [7] M. Condorcet.",
                "Essai sur lapplication de lanalyse `a la probabilit´e des d´ecisions rendues `a la pluralit´e des voix.",
                "Imprimerie Royale, Paris, 1785. [8] W. D. Cook and M. Kress.",
                "Ordinal ranking with intensity of preference.",
                "Management Science, 31(1):26-32, 1985. [9] N. Craswell and D. Hawking.",
                "Overview of the TREC-2002 Web Track.",
                "In Proceedings TREC2002.",
                "NIST Publication, 2002. [10] N. Craswell and D. Hawking.",
                "Overview of the TREC-2004 Web Track.",
                "In Proceedings of TREC2004.",
                "NIST Publication, 2004. [11] C. Dwork, S. R. Kumar, M. Naor, and D. Sivakumar.",
                "Rank aggregation methods for the Web.",
                "In Proceedings WWW2001, pages 613-622, 2001. [12] R. Fagin.",
                "Combining fuzzy information from multiple systems.",
                "JCSS, 58(1):83-99, 1999. [13] R. Fagin, R. Kumar, M. Mahdian, D. Sivakumar, and E. Vee.",
                "Comparing and aggregating rankings with ties.",
                "In PODS, pages 47-58, 2004. [14] R. Fagin, R. Kumar, and D. Sivakumar.",
                "Comparing top k lists.",
                "SIAM J. on Discrete Mathematics, 17(1):134-160, 2003. [15] E. A.",
                "Fox and J.",
                "A. Shaw.",
                "Combination of multiple searches.",
                "In Proceedings of TREC3.",
                "NIST Publication, 1994. [16] J. Katzer, M. McGill, J. Tessier, W. Frakes, and P. DasGupta.",
                "A study of the overlap among document representations.",
                "Information Technology: Research and Development, 1(4):261-274, 1982. [17] L. S. Larkey, M. E. Connell, and J. Callan.",
                "Collection selection and results merging with topically organized U.S. patents and TREC data.",
                "In Proceedings ACM-CIKM2000, pages 282-289.",
                "ACM Press, 2000. [18] A.",
                "Le Calv´e and J. Savoy.",
                "Database merging strategy based on logistic regression.",
                "IPM, 36(3):341-359, 2000. [19] J. H. Lee.",
                "Analyses of multiple evidence combination.",
                "In Proceedings ACM-SIGIR97, pages 267-276, 1997. [20] D. Lillis, F. Toolan, R. Collier, and J. Dunnion.",
                "Probfuse: a probabilistic approach to data fusion.",
                "In Proceedings ACM-SIGIR2006, pages 139-146.",
                "ACM Press, 2006. [21] J. I. Marden.",
                "Analyzing and Modeling Rank Data.",
                "Number 64 in Monographs on Statistics and Applied Probability.",
                "Chapman & Hall, 1995. [22] M. Montague and J.",
                "A. Aslam.",
                "Metasearch consistency.",
                "In Proceedings ACM-SIGIR2001, pages 386-387.",
                "ACM Press, 2001. [23] D. M. Pennock and E. Horvitz.",
                "Analysis of the axiomatic foundations of collaborative filtering.",
                "In Workshop on AI for Electronic Commerce at the 16th National Conference on Artificial Intelligence, 1999. [24] M. E. Renda and U. Straccia.",
                "Web metasearch: rank vs. score based rank aggregation methods.",
                "In Proceedings ACM-SAC2003, pages 841-846.",
                "ACM Press, 2003. [25] W. H. Riker.",
                "Liberalism against populism.",
                "Waveland Press, 1982. [26] B. Roy.",
                "The outranking approach and the foundations of ELECTRE methods.",
                "Theory and Decision, 31:49-73, 1991. [27] B. Roy and J. Hugonnard.",
                "Ranking of suburban line extension projects on the Paris metro system by a multicriteria method.",
                "Transportation Research, 16A(4):301-312, 1982. [28] L. Si and J. Callan.",
                "Using sampled data and regression to merge search engine results.",
                "In Proceedings ACM-SIGIR2002, pages 19-26.",
                "ACM Press, 2002. [29] M. Truchon.",
                "An extension of the Condorcet criterion and Kemeny orders.",
                "Cahier 9813, Centre de Recherche en Economie et Finance Appliqu´ees, Oct. 1998. [30] H. Turtle and W. B. Croft.",
                "Inference networks for document retrieval.",
                "In Proceedings of ACM-SIGIR90, pages 1-24.",
                "ACM Press, 1990. [31] C. C. Vogt and G. W. Cottrell.",
                "Fusion via a linear combination of scores.",
                "Information Retrieval, 1(3):151-173, 1999."
            ],
            "original_annotated_samples": [],
            "translated_annotated_samples": [
                "Experimentos y resultados 5.1 Configuración de prueba Para facilitar la investigación empírica de la metodología propuesta, desarrollamos un prototipo \"Metasearch Engine\" que implementa una versión de nuestro enfoque supervisado para la agregación de rango."
            ],
            "translated_text": "",
            "candidates": [
                "motor de metasearch",
                "Metasearch Engine"
            ],
            "error": []
        },
        "combsum and combmnz strategy": {
            "translated_key": "Estrategia Cocebro y Combmnz",
            "is_in_text": false,
            "original_annotated_sentences": [
                "An Outranking Approach for Rank Aggregation in Information Retrieval Mohamed Farah Lamsade, Paris Dauphine University Place du Mal de Lattre de Tassigny 75775 Paris Cedex 16, France farah@lamsade.dauphine.fr Daniel Vanderpooten Lamsade, Paris Dauphine University Place du Mal de Lattre de Tassigny 75775 Paris Cedex 16, France vdp@lamsade.dauphine.fr ABSTRACT Research in Information Retrieval usually shows performance improvement when many sources of evidence are combined to produce a ranking of documents (e.g., texts, pictures, sounds, etc.).",
                "In this paper, we focus on the rank aggregation problem, also called data fusion problem, where rankings of documents, searched into the same collection and provided by multiple methods, are combined in order to produce a new ranking.",
                "In this context, we propose a rank aggregation method within a multiple criteria framework using aggregation mechanisms based on decision rules identifying positive and negative reasons for judging whether a document should get a better rank than another.",
                "We show that the proposed method deals well with the Information Retrieval distinctive features.",
                "Experimental results are reported showing that the suggested method performs better than the well-known CombSUM and CombMNZ operators.",
                "Categories and Subject Descriptors: H.3.3 [Information Systems]: Information Search and Retrieval - Retrieval models.",
                "General Terms: Algorithms, Measurement, Experimentation, Performance, Theory. 1.",
                "INTRODUCTION A wide range of current Information Retrieval (IR) approaches are based on various search models (Boolean, Vector Space, Probabilistic, Language, etc. [2]) in order to retrieve relevant documents in response to a user request.",
                "The result lists produced by these approaches depend on the exact definition of the relevance concept.",
                "Rank aggregation approaches, also called data fusion approaches, consist in combining these result lists in order to produce a new and hopefully better ranking.",
                "Such approaches give rise to metasearch engines in the Web context.",
                "We consider, in the following, cases where only ranks are available and no other additional information is provided such as the relevance scores.",
                "This corresponds indeed to the reality, where only ordinal information is available.",
                "Data fusion is also relevant in other contexts, such as when the user writes several queries of his/her information need (e.g., a boolean query and a natural language query) [4], or when many document surrogates are available [16].",
                "Several studies argued that rank aggregation has the potential of combining effectively all the various sources of evidence considered in various input methods.",
                "For instance, experiments carried out in [16], [30], [4] and [19] showed that documents which appear in the lists of the majority of the input methods are more likely to be relevant.",
                "Moreover, Lee [19] and Vogt and Cottrell [31] found that various retrieval approaches often return very different irrelevant documents, but many of the same relevant documents.",
                "Bartell et al. [3] also found that rank aggregation methods improve the performances w.r.t. those of the input methods, even when some of them have weak individual performances.",
                "These methods also tend to smooth out biases of the input methods according to Montague and Aslam [22].",
                "Data fusion has recently been proved to improve performances for both the ad-hoc retrieval and categorization tasks within the TREC genomics track in 2005 [1].",
                "The rank aggregation problem was addressed in various fields such as i) in social choice theory which studies voting algorithms which specify winners of elections or winners of competitions in tournaments [29], ii) in statistics when studying correlation between rankings, iii) in distributed databases when results from different databases must be combined [12], and iv) in collaborative filtering [23].",
                "Most current rank aggregation methods consider each input ranking as a permutation over the same set of items.",
                "They also give rigid interpretation to the exact ranking of the items.",
                "Both of these assumptions are rather not valid in the IR context, as will be shown in the following sections.",
                "The remaining of the paper is organized as follows.",
                "We first review current rank aggregation methods in Section 2.",
                "Then we outline the specificities of the data fusion problem in the IR context (Section 3).",
                "In Section 4, we present a new aggregation method which is proven to best fit the IR context.",
                "Experimental results are presented in Section 5 and conclusions are provided in a final section. 2.",
                "RELATED WORK As pointed out by Riker [25], we can distinguish two families of rank aggregation methods: positional methods which assign scores to items to be ranked according to the ranks they receive and majoritarian methods which are based on pairwise comparisons of items to be ranked.",
                "These two families of methods find their roots in the pioneering works of Borda [5] and Condorcet [7], respectively, in the social choice literature. 2.1 Preliminaries We first introduce some basic notations to present the rank aggregation methods in a uniform way.",
                "Let D = {d1, d2, . . . , dnd } be a set of nd documents.",
                "A list or a ranking j is an ordering defined on Dj ⊆ D (j = 1, . . . , n).",
                "Thus, di j di means di is ranked better than di in j.",
                "When Dj = D, j is said to be a full list.",
                "Otherwise, it is a partial list.",
                "If di belongs to Dj, rj i denotes the rank or position of di in j.",
                "We assume that the best answer (document) is assigned the position 1 and the worst one is assigned the position |Dj|.",
                "Let D be the set of all permutations on D or all subsets of D. A profile is a n-tuple of rankings PR = ( 1, 2, . . . , n).",
                "Restricting PR to the rankings containing document di defines PRi.",
                "We also call the number of rankings which contain document di the rank hits of di [19].",
                "The rank aggregation or data fusion problem consists of finding a ranking function or mechanism Ψ (also called a social welfare function in the social choice theory terminology) defined by: Ψ : n D → D PR = ( 1, 2, . . . , n) → σ = Ψ(PR) where σ is called a consensus ranking. 2.2 Positional Methods 2.2.1 Borda Count This method [5] first assigns a score n j=1 rj i to each document di.",
                "Documents are then ranked by increasing order of this score, breaking ties, if any, arbitrarily. 2.2.2 Linear Combination Methods This family of methods basically combine scores of documents.",
                "When used for the rank aggregation problem, ranks are assumed to be scores or performances to be combined using aggregation operators such as the weighted sum or some variation of it [3, 31, 17, 28].",
                "For instance, Callan et al. [6] used the inference networks model [30] to combine rankings.",
                "Fox and Shaw [15] proposed several combination strategies which are CombSUM, CombMIN, CombMAX, CombANZ and CombMNZ.",
                "The first three operators correspond to the sum, min and max operators, respectively.",
                "CombANZ and CombMNZ respectively divides and multiplies the CombSUM score by the rank hits.",
                "It is shown in [19] that the CombSUM and CombMNZ operators perform better than the others.",
                "Metasearch engines such as SavvySearch and MetaCrawler use the CombSUM strategy to fuse rankings. 2.2.3 Footrule Optimal Aggregation In this method, a consensus ranking minimizes the Spearman footrule distance from the input rankings [21].",
                "Formally, given two full lists j and j , this distance is given by F( j, j ) = nd i=1 |rj i − rj i |.",
                "It extends to several lists as follows.",
                "Given a profile PR and a consensus ranking σ, the Spearman footrule distance of σ to PR is given by F(σ, PR) = n j=1 F(σ, j).",
                "Cook and Kress [8] proposed a similar method which consists in optimizing the distance D( j, j ) = 1 2 nd i,i =1 |rj i,i − rj i,i |, where rj i,i = rj i −rj i .",
                "This formulation has the advantage that it considers the intensity of preferences. 2.2.4 Probabilistic Methods This kind of methods assume that the performance of the input methods on a number of training queries is indicative of their future performance.",
                "During the training process, probabilities of relevance are calculated.",
                "For subsequent queries, documents are ranked based on these probabilities.",
                "For instance, in [20], each input ranking j is divided into a number of segments, and the conditional probability of relevance (R) of each document di depending on the segment k it occurs in, is computed, i.e. prob(R|di, k, j).",
                "For subsequent queries, the score of each document di is given by n j=1 prob(R|di,k, j ) k .",
                "Le Calve and Savoy [18] suggest using a logistic regression approach for combining scores.",
                "Training data is needed to infer the model parameters. 2.3 Majoritarian Methods 2.3.1 Condorcet Procedure The original Condorcet rule [7] specifies that a winner of the election is any item that beats or ties with every other item in a pairwise contest.",
                "Formally, let C(diσdi ) = { j∈ PR : di j di } be the coalition of rankings that are concordant with establishing diσdi , i.e. with the proposition di should be ranked better than di in the final ranking σ. di beats or ties with di iff |C(diσdi )| ≥ |C(di σdi)|.",
                "The repetitive application of the Condorcet algorithm can produce a ranking of items in a natural way: select the Condorcet winner, remove it from the lists, and repeat the previous two steps until there are no more documents to rank.",
                "Since there is not always Condorcet winners, variations of the Condorcet procedure have been developed within the multiple criteria decision aid theory, with methods such as ELECTRE [26]. 2.3.2 Kemeny Optimal Aggregation As in section 2.2.3, a consensus ranking minimizes a geometric distance from the input rankings, where the Kendall tau distance is used instead of the Spearman footrule distance.",
                "Formally, given two full lists j and j , the Kendall tau distance is given by K( j, j ) = |{(di, di ) : i < i , rj i < rj i , rj i > rj i }|, i.e. the number of pairwise disagreements between the two lists.",
                "It is easy to show that the consensus ranking corresponds to the geometric median of the input rankings and that the Kemeny optimal aggregation problem corresponds to the minimum feedback edge set problem. 2.3.3 Markov Chain Methods Markov chains (MCs) have been used by Dwork et al. [11] as a natural method to obtain a consensus ranking where states correspond to the documents to be ranked and the transition probabilities vary depending on the interpretation of the transition event.",
                "In the same reference, the authors proposed four specific MCs and experimental testing had shown that the following MC is the best performing one (see also [24]): • MC4: move from the current state di to the next state di by first choosing a document di uniformly from D. If for the majority of the rankings, we have rj i ≤ rj i , then move to di , else stay in di.",
                "The consensus ranking corresponds to the stationary distribution of MC4. 3.",
                "SPECIFICITIES OF THE RANK AGGREGATION PROBLEM IN THE IR CONTEXT 3.1 Limited Significance of the Rankings The exact positions of documents in one input ranking have limited significance and should not be overemphasized.",
                "For instance, having three relevant documents in the first three positions, any perturbation of these three items will have the same value.",
                "Indeed, in the IR context, the complete order provided by an input method may hide ties.",
                "In this case, we call such rankings semi orders.",
                "This was outlined in [13] as the problem of aggregation with ties.",
                "It is therefore important to build the consensus ranking based on robust information: • Documents with near positions in j are more likely to have similar interest or relevance.",
                "Thus a slight perturbation of the initial ranking is meaningless. • Assuming that document di is better ranked than document di in a ranking j, di is more likely to be definitively more relevant than di in j when the number of intermediate positions between di and di increases. 3.2 Partial Lists In real world applications, such as metasearch engines, rankings provided by the input methods are often partial lists.",
                "This was outlined in [14] as the problem of having to merge top-k results from various input lists.",
                "For instance, in the experiments carried out by Dwork et al. [11], authors found that among the top 100 best documents of 7 input search engines, 67% of the documents were present in only one search engine, whereas less than two documents were present in all the search engines.",
                "Rank aggregation of partial lists raises four major difficulties which we state hereafter, proposing for each of them various working assumptions: 1.",
                "Partial lists can have various lengths, which can favour long lists.",
                "We thus consider the following two working hypotheses: H1 k : We only consider the top k best documents from each input ranking.",
                "H1 all: We consider all the documents from each input ranking. 2.",
                "Since there are different documents in the input rankings, we must decide which documents should be kept in the consensus ranking.",
                "Two working hypotheses are therefore considered: H2 k : We only consider documents which are present in at least k input rankings (k > 1).",
                "H2 all: We consider all the documents which are ranked in at least one input ranking.",
                "Hereafter, we call documents which will be retained in the consensus ranking, candidate documents, and documents that will be excluded from the consensus ranking, excluded documents.",
                "We also call a candidate document which is missing in one or more rankings, a missing document. 3.",
                "Some candidate documents are missing documents in some input rankings.",
                "Main reasons for a missing document are that it was not indexed or it was indexed but deemed irrelevant ; usually this information is not available.",
                "We consider the following two working hypotheses: H3 yes: Each missing document in each j is assigned a position.",
                "H3 no: No assumption is made, that is each missing document is considered neither better nor worse than any other document. 4.",
                "When assumption H2 k holds, each input ranking may contain documents which will not be considered in the consensus ranking.",
                "Regarding the positions of the candidate documents, we can consider the following working hypotheses: H4 init: The initial positions of candidate documents are kept in each input ranking.",
                "H4 new: Candidate documents receive new positions in each input ranking, after discarding excluded ones.",
                "In the IR context, rank aggregation methods need to decide more or less explicitly which assumptions to retain w.r.t. the above-mentioned difficulties. 4.",
                "OUTRANKING APPROACH FOR RANK AGGREGATION 4.1 Presentation Positional methods consider implicitly that the positions of the documents in the input rankings are scores giving thus a cardinal meaning to an ordinal information.",
                "This constitutes a strong assumption that is questionable, especially when the input rankings have different lengths.",
                "Moreover, for positional methods, assumptions H3 and H4 , which are often arbitrary, have a strong impact on the results.",
                "For instance, let us consider an input ranking of 500 documents out of 1000 candidate documents.",
                "Whether we assign to each of the missing documents the position 1, 501, 750 or 1000 -corresponding to variations of H3 yes- will give rise to very contrasted results, especially regarding the top of the consensus ranking.",
                "Majoritarian methods do not suffer from the above-mentioned drawbacks of the positional methods since they build consensus rankings exploiting only ordinal information contained in the input rankings.",
                "Nevertheless, they suppose that such rankings are complete orders, ignoring that they may hide ties.",
                "Therefore, majoritarian methods base consensus rankings on illusory discriminant information rather than less discriminant but more robust information.",
                "Trying to overcome the limits of current rank aggregation methods, we found that outranking approaches, which were initially used for multiple criteria aggregation problems [26], can also be used for the rank aggregation purpose, where each ranking plays the role of a criterion.",
                "Therefore, in order to decide whether a document di should be ranked better than di in the consensus ranking σ, the two following conditions should be met: • a concordance condition which ensures that a majority of the input rankings are concordant with diσdi (majority principle). • a discordance condition which ensures that none of the discordant input rankings strongly refutes dσd (respect of minorities principle).",
                "Formally, the concordance coalition with diσdi is Csp (diσdi ) = { j∈ PR : rj i ≤ rj i − sp} where sp is a preference threshold which is the variation of document positions -whether it is absolute or relative to the ranking length- which draws the boundaries between an indifference and a preference situation between documents.",
                "The discordance coalition with diσdi is Dsv (diσdi ) = { j∈ PR : rj i ≥ rj i + sv} where sv is a veto threshold which is the variation of document positions -whether it is absolute or relative to the ranking length- which draws the boundaries between a weak and a strong opposition to diσdi .",
                "Depending on the exact definition of the preceding concordance and discordance coalitions leading to the definition of some decision rules, several outranking relations can be defined.",
                "They can be more or less demanding depending on i) the values of the thresholds sp and sv, ii) the importance or minimal size cmin required for the concordance coalition, and iii) the importance or maximum size dmax of the discordance coalition.",
                "A generic outranking relation can thus be defined as follows: diS(sp,sv,cmin,dmax)di ⇔ |Csp (diσdi )| ≥ cmin AND |Dsv (diσdi )| ≤ dmax This expression defines a family of nested outranking relations since S(sp,sv,cmin,dmax) ⊆ S(sp,sv,cmin,dmax) when cmin ≥ cmin and/or dmax ≤ dmax and/or sp ≥ sp and/or sv ≤ sv.",
                "This expression also generalizes the majority rule which corresponds to the particular relation S(0,∞, n 2 ,n).",
                "It also satisfies important properties of rank aggregation methods, called neutrality, Pareto-optimality, Condorcet property and Extended Condorcet property, in the social choice literature [29].",
                "Outranking relations are not necessarily transitive and do not necessarily correspond to rankings since directed cycles may exist.",
                "Therefore, we need specific procedures in order to derive a consensus ranking.",
                "We propose the following procedure which finds its roots in [27].",
                "It consists in partitioning the set of documents into r ranked classes.",
                "Each class Ch contains documents with the same relevance and results from the application of all relations (if possible) to the set of documents remaining after previous classes are computed.",
                "Documents within the same equivalence class are ranked arbitrarily.",
                "Formally, let • R be the set of candidate documents for a query, • S1 , S2 , . . . be a family of nested outranking relations, • Fk(di, E) = |{di ∈ E : diSk di }| be the number of documents in E(E ⊆ R) that could be considered worse than di according to relation Sk , • fk(di, E) = |{di ∈ E : di Sk di}| be the number of documents in E that could be considered better than di according to Sk , • sk(di, E) = Fk(di, E) − fk(di, E) be the qualification of di in E according to Sk .",
                "Each class Ch results from a distillation process.",
                "It corresponds to the last distillate of a series of sets E0 ⊇ E1 ⊇ . . . where E0 = R \\ (C1 ∪ . . . ∪ Ch−1) and Ek is a reduced subset of Ek−1 resulting from the application of the following procedure: 1. compute for each di ∈ Ek−1 its qualification according to Sk , i.e. sk(di, Ek−1), 2. define smax = maxdi∈Ek−1 {sk(di, Ek−1)}, then 3.",
                "Ek = {di ∈ Ek−1 : sk(di, Ek−1) = smax} When one outranking relation is used, the distillation process stops after the first application of the previous procedure, i.e., Ch corresponds to distillate E1.",
                "When different outranking relations are used, the distillation process stops when all the pre-defined outranking relations have been used or when |Ek| = 1. 4.2 Illustrative Example This section illustrates the concepts and procedures of section 4.1.",
                "Let us consider a set of candidate documents R = {d1, d2, d3, d4, d5}.",
                "The following table gives a profile PR of different rankings of the documents of R: PR = ( 1 , 2, 3, 4).",
                "Table 1: Rankings of documents rj i 1 2 3 4 d1 1 3 1 5 d2 2 1 3 3 d3 3 2 2 1 d4 4 4 5 2 d5 5 5 4 4 Let us suppose that the preference and veto thresholds are set to values 1 and 4 respectively, and that the concordance and discordance thresholds are set to values 2 and 1 respectively.",
                "The following tables give the concordance, discordance and outranking matrices.",
                "Each entry csp (di, di ) (dsv (di, di )) in the concordance (discordance) matrix gives the number of rankings that are concordant (discordant) with diσdi , i.e. csp (di, di ) = |Csp (diσdi )| and dsv (di, di ) = |Dsv (diσdi )|.",
                "Table 2: Computation of the outranking relation d1 d2 d3 d4 d5 d1 - 2 2 3 3 d2 2 - 2 3 4 d3 2 2 - 4 4 d4 1 1 0 - 3 d5 1 0 0 1Concordance Matrix d1 d2 d3 d4 d5 d1 - 0 1 0 0 d2 0 - 0 0 0 d3 0 0 - 0 0 d4 1 0 0 - 0 d5 1 1 0 0Discordance Matrix d1 d2 d3 d4 d5 d1 - 1 1 1 1 d2 1 - 1 1 1 d3 1 1 - 1 1 d4 0 0 0 - 1 d5 0 0 0 0Outranking Matrix (S1) For instance, the concordance coalition for the assertion d1σd4 is C1(d1σd4) = { 1, 2, 3} and the discordance coalition for the same assertion is D4(d1σd4) = ∅.",
                "Therefore, c1(d1, d4) = 3, d4(d1, d4) = 0 and d1S1 d4 holds.",
                "Notice that Fk(di, R) (fk(di, R)) is given by summing the values of the ith row (column) of the outranking matrix.",
                "The consensus ranking is obtained as follows: to get the first class C1, we compute the qualifications of all the documents of E0 = R with respect to S1 .",
                "They are respectively 2, 2, 2, -2 and -4.",
                "Therefore smax equals 2 and C1 = E1 = {d1, d2, d3}.",
                "Observe that, if we had used a second outranking relation S2(⊇ S1), these three documents could have been possibly discriminated.",
                "At this stage, we remove documents of C1 from the outranking matrix and compute the next class C2: we compute the new qualifications of the documents of E0 = R \\ C1 = {d4, d5}.",
                "They are respectively 1 and -1.",
                "So C3 = E1 = {d4}.",
                "The last document d5 is the only document of the last class C3.",
                "Thus, the consensus ranking is {d1, d2, d3} → {d4} → {d5}. 5.",
                "EXPERIMENTS AND RESULTS 5.1 Test Setting To facilitate empirical investigation of the proposed methodology, we developed a prototype metasearch engine that implements a version of our outranking approach for rank aggregation.",
                "In this paper, we apply our approach to the Topic Distillation (TD) task of TREC-2004 Web track [10].",
                "In this task, there are 75 topics where only a short description of each is given.",
                "For each query, we retained the rankings of the 10 best runs of the TD task which are provided by TREC-2004 participating teams.",
                "The performances of these runs are reported in table 3.",
                "Table 3: Performances of the 10 best runs of the TD task of TREC-2004 Run Id MAP P@10 S@1 S@5 S@10 uogWebCAU150 17.9% 24.9% 50.7% 77.3% 89.3% MSRAmixed1 17.8% 25.1% 38.7% 72.0% 88.0% MSRC04C12 16.5% 23.1% 38.7% 74.7% 80.0% humW04rdpl 16.3% 23.1% 37.3% 78.7% 90.7% THUIRmix042 14.7% 20.5% 21.3% 58.7% 74.7% UAmsT04MWScb 14.6% 20.9% 36.0% 66.7% 76.0% ICT04CIIS1AT 14.1% 20.8% 33.3% 64.0% 78.7% SJTUINCMIX5 12.9% 18.9% 29.3% 57.3% 72.0% MU04web1 11.5% 19.9% 33.3% 64.0% 76.0% MeijiHILw3 11.5% 15.3% 30.7% 54.7% 64.0% Average 14.7% 21.2% 34.9% 66.8% 78.94% For each query, each run provides a ranking of about 1000 documents.",
                "The number of documents retrieved by all these runs ranges from 543 to 5769.",
                "Their average (median) number is 3340 (3386).",
                "It is worth noting that we found similar distributions of the documents among the rankings as in [11].",
                "For evaluation, we used the trec eval standard tool which is used by the TREC community to calculate the standard measures of system effectiveness which are Mean Average Precision (MAP) and Success@n (S@n) for n=1, 5 and 10.",
                "Our approach effectiveness is compared against some high performing official results from TREC-2004 as well as against some standard rank aggregation algorithms.",
                "In the experiments, significance testing is mainly based on the t-student statistic which is computed on the basis of the MAP values of the compared runs.",
                "In the tables of the following section, statistically significant differences are marked with an asterisk.",
                "Values between brackets of the first column of each table, indicate the parameter value of the corresponding run. 5.2 Results We carried out several series of runs in order to i) study performance variations of the outranking approach when tuning the parameters and working assumptions, ii) compare performances of the outranking approach vs standard rank aggregation strategies , and iii) check whether rank aggregation performs better than the best input rankings.",
                "We set our basic run mcm with the following parameters.",
                "We considered that each input ranking is a complete order (sp = 0) and that an input ranking strongly refutes diσdi when the difference of both document positions is large enough (sv = 75%).",
                "Preference and veto thresholds are computed proportionally to the number of documents retained in each input ranking.",
                "They consequently may vary from one ranking to another.",
                "In addition, to accept the assertion diσdi , we supposed that the majority of the rankings must be concordant (cmin = 50%) and that every input ranking can impose its veto (dmax = 0).",
                "Concordance and discordance thresholds are computed for each tuple (di, di ) as the percentage of the input rankings of PRi ∩PRi .",
                "Thus, our choice of parameters leads to the definition of the outranking relation S(0,75%,50%,0).",
                "To test the run mcm, we had chosen the following assumptions.",
                "We retained the top 100 best documents from each input ranking (H1 100), only considered documents which are present in at least half of the input rankings (H2 5 ) and assumed H3 no and H4 new.",
                "In these conditions, the number of successful documents was about 100 on average, and the computation time per query was less than one second.",
                "Obviously, modifying the working assumptions should have deeper impact on the performances than tuning our model parameters.",
                "This was validated by preliminary experiments.",
                "Thus, we hereafter begin by studying performance variation when different sets of assumptions are considered.",
                "Afterwards, we study the impact of tuning parameters.",
                "Finally, we compare our model performances w.r.t. the input rankings as well as some standard data fusion algorithms. 5.2.1 Impact of the Working Assumptions Table 4 summarizes the performance variation of the outranking approach under different working hypotheses.",
                "In Table 4: Impact of the working assumptions Run Id MAP S@1 S@5 S@10 mcm 18.47% 41.33% 81.33% 86.67% mcm22 (H3 yes) 17.72% (-4.06%) 34.67% 81.33% 86.67% mcm23 (H4 init) 18.26% (-1.14%) 41.33% 81.33% 86.67% mcm24 (H1 all) 20.67% (+11.91%*) 38.66% 80.00% 86.66% mcm25 (H2 all) 21.68% (+17.38%*) 40.00% 78.66% 89.33% this table, we first show that run mcm22, in which missing documents are all put in the same last position of each input ranking, leads to performance drop w.r.t. run mcm.",
                "Moreover, S@1 moves from 41.33% to 34.67% (-16.11%).",
                "This shows that several relevant documents which were initially put at the first position of the consensus ranking in mcm, lose this first position but remain ranked in the top 5 documents since S@5 did not change.",
                "We also conclude that documents which have rather good positions in some input rankings are more likely to be relevant, even though they are missing in some other rankings.",
                "Consequently, when they are missing in some rankings, assigning worse ranks to these documents is harmful for performance.",
                "Also, from Table 4, we found that the performances of runs mcm and mcm23 are similar.",
                "Therefore, the outranking approach is not sensitive to keeping the initial positions of candidate documents or recomputing them by discarding excluded ones.",
                "From the same Table 4, performance of the outranking approach increases significantly for runs mcm24 and mcm25.",
                "Therefore, whether we consider all the documents which are present in half of the rankings (mcm24) or we consider all the documents which are ranked in the first 100 positions in one or more rankings (mcm25), increases performances.",
                "This result was predictable since in both cases we have more detailed information on the relative importance of documents.",
                "Tables 5 and 6 confirm this evidence.",
                "Table 5, where values between brackets of the first column give the number of documents which are retained from each input ranking, shows that selecting more documents from each input ranking leads to performance increase.",
                "It is worth mentioning that selecting more than 600 documents from each input ranking does not improve performance.",
                "Table 5: Impact of the number of retained documents Run Id MAP S@1 S@5 S@10 mcm (100) 18.47% 41.33% 81.33% 86.67% mcm24-1 (200) 19.32% (+4.60%) 42.67% 78.67% 88.00% mcm24-2 (400) 19.88% (+7.63%*) 37.33% 80.00% 88.00% mcm24-3 (600) 20.80% (+12.62%*) 40.00% 80.00% 88.00% mcm24-4 (800) 20.66% (+11.86%*) 40.00% 78.67% 86.67% mcm24 (1000) 20.67% (+11.91%*) 38.66% 80.00% 86.66% Table 6 reports runs corresponding to variations of H2 k .",
                "Values between brackets are rank hits.",
                "For instance, in the run mcm32, only documents which are present in 3 or more input rankings, were considered successful.",
                "This table shows that performance is significantly better when rare documents are considered, whereas it decreases significantly when these documents are discarded.",
                "Therefore, we conclude that many of the relevant documents are retrieved by a rather small set of IR models.",
                "Table 6: Performance considering different rank hits Run Id MAP S@1 S@5 S@10 mcm25 (1) 21.68% (+17.38%*) 40.00% 78.67% 89.33% mcm32 (3) 18.98% (+2.76%) 38.67% 80.00% 85.33% mcm (5) 18.47% 41.33% 81.33% 86.67% mcm33 (7) 15.83% (-14.29%*) 37.33% 78.67% 85.33% mcm34 (9) 10.96% (-40.66%*) 36.11% 66.67% 70.83% mcm35 (10) 7.42% (-59.83%*) 39.22% 62.75% 64.70% For both runs mcm24 and mcm25, the number of successful documents was about 1000 and therefore, the computation time per query increased and became around 5 seconds. 5.2.2 Impact of the Variation of the Parameters Table 7 shows performance variation of the outranking approach when different preference thresholds are considered.",
                "We found performance improvement up to threshold values of about 5%, then there is a decrease in the performance which becomes significant for threshold values greater than 10%.",
                "Moreover, S@1 improves from 41.33% to 46.67% when preference threshold changes from 0 to 5%.",
                "We can thus conclude that the input rankings are semi orders rather than complete orders.",
                "Table 8 shows the evolution of the performance measures w.r.t. the concordance threshold.",
                "We can conclude that in order to put document di before di in the consensus ranking, Table 7: Impact of the variation of the preference threshold from 0 to 12.5% Run Id MAP S@1 S@5 S@10 mcm (0%) 18.47% 41.33% 81.33% 86.67% mcm1 (1%) 18.57% (+0.54%) 41.33% 81.33% 86.67% mcm2 (2.5%) 18.63% (+0.87%) 42.67% 78.67% 86.67% mcm3 (5%) 18.69% (+1.19%) 46.67% 81.33% 86.67% mcm4 (7.5%) 18.24% (-1.25%) 46.67% 81.33% 86.67% mcm5 (10%) 17.93% (-2.92%) 40.00% 82.67% 86.67% mcm5b (12.5%) 17.51% (-5.20%*) 41.33% 80.00% 86.67% at least half of the input rankings of PRi ∩ PRi should be concordant.",
                "Performance drops significantly for very low and very high values of the concordance threshold.",
                "In fact, for such values, the concordance condition is either fulfilled rather always by too many document pairs or not fulfilled at all, respectively.",
                "Therefore, the outranking relation becomes either too weak or too strong respectively.",
                "Table 8: Impact of the variation of cmin Run Id MAP S@1 S@5 S@10 mcm11 (20%) 17.63% (-4.55%*) 41.33% 76.00% 85.33% mcm12 (40%) 18.37% (-0.54%) 42.67% 76.00% 86.67% mcm (50%) 18.47% 41.33% 81.33% 86.67% mcm13 (60%) 18.42% (-0.27%) 40.00% 78.67% 86.67% mcm14 (80%) 17.43% (-5.63%*) 40.00% 78.67% 86.67% mcm15 (100%) 16.12% (-12.72%*) 41.33% 70.67% 85.33% In the experiments, varying the veto threshold as well as the discordance threshold within reasonable intervals does not have significant impact on performance measures.",
                "In fact, runs with different veto thresholds (sv ∈ [50%; 100%]) had similar performances even though there is a slight advantage for runs with high threshold values which means that it is better not to allow the input rankings to put their veto easily.",
                "Also, tuning the discordance threshold was carried out for values 50% and 75% of the veto threshold.",
                "For these runs we did not get any noticeable performance variation, although for low discordance thresholds (dmax < 20%), performance slightly decreased. 5.2.3 Impact of the Variation of the Number of Input Rankings To study performance evolution when different sets of input rankings are considered, we carried three more runs where 2, 4, and 6 of the best performing sets of the input rankings are considered.",
                "Results reported in Table 9 are seemingly counter-intuitive and also do not support previous findings regarding rank aggregation research [3].",
                "Nevertheless, this result shows that low performing rankings bring more noise than information to the establishment of the consensus ranking.",
                "Therefore, when they are considered, performance decreases.",
                "Table 9: Performance considering different best performing sets of input rankings Run Id MAP S@1 S@5 S@10 mcm (10) 18.47% 41.33% 81.33% 86.67% mcm27 (6) 18.60% (+0.70%) 41.33% 80.00% 85.33% mcm28 (4) 19.02% (+2.98%) 40.00% 86.67% 88.00% mcm29 (2) 18.33% (-0.76%) 44.00% 76.00% 88.00% 5.2.4 Comparison of the Performance of Different Rank Aggregation Methods In this set of runs, we compare the outranking approach with some standard rank aggregation methods which were proven to have acceptable performance in previous studies: we considered two positional methods which are the CombSUM and the CombMNZ strategies.",
                "We also examined the performance of one majoritarian method which is the Markov chain method (MC4).",
                "For the comparisons, we considered a specific outranking relation S∗ = S(5%,50%,50%,30%) which results in good overall performances when tuning all the parameters.",
                "The first row of Table 10 gives performances of the rank aggregation methods w.r.t. a basic assumption set A1 = (H1 100, H2 5 , H4 new): we only consider the 100 first documents from each ranking, then retain documents present in 5 or more rankings and update ranks of successful documents.",
                "For positional methods, we place missing documents at the queue of the ranking (H3 yes) whereas for our method as well as for MC4, we retained hypothesis H3 no.",
                "The three following rows of Table 10 report performances when changing one element from the basic assumption set: the second row corresponds to the assumption set A2 = (H1 1000, H2 5 , H4 new), i.e. changing the number of retained documents from 100 to 1000.",
                "The third row corresponds to the assumption set A3 = (H1 100, H2 all, H4 new), i.e. considering the documents present in at least one ranking.",
                "The fourth row corresponds to the assumption set A4 = (H1 100, H2 5 , H4 init), i.e. keeping the original ranks of successful documents.",
                "The fifth row of Table 10, labeled A5, gives performance when all the 225 queries of the Web track of TREC-2004 are considered.",
                "Obviously, performance level cannot be compared with previous lines since the additional queries are different from the TD queries and correspond to other tasks (Home Page and Named Page tasks [10]) of TREC-2004 Web track.",
                "This set of runs aims to show whether relative performance of the various methods is task-dependent.",
                "The last row of Table 10, labeled A6, reports performance of the various methods considering the TD task of TREC2002 instead of TREC-2004: we fused the results of input rankings of the 10 best official runs for each of the 50 TD queries [9] considering the set of assumptions A1 of the first row.",
                "This aims to show whether relative performance of the various methods changes from year to year.",
                "Values between brackets of Table 10 are variations of performance of each rank aggregation method w.r.t. performance of the outranking approach.",
                "Table 10: Performance (MAP) of different rank aggregation methods under 3 different test collections mcm combSUM combMNZ markov A1 18.79% 17.54% (-6.65%*) 17.08% (-9.10%*) 18.63% (-0.85%) A2 21.36% 19.18% (-10.21%*) 18.61% (-12.87%*) 21.33% (-0.14%) A3 21.92% 21.38% (-2.46%) 20.88% (-4.74%) 19.35% (-11.72%*) A4 18.64% 17.58% (-5.69%*) 17.18% (-7.83%*) 18.63% (-0.05%) A5 55.39% 52.16% (-5.83%*) 49.70% (-10.27%*) 53.30% (-3.77%) A6 16.95% 15.65% (-7.67%*) 14.57% (-14.04%*) 16.39% (-3.30%) From the analysis of table 10 the following can be established: • for all the runs, considering all the documents in each input ranking (A2) significantly improves performance (MAP increases by 11.62% on average).",
                "This is predictable since some initially unreported relevant documents would receive better positions in the consensus ranking. • for all the runs, considering documents even those present in only one input ranking (A3) significantly improves performance.",
                "For mcm, combSUM and combMNZ, performance improvement is more important (MAP increases by 20.27% on average) than for the markov run (MAP increases by 3.86%). • preserving the initial positions of documents (A4) or recomputing them (A1) does not have a noticeable influence on performance for both positional and majoritarian methods. • considering all the queries of the Web track of TREC2004 (A5) as well as the TD queries of the Web track of TREC-2002 (A6) does not alter the relative performance of the different data fusion methods. • considering the TD queries of the Web track of TREC2002, performances of all the data fusion methods are lower than that of the best performing input ranking for which the MAP value equals 18.58%.",
                "This is because most of the fused input rankings have very low performances compared to the best one, which brings more noise to the consensus ranking. • performances of the data fusion methods mcm and markov are significantly better than that of the best input ranking uogWebCAU150.",
                "This remains true for runs combSUM and combMNZ only under assumptions H1 all or H2 all.",
                "This shows that majoritarian methods are less sensitive to assumptions than positional methods. • outranking approach always performs significantly better than positional methods combSUM and combMNZ.",
                "It has also better performances than the Markov chain method, especially under assumption H2 all where difference of performances becomes significant. 6.",
                "CONCLUSIONS In this paper, we address the rank aggregation problem where different, but not disjoint, lists of documents are to be fused.",
                "We noticed that the input rankings can hide ties, so they should not be considered as complete orders.",
                "Only robust information should be used from each input ranking.",
                "Current rank aggregation methods, and especially positional methods (e.g. combSUM [15]), are not initially designed to work with such rankings.",
                "They should be adapted by considering specific working assumptions.",
                "We propose a new outranking method for rank aggregation which is well adapted to the IR context.",
                "Indeed, it ranks two documents w.r.t. the intensity of their positions difference in each input ranking and also considering the number of the input rankings that are concordant and discordant in favor of a specific document.",
                "There is also no need to make specific assumptions on the positions of the missing documents.",
                "This is an important feature since the absence of a document from a ranking should not be necessarily interpreted negatively.",
                "Experimental results show that the outranking method significantly out-performs popular classical positional data fusion methods like combSUM and combMNZ strategies.",
                "It also out-performs a good performing majoritarian methods which is the Markov chain method.",
                "These results are tested against different test collections and queries.",
                "From the experiments, we can also conclude that in order to improve the performances, we should fuse result lists of well performing IR models, and that majoritarian data fusion methods perform better than positional methods.",
                "The proposed method can have a real impact on Web metasearch performances since only ranks are available from most primary search engines, whereas most of the current approaches need scores to merge result lists into one single list.",
                "Further work involves investigating whether the outranking approach performs well in various other contexts, e.g. using the document scores or some combination of document ranks and scores.",
                "Acknowledgments The authors would like to thank Jacques Savoy for his valuable comments on a preliminary version of this paper. 7.",
                "REFERENCES [1] A. Aronson, D. Demner-Fushman, S. Humphrey, J. Lin, H. Liu, P. Ruch, M. Ruiz, L. Smith, L. Tanabe, and W. Wilbur.",
                "Fusion of knowledge-intensive and statistical approaches for retrieving and annotating textual genomics documents.",
                "In Proceedings TREC2005.",
                "NIST Publication, 2005. [2] R. A. Baeza-Yates and B.",
                "A. Ribeiro-Neto.",
                "Modern Information Retrieval.",
                "ACM Press , 1999. [3] B. T. Bartell, G. W. Cottrell, and R. K. Belew.",
                "Automatic combination of multiple ranked retrieval systems.",
                "In Proceedings ACM-SIGIR94, pages 173-181.",
                "Springer-Verlag, 1994. [4] N. J. Belkin, P. Kantor, E. A.",
                "Fox, and J.",
                "A. Shaw.",
                "Combining evidence of multiple query representations for information retrieval.",
                "IPM, 31(3):431-448, 1995. [5] J. Borda.",
                "M´emoire sur les ´elections au scrutin.",
                "Histoire de lAcad´emie des Sciences, 1781. [6] J. P. Callan, Z. Lu, and W. B. Croft.",
                "Searching distributed collections with inference networks.",
                "In Proceedings ACM-SIGIR95, pages 21-28, 1995. [7] M. Condorcet.",
                "Essai sur lapplication de lanalyse `a la probabilit´e des d´ecisions rendues `a la pluralit´e des voix.",
                "Imprimerie Royale, Paris, 1785. [8] W. D. Cook and M. Kress.",
                "Ordinal ranking with intensity of preference.",
                "Management Science, 31(1):26-32, 1985. [9] N. Craswell and D. Hawking.",
                "Overview of the TREC-2002 Web Track.",
                "In Proceedings TREC2002.",
                "NIST Publication, 2002. [10] N. Craswell and D. Hawking.",
                "Overview of the TREC-2004 Web Track.",
                "In Proceedings of TREC2004.",
                "NIST Publication, 2004. [11] C. Dwork, S. R. Kumar, M. Naor, and D. Sivakumar.",
                "Rank aggregation methods for the Web.",
                "In Proceedings WWW2001, pages 613-622, 2001. [12] R. Fagin.",
                "Combining fuzzy information from multiple systems.",
                "JCSS, 58(1):83-99, 1999. [13] R. Fagin, R. Kumar, M. Mahdian, D. Sivakumar, and E. Vee.",
                "Comparing and aggregating rankings with ties.",
                "In PODS, pages 47-58, 2004. [14] R. Fagin, R. Kumar, and D. Sivakumar.",
                "Comparing top k lists.",
                "SIAM J. on Discrete Mathematics, 17(1):134-160, 2003. [15] E. A.",
                "Fox and J.",
                "A. Shaw.",
                "Combination of multiple searches.",
                "In Proceedings of TREC3.",
                "NIST Publication, 1994. [16] J. Katzer, M. McGill, J. Tessier, W. Frakes, and P. DasGupta.",
                "A study of the overlap among document representations.",
                "Information Technology: Research and Development, 1(4):261-274, 1982. [17] L. S. Larkey, M. E. Connell, and J. Callan.",
                "Collection selection and results merging with topically organized U.S. patents and TREC data.",
                "In Proceedings ACM-CIKM2000, pages 282-289.",
                "ACM Press, 2000. [18] A.",
                "Le Calv´e and J. Savoy.",
                "Database merging strategy based on logistic regression.",
                "IPM, 36(3):341-359, 2000. [19] J. H. Lee.",
                "Analyses of multiple evidence combination.",
                "In Proceedings ACM-SIGIR97, pages 267-276, 1997. [20] D. Lillis, F. Toolan, R. Collier, and J. Dunnion.",
                "Probfuse: a probabilistic approach to data fusion.",
                "In Proceedings ACM-SIGIR2006, pages 139-146.",
                "ACM Press, 2006. [21] J. I. Marden.",
                "Analyzing and Modeling Rank Data.",
                "Number 64 in Monographs on Statistics and Applied Probability.",
                "Chapman & Hall, 1995. [22] M. Montague and J.",
                "A. Aslam.",
                "Metasearch consistency.",
                "In Proceedings ACM-SIGIR2001, pages 386-387.",
                "ACM Press, 2001. [23] D. M. Pennock and E. Horvitz.",
                "Analysis of the axiomatic foundations of collaborative filtering.",
                "In Workshop on AI for Electronic Commerce at the 16th National Conference on Artificial Intelligence, 1999. [24] M. E. Renda and U. Straccia.",
                "Web metasearch: rank vs. score based rank aggregation methods.",
                "In Proceedings ACM-SAC2003, pages 841-846.",
                "ACM Press, 2003. [25] W. H. Riker.",
                "Liberalism against populism.",
                "Waveland Press, 1982. [26] B. Roy.",
                "The outranking approach and the foundations of ELECTRE methods.",
                "Theory and Decision, 31:49-73, 1991. [27] B. Roy and J. Hugonnard.",
                "Ranking of suburban line extension projects on the Paris metro system by a multicriteria method.",
                "Transportation Research, 16A(4):301-312, 1982. [28] L. Si and J. Callan.",
                "Using sampled data and regression to merge search engine results.",
                "In Proceedings ACM-SIGIR2002, pages 19-26.",
                "ACM Press, 2002. [29] M. Truchon.",
                "An extension of the Condorcet criterion and Kemeny orders.",
                "Cahier 9813, Centre de Recherche en Economie et Finance Appliqu´ees, Oct. 1998. [30] H. Turtle and W. B. Croft.",
                "Inference networks for document retrieval.",
                "In Proceedings of ACM-SIGIR90, pages 1-24.",
                "ACM Press, 1990. [31] C. C. Vogt and G. W. Cottrell.",
                "Fusion via a linear combination of scores.",
                "Information Retrieval, 1(3):151-173, 1999."
            ],
            "original_annotated_samples": [],
            "translated_annotated_samples": [],
            "translated_text": "",
            "candidates": [],
            "error": []
        },
        "majoritarian method": {
            "translated_key": "método mayoritario",
            "is_in_text": true,
            "original_annotated_sentences": [
                "An Outranking Approach for Rank Aggregation in Information Retrieval Mohamed Farah Lamsade, Paris Dauphine University Place du Mal de Lattre de Tassigny 75775 Paris Cedex 16, France farah@lamsade.dauphine.fr Daniel Vanderpooten Lamsade, Paris Dauphine University Place du Mal de Lattre de Tassigny 75775 Paris Cedex 16, France vdp@lamsade.dauphine.fr ABSTRACT Research in Information Retrieval usually shows performance improvement when many sources of evidence are combined to produce a ranking of documents (e.g., texts, pictures, sounds, etc.).",
                "In this paper, we focus on the rank aggregation problem, also called data fusion problem, where rankings of documents, searched into the same collection and provided by multiple methods, are combined in order to produce a new ranking.",
                "In this context, we propose a rank aggregation method within a multiple criteria framework using aggregation mechanisms based on decision rules identifying positive and negative reasons for judging whether a document should get a better rank than another.",
                "We show that the proposed method deals well with the Information Retrieval distinctive features.",
                "Experimental results are reported showing that the suggested method performs better than the well-known CombSUM and CombMNZ operators.",
                "Categories and Subject Descriptors: H.3.3 [Information Systems]: Information Search and Retrieval - Retrieval models.",
                "General Terms: Algorithms, Measurement, Experimentation, Performance, Theory. 1.",
                "INTRODUCTION A wide range of current Information Retrieval (IR) approaches are based on various search models (Boolean, Vector Space, Probabilistic, Language, etc. [2]) in order to retrieve relevant documents in response to a user request.",
                "The result lists produced by these approaches depend on the exact definition of the relevance concept.",
                "Rank aggregation approaches, also called data fusion approaches, consist in combining these result lists in order to produce a new and hopefully better ranking.",
                "Such approaches give rise to metasearch engines in the Web context.",
                "We consider, in the following, cases where only ranks are available and no other additional information is provided such as the relevance scores.",
                "This corresponds indeed to the reality, where only ordinal information is available.",
                "Data fusion is also relevant in other contexts, such as when the user writes several queries of his/her information need (e.g., a boolean query and a natural language query) [4], or when many document surrogates are available [16].",
                "Several studies argued that rank aggregation has the potential of combining effectively all the various sources of evidence considered in various input methods.",
                "For instance, experiments carried out in [16], [30], [4] and [19] showed that documents which appear in the lists of the majority of the input methods are more likely to be relevant.",
                "Moreover, Lee [19] and Vogt and Cottrell [31] found that various retrieval approaches often return very different irrelevant documents, but many of the same relevant documents.",
                "Bartell et al. [3] also found that rank aggregation methods improve the performances w.r.t. those of the input methods, even when some of them have weak individual performances.",
                "These methods also tend to smooth out biases of the input methods according to Montague and Aslam [22].",
                "Data fusion has recently been proved to improve performances for both the ad-hoc retrieval and categorization tasks within the TREC genomics track in 2005 [1].",
                "The rank aggregation problem was addressed in various fields such as i) in social choice theory which studies voting algorithms which specify winners of elections or winners of competitions in tournaments [29], ii) in statistics when studying correlation between rankings, iii) in distributed databases when results from different databases must be combined [12], and iv) in collaborative filtering [23].",
                "Most current rank aggregation methods consider each input ranking as a permutation over the same set of items.",
                "They also give rigid interpretation to the exact ranking of the items.",
                "Both of these assumptions are rather not valid in the IR context, as will be shown in the following sections.",
                "The remaining of the paper is organized as follows.",
                "We first review current rank aggregation methods in Section 2.",
                "Then we outline the specificities of the data fusion problem in the IR context (Section 3).",
                "In Section 4, we present a new aggregation method which is proven to best fit the IR context.",
                "Experimental results are presented in Section 5 and conclusions are provided in a final section. 2.",
                "RELATED WORK As pointed out by Riker [25], we can distinguish two families of rank aggregation methods: positional methods which assign scores to items to be ranked according to the ranks they receive and majoritarian methods which are based on pairwise comparisons of items to be ranked.",
                "These two families of methods find their roots in the pioneering works of Borda [5] and Condorcet [7], respectively, in the social choice literature. 2.1 Preliminaries We first introduce some basic notations to present the rank aggregation methods in a uniform way.",
                "Let D = {d1, d2, . . . , dnd } be a set of nd documents.",
                "A list or a ranking j is an ordering defined on Dj ⊆ D (j = 1, . . . , n).",
                "Thus, di j di means di is ranked better than di in j.",
                "When Dj = D, j is said to be a full list.",
                "Otherwise, it is a partial list.",
                "If di belongs to Dj, rj i denotes the rank or position of di in j.",
                "We assume that the best answer (document) is assigned the position 1 and the worst one is assigned the position |Dj|.",
                "Let D be the set of all permutations on D or all subsets of D. A profile is a n-tuple of rankings PR = ( 1, 2, . . . , n).",
                "Restricting PR to the rankings containing document di defines PRi.",
                "We also call the number of rankings which contain document di the rank hits of di [19].",
                "The rank aggregation or data fusion problem consists of finding a ranking function or mechanism Ψ (also called a social welfare function in the social choice theory terminology) defined by: Ψ : n D → D PR = ( 1, 2, . . . , n) → σ = Ψ(PR) where σ is called a consensus ranking. 2.2 Positional Methods 2.2.1 Borda Count This method [5] first assigns a score n j=1 rj i to each document di.",
                "Documents are then ranked by increasing order of this score, breaking ties, if any, arbitrarily. 2.2.2 Linear Combination Methods This family of methods basically combine scores of documents.",
                "When used for the rank aggregation problem, ranks are assumed to be scores or performances to be combined using aggregation operators such as the weighted sum or some variation of it [3, 31, 17, 28].",
                "For instance, Callan et al. [6] used the inference networks model [30] to combine rankings.",
                "Fox and Shaw [15] proposed several combination strategies which are CombSUM, CombMIN, CombMAX, CombANZ and CombMNZ.",
                "The first three operators correspond to the sum, min and max operators, respectively.",
                "CombANZ and CombMNZ respectively divides and multiplies the CombSUM score by the rank hits.",
                "It is shown in [19] that the CombSUM and CombMNZ operators perform better than the others.",
                "Metasearch engines such as SavvySearch and MetaCrawler use the CombSUM strategy to fuse rankings. 2.2.3 Footrule Optimal Aggregation In this method, a consensus ranking minimizes the Spearman footrule distance from the input rankings [21].",
                "Formally, given two full lists j and j , this distance is given by F( j, j ) = nd i=1 |rj i − rj i |.",
                "It extends to several lists as follows.",
                "Given a profile PR and a consensus ranking σ, the Spearman footrule distance of σ to PR is given by F(σ, PR) = n j=1 F(σ, j).",
                "Cook and Kress [8] proposed a similar method which consists in optimizing the distance D( j, j ) = 1 2 nd i,i =1 |rj i,i − rj i,i |, where rj i,i = rj i −rj i .",
                "This formulation has the advantage that it considers the intensity of preferences. 2.2.4 Probabilistic Methods This kind of methods assume that the performance of the input methods on a number of training queries is indicative of their future performance.",
                "During the training process, probabilities of relevance are calculated.",
                "For subsequent queries, documents are ranked based on these probabilities.",
                "For instance, in [20], each input ranking j is divided into a number of segments, and the conditional probability of relevance (R) of each document di depending on the segment k it occurs in, is computed, i.e. prob(R|di, k, j).",
                "For subsequent queries, the score of each document di is given by n j=1 prob(R|di,k, j ) k .",
                "Le Calve and Savoy [18] suggest using a logistic regression approach for combining scores.",
                "Training data is needed to infer the model parameters. 2.3 Majoritarian Methods 2.3.1 Condorcet Procedure The original Condorcet rule [7] specifies that a winner of the election is any item that beats or ties with every other item in a pairwise contest.",
                "Formally, let C(diσdi ) = { j∈ PR : di j di } be the coalition of rankings that are concordant with establishing diσdi , i.e. with the proposition di should be ranked better than di in the final ranking σ. di beats or ties with di iff |C(diσdi )| ≥ |C(di σdi)|.",
                "The repetitive application of the Condorcet algorithm can produce a ranking of items in a natural way: select the Condorcet winner, remove it from the lists, and repeat the previous two steps until there are no more documents to rank.",
                "Since there is not always Condorcet winners, variations of the Condorcet procedure have been developed within the multiple criteria decision aid theory, with methods such as ELECTRE [26]. 2.3.2 Kemeny Optimal Aggregation As in section 2.2.3, a consensus ranking minimizes a geometric distance from the input rankings, where the Kendall tau distance is used instead of the Spearman footrule distance.",
                "Formally, given two full lists j and j , the Kendall tau distance is given by K( j, j ) = |{(di, di ) : i < i , rj i < rj i , rj i > rj i }|, i.e. the number of pairwise disagreements between the two lists.",
                "It is easy to show that the consensus ranking corresponds to the geometric median of the input rankings and that the Kemeny optimal aggregation problem corresponds to the minimum feedback edge set problem. 2.3.3 Markov Chain Methods Markov chains (MCs) have been used by Dwork et al. [11] as a natural method to obtain a consensus ranking where states correspond to the documents to be ranked and the transition probabilities vary depending on the interpretation of the transition event.",
                "In the same reference, the authors proposed four specific MCs and experimental testing had shown that the following MC is the best performing one (see also [24]): • MC4: move from the current state di to the next state di by first choosing a document di uniformly from D. If for the majority of the rankings, we have rj i ≤ rj i , then move to di , else stay in di.",
                "The consensus ranking corresponds to the stationary distribution of MC4. 3.",
                "SPECIFICITIES OF THE RANK AGGREGATION PROBLEM IN THE IR CONTEXT 3.1 Limited Significance of the Rankings The exact positions of documents in one input ranking have limited significance and should not be overemphasized.",
                "For instance, having three relevant documents in the first three positions, any perturbation of these three items will have the same value.",
                "Indeed, in the IR context, the complete order provided by an input method may hide ties.",
                "In this case, we call such rankings semi orders.",
                "This was outlined in [13] as the problem of aggregation with ties.",
                "It is therefore important to build the consensus ranking based on robust information: • Documents with near positions in j are more likely to have similar interest or relevance.",
                "Thus a slight perturbation of the initial ranking is meaningless. • Assuming that document di is better ranked than document di in a ranking j, di is more likely to be definitively more relevant than di in j when the number of intermediate positions between di and di increases. 3.2 Partial Lists In real world applications, such as metasearch engines, rankings provided by the input methods are often partial lists.",
                "This was outlined in [14] as the problem of having to merge top-k results from various input lists.",
                "For instance, in the experiments carried out by Dwork et al. [11], authors found that among the top 100 best documents of 7 input search engines, 67% of the documents were present in only one search engine, whereas less than two documents were present in all the search engines.",
                "Rank aggregation of partial lists raises four major difficulties which we state hereafter, proposing for each of them various working assumptions: 1.",
                "Partial lists can have various lengths, which can favour long lists.",
                "We thus consider the following two working hypotheses: H1 k : We only consider the top k best documents from each input ranking.",
                "H1 all: We consider all the documents from each input ranking. 2.",
                "Since there are different documents in the input rankings, we must decide which documents should be kept in the consensus ranking.",
                "Two working hypotheses are therefore considered: H2 k : We only consider documents which are present in at least k input rankings (k > 1).",
                "H2 all: We consider all the documents which are ranked in at least one input ranking.",
                "Hereafter, we call documents which will be retained in the consensus ranking, candidate documents, and documents that will be excluded from the consensus ranking, excluded documents.",
                "We also call a candidate document which is missing in one or more rankings, a missing document. 3.",
                "Some candidate documents are missing documents in some input rankings.",
                "Main reasons for a missing document are that it was not indexed or it was indexed but deemed irrelevant ; usually this information is not available.",
                "We consider the following two working hypotheses: H3 yes: Each missing document in each j is assigned a position.",
                "H3 no: No assumption is made, that is each missing document is considered neither better nor worse than any other document. 4.",
                "When assumption H2 k holds, each input ranking may contain documents which will not be considered in the consensus ranking.",
                "Regarding the positions of the candidate documents, we can consider the following working hypotheses: H4 init: The initial positions of candidate documents are kept in each input ranking.",
                "H4 new: Candidate documents receive new positions in each input ranking, after discarding excluded ones.",
                "In the IR context, rank aggregation methods need to decide more or less explicitly which assumptions to retain w.r.t. the above-mentioned difficulties. 4.",
                "OUTRANKING APPROACH FOR RANK AGGREGATION 4.1 Presentation Positional methods consider implicitly that the positions of the documents in the input rankings are scores giving thus a cardinal meaning to an ordinal information.",
                "This constitutes a strong assumption that is questionable, especially when the input rankings have different lengths.",
                "Moreover, for positional methods, assumptions H3 and H4 , which are often arbitrary, have a strong impact on the results.",
                "For instance, let us consider an input ranking of 500 documents out of 1000 candidate documents.",
                "Whether we assign to each of the missing documents the position 1, 501, 750 or 1000 -corresponding to variations of H3 yes- will give rise to very contrasted results, especially regarding the top of the consensus ranking.",
                "Majoritarian methods do not suffer from the above-mentioned drawbacks of the positional methods since they build consensus rankings exploiting only ordinal information contained in the input rankings.",
                "Nevertheless, they suppose that such rankings are complete orders, ignoring that they may hide ties.",
                "Therefore, majoritarian methods base consensus rankings on illusory discriminant information rather than less discriminant but more robust information.",
                "Trying to overcome the limits of current rank aggregation methods, we found that outranking approaches, which were initially used for multiple criteria aggregation problems [26], can also be used for the rank aggregation purpose, where each ranking plays the role of a criterion.",
                "Therefore, in order to decide whether a document di should be ranked better than di in the consensus ranking σ, the two following conditions should be met: • a concordance condition which ensures that a majority of the input rankings are concordant with diσdi (majority principle). • a discordance condition which ensures that none of the discordant input rankings strongly refutes dσd (respect of minorities principle).",
                "Formally, the concordance coalition with diσdi is Csp (diσdi ) = { j∈ PR : rj i ≤ rj i − sp} where sp is a preference threshold which is the variation of document positions -whether it is absolute or relative to the ranking length- which draws the boundaries between an indifference and a preference situation between documents.",
                "The discordance coalition with diσdi is Dsv (diσdi ) = { j∈ PR : rj i ≥ rj i + sv} where sv is a veto threshold which is the variation of document positions -whether it is absolute or relative to the ranking length- which draws the boundaries between a weak and a strong opposition to diσdi .",
                "Depending on the exact definition of the preceding concordance and discordance coalitions leading to the definition of some decision rules, several outranking relations can be defined.",
                "They can be more or less demanding depending on i) the values of the thresholds sp and sv, ii) the importance or minimal size cmin required for the concordance coalition, and iii) the importance or maximum size dmax of the discordance coalition.",
                "A generic outranking relation can thus be defined as follows: diS(sp,sv,cmin,dmax)di ⇔ |Csp (diσdi )| ≥ cmin AND |Dsv (diσdi )| ≤ dmax This expression defines a family of nested outranking relations since S(sp,sv,cmin,dmax) ⊆ S(sp,sv,cmin,dmax) when cmin ≥ cmin and/or dmax ≤ dmax and/or sp ≥ sp and/or sv ≤ sv.",
                "This expression also generalizes the majority rule which corresponds to the particular relation S(0,∞, n 2 ,n).",
                "It also satisfies important properties of rank aggregation methods, called neutrality, Pareto-optimality, Condorcet property and Extended Condorcet property, in the social choice literature [29].",
                "Outranking relations are not necessarily transitive and do not necessarily correspond to rankings since directed cycles may exist.",
                "Therefore, we need specific procedures in order to derive a consensus ranking.",
                "We propose the following procedure which finds its roots in [27].",
                "It consists in partitioning the set of documents into r ranked classes.",
                "Each class Ch contains documents with the same relevance and results from the application of all relations (if possible) to the set of documents remaining after previous classes are computed.",
                "Documents within the same equivalence class are ranked arbitrarily.",
                "Formally, let • R be the set of candidate documents for a query, • S1 , S2 , . . . be a family of nested outranking relations, • Fk(di, E) = |{di ∈ E : diSk di }| be the number of documents in E(E ⊆ R) that could be considered worse than di according to relation Sk , • fk(di, E) = |{di ∈ E : di Sk di}| be the number of documents in E that could be considered better than di according to Sk , • sk(di, E) = Fk(di, E) − fk(di, E) be the qualification of di in E according to Sk .",
                "Each class Ch results from a distillation process.",
                "It corresponds to the last distillate of a series of sets E0 ⊇ E1 ⊇ . . . where E0 = R \\ (C1 ∪ . . . ∪ Ch−1) and Ek is a reduced subset of Ek−1 resulting from the application of the following procedure: 1. compute for each di ∈ Ek−1 its qualification according to Sk , i.e. sk(di, Ek−1), 2. define smax = maxdi∈Ek−1 {sk(di, Ek−1)}, then 3.",
                "Ek = {di ∈ Ek−1 : sk(di, Ek−1) = smax} When one outranking relation is used, the distillation process stops after the first application of the previous procedure, i.e., Ch corresponds to distillate E1.",
                "When different outranking relations are used, the distillation process stops when all the pre-defined outranking relations have been used or when |Ek| = 1. 4.2 Illustrative Example This section illustrates the concepts and procedures of section 4.1.",
                "Let us consider a set of candidate documents R = {d1, d2, d3, d4, d5}.",
                "The following table gives a profile PR of different rankings of the documents of R: PR = ( 1 , 2, 3, 4).",
                "Table 1: Rankings of documents rj i 1 2 3 4 d1 1 3 1 5 d2 2 1 3 3 d3 3 2 2 1 d4 4 4 5 2 d5 5 5 4 4 Let us suppose that the preference and veto thresholds are set to values 1 and 4 respectively, and that the concordance and discordance thresholds are set to values 2 and 1 respectively.",
                "The following tables give the concordance, discordance and outranking matrices.",
                "Each entry csp (di, di ) (dsv (di, di )) in the concordance (discordance) matrix gives the number of rankings that are concordant (discordant) with diσdi , i.e. csp (di, di ) = |Csp (diσdi )| and dsv (di, di ) = |Dsv (diσdi )|.",
                "Table 2: Computation of the outranking relation d1 d2 d3 d4 d5 d1 - 2 2 3 3 d2 2 - 2 3 4 d3 2 2 - 4 4 d4 1 1 0 - 3 d5 1 0 0 1Concordance Matrix d1 d2 d3 d4 d5 d1 - 0 1 0 0 d2 0 - 0 0 0 d3 0 0 - 0 0 d4 1 0 0 - 0 d5 1 1 0 0Discordance Matrix d1 d2 d3 d4 d5 d1 - 1 1 1 1 d2 1 - 1 1 1 d3 1 1 - 1 1 d4 0 0 0 - 1 d5 0 0 0 0Outranking Matrix (S1) For instance, the concordance coalition for the assertion d1σd4 is C1(d1σd4) = { 1, 2, 3} and the discordance coalition for the same assertion is D4(d1σd4) = ∅.",
                "Therefore, c1(d1, d4) = 3, d4(d1, d4) = 0 and d1S1 d4 holds.",
                "Notice that Fk(di, R) (fk(di, R)) is given by summing the values of the ith row (column) of the outranking matrix.",
                "The consensus ranking is obtained as follows: to get the first class C1, we compute the qualifications of all the documents of E0 = R with respect to S1 .",
                "They are respectively 2, 2, 2, -2 and -4.",
                "Therefore smax equals 2 and C1 = E1 = {d1, d2, d3}.",
                "Observe that, if we had used a second outranking relation S2(⊇ S1), these three documents could have been possibly discriminated.",
                "At this stage, we remove documents of C1 from the outranking matrix and compute the next class C2: we compute the new qualifications of the documents of E0 = R \\ C1 = {d4, d5}.",
                "They are respectively 1 and -1.",
                "So C3 = E1 = {d4}.",
                "The last document d5 is the only document of the last class C3.",
                "Thus, the consensus ranking is {d1, d2, d3} → {d4} → {d5}. 5.",
                "EXPERIMENTS AND RESULTS 5.1 Test Setting To facilitate empirical investigation of the proposed methodology, we developed a prototype metasearch engine that implements a version of our outranking approach for rank aggregation.",
                "In this paper, we apply our approach to the Topic Distillation (TD) task of TREC-2004 Web track [10].",
                "In this task, there are 75 topics where only a short description of each is given.",
                "For each query, we retained the rankings of the 10 best runs of the TD task which are provided by TREC-2004 participating teams.",
                "The performances of these runs are reported in table 3.",
                "Table 3: Performances of the 10 best runs of the TD task of TREC-2004 Run Id MAP P@10 S@1 S@5 S@10 uogWebCAU150 17.9% 24.9% 50.7% 77.3% 89.3% MSRAmixed1 17.8% 25.1% 38.7% 72.0% 88.0% MSRC04C12 16.5% 23.1% 38.7% 74.7% 80.0% humW04rdpl 16.3% 23.1% 37.3% 78.7% 90.7% THUIRmix042 14.7% 20.5% 21.3% 58.7% 74.7% UAmsT04MWScb 14.6% 20.9% 36.0% 66.7% 76.0% ICT04CIIS1AT 14.1% 20.8% 33.3% 64.0% 78.7% SJTUINCMIX5 12.9% 18.9% 29.3% 57.3% 72.0% MU04web1 11.5% 19.9% 33.3% 64.0% 76.0% MeijiHILw3 11.5% 15.3% 30.7% 54.7% 64.0% Average 14.7% 21.2% 34.9% 66.8% 78.94% For each query, each run provides a ranking of about 1000 documents.",
                "The number of documents retrieved by all these runs ranges from 543 to 5769.",
                "Their average (median) number is 3340 (3386).",
                "It is worth noting that we found similar distributions of the documents among the rankings as in [11].",
                "For evaluation, we used the trec eval standard tool which is used by the TREC community to calculate the standard measures of system effectiveness which are Mean Average Precision (MAP) and Success@n (S@n) for n=1, 5 and 10.",
                "Our approach effectiveness is compared against some high performing official results from TREC-2004 as well as against some standard rank aggregation algorithms.",
                "In the experiments, significance testing is mainly based on the t-student statistic which is computed on the basis of the MAP values of the compared runs.",
                "In the tables of the following section, statistically significant differences are marked with an asterisk.",
                "Values between brackets of the first column of each table, indicate the parameter value of the corresponding run. 5.2 Results We carried out several series of runs in order to i) study performance variations of the outranking approach when tuning the parameters and working assumptions, ii) compare performances of the outranking approach vs standard rank aggregation strategies , and iii) check whether rank aggregation performs better than the best input rankings.",
                "We set our basic run mcm with the following parameters.",
                "We considered that each input ranking is a complete order (sp = 0) and that an input ranking strongly refutes diσdi when the difference of both document positions is large enough (sv = 75%).",
                "Preference and veto thresholds are computed proportionally to the number of documents retained in each input ranking.",
                "They consequently may vary from one ranking to another.",
                "In addition, to accept the assertion diσdi , we supposed that the majority of the rankings must be concordant (cmin = 50%) and that every input ranking can impose its veto (dmax = 0).",
                "Concordance and discordance thresholds are computed for each tuple (di, di ) as the percentage of the input rankings of PRi ∩PRi .",
                "Thus, our choice of parameters leads to the definition of the outranking relation S(0,75%,50%,0).",
                "To test the run mcm, we had chosen the following assumptions.",
                "We retained the top 100 best documents from each input ranking (H1 100), only considered documents which are present in at least half of the input rankings (H2 5 ) and assumed H3 no and H4 new.",
                "In these conditions, the number of successful documents was about 100 on average, and the computation time per query was less than one second.",
                "Obviously, modifying the working assumptions should have deeper impact on the performances than tuning our model parameters.",
                "This was validated by preliminary experiments.",
                "Thus, we hereafter begin by studying performance variation when different sets of assumptions are considered.",
                "Afterwards, we study the impact of tuning parameters.",
                "Finally, we compare our model performances w.r.t. the input rankings as well as some standard data fusion algorithms. 5.2.1 Impact of the Working Assumptions Table 4 summarizes the performance variation of the outranking approach under different working hypotheses.",
                "In Table 4: Impact of the working assumptions Run Id MAP S@1 S@5 S@10 mcm 18.47% 41.33% 81.33% 86.67% mcm22 (H3 yes) 17.72% (-4.06%) 34.67% 81.33% 86.67% mcm23 (H4 init) 18.26% (-1.14%) 41.33% 81.33% 86.67% mcm24 (H1 all) 20.67% (+11.91%*) 38.66% 80.00% 86.66% mcm25 (H2 all) 21.68% (+17.38%*) 40.00% 78.66% 89.33% this table, we first show that run mcm22, in which missing documents are all put in the same last position of each input ranking, leads to performance drop w.r.t. run mcm.",
                "Moreover, S@1 moves from 41.33% to 34.67% (-16.11%).",
                "This shows that several relevant documents which were initially put at the first position of the consensus ranking in mcm, lose this first position but remain ranked in the top 5 documents since S@5 did not change.",
                "We also conclude that documents which have rather good positions in some input rankings are more likely to be relevant, even though they are missing in some other rankings.",
                "Consequently, when they are missing in some rankings, assigning worse ranks to these documents is harmful for performance.",
                "Also, from Table 4, we found that the performances of runs mcm and mcm23 are similar.",
                "Therefore, the outranking approach is not sensitive to keeping the initial positions of candidate documents or recomputing them by discarding excluded ones.",
                "From the same Table 4, performance of the outranking approach increases significantly for runs mcm24 and mcm25.",
                "Therefore, whether we consider all the documents which are present in half of the rankings (mcm24) or we consider all the documents which are ranked in the first 100 positions in one or more rankings (mcm25), increases performances.",
                "This result was predictable since in both cases we have more detailed information on the relative importance of documents.",
                "Tables 5 and 6 confirm this evidence.",
                "Table 5, where values between brackets of the first column give the number of documents which are retained from each input ranking, shows that selecting more documents from each input ranking leads to performance increase.",
                "It is worth mentioning that selecting more than 600 documents from each input ranking does not improve performance.",
                "Table 5: Impact of the number of retained documents Run Id MAP S@1 S@5 S@10 mcm (100) 18.47% 41.33% 81.33% 86.67% mcm24-1 (200) 19.32% (+4.60%) 42.67% 78.67% 88.00% mcm24-2 (400) 19.88% (+7.63%*) 37.33% 80.00% 88.00% mcm24-3 (600) 20.80% (+12.62%*) 40.00% 80.00% 88.00% mcm24-4 (800) 20.66% (+11.86%*) 40.00% 78.67% 86.67% mcm24 (1000) 20.67% (+11.91%*) 38.66% 80.00% 86.66% Table 6 reports runs corresponding to variations of H2 k .",
                "Values between brackets are rank hits.",
                "For instance, in the run mcm32, only documents which are present in 3 or more input rankings, were considered successful.",
                "This table shows that performance is significantly better when rare documents are considered, whereas it decreases significantly when these documents are discarded.",
                "Therefore, we conclude that many of the relevant documents are retrieved by a rather small set of IR models.",
                "Table 6: Performance considering different rank hits Run Id MAP S@1 S@5 S@10 mcm25 (1) 21.68% (+17.38%*) 40.00% 78.67% 89.33% mcm32 (3) 18.98% (+2.76%) 38.67% 80.00% 85.33% mcm (5) 18.47% 41.33% 81.33% 86.67% mcm33 (7) 15.83% (-14.29%*) 37.33% 78.67% 85.33% mcm34 (9) 10.96% (-40.66%*) 36.11% 66.67% 70.83% mcm35 (10) 7.42% (-59.83%*) 39.22% 62.75% 64.70% For both runs mcm24 and mcm25, the number of successful documents was about 1000 and therefore, the computation time per query increased and became around 5 seconds. 5.2.2 Impact of the Variation of the Parameters Table 7 shows performance variation of the outranking approach when different preference thresholds are considered.",
                "We found performance improvement up to threshold values of about 5%, then there is a decrease in the performance which becomes significant for threshold values greater than 10%.",
                "Moreover, S@1 improves from 41.33% to 46.67% when preference threshold changes from 0 to 5%.",
                "We can thus conclude that the input rankings are semi orders rather than complete orders.",
                "Table 8 shows the evolution of the performance measures w.r.t. the concordance threshold.",
                "We can conclude that in order to put document di before di in the consensus ranking, Table 7: Impact of the variation of the preference threshold from 0 to 12.5% Run Id MAP S@1 S@5 S@10 mcm (0%) 18.47% 41.33% 81.33% 86.67% mcm1 (1%) 18.57% (+0.54%) 41.33% 81.33% 86.67% mcm2 (2.5%) 18.63% (+0.87%) 42.67% 78.67% 86.67% mcm3 (5%) 18.69% (+1.19%) 46.67% 81.33% 86.67% mcm4 (7.5%) 18.24% (-1.25%) 46.67% 81.33% 86.67% mcm5 (10%) 17.93% (-2.92%) 40.00% 82.67% 86.67% mcm5b (12.5%) 17.51% (-5.20%*) 41.33% 80.00% 86.67% at least half of the input rankings of PRi ∩ PRi should be concordant.",
                "Performance drops significantly for very low and very high values of the concordance threshold.",
                "In fact, for such values, the concordance condition is either fulfilled rather always by too many document pairs or not fulfilled at all, respectively.",
                "Therefore, the outranking relation becomes either too weak or too strong respectively.",
                "Table 8: Impact of the variation of cmin Run Id MAP S@1 S@5 S@10 mcm11 (20%) 17.63% (-4.55%*) 41.33% 76.00% 85.33% mcm12 (40%) 18.37% (-0.54%) 42.67% 76.00% 86.67% mcm (50%) 18.47% 41.33% 81.33% 86.67% mcm13 (60%) 18.42% (-0.27%) 40.00% 78.67% 86.67% mcm14 (80%) 17.43% (-5.63%*) 40.00% 78.67% 86.67% mcm15 (100%) 16.12% (-12.72%*) 41.33% 70.67% 85.33% In the experiments, varying the veto threshold as well as the discordance threshold within reasonable intervals does not have significant impact on performance measures.",
                "In fact, runs with different veto thresholds (sv ∈ [50%; 100%]) had similar performances even though there is a slight advantage for runs with high threshold values which means that it is better not to allow the input rankings to put their veto easily.",
                "Also, tuning the discordance threshold was carried out for values 50% and 75% of the veto threshold.",
                "For these runs we did not get any noticeable performance variation, although for low discordance thresholds (dmax < 20%), performance slightly decreased. 5.2.3 Impact of the Variation of the Number of Input Rankings To study performance evolution when different sets of input rankings are considered, we carried three more runs where 2, 4, and 6 of the best performing sets of the input rankings are considered.",
                "Results reported in Table 9 are seemingly counter-intuitive and also do not support previous findings regarding rank aggregation research [3].",
                "Nevertheless, this result shows that low performing rankings bring more noise than information to the establishment of the consensus ranking.",
                "Therefore, when they are considered, performance decreases.",
                "Table 9: Performance considering different best performing sets of input rankings Run Id MAP S@1 S@5 S@10 mcm (10) 18.47% 41.33% 81.33% 86.67% mcm27 (6) 18.60% (+0.70%) 41.33% 80.00% 85.33% mcm28 (4) 19.02% (+2.98%) 40.00% 86.67% 88.00% mcm29 (2) 18.33% (-0.76%) 44.00% 76.00% 88.00% 5.2.4 Comparison of the Performance of Different Rank Aggregation Methods In this set of runs, we compare the outranking approach with some standard rank aggregation methods which were proven to have acceptable performance in previous studies: we considered two positional methods which are the CombSUM and the CombMNZ strategies.",
                "We also examined the performance of one <br>majoritarian method</br> which is the Markov chain method (MC4).",
                "For the comparisons, we considered a specific outranking relation S∗ = S(5%,50%,50%,30%) which results in good overall performances when tuning all the parameters.",
                "The first row of Table 10 gives performances of the rank aggregation methods w.r.t. a basic assumption set A1 = (H1 100, H2 5 , H4 new): we only consider the 100 first documents from each ranking, then retain documents present in 5 or more rankings and update ranks of successful documents.",
                "For positional methods, we place missing documents at the queue of the ranking (H3 yes) whereas for our method as well as for MC4, we retained hypothesis H3 no.",
                "The three following rows of Table 10 report performances when changing one element from the basic assumption set: the second row corresponds to the assumption set A2 = (H1 1000, H2 5 , H4 new), i.e. changing the number of retained documents from 100 to 1000.",
                "The third row corresponds to the assumption set A3 = (H1 100, H2 all, H4 new), i.e. considering the documents present in at least one ranking.",
                "The fourth row corresponds to the assumption set A4 = (H1 100, H2 5 , H4 init), i.e. keeping the original ranks of successful documents.",
                "The fifth row of Table 10, labeled A5, gives performance when all the 225 queries of the Web track of TREC-2004 are considered.",
                "Obviously, performance level cannot be compared with previous lines since the additional queries are different from the TD queries and correspond to other tasks (Home Page and Named Page tasks [10]) of TREC-2004 Web track.",
                "This set of runs aims to show whether relative performance of the various methods is task-dependent.",
                "The last row of Table 10, labeled A6, reports performance of the various methods considering the TD task of TREC2002 instead of TREC-2004: we fused the results of input rankings of the 10 best official runs for each of the 50 TD queries [9] considering the set of assumptions A1 of the first row.",
                "This aims to show whether relative performance of the various methods changes from year to year.",
                "Values between brackets of Table 10 are variations of performance of each rank aggregation method w.r.t. performance of the outranking approach.",
                "Table 10: Performance (MAP) of different rank aggregation methods under 3 different test collections mcm combSUM combMNZ markov A1 18.79% 17.54% (-6.65%*) 17.08% (-9.10%*) 18.63% (-0.85%) A2 21.36% 19.18% (-10.21%*) 18.61% (-12.87%*) 21.33% (-0.14%) A3 21.92% 21.38% (-2.46%) 20.88% (-4.74%) 19.35% (-11.72%*) A4 18.64% 17.58% (-5.69%*) 17.18% (-7.83%*) 18.63% (-0.05%) A5 55.39% 52.16% (-5.83%*) 49.70% (-10.27%*) 53.30% (-3.77%) A6 16.95% 15.65% (-7.67%*) 14.57% (-14.04%*) 16.39% (-3.30%) From the analysis of table 10 the following can be established: • for all the runs, considering all the documents in each input ranking (A2) significantly improves performance (MAP increases by 11.62% on average).",
                "This is predictable since some initially unreported relevant documents would receive better positions in the consensus ranking. • for all the runs, considering documents even those present in only one input ranking (A3) significantly improves performance.",
                "For mcm, combSUM and combMNZ, performance improvement is more important (MAP increases by 20.27% on average) than for the markov run (MAP increases by 3.86%). • preserving the initial positions of documents (A4) or recomputing them (A1) does not have a noticeable influence on performance for both positional and majoritarian methods. • considering all the queries of the Web track of TREC2004 (A5) as well as the TD queries of the Web track of TREC-2002 (A6) does not alter the relative performance of the different data fusion methods. • considering the TD queries of the Web track of TREC2002, performances of all the data fusion methods are lower than that of the best performing input ranking for which the MAP value equals 18.58%.",
                "This is because most of the fused input rankings have very low performances compared to the best one, which brings more noise to the consensus ranking. • performances of the data fusion methods mcm and markov are significantly better than that of the best input ranking uogWebCAU150.",
                "This remains true for runs combSUM and combMNZ only under assumptions H1 all or H2 all.",
                "This shows that majoritarian methods are less sensitive to assumptions than positional methods. • outranking approach always performs significantly better than positional methods combSUM and combMNZ.",
                "It has also better performances than the Markov chain method, especially under assumption H2 all where difference of performances becomes significant. 6.",
                "CONCLUSIONS In this paper, we address the rank aggregation problem where different, but not disjoint, lists of documents are to be fused.",
                "We noticed that the input rankings can hide ties, so they should not be considered as complete orders.",
                "Only robust information should be used from each input ranking.",
                "Current rank aggregation methods, and especially positional methods (e.g. combSUM [15]), are not initially designed to work with such rankings.",
                "They should be adapted by considering specific working assumptions.",
                "We propose a new outranking method for rank aggregation which is well adapted to the IR context.",
                "Indeed, it ranks two documents w.r.t. the intensity of their positions difference in each input ranking and also considering the number of the input rankings that are concordant and discordant in favor of a specific document.",
                "There is also no need to make specific assumptions on the positions of the missing documents.",
                "This is an important feature since the absence of a document from a ranking should not be necessarily interpreted negatively.",
                "Experimental results show that the outranking method significantly out-performs popular classical positional data fusion methods like combSUM and combMNZ strategies.",
                "It also out-performs a good performing majoritarian methods which is the Markov chain method.",
                "These results are tested against different test collections and queries.",
                "From the experiments, we can also conclude that in order to improve the performances, we should fuse result lists of well performing IR models, and that majoritarian data fusion methods perform better than positional methods.",
                "The proposed method can have a real impact on Web metasearch performances since only ranks are available from most primary search engines, whereas most of the current approaches need scores to merge result lists into one single list.",
                "Further work involves investigating whether the outranking approach performs well in various other contexts, e.g. using the document scores or some combination of document ranks and scores.",
                "Acknowledgments The authors would like to thank Jacques Savoy for his valuable comments on a preliminary version of this paper. 7.",
                "REFERENCES [1] A. Aronson, D. Demner-Fushman, S. Humphrey, J. Lin, H. Liu, P. Ruch, M. Ruiz, L. Smith, L. Tanabe, and W. Wilbur.",
                "Fusion of knowledge-intensive and statistical approaches for retrieving and annotating textual genomics documents.",
                "In Proceedings TREC2005.",
                "NIST Publication, 2005. [2] R. A. Baeza-Yates and B.",
                "A. Ribeiro-Neto.",
                "Modern Information Retrieval.",
                "ACM Press , 1999. [3] B. T. Bartell, G. W. Cottrell, and R. K. Belew.",
                "Automatic combination of multiple ranked retrieval systems.",
                "In Proceedings ACM-SIGIR94, pages 173-181.",
                "Springer-Verlag, 1994. [4] N. J. Belkin, P. Kantor, E. A.",
                "Fox, and J.",
                "A. Shaw.",
                "Combining evidence of multiple query representations for information retrieval.",
                "IPM, 31(3):431-448, 1995. [5] J. Borda.",
                "M´emoire sur les ´elections au scrutin.",
                "Histoire de lAcad´emie des Sciences, 1781. [6] J. P. Callan, Z. Lu, and W. B. Croft.",
                "Searching distributed collections with inference networks.",
                "In Proceedings ACM-SIGIR95, pages 21-28, 1995. [7] M. Condorcet.",
                "Essai sur lapplication de lanalyse `a la probabilit´e des d´ecisions rendues `a la pluralit´e des voix.",
                "Imprimerie Royale, Paris, 1785. [8] W. D. Cook and M. Kress.",
                "Ordinal ranking with intensity of preference.",
                "Management Science, 31(1):26-32, 1985. [9] N. Craswell and D. Hawking.",
                "Overview of the TREC-2002 Web Track.",
                "In Proceedings TREC2002.",
                "NIST Publication, 2002. [10] N. Craswell and D. Hawking.",
                "Overview of the TREC-2004 Web Track.",
                "In Proceedings of TREC2004.",
                "NIST Publication, 2004. [11] C. Dwork, S. R. Kumar, M. Naor, and D. Sivakumar.",
                "Rank aggregation methods for the Web.",
                "In Proceedings WWW2001, pages 613-622, 2001. [12] R. Fagin.",
                "Combining fuzzy information from multiple systems.",
                "JCSS, 58(1):83-99, 1999. [13] R. Fagin, R. Kumar, M. Mahdian, D. Sivakumar, and E. Vee.",
                "Comparing and aggregating rankings with ties.",
                "In PODS, pages 47-58, 2004. [14] R. Fagin, R. Kumar, and D. Sivakumar.",
                "Comparing top k lists.",
                "SIAM J. on Discrete Mathematics, 17(1):134-160, 2003. [15] E. A.",
                "Fox and J.",
                "A. Shaw.",
                "Combination of multiple searches.",
                "In Proceedings of TREC3.",
                "NIST Publication, 1994. [16] J. Katzer, M. McGill, J. Tessier, W. Frakes, and P. DasGupta.",
                "A study of the overlap among document representations.",
                "Information Technology: Research and Development, 1(4):261-274, 1982. [17] L. S. Larkey, M. E. Connell, and J. Callan.",
                "Collection selection and results merging with topically organized U.S. patents and TREC data.",
                "In Proceedings ACM-CIKM2000, pages 282-289.",
                "ACM Press, 2000. [18] A.",
                "Le Calv´e and J. Savoy.",
                "Database merging strategy based on logistic regression.",
                "IPM, 36(3):341-359, 2000. [19] J. H. Lee.",
                "Analyses of multiple evidence combination.",
                "In Proceedings ACM-SIGIR97, pages 267-276, 1997. [20] D. Lillis, F. Toolan, R. Collier, and J. Dunnion.",
                "Probfuse: a probabilistic approach to data fusion.",
                "In Proceedings ACM-SIGIR2006, pages 139-146.",
                "ACM Press, 2006. [21] J. I. Marden.",
                "Analyzing and Modeling Rank Data.",
                "Number 64 in Monographs on Statistics and Applied Probability.",
                "Chapman & Hall, 1995. [22] M. Montague and J.",
                "A. Aslam.",
                "Metasearch consistency.",
                "In Proceedings ACM-SIGIR2001, pages 386-387.",
                "ACM Press, 2001. [23] D. M. Pennock and E. Horvitz.",
                "Analysis of the axiomatic foundations of collaborative filtering.",
                "In Workshop on AI for Electronic Commerce at the 16th National Conference on Artificial Intelligence, 1999. [24] M. E. Renda and U. Straccia.",
                "Web metasearch: rank vs. score based rank aggregation methods.",
                "In Proceedings ACM-SAC2003, pages 841-846.",
                "ACM Press, 2003. [25] W. H. Riker.",
                "Liberalism against populism.",
                "Waveland Press, 1982. [26] B. Roy.",
                "The outranking approach and the foundations of ELECTRE methods.",
                "Theory and Decision, 31:49-73, 1991. [27] B. Roy and J. Hugonnard.",
                "Ranking of suburban line extension projects on the Paris metro system by a multicriteria method.",
                "Transportation Research, 16A(4):301-312, 1982. [28] L. Si and J. Callan.",
                "Using sampled data and regression to merge search engine results.",
                "In Proceedings ACM-SIGIR2002, pages 19-26.",
                "ACM Press, 2002. [29] M. Truchon.",
                "An extension of the Condorcet criterion and Kemeny orders.",
                "Cahier 9813, Centre de Recherche en Economie et Finance Appliqu´ees, Oct. 1998. [30] H. Turtle and W. B. Croft.",
                "Inference networks for document retrieval.",
                "In Proceedings of ACM-SIGIR90, pages 1-24.",
                "ACM Press, 1990. [31] C. C. Vogt and G. W. Cottrell.",
                "Fusion via a linear combination of scores.",
                "Information Retrieval, 1(3):151-173, 1999."
            ],
            "original_annotated_samples": [],
            "translated_annotated_samples": [
                "También examinamos el rendimiento de un \"método mayoritario\", que es el método de cadena de Markov (MC4)."
            ],
            "translated_text": "",
            "candidates": [
                "método mayoritario",
                "método mayoritario"
            ],
            "error": []
        },
        "ir model": {
            "translated_key": "modelo IR",
            "is_in_text": false,
            "original_annotated_sentences": [
                "An Outranking Approach for Rank Aggregation in Information Retrieval Mohamed Farah Lamsade, Paris Dauphine University Place du Mal de Lattre de Tassigny 75775 Paris Cedex 16, France farah@lamsade.dauphine.fr Daniel Vanderpooten Lamsade, Paris Dauphine University Place du Mal de Lattre de Tassigny 75775 Paris Cedex 16, France vdp@lamsade.dauphine.fr ABSTRACT Research in Information Retrieval usually shows performance improvement when many sources of evidence are combined to produce a ranking of documents (e.g., texts, pictures, sounds, etc.).",
                "In this paper, we focus on the rank aggregation problem, also called data fusion problem, where rankings of documents, searched into the same collection and provided by multiple methods, are combined in order to produce a new ranking.",
                "In this context, we propose a rank aggregation method within a multiple criteria framework using aggregation mechanisms based on decision rules identifying positive and negative reasons for judging whether a document should get a better rank than another.",
                "We show that the proposed method deals well with the Information Retrieval distinctive features.",
                "Experimental results are reported showing that the suggested method performs better than the well-known CombSUM and CombMNZ operators.",
                "Categories and Subject Descriptors: H.3.3 [Information Systems]: Information Search and Retrieval - Retrieval models.",
                "General Terms: Algorithms, Measurement, Experimentation, Performance, Theory. 1.",
                "INTRODUCTION A wide range of current Information Retrieval (IR) approaches are based on various search models (Boolean, Vector Space, Probabilistic, Language, etc. [2]) in order to retrieve relevant documents in response to a user request.",
                "The result lists produced by these approaches depend on the exact definition of the relevance concept.",
                "Rank aggregation approaches, also called data fusion approaches, consist in combining these result lists in order to produce a new and hopefully better ranking.",
                "Such approaches give rise to metasearch engines in the Web context.",
                "We consider, in the following, cases where only ranks are available and no other additional information is provided such as the relevance scores.",
                "This corresponds indeed to the reality, where only ordinal information is available.",
                "Data fusion is also relevant in other contexts, such as when the user writes several queries of his/her information need (e.g., a boolean query and a natural language query) [4], or when many document surrogates are available [16].",
                "Several studies argued that rank aggregation has the potential of combining effectively all the various sources of evidence considered in various input methods.",
                "For instance, experiments carried out in [16], [30], [4] and [19] showed that documents which appear in the lists of the majority of the input methods are more likely to be relevant.",
                "Moreover, Lee [19] and Vogt and Cottrell [31] found that various retrieval approaches often return very different irrelevant documents, but many of the same relevant documents.",
                "Bartell et al. [3] also found that rank aggregation methods improve the performances w.r.t. those of the input methods, even when some of them have weak individual performances.",
                "These methods also tend to smooth out biases of the input methods according to Montague and Aslam [22].",
                "Data fusion has recently been proved to improve performances for both the ad-hoc retrieval and categorization tasks within the TREC genomics track in 2005 [1].",
                "The rank aggregation problem was addressed in various fields such as i) in social choice theory which studies voting algorithms which specify winners of elections or winners of competitions in tournaments [29], ii) in statistics when studying correlation between rankings, iii) in distributed databases when results from different databases must be combined [12], and iv) in collaborative filtering [23].",
                "Most current rank aggregation methods consider each input ranking as a permutation over the same set of items.",
                "They also give rigid interpretation to the exact ranking of the items.",
                "Both of these assumptions are rather not valid in the IR context, as will be shown in the following sections.",
                "The remaining of the paper is organized as follows.",
                "We first review current rank aggregation methods in Section 2.",
                "Then we outline the specificities of the data fusion problem in the IR context (Section 3).",
                "In Section 4, we present a new aggregation method which is proven to best fit the IR context.",
                "Experimental results are presented in Section 5 and conclusions are provided in a final section. 2.",
                "RELATED WORK As pointed out by Riker [25], we can distinguish two families of rank aggregation methods: positional methods which assign scores to items to be ranked according to the ranks they receive and majoritarian methods which are based on pairwise comparisons of items to be ranked.",
                "These two families of methods find their roots in the pioneering works of Borda [5] and Condorcet [7], respectively, in the social choice literature. 2.1 Preliminaries We first introduce some basic notations to present the rank aggregation methods in a uniform way.",
                "Let D = {d1, d2, . . . , dnd } be a set of nd documents.",
                "A list or a ranking j is an ordering defined on Dj ⊆ D (j = 1, . . . , n).",
                "Thus, di j di means di is ranked better than di in j.",
                "When Dj = D, j is said to be a full list.",
                "Otherwise, it is a partial list.",
                "If di belongs to Dj, rj i denotes the rank or position of di in j.",
                "We assume that the best answer (document) is assigned the position 1 and the worst one is assigned the position |Dj|.",
                "Let D be the set of all permutations on D or all subsets of D. A profile is a n-tuple of rankings PR = ( 1, 2, . . . , n).",
                "Restricting PR to the rankings containing document di defines PRi.",
                "We also call the number of rankings which contain document di the rank hits of di [19].",
                "The rank aggregation or data fusion problem consists of finding a ranking function or mechanism Ψ (also called a social welfare function in the social choice theory terminology) defined by: Ψ : n D → D PR = ( 1, 2, . . . , n) → σ = Ψ(PR) where σ is called a consensus ranking. 2.2 Positional Methods 2.2.1 Borda Count This method [5] first assigns a score n j=1 rj i to each document di.",
                "Documents are then ranked by increasing order of this score, breaking ties, if any, arbitrarily. 2.2.2 Linear Combination Methods This family of methods basically combine scores of documents.",
                "When used for the rank aggregation problem, ranks are assumed to be scores or performances to be combined using aggregation operators such as the weighted sum or some variation of it [3, 31, 17, 28].",
                "For instance, Callan et al. [6] used the inference networks model [30] to combine rankings.",
                "Fox and Shaw [15] proposed several combination strategies which are CombSUM, CombMIN, CombMAX, CombANZ and CombMNZ.",
                "The first three operators correspond to the sum, min and max operators, respectively.",
                "CombANZ and CombMNZ respectively divides and multiplies the CombSUM score by the rank hits.",
                "It is shown in [19] that the CombSUM and CombMNZ operators perform better than the others.",
                "Metasearch engines such as SavvySearch and MetaCrawler use the CombSUM strategy to fuse rankings. 2.2.3 Footrule Optimal Aggregation In this method, a consensus ranking minimizes the Spearman footrule distance from the input rankings [21].",
                "Formally, given two full lists j and j , this distance is given by F( j, j ) = nd i=1 |rj i − rj i |.",
                "It extends to several lists as follows.",
                "Given a profile PR and a consensus ranking σ, the Spearman footrule distance of σ to PR is given by F(σ, PR) = n j=1 F(σ, j).",
                "Cook and Kress [8] proposed a similar method which consists in optimizing the distance D( j, j ) = 1 2 nd i,i =1 |rj i,i − rj i,i |, where rj i,i = rj i −rj i .",
                "This formulation has the advantage that it considers the intensity of preferences. 2.2.4 Probabilistic Methods This kind of methods assume that the performance of the input methods on a number of training queries is indicative of their future performance.",
                "During the training process, probabilities of relevance are calculated.",
                "For subsequent queries, documents are ranked based on these probabilities.",
                "For instance, in [20], each input ranking j is divided into a number of segments, and the conditional probability of relevance (R) of each document di depending on the segment k it occurs in, is computed, i.e. prob(R|di, k, j).",
                "For subsequent queries, the score of each document di is given by n j=1 prob(R|di,k, j ) k .",
                "Le Calve and Savoy [18] suggest using a logistic regression approach for combining scores.",
                "Training data is needed to infer the model parameters. 2.3 Majoritarian Methods 2.3.1 Condorcet Procedure The original Condorcet rule [7] specifies that a winner of the election is any item that beats or ties with every other item in a pairwise contest.",
                "Formally, let C(diσdi ) = { j∈ PR : di j di } be the coalition of rankings that are concordant with establishing diσdi , i.e. with the proposition di should be ranked better than di in the final ranking σ. di beats or ties with di iff |C(diσdi )| ≥ |C(di σdi)|.",
                "The repetitive application of the Condorcet algorithm can produce a ranking of items in a natural way: select the Condorcet winner, remove it from the lists, and repeat the previous two steps until there are no more documents to rank.",
                "Since there is not always Condorcet winners, variations of the Condorcet procedure have been developed within the multiple criteria decision aid theory, with methods such as ELECTRE [26]. 2.3.2 Kemeny Optimal Aggregation As in section 2.2.3, a consensus ranking minimizes a geometric distance from the input rankings, where the Kendall tau distance is used instead of the Spearman footrule distance.",
                "Formally, given two full lists j and j , the Kendall tau distance is given by K( j, j ) = |{(di, di ) : i < i , rj i < rj i , rj i > rj i }|, i.e. the number of pairwise disagreements between the two lists.",
                "It is easy to show that the consensus ranking corresponds to the geometric median of the input rankings and that the Kemeny optimal aggregation problem corresponds to the minimum feedback edge set problem. 2.3.3 Markov Chain Methods Markov chains (MCs) have been used by Dwork et al. [11] as a natural method to obtain a consensus ranking where states correspond to the documents to be ranked and the transition probabilities vary depending on the interpretation of the transition event.",
                "In the same reference, the authors proposed four specific MCs and experimental testing had shown that the following MC is the best performing one (see also [24]): • MC4: move from the current state di to the next state di by first choosing a document di uniformly from D. If for the majority of the rankings, we have rj i ≤ rj i , then move to di , else stay in di.",
                "The consensus ranking corresponds to the stationary distribution of MC4. 3.",
                "SPECIFICITIES OF THE RANK AGGREGATION PROBLEM IN THE IR CONTEXT 3.1 Limited Significance of the Rankings The exact positions of documents in one input ranking have limited significance and should not be overemphasized.",
                "For instance, having three relevant documents in the first three positions, any perturbation of these three items will have the same value.",
                "Indeed, in the IR context, the complete order provided by an input method may hide ties.",
                "In this case, we call such rankings semi orders.",
                "This was outlined in [13] as the problem of aggregation with ties.",
                "It is therefore important to build the consensus ranking based on robust information: • Documents with near positions in j are more likely to have similar interest or relevance.",
                "Thus a slight perturbation of the initial ranking is meaningless. • Assuming that document di is better ranked than document di in a ranking j, di is more likely to be definitively more relevant than di in j when the number of intermediate positions between di and di increases. 3.2 Partial Lists In real world applications, such as metasearch engines, rankings provided by the input methods are often partial lists.",
                "This was outlined in [14] as the problem of having to merge top-k results from various input lists.",
                "For instance, in the experiments carried out by Dwork et al. [11], authors found that among the top 100 best documents of 7 input search engines, 67% of the documents were present in only one search engine, whereas less than two documents were present in all the search engines.",
                "Rank aggregation of partial lists raises four major difficulties which we state hereafter, proposing for each of them various working assumptions: 1.",
                "Partial lists can have various lengths, which can favour long lists.",
                "We thus consider the following two working hypotheses: H1 k : We only consider the top k best documents from each input ranking.",
                "H1 all: We consider all the documents from each input ranking. 2.",
                "Since there are different documents in the input rankings, we must decide which documents should be kept in the consensus ranking.",
                "Two working hypotheses are therefore considered: H2 k : We only consider documents which are present in at least k input rankings (k > 1).",
                "H2 all: We consider all the documents which are ranked in at least one input ranking.",
                "Hereafter, we call documents which will be retained in the consensus ranking, candidate documents, and documents that will be excluded from the consensus ranking, excluded documents.",
                "We also call a candidate document which is missing in one or more rankings, a missing document. 3.",
                "Some candidate documents are missing documents in some input rankings.",
                "Main reasons for a missing document are that it was not indexed or it was indexed but deemed irrelevant ; usually this information is not available.",
                "We consider the following two working hypotheses: H3 yes: Each missing document in each j is assigned a position.",
                "H3 no: No assumption is made, that is each missing document is considered neither better nor worse than any other document. 4.",
                "When assumption H2 k holds, each input ranking may contain documents which will not be considered in the consensus ranking.",
                "Regarding the positions of the candidate documents, we can consider the following working hypotheses: H4 init: The initial positions of candidate documents are kept in each input ranking.",
                "H4 new: Candidate documents receive new positions in each input ranking, after discarding excluded ones.",
                "In the IR context, rank aggregation methods need to decide more or less explicitly which assumptions to retain w.r.t. the above-mentioned difficulties. 4.",
                "OUTRANKING APPROACH FOR RANK AGGREGATION 4.1 Presentation Positional methods consider implicitly that the positions of the documents in the input rankings are scores giving thus a cardinal meaning to an ordinal information.",
                "This constitutes a strong assumption that is questionable, especially when the input rankings have different lengths.",
                "Moreover, for positional methods, assumptions H3 and H4 , which are often arbitrary, have a strong impact on the results.",
                "For instance, let us consider an input ranking of 500 documents out of 1000 candidate documents.",
                "Whether we assign to each of the missing documents the position 1, 501, 750 or 1000 -corresponding to variations of H3 yes- will give rise to very contrasted results, especially regarding the top of the consensus ranking.",
                "Majoritarian methods do not suffer from the above-mentioned drawbacks of the positional methods since they build consensus rankings exploiting only ordinal information contained in the input rankings.",
                "Nevertheless, they suppose that such rankings are complete orders, ignoring that they may hide ties.",
                "Therefore, majoritarian methods base consensus rankings on illusory discriminant information rather than less discriminant but more robust information.",
                "Trying to overcome the limits of current rank aggregation methods, we found that outranking approaches, which were initially used for multiple criteria aggregation problems [26], can also be used for the rank aggregation purpose, where each ranking plays the role of a criterion.",
                "Therefore, in order to decide whether a document di should be ranked better than di in the consensus ranking σ, the two following conditions should be met: • a concordance condition which ensures that a majority of the input rankings are concordant with diσdi (majority principle). • a discordance condition which ensures that none of the discordant input rankings strongly refutes dσd (respect of minorities principle).",
                "Formally, the concordance coalition with diσdi is Csp (diσdi ) = { j∈ PR : rj i ≤ rj i − sp} where sp is a preference threshold which is the variation of document positions -whether it is absolute or relative to the ranking length- which draws the boundaries between an indifference and a preference situation between documents.",
                "The discordance coalition with diσdi is Dsv (diσdi ) = { j∈ PR : rj i ≥ rj i + sv} where sv is a veto threshold which is the variation of document positions -whether it is absolute or relative to the ranking length- which draws the boundaries between a weak and a strong opposition to diσdi .",
                "Depending on the exact definition of the preceding concordance and discordance coalitions leading to the definition of some decision rules, several outranking relations can be defined.",
                "They can be more or less demanding depending on i) the values of the thresholds sp and sv, ii) the importance or minimal size cmin required for the concordance coalition, and iii) the importance or maximum size dmax of the discordance coalition.",
                "A generic outranking relation can thus be defined as follows: diS(sp,sv,cmin,dmax)di ⇔ |Csp (diσdi )| ≥ cmin AND |Dsv (diσdi )| ≤ dmax This expression defines a family of nested outranking relations since S(sp,sv,cmin,dmax) ⊆ S(sp,sv,cmin,dmax) when cmin ≥ cmin and/or dmax ≤ dmax and/or sp ≥ sp and/or sv ≤ sv.",
                "This expression also generalizes the majority rule which corresponds to the particular relation S(0,∞, n 2 ,n).",
                "It also satisfies important properties of rank aggregation methods, called neutrality, Pareto-optimality, Condorcet property and Extended Condorcet property, in the social choice literature [29].",
                "Outranking relations are not necessarily transitive and do not necessarily correspond to rankings since directed cycles may exist.",
                "Therefore, we need specific procedures in order to derive a consensus ranking.",
                "We propose the following procedure which finds its roots in [27].",
                "It consists in partitioning the set of documents into r ranked classes.",
                "Each class Ch contains documents with the same relevance and results from the application of all relations (if possible) to the set of documents remaining after previous classes are computed.",
                "Documents within the same equivalence class are ranked arbitrarily.",
                "Formally, let • R be the set of candidate documents for a query, • S1 , S2 , . . . be a family of nested outranking relations, • Fk(di, E) = |{di ∈ E : diSk di }| be the number of documents in E(E ⊆ R) that could be considered worse than di according to relation Sk , • fk(di, E) = |{di ∈ E : di Sk di}| be the number of documents in E that could be considered better than di according to Sk , • sk(di, E) = Fk(di, E) − fk(di, E) be the qualification of di in E according to Sk .",
                "Each class Ch results from a distillation process.",
                "It corresponds to the last distillate of a series of sets E0 ⊇ E1 ⊇ . . . where E0 = R \\ (C1 ∪ . . . ∪ Ch−1) and Ek is a reduced subset of Ek−1 resulting from the application of the following procedure: 1. compute for each di ∈ Ek−1 its qualification according to Sk , i.e. sk(di, Ek−1), 2. define smax = maxdi∈Ek−1 {sk(di, Ek−1)}, then 3.",
                "Ek = {di ∈ Ek−1 : sk(di, Ek−1) = smax} When one outranking relation is used, the distillation process stops after the first application of the previous procedure, i.e., Ch corresponds to distillate E1.",
                "When different outranking relations are used, the distillation process stops when all the pre-defined outranking relations have been used or when |Ek| = 1. 4.2 Illustrative Example This section illustrates the concepts and procedures of section 4.1.",
                "Let us consider a set of candidate documents R = {d1, d2, d3, d4, d5}.",
                "The following table gives a profile PR of different rankings of the documents of R: PR = ( 1 , 2, 3, 4).",
                "Table 1: Rankings of documents rj i 1 2 3 4 d1 1 3 1 5 d2 2 1 3 3 d3 3 2 2 1 d4 4 4 5 2 d5 5 5 4 4 Let us suppose that the preference and veto thresholds are set to values 1 and 4 respectively, and that the concordance and discordance thresholds are set to values 2 and 1 respectively.",
                "The following tables give the concordance, discordance and outranking matrices.",
                "Each entry csp (di, di ) (dsv (di, di )) in the concordance (discordance) matrix gives the number of rankings that are concordant (discordant) with diσdi , i.e. csp (di, di ) = |Csp (diσdi )| and dsv (di, di ) = |Dsv (diσdi )|.",
                "Table 2: Computation of the outranking relation d1 d2 d3 d4 d5 d1 - 2 2 3 3 d2 2 - 2 3 4 d3 2 2 - 4 4 d4 1 1 0 - 3 d5 1 0 0 1Concordance Matrix d1 d2 d3 d4 d5 d1 - 0 1 0 0 d2 0 - 0 0 0 d3 0 0 - 0 0 d4 1 0 0 - 0 d5 1 1 0 0Discordance Matrix d1 d2 d3 d4 d5 d1 - 1 1 1 1 d2 1 - 1 1 1 d3 1 1 - 1 1 d4 0 0 0 - 1 d5 0 0 0 0Outranking Matrix (S1) For instance, the concordance coalition for the assertion d1σd4 is C1(d1σd4) = { 1, 2, 3} and the discordance coalition for the same assertion is D4(d1σd4) = ∅.",
                "Therefore, c1(d1, d4) = 3, d4(d1, d4) = 0 and d1S1 d4 holds.",
                "Notice that Fk(di, R) (fk(di, R)) is given by summing the values of the ith row (column) of the outranking matrix.",
                "The consensus ranking is obtained as follows: to get the first class C1, we compute the qualifications of all the documents of E0 = R with respect to S1 .",
                "They are respectively 2, 2, 2, -2 and -4.",
                "Therefore smax equals 2 and C1 = E1 = {d1, d2, d3}.",
                "Observe that, if we had used a second outranking relation S2(⊇ S1), these three documents could have been possibly discriminated.",
                "At this stage, we remove documents of C1 from the outranking matrix and compute the next class C2: we compute the new qualifications of the documents of E0 = R \\ C1 = {d4, d5}.",
                "They are respectively 1 and -1.",
                "So C3 = E1 = {d4}.",
                "The last document d5 is the only document of the last class C3.",
                "Thus, the consensus ranking is {d1, d2, d3} → {d4} → {d5}. 5.",
                "EXPERIMENTS AND RESULTS 5.1 Test Setting To facilitate empirical investigation of the proposed methodology, we developed a prototype metasearch engine that implements a version of our outranking approach for rank aggregation.",
                "In this paper, we apply our approach to the Topic Distillation (TD) task of TREC-2004 Web track [10].",
                "In this task, there are 75 topics where only a short description of each is given.",
                "For each query, we retained the rankings of the 10 best runs of the TD task which are provided by TREC-2004 participating teams.",
                "The performances of these runs are reported in table 3.",
                "Table 3: Performances of the 10 best runs of the TD task of TREC-2004 Run Id MAP P@10 S@1 S@5 S@10 uogWebCAU150 17.9% 24.9% 50.7% 77.3% 89.3% MSRAmixed1 17.8% 25.1% 38.7% 72.0% 88.0% MSRC04C12 16.5% 23.1% 38.7% 74.7% 80.0% humW04rdpl 16.3% 23.1% 37.3% 78.7% 90.7% THUIRmix042 14.7% 20.5% 21.3% 58.7% 74.7% UAmsT04MWScb 14.6% 20.9% 36.0% 66.7% 76.0% ICT04CIIS1AT 14.1% 20.8% 33.3% 64.0% 78.7% SJTUINCMIX5 12.9% 18.9% 29.3% 57.3% 72.0% MU04web1 11.5% 19.9% 33.3% 64.0% 76.0% MeijiHILw3 11.5% 15.3% 30.7% 54.7% 64.0% Average 14.7% 21.2% 34.9% 66.8% 78.94% For each query, each run provides a ranking of about 1000 documents.",
                "The number of documents retrieved by all these runs ranges from 543 to 5769.",
                "Their average (median) number is 3340 (3386).",
                "It is worth noting that we found similar distributions of the documents among the rankings as in [11].",
                "For evaluation, we used the trec eval standard tool which is used by the TREC community to calculate the standard measures of system effectiveness which are Mean Average Precision (MAP) and Success@n (S@n) for n=1, 5 and 10.",
                "Our approach effectiveness is compared against some high performing official results from TREC-2004 as well as against some standard rank aggregation algorithms.",
                "In the experiments, significance testing is mainly based on the t-student statistic which is computed on the basis of the MAP values of the compared runs.",
                "In the tables of the following section, statistically significant differences are marked with an asterisk.",
                "Values between brackets of the first column of each table, indicate the parameter value of the corresponding run. 5.2 Results We carried out several series of runs in order to i) study performance variations of the outranking approach when tuning the parameters and working assumptions, ii) compare performances of the outranking approach vs standard rank aggregation strategies , and iii) check whether rank aggregation performs better than the best input rankings.",
                "We set our basic run mcm with the following parameters.",
                "We considered that each input ranking is a complete order (sp = 0) and that an input ranking strongly refutes diσdi when the difference of both document positions is large enough (sv = 75%).",
                "Preference and veto thresholds are computed proportionally to the number of documents retained in each input ranking.",
                "They consequently may vary from one ranking to another.",
                "In addition, to accept the assertion diσdi , we supposed that the majority of the rankings must be concordant (cmin = 50%) and that every input ranking can impose its veto (dmax = 0).",
                "Concordance and discordance thresholds are computed for each tuple (di, di ) as the percentage of the input rankings of PRi ∩PRi .",
                "Thus, our choice of parameters leads to the definition of the outranking relation S(0,75%,50%,0).",
                "To test the run mcm, we had chosen the following assumptions.",
                "We retained the top 100 best documents from each input ranking (H1 100), only considered documents which are present in at least half of the input rankings (H2 5 ) and assumed H3 no and H4 new.",
                "In these conditions, the number of successful documents was about 100 on average, and the computation time per query was less than one second.",
                "Obviously, modifying the working assumptions should have deeper impact on the performances than tuning our model parameters.",
                "This was validated by preliminary experiments.",
                "Thus, we hereafter begin by studying performance variation when different sets of assumptions are considered.",
                "Afterwards, we study the impact of tuning parameters.",
                "Finally, we compare our model performances w.r.t. the input rankings as well as some standard data fusion algorithms. 5.2.1 Impact of the Working Assumptions Table 4 summarizes the performance variation of the outranking approach under different working hypotheses.",
                "In Table 4: Impact of the working assumptions Run Id MAP S@1 S@5 S@10 mcm 18.47% 41.33% 81.33% 86.67% mcm22 (H3 yes) 17.72% (-4.06%) 34.67% 81.33% 86.67% mcm23 (H4 init) 18.26% (-1.14%) 41.33% 81.33% 86.67% mcm24 (H1 all) 20.67% (+11.91%*) 38.66% 80.00% 86.66% mcm25 (H2 all) 21.68% (+17.38%*) 40.00% 78.66% 89.33% this table, we first show that run mcm22, in which missing documents are all put in the same last position of each input ranking, leads to performance drop w.r.t. run mcm.",
                "Moreover, S@1 moves from 41.33% to 34.67% (-16.11%).",
                "This shows that several relevant documents which were initially put at the first position of the consensus ranking in mcm, lose this first position but remain ranked in the top 5 documents since S@5 did not change.",
                "We also conclude that documents which have rather good positions in some input rankings are more likely to be relevant, even though they are missing in some other rankings.",
                "Consequently, when they are missing in some rankings, assigning worse ranks to these documents is harmful for performance.",
                "Also, from Table 4, we found that the performances of runs mcm and mcm23 are similar.",
                "Therefore, the outranking approach is not sensitive to keeping the initial positions of candidate documents or recomputing them by discarding excluded ones.",
                "From the same Table 4, performance of the outranking approach increases significantly for runs mcm24 and mcm25.",
                "Therefore, whether we consider all the documents which are present in half of the rankings (mcm24) or we consider all the documents which are ranked in the first 100 positions in one or more rankings (mcm25), increases performances.",
                "This result was predictable since in both cases we have more detailed information on the relative importance of documents.",
                "Tables 5 and 6 confirm this evidence.",
                "Table 5, where values between brackets of the first column give the number of documents which are retained from each input ranking, shows that selecting more documents from each input ranking leads to performance increase.",
                "It is worth mentioning that selecting more than 600 documents from each input ranking does not improve performance.",
                "Table 5: Impact of the number of retained documents Run Id MAP S@1 S@5 S@10 mcm (100) 18.47% 41.33% 81.33% 86.67% mcm24-1 (200) 19.32% (+4.60%) 42.67% 78.67% 88.00% mcm24-2 (400) 19.88% (+7.63%*) 37.33% 80.00% 88.00% mcm24-3 (600) 20.80% (+12.62%*) 40.00% 80.00% 88.00% mcm24-4 (800) 20.66% (+11.86%*) 40.00% 78.67% 86.67% mcm24 (1000) 20.67% (+11.91%*) 38.66% 80.00% 86.66% Table 6 reports runs corresponding to variations of H2 k .",
                "Values between brackets are rank hits.",
                "For instance, in the run mcm32, only documents which are present in 3 or more input rankings, were considered successful.",
                "This table shows that performance is significantly better when rare documents are considered, whereas it decreases significantly when these documents are discarded.",
                "Therefore, we conclude that many of the relevant documents are retrieved by a rather small set of IR models.",
                "Table 6: Performance considering different rank hits Run Id MAP S@1 S@5 S@10 mcm25 (1) 21.68% (+17.38%*) 40.00% 78.67% 89.33% mcm32 (3) 18.98% (+2.76%) 38.67% 80.00% 85.33% mcm (5) 18.47% 41.33% 81.33% 86.67% mcm33 (7) 15.83% (-14.29%*) 37.33% 78.67% 85.33% mcm34 (9) 10.96% (-40.66%*) 36.11% 66.67% 70.83% mcm35 (10) 7.42% (-59.83%*) 39.22% 62.75% 64.70% For both runs mcm24 and mcm25, the number of successful documents was about 1000 and therefore, the computation time per query increased and became around 5 seconds. 5.2.2 Impact of the Variation of the Parameters Table 7 shows performance variation of the outranking approach when different preference thresholds are considered.",
                "We found performance improvement up to threshold values of about 5%, then there is a decrease in the performance which becomes significant for threshold values greater than 10%.",
                "Moreover, S@1 improves from 41.33% to 46.67% when preference threshold changes from 0 to 5%.",
                "We can thus conclude that the input rankings are semi orders rather than complete orders.",
                "Table 8 shows the evolution of the performance measures w.r.t. the concordance threshold.",
                "We can conclude that in order to put document di before di in the consensus ranking, Table 7: Impact of the variation of the preference threshold from 0 to 12.5% Run Id MAP S@1 S@5 S@10 mcm (0%) 18.47% 41.33% 81.33% 86.67% mcm1 (1%) 18.57% (+0.54%) 41.33% 81.33% 86.67% mcm2 (2.5%) 18.63% (+0.87%) 42.67% 78.67% 86.67% mcm3 (5%) 18.69% (+1.19%) 46.67% 81.33% 86.67% mcm4 (7.5%) 18.24% (-1.25%) 46.67% 81.33% 86.67% mcm5 (10%) 17.93% (-2.92%) 40.00% 82.67% 86.67% mcm5b (12.5%) 17.51% (-5.20%*) 41.33% 80.00% 86.67% at least half of the input rankings of PRi ∩ PRi should be concordant.",
                "Performance drops significantly for very low and very high values of the concordance threshold.",
                "In fact, for such values, the concordance condition is either fulfilled rather always by too many document pairs or not fulfilled at all, respectively.",
                "Therefore, the outranking relation becomes either too weak or too strong respectively.",
                "Table 8: Impact of the variation of cmin Run Id MAP S@1 S@5 S@10 mcm11 (20%) 17.63% (-4.55%*) 41.33% 76.00% 85.33% mcm12 (40%) 18.37% (-0.54%) 42.67% 76.00% 86.67% mcm (50%) 18.47% 41.33% 81.33% 86.67% mcm13 (60%) 18.42% (-0.27%) 40.00% 78.67% 86.67% mcm14 (80%) 17.43% (-5.63%*) 40.00% 78.67% 86.67% mcm15 (100%) 16.12% (-12.72%*) 41.33% 70.67% 85.33% In the experiments, varying the veto threshold as well as the discordance threshold within reasonable intervals does not have significant impact on performance measures.",
                "In fact, runs with different veto thresholds (sv ∈ [50%; 100%]) had similar performances even though there is a slight advantage for runs with high threshold values which means that it is better not to allow the input rankings to put their veto easily.",
                "Also, tuning the discordance threshold was carried out for values 50% and 75% of the veto threshold.",
                "For these runs we did not get any noticeable performance variation, although for low discordance thresholds (dmax < 20%), performance slightly decreased. 5.2.3 Impact of the Variation of the Number of Input Rankings To study performance evolution when different sets of input rankings are considered, we carried three more runs where 2, 4, and 6 of the best performing sets of the input rankings are considered.",
                "Results reported in Table 9 are seemingly counter-intuitive and also do not support previous findings regarding rank aggregation research [3].",
                "Nevertheless, this result shows that low performing rankings bring more noise than information to the establishment of the consensus ranking.",
                "Therefore, when they are considered, performance decreases.",
                "Table 9: Performance considering different best performing sets of input rankings Run Id MAP S@1 S@5 S@10 mcm (10) 18.47% 41.33% 81.33% 86.67% mcm27 (6) 18.60% (+0.70%) 41.33% 80.00% 85.33% mcm28 (4) 19.02% (+2.98%) 40.00% 86.67% 88.00% mcm29 (2) 18.33% (-0.76%) 44.00% 76.00% 88.00% 5.2.4 Comparison of the Performance of Different Rank Aggregation Methods In this set of runs, we compare the outranking approach with some standard rank aggregation methods which were proven to have acceptable performance in previous studies: we considered two positional methods which are the CombSUM and the CombMNZ strategies.",
                "We also examined the performance of one majoritarian method which is the Markov chain method (MC4).",
                "For the comparisons, we considered a specific outranking relation S∗ = S(5%,50%,50%,30%) which results in good overall performances when tuning all the parameters.",
                "The first row of Table 10 gives performances of the rank aggregation methods w.r.t. a basic assumption set A1 = (H1 100, H2 5 , H4 new): we only consider the 100 first documents from each ranking, then retain documents present in 5 or more rankings and update ranks of successful documents.",
                "For positional methods, we place missing documents at the queue of the ranking (H3 yes) whereas for our method as well as for MC4, we retained hypothesis H3 no.",
                "The three following rows of Table 10 report performances when changing one element from the basic assumption set: the second row corresponds to the assumption set A2 = (H1 1000, H2 5 , H4 new), i.e. changing the number of retained documents from 100 to 1000.",
                "The third row corresponds to the assumption set A3 = (H1 100, H2 all, H4 new), i.e. considering the documents present in at least one ranking.",
                "The fourth row corresponds to the assumption set A4 = (H1 100, H2 5 , H4 init), i.e. keeping the original ranks of successful documents.",
                "The fifth row of Table 10, labeled A5, gives performance when all the 225 queries of the Web track of TREC-2004 are considered.",
                "Obviously, performance level cannot be compared with previous lines since the additional queries are different from the TD queries and correspond to other tasks (Home Page and Named Page tasks [10]) of TREC-2004 Web track.",
                "This set of runs aims to show whether relative performance of the various methods is task-dependent.",
                "The last row of Table 10, labeled A6, reports performance of the various methods considering the TD task of TREC2002 instead of TREC-2004: we fused the results of input rankings of the 10 best official runs for each of the 50 TD queries [9] considering the set of assumptions A1 of the first row.",
                "This aims to show whether relative performance of the various methods changes from year to year.",
                "Values between brackets of Table 10 are variations of performance of each rank aggregation method w.r.t. performance of the outranking approach.",
                "Table 10: Performance (MAP) of different rank aggregation methods under 3 different test collections mcm combSUM combMNZ markov A1 18.79% 17.54% (-6.65%*) 17.08% (-9.10%*) 18.63% (-0.85%) A2 21.36% 19.18% (-10.21%*) 18.61% (-12.87%*) 21.33% (-0.14%) A3 21.92% 21.38% (-2.46%) 20.88% (-4.74%) 19.35% (-11.72%*) A4 18.64% 17.58% (-5.69%*) 17.18% (-7.83%*) 18.63% (-0.05%) A5 55.39% 52.16% (-5.83%*) 49.70% (-10.27%*) 53.30% (-3.77%) A6 16.95% 15.65% (-7.67%*) 14.57% (-14.04%*) 16.39% (-3.30%) From the analysis of table 10 the following can be established: • for all the runs, considering all the documents in each input ranking (A2) significantly improves performance (MAP increases by 11.62% on average).",
                "This is predictable since some initially unreported relevant documents would receive better positions in the consensus ranking. • for all the runs, considering documents even those present in only one input ranking (A3) significantly improves performance.",
                "For mcm, combSUM and combMNZ, performance improvement is more important (MAP increases by 20.27% on average) than for the markov run (MAP increases by 3.86%). • preserving the initial positions of documents (A4) or recomputing them (A1) does not have a noticeable influence on performance for both positional and majoritarian methods. • considering all the queries of the Web track of TREC2004 (A5) as well as the TD queries of the Web track of TREC-2002 (A6) does not alter the relative performance of the different data fusion methods. • considering the TD queries of the Web track of TREC2002, performances of all the data fusion methods are lower than that of the best performing input ranking for which the MAP value equals 18.58%.",
                "This is because most of the fused input rankings have very low performances compared to the best one, which brings more noise to the consensus ranking. • performances of the data fusion methods mcm and markov are significantly better than that of the best input ranking uogWebCAU150.",
                "This remains true for runs combSUM and combMNZ only under assumptions H1 all or H2 all.",
                "This shows that majoritarian methods are less sensitive to assumptions than positional methods. • outranking approach always performs significantly better than positional methods combSUM and combMNZ.",
                "It has also better performances than the Markov chain method, especially under assumption H2 all where difference of performances becomes significant. 6.",
                "CONCLUSIONS In this paper, we address the rank aggregation problem where different, but not disjoint, lists of documents are to be fused.",
                "We noticed that the input rankings can hide ties, so they should not be considered as complete orders.",
                "Only robust information should be used from each input ranking.",
                "Current rank aggregation methods, and especially positional methods (e.g. combSUM [15]), are not initially designed to work with such rankings.",
                "They should be adapted by considering specific working assumptions.",
                "We propose a new outranking method for rank aggregation which is well adapted to the IR context.",
                "Indeed, it ranks two documents w.r.t. the intensity of their positions difference in each input ranking and also considering the number of the input rankings that are concordant and discordant in favor of a specific document.",
                "There is also no need to make specific assumptions on the positions of the missing documents.",
                "This is an important feature since the absence of a document from a ranking should not be necessarily interpreted negatively.",
                "Experimental results show that the outranking method significantly out-performs popular classical positional data fusion methods like combSUM and combMNZ strategies.",
                "It also out-performs a good performing majoritarian methods which is the Markov chain method.",
                "These results are tested against different test collections and queries.",
                "From the experiments, we can also conclude that in order to improve the performances, we should fuse result lists of well performing IR models, and that majoritarian data fusion methods perform better than positional methods.",
                "The proposed method can have a real impact on Web metasearch performances since only ranks are available from most primary search engines, whereas most of the current approaches need scores to merge result lists into one single list.",
                "Further work involves investigating whether the outranking approach performs well in various other contexts, e.g. using the document scores or some combination of document ranks and scores.",
                "Acknowledgments The authors would like to thank Jacques Savoy for his valuable comments on a preliminary version of this paper. 7.",
                "REFERENCES [1] A. Aronson, D. Demner-Fushman, S. Humphrey, J. Lin, H. Liu, P. Ruch, M. Ruiz, L. Smith, L. Tanabe, and W. Wilbur.",
                "Fusion of knowledge-intensive and statistical approaches for retrieving and annotating textual genomics documents.",
                "In Proceedings TREC2005.",
                "NIST Publication, 2005. [2] R. A. Baeza-Yates and B.",
                "A. Ribeiro-Neto.",
                "Modern Information Retrieval.",
                "ACM Press , 1999. [3] B. T. Bartell, G. W. Cottrell, and R. K. Belew.",
                "Automatic combination of multiple ranked retrieval systems.",
                "In Proceedings ACM-SIGIR94, pages 173-181.",
                "Springer-Verlag, 1994. [4] N. J. Belkin, P. Kantor, E. A.",
                "Fox, and J.",
                "A. Shaw.",
                "Combining evidence of multiple query representations for information retrieval.",
                "IPM, 31(3):431-448, 1995. [5] J. Borda.",
                "M´emoire sur les ´elections au scrutin.",
                "Histoire de lAcad´emie des Sciences, 1781. [6] J. P. Callan, Z. Lu, and W. B. Croft.",
                "Searching distributed collections with inference networks.",
                "In Proceedings ACM-SIGIR95, pages 21-28, 1995. [7] M. Condorcet.",
                "Essai sur lapplication de lanalyse `a la probabilit´e des d´ecisions rendues `a la pluralit´e des voix.",
                "Imprimerie Royale, Paris, 1785. [8] W. D. Cook and M. Kress.",
                "Ordinal ranking with intensity of preference.",
                "Management Science, 31(1):26-32, 1985. [9] N. Craswell and D. Hawking.",
                "Overview of the TREC-2002 Web Track.",
                "In Proceedings TREC2002.",
                "NIST Publication, 2002. [10] N. Craswell and D. Hawking.",
                "Overview of the TREC-2004 Web Track.",
                "In Proceedings of TREC2004.",
                "NIST Publication, 2004. [11] C. Dwork, S. R. Kumar, M. Naor, and D. Sivakumar.",
                "Rank aggregation methods for the Web.",
                "In Proceedings WWW2001, pages 613-622, 2001. [12] R. Fagin.",
                "Combining fuzzy information from multiple systems.",
                "JCSS, 58(1):83-99, 1999. [13] R. Fagin, R. Kumar, M. Mahdian, D. Sivakumar, and E. Vee.",
                "Comparing and aggregating rankings with ties.",
                "In PODS, pages 47-58, 2004. [14] R. Fagin, R. Kumar, and D. Sivakumar.",
                "Comparing top k lists.",
                "SIAM J. on Discrete Mathematics, 17(1):134-160, 2003. [15] E. A.",
                "Fox and J.",
                "A. Shaw.",
                "Combination of multiple searches.",
                "In Proceedings of TREC3.",
                "NIST Publication, 1994. [16] J. Katzer, M. McGill, J. Tessier, W. Frakes, and P. DasGupta.",
                "A study of the overlap among document representations.",
                "Information Technology: Research and Development, 1(4):261-274, 1982. [17] L. S. Larkey, M. E. Connell, and J. Callan.",
                "Collection selection and results merging with topically organized U.S. patents and TREC data.",
                "In Proceedings ACM-CIKM2000, pages 282-289.",
                "ACM Press, 2000. [18] A.",
                "Le Calv´e and J. Savoy.",
                "Database merging strategy based on logistic regression.",
                "IPM, 36(3):341-359, 2000. [19] J. H. Lee.",
                "Analyses of multiple evidence combination.",
                "In Proceedings ACM-SIGIR97, pages 267-276, 1997. [20] D. Lillis, F. Toolan, R. Collier, and J. Dunnion.",
                "Probfuse: a probabilistic approach to data fusion.",
                "In Proceedings ACM-SIGIR2006, pages 139-146.",
                "ACM Press, 2006. [21] J. I. Marden.",
                "Analyzing and Modeling Rank Data.",
                "Number 64 in Monographs on Statistics and Applied Probability.",
                "Chapman & Hall, 1995. [22] M. Montague and J.",
                "A. Aslam.",
                "Metasearch consistency.",
                "In Proceedings ACM-SIGIR2001, pages 386-387.",
                "ACM Press, 2001. [23] D. M. Pennock and E. Horvitz.",
                "Analysis of the axiomatic foundations of collaborative filtering.",
                "In Workshop on AI for Electronic Commerce at the 16th National Conference on Artificial Intelligence, 1999. [24] M. E. Renda and U. Straccia.",
                "Web metasearch: rank vs. score based rank aggregation methods.",
                "In Proceedings ACM-SAC2003, pages 841-846.",
                "ACM Press, 2003. [25] W. H. Riker.",
                "Liberalism against populism.",
                "Waveland Press, 1982. [26] B. Roy.",
                "The outranking approach and the foundations of ELECTRE methods.",
                "Theory and Decision, 31:49-73, 1991. [27] B. Roy and J. Hugonnard.",
                "Ranking of suburban line extension projects on the Paris metro system by a multicriteria method.",
                "Transportation Research, 16A(4):301-312, 1982. [28] L. Si and J. Callan.",
                "Using sampled data and regression to merge search engine results.",
                "In Proceedings ACM-SIGIR2002, pages 19-26.",
                "ACM Press, 2002. [29] M. Truchon.",
                "An extension of the Condorcet criterion and Kemeny orders.",
                "Cahier 9813, Centre de Recherche en Economie et Finance Appliqu´ees, Oct. 1998. [30] H. Turtle and W. B. Croft.",
                "Inference networks for document retrieval.",
                "In Proceedings of ACM-SIGIR90, pages 1-24.",
                "ACM Press, 1990. [31] C. C. Vogt and G. W. Cottrell.",
                "Fusion via a linear combination of scores.",
                "Information Retrieval, 1(3):151-173, 1999."
            ],
            "original_annotated_samples": [],
            "translated_annotated_samples": [],
            "translated_text": "",
            "candidates": [],
            "error": []
        },
        "datum fusion": {
            "translated_key": "fusión de dato",
            "is_in_text": false,
            "original_annotated_sentences": [
                "An Outranking Approach for Rank Aggregation in Information Retrieval Mohamed Farah Lamsade, Paris Dauphine University Place du Mal de Lattre de Tassigny 75775 Paris Cedex 16, France farah@lamsade.dauphine.fr Daniel Vanderpooten Lamsade, Paris Dauphine University Place du Mal de Lattre de Tassigny 75775 Paris Cedex 16, France vdp@lamsade.dauphine.fr ABSTRACT Research in Information Retrieval usually shows performance improvement when many sources of evidence are combined to produce a ranking of documents (e.g., texts, pictures, sounds, etc.).",
                "In this paper, we focus on the rank aggregation problem, also called data fusion problem, where rankings of documents, searched into the same collection and provided by multiple methods, are combined in order to produce a new ranking.",
                "In this context, we propose a rank aggregation method within a multiple criteria framework using aggregation mechanisms based on decision rules identifying positive and negative reasons for judging whether a document should get a better rank than another.",
                "We show that the proposed method deals well with the Information Retrieval distinctive features.",
                "Experimental results are reported showing that the suggested method performs better than the well-known CombSUM and CombMNZ operators.",
                "Categories and Subject Descriptors: H.3.3 [Information Systems]: Information Search and Retrieval - Retrieval models.",
                "General Terms: Algorithms, Measurement, Experimentation, Performance, Theory. 1.",
                "INTRODUCTION A wide range of current Information Retrieval (IR) approaches are based on various search models (Boolean, Vector Space, Probabilistic, Language, etc. [2]) in order to retrieve relevant documents in response to a user request.",
                "The result lists produced by these approaches depend on the exact definition of the relevance concept.",
                "Rank aggregation approaches, also called data fusion approaches, consist in combining these result lists in order to produce a new and hopefully better ranking.",
                "Such approaches give rise to metasearch engines in the Web context.",
                "We consider, in the following, cases where only ranks are available and no other additional information is provided such as the relevance scores.",
                "This corresponds indeed to the reality, where only ordinal information is available.",
                "Data fusion is also relevant in other contexts, such as when the user writes several queries of his/her information need (e.g., a boolean query and a natural language query) [4], or when many document surrogates are available [16].",
                "Several studies argued that rank aggregation has the potential of combining effectively all the various sources of evidence considered in various input methods.",
                "For instance, experiments carried out in [16], [30], [4] and [19] showed that documents which appear in the lists of the majority of the input methods are more likely to be relevant.",
                "Moreover, Lee [19] and Vogt and Cottrell [31] found that various retrieval approaches often return very different irrelevant documents, but many of the same relevant documents.",
                "Bartell et al. [3] also found that rank aggregation methods improve the performances w.r.t. those of the input methods, even when some of them have weak individual performances.",
                "These methods also tend to smooth out biases of the input methods according to Montague and Aslam [22].",
                "Data fusion has recently been proved to improve performances for both the ad-hoc retrieval and categorization tasks within the TREC genomics track in 2005 [1].",
                "The rank aggregation problem was addressed in various fields such as i) in social choice theory which studies voting algorithms which specify winners of elections or winners of competitions in tournaments [29], ii) in statistics when studying correlation between rankings, iii) in distributed databases when results from different databases must be combined [12], and iv) in collaborative filtering [23].",
                "Most current rank aggregation methods consider each input ranking as a permutation over the same set of items.",
                "They also give rigid interpretation to the exact ranking of the items.",
                "Both of these assumptions are rather not valid in the IR context, as will be shown in the following sections.",
                "The remaining of the paper is organized as follows.",
                "We first review current rank aggregation methods in Section 2.",
                "Then we outline the specificities of the data fusion problem in the IR context (Section 3).",
                "In Section 4, we present a new aggregation method which is proven to best fit the IR context.",
                "Experimental results are presented in Section 5 and conclusions are provided in a final section. 2.",
                "RELATED WORK As pointed out by Riker [25], we can distinguish two families of rank aggregation methods: positional methods which assign scores to items to be ranked according to the ranks they receive and majoritarian methods which are based on pairwise comparisons of items to be ranked.",
                "These two families of methods find their roots in the pioneering works of Borda [5] and Condorcet [7], respectively, in the social choice literature. 2.1 Preliminaries We first introduce some basic notations to present the rank aggregation methods in a uniform way.",
                "Let D = {d1, d2, . . . , dnd } be a set of nd documents.",
                "A list or a ranking j is an ordering defined on Dj ⊆ D (j = 1, . . . , n).",
                "Thus, di j di means di is ranked better than di in j.",
                "When Dj = D, j is said to be a full list.",
                "Otherwise, it is a partial list.",
                "If di belongs to Dj, rj i denotes the rank or position of di in j.",
                "We assume that the best answer (document) is assigned the position 1 and the worst one is assigned the position |Dj|.",
                "Let D be the set of all permutations on D or all subsets of D. A profile is a n-tuple of rankings PR = ( 1, 2, . . . , n).",
                "Restricting PR to the rankings containing document di defines PRi.",
                "We also call the number of rankings which contain document di the rank hits of di [19].",
                "The rank aggregation or data fusion problem consists of finding a ranking function or mechanism Ψ (also called a social welfare function in the social choice theory terminology) defined by: Ψ : n D → D PR = ( 1, 2, . . . , n) → σ = Ψ(PR) where σ is called a consensus ranking. 2.2 Positional Methods 2.2.1 Borda Count This method [5] first assigns a score n j=1 rj i to each document di.",
                "Documents are then ranked by increasing order of this score, breaking ties, if any, arbitrarily. 2.2.2 Linear Combination Methods This family of methods basically combine scores of documents.",
                "When used for the rank aggregation problem, ranks are assumed to be scores or performances to be combined using aggregation operators such as the weighted sum or some variation of it [3, 31, 17, 28].",
                "For instance, Callan et al. [6] used the inference networks model [30] to combine rankings.",
                "Fox and Shaw [15] proposed several combination strategies which are CombSUM, CombMIN, CombMAX, CombANZ and CombMNZ.",
                "The first three operators correspond to the sum, min and max operators, respectively.",
                "CombANZ and CombMNZ respectively divides and multiplies the CombSUM score by the rank hits.",
                "It is shown in [19] that the CombSUM and CombMNZ operators perform better than the others.",
                "Metasearch engines such as SavvySearch and MetaCrawler use the CombSUM strategy to fuse rankings. 2.2.3 Footrule Optimal Aggregation In this method, a consensus ranking minimizes the Spearman footrule distance from the input rankings [21].",
                "Formally, given two full lists j and j , this distance is given by F( j, j ) = nd i=1 |rj i − rj i |.",
                "It extends to several lists as follows.",
                "Given a profile PR and a consensus ranking σ, the Spearman footrule distance of σ to PR is given by F(σ, PR) = n j=1 F(σ, j).",
                "Cook and Kress [8] proposed a similar method which consists in optimizing the distance D( j, j ) = 1 2 nd i,i =1 |rj i,i − rj i,i |, where rj i,i = rj i −rj i .",
                "This formulation has the advantage that it considers the intensity of preferences. 2.2.4 Probabilistic Methods This kind of methods assume that the performance of the input methods on a number of training queries is indicative of their future performance.",
                "During the training process, probabilities of relevance are calculated.",
                "For subsequent queries, documents are ranked based on these probabilities.",
                "For instance, in [20], each input ranking j is divided into a number of segments, and the conditional probability of relevance (R) of each document di depending on the segment k it occurs in, is computed, i.e. prob(R|di, k, j).",
                "For subsequent queries, the score of each document di is given by n j=1 prob(R|di,k, j ) k .",
                "Le Calve and Savoy [18] suggest using a logistic regression approach for combining scores.",
                "Training data is needed to infer the model parameters. 2.3 Majoritarian Methods 2.3.1 Condorcet Procedure The original Condorcet rule [7] specifies that a winner of the election is any item that beats or ties with every other item in a pairwise contest.",
                "Formally, let C(diσdi ) = { j∈ PR : di j di } be the coalition of rankings that are concordant with establishing diσdi , i.e. with the proposition di should be ranked better than di in the final ranking σ. di beats or ties with di iff |C(diσdi )| ≥ |C(di σdi)|.",
                "The repetitive application of the Condorcet algorithm can produce a ranking of items in a natural way: select the Condorcet winner, remove it from the lists, and repeat the previous two steps until there are no more documents to rank.",
                "Since there is not always Condorcet winners, variations of the Condorcet procedure have been developed within the multiple criteria decision aid theory, with methods such as ELECTRE [26]. 2.3.2 Kemeny Optimal Aggregation As in section 2.2.3, a consensus ranking minimizes a geometric distance from the input rankings, where the Kendall tau distance is used instead of the Spearman footrule distance.",
                "Formally, given two full lists j and j , the Kendall tau distance is given by K( j, j ) = |{(di, di ) : i < i , rj i < rj i , rj i > rj i }|, i.e. the number of pairwise disagreements between the two lists.",
                "It is easy to show that the consensus ranking corresponds to the geometric median of the input rankings and that the Kemeny optimal aggregation problem corresponds to the minimum feedback edge set problem. 2.3.3 Markov Chain Methods Markov chains (MCs) have been used by Dwork et al. [11] as a natural method to obtain a consensus ranking where states correspond to the documents to be ranked and the transition probabilities vary depending on the interpretation of the transition event.",
                "In the same reference, the authors proposed four specific MCs and experimental testing had shown that the following MC is the best performing one (see also [24]): • MC4: move from the current state di to the next state di by first choosing a document di uniformly from D. If for the majority of the rankings, we have rj i ≤ rj i , then move to di , else stay in di.",
                "The consensus ranking corresponds to the stationary distribution of MC4. 3.",
                "SPECIFICITIES OF THE RANK AGGREGATION PROBLEM IN THE IR CONTEXT 3.1 Limited Significance of the Rankings The exact positions of documents in one input ranking have limited significance and should not be overemphasized.",
                "For instance, having three relevant documents in the first three positions, any perturbation of these three items will have the same value.",
                "Indeed, in the IR context, the complete order provided by an input method may hide ties.",
                "In this case, we call such rankings semi orders.",
                "This was outlined in [13] as the problem of aggregation with ties.",
                "It is therefore important to build the consensus ranking based on robust information: • Documents with near positions in j are more likely to have similar interest or relevance.",
                "Thus a slight perturbation of the initial ranking is meaningless. • Assuming that document di is better ranked than document di in a ranking j, di is more likely to be definitively more relevant than di in j when the number of intermediate positions between di and di increases. 3.2 Partial Lists In real world applications, such as metasearch engines, rankings provided by the input methods are often partial lists.",
                "This was outlined in [14] as the problem of having to merge top-k results from various input lists.",
                "For instance, in the experiments carried out by Dwork et al. [11], authors found that among the top 100 best documents of 7 input search engines, 67% of the documents were present in only one search engine, whereas less than two documents were present in all the search engines.",
                "Rank aggregation of partial lists raises four major difficulties which we state hereafter, proposing for each of them various working assumptions: 1.",
                "Partial lists can have various lengths, which can favour long lists.",
                "We thus consider the following two working hypotheses: H1 k : We only consider the top k best documents from each input ranking.",
                "H1 all: We consider all the documents from each input ranking. 2.",
                "Since there are different documents in the input rankings, we must decide which documents should be kept in the consensus ranking.",
                "Two working hypotheses are therefore considered: H2 k : We only consider documents which are present in at least k input rankings (k > 1).",
                "H2 all: We consider all the documents which are ranked in at least one input ranking.",
                "Hereafter, we call documents which will be retained in the consensus ranking, candidate documents, and documents that will be excluded from the consensus ranking, excluded documents.",
                "We also call a candidate document which is missing in one or more rankings, a missing document. 3.",
                "Some candidate documents are missing documents in some input rankings.",
                "Main reasons for a missing document are that it was not indexed or it was indexed but deemed irrelevant ; usually this information is not available.",
                "We consider the following two working hypotheses: H3 yes: Each missing document in each j is assigned a position.",
                "H3 no: No assumption is made, that is each missing document is considered neither better nor worse than any other document. 4.",
                "When assumption H2 k holds, each input ranking may contain documents which will not be considered in the consensus ranking.",
                "Regarding the positions of the candidate documents, we can consider the following working hypotheses: H4 init: The initial positions of candidate documents are kept in each input ranking.",
                "H4 new: Candidate documents receive new positions in each input ranking, after discarding excluded ones.",
                "In the IR context, rank aggregation methods need to decide more or less explicitly which assumptions to retain w.r.t. the above-mentioned difficulties. 4.",
                "OUTRANKING APPROACH FOR RANK AGGREGATION 4.1 Presentation Positional methods consider implicitly that the positions of the documents in the input rankings are scores giving thus a cardinal meaning to an ordinal information.",
                "This constitutes a strong assumption that is questionable, especially when the input rankings have different lengths.",
                "Moreover, for positional methods, assumptions H3 and H4 , which are often arbitrary, have a strong impact on the results.",
                "For instance, let us consider an input ranking of 500 documents out of 1000 candidate documents.",
                "Whether we assign to each of the missing documents the position 1, 501, 750 or 1000 -corresponding to variations of H3 yes- will give rise to very contrasted results, especially regarding the top of the consensus ranking.",
                "Majoritarian methods do not suffer from the above-mentioned drawbacks of the positional methods since they build consensus rankings exploiting only ordinal information contained in the input rankings.",
                "Nevertheless, they suppose that such rankings are complete orders, ignoring that they may hide ties.",
                "Therefore, majoritarian methods base consensus rankings on illusory discriminant information rather than less discriminant but more robust information.",
                "Trying to overcome the limits of current rank aggregation methods, we found that outranking approaches, which were initially used for multiple criteria aggregation problems [26], can also be used for the rank aggregation purpose, where each ranking plays the role of a criterion.",
                "Therefore, in order to decide whether a document di should be ranked better than di in the consensus ranking σ, the two following conditions should be met: • a concordance condition which ensures that a majority of the input rankings are concordant with diσdi (majority principle). • a discordance condition which ensures that none of the discordant input rankings strongly refutes dσd (respect of minorities principle).",
                "Formally, the concordance coalition with diσdi is Csp (diσdi ) = { j∈ PR : rj i ≤ rj i − sp} where sp is a preference threshold which is the variation of document positions -whether it is absolute or relative to the ranking length- which draws the boundaries between an indifference and a preference situation between documents.",
                "The discordance coalition with diσdi is Dsv (diσdi ) = { j∈ PR : rj i ≥ rj i + sv} where sv is a veto threshold which is the variation of document positions -whether it is absolute or relative to the ranking length- which draws the boundaries between a weak and a strong opposition to diσdi .",
                "Depending on the exact definition of the preceding concordance and discordance coalitions leading to the definition of some decision rules, several outranking relations can be defined.",
                "They can be more or less demanding depending on i) the values of the thresholds sp and sv, ii) the importance or minimal size cmin required for the concordance coalition, and iii) the importance or maximum size dmax of the discordance coalition.",
                "A generic outranking relation can thus be defined as follows: diS(sp,sv,cmin,dmax)di ⇔ |Csp (diσdi )| ≥ cmin AND |Dsv (diσdi )| ≤ dmax This expression defines a family of nested outranking relations since S(sp,sv,cmin,dmax) ⊆ S(sp,sv,cmin,dmax) when cmin ≥ cmin and/or dmax ≤ dmax and/or sp ≥ sp and/or sv ≤ sv.",
                "This expression also generalizes the majority rule which corresponds to the particular relation S(0,∞, n 2 ,n).",
                "It also satisfies important properties of rank aggregation methods, called neutrality, Pareto-optimality, Condorcet property and Extended Condorcet property, in the social choice literature [29].",
                "Outranking relations are not necessarily transitive and do not necessarily correspond to rankings since directed cycles may exist.",
                "Therefore, we need specific procedures in order to derive a consensus ranking.",
                "We propose the following procedure which finds its roots in [27].",
                "It consists in partitioning the set of documents into r ranked classes.",
                "Each class Ch contains documents with the same relevance and results from the application of all relations (if possible) to the set of documents remaining after previous classes are computed.",
                "Documents within the same equivalence class are ranked arbitrarily.",
                "Formally, let • R be the set of candidate documents for a query, • S1 , S2 , . . . be a family of nested outranking relations, • Fk(di, E) = |{di ∈ E : diSk di }| be the number of documents in E(E ⊆ R) that could be considered worse than di according to relation Sk , • fk(di, E) = |{di ∈ E : di Sk di}| be the number of documents in E that could be considered better than di according to Sk , • sk(di, E) = Fk(di, E) − fk(di, E) be the qualification of di in E according to Sk .",
                "Each class Ch results from a distillation process.",
                "It corresponds to the last distillate of a series of sets E0 ⊇ E1 ⊇ . . . where E0 = R \\ (C1 ∪ . . . ∪ Ch−1) and Ek is a reduced subset of Ek−1 resulting from the application of the following procedure: 1. compute for each di ∈ Ek−1 its qualification according to Sk , i.e. sk(di, Ek−1), 2. define smax = maxdi∈Ek−1 {sk(di, Ek−1)}, then 3.",
                "Ek = {di ∈ Ek−1 : sk(di, Ek−1) = smax} When one outranking relation is used, the distillation process stops after the first application of the previous procedure, i.e., Ch corresponds to distillate E1.",
                "When different outranking relations are used, the distillation process stops when all the pre-defined outranking relations have been used or when |Ek| = 1. 4.2 Illustrative Example This section illustrates the concepts and procedures of section 4.1.",
                "Let us consider a set of candidate documents R = {d1, d2, d3, d4, d5}.",
                "The following table gives a profile PR of different rankings of the documents of R: PR = ( 1 , 2, 3, 4).",
                "Table 1: Rankings of documents rj i 1 2 3 4 d1 1 3 1 5 d2 2 1 3 3 d3 3 2 2 1 d4 4 4 5 2 d5 5 5 4 4 Let us suppose that the preference and veto thresholds are set to values 1 and 4 respectively, and that the concordance and discordance thresholds are set to values 2 and 1 respectively.",
                "The following tables give the concordance, discordance and outranking matrices.",
                "Each entry csp (di, di ) (dsv (di, di )) in the concordance (discordance) matrix gives the number of rankings that are concordant (discordant) with diσdi , i.e. csp (di, di ) = |Csp (diσdi )| and dsv (di, di ) = |Dsv (diσdi )|.",
                "Table 2: Computation of the outranking relation d1 d2 d3 d4 d5 d1 - 2 2 3 3 d2 2 - 2 3 4 d3 2 2 - 4 4 d4 1 1 0 - 3 d5 1 0 0 1Concordance Matrix d1 d2 d3 d4 d5 d1 - 0 1 0 0 d2 0 - 0 0 0 d3 0 0 - 0 0 d4 1 0 0 - 0 d5 1 1 0 0Discordance Matrix d1 d2 d3 d4 d5 d1 - 1 1 1 1 d2 1 - 1 1 1 d3 1 1 - 1 1 d4 0 0 0 - 1 d5 0 0 0 0Outranking Matrix (S1) For instance, the concordance coalition for the assertion d1σd4 is C1(d1σd4) = { 1, 2, 3} and the discordance coalition for the same assertion is D4(d1σd4) = ∅.",
                "Therefore, c1(d1, d4) = 3, d4(d1, d4) = 0 and d1S1 d4 holds.",
                "Notice that Fk(di, R) (fk(di, R)) is given by summing the values of the ith row (column) of the outranking matrix.",
                "The consensus ranking is obtained as follows: to get the first class C1, we compute the qualifications of all the documents of E0 = R with respect to S1 .",
                "They are respectively 2, 2, 2, -2 and -4.",
                "Therefore smax equals 2 and C1 = E1 = {d1, d2, d3}.",
                "Observe that, if we had used a second outranking relation S2(⊇ S1), these three documents could have been possibly discriminated.",
                "At this stage, we remove documents of C1 from the outranking matrix and compute the next class C2: we compute the new qualifications of the documents of E0 = R \\ C1 = {d4, d5}.",
                "They are respectively 1 and -1.",
                "So C3 = E1 = {d4}.",
                "The last document d5 is the only document of the last class C3.",
                "Thus, the consensus ranking is {d1, d2, d3} → {d4} → {d5}. 5.",
                "EXPERIMENTS AND RESULTS 5.1 Test Setting To facilitate empirical investigation of the proposed methodology, we developed a prototype metasearch engine that implements a version of our outranking approach for rank aggregation.",
                "In this paper, we apply our approach to the Topic Distillation (TD) task of TREC-2004 Web track [10].",
                "In this task, there are 75 topics where only a short description of each is given.",
                "For each query, we retained the rankings of the 10 best runs of the TD task which are provided by TREC-2004 participating teams.",
                "The performances of these runs are reported in table 3.",
                "Table 3: Performances of the 10 best runs of the TD task of TREC-2004 Run Id MAP P@10 S@1 S@5 S@10 uogWebCAU150 17.9% 24.9% 50.7% 77.3% 89.3% MSRAmixed1 17.8% 25.1% 38.7% 72.0% 88.0% MSRC04C12 16.5% 23.1% 38.7% 74.7% 80.0% humW04rdpl 16.3% 23.1% 37.3% 78.7% 90.7% THUIRmix042 14.7% 20.5% 21.3% 58.7% 74.7% UAmsT04MWScb 14.6% 20.9% 36.0% 66.7% 76.0% ICT04CIIS1AT 14.1% 20.8% 33.3% 64.0% 78.7% SJTUINCMIX5 12.9% 18.9% 29.3% 57.3% 72.0% MU04web1 11.5% 19.9% 33.3% 64.0% 76.0% MeijiHILw3 11.5% 15.3% 30.7% 54.7% 64.0% Average 14.7% 21.2% 34.9% 66.8% 78.94% For each query, each run provides a ranking of about 1000 documents.",
                "The number of documents retrieved by all these runs ranges from 543 to 5769.",
                "Their average (median) number is 3340 (3386).",
                "It is worth noting that we found similar distributions of the documents among the rankings as in [11].",
                "For evaluation, we used the trec eval standard tool which is used by the TREC community to calculate the standard measures of system effectiveness which are Mean Average Precision (MAP) and Success@n (S@n) for n=1, 5 and 10.",
                "Our approach effectiveness is compared against some high performing official results from TREC-2004 as well as against some standard rank aggregation algorithms.",
                "In the experiments, significance testing is mainly based on the t-student statistic which is computed on the basis of the MAP values of the compared runs.",
                "In the tables of the following section, statistically significant differences are marked with an asterisk.",
                "Values between brackets of the first column of each table, indicate the parameter value of the corresponding run. 5.2 Results We carried out several series of runs in order to i) study performance variations of the outranking approach when tuning the parameters and working assumptions, ii) compare performances of the outranking approach vs standard rank aggregation strategies , and iii) check whether rank aggregation performs better than the best input rankings.",
                "We set our basic run mcm with the following parameters.",
                "We considered that each input ranking is a complete order (sp = 0) and that an input ranking strongly refutes diσdi when the difference of both document positions is large enough (sv = 75%).",
                "Preference and veto thresholds are computed proportionally to the number of documents retained in each input ranking.",
                "They consequently may vary from one ranking to another.",
                "In addition, to accept the assertion diσdi , we supposed that the majority of the rankings must be concordant (cmin = 50%) and that every input ranking can impose its veto (dmax = 0).",
                "Concordance and discordance thresholds are computed for each tuple (di, di ) as the percentage of the input rankings of PRi ∩PRi .",
                "Thus, our choice of parameters leads to the definition of the outranking relation S(0,75%,50%,0).",
                "To test the run mcm, we had chosen the following assumptions.",
                "We retained the top 100 best documents from each input ranking (H1 100), only considered documents which are present in at least half of the input rankings (H2 5 ) and assumed H3 no and H4 new.",
                "In these conditions, the number of successful documents was about 100 on average, and the computation time per query was less than one second.",
                "Obviously, modifying the working assumptions should have deeper impact on the performances than tuning our model parameters.",
                "This was validated by preliminary experiments.",
                "Thus, we hereafter begin by studying performance variation when different sets of assumptions are considered.",
                "Afterwards, we study the impact of tuning parameters.",
                "Finally, we compare our model performances w.r.t. the input rankings as well as some standard data fusion algorithms. 5.2.1 Impact of the Working Assumptions Table 4 summarizes the performance variation of the outranking approach under different working hypotheses.",
                "In Table 4: Impact of the working assumptions Run Id MAP S@1 S@5 S@10 mcm 18.47% 41.33% 81.33% 86.67% mcm22 (H3 yes) 17.72% (-4.06%) 34.67% 81.33% 86.67% mcm23 (H4 init) 18.26% (-1.14%) 41.33% 81.33% 86.67% mcm24 (H1 all) 20.67% (+11.91%*) 38.66% 80.00% 86.66% mcm25 (H2 all) 21.68% (+17.38%*) 40.00% 78.66% 89.33% this table, we first show that run mcm22, in which missing documents are all put in the same last position of each input ranking, leads to performance drop w.r.t. run mcm.",
                "Moreover, S@1 moves from 41.33% to 34.67% (-16.11%).",
                "This shows that several relevant documents which were initially put at the first position of the consensus ranking in mcm, lose this first position but remain ranked in the top 5 documents since S@5 did not change.",
                "We also conclude that documents which have rather good positions in some input rankings are more likely to be relevant, even though they are missing in some other rankings.",
                "Consequently, when they are missing in some rankings, assigning worse ranks to these documents is harmful for performance.",
                "Also, from Table 4, we found that the performances of runs mcm and mcm23 are similar.",
                "Therefore, the outranking approach is not sensitive to keeping the initial positions of candidate documents or recomputing them by discarding excluded ones.",
                "From the same Table 4, performance of the outranking approach increases significantly for runs mcm24 and mcm25.",
                "Therefore, whether we consider all the documents which are present in half of the rankings (mcm24) or we consider all the documents which are ranked in the first 100 positions in one or more rankings (mcm25), increases performances.",
                "This result was predictable since in both cases we have more detailed information on the relative importance of documents.",
                "Tables 5 and 6 confirm this evidence.",
                "Table 5, where values between brackets of the first column give the number of documents which are retained from each input ranking, shows that selecting more documents from each input ranking leads to performance increase.",
                "It is worth mentioning that selecting more than 600 documents from each input ranking does not improve performance.",
                "Table 5: Impact of the number of retained documents Run Id MAP S@1 S@5 S@10 mcm (100) 18.47% 41.33% 81.33% 86.67% mcm24-1 (200) 19.32% (+4.60%) 42.67% 78.67% 88.00% mcm24-2 (400) 19.88% (+7.63%*) 37.33% 80.00% 88.00% mcm24-3 (600) 20.80% (+12.62%*) 40.00% 80.00% 88.00% mcm24-4 (800) 20.66% (+11.86%*) 40.00% 78.67% 86.67% mcm24 (1000) 20.67% (+11.91%*) 38.66% 80.00% 86.66% Table 6 reports runs corresponding to variations of H2 k .",
                "Values between brackets are rank hits.",
                "For instance, in the run mcm32, only documents which are present in 3 or more input rankings, were considered successful.",
                "This table shows that performance is significantly better when rare documents are considered, whereas it decreases significantly when these documents are discarded.",
                "Therefore, we conclude that many of the relevant documents are retrieved by a rather small set of IR models.",
                "Table 6: Performance considering different rank hits Run Id MAP S@1 S@5 S@10 mcm25 (1) 21.68% (+17.38%*) 40.00% 78.67% 89.33% mcm32 (3) 18.98% (+2.76%) 38.67% 80.00% 85.33% mcm (5) 18.47% 41.33% 81.33% 86.67% mcm33 (7) 15.83% (-14.29%*) 37.33% 78.67% 85.33% mcm34 (9) 10.96% (-40.66%*) 36.11% 66.67% 70.83% mcm35 (10) 7.42% (-59.83%*) 39.22% 62.75% 64.70% For both runs mcm24 and mcm25, the number of successful documents was about 1000 and therefore, the computation time per query increased and became around 5 seconds. 5.2.2 Impact of the Variation of the Parameters Table 7 shows performance variation of the outranking approach when different preference thresholds are considered.",
                "We found performance improvement up to threshold values of about 5%, then there is a decrease in the performance which becomes significant for threshold values greater than 10%.",
                "Moreover, S@1 improves from 41.33% to 46.67% when preference threshold changes from 0 to 5%.",
                "We can thus conclude that the input rankings are semi orders rather than complete orders.",
                "Table 8 shows the evolution of the performance measures w.r.t. the concordance threshold.",
                "We can conclude that in order to put document di before di in the consensus ranking, Table 7: Impact of the variation of the preference threshold from 0 to 12.5% Run Id MAP S@1 S@5 S@10 mcm (0%) 18.47% 41.33% 81.33% 86.67% mcm1 (1%) 18.57% (+0.54%) 41.33% 81.33% 86.67% mcm2 (2.5%) 18.63% (+0.87%) 42.67% 78.67% 86.67% mcm3 (5%) 18.69% (+1.19%) 46.67% 81.33% 86.67% mcm4 (7.5%) 18.24% (-1.25%) 46.67% 81.33% 86.67% mcm5 (10%) 17.93% (-2.92%) 40.00% 82.67% 86.67% mcm5b (12.5%) 17.51% (-5.20%*) 41.33% 80.00% 86.67% at least half of the input rankings of PRi ∩ PRi should be concordant.",
                "Performance drops significantly for very low and very high values of the concordance threshold.",
                "In fact, for such values, the concordance condition is either fulfilled rather always by too many document pairs or not fulfilled at all, respectively.",
                "Therefore, the outranking relation becomes either too weak or too strong respectively.",
                "Table 8: Impact of the variation of cmin Run Id MAP S@1 S@5 S@10 mcm11 (20%) 17.63% (-4.55%*) 41.33% 76.00% 85.33% mcm12 (40%) 18.37% (-0.54%) 42.67% 76.00% 86.67% mcm (50%) 18.47% 41.33% 81.33% 86.67% mcm13 (60%) 18.42% (-0.27%) 40.00% 78.67% 86.67% mcm14 (80%) 17.43% (-5.63%*) 40.00% 78.67% 86.67% mcm15 (100%) 16.12% (-12.72%*) 41.33% 70.67% 85.33% In the experiments, varying the veto threshold as well as the discordance threshold within reasonable intervals does not have significant impact on performance measures.",
                "In fact, runs with different veto thresholds (sv ∈ [50%; 100%]) had similar performances even though there is a slight advantage for runs with high threshold values which means that it is better not to allow the input rankings to put their veto easily.",
                "Also, tuning the discordance threshold was carried out for values 50% and 75% of the veto threshold.",
                "For these runs we did not get any noticeable performance variation, although for low discordance thresholds (dmax < 20%), performance slightly decreased. 5.2.3 Impact of the Variation of the Number of Input Rankings To study performance evolution when different sets of input rankings are considered, we carried three more runs where 2, 4, and 6 of the best performing sets of the input rankings are considered.",
                "Results reported in Table 9 are seemingly counter-intuitive and also do not support previous findings regarding rank aggregation research [3].",
                "Nevertheless, this result shows that low performing rankings bring more noise than information to the establishment of the consensus ranking.",
                "Therefore, when they are considered, performance decreases.",
                "Table 9: Performance considering different best performing sets of input rankings Run Id MAP S@1 S@5 S@10 mcm (10) 18.47% 41.33% 81.33% 86.67% mcm27 (6) 18.60% (+0.70%) 41.33% 80.00% 85.33% mcm28 (4) 19.02% (+2.98%) 40.00% 86.67% 88.00% mcm29 (2) 18.33% (-0.76%) 44.00% 76.00% 88.00% 5.2.4 Comparison of the Performance of Different Rank Aggregation Methods In this set of runs, we compare the outranking approach with some standard rank aggregation methods which were proven to have acceptable performance in previous studies: we considered two positional methods which are the CombSUM and the CombMNZ strategies.",
                "We also examined the performance of one majoritarian method which is the Markov chain method (MC4).",
                "For the comparisons, we considered a specific outranking relation S∗ = S(5%,50%,50%,30%) which results in good overall performances when tuning all the parameters.",
                "The first row of Table 10 gives performances of the rank aggregation methods w.r.t. a basic assumption set A1 = (H1 100, H2 5 , H4 new): we only consider the 100 first documents from each ranking, then retain documents present in 5 or more rankings and update ranks of successful documents.",
                "For positional methods, we place missing documents at the queue of the ranking (H3 yes) whereas for our method as well as for MC4, we retained hypothesis H3 no.",
                "The three following rows of Table 10 report performances when changing one element from the basic assumption set: the second row corresponds to the assumption set A2 = (H1 1000, H2 5 , H4 new), i.e. changing the number of retained documents from 100 to 1000.",
                "The third row corresponds to the assumption set A3 = (H1 100, H2 all, H4 new), i.e. considering the documents present in at least one ranking.",
                "The fourth row corresponds to the assumption set A4 = (H1 100, H2 5 , H4 init), i.e. keeping the original ranks of successful documents.",
                "The fifth row of Table 10, labeled A5, gives performance when all the 225 queries of the Web track of TREC-2004 are considered.",
                "Obviously, performance level cannot be compared with previous lines since the additional queries are different from the TD queries and correspond to other tasks (Home Page and Named Page tasks [10]) of TREC-2004 Web track.",
                "This set of runs aims to show whether relative performance of the various methods is task-dependent.",
                "The last row of Table 10, labeled A6, reports performance of the various methods considering the TD task of TREC2002 instead of TREC-2004: we fused the results of input rankings of the 10 best official runs for each of the 50 TD queries [9] considering the set of assumptions A1 of the first row.",
                "This aims to show whether relative performance of the various methods changes from year to year.",
                "Values between brackets of Table 10 are variations of performance of each rank aggregation method w.r.t. performance of the outranking approach.",
                "Table 10: Performance (MAP) of different rank aggregation methods under 3 different test collections mcm combSUM combMNZ markov A1 18.79% 17.54% (-6.65%*) 17.08% (-9.10%*) 18.63% (-0.85%) A2 21.36% 19.18% (-10.21%*) 18.61% (-12.87%*) 21.33% (-0.14%) A3 21.92% 21.38% (-2.46%) 20.88% (-4.74%) 19.35% (-11.72%*) A4 18.64% 17.58% (-5.69%*) 17.18% (-7.83%*) 18.63% (-0.05%) A5 55.39% 52.16% (-5.83%*) 49.70% (-10.27%*) 53.30% (-3.77%) A6 16.95% 15.65% (-7.67%*) 14.57% (-14.04%*) 16.39% (-3.30%) From the analysis of table 10 the following can be established: • for all the runs, considering all the documents in each input ranking (A2) significantly improves performance (MAP increases by 11.62% on average).",
                "This is predictable since some initially unreported relevant documents would receive better positions in the consensus ranking. • for all the runs, considering documents even those present in only one input ranking (A3) significantly improves performance.",
                "For mcm, combSUM and combMNZ, performance improvement is more important (MAP increases by 20.27% on average) than for the markov run (MAP increases by 3.86%). • preserving the initial positions of documents (A4) or recomputing them (A1) does not have a noticeable influence on performance for both positional and majoritarian methods. • considering all the queries of the Web track of TREC2004 (A5) as well as the TD queries of the Web track of TREC-2002 (A6) does not alter the relative performance of the different data fusion methods. • considering the TD queries of the Web track of TREC2002, performances of all the data fusion methods are lower than that of the best performing input ranking for which the MAP value equals 18.58%.",
                "This is because most of the fused input rankings have very low performances compared to the best one, which brings more noise to the consensus ranking. • performances of the data fusion methods mcm and markov are significantly better than that of the best input ranking uogWebCAU150.",
                "This remains true for runs combSUM and combMNZ only under assumptions H1 all or H2 all.",
                "This shows that majoritarian methods are less sensitive to assumptions than positional methods. • outranking approach always performs significantly better than positional methods combSUM and combMNZ.",
                "It has also better performances than the Markov chain method, especially under assumption H2 all where difference of performances becomes significant. 6.",
                "CONCLUSIONS In this paper, we address the rank aggregation problem where different, but not disjoint, lists of documents are to be fused.",
                "We noticed that the input rankings can hide ties, so they should not be considered as complete orders.",
                "Only robust information should be used from each input ranking.",
                "Current rank aggregation methods, and especially positional methods (e.g. combSUM [15]), are not initially designed to work with such rankings.",
                "They should be adapted by considering specific working assumptions.",
                "We propose a new outranking method for rank aggregation which is well adapted to the IR context.",
                "Indeed, it ranks two documents w.r.t. the intensity of their positions difference in each input ranking and also considering the number of the input rankings that are concordant and discordant in favor of a specific document.",
                "There is also no need to make specific assumptions on the positions of the missing documents.",
                "This is an important feature since the absence of a document from a ranking should not be necessarily interpreted negatively.",
                "Experimental results show that the outranking method significantly out-performs popular classical positional data fusion methods like combSUM and combMNZ strategies.",
                "It also out-performs a good performing majoritarian methods which is the Markov chain method.",
                "These results are tested against different test collections and queries.",
                "From the experiments, we can also conclude that in order to improve the performances, we should fuse result lists of well performing IR models, and that majoritarian data fusion methods perform better than positional methods.",
                "The proposed method can have a real impact on Web metasearch performances since only ranks are available from most primary search engines, whereas most of the current approaches need scores to merge result lists into one single list.",
                "Further work involves investigating whether the outranking approach performs well in various other contexts, e.g. using the document scores or some combination of document ranks and scores.",
                "Acknowledgments The authors would like to thank Jacques Savoy for his valuable comments on a preliminary version of this paper. 7.",
                "REFERENCES [1] A. Aronson, D. Demner-Fushman, S. Humphrey, J. Lin, H. Liu, P. Ruch, M. Ruiz, L. Smith, L. Tanabe, and W. Wilbur.",
                "Fusion of knowledge-intensive and statistical approaches for retrieving and annotating textual genomics documents.",
                "In Proceedings TREC2005.",
                "NIST Publication, 2005. [2] R. A. Baeza-Yates and B.",
                "A. Ribeiro-Neto.",
                "Modern Information Retrieval.",
                "ACM Press , 1999. [3] B. T. Bartell, G. W. Cottrell, and R. K. Belew.",
                "Automatic combination of multiple ranked retrieval systems.",
                "In Proceedings ACM-SIGIR94, pages 173-181.",
                "Springer-Verlag, 1994. [4] N. J. Belkin, P. Kantor, E. A.",
                "Fox, and J.",
                "A. Shaw.",
                "Combining evidence of multiple query representations for information retrieval.",
                "IPM, 31(3):431-448, 1995. [5] J. Borda.",
                "M´emoire sur les ´elections au scrutin.",
                "Histoire de lAcad´emie des Sciences, 1781. [6] J. P. Callan, Z. Lu, and W. B. Croft.",
                "Searching distributed collections with inference networks.",
                "In Proceedings ACM-SIGIR95, pages 21-28, 1995. [7] M. Condorcet.",
                "Essai sur lapplication de lanalyse `a la probabilit´e des d´ecisions rendues `a la pluralit´e des voix.",
                "Imprimerie Royale, Paris, 1785. [8] W. D. Cook and M. Kress.",
                "Ordinal ranking with intensity of preference.",
                "Management Science, 31(1):26-32, 1985. [9] N. Craswell and D. Hawking.",
                "Overview of the TREC-2002 Web Track.",
                "In Proceedings TREC2002.",
                "NIST Publication, 2002. [10] N. Craswell and D. Hawking.",
                "Overview of the TREC-2004 Web Track.",
                "In Proceedings of TREC2004.",
                "NIST Publication, 2004. [11] C. Dwork, S. R. Kumar, M. Naor, and D. Sivakumar.",
                "Rank aggregation methods for the Web.",
                "In Proceedings WWW2001, pages 613-622, 2001. [12] R. Fagin.",
                "Combining fuzzy information from multiple systems.",
                "JCSS, 58(1):83-99, 1999. [13] R. Fagin, R. Kumar, M. Mahdian, D. Sivakumar, and E. Vee.",
                "Comparing and aggregating rankings with ties.",
                "In PODS, pages 47-58, 2004. [14] R. Fagin, R. Kumar, and D. Sivakumar.",
                "Comparing top k lists.",
                "SIAM J. on Discrete Mathematics, 17(1):134-160, 2003. [15] E. A.",
                "Fox and J.",
                "A. Shaw.",
                "Combination of multiple searches.",
                "In Proceedings of TREC3.",
                "NIST Publication, 1994. [16] J. Katzer, M. McGill, J. Tessier, W. Frakes, and P. DasGupta.",
                "A study of the overlap among document representations.",
                "Information Technology: Research and Development, 1(4):261-274, 1982. [17] L. S. Larkey, M. E. Connell, and J. Callan.",
                "Collection selection and results merging with topically organized U.S. patents and TREC data.",
                "In Proceedings ACM-CIKM2000, pages 282-289.",
                "ACM Press, 2000. [18] A.",
                "Le Calv´e and J. Savoy.",
                "Database merging strategy based on logistic regression.",
                "IPM, 36(3):341-359, 2000. [19] J. H. Lee.",
                "Analyses of multiple evidence combination.",
                "In Proceedings ACM-SIGIR97, pages 267-276, 1997. [20] D. Lillis, F. Toolan, R. Collier, and J. Dunnion.",
                "Probfuse: a probabilistic approach to data fusion.",
                "In Proceedings ACM-SIGIR2006, pages 139-146.",
                "ACM Press, 2006. [21] J. I. Marden.",
                "Analyzing and Modeling Rank Data.",
                "Number 64 in Monographs on Statistics and Applied Probability.",
                "Chapman & Hall, 1995. [22] M. Montague and J.",
                "A. Aslam.",
                "Metasearch consistency.",
                "In Proceedings ACM-SIGIR2001, pages 386-387.",
                "ACM Press, 2001. [23] D. M. Pennock and E. Horvitz.",
                "Analysis of the axiomatic foundations of collaborative filtering.",
                "In Workshop on AI for Electronic Commerce at the 16th National Conference on Artificial Intelligence, 1999. [24] M. E. Renda and U. Straccia.",
                "Web metasearch: rank vs. score based rank aggregation methods.",
                "In Proceedings ACM-SAC2003, pages 841-846.",
                "ACM Press, 2003. [25] W. H. Riker.",
                "Liberalism against populism.",
                "Waveland Press, 1982. [26] B. Roy.",
                "The outranking approach and the foundations of ELECTRE methods.",
                "Theory and Decision, 31:49-73, 1991. [27] B. Roy and J. Hugonnard.",
                "Ranking of suburban line extension projects on the Paris metro system by a multicriteria method.",
                "Transportation Research, 16A(4):301-312, 1982. [28] L. Si and J. Callan.",
                "Using sampled data and regression to merge search engine results.",
                "In Proceedings ACM-SIGIR2002, pages 19-26.",
                "ACM Press, 2002. [29] M. Truchon.",
                "An extension of the Condorcet criterion and Kemeny orders.",
                "Cahier 9813, Centre de Recherche en Economie et Finance Appliqu´ees, Oct. 1998. [30] H. Turtle and W. B. Croft.",
                "Inference networks for document retrieval.",
                "In Proceedings of ACM-SIGIR90, pages 1-24.",
                "ACM Press, 1990. [31] C. C. Vogt and G. W. Cottrell.",
                "Fusion via a linear combination of scores.",
                "Information Retrieval, 1(3):151-173, 1999."
            ],
            "original_annotated_samples": [],
            "translated_annotated_samples": [],
            "translated_text": "",
            "candidates": [],
            "error": []
        },
        "multiple criterium approach": {
            "translated_key": "enfoque de criterio múltiple",
            "is_in_text": false,
            "original_annotated_sentences": [
                "An Outranking Approach for Rank Aggregation in Information Retrieval Mohamed Farah Lamsade, Paris Dauphine University Place du Mal de Lattre de Tassigny 75775 Paris Cedex 16, France farah@lamsade.dauphine.fr Daniel Vanderpooten Lamsade, Paris Dauphine University Place du Mal de Lattre de Tassigny 75775 Paris Cedex 16, France vdp@lamsade.dauphine.fr ABSTRACT Research in Information Retrieval usually shows performance improvement when many sources of evidence are combined to produce a ranking of documents (e.g., texts, pictures, sounds, etc.).",
                "In this paper, we focus on the rank aggregation problem, also called data fusion problem, where rankings of documents, searched into the same collection and provided by multiple methods, are combined in order to produce a new ranking.",
                "In this context, we propose a rank aggregation method within a multiple criteria framework using aggregation mechanisms based on decision rules identifying positive and negative reasons for judging whether a document should get a better rank than another.",
                "We show that the proposed method deals well with the Information Retrieval distinctive features.",
                "Experimental results are reported showing that the suggested method performs better than the well-known CombSUM and CombMNZ operators.",
                "Categories and Subject Descriptors: H.3.3 [Information Systems]: Information Search and Retrieval - Retrieval models.",
                "General Terms: Algorithms, Measurement, Experimentation, Performance, Theory. 1.",
                "INTRODUCTION A wide range of current Information Retrieval (IR) approaches are based on various search models (Boolean, Vector Space, Probabilistic, Language, etc. [2]) in order to retrieve relevant documents in response to a user request.",
                "The result lists produced by these approaches depend on the exact definition of the relevance concept.",
                "Rank aggregation approaches, also called data fusion approaches, consist in combining these result lists in order to produce a new and hopefully better ranking.",
                "Such approaches give rise to metasearch engines in the Web context.",
                "We consider, in the following, cases where only ranks are available and no other additional information is provided such as the relevance scores.",
                "This corresponds indeed to the reality, where only ordinal information is available.",
                "Data fusion is also relevant in other contexts, such as when the user writes several queries of his/her information need (e.g., a boolean query and a natural language query) [4], or when many document surrogates are available [16].",
                "Several studies argued that rank aggregation has the potential of combining effectively all the various sources of evidence considered in various input methods.",
                "For instance, experiments carried out in [16], [30], [4] and [19] showed that documents which appear in the lists of the majority of the input methods are more likely to be relevant.",
                "Moreover, Lee [19] and Vogt and Cottrell [31] found that various retrieval approaches often return very different irrelevant documents, but many of the same relevant documents.",
                "Bartell et al. [3] also found that rank aggregation methods improve the performances w.r.t. those of the input methods, even when some of them have weak individual performances.",
                "These methods also tend to smooth out biases of the input methods according to Montague and Aslam [22].",
                "Data fusion has recently been proved to improve performances for both the ad-hoc retrieval and categorization tasks within the TREC genomics track in 2005 [1].",
                "The rank aggregation problem was addressed in various fields such as i) in social choice theory which studies voting algorithms which specify winners of elections or winners of competitions in tournaments [29], ii) in statistics when studying correlation between rankings, iii) in distributed databases when results from different databases must be combined [12], and iv) in collaborative filtering [23].",
                "Most current rank aggregation methods consider each input ranking as a permutation over the same set of items.",
                "They also give rigid interpretation to the exact ranking of the items.",
                "Both of these assumptions are rather not valid in the IR context, as will be shown in the following sections.",
                "The remaining of the paper is organized as follows.",
                "We first review current rank aggregation methods in Section 2.",
                "Then we outline the specificities of the data fusion problem in the IR context (Section 3).",
                "In Section 4, we present a new aggregation method which is proven to best fit the IR context.",
                "Experimental results are presented in Section 5 and conclusions are provided in a final section. 2.",
                "RELATED WORK As pointed out by Riker [25], we can distinguish two families of rank aggregation methods: positional methods which assign scores to items to be ranked according to the ranks they receive and majoritarian methods which are based on pairwise comparisons of items to be ranked.",
                "These two families of methods find their roots in the pioneering works of Borda [5] and Condorcet [7], respectively, in the social choice literature. 2.1 Preliminaries We first introduce some basic notations to present the rank aggregation methods in a uniform way.",
                "Let D = {d1, d2, . . . , dnd } be a set of nd documents.",
                "A list or a ranking j is an ordering defined on Dj ⊆ D (j = 1, . . . , n).",
                "Thus, di j di means di is ranked better than di in j.",
                "When Dj = D, j is said to be a full list.",
                "Otherwise, it is a partial list.",
                "If di belongs to Dj, rj i denotes the rank or position of di in j.",
                "We assume that the best answer (document) is assigned the position 1 and the worst one is assigned the position |Dj|.",
                "Let D be the set of all permutations on D or all subsets of D. A profile is a n-tuple of rankings PR = ( 1, 2, . . . , n).",
                "Restricting PR to the rankings containing document di defines PRi.",
                "We also call the number of rankings which contain document di the rank hits of di [19].",
                "The rank aggregation or data fusion problem consists of finding a ranking function or mechanism Ψ (also called a social welfare function in the social choice theory terminology) defined by: Ψ : n D → D PR = ( 1, 2, . . . , n) → σ = Ψ(PR) where σ is called a consensus ranking. 2.2 Positional Methods 2.2.1 Borda Count This method [5] first assigns a score n j=1 rj i to each document di.",
                "Documents are then ranked by increasing order of this score, breaking ties, if any, arbitrarily. 2.2.2 Linear Combination Methods This family of methods basically combine scores of documents.",
                "When used for the rank aggregation problem, ranks are assumed to be scores or performances to be combined using aggregation operators such as the weighted sum or some variation of it [3, 31, 17, 28].",
                "For instance, Callan et al. [6] used the inference networks model [30] to combine rankings.",
                "Fox and Shaw [15] proposed several combination strategies which are CombSUM, CombMIN, CombMAX, CombANZ and CombMNZ.",
                "The first three operators correspond to the sum, min and max operators, respectively.",
                "CombANZ and CombMNZ respectively divides and multiplies the CombSUM score by the rank hits.",
                "It is shown in [19] that the CombSUM and CombMNZ operators perform better than the others.",
                "Metasearch engines such as SavvySearch and MetaCrawler use the CombSUM strategy to fuse rankings. 2.2.3 Footrule Optimal Aggregation In this method, a consensus ranking minimizes the Spearman footrule distance from the input rankings [21].",
                "Formally, given two full lists j and j , this distance is given by F( j, j ) = nd i=1 |rj i − rj i |.",
                "It extends to several lists as follows.",
                "Given a profile PR and a consensus ranking σ, the Spearman footrule distance of σ to PR is given by F(σ, PR) = n j=1 F(σ, j).",
                "Cook and Kress [8] proposed a similar method which consists in optimizing the distance D( j, j ) = 1 2 nd i,i =1 |rj i,i − rj i,i |, where rj i,i = rj i −rj i .",
                "This formulation has the advantage that it considers the intensity of preferences. 2.2.4 Probabilistic Methods This kind of methods assume that the performance of the input methods on a number of training queries is indicative of their future performance.",
                "During the training process, probabilities of relevance are calculated.",
                "For subsequent queries, documents are ranked based on these probabilities.",
                "For instance, in [20], each input ranking j is divided into a number of segments, and the conditional probability of relevance (R) of each document di depending on the segment k it occurs in, is computed, i.e. prob(R|di, k, j).",
                "For subsequent queries, the score of each document di is given by n j=1 prob(R|di,k, j ) k .",
                "Le Calve and Savoy [18] suggest using a logistic regression approach for combining scores.",
                "Training data is needed to infer the model parameters. 2.3 Majoritarian Methods 2.3.1 Condorcet Procedure The original Condorcet rule [7] specifies that a winner of the election is any item that beats or ties with every other item in a pairwise contest.",
                "Formally, let C(diσdi ) = { j∈ PR : di j di } be the coalition of rankings that are concordant with establishing diσdi , i.e. with the proposition di should be ranked better than di in the final ranking σ. di beats or ties with di iff |C(diσdi )| ≥ |C(di σdi)|.",
                "The repetitive application of the Condorcet algorithm can produce a ranking of items in a natural way: select the Condorcet winner, remove it from the lists, and repeat the previous two steps until there are no more documents to rank.",
                "Since there is not always Condorcet winners, variations of the Condorcet procedure have been developed within the multiple criteria decision aid theory, with methods such as ELECTRE [26]. 2.3.2 Kemeny Optimal Aggregation As in section 2.2.3, a consensus ranking minimizes a geometric distance from the input rankings, where the Kendall tau distance is used instead of the Spearman footrule distance.",
                "Formally, given two full lists j and j , the Kendall tau distance is given by K( j, j ) = |{(di, di ) : i < i , rj i < rj i , rj i > rj i }|, i.e. the number of pairwise disagreements between the two lists.",
                "It is easy to show that the consensus ranking corresponds to the geometric median of the input rankings and that the Kemeny optimal aggregation problem corresponds to the minimum feedback edge set problem. 2.3.3 Markov Chain Methods Markov chains (MCs) have been used by Dwork et al. [11] as a natural method to obtain a consensus ranking where states correspond to the documents to be ranked and the transition probabilities vary depending on the interpretation of the transition event.",
                "In the same reference, the authors proposed four specific MCs and experimental testing had shown that the following MC is the best performing one (see also [24]): • MC4: move from the current state di to the next state di by first choosing a document di uniformly from D. If for the majority of the rankings, we have rj i ≤ rj i , then move to di , else stay in di.",
                "The consensus ranking corresponds to the stationary distribution of MC4. 3.",
                "SPECIFICITIES OF THE RANK AGGREGATION PROBLEM IN THE IR CONTEXT 3.1 Limited Significance of the Rankings The exact positions of documents in one input ranking have limited significance and should not be overemphasized.",
                "For instance, having three relevant documents in the first three positions, any perturbation of these three items will have the same value.",
                "Indeed, in the IR context, the complete order provided by an input method may hide ties.",
                "In this case, we call such rankings semi orders.",
                "This was outlined in [13] as the problem of aggregation with ties.",
                "It is therefore important to build the consensus ranking based on robust information: • Documents with near positions in j are more likely to have similar interest or relevance.",
                "Thus a slight perturbation of the initial ranking is meaningless. • Assuming that document di is better ranked than document di in a ranking j, di is more likely to be definitively more relevant than di in j when the number of intermediate positions between di and di increases. 3.2 Partial Lists In real world applications, such as metasearch engines, rankings provided by the input methods are often partial lists.",
                "This was outlined in [14] as the problem of having to merge top-k results from various input lists.",
                "For instance, in the experiments carried out by Dwork et al. [11], authors found that among the top 100 best documents of 7 input search engines, 67% of the documents were present in only one search engine, whereas less than two documents were present in all the search engines.",
                "Rank aggregation of partial lists raises four major difficulties which we state hereafter, proposing for each of them various working assumptions: 1.",
                "Partial lists can have various lengths, which can favour long lists.",
                "We thus consider the following two working hypotheses: H1 k : We only consider the top k best documents from each input ranking.",
                "H1 all: We consider all the documents from each input ranking. 2.",
                "Since there are different documents in the input rankings, we must decide which documents should be kept in the consensus ranking.",
                "Two working hypotheses are therefore considered: H2 k : We only consider documents which are present in at least k input rankings (k > 1).",
                "H2 all: We consider all the documents which are ranked in at least one input ranking.",
                "Hereafter, we call documents which will be retained in the consensus ranking, candidate documents, and documents that will be excluded from the consensus ranking, excluded documents.",
                "We also call a candidate document which is missing in one or more rankings, a missing document. 3.",
                "Some candidate documents are missing documents in some input rankings.",
                "Main reasons for a missing document are that it was not indexed or it was indexed but deemed irrelevant ; usually this information is not available.",
                "We consider the following two working hypotheses: H3 yes: Each missing document in each j is assigned a position.",
                "H3 no: No assumption is made, that is each missing document is considered neither better nor worse than any other document. 4.",
                "When assumption H2 k holds, each input ranking may contain documents which will not be considered in the consensus ranking.",
                "Regarding the positions of the candidate documents, we can consider the following working hypotheses: H4 init: The initial positions of candidate documents are kept in each input ranking.",
                "H4 new: Candidate documents receive new positions in each input ranking, after discarding excluded ones.",
                "In the IR context, rank aggregation methods need to decide more or less explicitly which assumptions to retain w.r.t. the above-mentioned difficulties. 4.",
                "OUTRANKING APPROACH FOR RANK AGGREGATION 4.1 Presentation Positional methods consider implicitly that the positions of the documents in the input rankings are scores giving thus a cardinal meaning to an ordinal information.",
                "This constitutes a strong assumption that is questionable, especially when the input rankings have different lengths.",
                "Moreover, for positional methods, assumptions H3 and H4 , which are often arbitrary, have a strong impact on the results.",
                "For instance, let us consider an input ranking of 500 documents out of 1000 candidate documents.",
                "Whether we assign to each of the missing documents the position 1, 501, 750 or 1000 -corresponding to variations of H3 yes- will give rise to very contrasted results, especially regarding the top of the consensus ranking.",
                "Majoritarian methods do not suffer from the above-mentioned drawbacks of the positional methods since they build consensus rankings exploiting only ordinal information contained in the input rankings.",
                "Nevertheless, they suppose that such rankings are complete orders, ignoring that they may hide ties.",
                "Therefore, majoritarian methods base consensus rankings on illusory discriminant information rather than less discriminant but more robust information.",
                "Trying to overcome the limits of current rank aggregation methods, we found that outranking approaches, which were initially used for multiple criteria aggregation problems [26], can also be used for the rank aggregation purpose, where each ranking plays the role of a criterion.",
                "Therefore, in order to decide whether a document di should be ranked better than di in the consensus ranking σ, the two following conditions should be met: • a concordance condition which ensures that a majority of the input rankings are concordant with diσdi (majority principle). • a discordance condition which ensures that none of the discordant input rankings strongly refutes dσd (respect of minorities principle).",
                "Formally, the concordance coalition with diσdi is Csp (diσdi ) = { j∈ PR : rj i ≤ rj i − sp} where sp is a preference threshold which is the variation of document positions -whether it is absolute or relative to the ranking length- which draws the boundaries between an indifference and a preference situation between documents.",
                "The discordance coalition with diσdi is Dsv (diσdi ) = { j∈ PR : rj i ≥ rj i + sv} where sv is a veto threshold which is the variation of document positions -whether it is absolute or relative to the ranking length- which draws the boundaries between a weak and a strong opposition to diσdi .",
                "Depending on the exact definition of the preceding concordance and discordance coalitions leading to the definition of some decision rules, several outranking relations can be defined.",
                "They can be more or less demanding depending on i) the values of the thresholds sp and sv, ii) the importance or minimal size cmin required for the concordance coalition, and iii) the importance or maximum size dmax of the discordance coalition.",
                "A generic outranking relation can thus be defined as follows: diS(sp,sv,cmin,dmax)di ⇔ |Csp (diσdi )| ≥ cmin AND |Dsv (diσdi )| ≤ dmax This expression defines a family of nested outranking relations since S(sp,sv,cmin,dmax) ⊆ S(sp,sv,cmin,dmax) when cmin ≥ cmin and/or dmax ≤ dmax and/or sp ≥ sp and/or sv ≤ sv.",
                "This expression also generalizes the majority rule which corresponds to the particular relation S(0,∞, n 2 ,n).",
                "It also satisfies important properties of rank aggregation methods, called neutrality, Pareto-optimality, Condorcet property and Extended Condorcet property, in the social choice literature [29].",
                "Outranking relations are not necessarily transitive and do not necessarily correspond to rankings since directed cycles may exist.",
                "Therefore, we need specific procedures in order to derive a consensus ranking.",
                "We propose the following procedure which finds its roots in [27].",
                "It consists in partitioning the set of documents into r ranked classes.",
                "Each class Ch contains documents with the same relevance and results from the application of all relations (if possible) to the set of documents remaining after previous classes are computed.",
                "Documents within the same equivalence class are ranked arbitrarily.",
                "Formally, let • R be the set of candidate documents for a query, • S1 , S2 , . . . be a family of nested outranking relations, • Fk(di, E) = |{di ∈ E : diSk di }| be the number of documents in E(E ⊆ R) that could be considered worse than di according to relation Sk , • fk(di, E) = |{di ∈ E : di Sk di}| be the number of documents in E that could be considered better than di according to Sk , • sk(di, E) = Fk(di, E) − fk(di, E) be the qualification of di in E according to Sk .",
                "Each class Ch results from a distillation process.",
                "It corresponds to the last distillate of a series of sets E0 ⊇ E1 ⊇ . . . where E0 = R \\ (C1 ∪ . . . ∪ Ch−1) and Ek is a reduced subset of Ek−1 resulting from the application of the following procedure: 1. compute for each di ∈ Ek−1 its qualification according to Sk , i.e. sk(di, Ek−1), 2. define smax = maxdi∈Ek−1 {sk(di, Ek−1)}, then 3.",
                "Ek = {di ∈ Ek−1 : sk(di, Ek−1) = smax} When one outranking relation is used, the distillation process stops after the first application of the previous procedure, i.e., Ch corresponds to distillate E1.",
                "When different outranking relations are used, the distillation process stops when all the pre-defined outranking relations have been used or when |Ek| = 1. 4.2 Illustrative Example This section illustrates the concepts and procedures of section 4.1.",
                "Let us consider a set of candidate documents R = {d1, d2, d3, d4, d5}.",
                "The following table gives a profile PR of different rankings of the documents of R: PR = ( 1 , 2, 3, 4).",
                "Table 1: Rankings of documents rj i 1 2 3 4 d1 1 3 1 5 d2 2 1 3 3 d3 3 2 2 1 d4 4 4 5 2 d5 5 5 4 4 Let us suppose that the preference and veto thresholds are set to values 1 and 4 respectively, and that the concordance and discordance thresholds are set to values 2 and 1 respectively.",
                "The following tables give the concordance, discordance and outranking matrices.",
                "Each entry csp (di, di ) (dsv (di, di )) in the concordance (discordance) matrix gives the number of rankings that are concordant (discordant) with diσdi , i.e. csp (di, di ) = |Csp (diσdi )| and dsv (di, di ) = |Dsv (diσdi )|.",
                "Table 2: Computation of the outranking relation d1 d2 d3 d4 d5 d1 - 2 2 3 3 d2 2 - 2 3 4 d3 2 2 - 4 4 d4 1 1 0 - 3 d5 1 0 0 1Concordance Matrix d1 d2 d3 d4 d5 d1 - 0 1 0 0 d2 0 - 0 0 0 d3 0 0 - 0 0 d4 1 0 0 - 0 d5 1 1 0 0Discordance Matrix d1 d2 d3 d4 d5 d1 - 1 1 1 1 d2 1 - 1 1 1 d3 1 1 - 1 1 d4 0 0 0 - 1 d5 0 0 0 0Outranking Matrix (S1) For instance, the concordance coalition for the assertion d1σd4 is C1(d1σd4) = { 1, 2, 3} and the discordance coalition for the same assertion is D4(d1σd4) = ∅.",
                "Therefore, c1(d1, d4) = 3, d4(d1, d4) = 0 and d1S1 d4 holds.",
                "Notice that Fk(di, R) (fk(di, R)) is given by summing the values of the ith row (column) of the outranking matrix.",
                "The consensus ranking is obtained as follows: to get the first class C1, we compute the qualifications of all the documents of E0 = R with respect to S1 .",
                "They are respectively 2, 2, 2, -2 and -4.",
                "Therefore smax equals 2 and C1 = E1 = {d1, d2, d3}.",
                "Observe that, if we had used a second outranking relation S2(⊇ S1), these three documents could have been possibly discriminated.",
                "At this stage, we remove documents of C1 from the outranking matrix and compute the next class C2: we compute the new qualifications of the documents of E0 = R \\ C1 = {d4, d5}.",
                "They are respectively 1 and -1.",
                "So C3 = E1 = {d4}.",
                "The last document d5 is the only document of the last class C3.",
                "Thus, the consensus ranking is {d1, d2, d3} → {d4} → {d5}. 5.",
                "EXPERIMENTS AND RESULTS 5.1 Test Setting To facilitate empirical investigation of the proposed methodology, we developed a prototype metasearch engine that implements a version of our outranking approach for rank aggregation.",
                "In this paper, we apply our approach to the Topic Distillation (TD) task of TREC-2004 Web track [10].",
                "In this task, there are 75 topics where only a short description of each is given.",
                "For each query, we retained the rankings of the 10 best runs of the TD task which are provided by TREC-2004 participating teams.",
                "The performances of these runs are reported in table 3.",
                "Table 3: Performances of the 10 best runs of the TD task of TREC-2004 Run Id MAP P@10 S@1 S@5 S@10 uogWebCAU150 17.9% 24.9% 50.7% 77.3% 89.3% MSRAmixed1 17.8% 25.1% 38.7% 72.0% 88.0% MSRC04C12 16.5% 23.1% 38.7% 74.7% 80.0% humW04rdpl 16.3% 23.1% 37.3% 78.7% 90.7% THUIRmix042 14.7% 20.5% 21.3% 58.7% 74.7% UAmsT04MWScb 14.6% 20.9% 36.0% 66.7% 76.0% ICT04CIIS1AT 14.1% 20.8% 33.3% 64.0% 78.7% SJTUINCMIX5 12.9% 18.9% 29.3% 57.3% 72.0% MU04web1 11.5% 19.9% 33.3% 64.0% 76.0% MeijiHILw3 11.5% 15.3% 30.7% 54.7% 64.0% Average 14.7% 21.2% 34.9% 66.8% 78.94% For each query, each run provides a ranking of about 1000 documents.",
                "The number of documents retrieved by all these runs ranges from 543 to 5769.",
                "Their average (median) number is 3340 (3386).",
                "It is worth noting that we found similar distributions of the documents among the rankings as in [11].",
                "For evaluation, we used the trec eval standard tool which is used by the TREC community to calculate the standard measures of system effectiveness which are Mean Average Precision (MAP) and Success@n (S@n) for n=1, 5 and 10.",
                "Our approach effectiveness is compared against some high performing official results from TREC-2004 as well as against some standard rank aggregation algorithms.",
                "In the experiments, significance testing is mainly based on the t-student statistic which is computed on the basis of the MAP values of the compared runs.",
                "In the tables of the following section, statistically significant differences are marked with an asterisk.",
                "Values between brackets of the first column of each table, indicate the parameter value of the corresponding run. 5.2 Results We carried out several series of runs in order to i) study performance variations of the outranking approach when tuning the parameters and working assumptions, ii) compare performances of the outranking approach vs standard rank aggregation strategies , and iii) check whether rank aggregation performs better than the best input rankings.",
                "We set our basic run mcm with the following parameters.",
                "We considered that each input ranking is a complete order (sp = 0) and that an input ranking strongly refutes diσdi when the difference of both document positions is large enough (sv = 75%).",
                "Preference and veto thresholds are computed proportionally to the number of documents retained in each input ranking.",
                "They consequently may vary from one ranking to another.",
                "In addition, to accept the assertion diσdi , we supposed that the majority of the rankings must be concordant (cmin = 50%) and that every input ranking can impose its veto (dmax = 0).",
                "Concordance and discordance thresholds are computed for each tuple (di, di ) as the percentage of the input rankings of PRi ∩PRi .",
                "Thus, our choice of parameters leads to the definition of the outranking relation S(0,75%,50%,0).",
                "To test the run mcm, we had chosen the following assumptions.",
                "We retained the top 100 best documents from each input ranking (H1 100), only considered documents which are present in at least half of the input rankings (H2 5 ) and assumed H3 no and H4 new.",
                "In these conditions, the number of successful documents was about 100 on average, and the computation time per query was less than one second.",
                "Obviously, modifying the working assumptions should have deeper impact on the performances than tuning our model parameters.",
                "This was validated by preliminary experiments.",
                "Thus, we hereafter begin by studying performance variation when different sets of assumptions are considered.",
                "Afterwards, we study the impact of tuning parameters.",
                "Finally, we compare our model performances w.r.t. the input rankings as well as some standard data fusion algorithms. 5.2.1 Impact of the Working Assumptions Table 4 summarizes the performance variation of the outranking approach under different working hypotheses.",
                "In Table 4: Impact of the working assumptions Run Id MAP S@1 S@5 S@10 mcm 18.47% 41.33% 81.33% 86.67% mcm22 (H3 yes) 17.72% (-4.06%) 34.67% 81.33% 86.67% mcm23 (H4 init) 18.26% (-1.14%) 41.33% 81.33% 86.67% mcm24 (H1 all) 20.67% (+11.91%*) 38.66% 80.00% 86.66% mcm25 (H2 all) 21.68% (+17.38%*) 40.00% 78.66% 89.33% this table, we first show that run mcm22, in which missing documents are all put in the same last position of each input ranking, leads to performance drop w.r.t. run mcm.",
                "Moreover, S@1 moves from 41.33% to 34.67% (-16.11%).",
                "This shows that several relevant documents which were initially put at the first position of the consensus ranking in mcm, lose this first position but remain ranked in the top 5 documents since S@5 did not change.",
                "We also conclude that documents which have rather good positions in some input rankings are more likely to be relevant, even though they are missing in some other rankings.",
                "Consequently, when they are missing in some rankings, assigning worse ranks to these documents is harmful for performance.",
                "Also, from Table 4, we found that the performances of runs mcm and mcm23 are similar.",
                "Therefore, the outranking approach is not sensitive to keeping the initial positions of candidate documents or recomputing them by discarding excluded ones.",
                "From the same Table 4, performance of the outranking approach increases significantly for runs mcm24 and mcm25.",
                "Therefore, whether we consider all the documents which are present in half of the rankings (mcm24) or we consider all the documents which are ranked in the first 100 positions in one or more rankings (mcm25), increases performances.",
                "This result was predictable since in both cases we have more detailed information on the relative importance of documents.",
                "Tables 5 and 6 confirm this evidence.",
                "Table 5, where values between brackets of the first column give the number of documents which are retained from each input ranking, shows that selecting more documents from each input ranking leads to performance increase.",
                "It is worth mentioning that selecting more than 600 documents from each input ranking does not improve performance.",
                "Table 5: Impact of the number of retained documents Run Id MAP S@1 S@5 S@10 mcm (100) 18.47% 41.33% 81.33% 86.67% mcm24-1 (200) 19.32% (+4.60%) 42.67% 78.67% 88.00% mcm24-2 (400) 19.88% (+7.63%*) 37.33% 80.00% 88.00% mcm24-3 (600) 20.80% (+12.62%*) 40.00% 80.00% 88.00% mcm24-4 (800) 20.66% (+11.86%*) 40.00% 78.67% 86.67% mcm24 (1000) 20.67% (+11.91%*) 38.66% 80.00% 86.66% Table 6 reports runs corresponding to variations of H2 k .",
                "Values between brackets are rank hits.",
                "For instance, in the run mcm32, only documents which are present in 3 or more input rankings, were considered successful.",
                "This table shows that performance is significantly better when rare documents are considered, whereas it decreases significantly when these documents are discarded.",
                "Therefore, we conclude that many of the relevant documents are retrieved by a rather small set of IR models.",
                "Table 6: Performance considering different rank hits Run Id MAP S@1 S@5 S@10 mcm25 (1) 21.68% (+17.38%*) 40.00% 78.67% 89.33% mcm32 (3) 18.98% (+2.76%) 38.67% 80.00% 85.33% mcm (5) 18.47% 41.33% 81.33% 86.67% mcm33 (7) 15.83% (-14.29%*) 37.33% 78.67% 85.33% mcm34 (9) 10.96% (-40.66%*) 36.11% 66.67% 70.83% mcm35 (10) 7.42% (-59.83%*) 39.22% 62.75% 64.70% For both runs mcm24 and mcm25, the number of successful documents was about 1000 and therefore, the computation time per query increased and became around 5 seconds. 5.2.2 Impact of the Variation of the Parameters Table 7 shows performance variation of the outranking approach when different preference thresholds are considered.",
                "We found performance improvement up to threshold values of about 5%, then there is a decrease in the performance which becomes significant for threshold values greater than 10%.",
                "Moreover, S@1 improves from 41.33% to 46.67% when preference threshold changes from 0 to 5%.",
                "We can thus conclude that the input rankings are semi orders rather than complete orders.",
                "Table 8 shows the evolution of the performance measures w.r.t. the concordance threshold.",
                "We can conclude that in order to put document di before di in the consensus ranking, Table 7: Impact of the variation of the preference threshold from 0 to 12.5% Run Id MAP S@1 S@5 S@10 mcm (0%) 18.47% 41.33% 81.33% 86.67% mcm1 (1%) 18.57% (+0.54%) 41.33% 81.33% 86.67% mcm2 (2.5%) 18.63% (+0.87%) 42.67% 78.67% 86.67% mcm3 (5%) 18.69% (+1.19%) 46.67% 81.33% 86.67% mcm4 (7.5%) 18.24% (-1.25%) 46.67% 81.33% 86.67% mcm5 (10%) 17.93% (-2.92%) 40.00% 82.67% 86.67% mcm5b (12.5%) 17.51% (-5.20%*) 41.33% 80.00% 86.67% at least half of the input rankings of PRi ∩ PRi should be concordant.",
                "Performance drops significantly for very low and very high values of the concordance threshold.",
                "In fact, for such values, the concordance condition is either fulfilled rather always by too many document pairs or not fulfilled at all, respectively.",
                "Therefore, the outranking relation becomes either too weak or too strong respectively.",
                "Table 8: Impact of the variation of cmin Run Id MAP S@1 S@5 S@10 mcm11 (20%) 17.63% (-4.55%*) 41.33% 76.00% 85.33% mcm12 (40%) 18.37% (-0.54%) 42.67% 76.00% 86.67% mcm (50%) 18.47% 41.33% 81.33% 86.67% mcm13 (60%) 18.42% (-0.27%) 40.00% 78.67% 86.67% mcm14 (80%) 17.43% (-5.63%*) 40.00% 78.67% 86.67% mcm15 (100%) 16.12% (-12.72%*) 41.33% 70.67% 85.33% In the experiments, varying the veto threshold as well as the discordance threshold within reasonable intervals does not have significant impact on performance measures.",
                "In fact, runs with different veto thresholds (sv ∈ [50%; 100%]) had similar performances even though there is a slight advantage for runs with high threshold values which means that it is better not to allow the input rankings to put their veto easily.",
                "Also, tuning the discordance threshold was carried out for values 50% and 75% of the veto threshold.",
                "For these runs we did not get any noticeable performance variation, although for low discordance thresholds (dmax < 20%), performance slightly decreased. 5.2.3 Impact of the Variation of the Number of Input Rankings To study performance evolution when different sets of input rankings are considered, we carried three more runs where 2, 4, and 6 of the best performing sets of the input rankings are considered.",
                "Results reported in Table 9 are seemingly counter-intuitive and also do not support previous findings regarding rank aggregation research [3].",
                "Nevertheless, this result shows that low performing rankings bring more noise than information to the establishment of the consensus ranking.",
                "Therefore, when they are considered, performance decreases.",
                "Table 9: Performance considering different best performing sets of input rankings Run Id MAP S@1 S@5 S@10 mcm (10) 18.47% 41.33% 81.33% 86.67% mcm27 (6) 18.60% (+0.70%) 41.33% 80.00% 85.33% mcm28 (4) 19.02% (+2.98%) 40.00% 86.67% 88.00% mcm29 (2) 18.33% (-0.76%) 44.00% 76.00% 88.00% 5.2.4 Comparison of the Performance of Different Rank Aggregation Methods In this set of runs, we compare the outranking approach with some standard rank aggregation methods which were proven to have acceptable performance in previous studies: we considered two positional methods which are the CombSUM and the CombMNZ strategies.",
                "We also examined the performance of one majoritarian method which is the Markov chain method (MC4).",
                "For the comparisons, we considered a specific outranking relation S∗ = S(5%,50%,50%,30%) which results in good overall performances when tuning all the parameters.",
                "The first row of Table 10 gives performances of the rank aggregation methods w.r.t. a basic assumption set A1 = (H1 100, H2 5 , H4 new): we only consider the 100 first documents from each ranking, then retain documents present in 5 or more rankings and update ranks of successful documents.",
                "For positional methods, we place missing documents at the queue of the ranking (H3 yes) whereas for our method as well as for MC4, we retained hypothesis H3 no.",
                "The three following rows of Table 10 report performances when changing one element from the basic assumption set: the second row corresponds to the assumption set A2 = (H1 1000, H2 5 , H4 new), i.e. changing the number of retained documents from 100 to 1000.",
                "The third row corresponds to the assumption set A3 = (H1 100, H2 all, H4 new), i.e. considering the documents present in at least one ranking.",
                "The fourth row corresponds to the assumption set A4 = (H1 100, H2 5 , H4 init), i.e. keeping the original ranks of successful documents.",
                "The fifth row of Table 10, labeled A5, gives performance when all the 225 queries of the Web track of TREC-2004 are considered.",
                "Obviously, performance level cannot be compared with previous lines since the additional queries are different from the TD queries and correspond to other tasks (Home Page and Named Page tasks [10]) of TREC-2004 Web track.",
                "This set of runs aims to show whether relative performance of the various methods is task-dependent.",
                "The last row of Table 10, labeled A6, reports performance of the various methods considering the TD task of TREC2002 instead of TREC-2004: we fused the results of input rankings of the 10 best official runs for each of the 50 TD queries [9] considering the set of assumptions A1 of the first row.",
                "This aims to show whether relative performance of the various methods changes from year to year.",
                "Values between brackets of Table 10 are variations of performance of each rank aggregation method w.r.t. performance of the outranking approach.",
                "Table 10: Performance (MAP) of different rank aggregation methods under 3 different test collections mcm combSUM combMNZ markov A1 18.79% 17.54% (-6.65%*) 17.08% (-9.10%*) 18.63% (-0.85%) A2 21.36% 19.18% (-10.21%*) 18.61% (-12.87%*) 21.33% (-0.14%) A3 21.92% 21.38% (-2.46%) 20.88% (-4.74%) 19.35% (-11.72%*) A4 18.64% 17.58% (-5.69%*) 17.18% (-7.83%*) 18.63% (-0.05%) A5 55.39% 52.16% (-5.83%*) 49.70% (-10.27%*) 53.30% (-3.77%) A6 16.95% 15.65% (-7.67%*) 14.57% (-14.04%*) 16.39% (-3.30%) From the analysis of table 10 the following can be established: • for all the runs, considering all the documents in each input ranking (A2) significantly improves performance (MAP increases by 11.62% on average).",
                "This is predictable since some initially unreported relevant documents would receive better positions in the consensus ranking. • for all the runs, considering documents even those present in only one input ranking (A3) significantly improves performance.",
                "For mcm, combSUM and combMNZ, performance improvement is more important (MAP increases by 20.27% on average) than for the markov run (MAP increases by 3.86%). • preserving the initial positions of documents (A4) or recomputing them (A1) does not have a noticeable influence on performance for both positional and majoritarian methods. • considering all the queries of the Web track of TREC2004 (A5) as well as the TD queries of the Web track of TREC-2002 (A6) does not alter the relative performance of the different data fusion methods. • considering the TD queries of the Web track of TREC2002, performances of all the data fusion methods are lower than that of the best performing input ranking for which the MAP value equals 18.58%.",
                "This is because most of the fused input rankings have very low performances compared to the best one, which brings more noise to the consensus ranking. • performances of the data fusion methods mcm and markov are significantly better than that of the best input ranking uogWebCAU150.",
                "This remains true for runs combSUM and combMNZ only under assumptions H1 all or H2 all.",
                "This shows that majoritarian methods are less sensitive to assumptions than positional methods. • outranking approach always performs significantly better than positional methods combSUM and combMNZ.",
                "It has also better performances than the Markov chain method, especially under assumption H2 all where difference of performances becomes significant. 6.",
                "CONCLUSIONS In this paper, we address the rank aggregation problem where different, but not disjoint, lists of documents are to be fused.",
                "We noticed that the input rankings can hide ties, so they should not be considered as complete orders.",
                "Only robust information should be used from each input ranking.",
                "Current rank aggregation methods, and especially positional methods (e.g. combSUM [15]), are not initially designed to work with such rankings.",
                "They should be adapted by considering specific working assumptions.",
                "We propose a new outranking method for rank aggregation which is well adapted to the IR context.",
                "Indeed, it ranks two documents w.r.t. the intensity of their positions difference in each input ranking and also considering the number of the input rankings that are concordant and discordant in favor of a specific document.",
                "There is also no need to make specific assumptions on the positions of the missing documents.",
                "This is an important feature since the absence of a document from a ranking should not be necessarily interpreted negatively.",
                "Experimental results show that the outranking method significantly out-performs popular classical positional data fusion methods like combSUM and combMNZ strategies.",
                "It also out-performs a good performing majoritarian methods which is the Markov chain method.",
                "These results are tested against different test collections and queries.",
                "From the experiments, we can also conclude that in order to improve the performances, we should fuse result lists of well performing IR models, and that majoritarian data fusion methods perform better than positional methods.",
                "The proposed method can have a real impact on Web metasearch performances since only ranks are available from most primary search engines, whereas most of the current approaches need scores to merge result lists into one single list.",
                "Further work involves investigating whether the outranking approach performs well in various other contexts, e.g. using the document scores or some combination of document ranks and scores.",
                "Acknowledgments The authors would like to thank Jacques Savoy for his valuable comments on a preliminary version of this paper. 7.",
                "REFERENCES [1] A. Aronson, D. Demner-Fushman, S. Humphrey, J. Lin, H. Liu, P. Ruch, M. Ruiz, L. Smith, L. Tanabe, and W. Wilbur.",
                "Fusion of knowledge-intensive and statistical approaches for retrieving and annotating textual genomics documents.",
                "In Proceedings TREC2005.",
                "NIST Publication, 2005. [2] R. A. Baeza-Yates and B.",
                "A. Ribeiro-Neto.",
                "Modern Information Retrieval.",
                "ACM Press , 1999. [3] B. T. Bartell, G. W. Cottrell, and R. K. Belew.",
                "Automatic combination of multiple ranked retrieval systems.",
                "In Proceedings ACM-SIGIR94, pages 173-181.",
                "Springer-Verlag, 1994. [4] N. J. Belkin, P. Kantor, E. A.",
                "Fox, and J.",
                "A. Shaw.",
                "Combining evidence of multiple query representations for information retrieval.",
                "IPM, 31(3):431-448, 1995. [5] J. Borda.",
                "M´emoire sur les ´elections au scrutin.",
                "Histoire de lAcad´emie des Sciences, 1781. [6] J. P. Callan, Z. Lu, and W. B. Croft.",
                "Searching distributed collections with inference networks.",
                "In Proceedings ACM-SIGIR95, pages 21-28, 1995. [7] M. Condorcet.",
                "Essai sur lapplication de lanalyse `a la probabilit´e des d´ecisions rendues `a la pluralit´e des voix.",
                "Imprimerie Royale, Paris, 1785. [8] W. D. Cook and M. Kress.",
                "Ordinal ranking with intensity of preference.",
                "Management Science, 31(1):26-32, 1985. [9] N. Craswell and D. Hawking.",
                "Overview of the TREC-2002 Web Track.",
                "In Proceedings TREC2002.",
                "NIST Publication, 2002. [10] N. Craswell and D. Hawking.",
                "Overview of the TREC-2004 Web Track.",
                "In Proceedings of TREC2004.",
                "NIST Publication, 2004. [11] C. Dwork, S. R. Kumar, M. Naor, and D. Sivakumar.",
                "Rank aggregation methods for the Web.",
                "In Proceedings WWW2001, pages 613-622, 2001. [12] R. Fagin.",
                "Combining fuzzy information from multiple systems.",
                "JCSS, 58(1):83-99, 1999. [13] R. Fagin, R. Kumar, M. Mahdian, D. Sivakumar, and E. Vee.",
                "Comparing and aggregating rankings with ties.",
                "In PODS, pages 47-58, 2004. [14] R. Fagin, R. Kumar, and D. Sivakumar.",
                "Comparing top k lists.",
                "SIAM J. on Discrete Mathematics, 17(1):134-160, 2003. [15] E. A.",
                "Fox and J.",
                "A. Shaw.",
                "Combination of multiple searches.",
                "In Proceedings of TREC3.",
                "NIST Publication, 1994. [16] J. Katzer, M. McGill, J. Tessier, W. Frakes, and P. DasGupta.",
                "A study of the overlap among document representations.",
                "Information Technology: Research and Development, 1(4):261-274, 1982. [17] L. S. Larkey, M. E. Connell, and J. Callan.",
                "Collection selection and results merging with topically organized U.S. patents and TREC data.",
                "In Proceedings ACM-CIKM2000, pages 282-289.",
                "ACM Press, 2000. [18] A.",
                "Le Calv´e and J. Savoy.",
                "Database merging strategy based on logistic regression.",
                "IPM, 36(3):341-359, 2000. [19] J. H. Lee.",
                "Analyses of multiple evidence combination.",
                "In Proceedings ACM-SIGIR97, pages 267-276, 1997. [20] D. Lillis, F. Toolan, R. Collier, and J. Dunnion.",
                "Probfuse: a probabilistic approach to data fusion.",
                "In Proceedings ACM-SIGIR2006, pages 139-146.",
                "ACM Press, 2006. [21] J. I. Marden.",
                "Analyzing and Modeling Rank Data.",
                "Number 64 in Monographs on Statistics and Applied Probability.",
                "Chapman & Hall, 1995. [22] M. Montague and J.",
                "A. Aslam.",
                "Metasearch consistency.",
                "In Proceedings ACM-SIGIR2001, pages 386-387.",
                "ACM Press, 2001. [23] D. M. Pennock and E. Horvitz.",
                "Analysis of the axiomatic foundations of collaborative filtering.",
                "In Workshop on AI for Electronic Commerce at the 16th National Conference on Artificial Intelligence, 1999. [24] M. E. Renda and U. Straccia.",
                "Web metasearch: rank vs. score based rank aggregation methods.",
                "In Proceedings ACM-SAC2003, pages 841-846.",
                "ACM Press, 2003. [25] W. H. Riker.",
                "Liberalism against populism.",
                "Waveland Press, 1982. [26] B. Roy.",
                "The outranking approach and the foundations of ELECTRE methods.",
                "Theory and Decision, 31:49-73, 1991. [27] B. Roy and J. Hugonnard.",
                "Ranking of suburban line extension projects on the Paris metro system by a multicriteria method.",
                "Transportation Research, 16A(4):301-312, 1982. [28] L. Si and J. Callan.",
                "Using sampled data and regression to merge search engine results.",
                "In Proceedings ACM-SIGIR2002, pages 19-26.",
                "ACM Press, 2002. [29] M. Truchon.",
                "An extension of the Condorcet criterion and Kemeny orders.",
                "Cahier 9813, Centre de Recherche en Economie et Finance Appliqu´ees, Oct. 1998. [30] H. Turtle and W. B. Croft.",
                "Inference networks for document retrieval.",
                "In Proceedings of ACM-SIGIR90, pages 1-24.",
                "ACM Press, 1990. [31] C. C. Vogt and G. W. Cottrell.",
                "Fusion via a linear combination of scores.",
                "Information Retrieval, 1(3):151-173, 1999."
            ],
            "original_annotated_samples": [],
            "translated_annotated_samples": [],
            "translated_text": "",
            "candidates": [],
            "error": []
        },
        "outrank method": {
            "translated_key": "Método de superación",
            "is_in_text": false,
            "original_annotated_sentences": [
                "An Outranking Approach for Rank Aggregation in Information Retrieval Mohamed Farah Lamsade, Paris Dauphine University Place du Mal de Lattre de Tassigny 75775 Paris Cedex 16, France farah@lamsade.dauphine.fr Daniel Vanderpooten Lamsade, Paris Dauphine University Place du Mal de Lattre de Tassigny 75775 Paris Cedex 16, France vdp@lamsade.dauphine.fr ABSTRACT Research in Information Retrieval usually shows performance improvement when many sources of evidence are combined to produce a ranking of documents (e.g., texts, pictures, sounds, etc.).",
                "In this paper, we focus on the rank aggregation problem, also called data fusion problem, where rankings of documents, searched into the same collection and provided by multiple methods, are combined in order to produce a new ranking.",
                "In this context, we propose a rank aggregation method within a multiple criteria framework using aggregation mechanisms based on decision rules identifying positive and negative reasons for judging whether a document should get a better rank than another.",
                "We show that the proposed method deals well with the Information Retrieval distinctive features.",
                "Experimental results are reported showing that the suggested method performs better than the well-known CombSUM and CombMNZ operators.",
                "Categories and Subject Descriptors: H.3.3 [Information Systems]: Information Search and Retrieval - Retrieval models.",
                "General Terms: Algorithms, Measurement, Experimentation, Performance, Theory. 1.",
                "INTRODUCTION A wide range of current Information Retrieval (IR) approaches are based on various search models (Boolean, Vector Space, Probabilistic, Language, etc. [2]) in order to retrieve relevant documents in response to a user request.",
                "The result lists produced by these approaches depend on the exact definition of the relevance concept.",
                "Rank aggregation approaches, also called data fusion approaches, consist in combining these result lists in order to produce a new and hopefully better ranking.",
                "Such approaches give rise to metasearch engines in the Web context.",
                "We consider, in the following, cases where only ranks are available and no other additional information is provided such as the relevance scores.",
                "This corresponds indeed to the reality, where only ordinal information is available.",
                "Data fusion is also relevant in other contexts, such as when the user writes several queries of his/her information need (e.g., a boolean query and a natural language query) [4], or when many document surrogates are available [16].",
                "Several studies argued that rank aggregation has the potential of combining effectively all the various sources of evidence considered in various input methods.",
                "For instance, experiments carried out in [16], [30], [4] and [19] showed that documents which appear in the lists of the majority of the input methods are more likely to be relevant.",
                "Moreover, Lee [19] and Vogt and Cottrell [31] found that various retrieval approaches often return very different irrelevant documents, but many of the same relevant documents.",
                "Bartell et al. [3] also found that rank aggregation methods improve the performances w.r.t. those of the input methods, even when some of them have weak individual performances.",
                "These methods also tend to smooth out biases of the input methods according to Montague and Aslam [22].",
                "Data fusion has recently been proved to improve performances for both the ad-hoc retrieval and categorization tasks within the TREC genomics track in 2005 [1].",
                "The rank aggregation problem was addressed in various fields such as i) in social choice theory which studies voting algorithms which specify winners of elections or winners of competitions in tournaments [29], ii) in statistics when studying correlation between rankings, iii) in distributed databases when results from different databases must be combined [12], and iv) in collaborative filtering [23].",
                "Most current rank aggregation methods consider each input ranking as a permutation over the same set of items.",
                "They also give rigid interpretation to the exact ranking of the items.",
                "Both of these assumptions are rather not valid in the IR context, as will be shown in the following sections.",
                "The remaining of the paper is organized as follows.",
                "We first review current rank aggregation methods in Section 2.",
                "Then we outline the specificities of the data fusion problem in the IR context (Section 3).",
                "In Section 4, we present a new aggregation method which is proven to best fit the IR context.",
                "Experimental results are presented in Section 5 and conclusions are provided in a final section. 2.",
                "RELATED WORK As pointed out by Riker [25], we can distinguish two families of rank aggregation methods: positional methods which assign scores to items to be ranked according to the ranks they receive and majoritarian methods which are based on pairwise comparisons of items to be ranked.",
                "These two families of methods find their roots in the pioneering works of Borda [5] and Condorcet [7], respectively, in the social choice literature. 2.1 Preliminaries We first introduce some basic notations to present the rank aggregation methods in a uniform way.",
                "Let D = {d1, d2, . . . , dnd } be a set of nd documents.",
                "A list or a ranking j is an ordering defined on Dj ⊆ D (j = 1, . . . , n).",
                "Thus, di j di means di is ranked better than di in j.",
                "When Dj = D, j is said to be a full list.",
                "Otherwise, it is a partial list.",
                "If di belongs to Dj, rj i denotes the rank or position of di in j.",
                "We assume that the best answer (document) is assigned the position 1 and the worst one is assigned the position |Dj|.",
                "Let D be the set of all permutations on D or all subsets of D. A profile is a n-tuple of rankings PR = ( 1, 2, . . . , n).",
                "Restricting PR to the rankings containing document di defines PRi.",
                "We also call the number of rankings which contain document di the rank hits of di [19].",
                "The rank aggregation or data fusion problem consists of finding a ranking function or mechanism Ψ (also called a social welfare function in the social choice theory terminology) defined by: Ψ : n D → D PR = ( 1, 2, . . . , n) → σ = Ψ(PR) where σ is called a consensus ranking. 2.2 Positional Methods 2.2.1 Borda Count This method [5] first assigns a score n j=1 rj i to each document di.",
                "Documents are then ranked by increasing order of this score, breaking ties, if any, arbitrarily. 2.2.2 Linear Combination Methods This family of methods basically combine scores of documents.",
                "When used for the rank aggregation problem, ranks are assumed to be scores or performances to be combined using aggregation operators such as the weighted sum or some variation of it [3, 31, 17, 28].",
                "For instance, Callan et al. [6] used the inference networks model [30] to combine rankings.",
                "Fox and Shaw [15] proposed several combination strategies which are CombSUM, CombMIN, CombMAX, CombANZ and CombMNZ.",
                "The first three operators correspond to the sum, min and max operators, respectively.",
                "CombANZ and CombMNZ respectively divides and multiplies the CombSUM score by the rank hits.",
                "It is shown in [19] that the CombSUM and CombMNZ operators perform better than the others.",
                "Metasearch engines such as SavvySearch and MetaCrawler use the CombSUM strategy to fuse rankings. 2.2.3 Footrule Optimal Aggregation In this method, a consensus ranking minimizes the Spearman footrule distance from the input rankings [21].",
                "Formally, given two full lists j and j , this distance is given by F( j, j ) = nd i=1 |rj i − rj i |.",
                "It extends to several lists as follows.",
                "Given a profile PR and a consensus ranking σ, the Spearman footrule distance of σ to PR is given by F(σ, PR) = n j=1 F(σ, j).",
                "Cook and Kress [8] proposed a similar method which consists in optimizing the distance D( j, j ) = 1 2 nd i,i =1 |rj i,i − rj i,i |, where rj i,i = rj i −rj i .",
                "This formulation has the advantage that it considers the intensity of preferences. 2.2.4 Probabilistic Methods This kind of methods assume that the performance of the input methods on a number of training queries is indicative of their future performance.",
                "During the training process, probabilities of relevance are calculated.",
                "For subsequent queries, documents are ranked based on these probabilities.",
                "For instance, in [20], each input ranking j is divided into a number of segments, and the conditional probability of relevance (R) of each document di depending on the segment k it occurs in, is computed, i.e. prob(R|di, k, j).",
                "For subsequent queries, the score of each document di is given by n j=1 prob(R|di,k, j ) k .",
                "Le Calve and Savoy [18] suggest using a logistic regression approach for combining scores.",
                "Training data is needed to infer the model parameters. 2.3 Majoritarian Methods 2.3.1 Condorcet Procedure The original Condorcet rule [7] specifies that a winner of the election is any item that beats or ties with every other item in a pairwise contest.",
                "Formally, let C(diσdi ) = { j∈ PR : di j di } be the coalition of rankings that are concordant with establishing diσdi , i.e. with the proposition di should be ranked better than di in the final ranking σ. di beats or ties with di iff |C(diσdi )| ≥ |C(di σdi)|.",
                "The repetitive application of the Condorcet algorithm can produce a ranking of items in a natural way: select the Condorcet winner, remove it from the lists, and repeat the previous two steps until there are no more documents to rank.",
                "Since there is not always Condorcet winners, variations of the Condorcet procedure have been developed within the multiple criteria decision aid theory, with methods such as ELECTRE [26]. 2.3.2 Kemeny Optimal Aggregation As in section 2.2.3, a consensus ranking minimizes a geometric distance from the input rankings, where the Kendall tau distance is used instead of the Spearman footrule distance.",
                "Formally, given two full lists j and j , the Kendall tau distance is given by K( j, j ) = |{(di, di ) : i < i , rj i < rj i , rj i > rj i }|, i.e. the number of pairwise disagreements between the two lists.",
                "It is easy to show that the consensus ranking corresponds to the geometric median of the input rankings and that the Kemeny optimal aggregation problem corresponds to the minimum feedback edge set problem. 2.3.3 Markov Chain Methods Markov chains (MCs) have been used by Dwork et al. [11] as a natural method to obtain a consensus ranking where states correspond to the documents to be ranked and the transition probabilities vary depending on the interpretation of the transition event.",
                "In the same reference, the authors proposed four specific MCs and experimental testing had shown that the following MC is the best performing one (see also [24]): • MC4: move from the current state di to the next state di by first choosing a document di uniformly from D. If for the majority of the rankings, we have rj i ≤ rj i , then move to di , else stay in di.",
                "The consensus ranking corresponds to the stationary distribution of MC4. 3.",
                "SPECIFICITIES OF THE RANK AGGREGATION PROBLEM IN THE IR CONTEXT 3.1 Limited Significance of the Rankings The exact positions of documents in one input ranking have limited significance and should not be overemphasized.",
                "For instance, having three relevant documents in the first three positions, any perturbation of these three items will have the same value.",
                "Indeed, in the IR context, the complete order provided by an input method may hide ties.",
                "In this case, we call such rankings semi orders.",
                "This was outlined in [13] as the problem of aggregation with ties.",
                "It is therefore important to build the consensus ranking based on robust information: • Documents with near positions in j are more likely to have similar interest or relevance.",
                "Thus a slight perturbation of the initial ranking is meaningless. • Assuming that document di is better ranked than document di in a ranking j, di is more likely to be definitively more relevant than di in j when the number of intermediate positions between di and di increases. 3.2 Partial Lists In real world applications, such as metasearch engines, rankings provided by the input methods are often partial lists.",
                "This was outlined in [14] as the problem of having to merge top-k results from various input lists.",
                "For instance, in the experiments carried out by Dwork et al. [11], authors found that among the top 100 best documents of 7 input search engines, 67% of the documents were present in only one search engine, whereas less than two documents were present in all the search engines.",
                "Rank aggregation of partial lists raises four major difficulties which we state hereafter, proposing for each of them various working assumptions: 1.",
                "Partial lists can have various lengths, which can favour long lists.",
                "We thus consider the following two working hypotheses: H1 k : We only consider the top k best documents from each input ranking.",
                "H1 all: We consider all the documents from each input ranking. 2.",
                "Since there are different documents in the input rankings, we must decide which documents should be kept in the consensus ranking.",
                "Two working hypotheses are therefore considered: H2 k : We only consider documents which are present in at least k input rankings (k > 1).",
                "H2 all: We consider all the documents which are ranked in at least one input ranking.",
                "Hereafter, we call documents which will be retained in the consensus ranking, candidate documents, and documents that will be excluded from the consensus ranking, excluded documents.",
                "We also call a candidate document which is missing in one or more rankings, a missing document. 3.",
                "Some candidate documents are missing documents in some input rankings.",
                "Main reasons for a missing document are that it was not indexed or it was indexed but deemed irrelevant ; usually this information is not available.",
                "We consider the following two working hypotheses: H3 yes: Each missing document in each j is assigned a position.",
                "H3 no: No assumption is made, that is each missing document is considered neither better nor worse than any other document. 4.",
                "When assumption H2 k holds, each input ranking may contain documents which will not be considered in the consensus ranking.",
                "Regarding the positions of the candidate documents, we can consider the following working hypotheses: H4 init: The initial positions of candidate documents are kept in each input ranking.",
                "H4 new: Candidate documents receive new positions in each input ranking, after discarding excluded ones.",
                "In the IR context, rank aggregation methods need to decide more or less explicitly which assumptions to retain w.r.t. the above-mentioned difficulties. 4.",
                "OUTRANKING APPROACH FOR RANK AGGREGATION 4.1 Presentation Positional methods consider implicitly that the positions of the documents in the input rankings are scores giving thus a cardinal meaning to an ordinal information.",
                "This constitutes a strong assumption that is questionable, especially when the input rankings have different lengths.",
                "Moreover, for positional methods, assumptions H3 and H4 , which are often arbitrary, have a strong impact on the results.",
                "For instance, let us consider an input ranking of 500 documents out of 1000 candidate documents.",
                "Whether we assign to each of the missing documents the position 1, 501, 750 or 1000 -corresponding to variations of H3 yes- will give rise to very contrasted results, especially regarding the top of the consensus ranking.",
                "Majoritarian methods do not suffer from the above-mentioned drawbacks of the positional methods since they build consensus rankings exploiting only ordinal information contained in the input rankings.",
                "Nevertheless, they suppose that such rankings are complete orders, ignoring that they may hide ties.",
                "Therefore, majoritarian methods base consensus rankings on illusory discriminant information rather than less discriminant but more robust information.",
                "Trying to overcome the limits of current rank aggregation methods, we found that outranking approaches, which were initially used for multiple criteria aggregation problems [26], can also be used for the rank aggregation purpose, where each ranking plays the role of a criterion.",
                "Therefore, in order to decide whether a document di should be ranked better than di in the consensus ranking σ, the two following conditions should be met: • a concordance condition which ensures that a majority of the input rankings are concordant with diσdi (majority principle). • a discordance condition which ensures that none of the discordant input rankings strongly refutes dσd (respect of minorities principle).",
                "Formally, the concordance coalition with diσdi is Csp (diσdi ) = { j∈ PR : rj i ≤ rj i − sp} where sp is a preference threshold which is the variation of document positions -whether it is absolute or relative to the ranking length- which draws the boundaries between an indifference and a preference situation between documents.",
                "The discordance coalition with diσdi is Dsv (diσdi ) = { j∈ PR : rj i ≥ rj i + sv} where sv is a veto threshold which is the variation of document positions -whether it is absolute or relative to the ranking length- which draws the boundaries between a weak and a strong opposition to diσdi .",
                "Depending on the exact definition of the preceding concordance and discordance coalitions leading to the definition of some decision rules, several outranking relations can be defined.",
                "They can be more or less demanding depending on i) the values of the thresholds sp and sv, ii) the importance or minimal size cmin required for the concordance coalition, and iii) the importance or maximum size dmax of the discordance coalition.",
                "A generic outranking relation can thus be defined as follows: diS(sp,sv,cmin,dmax)di ⇔ |Csp (diσdi )| ≥ cmin AND |Dsv (diσdi )| ≤ dmax This expression defines a family of nested outranking relations since S(sp,sv,cmin,dmax) ⊆ S(sp,sv,cmin,dmax) when cmin ≥ cmin and/or dmax ≤ dmax and/or sp ≥ sp and/or sv ≤ sv.",
                "This expression also generalizes the majority rule which corresponds to the particular relation S(0,∞, n 2 ,n).",
                "It also satisfies important properties of rank aggregation methods, called neutrality, Pareto-optimality, Condorcet property and Extended Condorcet property, in the social choice literature [29].",
                "Outranking relations are not necessarily transitive and do not necessarily correspond to rankings since directed cycles may exist.",
                "Therefore, we need specific procedures in order to derive a consensus ranking.",
                "We propose the following procedure which finds its roots in [27].",
                "It consists in partitioning the set of documents into r ranked classes.",
                "Each class Ch contains documents with the same relevance and results from the application of all relations (if possible) to the set of documents remaining after previous classes are computed.",
                "Documents within the same equivalence class are ranked arbitrarily.",
                "Formally, let • R be the set of candidate documents for a query, • S1 , S2 , . . . be a family of nested outranking relations, • Fk(di, E) = |{di ∈ E : diSk di }| be the number of documents in E(E ⊆ R) that could be considered worse than di according to relation Sk , • fk(di, E) = |{di ∈ E : di Sk di}| be the number of documents in E that could be considered better than di according to Sk , • sk(di, E) = Fk(di, E) − fk(di, E) be the qualification of di in E according to Sk .",
                "Each class Ch results from a distillation process.",
                "It corresponds to the last distillate of a series of sets E0 ⊇ E1 ⊇ . . . where E0 = R \\ (C1 ∪ . . . ∪ Ch−1) and Ek is a reduced subset of Ek−1 resulting from the application of the following procedure: 1. compute for each di ∈ Ek−1 its qualification according to Sk , i.e. sk(di, Ek−1), 2. define smax = maxdi∈Ek−1 {sk(di, Ek−1)}, then 3.",
                "Ek = {di ∈ Ek−1 : sk(di, Ek−1) = smax} When one outranking relation is used, the distillation process stops after the first application of the previous procedure, i.e., Ch corresponds to distillate E1.",
                "When different outranking relations are used, the distillation process stops when all the pre-defined outranking relations have been used or when |Ek| = 1. 4.2 Illustrative Example This section illustrates the concepts and procedures of section 4.1.",
                "Let us consider a set of candidate documents R = {d1, d2, d3, d4, d5}.",
                "The following table gives a profile PR of different rankings of the documents of R: PR = ( 1 , 2, 3, 4).",
                "Table 1: Rankings of documents rj i 1 2 3 4 d1 1 3 1 5 d2 2 1 3 3 d3 3 2 2 1 d4 4 4 5 2 d5 5 5 4 4 Let us suppose that the preference and veto thresholds are set to values 1 and 4 respectively, and that the concordance and discordance thresholds are set to values 2 and 1 respectively.",
                "The following tables give the concordance, discordance and outranking matrices.",
                "Each entry csp (di, di ) (dsv (di, di )) in the concordance (discordance) matrix gives the number of rankings that are concordant (discordant) with diσdi , i.e. csp (di, di ) = |Csp (diσdi )| and dsv (di, di ) = |Dsv (diσdi )|.",
                "Table 2: Computation of the outranking relation d1 d2 d3 d4 d5 d1 - 2 2 3 3 d2 2 - 2 3 4 d3 2 2 - 4 4 d4 1 1 0 - 3 d5 1 0 0 1Concordance Matrix d1 d2 d3 d4 d5 d1 - 0 1 0 0 d2 0 - 0 0 0 d3 0 0 - 0 0 d4 1 0 0 - 0 d5 1 1 0 0Discordance Matrix d1 d2 d3 d4 d5 d1 - 1 1 1 1 d2 1 - 1 1 1 d3 1 1 - 1 1 d4 0 0 0 - 1 d5 0 0 0 0Outranking Matrix (S1) For instance, the concordance coalition for the assertion d1σd4 is C1(d1σd4) = { 1, 2, 3} and the discordance coalition for the same assertion is D4(d1σd4) = ∅.",
                "Therefore, c1(d1, d4) = 3, d4(d1, d4) = 0 and d1S1 d4 holds.",
                "Notice that Fk(di, R) (fk(di, R)) is given by summing the values of the ith row (column) of the outranking matrix.",
                "The consensus ranking is obtained as follows: to get the first class C1, we compute the qualifications of all the documents of E0 = R with respect to S1 .",
                "They are respectively 2, 2, 2, -2 and -4.",
                "Therefore smax equals 2 and C1 = E1 = {d1, d2, d3}.",
                "Observe that, if we had used a second outranking relation S2(⊇ S1), these three documents could have been possibly discriminated.",
                "At this stage, we remove documents of C1 from the outranking matrix and compute the next class C2: we compute the new qualifications of the documents of E0 = R \\ C1 = {d4, d5}.",
                "They are respectively 1 and -1.",
                "So C3 = E1 = {d4}.",
                "The last document d5 is the only document of the last class C3.",
                "Thus, the consensus ranking is {d1, d2, d3} → {d4} → {d5}. 5.",
                "EXPERIMENTS AND RESULTS 5.1 Test Setting To facilitate empirical investigation of the proposed methodology, we developed a prototype metasearch engine that implements a version of our outranking approach for rank aggregation.",
                "In this paper, we apply our approach to the Topic Distillation (TD) task of TREC-2004 Web track [10].",
                "In this task, there are 75 topics where only a short description of each is given.",
                "For each query, we retained the rankings of the 10 best runs of the TD task which are provided by TREC-2004 participating teams.",
                "The performances of these runs are reported in table 3.",
                "Table 3: Performances of the 10 best runs of the TD task of TREC-2004 Run Id MAP P@10 S@1 S@5 S@10 uogWebCAU150 17.9% 24.9% 50.7% 77.3% 89.3% MSRAmixed1 17.8% 25.1% 38.7% 72.0% 88.0% MSRC04C12 16.5% 23.1% 38.7% 74.7% 80.0% humW04rdpl 16.3% 23.1% 37.3% 78.7% 90.7% THUIRmix042 14.7% 20.5% 21.3% 58.7% 74.7% UAmsT04MWScb 14.6% 20.9% 36.0% 66.7% 76.0% ICT04CIIS1AT 14.1% 20.8% 33.3% 64.0% 78.7% SJTUINCMIX5 12.9% 18.9% 29.3% 57.3% 72.0% MU04web1 11.5% 19.9% 33.3% 64.0% 76.0% MeijiHILw3 11.5% 15.3% 30.7% 54.7% 64.0% Average 14.7% 21.2% 34.9% 66.8% 78.94% For each query, each run provides a ranking of about 1000 documents.",
                "The number of documents retrieved by all these runs ranges from 543 to 5769.",
                "Their average (median) number is 3340 (3386).",
                "It is worth noting that we found similar distributions of the documents among the rankings as in [11].",
                "For evaluation, we used the trec eval standard tool which is used by the TREC community to calculate the standard measures of system effectiveness which are Mean Average Precision (MAP) and Success@n (S@n) for n=1, 5 and 10.",
                "Our approach effectiveness is compared against some high performing official results from TREC-2004 as well as against some standard rank aggregation algorithms.",
                "In the experiments, significance testing is mainly based on the t-student statistic which is computed on the basis of the MAP values of the compared runs.",
                "In the tables of the following section, statistically significant differences are marked with an asterisk.",
                "Values between brackets of the first column of each table, indicate the parameter value of the corresponding run. 5.2 Results We carried out several series of runs in order to i) study performance variations of the outranking approach when tuning the parameters and working assumptions, ii) compare performances of the outranking approach vs standard rank aggregation strategies , and iii) check whether rank aggregation performs better than the best input rankings.",
                "We set our basic run mcm with the following parameters.",
                "We considered that each input ranking is a complete order (sp = 0) and that an input ranking strongly refutes diσdi when the difference of both document positions is large enough (sv = 75%).",
                "Preference and veto thresholds are computed proportionally to the number of documents retained in each input ranking.",
                "They consequently may vary from one ranking to another.",
                "In addition, to accept the assertion diσdi , we supposed that the majority of the rankings must be concordant (cmin = 50%) and that every input ranking can impose its veto (dmax = 0).",
                "Concordance and discordance thresholds are computed for each tuple (di, di ) as the percentage of the input rankings of PRi ∩PRi .",
                "Thus, our choice of parameters leads to the definition of the outranking relation S(0,75%,50%,0).",
                "To test the run mcm, we had chosen the following assumptions.",
                "We retained the top 100 best documents from each input ranking (H1 100), only considered documents which are present in at least half of the input rankings (H2 5 ) and assumed H3 no and H4 new.",
                "In these conditions, the number of successful documents was about 100 on average, and the computation time per query was less than one second.",
                "Obviously, modifying the working assumptions should have deeper impact on the performances than tuning our model parameters.",
                "This was validated by preliminary experiments.",
                "Thus, we hereafter begin by studying performance variation when different sets of assumptions are considered.",
                "Afterwards, we study the impact of tuning parameters.",
                "Finally, we compare our model performances w.r.t. the input rankings as well as some standard data fusion algorithms. 5.2.1 Impact of the Working Assumptions Table 4 summarizes the performance variation of the outranking approach under different working hypotheses.",
                "In Table 4: Impact of the working assumptions Run Id MAP S@1 S@5 S@10 mcm 18.47% 41.33% 81.33% 86.67% mcm22 (H3 yes) 17.72% (-4.06%) 34.67% 81.33% 86.67% mcm23 (H4 init) 18.26% (-1.14%) 41.33% 81.33% 86.67% mcm24 (H1 all) 20.67% (+11.91%*) 38.66% 80.00% 86.66% mcm25 (H2 all) 21.68% (+17.38%*) 40.00% 78.66% 89.33% this table, we first show that run mcm22, in which missing documents are all put in the same last position of each input ranking, leads to performance drop w.r.t. run mcm.",
                "Moreover, S@1 moves from 41.33% to 34.67% (-16.11%).",
                "This shows that several relevant documents which were initially put at the first position of the consensus ranking in mcm, lose this first position but remain ranked in the top 5 documents since S@5 did not change.",
                "We also conclude that documents which have rather good positions in some input rankings are more likely to be relevant, even though they are missing in some other rankings.",
                "Consequently, when they are missing in some rankings, assigning worse ranks to these documents is harmful for performance.",
                "Also, from Table 4, we found that the performances of runs mcm and mcm23 are similar.",
                "Therefore, the outranking approach is not sensitive to keeping the initial positions of candidate documents or recomputing them by discarding excluded ones.",
                "From the same Table 4, performance of the outranking approach increases significantly for runs mcm24 and mcm25.",
                "Therefore, whether we consider all the documents which are present in half of the rankings (mcm24) or we consider all the documents which are ranked in the first 100 positions in one or more rankings (mcm25), increases performances.",
                "This result was predictable since in both cases we have more detailed information on the relative importance of documents.",
                "Tables 5 and 6 confirm this evidence.",
                "Table 5, where values between brackets of the first column give the number of documents which are retained from each input ranking, shows that selecting more documents from each input ranking leads to performance increase.",
                "It is worth mentioning that selecting more than 600 documents from each input ranking does not improve performance.",
                "Table 5: Impact of the number of retained documents Run Id MAP S@1 S@5 S@10 mcm (100) 18.47% 41.33% 81.33% 86.67% mcm24-1 (200) 19.32% (+4.60%) 42.67% 78.67% 88.00% mcm24-2 (400) 19.88% (+7.63%*) 37.33% 80.00% 88.00% mcm24-3 (600) 20.80% (+12.62%*) 40.00% 80.00% 88.00% mcm24-4 (800) 20.66% (+11.86%*) 40.00% 78.67% 86.67% mcm24 (1000) 20.67% (+11.91%*) 38.66% 80.00% 86.66% Table 6 reports runs corresponding to variations of H2 k .",
                "Values between brackets are rank hits.",
                "For instance, in the run mcm32, only documents which are present in 3 or more input rankings, were considered successful.",
                "This table shows that performance is significantly better when rare documents are considered, whereas it decreases significantly when these documents are discarded.",
                "Therefore, we conclude that many of the relevant documents are retrieved by a rather small set of IR models.",
                "Table 6: Performance considering different rank hits Run Id MAP S@1 S@5 S@10 mcm25 (1) 21.68% (+17.38%*) 40.00% 78.67% 89.33% mcm32 (3) 18.98% (+2.76%) 38.67% 80.00% 85.33% mcm (5) 18.47% 41.33% 81.33% 86.67% mcm33 (7) 15.83% (-14.29%*) 37.33% 78.67% 85.33% mcm34 (9) 10.96% (-40.66%*) 36.11% 66.67% 70.83% mcm35 (10) 7.42% (-59.83%*) 39.22% 62.75% 64.70% For both runs mcm24 and mcm25, the number of successful documents was about 1000 and therefore, the computation time per query increased and became around 5 seconds. 5.2.2 Impact of the Variation of the Parameters Table 7 shows performance variation of the outranking approach when different preference thresholds are considered.",
                "We found performance improvement up to threshold values of about 5%, then there is a decrease in the performance which becomes significant for threshold values greater than 10%.",
                "Moreover, S@1 improves from 41.33% to 46.67% when preference threshold changes from 0 to 5%.",
                "We can thus conclude that the input rankings are semi orders rather than complete orders.",
                "Table 8 shows the evolution of the performance measures w.r.t. the concordance threshold.",
                "We can conclude that in order to put document di before di in the consensus ranking, Table 7: Impact of the variation of the preference threshold from 0 to 12.5% Run Id MAP S@1 S@5 S@10 mcm (0%) 18.47% 41.33% 81.33% 86.67% mcm1 (1%) 18.57% (+0.54%) 41.33% 81.33% 86.67% mcm2 (2.5%) 18.63% (+0.87%) 42.67% 78.67% 86.67% mcm3 (5%) 18.69% (+1.19%) 46.67% 81.33% 86.67% mcm4 (7.5%) 18.24% (-1.25%) 46.67% 81.33% 86.67% mcm5 (10%) 17.93% (-2.92%) 40.00% 82.67% 86.67% mcm5b (12.5%) 17.51% (-5.20%*) 41.33% 80.00% 86.67% at least half of the input rankings of PRi ∩ PRi should be concordant.",
                "Performance drops significantly for very low and very high values of the concordance threshold.",
                "In fact, for such values, the concordance condition is either fulfilled rather always by too many document pairs or not fulfilled at all, respectively.",
                "Therefore, the outranking relation becomes either too weak or too strong respectively.",
                "Table 8: Impact of the variation of cmin Run Id MAP S@1 S@5 S@10 mcm11 (20%) 17.63% (-4.55%*) 41.33% 76.00% 85.33% mcm12 (40%) 18.37% (-0.54%) 42.67% 76.00% 86.67% mcm (50%) 18.47% 41.33% 81.33% 86.67% mcm13 (60%) 18.42% (-0.27%) 40.00% 78.67% 86.67% mcm14 (80%) 17.43% (-5.63%*) 40.00% 78.67% 86.67% mcm15 (100%) 16.12% (-12.72%*) 41.33% 70.67% 85.33% In the experiments, varying the veto threshold as well as the discordance threshold within reasonable intervals does not have significant impact on performance measures.",
                "In fact, runs with different veto thresholds (sv ∈ [50%; 100%]) had similar performances even though there is a slight advantage for runs with high threshold values which means that it is better not to allow the input rankings to put their veto easily.",
                "Also, tuning the discordance threshold was carried out for values 50% and 75% of the veto threshold.",
                "For these runs we did not get any noticeable performance variation, although for low discordance thresholds (dmax < 20%), performance slightly decreased. 5.2.3 Impact of the Variation of the Number of Input Rankings To study performance evolution when different sets of input rankings are considered, we carried three more runs where 2, 4, and 6 of the best performing sets of the input rankings are considered.",
                "Results reported in Table 9 are seemingly counter-intuitive and also do not support previous findings regarding rank aggregation research [3].",
                "Nevertheless, this result shows that low performing rankings bring more noise than information to the establishment of the consensus ranking.",
                "Therefore, when they are considered, performance decreases.",
                "Table 9: Performance considering different best performing sets of input rankings Run Id MAP S@1 S@5 S@10 mcm (10) 18.47% 41.33% 81.33% 86.67% mcm27 (6) 18.60% (+0.70%) 41.33% 80.00% 85.33% mcm28 (4) 19.02% (+2.98%) 40.00% 86.67% 88.00% mcm29 (2) 18.33% (-0.76%) 44.00% 76.00% 88.00% 5.2.4 Comparison of the Performance of Different Rank Aggregation Methods In this set of runs, we compare the outranking approach with some standard rank aggregation methods which were proven to have acceptable performance in previous studies: we considered two positional methods which are the CombSUM and the CombMNZ strategies.",
                "We also examined the performance of one majoritarian method which is the Markov chain method (MC4).",
                "For the comparisons, we considered a specific outranking relation S∗ = S(5%,50%,50%,30%) which results in good overall performances when tuning all the parameters.",
                "The first row of Table 10 gives performances of the rank aggregation methods w.r.t. a basic assumption set A1 = (H1 100, H2 5 , H4 new): we only consider the 100 first documents from each ranking, then retain documents present in 5 or more rankings and update ranks of successful documents.",
                "For positional methods, we place missing documents at the queue of the ranking (H3 yes) whereas for our method as well as for MC4, we retained hypothesis H3 no.",
                "The three following rows of Table 10 report performances when changing one element from the basic assumption set: the second row corresponds to the assumption set A2 = (H1 1000, H2 5 , H4 new), i.e. changing the number of retained documents from 100 to 1000.",
                "The third row corresponds to the assumption set A3 = (H1 100, H2 all, H4 new), i.e. considering the documents present in at least one ranking.",
                "The fourth row corresponds to the assumption set A4 = (H1 100, H2 5 , H4 init), i.e. keeping the original ranks of successful documents.",
                "The fifth row of Table 10, labeled A5, gives performance when all the 225 queries of the Web track of TREC-2004 are considered.",
                "Obviously, performance level cannot be compared with previous lines since the additional queries are different from the TD queries and correspond to other tasks (Home Page and Named Page tasks [10]) of TREC-2004 Web track.",
                "This set of runs aims to show whether relative performance of the various methods is task-dependent.",
                "The last row of Table 10, labeled A6, reports performance of the various methods considering the TD task of TREC2002 instead of TREC-2004: we fused the results of input rankings of the 10 best official runs for each of the 50 TD queries [9] considering the set of assumptions A1 of the first row.",
                "This aims to show whether relative performance of the various methods changes from year to year.",
                "Values between brackets of Table 10 are variations of performance of each rank aggregation method w.r.t. performance of the outranking approach.",
                "Table 10: Performance (MAP) of different rank aggregation methods under 3 different test collections mcm combSUM combMNZ markov A1 18.79% 17.54% (-6.65%*) 17.08% (-9.10%*) 18.63% (-0.85%) A2 21.36% 19.18% (-10.21%*) 18.61% (-12.87%*) 21.33% (-0.14%) A3 21.92% 21.38% (-2.46%) 20.88% (-4.74%) 19.35% (-11.72%*) A4 18.64% 17.58% (-5.69%*) 17.18% (-7.83%*) 18.63% (-0.05%) A5 55.39% 52.16% (-5.83%*) 49.70% (-10.27%*) 53.30% (-3.77%) A6 16.95% 15.65% (-7.67%*) 14.57% (-14.04%*) 16.39% (-3.30%) From the analysis of table 10 the following can be established: • for all the runs, considering all the documents in each input ranking (A2) significantly improves performance (MAP increases by 11.62% on average).",
                "This is predictable since some initially unreported relevant documents would receive better positions in the consensus ranking. • for all the runs, considering documents even those present in only one input ranking (A3) significantly improves performance.",
                "For mcm, combSUM and combMNZ, performance improvement is more important (MAP increases by 20.27% on average) than for the markov run (MAP increases by 3.86%). • preserving the initial positions of documents (A4) or recomputing them (A1) does not have a noticeable influence on performance for both positional and majoritarian methods. • considering all the queries of the Web track of TREC2004 (A5) as well as the TD queries of the Web track of TREC-2002 (A6) does not alter the relative performance of the different data fusion methods. • considering the TD queries of the Web track of TREC2002, performances of all the data fusion methods are lower than that of the best performing input ranking for which the MAP value equals 18.58%.",
                "This is because most of the fused input rankings have very low performances compared to the best one, which brings more noise to the consensus ranking. • performances of the data fusion methods mcm and markov are significantly better than that of the best input ranking uogWebCAU150.",
                "This remains true for runs combSUM and combMNZ only under assumptions H1 all or H2 all.",
                "This shows that majoritarian methods are less sensitive to assumptions than positional methods. • outranking approach always performs significantly better than positional methods combSUM and combMNZ.",
                "It has also better performances than the Markov chain method, especially under assumption H2 all where difference of performances becomes significant. 6.",
                "CONCLUSIONS In this paper, we address the rank aggregation problem where different, but not disjoint, lists of documents are to be fused.",
                "We noticed that the input rankings can hide ties, so they should not be considered as complete orders.",
                "Only robust information should be used from each input ranking.",
                "Current rank aggregation methods, and especially positional methods (e.g. combSUM [15]), are not initially designed to work with such rankings.",
                "They should be adapted by considering specific working assumptions.",
                "We propose a new outranking method for rank aggregation which is well adapted to the IR context.",
                "Indeed, it ranks two documents w.r.t. the intensity of their positions difference in each input ranking and also considering the number of the input rankings that are concordant and discordant in favor of a specific document.",
                "There is also no need to make specific assumptions on the positions of the missing documents.",
                "This is an important feature since the absence of a document from a ranking should not be necessarily interpreted negatively.",
                "Experimental results show that the outranking method significantly out-performs popular classical positional data fusion methods like combSUM and combMNZ strategies.",
                "It also out-performs a good performing majoritarian methods which is the Markov chain method.",
                "These results are tested against different test collections and queries.",
                "From the experiments, we can also conclude that in order to improve the performances, we should fuse result lists of well performing IR models, and that majoritarian data fusion methods perform better than positional methods.",
                "The proposed method can have a real impact on Web metasearch performances since only ranks are available from most primary search engines, whereas most of the current approaches need scores to merge result lists into one single list.",
                "Further work involves investigating whether the outranking approach performs well in various other contexts, e.g. using the document scores or some combination of document ranks and scores.",
                "Acknowledgments The authors would like to thank Jacques Savoy for his valuable comments on a preliminary version of this paper. 7.",
                "REFERENCES [1] A. Aronson, D. Demner-Fushman, S. Humphrey, J. Lin, H. Liu, P. Ruch, M. Ruiz, L. Smith, L. Tanabe, and W. Wilbur.",
                "Fusion of knowledge-intensive and statistical approaches for retrieving and annotating textual genomics documents.",
                "In Proceedings TREC2005.",
                "NIST Publication, 2005. [2] R. A. Baeza-Yates and B.",
                "A. Ribeiro-Neto.",
                "Modern Information Retrieval.",
                "ACM Press , 1999. [3] B. T. Bartell, G. W. Cottrell, and R. K. Belew.",
                "Automatic combination of multiple ranked retrieval systems.",
                "In Proceedings ACM-SIGIR94, pages 173-181.",
                "Springer-Verlag, 1994. [4] N. J. Belkin, P. Kantor, E. A.",
                "Fox, and J.",
                "A. Shaw.",
                "Combining evidence of multiple query representations for information retrieval.",
                "IPM, 31(3):431-448, 1995. [5] J. Borda.",
                "M´emoire sur les ´elections au scrutin.",
                "Histoire de lAcad´emie des Sciences, 1781. [6] J. P. Callan, Z. Lu, and W. B. Croft.",
                "Searching distributed collections with inference networks.",
                "In Proceedings ACM-SIGIR95, pages 21-28, 1995. [7] M. Condorcet.",
                "Essai sur lapplication de lanalyse `a la probabilit´e des d´ecisions rendues `a la pluralit´e des voix.",
                "Imprimerie Royale, Paris, 1785. [8] W. D. Cook and M. Kress.",
                "Ordinal ranking with intensity of preference.",
                "Management Science, 31(1):26-32, 1985. [9] N. Craswell and D. Hawking.",
                "Overview of the TREC-2002 Web Track.",
                "In Proceedings TREC2002.",
                "NIST Publication, 2002. [10] N. Craswell and D. Hawking.",
                "Overview of the TREC-2004 Web Track.",
                "In Proceedings of TREC2004.",
                "NIST Publication, 2004. [11] C. Dwork, S. R. Kumar, M. Naor, and D. Sivakumar.",
                "Rank aggregation methods for the Web.",
                "In Proceedings WWW2001, pages 613-622, 2001. [12] R. Fagin.",
                "Combining fuzzy information from multiple systems.",
                "JCSS, 58(1):83-99, 1999. [13] R. Fagin, R. Kumar, M. Mahdian, D. Sivakumar, and E. Vee.",
                "Comparing and aggregating rankings with ties.",
                "In PODS, pages 47-58, 2004. [14] R. Fagin, R. Kumar, and D. Sivakumar.",
                "Comparing top k lists.",
                "SIAM J. on Discrete Mathematics, 17(1):134-160, 2003. [15] E. A.",
                "Fox and J.",
                "A. Shaw.",
                "Combination of multiple searches.",
                "In Proceedings of TREC3.",
                "NIST Publication, 1994. [16] J. Katzer, M. McGill, J. Tessier, W. Frakes, and P. DasGupta.",
                "A study of the overlap among document representations.",
                "Information Technology: Research and Development, 1(4):261-274, 1982. [17] L. S. Larkey, M. E. Connell, and J. Callan.",
                "Collection selection and results merging with topically organized U.S. patents and TREC data.",
                "In Proceedings ACM-CIKM2000, pages 282-289.",
                "ACM Press, 2000. [18] A.",
                "Le Calv´e and J. Savoy.",
                "Database merging strategy based on logistic regression.",
                "IPM, 36(3):341-359, 2000. [19] J. H. Lee.",
                "Analyses of multiple evidence combination.",
                "In Proceedings ACM-SIGIR97, pages 267-276, 1997. [20] D. Lillis, F. Toolan, R. Collier, and J. Dunnion.",
                "Probfuse: a probabilistic approach to data fusion.",
                "In Proceedings ACM-SIGIR2006, pages 139-146.",
                "ACM Press, 2006. [21] J. I. Marden.",
                "Analyzing and Modeling Rank Data.",
                "Number 64 in Monographs on Statistics and Applied Probability.",
                "Chapman & Hall, 1995. [22] M. Montague and J.",
                "A. Aslam.",
                "Metasearch consistency.",
                "In Proceedings ACM-SIGIR2001, pages 386-387.",
                "ACM Press, 2001. [23] D. M. Pennock and E. Horvitz.",
                "Analysis of the axiomatic foundations of collaborative filtering.",
                "In Workshop on AI for Electronic Commerce at the 16th National Conference on Artificial Intelligence, 1999. [24] M. E. Renda and U. Straccia.",
                "Web metasearch: rank vs. score based rank aggregation methods.",
                "In Proceedings ACM-SAC2003, pages 841-846.",
                "ACM Press, 2003. [25] W. H. Riker.",
                "Liberalism against populism.",
                "Waveland Press, 1982. [26] B. Roy.",
                "The outranking approach and the foundations of ELECTRE methods.",
                "Theory and Decision, 31:49-73, 1991. [27] B. Roy and J. Hugonnard.",
                "Ranking of suburban line extension projects on the Paris metro system by a multicriteria method.",
                "Transportation Research, 16A(4):301-312, 1982. [28] L. Si and J. Callan.",
                "Using sampled data and regression to merge search engine results.",
                "In Proceedings ACM-SIGIR2002, pages 19-26.",
                "ACM Press, 2002. [29] M. Truchon.",
                "An extension of the Condorcet criterion and Kemeny orders.",
                "Cahier 9813, Centre de Recherche en Economie et Finance Appliqu´ees, Oct. 1998. [30] H. Turtle and W. B. Croft.",
                "Inference networks for document retrieval.",
                "In Proceedings of ACM-SIGIR90, pages 1-24.",
                "ACM Press, 1990. [31] C. C. Vogt and G. W. Cottrell.",
                "Fusion via a linear combination of scores.",
                "Information Retrieval, 1(3):151-173, 1999."
            ],
            "original_annotated_samples": [],
            "translated_annotated_samples": [],
            "translated_text": "",
            "candidates": [],
            "error": []
        }
    }
}