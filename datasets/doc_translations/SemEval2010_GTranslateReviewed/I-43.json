{
    "id": "I-43",
    "original_text": "Dynamics Based Control with an Application to Area-Sweeping Problems Zinovi Rabinovich Engineering and Computer Science Hebrew University of Jerusalem Jerusalem, Israel nomad@cs.huji.ac.il Jeffrey S. Rosenschein Engineering and Computer Science Hebrew University of Jerusalem Jerusalem, Israel jeff@cs.huji.ac.il Gal A. Kaminka The MAVERICK Group Department of Computer Science Bar Ilan University, Israel galk@cs.biu.ac.il ABSTRACT In this paper we introduce Dynamics Based Control (DBC), an approach to planning and control of an agent in stochastic environments. Unlike existing approaches, which seek to optimize expected rewards (e.g., in Partially Observable Markov Decision Problems (POMDPs)), DBC optimizes system behavior towards specified system dynamics. We show that a recently developed planning and control approach, Extended Markov Tracking (EMT) is an instantiation of DBC. EMT employs greedy action selection to provide an efficient control algorithm in Markovian environments. We exploit this efficiency in a set of experiments that applied multitarget EMT to a class of area-sweeping problems (searching for moving targets). We show that such problems can be naturally defined and efficiently solved using the DBC framework, and its EMT instantiation. Categories and Subject Descriptors I.2.8 [Problem Solving, Control Methods, and Search]: Control Theory; I.2.9 [Robotics]; I.2.11 [Distributed Artificial Intelligence]: Intelligent Agents General Terms Algorithms, Theory 1. INTRODUCTION Planning and control constitutes a central research area in multiagent systems and artificial intelligence. In recent years, Partially Observable Markov Decision Processes (POMDPs) [12] have become a popular formal basis for planning in stochastic environments. In this framework, the planning and control problem is often addressed by imposing a reward function, and computing a policy (of choosing actions) that is optimal, in the sense that it will result in the highest expected utility. While theoretically attractive, the complexity of optimally solving a POMDP is prohibitive [8, 7]. We take an alternative view of planning in stochastic environments. We do not use a (state-based) reward function, but instead optimize over a different criterion, a transition-based specification of the desired system dynamics. The idea here is to view planexecution as a process that compels a (stochastic) system to change, and a plan as a dynamic process that shapes that change according to desired criteria. We call this general planning framework Dynamics Based Control (DBC). In DBC, the goal of a planning (or control) process becomes to ensure that the system will change in accordance with specific (potentially stochastic) target dynamics. As actual system behavior may deviate from that which is specified by target dynamics (due to the stochastic nature of the system), planning in such environments needs to be continual [4], in a manner similar to classical closed-loop controllers [16]. Here, optimality is measured in terms of probability of deviation magnitudes. In this paper, we present the structure of Dynamics Based Control. We show that the recently developed Extended Markov Tracking (EMT) approach [13, 14, 15] is subsumed by DBC, with EMT employing greedy action selection, which is a specific parameterization among the options possible within DBC. EMT is an efficient instantiation of DBC. To evaluate DBC, we carried out a set of experiments applying multi-target EMT to the Tag Game [11]; this is a variant on the area sweeping problem, where an agent is trying to tag a moving target (quarry) whose position is not known with certainty. Experimental data demonstrates that even with a simple model of the environment and a simple design of target dynamics, high success rates can be produced both in catching the quarry, and in surprising the quarry (as expressed by the observed entropy of the controlled agents position). The paper is organized as follows. In Section 2 we motivate DBC using area-sweeping problems, and discuss related work. Section 3 introduces the Dynamics Based Control (DBC) structure, and its specialization to Markovian environments. This is followed by a review of the Extended Markov Tracking (EMT) approach as a DBC-structured control regimen in Section 4. That section also discusses the limitations of EMT-based control relative to the general DBC framework. Experimental settings and results are then presented in Section 5. Section 6 provides a short discussion of the overall approach, and Section 7 gives some concluding remarks and directions for future work. 790 978-81-904262-7-5 (RPS) c 2007 IFAAMAS 2. MOTIVATION AND RELATED WORK Many real-life scenarios naturally have a stochastic target dynamics specification, especially those domains where there exists no ultimate goal, but rather system behavior (with specific properties) that has to be continually supported. For example, security guards perform persistent sweeps of an area to detect any sign of intrusion. Cunning thieves will attempt to track these sweeps, and time their operation to key points of the guards motion. It is thus advisable to make the guards motion dynamics appear irregular and random. Recent work by Paruchuri et al. [10] has addressed such randomization in the context of single-agent and distributed POMDPs. The goal in that work was to generate policies that provide a measure of action-selection randomization, while maintaining rewards within some acceptable levels. Our focus differs from this work in that DBC does not optimize expected rewards-indeed we do not consider rewards at all-but instead maintains desired dynamics (including, but not limited to, randomization). The Game of Tag is another example of the applicability of the approach. It was introduced in the work by Pineau et al. [11]. There are two agents that can move about an area, which is divided into a grid. The grid may have blocked cells (holes) into which no agent can move. One agent (the hunter) seeks to move into a cell occupied by the other (the quarry), such that they are co-located (this is a successful tag). The quarry seeks to avoid the hunter agent, and is always aware of the hunters position, but does not know how the hunter will behave, which opens up the possibility for a hunter to surprise the prey. The hunter knows the quarrys probabilistic law of motion, but does not know its current location. Tag is an instance of a family of area-sweeping (pursuit-evasion) problems. In [11], the hunter modeled the problem using a POMDP. A reward function was defined, to reflect the desire to tag the quarry, and an action policy was computed to optimize the reward collected over time. Due to the intractable complexity of determining the optimal policy, the action policy computed in that paper was essentially an approximation. In this paper, instead of formulating a reward function, we use EMT to solve the problem, by directly specifying the target dynamics. In fact, any search problem with randomized motion, the socalled class of area sweeping problems, can be described through specification of such target system dynamics. Dynamics Based Control provides a natural approach to solving these problems. 3. DYNAMICS BASED CONTROL The specification of Dynamics Based Control (DBC) can be broken into three interacting levels: Environment Design Level, User Level, and Agent Level. • Environment Design Level is concerned with the formal specification and modeling of the environment. For example, this level would specify the laws of physics within the system, and set its parameters, such as the gravitation constant. • User Level in turn relies on the environment model produced by Environment Design to specify the target system dynamics it wishes to observe. The User Level also specifies the estimation or learning procedure for system dynamics, and the measure of deviation. In the museum guard scenario above, these would correspond to a stochastic sweep schedule, and a measure of relative surprise between the specified and actual sweeping. • Agent Level in turn combines the environment model from the Environment Design level, the dynamics estimation procedure, the deviation measure and the target dynamics specification from User Level, to produce a sequence of actions that create system dynamics as close as possible to the targeted specification. As we are interested in the continual development of a stochastic system, such as happens in classical control theory [16] and continual planning [4], as well as in our example of museum sweeps, the question becomes how the Agent Level is to treat the deviation measurements over time. To this end, we use a probability threshold-that is, we would like the Agent Level to maximize the probability that the deviation measure will remain below a certain threshold. Specific action selection then depends on system formalization. One possibility would be to create a mixture of available system trends, much like that which happens in Behavior-Based Robotic architectures [1]. The other alternative would be to rely on the estimation procedure provided by the User Level-to utilize the Environment Design Level model of the environment to choose actions, so as to manipulate the dynamics estimator into believing that a certain dynamics has been achieved. Notice that this manipulation is not direct, but via the environment. Thus, for strong enough estimator algorithms, successful manipulation would mean a successful simulation of the specified target dynamics (i.e., beyond discerning via the available sensory input). DBC levels can also have a back-flow of information (see Figure 1). For instance, the Agent Level could provide data about target dynamics feasibility, allowing the User Level to modify the requirement, perhaps focusing on attainable features of system behavior. Data would also be available about the system response to different actions performed; combined with a dynamics estimator defined by the User Level, this can provide an important tool for the environment model calibration at the Environment Design Level. UserEnv. Design Agent Model Ideal Dynamics Estimator Estimator Dynamics Feasibility System Response Data Figure 1: Data flow of the DBC framework Extending upon the idea of Actor-Critic algorithms [5], DBC data flow can provide a good basis for the design of a learning algorithm. For example, the User Level can operate as an exploratory device for a learning algorithm, inferring an ideal dynamics target from the environment model at hand that would expose and verify most critical features of system behavior. In this case, feasibility and system response data from the Agent Level would provide key information for an environment model update. In fact, the combination of feasibility and response data can provide a basis for the application of strong learning algorithms such as EM [2, 9]. 3.1 DBC for Markovian Environments For a Partially Observable Markovian Environment, DBC can be specified in a more rigorous manner. Notice how DBC discards rewards, and replaces it by another optimality criterion (structural differences are summarized in Table 1): • Environment Design level is to specify a tuple < S, A, T, O, Ω, s0 >, where: - S is the set of all possible environment states; - s0 is the initial state of the environment (which can also be viewed as a probability distribution over S); The Sixth Intl. Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) 791 - A is the set of all possible actions applicable in the environment; - T is the environments probabilistic transition function: T : S ×A → Π(S). That is, T(s |a, s) is the probability that the environment will move from state s to state s under action a; - O is the set of all possible observations. This is what the sensor input would look like for an outside observer; - Ω is the observation probability function: Ω : S × A × S → Π(O). That is, Ω(o|s , a, s) is the probability that one will observe o given that the environment has moved from state s to state s under action a. • User Level, in the case of Markovian environment, operates on the set of system dynamics described by a family of conditional probabilities F = {τ : S × A → Π(S)}. Thus specification of target dynamics can be expressed by q ∈ F, and the learning or tracking algorithm can be represented as a function L : O×(A×O)∗ → F; that is, it maps sequences of observations and actions performed so far into an estimate τ ∈ F of system dynamics. There are many possible variations available at the User Level to define divergence between system dynamics; several of them are: - Trace distance or L1 distance between two distributions p and q defined by D(p(·), q(·)) = 1 2 x |p(x) − q(x)| - Fidelity measure of distance F(p(·), q(·)) = x p(x)q(x) - Kullback-Leibler divergence DKL(p(·) q(·)) = x p(x) log p(x) q(x) Notice that the latter two are not actually metrics over the space of possible distributions, but nevertheless have meaningful and important interpretations. For instance, KullbackLeibler divergence is an important tool of information theory [3] that allows one to measure the price of encoding an information source governed by q, while assuming that it is governed by p. The User Level also defines the threshold of dynamics deviation probability θ. • Agent Level is then faced with a problem of selecting a control signal function a∗ to satisfy a minimization problem as follows: a∗ = arg min a Pr(d(τa, q) > θ) where d(τa, q) is a random variable describing deviation of the dynamics estimate τa, created by L under control signal a, from the ideal dynamics q. Implicit in this minimization problem is that L is manipulated via the environment, based on the environment model produced by the Environment Design Level. 3.2 DBC View of the State Space It is important to note the complementary view that DBC and POMDPs take on the state space of the environment. POMDPs regard state as a stationary snap-shot of the environment; whatever attributes of state sequencing one seeks are reached through properties of the control process, in this case reward accumulation. This can be viewed as if the sequencing of states and the attributes of that sequencing are only introduced by and for the controlling mechanism-the POMDP policy. DBC concentrates on the underlying principle of state sequencing, the system dynamics. DBCs target dynamics specification can use the environments state space as a means to describe, discern, and preserve changes that occur within the system. As a result, DBC has a greater ability to express state sequencing properties, which are grounded in the environment model and its state space definition. For example, consider the task of moving through rough terrain towards a goal and reaching it as fast as possible. POMDPs would encode terrain as state space points, while speed would be ensured by negative reward for every step taken without reaching the goalaccumulating higher reward can be reached only by faster motion. Alternatively, the state space could directly include the notion of speed. For POMDPs, this would mean that the same concept is encoded twice, in some sense: directly in the state space, and indirectly within reward accumulation. Now, even if the reward function would encode more, and finer, details of the properties of motion, the POMDP solution will have to search in a much larger space of policies, while still being guided by the implicit concept of the reward accumulation procedure. On the other hand, the tactical target expression of variations and correlations between position and speed of motion is now grounded in the state space representation. In this situation, any further constraints, e.g., smoothness of motion, speed limits in different locations, or speed reductions during sharp turns, are explicitly and uniformly expressed by the tactical target, and can result in faster and more effective action selection by a DBC algorithm. 4. EMT-BASED CONTROL AS A DBC Recently, a control algorithm was introduced called EMT-based Control [13], which instantiates the DBC framework. Although it provides an approximate greedy solution in the DBC sense, initial experiments using EMT-based control have been encouraging [14, 15]. EMT-based control is based on the Markovian environment definition, as in the case with POMDPs, but its User and Agent Levels are of the Markovian DBC type of optimality. • User Level of EMT-based control defines a limited-case target system dynamics independent of action: qEMT : S → Π(S). It then utilizes the Kullback-Leibler divergence measure to compose a momentary system dynamics estimator-the Extended Markov Tracking (EMT) algorithm. The algorithm keeps a system dynamics estimate τt EMT that is capable of explaining recent change in an auxiliary Bayesian system state estimator from pt−1 to pt, and updates it conservatively using Kullback-Leibler divergence. Since τt EMT and pt−1,t are respectively the conditional and marginal probabilities over the systems state space, explanation simply means that pt(s ) = s τt EMT (s |s)pt−1(s), and the dynamics estimate update is performed by solving a 792 The Sixth Intl. Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) Table 1: Structure of POMDP vs. Dynamics-Based Control in Markovian Environment Level Approach MDP Markovian DBC Environment < S, A, T, O, Ω >,where S - set of states A - set of actions Design T : S × A → Π(S) - transition O - observation set Ω : S × A × S → Π(O) User r : S × A × S → R q : S × A → Π(S) F(π∗ ) → r L(o1, ..., ot) → τ r - reward function q - ideal dynamics F - reward remodeling L - dynamics estimator θ - threshold Agent π∗ = arg max π E[ γt rt] π∗ = arg min π Prob(d(τ q) > θ) minimization problem: τt EMT = H[pt, pt−1, τt−1 EMT ] = arg min τ DKL(τ × pt−1 τt−1 EMT × pt−1) s.t. pt(s ) = s (τ × pt−1)(s , s) pt−1(s) = s (τ × pt−1)(s , s) • Agent Level in EMT-based control is suboptimal with respect to DBC (though it remains within the DBC framework), performing greedy action selection based on prediction of EMTs reaction. The prediction is based on the environment model provided by the Environment Design level, so that if we denote by Ta the environments transition function limited to action a, and pt−1 is the auxiliary Bayesian system state estimator, then the EMT-based control choice is described by a∗ = arg min a∈A DKL(H[Ta × pt, pt, τt EMT ] qEMT × pt−1) Note that this follows the Markovian DBC framework precisely: the rewarding optimality of POMDPs is discarded, and in its place a dynamics estimator (EMT in this case) is manipulated via action effects on the environment to produce an estimate close to the specified target system dynamics. Yet as we mentioned, naive EMTbased control is suboptimal in the DBC sense, and has several additional limitations that do not exist in the general DBC framework (discussed in Section 4.2). 4.1 Multi-Target EMT At times, there may exist several behavioral preferences. For example, in the case of museum guards, some art items are more heavily guarded, requiring that the guards stay more often in their close vicinity. On the other hand, no corner of the museum is to be left unchecked, which demands constant motion. Successful museum security would demand that the guards adhere to, and balance, both of these behaviors. For EMT-based control, this would mean facing several tactical targets {qk}K k=1, and the question becomes how to merge and balance them. A balancing mechanism can be applied to resolve this issue. Note that EMT-based control, while selecting an action, creates a preference vector over the set of actions based on their predicted performance with respect to a given target. If these preference vectors are normalized, they can be combined into a single unified preference. This requires replacement of standard EMT-based action selection by the algorithm below [15]: • Given: - a set of target dynamics {qk}K k=1, - vector of weights w(k) • Select action as follows - For each action a ∈ A predict the future state distribution ¯pa t+1 = Ta ∗ pt; - For each action, compute Da = H(¯pa t+1, pt, PDt) - For each a ∈ A and qk tactical target, denote V (a, k) = DKL (Da qk) pt . Let Vk(a) = 1 Zk V (a, k), where Zk = a∈A V (a, k) is a normalization factor. - Select a∗ = arg min a k k=1 w(k)Vk(a) The weights vector w = (w1, ..., wK ) allows the additional tuning of importance among target dynamics without the need to redesign the targets themselves. This balancing method is also seamlessly integrated into the EMT-based control flow of operation. 4.2 EMT-based Control Limitations EMT-based control is a sub-optimal (in the DBC sense) representative of the DBC structure. It limits the User by forcing EMT to be its dynamic tracking algorithm, and replaces Agent optimization by greedy action selection. This kind of combination, however, is common for on-line algorithms. Although further development of EMT-based controllers is necessary, evidence so far suggests that even the simplest form of the algorithm possesses a great deal of power, and displays trends that are optimal in the DBC sense of the word. There are two further, EMT-specific, limitations to EMT-based control that are evident at this point. Both already have partial solutions and are subjects of ongoing research. The first limitation is the problem of negative preference. In the POMDP framework for example, this is captured simply, through The Sixth Intl. Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) 793 the appearance of values with different signs within the reward structure. For EMT-based control, however, negative preference means that one would like to avoid a certain distribution over system development sequences; EMT-based control, however, concentrates on getting as close as possible to a distribution. Avoidance is thus unnatural in native EMT-based control. The second limitation comes from the fact that standard environment modeling can create pure sensory actions-actions that do not change the state of the world, and differ only in the way observations are received and the quality of observations received. Since the world state does not change, EMT-based control would not be able to differentiate between different sensory actions. Notice that both of these limitations of EMT-based control are absent from the general DBC framework, since it may have a tracking algorithm capable of considering pure sensory actions and, unlike Kullback-Leibler divergence, a distribution deviation measure that is capable of dealing with negative preference. 5. EMT PLAYING TAG The Game of Tag was first introduced in [11]. It is a single agent problem of capturing a quarry, and belongs to the class of area sweeping problems. An example domain is shown in Figure 2. 0 51 2 3 4 6 7 8 10 12 13 161514 17 18 19 2221 23 9 11Q A 20 Figure 2: Tag domain; an agent (A) attempts to seek and capture a quarry (Q) The Game of Tag extremely limits the agents perception, so that the agent is able to detect the quarry only if they are co-located in the same cell of the grid world. In the classical version of the game, co-location leads to a special observation, and the Tag action can be performed. We slightly modify this setting: the moment that both agents occupy the same cell, the game ends. As a result, both the agent and its quarry have the same motion capability, which allows them to move in four directions, North, South, East, and West. These form a formal space of actions within a Markovian environment. The state space of the formal Markovian environment is described by the cross-product of the agent and quarrys positions. For Figure 2, it would be S = {s0, ..., s23} × {s0, ..., s23}. The effects of an action taken by the agent are deterministic, but the environment in general has a stochastic response due to the motion of the quarry. With probability q0 1 it stays put, and with probability 1 − q0 it moves to an adjacent cell further away from the 1 In our experiments this was taken to be q0 = 0.2. agent. So for the instance shown in Figure 2 and q0 = 0.1: P(Q = s9|Q = s9, A = s11) = 0.1 P(Q = s2|Q = s9, A = s11) = 0.3 P(Q = s8|Q = s9, A = s11) = 0.3 P(Q = s14|Q = s9, A = s11) = 0.3 Although the evasive behavior of the quarry is known to the agent, the quarrys position is not. The only sensory information available to the agent is its own location. We use EMT and directly specify the target dynamics. For the Game of Tag, we can easily formulate three major trends: catching the quarry, staying mobile, and stalking the quarry. This results in the following three target dynamics: Tcatch(At+1 = si|Qt = sj, At = sa) ∝ 1 si = sj 0 otherwise Tmobile(At+1 = si|Qt = so, At = sj) ∝ 0 si = sj 1 otherwise Tstalk(At+1 = si|Qt = so, At = sj) ∝ 1 dist(si,so) Note that none of the above targets are directly achievable; for instance, if Qt = s9 and At = s11, there is no action that can move the agent to At+1 = s9 as required by the Tcatch target dynamics. We ran several experiments to evaluate EMT performance in the Tag Game. Three configurations of the domain shown in Figure 3 were used, each posing a different challenge to the agent due to partial observability. In each setting, a set of 1000 runs was performed with a time limit of 100 steps. In every run, the initial position of both the agent and its quarry was selected at random; this means that as far as the agent was concerned, the quarrys initial position was uniformly distributed over the entire domain cell space. We also used two variations of the environment observability function. In the first version, observability function mapped all joint positions of hunter and quarry into the position of the hunter as an observation. In the second, only those joint positions in which hunter and quarry occupied different locations were mapped into the hunters location. The second version in fact utilized and expressed the fact that once hunter and quarry occupy the same cell the game ends. The results of these experiments are shown in Table 2. Balancing [15] the catch, move, and stalk target dynamics described in the previous section by the weight vector [0.8, 0.1, 0.1], EMT produced stable performance in all three domains. Although direct comparisons are difficult to make, the EMT performance displayed notable efficiency vis-`a-vis the POMDP approach. In spite of a simple and inefficient Matlab implementation of the EMT algorithm, the decision time for any given step averaged significantly below 1 second in all experiments. For the irregular open arena domain, which proved to be the most difficult, 1000 experiment runs bounded by 100 steps each, a total of 42411 steps, were completed in slightly under 6 hours. That is, over 4 × 104 online steps took an order of magnitude less time than the offline computation of POMDP policy in [11]. The significance of this differential is made even more prominent by the fact that, should the environment model parameters change, the online nature of EMT would allow it to maintain its performance time, while the POMDP policy would need to be recomputed, requiring yet again a large overhead of computation. We also tested the behavior cell frequency entropy, empirical measures from trial data. As Figure 4 and Figure 5 show, empir794 The Sixth Intl. Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) A Q Q A 0 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 A Q Figure 3: These configurations of the Tag Game space were used: a) multiple dead-end, b) irregular open arena, c) circular corridor Table 2: Performance of the EMT-based solution in three Tag Game domains and two observability models: I) omniposition quarry, II) quarry is not at hunters position Model Domain Capture% E(Steps) Time/Step I Dead-ends 100 14.8 72(mSec) Arena 80.2 42.4 500(mSec) Circle 91.4 34.6 187(mSec) II Dead-ends 100 13.2 91(mSec) Arena 96.8 28.67 396(mSec) Circle 94.4 31.63 204(mSec) ical entropy grows with the length of interaction. For runs where the quarry was not captured immediately, the entropy reaches between 0.85 and 0.952 for different runs and scenarios. As the agent actively seeks the quarry, the entropy never reaches its maximum. One characteristic of the entropy graph for the open arena scenario particularly caught our attention in the case of the omniposition quarry observation model. Near the maximum limit of trial length (100 steps), entropy suddenly dropped. Further analysis of the data showed that under certain circumstances, a fluctuating behavior occurs in which the agent faces equally viable versions of quarry-following behavior. Since the EMT algorithm has greedy action selection, and the state space does not encode any form of commitment (not even speed or acceleration), the agent is locked within a small portion of cells. It is essentially attempting to simultaneously follow several courses of action, all of which are consistent with the target dynamics. This behavior did not occur in our second observation model, since it significantly reduced the set of eligible courses of action-essentially contributing to tie-breaking among them. 6. DISCUSSION The design of the EMT solution for the Tag Game exposes the core difference in approach to planning and control between EMT or DBC, on the one hand, and the more familiar POMDP approach, on the other. POMDP defines a reward structure to optimize, and influences system dynamics indirectly through that optimization. EMT discards any reward scheme, and instead measures and influences system dynamics directly. 2 Entropy was calculated using log base equal to the number of possible locations within the domain; this properly scales entropy expression into the range [0, 1] for all domains. Thus for the Tag Game, we did not search for a reward function that would encode and express our preference over the agents behavior, but rather directly set three (heuristic) behavior preferences as the basis for target dynamics to be maintained. Experimental data shows that these targets need not be directly achievable via the agents actions. However, the ratio between EMT performance and achievability of target dynamics remains to be explored. The tag game experiment data also revealed the different emphasis DBC and POMDPs place on the formulation of an environment state space. POMDPs rely entirely on the mechanism of reward accumulation maximization, i.e., formation of the action selection procedure to achieve necessary state sequencing. DBC, on the other hand, has two sources of sequencing specification: through the properties of an action selection procedure, and through direct specification within the target dynamics. The importance of the second source was underlined by the Tag Game experiment data, in which the greedy EMT algorithm, applied to a POMDP-type state space specification, failed, since target description over such a state space was incapable of encoding the necessary behavior tendencies, e.g., tie-breaking and commitment to directed motion. The structural differences between DBC (and EMT in particular), and POMDPs, prohibits direct performance comparison, and places them on complementary tracks, each within a suitable niche. For instance, POMDPs could be seen as a much more natural formulation of economic sequential decision-making problems, while EMT is a better fit for continual demand for stochastic change, as happens in many robotic or embodied-agent problems. The complementary properties of POMDPs and EMT can be further exploited. There is recent interest in using POMDPs in hybrid solutions [17], in which the POMDPs can be used together with other control approaches to provide results not easily achievable with either approach by itself. DBC can be an effective partner in such a hybrid solution. For instance, POMDPs have prohibitively large off-line time requirements for policy computation, but can be readily used in simpler settings to expose beneficial behavioral trends; this can serve as a form of target dynamics that are provided to EMT in a larger domain for on-line operation. 7. CONCLUSIONS AND FUTURE WORK In this paper, we have presented a novel perspective on the process of planning and control in stochastic environments, in the form of the Dynamics Based Control (DBC) framework. DBC formulates the task of planning as support of a specified target system dynamics, which describes the necessary properties of change within the environment. Optimality of DBC plans of action are measured The Sixth Intl. Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) 795 0 20 40 60 80 100 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1 Steps Entropy Dead−ends 0 20 40 60 80 100 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1 Steps Entropy Arena 0 20 40 60 80 100 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1 Steps Entropy Circle Figure 4: Observation Model I: Omniposition quarry. Entropy development with length of Tag Game for the three experiment scenarios: a) multiple dead-end, b) irregular open arena, c) circular corridor. 0 10 20 30 40 50 60 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1 Steps Entropy Dead−ends 0 20 40 60 80 100 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1 Steps Entropy Arena 0 20 40 60 80 100 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1 Steps Entropy Circle Figure 5: Observation Model II: quarry not observed at hunters position. Entropy development with length of Tag Game for the three experiment scenarios: a) multiple dead-end, b) irregular open arena, c) circular corridor. with respect to the deviation of actual system dynamics from the target dynamics. We show that a recently developed technique of Extended Markov Tracking (EMT) [13] is an instantiation of DBC. In fact, EMT can be seen as a specific case of DBC parameterization, which employs a greedy action selection procedure. Since EMT exhibits the key features of the general DBC framework, as well as polynomial time complexity, we used the multitarget version of EMT [15] to demonstrate that the class of area sweeping problems naturally lends itself to dynamics-based descriptions, as instantiated by our experiments in the Tag Game domain. As enumerated in Section 4.2, EMT has a number of limitations, such as difficulty in dealing with negative dynamic preference. This prevents direct application of EMT to such problems as Rendezvous-Evasion Games (e.g., [6]). However, DBC in general has no such limitations, and readily enables the formulation of evasion games. In future work, we intend to proceed with the development of dynamics-based controllers for these problems. 8. ACKNOWLEDGMENT The work of the first two authors was partially supported by Israel Science Foundation grant #898/05, and the third author was partially supported by a grant from Israels Ministry of Science and Technology. 9. REFERENCES [1] R. C. Arkin. Behavior-Based Robotics. MIT Press, 1998. [2] J. A. Bilmes. A gentle tutorial of the EM algorithm and its application to parameter estimation for Gaussian mixture and Hidden Markov Models. Technical Report TR-97-021, Department of Electrical Engeineering and Computer Science, University of California at Berkeley, 1998. [3] T. M. Cover and J. A. Thomas. Elements of information theory. Wiley, 1991. [4] M. E. desJardins, E. H. Durfee, C. L. Ortiz, and M. J. Wolverton. A survey of research in distributed, continual planning. AI Magazine, 4:13-22, 1999. [5] V. R. Konda and J. N. Tsitsiklis. Actor-Critic algorithms. SIAM Journal on Control and Optimization, 42(4):1143-1166, 2003. [6] W. S. Lim. A rendezvous-evasion game on discrete locations with joint randomization. Advances in Applied Probability, 29(4):1004-1017, December 1997. [7] M. L. Littman, T. L. Dean, and L. P. Kaelbling. On the complexity of solving Markov decision problems. In Proceedings of the 11th Annual Conference on Uncertainty in Artificial Intelligence (UAI-95), pages 394-402, 1995. [8] O. Madani, S. Hanks, and A. Condon. On the undecidability of probabilistic planning and related stochastic optimization problems. Artificial Intelligence Journal, 147(1-2):5-34, July 2003. [9] R. M. Neal and G. E. Hinton. A view of the EM algorithm 796 The Sixth Intl. Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) that justifies incremental, sparse, and other variants. In M. I. Jordan, editor, Learning in Graphical Models, pages 355-368. Kluwer Academic Publishers, 1998. [10] P. Paruchuri, M. Tambe, F. Ordonez, and S. Kraus. Security in multiagent systems by policy randomization. In Proceeding of AAMAS 2006, 2006. [11] J. Pineau, G. Gordon, and S. Thrun. Point-based value iteration: An anytime algorithm for pomdps. In International Joint Conference on Artificial Intelligence (IJCAI), pages 1025-1032, August 2003. [12] M. L. Puterman. Markov Decision Processes. Wiley Series in Probability and Mathematical Statistics: Applied Probability and Statistics Section. Wiley-Interscience Publication, New York, 1994. [13] Z. Rabinovich and J. S. Rosenschein. Extended Markov Tracking with an application to control. In The Workshop on Agent Tracking: Modeling Other Agents from Observations, at the Third International Joint Conference on Autonomous Agents and Multiagent Systems, pages 95-100, New-York, July 2004. [14] Z. Rabinovich and J. S. Rosenschein. Multiagent coordination by Extended Markov Tracking. In The Fourth International Joint Conference on Autonomous Agents and Multiagent Systems, pages 431-438, Utrecht, The Netherlands, July 2005. [15] Z. Rabinovich and J. S. Rosenschein. On the response of EMT-based control to interacting targets and models. In The Fifth International Joint Conference on Autonomous Agents and Multiagent Systems, pages 465-470, Hakodate, Japan, May 2006. [16] R. F. Stengel. Optimal Control and Estimation. Dover Publications, 1994. [17] M. Tambe, E. Bowring, H. Jung, G. Kaminka, R. Maheswaran, J. Marecki, J. Modi, R. Nair, J. Pearce, P. Paruchuri, D. Pynadath, P. Scerri, N. Schurr, and P. Varakantham. Conflicts in teamwork: Hybrids to the The Sixth Intl. Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) 797",
    "original_translation": "Control basado en la dinámica con una aplicación a problemas de barrido del área Zinovi Rabinovich Ingeniería y Ciencias de la Computación Universidad Hebrea de Jerusalén Jerusalén, Israel nomad@cs.huji.ac.il Jeffrey S. Rosenschein Ingeniería y ciencias de la computación Universidad Hebrea de Jerusalem Jerusalem, Israel Jeff@@@@@cs.huji.ac.il Gal A. Kaminka El Departamento de Barra de Informática del Grupo Maverick de la Universidad Ilan, israel galk@cs.biu.ac.il Resumen En este documento presentamos el control basado en la dinámica (DBC), un enfoque para la planificación ycontrol de un agente en entornos estocásticos. A diferencia de los enfoques existentes, que buscan optimizar las recompensas esperadas (por ejemplo, en problemas de decisión de Markov parcialmente observables (POMDPS)), DBC optimiza el comportamiento del sistema hacia la dinámica del sistema especificada. Mostramos que un enfoque de planificación y control recientemente desarrollado, el seguimiento extendido de Markov (EMT) es una instanciación de DBC. EMT emplea la selección de acción codiciosa para proporcionar un algoritmo de control eficiente en los entornos de Markovian. Explotamos esta eficiencia en un conjunto de experimentos que aplicaron MULTITERGET EMT a una clase de problemas de barrido del área (buscando objetivos móviles). Mostramos que tales problemas pueden definirse y resolverse de manera eficiente utilizando el marco DBC y su instanciación EMT. Categorías y descriptores de sujetos I.2.8 [Resolución de problemas, métodos de control y búsqueda]: teoría de control;I.2.9 [Robótica];I.2.11 [Inteligencia artificial distribuida]: Algoritmos de términos generales de agentes inteligentes, Teoría 1. La planificación y el control de la introducción constituye un área de investigación central en sistemas múltiples e inteligencia artificial. En los últimos años, los procesos de decisión de Markov parcialmente observables (POMDPS) [12] se han convertido en una base formal popular para la planificación en entornos estocásticos. En este marco, el problema de planificación y control a menudo se aborda imponiendo una función de recompensa y calculando una política (de elegir acciones) que sea óptima, en el sentido de que dará como resultado la utilidad más alta esperada. Aunque teóricamente atractiva, la complejidad de resolver de manera óptima un POMDP es prohibitiva [8, 7]. Tomamos una visión alternativa de la planificación en entornos estocásticos. No utilizamos una función de recompensa (basada en el estado), sino que optimizamos sobre un criterio diferente, una especificación basada en la transición de la dinámica del sistema deseada. La idea aquí es ver PlanExecution como un proceso que obliga a un sistema (estocástico) a cambiar, y un plan como un proceso dinámico que da forma a que cambia según los criterios deseados. Llamamos a este control basado en la dinámica del marco de planificación general (DBC). En DBC, el objetivo de un proceso de planificación (o control) se convierte en garantizar que el sistema cambie de acuerdo con una dinámica objetivo específica (potencialmente estocástica). Como el comportamiento real del sistema puede desviarse de lo que se especifica por la dinámica objetivo (debido a la naturaleza estocástica del sistema), la planificación en tales entornos debe ser continua [4], de manera similar a los controladores clásicos de circuito cerrado [16]. Aquí, la optimización se mide en términos de probabilidad de magnitudes de desviación. En este artículo, presentamos la estructura del control basado en la dinámica. Mostramos que el enfoque de seguimiento de Markov (EMT) extendido recientemente desarrollado [13, 14, 15] es subsumido por DBC, con EMT empleando una selección de acción codiciosa, que es una parametrización específica entre las opciones posibles dentro de DBC. EMT es una instanciación eficiente de DBC. Para evaluar DBC, llevamos a cabo un conjunto de experimentos que aplicaban EMT multi-objetivo al juego de etiqueta [11];Esta es una variante en el problema de barrido del área, donde un agente está tratando de etiquetar un objetivo móvil (cantera) cuya posición no se conoce con certeza. Los datos experimentales demuestran que incluso con un modelo simple del entorno y un diseño simple de la dinámica objetivo, se pueden producir altas tasas de éxito tanto para captar la cantera como en sorprender la cantera (como se expresa por la entropía observada de la posición de los agentes controlados). El papel está organizado de la siguiente manera. En la Sección 2 motivamos DBC utilizando problemas de barrido de área y discutimos el trabajo relacionado. La Sección 3 introduce la estructura de control basado en la dinámica (DBC) y su especialización a los entornos de Markovian. Esto es seguido por una revisión del enfoque de seguimiento de Markov (EMT) extendido como un régimen de control estructurado con DBC en la Sección 4. Esa sección también analiza las limitaciones del control basado en EMT en relación con el marco DBC general. Los ajustes y resultados experimentales se presentan luego en la Sección 5. La Sección 6 proporciona una breve discusión sobre el enfoque general, y la Sección 7 ofrece algunos comentarios e instrucciones finales para el trabajo futuro.790 978-81-904262-7-5 (RPS) C 2007 Ifaamas 2. Motivación y trabajo relacionado Muchos escenarios de la vida real, naturalmente, tienen una especificación de dinámica objetivo estocástica, especialmente aquellos dominios en los que no existe un objetivo final, sino un comportamiento del sistema (con propiedades específicas) que deben ser compatibles continuamente. Por ejemplo, los guardias de seguridad realizan barridos persistentes de un área para detectar cualquier signo de intrusión. Los ladrones astutos intentarán rastrear estos barridos, y cronometrarán su funcionamiento a los puntos clave del movimiento de los guardias. Por lo tanto, es aconsejable hacer que la dinámica de movimiento de los guardias parezca irregular y aleatoria. Trabajo reciente de Paruchuri et al.[10] ha abordado dicha aleatorización en el contexto de POMDP de agente único y distribuido. El objetivo en ese trabajo era generar políticas que proporcionen una medida de la aleatorización de la selección de la acción, al tiempo que mantiene las recompensas dentro de algunos niveles aceptables. Nuestro enfoque difiere de este trabajo en que DBC no optimiza las recompensas esperadas, de hecho, no consideramos las recompensas en todo, pero mantiene la dinámica deseada (incluida, entre otros, la aleatorización). El juego de la etiqueta es otro ejemplo de la aplicabilidad del enfoque. Fue introducido en el trabajo de Pineau et al.[11]. Hay dos agentes que pueden moverse alrededor de un área, que se divide en una cuadrícula. La cuadrícula puede haber bloqueado las células (agujeros) en las que ningún agente puede moverse. Un agente (el cazador) busca mudarse a una celda ocupada por la otra (la cantera), de modo que se ubican co-ubicados (esta es una etiqueta exitosa). La cantera busca evitar al agente del cazador, y siempre es consciente de la posición de los cazadores, pero no sabe cómo se comportará el cazador, lo que abre la posibilidad de que un cazador sorprenda a la presa. El cazador conoce la ley de movimiento probabilísticas de Quarrys, pero no conoce su ubicación actual. La etiqueta es una instancia de una familia de problemas de barrido del área (evasión de búsqueda). En [11], el cazador modeló el problema usando un POMDP. Se definió una función de recompensa para reflejar el deseo de etiquetar la cantera, y se calculó una política de acción para optimizar la recompensa recopilada con el tiempo. Debido a la complejidad intratable de determinar la política óptima, la política de acción calculada en ese documento fue esencialmente una aproximación. En este artículo, en lugar de formular una función de recompensa, usamos EMT para resolver el problema, especificando directamente la dinámica de destino. De hecho, cualquier problema de búsqueda con movimiento aleatorizado, la clase de problemas de barrido de área, se puede describir mediante la especificación de dicha dinámica del sistema objetivo. El control basado en la dinámica proporciona un enfoque natural para resolver estos problemas.3. Control basado en la dinámica La especificación del control basado en la dinámica (DBC) se puede dividir en tres niveles de interacción: nivel de diseño del entorno, nivel de usuario y nivel de agente.• El nivel de diseño del medio ambiente se refiere a la especificación formal y el modelado del entorno. Por ejemplo, este nivel especificaría las leyes de la física dentro del sistema y establecería sus parámetros, como la constante de gravitación.• El nivel de usuario a su vez se basa en el modelo de entorno producido por el diseño del entorno para especificar la dinámica del sistema de destino que desea observar. El nivel de usuario también especifica la estimación o procedimiento de aprendizaje para la dinámica del sistema y la medida de la desviación. En el escenario de la Guardia del Museo anterior, estos corresponderían a un horario estocástico de barrido, y una medida de sorpresa relativa entre el barrido especificado y real.• El nivel de agente a su vez combina el modelo de entorno desde el nivel de diseño del entorno, el procedimiento de estimación de la dinámica, la medida de desviación y la especificación de dinámica de destino desde el nivel del usuario, para producir una secuencia de acciones que crean dinámica del sistema lo más cerca posible de la especificación específica. Como estamos interesados en el desarrollo continuo de un sistema estocástico, como sucede en la teoría del control clásico [16] y la planificación continua [4], así como en nuestro ejemplo de barridos de museos, la pregunta se convierte en cómo el nivel de agente es tratarLas mediciones de desviación con el tiempo. Con este fin, utilizamos un umbral de probabilidad, es decir, nos gustaría que el nivel de agente maximice la probabilidad de que la medida de desviación permanezca por debajo de un cierto umbral. La selección de acción específica depende de la formalización del sistema. Una posibilidad sería crear una mezcla de tendencias del sistema disponibles, muy parecido a la que ocurre en las arquitecturas robóticas basadas en el comportamiento [1]. La otra alternativa sería confiar en el procedimiento de estimación proporcionado por el nivel del usuario para utilizar el modelo de nivel de diseño del entorno del entorno para elegir acciones, a fin de manipular el estimador de dinámica para que crea que se ha logrado una cierta dinámica. Observe que esta manipulación no es directa, sino a través del entorno. Por lo tanto, para los algoritmos estimadores lo suficientemente fuertes, la manipulación exitosa significaría una simulación exitosa de la dinámica de objetivo especificada (es decir, más allá de la discernimiento a través de la entrada sensorial disponible). Los niveles de DBC también pueden tener un flujo posterior de información (ver Figura 1). Por ejemplo, el nivel de agente podría proporcionar datos sobre la viabilidad de la dinámica de destino, lo que permite que el nivel del usuario modifique el requisito, tal vez centrándose en las características alcanzables del comportamiento del sistema. Los datos también estarían disponibles sobre la respuesta del sistema a diferentes acciones realizadas;Combinado con un estimador de dinámica definido por el nivel de usuario, esto puede proporcionar una herramienta importante para la calibración del modelo de entorno a nivel de diseño del entorno. Userenv. Modelo de diseño Modelo Ideal Dinámica Estimador Estimador Dinámica de viabilidad Sistema de respuesta Datos de respuesta Figura 1: Flujo de datos del marco DBC que se extiende sobre la idea de los algoritmos críticos de actores [5], el flujo de datos de DBC puede proporcionar una buena base para el diseño de un algoritmo de aprendizaje. Por ejemplo, el nivel de usuario puede funcionar como un dispositivo exploratorio para un algoritmo de aprendizaje, inferiendo un objetivo dinámico ideal del modelo de entorno en cuestión que expondría y verificaría la mayoría de las características críticas del comportamiento del sistema. En este caso, los datos de viabilidad y respuesta al sistema desde el nivel de agente proporcionarían información clave para una actualización del modelo de entorno. De hecho, la combinación de datos de viabilidad y respuesta puede proporcionar una base para la aplicación de algoritmos de aprendizaje fuertes como EM [2, 9].3.1 DBC Para entornos de Markovian para un entorno de Markovian parcialmente observable, DBC se puede especificar de una manera más rigurosa. Observe cómo DBC descarta las recompensas y la reemplaza por otro criterio de optimización (las diferencias estructurales se resumen en la Tabla 1): • El nivel de diseño del entorno es especificar una tuple <s, a, t, o, Ω, s0>, donde: - ses el conjunto de todos los estados de entorno posibles;- S0 es el estado inicial del medio ambiente (que también se puede ver como una distribución de probabilidad sobre S);El sexto intl. Conf.en agentes autónomos y sistemas de múltiples agentes (AAMAS 07) 791 - A es el conjunto de todas las acciones posibles aplicables en el entorno;- T es la función de transición probabilística de los entornos: T: S × A → π (s). Es decir, t (s | a, s) es la probabilidad de que el entorno se mueva de los estados a los estados bajo acción a;- O es el conjunto de todas las observaciones posibles. Así es como se vería la entrada del sensor para un observador externo;- ω es la función de probabilidad de observación: Ω: S × A × S → π (O). Es decir, ω (o | s, a, s) es la probabilidad de que uno observe o dado que el entorno se ha movido de los estados a los estados bajo acción a.• El nivel de usuario, en el caso del entorno de Markovian, funciona en el conjunto de dinámica del sistema descrita por una familia de probabilidades condicionales F = {τ: S × A → π (S)}. Por lo tanto, la especificación de la dinámica objetivo puede expresarse mediante q ∈ F, y el algoritmo de aprendizaje o seguimiento puede representarse en función L: O × (A × O) ∗ → F;Es decir, mapea secuencias de observaciones y acciones realizadas hasta ahora en una estimación τ ∈ F de la dinámica del sistema. Hay muchas variaciones posibles disponibles a nivel de usuario para definir la divergencia entre la dinámica del sistema;Varios de ellos son: - distancia de traza o distancia de L1 entre dos distribuciones P y Q definidas por d (p (·), q (·)) = 1 2 x | p (x) - q (x) |- Medida de fidelidad de la distancia f (p (·), q (·)) = x p (x) q (x) - kullback -leíbler divergencia dkl (p (·) q (·)) = x p (x) log p (x) Q (x) Observe que los dos últimos no son realmente métricas sobre el espacio de posibles distribuciones, sino que tienen interpretaciones significativas e importantes. Por ejemplo, la divergencia de Kullbackleibler es una herramienta importante de la teoría de la información [3] que le permite a uno medir el precio de codificar una fuente de información regida por Q, al tiempo que supone que se rige por p.El nivel de usuario también define el umbral de la probabilidad de desviación dinámica θ.• El nivel de agente se enfrenta con un problema de seleccionar una función de señal de control A ∗ para satisfacer un problema de minimización de la siguiente manera: a ∗ = arg min a pr (d (τa, q)> θ) donde d (τa, q) esUna variable aleatoria que describe la desviación de la estimación de la dinámica τa, creada por L bajo la señal de control A, a partir de la dinámica ideal q. Implícito en este problema de minimización es que L se manipula a través del entorno, basado en el modelo de entorno producido por el nivel de diseño del medio ambiente.3.2 Vista DBC del espacio de estado Es importante tener en cuenta la vista complementaria que DBC y POMDPS toman el espacio estatal del medio ambiente. Los POMDP consideran el estado estacionario como un instantáneo del entorno;Cualesquiera que sean los atributos de la secuencia de estado que se busca a través de propiedades del proceso de control, en este caso la acumulación de recompensas. Esto puede verse como si la secuenciación de los estados y los atributos de esa secuencia solo se introdujeran por y para el mecanismo de control, la política POMDP. DBC se concentra en el principio subyacente de la secuenciación estatal, la dinámica del sistema. La especificación de la dinámica del objetivo de DBCS puede usar el espacio de estado de los entornos como un medio para describir, discernir y preservar los cambios que ocurren dentro del sistema. Como resultado, DBC tiene una mayor capacidad para expresar propiedades de secuenciación de estado, que se basan en el modelo de entorno y su definición de espacio de estado. Por ejemplo, considere la tarea de moverse a través de un terreno aproximado hacia un gol y alcanzarlo lo más rápido posible. POMDPS codificaría el terreno como puntos espaciales estatales, mientras que la velocidad se garantizaría por una recompensa negativa por cada paso dado sin alcanzar el portería, la recompensa más alta se puede alcanzar solo por un movimiento más rápido. Alternativamente, el espacio de estado podría incluir directamente la noción de velocidad. Para los POMDP, esto significaría que el mismo concepto está codificado dos veces, en cierto sentido: directamente en el espacio de estado, e indirectamente dentro de la acumulación de recompensas. Ahora, incluso si la función de recompensa codificaría más, y más finos detalles de las propiedades del movimiento, la solución POMDP tendrá que buscar en un espacio de políticas mucho más grande, al tiempo que se guía por el concepto implícito del procedimiento de acumulación de recompensas. Por otro lado, la expresión táctica objetivo de variaciones y correlaciones entre la posición y la velocidad de movimiento ahora se basa en la representación del espacio de estado. En esta situación, cualquier restricción adicional, por ejemplo, suavidad del movimiento, límites de velocidad en diferentes ubicaciones o reducciones de velocidad durante giros agudos, se expresan explícita y uniformemente por el objetivo táctico, y pueden dar lugar a una selección de acción más rápida y efectiva por un DBCalgoritmo.4. Control basado en EMT Como DBC recientemente, se introdujo un algoritmo de control llamado control basado en EMT [13], que instancias el marco DBC. Aunque proporciona una solución codiciosa aproximada en el sentido de DBC, los experimentos iniciales que utilizan el control basado en EMT han sido alentadores [14, 15]. El control basado en EMT se basa en la definición del entorno de Markovian, como en el caso de POMDPS, pero sus niveles de usuario y agente son del tipo de optimización DBC Markovian.• El nivel de usuario del control basado en EMT define una dinámica del sistema de destino de caso limitado independientemente de la acción: QEMT: S → π (s). Luego utiliza la medida de divergencia Kullback-Leibler para componer un estimador de dinámica de sistema momentáneo: el algoritmo de seguimiento de Markov (EMT) extendido. El algoritmo mantiene una dinámica del sistema estimado τt EMT que es capaz de explicar un cambio reciente en un estimador de estado del sistema bayesiano auxiliar de PT-1 a PT, y lo actualiza de manera conservadora utilizando la divergencia de Kullback-Leíler. Dado que τt emt y pt - 1, t son respectivamente las probabilidades condicionales y marginales sobre el espacio de estado de los sistemas, la explicación simplemente significa que Pt (s) = s τt emt (s | s) pt - 1 (s) y la estimación de la dinámica estimadaLa actualización se realiza resolviendo un 792 el sexto intl. Conf.en agentes autónomos y sistemas de múltiples agentes (AAMAS 07) Tabla 1: Estructura de POMDP versus control basado en la dinámica en el enfoque de nivel de entorno de Markovian MDP Markovian DBC Environment <S, A, T, O, Ω>, donde s-conjunto de un conjunto de conjuntos deEstados A - Conjunto de Acciones Diseño T: S × A → π (S) - Transición O - Conjunto de observación ω: S × A × S → π (O) Usuario R: S × A × S → R Q: S × A→ π (s) f (π ∗) → r l (o1, ..., ot) → τ r - función de recompensa q - dinámica ideal f - remodelación de recompensa l - estimador de dinámica θ - agente de umbral π ∗ = arg max π π πE [γt RT] π ∗ = arg min π prob (d (τ q)> θ) Problema de minimización: τt emt = h [pt, pt - 1, τt - 1 emt] = arg min τ dkl (τ × pt−1 τt - 1 EMT × PT - 1) S.T.pt (s) = S (τ × pt-1) (s, s) pt-1 (s) = s (τ × pt-1) (s, s) • El nivel de agente en el control basado en EMT es subóptimo con respetoPara DBC (aunque permanece dentro del marco DBC), realizando una selección de acción codiciosa basada en la predicción de la reacción de EMTS. La predicción se basa en el modelo de entorno proporcionado por el nivel de diseño del entorno, de modo que si denotamos por TA la función de transición de los entornos limitado a la acción A, y PT-1 es el estimador de estado del sistema bayesiano auxiliar, entonces la elección de control basada en EMTse describe por A ∗ = arg min a∈A dkl (h [ta × pt, pt, τt emt] qemt × pt - 1) Tenga en cuenta que esto sigue el marco de DBC de Markovian con precisión: la optimización gratificante de POMDPS se descarta, y enSu lugar donde un estimador de dinámica (EMT en este caso) se manipula a través de los efectos de acción en el entorno para producir una estimación cercana a la dinámica del sistema objetivo especificado. Sin embargo, como mencionamos, el control ingenuo basado en EMT es subóptimo en el sentido de DBC, y tiene varias limitaciones adicionales que no existen en el marco DBC general (discutido en la Sección 4.2).4.1 EMT multi-objetivo A veces, puede existir varias preferencias de comportamiento. Por ejemplo, en el caso de los guardias de museos, algunos artículos de arte están más protegidos, lo que requiere que los guardias permanezcan más a menudo en sus cercanías. Por otro lado, no se deja sin control, lo que exige un movimiento constante. La seguridad exitosa del museo exigiría que los guardias se adhieran y equilibran a ambos comportamientos. Para el control basado en EMT, esto significaría enfrentar varios objetivos tácticos {Qk} k k = 1, y la pregunta es cómo fusionarlos y equilibrarlos. Se puede aplicar un mecanismo de equilibrio para resolver este problema. Tenga en cuenta que el control basado en EMT, al seleccionar una acción, crea un vector de preferencia sobre el conjunto de acciones basadas en su rendimiento previsto con respecto a un objetivo dado. Si estos vectores de preferencia están normalizados, se pueden combinar en una sola preferencia unificada. Esto requiere el reemplazo de la selección de acción basada en EMT estándar por el algoritmo a continuación [15]: • Dado: - Un conjunto de dinámicas de destino {Qk} k k = 1, - vector de pesos w (k) • Seleccione Acción de la siguiente manera - ParaCada acción a ∈ A predice la distribución del estado futuro ¯pa t+1 = ta ∗ pt;- Para cada acción, calcule DA = H (¯PA T+1, PT, PDT) - Para cada objetivo táctico A ∈ A y QK, denote v (a, k) = dkl (da qk) pt. Deje que Vk (a) = 1 zk v (a, k), donde zk = a∈A v (a, k) es un factor de normalización.- Seleccione a ∗ = arg min a k k = 1 w (k) vk (a) El vector de pesos w = (w1, ..., wk) permite el ajuste adicional de importancia entre la dinámica objetivo sin la necesidad de rediseñar los objetivos mismos. Este método de equilibrio también se integra perfectamente en el flujo de operación de control basado en EMT.4.2 Limitaciones de control basadas en EMT El control basado en EMT es un representante subóptimo (en el sentido de DBC) de la estructura DBC. Limita al usuario al obligar a EMT a ser su algoritmo de seguimiento dinámico y reemplaza la optimización del agente por la selección de acción codiciosa. Sin embargo, este tipo de combinación es común para los algoritmos en línea. Aunque es necesario un mayor desarrollo de controladores basados en EMT, la evidencia hasta ahora sugiere que incluso la forma más simple del algoritmo posee una gran cantidad de poder y muestra tendencias óptimas que son óptimas en el sentido de la palabra DBC. Hay dos limitaciones más específicas de EMT al control basado en EMT que son evidentes en este punto. Ambos ya tienen soluciones parciales y son temas de investigación en curso. La primera limitación es el problema de la preferencia negativa. En el marco POMDP, por ejemplo, esto se captura simplemente, a través del Sexto INTL. Conf.En agentes autónomos y sistemas de múltiples agentes (AAMAS 07) 793 la aparición de valores con diferentes signos dentro de la estructura de recompensa. Sin embargo, para el control basado en EMT, la preferencia negativa significa que a uno le gustaría evitar una cierta distribución sobre las secuencias de desarrollo del sistema;El control basado en EMT, sin embargo, se concentra en acercarse lo más posible a una distribución. La evitación no es natural en el control nativo basado en EMT. La segunda limitación proviene del hecho de que el modelado de entorno estándar puede crear acciones sensoriales puras que no cambian el estado del mundo, y difieren solo en la forma en que se reciben las observaciones y la calidad de las observaciones recibidas. Dado que el estado mundial no cambia, el control basado en EMT no podría diferenciar entre diferentes acciones sensoriales. Observe que ambas limitaciones del control basado en EMT están ausentes del marco DBC general, ya que puede tener un algoritmo de seguimiento capaz de considerar las acciones sensoriales puras y, a diferencia de la divergencia de Kullback-Leibbler, una medida de desviación de distribución que es capaz de tratar conPreferencia negativa.5. EMT jugando la etiqueta El juego de la etiqueta se introdujo por primera vez en [11]. Es un problema de agente único de capturar una cantera y pertenece a la clase de problemas de barrido de área. Se muestra un dominio de ejemplo en la Figura 2. 0 51 2 3 4 6 7 8 10 12 13 161514 17 18 19 2221 23 9 11q A 20 Figura 2: Dominio de etiqueta;Un agente (a) intenta buscar y capturar una cantera (q) El juego de la etiqueta limita la percepción de los agentes, de modo que el agente puede detectar la cantera solo si se ubica en la misma célula del mundo de la cuadrícula. En la versión clásica del juego, la ubicación conjunta conduce a una observación especial, y la acción de la etiqueta se puede realizar. Modificamos ligeramente esta configuración: el momento en que ambos agentes ocupan la misma celda, el juego termina. Como resultado, tanto el agente como su cantera tienen la misma capacidad de movimiento, lo que les permite moverse en cuatro direcciones, norte, sur, este y oeste. Estos forman un espacio formal de acciones dentro de un entorno de Markovian. El espacio estatal del entorno formal de Markovian se describe mediante el producto cruzado de las posiciones de agente y cantera. Para la Figura 2, sería S = {S0, ..., S23} × {S0, ..., S23}. Los efectos de una acción tomada por el agente son deterministas, pero el entorno en general tiene una respuesta estocástica debido al movimiento de la cantera. Con la probabilidad Q0 1, se mantiene, y con la probabilidad 1 - Q0 se mueve a una célula adyacente más lejos del 1 en nuestros experimentos, esto se consideró Q0 = 0.2.agente. Entonces, para la instancia que se muestra en la Figura 2 y Q0 = 0.1: P (Q = S9 | Q = S9, A = S11) = 0.1 P (Q = S2 | Q = S9, A = S11) = 0.3 P (Q = S8| Q = s9, a = s11) = 0.3 p (q = s14 | q = s9, a = s11) = 0.3 Aunque el comportamiento evasivo de la cantera es conocido por el agente, la posición de la cantera no lo es. La única información sensorial disponible para el agente es su propia ubicación. Usamos EMT y especificamos directamente la dinámica de destino. Para el juego de la etiqueta, podemos formular fácilmente tres tendencias principales: atrapar la cantera, permanecer móvil y acechar a la cantera. Esto da como resultado las siguientes tres dinámicas de destino: TCatch (AT+1 = Si | Qt = Sj, AT = SA) ∝ 1 Si = SJ 0 de lo contrario TMobile (AT+1 = Si | Qt = So, AT = SJ) ∝ 0Si = Sj 1 de lo contrario tstalk (a+1 = si | qt = entonces, at = sj) ∝ 1 dist (si, entonces) tenga en cuenta que ninguno de los objetivos anteriores se puede lograr directamente;Por ejemplo, si Qt = S9 y At = S11, no hay acción que pueda mover el agente a AT+1 = S9 como lo requiere la dinámica de destino TCatch. Ejecutamos varios experimentos para evaluar el rendimiento de EMT en el juego de etiquetas. Se utilizaron tres configuraciones del dominio que se muestra en la Figura 3, cada una con un desafío diferente al agente debido a la observabilidad parcial. En cada configuración, se realizó un conjunto de 1000 ejecuciones con un límite de tiempo de 100 pasos. En cada ejecución, la posición inicial del agente y su cantera se seleccionó al azar;Esto significa que en lo que respecta al agente, la posición inicial de Quarrys se distribuyó uniformemente en todo el espacio de las células de dominio. También utilizamos dos variaciones de la función de observabilidad del entorno. En la primera versión, la función de observabilidad mapeó todas las posiciones conjuntas del cazador y la cantera en la posición del cazador como observación. En el segundo, solo aquellas posiciones conjuntas en las que Hunter y Quarry ocupaban diferentes lugares se asignaron a la ubicación de los cazadores. La segunda versión, de hecho, utilizó y expresó el hecho de que una vez que Hunter y Quarry ocupan la misma celda, el juego termina. Los resultados de estos experimentos se muestran en la Tabla 2. Equilibrio [15] La dinámica objetivo de captura, movimiento y acechador descrita en la sección anterior por el vector de peso [0.8, 0.1, 0.1], EMT produjo un rendimiento estable en los tres dominios. Aunque las comparaciones directas son difíciles de hacer, el rendimiento de EMT mostró una eficiencia notable frente al enfoque POMDP. A pesar de una implementación de MATLAB simple e ineficiente del algoritmo EMT, el tiempo de decisión para cualquier paso dado promedió significativamente por debajo de 1 segundo en todos los experimentos. Para el dominio de la arena abierta irregular, que resultó ser el más difícil, 1000 experimentos se limitan a 100 pasos cada uno, un total de 42411 pasos, se completaron en un poco menos de 6 horas. Es decir, más de 4 × 104 pasos en línea tomaron un orden de magnitud menos tiempo que el cálculo fuera de línea de la política POMDP en [11]. La importancia de este diferencial se hace aún más prominente por el hecho de que, si los parámetros del modelo de entorno cambian, la naturaleza en línea de EMT le permitiría mantener su tiempo de rendimiento, mientras que la política de POMDP necesitaría ser recomputada, requiriendo una vez más unGran sobrecarga de cálculo. También probamos la entropía de frecuencia de las células de comportamiento, las medidas empíricas de los datos de prueba. Como muestran la Figura 4 y la Figura 5, Empir794 el sexto intl. Conf.en agentes autónomos y sistemas de múltiples agentes (AAMAS 07) A Q Q A 0 1 2 3 4 5 6 7 7 8 9 10 11 12 13 14 15 16 17 18 A Q Figura 3: Estas configuraciones del espacio de juego de etiqueta se usaron: a) Múltiples muertos-d, b) Arena abierta irregular, c) Corredor circular Tabla 2: rendimiento de la solución basada en EMT en tres dominios de juego de etiquetas y dos modelos de observabilidad: i) cantera de omniposición, ii) Quarry no está en el modelo de captura de la posición de la posición de cazadores%E (Pasos) Tiempo/Paso I Dead-Endes 100 14.8 72 (MSEC) Arena 80.2 42.4 500 (msec) Círculo 91.4 34.6 187 (msec) II II Dead-Endin 100 13.2 91 (MSEC) Arena 96.8 28.67 396 (MSEC) Círculo 94.431.63 204 (MSEC) La entropía ical crece con la duración de la interacción. Para las carreras donde la cantera no fue capturada de inmediato, la entropía alcanza entre 0.85 y 0.952 para diferentes ejecuciones y escenarios. A medida que el agente busca activamente la cantera, la entropía nunca alcanza su máximo. Una característica del gráfico de entropía para el escenario de Arena Abierta, particularmente llamó nuestra atención en el caso del modelo de observación de la cantera de omniposición. Cerca del límite máximo de la longitud de prueba (100 pasos), la entropía caída repentinamente. Un análisis posterior de los datos mostró que bajo ciertas circunstancias, se produce un comportamiento fluctuante en el que el agente enfrenta versiones igualmente viables del comportamiento de seguimiento de la cantera. Dado que el algoritmo EMT tiene una selección de acción codiciosa, y el espacio de estado no codifica ninguna forma de compromiso (ni siquiera la velocidad o la aceleración), el agente está bloqueado dentro de una pequeña porción de celdas. Esencialmente está intentando seguir simultáneamente varios cursos de acción, todos los cuales son consistentes con la dinámica objetivo. Este comportamiento no ocurrió en nuestro segundo modelo de observación, ya que redujo significativamente el conjunto de cursos de acción elegibles que contribuyen esencialmente a los lazos entre ellos.6. Discusión El diseño de la solución EMT para el juego de etiqueta expone la diferencia central en el enfoque de planificación y control entre EMT o DBC, por un lado, y el enfoque POMDP más familiar, por el otro. POMDP define una estructura de recompensa para optimizar e influye en la dinámica del sistema indirectamente a través de esa optimización. EMT descarta cualquier esquema de recompensa y, en cambio, mide e influye en la dinámica del sistema directamente.2 La entropía se calculó utilizando la base log igual al número de ubicaciones posibles dentro del dominio;Esto escala correctamente la expresión de entropía en el rango [0, 1] para todos los dominios. Por lo tanto, para el juego de etiqueta, no buscamos una función de recompensa que codifique y expresara nuestra preferencia sobre el comportamiento de los agentes, sino que estableciera directamente tres preferencias (heurísticas) de comportamiento como base para mantener la dinámica objetivo. Los datos experimentales muestran que estos objetivos no necesitan ser alcanzables directamente a través de las acciones de los agentes. Sin embargo, la relación entre el rendimiento de EMT y la capacidad de alcance de la dinámica objetivo queda por explorar. Los datos del experimento de juego de etiquetas también revelaron el diferente énfasis que DBC y POMDPS se encuentran en la formulación de un espacio de estado ambiental. Los POMDP dependen completamente del mecanismo de maximización de la acumulación de recompensas, es decir, la formación del procedimiento de selección de acción para lograr la secuenciación del estado necesaria. DBC, por otro lado, tiene dos fuentes de especificación de secuenciación: a través de las propiedades de un procedimiento de selección de acción y a través de la especificación directa dentro de la dinámica de destino. La importancia de la segunda fuente fue subrayada por los datos del experimento del juego de etiqueta, en el que el algoritmo EMT codicioso se aplicó a una especificación de espacio de estado de tipo POMDP, ya que falló, ya que la descripción del objetivo sobre dicho espacio de estado era incapaz de codificar las tendencias de comportamiento necesarias, por ejemplo, rompe la corbata y el compromiso con el movimiento dirigido. Las diferencias estructurales entre DBC (y EMT en particular), y POMDPS, prohíben la comparación de rendimiento directo y las coloca en pistas complementarias, cada una dentro de un nicho adecuado. Por ejemplo, los POMDP podrían verse como una formulación mucho más natural de los problemas de toma de decisiones secuenciales económicas, mientras que EMT es mejor para la demanda continua de cambio estocástico, como sucede en muchos problemas robóticos o de agentes incorporados. Las propiedades complementarias de POMDPS y EMT pueden explotarse aún más. Existe un interés reciente en el uso de POMDPS en soluciones híbridas [17], en el que los POMDP se pueden usar junto con otros enfoques de control para proporcionar resultados que no se pueden lograr fácilmente con ninguno de los enfoques por sí solo. DBC puede ser un socio efectivo en dicha solución híbrida. Por ejemplo, los POMDP tienen requisitos de tiempo fuera de línea prohibitivamente grandes para el cálculo de políticas, pero pueden usarse fácilmente en entornos más simples para exponer tendencias de comportamiento beneficiosas;Esto puede servir como una forma de dinámica objetivo que se proporciona a EMT en un dominio más grande para la operación en línea.7. Conclusiones y trabajo futuro En este documento, hemos presentado una perspectiva novedosa sobre el proceso de planificación y control en entornos estocásticos, en forma del marco de control basado en la dinámica (DBC). DBC formula la tarea de planificar como soporte de una dinámica del sistema objetivo especificado, que describe las propiedades necesarias del cambio dentro del entorno. La optimización de los planes de acción de DBC se miden el Sexto INTL. Conf.en agentes autónomos y sistemas de múltiples agentes (aamas 07) 795 0 20 40 60 80 100 0.2 0.3 0.4 0.5 0.6 0.7 0.7 0.9 1 1 Pasos Entrada muerta 0 20 40 60 80 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1 Pasos ContropíaArena 0 20 40 60 80 100 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1 Pasos Círculo de entropía Figura 4: Modelo de observación I: cantera de omniposición. Desarrollo de entropía con longitud del juego de etiqueta para los tres escenarios de experimentos: a) múltiples de fin de vía muerta, b) Arena abierta irregular, c) Corredor circular.0 10 20 30 40 50 60 60 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1 Pasos Controlación Dead - EDENS 0 20 40 60 80 100 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1 Pasos Arena de entropía 0 20 40 60 80 0.2 0.4 0.4 0.5 0.6 0.60.7 0.8 0.9 1 Pasos Círculo de entropía Figura 5: Modelo de observación II: Quarry no se observa en la posición de los cazadores. Desarrollo de entropía con longitud del juego de etiqueta para los tres escenarios de experimentos: a) múltiples de fin de vía muerta, b) Arena abierta irregular, c) Corredor circular.con respecto a la desviación de la dinámica del sistema real de la dinámica de destino. Mostramos que una técnica recientemente desarrollada de seguimiento extendido de Markov (EMT) [13] es una instanciación de DBC. De hecho, EMT puede verse como un caso específico de parametrización de DBC, que emplea un procedimiento de selección de acción codiciosa. Dado que EMT exhibe las características clave del marco DBC general, así como la complejidad del tiempo polinómico, utilizamos la versión multitarget de EMT [15] para demostrar que la clase de problemas de barrido de área se presta naturalmente a descripciones basadas en la dinámica, como está instanciado porNuestros experimentos en el dominio del juego de etiquetas. Como se enumeró en la Sección 4.2, EMT tiene una serie de limitaciones, como la dificultad para lidiar con la preferencia dinámica negativa. Esto evita la aplicación directa de EMT a problemas como juegos de evasión Rendezvous (por ejemplo, [6]). Sin embargo, DBC en general no tiene tales limitaciones y permite fácilmente la formulación de los juegos de evasión. En el trabajo futuro, tenemos la intención de proceder con el desarrollo de controladores basados en dinámica para estos problemas.8. Reconocimiento El trabajo de los dos primeros autores fue parcialmente apoyado por la subvención de la Fundación de Ciencias de Israel #898/05, y el tercer autor recibió parcialmente apoyado por una subvención del Ministerio de Ciencia y Tecnología de Israel.9. Referencias [1] R. C. Arkin. Robótica basada en el comportamiento. MIT Press, 1998. [2] J. A. Bilmes. Un tutorial suave del algoritmo EM y su aplicación a la estimación de parámetros para la mezcla gaussiana y los modelos ocultos de Markov. Informe técnico TR-97-021, Departamento de Ingeniería Eléctrica e Informática, Universidad de California en Berkeley, 1998. [3] T. M. Cover y J. A. Thomas. Elementos de la teoría de la información. Wiley, 1991. [4] M. E. Desjardins, E. H. Durfee, C. L. Ortiz y M. J. Wolverton. Una encuesta de investigación en planificación distribuida y continua. AI Magazine, 4: 13-22, 1999. [5] V. R. Konda y J. N. Tsitsiklis. Algoritmos de actor-crítico. SIAM Journal on Control and Optimization, 42 (4): 1143-1166, 2003. [6] W. S. Lim. Un juego de evasión de encuentro en ubicaciones discretas con aleatorización conjunta. Avances en probabilidad aplicada, 29 (4): 1004-1017, diciembre de 1997. [7] M. L. Littman, T. L. Dean y L. P. Kaelbling. Sobre la complejidad de resolver problemas de decisión de Markov. En Actas de la 11ª Conferencia Anual sobre Incertidumbre en Inteligencia Artificial (UAI-95), páginas 394-402, 1995. [8] O. Madani, S. Hanks y A. Condon. Sobre la indecidibilidad de la planificación probabilística y los problemas de optimización estocástica relacionadas. Artificial Intelligence Journal, 147 (1-2): 5-34, julio de 2003. [9] R. M. Neal y G. E. Hinton. Una vista del algoritmo EM 796 el sexto intl. Conf.En agentes autónomos y sistemas de múltiples agentes (AAMAS 07), eso justifica variantes incrementales, escasas y otras. En M. I. Jordan, editor, aprendizaje en modelos gráficos, páginas 355-368. Kluwer Academic Publishers, 1998. [10] P. Paruchuri, M. Tambe, F. Ordonez y S. Kraus. Seguridad en sistemas multiagentes por aleatorización de políticas. En el procedimiento de Aamas 2006, 2006. [11] J. Pineau, G. Gordon y S. Thrun. Iteración de valor basado en puntos: un algoritmo en cualquier momento para POMDPS. En Conferencia Conjunta Internacional sobre Inteligencia Artificial (IJCAI), páginas 1025-1032, agosto de 2003. [12] M. L. Puterman. Procesos de decisión de Markov. Serie Wiley en Probabilidad y Estadísticas Matemáticas: Sección de Probabilidad y Estadística aplicadas. Publicación de Wiley-Interscience, Nueva York, 1994. [13] Z. Rabinovich y J. S. Rosenschein. Seguimiento extendido de Markov con una aplicación para controlar. En el taller sobre el seguimiento de los agentes: modelando a otros agentes de las observaciones, en la tercera conferencia internacional conjunta sobre agentes autónomos y sistemas multiagentes, páginas 95-100, Nueva York, julio de 2004. [14] Z. Rabinovich y J. S. Rosenschein. Coordinación multiagente por seguimiento extendido de Markov. En la cuarta conferencia internacional conjunta sobre agentes autónomos y sistemas multiagentes, páginas 431-438, Utrecht, Países Bajos, julio de 2005. [15] Z. Rabinovich y J. S. Rosenschein. Sobre la respuesta del control basado en EMT a los objetivos y modelos interactuantes. En la Quinta Conferencia Internacional Conjunta sobre Agentes Autónomos y Sistemas Multiagentes, páginas 465-470, Hakodate, Japón, mayo de 2006. [16] R. F. Stengel. Control y estimación óptimos. Dover Publications, 1994. [17] M. Tambe, E. Bowring, H. Jung, G. Kaminka, R. Maheswaran, J. Marecki, J. Modi, R. Nair, J. Pearce, P. Paruchuri, D.Pynadath, P. Scerri, N. Schurr y P. Varakantham. Conflictos en el trabajo en equipo: híbridos para el sexto intl. Conf.sobre agentes autónomos y sistemas de múltiples agentes (AAMAS 07) 797",
    "original_sentences": [
        "Dynamics Based Control with an Application to Area-Sweeping Problems Zinovi Rabinovich Engineering and Computer Science Hebrew University of Jerusalem Jerusalem, Israel nomad@cs.huji.ac.il Jeffrey S. Rosenschein Engineering and Computer Science Hebrew University of Jerusalem Jerusalem, Israel jeff@cs.huji.ac.il Gal A. Kaminka The MAVERICK Group Department of Computer Science Bar Ilan University, Israel galk@cs.biu.ac.il ABSTRACT In this paper we introduce Dynamics Based Control (DBC), an approach to planning and control of an agent in stochastic environments.",
        "Unlike existing approaches, which seek to optimize expected rewards (e.g., in Partially Observable Markov Decision Problems (POMDPs)), DBC optimizes system behavior towards specified system dynamics.",
        "We show that a recently developed planning and control approach, Extended Markov Tracking (EMT) is an instantiation of DBC.",
        "EMT employs greedy action selection to provide an efficient control algorithm in Markovian environments.",
        "We exploit this efficiency in a set of experiments that applied multitarget EMT to a class of area-sweeping problems (searching for moving targets).",
        "We show that such problems can be naturally defined and efficiently solved using the DBC framework, and its EMT instantiation.",
        "Categories and Subject Descriptors I.2.8 [Problem Solving, Control Methods, and Search]: Control Theory; I.2.9 [Robotics]; I.2.11 [Distributed Artificial Intelligence]: Intelligent Agents General Terms Algorithms, Theory 1.",
        "INTRODUCTION Planning and control constitutes a central research area in multiagent systems and artificial intelligence.",
        "In recent years, Partially Observable Markov Decision Processes (POMDPs) [12] have become a popular formal basis for planning in stochastic environments.",
        "In this framework, the planning and control problem is often addressed by imposing a reward function, and computing a policy (of choosing actions) that is optimal, in the sense that it will result in the highest expected utility.",
        "While theoretically attractive, the complexity of optimally solving a POMDP is prohibitive [8, 7].",
        "We take an alternative view of planning in stochastic environments.",
        "We do not use a (state-based) reward function, but instead optimize over a different criterion, a transition-based specification of the desired system dynamics.",
        "The idea here is to view planexecution as a process that compels a (stochastic) system to change, and a plan as a dynamic process that shapes that change according to desired criteria.",
        "We call this general planning framework Dynamics Based Control (DBC).",
        "In DBC, the goal of a planning (or control) process becomes to ensure that the system will change in accordance with specific (potentially stochastic) target dynamics.",
        "As actual system behavior may deviate from that which is specified by target dynamics (due to the stochastic nature of the system), planning in such environments needs to be continual [4], in a manner similar to classical closed-loop controllers [16].",
        "Here, optimality is measured in terms of probability of deviation magnitudes.",
        "In this paper, we present the structure of Dynamics Based Control.",
        "We show that the recently developed Extended Markov Tracking (EMT) approach [13, 14, 15] is subsumed by DBC, with EMT employing greedy action selection, which is a specific parameterization among the options possible within DBC.",
        "EMT is an efficient instantiation of DBC.",
        "To evaluate DBC, we carried out a set of experiments applying multi-target EMT to the Tag Game [11]; this is a variant on the area sweeping problem, where an agent is trying to tag a moving target (quarry) whose position is not known with certainty.",
        "Experimental data demonstrates that even with a simple model of the environment and a simple design of target dynamics, high success rates can be produced both in catching the quarry, and in surprising the quarry (as expressed by the observed entropy of the controlled agents position).",
        "The paper is organized as follows.",
        "In Section 2 we motivate DBC using area-sweeping problems, and discuss related work.",
        "Section 3 introduces the Dynamics Based Control (DBC) structure, and its specialization to Markovian environments.",
        "This is followed by a review of the Extended Markov Tracking (EMT) approach as a DBC-structured control regimen in Section 4.",
        "That section also discusses the limitations of EMT-based control relative to the general DBC framework.",
        "Experimental settings and results are then presented in Section 5.",
        "Section 6 provides a short discussion of the overall approach, and Section 7 gives some concluding remarks and directions for future work. 790 978-81-904262-7-5 (RPS) c 2007 IFAAMAS 2.",
        "MOTIVATION AND RELATED WORK Many real-life scenarios naturally have a stochastic target dynamics specification, especially those domains where there exists no ultimate goal, but rather system behavior (with specific properties) that has to be continually supported.",
        "For example, security guards perform persistent sweeps of an area to detect any sign of intrusion.",
        "Cunning thieves will attempt to track these sweeps, and time their operation to key points of the guards motion.",
        "It is thus advisable to make the guards motion dynamics appear irregular and random.",
        "Recent work by Paruchuri et al. [10] has addressed such randomization in the context of single-agent and distributed POMDPs.",
        "The goal in that work was to generate policies that provide a measure of action-selection randomization, while maintaining rewards within some acceptable levels.",
        "Our focus differs from this work in that DBC does not optimize expected rewards-indeed we do not consider rewards at all-but instead maintains desired dynamics (including, but not limited to, randomization).",
        "The Game of Tag is another example of the applicability of the approach.",
        "It was introduced in the work by Pineau et al. [11].",
        "There are two agents that can move about an area, which is divided into a grid.",
        "The grid may have blocked cells (holes) into which no agent can move.",
        "One agent (the hunter) seeks to move into a cell occupied by the other (the quarry), such that they are co-located (this is a successful tag).",
        "The quarry seeks to avoid the hunter agent, and is always aware of the hunters position, but does not know how the hunter will behave, which opens up the possibility for a hunter to surprise the prey.",
        "The hunter knows the quarrys probabilistic law of motion, but does not know its current location.",
        "Tag is an instance of a family of area-sweeping (pursuit-evasion) problems.",
        "In [11], the hunter modeled the problem using a POMDP.",
        "A reward function was defined, to reflect the desire to tag the quarry, and an action policy was computed to optimize the reward collected over time.",
        "Due to the intractable complexity of determining the optimal policy, the action policy computed in that paper was essentially an approximation.",
        "In this paper, instead of formulating a reward function, we use EMT to solve the problem, by directly specifying the target dynamics.",
        "In fact, any search problem with randomized motion, the socalled class of area sweeping problems, can be described through specification of such target system dynamics.",
        "Dynamics Based Control provides a natural approach to solving these problems. 3.",
        "DYNAMICS BASED CONTROL The specification of Dynamics Based Control (DBC) can be broken into three interacting levels: Environment Design Level, User Level, and Agent Level. • Environment Design Level is concerned with the formal specification and modeling of the environment.",
        "For example, this level would specify the laws of physics within the system, and set its parameters, such as the gravitation constant. • User Level in turn relies on the environment model produced by Environment Design to specify the target system dynamics it wishes to observe.",
        "The User Level also specifies the estimation or learning procedure for system dynamics, and the measure of deviation.",
        "In the museum guard scenario above, these would correspond to a stochastic sweep schedule, and a measure of relative surprise between the specified and actual sweeping. • Agent Level in turn combines the environment model from the Environment Design level, the dynamics estimation procedure, the deviation measure and the target dynamics specification from User Level, to produce a sequence of actions that create system dynamics as close as possible to the targeted specification.",
        "As we are interested in the continual development of a stochastic system, such as happens in classical control theory [16] and continual planning [4], as well as in our example of museum sweeps, the question becomes how the Agent Level is to treat the deviation measurements over time.",
        "To this end, we use a probability threshold-that is, we would like the Agent Level to maximize the probability that the deviation measure will remain below a certain threshold.",
        "Specific action selection then depends on system formalization.",
        "One possibility would be to create a mixture of available system trends, much like that which happens in Behavior-Based Robotic architectures [1].",
        "The other alternative would be to rely on the estimation procedure provided by the User Level-to utilize the Environment Design Level model of the environment to choose actions, so as to manipulate the dynamics estimator into believing that a certain dynamics has been achieved.",
        "Notice that this manipulation is not direct, but via the environment.",
        "Thus, for strong enough estimator algorithms, successful manipulation would mean a successful simulation of the specified target dynamics (i.e., beyond discerning via the available sensory input).",
        "DBC levels can also have a back-flow of information (see Figure 1).",
        "For instance, the Agent Level could provide data about target dynamics feasibility, allowing the User Level to modify the requirement, perhaps focusing on attainable features of system behavior.",
        "Data would also be available about the system response to different actions performed; combined with a dynamics estimator defined by the User Level, this can provide an important tool for the environment model calibration at the Environment Design Level.",
        "UserEnv.",
        "Design Agent Model Ideal Dynamics Estimator Estimator Dynamics Feasibility System Response Data Figure 1: Data flow of the DBC framework Extending upon the idea of Actor-Critic algorithms [5], DBC data flow can provide a good basis for the design of a learning algorithm.",
        "For example, the User Level can operate as an exploratory device for a learning algorithm, inferring an ideal dynamics target from the environment model at hand that would expose and verify most critical features of system behavior.",
        "In this case, feasibility and system response data from the Agent Level would provide key information for an environment model update.",
        "In fact, the combination of feasibility and response data can provide a basis for the application of strong learning algorithms such as EM [2, 9]. 3.1 DBC for Markovian Environments For a Partially Observable Markovian Environment, DBC can be specified in a more rigorous manner.",
        "Notice how DBC discards rewards, and replaces it by another optimality criterion (structural differences are summarized in Table 1): • Environment Design level is to specify a tuple < S, A, T, O, Ω, s0 >, where: - S is the set of all possible environment states; - s0 is the initial state of the environment (which can also be viewed as a probability distribution over S); The Sixth Intl.",
        "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) 791 - A is the set of all possible actions applicable in the environment; - T is the environments probabilistic transition function: T : S ×A → Π(S).",
        "That is, T(s |a, s) is the probability that the environment will move from state s to state s under action a; - O is the set of all possible observations.",
        "This is what the sensor input would look like for an outside observer; - Ω is the observation probability function: Ω : S × A × S → Π(O).",
        "That is, Ω(o|s , a, s) is the probability that one will observe o given that the environment has moved from state s to state s under action a. • User Level, in the case of Markovian environment, operates on the set of system dynamics described by a family of conditional probabilities F = {τ : S × A → Π(S)}.",
        "Thus specification of target dynamics can be expressed by q ∈ F, and the learning or tracking algorithm can be represented as a function L : O×(A×O)∗ → F; that is, it maps sequences of observations and actions performed so far into an estimate τ ∈ F of system dynamics.",
        "There are many possible variations available at the User Level to define divergence between system dynamics; several of them are: - Trace distance or L1 distance between two distributions p and q defined by D(p(·), q(·)) = 1 2 x |p(x) − q(x)| - Fidelity measure of distance F(p(·), q(·)) = x p(x)q(x) - Kullback-Leibler divergence DKL(p(·) q(·)) = x p(x) log p(x) q(x) Notice that the latter two are not actually metrics over the space of possible distributions, but nevertheless have meaningful and important interpretations.",
        "For instance, KullbackLeibler divergence is an important tool of information theory [3] that allows one to measure the price of encoding an information source governed by q, while assuming that it is governed by p. The User Level also defines the threshold of dynamics deviation probability θ. • Agent Level is then faced with a problem of selecting a control signal function a∗ to satisfy a minimization problem as follows: a∗ = arg min a Pr(d(τa, q) > θ) where d(τa, q) is a random variable describing deviation of the dynamics estimate τa, created by L under control signal a, from the ideal dynamics q.",
        "Implicit in this minimization problem is that L is manipulated via the environment, based on the environment model produced by the Environment Design Level. 3.2 DBC View of the State Space It is important to note the complementary view that DBC and POMDPs take on the state space of the environment.",
        "POMDPs regard state as a stationary snap-shot of the environment; whatever attributes of state sequencing one seeks are reached through properties of the control process, in this case reward accumulation.",
        "This can be viewed as if the sequencing of states and the attributes of that sequencing are only introduced by and for the controlling mechanism-the POMDP policy.",
        "DBC concentrates on the underlying principle of state sequencing, the system dynamics.",
        "DBCs target dynamics specification can use the environments state space as a means to describe, discern, and preserve changes that occur within the system.",
        "As a result, DBC has a greater ability to express state sequencing properties, which are grounded in the environment model and its state space definition.",
        "For example, consider the task of moving through rough terrain towards a goal and reaching it as fast as possible.",
        "POMDPs would encode terrain as state space points, while speed would be ensured by negative reward for every step taken without reaching the goalaccumulating higher reward can be reached only by faster motion.",
        "Alternatively, the state space could directly include the notion of speed.",
        "For POMDPs, this would mean that the same concept is encoded twice, in some sense: directly in the state space, and indirectly within reward accumulation.",
        "Now, even if the reward function would encode more, and finer, details of the properties of motion, the POMDP solution will have to search in a much larger space of policies, while still being guided by the implicit concept of the reward accumulation procedure.",
        "On the other hand, the tactical target expression of variations and correlations between position and speed of motion is now grounded in the state space representation.",
        "In this situation, any further constraints, e.g., smoothness of motion, speed limits in different locations, or speed reductions during sharp turns, are explicitly and uniformly expressed by the tactical target, and can result in faster and more effective action selection by a DBC algorithm. 4.",
        "EMT-BASED CONTROL AS A DBC Recently, a control algorithm was introduced called EMT-based Control [13], which instantiates the DBC framework.",
        "Although it provides an approximate greedy solution in the DBC sense, initial experiments using EMT-based control have been encouraging [14, 15].",
        "EMT-based control is based on the Markovian environment definition, as in the case with POMDPs, but its User and Agent Levels are of the Markovian DBC type of optimality. • User Level of EMT-based control defines a limited-case target system dynamics independent of action: qEMT : S → Π(S).",
        "It then utilizes the Kullback-Leibler divergence measure to compose a momentary system dynamics estimator-the Extended Markov Tracking (EMT) algorithm.",
        "The algorithm keeps a system dynamics estimate τt EMT that is capable of explaining recent change in an auxiliary Bayesian system state estimator from pt−1 to pt, and updates it conservatively using Kullback-Leibler divergence.",
        "Since τt EMT and pt−1,t are respectively the conditional and marginal probabilities over the systems state space, explanation simply means that pt(s ) = s τt EMT (s |s)pt−1(s), and the dynamics estimate update is performed by solving a 792 The Sixth Intl.",
        "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) Table 1: Structure of POMDP vs. Dynamics-Based Control in Markovian Environment Level Approach MDP Markovian DBC Environment < S, A, T, O, Ω >,where S - set of states A - set of actions Design T : S × A → Π(S) - transition O - observation set Ω : S × A × S → Π(O) User r : S × A × S → R q : S × A → Π(S) F(π∗ ) → r L(o1, ..., ot) → τ r - reward function q - ideal dynamics F - reward remodeling L - dynamics estimator θ - threshold Agent π∗ = arg max π E[ γt rt] π∗ = arg min π Prob(d(τ q) > θ) minimization problem: τt EMT = H[pt, pt−1, τt−1 EMT ] = arg min τ DKL(τ × pt−1 τt−1 EMT × pt−1) s.t. pt(s ) = s (τ × pt−1)(s , s) pt−1(s) = s (τ × pt−1)(s , s) • Agent Level in EMT-based control is suboptimal with respect to DBC (though it remains within the DBC framework), performing greedy action selection based on prediction of EMTs reaction.",
        "The prediction is based on the environment model provided by the Environment Design level, so that if we denote by Ta the environments transition function limited to action a, and pt−1 is the auxiliary Bayesian system state estimator, then the EMT-based control choice is described by a∗ = arg min a∈A DKL(H[Ta × pt, pt, τt EMT ] qEMT × pt−1) Note that this follows the Markovian DBC framework precisely: the rewarding optimality of POMDPs is discarded, and in its place a dynamics estimator (EMT in this case) is manipulated via action effects on the environment to produce an estimate close to the specified target system dynamics.",
        "Yet as we mentioned, naive EMTbased control is suboptimal in the DBC sense, and has several additional limitations that do not exist in the general DBC framework (discussed in Section 4.2). 4.1 Multi-Target EMT At times, there may exist several behavioral preferences.",
        "For example, in the case of museum guards, some art items are more heavily guarded, requiring that the guards stay more often in their close vicinity.",
        "On the other hand, no corner of the museum is to be left unchecked, which demands constant motion.",
        "Successful museum security would demand that the guards adhere to, and balance, both of these behaviors.",
        "For EMT-based control, this would mean facing several tactical targets {qk}K k=1, and the question becomes how to merge and balance them.",
        "A balancing mechanism can be applied to resolve this issue.",
        "Note that EMT-based control, while selecting an action, creates a preference vector over the set of actions based on their predicted performance with respect to a given target.",
        "If these preference vectors are normalized, they can be combined into a single unified preference.",
        "This requires replacement of standard EMT-based action selection by the algorithm below [15]: • Given: - a set of target dynamics {qk}K k=1, - vector of weights w(k) • Select action as follows - For each action a ∈ A predict the future state distribution ¯pa t+1 = Ta ∗ pt; - For each action, compute Da = H(¯pa t+1, pt, PDt) - For each a ∈ A and qk tactical target, denote V (a, k) = DKL (Da qk) pt .",
        "Let Vk(a) = 1 Zk V (a, k), where Zk = a∈A V (a, k) is a normalization factor. - Select a∗ = arg min a k k=1 w(k)Vk(a) The weights vector w = (w1, ..., wK ) allows the additional tuning of importance among target dynamics without the need to redesign the targets themselves.",
        "This balancing method is also seamlessly integrated into the EMT-based control flow of operation. 4.2 EMT-based Control Limitations EMT-based control is a sub-optimal (in the DBC sense) representative of the DBC structure.",
        "It limits the User by forcing EMT to be its dynamic tracking algorithm, and replaces Agent optimization by greedy action selection.",
        "This kind of combination, however, is common for on-line algorithms.",
        "Although further development of EMT-based controllers is necessary, evidence so far suggests that even the simplest form of the algorithm possesses a great deal of power, and displays trends that are optimal in the DBC sense of the word.",
        "There are two further, EMT-specific, limitations to EMT-based control that are evident at this point.",
        "Both already have partial solutions and are subjects of ongoing research.",
        "The first limitation is the problem of negative preference.",
        "In the POMDP framework for example, this is captured simply, through The Sixth Intl.",
        "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) 793 the appearance of values with different signs within the reward structure.",
        "For EMT-based control, however, negative preference means that one would like to avoid a certain distribution over system development sequences; EMT-based control, however, concentrates on getting as close as possible to a distribution.",
        "Avoidance is thus unnatural in native EMT-based control.",
        "The second limitation comes from the fact that standard environment modeling can create pure sensory actions-actions that do not change the state of the world, and differ only in the way observations are received and the quality of observations received.",
        "Since the world state does not change, EMT-based control would not be able to differentiate between different sensory actions.",
        "Notice that both of these limitations of EMT-based control are absent from the general DBC framework, since it may have a tracking algorithm capable of considering pure sensory actions and, unlike Kullback-Leibler divergence, a distribution deviation measure that is capable of dealing with negative preference. 5.",
        "EMT PLAYING TAG The Game of Tag was first introduced in [11].",
        "It is a single agent problem of capturing a quarry, and belongs to the class of area sweeping problems.",
        "An example domain is shown in Figure 2. 0 51 2 3 4 6 7 8 10 12 13 161514 17 18 19 2221 23 9 11Q A 20 Figure 2: Tag domain; an agent (A) attempts to seek and capture a quarry (Q) The Game of Tag extremely limits the agents perception, so that the agent is able to detect the quarry only if they are co-located in the same cell of the grid world.",
        "In the classical version of the game, co-location leads to a special observation, and the Tag action can be performed.",
        "We slightly modify this setting: the moment that both agents occupy the same cell, the game ends.",
        "As a result, both the agent and its quarry have the same motion capability, which allows them to move in four directions, North, South, East, and West.",
        "These form a formal space of actions within a Markovian environment.",
        "The state space of the formal Markovian environment is described by the cross-product of the agent and quarrys positions.",
        "For Figure 2, it would be S = {s0, ..., s23} × {s0, ..., s23}.",
        "The effects of an action taken by the agent are deterministic, but the environment in general has a stochastic response due to the motion of the quarry.",
        "With probability q0 1 it stays put, and with probability 1 − q0 it moves to an adjacent cell further away from the 1 In our experiments this was taken to be q0 = 0.2. agent.",
        "So for the instance shown in Figure 2 and q0 = 0.1: P(Q = s9|Q = s9, A = s11) = 0.1 P(Q = s2|Q = s9, A = s11) = 0.3 P(Q = s8|Q = s9, A = s11) = 0.3 P(Q = s14|Q = s9, A = s11) = 0.3 Although the evasive behavior of the quarry is known to the agent, the quarrys position is not.",
        "The only sensory information available to the agent is its own location.",
        "We use EMT and directly specify the target dynamics.",
        "For the Game of Tag, we can easily formulate three major trends: catching the quarry, staying mobile, and stalking the quarry.",
        "This results in the following three target dynamics: Tcatch(At+1 = si|Qt = sj, At = sa) ∝ 1 si = sj 0 otherwise Tmobile(At+1 = si|Qt = so, At = sj) ∝ 0 si = sj 1 otherwise Tstalk(At+1 = si|Qt = so, At = sj) ∝ 1 dist(si,so) Note that none of the above targets are directly achievable; for instance, if Qt = s9 and At = s11, there is no action that can move the agent to At+1 = s9 as required by the Tcatch target dynamics.",
        "We ran several experiments to evaluate EMT performance in the Tag Game.",
        "Three configurations of the domain shown in Figure 3 were used, each posing a different challenge to the agent due to partial observability.",
        "In each setting, a set of 1000 runs was performed with a time limit of 100 steps.",
        "In every run, the initial position of both the agent and its quarry was selected at random; this means that as far as the agent was concerned, the quarrys initial position was uniformly distributed over the entire domain cell space.",
        "We also used two variations of the environment observability function.",
        "In the first version, observability function mapped all joint positions of hunter and quarry into the position of the hunter as an observation.",
        "In the second, only those joint positions in which hunter and quarry occupied different locations were mapped into the hunters location.",
        "The second version in fact utilized and expressed the fact that once hunter and quarry occupy the same cell the game ends.",
        "The results of these experiments are shown in Table 2.",
        "Balancing [15] the catch, move, and stalk target dynamics described in the previous section by the weight vector [0.8, 0.1, 0.1], EMT produced stable performance in all three domains.",
        "Although direct comparisons are difficult to make, the EMT performance displayed notable efficiency vis-`a-vis the POMDP approach.",
        "In spite of a simple and inefficient Matlab implementation of the EMT algorithm, the decision time for any given step averaged significantly below 1 second in all experiments.",
        "For the irregular open arena domain, which proved to be the most difficult, 1000 experiment runs bounded by 100 steps each, a total of 42411 steps, were completed in slightly under 6 hours.",
        "That is, over 4 × 104 online steps took an order of magnitude less time than the offline computation of POMDP policy in [11].",
        "The significance of this differential is made even more prominent by the fact that, should the environment model parameters change, the online nature of EMT would allow it to maintain its performance time, while the POMDP policy would need to be recomputed, requiring yet again a large overhead of computation.",
        "We also tested the behavior cell frequency entropy, empirical measures from trial data.",
        "As Figure 4 and Figure 5 show, empir794 The Sixth Intl.",
        "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) A Q Q A 0 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 A Q Figure 3: These configurations of the Tag Game space were used: a) multiple dead-end, b) irregular open arena, c) circular corridor Table 2: Performance of the EMT-based solution in three Tag Game domains and two observability models: I) omniposition quarry, II) quarry is not at hunters position Model Domain Capture% E(Steps) Time/Step I Dead-ends 100 14.8 72(mSec) Arena 80.2 42.4 500(mSec) Circle 91.4 34.6 187(mSec) II Dead-ends 100 13.2 91(mSec) Arena 96.8 28.67 396(mSec) Circle 94.4 31.63 204(mSec) ical entropy grows with the length of interaction.",
        "For runs where the quarry was not captured immediately, the entropy reaches between 0.85 and 0.952 for different runs and scenarios.",
        "As the agent actively seeks the quarry, the entropy never reaches its maximum.",
        "One characteristic of the entropy graph for the open arena scenario particularly caught our attention in the case of the omniposition quarry observation model.",
        "Near the maximum limit of trial length (100 steps), entropy suddenly dropped.",
        "Further analysis of the data showed that under certain circumstances, a fluctuating behavior occurs in which the agent faces equally viable versions of quarry-following behavior.",
        "Since the EMT algorithm has greedy action selection, and the state space does not encode any form of commitment (not even speed or acceleration), the agent is locked within a small portion of cells.",
        "It is essentially attempting to simultaneously follow several courses of action, all of which are consistent with the target dynamics.",
        "This behavior did not occur in our second observation model, since it significantly reduced the set of eligible courses of action-essentially contributing to tie-breaking among them. 6.",
        "DISCUSSION The design of the EMT solution for the Tag Game exposes the core difference in approach to planning and control between EMT or DBC, on the one hand, and the more familiar POMDP approach, on the other.",
        "POMDP defines a reward structure to optimize, and influences system dynamics indirectly through that optimization.",
        "EMT discards any reward scheme, and instead measures and influences system dynamics directly. 2 Entropy was calculated using log base equal to the number of possible locations within the domain; this properly scales entropy expression into the range [0, 1] for all domains.",
        "Thus for the Tag Game, we did not search for a reward function that would encode and express our preference over the agents behavior, but rather directly set three (heuristic) behavior preferences as the basis for target dynamics to be maintained.",
        "Experimental data shows that these targets need not be directly achievable via the agents actions.",
        "However, the ratio between EMT performance and achievability of target dynamics remains to be explored.",
        "The tag game experiment data also revealed the different emphasis DBC and POMDPs place on the formulation of an environment state space.",
        "POMDPs rely entirely on the mechanism of reward accumulation maximization, i.e., formation of the action selection procedure to achieve necessary state sequencing.",
        "DBC, on the other hand, has two sources of sequencing specification: through the properties of an action selection procedure, and through direct specification within the target dynamics.",
        "The importance of the second source was underlined by the Tag Game experiment data, in which the greedy EMT algorithm, applied to a POMDP-type state space specification, failed, since target description over such a state space was incapable of encoding the necessary behavior tendencies, e.g., tie-breaking and commitment to directed motion.",
        "The structural differences between DBC (and EMT in particular), and POMDPs, prohibits direct performance comparison, and places them on complementary tracks, each within a suitable niche.",
        "For instance, POMDPs could be seen as a much more natural formulation of economic sequential decision-making problems, while EMT is a better fit for continual demand for stochastic change, as happens in many robotic or embodied-agent problems.",
        "The complementary properties of POMDPs and EMT can be further exploited.",
        "There is recent interest in using POMDPs in hybrid solutions [17], in which the POMDPs can be used together with other control approaches to provide results not easily achievable with either approach by itself.",
        "DBC can be an effective partner in such a hybrid solution.",
        "For instance, POMDPs have prohibitively large off-line time requirements for policy computation, but can be readily used in simpler settings to expose beneficial behavioral trends; this can serve as a form of target dynamics that are provided to EMT in a larger domain for on-line operation. 7.",
        "CONCLUSIONS AND FUTURE WORK In this paper, we have presented a novel perspective on the process of planning and control in stochastic environments, in the form of the Dynamics Based Control (DBC) framework.",
        "DBC formulates the task of planning as support of a specified target system dynamics, which describes the necessary properties of change within the environment.",
        "Optimality of DBC plans of action are measured The Sixth Intl.",
        "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) 795 0 20 40 60 80 100 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1 Steps Entropy Dead−ends 0 20 40 60 80 100 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1 Steps Entropy Arena 0 20 40 60 80 100 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1 Steps Entropy Circle Figure 4: Observation Model I: Omniposition quarry.",
        "Entropy development with length of Tag Game for the three experiment scenarios: a) multiple dead-end, b) irregular open arena, c) circular corridor. 0 10 20 30 40 50 60 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1 Steps Entropy Dead−ends 0 20 40 60 80 100 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1 Steps Entropy Arena 0 20 40 60 80 100 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1 Steps Entropy Circle Figure 5: Observation Model II: quarry not observed at hunters position.",
        "Entropy development with length of Tag Game for the three experiment scenarios: a) multiple dead-end, b) irregular open arena, c) circular corridor. with respect to the deviation of actual system dynamics from the target dynamics.",
        "We show that a recently developed technique of Extended Markov Tracking (EMT) [13] is an instantiation of DBC.",
        "In fact, EMT can be seen as a specific case of DBC parameterization, which employs a greedy action selection procedure.",
        "Since EMT exhibits the key features of the general DBC framework, as well as polynomial time complexity, we used the multitarget version of EMT [15] to demonstrate that the class of area sweeping problems naturally lends itself to dynamics-based descriptions, as instantiated by our experiments in the Tag Game domain.",
        "As enumerated in Section 4.2, EMT has a number of limitations, such as difficulty in dealing with negative dynamic preference.",
        "This prevents direct application of EMT to such problems as Rendezvous-Evasion Games (e.g., [6]).",
        "However, DBC in general has no such limitations, and readily enables the formulation of evasion games.",
        "In future work, we intend to proceed with the development of dynamics-based controllers for these problems. 8.",
        "ACKNOWLEDGMENT The work of the first two authors was partially supported by Israel Science Foundation grant #898/05, and the third author was partially supported by a grant from Israels Ministry of Science and Technology. 9.",
        "REFERENCES [1] R. C. Arkin.",
        "Behavior-Based Robotics.",
        "MIT Press, 1998. [2] J.",
        "A. Bilmes.",
        "A gentle tutorial of the EM algorithm and its application to parameter estimation for Gaussian mixture and Hidden Markov Models.",
        "Technical Report TR-97-021, Department of Electrical Engeineering and Computer Science, University of California at Berkeley, 1998. [3] T. M. Cover and J.",
        "A. Thomas.",
        "Elements of information theory.",
        "Wiley, 1991. [4] M. E. desJardins, E. H. Durfee, C. L. Ortiz, and M. J. Wolverton.",
        "A survey of research in distributed, continual planning.",
        "AI Magazine, 4:13-22, 1999. [5] V. R. Konda and J. N. Tsitsiklis.",
        "Actor-Critic algorithms.",
        "SIAM Journal on Control and Optimization, 42(4):1143-1166, 2003. [6] W. S. Lim.",
        "A rendezvous-evasion game on discrete locations with joint randomization.",
        "Advances in Applied Probability, 29(4):1004-1017, December 1997. [7] M. L. Littman, T. L. Dean, and L. P. Kaelbling.",
        "On the complexity of solving Markov decision problems.",
        "In Proceedings of the 11th Annual Conference on Uncertainty in Artificial Intelligence (UAI-95), pages 394-402, 1995. [8] O. Madani, S. Hanks, and A. Condon.",
        "On the undecidability of probabilistic planning and related stochastic optimization problems.",
        "Artificial Intelligence Journal, 147(1-2):5-34, July 2003. [9] R. M. Neal and G. E. Hinton.",
        "A view of the EM algorithm 796 The Sixth Intl.",
        "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) that justifies incremental, sparse, and other variants.",
        "In M. I. Jordan, editor, Learning in Graphical Models, pages 355-368.",
        "Kluwer Academic Publishers, 1998. [10] P. Paruchuri, M. Tambe, F. Ordonez, and S. Kraus.",
        "Security in multiagent systems by policy randomization.",
        "In Proceeding of AAMAS 2006, 2006. [11] J. Pineau, G. Gordon, and S. Thrun.",
        "Point-based value iteration: An anytime algorithm for pomdps.",
        "In International Joint Conference on Artificial Intelligence (IJCAI), pages 1025-1032, August 2003. [12] M. L. Puterman.",
        "Markov Decision Processes.",
        "Wiley Series in Probability and Mathematical Statistics: Applied Probability and Statistics Section.",
        "Wiley-Interscience Publication, New York, 1994. [13] Z. Rabinovich and J. S. Rosenschein.",
        "Extended Markov Tracking with an application to control.",
        "In The Workshop on Agent Tracking: Modeling Other Agents from Observations, at the Third International Joint Conference on Autonomous Agents and Multiagent Systems, pages 95-100, New-York, July 2004. [14] Z. Rabinovich and J. S. Rosenschein.",
        "Multiagent coordination by Extended Markov Tracking.",
        "In The Fourth International Joint Conference on Autonomous Agents and Multiagent Systems, pages 431-438, Utrecht, The Netherlands, July 2005. [15] Z. Rabinovich and J. S. Rosenschein.",
        "On the response of EMT-based control to interacting targets and models.",
        "In The Fifth International Joint Conference on Autonomous Agents and Multiagent Systems, pages 465-470, Hakodate, Japan, May 2006. [16] R. F. Stengel.",
        "Optimal Control and Estimation.",
        "Dover Publications, 1994. [17] M. Tambe, E. Bowring, H. Jung, G. Kaminka, R. Maheswaran, J. Marecki, J. Modi, R. Nair, J. Pearce, P. Paruchuri, D. Pynadath, P. Scerri, N. Schurr, and P. Varakantham.",
        "Conflicts in teamwork: Hybrids to the The Sixth Intl.",
        "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) 797"
    ],
    "error_count": 0,
    "keys": {
        "dynamics based control": {
            "translated_key": "control basado en la dinámica",
            "is_in_text": true,
            "original_annotated_sentences": [
                "<br>dynamics based control</br> with an Application to Area-Sweeping Problems Zinovi Rabinovich Engineering and Computer Science Hebrew University of Jerusalem Jerusalem, Israel nomad@cs.huji.ac.il Jeffrey S. Rosenschein Engineering and Computer Science Hebrew University of Jerusalem Jerusalem, Israel jeff@cs.huji.ac.il Gal A. Kaminka The MAVERICK Group Department of Computer Science Bar Ilan University, Israel galk@cs.biu.ac.il ABSTRACT In this paper we introduce <br>dynamics based control</br> (DBC), an approach to planning and control of an agent in stochastic environments.",
                "Unlike existing approaches, which seek to optimize expected rewards (e.g., in Partially Observable Markov Decision Problems (POMDPs)), DBC optimizes system behavior towards specified system dynamics.",
                "We show that a recently developed planning and control approach, Extended Markov Tracking (EMT) is an instantiation of DBC.",
                "EMT employs greedy action selection to provide an efficient control algorithm in Markovian environments.",
                "We exploit this efficiency in a set of experiments that applied multitarget EMT to a class of area-sweeping problems (searching for moving targets).",
                "We show that such problems can be naturally defined and efficiently solved using the DBC framework, and its EMT instantiation.",
                "Categories and Subject Descriptors I.2.8 [Problem Solving, Control Methods, and Search]: Control Theory; I.2.9 [Robotics]; I.2.11 [Distributed Artificial Intelligence]: Intelligent Agents General Terms Algorithms, Theory 1.",
                "INTRODUCTION Planning and control constitutes a central research area in multiagent systems and artificial intelligence.",
                "In recent years, Partially Observable Markov Decision Processes (POMDPs) [12] have become a popular formal basis for planning in stochastic environments.",
                "In this framework, the planning and control problem is often addressed by imposing a reward function, and computing a policy (of choosing actions) that is optimal, in the sense that it will result in the highest expected utility.",
                "While theoretically attractive, the complexity of optimally solving a POMDP is prohibitive [8, 7].",
                "We take an alternative view of planning in stochastic environments.",
                "We do not use a (state-based) reward function, but instead optimize over a different criterion, a transition-based specification of the desired system dynamics.",
                "The idea here is to view planexecution as a process that compels a (stochastic) system to change, and a plan as a dynamic process that shapes that change according to desired criteria.",
                "We call this general planning framework <br>dynamics based control</br> (DBC).",
                "In DBC, the goal of a planning (or control) process becomes to ensure that the system will change in accordance with specific (potentially stochastic) target dynamics.",
                "As actual system behavior may deviate from that which is specified by target dynamics (due to the stochastic nature of the system), planning in such environments needs to be continual [4], in a manner similar to classical closed-loop controllers [16].",
                "Here, optimality is measured in terms of probability of deviation magnitudes.",
                "In this paper, we present the structure of <br>dynamics based control</br>.",
                "We show that the recently developed Extended Markov Tracking (EMT) approach [13, 14, 15] is subsumed by DBC, with EMT employing greedy action selection, which is a specific parameterization among the options possible within DBC.",
                "EMT is an efficient instantiation of DBC.",
                "To evaluate DBC, we carried out a set of experiments applying multi-target EMT to the Tag Game [11]; this is a variant on the area sweeping problem, where an agent is trying to tag a moving target (quarry) whose position is not known with certainty.",
                "Experimental data demonstrates that even with a simple model of the environment and a simple design of target dynamics, high success rates can be produced both in catching the quarry, and in surprising the quarry (as expressed by the observed entropy of the controlled agents position).",
                "The paper is organized as follows.",
                "In Section 2 we motivate DBC using area-sweeping problems, and discuss related work.",
                "Section 3 introduces the <br>dynamics based control</br> (DBC) structure, and its specialization to Markovian environments.",
                "This is followed by a review of the Extended Markov Tracking (EMT) approach as a DBC-structured control regimen in Section 4.",
                "That section also discusses the limitations of EMT-based control relative to the general DBC framework.",
                "Experimental settings and results are then presented in Section 5.",
                "Section 6 provides a short discussion of the overall approach, and Section 7 gives some concluding remarks and directions for future work. 790 978-81-904262-7-5 (RPS) c 2007 IFAAMAS 2.",
                "MOTIVATION AND RELATED WORK Many real-life scenarios naturally have a stochastic target dynamics specification, especially those domains where there exists no ultimate goal, but rather system behavior (with specific properties) that has to be continually supported.",
                "For example, security guards perform persistent sweeps of an area to detect any sign of intrusion.",
                "Cunning thieves will attempt to track these sweeps, and time their operation to key points of the guards motion.",
                "It is thus advisable to make the guards motion dynamics appear irregular and random.",
                "Recent work by Paruchuri et al. [10] has addressed such randomization in the context of single-agent and distributed POMDPs.",
                "The goal in that work was to generate policies that provide a measure of action-selection randomization, while maintaining rewards within some acceptable levels.",
                "Our focus differs from this work in that DBC does not optimize expected rewards-indeed we do not consider rewards at all-but instead maintains desired dynamics (including, but not limited to, randomization).",
                "The Game of Tag is another example of the applicability of the approach.",
                "It was introduced in the work by Pineau et al. [11].",
                "There are two agents that can move about an area, which is divided into a grid.",
                "The grid may have blocked cells (holes) into which no agent can move.",
                "One agent (the hunter) seeks to move into a cell occupied by the other (the quarry), such that they are co-located (this is a successful tag).",
                "The quarry seeks to avoid the hunter agent, and is always aware of the hunters position, but does not know how the hunter will behave, which opens up the possibility for a hunter to surprise the prey.",
                "The hunter knows the quarrys probabilistic law of motion, but does not know its current location.",
                "Tag is an instance of a family of area-sweeping (pursuit-evasion) problems.",
                "In [11], the hunter modeled the problem using a POMDP.",
                "A reward function was defined, to reflect the desire to tag the quarry, and an action policy was computed to optimize the reward collected over time.",
                "Due to the intractable complexity of determining the optimal policy, the action policy computed in that paper was essentially an approximation.",
                "In this paper, instead of formulating a reward function, we use EMT to solve the problem, by directly specifying the target dynamics.",
                "In fact, any search problem with randomized motion, the socalled class of area sweeping problems, can be described through specification of such target system dynamics.",
                "<br>dynamics based control</br> provides a natural approach to solving these problems. 3.",
                "<br>dynamics based control</br> The specification of <br>dynamics based control</br> (DBC) can be broken into three interacting levels: Environment Design Level, User Level, and Agent Level. • Environment Design Level is concerned with the formal specification and modeling of the environment.",
                "For example, this level would specify the laws of physics within the system, and set its parameters, such as the gravitation constant. • User Level in turn relies on the environment model produced by Environment Design to specify the target system dynamics it wishes to observe.",
                "The User Level also specifies the estimation or learning procedure for system dynamics, and the measure of deviation.",
                "In the museum guard scenario above, these would correspond to a stochastic sweep schedule, and a measure of relative surprise between the specified and actual sweeping. • Agent Level in turn combines the environment model from the Environment Design level, the dynamics estimation procedure, the deviation measure and the target dynamics specification from User Level, to produce a sequence of actions that create system dynamics as close as possible to the targeted specification.",
                "As we are interested in the continual development of a stochastic system, such as happens in classical control theory [16] and continual planning [4], as well as in our example of museum sweeps, the question becomes how the Agent Level is to treat the deviation measurements over time.",
                "To this end, we use a probability threshold-that is, we would like the Agent Level to maximize the probability that the deviation measure will remain below a certain threshold.",
                "Specific action selection then depends on system formalization.",
                "One possibility would be to create a mixture of available system trends, much like that which happens in Behavior-Based Robotic architectures [1].",
                "The other alternative would be to rely on the estimation procedure provided by the User Level-to utilize the Environment Design Level model of the environment to choose actions, so as to manipulate the dynamics estimator into believing that a certain dynamics has been achieved.",
                "Notice that this manipulation is not direct, but via the environment.",
                "Thus, for strong enough estimator algorithms, successful manipulation would mean a successful simulation of the specified target dynamics (i.e., beyond discerning via the available sensory input).",
                "DBC levels can also have a back-flow of information (see Figure 1).",
                "For instance, the Agent Level could provide data about target dynamics feasibility, allowing the User Level to modify the requirement, perhaps focusing on attainable features of system behavior.",
                "Data would also be available about the system response to different actions performed; combined with a dynamics estimator defined by the User Level, this can provide an important tool for the environment model calibration at the Environment Design Level.",
                "UserEnv.",
                "Design Agent Model Ideal Dynamics Estimator Estimator Dynamics Feasibility System Response Data Figure 1: Data flow of the DBC framework Extending upon the idea of Actor-Critic algorithms [5], DBC data flow can provide a good basis for the design of a learning algorithm.",
                "For example, the User Level can operate as an exploratory device for a learning algorithm, inferring an ideal dynamics target from the environment model at hand that would expose and verify most critical features of system behavior.",
                "In this case, feasibility and system response data from the Agent Level would provide key information for an environment model update.",
                "In fact, the combination of feasibility and response data can provide a basis for the application of strong learning algorithms such as EM [2, 9]. 3.1 DBC for Markovian Environments For a Partially Observable Markovian Environment, DBC can be specified in a more rigorous manner.",
                "Notice how DBC discards rewards, and replaces it by another optimality criterion (structural differences are summarized in Table 1): • Environment Design level is to specify a tuple < S, A, T, O, Ω, s0 >, where: - S is the set of all possible environment states; - s0 is the initial state of the environment (which can also be viewed as a probability distribution over S); The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) 791 - A is the set of all possible actions applicable in the environment; - T is the environments probabilistic transition function: T : S ×A → Π(S).",
                "That is, T(s |a, s) is the probability that the environment will move from state s to state s under action a; - O is the set of all possible observations.",
                "This is what the sensor input would look like for an outside observer; - Ω is the observation probability function: Ω : S × A × S → Π(O).",
                "That is, Ω(o|s , a, s) is the probability that one will observe o given that the environment has moved from state s to state s under action a. • User Level, in the case of Markovian environment, operates on the set of system dynamics described by a family of conditional probabilities F = {τ : S × A → Π(S)}.",
                "Thus specification of target dynamics can be expressed by q ∈ F, and the learning or tracking algorithm can be represented as a function L : O×(A×O)∗ → F; that is, it maps sequences of observations and actions performed so far into an estimate τ ∈ F of system dynamics.",
                "There are many possible variations available at the User Level to define divergence between system dynamics; several of them are: - Trace distance or L1 distance between two distributions p and q defined by D(p(·), q(·)) = 1 2 x |p(x) − q(x)| - Fidelity measure of distance F(p(·), q(·)) = x p(x)q(x) - Kullback-Leibler divergence DKL(p(·) q(·)) = x p(x) log p(x) q(x) Notice that the latter two are not actually metrics over the space of possible distributions, but nevertheless have meaningful and important interpretations.",
                "For instance, KullbackLeibler divergence is an important tool of information theory [3] that allows one to measure the price of encoding an information source governed by q, while assuming that it is governed by p. The User Level also defines the threshold of dynamics deviation probability θ. • Agent Level is then faced with a problem of selecting a control signal function a∗ to satisfy a minimization problem as follows: a∗ = arg min a Pr(d(τa, q) > θ) where d(τa, q) is a random variable describing deviation of the dynamics estimate τa, created by L under control signal a, from the ideal dynamics q.",
                "Implicit in this minimization problem is that L is manipulated via the environment, based on the environment model produced by the Environment Design Level. 3.2 DBC View of the State Space It is important to note the complementary view that DBC and POMDPs take on the state space of the environment.",
                "POMDPs regard state as a stationary snap-shot of the environment; whatever attributes of state sequencing one seeks are reached through properties of the control process, in this case reward accumulation.",
                "This can be viewed as if the sequencing of states and the attributes of that sequencing are only introduced by and for the controlling mechanism-the POMDP policy.",
                "DBC concentrates on the underlying principle of state sequencing, the system dynamics.",
                "DBCs target dynamics specification can use the environments state space as a means to describe, discern, and preserve changes that occur within the system.",
                "As a result, DBC has a greater ability to express state sequencing properties, which are grounded in the environment model and its state space definition.",
                "For example, consider the task of moving through rough terrain towards a goal and reaching it as fast as possible.",
                "POMDPs would encode terrain as state space points, while speed would be ensured by negative reward for every step taken without reaching the goalaccumulating higher reward can be reached only by faster motion.",
                "Alternatively, the state space could directly include the notion of speed.",
                "For POMDPs, this would mean that the same concept is encoded twice, in some sense: directly in the state space, and indirectly within reward accumulation.",
                "Now, even if the reward function would encode more, and finer, details of the properties of motion, the POMDP solution will have to search in a much larger space of policies, while still being guided by the implicit concept of the reward accumulation procedure.",
                "On the other hand, the tactical target expression of variations and correlations between position and speed of motion is now grounded in the state space representation.",
                "In this situation, any further constraints, e.g., smoothness of motion, speed limits in different locations, or speed reductions during sharp turns, are explicitly and uniformly expressed by the tactical target, and can result in faster and more effective action selection by a DBC algorithm. 4.",
                "EMT-BASED CONTROL AS A DBC Recently, a control algorithm was introduced called EMT-based Control [13], which instantiates the DBC framework.",
                "Although it provides an approximate greedy solution in the DBC sense, initial experiments using EMT-based control have been encouraging [14, 15].",
                "EMT-based control is based on the Markovian environment definition, as in the case with POMDPs, but its User and Agent Levels are of the Markovian DBC type of optimality. • User Level of EMT-based control defines a limited-case target system dynamics independent of action: qEMT : S → Π(S).",
                "It then utilizes the Kullback-Leibler divergence measure to compose a momentary system dynamics estimator-the Extended Markov Tracking (EMT) algorithm.",
                "The algorithm keeps a system dynamics estimate τt EMT that is capable of explaining recent change in an auxiliary Bayesian system state estimator from pt−1 to pt, and updates it conservatively using Kullback-Leibler divergence.",
                "Since τt EMT and pt−1,t are respectively the conditional and marginal probabilities over the systems state space, explanation simply means that pt(s ) = s τt EMT (s |s)pt−1(s), and the dynamics estimate update is performed by solving a 792 The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) Table 1: Structure of POMDP vs. Dynamics-Based Control in Markovian Environment Level Approach MDP Markovian DBC Environment < S, A, T, O, Ω >,where S - set of states A - set of actions Design T : S × A → Π(S) - transition O - observation set Ω : S × A × S → Π(O) User r : S × A × S → R q : S × A → Π(S) F(π∗ ) → r L(o1, ..., ot) → τ r - reward function q - ideal dynamics F - reward remodeling L - dynamics estimator θ - threshold Agent π∗ = arg max π E[ γt rt] π∗ = arg min π Prob(d(τ q) > θ) minimization problem: τt EMT = H[pt, pt−1, τt−1 EMT ] = arg min τ DKL(τ × pt−1 τt−1 EMT × pt−1) s.t. pt(s ) = s (τ × pt−1)(s , s) pt−1(s) = s (τ × pt−1)(s , s) • Agent Level in EMT-based control is suboptimal with respect to DBC (though it remains within the DBC framework), performing greedy action selection based on prediction of EMTs reaction.",
                "The prediction is based on the environment model provided by the Environment Design level, so that if we denote by Ta the environments transition function limited to action a, and pt−1 is the auxiliary Bayesian system state estimator, then the EMT-based control choice is described by a∗ = arg min a∈A DKL(H[Ta × pt, pt, τt EMT ] qEMT × pt−1) Note that this follows the Markovian DBC framework precisely: the rewarding optimality of POMDPs is discarded, and in its place a dynamics estimator (EMT in this case) is manipulated via action effects on the environment to produce an estimate close to the specified target system dynamics.",
                "Yet as we mentioned, naive EMTbased control is suboptimal in the DBC sense, and has several additional limitations that do not exist in the general DBC framework (discussed in Section 4.2). 4.1 Multi-Target EMT At times, there may exist several behavioral preferences.",
                "For example, in the case of museum guards, some art items are more heavily guarded, requiring that the guards stay more often in their close vicinity.",
                "On the other hand, no corner of the museum is to be left unchecked, which demands constant motion.",
                "Successful museum security would demand that the guards adhere to, and balance, both of these behaviors.",
                "For EMT-based control, this would mean facing several tactical targets {qk}K k=1, and the question becomes how to merge and balance them.",
                "A balancing mechanism can be applied to resolve this issue.",
                "Note that EMT-based control, while selecting an action, creates a preference vector over the set of actions based on their predicted performance with respect to a given target.",
                "If these preference vectors are normalized, they can be combined into a single unified preference.",
                "This requires replacement of standard EMT-based action selection by the algorithm below [15]: • Given: - a set of target dynamics {qk}K k=1, - vector of weights w(k) • Select action as follows - For each action a ∈ A predict the future state distribution ¯pa t+1 = Ta ∗ pt; - For each action, compute Da = H(¯pa t+1, pt, PDt) - For each a ∈ A and qk tactical target, denote V (a, k) = DKL (Da qk) pt .",
                "Let Vk(a) = 1 Zk V (a, k), where Zk = a∈A V (a, k) is a normalization factor. - Select a∗ = arg min a k k=1 w(k)Vk(a) The weights vector w = (w1, ..., wK ) allows the additional tuning of importance among target dynamics without the need to redesign the targets themselves.",
                "This balancing method is also seamlessly integrated into the EMT-based control flow of operation. 4.2 EMT-based Control Limitations EMT-based control is a sub-optimal (in the DBC sense) representative of the DBC structure.",
                "It limits the User by forcing EMT to be its dynamic tracking algorithm, and replaces Agent optimization by greedy action selection.",
                "This kind of combination, however, is common for on-line algorithms.",
                "Although further development of EMT-based controllers is necessary, evidence so far suggests that even the simplest form of the algorithm possesses a great deal of power, and displays trends that are optimal in the DBC sense of the word.",
                "There are two further, EMT-specific, limitations to EMT-based control that are evident at this point.",
                "Both already have partial solutions and are subjects of ongoing research.",
                "The first limitation is the problem of negative preference.",
                "In the POMDP framework for example, this is captured simply, through The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) 793 the appearance of values with different signs within the reward structure.",
                "For EMT-based control, however, negative preference means that one would like to avoid a certain distribution over system development sequences; EMT-based control, however, concentrates on getting as close as possible to a distribution.",
                "Avoidance is thus unnatural in native EMT-based control.",
                "The second limitation comes from the fact that standard environment modeling can create pure sensory actions-actions that do not change the state of the world, and differ only in the way observations are received and the quality of observations received.",
                "Since the world state does not change, EMT-based control would not be able to differentiate between different sensory actions.",
                "Notice that both of these limitations of EMT-based control are absent from the general DBC framework, since it may have a tracking algorithm capable of considering pure sensory actions and, unlike Kullback-Leibler divergence, a distribution deviation measure that is capable of dealing with negative preference. 5.",
                "EMT PLAYING TAG The Game of Tag was first introduced in [11].",
                "It is a single agent problem of capturing a quarry, and belongs to the class of area sweeping problems.",
                "An example domain is shown in Figure 2. 0 51 2 3 4 6 7 8 10 12 13 161514 17 18 19 2221 23 9 11Q A 20 Figure 2: Tag domain; an agent (A) attempts to seek and capture a quarry (Q) The Game of Tag extremely limits the agents perception, so that the agent is able to detect the quarry only if they are co-located in the same cell of the grid world.",
                "In the classical version of the game, co-location leads to a special observation, and the Tag action can be performed.",
                "We slightly modify this setting: the moment that both agents occupy the same cell, the game ends.",
                "As a result, both the agent and its quarry have the same motion capability, which allows them to move in four directions, North, South, East, and West.",
                "These form a formal space of actions within a Markovian environment.",
                "The state space of the formal Markovian environment is described by the cross-product of the agent and quarrys positions.",
                "For Figure 2, it would be S = {s0, ..., s23} × {s0, ..., s23}.",
                "The effects of an action taken by the agent are deterministic, but the environment in general has a stochastic response due to the motion of the quarry.",
                "With probability q0 1 it stays put, and with probability 1 − q0 it moves to an adjacent cell further away from the 1 In our experiments this was taken to be q0 = 0.2. agent.",
                "So for the instance shown in Figure 2 and q0 = 0.1: P(Q = s9|Q = s9, A = s11) = 0.1 P(Q = s2|Q = s9, A = s11) = 0.3 P(Q = s8|Q = s9, A = s11) = 0.3 P(Q = s14|Q = s9, A = s11) = 0.3 Although the evasive behavior of the quarry is known to the agent, the quarrys position is not.",
                "The only sensory information available to the agent is its own location.",
                "We use EMT and directly specify the target dynamics.",
                "For the Game of Tag, we can easily formulate three major trends: catching the quarry, staying mobile, and stalking the quarry.",
                "This results in the following three target dynamics: Tcatch(At+1 = si|Qt = sj, At = sa) ∝ 1 si = sj 0 otherwise Tmobile(At+1 = si|Qt = so, At = sj) ∝ 0 si = sj 1 otherwise Tstalk(At+1 = si|Qt = so, At = sj) ∝ 1 dist(si,so) Note that none of the above targets are directly achievable; for instance, if Qt = s9 and At = s11, there is no action that can move the agent to At+1 = s9 as required by the Tcatch target dynamics.",
                "We ran several experiments to evaluate EMT performance in the Tag Game.",
                "Three configurations of the domain shown in Figure 3 were used, each posing a different challenge to the agent due to partial observability.",
                "In each setting, a set of 1000 runs was performed with a time limit of 100 steps.",
                "In every run, the initial position of both the agent and its quarry was selected at random; this means that as far as the agent was concerned, the quarrys initial position was uniformly distributed over the entire domain cell space.",
                "We also used two variations of the environment observability function.",
                "In the first version, observability function mapped all joint positions of hunter and quarry into the position of the hunter as an observation.",
                "In the second, only those joint positions in which hunter and quarry occupied different locations were mapped into the hunters location.",
                "The second version in fact utilized and expressed the fact that once hunter and quarry occupy the same cell the game ends.",
                "The results of these experiments are shown in Table 2.",
                "Balancing [15] the catch, move, and stalk target dynamics described in the previous section by the weight vector [0.8, 0.1, 0.1], EMT produced stable performance in all three domains.",
                "Although direct comparisons are difficult to make, the EMT performance displayed notable efficiency vis-`a-vis the POMDP approach.",
                "In spite of a simple and inefficient Matlab implementation of the EMT algorithm, the decision time for any given step averaged significantly below 1 second in all experiments.",
                "For the irregular open arena domain, which proved to be the most difficult, 1000 experiment runs bounded by 100 steps each, a total of 42411 steps, were completed in slightly under 6 hours.",
                "That is, over 4 × 104 online steps took an order of magnitude less time than the offline computation of POMDP policy in [11].",
                "The significance of this differential is made even more prominent by the fact that, should the environment model parameters change, the online nature of EMT would allow it to maintain its performance time, while the POMDP policy would need to be recomputed, requiring yet again a large overhead of computation.",
                "We also tested the behavior cell frequency entropy, empirical measures from trial data.",
                "As Figure 4 and Figure 5 show, empir794 The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) A Q Q A 0 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 A Q Figure 3: These configurations of the Tag Game space were used: a) multiple dead-end, b) irregular open arena, c) circular corridor Table 2: Performance of the EMT-based solution in three Tag Game domains and two observability models: I) omniposition quarry, II) quarry is not at hunters position Model Domain Capture% E(Steps) Time/Step I Dead-ends 100 14.8 72(mSec) Arena 80.2 42.4 500(mSec) Circle 91.4 34.6 187(mSec) II Dead-ends 100 13.2 91(mSec) Arena 96.8 28.67 396(mSec) Circle 94.4 31.63 204(mSec) ical entropy grows with the length of interaction.",
                "For runs where the quarry was not captured immediately, the entropy reaches between 0.85 and 0.952 for different runs and scenarios.",
                "As the agent actively seeks the quarry, the entropy never reaches its maximum.",
                "One characteristic of the entropy graph for the open arena scenario particularly caught our attention in the case of the omniposition quarry observation model.",
                "Near the maximum limit of trial length (100 steps), entropy suddenly dropped.",
                "Further analysis of the data showed that under certain circumstances, a fluctuating behavior occurs in which the agent faces equally viable versions of quarry-following behavior.",
                "Since the EMT algorithm has greedy action selection, and the state space does not encode any form of commitment (not even speed or acceleration), the agent is locked within a small portion of cells.",
                "It is essentially attempting to simultaneously follow several courses of action, all of which are consistent with the target dynamics.",
                "This behavior did not occur in our second observation model, since it significantly reduced the set of eligible courses of action-essentially contributing to tie-breaking among them. 6.",
                "DISCUSSION The design of the EMT solution for the Tag Game exposes the core difference in approach to planning and control between EMT or DBC, on the one hand, and the more familiar POMDP approach, on the other.",
                "POMDP defines a reward structure to optimize, and influences system dynamics indirectly through that optimization.",
                "EMT discards any reward scheme, and instead measures and influences system dynamics directly. 2 Entropy was calculated using log base equal to the number of possible locations within the domain; this properly scales entropy expression into the range [0, 1] for all domains.",
                "Thus for the Tag Game, we did not search for a reward function that would encode and express our preference over the agents behavior, but rather directly set three (heuristic) behavior preferences as the basis for target dynamics to be maintained.",
                "Experimental data shows that these targets need not be directly achievable via the agents actions.",
                "However, the ratio between EMT performance and achievability of target dynamics remains to be explored.",
                "The tag game experiment data also revealed the different emphasis DBC and POMDPs place on the formulation of an environment state space.",
                "POMDPs rely entirely on the mechanism of reward accumulation maximization, i.e., formation of the action selection procedure to achieve necessary state sequencing.",
                "DBC, on the other hand, has two sources of sequencing specification: through the properties of an action selection procedure, and through direct specification within the target dynamics.",
                "The importance of the second source was underlined by the Tag Game experiment data, in which the greedy EMT algorithm, applied to a POMDP-type state space specification, failed, since target description over such a state space was incapable of encoding the necessary behavior tendencies, e.g., tie-breaking and commitment to directed motion.",
                "The structural differences between DBC (and EMT in particular), and POMDPs, prohibits direct performance comparison, and places them on complementary tracks, each within a suitable niche.",
                "For instance, POMDPs could be seen as a much more natural formulation of economic sequential decision-making problems, while EMT is a better fit for continual demand for stochastic change, as happens in many robotic or embodied-agent problems.",
                "The complementary properties of POMDPs and EMT can be further exploited.",
                "There is recent interest in using POMDPs in hybrid solutions [17], in which the POMDPs can be used together with other control approaches to provide results not easily achievable with either approach by itself.",
                "DBC can be an effective partner in such a hybrid solution.",
                "For instance, POMDPs have prohibitively large off-line time requirements for policy computation, but can be readily used in simpler settings to expose beneficial behavioral trends; this can serve as a form of target dynamics that are provided to EMT in a larger domain for on-line operation. 7.",
                "CONCLUSIONS AND FUTURE WORK In this paper, we have presented a novel perspective on the process of planning and control in stochastic environments, in the form of the <br>dynamics based control</br> (DBC) framework.",
                "DBC formulates the task of planning as support of a specified target system dynamics, which describes the necessary properties of change within the environment.",
                "Optimality of DBC plans of action are measured The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) 795 0 20 40 60 80 100 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1 Steps Entropy Dead−ends 0 20 40 60 80 100 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1 Steps Entropy Arena 0 20 40 60 80 100 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1 Steps Entropy Circle Figure 4: Observation Model I: Omniposition quarry.",
                "Entropy development with length of Tag Game for the three experiment scenarios: a) multiple dead-end, b) irregular open arena, c) circular corridor. 0 10 20 30 40 50 60 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1 Steps Entropy Dead−ends 0 20 40 60 80 100 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1 Steps Entropy Arena 0 20 40 60 80 100 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1 Steps Entropy Circle Figure 5: Observation Model II: quarry not observed at hunters position.",
                "Entropy development with length of Tag Game for the three experiment scenarios: a) multiple dead-end, b) irregular open arena, c) circular corridor. with respect to the deviation of actual system dynamics from the target dynamics.",
                "We show that a recently developed technique of Extended Markov Tracking (EMT) [13] is an instantiation of DBC.",
                "In fact, EMT can be seen as a specific case of DBC parameterization, which employs a greedy action selection procedure.",
                "Since EMT exhibits the key features of the general DBC framework, as well as polynomial time complexity, we used the multitarget version of EMT [15] to demonstrate that the class of area sweeping problems naturally lends itself to dynamics-based descriptions, as instantiated by our experiments in the Tag Game domain.",
                "As enumerated in Section 4.2, EMT has a number of limitations, such as difficulty in dealing with negative dynamic preference.",
                "This prevents direct application of EMT to such problems as Rendezvous-Evasion Games (e.g., [6]).",
                "However, DBC in general has no such limitations, and readily enables the formulation of evasion games.",
                "In future work, we intend to proceed with the development of dynamics-based controllers for these problems. 8.",
                "ACKNOWLEDGMENT The work of the first two authors was partially supported by Israel Science Foundation grant #898/05, and the third author was partially supported by a grant from Israels Ministry of Science and Technology. 9.",
                "REFERENCES [1] R. C. Arkin.",
                "Behavior-Based Robotics.",
                "MIT Press, 1998. [2] J.",
                "A. Bilmes.",
                "A gentle tutorial of the EM algorithm and its application to parameter estimation for Gaussian mixture and Hidden Markov Models.",
                "Technical Report TR-97-021, Department of Electrical Engeineering and Computer Science, University of California at Berkeley, 1998. [3] T. M. Cover and J.",
                "A. Thomas.",
                "Elements of information theory.",
                "Wiley, 1991. [4] M. E. desJardins, E. H. Durfee, C. L. Ortiz, and M. J. Wolverton.",
                "A survey of research in distributed, continual planning.",
                "AI Magazine, 4:13-22, 1999. [5] V. R. Konda and J. N. Tsitsiklis.",
                "Actor-Critic algorithms.",
                "SIAM Journal on Control and Optimization, 42(4):1143-1166, 2003. [6] W. S. Lim.",
                "A rendezvous-evasion game on discrete locations with joint randomization.",
                "Advances in Applied Probability, 29(4):1004-1017, December 1997. [7] M. L. Littman, T. L. Dean, and L. P. Kaelbling.",
                "On the complexity of solving Markov decision problems.",
                "In Proceedings of the 11th Annual Conference on Uncertainty in Artificial Intelligence (UAI-95), pages 394-402, 1995. [8] O. Madani, S. Hanks, and A. Condon.",
                "On the undecidability of probabilistic planning and related stochastic optimization problems.",
                "Artificial Intelligence Journal, 147(1-2):5-34, July 2003. [9] R. M. Neal and G. E. Hinton.",
                "A view of the EM algorithm 796 The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) that justifies incremental, sparse, and other variants.",
                "In M. I. Jordan, editor, Learning in Graphical Models, pages 355-368.",
                "Kluwer Academic Publishers, 1998. [10] P. Paruchuri, M. Tambe, F. Ordonez, and S. Kraus.",
                "Security in multiagent systems by policy randomization.",
                "In Proceeding of AAMAS 2006, 2006. [11] J. Pineau, G. Gordon, and S. Thrun.",
                "Point-based value iteration: An anytime algorithm for pomdps.",
                "In International Joint Conference on Artificial Intelligence (IJCAI), pages 1025-1032, August 2003. [12] M. L. Puterman.",
                "Markov Decision Processes.",
                "Wiley Series in Probability and Mathematical Statistics: Applied Probability and Statistics Section.",
                "Wiley-Interscience Publication, New York, 1994. [13] Z. Rabinovich and J. S. Rosenschein.",
                "Extended Markov Tracking with an application to control.",
                "In The Workshop on Agent Tracking: Modeling Other Agents from Observations, at the Third International Joint Conference on Autonomous Agents and Multiagent Systems, pages 95-100, New-York, July 2004. [14] Z. Rabinovich and J. S. Rosenschein.",
                "Multiagent coordination by Extended Markov Tracking.",
                "In The Fourth International Joint Conference on Autonomous Agents and Multiagent Systems, pages 431-438, Utrecht, The Netherlands, July 2005. [15] Z. Rabinovich and J. S. Rosenschein.",
                "On the response of EMT-based control to interacting targets and models.",
                "In The Fifth International Joint Conference on Autonomous Agents and Multiagent Systems, pages 465-470, Hakodate, Japan, May 2006. [16] R. F. Stengel.",
                "Optimal Control and Estimation.",
                "Dover Publications, 1994. [17] M. Tambe, E. Bowring, H. Jung, G. Kaminka, R. Maheswaran, J. Marecki, J. Modi, R. Nair, J. Pearce, P. Paruchuri, D. Pynadath, P. Scerri, N. Schurr, and P. Varakantham.",
                "Conflicts in teamwork: Hybrids to the The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) 797"
            ],
            "original_annotated_samples": [],
            "translated_annotated_samples": [
                "\"Control basado en la dinámica\" con una aplicación a los problemas de barrido del área Zinovi Rabinovich Ingeniería y ciencias de la computación Universidad Hebrea de Jerusalén Jerusalén, Israel nomad@cs.huji.ac.il Jeffrey S. Rosenschein Ingeniería y ciencias de la informática Universidad Hebrea de Jerusalem Jerusalem, Israel ISRAELELELELELELELELELELELELELELELEELEELEELEEL.jeff@cs.huji.ac.il Gal A. Kaminka El Departamento de Barra de Ciencias de la Computación del Grupo Maverick Ilan University, Israel galk@cs.biu.ac.il Resumen En este documento presentamos \"Control basado en la dinámica\" (DBC), unEnfoque para la planificación y el control de un agente en entornos estocásticos.",
                "Llamamos a este marco de planificación general \"control basado en la dinámica\" (DBC).",
                "En este artículo, presentamos la estructura del \"control basado en la dinámica\".",
                "La Sección 3 introduce la estructura del \"control basado en la dinámica\" (DBC), y su especialización a los entornos de Markovian.",
                "El \"control basado en la dinámica\" proporciona un enfoque natural para resolver estos problemas.3.",
                "\"Control basado en la dinámica\" La especificación del \"control basado en la dinámica\" (DBC) se puede dividir en tres niveles de interacción: nivel de diseño del entorno, nivel de usuario y nivel de agente.• El nivel de diseño del medio ambiente se refiere a la especificación formal y el modelado del entorno.",
                "Conclusiones y trabajo futuro En este documento, hemos presentado una perspectiva novedosa sobre el proceso de planificación y control en entornos estocásticos, en forma del marco \"Control basado en la dinámica\" (DBC)."
            ],
            "translated_text": "",
            "candidates": [
                "control basado en la dinámica",
                "Control basado en la dinámica",
                "Control basado en la dinámica",
                "control basado en la dinámica",
                "control basado en la dinámica",
                "control basado en la dinámica",
                "control basado en la dinámica",
                "control basado en la dinámica",
                "control basado en la dinámica",
                "Control basado en la dinámica",
                "control basado en la dinámica",
                "control basado en la dinámica",
                "Control basado en la dinámica",
                "control basado en la dinámica",
                "control basado en la dinámica",
                "Control basado en la dinámica"
            ],
            "error": []
        },
        "area-sweeping problem": {
            "translated_key": "Problema de barrido del área",
            "is_in_text": true,
            "original_annotated_sentences": [
                "Dynamics Based Control with an Application to Area-Sweeping Problems Zinovi Rabinovich Engineering and Computer Science Hebrew University of Jerusalem Jerusalem, Israel nomad@cs.huji.ac.il Jeffrey S. Rosenschein Engineering and Computer Science Hebrew University of Jerusalem Jerusalem, Israel jeff@cs.huji.ac.il Gal A. Kaminka The MAVERICK Group Department of Computer Science Bar Ilan University, Israel galk@cs.biu.ac.il ABSTRACT In this paper we introduce Dynamics Based Control (DBC), an approach to planning and control of an agent in stochastic environments.",
                "Unlike existing approaches, which seek to optimize expected rewards (e.g., in Partially Observable Markov Decision Problems (POMDPs)), DBC optimizes system behavior towards specified system dynamics.",
                "We show that a recently developed planning and control approach, Extended Markov Tracking (EMT) is an instantiation of DBC.",
                "EMT employs greedy action selection to provide an efficient control algorithm in Markovian environments.",
                "We exploit this efficiency in a set of experiments that applied multitarget EMT to a class of <br>area-sweeping problem</br>s (searching for moving targets).",
                "We show that such problems can be naturally defined and efficiently solved using the DBC framework, and its EMT instantiation.",
                "Categories and Subject Descriptors I.2.8 [Problem Solving, Control Methods, and Search]: Control Theory; I.2.9 [Robotics]; I.2.11 [Distributed Artificial Intelligence]: Intelligent Agents General Terms Algorithms, Theory 1.",
                "INTRODUCTION Planning and control constitutes a central research area in multiagent systems and artificial intelligence.",
                "In recent years, Partially Observable Markov Decision Processes (POMDPs) [12] have become a popular formal basis for planning in stochastic environments.",
                "In this framework, the planning and control problem is often addressed by imposing a reward function, and computing a policy (of choosing actions) that is optimal, in the sense that it will result in the highest expected utility.",
                "While theoretically attractive, the complexity of optimally solving a POMDP is prohibitive [8, 7].",
                "We take an alternative view of planning in stochastic environments.",
                "We do not use a (state-based) reward function, but instead optimize over a different criterion, a transition-based specification of the desired system dynamics.",
                "The idea here is to view planexecution as a process that compels a (stochastic) system to change, and a plan as a dynamic process that shapes that change according to desired criteria.",
                "We call this general planning framework Dynamics Based Control (DBC).",
                "In DBC, the goal of a planning (or control) process becomes to ensure that the system will change in accordance with specific (potentially stochastic) target dynamics.",
                "As actual system behavior may deviate from that which is specified by target dynamics (due to the stochastic nature of the system), planning in such environments needs to be continual [4], in a manner similar to classical closed-loop controllers [16].",
                "Here, optimality is measured in terms of probability of deviation magnitudes.",
                "In this paper, we present the structure of Dynamics Based Control.",
                "We show that the recently developed Extended Markov Tracking (EMT) approach [13, 14, 15] is subsumed by DBC, with EMT employing greedy action selection, which is a specific parameterization among the options possible within DBC.",
                "EMT is an efficient instantiation of DBC.",
                "To evaluate DBC, we carried out a set of experiments applying multi-target EMT to the Tag Game [11]; this is a variant on the area sweeping problem, where an agent is trying to tag a moving target (quarry) whose position is not known with certainty.",
                "Experimental data demonstrates that even with a simple model of the environment and a simple design of target dynamics, high success rates can be produced both in catching the quarry, and in surprising the quarry (as expressed by the observed entropy of the controlled agents position).",
                "The paper is organized as follows.",
                "In Section 2 we motivate DBC using <br>area-sweeping problem</br>s, and discuss related work.",
                "Section 3 introduces the Dynamics Based Control (DBC) structure, and its specialization to Markovian environments.",
                "This is followed by a review of the Extended Markov Tracking (EMT) approach as a DBC-structured control regimen in Section 4.",
                "That section also discusses the limitations of EMT-based control relative to the general DBC framework.",
                "Experimental settings and results are then presented in Section 5.",
                "Section 6 provides a short discussion of the overall approach, and Section 7 gives some concluding remarks and directions for future work. 790 978-81-904262-7-5 (RPS) c 2007 IFAAMAS 2.",
                "MOTIVATION AND RELATED WORK Many real-life scenarios naturally have a stochastic target dynamics specification, especially those domains where there exists no ultimate goal, but rather system behavior (with specific properties) that has to be continually supported.",
                "For example, security guards perform persistent sweeps of an area to detect any sign of intrusion.",
                "Cunning thieves will attempt to track these sweeps, and time their operation to key points of the guards motion.",
                "It is thus advisable to make the guards motion dynamics appear irregular and random.",
                "Recent work by Paruchuri et al. [10] has addressed such randomization in the context of single-agent and distributed POMDPs.",
                "The goal in that work was to generate policies that provide a measure of action-selection randomization, while maintaining rewards within some acceptable levels.",
                "Our focus differs from this work in that DBC does not optimize expected rewards-indeed we do not consider rewards at all-but instead maintains desired dynamics (including, but not limited to, randomization).",
                "The Game of Tag is another example of the applicability of the approach.",
                "It was introduced in the work by Pineau et al. [11].",
                "There are two agents that can move about an area, which is divided into a grid.",
                "The grid may have blocked cells (holes) into which no agent can move.",
                "One agent (the hunter) seeks to move into a cell occupied by the other (the quarry), such that they are co-located (this is a successful tag).",
                "The quarry seeks to avoid the hunter agent, and is always aware of the hunters position, but does not know how the hunter will behave, which opens up the possibility for a hunter to surprise the prey.",
                "The hunter knows the quarrys probabilistic law of motion, but does not know its current location.",
                "Tag is an instance of a family of area-sweeping (pursuit-evasion) problems.",
                "In [11], the hunter modeled the problem using a POMDP.",
                "A reward function was defined, to reflect the desire to tag the quarry, and an action policy was computed to optimize the reward collected over time.",
                "Due to the intractable complexity of determining the optimal policy, the action policy computed in that paper was essentially an approximation.",
                "In this paper, instead of formulating a reward function, we use EMT to solve the problem, by directly specifying the target dynamics.",
                "In fact, any search problem with randomized motion, the socalled class of area sweeping problems, can be described through specification of such target system dynamics.",
                "Dynamics Based Control provides a natural approach to solving these problems. 3.",
                "DYNAMICS BASED CONTROL The specification of Dynamics Based Control (DBC) can be broken into three interacting levels: Environment Design Level, User Level, and Agent Level. • Environment Design Level is concerned with the formal specification and modeling of the environment.",
                "For example, this level would specify the laws of physics within the system, and set its parameters, such as the gravitation constant. • User Level in turn relies on the environment model produced by Environment Design to specify the target system dynamics it wishes to observe.",
                "The User Level also specifies the estimation or learning procedure for system dynamics, and the measure of deviation.",
                "In the museum guard scenario above, these would correspond to a stochastic sweep schedule, and a measure of relative surprise between the specified and actual sweeping. • Agent Level in turn combines the environment model from the Environment Design level, the dynamics estimation procedure, the deviation measure and the target dynamics specification from User Level, to produce a sequence of actions that create system dynamics as close as possible to the targeted specification.",
                "As we are interested in the continual development of a stochastic system, such as happens in classical control theory [16] and continual planning [4], as well as in our example of museum sweeps, the question becomes how the Agent Level is to treat the deviation measurements over time.",
                "To this end, we use a probability threshold-that is, we would like the Agent Level to maximize the probability that the deviation measure will remain below a certain threshold.",
                "Specific action selection then depends on system formalization.",
                "One possibility would be to create a mixture of available system trends, much like that which happens in Behavior-Based Robotic architectures [1].",
                "The other alternative would be to rely on the estimation procedure provided by the User Level-to utilize the Environment Design Level model of the environment to choose actions, so as to manipulate the dynamics estimator into believing that a certain dynamics has been achieved.",
                "Notice that this manipulation is not direct, but via the environment.",
                "Thus, for strong enough estimator algorithms, successful manipulation would mean a successful simulation of the specified target dynamics (i.e., beyond discerning via the available sensory input).",
                "DBC levels can also have a back-flow of information (see Figure 1).",
                "For instance, the Agent Level could provide data about target dynamics feasibility, allowing the User Level to modify the requirement, perhaps focusing on attainable features of system behavior.",
                "Data would also be available about the system response to different actions performed; combined with a dynamics estimator defined by the User Level, this can provide an important tool for the environment model calibration at the Environment Design Level.",
                "UserEnv.",
                "Design Agent Model Ideal Dynamics Estimator Estimator Dynamics Feasibility System Response Data Figure 1: Data flow of the DBC framework Extending upon the idea of Actor-Critic algorithms [5], DBC data flow can provide a good basis for the design of a learning algorithm.",
                "For example, the User Level can operate as an exploratory device for a learning algorithm, inferring an ideal dynamics target from the environment model at hand that would expose and verify most critical features of system behavior.",
                "In this case, feasibility and system response data from the Agent Level would provide key information for an environment model update.",
                "In fact, the combination of feasibility and response data can provide a basis for the application of strong learning algorithms such as EM [2, 9]. 3.1 DBC for Markovian Environments For a Partially Observable Markovian Environment, DBC can be specified in a more rigorous manner.",
                "Notice how DBC discards rewards, and replaces it by another optimality criterion (structural differences are summarized in Table 1): • Environment Design level is to specify a tuple < S, A, T, O, Ω, s0 >, where: - S is the set of all possible environment states; - s0 is the initial state of the environment (which can also be viewed as a probability distribution over S); The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) 791 - A is the set of all possible actions applicable in the environment; - T is the environments probabilistic transition function: T : S ×A → Π(S).",
                "That is, T(s |a, s) is the probability that the environment will move from state s to state s under action a; - O is the set of all possible observations.",
                "This is what the sensor input would look like for an outside observer; - Ω is the observation probability function: Ω : S × A × S → Π(O).",
                "That is, Ω(o|s , a, s) is the probability that one will observe o given that the environment has moved from state s to state s under action a. • User Level, in the case of Markovian environment, operates on the set of system dynamics described by a family of conditional probabilities F = {τ : S × A → Π(S)}.",
                "Thus specification of target dynamics can be expressed by q ∈ F, and the learning or tracking algorithm can be represented as a function L : O×(A×O)∗ → F; that is, it maps sequences of observations and actions performed so far into an estimate τ ∈ F of system dynamics.",
                "There are many possible variations available at the User Level to define divergence between system dynamics; several of them are: - Trace distance or L1 distance between two distributions p and q defined by D(p(·), q(·)) = 1 2 x |p(x) − q(x)| - Fidelity measure of distance F(p(·), q(·)) = x p(x)q(x) - Kullback-Leibler divergence DKL(p(·) q(·)) = x p(x) log p(x) q(x) Notice that the latter two are not actually metrics over the space of possible distributions, but nevertheless have meaningful and important interpretations.",
                "For instance, KullbackLeibler divergence is an important tool of information theory [3] that allows one to measure the price of encoding an information source governed by q, while assuming that it is governed by p. The User Level also defines the threshold of dynamics deviation probability θ. • Agent Level is then faced with a problem of selecting a control signal function a∗ to satisfy a minimization problem as follows: a∗ = arg min a Pr(d(τa, q) > θ) where d(τa, q) is a random variable describing deviation of the dynamics estimate τa, created by L under control signal a, from the ideal dynamics q.",
                "Implicit in this minimization problem is that L is manipulated via the environment, based on the environment model produced by the Environment Design Level. 3.2 DBC View of the State Space It is important to note the complementary view that DBC and POMDPs take on the state space of the environment.",
                "POMDPs regard state as a stationary snap-shot of the environment; whatever attributes of state sequencing one seeks are reached through properties of the control process, in this case reward accumulation.",
                "This can be viewed as if the sequencing of states and the attributes of that sequencing are only introduced by and for the controlling mechanism-the POMDP policy.",
                "DBC concentrates on the underlying principle of state sequencing, the system dynamics.",
                "DBCs target dynamics specification can use the environments state space as a means to describe, discern, and preserve changes that occur within the system.",
                "As a result, DBC has a greater ability to express state sequencing properties, which are grounded in the environment model and its state space definition.",
                "For example, consider the task of moving through rough terrain towards a goal and reaching it as fast as possible.",
                "POMDPs would encode terrain as state space points, while speed would be ensured by negative reward for every step taken without reaching the goalaccumulating higher reward can be reached only by faster motion.",
                "Alternatively, the state space could directly include the notion of speed.",
                "For POMDPs, this would mean that the same concept is encoded twice, in some sense: directly in the state space, and indirectly within reward accumulation.",
                "Now, even if the reward function would encode more, and finer, details of the properties of motion, the POMDP solution will have to search in a much larger space of policies, while still being guided by the implicit concept of the reward accumulation procedure.",
                "On the other hand, the tactical target expression of variations and correlations between position and speed of motion is now grounded in the state space representation.",
                "In this situation, any further constraints, e.g., smoothness of motion, speed limits in different locations, or speed reductions during sharp turns, are explicitly and uniformly expressed by the tactical target, and can result in faster and more effective action selection by a DBC algorithm. 4.",
                "EMT-BASED CONTROL AS A DBC Recently, a control algorithm was introduced called EMT-based Control [13], which instantiates the DBC framework.",
                "Although it provides an approximate greedy solution in the DBC sense, initial experiments using EMT-based control have been encouraging [14, 15].",
                "EMT-based control is based on the Markovian environment definition, as in the case with POMDPs, but its User and Agent Levels are of the Markovian DBC type of optimality. • User Level of EMT-based control defines a limited-case target system dynamics independent of action: qEMT : S → Π(S).",
                "It then utilizes the Kullback-Leibler divergence measure to compose a momentary system dynamics estimator-the Extended Markov Tracking (EMT) algorithm.",
                "The algorithm keeps a system dynamics estimate τt EMT that is capable of explaining recent change in an auxiliary Bayesian system state estimator from pt−1 to pt, and updates it conservatively using Kullback-Leibler divergence.",
                "Since τt EMT and pt−1,t are respectively the conditional and marginal probabilities over the systems state space, explanation simply means that pt(s ) = s τt EMT (s |s)pt−1(s), and the dynamics estimate update is performed by solving a 792 The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) Table 1: Structure of POMDP vs. Dynamics-Based Control in Markovian Environment Level Approach MDP Markovian DBC Environment < S, A, T, O, Ω >,where S - set of states A - set of actions Design T : S × A → Π(S) - transition O - observation set Ω : S × A × S → Π(O) User r : S × A × S → R q : S × A → Π(S) F(π∗ ) → r L(o1, ..., ot) → τ r - reward function q - ideal dynamics F - reward remodeling L - dynamics estimator θ - threshold Agent π∗ = arg max π E[ γt rt] π∗ = arg min π Prob(d(τ q) > θ) minimization problem: τt EMT = H[pt, pt−1, τt−1 EMT ] = arg min τ DKL(τ × pt−1 τt−1 EMT × pt−1) s.t. pt(s ) = s (τ × pt−1)(s , s) pt−1(s) = s (τ × pt−1)(s , s) • Agent Level in EMT-based control is suboptimal with respect to DBC (though it remains within the DBC framework), performing greedy action selection based on prediction of EMTs reaction.",
                "The prediction is based on the environment model provided by the Environment Design level, so that if we denote by Ta the environments transition function limited to action a, and pt−1 is the auxiliary Bayesian system state estimator, then the EMT-based control choice is described by a∗ = arg min a∈A DKL(H[Ta × pt, pt, τt EMT ] qEMT × pt−1) Note that this follows the Markovian DBC framework precisely: the rewarding optimality of POMDPs is discarded, and in its place a dynamics estimator (EMT in this case) is manipulated via action effects on the environment to produce an estimate close to the specified target system dynamics.",
                "Yet as we mentioned, naive EMTbased control is suboptimal in the DBC sense, and has several additional limitations that do not exist in the general DBC framework (discussed in Section 4.2). 4.1 Multi-Target EMT At times, there may exist several behavioral preferences.",
                "For example, in the case of museum guards, some art items are more heavily guarded, requiring that the guards stay more often in their close vicinity.",
                "On the other hand, no corner of the museum is to be left unchecked, which demands constant motion.",
                "Successful museum security would demand that the guards adhere to, and balance, both of these behaviors.",
                "For EMT-based control, this would mean facing several tactical targets {qk}K k=1, and the question becomes how to merge and balance them.",
                "A balancing mechanism can be applied to resolve this issue.",
                "Note that EMT-based control, while selecting an action, creates a preference vector over the set of actions based on their predicted performance with respect to a given target.",
                "If these preference vectors are normalized, they can be combined into a single unified preference.",
                "This requires replacement of standard EMT-based action selection by the algorithm below [15]: • Given: - a set of target dynamics {qk}K k=1, - vector of weights w(k) • Select action as follows - For each action a ∈ A predict the future state distribution ¯pa t+1 = Ta ∗ pt; - For each action, compute Da = H(¯pa t+1, pt, PDt) - For each a ∈ A and qk tactical target, denote V (a, k) = DKL (Da qk) pt .",
                "Let Vk(a) = 1 Zk V (a, k), where Zk = a∈A V (a, k) is a normalization factor. - Select a∗ = arg min a k k=1 w(k)Vk(a) The weights vector w = (w1, ..., wK ) allows the additional tuning of importance among target dynamics without the need to redesign the targets themselves.",
                "This balancing method is also seamlessly integrated into the EMT-based control flow of operation. 4.2 EMT-based Control Limitations EMT-based control is a sub-optimal (in the DBC sense) representative of the DBC structure.",
                "It limits the User by forcing EMT to be its dynamic tracking algorithm, and replaces Agent optimization by greedy action selection.",
                "This kind of combination, however, is common for on-line algorithms.",
                "Although further development of EMT-based controllers is necessary, evidence so far suggests that even the simplest form of the algorithm possesses a great deal of power, and displays trends that are optimal in the DBC sense of the word.",
                "There are two further, EMT-specific, limitations to EMT-based control that are evident at this point.",
                "Both already have partial solutions and are subjects of ongoing research.",
                "The first limitation is the problem of negative preference.",
                "In the POMDP framework for example, this is captured simply, through The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) 793 the appearance of values with different signs within the reward structure.",
                "For EMT-based control, however, negative preference means that one would like to avoid a certain distribution over system development sequences; EMT-based control, however, concentrates on getting as close as possible to a distribution.",
                "Avoidance is thus unnatural in native EMT-based control.",
                "The second limitation comes from the fact that standard environment modeling can create pure sensory actions-actions that do not change the state of the world, and differ only in the way observations are received and the quality of observations received.",
                "Since the world state does not change, EMT-based control would not be able to differentiate between different sensory actions.",
                "Notice that both of these limitations of EMT-based control are absent from the general DBC framework, since it may have a tracking algorithm capable of considering pure sensory actions and, unlike Kullback-Leibler divergence, a distribution deviation measure that is capable of dealing with negative preference. 5.",
                "EMT PLAYING TAG The Game of Tag was first introduced in [11].",
                "It is a single agent problem of capturing a quarry, and belongs to the class of area sweeping problems.",
                "An example domain is shown in Figure 2. 0 51 2 3 4 6 7 8 10 12 13 161514 17 18 19 2221 23 9 11Q A 20 Figure 2: Tag domain; an agent (A) attempts to seek and capture a quarry (Q) The Game of Tag extremely limits the agents perception, so that the agent is able to detect the quarry only if they are co-located in the same cell of the grid world.",
                "In the classical version of the game, co-location leads to a special observation, and the Tag action can be performed.",
                "We slightly modify this setting: the moment that both agents occupy the same cell, the game ends.",
                "As a result, both the agent and its quarry have the same motion capability, which allows them to move in four directions, North, South, East, and West.",
                "These form a formal space of actions within a Markovian environment.",
                "The state space of the formal Markovian environment is described by the cross-product of the agent and quarrys positions.",
                "For Figure 2, it would be S = {s0, ..., s23} × {s0, ..., s23}.",
                "The effects of an action taken by the agent are deterministic, but the environment in general has a stochastic response due to the motion of the quarry.",
                "With probability q0 1 it stays put, and with probability 1 − q0 it moves to an adjacent cell further away from the 1 In our experiments this was taken to be q0 = 0.2. agent.",
                "So for the instance shown in Figure 2 and q0 = 0.1: P(Q = s9|Q = s9, A = s11) = 0.1 P(Q = s2|Q = s9, A = s11) = 0.3 P(Q = s8|Q = s9, A = s11) = 0.3 P(Q = s14|Q = s9, A = s11) = 0.3 Although the evasive behavior of the quarry is known to the agent, the quarrys position is not.",
                "The only sensory information available to the agent is its own location.",
                "We use EMT and directly specify the target dynamics.",
                "For the Game of Tag, we can easily formulate three major trends: catching the quarry, staying mobile, and stalking the quarry.",
                "This results in the following three target dynamics: Tcatch(At+1 = si|Qt = sj, At = sa) ∝ 1 si = sj 0 otherwise Tmobile(At+1 = si|Qt = so, At = sj) ∝ 0 si = sj 1 otherwise Tstalk(At+1 = si|Qt = so, At = sj) ∝ 1 dist(si,so) Note that none of the above targets are directly achievable; for instance, if Qt = s9 and At = s11, there is no action that can move the agent to At+1 = s9 as required by the Tcatch target dynamics.",
                "We ran several experiments to evaluate EMT performance in the Tag Game.",
                "Three configurations of the domain shown in Figure 3 were used, each posing a different challenge to the agent due to partial observability.",
                "In each setting, a set of 1000 runs was performed with a time limit of 100 steps.",
                "In every run, the initial position of both the agent and its quarry was selected at random; this means that as far as the agent was concerned, the quarrys initial position was uniformly distributed over the entire domain cell space.",
                "We also used two variations of the environment observability function.",
                "In the first version, observability function mapped all joint positions of hunter and quarry into the position of the hunter as an observation.",
                "In the second, only those joint positions in which hunter and quarry occupied different locations were mapped into the hunters location.",
                "The second version in fact utilized and expressed the fact that once hunter and quarry occupy the same cell the game ends.",
                "The results of these experiments are shown in Table 2.",
                "Balancing [15] the catch, move, and stalk target dynamics described in the previous section by the weight vector [0.8, 0.1, 0.1], EMT produced stable performance in all three domains.",
                "Although direct comparisons are difficult to make, the EMT performance displayed notable efficiency vis-`a-vis the POMDP approach.",
                "In spite of a simple and inefficient Matlab implementation of the EMT algorithm, the decision time for any given step averaged significantly below 1 second in all experiments.",
                "For the irregular open arena domain, which proved to be the most difficult, 1000 experiment runs bounded by 100 steps each, a total of 42411 steps, were completed in slightly under 6 hours.",
                "That is, over 4 × 104 online steps took an order of magnitude less time than the offline computation of POMDP policy in [11].",
                "The significance of this differential is made even more prominent by the fact that, should the environment model parameters change, the online nature of EMT would allow it to maintain its performance time, while the POMDP policy would need to be recomputed, requiring yet again a large overhead of computation.",
                "We also tested the behavior cell frequency entropy, empirical measures from trial data.",
                "As Figure 4 and Figure 5 show, empir794 The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) A Q Q A 0 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 A Q Figure 3: These configurations of the Tag Game space were used: a) multiple dead-end, b) irregular open arena, c) circular corridor Table 2: Performance of the EMT-based solution in three Tag Game domains and two observability models: I) omniposition quarry, II) quarry is not at hunters position Model Domain Capture% E(Steps) Time/Step I Dead-ends 100 14.8 72(mSec) Arena 80.2 42.4 500(mSec) Circle 91.4 34.6 187(mSec) II Dead-ends 100 13.2 91(mSec) Arena 96.8 28.67 396(mSec) Circle 94.4 31.63 204(mSec) ical entropy grows with the length of interaction.",
                "For runs where the quarry was not captured immediately, the entropy reaches between 0.85 and 0.952 for different runs and scenarios.",
                "As the agent actively seeks the quarry, the entropy never reaches its maximum.",
                "One characteristic of the entropy graph for the open arena scenario particularly caught our attention in the case of the omniposition quarry observation model.",
                "Near the maximum limit of trial length (100 steps), entropy suddenly dropped.",
                "Further analysis of the data showed that under certain circumstances, a fluctuating behavior occurs in which the agent faces equally viable versions of quarry-following behavior.",
                "Since the EMT algorithm has greedy action selection, and the state space does not encode any form of commitment (not even speed or acceleration), the agent is locked within a small portion of cells.",
                "It is essentially attempting to simultaneously follow several courses of action, all of which are consistent with the target dynamics.",
                "This behavior did not occur in our second observation model, since it significantly reduced the set of eligible courses of action-essentially contributing to tie-breaking among them. 6.",
                "DISCUSSION The design of the EMT solution for the Tag Game exposes the core difference in approach to planning and control between EMT or DBC, on the one hand, and the more familiar POMDP approach, on the other.",
                "POMDP defines a reward structure to optimize, and influences system dynamics indirectly through that optimization.",
                "EMT discards any reward scheme, and instead measures and influences system dynamics directly. 2 Entropy was calculated using log base equal to the number of possible locations within the domain; this properly scales entropy expression into the range [0, 1] for all domains.",
                "Thus for the Tag Game, we did not search for a reward function that would encode and express our preference over the agents behavior, but rather directly set three (heuristic) behavior preferences as the basis for target dynamics to be maintained.",
                "Experimental data shows that these targets need not be directly achievable via the agents actions.",
                "However, the ratio between EMT performance and achievability of target dynamics remains to be explored.",
                "The tag game experiment data also revealed the different emphasis DBC and POMDPs place on the formulation of an environment state space.",
                "POMDPs rely entirely on the mechanism of reward accumulation maximization, i.e., formation of the action selection procedure to achieve necessary state sequencing.",
                "DBC, on the other hand, has two sources of sequencing specification: through the properties of an action selection procedure, and through direct specification within the target dynamics.",
                "The importance of the second source was underlined by the Tag Game experiment data, in which the greedy EMT algorithm, applied to a POMDP-type state space specification, failed, since target description over such a state space was incapable of encoding the necessary behavior tendencies, e.g., tie-breaking and commitment to directed motion.",
                "The structural differences between DBC (and EMT in particular), and POMDPs, prohibits direct performance comparison, and places them on complementary tracks, each within a suitable niche.",
                "For instance, POMDPs could be seen as a much more natural formulation of economic sequential decision-making problems, while EMT is a better fit for continual demand for stochastic change, as happens in many robotic or embodied-agent problems.",
                "The complementary properties of POMDPs and EMT can be further exploited.",
                "There is recent interest in using POMDPs in hybrid solutions [17], in which the POMDPs can be used together with other control approaches to provide results not easily achievable with either approach by itself.",
                "DBC can be an effective partner in such a hybrid solution.",
                "For instance, POMDPs have prohibitively large off-line time requirements for policy computation, but can be readily used in simpler settings to expose beneficial behavioral trends; this can serve as a form of target dynamics that are provided to EMT in a larger domain for on-line operation. 7.",
                "CONCLUSIONS AND FUTURE WORK In this paper, we have presented a novel perspective on the process of planning and control in stochastic environments, in the form of the Dynamics Based Control (DBC) framework.",
                "DBC formulates the task of planning as support of a specified target system dynamics, which describes the necessary properties of change within the environment.",
                "Optimality of DBC plans of action are measured The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) 795 0 20 40 60 80 100 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1 Steps Entropy Dead−ends 0 20 40 60 80 100 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1 Steps Entropy Arena 0 20 40 60 80 100 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1 Steps Entropy Circle Figure 4: Observation Model I: Omniposition quarry.",
                "Entropy development with length of Tag Game for the three experiment scenarios: a) multiple dead-end, b) irregular open arena, c) circular corridor. 0 10 20 30 40 50 60 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1 Steps Entropy Dead−ends 0 20 40 60 80 100 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1 Steps Entropy Arena 0 20 40 60 80 100 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1 Steps Entropy Circle Figure 5: Observation Model II: quarry not observed at hunters position.",
                "Entropy development with length of Tag Game for the three experiment scenarios: a) multiple dead-end, b) irregular open arena, c) circular corridor. with respect to the deviation of actual system dynamics from the target dynamics.",
                "We show that a recently developed technique of Extended Markov Tracking (EMT) [13] is an instantiation of DBC.",
                "In fact, EMT can be seen as a specific case of DBC parameterization, which employs a greedy action selection procedure.",
                "Since EMT exhibits the key features of the general DBC framework, as well as polynomial time complexity, we used the multitarget version of EMT [15] to demonstrate that the class of area sweeping problems naturally lends itself to dynamics-based descriptions, as instantiated by our experiments in the Tag Game domain.",
                "As enumerated in Section 4.2, EMT has a number of limitations, such as difficulty in dealing with negative dynamic preference.",
                "This prevents direct application of EMT to such problems as Rendezvous-Evasion Games (e.g., [6]).",
                "However, DBC in general has no such limitations, and readily enables the formulation of evasion games.",
                "In future work, we intend to proceed with the development of dynamics-based controllers for these problems. 8.",
                "ACKNOWLEDGMENT The work of the first two authors was partially supported by Israel Science Foundation grant #898/05, and the third author was partially supported by a grant from Israels Ministry of Science and Technology. 9.",
                "REFERENCES [1] R. C. Arkin.",
                "Behavior-Based Robotics.",
                "MIT Press, 1998. [2] J.",
                "A. Bilmes.",
                "A gentle tutorial of the EM algorithm and its application to parameter estimation for Gaussian mixture and Hidden Markov Models.",
                "Technical Report TR-97-021, Department of Electrical Engeineering and Computer Science, University of California at Berkeley, 1998. [3] T. M. Cover and J.",
                "A. Thomas.",
                "Elements of information theory.",
                "Wiley, 1991. [4] M. E. desJardins, E. H. Durfee, C. L. Ortiz, and M. J. Wolverton.",
                "A survey of research in distributed, continual planning.",
                "AI Magazine, 4:13-22, 1999. [5] V. R. Konda and J. N. Tsitsiklis.",
                "Actor-Critic algorithms.",
                "SIAM Journal on Control and Optimization, 42(4):1143-1166, 2003. [6] W. S. Lim.",
                "A rendezvous-evasion game on discrete locations with joint randomization.",
                "Advances in Applied Probability, 29(4):1004-1017, December 1997. [7] M. L. Littman, T. L. Dean, and L. P. Kaelbling.",
                "On the complexity of solving Markov decision problems.",
                "In Proceedings of the 11th Annual Conference on Uncertainty in Artificial Intelligence (UAI-95), pages 394-402, 1995. [8] O. Madani, S. Hanks, and A. Condon.",
                "On the undecidability of probabilistic planning and related stochastic optimization problems.",
                "Artificial Intelligence Journal, 147(1-2):5-34, July 2003. [9] R. M. Neal and G. E. Hinton.",
                "A view of the EM algorithm 796 The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) that justifies incremental, sparse, and other variants.",
                "In M. I. Jordan, editor, Learning in Graphical Models, pages 355-368.",
                "Kluwer Academic Publishers, 1998. [10] P. Paruchuri, M. Tambe, F. Ordonez, and S. Kraus.",
                "Security in multiagent systems by policy randomization.",
                "In Proceeding of AAMAS 2006, 2006. [11] J. Pineau, G. Gordon, and S. Thrun.",
                "Point-based value iteration: An anytime algorithm for pomdps.",
                "In International Joint Conference on Artificial Intelligence (IJCAI), pages 1025-1032, August 2003. [12] M. L. Puterman.",
                "Markov Decision Processes.",
                "Wiley Series in Probability and Mathematical Statistics: Applied Probability and Statistics Section.",
                "Wiley-Interscience Publication, New York, 1994. [13] Z. Rabinovich and J. S. Rosenschein.",
                "Extended Markov Tracking with an application to control.",
                "In The Workshop on Agent Tracking: Modeling Other Agents from Observations, at the Third International Joint Conference on Autonomous Agents and Multiagent Systems, pages 95-100, New-York, July 2004. [14] Z. Rabinovich and J. S. Rosenschein.",
                "Multiagent coordination by Extended Markov Tracking.",
                "In The Fourth International Joint Conference on Autonomous Agents and Multiagent Systems, pages 431-438, Utrecht, The Netherlands, July 2005. [15] Z. Rabinovich and J. S. Rosenschein.",
                "On the response of EMT-based control to interacting targets and models.",
                "In The Fifth International Joint Conference on Autonomous Agents and Multiagent Systems, pages 465-470, Hakodate, Japan, May 2006. [16] R. F. Stengel.",
                "Optimal Control and Estimation.",
                "Dover Publications, 1994. [17] M. Tambe, E. Bowring, H. Jung, G. Kaminka, R. Maheswaran, J. Marecki, J. Modi, R. Nair, J. Pearce, P. Paruchuri, D. Pynadath, P. Scerri, N. Schurr, and P. Varakantham.",
                "Conflicts in teamwork: Hybrids to the The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) 797"
            ],
            "original_annotated_samples": [],
            "translated_annotated_samples": [
                "Explotamos esta eficiencia en un conjunto de experimentos que aplicaron MULTITERGET EMT a una clase de \"Problema de barrido del área\" (buscando objetivos móviles).",
                "En la Sección 2 motivamos DBC utilizando el \"problema de barrido del área\" y discutimos el trabajo relacionado."
            ],
            "translated_text": "",
            "candidates": [
                "Problema de barrido de área",
                "Problema de barrido del área",
                "Problema de barrido de área",
                "problema de barrido del área"
            ],
            "error": []
        },
        "stochastic environment": {
            "translated_key": "entorno estocástico",
            "is_in_text": true,
            "original_annotated_sentences": [
                "Dynamics Based Control with an Application to Area-Sweeping Problems Zinovi Rabinovich Engineering and Computer Science Hebrew University of Jerusalem Jerusalem, Israel nomad@cs.huji.ac.il Jeffrey S. Rosenschein Engineering and Computer Science Hebrew University of Jerusalem Jerusalem, Israel jeff@cs.huji.ac.il Gal A. Kaminka The MAVERICK Group Department of Computer Science Bar Ilan University, Israel galk@cs.biu.ac.il ABSTRACT In this paper we introduce Dynamics Based Control (DBC), an approach to planning and control of an agent in <br>stochastic environment</br>s.",
                "Unlike existing approaches, which seek to optimize expected rewards (e.g., in Partially Observable Markov Decision Problems (POMDPs)), DBC optimizes system behavior towards specified system dynamics.",
                "We show that a recently developed planning and control approach, Extended Markov Tracking (EMT) is an instantiation of DBC.",
                "EMT employs greedy action selection to provide an efficient control algorithm in Markovian environments.",
                "We exploit this efficiency in a set of experiments that applied multitarget EMT to a class of area-sweeping problems (searching for moving targets).",
                "We show that such problems can be naturally defined and efficiently solved using the DBC framework, and its EMT instantiation.",
                "Categories and Subject Descriptors I.2.8 [Problem Solving, Control Methods, and Search]: Control Theory; I.2.9 [Robotics]; I.2.11 [Distributed Artificial Intelligence]: Intelligent Agents General Terms Algorithms, Theory 1.",
                "INTRODUCTION Planning and control constitutes a central research area in multiagent systems and artificial intelligence.",
                "In recent years, Partially Observable Markov Decision Processes (POMDPs) [12] have become a popular formal basis for planning in <br>stochastic environment</br>s.",
                "In this framework, the planning and control problem is often addressed by imposing a reward function, and computing a policy (of choosing actions) that is optimal, in the sense that it will result in the highest expected utility.",
                "While theoretically attractive, the complexity of optimally solving a POMDP is prohibitive [8, 7].",
                "We take an alternative view of planning in <br>stochastic environment</br>s.",
                "We do not use a (state-based) reward function, but instead optimize over a different criterion, a transition-based specification of the desired system dynamics.",
                "The idea here is to view planexecution as a process that compels a (stochastic) system to change, and a plan as a dynamic process that shapes that change according to desired criteria.",
                "We call this general planning framework Dynamics Based Control (DBC).",
                "In DBC, the goal of a planning (or control) process becomes to ensure that the system will change in accordance with specific (potentially stochastic) target dynamics.",
                "As actual system behavior may deviate from that which is specified by target dynamics (due to the stochastic nature of the system), planning in such environments needs to be continual [4], in a manner similar to classical closed-loop controllers [16].",
                "Here, optimality is measured in terms of probability of deviation magnitudes.",
                "In this paper, we present the structure of Dynamics Based Control.",
                "We show that the recently developed Extended Markov Tracking (EMT) approach [13, 14, 15] is subsumed by DBC, with EMT employing greedy action selection, which is a specific parameterization among the options possible within DBC.",
                "EMT is an efficient instantiation of DBC.",
                "To evaluate DBC, we carried out a set of experiments applying multi-target EMT to the Tag Game [11]; this is a variant on the area sweeping problem, where an agent is trying to tag a moving target (quarry) whose position is not known with certainty.",
                "Experimental data demonstrates that even with a simple model of the environment and a simple design of target dynamics, high success rates can be produced both in catching the quarry, and in surprising the quarry (as expressed by the observed entropy of the controlled agents position).",
                "The paper is organized as follows.",
                "In Section 2 we motivate DBC using area-sweeping problems, and discuss related work.",
                "Section 3 introduces the Dynamics Based Control (DBC) structure, and its specialization to Markovian environments.",
                "This is followed by a review of the Extended Markov Tracking (EMT) approach as a DBC-structured control regimen in Section 4.",
                "That section also discusses the limitations of EMT-based control relative to the general DBC framework.",
                "Experimental settings and results are then presented in Section 5.",
                "Section 6 provides a short discussion of the overall approach, and Section 7 gives some concluding remarks and directions for future work. 790 978-81-904262-7-5 (RPS) c 2007 IFAAMAS 2.",
                "MOTIVATION AND RELATED WORK Many real-life scenarios naturally have a stochastic target dynamics specification, especially those domains where there exists no ultimate goal, but rather system behavior (with specific properties) that has to be continually supported.",
                "For example, security guards perform persistent sweeps of an area to detect any sign of intrusion.",
                "Cunning thieves will attempt to track these sweeps, and time their operation to key points of the guards motion.",
                "It is thus advisable to make the guards motion dynamics appear irregular and random.",
                "Recent work by Paruchuri et al. [10] has addressed such randomization in the context of single-agent and distributed POMDPs.",
                "The goal in that work was to generate policies that provide a measure of action-selection randomization, while maintaining rewards within some acceptable levels.",
                "Our focus differs from this work in that DBC does not optimize expected rewards-indeed we do not consider rewards at all-but instead maintains desired dynamics (including, but not limited to, randomization).",
                "The Game of Tag is another example of the applicability of the approach.",
                "It was introduced in the work by Pineau et al. [11].",
                "There are two agents that can move about an area, which is divided into a grid.",
                "The grid may have blocked cells (holes) into which no agent can move.",
                "One agent (the hunter) seeks to move into a cell occupied by the other (the quarry), such that they are co-located (this is a successful tag).",
                "The quarry seeks to avoid the hunter agent, and is always aware of the hunters position, but does not know how the hunter will behave, which opens up the possibility for a hunter to surprise the prey.",
                "The hunter knows the quarrys probabilistic law of motion, but does not know its current location.",
                "Tag is an instance of a family of area-sweeping (pursuit-evasion) problems.",
                "In [11], the hunter modeled the problem using a POMDP.",
                "A reward function was defined, to reflect the desire to tag the quarry, and an action policy was computed to optimize the reward collected over time.",
                "Due to the intractable complexity of determining the optimal policy, the action policy computed in that paper was essentially an approximation.",
                "In this paper, instead of formulating a reward function, we use EMT to solve the problem, by directly specifying the target dynamics.",
                "In fact, any search problem with randomized motion, the socalled class of area sweeping problems, can be described through specification of such target system dynamics.",
                "Dynamics Based Control provides a natural approach to solving these problems. 3.",
                "DYNAMICS BASED CONTROL The specification of Dynamics Based Control (DBC) can be broken into three interacting levels: Environment Design Level, User Level, and Agent Level. • Environment Design Level is concerned with the formal specification and modeling of the environment.",
                "For example, this level would specify the laws of physics within the system, and set its parameters, such as the gravitation constant. • User Level in turn relies on the environment model produced by Environment Design to specify the target system dynamics it wishes to observe.",
                "The User Level also specifies the estimation or learning procedure for system dynamics, and the measure of deviation.",
                "In the museum guard scenario above, these would correspond to a stochastic sweep schedule, and a measure of relative surprise between the specified and actual sweeping. • Agent Level in turn combines the environment model from the Environment Design level, the dynamics estimation procedure, the deviation measure and the target dynamics specification from User Level, to produce a sequence of actions that create system dynamics as close as possible to the targeted specification.",
                "As we are interested in the continual development of a stochastic system, such as happens in classical control theory [16] and continual planning [4], as well as in our example of museum sweeps, the question becomes how the Agent Level is to treat the deviation measurements over time.",
                "To this end, we use a probability threshold-that is, we would like the Agent Level to maximize the probability that the deviation measure will remain below a certain threshold.",
                "Specific action selection then depends on system formalization.",
                "One possibility would be to create a mixture of available system trends, much like that which happens in Behavior-Based Robotic architectures [1].",
                "The other alternative would be to rely on the estimation procedure provided by the User Level-to utilize the Environment Design Level model of the environment to choose actions, so as to manipulate the dynamics estimator into believing that a certain dynamics has been achieved.",
                "Notice that this manipulation is not direct, but via the environment.",
                "Thus, for strong enough estimator algorithms, successful manipulation would mean a successful simulation of the specified target dynamics (i.e., beyond discerning via the available sensory input).",
                "DBC levels can also have a back-flow of information (see Figure 1).",
                "For instance, the Agent Level could provide data about target dynamics feasibility, allowing the User Level to modify the requirement, perhaps focusing on attainable features of system behavior.",
                "Data would also be available about the system response to different actions performed; combined with a dynamics estimator defined by the User Level, this can provide an important tool for the environment model calibration at the Environment Design Level.",
                "UserEnv.",
                "Design Agent Model Ideal Dynamics Estimator Estimator Dynamics Feasibility System Response Data Figure 1: Data flow of the DBC framework Extending upon the idea of Actor-Critic algorithms [5], DBC data flow can provide a good basis for the design of a learning algorithm.",
                "For example, the User Level can operate as an exploratory device for a learning algorithm, inferring an ideal dynamics target from the environment model at hand that would expose and verify most critical features of system behavior.",
                "In this case, feasibility and system response data from the Agent Level would provide key information for an environment model update.",
                "In fact, the combination of feasibility and response data can provide a basis for the application of strong learning algorithms such as EM [2, 9]. 3.1 DBC for Markovian Environments For a Partially Observable Markovian Environment, DBC can be specified in a more rigorous manner.",
                "Notice how DBC discards rewards, and replaces it by another optimality criterion (structural differences are summarized in Table 1): • Environment Design level is to specify a tuple < S, A, T, O, Ω, s0 >, where: - S is the set of all possible environment states; - s0 is the initial state of the environment (which can also be viewed as a probability distribution over S); The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) 791 - A is the set of all possible actions applicable in the environment; - T is the environments probabilistic transition function: T : S ×A → Π(S).",
                "That is, T(s |a, s) is the probability that the environment will move from state s to state s under action a; - O is the set of all possible observations.",
                "This is what the sensor input would look like for an outside observer; - Ω is the observation probability function: Ω : S × A × S → Π(O).",
                "That is, Ω(o|s , a, s) is the probability that one will observe o given that the environment has moved from state s to state s under action a. • User Level, in the case of Markovian environment, operates on the set of system dynamics described by a family of conditional probabilities F = {τ : S × A → Π(S)}.",
                "Thus specification of target dynamics can be expressed by q ∈ F, and the learning or tracking algorithm can be represented as a function L : O×(A×O)∗ → F; that is, it maps sequences of observations and actions performed so far into an estimate τ ∈ F of system dynamics.",
                "There are many possible variations available at the User Level to define divergence between system dynamics; several of them are: - Trace distance or L1 distance between two distributions p and q defined by D(p(·), q(·)) = 1 2 x |p(x) − q(x)| - Fidelity measure of distance F(p(·), q(·)) = x p(x)q(x) - Kullback-Leibler divergence DKL(p(·) q(·)) = x p(x) log p(x) q(x) Notice that the latter two are not actually metrics over the space of possible distributions, but nevertheless have meaningful and important interpretations.",
                "For instance, KullbackLeibler divergence is an important tool of information theory [3] that allows one to measure the price of encoding an information source governed by q, while assuming that it is governed by p. The User Level also defines the threshold of dynamics deviation probability θ. • Agent Level is then faced with a problem of selecting a control signal function a∗ to satisfy a minimization problem as follows: a∗ = arg min a Pr(d(τa, q) > θ) where d(τa, q) is a random variable describing deviation of the dynamics estimate τa, created by L under control signal a, from the ideal dynamics q.",
                "Implicit in this minimization problem is that L is manipulated via the environment, based on the environment model produced by the Environment Design Level. 3.2 DBC View of the State Space It is important to note the complementary view that DBC and POMDPs take on the state space of the environment.",
                "POMDPs regard state as a stationary snap-shot of the environment; whatever attributes of state sequencing one seeks are reached through properties of the control process, in this case reward accumulation.",
                "This can be viewed as if the sequencing of states and the attributes of that sequencing are only introduced by and for the controlling mechanism-the POMDP policy.",
                "DBC concentrates on the underlying principle of state sequencing, the system dynamics.",
                "DBCs target dynamics specification can use the environments state space as a means to describe, discern, and preserve changes that occur within the system.",
                "As a result, DBC has a greater ability to express state sequencing properties, which are grounded in the environment model and its state space definition.",
                "For example, consider the task of moving through rough terrain towards a goal and reaching it as fast as possible.",
                "POMDPs would encode terrain as state space points, while speed would be ensured by negative reward for every step taken without reaching the goalaccumulating higher reward can be reached only by faster motion.",
                "Alternatively, the state space could directly include the notion of speed.",
                "For POMDPs, this would mean that the same concept is encoded twice, in some sense: directly in the state space, and indirectly within reward accumulation.",
                "Now, even if the reward function would encode more, and finer, details of the properties of motion, the POMDP solution will have to search in a much larger space of policies, while still being guided by the implicit concept of the reward accumulation procedure.",
                "On the other hand, the tactical target expression of variations and correlations between position and speed of motion is now grounded in the state space representation.",
                "In this situation, any further constraints, e.g., smoothness of motion, speed limits in different locations, or speed reductions during sharp turns, are explicitly and uniformly expressed by the tactical target, and can result in faster and more effective action selection by a DBC algorithm. 4.",
                "EMT-BASED CONTROL AS A DBC Recently, a control algorithm was introduced called EMT-based Control [13], which instantiates the DBC framework.",
                "Although it provides an approximate greedy solution in the DBC sense, initial experiments using EMT-based control have been encouraging [14, 15].",
                "EMT-based control is based on the Markovian environment definition, as in the case with POMDPs, but its User and Agent Levels are of the Markovian DBC type of optimality. • User Level of EMT-based control defines a limited-case target system dynamics independent of action: qEMT : S → Π(S).",
                "It then utilizes the Kullback-Leibler divergence measure to compose a momentary system dynamics estimator-the Extended Markov Tracking (EMT) algorithm.",
                "The algorithm keeps a system dynamics estimate τt EMT that is capable of explaining recent change in an auxiliary Bayesian system state estimator from pt−1 to pt, and updates it conservatively using Kullback-Leibler divergence.",
                "Since τt EMT and pt−1,t are respectively the conditional and marginal probabilities over the systems state space, explanation simply means that pt(s ) = s τt EMT (s |s)pt−1(s), and the dynamics estimate update is performed by solving a 792 The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) Table 1: Structure of POMDP vs. Dynamics-Based Control in Markovian Environment Level Approach MDP Markovian DBC Environment < S, A, T, O, Ω >,where S - set of states A - set of actions Design T : S × A → Π(S) - transition O - observation set Ω : S × A × S → Π(O) User r : S × A × S → R q : S × A → Π(S) F(π∗ ) → r L(o1, ..., ot) → τ r - reward function q - ideal dynamics F - reward remodeling L - dynamics estimator θ - threshold Agent π∗ = arg max π E[ γt rt] π∗ = arg min π Prob(d(τ q) > θ) minimization problem: τt EMT = H[pt, pt−1, τt−1 EMT ] = arg min τ DKL(τ × pt−1 τt−1 EMT × pt−1) s.t. pt(s ) = s (τ × pt−1)(s , s) pt−1(s) = s (τ × pt−1)(s , s) • Agent Level in EMT-based control is suboptimal with respect to DBC (though it remains within the DBC framework), performing greedy action selection based on prediction of EMTs reaction.",
                "The prediction is based on the environment model provided by the Environment Design level, so that if we denote by Ta the environments transition function limited to action a, and pt−1 is the auxiliary Bayesian system state estimator, then the EMT-based control choice is described by a∗ = arg min a∈A DKL(H[Ta × pt, pt, τt EMT ] qEMT × pt−1) Note that this follows the Markovian DBC framework precisely: the rewarding optimality of POMDPs is discarded, and in its place a dynamics estimator (EMT in this case) is manipulated via action effects on the environment to produce an estimate close to the specified target system dynamics.",
                "Yet as we mentioned, naive EMTbased control is suboptimal in the DBC sense, and has several additional limitations that do not exist in the general DBC framework (discussed in Section 4.2). 4.1 Multi-Target EMT At times, there may exist several behavioral preferences.",
                "For example, in the case of museum guards, some art items are more heavily guarded, requiring that the guards stay more often in their close vicinity.",
                "On the other hand, no corner of the museum is to be left unchecked, which demands constant motion.",
                "Successful museum security would demand that the guards adhere to, and balance, both of these behaviors.",
                "For EMT-based control, this would mean facing several tactical targets {qk}K k=1, and the question becomes how to merge and balance them.",
                "A balancing mechanism can be applied to resolve this issue.",
                "Note that EMT-based control, while selecting an action, creates a preference vector over the set of actions based on their predicted performance with respect to a given target.",
                "If these preference vectors are normalized, they can be combined into a single unified preference.",
                "This requires replacement of standard EMT-based action selection by the algorithm below [15]: • Given: - a set of target dynamics {qk}K k=1, - vector of weights w(k) • Select action as follows - For each action a ∈ A predict the future state distribution ¯pa t+1 = Ta ∗ pt; - For each action, compute Da = H(¯pa t+1, pt, PDt) - For each a ∈ A and qk tactical target, denote V (a, k) = DKL (Da qk) pt .",
                "Let Vk(a) = 1 Zk V (a, k), where Zk = a∈A V (a, k) is a normalization factor. - Select a∗ = arg min a k k=1 w(k)Vk(a) The weights vector w = (w1, ..., wK ) allows the additional tuning of importance among target dynamics without the need to redesign the targets themselves.",
                "This balancing method is also seamlessly integrated into the EMT-based control flow of operation. 4.2 EMT-based Control Limitations EMT-based control is a sub-optimal (in the DBC sense) representative of the DBC structure.",
                "It limits the User by forcing EMT to be its dynamic tracking algorithm, and replaces Agent optimization by greedy action selection.",
                "This kind of combination, however, is common for on-line algorithms.",
                "Although further development of EMT-based controllers is necessary, evidence so far suggests that even the simplest form of the algorithm possesses a great deal of power, and displays trends that are optimal in the DBC sense of the word.",
                "There are two further, EMT-specific, limitations to EMT-based control that are evident at this point.",
                "Both already have partial solutions and are subjects of ongoing research.",
                "The first limitation is the problem of negative preference.",
                "In the POMDP framework for example, this is captured simply, through The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) 793 the appearance of values with different signs within the reward structure.",
                "For EMT-based control, however, negative preference means that one would like to avoid a certain distribution over system development sequences; EMT-based control, however, concentrates on getting as close as possible to a distribution.",
                "Avoidance is thus unnatural in native EMT-based control.",
                "The second limitation comes from the fact that standard environment modeling can create pure sensory actions-actions that do not change the state of the world, and differ only in the way observations are received and the quality of observations received.",
                "Since the world state does not change, EMT-based control would not be able to differentiate between different sensory actions.",
                "Notice that both of these limitations of EMT-based control are absent from the general DBC framework, since it may have a tracking algorithm capable of considering pure sensory actions and, unlike Kullback-Leibler divergence, a distribution deviation measure that is capable of dealing with negative preference. 5.",
                "EMT PLAYING TAG The Game of Tag was first introduced in [11].",
                "It is a single agent problem of capturing a quarry, and belongs to the class of area sweeping problems.",
                "An example domain is shown in Figure 2. 0 51 2 3 4 6 7 8 10 12 13 161514 17 18 19 2221 23 9 11Q A 20 Figure 2: Tag domain; an agent (A) attempts to seek and capture a quarry (Q) The Game of Tag extremely limits the agents perception, so that the agent is able to detect the quarry only if they are co-located in the same cell of the grid world.",
                "In the classical version of the game, co-location leads to a special observation, and the Tag action can be performed.",
                "We slightly modify this setting: the moment that both agents occupy the same cell, the game ends.",
                "As a result, both the agent and its quarry have the same motion capability, which allows them to move in four directions, North, South, East, and West.",
                "These form a formal space of actions within a Markovian environment.",
                "The state space of the formal Markovian environment is described by the cross-product of the agent and quarrys positions.",
                "For Figure 2, it would be S = {s0, ..., s23} × {s0, ..., s23}.",
                "The effects of an action taken by the agent are deterministic, but the environment in general has a stochastic response due to the motion of the quarry.",
                "With probability q0 1 it stays put, and with probability 1 − q0 it moves to an adjacent cell further away from the 1 In our experiments this was taken to be q0 = 0.2. agent.",
                "So for the instance shown in Figure 2 and q0 = 0.1: P(Q = s9|Q = s9, A = s11) = 0.1 P(Q = s2|Q = s9, A = s11) = 0.3 P(Q = s8|Q = s9, A = s11) = 0.3 P(Q = s14|Q = s9, A = s11) = 0.3 Although the evasive behavior of the quarry is known to the agent, the quarrys position is not.",
                "The only sensory information available to the agent is its own location.",
                "We use EMT and directly specify the target dynamics.",
                "For the Game of Tag, we can easily formulate three major trends: catching the quarry, staying mobile, and stalking the quarry.",
                "This results in the following three target dynamics: Tcatch(At+1 = si|Qt = sj, At = sa) ∝ 1 si = sj 0 otherwise Tmobile(At+1 = si|Qt = so, At = sj) ∝ 0 si = sj 1 otherwise Tstalk(At+1 = si|Qt = so, At = sj) ∝ 1 dist(si,so) Note that none of the above targets are directly achievable; for instance, if Qt = s9 and At = s11, there is no action that can move the agent to At+1 = s9 as required by the Tcatch target dynamics.",
                "We ran several experiments to evaluate EMT performance in the Tag Game.",
                "Three configurations of the domain shown in Figure 3 were used, each posing a different challenge to the agent due to partial observability.",
                "In each setting, a set of 1000 runs was performed with a time limit of 100 steps.",
                "In every run, the initial position of both the agent and its quarry was selected at random; this means that as far as the agent was concerned, the quarrys initial position was uniformly distributed over the entire domain cell space.",
                "We also used two variations of the environment observability function.",
                "In the first version, observability function mapped all joint positions of hunter and quarry into the position of the hunter as an observation.",
                "In the second, only those joint positions in which hunter and quarry occupied different locations were mapped into the hunters location.",
                "The second version in fact utilized and expressed the fact that once hunter and quarry occupy the same cell the game ends.",
                "The results of these experiments are shown in Table 2.",
                "Balancing [15] the catch, move, and stalk target dynamics described in the previous section by the weight vector [0.8, 0.1, 0.1], EMT produced stable performance in all three domains.",
                "Although direct comparisons are difficult to make, the EMT performance displayed notable efficiency vis-`a-vis the POMDP approach.",
                "In spite of a simple and inefficient Matlab implementation of the EMT algorithm, the decision time for any given step averaged significantly below 1 second in all experiments.",
                "For the irregular open arena domain, which proved to be the most difficult, 1000 experiment runs bounded by 100 steps each, a total of 42411 steps, were completed in slightly under 6 hours.",
                "That is, over 4 × 104 online steps took an order of magnitude less time than the offline computation of POMDP policy in [11].",
                "The significance of this differential is made even more prominent by the fact that, should the environment model parameters change, the online nature of EMT would allow it to maintain its performance time, while the POMDP policy would need to be recomputed, requiring yet again a large overhead of computation.",
                "We also tested the behavior cell frequency entropy, empirical measures from trial data.",
                "As Figure 4 and Figure 5 show, empir794 The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) A Q Q A 0 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 A Q Figure 3: These configurations of the Tag Game space were used: a) multiple dead-end, b) irregular open arena, c) circular corridor Table 2: Performance of the EMT-based solution in three Tag Game domains and two observability models: I) omniposition quarry, II) quarry is not at hunters position Model Domain Capture% E(Steps) Time/Step I Dead-ends 100 14.8 72(mSec) Arena 80.2 42.4 500(mSec) Circle 91.4 34.6 187(mSec) II Dead-ends 100 13.2 91(mSec) Arena 96.8 28.67 396(mSec) Circle 94.4 31.63 204(mSec) ical entropy grows with the length of interaction.",
                "For runs where the quarry was not captured immediately, the entropy reaches between 0.85 and 0.952 for different runs and scenarios.",
                "As the agent actively seeks the quarry, the entropy never reaches its maximum.",
                "One characteristic of the entropy graph for the open arena scenario particularly caught our attention in the case of the omniposition quarry observation model.",
                "Near the maximum limit of trial length (100 steps), entropy suddenly dropped.",
                "Further analysis of the data showed that under certain circumstances, a fluctuating behavior occurs in which the agent faces equally viable versions of quarry-following behavior.",
                "Since the EMT algorithm has greedy action selection, and the state space does not encode any form of commitment (not even speed or acceleration), the agent is locked within a small portion of cells.",
                "It is essentially attempting to simultaneously follow several courses of action, all of which are consistent with the target dynamics.",
                "This behavior did not occur in our second observation model, since it significantly reduced the set of eligible courses of action-essentially contributing to tie-breaking among them. 6.",
                "DISCUSSION The design of the EMT solution for the Tag Game exposes the core difference in approach to planning and control between EMT or DBC, on the one hand, and the more familiar POMDP approach, on the other.",
                "POMDP defines a reward structure to optimize, and influences system dynamics indirectly through that optimization.",
                "EMT discards any reward scheme, and instead measures and influences system dynamics directly. 2 Entropy was calculated using log base equal to the number of possible locations within the domain; this properly scales entropy expression into the range [0, 1] for all domains.",
                "Thus for the Tag Game, we did not search for a reward function that would encode and express our preference over the agents behavior, but rather directly set three (heuristic) behavior preferences as the basis for target dynamics to be maintained.",
                "Experimental data shows that these targets need not be directly achievable via the agents actions.",
                "However, the ratio between EMT performance and achievability of target dynamics remains to be explored.",
                "The tag game experiment data also revealed the different emphasis DBC and POMDPs place on the formulation of an environment state space.",
                "POMDPs rely entirely on the mechanism of reward accumulation maximization, i.e., formation of the action selection procedure to achieve necessary state sequencing.",
                "DBC, on the other hand, has two sources of sequencing specification: through the properties of an action selection procedure, and through direct specification within the target dynamics.",
                "The importance of the second source was underlined by the Tag Game experiment data, in which the greedy EMT algorithm, applied to a POMDP-type state space specification, failed, since target description over such a state space was incapable of encoding the necessary behavior tendencies, e.g., tie-breaking and commitment to directed motion.",
                "The structural differences between DBC (and EMT in particular), and POMDPs, prohibits direct performance comparison, and places them on complementary tracks, each within a suitable niche.",
                "For instance, POMDPs could be seen as a much more natural formulation of economic sequential decision-making problems, while EMT is a better fit for continual demand for stochastic change, as happens in many robotic or embodied-agent problems.",
                "The complementary properties of POMDPs and EMT can be further exploited.",
                "There is recent interest in using POMDPs in hybrid solutions [17], in which the POMDPs can be used together with other control approaches to provide results not easily achievable with either approach by itself.",
                "DBC can be an effective partner in such a hybrid solution.",
                "For instance, POMDPs have prohibitively large off-line time requirements for policy computation, but can be readily used in simpler settings to expose beneficial behavioral trends; this can serve as a form of target dynamics that are provided to EMT in a larger domain for on-line operation. 7.",
                "CONCLUSIONS AND FUTURE WORK In this paper, we have presented a novel perspective on the process of planning and control in <br>stochastic environment</br>s, in the form of the Dynamics Based Control (DBC) framework.",
                "DBC formulates the task of planning as support of a specified target system dynamics, which describes the necessary properties of change within the environment.",
                "Optimality of DBC plans of action are measured The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) 795 0 20 40 60 80 100 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1 Steps Entropy Dead−ends 0 20 40 60 80 100 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1 Steps Entropy Arena 0 20 40 60 80 100 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1 Steps Entropy Circle Figure 4: Observation Model I: Omniposition quarry.",
                "Entropy development with length of Tag Game for the three experiment scenarios: a) multiple dead-end, b) irregular open arena, c) circular corridor. 0 10 20 30 40 50 60 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1 Steps Entropy Dead−ends 0 20 40 60 80 100 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1 Steps Entropy Arena 0 20 40 60 80 100 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1 Steps Entropy Circle Figure 5: Observation Model II: quarry not observed at hunters position.",
                "Entropy development with length of Tag Game for the three experiment scenarios: a) multiple dead-end, b) irregular open arena, c) circular corridor. with respect to the deviation of actual system dynamics from the target dynamics.",
                "We show that a recently developed technique of Extended Markov Tracking (EMT) [13] is an instantiation of DBC.",
                "In fact, EMT can be seen as a specific case of DBC parameterization, which employs a greedy action selection procedure.",
                "Since EMT exhibits the key features of the general DBC framework, as well as polynomial time complexity, we used the multitarget version of EMT [15] to demonstrate that the class of area sweeping problems naturally lends itself to dynamics-based descriptions, as instantiated by our experiments in the Tag Game domain.",
                "As enumerated in Section 4.2, EMT has a number of limitations, such as difficulty in dealing with negative dynamic preference.",
                "This prevents direct application of EMT to such problems as Rendezvous-Evasion Games (e.g., [6]).",
                "However, DBC in general has no such limitations, and readily enables the formulation of evasion games.",
                "In future work, we intend to proceed with the development of dynamics-based controllers for these problems. 8.",
                "ACKNOWLEDGMENT The work of the first two authors was partially supported by Israel Science Foundation grant #898/05, and the third author was partially supported by a grant from Israels Ministry of Science and Technology. 9.",
                "REFERENCES [1] R. C. Arkin.",
                "Behavior-Based Robotics.",
                "MIT Press, 1998. [2] J.",
                "A. Bilmes.",
                "A gentle tutorial of the EM algorithm and its application to parameter estimation for Gaussian mixture and Hidden Markov Models.",
                "Technical Report TR-97-021, Department of Electrical Engeineering and Computer Science, University of California at Berkeley, 1998. [3] T. M. Cover and J.",
                "A. Thomas.",
                "Elements of information theory.",
                "Wiley, 1991. [4] M. E. desJardins, E. H. Durfee, C. L. Ortiz, and M. J. Wolverton.",
                "A survey of research in distributed, continual planning.",
                "AI Magazine, 4:13-22, 1999. [5] V. R. Konda and J. N. Tsitsiklis.",
                "Actor-Critic algorithms.",
                "SIAM Journal on Control and Optimization, 42(4):1143-1166, 2003. [6] W. S. Lim.",
                "A rendezvous-evasion game on discrete locations with joint randomization.",
                "Advances in Applied Probability, 29(4):1004-1017, December 1997. [7] M. L. Littman, T. L. Dean, and L. P. Kaelbling.",
                "On the complexity of solving Markov decision problems.",
                "In Proceedings of the 11th Annual Conference on Uncertainty in Artificial Intelligence (UAI-95), pages 394-402, 1995. [8] O. Madani, S. Hanks, and A. Condon.",
                "On the undecidability of probabilistic planning and related stochastic optimization problems.",
                "Artificial Intelligence Journal, 147(1-2):5-34, July 2003. [9] R. M. Neal and G. E. Hinton.",
                "A view of the EM algorithm 796 The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) that justifies incremental, sparse, and other variants.",
                "In M. I. Jordan, editor, Learning in Graphical Models, pages 355-368.",
                "Kluwer Academic Publishers, 1998. [10] P. Paruchuri, M. Tambe, F. Ordonez, and S. Kraus.",
                "Security in multiagent systems by policy randomization.",
                "In Proceeding of AAMAS 2006, 2006. [11] J. Pineau, G. Gordon, and S. Thrun.",
                "Point-based value iteration: An anytime algorithm for pomdps.",
                "In International Joint Conference on Artificial Intelligence (IJCAI), pages 1025-1032, August 2003. [12] M. L. Puterman.",
                "Markov Decision Processes.",
                "Wiley Series in Probability and Mathematical Statistics: Applied Probability and Statistics Section.",
                "Wiley-Interscience Publication, New York, 1994. [13] Z. Rabinovich and J. S. Rosenschein.",
                "Extended Markov Tracking with an application to control.",
                "In The Workshop on Agent Tracking: Modeling Other Agents from Observations, at the Third International Joint Conference on Autonomous Agents and Multiagent Systems, pages 95-100, New-York, July 2004. [14] Z. Rabinovich and J. S. Rosenschein.",
                "Multiagent coordination by Extended Markov Tracking.",
                "In The Fourth International Joint Conference on Autonomous Agents and Multiagent Systems, pages 431-438, Utrecht, The Netherlands, July 2005. [15] Z. Rabinovich and J. S. Rosenschein.",
                "On the response of EMT-based control to interacting targets and models.",
                "In The Fifth International Joint Conference on Autonomous Agents and Multiagent Systems, pages 465-470, Hakodate, Japan, May 2006. [16] R. F. Stengel.",
                "Optimal Control and Estimation.",
                "Dover Publications, 1994. [17] M. Tambe, E. Bowring, H. Jung, G. Kaminka, R. Maheswaran, J. Marecki, J. Modi, R. Nair, J. Pearce, P. Paruchuri, D. Pynadath, P. Scerri, N. Schurr, and P. Varakantham.",
                "Conflicts in teamwork: Hybrids to the The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) 797"
            ],
            "original_annotated_samples": [],
            "translated_annotated_samples": [
                "Control basado en la dinámica con una aplicación a problemas de barrido del área Zinovi Rabinovich Ingeniería y Ciencias de la Computación Universidad Hebrea de Jerusalén Jerusalén, Israel nomad@cs.huji.ac.il Jeffrey S. Rosenschein Ingeniería y ciencias de la computación Universidad Hebrea de Jerusalem Jerusalem, Israel Jeff@@@@@cs.huji.ac.il Gal A. Kaminka El Departamento de Barra de Informática del Grupo Maverick de la Universidad Ilan, israel galk@cs.biu.ac.il Resumen En este documento presentamos el control basado en la dinámica (DBC), un enfoque para la planificación yControl de un agente en \"entorno estocástico\" s.",
                "En los últimos años, los procesos de decisión de Markov parcialmente observables (POMDPS) [12] se han convertido en una base formal popular para la planificación en el \"entorno estocástico\".",
                "Tomamos una visión alternativa de la planificación en el \"entorno estocástico\" s.",
                "Conclusiones y trabajo futuro En este documento, hemos presentado una nueva perspectiva sobre el proceso de planificación y control en el \"entorno estocástico\", en forma del marco de control basado en la dinámica (DBC)."
            ],
            "translated_text": "",
            "candidates": [
                "ambiente estocástico",
                "entorno estocástico",
                "ambiente estocástico",
                "entorno estocástico",
                "ambiente estocástico",
                "entorno estocástico",
                "ambiente estocástico",
                "entorno estocástico"
            ],
            "error": []
        },
        "reward function": {
            "translated_key": "función de recompensa",
            "is_in_text": true,
            "original_annotated_sentences": [
                "Dynamics Based Control with an Application to Area-Sweeping Problems Zinovi Rabinovich Engineering and Computer Science Hebrew University of Jerusalem Jerusalem, Israel nomad@cs.huji.ac.il Jeffrey S. Rosenschein Engineering and Computer Science Hebrew University of Jerusalem Jerusalem, Israel jeff@cs.huji.ac.il Gal A. Kaminka The MAVERICK Group Department of Computer Science Bar Ilan University, Israel galk@cs.biu.ac.il ABSTRACT In this paper we introduce Dynamics Based Control (DBC), an approach to planning and control of an agent in stochastic environments.",
                "Unlike existing approaches, which seek to optimize expected rewards (e.g., in Partially Observable Markov Decision Problems (POMDPs)), DBC optimizes system behavior towards specified system dynamics.",
                "We show that a recently developed planning and control approach, Extended Markov Tracking (EMT) is an instantiation of DBC.",
                "EMT employs greedy action selection to provide an efficient control algorithm in Markovian environments.",
                "We exploit this efficiency in a set of experiments that applied multitarget EMT to a class of area-sweeping problems (searching for moving targets).",
                "We show that such problems can be naturally defined and efficiently solved using the DBC framework, and its EMT instantiation.",
                "Categories and Subject Descriptors I.2.8 [Problem Solving, Control Methods, and Search]: Control Theory; I.2.9 [Robotics]; I.2.11 [Distributed Artificial Intelligence]: Intelligent Agents General Terms Algorithms, Theory 1.",
                "INTRODUCTION Planning and control constitutes a central research area in multiagent systems and artificial intelligence.",
                "In recent years, Partially Observable Markov Decision Processes (POMDPs) [12] have become a popular formal basis for planning in stochastic environments.",
                "In this framework, the planning and control problem is often addressed by imposing a <br>reward function</br>, and computing a policy (of choosing actions) that is optimal, in the sense that it will result in the highest expected utility.",
                "While theoretically attractive, the complexity of optimally solving a POMDP is prohibitive [8, 7].",
                "We take an alternative view of planning in stochastic environments.",
                "We do not use a (state-based) <br>reward function</br>, but instead optimize over a different criterion, a transition-based specification of the desired system dynamics.",
                "The idea here is to view planexecution as a process that compels a (stochastic) system to change, and a plan as a dynamic process that shapes that change according to desired criteria.",
                "We call this general planning framework Dynamics Based Control (DBC).",
                "In DBC, the goal of a planning (or control) process becomes to ensure that the system will change in accordance with specific (potentially stochastic) target dynamics.",
                "As actual system behavior may deviate from that which is specified by target dynamics (due to the stochastic nature of the system), planning in such environments needs to be continual [4], in a manner similar to classical closed-loop controllers [16].",
                "Here, optimality is measured in terms of probability of deviation magnitudes.",
                "In this paper, we present the structure of Dynamics Based Control.",
                "We show that the recently developed Extended Markov Tracking (EMT) approach [13, 14, 15] is subsumed by DBC, with EMT employing greedy action selection, which is a specific parameterization among the options possible within DBC.",
                "EMT is an efficient instantiation of DBC.",
                "To evaluate DBC, we carried out a set of experiments applying multi-target EMT to the Tag Game [11]; this is a variant on the area sweeping problem, where an agent is trying to tag a moving target (quarry) whose position is not known with certainty.",
                "Experimental data demonstrates that even with a simple model of the environment and a simple design of target dynamics, high success rates can be produced both in catching the quarry, and in surprising the quarry (as expressed by the observed entropy of the controlled agents position).",
                "The paper is organized as follows.",
                "In Section 2 we motivate DBC using area-sweeping problems, and discuss related work.",
                "Section 3 introduces the Dynamics Based Control (DBC) structure, and its specialization to Markovian environments.",
                "This is followed by a review of the Extended Markov Tracking (EMT) approach as a DBC-structured control regimen in Section 4.",
                "That section also discusses the limitations of EMT-based control relative to the general DBC framework.",
                "Experimental settings and results are then presented in Section 5.",
                "Section 6 provides a short discussion of the overall approach, and Section 7 gives some concluding remarks and directions for future work. 790 978-81-904262-7-5 (RPS) c 2007 IFAAMAS 2.",
                "MOTIVATION AND RELATED WORK Many real-life scenarios naturally have a stochastic target dynamics specification, especially those domains where there exists no ultimate goal, but rather system behavior (with specific properties) that has to be continually supported.",
                "For example, security guards perform persistent sweeps of an area to detect any sign of intrusion.",
                "Cunning thieves will attempt to track these sweeps, and time their operation to key points of the guards motion.",
                "It is thus advisable to make the guards motion dynamics appear irregular and random.",
                "Recent work by Paruchuri et al. [10] has addressed such randomization in the context of single-agent and distributed POMDPs.",
                "The goal in that work was to generate policies that provide a measure of action-selection randomization, while maintaining rewards within some acceptable levels.",
                "Our focus differs from this work in that DBC does not optimize expected rewards-indeed we do not consider rewards at all-but instead maintains desired dynamics (including, but not limited to, randomization).",
                "The Game of Tag is another example of the applicability of the approach.",
                "It was introduced in the work by Pineau et al. [11].",
                "There are two agents that can move about an area, which is divided into a grid.",
                "The grid may have blocked cells (holes) into which no agent can move.",
                "One agent (the hunter) seeks to move into a cell occupied by the other (the quarry), such that they are co-located (this is a successful tag).",
                "The quarry seeks to avoid the hunter agent, and is always aware of the hunters position, but does not know how the hunter will behave, which opens up the possibility for a hunter to surprise the prey.",
                "The hunter knows the quarrys probabilistic law of motion, but does not know its current location.",
                "Tag is an instance of a family of area-sweeping (pursuit-evasion) problems.",
                "In [11], the hunter modeled the problem using a POMDP.",
                "A <br>reward function</br> was defined, to reflect the desire to tag the quarry, and an action policy was computed to optimize the reward collected over time.",
                "Due to the intractable complexity of determining the optimal policy, the action policy computed in that paper was essentially an approximation.",
                "In this paper, instead of formulating a <br>reward function</br>, we use EMT to solve the problem, by directly specifying the target dynamics.",
                "In fact, any search problem with randomized motion, the socalled class of area sweeping problems, can be described through specification of such target system dynamics.",
                "Dynamics Based Control provides a natural approach to solving these problems. 3.",
                "DYNAMICS BASED CONTROL The specification of Dynamics Based Control (DBC) can be broken into three interacting levels: Environment Design Level, User Level, and Agent Level. • Environment Design Level is concerned with the formal specification and modeling of the environment.",
                "For example, this level would specify the laws of physics within the system, and set its parameters, such as the gravitation constant. • User Level in turn relies on the environment model produced by Environment Design to specify the target system dynamics it wishes to observe.",
                "The User Level also specifies the estimation or learning procedure for system dynamics, and the measure of deviation.",
                "In the museum guard scenario above, these would correspond to a stochastic sweep schedule, and a measure of relative surprise between the specified and actual sweeping. • Agent Level in turn combines the environment model from the Environment Design level, the dynamics estimation procedure, the deviation measure and the target dynamics specification from User Level, to produce a sequence of actions that create system dynamics as close as possible to the targeted specification.",
                "As we are interested in the continual development of a stochastic system, such as happens in classical control theory [16] and continual planning [4], as well as in our example of museum sweeps, the question becomes how the Agent Level is to treat the deviation measurements over time.",
                "To this end, we use a probability threshold-that is, we would like the Agent Level to maximize the probability that the deviation measure will remain below a certain threshold.",
                "Specific action selection then depends on system formalization.",
                "One possibility would be to create a mixture of available system trends, much like that which happens in Behavior-Based Robotic architectures [1].",
                "The other alternative would be to rely on the estimation procedure provided by the User Level-to utilize the Environment Design Level model of the environment to choose actions, so as to manipulate the dynamics estimator into believing that a certain dynamics has been achieved.",
                "Notice that this manipulation is not direct, but via the environment.",
                "Thus, for strong enough estimator algorithms, successful manipulation would mean a successful simulation of the specified target dynamics (i.e., beyond discerning via the available sensory input).",
                "DBC levels can also have a back-flow of information (see Figure 1).",
                "For instance, the Agent Level could provide data about target dynamics feasibility, allowing the User Level to modify the requirement, perhaps focusing on attainable features of system behavior.",
                "Data would also be available about the system response to different actions performed; combined with a dynamics estimator defined by the User Level, this can provide an important tool for the environment model calibration at the Environment Design Level.",
                "UserEnv.",
                "Design Agent Model Ideal Dynamics Estimator Estimator Dynamics Feasibility System Response Data Figure 1: Data flow of the DBC framework Extending upon the idea of Actor-Critic algorithms [5], DBC data flow can provide a good basis for the design of a learning algorithm.",
                "For example, the User Level can operate as an exploratory device for a learning algorithm, inferring an ideal dynamics target from the environment model at hand that would expose and verify most critical features of system behavior.",
                "In this case, feasibility and system response data from the Agent Level would provide key information for an environment model update.",
                "In fact, the combination of feasibility and response data can provide a basis for the application of strong learning algorithms such as EM [2, 9]. 3.1 DBC for Markovian Environments For a Partially Observable Markovian Environment, DBC can be specified in a more rigorous manner.",
                "Notice how DBC discards rewards, and replaces it by another optimality criterion (structural differences are summarized in Table 1): • Environment Design level is to specify a tuple < S, A, T, O, Ω, s0 >, where: - S is the set of all possible environment states; - s0 is the initial state of the environment (which can also be viewed as a probability distribution over S); The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) 791 - A is the set of all possible actions applicable in the environment; - T is the environments probabilistic transition function: T : S ×A → Π(S).",
                "That is, T(s |a, s) is the probability that the environment will move from state s to state s under action a; - O is the set of all possible observations.",
                "This is what the sensor input would look like for an outside observer; - Ω is the observation probability function: Ω : S × A × S → Π(O).",
                "That is, Ω(o|s , a, s) is the probability that one will observe o given that the environment has moved from state s to state s under action a. • User Level, in the case of Markovian environment, operates on the set of system dynamics described by a family of conditional probabilities F = {τ : S × A → Π(S)}.",
                "Thus specification of target dynamics can be expressed by q ∈ F, and the learning or tracking algorithm can be represented as a function L : O×(A×O)∗ → F; that is, it maps sequences of observations and actions performed so far into an estimate τ ∈ F of system dynamics.",
                "There are many possible variations available at the User Level to define divergence between system dynamics; several of them are: - Trace distance or L1 distance between two distributions p and q defined by D(p(·), q(·)) = 1 2 x |p(x) − q(x)| - Fidelity measure of distance F(p(·), q(·)) = x p(x)q(x) - Kullback-Leibler divergence DKL(p(·) q(·)) = x p(x) log p(x) q(x) Notice that the latter two are not actually metrics over the space of possible distributions, but nevertheless have meaningful and important interpretations.",
                "For instance, KullbackLeibler divergence is an important tool of information theory [3] that allows one to measure the price of encoding an information source governed by q, while assuming that it is governed by p. The User Level also defines the threshold of dynamics deviation probability θ. • Agent Level is then faced with a problem of selecting a control signal function a∗ to satisfy a minimization problem as follows: a∗ = arg min a Pr(d(τa, q) > θ) where d(τa, q) is a random variable describing deviation of the dynamics estimate τa, created by L under control signal a, from the ideal dynamics q.",
                "Implicit in this minimization problem is that L is manipulated via the environment, based on the environment model produced by the Environment Design Level. 3.2 DBC View of the State Space It is important to note the complementary view that DBC and POMDPs take on the state space of the environment.",
                "POMDPs regard state as a stationary snap-shot of the environment; whatever attributes of state sequencing one seeks are reached through properties of the control process, in this case reward accumulation.",
                "This can be viewed as if the sequencing of states and the attributes of that sequencing are only introduced by and for the controlling mechanism-the POMDP policy.",
                "DBC concentrates on the underlying principle of state sequencing, the system dynamics.",
                "DBCs target dynamics specification can use the environments state space as a means to describe, discern, and preserve changes that occur within the system.",
                "As a result, DBC has a greater ability to express state sequencing properties, which are grounded in the environment model and its state space definition.",
                "For example, consider the task of moving through rough terrain towards a goal and reaching it as fast as possible.",
                "POMDPs would encode terrain as state space points, while speed would be ensured by negative reward for every step taken without reaching the goalaccumulating higher reward can be reached only by faster motion.",
                "Alternatively, the state space could directly include the notion of speed.",
                "For POMDPs, this would mean that the same concept is encoded twice, in some sense: directly in the state space, and indirectly within reward accumulation.",
                "Now, even if the <br>reward function</br> would encode more, and finer, details of the properties of motion, the POMDP solution will have to search in a much larger space of policies, while still being guided by the implicit concept of the reward accumulation procedure.",
                "On the other hand, the tactical target expression of variations and correlations between position and speed of motion is now grounded in the state space representation.",
                "In this situation, any further constraints, e.g., smoothness of motion, speed limits in different locations, or speed reductions during sharp turns, are explicitly and uniformly expressed by the tactical target, and can result in faster and more effective action selection by a DBC algorithm. 4.",
                "EMT-BASED CONTROL AS A DBC Recently, a control algorithm was introduced called EMT-based Control [13], which instantiates the DBC framework.",
                "Although it provides an approximate greedy solution in the DBC sense, initial experiments using EMT-based control have been encouraging [14, 15].",
                "EMT-based control is based on the Markovian environment definition, as in the case with POMDPs, but its User and Agent Levels are of the Markovian DBC type of optimality. • User Level of EMT-based control defines a limited-case target system dynamics independent of action: qEMT : S → Π(S).",
                "It then utilizes the Kullback-Leibler divergence measure to compose a momentary system dynamics estimator-the Extended Markov Tracking (EMT) algorithm.",
                "The algorithm keeps a system dynamics estimate τt EMT that is capable of explaining recent change in an auxiliary Bayesian system state estimator from pt−1 to pt, and updates it conservatively using Kullback-Leibler divergence.",
                "Since τt EMT and pt−1,t are respectively the conditional and marginal probabilities over the systems state space, explanation simply means that pt(s ) = s τt EMT (s |s)pt−1(s), and the dynamics estimate update is performed by solving a 792 The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) Table 1: Structure of POMDP vs. Dynamics-Based Control in Markovian Environment Level Approach MDP Markovian DBC Environment < S, A, T, O, Ω >,where S - set of states A - set of actions Design T : S × A → Π(S) - transition O - observation set Ω : S × A × S → Π(O) User r : S × A × S → R q : S × A → Π(S) F(π∗ ) → r L(o1, ..., ot) → τ r - <br>reward function</br> q - ideal dynamics F - reward remodeling L - dynamics estimator θ - threshold Agent π∗ = arg max π E[ γt rt] π∗ = arg min π Prob(d(τ q) > θ) minimization problem: τt EMT = H[pt, pt−1, τt−1 EMT ] = arg min τ DKL(τ × pt−1 τt−1 EMT × pt−1) s.t. pt(s ) = s (τ × pt−1)(s , s) pt−1(s) = s (τ × pt−1)(s , s) • Agent Level in EMT-based control is suboptimal with respect to DBC (though it remains within the DBC framework), performing greedy action selection based on prediction of EMTs reaction.",
                "The prediction is based on the environment model provided by the Environment Design level, so that if we denote by Ta the environments transition function limited to action a, and pt−1 is the auxiliary Bayesian system state estimator, then the EMT-based control choice is described by a∗ = arg min a∈A DKL(H[Ta × pt, pt, τt EMT ] qEMT × pt−1) Note that this follows the Markovian DBC framework precisely: the rewarding optimality of POMDPs is discarded, and in its place a dynamics estimator (EMT in this case) is manipulated via action effects on the environment to produce an estimate close to the specified target system dynamics.",
                "Yet as we mentioned, naive EMTbased control is suboptimal in the DBC sense, and has several additional limitations that do not exist in the general DBC framework (discussed in Section 4.2). 4.1 Multi-Target EMT At times, there may exist several behavioral preferences.",
                "For example, in the case of museum guards, some art items are more heavily guarded, requiring that the guards stay more often in their close vicinity.",
                "On the other hand, no corner of the museum is to be left unchecked, which demands constant motion.",
                "Successful museum security would demand that the guards adhere to, and balance, both of these behaviors.",
                "For EMT-based control, this would mean facing several tactical targets {qk}K k=1, and the question becomes how to merge and balance them.",
                "A balancing mechanism can be applied to resolve this issue.",
                "Note that EMT-based control, while selecting an action, creates a preference vector over the set of actions based on their predicted performance with respect to a given target.",
                "If these preference vectors are normalized, they can be combined into a single unified preference.",
                "This requires replacement of standard EMT-based action selection by the algorithm below [15]: • Given: - a set of target dynamics {qk}K k=1, - vector of weights w(k) • Select action as follows - For each action a ∈ A predict the future state distribution ¯pa t+1 = Ta ∗ pt; - For each action, compute Da = H(¯pa t+1, pt, PDt) - For each a ∈ A and qk tactical target, denote V (a, k) = DKL (Da qk) pt .",
                "Let Vk(a) = 1 Zk V (a, k), where Zk = a∈A V (a, k) is a normalization factor. - Select a∗ = arg min a k k=1 w(k)Vk(a) The weights vector w = (w1, ..., wK ) allows the additional tuning of importance among target dynamics without the need to redesign the targets themselves.",
                "This balancing method is also seamlessly integrated into the EMT-based control flow of operation. 4.2 EMT-based Control Limitations EMT-based control is a sub-optimal (in the DBC sense) representative of the DBC structure.",
                "It limits the User by forcing EMT to be its dynamic tracking algorithm, and replaces Agent optimization by greedy action selection.",
                "This kind of combination, however, is common for on-line algorithms.",
                "Although further development of EMT-based controllers is necessary, evidence so far suggests that even the simplest form of the algorithm possesses a great deal of power, and displays trends that are optimal in the DBC sense of the word.",
                "There are two further, EMT-specific, limitations to EMT-based control that are evident at this point.",
                "Both already have partial solutions and are subjects of ongoing research.",
                "The first limitation is the problem of negative preference.",
                "In the POMDP framework for example, this is captured simply, through The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) 793 the appearance of values with different signs within the reward structure.",
                "For EMT-based control, however, negative preference means that one would like to avoid a certain distribution over system development sequences; EMT-based control, however, concentrates on getting as close as possible to a distribution.",
                "Avoidance is thus unnatural in native EMT-based control.",
                "The second limitation comes from the fact that standard environment modeling can create pure sensory actions-actions that do not change the state of the world, and differ only in the way observations are received and the quality of observations received.",
                "Since the world state does not change, EMT-based control would not be able to differentiate between different sensory actions.",
                "Notice that both of these limitations of EMT-based control are absent from the general DBC framework, since it may have a tracking algorithm capable of considering pure sensory actions and, unlike Kullback-Leibler divergence, a distribution deviation measure that is capable of dealing with negative preference. 5.",
                "EMT PLAYING TAG The Game of Tag was first introduced in [11].",
                "It is a single agent problem of capturing a quarry, and belongs to the class of area sweeping problems.",
                "An example domain is shown in Figure 2. 0 51 2 3 4 6 7 8 10 12 13 161514 17 18 19 2221 23 9 11Q A 20 Figure 2: Tag domain; an agent (A) attempts to seek and capture a quarry (Q) The Game of Tag extremely limits the agents perception, so that the agent is able to detect the quarry only if they are co-located in the same cell of the grid world.",
                "In the classical version of the game, co-location leads to a special observation, and the Tag action can be performed.",
                "We slightly modify this setting: the moment that both agents occupy the same cell, the game ends.",
                "As a result, both the agent and its quarry have the same motion capability, which allows them to move in four directions, North, South, East, and West.",
                "These form a formal space of actions within a Markovian environment.",
                "The state space of the formal Markovian environment is described by the cross-product of the agent and quarrys positions.",
                "For Figure 2, it would be S = {s0, ..., s23} × {s0, ..., s23}.",
                "The effects of an action taken by the agent are deterministic, but the environment in general has a stochastic response due to the motion of the quarry.",
                "With probability q0 1 it stays put, and with probability 1 − q0 it moves to an adjacent cell further away from the 1 In our experiments this was taken to be q0 = 0.2. agent.",
                "So for the instance shown in Figure 2 and q0 = 0.1: P(Q = s9|Q = s9, A = s11) = 0.1 P(Q = s2|Q = s9, A = s11) = 0.3 P(Q = s8|Q = s9, A = s11) = 0.3 P(Q = s14|Q = s9, A = s11) = 0.3 Although the evasive behavior of the quarry is known to the agent, the quarrys position is not.",
                "The only sensory information available to the agent is its own location.",
                "We use EMT and directly specify the target dynamics.",
                "For the Game of Tag, we can easily formulate three major trends: catching the quarry, staying mobile, and stalking the quarry.",
                "This results in the following three target dynamics: Tcatch(At+1 = si|Qt = sj, At = sa) ∝ 1 si = sj 0 otherwise Tmobile(At+1 = si|Qt = so, At = sj) ∝ 0 si = sj 1 otherwise Tstalk(At+1 = si|Qt = so, At = sj) ∝ 1 dist(si,so) Note that none of the above targets are directly achievable; for instance, if Qt = s9 and At = s11, there is no action that can move the agent to At+1 = s9 as required by the Tcatch target dynamics.",
                "We ran several experiments to evaluate EMT performance in the Tag Game.",
                "Three configurations of the domain shown in Figure 3 were used, each posing a different challenge to the agent due to partial observability.",
                "In each setting, a set of 1000 runs was performed with a time limit of 100 steps.",
                "In every run, the initial position of both the agent and its quarry was selected at random; this means that as far as the agent was concerned, the quarrys initial position was uniformly distributed over the entire domain cell space.",
                "We also used two variations of the environment observability function.",
                "In the first version, observability function mapped all joint positions of hunter and quarry into the position of the hunter as an observation.",
                "In the second, only those joint positions in which hunter and quarry occupied different locations were mapped into the hunters location.",
                "The second version in fact utilized and expressed the fact that once hunter and quarry occupy the same cell the game ends.",
                "The results of these experiments are shown in Table 2.",
                "Balancing [15] the catch, move, and stalk target dynamics described in the previous section by the weight vector [0.8, 0.1, 0.1], EMT produced stable performance in all three domains.",
                "Although direct comparisons are difficult to make, the EMT performance displayed notable efficiency vis-`a-vis the POMDP approach.",
                "In spite of a simple and inefficient Matlab implementation of the EMT algorithm, the decision time for any given step averaged significantly below 1 second in all experiments.",
                "For the irregular open arena domain, which proved to be the most difficult, 1000 experiment runs bounded by 100 steps each, a total of 42411 steps, were completed in slightly under 6 hours.",
                "That is, over 4 × 104 online steps took an order of magnitude less time than the offline computation of POMDP policy in [11].",
                "The significance of this differential is made even more prominent by the fact that, should the environment model parameters change, the online nature of EMT would allow it to maintain its performance time, while the POMDP policy would need to be recomputed, requiring yet again a large overhead of computation.",
                "We also tested the behavior cell frequency entropy, empirical measures from trial data.",
                "As Figure 4 and Figure 5 show, empir794 The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) A Q Q A 0 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 A Q Figure 3: These configurations of the Tag Game space were used: a) multiple dead-end, b) irregular open arena, c) circular corridor Table 2: Performance of the EMT-based solution in three Tag Game domains and two observability models: I) omniposition quarry, II) quarry is not at hunters position Model Domain Capture% E(Steps) Time/Step I Dead-ends 100 14.8 72(mSec) Arena 80.2 42.4 500(mSec) Circle 91.4 34.6 187(mSec) II Dead-ends 100 13.2 91(mSec) Arena 96.8 28.67 396(mSec) Circle 94.4 31.63 204(mSec) ical entropy grows with the length of interaction.",
                "For runs where the quarry was not captured immediately, the entropy reaches between 0.85 and 0.952 for different runs and scenarios.",
                "As the agent actively seeks the quarry, the entropy never reaches its maximum.",
                "One characteristic of the entropy graph for the open arena scenario particularly caught our attention in the case of the omniposition quarry observation model.",
                "Near the maximum limit of trial length (100 steps), entropy suddenly dropped.",
                "Further analysis of the data showed that under certain circumstances, a fluctuating behavior occurs in which the agent faces equally viable versions of quarry-following behavior.",
                "Since the EMT algorithm has greedy action selection, and the state space does not encode any form of commitment (not even speed or acceleration), the agent is locked within a small portion of cells.",
                "It is essentially attempting to simultaneously follow several courses of action, all of which are consistent with the target dynamics.",
                "This behavior did not occur in our second observation model, since it significantly reduced the set of eligible courses of action-essentially contributing to tie-breaking among them. 6.",
                "DISCUSSION The design of the EMT solution for the Tag Game exposes the core difference in approach to planning and control between EMT or DBC, on the one hand, and the more familiar POMDP approach, on the other.",
                "POMDP defines a reward structure to optimize, and influences system dynamics indirectly through that optimization.",
                "EMT discards any reward scheme, and instead measures and influences system dynamics directly. 2 Entropy was calculated using log base equal to the number of possible locations within the domain; this properly scales entropy expression into the range [0, 1] for all domains.",
                "Thus for the Tag Game, we did not search for a <br>reward function</br> that would encode and express our preference over the agents behavior, but rather directly set three (heuristic) behavior preferences as the basis for target dynamics to be maintained.",
                "Experimental data shows that these targets need not be directly achievable via the agents actions.",
                "However, the ratio between EMT performance and achievability of target dynamics remains to be explored.",
                "The tag game experiment data also revealed the different emphasis DBC and POMDPs place on the formulation of an environment state space.",
                "POMDPs rely entirely on the mechanism of reward accumulation maximization, i.e., formation of the action selection procedure to achieve necessary state sequencing.",
                "DBC, on the other hand, has two sources of sequencing specification: through the properties of an action selection procedure, and through direct specification within the target dynamics.",
                "The importance of the second source was underlined by the Tag Game experiment data, in which the greedy EMT algorithm, applied to a POMDP-type state space specification, failed, since target description over such a state space was incapable of encoding the necessary behavior tendencies, e.g., tie-breaking and commitment to directed motion.",
                "The structural differences between DBC (and EMT in particular), and POMDPs, prohibits direct performance comparison, and places them on complementary tracks, each within a suitable niche.",
                "For instance, POMDPs could be seen as a much more natural formulation of economic sequential decision-making problems, while EMT is a better fit for continual demand for stochastic change, as happens in many robotic or embodied-agent problems.",
                "The complementary properties of POMDPs and EMT can be further exploited.",
                "There is recent interest in using POMDPs in hybrid solutions [17], in which the POMDPs can be used together with other control approaches to provide results not easily achievable with either approach by itself.",
                "DBC can be an effective partner in such a hybrid solution.",
                "For instance, POMDPs have prohibitively large off-line time requirements for policy computation, but can be readily used in simpler settings to expose beneficial behavioral trends; this can serve as a form of target dynamics that are provided to EMT in a larger domain for on-line operation. 7.",
                "CONCLUSIONS AND FUTURE WORK In this paper, we have presented a novel perspective on the process of planning and control in stochastic environments, in the form of the Dynamics Based Control (DBC) framework.",
                "DBC formulates the task of planning as support of a specified target system dynamics, which describes the necessary properties of change within the environment.",
                "Optimality of DBC plans of action are measured The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) 795 0 20 40 60 80 100 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1 Steps Entropy Dead−ends 0 20 40 60 80 100 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1 Steps Entropy Arena 0 20 40 60 80 100 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1 Steps Entropy Circle Figure 4: Observation Model I: Omniposition quarry.",
                "Entropy development with length of Tag Game for the three experiment scenarios: a) multiple dead-end, b) irregular open arena, c) circular corridor. 0 10 20 30 40 50 60 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1 Steps Entropy Dead−ends 0 20 40 60 80 100 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1 Steps Entropy Arena 0 20 40 60 80 100 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1 Steps Entropy Circle Figure 5: Observation Model II: quarry not observed at hunters position.",
                "Entropy development with length of Tag Game for the three experiment scenarios: a) multiple dead-end, b) irregular open arena, c) circular corridor. with respect to the deviation of actual system dynamics from the target dynamics.",
                "We show that a recently developed technique of Extended Markov Tracking (EMT) [13] is an instantiation of DBC.",
                "In fact, EMT can be seen as a specific case of DBC parameterization, which employs a greedy action selection procedure.",
                "Since EMT exhibits the key features of the general DBC framework, as well as polynomial time complexity, we used the multitarget version of EMT [15] to demonstrate that the class of area sweeping problems naturally lends itself to dynamics-based descriptions, as instantiated by our experiments in the Tag Game domain.",
                "As enumerated in Section 4.2, EMT has a number of limitations, such as difficulty in dealing with negative dynamic preference.",
                "This prevents direct application of EMT to such problems as Rendezvous-Evasion Games (e.g., [6]).",
                "However, DBC in general has no such limitations, and readily enables the formulation of evasion games.",
                "In future work, we intend to proceed with the development of dynamics-based controllers for these problems. 8.",
                "ACKNOWLEDGMENT The work of the first two authors was partially supported by Israel Science Foundation grant #898/05, and the third author was partially supported by a grant from Israels Ministry of Science and Technology. 9.",
                "REFERENCES [1] R. C. Arkin.",
                "Behavior-Based Robotics.",
                "MIT Press, 1998. [2] J.",
                "A. Bilmes.",
                "A gentle tutorial of the EM algorithm and its application to parameter estimation for Gaussian mixture and Hidden Markov Models.",
                "Technical Report TR-97-021, Department of Electrical Engeineering and Computer Science, University of California at Berkeley, 1998. [3] T. M. Cover and J.",
                "A. Thomas.",
                "Elements of information theory.",
                "Wiley, 1991. [4] M. E. desJardins, E. H. Durfee, C. L. Ortiz, and M. J. Wolverton.",
                "A survey of research in distributed, continual planning.",
                "AI Magazine, 4:13-22, 1999. [5] V. R. Konda and J. N. Tsitsiklis.",
                "Actor-Critic algorithms.",
                "SIAM Journal on Control and Optimization, 42(4):1143-1166, 2003. [6] W. S. Lim.",
                "A rendezvous-evasion game on discrete locations with joint randomization.",
                "Advances in Applied Probability, 29(4):1004-1017, December 1997. [7] M. L. Littman, T. L. Dean, and L. P. Kaelbling.",
                "On the complexity of solving Markov decision problems.",
                "In Proceedings of the 11th Annual Conference on Uncertainty in Artificial Intelligence (UAI-95), pages 394-402, 1995. [8] O. Madani, S. Hanks, and A. Condon.",
                "On the undecidability of probabilistic planning and related stochastic optimization problems.",
                "Artificial Intelligence Journal, 147(1-2):5-34, July 2003. [9] R. M. Neal and G. E. Hinton.",
                "A view of the EM algorithm 796 The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) that justifies incremental, sparse, and other variants.",
                "In M. I. Jordan, editor, Learning in Graphical Models, pages 355-368.",
                "Kluwer Academic Publishers, 1998. [10] P. Paruchuri, M. Tambe, F. Ordonez, and S. Kraus.",
                "Security in multiagent systems by policy randomization.",
                "In Proceeding of AAMAS 2006, 2006. [11] J. Pineau, G. Gordon, and S. Thrun.",
                "Point-based value iteration: An anytime algorithm for pomdps.",
                "In International Joint Conference on Artificial Intelligence (IJCAI), pages 1025-1032, August 2003. [12] M. L. Puterman.",
                "Markov Decision Processes.",
                "Wiley Series in Probability and Mathematical Statistics: Applied Probability and Statistics Section.",
                "Wiley-Interscience Publication, New York, 1994. [13] Z. Rabinovich and J. S. Rosenschein.",
                "Extended Markov Tracking with an application to control.",
                "In The Workshop on Agent Tracking: Modeling Other Agents from Observations, at the Third International Joint Conference on Autonomous Agents and Multiagent Systems, pages 95-100, New-York, July 2004. [14] Z. Rabinovich and J. S. Rosenschein.",
                "Multiagent coordination by Extended Markov Tracking.",
                "In The Fourth International Joint Conference on Autonomous Agents and Multiagent Systems, pages 431-438, Utrecht, The Netherlands, July 2005. [15] Z. Rabinovich and J. S. Rosenschein.",
                "On the response of EMT-based control to interacting targets and models.",
                "In The Fifth International Joint Conference on Autonomous Agents and Multiagent Systems, pages 465-470, Hakodate, Japan, May 2006. [16] R. F. Stengel.",
                "Optimal Control and Estimation.",
                "Dover Publications, 1994. [17] M. Tambe, E. Bowring, H. Jung, G. Kaminka, R. Maheswaran, J. Marecki, J. Modi, R. Nair, J. Pearce, P. Paruchuri, D. Pynadath, P. Scerri, N. Schurr, and P. Varakantham.",
                "Conflicts in teamwork: Hybrids to the The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) 797"
            ],
            "original_annotated_samples": [],
            "translated_annotated_samples": [
                "En este marco, el problema de planificación y control a menudo se aborda imponiendo una \"función de recompensa\" y calculando una política (de elegir acciones) que sea óptima, en el sentido de que dará como resultado la utilidad más alta esperada.",
                "No utilizamos una \"función de recompensa\" (basada en el estado), sino que optimizamos sobre un criterio diferente, una especificación basada en la transición de la dinámica del sistema deseada.",
                "Se definió una \"función de recompensa\" para reflejar el deseo de etiquetar la cantera, y se calculó una política de acción para optimizar la recompensa recopilada con el tiempo.",
                "En este artículo, en lugar de formular una \"función de recompensa\", usamos EMT para resolver el problema, especificando directamente la dinámica de destino.",
                "Ahora, incluso si la \"función de recompensa\" codificaría más, y más finos detalles de las propiedades del movimiento, la solución POMDP tendrá que buscar en un espacio de políticas mucho más grande, al tiempo que se guía por el concepto implícito de la acumulación de recompensasprocedimiento.",
                "Conf.en agentes autónomos y sistemas de múltiples agentes (AAMAS 07) Tabla 1: Estructura de POMDP versus control basado en la dinámica en el enfoque de nivel de entorno de Markovian MDP Markovian DBC Environment <S, A, T, O, Ω>, donde s-conjunto de un conjunto de conjuntos deEstados A - Conjunto de Acciones Diseño T: S × A → π (S) - Transición O - Conjunto de observación ω: S × A × S → π (O) Usuario R: S × A × S → R Q: S × A→ π (s) f (π ∗) → r l (o1, ..., ot) → τ r - \"función de recompensa\" Q - dinámica ideal f - remodelación de recompensa l - estimador de dinámica θ - agente de umbral π ∗ = arg = argmax π e [γt rt] π ∗ = arg min π prob (d (τ q)> θ) Problema de minimización: τt emt = h [pt, pt - 1, τt - 1 emt] = arg min τ dkl (τ ×PT - 1 τt - 1 EMT × PT - 1) S.T.pt (s) = S (τ × pt-1) (s, s) pt-1 (s) = s (τ × pt-1) (s, s) • El nivel de agente en el control basado en EMT es subóptimo con respetoPara DBC (aunque permanece dentro del marco DBC), realizando una selección de acción codiciosa basada en la predicción de la reacción de EMTS.",
                "Por lo tanto, para el juego de etiqueta, no buscamos una \"función de recompensa\" que codifique y expresara nuestra preferencia sobre el comportamiento de los agentes, sino que estableciera directamente tres preferencias de comportamiento (heurística) como base para que se mantenga la dinámica objetivo."
            ],
            "translated_text": "",
            "candidates": [
                "función de recompensa",
                "función de recompensa",
                "función de recompensa",
                "función de recompensa",
                "función de recompensa",
                "función de recompensa",
                "función de recompensa",
                "función de recompensa",
                "función de recompensa",
                "función de recompensa",
                "función de recompensa",
                "función de recompensa",
                "función de recompensa",
                "función de recompensa"
            ],
            "error": []
        },
        "partially observable markov decision problem": {
            "translated_key": "Problema de decisión de Markov parcialmente observable",
            "is_in_text": false,
            "original_annotated_sentences": [
                "Dynamics Based Control with an Application to Area-Sweeping Problems Zinovi Rabinovich Engineering and Computer Science Hebrew University of Jerusalem Jerusalem, Israel nomad@cs.huji.ac.il Jeffrey S. Rosenschein Engineering and Computer Science Hebrew University of Jerusalem Jerusalem, Israel jeff@cs.huji.ac.il Gal A. Kaminka The MAVERICK Group Department of Computer Science Bar Ilan University, Israel galk@cs.biu.ac.il ABSTRACT In this paper we introduce Dynamics Based Control (DBC), an approach to planning and control of an agent in stochastic environments.",
                "Unlike existing approaches, which seek to optimize expected rewards (e.g., in Partially Observable Markov Decision Problems (POMDPs)), DBC optimizes system behavior towards specified system dynamics.",
                "We show that a recently developed planning and control approach, Extended Markov Tracking (EMT) is an instantiation of DBC.",
                "EMT employs greedy action selection to provide an efficient control algorithm in Markovian environments.",
                "We exploit this efficiency in a set of experiments that applied multitarget EMT to a class of area-sweeping problems (searching for moving targets).",
                "We show that such problems can be naturally defined and efficiently solved using the DBC framework, and its EMT instantiation.",
                "Categories and Subject Descriptors I.2.8 [Problem Solving, Control Methods, and Search]: Control Theory; I.2.9 [Robotics]; I.2.11 [Distributed Artificial Intelligence]: Intelligent Agents General Terms Algorithms, Theory 1.",
                "INTRODUCTION Planning and control constitutes a central research area in multiagent systems and artificial intelligence.",
                "In recent years, Partially Observable Markov Decision Processes (POMDPs) [12] have become a popular formal basis for planning in stochastic environments.",
                "In this framework, the planning and control problem is often addressed by imposing a reward function, and computing a policy (of choosing actions) that is optimal, in the sense that it will result in the highest expected utility.",
                "While theoretically attractive, the complexity of optimally solving a POMDP is prohibitive [8, 7].",
                "We take an alternative view of planning in stochastic environments.",
                "We do not use a (state-based) reward function, but instead optimize over a different criterion, a transition-based specification of the desired system dynamics.",
                "The idea here is to view planexecution as a process that compels a (stochastic) system to change, and a plan as a dynamic process that shapes that change according to desired criteria.",
                "We call this general planning framework Dynamics Based Control (DBC).",
                "In DBC, the goal of a planning (or control) process becomes to ensure that the system will change in accordance with specific (potentially stochastic) target dynamics.",
                "As actual system behavior may deviate from that which is specified by target dynamics (due to the stochastic nature of the system), planning in such environments needs to be continual [4], in a manner similar to classical closed-loop controllers [16].",
                "Here, optimality is measured in terms of probability of deviation magnitudes.",
                "In this paper, we present the structure of Dynamics Based Control.",
                "We show that the recently developed Extended Markov Tracking (EMT) approach [13, 14, 15] is subsumed by DBC, with EMT employing greedy action selection, which is a specific parameterization among the options possible within DBC.",
                "EMT is an efficient instantiation of DBC.",
                "To evaluate DBC, we carried out a set of experiments applying multi-target EMT to the Tag Game [11]; this is a variant on the area sweeping problem, where an agent is trying to tag a moving target (quarry) whose position is not known with certainty.",
                "Experimental data demonstrates that even with a simple model of the environment and a simple design of target dynamics, high success rates can be produced both in catching the quarry, and in surprising the quarry (as expressed by the observed entropy of the controlled agents position).",
                "The paper is organized as follows.",
                "In Section 2 we motivate DBC using area-sweeping problems, and discuss related work.",
                "Section 3 introduces the Dynamics Based Control (DBC) structure, and its specialization to Markovian environments.",
                "This is followed by a review of the Extended Markov Tracking (EMT) approach as a DBC-structured control regimen in Section 4.",
                "That section also discusses the limitations of EMT-based control relative to the general DBC framework.",
                "Experimental settings and results are then presented in Section 5.",
                "Section 6 provides a short discussion of the overall approach, and Section 7 gives some concluding remarks and directions for future work. 790 978-81-904262-7-5 (RPS) c 2007 IFAAMAS 2.",
                "MOTIVATION AND RELATED WORK Many real-life scenarios naturally have a stochastic target dynamics specification, especially those domains where there exists no ultimate goal, but rather system behavior (with specific properties) that has to be continually supported.",
                "For example, security guards perform persistent sweeps of an area to detect any sign of intrusion.",
                "Cunning thieves will attempt to track these sweeps, and time their operation to key points of the guards motion.",
                "It is thus advisable to make the guards motion dynamics appear irregular and random.",
                "Recent work by Paruchuri et al. [10] has addressed such randomization in the context of single-agent and distributed POMDPs.",
                "The goal in that work was to generate policies that provide a measure of action-selection randomization, while maintaining rewards within some acceptable levels.",
                "Our focus differs from this work in that DBC does not optimize expected rewards-indeed we do not consider rewards at all-but instead maintains desired dynamics (including, but not limited to, randomization).",
                "The Game of Tag is another example of the applicability of the approach.",
                "It was introduced in the work by Pineau et al. [11].",
                "There are two agents that can move about an area, which is divided into a grid.",
                "The grid may have blocked cells (holes) into which no agent can move.",
                "One agent (the hunter) seeks to move into a cell occupied by the other (the quarry), such that they are co-located (this is a successful tag).",
                "The quarry seeks to avoid the hunter agent, and is always aware of the hunters position, but does not know how the hunter will behave, which opens up the possibility for a hunter to surprise the prey.",
                "The hunter knows the quarrys probabilistic law of motion, but does not know its current location.",
                "Tag is an instance of a family of area-sweeping (pursuit-evasion) problems.",
                "In [11], the hunter modeled the problem using a POMDP.",
                "A reward function was defined, to reflect the desire to tag the quarry, and an action policy was computed to optimize the reward collected over time.",
                "Due to the intractable complexity of determining the optimal policy, the action policy computed in that paper was essentially an approximation.",
                "In this paper, instead of formulating a reward function, we use EMT to solve the problem, by directly specifying the target dynamics.",
                "In fact, any search problem with randomized motion, the socalled class of area sweeping problems, can be described through specification of such target system dynamics.",
                "Dynamics Based Control provides a natural approach to solving these problems. 3.",
                "DYNAMICS BASED CONTROL The specification of Dynamics Based Control (DBC) can be broken into three interacting levels: Environment Design Level, User Level, and Agent Level. • Environment Design Level is concerned with the formal specification and modeling of the environment.",
                "For example, this level would specify the laws of physics within the system, and set its parameters, such as the gravitation constant. • User Level in turn relies on the environment model produced by Environment Design to specify the target system dynamics it wishes to observe.",
                "The User Level also specifies the estimation or learning procedure for system dynamics, and the measure of deviation.",
                "In the museum guard scenario above, these would correspond to a stochastic sweep schedule, and a measure of relative surprise between the specified and actual sweeping. • Agent Level in turn combines the environment model from the Environment Design level, the dynamics estimation procedure, the deviation measure and the target dynamics specification from User Level, to produce a sequence of actions that create system dynamics as close as possible to the targeted specification.",
                "As we are interested in the continual development of a stochastic system, such as happens in classical control theory [16] and continual planning [4], as well as in our example of museum sweeps, the question becomes how the Agent Level is to treat the deviation measurements over time.",
                "To this end, we use a probability threshold-that is, we would like the Agent Level to maximize the probability that the deviation measure will remain below a certain threshold.",
                "Specific action selection then depends on system formalization.",
                "One possibility would be to create a mixture of available system trends, much like that which happens in Behavior-Based Robotic architectures [1].",
                "The other alternative would be to rely on the estimation procedure provided by the User Level-to utilize the Environment Design Level model of the environment to choose actions, so as to manipulate the dynamics estimator into believing that a certain dynamics has been achieved.",
                "Notice that this manipulation is not direct, but via the environment.",
                "Thus, for strong enough estimator algorithms, successful manipulation would mean a successful simulation of the specified target dynamics (i.e., beyond discerning via the available sensory input).",
                "DBC levels can also have a back-flow of information (see Figure 1).",
                "For instance, the Agent Level could provide data about target dynamics feasibility, allowing the User Level to modify the requirement, perhaps focusing on attainable features of system behavior.",
                "Data would also be available about the system response to different actions performed; combined with a dynamics estimator defined by the User Level, this can provide an important tool for the environment model calibration at the Environment Design Level.",
                "UserEnv.",
                "Design Agent Model Ideal Dynamics Estimator Estimator Dynamics Feasibility System Response Data Figure 1: Data flow of the DBC framework Extending upon the idea of Actor-Critic algorithms [5], DBC data flow can provide a good basis for the design of a learning algorithm.",
                "For example, the User Level can operate as an exploratory device for a learning algorithm, inferring an ideal dynamics target from the environment model at hand that would expose and verify most critical features of system behavior.",
                "In this case, feasibility and system response data from the Agent Level would provide key information for an environment model update.",
                "In fact, the combination of feasibility and response data can provide a basis for the application of strong learning algorithms such as EM [2, 9]. 3.1 DBC for Markovian Environments For a Partially Observable Markovian Environment, DBC can be specified in a more rigorous manner.",
                "Notice how DBC discards rewards, and replaces it by another optimality criterion (structural differences are summarized in Table 1): • Environment Design level is to specify a tuple < S, A, T, O, Ω, s0 >, where: - S is the set of all possible environment states; - s0 is the initial state of the environment (which can also be viewed as a probability distribution over S); The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) 791 - A is the set of all possible actions applicable in the environment; - T is the environments probabilistic transition function: T : S ×A → Π(S).",
                "That is, T(s |a, s) is the probability that the environment will move from state s to state s under action a; - O is the set of all possible observations.",
                "This is what the sensor input would look like for an outside observer; - Ω is the observation probability function: Ω : S × A × S → Π(O).",
                "That is, Ω(o|s , a, s) is the probability that one will observe o given that the environment has moved from state s to state s under action a. • User Level, in the case of Markovian environment, operates on the set of system dynamics described by a family of conditional probabilities F = {τ : S × A → Π(S)}.",
                "Thus specification of target dynamics can be expressed by q ∈ F, and the learning or tracking algorithm can be represented as a function L : O×(A×O)∗ → F; that is, it maps sequences of observations and actions performed so far into an estimate τ ∈ F of system dynamics.",
                "There are many possible variations available at the User Level to define divergence between system dynamics; several of them are: - Trace distance or L1 distance between two distributions p and q defined by D(p(·), q(·)) = 1 2 x |p(x) − q(x)| - Fidelity measure of distance F(p(·), q(·)) = x p(x)q(x) - Kullback-Leibler divergence DKL(p(·) q(·)) = x p(x) log p(x) q(x) Notice that the latter two are not actually metrics over the space of possible distributions, but nevertheless have meaningful and important interpretations.",
                "For instance, KullbackLeibler divergence is an important tool of information theory [3] that allows one to measure the price of encoding an information source governed by q, while assuming that it is governed by p. The User Level also defines the threshold of dynamics deviation probability θ. • Agent Level is then faced with a problem of selecting a control signal function a∗ to satisfy a minimization problem as follows: a∗ = arg min a Pr(d(τa, q) > θ) where d(τa, q) is a random variable describing deviation of the dynamics estimate τa, created by L under control signal a, from the ideal dynamics q.",
                "Implicit in this minimization problem is that L is manipulated via the environment, based on the environment model produced by the Environment Design Level. 3.2 DBC View of the State Space It is important to note the complementary view that DBC and POMDPs take on the state space of the environment.",
                "POMDPs regard state as a stationary snap-shot of the environment; whatever attributes of state sequencing one seeks are reached through properties of the control process, in this case reward accumulation.",
                "This can be viewed as if the sequencing of states and the attributes of that sequencing are only introduced by and for the controlling mechanism-the POMDP policy.",
                "DBC concentrates on the underlying principle of state sequencing, the system dynamics.",
                "DBCs target dynamics specification can use the environments state space as a means to describe, discern, and preserve changes that occur within the system.",
                "As a result, DBC has a greater ability to express state sequencing properties, which are grounded in the environment model and its state space definition.",
                "For example, consider the task of moving through rough terrain towards a goal and reaching it as fast as possible.",
                "POMDPs would encode terrain as state space points, while speed would be ensured by negative reward for every step taken without reaching the goalaccumulating higher reward can be reached only by faster motion.",
                "Alternatively, the state space could directly include the notion of speed.",
                "For POMDPs, this would mean that the same concept is encoded twice, in some sense: directly in the state space, and indirectly within reward accumulation.",
                "Now, even if the reward function would encode more, and finer, details of the properties of motion, the POMDP solution will have to search in a much larger space of policies, while still being guided by the implicit concept of the reward accumulation procedure.",
                "On the other hand, the tactical target expression of variations and correlations between position and speed of motion is now grounded in the state space representation.",
                "In this situation, any further constraints, e.g., smoothness of motion, speed limits in different locations, or speed reductions during sharp turns, are explicitly and uniformly expressed by the tactical target, and can result in faster and more effective action selection by a DBC algorithm. 4.",
                "EMT-BASED CONTROL AS A DBC Recently, a control algorithm was introduced called EMT-based Control [13], which instantiates the DBC framework.",
                "Although it provides an approximate greedy solution in the DBC sense, initial experiments using EMT-based control have been encouraging [14, 15].",
                "EMT-based control is based on the Markovian environment definition, as in the case with POMDPs, but its User and Agent Levels are of the Markovian DBC type of optimality. • User Level of EMT-based control defines a limited-case target system dynamics independent of action: qEMT : S → Π(S).",
                "It then utilizes the Kullback-Leibler divergence measure to compose a momentary system dynamics estimator-the Extended Markov Tracking (EMT) algorithm.",
                "The algorithm keeps a system dynamics estimate τt EMT that is capable of explaining recent change in an auxiliary Bayesian system state estimator from pt−1 to pt, and updates it conservatively using Kullback-Leibler divergence.",
                "Since τt EMT and pt−1,t are respectively the conditional and marginal probabilities over the systems state space, explanation simply means that pt(s ) = s τt EMT (s |s)pt−1(s), and the dynamics estimate update is performed by solving a 792 The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) Table 1: Structure of POMDP vs. Dynamics-Based Control in Markovian Environment Level Approach MDP Markovian DBC Environment < S, A, T, O, Ω >,where S - set of states A - set of actions Design T : S × A → Π(S) - transition O - observation set Ω : S × A × S → Π(O) User r : S × A × S → R q : S × A → Π(S) F(π∗ ) → r L(o1, ..., ot) → τ r - reward function q - ideal dynamics F - reward remodeling L - dynamics estimator θ - threshold Agent π∗ = arg max π E[ γt rt] π∗ = arg min π Prob(d(τ q) > θ) minimization problem: τt EMT = H[pt, pt−1, τt−1 EMT ] = arg min τ DKL(τ × pt−1 τt−1 EMT × pt−1) s.t. pt(s ) = s (τ × pt−1)(s , s) pt−1(s) = s (τ × pt−1)(s , s) • Agent Level in EMT-based control is suboptimal with respect to DBC (though it remains within the DBC framework), performing greedy action selection based on prediction of EMTs reaction.",
                "The prediction is based on the environment model provided by the Environment Design level, so that if we denote by Ta the environments transition function limited to action a, and pt−1 is the auxiliary Bayesian system state estimator, then the EMT-based control choice is described by a∗ = arg min a∈A DKL(H[Ta × pt, pt, τt EMT ] qEMT × pt−1) Note that this follows the Markovian DBC framework precisely: the rewarding optimality of POMDPs is discarded, and in its place a dynamics estimator (EMT in this case) is manipulated via action effects on the environment to produce an estimate close to the specified target system dynamics.",
                "Yet as we mentioned, naive EMTbased control is suboptimal in the DBC sense, and has several additional limitations that do not exist in the general DBC framework (discussed in Section 4.2). 4.1 Multi-Target EMT At times, there may exist several behavioral preferences.",
                "For example, in the case of museum guards, some art items are more heavily guarded, requiring that the guards stay more often in their close vicinity.",
                "On the other hand, no corner of the museum is to be left unchecked, which demands constant motion.",
                "Successful museum security would demand that the guards adhere to, and balance, both of these behaviors.",
                "For EMT-based control, this would mean facing several tactical targets {qk}K k=1, and the question becomes how to merge and balance them.",
                "A balancing mechanism can be applied to resolve this issue.",
                "Note that EMT-based control, while selecting an action, creates a preference vector over the set of actions based on their predicted performance with respect to a given target.",
                "If these preference vectors are normalized, they can be combined into a single unified preference.",
                "This requires replacement of standard EMT-based action selection by the algorithm below [15]: • Given: - a set of target dynamics {qk}K k=1, - vector of weights w(k) • Select action as follows - For each action a ∈ A predict the future state distribution ¯pa t+1 = Ta ∗ pt; - For each action, compute Da = H(¯pa t+1, pt, PDt) - For each a ∈ A and qk tactical target, denote V (a, k) = DKL (Da qk) pt .",
                "Let Vk(a) = 1 Zk V (a, k), where Zk = a∈A V (a, k) is a normalization factor. - Select a∗ = arg min a k k=1 w(k)Vk(a) The weights vector w = (w1, ..., wK ) allows the additional tuning of importance among target dynamics without the need to redesign the targets themselves.",
                "This balancing method is also seamlessly integrated into the EMT-based control flow of operation. 4.2 EMT-based Control Limitations EMT-based control is a sub-optimal (in the DBC sense) representative of the DBC structure.",
                "It limits the User by forcing EMT to be its dynamic tracking algorithm, and replaces Agent optimization by greedy action selection.",
                "This kind of combination, however, is common for on-line algorithms.",
                "Although further development of EMT-based controllers is necessary, evidence so far suggests that even the simplest form of the algorithm possesses a great deal of power, and displays trends that are optimal in the DBC sense of the word.",
                "There are two further, EMT-specific, limitations to EMT-based control that are evident at this point.",
                "Both already have partial solutions and are subjects of ongoing research.",
                "The first limitation is the problem of negative preference.",
                "In the POMDP framework for example, this is captured simply, through The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) 793 the appearance of values with different signs within the reward structure.",
                "For EMT-based control, however, negative preference means that one would like to avoid a certain distribution over system development sequences; EMT-based control, however, concentrates on getting as close as possible to a distribution.",
                "Avoidance is thus unnatural in native EMT-based control.",
                "The second limitation comes from the fact that standard environment modeling can create pure sensory actions-actions that do not change the state of the world, and differ only in the way observations are received and the quality of observations received.",
                "Since the world state does not change, EMT-based control would not be able to differentiate between different sensory actions.",
                "Notice that both of these limitations of EMT-based control are absent from the general DBC framework, since it may have a tracking algorithm capable of considering pure sensory actions and, unlike Kullback-Leibler divergence, a distribution deviation measure that is capable of dealing with negative preference. 5.",
                "EMT PLAYING TAG The Game of Tag was first introduced in [11].",
                "It is a single agent problem of capturing a quarry, and belongs to the class of area sweeping problems.",
                "An example domain is shown in Figure 2. 0 51 2 3 4 6 7 8 10 12 13 161514 17 18 19 2221 23 9 11Q A 20 Figure 2: Tag domain; an agent (A) attempts to seek and capture a quarry (Q) The Game of Tag extremely limits the agents perception, so that the agent is able to detect the quarry only if they are co-located in the same cell of the grid world.",
                "In the classical version of the game, co-location leads to a special observation, and the Tag action can be performed.",
                "We slightly modify this setting: the moment that both agents occupy the same cell, the game ends.",
                "As a result, both the agent and its quarry have the same motion capability, which allows them to move in four directions, North, South, East, and West.",
                "These form a formal space of actions within a Markovian environment.",
                "The state space of the formal Markovian environment is described by the cross-product of the agent and quarrys positions.",
                "For Figure 2, it would be S = {s0, ..., s23} × {s0, ..., s23}.",
                "The effects of an action taken by the agent are deterministic, but the environment in general has a stochastic response due to the motion of the quarry.",
                "With probability q0 1 it stays put, and with probability 1 − q0 it moves to an adjacent cell further away from the 1 In our experiments this was taken to be q0 = 0.2. agent.",
                "So for the instance shown in Figure 2 and q0 = 0.1: P(Q = s9|Q = s9, A = s11) = 0.1 P(Q = s2|Q = s9, A = s11) = 0.3 P(Q = s8|Q = s9, A = s11) = 0.3 P(Q = s14|Q = s9, A = s11) = 0.3 Although the evasive behavior of the quarry is known to the agent, the quarrys position is not.",
                "The only sensory information available to the agent is its own location.",
                "We use EMT and directly specify the target dynamics.",
                "For the Game of Tag, we can easily formulate three major trends: catching the quarry, staying mobile, and stalking the quarry.",
                "This results in the following three target dynamics: Tcatch(At+1 = si|Qt = sj, At = sa) ∝ 1 si = sj 0 otherwise Tmobile(At+1 = si|Qt = so, At = sj) ∝ 0 si = sj 1 otherwise Tstalk(At+1 = si|Qt = so, At = sj) ∝ 1 dist(si,so) Note that none of the above targets are directly achievable; for instance, if Qt = s9 and At = s11, there is no action that can move the agent to At+1 = s9 as required by the Tcatch target dynamics.",
                "We ran several experiments to evaluate EMT performance in the Tag Game.",
                "Three configurations of the domain shown in Figure 3 were used, each posing a different challenge to the agent due to partial observability.",
                "In each setting, a set of 1000 runs was performed with a time limit of 100 steps.",
                "In every run, the initial position of both the agent and its quarry was selected at random; this means that as far as the agent was concerned, the quarrys initial position was uniformly distributed over the entire domain cell space.",
                "We also used two variations of the environment observability function.",
                "In the first version, observability function mapped all joint positions of hunter and quarry into the position of the hunter as an observation.",
                "In the second, only those joint positions in which hunter and quarry occupied different locations were mapped into the hunters location.",
                "The second version in fact utilized and expressed the fact that once hunter and quarry occupy the same cell the game ends.",
                "The results of these experiments are shown in Table 2.",
                "Balancing [15] the catch, move, and stalk target dynamics described in the previous section by the weight vector [0.8, 0.1, 0.1], EMT produced stable performance in all three domains.",
                "Although direct comparisons are difficult to make, the EMT performance displayed notable efficiency vis-`a-vis the POMDP approach.",
                "In spite of a simple and inefficient Matlab implementation of the EMT algorithm, the decision time for any given step averaged significantly below 1 second in all experiments.",
                "For the irregular open arena domain, which proved to be the most difficult, 1000 experiment runs bounded by 100 steps each, a total of 42411 steps, were completed in slightly under 6 hours.",
                "That is, over 4 × 104 online steps took an order of magnitude less time than the offline computation of POMDP policy in [11].",
                "The significance of this differential is made even more prominent by the fact that, should the environment model parameters change, the online nature of EMT would allow it to maintain its performance time, while the POMDP policy would need to be recomputed, requiring yet again a large overhead of computation.",
                "We also tested the behavior cell frequency entropy, empirical measures from trial data.",
                "As Figure 4 and Figure 5 show, empir794 The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) A Q Q A 0 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 A Q Figure 3: These configurations of the Tag Game space were used: a) multiple dead-end, b) irregular open arena, c) circular corridor Table 2: Performance of the EMT-based solution in three Tag Game domains and two observability models: I) omniposition quarry, II) quarry is not at hunters position Model Domain Capture% E(Steps) Time/Step I Dead-ends 100 14.8 72(mSec) Arena 80.2 42.4 500(mSec) Circle 91.4 34.6 187(mSec) II Dead-ends 100 13.2 91(mSec) Arena 96.8 28.67 396(mSec) Circle 94.4 31.63 204(mSec) ical entropy grows with the length of interaction.",
                "For runs where the quarry was not captured immediately, the entropy reaches between 0.85 and 0.952 for different runs and scenarios.",
                "As the agent actively seeks the quarry, the entropy never reaches its maximum.",
                "One characteristic of the entropy graph for the open arena scenario particularly caught our attention in the case of the omniposition quarry observation model.",
                "Near the maximum limit of trial length (100 steps), entropy suddenly dropped.",
                "Further analysis of the data showed that under certain circumstances, a fluctuating behavior occurs in which the agent faces equally viable versions of quarry-following behavior.",
                "Since the EMT algorithm has greedy action selection, and the state space does not encode any form of commitment (not even speed or acceleration), the agent is locked within a small portion of cells.",
                "It is essentially attempting to simultaneously follow several courses of action, all of which are consistent with the target dynamics.",
                "This behavior did not occur in our second observation model, since it significantly reduced the set of eligible courses of action-essentially contributing to tie-breaking among them. 6.",
                "DISCUSSION The design of the EMT solution for the Tag Game exposes the core difference in approach to planning and control between EMT or DBC, on the one hand, and the more familiar POMDP approach, on the other.",
                "POMDP defines a reward structure to optimize, and influences system dynamics indirectly through that optimization.",
                "EMT discards any reward scheme, and instead measures and influences system dynamics directly. 2 Entropy was calculated using log base equal to the number of possible locations within the domain; this properly scales entropy expression into the range [0, 1] for all domains.",
                "Thus for the Tag Game, we did not search for a reward function that would encode and express our preference over the agents behavior, but rather directly set three (heuristic) behavior preferences as the basis for target dynamics to be maintained.",
                "Experimental data shows that these targets need not be directly achievable via the agents actions.",
                "However, the ratio between EMT performance and achievability of target dynamics remains to be explored.",
                "The tag game experiment data also revealed the different emphasis DBC and POMDPs place on the formulation of an environment state space.",
                "POMDPs rely entirely on the mechanism of reward accumulation maximization, i.e., formation of the action selection procedure to achieve necessary state sequencing.",
                "DBC, on the other hand, has two sources of sequencing specification: through the properties of an action selection procedure, and through direct specification within the target dynamics.",
                "The importance of the second source was underlined by the Tag Game experiment data, in which the greedy EMT algorithm, applied to a POMDP-type state space specification, failed, since target description over such a state space was incapable of encoding the necessary behavior tendencies, e.g., tie-breaking and commitment to directed motion.",
                "The structural differences between DBC (and EMT in particular), and POMDPs, prohibits direct performance comparison, and places them on complementary tracks, each within a suitable niche.",
                "For instance, POMDPs could be seen as a much more natural formulation of economic sequential decision-making problems, while EMT is a better fit for continual demand for stochastic change, as happens in many robotic or embodied-agent problems.",
                "The complementary properties of POMDPs and EMT can be further exploited.",
                "There is recent interest in using POMDPs in hybrid solutions [17], in which the POMDPs can be used together with other control approaches to provide results not easily achievable with either approach by itself.",
                "DBC can be an effective partner in such a hybrid solution.",
                "For instance, POMDPs have prohibitively large off-line time requirements for policy computation, but can be readily used in simpler settings to expose beneficial behavioral trends; this can serve as a form of target dynamics that are provided to EMT in a larger domain for on-line operation. 7.",
                "CONCLUSIONS AND FUTURE WORK In this paper, we have presented a novel perspective on the process of planning and control in stochastic environments, in the form of the Dynamics Based Control (DBC) framework.",
                "DBC formulates the task of planning as support of a specified target system dynamics, which describes the necessary properties of change within the environment.",
                "Optimality of DBC plans of action are measured The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) 795 0 20 40 60 80 100 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1 Steps Entropy Dead−ends 0 20 40 60 80 100 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1 Steps Entropy Arena 0 20 40 60 80 100 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1 Steps Entropy Circle Figure 4: Observation Model I: Omniposition quarry.",
                "Entropy development with length of Tag Game for the three experiment scenarios: a) multiple dead-end, b) irregular open arena, c) circular corridor. 0 10 20 30 40 50 60 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1 Steps Entropy Dead−ends 0 20 40 60 80 100 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1 Steps Entropy Arena 0 20 40 60 80 100 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1 Steps Entropy Circle Figure 5: Observation Model II: quarry not observed at hunters position.",
                "Entropy development with length of Tag Game for the three experiment scenarios: a) multiple dead-end, b) irregular open arena, c) circular corridor. with respect to the deviation of actual system dynamics from the target dynamics.",
                "We show that a recently developed technique of Extended Markov Tracking (EMT) [13] is an instantiation of DBC.",
                "In fact, EMT can be seen as a specific case of DBC parameterization, which employs a greedy action selection procedure.",
                "Since EMT exhibits the key features of the general DBC framework, as well as polynomial time complexity, we used the multitarget version of EMT [15] to demonstrate that the class of area sweeping problems naturally lends itself to dynamics-based descriptions, as instantiated by our experiments in the Tag Game domain.",
                "As enumerated in Section 4.2, EMT has a number of limitations, such as difficulty in dealing with negative dynamic preference.",
                "This prevents direct application of EMT to such problems as Rendezvous-Evasion Games (e.g., [6]).",
                "However, DBC in general has no such limitations, and readily enables the formulation of evasion games.",
                "In future work, we intend to proceed with the development of dynamics-based controllers for these problems. 8.",
                "ACKNOWLEDGMENT The work of the first two authors was partially supported by Israel Science Foundation grant #898/05, and the third author was partially supported by a grant from Israels Ministry of Science and Technology. 9.",
                "REFERENCES [1] R. C. Arkin.",
                "Behavior-Based Robotics.",
                "MIT Press, 1998. [2] J.",
                "A. Bilmes.",
                "A gentle tutorial of the EM algorithm and its application to parameter estimation for Gaussian mixture and Hidden Markov Models.",
                "Technical Report TR-97-021, Department of Electrical Engeineering and Computer Science, University of California at Berkeley, 1998. [3] T. M. Cover and J.",
                "A. Thomas.",
                "Elements of information theory.",
                "Wiley, 1991. [4] M. E. desJardins, E. H. Durfee, C. L. Ortiz, and M. J. Wolverton.",
                "A survey of research in distributed, continual planning.",
                "AI Magazine, 4:13-22, 1999. [5] V. R. Konda and J. N. Tsitsiklis.",
                "Actor-Critic algorithms.",
                "SIAM Journal on Control and Optimization, 42(4):1143-1166, 2003. [6] W. S. Lim.",
                "A rendezvous-evasion game on discrete locations with joint randomization.",
                "Advances in Applied Probability, 29(4):1004-1017, December 1997. [7] M. L. Littman, T. L. Dean, and L. P. Kaelbling.",
                "On the complexity of solving Markov decision problems.",
                "In Proceedings of the 11th Annual Conference on Uncertainty in Artificial Intelligence (UAI-95), pages 394-402, 1995. [8] O. Madani, S. Hanks, and A. Condon.",
                "On the undecidability of probabilistic planning and related stochastic optimization problems.",
                "Artificial Intelligence Journal, 147(1-2):5-34, July 2003. [9] R. M. Neal and G. E. Hinton.",
                "A view of the EM algorithm 796 The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) that justifies incremental, sparse, and other variants.",
                "In M. I. Jordan, editor, Learning in Graphical Models, pages 355-368.",
                "Kluwer Academic Publishers, 1998. [10] P. Paruchuri, M. Tambe, F. Ordonez, and S. Kraus.",
                "Security in multiagent systems by policy randomization.",
                "In Proceeding of AAMAS 2006, 2006. [11] J. Pineau, G. Gordon, and S. Thrun.",
                "Point-based value iteration: An anytime algorithm for pomdps.",
                "In International Joint Conference on Artificial Intelligence (IJCAI), pages 1025-1032, August 2003. [12] M. L. Puterman.",
                "Markov Decision Processes.",
                "Wiley Series in Probability and Mathematical Statistics: Applied Probability and Statistics Section.",
                "Wiley-Interscience Publication, New York, 1994. [13] Z. Rabinovich and J. S. Rosenschein.",
                "Extended Markov Tracking with an application to control.",
                "In The Workshop on Agent Tracking: Modeling Other Agents from Observations, at the Third International Joint Conference on Autonomous Agents and Multiagent Systems, pages 95-100, New-York, July 2004. [14] Z. Rabinovich and J. S. Rosenschein.",
                "Multiagent coordination by Extended Markov Tracking.",
                "In The Fourth International Joint Conference on Autonomous Agents and Multiagent Systems, pages 431-438, Utrecht, The Netherlands, July 2005. [15] Z. Rabinovich and J. S. Rosenschein.",
                "On the response of EMT-based control to interacting targets and models.",
                "In The Fifth International Joint Conference on Autonomous Agents and Multiagent Systems, pages 465-470, Hakodate, Japan, May 2006. [16] R. F. Stengel.",
                "Optimal Control and Estimation.",
                "Dover Publications, 1994. [17] M. Tambe, E. Bowring, H. Jung, G. Kaminka, R. Maheswaran, J. Marecki, J. Modi, R. Nair, J. Pearce, P. Paruchuri, D. Pynadath, P. Scerri, N. Schurr, and P. Varakantham.",
                "Conflicts in teamwork: Hybrids to the The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) 797"
            ],
            "original_annotated_samples": [],
            "translated_annotated_samples": [],
            "translated_text": "",
            "candidates": [],
            "error": []
        },
        "extended markov tracking": {
            "translated_key": "seguimiento extendido de Markov",
            "is_in_text": true,
            "original_annotated_sentences": [
                "Dynamics Based Control with an Application to Area-Sweeping Problems Zinovi Rabinovich Engineering and Computer Science Hebrew University of Jerusalem Jerusalem, Israel nomad@cs.huji.ac.il Jeffrey S. Rosenschein Engineering and Computer Science Hebrew University of Jerusalem Jerusalem, Israel jeff@cs.huji.ac.il Gal A. Kaminka The MAVERICK Group Department of Computer Science Bar Ilan University, Israel galk@cs.biu.ac.il ABSTRACT In this paper we introduce Dynamics Based Control (DBC), an approach to planning and control of an agent in stochastic environments.",
                "Unlike existing approaches, which seek to optimize expected rewards (e.g., in Partially Observable Markov Decision Problems (POMDPs)), DBC optimizes system behavior towards specified system dynamics.",
                "We show that a recently developed planning and control approach, <br>extended markov tracking</br> (EMT) is an instantiation of DBC.",
                "EMT employs greedy action selection to provide an efficient control algorithm in Markovian environments.",
                "We exploit this efficiency in a set of experiments that applied multitarget EMT to a class of area-sweeping problems (searching for moving targets).",
                "We show that such problems can be naturally defined and efficiently solved using the DBC framework, and its EMT instantiation.",
                "Categories and Subject Descriptors I.2.8 [Problem Solving, Control Methods, and Search]: Control Theory; I.2.9 [Robotics]; I.2.11 [Distributed Artificial Intelligence]: Intelligent Agents General Terms Algorithms, Theory 1.",
                "INTRODUCTION Planning and control constitutes a central research area in multiagent systems and artificial intelligence.",
                "In recent years, Partially Observable Markov Decision Processes (POMDPs) [12] have become a popular formal basis for planning in stochastic environments.",
                "In this framework, the planning and control problem is often addressed by imposing a reward function, and computing a policy (of choosing actions) that is optimal, in the sense that it will result in the highest expected utility.",
                "While theoretically attractive, the complexity of optimally solving a POMDP is prohibitive [8, 7].",
                "We take an alternative view of planning in stochastic environments.",
                "We do not use a (state-based) reward function, but instead optimize over a different criterion, a transition-based specification of the desired system dynamics.",
                "The idea here is to view planexecution as a process that compels a (stochastic) system to change, and a plan as a dynamic process that shapes that change according to desired criteria.",
                "We call this general planning framework Dynamics Based Control (DBC).",
                "In DBC, the goal of a planning (or control) process becomes to ensure that the system will change in accordance with specific (potentially stochastic) target dynamics.",
                "As actual system behavior may deviate from that which is specified by target dynamics (due to the stochastic nature of the system), planning in such environments needs to be continual [4], in a manner similar to classical closed-loop controllers [16].",
                "Here, optimality is measured in terms of probability of deviation magnitudes.",
                "In this paper, we present the structure of Dynamics Based Control.",
                "We show that the recently developed <br>extended markov tracking</br> (EMT) approach [13, 14, 15] is subsumed by DBC, with EMT employing greedy action selection, which is a specific parameterization among the options possible within DBC.",
                "EMT is an efficient instantiation of DBC.",
                "To evaluate DBC, we carried out a set of experiments applying multi-target EMT to the Tag Game [11]; this is a variant on the area sweeping problem, where an agent is trying to tag a moving target (quarry) whose position is not known with certainty.",
                "Experimental data demonstrates that even with a simple model of the environment and a simple design of target dynamics, high success rates can be produced both in catching the quarry, and in surprising the quarry (as expressed by the observed entropy of the controlled agents position).",
                "The paper is organized as follows.",
                "In Section 2 we motivate DBC using area-sweeping problems, and discuss related work.",
                "Section 3 introduces the Dynamics Based Control (DBC) structure, and its specialization to Markovian environments.",
                "This is followed by a review of the <br>extended markov tracking</br> (EMT) approach as a DBC-structured control regimen in Section 4.",
                "That section also discusses the limitations of EMT-based control relative to the general DBC framework.",
                "Experimental settings and results are then presented in Section 5.",
                "Section 6 provides a short discussion of the overall approach, and Section 7 gives some concluding remarks and directions for future work. 790 978-81-904262-7-5 (RPS) c 2007 IFAAMAS 2.",
                "MOTIVATION AND RELATED WORK Many real-life scenarios naturally have a stochastic target dynamics specification, especially those domains where there exists no ultimate goal, but rather system behavior (with specific properties) that has to be continually supported.",
                "For example, security guards perform persistent sweeps of an area to detect any sign of intrusion.",
                "Cunning thieves will attempt to track these sweeps, and time their operation to key points of the guards motion.",
                "It is thus advisable to make the guards motion dynamics appear irregular and random.",
                "Recent work by Paruchuri et al. [10] has addressed such randomization in the context of single-agent and distributed POMDPs.",
                "The goal in that work was to generate policies that provide a measure of action-selection randomization, while maintaining rewards within some acceptable levels.",
                "Our focus differs from this work in that DBC does not optimize expected rewards-indeed we do not consider rewards at all-but instead maintains desired dynamics (including, but not limited to, randomization).",
                "The Game of Tag is another example of the applicability of the approach.",
                "It was introduced in the work by Pineau et al. [11].",
                "There are two agents that can move about an area, which is divided into a grid.",
                "The grid may have blocked cells (holes) into which no agent can move.",
                "One agent (the hunter) seeks to move into a cell occupied by the other (the quarry), such that they are co-located (this is a successful tag).",
                "The quarry seeks to avoid the hunter agent, and is always aware of the hunters position, but does not know how the hunter will behave, which opens up the possibility for a hunter to surprise the prey.",
                "The hunter knows the quarrys probabilistic law of motion, but does not know its current location.",
                "Tag is an instance of a family of area-sweeping (pursuit-evasion) problems.",
                "In [11], the hunter modeled the problem using a POMDP.",
                "A reward function was defined, to reflect the desire to tag the quarry, and an action policy was computed to optimize the reward collected over time.",
                "Due to the intractable complexity of determining the optimal policy, the action policy computed in that paper was essentially an approximation.",
                "In this paper, instead of formulating a reward function, we use EMT to solve the problem, by directly specifying the target dynamics.",
                "In fact, any search problem with randomized motion, the socalled class of area sweeping problems, can be described through specification of such target system dynamics.",
                "Dynamics Based Control provides a natural approach to solving these problems. 3.",
                "DYNAMICS BASED CONTROL The specification of Dynamics Based Control (DBC) can be broken into three interacting levels: Environment Design Level, User Level, and Agent Level. • Environment Design Level is concerned with the formal specification and modeling of the environment.",
                "For example, this level would specify the laws of physics within the system, and set its parameters, such as the gravitation constant. • User Level in turn relies on the environment model produced by Environment Design to specify the target system dynamics it wishes to observe.",
                "The User Level also specifies the estimation or learning procedure for system dynamics, and the measure of deviation.",
                "In the museum guard scenario above, these would correspond to a stochastic sweep schedule, and a measure of relative surprise between the specified and actual sweeping. • Agent Level in turn combines the environment model from the Environment Design level, the dynamics estimation procedure, the deviation measure and the target dynamics specification from User Level, to produce a sequence of actions that create system dynamics as close as possible to the targeted specification.",
                "As we are interested in the continual development of a stochastic system, such as happens in classical control theory [16] and continual planning [4], as well as in our example of museum sweeps, the question becomes how the Agent Level is to treat the deviation measurements over time.",
                "To this end, we use a probability threshold-that is, we would like the Agent Level to maximize the probability that the deviation measure will remain below a certain threshold.",
                "Specific action selection then depends on system formalization.",
                "One possibility would be to create a mixture of available system trends, much like that which happens in Behavior-Based Robotic architectures [1].",
                "The other alternative would be to rely on the estimation procedure provided by the User Level-to utilize the Environment Design Level model of the environment to choose actions, so as to manipulate the dynamics estimator into believing that a certain dynamics has been achieved.",
                "Notice that this manipulation is not direct, but via the environment.",
                "Thus, for strong enough estimator algorithms, successful manipulation would mean a successful simulation of the specified target dynamics (i.e., beyond discerning via the available sensory input).",
                "DBC levels can also have a back-flow of information (see Figure 1).",
                "For instance, the Agent Level could provide data about target dynamics feasibility, allowing the User Level to modify the requirement, perhaps focusing on attainable features of system behavior.",
                "Data would also be available about the system response to different actions performed; combined with a dynamics estimator defined by the User Level, this can provide an important tool for the environment model calibration at the Environment Design Level.",
                "UserEnv.",
                "Design Agent Model Ideal Dynamics Estimator Estimator Dynamics Feasibility System Response Data Figure 1: Data flow of the DBC framework Extending upon the idea of Actor-Critic algorithms [5], DBC data flow can provide a good basis for the design of a learning algorithm.",
                "For example, the User Level can operate as an exploratory device for a learning algorithm, inferring an ideal dynamics target from the environment model at hand that would expose and verify most critical features of system behavior.",
                "In this case, feasibility and system response data from the Agent Level would provide key information for an environment model update.",
                "In fact, the combination of feasibility and response data can provide a basis for the application of strong learning algorithms such as EM [2, 9]. 3.1 DBC for Markovian Environments For a Partially Observable Markovian Environment, DBC can be specified in a more rigorous manner.",
                "Notice how DBC discards rewards, and replaces it by another optimality criterion (structural differences are summarized in Table 1): • Environment Design level is to specify a tuple < S, A, T, O, Ω, s0 >, where: - S is the set of all possible environment states; - s0 is the initial state of the environment (which can also be viewed as a probability distribution over S); The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) 791 - A is the set of all possible actions applicable in the environment; - T is the environments probabilistic transition function: T : S ×A → Π(S).",
                "That is, T(s |a, s) is the probability that the environment will move from state s to state s under action a; - O is the set of all possible observations.",
                "This is what the sensor input would look like for an outside observer; - Ω is the observation probability function: Ω : S × A × S → Π(O).",
                "That is, Ω(o|s , a, s) is the probability that one will observe o given that the environment has moved from state s to state s under action a. • User Level, in the case of Markovian environment, operates on the set of system dynamics described by a family of conditional probabilities F = {τ : S × A → Π(S)}.",
                "Thus specification of target dynamics can be expressed by q ∈ F, and the learning or tracking algorithm can be represented as a function L : O×(A×O)∗ → F; that is, it maps sequences of observations and actions performed so far into an estimate τ ∈ F of system dynamics.",
                "There are many possible variations available at the User Level to define divergence between system dynamics; several of them are: - Trace distance or L1 distance between two distributions p and q defined by D(p(·), q(·)) = 1 2 x |p(x) − q(x)| - Fidelity measure of distance F(p(·), q(·)) = x p(x)q(x) - Kullback-Leibler divergence DKL(p(·) q(·)) = x p(x) log p(x) q(x) Notice that the latter two are not actually metrics over the space of possible distributions, but nevertheless have meaningful and important interpretations.",
                "For instance, KullbackLeibler divergence is an important tool of information theory [3] that allows one to measure the price of encoding an information source governed by q, while assuming that it is governed by p. The User Level also defines the threshold of dynamics deviation probability θ. • Agent Level is then faced with a problem of selecting a control signal function a∗ to satisfy a minimization problem as follows: a∗ = arg min a Pr(d(τa, q) > θ) where d(τa, q) is a random variable describing deviation of the dynamics estimate τa, created by L under control signal a, from the ideal dynamics q.",
                "Implicit in this minimization problem is that L is manipulated via the environment, based on the environment model produced by the Environment Design Level. 3.2 DBC View of the State Space It is important to note the complementary view that DBC and POMDPs take on the state space of the environment.",
                "POMDPs regard state as a stationary snap-shot of the environment; whatever attributes of state sequencing one seeks are reached through properties of the control process, in this case reward accumulation.",
                "This can be viewed as if the sequencing of states and the attributes of that sequencing are only introduced by and for the controlling mechanism-the POMDP policy.",
                "DBC concentrates on the underlying principle of state sequencing, the system dynamics.",
                "DBCs target dynamics specification can use the environments state space as a means to describe, discern, and preserve changes that occur within the system.",
                "As a result, DBC has a greater ability to express state sequencing properties, which are grounded in the environment model and its state space definition.",
                "For example, consider the task of moving through rough terrain towards a goal and reaching it as fast as possible.",
                "POMDPs would encode terrain as state space points, while speed would be ensured by negative reward for every step taken without reaching the goalaccumulating higher reward can be reached only by faster motion.",
                "Alternatively, the state space could directly include the notion of speed.",
                "For POMDPs, this would mean that the same concept is encoded twice, in some sense: directly in the state space, and indirectly within reward accumulation.",
                "Now, even if the reward function would encode more, and finer, details of the properties of motion, the POMDP solution will have to search in a much larger space of policies, while still being guided by the implicit concept of the reward accumulation procedure.",
                "On the other hand, the tactical target expression of variations and correlations between position and speed of motion is now grounded in the state space representation.",
                "In this situation, any further constraints, e.g., smoothness of motion, speed limits in different locations, or speed reductions during sharp turns, are explicitly and uniformly expressed by the tactical target, and can result in faster and more effective action selection by a DBC algorithm. 4.",
                "EMT-BASED CONTROL AS A DBC Recently, a control algorithm was introduced called EMT-based Control [13], which instantiates the DBC framework.",
                "Although it provides an approximate greedy solution in the DBC sense, initial experiments using EMT-based control have been encouraging [14, 15].",
                "EMT-based control is based on the Markovian environment definition, as in the case with POMDPs, but its User and Agent Levels are of the Markovian DBC type of optimality. • User Level of EMT-based control defines a limited-case target system dynamics independent of action: qEMT : S → Π(S).",
                "It then utilizes the Kullback-Leibler divergence measure to compose a momentary system dynamics estimator-the <br>extended markov tracking</br> (EMT) algorithm.",
                "The algorithm keeps a system dynamics estimate τt EMT that is capable of explaining recent change in an auxiliary Bayesian system state estimator from pt−1 to pt, and updates it conservatively using Kullback-Leibler divergence.",
                "Since τt EMT and pt−1,t are respectively the conditional and marginal probabilities over the systems state space, explanation simply means that pt(s ) = s τt EMT (s |s)pt−1(s), and the dynamics estimate update is performed by solving a 792 The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) Table 1: Structure of POMDP vs. Dynamics-Based Control in Markovian Environment Level Approach MDP Markovian DBC Environment < S, A, T, O, Ω >,where S - set of states A - set of actions Design T : S × A → Π(S) - transition O - observation set Ω : S × A × S → Π(O) User r : S × A × S → R q : S × A → Π(S) F(π∗ ) → r L(o1, ..., ot) → τ r - reward function q - ideal dynamics F - reward remodeling L - dynamics estimator θ - threshold Agent π∗ = arg max π E[ γt rt] π∗ = arg min π Prob(d(τ q) > θ) minimization problem: τt EMT = H[pt, pt−1, τt−1 EMT ] = arg min τ DKL(τ × pt−1 τt−1 EMT × pt−1) s.t. pt(s ) = s (τ × pt−1)(s , s) pt−1(s) = s (τ × pt−1)(s , s) • Agent Level in EMT-based control is suboptimal with respect to DBC (though it remains within the DBC framework), performing greedy action selection based on prediction of EMTs reaction.",
                "The prediction is based on the environment model provided by the Environment Design level, so that if we denote by Ta the environments transition function limited to action a, and pt−1 is the auxiliary Bayesian system state estimator, then the EMT-based control choice is described by a∗ = arg min a∈A DKL(H[Ta × pt, pt, τt EMT ] qEMT × pt−1) Note that this follows the Markovian DBC framework precisely: the rewarding optimality of POMDPs is discarded, and in its place a dynamics estimator (EMT in this case) is manipulated via action effects on the environment to produce an estimate close to the specified target system dynamics.",
                "Yet as we mentioned, naive EMTbased control is suboptimal in the DBC sense, and has several additional limitations that do not exist in the general DBC framework (discussed in Section 4.2). 4.1 Multi-Target EMT At times, there may exist several behavioral preferences.",
                "For example, in the case of museum guards, some art items are more heavily guarded, requiring that the guards stay more often in their close vicinity.",
                "On the other hand, no corner of the museum is to be left unchecked, which demands constant motion.",
                "Successful museum security would demand that the guards adhere to, and balance, both of these behaviors.",
                "For EMT-based control, this would mean facing several tactical targets {qk}K k=1, and the question becomes how to merge and balance them.",
                "A balancing mechanism can be applied to resolve this issue.",
                "Note that EMT-based control, while selecting an action, creates a preference vector over the set of actions based on their predicted performance with respect to a given target.",
                "If these preference vectors are normalized, they can be combined into a single unified preference.",
                "This requires replacement of standard EMT-based action selection by the algorithm below [15]: • Given: - a set of target dynamics {qk}K k=1, - vector of weights w(k) • Select action as follows - For each action a ∈ A predict the future state distribution ¯pa t+1 = Ta ∗ pt; - For each action, compute Da = H(¯pa t+1, pt, PDt) - For each a ∈ A and qk tactical target, denote V (a, k) = DKL (Da qk) pt .",
                "Let Vk(a) = 1 Zk V (a, k), where Zk = a∈A V (a, k) is a normalization factor. - Select a∗ = arg min a k k=1 w(k)Vk(a) The weights vector w = (w1, ..., wK ) allows the additional tuning of importance among target dynamics without the need to redesign the targets themselves.",
                "This balancing method is also seamlessly integrated into the EMT-based control flow of operation. 4.2 EMT-based Control Limitations EMT-based control is a sub-optimal (in the DBC sense) representative of the DBC structure.",
                "It limits the User by forcing EMT to be its dynamic tracking algorithm, and replaces Agent optimization by greedy action selection.",
                "This kind of combination, however, is common for on-line algorithms.",
                "Although further development of EMT-based controllers is necessary, evidence so far suggests that even the simplest form of the algorithm possesses a great deal of power, and displays trends that are optimal in the DBC sense of the word.",
                "There are two further, EMT-specific, limitations to EMT-based control that are evident at this point.",
                "Both already have partial solutions and are subjects of ongoing research.",
                "The first limitation is the problem of negative preference.",
                "In the POMDP framework for example, this is captured simply, through The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) 793 the appearance of values with different signs within the reward structure.",
                "For EMT-based control, however, negative preference means that one would like to avoid a certain distribution over system development sequences; EMT-based control, however, concentrates on getting as close as possible to a distribution.",
                "Avoidance is thus unnatural in native EMT-based control.",
                "The second limitation comes from the fact that standard environment modeling can create pure sensory actions-actions that do not change the state of the world, and differ only in the way observations are received and the quality of observations received.",
                "Since the world state does not change, EMT-based control would not be able to differentiate between different sensory actions.",
                "Notice that both of these limitations of EMT-based control are absent from the general DBC framework, since it may have a tracking algorithm capable of considering pure sensory actions and, unlike Kullback-Leibler divergence, a distribution deviation measure that is capable of dealing with negative preference. 5.",
                "EMT PLAYING TAG The Game of Tag was first introduced in [11].",
                "It is a single agent problem of capturing a quarry, and belongs to the class of area sweeping problems.",
                "An example domain is shown in Figure 2. 0 51 2 3 4 6 7 8 10 12 13 161514 17 18 19 2221 23 9 11Q A 20 Figure 2: Tag domain; an agent (A) attempts to seek and capture a quarry (Q) The Game of Tag extremely limits the agents perception, so that the agent is able to detect the quarry only if they are co-located in the same cell of the grid world.",
                "In the classical version of the game, co-location leads to a special observation, and the Tag action can be performed.",
                "We slightly modify this setting: the moment that both agents occupy the same cell, the game ends.",
                "As a result, both the agent and its quarry have the same motion capability, which allows them to move in four directions, North, South, East, and West.",
                "These form a formal space of actions within a Markovian environment.",
                "The state space of the formal Markovian environment is described by the cross-product of the agent and quarrys positions.",
                "For Figure 2, it would be S = {s0, ..., s23} × {s0, ..., s23}.",
                "The effects of an action taken by the agent are deterministic, but the environment in general has a stochastic response due to the motion of the quarry.",
                "With probability q0 1 it stays put, and with probability 1 − q0 it moves to an adjacent cell further away from the 1 In our experiments this was taken to be q0 = 0.2. agent.",
                "So for the instance shown in Figure 2 and q0 = 0.1: P(Q = s9|Q = s9, A = s11) = 0.1 P(Q = s2|Q = s9, A = s11) = 0.3 P(Q = s8|Q = s9, A = s11) = 0.3 P(Q = s14|Q = s9, A = s11) = 0.3 Although the evasive behavior of the quarry is known to the agent, the quarrys position is not.",
                "The only sensory information available to the agent is its own location.",
                "We use EMT and directly specify the target dynamics.",
                "For the Game of Tag, we can easily formulate three major trends: catching the quarry, staying mobile, and stalking the quarry.",
                "This results in the following three target dynamics: Tcatch(At+1 = si|Qt = sj, At = sa) ∝ 1 si = sj 0 otherwise Tmobile(At+1 = si|Qt = so, At = sj) ∝ 0 si = sj 1 otherwise Tstalk(At+1 = si|Qt = so, At = sj) ∝ 1 dist(si,so) Note that none of the above targets are directly achievable; for instance, if Qt = s9 and At = s11, there is no action that can move the agent to At+1 = s9 as required by the Tcatch target dynamics.",
                "We ran several experiments to evaluate EMT performance in the Tag Game.",
                "Three configurations of the domain shown in Figure 3 were used, each posing a different challenge to the agent due to partial observability.",
                "In each setting, a set of 1000 runs was performed with a time limit of 100 steps.",
                "In every run, the initial position of both the agent and its quarry was selected at random; this means that as far as the agent was concerned, the quarrys initial position was uniformly distributed over the entire domain cell space.",
                "We also used two variations of the environment observability function.",
                "In the first version, observability function mapped all joint positions of hunter and quarry into the position of the hunter as an observation.",
                "In the second, only those joint positions in which hunter and quarry occupied different locations were mapped into the hunters location.",
                "The second version in fact utilized and expressed the fact that once hunter and quarry occupy the same cell the game ends.",
                "The results of these experiments are shown in Table 2.",
                "Balancing [15] the catch, move, and stalk target dynamics described in the previous section by the weight vector [0.8, 0.1, 0.1], EMT produced stable performance in all three domains.",
                "Although direct comparisons are difficult to make, the EMT performance displayed notable efficiency vis-`a-vis the POMDP approach.",
                "In spite of a simple and inefficient Matlab implementation of the EMT algorithm, the decision time for any given step averaged significantly below 1 second in all experiments.",
                "For the irregular open arena domain, which proved to be the most difficult, 1000 experiment runs bounded by 100 steps each, a total of 42411 steps, were completed in slightly under 6 hours.",
                "That is, over 4 × 104 online steps took an order of magnitude less time than the offline computation of POMDP policy in [11].",
                "The significance of this differential is made even more prominent by the fact that, should the environment model parameters change, the online nature of EMT would allow it to maintain its performance time, while the POMDP policy would need to be recomputed, requiring yet again a large overhead of computation.",
                "We also tested the behavior cell frequency entropy, empirical measures from trial data.",
                "As Figure 4 and Figure 5 show, empir794 The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) A Q Q A 0 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 A Q Figure 3: These configurations of the Tag Game space were used: a) multiple dead-end, b) irregular open arena, c) circular corridor Table 2: Performance of the EMT-based solution in three Tag Game domains and two observability models: I) omniposition quarry, II) quarry is not at hunters position Model Domain Capture% E(Steps) Time/Step I Dead-ends 100 14.8 72(mSec) Arena 80.2 42.4 500(mSec) Circle 91.4 34.6 187(mSec) II Dead-ends 100 13.2 91(mSec) Arena 96.8 28.67 396(mSec) Circle 94.4 31.63 204(mSec) ical entropy grows with the length of interaction.",
                "For runs where the quarry was not captured immediately, the entropy reaches between 0.85 and 0.952 for different runs and scenarios.",
                "As the agent actively seeks the quarry, the entropy never reaches its maximum.",
                "One characteristic of the entropy graph for the open arena scenario particularly caught our attention in the case of the omniposition quarry observation model.",
                "Near the maximum limit of trial length (100 steps), entropy suddenly dropped.",
                "Further analysis of the data showed that under certain circumstances, a fluctuating behavior occurs in which the agent faces equally viable versions of quarry-following behavior.",
                "Since the EMT algorithm has greedy action selection, and the state space does not encode any form of commitment (not even speed or acceleration), the agent is locked within a small portion of cells.",
                "It is essentially attempting to simultaneously follow several courses of action, all of which are consistent with the target dynamics.",
                "This behavior did not occur in our second observation model, since it significantly reduced the set of eligible courses of action-essentially contributing to tie-breaking among them. 6.",
                "DISCUSSION The design of the EMT solution for the Tag Game exposes the core difference in approach to planning and control between EMT or DBC, on the one hand, and the more familiar POMDP approach, on the other.",
                "POMDP defines a reward structure to optimize, and influences system dynamics indirectly through that optimization.",
                "EMT discards any reward scheme, and instead measures and influences system dynamics directly. 2 Entropy was calculated using log base equal to the number of possible locations within the domain; this properly scales entropy expression into the range [0, 1] for all domains.",
                "Thus for the Tag Game, we did not search for a reward function that would encode and express our preference over the agents behavior, but rather directly set three (heuristic) behavior preferences as the basis for target dynamics to be maintained.",
                "Experimental data shows that these targets need not be directly achievable via the agents actions.",
                "However, the ratio between EMT performance and achievability of target dynamics remains to be explored.",
                "The tag game experiment data also revealed the different emphasis DBC and POMDPs place on the formulation of an environment state space.",
                "POMDPs rely entirely on the mechanism of reward accumulation maximization, i.e., formation of the action selection procedure to achieve necessary state sequencing.",
                "DBC, on the other hand, has two sources of sequencing specification: through the properties of an action selection procedure, and through direct specification within the target dynamics.",
                "The importance of the second source was underlined by the Tag Game experiment data, in which the greedy EMT algorithm, applied to a POMDP-type state space specification, failed, since target description over such a state space was incapable of encoding the necessary behavior tendencies, e.g., tie-breaking and commitment to directed motion.",
                "The structural differences between DBC (and EMT in particular), and POMDPs, prohibits direct performance comparison, and places them on complementary tracks, each within a suitable niche.",
                "For instance, POMDPs could be seen as a much more natural formulation of economic sequential decision-making problems, while EMT is a better fit for continual demand for stochastic change, as happens in many robotic or embodied-agent problems.",
                "The complementary properties of POMDPs and EMT can be further exploited.",
                "There is recent interest in using POMDPs in hybrid solutions [17], in which the POMDPs can be used together with other control approaches to provide results not easily achievable with either approach by itself.",
                "DBC can be an effective partner in such a hybrid solution.",
                "For instance, POMDPs have prohibitively large off-line time requirements for policy computation, but can be readily used in simpler settings to expose beneficial behavioral trends; this can serve as a form of target dynamics that are provided to EMT in a larger domain for on-line operation. 7.",
                "CONCLUSIONS AND FUTURE WORK In this paper, we have presented a novel perspective on the process of planning and control in stochastic environments, in the form of the Dynamics Based Control (DBC) framework.",
                "DBC formulates the task of planning as support of a specified target system dynamics, which describes the necessary properties of change within the environment.",
                "Optimality of DBC plans of action are measured The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) 795 0 20 40 60 80 100 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1 Steps Entropy Dead−ends 0 20 40 60 80 100 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1 Steps Entropy Arena 0 20 40 60 80 100 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1 Steps Entropy Circle Figure 4: Observation Model I: Omniposition quarry.",
                "Entropy development with length of Tag Game for the three experiment scenarios: a) multiple dead-end, b) irregular open arena, c) circular corridor. 0 10 20 30 40 50 60 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1 Steps Entropy Dead−ends 0 20 40 60 80 100 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1 Steps Entropy Arena 0 20 40 60 80 100 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1 Steps Entropy Circle Figure 5: Observation Model II: quarry not observed at hunters position.",
                "Entropy development with length of Tag Game for the three experiment scenarios: a) multiple dead-end, b) irregular open arena, c) circular corridor. with respect to the deviation of actual system dynamics from the target dynamics.",
                "We show that a recently developed technique of <br>extended markov tracking</br> (EMT) [13] is an instantiation of DBC.",
                "In fact, EMT can be seen as a specific case of DBC parameterization, which employs a greedy action selection procedure.",
                "Since EMT exhibits the key features of the general DBC framework, as well as polynomial time complexity, we used the multitarget version of EMT [15] to demonstrate that the class of area sweeping problems naturally lends itself to dynamics-based descriptions, as instantiated by our experiments in the Tag Game domain.",
                "As enumerated in Section 4.2, EMT has a number of limitations, such as difficulty in dealing with negative dynamic preference.",
                "This prevents direct application of EMT to such problems as Rendezvous-Evasion Games (e.g., [6]).",
                "However, DBC in general has no such limitations, and readily enables the formulation of evasion games.",
                "In future work, we intend to proceed with the development of dynamics-based controllers for these problems. 8.",
                "ACKNOWLEDGMENT The work of the first two authors was partially supported by Israel Science Foundation grant #898/05, and the third author was partially supported by a grant from Israels Ministry of Science and Technology. 9.",
                "REFERENCES [1] R. C. Arkin.",
                "Behavior-Based Robotics.",
                "MIT Press, 1998. [2] J.",
                "A. Bilmes.",
                "A gentle tutorial of the EM algorithm and its application to parameter estimation for Gaussian mixture and Hidden Markov Models.",
                "Technical Report TR-97-021, Department of Electrical Engeineering and Computer Science, University of California at Berkeley, 1998. [3] T. M. Cover and J.",
                "A. Thomas.",
                "Elements of information theory.",
                "Wiley, 1991. [4] M. E. desJardins, E. H. Durfee, C. L. Ortiz, and M. J. Wolverton.",
                "A survey of research in distributed, continual planning.",
                "AI Magazine, 4:13-22, 1999. [5] V. R. Konda and J. N. Tsitsiklis.",
                "Actor-Critic algorithms.",
                "SIAM Journal on Control and Optimization, 42(4):1143-1166, 2003. [6] W. S. Lim.",
                "A rendezvous-evasion game on discrete locations with joint randomization.",
                "Advances in Applied Probability, 29(4):1004-1017, December 1997. [7] M. L. Littman, T. L. Dean, and L. P. Kaelbling.",
                "On the complexity of solving Markov decision problems.",
                "In Proceedings of the 11th Annual Conference on Uncertainty in Artificial Intelligence (UAI-95), pages 394-402, 1995. [8] O. Madani, S. Hanks, and A. Condon.",
                "On the undecidability of probabilistic planning and related stochastic optimization problems.",
                "Artificial Intelligence Journal, 147(1-2):5-34, July 2003. [9] R. M. Neal and G. E. Hinton.",
                "A view of the EM algorithm 796 The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) that justifies incremental, sparse, and other variants.",
                "In M. I. Jordan, editor, Learning in Graphical Models, pages 355-368.",
                "Kluwer Academic Publishers, 1998. [10] P. Paruchuri, M. Tambe, F. Ordonez, and S. Kraus.",
                "Security in multiagent systems by policy randomization.",
                "In Proceeding of AAMAS 2006, 2006. [11] J. Pineau, G. Gordon, and S. Thrun.",
                "Point-based value iteration: An anytime algorithm for pomdps.",
                "In International Joint Conference on Artificial Intelligence (IJCAI), pages 1025-1032, August 2003. [12] M. L. Puterman.",
                "Markov Decision Processes.",
                "Wiley Series in Probability and Mathematical Statistics: Applied Probability and Statistics Section.",
                "Wiley-Interscience Publication, New York, 1994. [13] Z. Rabinovich and J. S. Rosenschein.",
                "<br>extended markov tracking</br> with an application to control.",
                "In The Workshop on Agent Tracking: Modeling Other Agents from Observations, at the Third International Joint Conference on Autonomous Agents and Multiagent Systems, pages 95-100, New-York, July 2004. [14] Z. Rabinovich and J. S. Rosenschein.",
                "Multiagent coordination by <br>extended markov tracking</br>.",
                "In The Fourth International Joint Conference on Autonomous Agents and Multiagent Systems, pages 431-438, Utrecht, The Netherlands, July 2005. [15] Z. Rabinovich and J. S. Rosenschein.",
                "On the response of EMT-based control to interacting targets and models.",
                "In The Fifth International Joint Conference on Autonomous Agents and Multiagent Systems, pages 465-470, Hakodate, Japan, May 2006. [16] R. F. Stengel.",
                "Optimal Control and Estimation.",
                "Dover Publications, 1994. [17] M. Tambe, E. Bowring, H. Jung, G. Kaminka, R. Maheswaran, J. Marecki, J. Modi, R. Nair, J. Pearce, P. Paruchuri, D. Pynadath, P. Scerri, N. Schurr, and P. Varakantham.",
                "Conflicts in teamwork: Hybrids to the The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) 797"
            ],
            "original_annotated_samples": [],
            "translated_annotated_samples": [
                "Mostramos que un enfoque de planificación y control recientemente desarrollado, \"Seguimiento extendido de Markov\" (EMT) es una instanciación de DBC.",
                "Mostramos que el enfoque DBC de \"EMT) recientemente desarrollado\" EMT) [13, 14, 15] es subsumido por DBC, con EMT empleando una selección de acción codiciosa, que es una parametrización específica entre las opciones posibles dentro de DBC.",
                "Esto es seguido por una revisión del enfoque de \"seguimiento de Markov extendido\" (EMT) como un régimen de control estructurado con DBC en la Sección 4.",
                "Luego utiliza la medida de divergencia Kullback-Leibler para componer un estimador de dinámica del sistema momentáneo: el algoritmo de \"seguimiento de Markov extendido\" (EMT).",
                "Mostramos que una técnica desarrollada recientemente de \"seguimiento extendido de Markov\" (EMT) [13] es una instanciación de DBC.",
                "\"Seguimiento extendido de Markov\" con una aplicación para controlar.",
                "Coordinación múltiple por \"seguimiento extendido de Markov\"."
            ],
            "translated_text": "",
            "candidates": [
                "seguimiento extendido de Markov",
                "Seguimiento extendido de Markov",
                "seguimiento extendido de Markov",
                "EMT) recientemente desarrollado",
                "Seguimiento de Markov extendido",
                "seguimiento de Markov extendido",
                "seguimiento extendido de Markov",
                "seguimiento de Markov extendido",
                "seguimiento extendido de Markov",
                "seguimiento extendido de Markov",
                "seguimiento extendido de Markov",
                "Seguimiento extendido de Markov",
                "seguimiento extendido de Markov",
                "seguimiento extendido de Markov"
            ],
            "error": []
        },
        "multi-agent system": {
            "translated_key": "sistema de múltiples agentes",
            "is_in_text": false,
            "original_annotated_sentences": [
                "Dynamics Based Control with an Application to Area-Sweeping Problems Zinovi Rabinovich Engineering and Computer Science Hebrew University of Jerusalem Jerusalem, Israel nomad@cs.huji.ac.il Jeffrey S. Rosenschein Engineering and Computer Science Hebrew University of Jerusalem Jerusalem, Israel jeff@cs.huji.ac.il Gal A. Kaminka The MAVERICK Group Department of Computer Science Bar Ilan University, Israel galk@cs.biu.ac.il ABSTRACT In this paper we introduce Dynamics Based Control (DBC), an approach to planning and control of an agent in stochastic environments.",
                "Unlike existing approaches, which seek to optimize expected rewards (e.g., in Partially Observable Markov Decision Problems (POMDPs)), DBC optimizes system behavior towards specified system dynamics.",
                "We show that a recently developed planning and control approach, Extended Markov Tracking (EMT) is an instantiation of DBC.",
                "EMT employs greedy action selection to provide an efficient control algorithm in Markovian environments.",
                "We exploit this efficiency in a set of experiments that applied multitarget EMT to a class of area-sweeping problems (searching for moving targets).",
                "We show that such problems can be naturally defined and efficiently solved using the DBC framework, and its EMT instantiation.",
                "Categories and Subject Descriptors I.2.8 [Problem Solving, Control Methods, and Search]: Control Theory; I.2.9 [Robotics]; I.2.11 [Distributed Artificial Intelligence]: Intelligent Agents General Terms Algorithms, Theory 1.",
                "INTRODUCTION Planning and control constitutes a central research area in multiagent systems and artificial intelligence.",
                "In recent years, Partially Observable Markov Decision Processes (POMDPs) [12] have become a popular formal basis for planning in stochastic environments.",
                "In this framework, the planning and control problem is often addressed by imposing a reward function, and computing a policy (of choosing actions) that is optimal, in the sense that it will result in the highest expected utility.",
                "While theoretically attractive, the complexity of optimally solving a POMDP is prohibitive [8, 7].",
                "We take an alternative view of planning in stochastic environments.",
                "We do not use a (state-based) reward function, but instead optimize over a different criterion, a transition-based specification of the desired system dynamics.",
                "The idea here is to view planexecution as a process that compels a (stochastic) system to change, and a plan as a dynamic process that shapes that change according to desired criteria.",
                "We call this general planning framework Dynamics Based Control (DBC).",
                "In DBC, the goal of a planning (or control) process becomes to ensure that the system will change in accordance with specific (potentially stochastic) target dynamics.",
                "As actual system behavior may deviate from that which is specified by target dynamics (due to the stochastic nature of the system), planning in such environments needs to be continual [4], in a manner similar to classical closed-loop controllers [16].",
                "Here, optimality is measured in terms of probability of deviation magnitudes.",
                "In this paper, we present the structure of Dynamics Based Control.",
                "We show that the recently developed Extended Markov Tracking (EMT) approach [13, 14, 15] is subsumed by DBC, with EMT employing greedy action selection, which is a specific parameterization among the options possible within DBC.",
                "EMT is an efficient instantiation of DBC.",
                "To evaluate DBC, we carried out a set of experiments applying multi-target EMT to the Tag Game [11]; this is a variant on the area sweeping problem, where an agent is trying to tag a moving target (quarry) whose position is not known with certainty.",
                "Experimental data demonstrates that even with a simple model of the environment and a simple design of target dynamics, high success rates can be produced both in catching the quarry, and in surprising the quarry (as expressed by the observed entropy of the controlled agents position).",
                "The paper is organized as follows.",
                "In Section 2 we motivate DBC using area-sweeping problems, and discuss related work.",
                "Section 3 introduces the Dynamics Based Control (DBC) structure, and its specialization to Markovian environments.",
                "This is followed by a review of the Extended Markov Tracking (EMT) approach as a DBC-structured control regimen in Section 4.",
                "That section also discusses the limitations of EMT-based control relative to the general DBC framework.",
                "Experimental settings and results are then presented in Section 5.",
                "Section 6 provides a short discussion of the overall approach, and Section 7 gives some concluding remarks and directions for future work. 790 978-81-904262-7-5 (RPS) c 2007 IFAAMAS 2.",
                "MOTIVATION AND RELATED WORK Many real-life scenarios naturally have a stochastic target dynamics specification, especially those domains where there exists no ultimate goal, but rather system behavior (with specific properties) that has to be continually supported.",
                "For example, security guards perform persistent sweeps of an area to detect any sign of intrusion.",
                "Cunning thieves will attempt to track these sweeps, and time their operation to key points of the guards motion.",
                "It is thus advisable to make the guards motion dynamics appear irregular and random.",
                "Recent work by Paruchuri et al. [10] has addressed such randomization in the context of single-agent and distributed POMDPs.",
                "The goal in that work was to generate policies that provide a measure of action-selection randomization, while maintaining rewards within some acceptable levels.",
                "Our focus differs from this work in that DBC does not optimize expected rewards-indeed we do not consider rewards at all-but instead maintains desired dynamics (including, but not limited to, randomization).",
                "The Game of Tag is another example of the applicability of the approach.",
                "It was introduced in the work by Pineau et al. [11].",
                "There are two agents that can move about an area, which is divided into a grid.",
                "The grid may have blocked cells (holes) into which no agent can move.",
                "One agent (the hunter) seeks to move into a cell occupied by the other (the quarry), such that they are co-located (this is a successful tag).",
                "The quarry seeks to avoid the hunter agent, and is always aware of the hunters position, but does not know how the hunter will behave, which opens up the possibility for a hunter to surprise the prey.",
                "The hunter knows the quarrys probabilistic law of motion, but does not know its current location.",
                "Tag is an instance of a family of area-sweeping (pursuit-evasion) problems.",
                "In [11], the hunter modeled the problem using a POMDP.",
                "A reward function was defined, to reflect the desire to tag the quarry, and an action policy was computed to optimize the reward collected over time.",
                "Due to the intractable complexity of determining the optimal policy, the action policy computed in that paper was essentially an approximation.",
                "In this paper, instead of formulating a reward function, we use EMT to solve the problem, by directly specifying the target dynamics.",
                "In fact, any search problem with randomized motion, the socalled class of area sweeping problems, can be described through specification of such target system dynamics.",
                "Dynamics Based Control provides a natural approach to solving these problems. 3.",
                "DYNAMICS BASED CONTROL The specification of Dynamics Based Control (DBC) can be broken into three interacting levels: Environment Design Level, User Level, and Agent Level. • Environment Design Level is concerned with the formal specification and modeling of the environment.",
                "For example, this level would specify the laws of physics within the system, and set its parameters, such as the gravitation constant. • User Level in turn relies on the environment model produced by Environment Design to specify the target system dynamics it wishes to observe.",
                "The User Level also specifies the estimation or learning procedure for system dynamics, and the measure of deviation.",
                "In the museum guard scenario above, these would correspond to a stochastic sweep schedule, and a measure of relative surprise between the specified and actual sweeping. • Agent Level in turn combines the environment model from the Environment Design level, the dynamics estimation procedure, the deviation measure and the target dynamics specification from User Level, to produce a sequence of actions that create system dynamics as close as possible to the targeted specification.",
                "As we are interested in the continual development of a stochastic system, such as happens in classical control theory [16] and continual planning [4], as well as in our example of museum sweeps, the question becomes how the Agent Level is to treat the deviation measurements over time.",
                "To this end, we use a probability threshold-that is, we would like the Agent Level to maximize the probability that the deviation measure will remain below a certain threshold.",
                "Specific action selection then depends on system formalization.",
                "One possibility would be to create a mixture of available system trends, much like that which happens in Behavior-Based Robotic architectures [1].",
                "The other alternative would be to rely on the estimation procedure provided by the User Level-to utilize the Environment Design Level model of the environment to choose actions, so as to manipulate the dynamics estimator into believing that a certain dynamics has been achieved.",
                "Notice that this manipulation is not direct, but via the environment.",
                "Thus, for strong enough estimator algorithms, successful manipulation would mean a successful simulation of the specified target dynamics (i.e., beyond discerning via the available sensory input).",
                "DBC levels can also have a back-flow of information (see Figure 1).",
                "For instance, the Agent Level could provide data about target dynamics feasibility, allowing the User Level to modify the requirement, perhaps focusing on attainable features of system behavior.",
                "Data would also be available about the system response to different actions performed; combined with a dynamics estimator defined by the User Level, this can provide an important tool for the environment model calibration at the Environment Design Level.",
                "UserEnv.",
                "Design Agent Model Ideal Dynamics Estimator Estimator Dynamics Feasibility System Response Data Figure 1: Data flow of the DBC framework Extending upon the idea of Actor-Critic algorithms [5], DBC data flow can provide a good basis for the design of a learning algorithm.",
                "For example, the User Level can operate as an exploratory device for a learning algorithm, inferring an ideal dynamics target from the environment model at hand that would expose and verify most critical features of system behavior.",
                "In this case, feasibility and system response data from the Agent Level would provide key information for an environment model update.",
                "In fact, the combination of feasibility and response data can provide a basis for the application of strong learning algorithms such as EM [2, 9]. 3.1 DBC for Markovian Environments For a Partially Observable Markovian Environment, DBC can be specified in a more rigorous manner.",
                "Notice how DBC discards rewards, and replaces it by another optimality criterion (structural differences are summarized in Table 1): • Environment Design level is to specify a tuple < S, A, T, O, Ω, s0 >, where: - S is the set of all possible environment states; - s0 is the initial state of the environment (which can also be viewed as a probability distribution over S); The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) 791 - A is the set of all possible actions applicable in the environment; - T is the environments probabilistic transition function: T : S ×A → Π(S).",
                "That is, T(s |a, s) is the probability that the environment will move from state s to state s under action a; - O is the set of all possible observations.",
                "This is what the sensor input would look like for an outside observer; - Ω is the observation probability function: Ω : S × A × S → Π(O).",
                "That is, Ω(o|s , a, s) is the probability that one will observe o given that the environment has moved from state s to state s under action a. • User Level, in the case of Markovian environment, operates on the set of system dynamics described by a family of conditional probabilities F = {τ : S × A → Π(S)}.",
                "Thus specification of target dynamics can be expressed by q ∈ F, and the learning or tracking algorithm can be represented as a function L : O×(A×O)∗ → F; that is, it maps sequences of observations and actions performed so far into an estimate τ ∈ F of system dynamics.",
                "There are many possible variations available at the User Level to define divergence between system dynamics; several of them are: - Trace distance or L1 distance between two distributions p and q defined by D(p(·), q(·)) = 1 2 x |p(x) − q(x)| - Fidelity measure of distance F(p(·), q(·)) = x p(x)q(x) - Kullback-Leibler divergence DKL(p(·) q(·)) = x p(x) log p(x) q(x) Notice that the latter two are not actually metrics over the space of possible distributions, but nevertheless have meaningful and important interpretations.",
                "For instance, KullbackLeibler divergence is an important tool of information theory [3] that allows one to measure the price of encoding an information source governed by q, while assuming that it is governed by p. The User Level also defines the threshold of dynamics deviation probability θ. • Agent Level is then faced with a problem of selecting a control signal function a∗ to satisfy a minimization problem as follows: a∗ = arg min a Pr(d(τa, q) > θ) where d(τa, q) is a random variable describing deviation of the dynamics estimate τa, created by L under control signal a, from the ideal dynamics q.",
                "Implicit in this minimization problem is that L is manipulated via the environment, based on the environment model produced by the Environment Design Level. 3.2 DBC View of the State Space It is important to note the complementary view that DBC and POMDPs take on the state space of the environment.",
                "POMDPs regard state as a stationary snap-shot of the environment; whatever attributes of state sequencing one seeks are reached through properties of the control process, in this case reward accumulation.",
                "This can be viewed as if the sequencing of states and the attributes of that sequencing are only introduced by and for the controlling mechanism-the POMDP policy.",
                "DBC concentrates on the underlying principle of state sequencing, the system dynamics.",
                "DBCs target dynamics specification can use the environments state space as a means to describe, discern, and preserve changes that occur within the system.",
                "As a result, DBC has a greater ability to express state sequencing properties, which are grounded in the environment model and its state space definition.",
                "For example, consider the task of moving through rough terrain towards a goal and reaching it as fast as possible.",
                "POMDPs would encode terrain as state space points, while speed would be ensured by negative reward for every step taken without reaching the goalaccumulating higher reward can be reached only by faster motion.",
                "Alternatively, the state space could directly include the notion of speed.",
                "For POMDPs, this would mean that the same concept is encoded twice, in some sense: directly in the state space, and indirectly within reward accumulation.",
                "Now, even if the reward function would encode more, and finer, details of the properties of motion, the POMDP solution will have to search in a much larger space of policies, while still being guided by the implicit concept of the reward accumulation procedure.",
                "On the other hand, the tactical target expression of variations and correlations between position and speed of motion is now grounded in the state space representation.",
                "In this situation, any further constraints, e.g., smoothness of motion, speed limits in different locations, or speed reductions during sharp turns, are explicitly and uniformly expressed by the tactical target, and can result in faster and more effective action selection by a DBC algorithm. 4.",
                "EMT-BASED CONTROL AS A DBC Recently, a control algorithm was introduced called EMT-based Control [13], which instantiates the DBC framework.",
                "Although it provides an approximate greedy solution in the DBC sense, initial experiments using EMT-based control have been encouraging [14, 15].",
                "EMT-based control is based on the Markovian environment definition, as in the case with POMDPs, but its User and Agent Levels are of the Markovian DBC type of optimality. • User Level of EMT-based control defines a limited-case target system dynamics independent of action: qEMT : S → Π(S).",
                "It then utilizes the Kullback-Leibler divergence measure to compose a momentary system dynamics estimator-the Extended Markov Tracking (EMT) algorithm.",
                "The algorithm keeps a system dynamics estimate τt EMT that is capable of explaining recent change in an auxiliary Bayesian system state estimator from pt−1 to pt, and updates it conservatively using Kullback-Leibler divergence.",
                "Since τt EMT and pt−1,t are respectively the conditional and marginal probabilities over the systems state space, explanation simply means that pt(s ) = s τt EMT (s |s)pt−1(s), and the dynamics estimate update is performed by solving a 792 The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) Table 1: Structure of POMDP vs. Dynamics-Based Control in Markovian Environment Level Approach MDP Markovian DBC Environment < S, A, T, O, Ω >,where S - set of states A - set of actions Design T : S × A → Π(S) - transition O - observation set Ω : S × A × S → Π(O) User r : S × A × S → R q : S × A → Π(S) F(π∗ ) → r L(o1, ..., ot) → τ r - reward function q - ideal dynamics F - reward remodeling L - dynamics estimator θ - threshold Agent π∗ = arg max π E[ γt rt] π∗ = arg min π Prob(d(τ q) > θ) minimization problem: τt EMT = H[pt, pt−1, τt−1 EMT ] = arg min τ DKL(τ × pt−1 τt−1 EMT × pt−1) s.t. pt(s ) = s (τ × pt−1)(s , s) pt−1(s) = s (τ × pt−1)(s , s) • Agent Level in EMT-based control is suboptimal with respect to DBC (though it remains within the DBC framework), performing greedy action selection based on prediction of EMTs reaction.",
                "The prediction is based on the environment model provided by the Environment Design level, so that if we denote by Ta the environments transition function limited to action a, and pt−1 is the auxiliary Bayesian system state estimator, then the EMT-based control choice is described by a∗ = arg min a∈A DKL(H[Ta × pt, pt, τt EMT ] qEMT × pt−1) Note that this follows the Markovian DBC framework precisely: the rewarding optimality of POMDPs is discarded, and in its place a dynamics estimator (EMT in this case) is manipulated via action effects on the environment to produce an estimate close to the specified target system dynamics.",
                "Yet as we mentioned, naive EMTbased control is suboptimal in the DBC sense, and has several additional limitations that do not exist in the general DBC framework (discussed in Section 4.2). 4.1 Multi-Target EMT At times, there may exist several behavioral preferences.",
                "For example, in the case of museum guards, some art items are more heavily guarded, requiring that the guards stay more often in their close vicinity.",
                "On the other hand, no corner of the museum is to be left unchecked, which demands constant motion.",
                "Successful museum security would demand that the guards adhere to, and balance, both of these behaviors.",
                "For EMT-based control, this would mean facing several tactical targets {qk}K k=1, and the question becomes how to merge and balance them.",
                "A balancing mechanism can be applied to resolve this issue.",
                "Note that EMT-based control, while selecting an action, creates a preference vector over the set of actions based on their predicted performance with respect to a given target.",
                "If these preference vectors are normalized, they can be combined into a single unified preference.",
                "This requires replacement of standard EMT-based action selection by the algorithm below [15]: • Given: - a set of target dynamics {qk}K k=1, - vector of weights w(k) • Select action as follows - For each action a ∈ A predict the future state distribution ¯pa t+1 = Ta ∗ pt; - For each action, compute Da = H(¯pa t+1, pt, PDt) - For each a ∈ A and qk tactical target, denote V (a, k) = DKL (Da qk) pt .",
                "Let Vk(a) = 1 Zk V (a, k), where Zk = a∈A V (a, k) is a normalization factor. - Select a∗ = arg min a k k=1 w(k)Vk(a) The weights vector w = (w1, ..., wK ) allows the additional tuning of importance among target dynamics without the need to redesign the targets themselves.",
                "This balancing method is also seamlessly integrated into the EMT-based control flow of operation. 4.2 EMT-based Control Limitations EMT-based control is a sub-optimal (in the DBC sense) representative of the DBC structure.",
                "It limits the User by forcing EMT to be its dynamic tracking algorithm, and replaces Agent optimization by greedy action selection.",
                "This kind of combination, however, is common for on-line algorithms.",
                "Although further development of EMT-based controllers is necessary, evidence so far suggests that even the simplest form of the algorithm possesses a great deal of power, and displays trends that are optimal in the DBC sense of the word.",
                "There are two further, EMT-specific, limitations to EMT-based control that are evident at this point.",
                "Both already have partial solutions and are subjects of ongoing research.",
                "The first limitation is the problem of negative preference.",
                "In the POMDP framework for example, this is captured simply, through The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) 793 the appearance of values with different signs within the reward structure.",
                "For EMT-based control, however, negative preference means that one would like to avoid a certain distribution over system development sequences; EMT-based control, however, concentrates on getting as close as possible to a distribution.",
                "Avoidance is thus unnatural in native EMT-based control.",
                "The second limitation comes from the fact that standard environment modeling can create pure sensory actions-actions that do not change the state of the world, and differ only in the way observations are received and the quality of observations received.",
                "Since the world state does not change, EMT-based control would not be able to differentiate between different sensory actions.",
                "Notice that both of these limitations of EMT-based control are absent from the general DBC framework, since it may have a tracking algorithm capable of considering pure sensory actions and, unlike Kullback-Leibler divergence, a distribution deviation measure that is capable of dealing with negative preference. 5.",
                "EMT PLAYING TAG The Game of Tag was first introduced in [11].",
                "It is a single agent problem of capturing a quarry, and belongs to the class of area sweeping problems.",
                "An example domain is shown in Figure 2. 0 51 2 3 4 6 7 8 10 12 13 161514 17 18 19 2221 23 9 11Q A 20 Figure 2: Tag domain; an agent (A) attempts to seek and capture a quarry (Q) The Game of Tag extremely limits the agents perception, so that the agent is able to detect the quarry only if they are co-located in the same cell of the grid world.",
                "In the classical version of the game, co-location leads to a special observation, and the Tag action can be performed.",
                "We slightly modify this setting: the moment that both agents occupy the same cell, the game ends.",
                "As a result, both the agent and its quarry have the same motion capability, which allows them to move in four directions, North, South, East, and West.",
                "These form a formal space of actions within a Markovian environment.",
                "The state space of the formal Markovian environment is described by the cross-product of the agent and quarrys positions.",
                "For Figure 2, it would be S = {s0, ..., s23} × {s0, ..., s23}.",
                "The effects of an action taken by the agent are deterministic, but the environment in general has a stochastic response due to the motion of the quarry.",
                "With probability q0 1 it stays put, and with probability 1 − q0 it moves to an adjacent cell further away from the 1 In our experiments this was taken to be q0 = 0.2. agent.",
                "So for the instance shown in Figure 2 and q0 = 0.1: P(Q = s9|Q = s9, A = s11) = 0.1 P(Q = s2|Q = s9, A = s11) = 0.3 P(Q = s8|Q = s9, A = s11) = 0.3 P(Q = s14|Q = s9, A = s11) = 0.3 Although the evasive behavior of the quarry is known to the agent, the quarrys position is not.",
                "The only sensory information available to the agent is its own location.",
                "We use EMT and directly specify the target dynamics.",
                "For the Game of Tag, we can easily formulate three major trends: catching the quarry, staying mobile, and stalking the quarry.",
                "This results in the following three target dynamics: Tcatch(At+1 = si|Qt = sj, At = sa) ∝ 1 si = sj 0 otherwise Tmobile(At+1 = si|Qt = so, At = sj) ∝ 0 si = sj 1 otherwise Tstalk(At+1 = si|Qt = so, At = sj) ∝ 1 dist(si,so) Note that none of the above targets are directly achievable; for instance, if Qt = s9 and At = s11, there is no action that can move the agent to At+1 = s9 as required by the Tcatch target dynamics.",
                "We ran several experiments to evaluate EMT performance in the Tag Game.",
                "Three configurations of the domain shown in Figure 3 were used, each posing a different challenge to the agent due to partial observability.",
                "In each setting, a set of 1000 runs was performed with a time limit of 100 steps.",
                "In every run, the initial position of both the agent and its quarry was selected at random; this means that as far as the agent was concerned, the quarrys initial position was uniformly distributed over the entire domain cell space.",
                "We also used two variations of the environment observability function.",
                "In the first version, observability function mapped all joint positions of hunter and quarry into the position of the hunter as an observation.",
                "In the second, only those joint positions in which hunter and quarry occupied different locations were mapped into the hunters location.",
                "The second version in fact utilized and expressed the fact that once hunter and quarry occupy the same cell the game ends.",
                "The results of these experiments are shown in Table 2.",
                "Balancing [15] the catch, move, and stalk target dynamics described in the previous section by the weight vector [0.8, 0.1, 0.1], EMT produced stable performance in all three domains.",
                "Although direct comparisons are difficult to make, the EMT performance displayed notable efficiency vis-`a-vis the POMDP approach.",
                "In spite of a simple and inefficient Matlab implementation of the EMT algorithm, the decision time for any given step averaged significantly below 1 second in all experiments.",
                "For the irregular open arena domain, which proved to be the most difficult, 1000 experiment runs bounded by 100 steps each, a total of 42411 steps, were completed in slightly under 6 hours.",
                "That is, over 4 × 104 online steps took an order of magnitude less time than the offline computation of POMDP policy in [11].",
                "The significance of this differential is made even more prominent by the fact that, should the environment model parameters change, the online nature of EMT would allow it to maintain its performance time, while the POMDP policy would need to be recomputed, requiring yet again a large overhead of computation.",
                "We also tested the behavior cell frequency entropy, empirical measures from trial data.",
                "As Figure 4 and Figure 5 show, empir794 The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) A Q Q A 0 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 A Q Figure 3: These configurations of the Tag Game space were used: a) multiple dead-end, b) irregular open arena, c) circular corridor Table 2: Performance of the EMT-based solution in three Tag Game domains and two observability models: I) omniposition quarry, II) quarry is not at hunters position Model Domain Capture% E(Steps) Time/Step I Dead-ends 100 14.8 72(mSec) Arena 80.2 42.4 500(mSec) Circle 91.4 34.6 187(mSec) II Dead-ends 100 13.2 91(mSec) Arena 96.8 28.67 396(mSec) Circle 94.4 31.63 204(mSec) ical entropy grows with the length of interaction.",
                "For runs where the quarry was not captured immediately, the entropy reaches between 0.85 and 0.952 for different runs and scenarios.",
                "As the agent actively seeks the quarry, the entropy never reaches its maximum.",
                "One characteristic of the entropy graph for the open arena scenario particularly caught our attention in the case of the omniposition quarry observation model.",
                "Near the maximum limit of trial length (100 steps), entropy suddenly dropped.",
                "Further analysis of the data showed that under certain circumstances, a fluctuating behavior occurs in which the agent faces equally viable versions of quarry-following behavior.",
                "Since the EMT algorithm has greedy action selection, and the state space does not encode any form of commitment (not even speed or acceleration), the agent is locked within a small portion of cells.",
                "It is essentially attempting to simultaneously follow several courses of action, all of which are consistent with the target dynamics.",
                "This behavior did not occur in our second observation model, since it significantly reduced the set of eligible courses of action-essentially contributing to tie-breaking among them. 6.",
                "DISCUSSION The design of the EMT solution for the Tag Game exposes the core difference in approach to planning and control between EMT or DBC, on the one hand, and the more familiar POMDP approach, on the other.",
                "POMDP defines a reward structure to optimize, and influences system dynamics indirectly through that optimization.",
                "EMT discards any reward scheme, and instead measures and influences system dynamics directly. 2 Entropy was calculated using log base equal to the number of possible locations within the domain; this properly scales entropy expression into the range [0, 1] for all domains.",
                "Thus for the Tag Game, we did not search for a reward function that would encode and express our preference over the agents behavior, but rather directly set three (heuristic) behavior preferences as the basis for target dynamics to be maintained.",
                "Experimental data shows that these targets need not be directly achievable via the agents actions.",
                "However, the ratio between EMT performance and achievability of target dynamics remains to be explored.",
                "The tag game experiment data also revealed the different emphasis DBC and POMDPs place on the formulation of an environment state space.",
                "POMDPs rely entirely on the mechanism of reward accumulation maximization, i.e., formation of the action selection procedure to achieve necessary state sequencing.",
                "DBC, on the other hand, has two sources of sequencing specification: through the properties of an action selection procedure, and through direct specification within the target dynamics.",
                "The importance of the second source was underlined by the Tag Game experiment data, in which the greedy EMT algorithm, applied to a POMDP-type state space specification, failed, since target description over such a state space was incapable of encoding the necessary behavior tendencies, e.g., tie-breaking and commitment to directed motion.",
                "The structural differences between DBC (and EMT in particular), and POMDPs, prohibits direct performance comparison, and places them on complementary tracks, each within a suitable niche.",
                "For instance, POMDPs could be seen as a much more natural formulation of economic sequential decision-making problems, while EMT is a better fit for continual demand for stochastic change, as happens in many robotic or embodied-agent problems.",
                "The complementary properties of POMDPs and EMT can be further exploited.",
                "There is recent interest in using POMDPs in hybrid solutions [17], in which the POMDPs can be used together with other control approaches to provide results not easily achievable with either approach by itself.",
                "DBC can be an effective partner in such a hybrid solution.",
                "For instance, POMDPs have prohibitively large off-line time requirements for policy computation, but can be readily used in simpler settings to expose beneficial behavioral trends; this can serve as a form of target dynamics that are provided to EMT in a larger domain for on-line operation. 7.",
                "CONCLUSIONS AND FUTURE WORK In this paper, we have presented a novel perspective on the process of planning and control in stochastic environments, in the form of the Dynamics Based Control (DBC) framework.",
                "DBC formulates the task of planning as support of a specified target system dynamics, which describes the necessary properties of change within the environment.",
                "Optimality of DBC plans of action are measured The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) 795 0 20 40 60 80 100 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1 Steps Entropy Dead−ends 0 20 40 60 80 100 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1 Steps Entropy Arena 0 20 40 60 80 100 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1 Steps Entropy Circle Figure 4: Observation Model I: Omniposition quarry.",
                "Entropy development with length of Tag Game for the three experiment scenarios: a) multiple dead-end, b) irregular open arena, c) circular corridor. 0 10 20 30 40 50 60 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1 Steps Entropy Dead−ends 0 20 40 60 80 100 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1 Steps Entropy Arena 0 20 40 60 80 100 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1 Steps Entropy Circle Figure 5: Observation Model II: quarry not observed at hunters position.",
                "Entropy development with length of Tag Game for the three experiment scenarios: a) multiple dead-end, b) irregular open arena, c) circular corridor. with respect to the deviation of actual system dynamics from the target dynamics.",
                "We show that a recently developed technique of Extended Markov Tracking (EMT) [13] is an instantiation of DBC.",
                "In fact, EMT can be seen as a specific case of DBC parameterization, which employs a greedy action selection procedure.",
                "Since EMT exhibits the key features of the general DBC framework, as well as polynomial time complexity, we used the multitarget version of EMT [15] to demonstrate that the class of area sweeping problems naturally lends itself to dynamics-based descriptions, as instantiated by our experiments in the Tag Game domain.",
                "As enumerated in Section 4.2, EMT has a number of limitations, such as difficulty in dealing with negative dynamic preference.",
                "This prevents direct application of EMT to such problems as Rendezvous-Evasion Games (e.g., [6]).",
                "However, DBC in general has no such limitations, and readily enables the formulation of evasion games.",
                "In future work, we intend to proceed with the development of dynamics-based controllers for these problems. 8.",
                "ACKNOWLEDGMENT The work of the first two authors was partially supported by Israel Science Foundation grant #898/05, and the third author was partially supported by a grant from Israels Ministry of Science and Technology. 9.",
                "REFERENCES [1] R. C. Arkin.",
                "Behavior-Based Robotics.",
                "MIT Press, 1998. [2] J.",
                "A. Bilmes.",
                "A gentle tutorial of the EM algorithm and its application to parameter estimation for Gaussian mixture and Hidden Markov Models.",
                "Technical Report TR-97-021, Department of Electrical Engeineering and Computer Science, University of California at Berkeley, 1998. [3] T. M. Cover and J.",
                "A. Thomas.",
                "Elements of information theory.",
                "Wiley, 1991. [4] M. E. desJardins, E. H. Durfee, C. L. Ortiz, and M. J. Wolverton.",
                "A survey of research in distributed, continual planning.",
                "AI Magazine, 4:13-22, 1999. [5] V. R. Konda and J. N. Tsitsiklis.",
                "Actor-Critic algorithms.",
                "SIAM Journal on Control and Optimization, 42(4):1143-1166, 2003. [6] W. S. Lim.",
                "A rendezvous-evasion game on discrete locations with joint randomization.",
                "Advances in Applied Probability, 29(4):1004-1017, December 1997. [7] M. L. Littman, T. L. Dean, and L. P. Kaelbling.",
                "On the complexity of solving Markov decision problems.",
                "In Proceedings of the 11th Annual Conference on Uncertainty in Artificial Intelligence (UAI-95), pages 394-402, 1995. [8] O. Madani, S. Hanks, and A. Condon.",
                "On the undecidability of probabilistic planning and related stochastic optimization problems.",
                "Artificial Intelligence Journal, 147(1-2):5-34, July 2003. [9] R. M. Neal and G. E. Hinton.",
                "A view of the EM algorithm 796 The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) that justifies incremental, sparse, and other variants.",
                "In M. I. Jordan, editor, Learning in Graphical Models, pages 355-368.",
                "Kluwer Academic Publishers, 1998. [10] P. Paruchuri, M. Tambe, F. Ordonez, and S. Kraus.",
                "Security in multiagent systems by policy randomization.",
                "In Proceeding of AAMAS 2006, 2006. [11] J. Pineau, G. Gordon, and S. Thrun.",
                "Point-based value iteration: An anytime algorithm for pomdps.",
                "In International Joint Conference on Artificial Intelligence (IJCAI), pages 1025-1032, August 2003. [12] M. L. Puterman.",
                "Markov Decision Processes.",
                "Wiley Series in Probability and Mathematical Statistics: Applied Probability and Statistics Section.",
                "Wiley-Interscience Publication, New York, 1994. [13] Z. Rabinovich and J. S. Rosenschein.",
                "Extended Markov Tracking with an application to control.",
                "In The Workshop on Agent Tracking: Modeling Other Agents from Observations, at the Third International Joint Conference on Autonomous Agents and Multiagent Systems, pages 95-100, New-York, July 2004. [14] Z. Rabinovich and J. S. Rosenschein.",
                "Multiagent coordination by Extended Markov Tracking.",
                "In The Fourth International Joint Conference on Autonomous Agents and Multiagent Systems, pages 431-438, Utrecht, The Netherlands, July 2005. [15] Z. Rabinovich and J. S. Rosenschein.",
                "On the response of EMT-based control to interacting targets and models.",
                "In The Fifth International Joint Conference on Autonomous Agents and Multiagent Systems, pages 465-470, Hakodate, Japan, May 2006. [16] R. F. Stengel.",
                "Optimal Control and Estimation.",
                "Dover Publications, 1994. [17] M. Tambe, E. Bowring, H. Jung, G. Kaminka, R. Maheswaran, J. Marecki, J. Modi, R. Nair, J. Pearce, P. Paruchuri, D. Pynadath, P. Scerri, N. Schurr, and P. Varakantham.",
                "Conflicts in teamwork: Hybrids to the The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) 797"
            ],
            "original_annotated_samples": [],
            "translated_annotated_samples": [],
            "translated_text": "",
            "candidates": [],
            "error": []
        },
        "target dynamics": {
            "translated_key": "dinámica de objetivos",
            "is_in_text": true,
            "original_annotated_sentences": [
                "Dynamics Based Control with an Application to Area-Sweeping Problems Zinovi Rabinovich Engineering and Computer Science Hebrew University of Jerusalem Jerusalem, Israel nomad@cs.huji.ac.il Jeffrey S. Rosenschein Engineering and Computer Science Hebrew University of Jerusalem Jerusalem, Israel jeff@cs.huji.ac.il Gal A. Kaminka The MAVERICK Group Department of Computer Science Bar Ilan University, Israel galk@cs.biu.ac.il ABSTRACT In this paper we introduce Dynamics Based Control (DBC), an approach to planning and control of an agent in stochastic environments.",
                "Unlike existing approaches, which seek to optimize expected rewards (e.g., in Partially Observable Markov Decision Problems (POMDPs)), DBC optimizes system behavior towards specified system dynamics.",
                "We show that a recently developed planning and control approach, Extended Markov Tracking (EMT) is an instantiation of DBC.",
                "EMT employs greedy action selection to provide an efficient control algorithm in Markovian environments.",
                "We exploit this efficiency in a set of experiments that applied multitarget EMT to a class of area-sweeping problems (searching for moving targets).",
                "We show that such problems can be naturally defined and efficiently solved using the DBC framework, and its EMT instantiation.",
                "Categories and Subject Descriptors I.2.8 [Problem Solving, Control Methods, and Search]: Control Theory; I.2.9 [Robotics]; I.2.11 [Distributed Artificial Intelligence]: Intelligent Agents General Terms Algorithms, Theory 1.",
                "INTRODUCTION Planning and control constitutes a central research area in multiagent systems and artificial intelligence.",
                "In recent years, Partially Observable Markov Decision Processes (POMDPs) [12] have become a popular formal basis for planning in stochastic environments.",
                "In this framework, the planning and control problem is often addressed by imposing a reward function, and computing a policy (of choosing actions) that is optimal, in the sense that it will result in the highest expected utility.",
                "While theoretically attractive, the complexity of optimally solving a POMDP is prohibitive [8, 7].",
                "We take an alternative view of planning in stochastic environments.",
                "We do not use a (state-based) reward function, but instead optimize over a different criterion, a transition-based specification of the desired system dynamics.",
                "The idea here is to view planexecution as a process that compels a (stochastic) system to change, and a plan as a dynamic process that shapes that change according to desired criteria.",
                "We call this general planning framework Dynamics Based Control (DBC).",
                "In DBC, the goal of a planning (or control) process becomes to ensure that the system will change in accordance with specific (potentially stochastic) <br>target dynamics</br>.",
                "As actual system behavior may deviate from that which is specified by <br>target dynamics</br> (due to the stochastic nature of the system), planning in such environments needs to be continual [4], in a manner similar to classical closed-loop controllers [16].",
                "Here, optimality is measured in terms of probability of deviation magnitudes.",
                "In this paper, we present the structure of Dynamics Based Control.",
                "We show that the recently developed Extended Markov Tracking (EMT) approach [13, 14, 15] is subsumed by DBC, with EMT employing greedy action selection, which is a specific parameterization among the options possible within DBC.",
                "EMT is an efficient instantiation of DBC.",
                "To evaluate DBC, we carried out a set of experiments applying multi-target EMT to the Tag Game [11]; this is a variant on the area sweeping problem, where an agent is trying to tag a moving target (quarry) whose position is not known with certainty.",
                "Experimental data demonstrates that even with a simple model of the environment and a simple design of <br>target dynamics</br>, high success rates can be produced both in catching the quarry, and in surprising the quarry (as expressed by the observed entropy of the controlled agents position).",
                "The paper is organized as follows.",
                "In Section 2 we motivate DBC using area-sweeping problems, and discuss related work.",
                "Section 3 introduces the Dynamics Based Control (DBC) structure, and its specialization to Markovian environments.",
                "This is followed by a review of the Extended Markov Tracking (EMT) approach as a DBC-structured control regimen in Section 4.",
                "That section also discusses the limitations of EMT-based control relative to the general DBC framework.",
                "Experimental settings and results are then presented in Section 5.",
                "Section 6 provides a short discussion of the overall approach, and Section 7 gives some concluding remarks and directions for future work. 790 978-81-904262-7-5 (RPS) c 2007 IFAAMAS 2.",
                "MOTIVATION AND RELATED WORK Many real-life scenarios naturally have a stochastic <br>target dynamics</br> specification, especially those domains where there exists no ultimate goal, but rather system behavior (with specific properties) that has to be continually supported.",
                "For example, security guards perform persistent sweeps of an area to detect any sign of intrusion.",
                "Cunning thieves will attempt to track these sweeps, and time their operation to key points of the guards motion.",
                "It is thus advisable to make the guards motion dynamics appear irregular and random.",
                "Recent work by Paruchuri et al. [10] has addressed such randomization in the context of single-agent and distributed POMDPs.",
                "The goal in that work was to generate policies that provide a measure of action-selection randomization, while maintaining rewards within some acceptable levels.",
                "Our focus differs from this work in that DBC does not optimize expected rewards-indeed we do not consider rewards at all-but instead maintains desired dynamics (including, but not limited to, randomization).",
                "The Game of Tag is another example of the applicability of the approach.",
                "It was introduced in the work by Pineau et al. [11].",
                "There are two agents that can move about an area, which is divided into a grid.",
                "The grid may have blocked cells (holes) into which no agent can move.",
                "One agent (the hunter) seeks to move into a cell occupied by the other (the quarry), such that they are co-located (this is a successful tag).",
                "The quarry seeks to avoid the hunter agent, and is always aware of the hunters position, but does not know how the hunter will behave, which opens up the possibility for a hunter to surprise the prey.",
                "The hunter knows the quarrys probabilistic law of motion, but does not know its current location.",
                "Tag is an instance of a family of area-sweeping (pursuit-evasion) problems.",
                "In [11], the hunter modeled the problem using a POMDP.",
                "A reward function was defined, to reflect the desire to tag the quarry, and an action policy was computed to optimize the reward collected over time.",
                "Due to the intractable complexity of determining the optimal policy, the action policy computed in that paper was essentially an approximation.",
                "In this paper, instead of formulating a reward function, we use EMT to solve the problem, by directly specifying the <br>target dynamics</br>.",
                "In fact, any search problem with randomized motion, the socalled class of area sweeping problems, can be described through specification of such target system dynamics.",
                "Dynamics Based Control provides a natural approach to solving these problems. 3.",
                "DYNAMICS BASED CONTROL The specification of Dynamics Based Control (DBC) can be broken into three interacting levels: Environment Design Level, User Level, and Agent Level. • Environment Design Level is concerned with the formal specification and modeling of the environment.",
                "For example, this level would specify the laws of physics within the system, and set its parameters, such as the gravitation constant. • User Level in turn relies on the environment model produced by Environment Design to specify the target system dynamics it wishes to observe.",
                "The User Level also specifies the estimation or learning procedure for system dynamics, and the measure of deviation.",
                "In the museum guard scenario above, these would correspond to a stochastic sweep schedule, and a measure of relative surprise between the specified and actual sweeping. • Agent Level in turn combines the environment model from the Environment Design level, the dynamics estimation procedure, the deviation measure and the <br>target dynamics</br> specification from User Level, to produce a sequence of actions that create system dynamics as close as possible to the targeted specification.",
                "As we are interested in the continual development of a stochastic system, such as happens in classical control theory [16] and continual planning [4], as well as in our example of museum sweeps, the question becomes how the Agent Level is to treat the deviation measurements over time.",
                "To this end, we use a probability threshold-that is, we would like the Agent Level to maximize the probability that the deviation measure will remain below a certain threshold.",
                "Specific action selection then depends on system formalization.",
                "One possibility would be to create a mixture of available system trends, much like that which happens in Behavior-Based Robotic architectures [1].",
                "The other alternative would be to rely on the estimation procedure provided by the User Level-to utilize the Environment Design Level model of the environment to choose actions, so as to manipulate the dynamics estimator into believing that a certain dynamics has been achieved.",
                "Notice that this manipulation is not direct, but via the environment.",
                "Thus, for strong enough estimator algorithms, successful manipulation would mean a successful simulation of the specified <br>target dynamics</br> (i.e., beyond discerning via the available sensory input).",
                "DBC levels can also have a back-flow of information (see Figure 1).",
                "For instance, the Agent Level could provide data about <br>target dynamics</br> feasibility, allowing the User Level to modify the requirement, perhaps focusing on attainable features of system behavior.",
                "Data would also be available about the system response to different actions performed; combined with a dynamics estimator defined by the User Level, this can provide an important tool for the environment model calibration at the Environment Design Level.",
                "UserEnv.",
                "Design Agent Model Ideal Dynamics Estimator Estimator Dynamics Feasibility System Response Data Figure 1: Data flow of the DBC framework Extending upon the idea of Actor-Critic algorithms [5], DBC data flow can provide a good basis for the design of a learning algorithm.",
                "For example, the User Level can operate as an exploratory device for a learning algorithm, inferring an ideal dynamics target from the environment model at hand that would expose and verify most critical features of system behavior.",
                "In this case, feasibility and system response data from the Agent Level would provide key information for an environment model update.",
                "In fact, the combination of feasibility and response data can provide a basis for the application of strong learning algorithms such as EM [2, 9]. 3.1 DBC for Markovian Environments For a Partially Observable Markovian Environment, DBC can be specified in a more rigorous manner.",
                "Notice how DBC discards rewards, and replaces it by another optimality criterion (structural differences are summarized in Table 1): • Environment Design level is to specify a tuple < S, A, T, O, Ω, s0 >, where: - S is the set of all possible environment states; - s0 is the initial state of the environment (which can also be viewed as a probability distribution over S); The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) 791 - A is the set of all possible actions applicable in the environment; - T is the environments probabilistic transition function: T : S ×A → Π(S).",
                "That is, T(s |a, s) is the probability that the environment will move from state s to state s under action a; - O is the set of all possible observations.",
                "This is what the sensor input would look like for an outside observer; - Ω is the observation probability function: Ω : S × A × S → Π(O).",
                "That is, Ω(o|s , a, s) is the probability that one will observe o given that the environment has moved from state s to state s under action a. • User Level, in the case of Markovian environment, operates on the set of system dynamics described by a family of conditional probabilities F = {τ : S × A → Π(S)}.",
                "Thus specification of <br>target dynamics</br> can be expressed by q ∈ F, and the learning or tracking algorithm can be represented as a function L : O×(A×O)∗ → F; that is, it maps sequences of observations and actions performed so far into an estimate τ ∈ F of system dynamics.",
                "There are many possible variations available at the User Level to define divergence between system dynamics; several of them are: - Trace distance or L1 distance between two distributions p and q defined by D(p(·), q(·)) = 1 2 x |p(x) − q(x)| - Fidelity measure of distance F(p(·), q(·)) = x p(x)q(x) - Kullback-Leibler divergence DKL(p(·) q(·)) = x p(x) log p(x) q(x) Notice that the latter two are not actually metrics over the space of possible distributions, but nevertheless have meaningful and important interpretations.",
                "For instance, KullbackLeibler divergence is an important tool of information theory [3] that allows one to measure the price of encoding an information source governed by q, while assuming that it is governed by p. The User Level also defines the threshold of dynamics deviation probability θ. • Agent Level is then faced with a problem of selecting a control signal function a∗ to satisfy a minimization problem as follows: a∗ = arg min a Pr(d(τa, q) > θ) where d(τa, q) is a random variable describing deviation of the dynamics estimate τa, created by L under control signal a, from the ideal dynamics q.",
                "Implicit in this minimization problem is that L is manipulated via the environment, based on the environment model produced by the Environment Design Level. 3.2 DBC View of the State Space It is important to note the complementary view that DBC and POMDPs take on the state space of the environment.",
                "POMDPs regard state as a stationary snap-shot of the environment; whatever attributes of state sequencing one seeks are reached through properties of the control process, in this case reward accumulation.",
                "This can be viewed as if the sequencing of states and the attributes of that sequencing are only introduced by and for the controlling mechanism-the POMDP policy.",
                "DBC concentrates on the underlying principle of state sequencing, the system dynamics.",
                "DBCs <br>target dynamics</br> specification can use the environments state space as a means to describe, discern, and preserve changes that occur within the system.",
                "As a result, DBC has a greater ability to express state sequencing properties, which are grounded in the environment model and its state space definition.",
                "For example, consider the task of moving through rough terrain towards a goal and reaching it as fast as possible.",
                "POMDPs would encode terrain as state space points, while speed would be ensured by negative reward for every step taken without reaching the goalaccumulating higher reward can be reached only by faster motion.",
                "Alternatively, the state space could directly include the notion of speed.",
                "For POMDPs, this would mean that the same concept is encoded twice, in some sense: directly in the state space, and indirectly within reward accumulation.",
                "Now, even if the reward function would encode more, and finer, details of the properties of motion, the POMDP solution will have to search in a much larger space of policies, while still being guided by the implicit concept of the reward accumulation procedure.",
                "On the other hand, the tactical target expression of variations and correlations between position and speed of motion is now grounded in the state space representation.",
                "In this situation, any further constraints, e.g., smoothness of motion, speed limits in different locations, or speed reductions during sharp turns, are explicitly and uniformly expressed by the tactical target, and can result in faster and more effective action selection by a DBC algorithm. 4.",
                "EMT-BASED CONTROL AS A DBC Recently, a control algorithm was introduced called EMT-based Control [13], which instantiates the DBC framework.",
                "Although it provides an approximate greedy solution in the DBC sense, initial experiments using EMT-based control have been encouraging [14, 15].",
                "EMT-based control is based on the Markovian environment definition, as in the case with POMDPs, but its User and Agent Levels are of the Markovian DBC type of optimality. • User Level of EMT-based control defines a limited-case target system dynamics independent of action: qEMT : S → Π(S).",
                "It then utilizes the Kullback-Leibler divergence measure to compose a momentary system dynamics estimator-the Extended Markov Tracking (EMT) algorithm.",
                "The algorithm keeps a system dynamics estimate τt EMT that is capable of explaining recent change in an auxiliary Bayesian system state estimator from pt−1 to pt, and updates it conservatively using Kullback-Leibler divergence.",
                "Since τt EMT and pt−1,t are respectively the conditional and marginal probabilities over the systems state space, explanation simply means that pt(s ) = s τt EMT (s |s)pt−1(s), and the dynamics estimate update is performed by solving a 792 The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) Table 1: Structure of POMDP vs. Dynamics-Based Control in Markovian Environment Level Approach MDP Markovian DBC Environment < S, A, T, O, Ω >,where S - set of states A - set of actions Design T : S × A → Π(S) - transition O - observation set Ω : S × A × S → Π(O) User r : S × A × S → R q : S × A → Π(S) F(π∗ ) → r L(o1, ..., ot) → τ r - reward function q - ideal dynamics F - reward remodeling L - dynamics estimator θ - threshold Agent π∗ = arg max π E[ γt rt] π∗ = arg min π Prob(d(τ q) > θ) minimization problem: τt EMT = H[pt, pt−1, τt−1 EMT ] = arg min τ DKL(τ × pt−1 τt−1 EMT × pt−1) s.t. pt(s ) = s (τ × pt−1)(s , s) pt−1(s) = s (τ × pt−1)(s , s) • Agent Level in EMT-based control is suboptimal with respect to DBC (though it remains within the DBC framework), performing greedy action selection based on prediction of EMTs reaction.",
                "The prediction is based on the environment model provided by the Environment Design level, so that if we denote by Ta the environments transition function limited to action a, and pt−1 is the auxiliary Bayesian system state estimator, then the EMT-based control choice is described by a∗ = arg min a∈A DKL(H[Ta × pt, pt, τt EMT ] qEMT × pt−1) Note that this follows the Markovian DBC framework precisely: the rewarding optimality of POMDPs is discarded, and in its place a dynamics estimator (EMT in this case) is manipulated via action effects on the environment to produce an estimate close to the specified target system dynamics.",
                "Yet as we mentioned, naive EMTbased control is suboptimal in the DBC sense, and has several additional limitations that do not exist in the general DBC framework (discussed in Section 4.2). 4.1 Multi-Target EMT At times, there may exist several behavioral preferences.",
                "For example, in the case of museum guards, some art items are more heavily guarded, requiring that the guards stay more often in their close vicinity.",
                "On the other hand, no corner of the museum is to be left unchecked, which demands constant motion.",
                "Successful museum security would demand that the guards adhere to, and balance, both of these behaviors.",
                "For EMT-based control, this would mean facing several tactical targets {qk}K k=1, and the question becomes how to merge and balance them.",
                "A balancing mechanism can be applied to resolve this issue.",
                "Note that EMT-based control, while selecting an action, creates a preference vector over the set of actions based on their predicted performance with respect to a given target.",
                "If these preference vectors are normalized, they can be combined into a single unified preference.",
                "This requires replacement of standard EMT-based action selection by the algorithm below [15]: • Given: - a set of <br>target dynamics</br> {qk}K k=1, - vector of weights w(k) • Select action as follows - For each action a ∈ A predict the future state distribution ¯pa t+1 = Ta ∗ pt; - For each action, compute Da = H(¯pa t+1, pt, PDt) - For each a ∈ A and qk tactical target, denote V (a, k) = DKL (Da qk) pt .",
                "Let Vk(a) = 1 Zk V (a, k), where Zk = a∈A V (a, k) is a normalization factor. - Select a∗ = arg min a k k=1 w(k)Vk(a) The weights vector w = (w1, ..., wK ) allows the additional tuning of importance among <br>target dynamics</br> without the need to redesign the targets themselves.",
                "This balancing method is also seamlessly integrated into the EMT-based control flow of operation. 4.2 EMT-based Control Limitations EMT-based control is a sub-optimal (in the DBC sense) representative of the DBC structure.",
                "It limits the User by forcing EMT to be its dynamic tracking algorithm, and replaces Agent optimization by greedy action selection.",
                "This kind of combination, however, is common for on-line algorithms.",
                "Although further development of EMT-based controllers is necessary, evidence so far suggests that even the simplest form of the algorithm possesses a great deal of power, and displays trends that are optimal in the DBC sense of the word.",
                "There are two further, EMT-specific, limitations to EMT-based control that are evident at this point.",
                "Both already have partial solutions and are subjects of ongoing research.",
                "The first limitation is the problem of negative preference.",
                "In the POMDP framework for example, this is captured simply, through The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) 793 the appearance of values with different signs within the reward structure.",
                "For EMT-based control, however, negative preference means that one would like to avoid a certain distribution over system development sequences; EMT-based control, however, concentrates on getting as close as possible to a distribution.",
                "Avoidance is thus unnatural in native EMT-based control.",
                "The second limitation comes from the fact that standard environment modeling can create pure sensory actions-actions that do not change the state of the world, and differ only in the way observations are received and the quality of observations received.",
                "Since the world state does not change, EMT-based control would not be able to differentiate between different sensory actions.",
                "Notice that both of these limitations of EMT-based control are absent from the general DBC framework, since it may have a tracking algorithm capable of considering pure sensory actions and, unlike Kullback-Leibler divergence, a distribution deviation measure that is capable of dealing with negative preference. 5.",
                "EMT PLAYING TAG The Game of Tag was first introduced in [11].",
                "It is a single agent problem of capturing a quarry, and belongs to the class of area sweeping problems.",
                "An example domain is shown in Figure 2. 0 51 2 3 4 6 7 8 10 12 13 161514 17 18 19 2221 23 9 11Q A 20 Figure 2: Tag domain; an agent (A) attempts to seek and capture a quarry (Q) The Game of Tag extremely limits the agents perception, so that the agent is able to detect the quarry only if they are co-located in the same cell of the grid world.",
                "In the classical version of the game, co-location leads to a special observation, and the Tag action can be performed.",
                "We slightly modify this setting: the moment that both agents occupy the same cell, the game ends.",
                "As a result, both the agent and its quarry have the same motion capability, which allows them to move in four directions, North, South, East, and West.",
                "These form a formal space of actions within a Markovian environment.",
                "The state space of the formal Markovian environment is described by the cross-product of the agent and quarrys positions.",
                "For Figure 2, it would be S = {s0, ..., s23} × {s0, ..., s23}.",
                "The effects of an action taken by the agent are deterministic, but the environment in general has a stochastic response due to the motion of the quarry.",
                "With probability q0 1 it stays put, and with probability 1 − q0 it moves to an adjacent cell further away from the 1 In our experiments this was taken to be q0 = 0.2. agent.",
                "So for the instance shown in Figure 2 and q0 = 0.1: P(Q = s9|Q = s9, A = s11) = 0.1 P(Q = s2|Q = s9, A = s11) = 0.3 P(Q = s8|Q = s9, A = s11) = 0.3 P(Q = s14|Q = s9, A = s11) = 0.3 Although the evasive behavior of the quarry is known to the agent, the quarrys position is not.",
                "The only sensory information available to the agent is its own location.",
                "We use EMT and directly specify the <br>target dynamics</br>.",
                "For the Game of Tag, we can easily formulate three major trends: catching the quarry, staying mobile, and stalking the quarry.",
                "This results in the following three <br>target dynamics</br>: Tcatch(At+1 = si|Qt = sj, At = sa) ∝ 1 si = sj 0 otherwise Tmobile(At+1 = si|Qt = so, At = sj) ∝ 0 si = sj 1 otherwise Tstalk(At+1 = si|Qt = so, At = sj) ∝ 1 dist(si,so) Note that none of the above targets are directly achievable; for instance, if Qt = s9 and At = s11, there is no action that can move the agent to At+1 = s9 as required by the Tcatch <br>target dynamics</br>.",
                "We ran several experiments to evaluate EMT performance in the Tag Game.",
                "Three configurations of the domain shown in Figure 3 were used, each posing a different challenge to the agent due to partial observability.",
                "In each setting, a set of 1000 runs was performed with a time limit of 100 steps.",
                "In every run, the initial position of both the agent and its quarry was selected at random; this means that as far as the agent was concerned, the quarrys initial position was uniformly distributed over the entire domain cell space.",
                "We also used two variations of the environment observability function.",
                "In the first version, observability function mapped all joint positions of hunter and quarry into the position of the hunter as an observation.",
                "In the second, only those joint positions in which hunter and quarry occupied different locations were mapped into the hunters location.",
                "The second version in fact utilized and expressed the fact that once hunter and quarry occupy the same cell the game ends.",
                "The results of these experiments are shown in Table 2.",
                "Balancing [15] the catch, move, and stalk <br>target dynamics</br> described in the previous section by the weight vector [0.8, 0.1, 0.1], EMT produced stable performance in all three domains.",
                "Although direct comparisons are difficult to make, the EMT performance displayed notable efficiency vis-`a-vis the POMDP approach.",
                "In spite of a simple and inefficient Matlab implementation of the EMT algorithm, the decision time for any given step averaged significantly below 1 second in all experiments.",
                "For the irregular open arena domain, which proved to be the most difficult, 1000 experiment runs bounded by 100 steps each, a total of 42411 steps, were completed in slightly under 6 hours.",
                "That is, over 4 × 104 online steps took an order of magnitude less time than the offline computation of POMDP policy in [11].",
                "The significance of this differential is made even more prominent by the fact that, should the environment model parameters change, the online nature of EMT would allow it to maintain its performance time, while the POMDP policy would need to be recomputed, requiring yet again a large overhead of computation.",
                "We also tested the behavior cell frequency entropy, empirical measures from trial data.",
                "As Figure 4 and Figure 5 show, empir794 The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) A Q Q A 0 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 A Q Figure 3: These configurations of the Tag Game space were used: a) multiple dead-end, b) irregular open arena, c) circular corridor Table 2: Performance of the EMT-based solution in three Tag Game domains and two observability models: I) omniposition quarry, II) quarry is not at hunters position Model Domain Capture% E(Steps) Time/Step I Dead-ends 100 14.8 72(mSec) Arena 80.2 42.4 500(mSec) Circle 91.4 34.6 187(mSec) II Dead-ends 100 13.2 91(mSec) Arena 96.8 28.67 396(mSec) Circle 94.4 31.63 204(mSec) ical entropy grows with the length of interaction.",
                "For runs where the quarry was not captured immediately, the entropy reaches between 0.85 and 0.952 for different runs and scenarios.",
                "As the agent actively seeks the quarry, the entropy never reaches its maximum.",
                "One characteristic of the entropy graph for the open arena scenario particularly caught our attention in the case of the omniposition quarry observation model.",
                "Near the maximum limit of trial length (100 steps), entropy suddenly dropped.",
                "Further analysis of the data showed that under certain circumstances, a fluctuating behavior occurs in which the agent faces equally viable versions of quarry-following behavior.",
                "Since the EMT algorithm has greedy action selection, and the state space does not encode any form of commitment (not even speed or acceleration), the agent is locked within a small portion of cells.",
                "It is essentially attempting to simultaneously follow several courses of action, all of which are consistent with the <br>target dynamics</br>.",
                "This behavior did not occur in our second observation model, since it significantly reduced the set of eligible courses of action-essentially contributing to tie-breaking among them. 6.",
                "DISCUSSION The design of the EMT solution for the Tag Game exposes the core difference in approach to planning and control between EMT or DBC, on the one hand, and the more familiar POMDP approach, on the other.",
                "POMDP defines a reward structure to optimize, and influences system dynamics indirectly through that optimization.",
                "EMT discards any reward scheme, and instead measures and influences system dynamics directly. 2 Entropy was calculated using log base equal to the number of possible locations within the domain; this properly scales entropy expression into the range [0, 1] for all domains.",
                "Thus for the Tag Game, we did not search for a reward function that would encode and express our preference over the agents behavior, but rather directly set three (heuristic) behavior preferences as the basis for <br>target dynamics</br> to be maintained.",
                "Experimental data shows that these targets need not be directly achievable via the agents actions.",
                "However, the ratio between EMT performance and achievability of <br>target dynamics</br> remains to be explored.",
                "The tag game experiment data also revealed the different emphasis DBC and POMDPs place on the formulation of an environment state space.",
                "POMDPs rely entirely on the mechanism of reward accumulation maximization, i.e., formation of the action selection procedure to achieve necessary state sequencing.",
                "DBC, on the other hand, has two sources of sequencing specification: through the properties of an action selection procedure, and through direct specification within the <br>target dynamics</br>.",
                "The importance of the second source was underlined by the Tag Game experiment data, in which the greedy EMT algorithm, applied to a POMDP-type state space specification, failed, since target description over such a state space was incapable of encoding the necessary behavior tendencies, e.g., tie-breaking and commitment to directed motion.",
                "The structural differences between DBC (and EMT in particular), and POMDPs, prohibits direct performance comparison, and places them on complementary tracks, each within a suitable niche.",
                "For instance, POMDPs could be seen as a much more natural formulation of economic sequential decision-making problems, while EMT is a better fit for continual demand for stochastic change, as happens in many robotic or embodied-agent problems.",
                "The complementary properties of POMDPs and EMT can be further exploited.",
                "There is recent interest in using POMDPs in hybrid solutions [17], in which the POMDPs can be used together with other control approaches to provide results not easily achievable with either approach by itself.",
                "DBC can be an effective partner in such a hybrid solution.",
                "For instance, POMDPs have prohibitively large off-line time requirements for policy computation, but can be readily used in simpler settings to expose beneficial behavioral trends; this can serve as a form of <br>target dynamics</br> that are provided to EMT in a larger domain for on-line operation. 7.",
                "CONCLUSIONS AND FUTURE WORK In this paper, we have presented a novel perspective on the process of planning and control in stochastic environments, in the form of the Dynamics Based Control (DBC) framework.",
                "DBC formulates the task of planning as support of a specified target system dynamics, which describes the necessary properties of change within the environment.",
                "Optimality of DBC plans of action are measured The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) 795 0 20 40 60 80 100 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1 Steps Entropy Dead−ends 0 20 40 60 80 100 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1 Steps Entropy Arena 0 20 40 60 80 100 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1 Steps Entropy Circle Figure 4: Observation Model I: Omniposition quarry.",
                "Entropy development with length of Tag Game for the three experiment scenarios: a) multiple dead-end, b) irregular open arena, c) circular corridor. 0 10 20 30 40 50 60 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1 Steps Entropy Dead−ends 0 20 40 60 80 100 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1 Steps Entropy Arena 0 20 40 60 80 100 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1 Steps Entropy Circle Figure 5: Observation Model II: quarry not observed at hunters position.",
                "Entropy development with length of Tag Game for the three experiment scenarios: a) multiple dead-end, b) irregular open arena, c) circular corridor. with respect to the deviation of actual system dynamics from the <br>target dynamics</br>.",
                "We show that a recently developed technique of Extended Markov Tracking (EMT) [13] is an instantiation of DBC.",
                "In fact, EMT can be seen as a specific case of DBC parameterization, which employs a greedy action selection procedure.",
                "Since EMT exhibits the key features of the general DBC framework, as well as polynomial time complexity, we used the multitarget version of EMT [15] to demonstrate that the class of area sweeping problems naturally lends itself to dynamics-based descriptions, as instantiated by our experiments in the Tag Game domain.",
                "As enumerated in Section 4.2, EMT has a number of limitations, such as difficulty in dealing with negative dynamic preference.",
                "This prevents direct application of EMT to such problems as Rendezvous-Evasion Games (e.g., [6]).",
                "However, DBC in general has no such limitations, and readily enables the formulation of evasion games.",
                "In future work, we intend to proceed with the development of dynamics-based controllers for these problems. 8.",
                "ACKNOWLEDGMENT The work of the first two authors was partially supported by Israel Science Foundation grant #898/05, and the third author was partially supported by a grant from Israels Ministry of Science and Technology. 9.",
                "REFERENCES [1] R. C. Arkin.",
                "Behavior-Based Robotics.",
                "MIT Press, 1998. [2] J.",
                "A. Bilmes.",
                "A gentle tutorial of the EM algorithm and its application to parameter estimation for Gaussian mixture and Hidden Markov Models.",
                "Technical Report TR-97-021, Department of Electrical Engeineering and Computer Science, University of California at Berkeley, 1998. [3] T. M. Cover and J.",
                "A. Thomas.",
                "Elements of information theory.",
                "Wiley, 1991. [4] M. E. desJardins, E. H. Durfee, C. L. Ortiz, and M. J. Wolverton.",
                "A survey of research in distributed, continual planning.",
                "AI Magazine, 4:13-22, 1999. [5] V. R. Konda and J. N. Tsitsiklis.",
                "Actor-Critic algorithms.",
                "SIAM Journal on Control and Optimization, 42(4):1143-1166, 2003. [6] W. S. Lim.",
                "A rendezvous-evasion game on discrete locations with joint randomization.",
                "Advances in Applied Probability, 29(4):1004-1017, December 1997. [7] M. L. Littman, T. L. Dean, and L. P. Kaelbling.",
                "On the complexity of solving Markov decision problems.",
                "In Proceedings of the 11th Annual Conference on Uncertainty in Artificial Intelligence (UAI-95), pages 394-402, 1995. [8] O. Madani, S. Hanks, and A. Condon.",
                "On the undecidability of probabilistic planning and related stochastic optimization problems.",
                "Artificial Intelligence Journal, 147(1-2):5-34, July 2003. [9] R. M. Neal and G. E. Hinton.",
                "A view of the EM algorithm 796 The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) that justifies incremental, sparse, and other variants.",
                "In M. I. Jordan, editor, Learning in Graphical Models, pages 355-368.",
                "Kluwer Academic Publishers, 1998. [10] P. Paruchuri, M. Tambe, F. Ordonez, and S. Kraus.",
                "Security in multiagent systems by policy randomization.",
                "In Proceeding of AAMAS 2006, 2006. [11] J. Pineau, G. Gordon, and S. Thrun.",
                "Point-based value iteration: An anytime algorithm for pomdps.",
                "In International Joint Conference on Artificial Intelligence (IJCAI), pages 1025-1032, August 2003. [12] M. L. Puterman.",
                "Markov Decision Processes.",
                "Wiley Series in Probability and Mathematical Statistics: Applied Probability and Statistics Section.",
                "Wiley-Interscience Publication, New York, 1994. [13] Z. Rabinovich and J. S. Rosenschein.",
                "Extended Markov Tracking with an application to control.",
                "In The Workshop on Agent Tracking: Modeling Other Agents from Observations, at the Third International Joint Conference on Autonomous Agents and Multiagent Systems, pages 95-100, New-York, July 2004. [14] Z. Rabinovich and J. S. Rosenschein.",
                "Multiagent coordination by Extended Markov Tracking.",
                "In The Fourth International Joint Conference on Autonomous Agents and Multiagent Systems, pages 431-438, Utrecht, The Netherlands, July 2005. [15] Z. Rabinovich and J. S. Rosenschein.",
                "On the response of EMT-based control to interacting targets and models.",
                "In The Fifth International Joint Conference on Autonomous Agents and Multiagent Systems, pages 465-470, Hakodate, Japan, May 2006. [16] R. F. Stengel.",
                "Optimal Control and Estimation.",
                "Dover Publications, 1994. [17] M. Tambe, E. Bowring, H. Jung, G. Kaminka, R. Maheswaran, J. Marecki, J. Modi, R. Nair, J. Pearce, P. Paruchuri, D. Pynadath, P. Scerri, N. Schurr, and P. Varakantham.",
                "Conflicts in teamwork: Hybrids to the The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) 797"
            ],
            "original_annotated_samples": [],
            "translated_annotated_samples": [
                "En DBC, el objetivo de un proceso de planificación (o control) se convierte en garantizar que el sistema cambie de acuerdo con la \"dinámica objetivo\" específica (potencialmente estocástica).",
                "Como el comportamiento real del sistema puede desviarse de lo que se especifica por la \"dinámica objetivo\" (debido a la naturaleza estocástica del sistema), la planificación en tales entornos debe ser continua [4], de manera similar a los controladores clásicos de circuito cerrado [dieciséis].",
                "Los datos experimentales demuestran que incluso con un modelo simple del entorno y un diseño simple de \"dinámica objetivo\", se pueden producir altas tasas de éxito tanto para captar la cantera como para sorprender la cantera (como se expresa por la entropía observada de los agentes controladosposición).",
                "Motivación y trabajo relacionado Muchos escenarios de la vida real, naturalmente, tienen una especificación estocástica de \"dinámica objetivo\", especialmente aquellos dominios donde no existe un objetivo final, sino un comportamiento del sistema (con propiedades específicas) que deben ser compatibles continuamente.",
                "En este artículo, en lugar de formular una función de recompensa, usamos EMT para resolver el problema, especificando directamente la \"dinámica de destino\".",
                "En el escenario de la Guardia del Museo anterior, estos corresponderían a un horario estocástico de barrido, y una medida de sorpresa relativa entre el barrido especificado y real.• El nivel de agente a su vez combina el modelo de entorno desde el nivel de diseño del entorno, el procedimiento de estimación de la dinámica, la medida de desviación y la especificación de \"dinámica de destino\" desde el nivel del usuario, para producir una secuencia de acciones que crean dinámicas del sistema lo más cerca posible de laEspecificación dirigida.",
                "Por lo tanto, para los algoritmos estimadores lo suficientemente fuertes, la manipulación exitosa significaría una simulación exitosa de la \"dinámica objetivo\" especificada (es decir, más allá de la discernimiento a través de la entrada sensorial disponible).",
                "Por ejemplo, el nivel de agente podría proporcionar datos sobre la viabilidad de la \"dinámica de destino\", lo que permite al nivel del usuario modificar el requisito, tal vez centrarse en las características alcanzables del comportamiento del sistema.",
                "Por lo tanto, la especificación de la \"dinámica objetivo\" puede expresarse mediante q ∈ F, y el algoritmo de aprendizaje o seguimiento puede representarse en función l: o × (a × o) ∗ → f;Es decir, mapea secuencias de observaciones y acciones realizadas hasta ahora en una estimación τ ∈ F de la dinámica del sistema.",
                "La especificación de \"dinámica de destino\" de DBCS puede usar el espacio de estado de los entornos como un medio para describir, discernir y preservar los cambios que ocurren dentro del sistema.",
                "Esto requiere el reemplazo de la selección de acción basada en EMT estándar por el algoritmo a continuación [15]: • Dado: - Un conjunto de \"Dinámica de destino\" {Qk} k k = 1, - vector de pesos w (k) • Seleccione Acción de la siguiente manera- Para cada acción a ∈ A predecir la distribución de estado futuro ¯pa t+1 = ta ∗ pt;- Para cada acción, calcule DA = H (¯PA T+1, PT, PDT) - Para cada objetivo táctico A ∈ A y QK, denote v (a, k) = dkl (da qk) pt.",
                "Deje que Vk (a) = 1 zk v (a, k), donde zk = a∈A v (a, k) es un factor de normalización.- Seleccione a ∗ = arg min a k k = 1 w (k) vk (a) El vector de pesos w = (w1, ..., wk) permite el ajuste adicional de importancia entre la \"dinámica objetivo\" sin la necesidad de rediseñar los objetivosellos mismos.",
                "Usamos EMT y especificamos directamente la \"dinámica de destino\".",
                "Esto da como resultado las siguientes tres \"dinámica de destino\": tcatch (a+1 = si | qt = sj, at = sa) ∝ 1 si = sj 0 de lo contrario tmobile (a+1 = si | qt = so, at = sj)∝ 0 Si = Sj 1 de lo contrario tstalk (a+1 = si | qt = entonces, at = sj) ∝ 1 dist (si, entonces) tenga en cuenta que ninguno de los objetivos anteriores se puede lograr directamente;Por ejemplo, si Qt = S9 y At = S11, no hay acción que pueda mover el agente a AT+1 = S9 como lo requiere la \"Dinámica de destino\" de TCatch.",
                "Equilibrio [15] La \"dinámica objetivo\" de captura, movimiento y acechador descrita en la sección anterior por el vector de peso [0.8, 0.1, 0.1], EMT produjo un rendimiento estable en los tres dominios.",
                "Esencialmente está intentando seguir simultáneamente varios cursos de acción, todos los cuales son consistentes con la \"dinámica objetivo\".",
                "Por lo tanto, para el juego de etiqueta, no buscamos una función de recompensa que codifique y expresara nuestra preferencia sobre el comportamiento de los agentes, sino que estableciera directamente tres preferencias (heurísticas) de comportamiento como base para mantener la \"dinámica objetivo\".",
                "Sin embargo, la relación entre el rendimiento de EMT y la capacidad de alcance de la \"dinámica objetivo\" queda por explorar.",
                "DBC, por otro lado, tiene dos fuentes de especificación de secuenciación: a través de las propiedades de un procedimiento de selección de acción y a través de la especificación directa dentro de la \"dinámica de destino\".",
                "Por ejemplo, los POMDP tienen requisitos de tiempo fuera de línea prohibitivamente grandes para el cálculo de políticas, pero pueden usarse fácilmente en entornos más simples para exponer tendencias de comportamiento beneficiosas;Esto puede servir como una forma de \"dinámica objetivo\" que se proporcionan a EMT en un dominio más grande para la operación en línea.7.",
                "Desarrollo de entropía con longitud del juego de etiqueta para los tres escenarios de experimentos: a) múltiples de fin de vía muerta, b) Arena abierta irregular, c) Corredor circular.con respecto a la desviación de la dinámica del sistema real de la \"dinámica de destino\"."
            ],
            "translated_text": "",
            "candidates": [
                "dinámica de objetivos",
                "dinámica objetivo",
                "dinámica de objetivos",
                "dinámica objetivo",
                "dinámica de objetivos",
                "dinámica objetivo",
                "dinámica de objetivos",
                "dinámica objetivo",
                "dinámica de objetivos",
                "dinámica de destino",
                "dinámica de objetivos",
                "dinámica de destino",
                "dinámica de objetivos",
                "dinámica objetivo",
                "dinámica de objetivos",
                "dinámica de destino",
                "dinámica de objetivos",
                "dinámica objetivo",
                "dinámica de objetivos",
                "dinámica de destino",
                "dinámica de objetivos",
                "Dinámica de destino",
                "dinámica de objetivos",
                "dinámica objetivo",
                "dinámica de objetivos",
                "dinámica de destino",
                "dinámica de objetivos",
                "dinámica de destino",
                "Dinámica de destino",
                "dinámica de objetivos",
                "dinámica objetivo",
                "dinámica de objetivos",
                "dinámica objetivo",
                "dinámica de objetivos",
                "dinámica objetivo",
                "dinámica de objetivos",
                "dinámica objetivo",
                "dinámica de objetivos",
                "dinámica de destino",
                "Dinámica del objetivo",
                "dinámica objetivo",
                "dinámica de objetivos",
                "dinámica de destino"
            ],
            "error": []
        },
        "action-selection randomization": {
            "translated_key": "aleatorización de la selección de acción",
            "is_in_text": true,
            "original_annotated_sentences": [
                "Dynamics Based Control with an Application to Area-Sweeping Problems Zinovi Rabinovich Engineering and Computer Science Hebrew University of Jerusalem Jerusalem, Israel nomad@cs.huji.ac.il Jeffrey S. Rosenschein Engineering and Computer Science Hebrew University of Jerusalem Jerusalem, Israel jeff@cs.huji.ac.il Gal A. Kaminka The MAVERICK Group Department of Computer Science Bar Ilan University, Israel galk@cs.biu.ac.il ABSTRACT In this paper we introduce Dynamics Based Control (DBC), an approach to planning and control of an agent in stochastic environments.",
                "Unlike existing approaches, which seek to optimize expected rewards (e.g., in Partially Observable Markov Decision Problems (POMDPs)), DBC optimizes system behavior towards specified system dynamics.",
                "We show that a recently developed planning and control approach, Extended Markov Tracking (EMT) is an instantiation of DBC.",
                "EMT employs greedy action selection to provide an efficient control algorithm in Markovian environments.",
                "We exploit this efficiency in a set of experiments that applied multitarget EMT to a class of area-sweeping problems (searching for moving targets).",
                "We show that such problems can be naturally defined and efficiently solved using the DBC framework, and its EMT instantiation.",
                "Categories and Subject Descriptors I.2.8 [Problem Solving, Control Methods, and Search]: Control Theory; I.2.9 [Robotics]; I.2.11 [Distributed Artificial Intelligence]: Intelligent Agents General Terms Algorithms, Theory 1.",
                "INTRODUCTION Planning and control constitutes a central research area in multiagent systems and artificial intelligence.",
                "In recent years, Partially Observable Markov Decision Processes (POMDPs) [12] have become a popular formal basis for planning in stochastic environments.",
                "In this framework, the planning and control problem is often addressed by imposing a reward function, and computing a policy (of choosing actions) that is optimal, in the sense that it will result in the highest expected utility.",
                "While theoretically attractive, the complexity of optimally solving a POMDP is prohibitive [8, 7].",
                "We take an alternative view of planning in stochastic environments.",
                "We do not use a (state-based) reward function, but instead optimize over a different criterion, a transition-based specification of the desired system dynamics.",
                "The idea here is to view planexecution as a process that compels a (stochastic) system to change, and a plan as a dynamic process that shapes that change according to desired criteria.",
                "We call this general planning framework Dynamics Based Control (DBC).",
                "In DBC, the goal of a planning (or control) process becomes to ensure that the system will change in accordance with specific (potentially stochastic) target dynamics.",
                "As actual system behavior may deviate from that which is specified by target dynamics (due to the stochastic nature of the system), planning in such environments needs to be continual [4], in a manner similar to classical closed-loop controllers [16].",
                "Here, optimality is measured in terms of probability of deviation magnitudes.",
                "In this paper, we present the structure of Dynamics Based Control.",
                "We show that the recently developed Extended Markov Tracking (EMT) approach [13, 14, 15] is subsumed by DBC, with EMT employing greedy action selection, which is a specific parameterization among the options possible within DBC.",
                "EMT is an efficient instantiation of DBC.",
                "To evaluate DBC, we carried out a set of experiments applying multi-target EMT to the Tag Game [11]; this is a variant on the area sweeping problem, where an agent is trying to tag a moving target (quarry) whose position is not known with certainty.",
                "Experimental data demonstrates that even with a simple model of the environment and a simple design of target dynamics, high success rates can be produced both in catching the quarry, and in surprising the quarry (as expressed by the observed entropy of the controlled agents position).",
                "The paper is organized as follows.",
                "In Section 2 we motivate DBC using area-sweeping problems, and discuss related work.",
                "Section 3 introduces the Dynamics Based Control (DBC) structure, and its specialization to Markovian environments.",
                "This is followed by a review of the Extended Markov Tracking (EMT) approach as a DBC-structured control regimen in Section 4.",
                "That section also discusses the limitations of EMT-based control relative to the general DBC framework.",
                "Experimental settings and results are then presented in Section 5.",
                "Section 6 provides a short discussion of the overall approach, and Section 7 gives some concluding remarks and directions for future work. 790 978-81-904262-7-5 (RPS) c 2007 IFAAMAS 2.",
                "MOTIVATION AND RELATED WORK Many real-life scenarios naturally have a stochastic target dynamics specification, especially those domains where there exists no ultimate goal, but rather system behavior (with specific properties) that has to be continually supported.",
                "For example, security guards perform persistent sweeps of an area to detect any sign of intrusion.",
                "Cunning thieves will attempt to track these sweeps, and time their operation to key points of the guards motion.",
                "It is thus advisable to make the guards motion dynamics appear irregular and random.",
                "Recent work by Paruchuri et al. [10] has addressed such randomization in the context of single-agent and distributed POMDPs.",
                "The goal in that work was to generate policies that provide a measure of <br>action-selection randomization</br>, while maintaining rewards within some acceptable levels.",
                "Our focus differs from this work in that DBC does not optimize expected rewards-indeed we do not consider rewards at all-but instead maintains desired dynamics (including, but not limited to, randomization).",
                "The Game of Tag is another example of the applicability of the approach.",
                "It was introduced in the work by Pineau et al. [11].",
                "There are two agents that can move about an area, which is divided into a grid.",
                "The grid may have blocked cells (holes) into which no agent can move.",
                "One agent (the hunter) seeks to move into a cell occupied by the other (the quarry), such that they are co-located (this is a successful tag).",
                "The quarry seeks to avoid the hunter agent, and is always aware of the hunters position, but does not know how the hunter will behave, which opens up the possibility for a hunter to surprise the prey.",
                "The hunter knows the quarrys probabilistic law of motion, but does not know its current location.",
                "Tag is an instance of a family of area-sweeping (pursuit-evasion) problems.",
                "In [11], the hunter modeled the problem using a POMDP.",
                "A reward function was defined, to reflect the desire to tag the quarry, and an action policy was computed to optimize the reward collected over time.",
                "Due to the intractable complexity of determining the optimal policy, the action policy computed in that paper was essentially an approximation.",
                "In this paper, instead of formulating a reward function, we use EMT to solve the problem, by directly specifying the target dynamics.",
                "In fact, any search problem with randomized motion, the socalled class of area sweeping problems, can be described through specification of such target system dynamics.",
                "Dynamics Based Control provides a natural approach to solving these problems. 3.",
                "DYNAMICS BASED CONTROL The specification of Dynamics Based Control (DBC) can be broken into three interacting levels: Environment Design Level, User Level, and Agent Level. • Environment Design Level is concerned with the formal specification and modeling of the environment.",
                "For example, this level would specify the laws of physics within the system, and set its parameters, such as the gravitation constant. • User Level in turn relies on the environment model produced by Environment Design to specify the target system dynamics it wishes to observe.",
                "The User Level also specifies the estimation or learning procedure for system dynamics, and the measure of deviation.",
                "In the museum guard scenario above, these would correspond to a stochastic sweep schedule, and a measure of relative surprise between the specified and actual sweeping. • Agent Level in turn combines the environment model from the Environment Design level, the dynamics estimation procedure, the deviation measure and the target dynamics specification from User Level, to produce a sequence of actions that create system dynamics as close as possible to the targeted specification.",
                "As we are interested in the continual development of a stochastic system, such as happens in classical control theory [16] and continual planning [4], as well as in our example of museum sweeps, the question becomes how the Agent Level is to treat the deviation measurements over time.",
                "To this end, we use a probability threshold-that is, we would like the Agent Level to maximize the probability that the deviation measure will remain below a certain threshold.",
                "Specific action selection then depends on system formalization.",
                "One possibility would be to create a mixture of available system trends, much like that which happens in Behavior-Based Robotic architectures [1].",
                "The other alternative would be to rely on the estimation procedure provided by the User Level-to utilize the Environment Design Level model of the environment to choose actions, so as to manipulate the dynamics estimator into believing that a certain dynamics has been achieved.",
                "Notice that this manipulation is not direct, but via the environment.",
                "Thus, for strong enough estimator algorithms, successful manipulation would mean a successful simulation of the specified target dynamics (i.e., beyond discerning via the available sensory input).",
                "DBC levels can also have a back-flow of information (see Figure 1).",
                "For instance, the Agent Level could provide data about target dynamics feasibility, allowing the User Level to modify the requirement, perhaps focusing on attainable features of system behavior.",
                "Data would also be available about the system response to different actions performed; combined with a dynamics estimator defined by the User Level, this can provide an important tool for the environment model calibration at the Environment Design Level.",
                "UserEnv.",
                "Design Agent Model Ideal Dynamics Estimator Estimator Dynamics Feasibility System Response Data Figure 1: Data flow of the DBC framework Extending upon the idea of Actor-Critic algorithms [5], DBC data flow can provide a good basis for the design of a learning algorithm.",
                "For example, the User Level can operate as an exploratory device for a learning algorithm, inferring an ideal dynamics target from the environment model at hand that would expose and verify most critical features of system behavior.",
                "In this case, feasibility and system response data from the Agent Level would provide key information for an environment model update.",
                "In fact, the combination of feasibility and response data can provide a basis for the application of strong learning algorithms such as EM [2, 9]. 3.1 DBC for Markovian Environments For a Partially Observable Markovian Environment, DBC can be specified in a more rigorous manner.",
                "Notice how DBC discards rewards, and replaces it by another optimality criterion (structural differences are summarized in Table 1): • Environment Design level is to specify a tuple < S, A, T, O, Ω, s0 >, where: - S is the set of all possible environment states; - s0 is the initial state of the environment (which can also be viewed as a probability distribution over S); The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) 791 - A is the set of all possible actions applicable in the environment; - T is the environments probabilistic transition function: T : S ×A → Π(S).",
                "That is, T(s |a, s) is the probability that the environment will move from state s to state s under action a; - O is the set of all possible observations.",
                "This is what the sensor input would look like for an outside observer; - Ω is the observation probability function: Ω : S × A × S → Π(O).",
                "That is, Ω(o|s , a, s) is the probability that one will observe o given that the environment has moved from state s to state s under action a. • User Level, in the case of Markovian environment, operates on the set of system dynamics described by a family of conditional probabilities F = {τ : S × A → Π(S)}.",
                "Thus specification of target dynamics can be expressed by q ∈ F, and the learning or tracking algorithm can be represented as a function L : O×(A×O)∗ → F; that is, it maps sequences of observations and actions performed so far into an estimate τ ∈ F of system dynamics.",
                "There are many possible variations available at the User Level to define divergence between system dynamics; several of them are: - Trace distance or L1 distance between two distributions p and q defined by D(p(·), q(·)) = 1 2 x |p(x) − q(x)| - Fidelity measure of distance F(p(·), q(·)) = x p(x)q(x) - Kullback-Leibler divergence DKL(p(·) q(·)) = x p(x) log p(x) q(x) Notice that the latter two are not actually metrics over the space of possible distributions, but nevertheless have meaningful and important interpretations.",
                "For instance, KullbackLeibler divergence is an important tool of information theory [3] that allows one to measure the price of encoding an information source governed by q, while assuming that it is governed by p. The User Level also defines the threshold of dynamics deviation probability θ. • Agent Level is then faced with a problem of selecting a control signal function a∗ to satisfy a minimization problem as follows: a∗ = arg min a Pr(d(τa, q) > θ) where d(τa, q) is a random variable describing deviation of the dynamics estimate τa, created by L under control signal a, from the ideal dynamics q.",
                "Implicit in this minimization problem is that L is manipulated via the environment, based on the environment model produced by the Environment Design Level. 3.2 DBC View of the State Space It is important to note the complementary view that DBC and POMDPs take on the state space of the environment.",
                "POMDPs regard state as a stationary snap-shot of the environment; whatever attributes of state sequencing one seeks are reached through properties of the control process, in this case reward accumulation.",
                "This can be viewed as if the sequencing of states and the attributes of that sequencing are only introduced by and for the controlling mechanism-the POMDP policy.",
                "DBC concentrates on the underlying principle of state sequencing, the system dynamics.",
                "DBCs target dynamics specification can use the environments state space as a means to describe, discern, and preserve changes that occur within the system.",
                "As a result, DBC has a greater ability to express state sequencing properties, which are grounded in the environment model and its state space definition.",
                "For example, consider the task of moving through rough terrain towards a goal and reaching it as fast as possible.",
                "POMDPs would encode terrain as state space points, while speed would be ensured by negative reward for every step taken without reaching the goalaccumulating higher reward can be reached only by faster motion.",
                "Alternatively, the state space could directly include the notion of speed.",
                "For POMDPs, this would mean that the same concept is encoded twice, in some sense: directly in the state space, and indirectly within reward accumulation.",
                "Now, even if the reward function would encode more, and finer, details of the properties of motion, the POMDP solution will have to search in a much larger space of policies, while still being guided by the implicit concept of the reward accumulation procedure.",
                "On the other hand, the tactical target expression of variations and correlations between position and speed of motion is now grounded in the state space representation.",
                "In this situation, any further constraints, e.g., smoothness of motion, speed limits in different locations, or speed reductions during sharp turns, are explicitly and uniformly expressed by the tactical target, and can result in faster and more effective action selection by a DBC algorithm. 4.",
                "EMT-BASED CONTROL AS A DBC Recently, a control algorithm was introduced called EMT-based Control [13], which instantiates the DBC framework.",
                "Although it provides an approximate greedy solution in the DBC sense, initial experiments using EMT-based control have been encouraging [14, 15].",
                "EMT-based control is based on the Markovian environment definition, as in the case with POMDPs, but its User and Agent Levels are of the Markovian DBC type of optimality. • User Level of EMT-based control defines a limited-case target system dynamics independent of action: qEMT : S → Π(S).",
                "It then utilizes the Kullback-Leibler divergence measure to compose a momentary system dynamics estimator-the Extended Markov Tracking (EMT) algorithm.",
                "The algorithm keeps a system dynamics estimate τt EMT that is capable of explaining recent change in an auxiliary Bayesian system state estimator from pt−1 to pt, and updates it conservatively using Kullback-Leibler divergence.",
                "Since τt EMT and pt−1,t are respectively the conditional and marginal probabilities over the systems state space, explanation simply means that pt(s ) = s τt EMT (s |s)pt−1(s), and the dynamics estimate update is performed by solving a 792 The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) Table 1: Structure of POMDP vs. Dynamics-Based Control in Markovian Environment Level Approach MDP Markovian DBC Environment < S, A, T, O, Ω >,where S - set of states A - set of actions Design T : S × A → Π(S) - transition O - observation set Ω : S × A × S → Π(O) User r : S × A × S → R q : S × A → Π(S) F(π∗ ) → r L(o1, ..., ot) → τ r - reward function q - ideal dynamics F - reward remodeling L - dynamics estimator θ - threshold Agent π∗ = arg max π E[ γt rt] π∗ = arg min π Prob(d(τ q) > θ) minimization problem: τt EMT = H[pt, pt−1, τt−1 EMT ] = arg min τ DKL(τ × pt−1 τt−1 EMT × pt−1) s.t. pt(s ) = s (τ × pt−1)(s , s) pt−1(s) = s (τ × pt−1)(s , s) • Agent Level in EMT-based control is suboptimal with respect to DBC (though it remains within the DBC framework), performing greedy action selection based on prediction of EMTs reaction.",
                "The prediction is based on the environment model provided by the Environment Design level, so that if we denote by Ta the environments transition function limited to action a, and pt−1 is the auxiliary Bayesian system state estimator, then the EMT-based control choice is described by a∗ = arg min a∈A DKL(H[Ta × pt, pt, τt EMT ] qEMT × pt−1) Note that this follows the Markovian DBC framework precisely: the rewarding optimality of POMDPs is discarded, and in its place a dynamics estimator (EMT in this case) is manipulated via action effects on the environment to produce an estimate close to the specified target system dynamics.",
                "Yet as we mentioned, naive EMTbased control is suboptimal in the DBC sense, and has several additional limitations that do not exist in the general DBC framework (discussed in Section 4.2). 4.1 Multi-Target EMT At times, there may exist several behavioral preferences.",
                "For example, in the case of museum guards, some art items are more heavily guarded, requiring that the guards stay more often in their close vicinity.",
                "On the other hand, no corner of the museum is to be left unchecked, which demands constant motion.",
                "Successful museum security would demand that the guards adhere to, and balance, both of these behaviors.",
                "For EMT-based control, this would mean facing several tactical targets {qk}K k=1, and the question becomes how to merge and balance them.",
                "A balancing mechanism can be applied to resolve this issue.",
                "Note that EMT-based control, while selecting an action, creates a preference vector over the set of actions based on their predicted performance with respect to a given target.",
                "If these preference vectors are normalized, they can be combined into a single unified preference.",
                "This requires replacement of standard EMT-based action selection by the algorithm below [15]: • Given: - a set of target dynamics {qk}K k=1, - vector of weights w(k) • Select action as follows - For each action a ∈ A predict the future state distribution ¯pa t+1 = Ta ∗ pt; - For each action, compute Da = H(¯pa t+1, pt, PDt) - For each a ∈ A and qk tactical target, denote V (a, k) = DKL (Da qk) pt .",
                "Let Vk(a) = 1 Zk V (a, k), where Zk = a∈A V (a, k) is a normalization factor. - Select a∗ = arg min a k k=1 w(k)Vk(a) The weights vector w = (w1, ..., wK ) allows the additional tuning of importance among target dynamics without the need to redesign the targets themselves.",
                "This balancing method is also seamlessly integrated into the EMT-based control flow of operation. 4.2 EMT-based Control Limitations EMT-based control is a sub-optimal (in the DBC sense) representative of the DBC structure.",
                "It limits the User by forcing EMT to be its dynamic tracking algorithm, and replaces Agent optimization by greedy action selection.",
                "This kind of combination, however, is common for on-line algorithms.",
                "Although further development of EMT-based controllers is necessary, evidence so far suggests that even the simplest form of the algorithm possesses a great deal of power, and displays trends that are optimal in the DBC sense of the word.",
                "There are two further, EMT-specific, limitations to EMT-based control that are evident at this point.",
                "Both already have partial solutions and are subjects of ongoing research.",
                "The first limitation is the problem of negative preference.",
                "In the POMDP framework for example, this is captured simply, through The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) 793 the appearance of values with different signs within the reward structure.",
                "For EMT-based control, however, negative preference means that one would like to avoid a certain distribution over system development sequences; EMT-based control, however, concentrates on getting as close as possible to a distribution.",
                "Avoidance is thus unnatural in native EMT-based control.",
                "The second limitation comes from the fact that standard environment modeling can create pure sensory actions-actions that do not change the state of the world, and differ only in the way observations are received and the quality of observations received.",
                "Since the world state does not change, EMT-based control would not be able to differentiate between different sensory actions.",
                "Notice that both of these limitations of EMT-based control are absent from the general DBC framework, since it may have a tracking algorithm capable of considering pure sensory actions and, unlike Kullback-Leibler divergence, a distribution deviation measure that is capable of dealing with negative preference. 5.",
                "EMT PLAYING TAG The Game of Tag was first introduced in [11].",
                "It is a single agent problem of capturing a quarry, and belongs to the class of area sweeping problems.",
                "An example domain is shown in Figure 2. 0 51 2 3 4 6 7 8 10 12 13 161514 17 18 19 2221 23 9 11Q A 20 Figure 2: Tag domain; an agent (A) attempts to seek and capture a quarry (Q) The Game of Tag extremely limits the agents perception, so that the agent is able to detect the quarry only if they are co-located in the same cell of the grid world.",
                "In the classical version of the game, co-location leads to a special observation, and the Tag action can be performed.",
                "We slightly modify this setting: the moment that both agents occupy the same cell, the game ends.",
                "As a result, both the agent and its quarry have the same motion capability, which allows them to move in four directions, North, South, East, and West.",
                "These form a formal space of actions within a Markovian environment.",
                "The state space of the formal Markovian environment is described by the cross-product of the agent and quarrys positions.",
                "For Figure 2, it would be S = {s0, ..., s23} × {s0, ..., s23}.",
                "The effects of an action taken by the agent are deterministic, but the environment in general has a stochastic response due to the motion of the quarry.",
                "With probability q0 1 it stays put, and with probability 1 − q0 it moves to an adjacent cell further away from the 1 In our experiments this was taken to be q0 = 0.2. agent.",
                "So for the instance shown in Figure 2 and q0 = 0.1: P(Q = s9|Q = s9, A = s11) = 0.1 P(Q = s2|Q = s9, A = s11) = 0.3 P(Q = s8|Q = s9, A = s11) = 0.3 P(Q = s14|Q = s9, A = s11) = 0.3 Although the evasive behavior of the quarry is known to the agent, the quarrys position is not.",
                "The only sensory information available to the agent is its own location.",
                "We use EMT and directly specify the target dynamics.",
                "For the Game of Tag, we can easily formulate three major trends: catching the quarry, staying mobile, and stalking the quarry.",
                "This results in the following three target dynamics: Tcatch(At+1 = si|Qt = sj, At = sa) ∝ 1 si = sj 0 otherwise Tmobile(At+1 = si|Qt = so, At = sj) ∝ 0 si = sj 1 otherwise Tstalk(At+1 = si|Qt = so, At = sj) ∝ 1 dist(si,so) Note that none of the above targets are directly achievable; for instance, if Qt = s9 and At = s11, there is no action that can move the agent to At+1 = s9 as required by the Tcatch target dynamics.",
                "We ran several experiments to evaluate EMT performance in the Tag Game.",
                "Three configurations of the domain shown in Figure 3 were used, each posing a different challenge to the agent due to partial observability.",
                "In each setting, a set of 1000 runs was performed with a time limit of 100 steps.",
                "In every run, the initial position of both the agent and its quarry was selected at random; this means that as far as the agent was concerned, the quarrys initial position was uniformly distributed over the entire domain cell space.",
                "We also used two variations of the environment observability function.",
                "In the first version, observability function mapped all joint positions of hunter and quarry into the position of the hunter as an observation.",
                "In the second, only those joint positions in which hunter and quarry occupied different locations were mapped into the hunters location.",
                "The second version in fact utilized and expressed the fact that once hunter and quarry occupy the same cell the game ends.",
                "The results of these experiments are shown in Table 2.",
                "Balancing [15] the catch, move, and stalk target dynamics described in the previous section by the weight vector [0.8, 0.1, 0.1], EMT produced stable performance in all three domains.",
                "Although direct comparisons are difficult to make, the EMT performance displayed notable efficiency vis-`a-vis the POMDP approach.",
                "In spite of a simple and inefficient Matlab implementation of the EMT algorithm, the decision time for any given step averaged significantly below 1 second in all experiments.",
                "For the irregular open arena domain, which proved to be the most difficult, 1000 experiment runs bounded by 100 steps each, a total of 42411 steps, were completed in slightly under 6 hours.",
                "That is, over 4 × 104 online steps took an order of magnitude less time than the offline computation of POMDP policy in [11].",
                "The significance of this differential is made even more prominent by the fact that, should the environment model parameters change, the online nature of EMT would allow it to maintain its performance time, while the POMDP policy would need to be recomputed, requiring yet again a large overhead of computation.",
                "We also tested the behavior cell frequency entropy, empirical measures from trial data.",
                "As Figure 4 and Figure 5 show, empir794 The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) A Q Q A 0 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 A Q Figure 3: These configurations of the Tag Game space were used: a) multiple dead-end, b) irregular open arena, c) circular corridor Table 2: Performance of the EMT-based solution in three Tag Game domains and two observability models: I) omniposition quarry, II) quarry is not at hunters position Model Domain Capture% E(Steps) Time/Step I Dead-ends 100 14.8 72(mSec) Arena 80.2 42.4 500(mSec) Circle 91.4 34.6 187(mSec) II Dead-ends 100 13.2 91(mSec) Arena 96.8 28.67 396(mSec) Circle 94.4 31.63 204(mSec) ical entropy grows with the length of interaction.",
                "For runs where the quarry was not captured immediately, the entropy reaches between 0.85 and 0.952 for different runs and scenarios.",
                "As the agent actively seeks the quarry, the entropy never reaches its maximum.",
                "One characteristic of the entropy graph for the open arena scenario particularly caught our attention in the case of the omniposition quarry observation model.",
                "Near the maximum limit of trial length (100 steps), entropy suddenly dropped.",
                "Further analysis of the data showed that under certain circumstances, a fluctuating behavior occurs in which the agent faces equally viable versions of quarry-following behavior.",
                "Since the EMT algorithm has greedy action selection, and the state space does not encode any form of commitment (not even speed or acceleration), the agent is locked within a small portion of cells.",
                "It is essentially attempting to simultaneously follow several courses of action, all of which are consistent with the target dynamics.",
                "This behavior did not occur in our second observation model, since it significantly reduced the set of eligible courses of action-essentially contributing to tie-breaking among them. 6.",
                "DISCUSSION The design of the EMT solution for the Tag Game exposes the core difference in approach to planning and control between EMT or DBC, on the one hand, and the more familiar POMDP approach, on the other.",
                "POMDP defines a reward structure to optimize, and influences system dynamics indirectly through that optimization.",
                "EMT discards any reward scheme, and instead measures and influences system dynamics directly. 2 Entropy was calculated using log base equal to the number of possible locations within the domain; this properly scales entropy expression into the range [0, 1] for all domains.",
                "Thus for the Tag Game, we did not search for a reward function that would encode and express our preference over the agents behavior, but rather directly set three (heuristic) behavior preferences as the basis for target dynamics to be maintained.",
                "Experimental data shows that these targets need not be directly achievable via the agents actions.",
                "However, the ratio between EMT performance and achievability of target dynamics remains to be explored.",
                "The tag game experiment data also revealed the different emphasis DBC and POMDPs place on the formulation of an environment state space.",
                "POMDPs rely entirely on the mechanism of reward accumulation maximization, i.e., formation of the action selection procedure to achieve necessary state sequencing.",
                "DBC, on the other hand, has two sources of sequencing specification: through the properties of an action selection procedure, and through direct specification within the target dynamics.",
                "The importance of the second source was underlined by the Tag Game experiment data, in which the greedy EMT algorithm, applied to a POMDP-type state space specification, failed, since target description over such a state space was incapable of encoding the necessary behavior tendencies, e.g., tie-breaking and commitment to directed motion.",
                "The structural differences between DBC (and EMT in particular), and POMDPs, prohibits direct performance comparison, and places them on complementary tracks, each within a suitable niche.",
                "For instance, POMDPs could be seen as a much more natural formulation of economic sequential decision-making problems, while EMT is a better fit for continual demand for stochastic change, as happens in many robotic or embodied-agent problems.",
                "The complementary properties of POMDPs and EMT can be further exploited.",
                "There is recent interest in using POMDPs in hybrid solutions [17], in which the POMDPs can be used together with other control approaches to provide results not easily achievable with either approach by itself.",
                "DBC can be an effective partner in such a hybrid solution.",
                "For instance, POMDPs have prohibitively large off-line time requirements for policy computation, but can be readily used in simpler settings to expose beneficial behavioral trends; this can serve as a form of target dynamics that are provided to EMT in a larger domain for on-line operation. 7.",
                "CONCLUSIONS AND FUTURE WORK In this paper, we have presented a novel perspective on the process of planning and control in stochastic environments, in the form of the Dynamics Based Control (DBC) framework.",
                "DBC formulates the task of planning as support of a specified target system dynamics, which describes the necessary properties of change within the environment.",
                "Optimality of DBC plans of action are measured The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) 795 0 20 40 60 80 100 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1 Steps Entropy Dead−ends 0 20 40 60 80 100 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1 Steps Entropy Arena 0 20 40 60 80 100 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1 Steps Entropy Circle Figure 4: Observation Model I: Omniposition quarry.",
                "Entropy development with length of Tag Game for the three experiment scenarios: a) multiple dead-end, b) irregular open arena, c) circular corridor. 0 10 20 30 40 50 60 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1 Steps Entropy Dead−ends 0 20 40 60 80 100 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1 Steps Entropy Arena 0 20 40 60 80 100 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1 Steps Entropy Circle Figure 5: Observation Model II: quarry not observed at hunters position.",
                "Entropy development with length of Tag Game for the three experiment scenarios: a) multiple dead-end, b) irregular open arena, c) circular corridor. with respect to the deviation of actual system dynamics from the target dynamics.",
                "We show that a recently developed technique of Extended Markov Tracking (EMT) [13] is an instantiation of DBC.",
                "In fact, EMT can be seen as a specific case of DBC parameterization, which employs a greedy action selection procedure.",
                "Since EMT exhibits the key features of the general DBC framework, as well as polynomial time complexity, we used the multitarget version of EMT [15] to demonstrate that the class of area sweeping problems naturally lends itself to dynamics-based descriptions, as instantiated by our experiments in the Tag Game domain.",
                "As enumerated in Section 4.2, EMT has a number of limitations, such as difficulty in dealing with negative dynamic preference.",
                "This prevents direct application of EMT to such problems as Rendezvous-Evasion Games (e.g., [6]).",
                "However, DBC in general has no such limitations, and readily enables the formulation of evasion games.",
                "In future work, we intend to proceed with the development of dynamics-based controllers for these problems. 8.",
                "ACKNOWLEDGMENT The work of the first two authors was partially supported by Israel Science Foundation grant #898/05, and the third author was partially supported by a grant from Israels Ministry of Science and Technology. 9.",
                "REFERENCES [1] R. C. Arkin.",
                "Behavior-Based Robotics.",
                "MIT Press, 1998. [2] J.",
                "A. Bilmes.",
                "A gentle tutorial of the EM algorithm and its application to parameter estimation for Gaussian mixture and Hidden Markov Models.",
                "Technical Report TR-97-021, Department of Electrical Engeineering and Computer Science, University of California at Berkeley, 1998. [3] T. M. Cover and J.",
                "A. Thomas.",
                "Elements of information theory.",
                "Wiley, 1991. [4] M. E. desJardins, E. H. Durfee, C. L. Ortiz, and M. J. Wolverton.",
                "A survey of research in distributed, continual planning.",
                "AI Magazine, 4:13-22, 1999. [5] V. R. Konda and J. N. Tsitsiklis.",
                "Actor-Critic algorithms.",
                "SIAM Journal on Control and Optimization, 42(4):1143-1166, 2003. [6] W. S. Lim.",
                "A rendezvous-evasion game on discrete locations with joint randomization.",
                "Advances in Applied Probability, 29(4):1004-1017, December 1997. [7] M. L. Littman, T. L. Dean, and L. P. Kaelbling.",
                "On the complexity of solving Markov decision problems.",
                "In Proceedings of the 11th Annual Conference on Uncertainty in Artificial Intelligence (UAI-95), pages 394-402, 1995. [8] O. Madani, S. Hanks, and A. Condon.",
                "On the undecidability of probabilistic planning and related stochastic optimization problems.",
                "Artificial Intelligence Journal, 147(1-2):5-34, July 2003. [9] R. M. Neal and G. E. Hinton.",
                "A view of the EM algorithm 796 The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) that justifies incremental, sparse, and other variants.",
                "In M. I. Jordan, editor, Learning in Graphical Models, pages 355-368.",
                "Kluwer Academic Publishers, 1998. [10] P. Paruchuri, M. Tambe, F. Ordonez, and S. Kraus.",
                "Security in multiagent systems by policy randomization.",
                "In Proceeding of AAMAS 2006, 2006. [11] J. Pineau, G. Gordon, and S. Thrun.",
                "Point-based value iteration: An anytime algorithm for pomdps.",
                "In International Joint Conference on Artificial Intelligence (IJCAI), pages 1025-1032, August 2003. [12] M. L. Puterman.",
                "Markov Decision Processes.",
                "Wiley Series in Probability and Mathematical Statistics: Applied Probability and Statistics Section.",
                "Wiley-Interscience Publication, New York, 1994. [13] Z. Rabinovich and J. S. Rosenschein.",
                "Extended Markov Tracking with an application to control.",
                "In The Workshop on Agent Tracking: Modeling Other Agents from Observations, at the Third International Joint Conference on Autonomous Agents and Multiagent Systems, pages 95-100, New-York, July 2004. [14] Z. Rabinovich and J. S. Rosenschein.",
                "Multiagent coordination by Extended Markov Tracking.",
                "In The Fourth International Joint Conference on Autonomous Agents and Multiagent Systems, pages 431-438, Utrecht, The Netherlands, July 2005. [15] Z. Rabinovich and J. S. Rosenschein.",
                "On the response of EMT-based control to interacting targets and models.",
                "In The Fifth International Joint Conference on Autonomous Agents and Multiagent Systems, pages 465-470, Hakodate, Japan, May 2006. [16] R. F. Stengel.",
                "Optimal Control and Estimation.",
                "Dover Publications, 1994. [17] M. Tambe, E. Bowring, H. Jung, G. Kaminka, R. Maheswaran, J. Marecki, J. Modi, R. Nair, J. Pearce, P. Paruchuri, D. Pynadath, P. Scerri, N. Schurr, and P. Varakantham.",
                "Conflicts in teamwork: Hybrids to the The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) 797"
            ],
            "original_annotated_samples": [],
            "translated_annotated_samples": [
                "El objetivo en ese trabajo era generar políticas que proporcionen una medida de \"aleatorización de la selección de acción\", al tiempo que mantiene las recompensas dentro de algunos niveles aceptables."
            ],
            "translated_text": "",
            "candidates": [
                "aleatorización de la selección de acción",
                "aleatorización de la selección de acción"
            ],
            "error": []
        },
        "game of tag": {
            "translated_key": "juego de etiqueta",
            "is_in_text": true,
            "original_annotated_sentences": [
                "Dynamics Based Control with an Application to Area-Sweeping Problems Zinovi Rabinovich Engineering and Computer Science Hebrew University of Jerusalem Jerusalem, Israel nomad@cs.huji.ac.il Jeffrey S. Rosenschein Engineering and Computer Science Hebrew University of Jerusalem Jerusalem, Israel jeff@cs.huji.ac.il Gal A. Kaminka The MAVERICK Group Department of Computer Science Bar Ilan University, Israel galk@cs.biu.ac.il ABSTRACT In this paper we introduce Dynamics Based Control (DBC), an approach to planning and control of an agent in stochastic environments.",
                "Unlike existing approaches, which seek to optimize expected rewards (e.g., in Partially Observable Markov Decision Problems (POMDPs)), DBC optimizes system behavior towards specified system dynamics.",
                "We show that a recently developed planning and control approach, Extended Markov Tracking (EMT) is an instantiation of DBC.",
                "EMT employs greedy action selection to provide an efficient control algorithm in Markovian environments.",
                "We exploit this efficiency in a set of experiments that applied multitarget EMT to a class of area-sweeping problems (searching for moving targets).",
                "We show that such problems can be naturally defined and efficiently solved using the DBC framework, and its EMT instantiation.",
                "Categories and Subject Descriptors I.2.8 [Problem Solving, Control Methods, and Search]: Control Theory; I.2.9 [Robotics]; I.2.11 [Distributed Artificial Intelligence]: Intelligent Agents General Terms Algorithms, Theory 1.",
                "INTRODUCTION Planning and control constitutes a central research area in multiagent systems and artificial intelligence.",
                "In recent years, Partially Observable Markov Decision Processes (POMDPs) [12] have become a popular formal basis for planning in stochastic environments.",
                "In this framework, the planning and control problem is often addressed by imposing a reward function, and computing a policy (of choosing actions) that is optimal, in the sense that it will result in the highest expected utility.",
                "While theoretically attractive, the complexity of optimally solving a POMDP is prohibitive [8, 7].",
                "We take an alternative view of planning in stochastic environments.",
                "We do not use a (state-based) reward function, but instead optimize over a different criterion, a transition-based specification of the desired system dynamics.",
                "The idea here is to view planexecution as a process that compels a (stochastic) system to change, and a plan as a dynamic process that shapes that change according to desired criteria.",
                "We call this general planning framework Dynamics Based Control (DBC).",
                "In DBC, the goal of a planning (or control) process becomes to ensure that the system will change in accordance with specific (potentially stochastic) target dynamics.",
                "As actual system behavior may deviate from that which is specified by target dynamics (due to the stochastic nature of the system), planning in such environments needs to be continual [4], in a manner similar to classical closed-loop controllers [16].",
                "Here, optimality is measured in terms of probability of deviation magnitudes.",
                "In this paper, we present the structure of Dynamics Based Control.",
                "We show that the recently developed Extended Markov Tracking (EMT) approach [13, 14, 15] is subsumed by DBC, with EMT employing greedy action selection, which is a specific parameterization among the options possible within DBC.",
                "EMT is an efficient instantiation of DBC.",
                "To evaluate DBC, we carried out a set of experiments applying multi-target EMT to the Tag Game [11]; this is a variant on the area sweeping problem, where an agent is trying to tag a moving target (quarry) whose position is not known with certainty.",
                "Experimental data demonstrates that even with a simple model of the environment and a simple design of target dynamics, high success rates can be produced both in catching the quarry, and in surprising the quarry (as expressed by the observed entropy of the controlled agents position).",
                "The paper is organized as follows.",
                "In Section 2 we motivate DBC using area-sweeping problems, and discuss related work.",
                "Section 3 introduces the Dynamics Based Control (DBC) structure, and its specialization to Markovian environments.",
                "This is followed by a review of the Extended Markov Tracking (EMT) approach as a DBC-structured control regimen in Section 4.",
                "That section also discusses the limitations of EMT-based control relative to the general DBC framework.",
                "Experimental settings and results are then presented in Section 5.",
                "Section 6 provides a short discussion of the overall approach, and Section 7 gives some concluding remarks and directions for future work. 790 978-81-904262-7-5 (RPS) c 2007 IFAAMAS 2.",
                "MOTIVATION AND RELATED WORK Many real-life scenarios naturally have a stochastic target dynamics specification, especially those domains where there exists no ultimate goal, but rather system behavior (with specific properties) that has to be continually supported.",
                "For example, security guards perform persistent sweeps of an area to detect any sign of intrusion.",
                "Cunning thieves will attempt to track these sweeps, and time their operation to key points of the guards motion.",
                "It is thus advisable to make the guards motion dynamics appear irregular and random.",
                "Recent work by Paruchuri et al. [10] has addressed such randomization in the context of single-agent and distributed POMDPs.",
                "The goal in that work was to generate policies that provide a measure of action-selection randomization, while maintaining rewards within some acceptable levels.",
                "Our focus differs from this work in that DBC does not optimize expected rewards-indeed we do not consider rewards at all-but instead maintains desired dynamics (including, but not limited to, randomization).",
                "The <br>game of tag</br> is another example of the applicability of the approach.",
                "It was introduced in the work by Pineau et al. [11].",
                "There are two agents that can move about an area, which is divided into a grid.",
                "The grid may have blocked cells (holes) into which no agent can move.",
                "One agent (the hunter) seeks to move into a cell occupied by the other (the quarry), such that they are co-located (this is a successful tag).",
                "The quarry seeks to avoid the hunter agent, and is always aware of the hunters position, but does not know how the hunter will behave, which opens up the possibility for a hunter to surprise the prey.",
                "The hunter knows the quarrys probabilistic law of motion, but does not know its current location.",
                "Tag is an instance of a family of area-sweeping (pursuit-evasion) problems.",
                "In [11], the hunter modeled the problem using a POMDP.",
                "A reward function was defined, to reflect the desire to tag the quarry, and an action policy was computed to optimize the reward collected over time.",
                "Due to the intractable complexity of determining the optimal policy, the action policy computed in that paper was essentially an approximation.",
                "In this paper, instead of formulating a reward function, we use EMT to solve the problem, by directly specifying the target dynamics.",
                "In fact, any search problem with randomized motion, the socalled class of area sweeping problems, can be described through specification of such target system dynamics.",
                "Dynamics Based Control provides a natural approach to solving these problems. 3.",
                "DYNAMICS BASED CONTROL The specification of Dynamics Based Control (DBC) can be broken into three interacting levels: Environment Design Level, User Level, and Agent Level. • Environment Design Level is concerned with the formal specification and modeling of the environment.",
                "For example, this level would specify the laws of physics within the system, and set its parameters, such as the gravitation constant. • User Level in turn relies on the environment model produced by Environment Design to specify the target system dynamics it wishes to observe.",
                "The User Level also specifies the estimation or learning procedure for system dynamics, and the measure of deviation.",
                "In the museum guard scenario above, these would correspond to a stochastic sweep schedule, and a measure of relative surprise between the specified and actual sweeping. • Agent Level in turn combines the environment model from the Environment Design level, the dynamics estimation procedure, the deviation measure and the target dynamics specification from User Level, to produce a sequence of actions that create system dynamics as close as possible to the targeted specification.",
                "As we are interested in the continual development of a stochastic system, such as happens in classical control theory [16] and continual planning [4], as well as in our example of museum sweeps, the question becomes how the Agent Level is to treat the deviation measurements over time.",
                "To this end, we use a probability threshold-that is, we would like the Agent Level to maximize the probability that the deviation measure will remain below a certain threshold.",
                "Specific action selection then depends on system formalization.",
                "One possibility would be to create a mixture of available system trends, much like that which happens in Behavior-Based Robotic architectures [1].",
                "The other alternative would be to rely on the estimation procedure provided by the User Level-to utilize the Environment Design Level model of the environment to choose actions, so as to manipulate the dynamics estimator into believing that a certain dynamics has been achieved.",
                "Notice that this manipulation is not direct, but via the environment.",
                "Thus, for strong enough estimator algorithms, successful manipulation would mean a successful simulation of the specified target dynamics (i.e., beyond discerning via the available sensory input).",
                "DBC levels can also have a back-flow of information (see Figure 1).",
                "For instance, the Agent Level could provide data about target dynamics feasibility, allowing the User Level to modify the requirement, perhaps focusing on attainable features of system behavior.",
                "Data would also be available about the system response to different actions performed; combined with a dynamics estimator defined by the User Level, this can provide an important tool for the environment model calibration at the Environment Design Level.",
                "UserEnv.",
                "Design Agent Model Ideal Dynamics Estimator Estimator Dynamics Feasibility System Response Data Figure 1: Data flow of the DBC framework Extending upon the idea of Actor-Critic algorithms [5], DBC data flow can provide a good basis for the design of a learning algorithm.",
                "For example, the User Level can operate as an exploratory device for a learning algorithm, inferring an ideal dynamics target from the environment model at hand that would expose and verify most critical features of system behavior.",
                "In this case, feasibility and system response data from the Agent Level would provide key information for an environment model update.",
                "In fact, the combination of feasibility and response data can provide a basis for the application of strong learning algorithms such as EM [2, 9]. 3.1 DBC for Markovian Environments For a Partially Observable Markovian Environment, DBC can be specified in a more rigorous manner.",
                "Notice how DBC discards rewards, and replaces it by another optimality criterion (structural differences are summarized in Table 1): • Environment Design level is to specify a tuple < S, A, T, O, Ω, s0 >, where: - S is the set of all possible environment states; - s0 is the initial state of the environment (which can also be viewed as a probability distribution over S); The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) 791 - A is the set of all possible actions applicable in the environment; - T is the environments probabilistic transition function: T : S ×A → Π(S).",
                "That is, T(s |a, s) is the probability that the environment will move from state s to state s under action a; - O is the set of all possible observations.",
                "This is what the sensor input would look like for an outside observer; - Ω is the observation probability function: Ω : S × A × S → Π(O).",
                "That is, Ω(o|s , a, s) is the probability that one will observe o given that the environment has moved from state s to state s under action a. • User Level, in the case of Markovian environment, operates on the set of system dynamics described by a family of conditional probabilities F = {τ : S × A → Π(S)}.",
                "Thus specification of target dynamics can be expressed by q ∈ F, and the learning or tracking algorithm can be represented as a function L : O×(A×O)∗ → F; that is, it maps sequences of observations and actions performed so far into an estimate τ ∈ F of system dynamics.",
                "There are many possible variations available at the User Level to define divergence between system dynamics; several of them are: - Trace distance or L1 distance between two distributions p and q defined by D(p(·), q(·)) = 1 2 x |p(x) − q(x)| - Fidelity measure of distance F(p(·), q(·)) = x p(x)q(x) - Kullback-Leibler divergence DKL(p(·) q(·)) = x p(x) log p(x) q(x) Notice that the latter two are not actually metrics over the space of possible distributions, but nevertheless have meaningful and important interpretations.",
                "For instance, KullbackLeibler divergence is an important tool of information theory [3] that allows one to measure the price of encoding an information source governed by q, while assuming that it is governed by p. The User Level also defines the threshold of dynamics deviation probability θ. • Agent Level is then faced with a problem of selecting a control signal function a∗ to satisfy a minimization problem as follows: a∗ = arg min a Pr(d(τa, q) > θ) where d(τa, q) is a random variable describing deviation of the dynamics estimate τa, created by L under control signal a, from the ideal dynamics q.",
                "Implicit in this minimization problem is that L is manipulated via the environment, based on the environment model produced by the Environment Design Level. 3.2 DBC View of the State Space It is important to note the complementary view that DBC and POMDPs take on the state space of the environment.",
                "POMDPs regard state as a stationary snap-shot of the environment; whatever attributes of state sequencing one seeks are reached through properties of the control process, in this case reward accumulation.",
                "This can be viewed as if the sequencing of states and the attributes of that sequencing are only introduced by and for the controlling mechanism-the POMDP policy.",
                "DBC concentrates on the underlying principle of state sequencing, the system dynamics.",
                "DBCs target dynamics specification can use the environments state space as a means to describe, discern, and preserve changes that occur within the system.",
                "As a result, DBC has a greater ability to express state sequencing properties, which are grounded in the environment model and its state space definition.",
                "For example, consider the task of moving through rough terrain towards a goal and reaching it as fast as possible.",
                "POMDPs would encode terrain as state space points, while speed would be ensured by negative reward for every step taken without reaching the goalaccumulating higher reward can be reached only by faster motion.",
                "Alternatively, the state space could directly include the notion of speed.",
                "For POMDPs, this would mean that the same concept is encoded twice, in some sense: directly in the state space, and indirectly within reward accumulation.",
                "Now, even if the reward function would encode more, and finer, details of the properties of motion, the POMDP solution will have to search in a much larger space of policies, while still being guided by the implicit concept of the reward accumulation procedure.",
                "On the other hand, the tactical target expression of variations and correlations between position and speed of motion is now grounded in the state space representation.",
                "In this situation, any further constraints, e.g., smoothness of motion, speed limits in different locations, or speed reductions during sharp turns, are explicitly and uniformly expressed by the tactical target, and can result in faster and more effective action selection by a DBC algorithm. 4.",
                "EMT-BASED CONTROL AS A DBC Recently, a control algorithm was introduced called EMT-based Control [13], which instantiates the DBC framework.",
                "Although it provides an approximate greedy solution in the DBC sense, initial experiments using EMT-based control have been encouraging [14, 15].",
                "EMT-based control is based on the Markovian environment definition, as in the case with POMDPs, but its User and Agent Levels are of the Markovian DBC type of optimality. • User Level of EMT-based control defines a limited-case target system dynamics independent of action: qEMT : S → Π(S).",
                "It then utilizes the Kullback-Leibler divergence measure to compose a momentary system dynamics estimator-the Extended Markov Tracking (EMT) algorithm.",
                "The algorithm keeps a system dynamics estimate τt EMT that is capable of explaining recent change in an auxiliary Bayesian system state estimator from pt−1 to pt, and updates it conservatively using Kullback-Leibler divergence.",
                "Since τt EMT and pt−1,t are respectively the conditional and marginal probabilities over the systems state space, explanation simply means that pt(s ) = s τt EMT (s |s)pt−1(s), and the dynamics estimate update is performed by solving a 792 The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) Table 1: Structure of POMDP vs. Dynamics-Based Control in Markovian Environment Level Approach MDP Markovian DBC Environment < S, A, T, O, Ω >,where S - set of states A - set of actions Design T : S × A → Π(S) - transition O - observation set Ω : S × A × S → Π(O) User r : S × A × S → R q : S × A → Π(S) F(π∗ ) → r L(o1, ..., ot) → τ r - reward function q - ideal dynamics F - reward remodeling L - dynamics estimator θ - threshold Agent π∗ = arg max π E[ γt rt] π∗ = arg min π Prob(d(τ q) > θ) minimization problem: τt EMT = H[pt, pt−1, τt−1 EMT ] = arg min τ DKL(τ × pt−1 τt−1 EMT × pt−1) s.t. pt(s ) = s (τ × pt−1)(s , s) pt−1(s) = s (τ × pt−1)(s , s) • Agent Level in EMT-based control is suboptimal with respect to DBC (though it remains within the DBC framework), performing greedy action selection based on prediction of EMTs reaction.",
                "The prediction is based on the environment model provided by the Environment Design level, so that if we denote by Ta the environments transition function limited to action a, and pt−1 is the auxiliary Bayesian system state estimator, then the EMT-based control choice is described by a∗ = arg min a∈A DKL(H[Ta × pt, pt, τt EMT ] qEMT × pt−1) Note that this follows the Markovian DBC framework precisely: the rewarding optimality of POMDPs is discarded, and in its place a dynamics estimator (EMT in this case) is manipulated via action effects on the environment to produce an estimate close to the specified target system dynamics.",
                "Yet as we mentioned, naive EMTbased control is suboptimal in the DBC sense, and has several additional limitations that do not exist in the general DBC framework (discussed in Section 4.2). 4.1 Multi-Target EMT At times, there may exist several behavioral preferences.",
                "For example, in the case of museum guards, some art items are more heavily guarded, requiring that the guards stay more often in their close vicinity.",
                "On the other hand, no corner of the museum is to be left unchecked, which demands constant motion.",
                "Successful museum security would demand that the guards adhere to, and balance, both of these behaviors.",
                "For EMT-based control, this would mean facing several tactical targets {qk}K k=1, and the question becomes how to merge and balance them.",
                "A balancing mechanism can be applied to resolve this issue.",
                "Note that EMT-based control, while selecting an action, creates a preference vector over the set of actions based on their predicted performance with respect to a given target.",
                "If these preference vectors are normalized, they can be combined into a single unified preference.",
                "This requires replacement of standard EMT-based action selection by the algorithm below [15]: • Given: - a set of target dynamics {qk}K k=1, - vector of weights w(k) • Select action as follows - For each action a ∈ A predict the future state distribution ¯pa t+1 = Ta ∗ pt; - For each action, compute Da = H(¯pa t+1, pt, PDt) - For each a ∈ A and qk tactical target, denote V (a, k) = DKL (Da qk) pt .",
                "Let Vk(a) = 1 Zk V (a, k), where Zk = a∈A V (a, k) is a normalization factor. - Select a∗ = arg min a k k=1 w(k)Vk(a) The weights vector w = (w1, ..., wK ) allows the additional tuning of importance among target dynamics without the need to redesign the targets themselves.",
                "This balancing method is also seamlessly integrated into the EMT-based control flow of operation. 4.2 EMT-based Control Limitations EMT-based control is a sub-optimal (in the DBC sense) representative of the DBC structure.",
                "It limits the User by forcing EMT to be its dynamic tracking algorithm, and replaces Agent optimization by greedy action selection.",
                "This kind of combination, however, is common for on-line algorithms.",
                "Although further development of EMT-based controllers is necessary, evidence so far suggests that even the simplest form of the algorithm possesses a great deal of power, and displays trends that are optimal in the DBC sense of the word.",
                "There are two further, EMT-specific, limitations to EMT-based control that are evident at this point.",
                "Both already have partial solutions and are subjects of ongoing research.",
                "The first limitation is the problem of negative preference.",
                "In the POMDP framework for example, this is captured simply, through The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) 793 the appearance of values with different signs within the reward structure.",
                "For EMT-based control, however, negative preference means that one would like to avoid a certain distribution over system development sequences; EMT-based control, however, concentrates on getting as close as possible to a distribution.",
                "Avoidance is thus unnatural in native EMT-based control.",
                "The second limitation comes from the fact that standard environment modeling can create pure sensory actions-actions that do not change the state of the world, and differ only in the way observations are received and the quality of observations received.",
                "Since the world state does not change, EMT-based control would not be able to differentiate between different sensory actions.",
                "Notice that both of these limitations of EMT-based control are absent from the general DBC framework, since it may have a tracking algorithm capable of considering pure sensory actions and, unlike Kullback-Leibler divergence, a distribution deviation measure that is capable of dealing with negative preference. 5.",
                "EMT PLAYING TAG The <br>game of tag</br> was first introduced in [11].",
                "It is a single agent problem of capturing a quarry, and belongs to the class of area sweeping problems.",
                "An example domain is shown in Figure 2. 0 51 2 3 4 6 7 8 10 12 13 161514 17 18 19 2221 23 9 11Q A 20 Figure 2: Tag domain; an agent (A) attempts to seek and capture a quarry (Q) The <br>game of tag</br> extremely limits the agents perception, so that the agent is able to detect the quarry only if they are co-located in the same cell of the grid world.",
                "In the classical version of the game, co-location leads to a special observation, and the Tag action can be performed.",
                "We slightly modify this setting: the moment that both agents occupy the same cell, the game ends.",
                "As a result, both the agent and its quarry have the same motion capability, which allows them to move in four directions, North, South, East, and West.",
                "These form a formal space of actions within a Markovian environment.",
                "The state space of the formal Markovian environment is described by the cross-product of the agent and quarrys positions.",
                "For Figure 2, it would be S = {s0, ..., s23} × {s0, ..., s23}.",
                "The effects of an action taken by the agent are deterministic, but the environment in general has a stochastic response due to the motion of the quarry.",
                "With probability q0 1 it stays put, and with probability 1 − q0 it moves to an adjacent cell further away from the 1 In our experiments this was taken to be q0 = 0.2. agent.",
                "So for the instance shown in Figure 2 and q0 = 0.1: P(Q = s9|Q = s9, A = s11) = 0.1 P(Q = s2|Q = s9, A = s11) = 0.3 P(Q = s8|Q = s9, A = s11) = 0.3 P(Q = s14|Q = s9, A = s11) = 0.3 Although the evasive behavior of the quarry is known to the agent, the quarrys position is not.",
                "The only sensory information available to the agent is its own location.",
                "We use EMT and directly specify the target dynamics.",
                "For the <br>game of tag</br>, we can easily formulate three major trends: catching the quarry, staying mobile, and stalking the quarry.",
                "This results in the following three target dynamics: Tcatch(At+1 = si|Qt = sj, At = sa) ∝ 1 si = sj 0 otherwise Tmobile(At+1 = si|Qt = so, At = sj) ∝ 0 si = sj 1 otherwise Tstalk(At+1 = si|Qt = so, At = sj) ∝ 1 dist(si,so) Note that none of the above targets are directly achievable; for instance, if Qt = s9 and At = s11, there is no action that can move the agent to At+1 = s9 as required by the Tcatch target dynamics.",
                "We ran several experiments to evaluate EMT performance in the Tag Game.",
                "Three configurations of the domain shown in Figure 3 were used, each posing a different challenge to the agent due to partial observability.",
                "In each setting, a set of 1000 runs was performed with a time limit of 100 steps.",
                "In every run, the initial position of both the agent and its quarry was selected at random; this means that as far as the agent was concerned, the quarrys initial position was uniformly distributed over the entire domain cell space.",
                "We also used two variations of the environment observability function.",
                "In the first version, observability function mapped all joint positions of hunter and quarry into the position of the hunter as an observation.",
                "In the second, only those joint positions in which hunter and quarry occupied different locations were mapped into the hunters location.",
                "The second version in fact utilized and expressed the fact that once hunter and quarry occupy the same cell the game ends.",
                "The results of these experiments are shown in Table 2.",
                "Balancing [15] the catch, move, and stalk target dynamics described in the previous section by the weight vector [0.8, 0.1, 0.1], EMT produced stable performance in all three domains.",
                "Although direct comparisons are difficult to make, the EMT performance displayed notable efficiency vis-`a-vis the POMDP approach.",
                "In spite of a simple and inefficient Matlab implementation of the EMT algorithm, the decision time for any given step averaged significantly below 1 second in all experiments.",
                "For the irregular open arena domain, which proved to be the most difficult, 1000 experiment runs bounded by 100 steps each, a total of 42411 steps, were completed in slightly under 6 hours.",
                "That is, over 4 × 104 online steps took an order of magnitude less time than the offline computation of POMDP policy in [11].",
                "The significance of this differential is made even more prominent by the fact that, should the environment model parameters change, the online nature of EMT would allow it to maintain its performance time, while the POMDP policy would need to be recomputed, requiring yet again a large overhead of computation.",
                "We also tested the behavior cell frequency entropy, empirical measures from trial data.",
                "As Figure 4 and Figure 5 show, empir794 The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) A Q Q A 0 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 A Q Figure 3: These configurations of the Tag Game space were used: a) multiple dead-end, b) irregular open arena, c) circular corridor Table 2: Performance of the EMT-based solution in three Tag Game domains and two observability models: I) omniposition quarry, II) quarry is not at hunters position Model Domain Capture% E(Steps) Time/Step I Dead-ends 100 14.8 72(mSec) Arena 80.2 42.4 500(mSec) Circle 91.4 34.6 187(mSec) II Dead-ends 100 13.2 91(mSec) Arena 96.8 28.67 396(mSec) Circle 94.4 31.63 204(mSec) ical entropy grows with the length of interaction.",
                "For runs where the quarry was not captured immediately, the entropy reaches between 0.85 and 0.952 for different runs and scenarios.",
                "As the agent actively seeks the quarry, the entropy never reaches its maximum.",
                "One characteristic of the entropy graph for the open arena scenario particularly caught our attention in the case of the omniposition quarry observation model.",
                "Near the maximum limit of trial length (100 steps), entropy suddenly dropped.",
                "Further analysis of the data showed that under certain circumstances, a fluctuating behavior occurs in which the agent faces equally viable versions of quarry-following behavior.",
                "Since the EMT algorithm has greedy action selection, and the state space does not encode any form of commitment (not even speed or acceleration), the agent is locked within a small portion of cells.",
                "It is essentially attempting to simultaneously follow several courses of action, all of which are consistent with the target dynamics.",
                "This behavior did not occur in our second observation model, since it significantly reduced the set of eligible courses of action-essentially contributing to tie-breaking among them. 6.",
                "DISCUSSION The design of the EMT solution for the Tag Game exposes the core difference in approach to planning and control between EMT or DBC, on the one hand, and the more familiar POMDP approach, on the other.",
                "POMDP defines a reward structure to optimize, and influences system dynamics indirectly through that optimization.",
                "EMT discards any reward scheme, and instead measures and influences system dynamics directly. 2 Entropy was calculated using log base equal to the number of possible locations within the domain; this properly scales entropy expression into the range [0, 1] for all domains.",
                "Thus for the Tag Game, we did not search for a reward function that would encode and express our preference over the agents behavior, but rather directly set three (heuristic) behavior preferences as the basis for target dynamics to be maintained.",
                "Experimental data shows that these targets need not be directly achievable via the agents actions.",
                "However, the ratio between EMT performance and achievability of target dynamics remains to be explored.",
                "The tag game experiment data also revealed the different emphasis DBC and POMDPs place on the formulation of an environment state space.",
                "POMDPs rely entirely on the mechanism of reward accumulation maximization, i.e., formation of the action selection procedure to achieve necessary state sequencing.",
                "DBC, on the other hand, has two sources of sequencing specification: through the properties of an action selection procedure, and through direct specification within the target dynamics.",
                "The importance of the second source was underlined by the Tag Game experiment data, in which the greedy EMT algorithm, applied to a POMDP-type state space specification, failed, since target description over such a state space was incapable of encoding the necessary behavior tendencies, e.g., tie-breaking and commitment to directed motion.",
                "The structural differences between DBC (and EMT in particular), and POMDPs, prohibits direct performance comparison, and places them on complementary tracks, each within a suitable niche.",
                "For instance, POMDPs could be seen as a much more natural formulation of economic sequential decision-making problems, while EMT is a better fit for continual demand for stochastic change, as happens in many robotic or embodied-agent problems.",
                "The complementary properties of POMDPs and EMT can be further exploited.",
                "There is recent interest in using POMDPs in hybrid solutions [17], in which the POMDPs can be used together with other control approaches to provide results not easily achievable with either approach by itself.",
                "DBC can be an effective partner in such a hybrid solution.",
                "For instance, POMDPs have prohibitively large off-line time requirements for policy computation, but can be readily used in simpler settings to expose beneficial behavioral trends; this can serve as a form of target dynamics that are provided to EMT in a larger domain for on-line operation. 7.",
                "CONCLUSIONS AND FUTURE WORK In this paper, we have presented a novel perspective on the process of planning and control in stochastic environments, in the form of the Dynamics Based Control (DBC) framework.",
                "DBC formulates the task of planning as support of a specified target system dynamics, which describes the necessary properties of change within the environment.",
                "Optimality of DBC plans of action are measured The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) 795 0 20 40 60 80 100 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1 Steps Entropy Dead−ends 0 20 40 60 80 100 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1 Steps Entropy Arena 0 20 40 60 80 100 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1 Steps Entropy Circle Figure 4: Observation Model I: Omniposition quarry.",
                "Entropy development with length of Tag Game for the three experiment scenarios: a) multiple dead-end, b) irregular open arena, c) circular corridor. 0 10 20 30 40 50 60 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1 Steps Entropy Dead−ends 0 20 40 60 80 100 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1 Steps Entropy Arena 0 20 40 60 80 100 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1 Steps Entropy Circle Figure 5: Observation Model II: quarry not observed at hunters position.",
                "Entropy development with length of Tag Game for the three experiment scenarios: a) multiple dead-end, b) irregular open arena, c) circular corridor. with respect to the deviation of actual system dynamics from the target dynamics.",
                "We show that a recently developed technique of Extended Markov Tracking (EMT) [13] is an instantiation of DBC.",
                "In fact, EMT can be seen as a specific case of DBC parameterization, which employs a greedy action selection procedure.",
                "Since EMT exhibits the key features of the general DBC framework, as well as polynomial time complexity, we used the multitarget version of EMT [15] to demonstrate that the class of area sweeping problems naturally lends itself to dynamics-based descriptions, as instantiated by our experiments in the Tag Game domain.",
                "As enumerated in Section 4.2, EMT has a number of limitations, such as difficulty in dealing with negative dynamic preference.",
                "This prevents direct application of EMT to such problems as Rendezvous-Evasion Games (e.g., [6]).",
                "However, DBC in general has no such limitations, and readily enables the formulation of evasion games.",
                "In future work, we intend to proceed with the development of dynamics-based controllers for these problems. 8.",
                "ACKNOWLEDGMENT The work of the first two authors was partially supported by Israel Science Foundation grant #898/05, and the third author was partially supported by a grant from Israels Ministry of Science and Technology. 9.",
                "REFERENCES [1] R. C. Arkin.",
                "Behavior-Based Robotics.",
                "MIT Press, 1998. [2] J.",
                "A. Bilmes.",
                "A gentle tutorial of the EM algorithm and its application to parameter estimation for Gaussian mixture and Hidden Markov Models.",
                "Technical Report TR-97-021, Department of Electrical Engeineering and Computer Science, University of California at Berkeley, 1998. [3] T. M. Cover and J.",
                "A. Thomas.",
                "Elements of information theory.",
                "Wiley, 1991. [4] M. E. desJardins, E. H. Durfee, C. L. Ortiz, and M. J. Wolverton.",
                "A survey of research in distributed, continual planning.",
                "AI Magazine, 4:13-22, 1999. [5] V. R. Konda and J. N. Tsitsiklis.",
                "Actor-Critic algorithms.",
                "SIAM Journal on Control and Optimization, 42(4):1143-1166, 2003. [6] W. S. Lim.",
                "A rendezvous-evasion game on discrete locations with joint randomization.",
                "Advances in Applied Probability, 29(4):1004-1017, December 1997. [7] M. L. Littman, T. L. Dean, and L. P. Kaelbling.",
                "On the complexity of solving Markov decision problems.",
                "In Proceedings of the 11th Annual Conference on Uncertainty in Artificial Intelligence (UAI-95), pages 394-402, 1995. [8] O. Madani, S. Hanks, and A. Condon.",
                "On the undecidability of probabilistic planning and related stochastic optimization problems.",
                "Artificial Intelligence Journal, 147(1-2):5-34, July 2003. [9] R. M. Neal and G. E. Hinton.",
                "A view of the EM algorithm 796 The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) that justifies incremental, sparse, and other variants.",
                "In M. I. Jordan, editor, Learning in Graphical Models, pages 355-368.",
                "Kluwer Academic Publishers, 1998. [10] P. Paruchuri, M. Tambe, F. Ordonez, and S. Kraus.",
                "Security in multiagent systems by policy randomization.",
                "In Proceeding of AAMAS 2006, 2006. [11] J. Pineau, G. Gordon, and S. Thrun.",
                "Point-based value iteration: An anytime algorithm for pomdps.",
                "In International Joint Conference on Artificial Intelligence (IJCAI), pages 1025-1032, August 2003. [12] M. L. Puterman.",
                "Markov Decision Processes.",
                "Wiley Series in Probability and Mathematical Statistics: Applied Probability and Statistics Section.",
                "Wiley-Interscience Publication, New York, 1994. [13] Z. Rabinovich and J. S. Rosenschein.",
                "Extended Markov Tracking with an application to control.",
                "In The Workshop on Agent Tracking: Modeling Other Agents from Observations, at the Third International Joint Conference on Autonomous Agents and Multiagent Systems, pages 95-100, New-York, July 2004. [14] Z. Rabinovich and J. S. Rosenschein.",
                "Multiagent coordination by Extended Markov Tracking.",
                "In The Fourth International Joint Conference on Autonomous Agents and Multiagent Systems, pages 431-438, Utrecht, The Netherlands, July 2005. [15] Z. Rabinovich and J. S. Rosenschein.",
                "On the response of EMT-based control to interacting targets and models.",
                "In The Fifth International Joint Conference on Autonomous Agents and Multiagent Systems, pages 465-470, Hakodate, Japan, May 2006. [16] R. F. Stengel.",
                "Optimal Control and Estimation.",
                "Dover Publications, 1994. [17] M. Tambe, E. Bowring, H. Jung, G. Kaminka, R. Maheswaran, J. Marecki, J. Modi, R. Nair, J. Pearce, P. Paruchuri, D. Pynadath, P. Scerri, N. Schurr, and P. Varakantham.",
                "Conflicts in teamwork: Hybrids to the The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) 797"
            ],
            "original_annotated_samples": [],
            "translated_annotated_samples": [
                "El \"Juego de la etiqueta\" es otro ejemplo de la aplicabilidad del enfoque.",
                "EMT jugando la etiqueta El \"juego de la etiqueta\" se introdujo por primera vez en [11].",
                "Se muestra un dominio de ejemplo en la Figura 2. 0 51 2 3 4 6 7 8 10 12 13 161514 17 18 19 2221 23 9 11q A 20 Figura 2: Dominio de etiqueta;Un agente (a) intenta buscar y capturar una cantera (q) El \"juego de la etiqueta\" limita extremadamente la percepción de los agentes, de modo que el agente puede detectar la cantera solo si se ubican en la misma celda delMundo de la cuadrícula.",
                "Para el \"juego de la etiqueta\", podemos formular fácilmente tres tendencias principales: atrapar la cantera, permanecer móvil y acechar a la cantera."
            ],
            "translated_text": "",
            "candidates": [
                "juego de etiqueta",
                "Juego de la etiqueta",
                "juego de etiqueta",
                "juego de la etiqueta",
                "juego de etiqueta",
                "juego de la etiqueta",
                "juego de etiqueta",
                "juego de la etiqueta"
            ],
            "error": []
        },
        "tag game": {
            "translated_key": "juego de etiqueta",
            "is_in_text": true,
            "original_annotated_sentences": [
                "Dynamics Based Control with an Application to Area-Sweeping Problems Zinovi Rabinovich Engineering and Computer Science Hebrew University of Jerusalem Jerusalem, Israel nomad@cs.huji.ac.il Jeffrey S. Rosenschein Engineering and Computer Science Hebrew University of Jerusalem Jerusalem, Israel jeff@cs.huji.ac.il Gal A. Kaminka The MAVERICK Group Department of Computer Science Bar Ilan University, Israel galk@cs.biu.ac.il ABSTRACT In this paper we introduce Dynamics Based Control (DBC), an approach to planning and control of an agent in stochastic environments.",
                "Unlike existing approaches, which seek to optimize expected rewards (e.g., in Partially Observable Markov Decision Problems (POMDPs)), DBC optimizes system behavior towards specified system dynamics.",
                "We show that a recently developed planning and control approach, Extended Markov Tracking (EMT) is an instantiation of DBC.",
                "EMT employs greedy action selection to provide an efficient control algorithm in Markovian environments.",
                "We exploit this efficiency in a set of experiments that applied multitarget EMT to a class of area-sweeping problems (searching for moving targets).",
                "We show that such problems can be naturally defined and efficiently solved using the DBC framework, and its EMT instantiation.",
                "Categories and Subject Descriptors I.2.8 [Problem Solving, Control Methods, and Search]: Control Theory; I.2.9 [Robotics]; I.2.11 [Distributed Artificial Intelligence]: Intelligent Agents General Terms Algorithms, Theory 1.",
                "INTRODUCTION Planning and control constitutes a central research area in multiagent systems and artificial intelligence.",
                "In recent years, Partially Observable Markov Decision Processes (POMDPs) [12] have become a popular formal basis for planning in stochastic environments.",
                "In this framework, the planning and control problem is often addressed by imposing a reward function, and computing a policy (of choosing actions) that is optimal, in the sense that it will result in the highest expected utility.",
                "While theoretically attractive, the complexity of optimally solving a POMDP is prohibitive [8, 7].",
                "We take an alternative view of planning in stochastic environments.",
                "We do not use a (state-based) reward function, but instead optimize over a different criterion, a transition-based specification of the desired system dynamics.",
                "The idea here is to view planexecution as a process that compels a (stochastic) system to change, and a plan as a dynamic process that shapes that change according to desired criteria.",
                "We call this general planning framework Dynamics Based Control (DBC).",
                "In DBC, the goal of a planning (or control) process becomes to ensure that the system will change in accordance with specific (potentially stochastic) target dynamics.",
                "As actual system behavior may deviate from that which is specified by target dynamics (due to the stochastic nature of the system), planning in such environments needs to be continual [4], in a manner similar to classical closed-loop controllers [16].",
                "Here, optimality is measured in terms of probability of deviation magnitudes.",
                "In this paper, we present the structure of Dynamics Based Control.",
                "We show that the recently developed Extended Markov Tracking (EMT) approach [13, 14, 15] is subsumed by DBC, with EMT employing greedy action selection, which is a specific parameterization among the options possible within DBC.",
                "EMT is an efficient instantiation of DBC.",
                "To evaluate DBC, we carried out a set of experiments applying multi-target EMT to the <br>tag game</br> [11]; this is a variant on the area sweeping problem, where an agent is trying to tag a moving target (quarry) whose position is not known with certainty.",
                "Experimental data demonstrates that even with a simple model of the environment and a simple design of target dynamics, high success rates can be produced both in catching the quarry, and in surprising the quarry (as expressed by the observed entropy of the controlled agents position).",
                "The paper is organized as follows.",
                "In Section 2 we motivate DBC using area-sweeping problems, and discuss related work.",
                "Section 3 introduces the Dynamics Based Control (DBC) structure, and its specialization to Markovian environments.",
                "This is followed by a review of the Extended Markov Tracking (EMT) approach as a DBC-structured control regimen in Section 4.",
                "That section also discusses the limitations of EMT-based control relative to the general DBC framework.",
                "Experimental settings and results are then presented in Section 5.",
                "Section 6 provides a short discussion of the overall approach, and Section 7 gives some concluding remarks and directions for future work. 790 978-81-904262-7-5 (RPS) c 2007 IFAAMAS 2.",
                "MOTIVATION AND RELATED WORK Many real-life scenarios naturally have a stochastic target dynamics specification, especially those domains where there exists no ultimate goal, but rather system behavior (with specific properties) that has to be continually supported.",
                "For example, security guards perform persistent sweeps of an area to detect any sign of intrusion.",
                "Cunning thieves will attempt to track these sweeps, and time their operation to key points of the guards motion.",
                "It is thus advisable to make the guards motion dynamics appear irregular and random.",
                "Recent work by Paruchuri et al. [10] has addressed such randomization in the context of single-agent and distributed POMDPs.",
                "The goal in that work was to generate policies that provide a measure of action-selection randomization, while maintaining rewards within some acceptable levels.",
                "Our focus differs from this work in that DBC does not optimize expected rewards-indeed we do not consider rewards at all-but instead maintains desired dynamics (including, but not limited to, randomization).",
                "The Game of Tag is another example of the applicability of the approach.",
                "It was introduced in the work by Pineau et al. [11].",
                "There are two agents that can move about an area, which is divided into a grid.",
                "The grid may have blocked cells (holes) into which no agent can move.",
                "One agent (the hunter) seeks to move into a cell occupied by the other (the quarry), such that they are co-located (this is a successful tag).",
                "The quarry seeks to avoid the hunter agent, and is always aware of the hunters position, but does not know how the hunter will behave, which opens up the possibility for a hunter to surprise the prey.",
                "The hunter knows the quarrys probabilistic law of motion, but does not know its current location.",
                "Tag is an instance of a family of area-sweeping (pursuit-evasion) problems.",
                "In [11], the hunter modeled the problem using a POMDP.",
                "A reward function was defined, to reflect the desire to tag the quarry, and an action policy was computed to optimize the reward collected over time.",
                "Due to the intractable complexity of determining the optimal policy, the action policy computed in that paper was essentially an approximation.",
                "In this paper, instead of formulating a reward function, we use EMT to solve the problem, by directly specifying the target dynamics.",
                "In fact, any search problem with randomized motion, the socalled class of area sweeping problems, can be described through specification of such target system dynamics.",
                "Dynamics Based Control provides a natural approach to solving these problems. 3.",
                "DYNAMICS BASED CONTROL The specification of Dynamics Based Control (DBC) can be broken into three interacting levels: Environment Design Level, User Level, and Agent Level. • Environment Design Level is concerned with the formal specification and modeling of the environment.",
                "For example, this level would specify the laws of physics within the system, and set its parameters, such as the gravitation constant. • User Level in turn relies on the environment model produced by Environment Design to specify the target system dynamics it wishes to observe.",
                "The User Level also specifies the estimation or learning procedure for system dynamics, and the measure of deviation.",
                "In the museum guard scenario above, these would correspond to a stochastic sweep schedule, and a measure of relative surprise between the specified and actual sweeping. • Agent Level in turn combines the environment model from the Environment Design level, the dynamics estimation procedure, the deviation measure and the target dynamics specification from User Level, to produce a sequence of actions that create system dynamics as close as possible to the targeted specification.",
                "As we are interested in the continual development of a stochastic system, such as happens in classical control theory [16] and continual planning [4], as well as in our example of museum sweeps, the question becomes how the Agent Level is to treat the deviation measurements over time.",
                "To this end, we use a probability threshold-that is, we would like the Agent Level to maximize the probability that the deviation measure will remain below a certain threshold.",
                "Specific action selection then depends on system formalization.",
                "One possibility would be to create a mixture of available system trends, much like that which happens in Behavior-Based Robotic architectures [1].",
                "The other alternative would be to rely on the estimation procedure provided by the User Level-to utilize the Environment Design Level model of the environment to choose actions, so as to manipulate the dynamics estimator into believing that a certain dynamics has been achieved.",
                "Notice that this manipulation is not direct, but via the environment.",
                "Thus, for strong enough estimator algorithms, successful manipulation would mean a successful simulation of the specified target dynamics (i.e., beyond discerning via the available sensory input).",
                "DBC levels can also have a back-flow of information (see Figure 1).",
                "For instance, the Agent Level could provide data about target dynamics feasibility, allowing the User Level to modify the requirement, perhaps focusing on attainable features of system behavior.",
                "Data would also be available about the system response to different actions performed; combined with a dynamics estimator defined by the User Level, this can provide an important tool for the environment model calibration at the Environment Design Level.",
                "UserEnv.",
                "Design Agent Model Ideal Dynamics Estimator Estimator Dynamics Feasibility System Response Data Figure 1: Data flow of the DBC framework Extending upon the idea of Actor-Critic algorithms [5], DBC data flow can provide a good basis for the design of a learning algorithm.",
                "For example, the User Level can operate as an exploratory device for a learning algorithm, inferring an ideal dynamics target from the environment model at hand that would expose and verify most critical features of system behavior.",
                "In this case, feasibility and system response data from the Agent Level would provide key information for an environment model update.",
                "In fact, the combination of feasibility and response data can provide a basis for the application of strong learning algorithms such as EM [2, 9]. 3.1 DBC for Markovian Environments For a Partially Observable Markovian Environment, DBC can be specified in a more rigorous manner.",
                "Notice how DBC discards rewards, and replaces it by another optimality criterion (structural differences are summarized in Table 1): • Environment Design level is to specify a tuple < S, A, T, O, Ω, s0 >, where: - S is the set of all possible environment states; - s0 is the initial state of the environment (which can also be viewed as a probability distribution over S); The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) 791 - A is the set of all possible actions applicable in the environment; - T is the environments probabilistic transition function: T : S ×A → Π(S).",
                "That is, T(s |a, s) is the probability that the environment will move from state s to state s under action a; - O is the set of all possible observations.",
                "This is what the sensor input would look like for an outside observer; - Ω is the observation probability function: Ω : S × A × S → Π(O).",
                "That is, Ω(o|s , a, s) is the probability that one will observe o given that the environment has moved from state s to state s under action a. • User Level, in the case of Markovian environment, operates on the set of system dynamics described by a family of conditional probabilities F = {τ : S × A → Π(S)}.",
                "Thus specification of target dynamics can be expressed by q ∈ F, and the learning or tracking algorithm can be represented as a function L : O×(A×O)∗ → F; that is, it maps sequences of observations and actions performed so far into an estimate τ ∈ F of system dynamics.",
                "There are many possible variations available at the User Level to define divergence between system dynamics; several of them are: - Trace distance or L1 distance between two distributions p and q defined by D(p(·), q(·)) = 1 2 x |p(x) − q(x)| - Fidelity measure of distance F(p(·), q(·)) = x p(x)q(x) - Kullback-Leibler divergence DKL(p(·) q(·)) = x p(x) log p(x) q(x) Notice that the latter two are not actually metrics over the space of possible distributions, but nevertheless have meaningful and important interpretations.",
                "For instance, KullbackLeibler divergence is an important tool of information theory [3] that allows one to measure the price of encoding an information source governed by q, while assuming that it is governed by p. The User Level also defines the threshold of dynamics deviation probability θ. • Agent Level is then faced with a problem of selecting a control signal function a∗ to satisfy a minimization problem as follows: a∗ = arg min a Pr(d(τa, q) > θ) where d(τa, q) is a random variable describing deviation of the dynamics estimate τa, created by L under control signal a, from the ideal dynamics q.",
                "Implicit in this minimization problem is that L is manipulated via the environment, based on the environment model produced by the Environment Design Level. 3.2 DBC View of the State Space It is important to note the complementary view that DBC and POMDPs take on the state space of the environment.",
                "POMDPs regard state as a stationary snap-shot of the environment; whatever attributes of state sequencing one seeks are reached through properties of the control process, in this case reward accumulation.",
                "This can be viewed as if the sequencing of states and the attributes of that sequencing are only introduced by and for the controlling mechanism-the POMDP policy.",
                "DBC concentrates on the underlying principle of state sequencing, the system dynamics.",
                "DBCs target dynamics specification can use the environments state space as a means to describe, discern, and preserve changes that occur within the system.",
                "As a result, DBC has a greater ability to express state sequencing properties, which are grounded in the environment model and its state space definition.",
                "For example, consider the task of moving through rough terrain towards a goal and reaching it as fast as possible.",
                "POMDPs would encode terrain as state space points, while speed would be ensured by negative reward for every step taken without reaching the goalaccumulating higher reward can be reached only by faster motion.",
                "Alternatively, the state space could directly include the notion of speed.",
                "For POMDPs, this would mean that the same concept is encoded twice, in some sense: directly in the state space, and indirectly within reward accumulation.",
                "Now, even if the reward function would encode more, and finer, details of the properties of motion, the POMDP solution will have to search in a much larger space of policies, while still being guided by the implicit concept of the reward accumulation procedure.",
                "On the other hand, the tactical target expression of variations and correlations between position and speed of motion is now grounded in the state space representation.",
                "In this situation, any further constraints, e.g., smoothness of motion, speed limits in different locations, or speed reductions during sharp turns, are explicitly and uniformly expressed by the tactical target, and can result in faster and more effective action selection by a DBC algorithm. 4.",
                "EMT-BASED CONTROL AS A DBC Recently, a control algorithm was introduced called EMT-based Control [13], which instantiates the DBC framework.",
                "Although it provides an approximate greedy solution in the DBC sense, initial experiments using EMT-based control have been encouraging [14, 15].",
                "EMT-based control is based on the Markovian environment definition, as in the case with POMDPs, but its User and Agent Levels are of the Markovian DBC type of optimality. • User Level of EMT-based control defines a limited-case target system dynamics independent of action: qEMT : S → Π(S).",
                "It then utilizes the Kullback-Leibler divergence measure to compose a momentary system dynamics estimator-the Extended Markov Tracking (EMT) algorithm.",
                "The algorithm keeps a system dynamics estimate τt EMT that is capable of explaining recent change in an auxiliary Bayesian system state estimator from pt−1 to pt, and updates it conservatively using Kullback-Leibler divergence.",
                "Since τt EMT and pt−1,t are respectively the conditional and marginal probabilities over the systems state space, explanation simply means that pt(s ) = s τt EMT (s |s)pt−1(s), and the dynamics estimate update is performed by solving a 792 The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) Table 1: Structure of POMDP vs. Dynamics-Based Control in Markovian Environment Level Approach MDP Markovian DBC Environment < S, A, T, O, Ω >,where S - set of states A - set of actions Design T : S × A → Π(S) - transition O - observation set Ω : S × A × S → Π(O) User r : S × A × S → R q : S × A → Π(S) F(π∗ ) → r L(o1, ..., ot) → τ r - reward function q - ideal dynamics F - reward remodeling L - dynamics estimator θ - threshold Agent π∗ = arg max π E[ γt rt] π∗ = arg min π Prob(d(τ q) > θ) minimization problem: τt EMT = H[pt, pt−1, τt−1 EMT ] = arg min τ DKL(τ × pt−1 τt−1 EMT × pt−1) s.t. pt(s ) = s (τ × pt−1)(s , s) pt−1(s) = s (τ × pt−1)(s , s) • Agent Level in EMT-based control is suboptimal with respect to DBC (though it remains within the DBC framework), performing greedy action selection based on prediction of EMTs reaction.",
                "The prediction is based on the environment model provided by the Environment Design level, so that if we denote by Ta the environments transition function limited to action a, and pt−1 is the auxiliary Bayesian system state estimator, then the EMT-based control choice is described by a∗ = arg min a∈A DKL(H[Ta × pt, pt, τt EMT ] qEMT × pt−1) Note that this follows the Markovian DBC framework precisely: the rewarding optimality of POMDPs is discarded, and in its place a dynamics estimator (EMT in this case) is manipulated via action effects on the environment to produce an estimate close to the specified target system dynamics.",
                "Yet as we mentioned, naive EMTbased control is suboptimal in the DBC sense, and has several additional limitations that do not exist in the general DBC framework (discussed in Section 4.2). 4.1 Multi-Target EMT At times, there may exist several behavioral preferences.",
                "For example, in the case of museum guards, some art items are more heavily guarded, requiring that the guards stay more often in their close vicinity.",
                "On the other hand, no corner of the museum is to be left unchecked, which demands constant motion.",
                "Successful museum security would demand that the guards adhere to, and balance, both of these behaviors.",
                "For EMT-based control, this would mean facing several tactical targets {qk}K k=1, and the question becomes how to merge and balance them.",
                "A balancing mechanism can be applied to resolve this issue.",
                "Note that EMT-based control, while selecting an action, creates a preference vector over the set of actions based on their predicted performance with respect to a given target.",
                "If these preference vectors are normalized, they can be combined into a single unified preference.",
                "This requires replacement of standard EMT-based action selection by the algorithm below [15]: • Given: - a set of target dynamics {qk}K k=1, - vector of weights w(k) • Select action as follows - For each action a ∈ A predict the future state distribution ¯pa t+1 = Ta ∗ pt; - For each action, compute Da = H(¯pa t+1, pt, PDt) - For each a ∈ A and qk tactical target, denote V (a, k) = DKL (Da qk) pt .",
                "Let Vk(a) = 1 Zk V (a, k), where Zk = a∈A V (a, k) is a normalization factor. - Select a∗ = arg min a k k=1 w(k)Vk(a) The weights vector w = (w1, ..., wK ) allows the additional tuning of importance among target dynamics without the need to redesign the targets themselves.",
                "This balancing method is also seamlessly integrated into the EMT-based control flow of operation. 4.2 EMT-based Control Limitations EMT-based control is a sub-optimal (in the DBC sense) representative of the DBC structure.",
                "It limits the User by forcing EMT to be its dynamic tracking algorithm, and replaces Agent optimization by greedy action selection.",
                "This kind of combination, however, is common for on-line algorithms.",
                "Although further development of EMT-based controllers is necessary, evidence so far suggests that even the simplest form of the algorithm possesses a great deal of power, and displays trends that are optimal in the DBC sense of the word.",
                "There are two further, EMT-specific, limitations to EMT-based control that are evident at this point.",
                "Both already have partial solutions and are subjects of ongoing research.",
                "The first limitation is the problem of negative preference.",
                "In the POMDP framework for example, this is captured simply, through The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) 793 the appearance of values with different signs within the reward structure.",
                "For EMT-based control, however, negative preference means that one would like to avoid a certain distribution over system development sequences; EMT-based control, however, concentrates on getting as close as possible to a distribution.",
                "Avoidance is thus unnatural in native EMT-based control.",
                "The second limitation comes from the fact that standard environment modeling can create pure sensory actions-actions that do not change the state of the world, and differ only in the way observations are received and the quality of observations received.",
                "Since the world state does not change, EMT-based control would not be able to differentiate between different sensory actions.",
                "Notice that both of these limitations of EMT-based control are absent from the general DBC framework, since it may have a tracking algorithm capable of considering pure sensory actions and, unlike Kullback-Leibler divergence, a distribution deviation measure that is capable of dealing with negative preference. 5.",
                "EMT PLAYING TAG The Game of Tag was first introduced in [11].",
                "It is a single agent problem of capturing a quarry, and belongs to the class of area sweeping problems.",
                "An example domain is shown in Figure 2. 0 51 2 3 4 6 7 8 10 12 13 161514 17 18 19 2221 23 9 11Q A 20 Figure 2: Tag domain; an agent (A) attempts to seek and capture a quarry (Q) The Game of Tag extremely limits the agents perception, so that the agent is able to detect the quarry only if they are co-located in the same cell of the grid world.",
                "In the classical version of the game, co-location leads to a special observation, and the Tag action can be performed.",
                "We slightly modify this setting: the moment that both agents occupy the same cell, the game ends.",
                "As a result, both the agent and its quarry have the same motion capability, which allows them to move in four directions, North, South, East, and West.",
                "These form a formal space of actions within a Markovian environment.",
                "The state space of the formal Markovian environment is described by the cross-product of the agent and quarrys positions.",
                "For Figure 2, it would be S = {s0, ..., s23} × {s0, ..., s23}.",
                "The effects of an action taken by the agent are deterministic, but the environment in general has a stochastic response due to the motion of the quarry.",
                "With probability q0 1 it stays put, and with probability 1 − q0 it moves to an adjacent cell further away from the 1 In our experiments this was taken to be q0 = 0.2. agent.",
                "So for the instance shown in Figure 2 and q0 = 0.1: P(Q = s9|Q = s9, A = s11) = 0.1 P(Q = s2|Q = s9, A = s11) = 0.3 P(Q = s8|Q = s9, A = s11) = 0.3 P(Q = s14|Q = s9, A = s11) = 0.3 Although the evasive behavior of the quarry is known to the agent, the quarrys position is not.",
                "The only sensory information available to the agent is its own location.",
                "We use EMT and directly specify the target dynamics.",
                "For the Game of Tag, we can easily formulate three major trends: catching the quarry, staying mobile, and stalking the quarry.",
                "This results in the following three target dynamics: Tcatch(At+1 = si|Qt = sj, At = sa) ∝ 1 si = sj 0 otherwise Tmobile(At+1 = si|Qt = so, At = sj) ∝ 0 si = sj 1 otherwise Tstalk(At+1 = si|Qt = so, At = sj) ∝ 1 dist(si,so) Note that none of the above targets are directly achievable; for instance, if Qt = s9 and At = s11, there is no action that can move the agent to At+1 = s9 as required by the Tcatch target dynamics.",
                "We ran several experiments to evaluate EMT performance in the <br>tag game</br>.",
                "Three configurations of the domain shown in Figure 3 were used, each posing a different challenge to the agent due to partial observability.",
                "In each setting, a set of 1000 runs was performed with a time limit of 100 steps.",
                "In every run, the initial position of both the agent and its quarry was selected at random; this means that as far as the agent was concerned, the quarrys initial position was uniformly distributed over the entire domain cell space.",
                "We also used two variations of the environment observability function.",
                "In the first version, observability function mapped all joint positions of hunter and quarry into the position of the hunter as an observation.",
                "In the second, only those joint positions in which hunter and quarry occupied different locations were mapped into the hunters location.",
                "The second version in fact utilized and expressed the fact that once hunter and quarry occupy the same cell the game ends.",
                "The results of these experiments are shown in Table 2.",
                "Balancing [15] the catch, move, and stalk target dynamics described in the previous section by the weight vector [0.8, 0.1, 0.1], EMT produced stable performance in all three domains.",
                "Although direct comparisons are difficult to make, the EMT performance displayed notable efficiency vis-`a-vis the POMDP approach.",
                "In spite of a simple and inefficient Matlab implementation of the EMT algorithm, the decision time for any given step averaged significantly below 1 second in all experiments.",
                "For the irregular open arena domain, which proved to be the most difficult, 1000 experiment runs bounded by 100 steps each, a total of 42411 steps, were completed in slightly under 6 hours.",
                "That is, over 4 × 104 online steps took an order of magnitude less time than the offline computation of POMDP policy in [11].",
                "The significance of this differential is made even more prominent by the fact that, should the environment model parameters change, the online nature of EMT would allow it to maintain its performance time, while the POMDP policy would need to be recomputed, requiring yet again a large overhead of computation.",
                "We also tested the behavior cell frequency entropy, empirical measures from trial data.",
                "As Figure 4 and Figure 5 show, empir794 The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) A Q Q A 0 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 A Q Figure 3: These configurations of the <br>tag game</br> space were used: a) multiple dead-end, b) irregular open arena, c) circular corridor Table 2: Performance of the EMT-based solution in three <br>tag game</br> domains and two observability models: I) omniposition quarry, II) quarry is not at hunters position Model Domain Capture% E(Steps) Time/Step I Dead-ends 100 14.8 72(mSec) Arena 80.2 42.4 500(mSec) Circle 91.4 34.6 187(mSec) II Dead-ends 100 13.2 91(mSec) Arena 96.8 28.67 396(mSec) Circle 94.4 31.63 204(mSec) ical entropy grows with the length of interaction.",
                "For runs where the quarry was not captured immediately, the entropy reaches between 0.85 and 0.952 for different runs and scenarios.",
                "As the agent actively seeks the quarry, the entropy never reaches its maximum.",
                "One characteristic of the entropy graph for the open arena scenario particularly caught our attention in the case of the omniposition quarry observation model.",
                "Near the maximum limit of trial length (100 steps), entropy suddenly dropped.",
                "Further analysis of the data showed that under certain circumstances, a fluctuating behavior occurs in which the agent faces equally viable versions of quarry-following behavior.",
                "Since the EMT algorithm has greedy action selection, and the state space does not encode any form of commitment (not even speed or acceleration), the agent is locked within a small portion of cells.",
                "It is essentially attempting to simultaneously follow several courses of action, all of which are consistent with the target dynamics.",
                "This behavior did not occur in our second observation model, since it significantly reduced the set of eligible courses of action-essentially contributing to tie-breaking among them. 6.",
                "DISCUSSION The design of the EMT solution for the <br>tag game</br> exposes the core difference in approach to planning and control between EMT or DBC, on the one hand, and the more familiar POMDP approach, on the other.",
                "POMDP defines a reward structure to optimize, and influences system dynamics indirectly through that optimization.",
                "EMT discards any reward scheme, and instead measures and influences system dynamics directly. 2 Entropy was calculated using log base equal to the number of possible locations within the domain; this properly scales entropy expression into the range [0, 1] for all domains.",
                "Thus for the <br>tag game</br>, we did not search for a reward function that would encode and express our preference over the agents behavior, but rather directly set three (heuristic) behavior preferences as the basis for target dynamics to be maintained.",
                "Experimental data shows that these targets need not be directly achievable via the agents actions.",
                "However, the ratio between EMT performance and achievability of target dynamics remains to be explored.",
                "The <br>tag game</br> experiment data also revealed the different emphasis DBC and POMDPs place on the formulation of an environment state space.",
                "POMDPs rely entirely on the mechanism of reward accumulation maximization, i.e., formation of the action selection procedure to achieve necessary state sequencing.",
                "DBC, on the other hand, has two sources of sequencing specification: through the properties of an action selection procedure, and through direct specification within the target dynamics.",
                "The importance of the second source was underlined by the <br>tag game</br> experiment data, in which the greedy EMT algorithm, applied to a POMDP-type state space specification, failed, since target description over such a state space was incapable of encoding the necessary behavior tendencies, e.g., tie-breaking and commitment to directed motion.",
                "The structural differences between DBC (and EMT in particular), and POMDPs, prohibits direct performance comparison, and places them on complementary tracks, each within a suitable niche.",
                "For instance, POMDPs could be seen as a much more natural formulation of economic sequential decision-making problems, while EMT is a better fit for continual demand for stochastic change, as happens in many robotic or embodied-agent problems.",
                "The complementary properties of POMDPs and EMT can be further exploited.",
                "There is recent interest in using POMDPs in hybrid solutions [17], in which the POMDPs can be used together with other control approaches to provide results not easily achievable with either approach by itself.",
                "DBC can be an effective partner in such a hybrid solution.",
                "For instance, POMDPs have prohibitively large off-line time requirements for policy computation, but can be readily used in simpler settings to expose beneficial behavioral trends; this can serve as a form of target dynamics that are provided to EMT in a larger domain for on-line operation. 7.",
                "CONCLUSIONS AND FUTURE WORK In this paper, we have presented a novel perspective on the process of planning and control in stochastic environments, in the form of the Dynamics Based Control (DBC) framework.",
                "DBC formulates the task of planning as support of a specified target system dynamics, which describes the necessary properties of change within the environment.",
                "Optimality of DBC plans of action are measured The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) 795 0 20 40 60 80 100 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1 Steps Entropy Dead−ends 0 20 40 60 80 100 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1 Steps Entropy Arena 0 20 40 60 80 100 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1 Steps Entropy Circle Figure 4: Observation Model I: Omniposition quarry.",
                "Entropy development with length of <br>tag game</br> for the three experiment scenarios: a) multiple dead-end, b) irregular open arena, c) circular corridor. 0 10 20 30 40 50 60 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1 Steps Entropy Dead−ends 0 20 40 60 80 100 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1 Steps Entropy Arena 0 20 40 60 80 100 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1 Steps Entropy Circle Figure 5: Observation Model II: quarry not observed at hunters position.",
                "Entropy development with length of <br>tag game</br> for the three experiment scenarios: a) multiple dead-end, b) irregular open arena, c) circular corridor. with respect to the deviation of actual system dynamics from the target dynamics.",
                "We show that a recently developed technique of Extended Markov Tracking (EMT) [13] is an instantiation of DBC.",
                "In fact, EMT can be seen as a specific case of DBC parameterization, which employs a greedy action selection procedure.",
                "Since EMT exhibits the key features of the general DBC framework, as well as polynomial time complexity, we used the multitarget version of EMT [15] to demonstrate that the class of area sweeping problems naturally lends itself to dynamics-based descriptions, as instantiated by our experiments in the <br>tag game</br> domain.",
                "As enumerated in Section 4.2, EMT has a number of limitations, such as difficulty in dealing with negative dynamic preference.",
                "This prevents direct application of EMT to such problems as Rendezvous-Evasion Games (e.g., [6]).",
                "However, DBC in general has no such limitations, and readily enables the formulation of evasion games.",
                "In future work, we intend to proceed with the development of dynamics-based controllers for these problems. 8.",
                "ACKNOWLEDGMENT The work of the first two authors was partially supported by Israel Science Foundation grant #898/05, and the third author was partially supported by a grant from Israels Ministry of Science and Technology. 9.",
                "REFERENCES [1] R. C. Arkin.",
                "Behavior-Based Robotics.",
                "MIT Press, 1998. [2] J.",
                "A. Bilmes.",
                "A gentle tutorial of the EM algorithm and its application to parameter estimation for Gaussian mixture and Hidden Markov Models.",
                "Technical Report TR-97-021, Department of Electrical Engeineering and Computer Science, University of California at Berkeley, 1998. [3] T. M. Cover and J.",
                "A. Thomas.",
                "Elements of information theory.",
                "Wiley, 1991. [4] M. E. desJardins, E. H. Durfee, C. L. Ortiz, and M. J. Wolverton.",
                "A survey of research in distributed, continual planning.",
                "AI Magazine, 4:13-22, 1999. [5] V. R. Konda and J. N. Tsitsiklis.",
                "Actor-Critic algorithms.",
                "SIAM Journal on Control and Optimization, 42(4):1143-1166, 2003. [6] W. S. Lim.",
                "A rendezvous-evasion game on discrete locations with joint randomization.",
                "Advances in Applied Probability, 29(4):1004-1017, December 1997. [7] M. L. Littman, T. L. Dean, and L. P. Kaelbling.",
                "On the complexity of solving Markov decision problems.",
                "In Proceedings of the 11th Annual Conference on Uncertainty in Artificial Intelligence (UAI-95), pages 394-402, 1995. [8] O. Madani, S. Hanks, and A. Condon.",
                "On the undecidability of probabilistic planning and related stochastic optimization problems.",
                "Artificial Intelligence Journal, 147(1-2):5-34, July 2003. [9] R. M. Neal and G. E. Hinton.",
                "A view of the EM algorithm 796 The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) that justifies incremental, sparse, and other variants.",
                "In M. I. Jordan, editor, Learning in Graphical Models, pages 355-368.",
                "Kluwer Academic Publishers, 1998. [10] P. Paruchuri, M. Tambe, F. Ordonez, and S. Kraus.",
                "Security in multiagent systems by policy randomization.",
                "In Proceeding of AAMAS 2006, 2006. [11] J. Pineau, G. Gordon, and S. Thrun.",
                "Point-based value iteration: An anytime algorithm for pomdps.",
                "In International Joint Conference on Artificial Intelligence (IJCAI), pages 1025-1032, August 2003. [12] M. L. Puterman.",
                "Markov Decision Processes.",
                "Wiley Series in Probability and Mathematical Statistics: Applied Probability and Statistics Section.",
                "Wiley-Interscience Publication, New York, 1994. [13] Z. Rabinovich and J. S. Rosenschein.",
                "Extended Markov Tracking with an application to control.",
                "In The Workshop on Agent Tracking: Modeling Other Agents from Observations, at the Third International Joint Conference on Autonomous Agents and Multiagent Systems, pages 95-100, New-York, July 2004. [14] Z. Rabinovich and J. S. Rosenschein.",
                "Multiagent coordination by Extended Markov Tracking.",
                "In The Fourth International Joint Conference on Autonomous Agents and Multiagent Systems, pages 431-438, Utrecht, The Netherlands, July 2005. [15] Z. Rabinovich and J. S. Rosenschein.",
                "On the response of EMT-based control to interacting targets and models.",
                "In The Fifth International Joint Conference on Autonomous Agents and Multiagent Systems, pages 465-470, Hakodate, Japan, May 2006. [16] R. F. Stengel.",
                "Optimal Control and Estimation.",
                "Dover Publications, 1994. [17] M. Tambe, E. Bowring, H. Jung, G. Kaminka, R. Maheswaran, J. Marecki, J. Modi, R. Nair, J. Pearce, P. Paruchuri, D. Pynadath, P. Scerri, N. Schurr, and P. Varakantham.",
                "Conflicts in teamwork: Hybrids to the The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) 797"
            ],
            "original_annotated_samples": [],
            "translated_annotated_samples": [
                "Para evaluar DBC, llevamos a cabo un conjunto de experimentos que aplicaban EMT multi-objetivo al \"juego de etiquetas\" [11];Esta es una variante en el problema de barrido del área, donde un agente está tratando de etiquetar un objetivo móvil (cantera) cuya posición no se conoce con certeza.",
                "Ejecutamos varios experimentos para evaluar el rendimiento de EMT en el \"juego de etiquetas\".",
                "Conf.en agentes autónomos y sistemas de múltiples agentes (AAMAS 07) A Q Q A 0 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 A Q Figura 3: Estas configuraciones del espacio de \"juego de etiqueta\" se utilizaron: A)Múltiple extremo muerto, b) Arena abierta irregular, c) Corredor circular Tabla 2: rendimiento de la solución basada en EMT en tres dominios de \"juego de etiqueta\" y dos modelos de observabilidad: i) cantera de omniposición, ii) Quarry no está en posición de cazadoresCaptura del dominio del modelo% E (Pasos) Tiempo/Paso I Dead-Enden 100 14.8 72 (MSEC) Arena 80.2 42.4 500 (MSEC) Círculo 91.4 34.6 187 (MSEC) II Deates sin salida 100 13.2 91 (MSEC) Arena 96.8 28.67 396 (MSEC) Círculo 94.4 31.63 204 (MSEC) La entropía ical crece con la longitud de la interacción.",
                "Discusión El diseño de la solución EMT para el \"juego de etiqueta\" expone la diferencia central en el enfoque de planificación y control entre EMT o DBC, por un lado, y el enfoque POMDP más familiar, por el otro.",
                "Por lo tanto, para el \"juego de etiqueta\", no buscamos una función de recompensa que codifique y expresara nuestra preferencia sobre el comportamiento de los agentes, sino que estableciera directamente tres preferencias (heurísticas) de comportamiento como base para que se mantenga la dinámica objetivo.",
                "Los datos del experimento \"Juego de etiquetas\" también revelaron el diferente énfasis que DBC y POMDPS se encuentran en la formulación de un espacio de estado ambiental.",
                "La importancia de la segunda fuente fue subrayada por los datos del experimento del \"juego de etiqueta\", en el que el algoritmo EMT codicioso se aplicó a una especificación de espacio de estado de tipo POMDP, ya que la descripción del objetivo sobre dicho espacio de estado era incapaz de codificar lo necesarioTendencias de comportamiento, por ejemplo, rompiendo y compromiso con el movimiento dirigido.",
                "Desarrollo de entropía con longitud del \"juego de etiquetas\" para los tres escenarios de experimentos: a) múltiples de extremo muerto, b) Arena abierta irregular, c) Corredor circular.0 10 20 30 40 50 60 60 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1 Pasos Controlación Dead - EDENS 0 20 40 60 80 100 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1 Pasos Arena de entropía 0 20 40 60 80 0.2 0.4 0.4 0.5 0.6 0.60.7 0.8 0.9 1 Pasos Círculo de entropía Figura 5: Modelo de observación II: Quarry no se observa en la posición de los cazadores.",
                "Desarrollo de entropía con longitud del \"juego de etiquetas\" para los tres escenarios de experimentos: a) múltiples de extremo muerto, b) Arena abierta irregular, c) Corredor circular.con respecto a la desviación de la dinámica del sistema real de la dinámica de destino.",
                "Dado que EMT exhibe las características clave del marco DBC general, así como la complejidad del tiempo polinómico, utilizamos la versión multitarget de EMT [15] para demostrar que la clase de problemas de barrido de área se presta naturalmente a descripciones basadas en la dinámica, como está instanciado porNuestros experimentos en el dominio del \"juego de etiqueta\"."
            ],
            "translated_text": "",
            "candidates": [
                "juego de etiqueta",
                "juego de etiquetas",
                "juego de etiqueta",
                "juego de etiquetas",
                "juego de etiqueta",
                "juego de etiqueta",
                "juego de etiqueta",
                "juego de etiqueta",
                "juego de etiqueta",
                "juego de etiqueta",
                "juego de etiqueta",
                "juego de etiqueta",
                "Juego de etiquetas",
                "juego de etiqueta",
                "juego de etiqueta",
                "juego de etiqueta",
                "juego de etiquetas",
                "juego de etiqueta",
                "juego de etiquetas",
                "juego de etiqueta",
                "juego de etiqueta"
            ],
            "error": []
        },
        "environment design level": {
            "translated_key": "Nivel de diseño del entorno",
            "is_in_text": true,
            "original_annotated_sentences": [
                "Dynamics Based Control with an Application to Area-Sweeping Problems Zinovi Rabinovich Engineering and Computer Science Hebrew University of Jerusalem Jerusalem, Israel nomad@cs.huji.ac.il Jeffrey S. Rosenschein Engineering and Computer Science Hebrew University of Jerusalem Jerusalem, Israel jeff@cs.huji.ac.il Gal A. Kaminka The MAVERICK Group Department of Computer Science Bar Ilan University, Israel galk@cs.biu.ac.il ABSTRACT In this paper we introduce Dynamics Based Control (DBC), an approach to planning and control of an agent in stochastic environments.",
                "Unlike existing approaches, which seek to optimize expected rewards (e.g., in Partially Observable Markov Decision Problems (POMDPs)), DBC optimizes system behavior towards specified system dynamics.",
                "We show that a recently developed planning and control approach, Extended Markov Tracking (EMT) is an instantiation of DBC.",
                "EMT employs greedy action selection to provide an efficient control algorithm in Markovian environments.",
                "We exploit this efficiency in a set of experiments that applied multitarget EMT to a class of area-sweeping problems (searching for moving targets).",
                "We show that such problems can be naturally defined and efficiently solved using the DBC framework, and its EMT instantiation.",
                "Categories and Subject Descriptors I.2.8 [Problem Solving, Control Methods, and Search]: Control Theory; I.2.9 [Robotics]; I.2.11 [Distributed Artificial Intelligence]: Intelligent Agents General Terms Algorithms, Theory 1.",
                "INTRODUCTION Planning and control constitutes a central research area in multiagent systems and artificial intelligence.",
                "In recent years, Partially Observable Markov Decision Processes (POMDPs) [12] have become a popular formal basis for planning in stochastic environments.",
                "In this framework, the planning and control problem is often addressed by imposing a reward function, and computing a policy (of choosing actions) that is optimal, in the sense that it will result in the highest expected utility.",
                "While theoretically attractive, the complexity of optimally solving a POMDP is prohibitive [8, 7].",
                "We take an alternative view of planning in stochastic environments.",
                "We do not use a (state-based) reward function, but instead optimize over a different criterion, a transition-based specification of the desired system dynamics.",
                "The idea here is to view planexecution as a process that compels a (stochastic) system to change, and a plan as a dynamic process that shapes that change according to desired criteria.",
                "We call this general planning framework Dynamics Based Control (DBC).",
                "In DBC, the goal of a planning (or control) process becomes to ensure that the system will change in accordance with specific (potentially stochastic) target dynamics.",
                "As actual system behavior may deviate from that which is specified by target dynamics (due to the stochastic nature of the system), planning in such environments needs to be continual [4], in a manner similar to classical closed-loop controllers [16].",
                "Here, optimality is measured in terms of probability of deviation magnitudes.",
                "In this paper, we present the structure of Dynamics Based Control.",
                "We show that the recently developed Extended Markov Tracking (EMT) approach [13, 14, 15] is subsumed by DBC, with EMT employing greedy action selection, which is a specific parameterization among the options possible within DBC.",
                "EMT is an efficient instantiation of DBC.",
                "To evaluate DBC, we carried out a set of experiments applying multi-target EMT to the Tag Game [11]; this is a variant on the area sweeping problem, where an agent is trying to tag a moving target (quarry) whose position is not known with certainty.",
                "Experimental data demonstrates that even with a simple model of the environment and a simple design of target dynamics, high success rates can be produced both in catching the quarry, and in surprising the quarry (as expressed by the observed entropy of the controlled agents position).",
                "The paper is organized as follows.",
                "In Section 2 we motivate DBC using area-sweeping problems, and discuss related work.",
                "Section 3 introduces the Dynamics Based Control (DBC) structure, and its specialization to Markovian environments.",
                "This is followed by a review of the Extended Markov Tracking (EMT) approach as a DBC-structured control regimen in Section 4.",
                "That section also discusses the limitations of EMT-based control relative to the general DBC framework.",
                "Experimental settings and results are then presented in Section 5.",
                "Section 6 provides a short discussion of the overall approach, and Section 7 gives some concluding remarks and directions for future work. 790 978-81-904262-7-5 (RPS) c 2007 IFAAMAS 2.",
                "MOTIVATION AND RELATED WORK Many real-life scenarios naturally have a stochastic target dynamics specification, especially those domains where there exists no ultimate goal, but rather system behavior (with specific properties) that has to be continually supported.",
                "For example, security guards perform persistent sweeps of an area to detect any sign of intrusion.",
                "Cunning thieves will attempt to track these sweeps, and time their operation to key points of the guards motion.",
                "It is thus advisable to make the guards motion dynamics appear irregular and random.",
                "Recent work by Paruchuri et al. [10] has addressed such randomization in the context of single-agent and distributed POMDPs.",
                "The goal in that work was to generate policies that provide a measure of action-selection randomization, while maintaining rewards within some acceptable levels.",
                "Our focus differs from this work in that DBC does not optimize expected rewards-indeed we do not consider rewards at all-but instead maintains desired dynamics (including, but not limited to, randomization).",
                "The Game of Tag is another example of the applicability of the approach.",
                "It was introduced in the work by Pineau et al. [11].",
                "There are two agents that can move about an area, which is divided into a grid.",
                "The grid may have blocked cells (holes) into which no agent can move.",
                "One agent (the hunter) seeks to move into a cell occupied by the other (the quarry), such that they are co-located (this is a successful tag).",
                "The quarry seeks to avoid the hunter agent, and is always aware of the hunters position, but does not know how the hunter will behave, which opens up the possibility for a hunter to surprise the prey.",
                "The hunter knows the quarrys probabilistic law of motion, but does not know its current location.",
                "Tag is an instance of a family of area-sweeping (pursuit-evasion) problems.",
                "In [11], the hunter modeled the problem using a POMDP.",
                "A reward function was defined, to reflect the desire to tag the quarry, and an action policy was computed to optimize the reward collected over time.",
                "Due to the intractable complexity of determining the optimal policy, the action policy computed in that paper was essentially an approximation.",
                "In this paper, instead of formulating a reward function, we use EMT to solve the problem, by directly specifying the target dynamics.",
                "In fact, any search problem with randomized motion, the socalled class of area sweeping problems, can be described through specification of such target system dynamics.",
                "Dynamics Based Control provides a natural approach to solving these problems. 3.",
                "DYNAMICS BASED CONTROL The specification of Dynamics Based Control (DBC) can be broken into three interacting levels: <br>environment design level</br>, User Level, and Agent Level. • <br>environment design level</br> is concerned with the formal specification and modeling of the environment.",
                "For example, this level would specify the laws of physics within the system, and set its parameters, such as the gravitation constant. • User Level in turn relies on the environment model produced by Environment Design to specify the target system dynamics it wishes to observe.",
                "The User Level also specifies the estimation or learning procedure for system dynamics, and the measure of deviation.",
                "In the museum guard scenario above, these would correspond to a stochastic sweep schedule, and a measure of relative surprise between the specified and actual sweeping. • Agent Level in turn combines the environment model from the <br>environment design level</br>, the dynamics estimation procedure, the deviation measure and the target dynamics specification from User Level, to produce a sequence of actions that create system dynamics as close as possible to the targeted specification.",
                "As we are interested in the continual development of a stochastic system, such as happens in classical control theory [16] and continual planning [4], as well as in our example of museum sweeps, the question becomes how the Agent Level is to treat the deviation measurements over time.",
                "To this end, we use a probability threshold-that is, we would like the Agent Level to maximize the probability that the deviation measure will remain below a certain threshold.",
                "Specific action selection then depends on system formalization.",
                "One possibility would be to create a mixture of available system trends, much like that which happens in Behavior-Based Robotic architectures [1].",
                "The other alternative would be to rely on the estimation procedure provided by the User Level-to utilize the <br>environment design level</br> model of the environment to choose actions, so as to manipulate the dynamics estimator into believing that a certain dynamics has been achieved.",
                "Notice that this manipulation is not direct, but via the environment.",
                "Thus, for strong enough estimator algorithms, successful manipulation would mean a successful simulation of the specified target dynamics (i.e., beyond discerning via the available sensory input).",
                "DBC levels can also have a back-flow of information (see Figure 1).",
                "For instance, the Agent Level could provide data about target dynamics feasibility, allowing the User Level to modify the requirement, perhaps focusing on attainable features of system behavior.",
                "Data would also be available about the system response to different actions performed; combined with a dynamics estimator defined by the User Level, this can provide an important tool for the environment model calibration at the <br>environment design level</br>.",
                "UserEnv.",
                "Design Agent Model Ideal Dynamics Estimator Estimator Dynamics Feasibility System Response Data Figure 1: Data flow of the DBC framework Extending upon the idea of Actor-Critic algorithms [5], DBC data flow can provide a good basis for the design of a learning algorithm.",
                "For example, the User Level can operate as an exploratory device for a learning algorithm, inferring an ideal dynamics target from the environment model at hand that would expose and verify most critical features of system behavior.",
                "In this case, feasibility and system response data from the Agent Level would provide key information for an environment model update.",
                "In fact, the combination of feasibility and response data can provide a basis for the application of strong learning algorithms such as EM [2, 9]. 3.1 DBC for Markovian Environments For a Partially Observable Markovian Environment, DBC can be specified in a more rigorous manner.",
                "Notice how DBC discards rewards, and replaces it by another optimality criterion (structural differences are summarized in Table 1): • <br>environment design level</br> is to specify a tuple < S, A, T, O, Ω, s0 >, where: - S is the set of all possible environment states; - s0 is the initial state of the environment (which can also be viewed as a probability distribution over S); The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) 791 - A is the set of all possible actions applicable in the environment; - T is the environments probabilistic transition function: T : S ×A → Π(S).",
                "That is, T(s |a, s) is the probability that the environment will move from state s to state s under action a; - O is the set of all possible observations.",
                "This is what the sensor input would look like for an outside observer; - Ω is the observation probability function: Ω : S × A × S → Π(O).",
                "That is, Ω(o|s , a, s) is the probability that one will observe o given that the environment has moved from state s to state s under action a. • User Level, in the case of Markovian environment, operates on the set of system dynamics described by a family of conditional probabilities F = {τ : S × A → Π(S)}.",
                "Thus specification of target dynamics can be expressed by q ∈ F, and the learning or tracking algorithm can be represented as a function L : O×(A×O)∗ → F; that is, it maps sequences of observations and actions performed so far into an estimate τ ∈ F of system dynamics.",
                "There are many possible variations available at the User Level to define divergence between system dynamics; several of them are: - Trace distance or L1 distance between two distributions p and q defined by D(p(·), q(·)) = 1 2 x |p(x) − q(x)| - Fidelity measure of distance F(p(·), q(·)) = x p(x)q(x) - Kullback-Leibler divergence DKL(p(·) q(·)) = x p(x) log p(x) q(x) Notice that the latter two are not actually metrics over the space of possible distributions, but nevertheless have meaningful and important interpretations.",
                "For instance, KullbackLeibler divergence is an important tool of information theory [3] that allows one to measure the price of encoding an information source governed by q, while assuming that it is governed by p. The User Level also defines the threshold of dynamics deviation probability θ. • Agent Level is then faced with a problem of selecting a control signal function a∗ to satisfy a minimization problem as follows: a∗ = arg min a Pr(d(τa, q) > θ) where d(τa, q) is a random variable describing deviation of the dynamics estimate τa, created by L under control signal a, from the ideal dynamics q.",
                "Implicit in this minimization problem is that L is manipulated via the environment, based on the environment model produced by the <br>environment design level</br>. 3.2 DBC View of the State Space It is important to note the complementary view that DBC and POMDPs take on the state space of the environment.",
                "POMDPs regard state as a stationary snap-shot of the environment; whatever attributes of state sequencing one seeks are reached through properties of the control process, in this case reward accumulation.",
                "This can be viewed as if the sequencing of states and the attributes of that sequencing are only introduced by and for the controlling mechanism-the POMDP policy.",
                "DBC concentrates on the underlying principle of state sequencing, the system dynamics.",
                "DBCs target dynamics specification can use the environments state space as a means to describe, discern, and preserve changes that occur within the system.",
                "As a result, DBC has a greater ability to express state sequencing properties, which are grounded in the environment model and its state space definition.",
                "For example, consider the task of moving through rough terrain towards a goal and reaching it as fast as possible.",
                "POMDPs would encode terrain as state space points, while speed would be ensured by negative reward for every step taken without reaching the goalaccumulating higher reward can be reached only by faster motion.",
                "Alternatively, the state space could directly include the notion of speed.",
                "For POMDPs, this would mean that the same concept is encoded twice, in some sense: directly in the state space, and indirectly within reward accumulation.",
                "Now, even if the reward function would encode more, and finer, details of the properties of motion, the POMDP solution will have to search in a much larger space of policies, while still being guided by the implicit concept of the reward accumulation procedure.",
                "On the other hand, the tactical target expression of variations and correlations between position and speed of motion is now grounded in the state space representation.",
                "In this situation, any further constraints, e.g., smoothness of motion, speed limits in different locations, or speed reductions during sharp turns, are explicitly and uniformly expressed by the tactical target, and can result in faster and more effective action selection by a DBC algorithm. 4.",
                "EMT-BASED CONTROL AS A DBC Recently, a control algorithm was introduced called EMT-based Control [13], which instantiates the DBC framework.",
                "Although it provides an approximate greedy solution in the DBC sense, initial experiments using EMT-based control have been encouraging [14, 15].",
                "EMT-based control is based on the Markovian environment definition, as in the case with POMDPs, but its User and Agent Levels are of the Markovian DBC type of optimality. • User Level of EMT-based control defines a limited-case target system dynamics independent of action: qEMT : S → Π(S).",
                "It then utilizes the Kullback-Leibler divergence measure to compose a momentary system dynamics estimator-the Extended Markov Tracking (EMT) algorithm.",
                "The algorithm keeps a system dynamics estimate τt EMT that is capable of explaining recent change in an auxiliary Bayesian system state estimator from pt−1 to pt, and updates it conservatively using Kullback-Leibler divergence.",
                "Since τt EMT and pt−1,t are respectively the conditional and marginal probabilities over the systems state space, explanation simply means that pt(s ) = s τt EMT (s |s)pt−1(s), and the dynamics estimate update is performed by solving a 792 The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) Table 1: Structure of POMDP vs. Dynamics-Based Control in Markovian Environment Level Approach MDP Markovian DBC Environment < S, A, T, O, Ω >,where S - set of states A - set of actions Design T : S × A → Π(S) - transition O - observation set Ω : S × A × S → Π(O) User r : S × A × S → R q : S × A → Π(S) F(π∗ ) → r L(o1, ..., ot) → τ r - reward function q - ideal dynamics F - reward remodeling L - dynamics estimator θ - threshold Agent π∗ = arg max π E[ γt rt] π∗ = arg min π Prob(d(τ q) > θ) minimization problem: τt EMT = H[pt, pt−1, τt−1 EMT ] = arg min τ DKL(τ × pt−1 τt−1 EMT × pt−1) s.t. pt(s ) = s (τ × pt−1)(s , s) pt−1(s) = s (τ × pt−1)(s , s) • Agent Level in EMT-based control is suboptimal with respect to DBC (though it remains within the DBC framework), performing greedy action selection based on prediction of EMTs reaction.",
                "The prediction is based on the environment model provided by the <br>environment design level</br>, so that if we denote by Ta the environments transition function limited to action a, and pt−1 is the auxiliary Bayesian system state estimator, then the EMT-based control choice is described by a∗ = arg min a∈A DKL(H[Ta × pt, pt, τt EMT ] qEMT × pt−1) Note that this follows the Markovian DBC framework precisely: the rewarding optimality of POMDPs is discarded, and in its place a dynamics estimator (EMT in this case) is manipulated via action effects on the environment to produce an estimate close to the specified target system dynamics.",
                "Yet as we mentioned, naive EMTbased control is suboptimal in the DBC sense, and has several additional limitations that do not exist in the general DBC framework (discussed in Section 4.2). 4.1 Multi-Target EMT At times, there may exist several behavioral preferences.",
                "For example, in the case of museum guards, some art items are more heavily guarded, requiring that the guards stay more often in their close vicinity.",
                "On the other hand, no corner of the museum is to be left unchecked, which demands constant motion.",
                "Successful museum security would demand that the guards adhere to, and balance, both of these behaviors.",
                "For EMT-based control, this would mean facing several tactical targets {qk}K k=1, and the question becomes how to merge and balance them.",
                "A balancing mechanism can be applied to resolve this issue.",
                "Note that EMT-based control, while selecting an action, creates a preference vector over the set of actions based on their predicted performance with respect to a given target.",
                "If these preference vectors are normalized, they can be combined into a single unified preference.",
                "This requires replacement of standard EMT-based action selection by the algorithm below [15]: • Given: - a set of target dynamics {qk}K k=1, - vector of weights w(k) • Select action as follows - For each action a ∈ A predict the future state distribution ¯pa t+1 = Ta ∗ pt; - For each action, compute Da = H(¯pa t+1, pt, PDt) - For each a ∈ A and qk tactical target, denote V (a, k) = DKL (Da qk) pt .",
                "Let Vk(a) = 1 Zk V (a, k), where Zk = a∈A V (a, k) is a normalization factor. - Select a∗ = arg min a k k=1 w(k)Vk(a) The weights vector w = (w1, ..., wK ) allows the additional tuning of importance among target dynamics without the need to redesign the targets themselves.",
                "This balancing method is also seamlessly integrated into the EMT-based control flow of operation. 4.2 EMT-based Control Limitations EMT-based control is a sub-optimal (in the DBC sense) representative of the DBC structure.",
                "It limits the User by forcing EMT to be its dynamic tracking algorithm, and replaces Agent optimization by greedy action selection.",
                "This kind of combination, however, is common for on-line algorithms.",
                "Although further development of EMT-based controllers is necessary, evidence so far suggests that even the simplest form of the algorithm possesses a great deal of power, and displays trends that are optimal in the DBC sense of the word.",
                "There are two further, EMT-specific, limitations to EMT-based control that are evident at this point.",
                "Both already have partial solutions and are subjects of ongoing research.",
                "The first limitation is the problem of negative preference.",
                "In the POMDP framework for example, this is captured simply, through The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) 793 the appearance of values with different signs within the reward structure.",
                "For EMT-based control, however, negative preference means that one would like to avoid a certain distribution over system development sequences; EMT-based control, however, concentrates on getting as close as possible to a distribution.",
                "Avoidance is thus unnatural in native EMT-based control.",
                "The second limitation comes from the fact that standard environment modeling can create pure sensory actions-actions that do not change the state of the world, and differ only in the way observations are received and the quality of observations received.",
                "Since the world state does not change, EMT-based control would not be able to differentiate between different sensory actions.",
                "Notice that both of these limitations of EMT-based control are absent from the general DBC framework, since it may have a tracking algorithm capable of considering pure sensory actions and, unlike Kullback-Leibler divergence, a distribution deviation measure that is capable of dealing with negative preference. 5.",
                "EMT PLAYING TAG The Game of Tag was first introduced in [11].",
                "It is a single agent problem of capturing a quarry, and belongs to the class of area sweeping problems.",
                "An example domain is shown in Figure 2. 0 51 2 3 4 6 7 8 10 12 13 161514 17 18 19 2221 23 9 11Q A 20 Figure 2: Tag domain; an agent (A) attempts to seek and capture a quarry (Q) The Game of Tag extremely limits the agents perception, so that the agent is able to detect the quarry only if they are co-located in the same cell of the grid world.",
                "In the classical version of the game, co-location leads to a special observation, and the Tag action can be performed.",
                "We slightly modify this setting: the moment that both agents occupy the same cell, the game ends.",
                "As a result, both the agent and its quarry have the same motion capability, which allows them to move in four directions, North, South, East, and West.",
                "These form a formal space of actions within a Markovian environment.",
                "The state space of the formal Markovian environment is described by the cross-product of the agent and quarrys positions.",
                "For Figure 2, it would be S = {s0, ..., s23} × {s0, ..., s23}.",
                "The effects of an action taken by the agent are deterministic, but the environment in general has a stochastic response due to the motion of the quarry.",
                "With probability q0 1 it stays put, and with probability 1 − q0 it moves to an adjacent cell further away from the 1 In our experiments this was taken to be q0 = 0.2. agent.",
                "So for the instance shown in Figure 2 and q0 = 0.1: P(Q = s9|Q = s9, A = s11) = 0.1 P(Q = s2|Q = s9, A = s11) = 0.3 P(Q = s8|Q = s9, A = s11) = 0.3 P(Q = s14|Q = s9, A = s11) = 0.3 Although the evasive behavior of the quarry is known to the agent, the quarrys position is not.",
                "The only sensory information available to the agent is its own location.",
                "We use EMT and directly specify the target dynamics.",
                "For the Game of Tag, we can easily formulate three major trends: catching the quarry, staying mobile, and stalking the quarry.",
                "This results in the following three target dynamics: Tcatch(At+1 = si|Qt = sj, At = sa) ∝ 1 si = sj 0 otherwise Tmobile(At+1 = si|Qt = so, At = sj) ∝ 0 si = sj 1 otherwise Tstalk(At+1 = si|Qt = so, At = sj) ∝ 1 dist(si,so) Note that none of the above targets are directly achievable; for instance, if Qt = s9 and At = s11, there is no action that can move the agent to At+1 = s9 as required by the Tcatch target dynamics.",
                "We ran several experiments to evaluate EMT performance in the Tag Game.",
                "Three configurations of the domain shown in Figure 3 were used, each posing a different challenge to the agent due to partial observability.",
                "In each setting, a set of 1000 runs was performed with a time limit of 100 steps.",
                "In every run, the initial position of both the agent and its quarry was selected at random; this means that as far as the agent was concerned, the quarrys initial position was uniformly distributed over the entire domain cell space.",
                "We also used two variations of the environment observability function.",
                "In the first version, observability function mapped all joint positions of hunter and quarry into the position of the hunter as an observation.",
                "In the second, only those joint positions in which hunter and quarry occupied different locations were mapped into the hunters location.",
                "The second version in fact utilized and expressed the fact that once hunter and quarry occupy the same cell the game ends.",
                "The results of these experiments are shown in Table 2.",
                "Balancing [15] the catch, move, and stalk target dynamics described in the previous section by the weight vector [0.8, 0.1, 0.1], EMT produced stable performance in all three domains.",
                "Although direct comparisons are difficult to make, the EMT performance displayed notable efficiency vis-`a-vis the POMDP approach.",
                "In spite of a simple and inefficient Matlab implementation of the EMT algorithm, the decision time for any given step averaged significantly below 1 second in all experiments.",
                "For the irregular open arena domain, which proved to be the most difficult, 1000 experiment runs bounded by 100 steps each, a total of 42411 steps, were completed in slightly under 6 hours.",
                "That is, over 4 × 104 online steps took an order of magnitude less time than the offline computation of POMDP policy in [11].",
                "The significance of this differential is made even more prominent by the fact that, should the environment model parameters change, the online nature of EMT would allow it to maintain its performance time, while the POMDP policy would need to be recomputed, requiring yet again a large overhead of computation.",
                "We also tested the behavior cell frequency entropy, empirical measures from trial data.",
                "As Figure 4 and Figure 5 show, empir794 The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) A Q Q A 0 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 A Q Figure 3: These configurations of the Tag Game space were used: a) multiple dead-end, b) irregular open arena, c) circular corridor Table 2: Performance of the EMT-based solution in three Tag Game domains and two observability models: I) omniposition quarry, II) quarry is not at hunters position Model Domain Capture% E(Steps) Time/Step I Dead-ends 100 14.8 72(mSec) Arena 80.2 42.4 500(mSec) Circle 91.4 34.6 187(mSec) II Dead-ends 100 13.2 91(mSec) Arena 96.8 28.67 396(mSec) Circle 94.4 31.63 204(mSec) ical entropy grows with the length of interaction.",
                "For runs where the quarry was not captured immediately, the entropy reaches between 0.85 and 0.952 for different runs and scenarios.",
                "As the agent actively seeks the quarry, the entropy never reaches its maximum.",
                "One characteristic of the entropy graph for the open arena scenario particularly caught our attention in the case of the omniposition quarry observation model.",
                "Near the maximum limit of trial length (100 steps), entropy suddenly dropped.",
                "Further analysis of the data showed that under certain circumstances, a fluctuating behavior occurs in which the agent faces equally viable versions of quarry-following behavior.",
                "Since the EMT algorithm has greedy action selection, and the state space does not encode any form of commitment (not even speed or acceleration), the agent is locked within a small portion of cells.",
                "It is essentially attempting to simultaneously follow several courses of action, all of which are consistent with the target dynamics.",
                "This behavior did not occur in our second observation model, since it significantly reduced the set of eligible courses of action-essentially contributing to tie-breaking among them. 6.",
                "DISCUSSION The design of the EMT solution for the Tag Game exposes the core difference in approach to planning and control between EMT or DBC, on the one hand, and the more familiar POMDP approach, on the other.",
                "POMDP defines a reward structure to optimize, and influences system dynamics indirectly through that optimization.",
                "EMT discards any reward scheme, and instead measures and influences system dynamics directly. 2 Entropy was calculated using log base equal to the number of possible locations within the domain; this properly scales entropy expression into the range [0, 1] for all domains.",
                "Thus for the Tag Game, we did not search for a reward function that would encode and express our preference over the agents behavior, but rather directly set three (heuristic) behavior preferences as the basis for target dynamics to be maintained.",
                "Experimental data shows that these targets need not be directly achievable via the agents actions.",
                "However, the ratio between EMT performance and achievability of target dynamics remains to be explored.",
                "The tag game experiment data also revealed the different emphasis DBC and POMDPs place on the formulation of an environment state space.",
                "POMDPs rely entirely on the mechanism of reward accumulation maximization, i.e., formation of the action selection procedure to achieve necessary state sequencing.",
                "DBC, on the other hand, has two sources of sequencing specification: through the properties of an action selection procedure, and through direct specification within the target dynamics.",
                "The importance of the second source was underlined by the Tag Game experiment data, in which the greedy EMT algorithm, applied to a POMDP-type state space specification, failed, since target description over such a state space was incapable of encoding the necessary behavior tendencies, e.g., tie-breaking and commitment to directed motion.",
                "The structural differences between DBC (and EMT in particular), and POMDPs, prohibits direct performance comparison, and places them on complementary tracks, each within a suitable niche.",
                "For instance, POMDPs could be seen as a much more natural formulation of economic sequential decision-making problems, while EMT is a better fit for continual demand for stochastic change, as happens in many robotic or embodied-agent problems.",
                "The complementary properties of POMDPs and EMT can be further exploited.",
                "There is recent interest in using POMDPs in hybrid solutions [17], in which the POMDPs can be used together with other control approaches to provide results not easily achievable with either approach by itself.",
                "DBC can be an effective partner in such a hybrid solution.",
                "For instance, POMDPs have prohibitively large off-line time requirements for policy computation, but can be readily used in simpler settings to expose beneficial behavioral trends; this can serve as a form of target dynamics that are provided to EMT in a larger domain for on-line operation. 7.",
                "CONCLUSIONS AND FUTURE WORK In this paper, we have presented a novel perspective on the process of planning and control in stochastic environments, in the form of the Dynamics Based Control (DBC) framework.",
                "DBC formulates the task of planning as support of a specified target system dynamics, which describes the necessary properties of change within the environment.",
                "Optimality of DBC plans of action are measured The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) 795 0 20 40 60 80 100 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1 Steps Entropy Dead−ends 0 20 40 60 80 100 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1 Steps Entropy Arena 0 20 40 60 80 100 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1 Steps Entropy Circle Figure 4: Observation Model I: Omniposition quarry.",
                "Entropy development with length of Tag Game for the three experiment scenarios: a) multiple dead-end, b) irregular open arena, c) circular corridor. 0 10 20 30 40 50 60 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1 Steps Entropy Dead−ends 0 20 40 60 80 100 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1 Steps Entropy Arena 0 20 40 60 80 100 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1 Steps Entropy Circle Figure 5: Observation Model II: quarry not observed at hunters position.",
                "Entropy development with length of Tag Game for the three experiment scenarios: a) multiple dead-end, b) irregular open arena, c) circular corridor. with respect to the deviation of actual system dynamics from the target dynamics.",
                "We show that a recently developed technique of Extended Markov Tracking (EMT) [13] is an instantiation of DBC.",
                "In fact, EMT can be seen as a specific case of DBC parameterization, which employs a greedy action selection procedure.",
                "Since EMT exhibits the key features of the general DBC framework, as well as polynomial time complexity, we used the multitarget version of EMT [15] to demonstrate that the class of area sweeping problems naturally lends itself to dynamics-based descriptions, as instantiated by our experiments in the Tag Game domain.",
                "As enumerated in Section 4.2, EMT has a number of limitations, such as difficulty in dealing with negative dynamic preference.",
                "This prevents direct application of EMT to such problems as Rendezvous-Evasion Games (e.g., [6]).",
                "However, DBC in general has no such limitations, and readily enables the formulation of evasion games.",
                "In future work, we intend to proceed with the development of dynamics-based controllers for these problems. 8.",
                "ACKNOWLEDGMENT The work of the first two authors was partially supported by Israel Science Foundation grant #898/05, and the third author was partially supported by a grant from Israels Ministry of Science and Technology. 9.",
                "REFERENCES [1] R. C. Arkin.",
                "Behavior-Based Robotics.",
                "MIT Press, 1998. [2] J.",
                "A. Bilmes.",
                "A gentle tutorial of the EM algorithm and its application to parameter estimation for Gaussian mixture and Hidden Markov Models.",
                "Technical Report TR-97-021, Department of Electrical Engeineering and Computer Science, University of California at Berkeley, 1998. [3] T. M. Cover and J.",
                "A. Thomas.",
                "Elements of information theory.",
                "Wiley, 1991. [4] M. E. desJardins, E. H. Durfee, C. L. Ortiz, and M. J. Wolverton.",
                "A survey of research in distributed, continual planning.",
                "AI Magazine, 4:13-22, 1999. [5] V. R. Konda and J. N. Tsitsiklis.",
                "Actor-Critic algorithms.",
                "SIAM Journal on Control and Optimization, 42(4):1143-1166, 2003. [6] W. S. Lim.",
                "A rendezvous-evasion game on discrete locations with joint randomization.",
                "Advances in Applied Probability, 29(4):1004-1017, December 1997. [7] M. L. Littman, T. L. Dean, and L. P. Kaelbling.",
                "On the complexity of solving Markov decision problems.",
                "In Proceedings of the 11th Annual Conference on Uncertainty in Artificial Intelligence (UAI-95), pages 394-402, 1995. [8] O. Madani, S. Hanks, and A. Condon.",
                "On the undecidability of probabilistic planning and related stochastic optimization problems.",
                "Artificial Intelligence Journal, 147(1-2):5-34, July 2003. [9] R. M. Neal and G. E. Hinton.",
                "A view of the EM algorithm 796 The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) that justifies incremental, sparse, and other variants.",
                "In M. I. Jordan, editor, Learning in Graphical Models, pages 355-368.",
                "Kluwer Academic Publishers, 1998. [10] P. Paruchuri, M. Tambe, F. Ordonez, and S. Kraus.",
                "Security in multiagent systems by policy randomization.",
                "In Proceeding of AAMAS 2006, 2006. [11] J. Pineau, G. Gordon, and S. Thrun.",
                "Point-based value iteration: An anytime algorithm for pomdps.",
                "In International Joint Conference on Artificial Intelligence (IJCAI), pages 1025-1032, August 2003. [12] M. L. Puterman.",
                "Markov Decision Processes.",
                "Wiley Series in Probability and Mathematical Statistics: Applied Probability and Statistics Section.",
                "Wiley-Interscience Publication, New York, 1994. [13] Z. Rabinovich and J. S. Rosenschein.",
                "Extended Markov Tracking with an application to control.",
                "In The Workshop on Agent Tracking: Modeling Other Agents from Observations, at the Third International Joint Conference on Autonomous Agents and Multiagent Systems, pages 95-100, New-York, July 2004. [14] Z. Rabinovich and J. S. Rosenschein.",
                "Multiagent coordination by Extended Markov Tracking.",
                "In The Fourth International Joint Conference on Autonomous Agents and Multiagent Systems, pages 431-438, Utrecht, The Netherlands, July 2005. [15] Z. Rabinovich and J. S. Rosenschein.",
                "On the response of EMT-based control to interacting targets and models.",
                "In The Fifth International Joint Conference on Autonomous Agents and Multiagent Systems, pages 465-470, Hakodate, Japan, May 2006. [16] R. F. Stengel.",
                "Optimal Control and Estimation.",
                "Dover Publications, 1994. [17] M. Tambe, E. Bowring, H. Jung, G. Kaminka, R. Maheswaran, J. Marecki, J. Modi, R. Nair, J. Pearce, P. Paruchuri, D. Pynadath, P. Scerri, N. Schurr, and P. Varakantham.",
                "Conflicts in teamwork: Hybrids to the The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) 797"
            ],
            "original_annotated_samples": [],
            "translated_annotated_samples": [
                "Control basado en la dinámica La especificación del control basado en la dinámica (DBC) se puede dividir en tres niveles de interacción: \"Nivel de diseño del entorno\", nivel de usuario y nivel de agente.• El \"nivel de diseño del medio ambiente\" se refiere a la especificación formal y al modelado del entorno.",
                "En el escenario de la Guardia del Museo anterior, estos corresponderían a un horario estocástico de barrido, y una medida de sorpresa relativa entre el barrido especificado y real.• El nivel de agente a su vez combina el modelo de entorno desde el \"nivel de diseño del entorno\", el procedimiento de estimación de la dinámica, la medida de desviación y la especificación de dinámica de destino desde el nivel del usuario, para producir una secuencia de acciones que crean dinámicas del sistema lo más cerca posible de laEspecificación dirigida.",
                "La otra alternativa sería confiar en el procedimiento de estimación proporcionado por el nivel del usuario para utilizar el modelo de \"nivel de diseño del entorno\" del entorno para elegir acciones, a fin de manipular el estimador de dinámica para que crea que se ha logrado una cierta dinámica.",
                "Los datos también estarían disponibles sobre la respuesta del sistema a diferentes acciones realizadas;Combinado con un estimador de dinámica definido por el nivel de usuario, esto puede proporcionar una herramienta importante para la calibración del modelo de entorno en el \"nivel de diseño del entorno\".",
                "Observe cómo DBC descarta las recompensas y la reemplaza por otro criterio de optimización (las diferencias estructurales se resumen en la Tabla 1): • \"Nivel de diseño del entorno\" es especificar una tuple <s, a, t, o, Ω, s0>, dónde::- S es el conjunto de todos los estados de entorno posibles;- S0 es el estado inicial del medio ambiente (que también se puede ver como una distribución de probabilidad sobre S);El sexto intl.",
                "Implícito en este problema de minimización es que L se manipula a través del entorno, basado en el modelo de entorno producido por el \"nivel de diseño del entorno\".3.2 Vista DBC del espacio de estado Es importante tener en cuenta la vista complementaria que DBC y POMDPS toman el espacio estatal del medio ambiente.",
                "La predicción se basa en el modelo de entorno proporcionado por el \"nivel de diseño del entorno\", de modo que si denotamos por TA la función de transición de los entornos limitados a la acción A, y PT-1 es el estimador de estado del sistema bayesiano auxiliar, entonces la EMT basada en EMTLa elección de control se describe mediante un ∗ = arg min a∈A dkl (h [ta × pt, pt, τt emt] qemt × pt - 1) Tenga en cuenta que esto sigue el marco de DBC de Markovian con precisión: la optimización gratificante de POMDPS se descarta, se descarta,y en su lugar, se manipula un estimador de dinámica (EMT en este caso) mediante efectos de acción en el entorno para producir una estimación cercana a la dinámica del sistema objetivo especificada."
            ],
            "translated_text": "",
            "candidates": [
                "Nivel de diseño del medio ambiente",
                "Nivel de diseño del entorno",
                "nivel de diseño del medio ambiente",
                "Nivel de diseño del medio ambiente",
                "nivel de diseño del entorno",
                "Nivel de diseño del medio ambiente",
                "nivel de diseño del entorno",
                "Nivel de diseño del medio ambiente",
                "nivel de diseño del entorno",
                "Nivel de diseño del medio ambiente",
                "Nivel de diseño del entorno",
                "Nivel de diseño del medio ambiente",
                "nivel de diseño del entorno",
                "Nivel de diseño del medio ambiente",
                "nivel de diseño del entorno"
            ],
            "error": []
        },
        "user level": {
            "translated_key": "nivel de usuario",
            "is_in_text": true,
            "original_annotated_sentences": [
                "Dynamics Based Control with an Application to Area-Sweeping Problems Zinovi Rabinovich Engineering and Computer Science Hebrew University of Jerusalem Jerusalem, Israel nomad@cs.huji.ac.il Jeffrey S. Rosenschein Engineering and Computer Science Hebrew University of Jerusalem Jerusalem, Israel jeff@cs.huji.ac.il Gal A. Kaminka The MAVERICK Group Department of Computer Science Bar Ilan University, Israel galk@cs.biu.ac.il ABSTRACT In this paper we introduce Dynamics Based Control (DBC), an approach to planning and control of an agent in stochastic environments.",
                "Unlike existing approaches, which seek to optimize expected rewards (e.g., in Partially Observable Markov Decision Problems (POMDPs)), DBC optimizes system behavior towards specified system dynamics.",
                "We show that a recently developed planning and control approach, Extended Markov Tracking (EMT) is an instantiation of DBC.",
                "EMT employs greedy action selection to provide an efficient control algorithm in Markovian environments.",
                "We exploit this efficiency in a set of experiments that applied multitarget EMT to a class of area-sweeping problems (searching for moving targets).",
                "We show that such problems can be naturally defined and efficiently solved using the DBC framework, and its EMT instantiation.",
                "Categories and Subject Descriptors I.2.8 [Problem Solving, Control Methods, and Search]: Control Theory; I.2.9 [Robotics]; I.2.11 [Distributed Artificial Intelligence]: Intelligent Agents General Terms Algorithms, Theory 1.",
                "INTRODUCTION Planning and control constitutes a central research area in multiagent systems and artificial intelligence.",
                "In recent years, Partially Observable Markov Decision Processes (POMDPs) [12] have become a popular formal basis for planning in stochastic environments.",
                "In this framework, the planning and control problem is often addressed by imposing a reward function, and computing a policy (of choosing actions) that is optimal, in the sense that it will result in the highest expected utility.",
                "While theoretically attractive, the complexity of optimally solving a POMDP is prohibitive [8, 7].",
                "We take an alternative view of planning in stochastic environments.",
                "We do not use a (state-based) reward function, but instead optimize over a different criterion, a transition-based specification of the desired system dynamics.",
                "The idea here is to view planexecution as a process that compels a (stochastic) system to change, and a plan as a dynamic process that shapes that change according to desired criteria.",
                "We call this general planning framework Dynamics Based Control (DBC).",
                "In DBC, the goal of a planning (or control) process becomes to ensure that the system will change in accordance with specific (potentially stochastic) target dynamics.",
                "As actual system behavior may deviate from that which is specified by target dynamics (due to the stochastic nature of the system), planning in such environments needs to be continual [4], in a manner similar to classical closed-loop controllers [16].",
                "Here, optimality is measured in terms of probability of deviation magnitudes.",
                "In this paper, we present the structure of Dynamics Based Control.",
                "We show that the recently developed Extended Markov Tracking (EMT) approach [13, 14, 15] is subsumed by DBC, with EMT employing greedy action selection, which is a specific parameterization among the options possible within DBC.",
                "EMT is an efficient instantiation of DBC.",
                "To evaluate DBC, we carried out a set of experiments applying multi-target EMT to the Tag Game [11]; this is a variant on the area sweeping problem, where an agent is trying to tag a moving target (quarry) whose position is not known with certainty.",
                "Experimental data demonstrates that even with a simple model of the environment and a simple design of target dynamics, high success rates can be produced both in catching the quarry, and in surprising the quarry (as expressed by the observed entropy of the controlled agents position).",
                "The paper is organized as follows.",
                "In Section 2 we motivate DBC using area-sweeping problems, and discuss related work.",
                "Section 3 introduces the Dynamics Based Control (DBC) structure, and its specialization to Markovian environments.",
                "This is followed by a review of the Extended Markov Tracking (EMT) approach as a DBC-structured control regimen in Section 4.",
                "That section also discusses the limitations of EMT-based control relative to the general DBC framework.",
                "Experimental settings and results are then presented in Section 5.",
                "Section 6 provides a short discussion of the overall approach, and Section 7 gives some concluding remarks and directions for future work. 790 978-81-904262-7-5 (RPS) c 2007 IFAAMAS 2.",
                "MOTIVATION AND RELATED WORK Many real-life scenarios naturally have a stochastic target dynamics specification, especially those domains where there exists no ultimate goal, but rather system behavior (with specific properties) that has to be continually supported.",
                "For example, security guards perform persistent sweeps of an area to detect any sign of intrusion.",
                "Cunning thieves will attempt to track these sweeps, and time their operation to key points of the guards motion.",
                "It is thus advisable to make the guards motion dynamics appear irregular and random.",
                "Recent work by Paruchuri et al. [10] has addressed such randomization in the context of single-agent and distributed POMDPs.",
                "The goal in that work was to generate policies that provide a measure of action-selection randomization, while maintaining rewards within some acceptable levels.",
                "Our focus differs from this work in that DBC does not optimize expected rewards-indeed we do not consider rewards at all-but instead maintains desired dynamics (including, but not limited to, randomization).",
                "The Game of Tag is another example of the applicability of the approach.",
                "It was introduced in the work by Pineau et al. [11].",
                "There are two agents that can move about an area, which is divided into a grid.",
                "The grid may have blocked cells (holes) into which no agent can move.",
                "One agent (the hunter) seeks to move into a cell occupied by the other (the quarry), such that they are co-located (this is a successful tag).",
                "The quarry seeks to avoid the hunter agent, and is always aware of the hunters position, but does not know how the hunter will behave, which opens up the possibility for a hunter to surprise the prey.",
                "The hunter knows the quarrys probabilistic law of motion, but does not know its current location.",
                "Tag is an instance of a family of area-sweeping (pursuit-evasion) problems.",
                "In [11], the hunter modeled the problem using a POMDP.",
                "A reward function was defined, to reflect the desire to tag the quarry, and an action policy was computed to optimize the reward collected over time.",
                "Due to the intractable complexity of determining the optimal policy, the action policy computed in that paper was essentially an approximation.",
                "In this paper, instead of formulating a reward function, we use EMT to solve the problem, by directly specifying the target dynamics.",
                "In fact, any search problem with randomized motion, the socalled class of area sweeping problems, can be described through specification of such target system dynamics.",
                "Dynamics Based Control provides a natural approach to solving these problems. 3.",
                "DYNAMICS BASED CONTROL The specification of Dynamics Based Control (DBC) can be broken into three interacting levels: Environment Design Level, <br>user level</br>, and Agent Level. • Environment Design Level is concerned with the formal specification and modeling of the environment.",
                "For example, this level would specify the laws of physics within the system, and set its parameters, such as the gravitation constant. • <br>user level</br> in turn relies on the environment model produced by Environment Design to specify the target system dynamics it wishes to observe.",
                "The <br>user level</br> also specifies the estimation or learning procedure for system dynamics, and the measure of deviation.",
                "In the museum guard scenario above, these would correspond to a stochastic sweep schedule, and a measure of relative surprise between the specified and actual sweeping. • Agent Level in turn combines the environment model from the Environment Design level, the dynamics estimation procedure, the deviation measure and the target dynamics specification from <br>user level</br>, to produce a sequence of actions that create system dynamics as close as possible to the targeted specification.",
                "As we are interested in the continual development of a stochastic system, such as happens in classical control theory [16] and continual planning [4], as well as in our example of museum sweeps, the question becomes how the Agent Level is to treat the deviation measurements over time.",
                "To this end, we use a probability threshold-that is, we would like the Agent Level to maximize the probability that the deviation measure will remain below a certain threshold.",
                "Specific action selection then depends on system formalization.",
                "One possibility would be to create a mixture of available system trends, much like that which happens in Behavior-Based Robotic architectures [1].",
                "The other alternative would be to rely on the estimation procedure provided by the <br>user level</br>-to utilize the Environment Design Level model of the environment to choose actions, so as to manipulate the dynamics estimator into believing that a certain dynamics has been achieved.",
                "Notice that this manipulation is not direct, but via the environment.",
                "Thus, for strong enough estimator algorithms, successful manipulation would mean a successful simulation of the specified target dynamics (i.e., beyond discerning via the available sensory input).",
                "DBC levels can also have a back-flow of information (see Figure 1).",
                "For instance, the Agent Level could provide data about target dynamics feasibility, allowing the <br>user level</br> to modify the requirement, perhaps focusing on attainable features of system behavior.",
                "Data would also be available about the system response to different actions performed; combined with a dynamics estimator defined by the <br>user level</br>, this can provide an important tool for the environment model calibration at the Environment Design Level.",
                "UserEnv.",
                "Design Agent Model Ideal Dynamics Estimator Estimator Dynamics Feasibility System Response Data Figure 1: Data flow of the DBC framework Extending upon the idea of Actor-Critic algorithms [5], DBC data flow can provide a good basis for the design of a learning algorithm.",
                "For example, the <br>user level</br> can operate as an exploratory device for a learning algorithm, inferring an ideal dynamics target from the environment model at hand that would expose and verify most critical features of system behavior.",
                "In this case, feasibility and system response data from the Agent Level would provide key information for an environment model update.",
                "In fact, the combination of feasibility and response data can provide a basis for the application of strong learning algorithms such as EM [2, 9]. 3.1 DBC for Markovian Environments For a Partially Observable Markovian Environment, DBC can be specified in a more rigorous manner.",
                "Notice how DBC discards rewards, and replaces it by another optimality criterion (structural differences are summarized in Table 1): • Environment Design level is to specify a tuple < S, A, T, O, Ω, s0 >, where: - S is the set of all possible environment states; - s0 is the initial state of the environment (which can also be viewed as a probability distribution over S); The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) 791 - A is the set of all possible actions applicable in the environment; - T is the environments probabilistic transition function: T : S ×A → Π(S).",
                "That is, T(s |a, s) is the probability that the environment will move from state s to state s under action a; - O is the set of all possible observations.",
                "This is what the sensor input would look like for an outside observer; - Ω is the observation probability function: Ω : S × A × S → Π(O).",
                "That is, Ω(o|s , a, s) is the probability that one will observe o given that the environment has moved from state s to state s under action a. • <br>user level</br>, in the case of Markovian environment, operates on the set of system dynamics described by a family of conditional probabilities F = {τ : S × A → Π(S)}.",
                "Thus specification of target dynamics can be expressed by q ∈ F, and the learning or tracking algorithm can be represented as a function L : O×(A×O)∗ → F; that is, it maps sequences of observations and actions performed so far into an estimate τ ∈ F of system dynamics.",
                "There are many possible variations available at the <br>user level</br> to define divergence between system dynamics; several of them are: - Trace distance or L1 distance between two distributions p and q defined by D(p(·), q(·)) = 1 2 x |p(x) − q(x)| - Fidelity measure of distance F(p(·), q(·)) = x p(x)q(x) - Kullback-Leibler divergence DKL(p(·) q(·)) = x p(x) log p(x) q(x) Notice that the latter two are not actually metrics over the space of possible distributions, but nevertheless have meaningful and important interpretations.",
                "For instance, KullbackLeibler divergence is an important tool of information theory [3] that allows one to measure the price of encoding an information source governed by q, while assuming that it is governed by p. The <br>user level</br> also defines the threshold of dynamics deviation probability θ. • Agent Level is then faced with a problem of selecting a control signal function a∗ to satisfy a minimization problem as follows: a∗ = arg min a Pr(d(τa, q) > θ) where d(τa, q) is a random variable describing deviation of the dynamics estimate τa, created by L under control signal a, from the ideal dynamics q.",
                "Implicit in this minimization problem is that L is manipulated via the environment, based on the environment model produced by the Environment Design Level. 3.2 DBC View of the State Space It is important to note the complementary view that DBC and POMDPs take on the state space of the environment.",
                "POMDPs regard state as a stationary snap-shot of the environment; whatever attributes of state sequencing one seeks are reached through properties of the control process, in this case reward accumulation.",
                "This can be viewed as if the sequencing of states and the attributes of that sequencing are only introduced by and for the controlling mechanism-the POMDP policy.",
                "DBC concentrates on the underlying principle of state sequencing, the system dynamics.",
                "DBCs target dynamics specification can use the environments state space as a means to describe, discern, and preserve changes that occur within the system.",
                "As a result, DBC has a greater ability to express state sequencing properties, which are grounded in the environment model and its state space definition.",
                "For example, consider the task of moving through rough terrain towards a goal and reaching it as fast as possible.",
                "POMDPs would encode terrain as state space points, while speed would be ensured by negative reward for every step taken without reaching the goalaccumulating higher reward can be reached only by faster motion.",
                "Alternatively, the state space could directly include the notion of speed.",
                "For POMDPs, this would mean that the same concept is encoded twice, in some sense: directly in the state space, and indirectly within reward accumulation.",
                "Now, even if the reward function would encode more, and finer, details of the properties of motion, the POMDP solution will have to search in a much larger space of policies, while still being guided by the implicit concept of the reward accumulation procedure.",
                "On the other hand, the tactical target expression of variations and correlations between position and speed of motion is now grounded in the state space representation.",
                "In this situation, any further constraints, e.g., smoothness of motion, speed limits in different locations, or speed reductions during sharp turns, are explicitly and uniformly expressed by the tactical target, and can result in faster and more effective action selection by a DBC algorithm. 4.",
                "EMT-BASED CONTROL AS A DBC Recently, a control algorithm was introduced called EMT-based Control [13], which instantiates the DBC framework.",
                "Although it provides an approximate greedy solution in the DBC sense, initial experiments using EMT-based control have been encouraging [14, 15].",
                "EMT-based control is based on the Markovian environment definition, as in the case with POMDPs, but its User and Agent Levels are of the Markovian DBC type of optimality. • <br>user level</br> of EMT-based control defines a limited-case target system dynamics independent of action: qEMT : S → Π(S).",
                "It then utilizes the Kullback-Leibler divergence measure to compose a momentary system dynamics estimator-the Extended Markov Tracking (EMT) algorithm.",
                "The algorithm keeps a system dynamics estimate τt EMT that is capable of explaining recent change in an auxiliary Bayesian system state estimator from pt−1 to pt, and updates it conservatively using Kullback-Leibler divergence.",
                "Since τt EMT and pt−1,t are respectively the conditional and marginal probabilities over the systems state space, explanation simply means that pt(s ) = s τt EMT (s |s)pt−1(s), and the dynamics estimate update is performed by solving a 792 The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) Table 1: Structure of POMDP vs. Dynamics-Based Control in Markovian Environment Level Approach MDP Markovian DBC Environment < S, A, T, O, Ω >,where S - set of states A - set of actions Design T : S × A → Π(S) - transition O - observation set Ω : S × A × S → Π(O) User r : S × A × S → R q : S × A → Π(S) F(π∗ ) → r L(o1, ..., ot) → τ r - reward function q - ideal dynamics F - reward remodeling L - dynamics estimator θ - threshold Agent π∗ = arg max π E[ γt rt] π∗ = arg min π Prob(d(τ q) > θ) minimization problem: τt EMT = H[pt, pt−1, τt−1 EMT ] = arg min τ DKL(τ × pt−1 τt−1 EMT × pt−1) s.t. pt(s ) = s (τ × pt−1)(s , s) pt−1(s) = s (τ × pt−1)(s , s) • Agent Level in EMT-based control is suboptimal with respect to DBC (though it remains within the DBC framework), performing greedy action selection based on prediction of EMTs reaction.",
                "The prediction is based on the environment model provided by the Environment Design level, so that if we denote by Ta the environments transition function limited to action a, and pt−1 is the auxiliary Bayesian system state estimator, then the EMT-based control choice is described by a∗ = arg min a∈A DKL(H[Ta × pt, pt, τt EMT ] qEMT × pt−1) Note that this follows the Markovian DBC framework precisely: the rewarding optimality of POMDPs is discarded, and in its place a dynamics estimator (EMT in this case) is manipulated via action effects on the environment to produce an estimate close to the specified target system dynamics.",
                "Yet as we mentioned, naive EMTbased control is suboptimal in the DBC sense, and has several additional limitations that do not exist in the general DBC framework (discussed in Section 4.2). 4.1 Multi-Target EMT At times, there may exist several behavioral preferences.",
                "For example, in the case of museum guards, some art items are more heavily guarded, requiring that the guards stay more often in their close vicinity.",
                "On the other hand, no corner of the museum is to be left unchecked, which demands constant motion.",
                "Successful museum security would demand that the guards adhere to, and balance, both of these behaviors.",
                "For EMT-based control, this would mean facing several tactical targets {qk}K k=1, and the question becomes how to merge and balance them.",
                "A balancing mechanism can be applied to resolve this issue.",
                "Note that EMT-based control, while selecting an action, creates a preference vector over the set of actions based on their predicted performance with respect to a given target.",
                "If these preference vectors are normalized, they can be combined into a single unified preference.",
                "This requires replacement of standard EMT-based action selection by the algorithm below [15]: • Given: - a set of target dynamics {qk}K k=1, - vector of weights w(k) • Select action as follows - For each action a ∈ A predict the future state distribution ¯pa t+1 = Ta ∗ pt; - For each action, compute Da = H(¯pa t+1, pt, PDt) - For each a ∈ A and qk tactical target, denote V (a, k) = DKL (Da qk) pt .",
                "Let Vk(a) = 1 Zk V (a, k), where Zk = a∈A V (a, k) is a normalization factor. - Select a∗ = arg min a k k=1 w(k)Vk(a) The weights vector w = (w1, ..., wK ) allows the additional tuning of importance among target dynamics without the need to redesign the targets themselves.",
                "This balancing method is also seamlessly integrated into the EMT-based control flow of operation. 4.2 EMT-based Control Limitations EMT-based control is a sub-optimal (in the DBC sense) representative of the DBC structure.",
                "It limits the User by forcing EMT to be its dynamic tracking algorithm, and replaces Agent optimization by greedy action selection.",
                "This kind of combination, however, is common for on-line algorithms.",
                "Although further development of EMT-based controllers is necessary, evidence so far suggests that even the simplest form of the algorithm possesses a great deal of power, and displays trends that are optimal in the DBC sense of the word.",
                "There are two further, EMT-specific, limitations to EMT-based control that are evident at this point.",
                "Both already have partial solutions and are subjects of ongoing research.",
                "The first limitation is the problem of negative preference.",
                "In the POMDP framework for example, this is captured simply, through The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) 793 the appearance of values with different signs within the reward structure.",
                "For EMT-based control, however, negative preference means that one would like to avoid a certain distribution over system development sequences; EMT-based control, however, concentrates on getting as close as possible to a distribution.",
                "Avoidance is thus unnatural in native EMT-based control.",
                "The second limitation comes from the fact that standard environment modeling can create pure sensory actions-actions that do not change the state of the world, and differ only in the way observations are received and the quality of observations received.",
                "Since the world state does not change, EMT-based control would not be able to differentiate between different sensory actions.",
                "Notice that both of these limitations of EMT-based control are absent from the general DBC framework, since it may have a tracking algorithm capable of considering pure sensory actions and, unlike Kullback-Leibler divergence, a distribution deviation measure that is capable of dealing with negative preference. 5.",
                "EMT PLAYING TAG The Game of Tag was first introduced in [11].",
                "It is a single agent problem of capturing a quarry, and belongs to the class of area sweeping problems.",
                "An example domain is shown in Figure 2. 0 51 2 3 4 6 7 8 10 12 13 161514 17 18 19 2221 23 9 11Q A 20 Figure 2: Tag domain; an agent (A) attempts to seek and capture a quarry (Q) The Game of Tag extremely limits the agents perception, so that the agent is able to detect the quarry only if they are co-located in the same cell of the grid world.",
                "In the classical version of the game, co-location leads to a special observation, and the Tag action can be performed.",
                "We slightly modify this setting: the moment that both agents occupy the same cell, the game ends.",
                "As a result, both the agent and its quarry have the same motion capability, which allows them to move in four directions, North, South, East, and West.",
                "These form a formal space of actions within a Markovian environment.",
                "The state space of the formal Markovian environment is described by the cross-product of the agent and quarrys positions.",
                "For Figure 2, it would be S = {s0, ..., s23} × {s0, ..., s23}.",
                "The effects of an action taken by the agent are deterministic, but the environment in general has a stochastic response due to the motion of the quarry.",
                "With probability q0 1 it stays put, and with probability 1 − q0 it moves to an adjacent cell further away from the 1 In our experiments this was taken to be q0 = 0.2. agent.",
                "So for the instance shown in Figure 2 and q0 = 0.1: P(Q = s9|Q = s9, A = s11) = 0.1 P(Q = s2|Q = s9, A = s11) = 0.3 P(Q = s8|Q = s9, A = s11) = 0.3 P(Q = s14|Q = s9, A = s11) = 0.3 Although the evasive behavior of the quarry is known to the agent, the quarrys position is not.",
                "The only sensory information available to the agent is its own location.",
                "We use EMT and directly specify the target dynamics.",
                "For the Game of Tag, we can easily formulate three major trends: catching the quarry, staying mobile, and stalking the quarry.",
                "This results in the following three target dynamics: Tcatch(At+1 = si|Qt = sj, At = sa) ∝ 1 si = sj 0 otherwise Tmobile(At+1 = si|Qt = so, At = sj) ∝ 0 si = sj 1 otherwise Tstalk(At+1 = si|Qt = so, At = sj) ∝ 1 dist(si,so) Note that none of the above targets are directly achievable; for instance, if Qt = s9 and At = s11, there is no action that can move the agent to At+1 = s9 as required by the Tcatch target dynamics.",
                "We ran several experiments to evaluate EMT performance in the Tag Game.",
                "Three configurations of the domain shown in Figure 3 were used, each posing a different challenge to the agent due to partial observability.",
                "In each setting, a set of 1000 runs was performed with a time limit of 100 steps.",
                "In every run, the initial position of both the agent and its quarry was selected at random; this means that as far as the agent was concerned, the quarrys initial position was uniformly distributed over the entire domain cell space.",
                "We also used two variations of the environment observability function.",
                "In the first version, observability function mapped all joint positions of hunter and quarry into the position of the hunter as an observation.",
                "In the second, only those joint positions in which hunter and quarry occupied different locations were mapped into the hunters location.",
                "The second version in fact utilized and expressed the fact that once hunter and quarry occupy the same cell the game ends.",
                "The results of these experiments are shown in Table 2.",
                "Balancing [15] the catch, move, and stalk target dynamics described in the previous section by the weight vector [0.8, 0.1, 0.1], EMT produced stable performance in all three domains.",
                "Although direct comparisons are difficult to make, the EMT performance displayed notable efficiency vis-`a-vis the POMDP approach.",
                "In spite of a simple and inefficient Matlab implementation of the EMT algorithm, the decision time for any given step averaged significantly below 1 second in all experiments.",
                "For the irregular open arena domain, which proved to be the most difficult, 1000 experiment runs bounded by 100 steps each, a total of 42411 steps, were completed in slightly under 6 hours.",
                "That is, over 4 × 104 online steps took an order of magnitude less time than the offline computation of POMDP policy in [11].",
                "The significance of this differential is made even more prominent by the fact that, should the environment model parameters change, the online nature of EMT would allow it to maintain its performance time, while the POMDP policy would need to be recomputed, requiring yet again a large overhead of computation.",
                "We also tested the behavior cell frequency entropy, empirical measures from trial data.",
                "As Figure 4 and Figure 5 show, empir794 The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) A Q Q A 0 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 A Q Figure 3: These configurations of the Tag Game space were used: a) multiple dead-end, b) irregular open arena, c) circular corridor Table 2: Performance of the EMT-based solution in three Tag Game domains and two observability models: I) omniposition quarry, II) quarry is not at hunters position Model Domain Capture% E(Steps) Time/Step I Dead-ends 100 14.8 72(mSec) Arena 80.2 42.4 500(mSec) Circle 91.4 34.6 187(mSec) II Dead-ends 100 13.2 91(mSec) Arena 96.8 28.67 396(mSec) Circle 94.4 31.63 204(mSec) ical entropy grows with the length of interaction.",
                "For runs where the quarry was not captured immediately, the entropy reaches between 0.85 and 0.952 for different runs and scenarios.",
                "As the agent actively seeks the quarry, the entropy never reaches its maximum.",
                "One characteristic of the entropy graph for the open arena scenario particularly caught our attention in the case of the omniposition quarry observation model.",
                "Near the maximum limit of trial length (100 steps), entropy suddenly dropped.",
                "Further analysis of the data showed that under certain circumstances, a fluctuating behavior occurs in which the agent faces equally viable versions of quarry-following behavior.",
                "Since the EMT algorithm has greedy action selection, and the state space does not encode any form of commitment (not even speed or acceleration), the agent is locked within a small portion of cells.",
                "It is essentially attempting to simultaneously follow several courses of action, all of which are consistent with the target dynamics.",
                "This behavior did not occur in our second observation model, since it significantly reduced the set of eligible courses of action-essentially contributing to tie-breaking among them. 6.",
                "DISCUSSION The design of the EMT solution for the Tag Game exposes the core difference in approach to planning and control between EMT or DBC, on the one hand, and the more familiar POMDP approach, on the other.",
                "POMDP defines a reward structure to optimize, and influences system dynamics indirectly through that optimization.",
                "EMT discards any reward scheme, and instead measures and influences system dynamics directly. 2 Entropy was calculated using log base equal to the number of possible locations within the domain; this properly scales entropy expression into the range [0, 1] for all domains.",
                "Thus for the Tag Game, we did not search for a reward function that would encode and express our preference over the agents behavior, but rather directly set three (heuristic) behavior preferences as the basis for target dynamics to be maintained.",
                "Experimental data shows that these targets need not be directly achievable via the agents actions.",
                "However, the ratio between EMT performance and achievability of target dynamics remains to be explored.",
                "The tag game experiment data also revealed the different emphasis DBC and POMDPs place on the formulation of an environment state space.",
                "POMDPs rely entirely on the mechanism of reward accumulation maximization, i.e., formation of the action selection procedure to achieve necessary state sequencing.",
                "DBC, on the other hand, has two sources of sequencing specification: through the properties of an action selection procedure, and through direct specification within the target dynamics.",
                "The importance of the second source was underlined by the Tag Game experiment data, in which the greedy EMT algorithm, applied to a POMDP-type state space specification, failed, since target description over such a state space was incapable of encoding the necessary behavior tendencies, e.g., tie-breaking and commitment to directed motion.",
                "The structural differences between DBC (and EMT in particular), and POMDPs, prohibits direct performance comparison, and places them on complementary tracks, each within a suitable niche.",
                "For instance, POMDPs could be seen as a much more natural formulation of economic sequential decision-making problems, while EMT is a better fit for continual demand for stochastic change, as happens in many robotic or embodied-agent problems.",
                "The complementary properties of POMDPs and EMT can be further exploited.",
                "There is recent interest in using POMDPs in hybrid solutions [17], in which the POMDPs can be used together with other control approaches to provide results not easily achievable with either approach by itself.",
                "DBC can be an effective partner in such a hybrid solution.",
                "For instance, POMDPs have prohibitively large off-line time requirements for policy computation, but can be readily used in simpler settings to expose beneficial behavioral trends; this can serve as a form of target dynamics that are provided to EMT in a larger domain for on-line operation. 7.",
                "CONCLUSIONS AND FUTURE WORK In this paper, we have presented a novel perspective on the process of planning and control in stochastic environments, in the form of the Dynamics Based Control (DBC) framework.",
                "DBC formulates the task of planning as support of a specified target system dynamics, which describes the necessary properties of change within the environment.",
                "Optimality of DBC plans of action are measured The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) 795 0 20 40 60 80 100 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1 Steps Entropy Dead−ends 0 20 40 60 80 100 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1 Steps Entropy Arena 0 20 40 60 80 100 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1 Steps Entropy Circle Figure 4: Observation Model I: Omniposition quarry.",
                "Entropy development with length of Tag Game for the three experiment scenarios: a) multiple dead-end, b) irregular open arena, c) circular corridor. 0 10 20 30 40 50 60 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1 Steps Entropy Dead−ends 0 20 40 60 80 100 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1 Steps Entropy Arena 0 20 40 60 80 100 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1 Steps Entropy Circle Figure 5: Observation Model II: quarry not observed at hunters position.",
                "Entropy development with length of Tag Game for the three experiment scenarios: a) multiple dead-end, b) irregular open arena, c) circular corridor. with respect to the deviation of actual system dynamics from the target dynamics.",
                "We show that a recently developed technique of Extended Markov Tracking (EMT) [13] is an instantiation of DBC.",
                "In fact, EMT can be seen as a specific case of DBC parameterization, which employs a greedy action selection procedure.",
                "Since EMT exhibits the key features of the general DBC framework, as well as polynomial time complexity, we used the multitarget version of EMT [15] to demonstrate that the class of area sweeping problems naturally lends itself to dynamics-based descriptions, as instantiated by our experiments in the Tag Game domain.",
                "As enumerated in Section 4.2, EMT has a number of limitations, such as difficulty in dealing with negative dynamic preference.",
                "This prevents direct application of EMT to such problems as Rendezvous-Evasion Games (e.g., [6]).",
                "However, DBC in general has no such limitations, and readily enables the formulation of evasion games.",
                "In future work, we intend to proceed with the development of dynamics-based controllers for these problems. 8.",
                "ACKNOWLEDGMENT The work of the first two authors was partially supported by Israel Science Foundation grant #898/05, and the third author was partially supported by a grant from Israels Ministry of Science and Technology. 9.",
                "REFERENCES [1] R. C. Arkin.",
                "Behavior-Based Robotics.",
                "MIT Press, 1998. [2] J.",
                "A. Bilmes.",
                "A gentle tutorial of the EM algorithm and its application to parameter estimation for Gaussian mixture and Hidden Markov Models.",
                "Technical Report TR-97-021, Department of Electrical Engeineering and Computer Science, University of California at Berkeley, 1998. [3] T. M. Cover and J.",
                "A. Thomas.",
                "Elements of information theory.",
                "Wiley, 1991. [4] M. E. desJardins, E. H. Durfee, C. L. Ortiz, and M. J. Wolverton.",
                "A survey of research in distributed, continual planning.",
                "AI Magazine, 4:13-22, 1999. [5] V. R. Konda and J. N. Tsitsiklis.",
                "Actor-Critic algorithms.",
                "SIAM Journal on Control and Optimization, 42(4):1143-1166, 2003. [6] W. S. Lim.",
                "A rendezvous-evasion game on discrete locations with joint randomization.",
                "Advances in Applied Probability, 29(4):1004-1017, December 1997. [7] M. L. Littman, T. L. Dean, and L. P. Kaelbling.",
                "On the complexity of solving Markov decision problems.",
                "In Proceedings of the 11th Annual Conference on Uncertainty in Artificial Intelligence (UAI-95), pages 394-402, 1995. [8] O. Madani, S. Hanks, and A. Condon.",
                "On the undecidability of probabilistic planning and related stochastic optimization problems.",
                "Artificial Intelligence Journal, 147(1-2):5-34, July 2003. [9] R. M. Neal and G. E. Hinton.",
                "A view of the EM algorithm 796 The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) that justifies incremental, sparse, and other variants.",
                "In M. I. Jordan, editor, Learning in Graphical Models, pages 355-368.",
                "Kluwer Academic Publishers, 1998. [10] P. Paruchuri, M. Tambe, F. Ordonez, and S. Kraus.",
                "Security in multiagent systems by policy randomization.",
                "In Proceeding of AAMAS 2006, 2006. [11] J. Pineau, G. Gordon, and S. Thrun.",
                "Point-based value iteration: An anytime algorithm for pomdps.",
                "In International Joint Conference on Artificial Intelligence (IJCAI), pages 1025-1032, August 2003. [12] M. L. Puterman.",
                "Markov Decision Processes.",
                "Wiley Series in Probability and Mathematical Statistics: Applied Probability and Statistics Section.",
                "Wiley-Interscience Publication, New York, 1994. [13] Z. Rabinovich and J. S. Rosenschein.",
                "Extended Markov Tracking with an application to control.",
                "In The Workshop on Agent Tracking: Modeling Other Agents from Observations, at the Third International Joint Conference on Autonomous Agents and Multiagent Systems, pages 95-100, New-York, July 2004. [14] Z. Rabinovich and J. S. Rosenschein.",
                "Multiagent coordination by Extended Markov Tracking.",
                "In The Fourth International Joint Conference on Autonomous Agents and Multiagent Systems, pages 431-438, Utrecht, The Netherlands, July 2005. [15] Z. Rabinovich and J. S. Rosenschein.",
                "On the response of EMT-based control to interacting targets and models.",
                "In The Fifth International Joint Conference on Autonomous Agents and Multiagent Systems, pages 465-470, Hakodate, Japan, May 2006. [16] R. F. Stengel.",
                "Optimal Control and Estimation.",
                "Dover Publications, 1994. [17] M. Tambe, E. Bowring, H. Jung, G. Kaminka, R. Maheswaran, J. Marecki, J. Modi, R. Nair, J. Pearce, P. Paruchuri, D. Pynadath, P. Scerri, N. Schurr, and P. Varakantham.",
                "Conflicts in teamwork: Hybrids to the The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) 797"
            ],
            "original_annotated_samples": [],
            "translated_annotated_samples": [
                "Control basado en la dinámica La especificación del control basado en la dinámica (DBC) se puede dividir en tres niveles de interacción: nivel de diseño del entorno, \"nivel de usuario\" y nivel de agente.• El nivel de diseño del medio ambiente se refiere a la especificación formal y el modelado del entorno.",
                "Por ejemplo, este nivel especificaría las leyes de la física dentro del sistema y establecería sus parámetros, como la constante de gravitación.• El \"nivel de usuario\" a su vez se basa en el modelo de entorno producido por el diseño del entorno para especificar la dinámica del sistema de destino que desea observar.",
                "El \"nivel de usuario\" también especifica la estimación o procedimiento de aprendizaje para la dinámica del sistema y la medida de la desviación.",
                "En el escenario de la Guardia del Museo anterior, estos corresponderían a un horario estocástico de barrido, y una medida de sorpresa relativa entre el barrido especificado y real.• El nivel de agente a su vez combina el modelo de entorno desde el nivel de diseño del entorno, el procedimiento de estimación de la dinámica, la medida de desviación y la especificación de dinámica de destino desde el \"nivel de usuario\", para producir una secuencia de acciones que crean dinámicas del sistema lo más cerca posible de laEspecificación dirigida.",
                "La otra alternativa sería confiar en el procedimiento de estimación proporcionado por el \"nivel de usuario\", para utilizar el modelo de nivel de diseño del entorno del entorno para elegir acciones, a fin de manipular el estimador de la dinámica para que crea que se ha logrado una cierta dinámica.",
                "Por ejemplo, el nivel de agente podría proporcionar datos sobre la viabilidad de la dinámica de destino, lo que permite que el \"nivel de usuario\" modifique el requisito, tal vez centrándose en las características alcanzables del comportamiento del sistema.",
                "Los datos también estarían disponibles sobre la respuesta del sistema a diferentes acciones realizadas;Combinado con un estimador de dinámica definido por el \"nivel de usuario\", esto puede proporcionar una herramienta importante para la calibración del modelo de entorno a nivel de diseño del entorno.",
                "Por ejemplo, el \"nivel de usuario\" puede funcionar como un dispositivo exploratorio para un algoritmo de aprendizaje, inferiendo un objetivo de dinámica ideal del modelo de entorno en cuestión que expondría y verificaría la mayoría de las características críticas del comportamiento del sistema.",
                "Es decir, ω (o | s, a, s) es la probabilidad de que uno observe o dado que el entorno se ha movido de los estados a los estados bajo acción a.• \"Nivel de usuario\", en el caso del entorno de Markovian, funciona en el conjunto de dinámicas del sistema descrita por una familia de probabilidades condicionales F = {τ: S × A → π (s)}.",
                "Hay muchas variaciones posibles disponibles en el \"nivel de usuario\" para definir la divergencia entre la dinámica del sistema;Varios de ellos son: - distancia de traza o distancia de L1 entre dos distribuciones P y Q definidas por d (p (·), q (·)) = 1 2 x | p (x) - q (x) |- Medida de fidelidad de la distancia f (p (·), q (·)) = x p (x) q (x) - kullback -leíbler divergencia dkl (p (·) q (·)) = x p (x) log p (x) Q (x) Observe que los dos últimos no son realmente métricas sobre el espacio de posibles distribuciones, sino que tienen interpretaciones significativas e importantes.",
                "Por ejemplo, la divergencia de Kullbackleibler es una herramienta importante de la teoría de la información [3] que le permite a uno medir el precio de codificar una fuente de información regida por Q, al tiempo que supone que se rige por p.El \"nivel de usuario\" también define el umbral de la probabilidad de desviación dinámica θ.• El nivel de agente se enfrenta con un problema de seleccionar una función de señal de control A ∗ para satisfacer un problema de minimización de la siguiente manera: a ∗ = arg min a pr (d (τa, q)> θ) donde d (τa, q) esUna variable aleatoria que describe la desviación de la estimación de la dinámica τa, creada por L bajo la señal de control A, a partir de la dinámica ideal q.",
                "El control basado en EMT se basa en la definición del entorno de Markovian, como en el caso de POMDPS, pero sus niveles de usuario y agente son del tipo de optimización DBC Markovian.• El \"nivel de usuario\" del control basado en EMT define una dinámica del sistema de destino de casos limitados independientemente de la acción: QEMT: S → π (S)."
            ],
            "translated_text": "",
            "candidates": [
                "nivel de usuario",
                "nivel de usuario",
                "nivel de usuario",
                "nivel de usuario",
                "nivel de usuario",
                "nivel de usuario",
                "nivel de usuario",
                "nivel de usuario",
                "nivel de usuario",
                "nivel de usuario",
                "nivel de usuario",
                "nivel de usuario",
                "nivel de usuario",
                "nivel de usuario",
                "nivel de usuario",
                "nivel de usuario",
                "nivel de usuario",
                "Nivel de usuario",
                "nivel de usuario",
                "nivel de usuario",
                "nivel de usuario",
                "nivel de usuario",
                "nivel de usuario",
                "nivel de usuario"
            ],
            "error": []
        },
        "agent level": {
            "translated_key": "nivel de agente",
            "is_in_text": true,
            "original_annotated_sentences": [
                "Dynamics Based Control with an Application to Area-Sweeping Problems Zinovi Rabinovich Engineering and Computer Science Hebrew University of Jerusalem Jerusalem, Israel nomad@cs.huji.ac.il Jeffrey S. Rosenschein Engineering and Computer Science Hebrew University of Jerusalem Jerusalem, Israel jeff@cs.huji.ac.il Gal A. Kaminka The MAVERICK Group Department of Computer Science Bar Ilan University, Israel galk@cs.biu.ac.il ABSTRACT In this paper we introduce Dynamics Based Control (DBC), an approach to planning and control of an agent in stochastic environments.",
                "Unlike existing approaches, which seek to optimize expected rewards (e.g., in Partially Observable Markov Decision Problems (POMDPs)), DBC optimizes system behavior towards specified system dynamics.",
                "We show that a recently developed planning and control approach, Extended Markov Tracking (EMT) is an instantiation of DBC.",
                "EMT employs greedy action selection to provide an efficient control algorithm in Markovian environments.",
                "We exploit this efficiency in a set of experiments that applied multitarget EMT to a class of area-sweeping problems (searching for moving targets).",
                "We show that such problems can be naturally defined and efficiently solved using the DBC framework, and its EMT instantiation.",
                "Categories and Subject Descriptors I.2.8 [Problem Solving, Control Methods, and Search]: Control Theory; I.2.9 [Robotics]; I.2.11 [Distributed Artificial Intelligence]: Intelligent Agents General Terms Algorithms, Theory 1.",
                "INTRODUCTION Planning and control constitutes a central research area in multiagent systems and artificial intelligence.",
                "In recent years, Partially Observable Markov Decision Processes (POMDPs) [12] have become a popular formal basis for planning in stochastic environments.",
                "In this framework, the planning and control problem is often addressed by imposing a reward function, and computing a policy (of choosing actions) that is optimal, in the sense that it will result in the highest expected utility.",
                "While theoretically attractive, the complexity of optimally solving a POMDP is prohibitive [8, 7].",
                "We take an alternative view of planning in stochastic environments.",
                "We do not use a (state-based) reward function, but instead optimize over a different criterion, a transition-based specification of the desired system dynamics.",
                "The idea here is to view planexecution as a process that compels a (stochastic) system to change, and a plan as a dynamic process that shapes that change according to desired criteria.",
                "We call this general planning framework Dynamics Based Control (DBC).",
                "In DBC, the goal of a planning (or control) process becomes to ensure that the system will change in accordance with specific (potentially stochastic) target dynamics.",
                "As actual system behavior may deviate from that which is specified by target dynamics (due to the stochastic nature of the system), planning in such environments needs to be continual [4], in a manner similar to classical closed-loop controllers [16].",
                "Here, optimality is measured in terms of probability of deviation magnitudes.",
                "In this paper, we present the structure of Dynamics Based Control.",
                "We show that the recently developed Extended Markov Tracking (EMT) approach [13, 14, 15] is subsumed by DBC, with EMT employing greedy action selection, which is a specific parameterization among the options possible within DBC.",
                "EMT is an efficient instantiation of DBC.",
                "To evaluate DBC, we carried out a set of experiments applying multi-target EMT to the Tag Game [11]; this is a variant on the area sweeping problem, where an agent is trying to tag a moving target (quarry) whose position is not known with certainty.",
                "Experimental data demonstrates that even with a simple model of the environment and a simple design of target dynamics, high success rates can be produced both in catching the quarry, and in surprising the quarry (as expressed by the observed entropy of the controlled agents position).",
                "The paper is organized as follows.",
                "In Section 2 we motivate DBC using area-sweeping problems, and discuss related work.",
                "Section 3 introduces the Dynamics Based Control (DBC) structure, and its specialization to Markovian environments.",
                "This is followed by a review of the Extended Markov Tracking (EMT) approach as a DBC-structured control regimen in Section 4.",
                "That section also discusses the limitations of EMT-based control relative to the general DBC framework.",
                "Experimental settings and results are then presented in Section 5.",
                "Section 6 provides a short discussion of the overall approach, and Section 7 gives some concluding remarks and directions for future work. 790 978-81-904262-7-5 (RPS) c 2007 IFAAMAS 2.",
                "MOTIVATION AND RELATED WORK Many real-life scenarios naturally have a stochastic target dynamics specification, especially those domains where there exists no ultimate goal, but rather system behavior (with specific properties) that has to be continually supported.",
                "For example, security guards perform persistent sweeps of an area to detect any sign of intrusion.",
                "Cunning thieves will attempt to track these sweeps, and time their operation to key points of the guards motion.",
                "It is thus advisable to make the guards motion dynamics appear irregular and random.",
                "Recent work by Paruchuri et al. [10] has addressed such randomization in the context of single-agent and distributed POMDPs.",
                "The goal in that work was to generate policies that provide a measure of action-selection randomization, while maintaining rewards within some acceptable levels.",
                "Our focus differs from this work in that DBC does not optimize expected rewards-indeed we do not consider rewards at all-but instead maintains desired dynamics (including, but not limited to, randomization).",
                "The Game of Tag is another example of the applicability of the approach.",
                "It was introduced in the work by Pineau et al. [11].",
                "There are two agents that can move about an area, which is divided into a grid.",
                "The grid may have blocked cells (holes) into which no agent can move.",
                "One agent (the hunter) seeks to move into a cell occupied by the other (the quarry), such that they are co-located (this is a successful tag).",
                "The quarry seeks to avoid the hunter agent, and is always aware of the hunters position, but does not know how the hunter will behave, which opens up the possibility for a hunter to surprise the prey.",
                "The hunter knows the quarrys probabilistic law of motion, but does not know its current location.",
                "Tag is an instance of a family of area-sweeping (pursuit-evasion) problems.",
                "In [11], the hunter modeled the problem using a POMDP.",
                "A reward function was defined, to reflect the desire to tag the quarry, and an action policy was computed to optimize the reward collected over time.",
                "Due to the intractable complexity of determining the optimal policy, the action policy computed in that paper was essentially an approximation.",
                "In this paper, instead of formulating a reward function, we use EMT to solve the problem, by directly specifying the target dynamics.",
                "In fact, any search problem with randomized motion, the socalled class of area sweeping problems, can be described through specification of such target system dynamics.",
                "Dynamics Based Control provides a natural approach to solving these problems. 3.",
                "DYNAMICS BASED CONTROL The specification of Dynamics Based Control (DBC) can be broken into three interacting levels: Environment Design Level, User Level, and <br>agent level</br>. • Environment Design Level is concerned with the formal specification and modeling of the environment.",
                "For example, this level would specify the laws of physics within the system, and set its parameters, such as the gravitation constant. • User Level in turn relies on the environment model produced by Environment Design to specify the target system dynamics it wishes to observe.",
                "The User Level also specifies the estimation or learning procedure for system dynamics, and the measure of deviation.",
                "In the museum guard scenario above, these would correspond to a stochastic sweep schedule, and a measure of relative surprise between the specified and actual sweeping. • <br>agent level</br> in turn combines the environment model from the Environment Design level, the dynamics estimation procedure, the deviation measure and the target dynamics specification from User Level, to produce a sequence of actions that create system dynamics as close as possible to the targeted specification.",
                "As we are interested in the continual development of a stochastic system, such as happens in classical control theory [16] and continual planning [4], as well as in our example of museum sweeps, the question becomes how the <br>agent level</br> is to treat the deviation measurements over time.",
                "To this end, we use a probability threshold-that is, we would like the <br>agent level</br> to maximize the probability that the deviation measure will remain below a certain threshold.",
                "Specific action selection then depends on system formalization.",
                "One possibility would be to create a mixture of available system trends, much like that which happens in Behavior-Based Robotic architectures [1].",
                "The other alternative would be to rely on the estimation procedure provided by the User Level-to utilize the Environment Design Level model of the environment to choose actions, so as to manipulate the dynamics estimator into believing that a certain dynamics has been achieved.",
                "Notice that this manipulation is not direct, but via the environment.",
                "Thus, for strong enough estimator algorithms, successful manipulation would mean a successful simulation of the specified target dynamics (i.e., beyond discerning via the available sensory input).",
                "DBC levels can also have a back-flow of information (see Figure 1).",
                "For instance, the <br>agent level</br> could provide data about target dynamics feasibility, allowing the User Level to modify the requirement, perhaps focusing on attainable features of system behavior.",
                "Data would also be available about the system response to different actions performed; combined with a dynamics estimator defined by the User Level, this can provide an important tool for the environment model calibration at the Environment Design Level.",
                "UserEnv.",
                "Design Agent Model Ideal Dynamics Estimator Estimator Dynamics Feasibility System Response Data Figure 1: Data flow of the DBC framework Extending upon the idea of Actor-Critic algorithms [5], DBC data flow can provide a good basis for the design of a learning algorithm.",
                "For example, the User Level can operate as an exploratory device for a learning algorithm, inferring an ideal dynamics target from the environment model at hand that would expose and verify most critical features of system behavior.",
                "In this case, feasibility and system response data from the <br>agent level</br> would provide key information for an environment model update.",
                "In fact, the combination of feasibility and response data can provide a basis for the application of strong learning algorithms such as EM [2, 9]. 3.1 DBC for Markovian Environments For a Partially Observable Markovian Environment, DBC can be specified in a more rigorous manner.",
                "Notice how DBC discards rewards, and replaces it by another optimality criterion (structural differences are summarized in Table 1): • Environment Design level is to specify a tuple < S, A, T, O, Ω, s0 >, where: - S is the set of all possible environment states; - s0 is the initial state of the environment (which can also be viewed as a probability distribution over S); The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) 791 - A is the set of all possible actions applicable in the environment; - T is the environments probabilistic transition function: T : S ×A → Π(S).",
                "That is, T(s |a, s) is the probability that the environment will move from state s to state s under action a; - O is the set of all possible observations.",
                "This is what the sensor input would look like for an outside observer; - Ω is the observation probability function: Ω : S × A × S → Π(O).",
                "That is, Ω(o|s , a, s) is the probability that one will observe o given that the environment has moved from state s to state s under action a. • User Level, in the case of Markovian environment, operates on the set of system dynamics described by a family of conditional probabilities F = {τ : S × A → Π(S)}.",
                "Thus specification of target dynamics can be expressed by q ∈ F, and the learning or tracking algorithm can be represented as a function L : O×(A×O)∗ → F; that is, it maps sequences of observations and actions performed so far into an estimate τ ∈ F of system dynamics.",
                "There are many possible variations available at the User Level to define divergence between system dynamics; several of them are: - Trace distance or L1 distance between two distributions p and q defined by D(p(·), q(·)) = 1 2 x |p(x) − q(x)| - Fidelity measure of distance F(p(·), q(·)) = x p(x)q(x) - Kullback-Leibler divergence DKL(p(·) q(·)) = x p(x) log p(x) q(x) Notice that the latter two are not actually metrics over the space of possible distributions, but nevertheless have meaningful and important interpretations.",
                "For instance, KullbackLeibler divergence is an important tool of information theory [3] that allows one to measure the price of encoding an information source governed by q, while assuming that it is governed by p. The User Level also defines the threshold of dynamics deviation probability θ. • <br>agent level</br> is then faced with a problem of selecting a control signal function a∗ to satisfy a minimization problem as follows: a∗ = arg min a Pr(d(τa, q) > θ) where d(τa, q) is a random variable describing deviation of the dynamics estimate τa, created by L under control signal a, from the ideal dynamics q.",
                "Implicit in this minimization problem is that L is manipulated via the environment, based on the environment model produced by the Environment Design Level. 3.2 DBC View of the State Space It is important to note the complementary view that DBC and POMDPs take on the state space of the environment.",
                "POMDPs regard state as a stationary snap-shot of the environment; whatever attributes of state sequencing one seeks are reached through properties of the control process, in this case reward accumulation.",
                "This can be viewed as if the sequencing of states and the attributes of that sequencing are only introduced by and for the controlling mechanism-the POMDP policy.",
                "DBC concentrates on the underlying principle of state sequencing, the system dynamics.",
                "DBCs target dynamics specification can use the environments state space as a means to describe, discern, and preserve changes that occur within the system.",
                "As a result, DBC has a greater ability to express state sequencing properties, which are grounded in the environment model and its state space definition.",
                "For example, consider the task of moving through rough terrain towards a goal and reaching it as fast as possible.",
                "POMDPs would encode terrain as state space points, while speed would be ensured by negative reward for every step taken without reaching the goalaccumulating higher reward can be reached only by faster motion.",
                "Alternatively, the state space could directly include the notion of speed.",
                "For POMDPs, this would mean that the same concept is encoded twice, in some sense: directly in the state space, and indirectly within reward accumulation.",
                "Now, even if the reward function would encode more, and finer, details of the properties of motion, the POMDP solution will have to search in a much larger space of policies, while still being guided by the implicit concept of the reward accumulation procedure.",
                "On the other hand, the tactical target expression of variations and correlations between position and speed of motion is now grounded in the state space representation.",
                "In this situation, any further constraints, e.g., smoothness of motion, speed limits in different locations, or speed reductions during sharp turns, are explicitly and uniformly expressed by the tactical target, and can result in faster and more effective action selection by a DBC algorithm. 4.",
                "EMT-BASED CONTROL AS A DBC Recently, a control algorithm was introduced called EMT-based Control [13], which instantiates the DBC framework.",
                "Although it provides an approximate greedy solution in the DBC sense, initial experiments using EMT-based control have been encouraging [14, 15].",
                "EMT-based control is based on the Markovian environment definition, as in the case with POMDPs, but its User and Agent Levels are of the Markovian DBC type of optimality. • User Level of EMT-based control defines a limited-case target system dynamics independent of action: qEMT : S → Π(S).",
                "It then utilizes the Kullback-Leibler divergence measure to compose a momentary system dynamics estimator-the Extended Markov Tracking (EMT) algorithm.",
                "The algorithm keeps a system dynamics estimate τt EMT that is capable of explaining recent change in an auxiliary Bayesian system state estimator from pt−1 to pt, and updates it conservatively using Kullback-Leibler divergence.",
                "Since τt EMT and pt−1,t are respectively the conditional and marginal probabilities over the systems state space, explanation simply means that pt(s ) = s τt EMT (s |s)pt−1(s), and the dynamics estimate update is performed by solving a 792 The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) Table 1: Structure of POMDP vs. Dynamics-Based Control in Markovian Environment Level Approach MDP Markovian DBC Environment < S, A, T, O, Ω >,where S - set of states A - set of actions Design T : S × A → Π(S) - transition O - observation set Ω : S × A × S → Π(O) User r : S × A × S → R q : S × A → Π(S) F(π∗ ) → r L(o1, ..., ot) → τ r - reward function q - ideal dynamics F - reward remodeling L - dynamics estimator θ - threshold Agent π∗ = arg max π E[ γt rt] π∗ = arg min π Prob(d(τ q) > θ) minimization problem: τt EMT = H[pt, pt−1, τt−1 EMT ] = arg min τ DKL(τ × pt−1 τt−1 EMT × pt−1) s.t. pt(s ) = s (τ × pt−1)(s , s) pt−1(s) = s (τ × pt−1)(s , s) • <br>agent level</br> in EMT-based control is suboptimal with respect to DBC (though it remains within the DBC framework), performing greedy action selection based on prediction of EMTs reaction.",
                "The prediction is based on the environment model provided by the Environment Design level, so that if we denote by Ta the environments transition function limited to action a, and pt−1 is the auxiliary Bayesian system state estimator, then the EMT-based control choice is described by a∗ = arg min a∈A DKL(H[Ta × pt, pt, τt EMT ] qEMT × pt−1) Note that this follows the Markovian DBC framework precisely: the rewarding optimality of POMDPs is discarded, and in its place a dynamics estimator (EMT in this case) is manipulated via action effects on the environment to produce an estimate close to the specified target system dynamics.",
                "Yet as we mentioned, naive EMTbased control is suboptimal in the DBC sense, and has several additional limitations that do not exist in the general DBC framework (discussed in Section 4.2). 4.1 Multi-Target EMT At times, there may exist several behavioral preferences.",
                "For example, in the case of museum guards, some art items are more heavily guarded, requiring that the guards stay more often in their close vicinity.",
                "On the other hand, no corner of the museum is to be left unchecked, which demands constant motion.",
                "Successful museum security would demand that the guards adhere to, and balance, both of these behaviors.",
                "For EMT-based control, this would mean facing several tactical targets {qk}K k=1, and the question becomes how to merge and balance them.",
                "A balancing mechanism can be applied to resolve this issue.",
                "Note that EMT-based control, while selecting an action, creates a preference vector over the set of actions based on their predicted performance with respect to a given target.",
                "If these preference vectors are normalized, they can be combined into a single unified preference.",
                "This requires replacement of standard EMT-based action selection by the algorithm below [15]: • Given: - a set of target dynamics {qk}K k=1, - vector of weights w(k) • Select action as follows - For each action a ∈ A predict the future state distribution ¯pa t+1 = Ta ∗ pt; - For each action, compute Da = H(¯pa t+1, pt, PDt) - For each a ∈ A and qk tactical target, denote V (a, k) = DKL (Da qk) pt .",
                "Let Vk(a) = 1 Zk V (a, k), where Zk = a∈A V (a, k) is a normalization factor. - Select a∗ = arg min a k k=1 w(k)Vk(a) The weights vector w = (w1, ..., wK ) allows the additional tuning of importance among target dynamics without the need to redesign the targets themselves.",
                "This balancing method is also seamlessly integrated into the EMT-based control flow of operation. 4.2 EMT-based Control Limitations EMT-based control is a sub-optimal (in the DBC sense) representative of the DBC structure.",
                "It limits the User by forcing EMT to be its dynamic tracking algorithm, and replaces Agent optimization by greedy action selection.",
                "This kind of combination, however, is common for on-line algorithms.",
                "Although further development of EMT-based controllers is necessary, evidence so far suggests that even the simplest form of the algorithm possesses a great deal of power, and displays trends that are optimal in the DBC sense of the word.",
                "There are two further, EMT-specific, limitations to EMT-based control that are evident at this point.",
                "Both already have partial solutions and are subjects of ongoing research.",
                "The first limitation is the problem of negative preference.",
                "In the POMDP framework for example, this is captured simply, through The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) 793 the appearance of values with different signs within the reward structure.",
                "For EMT-based control, however, negative preference means that one would like to avoid a certain distribution over system development sequences; EMT-based control, however, concentrates on getting as close as possible to a distribution.",
                "Avoidance is thus unnatural in native EMT-based control.",
                "The second limitation comes from the fact that standard environment modeling can create pure sensory actions-actions that do not change the state of the world, and differ only in the way observations are received and the quality of observations received.",
                "Since the world state does not change, EMT-based control would not be able to differentiate between different sensory actions.",
                "Notice that both of these limitations of EMT-based control are absent from the general DBC framework, since it may have a tracking algorithm capable of considering pure sensory actions and, unlike Kullback-Leibler divergence, a distribution deviation measure that is capable of dealing with negative preference. 5.",
                "EMT PLAYING TAG The Game of Tag was first introduced in [11].",
                "It is a single agent problem of capturing a quarry, and belongs to the class of area sweeping problems.",
                "An example domain is shown in Figure 2. 0 51 2 3 4 6 7 8 10 12 13 161514 17 18 19 2221 23 9 11Q A 20 Figure 2: Tag domain; an agent (A) attempts to seek and capture a quarry (Q) The Game of Tag extremely limits the agents perception, so that the agent is able to detect the quarry only if they are co-located in the same cell of the grid world.",
                "In the classical version of the game, co-location leads to a special observation, and the Tag action can be performed.",
                "We slightly modify this setting: the moment that both agents occupy the same cell, the game ends.",
                "As a result, both the agent and its quarry have the same motion capability, which allows them to move in four directions, North, South, East, and West.",
                "These form a formal space of actions within a Markovian environment.",
                "The state space of the formal Markovian environment is described by the cross-product of the agent and quarrys positions.",
                "For Figure 2, it would be S = {s0, ..., s23} × {s0, ..., s23}.",
                "The effects of an action taken by the agent are deterministic, but the environment in general has a stochastic response due to the motion of the quarry.",
                "With probability q0 1 it stays put, and with probability 1 − q0 it moves to an adjacent cell further away from the 1 In our experiments this was taken to be q0 = 0.2. agent.",
                "So for the instance shown in Figure 2 and q0 = 0.1: P(Q = s9|Q = s9, A = s11) = 0.1 P(Q = s2|Q = s9, A = s11) = 0.3 P(Q = s8|Q = s9, A = s11) = 0.3 P(Q = s14|Q = s9, A = s11) = 0.3 Although the evasive behavior of the quarry is known to the agent, the quarrys position is not.",
                "The only sensory information available to the agent is its own location.",
                "We use EMT and directly specify the target dynamics.",
                "For the Game of Tag, we can easily formulate three major trends: catching the quarry, staying mobile, and stalking the quarry.",
                "This results in the following three target dynamics: Tcatch(At+1 = si|Qt = sj, At = sa) ∝ 1 si = sj 0 otherwise Tmobile(At+1 = si|Qt = so, At = sj) ∝ 0 si = sj 1 otherwise Tstalk(At+1 = si|Qt = so, At = sj) ∝ 1 dist(si,so) Note that none of the above targets are directly achievable; for instance, if Qt = s9 and At = s11, there is no action that can move the agent to At+1 = s9 as required by the Tcatch target dynamics.",
                "We ran several experiments to evaluate EMT performance in the Tag Game.",
                "Three configurations of the domain shown in Figure 3 were used, each posing a different challenge to the agent due to partial observability.",
                "In each setting, a set of 1000 runs was performed with a time limit of 100 steps.",
                "In every run, the initial position of both the agent and its quarry was selected at random; this means that as far as the agent was concerned, the quarrys initial position was uniformly distributed over the entire domain cell space.",
                "We also used two variations of the environment observability function.",
                "In the first version, observability function mapped all joint positions of hunter and quarry into the position of the hunter as an observation.",
                "In the second, only those joint positions in which hunter and quarry occupied different locations were mapped into the hunters location.",
                "The second version in fact utilized and expressed the fact that once hunter and quarry occupy the same cell the game ends.",
                "The results of these experiments are shown in Table 2.",
                "Balancing [15] the catch, move, and stalk target dynamics described in the previous section by the weight vector [0.8, 0.1, 0.1], EMT produced stable performance in all three domains.",
                "Although direct comparisons are difficult to make, the EMT performance displayed notable efficiency vis-`a-vis the POMDP approach.",
                "In spite of a simple and inefficient Matlab implementation of the EMT algorithm, the decision time for any given step averaged significantly below 1 second in all experiments.",
                "For the irregular open arena domain, which proved to be the most difficult, 1000 experiment runs bounded by 100 steps each, a total of 42411 steps, were completed in slightly under 6 hours.",
                "That is, over 4 × 104 online steps took an order of magnitude less time than the offline computation of POMDP policy in [11].",
                "The significance of this differential is made even more prominent by the fact that, should the environment model parameters change, the online nature of EMT would allow it to maintain its performance time, while the POMDP policy would need to be recomputed, requiring yet again a large overhead of computation.",
                "We also tested the behavior cell frequency entropy, empirical measures from trial data.",
                "As Figure 4 and Figure 5 show, empir794 The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) A Q Q A 0 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 A Q Figure 3: These configurations of the Tag Game space were used: a) multiple dead-end, b) irregular open arena, c) circular corridor Table 2: Performance of the EMT-based solution in three Tag Game domains and two observability models: I) omniposition quarry, II) quarry is not at hunters position Model Domain Capture% E(Steps) Time/Step I Dead-ends 100 14.8 72(mSec) Arena 80.2 42.4 500(mSec) Circle 91.4 34.6 187(mSec) II Dead-ends 100 13.2 91(mSec) Arena 96.8 28.67 396(mSec) Circle 94.4 31.63 204(mSec) ical entropy grows with the length of interaction.",
                "For runs where the quarry was not captured immediately, the entropy reaches between 0.85 and 0.952 for different runs and scenarios.",
                "As the agent actively seeks the quarry, the entropy never reaches its maximum.",
                "One characteristic of the entropy graph for the open arena scenario particularly caught our attention in the case of the omniposition quarry observation model.",
                "Near the maximum limit of trial length (100 steps), entropy suddenly dropped.",
                "Further analysis of the data showed that under certain circumstances, a fluctuating behavior occurs in which the agent faces equally viable versions of quarry-following behavior.",
                "Since the EMT algorithm has greedy action selection, and the state space does not encode any form of commitment (not even speed or acceleration), the agent is locked within a small portion of cells.",
                "It is essentially attempting to simultaneously follow several courses of action, all of which are consistent with the target dynamics.",
                "This behavior did not occur in our second observation model, since it significantly reduced the set of eligible courses of action-essentially contributing to tie-breaking among them. 6.",
                "DISCUSSION The design of the EMT solution for the Tag Game exposes the core difference in approach to planning and control between EMT or DBC, on the one hand, and the more familiar POMDP approach, on the other.",
                "POMDP defines a reward structure to optimize, and influences system dynamics indirectly through that optimization.",
                "EMT discards any reward scheme, and instead measures and influences system dynamics directly. 2 Entropy was calculated using log base equal to the number of possible locations within the domain; this properly scales entropy expression into the range [0, 1] for all domains.",
                "Thus for the Tag Game, we did not search for a reward function that would encode and express our preference over the agents behavior, but rather directly set three (heuristic) behavior preferences as the basis for target dynamics to be maintained.",
                "Experimental data shows that these targets need not be directly achievable via the agents actions.",
                "However, the ratio between EMT performance and achievability of target dynamics remains to be explored.",
                "The tag game experiment data also revealed the different emphasis DBC and POMDPs place on the formulation of an environment state space.",
                "POMDPs rely entirely on the mechanism of reward accumulation maximization, i.e., formation of the action selection procedure to achieve necessary state sequencing.",
                "DBC, on the other hand, has two sources of sequencing specification: through the properties of an action selection procedure, and through direct specification within the target dynamics.",
                "The importance of the second source was underlined by the Tag Game experiment data, in which the greedy EMT algorithm, applied to a POMDP-type state space specification, failed, since target description over such a state space was incapable of encoding the necessary behavior tendencies, e.g., tie-breaking and commitment to directed motion.",
                "The structural differences between DBC (and EMT in particular), and POMDPs, prohibits direct performance comparison, and places them on complementary tracks, each within a suitable niche.",
                "For instance, POMDPs could be seen as a much more natural formulation of economic sequential decision-making problems, while EMT is a better fit for continual demand for stochastic change, as happens in many robotic or embodied-agent problems.",
                "The complementary properties of POMDPs and EMT can be further exploited.",
                "There is recent interest in using POMDPs in hybrid solutions [17], in which the POMDPs can be used together with other control approaches to provide results not easily achievable with either approach by itself.",
                "DBC can be an effective partner in such a hybrid solution.",
                "For instance, POMDPs have prohibitively large off-line time requirements for policy computation, but can be readily used in simpler settings to expose beneficial behavioral trends; this can serve as a form of target dynamics that are provided to EMT in a larger domain for on-line operation. 7.",
                "CONCLUSIONS AND FUTURE WORK In this paper, we have presented a novel perspective on the process of planning and control in stochastic environments, in the form of the Dynamics Based Control (DBC) framework.",
                "DBC formulates the task of planning as support of a specified target system dynamics, which describes the necessary properties of change within the environment.",
                "Optimality of DBC plans of action are measured The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) 795 0 20 40 60 80 100 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1 Steps Entropy Dead−ends 0 20 40 60 80 100 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1 Steps Entropy Arena 0 20 40 60 80 100 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1 Steps Entropy Circle Figure 4: Observation Model I: Omniposition quarry.",
                "Entropy development with length of Tag Game for the three experiment scenarios: a) multiple dead-end, b) irregular open arena, c) circular corridor. 0 10 20 30 40 50 60 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1 Steps Entropy Dead−ends 0 20 40 60 80 100 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1 Steps Entropy Arena 0 20 40 60 80 100 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1 Steps Entropy Circle Figure 5: Observation Model II: quarry not observed at hunters position.",
                "Entropy development with length of Tag Game for the three experiment scenarios: a) multiple dead-end, b) irregular open arena, c) circular corridor. with respect to the deviation of actual system dynamics from the target dynamics.",
                "We show that a recently developed technique of Extended Markov Tracking (EMT) [13] is an instantiation of DBC.",
                "In fact, EMT can be seen as a specific case of DBC parameterization, which employs a greedy action selection procedure.",
                "Since EMT exhibits the key features of the general DBC framework, as well as polynomial time complexity, we used the multitarget version of EMT [15] to demonstrate that the class of area sweeping problems naturally lends itself to dynamics-based descriptions, as instantiated by our experiments in the Tag Game domain.",
                "As enumerated in Section 4.2, EMT has a number of limitations, such as difficulty in dealing with negative dynamic preference.",
                "This prevents direct application of EMT to such problems as Rendezvous-Evasion Games (e.g., [6]).",
                "However, DBC in general has no such limitations, and readily enables the formulation of evasion games.",
                "In future work, we intend to proceed with the development of dynamics-based controllers for these problems. 8.",
                "ACKNOWLEDGMENT The work of the first two authors was partially supported by Israel Science Foundation grant #898/05, and the third author was partially supported by a grant from Israels Ministry of Science and Technology. 9.",
                "REFERENCES [1] R. C. Arkin.",
                "Behavior-Based Robotics.",
                "MIT Press, 1998. [2] J.",
                "A. Bilmes.",
                "A gentle tutorial of the EM algorithm and its application to parameter estimation for Gaussian mixture and Hidden Markov Models.",
                "Technical Report TR-97-021, Department of Electrical Engeineering and Computer Science, University of California at Berkeley, 1998. [3] T. M. Cover and J.",
                "A. Thomas.",
                "Elements of information theory.",
                "Wiley, 1991. [4] M. E. desJardins, E. H. Durfee, C. L. Ortiz, and M. J. Wolverton.",
                "A survey of research in distributed, continual planning.",
                "AI Magazine, 4:13-22, 1999. [5] V. R. Konda and J. N. Tsitsiklis.",
                "Actor-Critic algorithms.",
                "SIAM Journal on Control and Optimization, 42(4):1143-1166, 2003. [6] W. S. Lim.",
                "A rendezvous-evasion game on discrete locations with joint randomization.",
                "Advances in Applied Probability, 29(4):1004-1017, December 1997. [7] M. L. Littman, T. L. Dean, and L. P. Kaelbling.",
                "On the complexity of solving Markov decision problems.",
                "In Proceedings of the 11th Annual Conference on Uncertainty in Artificial Intelligence (UAI-95), pages 394-402, 1995. [8] O. Madani, S. Hanks, and A. Condon.",
                "On the undecidability of probabilistic planning and related stochastic optimization problems.",
                "Artificial Intelligence Journal, 147(1-2):5-34, July 2003. [9] R. M. Neal and G. E. Hinton.",
                "A view of the EM algorithm 796 The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) that justifies incremental, sparse, and other variants.",
                "In M. I. Jordan, editor, Learning in Graphical Models, pages 355-368.",
                "Kluwer Academic Publishers, 1998. [10] P. Paruchuri, M. Tambe, F. Ordonez, and S. Kraus.",
                "Security in multiagent systems by policy randomization.",
                "In Proceeding of AAMAS 2006, 2006. [11] J. Pineau, G. Gordon, and S. Thrun.",
                "Point-based value iteration: An anytime algorithm for pomdps.",
                "In International Joint Conference on Artificial Intelligence (IJCAI), pages 1025-1032, August 2003. [12] M. L. Puterman.",
                "Markov Decision Processes.",
                "Wiley Series in Probability and Mathematical Statistics: Applied Probability and Statistics Section.",
                "Wiley-Interscience Publication, New York, 1994. [13] Z. Rabinovich and J. S. Rosenschein.",
                "Extended Markov Tracking with an application to control.",
                "In The Workshop on Agent Tracking: Modeling Other Agents from Observations, at the Third International Joint Conference on Autonomous Agents and Multiagent Systems, pages 95-100, New-York, July 2004. [14] Z. Rabinovich and J. S. Rosenschein.",
                "Multiagent coordination by Extended Markov Tracking.",
                "In The Fourth International Joint Conference on Autonomous Agents and Multiagent Systems, pages 431-438, Utrecht, The Netherlands, July 2005. [15] Z. Rabinovich and J. S. Rosenschein.",
                "On the response of EMT-based control to interacting targets and models.",
                "In The Fifth International Joint Conference on Autonomous Agents and Multiagent Systems, pages 465-470, Hakodate, Japan, May 2006. [16] R. F. Stengel.",
                "Optimal Control and Estimation.",
                "Dover Publications, 1994. [17] M. Tambe, E. Bowring, H. Jung, G. Kaminka, R. Maheswaran, J. Marecki, J. Modi, R. Nair, J. Pearce, P. Paruchuri, D. Pynadath, P. Scerri, N. Schurr, and P. Varakantham.",
                "Conflicts in teamwork: Hybrids to the The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) 797"
            ],
            "original_annotated_samples": [],
            "translated_annotated_samples": [
                "Control basado en la dinámica La especificación del control basado en la dinámica (DBC) se puede dividir en tres niveles de interacción: nivel de diseño del entorno, nivel de usuario y \"nivel de agente\".• El nivel de diseño del medio ambiente se refiere a la especificación formal y el modelado del entorno.",
                "En el escenario de la Guardia del Museo anterior, estos corresponderían a un horario estocástico de barrido, y una medida de sorpresa relativa entre el barrido especificado y real.• El \"nivel de agente\" a su vez combina el modelo de entorno desde el nivel de diseño del entorno, el procedimiento de estimación de la dinámica, la medida de desviación y la especificación de dinámica de destino desde el nivel del usuario, para producir una secuencia de acciones que crean dinámicas del sistema lo más cerca posible de laEspecificación dirigida.",
                "Como estamos interesados en el desarrollo continuo de un sistema estocástico, como sucede en la teoría del control clásico [16] y la planificación continua [4], así como en nuestro ejemplo de barridos de museos, la pregunta se convierte en cómo es el \"nivel de agente\"Para tratar las mediciones de desviación con el tiempo.",
                "Con este fin, usamos un umbral de probabilidad, es decir, nos gustaría que el \"nivel de agente\" maximice la probabilidad de que la medida de desviación permanezca por debajo de un cierto umbral.",
                "Por ejemplo, el \"nivel de agente\" podría proporcionar datos sobre la viabilidad de la dinámica de destino, lo que permite al nivel del usuario modificar el requisito, tal vez centrarse en las características alcanzables del comportamiento del sistema.",
                "En este caso, los datos de viabilidad y respuesta al sistema del \"nivel de agente\" proporcionarían información clave para una actualización del modelo de entorno.",
                "Por ejemplo, la divergencia de Kullbackleibler es una herramienta importante de la teoría de la información [3] que le permite a uno medir el precio de codificar una fuente de información regida por Q, al tiempo que supone que se rige por p.El nivel de usuario también define el umbral de la probabilidad de desviación dinámica θ.• El \"nivel de agente\" se enfrenta con el problema de seleccionar una función de señal de control A ∗ para satisfacer un problema de minimización de la siguiente manera: a ∗ = arg min a pr (d (τa, q)> θ) donde d (τa, q) es una variable aleatoria que describe la desviación de la estimación de la dinámica τa, creada por L bajo la señal de control A, a partir de la dinámica ideal q.",
                "Conf.en agentes autónomos y sistemas de múltiples agentes (AAMAS 07) Tabla 1: Estructura de POMDP versus control basado en la dinámica en el enfoque de nivel de entorno de Markovian MDP Markovian DBC Environment <S, A, T, O, Ω>, donde s-conjunto de un conjunto de conjuntos deEstados A - Conjunto de Acciones Diseño T: S × A → π (S) - Transición O - Conjunto de observación ω: S × A × S → π (O) Usuario R: S × A × S → R Q: S × A→ π (s) f (π ∗) → r l (o1, ..., ot) → τ r - función de recompensa q - dinámica ideal f - remodelación de recompensa l - estimador de dinámica θ - agente de umbral π ∗ = arg max π π πE [γt RT] π ∗ = arg min π prob (d (τ q)> θ) Problema de minimización: τt emt = h [pt, pt - 1, τt - 1 emt] = arg min τ dkl (τ × pt−1 τt - 1 EMT × PT - 1) S.T.pt (s) = s (τ × pt-1) (s, s) pt-1 (s) = s (τ × pt-1) (s, s) • El \"nivel de agente\" en el control basado en EMT es subóptimoCon respecto a DBC (aunque permanece dentro del marco DBC), realizando una selección de acción codiciosa basada en la predicción de la reacción de EMTS."
            ],
            "translated_text": "",
            "candidates": [
                "nivel de agente",
                "nivel de agente",
                "nivel de agente",
                "nivel de agente",
                "nivel de agente",
                "nivel de agente",
                "nivel de agente",
                "nivel de agente",
                "nivel de agente",
                "nivel de agente",
                "nivel de agente",
                "nivel de agente",
                "nivel de agente",
                "nivel de agente",
                "nivel de agente",
                "nivel de agente"
            ],
            "error": []
        },
        "system dynamics": {
            "translated_key": "dinámica del sistema",
            "is_in_text": true,
            "original_annotated_sentences": [
                "Dynamics Based Control with an Application to Area-Sweeping Problems Zinovi Rabinovich Engineering and Computer Science Hebrew University of Jerusalem Jerusalem, Israel nomad@cs.huji.ac.il Jeffrey S. Rosenschein Engineering and Computer Science Hebrew University of Jerusalem Jerusalem, Israel jeff@cs.huji.ac.il Gal A. Kaminka The MAVERICK Group Department of Computer Science Bar Ilan University, Israel galk@cs.biu.ac.il ABSTRACT In this paper we introduce Dynamics Based Control (DBC), an approach to planning and control of an agent in stochastic environments.",
                "Unlike existing approaches, which seek to optimize expected rewards (e.g., in Partially Observable Markov Decision Problems (POMDPs)), DBC optimizes system behavior towards specified <br>system dynamics</br>.",
                "We show that a recently developed planning and control approach, Extended Markov Tracking (EMT) is an instantiation of DBC.",
                "EMT employs greedy action selection to provide an efficient control algorithm in Markovian environments.",
                "We exploit this efficiency in a set of experiments that applied multitarget EMT to a class of area-sweeping problems (searching for moving targets).",
                "We show that such problems can be naturally defined and efficiently solved using the DBC framework, and its EMT instantiation.",
                "Categories and Subject Descriptors I.2.8 [Problem Solving, Control Methods, and Search]: Control Theory; I.2.9 [Robotics]; I.2.11 [Distributed Artificial Intelligence]: Intelligent Agents General Terms Algorithms, Theory 1.",
                "INTRODUCTION Planning and control constitutes a central research area in multiagent systems and artificial intelligence.",
                "In recent years, Partially Observable Markov Decision Processes (POMDPs) [12] have become a popular formal basis for planning in stochastic environments.",
                "In this framework, the planning and control problem is often addressed by imposing a reward function, and computing a policy (of choosing actions) that is optimal, in the sense that it will result in the highest expected utility.",
                "While theoretically attractive, the complexity of optimally solving a POMDP is prohibitive [8, 7].",
                "We take an alternative view of planning in stochastic environments.",
                "We do not use a (state-based) reward function, but instead optimize over a different criterion, a transition-based specification of the desired <br>system dynamics</br>.",
                "The idea here is to view planexecution as a process that compels a (stochastic) system to change, and a plan as a dynamic process that shapes that change according to desired criteria.",
                "We call this general planning framework Dynamics Based Control (DBC).",
                "In DBC, the goal of a planning (or control) process becomes to ensure that the system will change in accordance with specific (potentially stochastic) target dynamics.",
                "As actual system behavior may deviate from that which is specified by target dynamics (due to the stochastic nature of the system), planning in such environments needs to be continual [4], in a manner similar to classical closed-loop controllers [16].",
                "Here, optimality is measured in terms of probability of deviation magnitudes.",
                "In this paper, we present the structure of Dynamics Based Control.",
                "We show that the recently developed Extended Markov Tracking (EMT) approach [13, 14, 15] is subsumed by DBC, with EMT employing greedy action selection, which is a specific parameterization among the options possible within DBC.",
                "EMT is an efficient instantiation of DBC.",
                "To evaluate DBC, we carried out a set of experiments applying multi-target EMT to the Tag Game [11]; this is a variant on the area sweeping problem, where an agent is trying to tag a moving target (quarry) whose position is not known with certainty.",
                "Experimental data demonstrates that even with a simple model of the environment and a simple design of target dynamics, high success rates can be produced both in catching the quarry, and in surprising the quarry (as expressed by the observed entropy of the controlled agents position).",
                "The paper is organized as follows.",
                "In Section 2 we motivate DBC using area-sweeping problems, and discuss related work.",
                "Section 3 introduces the Dynamics Based Control (DBC) structure, and its specialization to Markovian environments.",
                "This is followed by a review of the Extended Markov Tracking (EMT) approach as a DBC-structured control regimen in Section 4.",
                "That section also discusses the limitations of EMT-based control relative to the general DBC framework.",
                "Experimental settings and results are then presented in Section 5.",
                "Section 6 provides a short discussion of the overall approach, and Section 7 gives some concluding remarks and directions for future work. 790 978-81-904262-7-5 (RPS) c 2007 IFAAMAS 2.",
                "MOTIVATION AND RELATED WORK Many real-life scenarios naturally have a stochastic target dynamics specification, especially those domains where there exists no ultimate goal, but rather system behavior (with specific properties) that has to be continually supported.",
                "For example, security guards perform persistent sweeps of an area to detect any sign of intrusion.",
                "Cunning thieves will attempt to track these sweeps, and time their operation to key points of the guards motion.",
                "It is thus advisable to make the guards motion dynamics appear irregular and random.",
                "Recent work by Paruchuri et al. [10] has addressed such randomization in the context of single-agent and distributed POMDPs.",
                "The goal in that work was to generate policies that provide a measure of action-selection randomization, while maintaining rewards within some acceptable levels.",
                "Our focus differs from this work in that DBC does not optimize expected rewards-indeed we do not consider rewards at all-but instead maintains desired dynamics (including, but not limited to, randomization).",
                "The Game of Tag is another example of the applicability of the approach.",
                "It was introduced in the work by Pineau et al. [11].",
                "There are two agents that can move about an area, which is divided into a grid.",
                "The grid may have blocked cells (holes) into which no agent can move.",
                "One agent (the hunter) seeks to move into a cell occupied by the other (the quarry), such that they are co-located (this is a successful tag).",
                "The quarry seeks to avoid the hunter agent, and is always aware of the hunters position, but does not know how the hunter will behave, which opens up the possibility for a hunter to surprise the prey.",
                "The hunter knows the quarrys probabilistic law of motion, but does not know its current location.",
                "Tag is an instance of a family of area-sweeping (pursuit-evasion) problems.",
                "In [11], the hunter modeled the problem using a POMDP.",
                "A reward function was defined, to reflect the desire to tag the quarry, and an action policy was computed to optimize the reward collected over time.",
                "Due to the intractable complexity of determining the optimal policy, the action policy computed in that paper was essentially an approximation.",
                "In this paper, instead of formulating a reward function, we use EMT to solve the problem, by directly specifying the target dynamics.",
                "In fact, any search problem with randomized motion, the socalled class of area sweeping problems, can be described through specification of such target <br>system dynamics</br>.",
                "Dynamics Based Control provides a natural approach to solving these problems. 3.",
                "DYNAMICS BASED CONTROL The specification of Dynamics Based Control (DBC) can be broken into three interacting levels: Environment Design Level, User Level, and Agent Level. • Environment Design Level is concerned with the formal specification and modeling of the environment.",
                "For example, this level would specify the laws of physics within the system, and set its parameters, such as the gravitation constant. • User Level in turn relies on the environment model produced by Environment Design to specify the target <br>system dynamics</br> it wishes to observe.",
                "The User Level also specifies the estimation or learning procedure for <br>system dynamics</br>, and the measure of deviation.",
                "In the museum guard scenario above, these would correspond to a stochastic sweep schedule, and a measure of relative surprise between the specified and actual sweeping. • Agent Level in turn combines the environment model from the Environment Design level, the dynamics estimation procedure, the deviation measure and the target dynamics specification from User Level, to produce a sequence of actions that create <br>system dynamics</br> as close as possible to the targeted specification.",
                "As we are interested in the continual development of a stochastic system, such as happens in classical control theory [16] and continual planning [4], as well as in our example of museum sweeps, the question becomes how the Agent Level is to treat the deviation measurements over time.",
                "To this end, we use a probability threshold-that is, we would like the Agent Level to maximize the probability that the deviation measure will remain below a certain threshold.",
                "Specific action selection then depends on system formalization.",
                "One possibility would be to create a mixture of available system trends, much like that which happens in Behavior-Based Robotic architectures [1].",
                "The other alternative would be to rely on the estimation procedure provided by the User Level-to utilize the Environment Design Level model of the environment to choose actions, so as to manipulate the dynamics estimator into believing that a certain dynamics has been achieved.",
                "Notice that this manipulation is not direct, but via the environment.",
                "Thus, for strong enough estimator algorithms, successful manipulation would mean a successful simulation of the specified target dynamics (i.e., beyond discerning via the available sensory input).",
                "DBC levels can also have a back-flow of information (see Figure 1).",
                "For instance, the Agent Level could provide data about target dynamics feasibility, allowing the User Level to modify the requirement, perhaps focusing on attainable features of system behavior.",
                "Data would also be available about the system response to different actions performed; combined with a dynamics estimator defined by the User Level, this can provide an important tool for the environment model calibration at the Environment Design Level.",
                "UserEnv.",
                "Design Agent Model Ideal Dynamics Estimator Estimator Dynamics Feasibility System Response Data Figure 1: Data flow of the DBC framework Extending upon the idea of Actor-Critic algorithms [5], DBC data flow can provide a good basis for the design of a learning algorithm.",
                "For example, the User Level can operate as an exploratory device for a learning algorithm, inferring an ideal dynamics target from the environment model at hand that would expose and verify most critical features of system behavior.",
                "In this case, feasibility and system response data from the Agent Level would provide key information for an environment model update.",
                "In fact, the combination of feasibility and response data can provide a basis for the application of strong learning algorithms such as EM [2, 9]. 3.1 DBC for Markovian Environments For a Partially Observable Markovian Environment, DBC can be specified in a more rigorous manner.",
                "Notice how DBC discards rewards, and replaces it by another optimality criterion (structural differences are summarized in Table 1): • Environment Design level is to specify a tuple < S, A, T, O, Ω, s0 >, where: - S is the set of all possible environment states; - s0 is the initial state of the environment (which can also be viewed as a probability distribution over S); The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) 791 - A is the set of all possible actions applicable in the environment; - T is the environments probabilistic transition function: T : S ×A → Π(S).",
                "That is, T(s |a, s) is the probability that the environment will move from state s to state s under action a; - O is the set of all possible observations.",
                "This is what the sensor input would look like for an outside observer; - Ω is the observation probability function: Ω : S × A × S → Π(O).",
                "That is, Ω(o|s , a, s) is the probability that one will observe o given that the environment has moved from state s to state s under action a. • User Level, in the case of Markovian environment, operates on the set of <br>system dynamics</br> described by a family of conditional probabilities F = {τ : S × A → Π(S)}.",
                "Thus specification of target dynamics can be expressed by q ∈ F, and the learning or tracking algorithm can be represented as a function L : O×(A×O)∗ → F; that is, it maps sequences of observations and actions performed so far into an estimate τ ∈ F of <br>system dynamics</br>.",
                "There are many possible variations available at the User Level to define divergence between <br>system dynamics</br>; several of them are: - Trace distance or L1 distance between two distributions p and q defined by D(p(·), q(·)) = 1 2 x |p(x) − q(x)| - Fidelity measure of distance F(p(·), q(·)) = x p(x)q(x) - Kullback-Leibler divergence DKL(p(·) q(·)) = x p(x) log p(x) q(x) Notice that the latter two are not actually metrics over the space of possible distributions, but nevertheless have meaningful and important interpretations.",
                "For instance, KullbackLeibler divergence is an important tool of information theory [3] that allows one to measure the price of encoding an information source governed by q, while assuming that it is governed by p. The User Level also defines the threshold of dynamics deviation probability θ. • Agent Level is then faced with a problem of selecting a control signal function a∗ to satisfy a minimization problem as follows: a∗ = arg min a Pr(d(τa, q) > θ) where d(τa, q) is a random variable describing deviation of the dynamics estimate τa, created by L under control signal a, from the ideal dynamics q.",
                "Implicit in this minimization problem is that L is manipulated via the environment, based on the environment model produced by the Environment Design Level. 3.2 DBC View of the State Space It is important to note the complementary view that DBC and POMDPs take on the state space of the environment.",
                "POMDPs regard state as a stationary snap-shot of the environment; whatever attributes of state sequencing one seeks are reached through properties of the control process, in this case reward accumulation.",
                "This can be viewed as if the sequencing of states and the attributes of that sequencing are only introduced by and for the controlling mechanism-the POMDP policy.",
                "DBC concentrates on the underlying principle of state sequencing, the <br>system dynamics</br>.",
                "DBCs target dynamics specification can use the environments state space as a means to describe, discern, and preserve changes that occur within the system.",
                "As a result, DBC has a greater ability to express state sequencing properties, which are grounded in the environment model and its state space definition.",
                "For example, consider the task of moving through rough terrain towards a goal and reaching it as fast as possible.",
                "POMDPs would encode terrain as state space points, while speed would be ensured by negative reward for every step taken without reaching the goalaccumulating higher reward can be reached only by faster motion.",
                "Alternatively, the state space could directly include the notion of speed.",
                "For POMDPs, this would mean that the same concept is encoded twice, in some sense: directly in the state space, and indirectly within reward accumulation.",
                "Now, even if the reward function would encode more, and finer, details of the properties of motion, the POMDP solution will have to search in a much larger space of policies, while still being guided by the implicit concept of the reward accumulation procedure.",
                "On the other hand, the tactical target expression of variations and correlations between position and speed of motion is now grounded in the state space representation.",
                "In this situation, any further constraints, e.g., smoothness of motion, speed limits in different locations, or speed reductions during sharp turns, are explicitly and uniformly expressed by the tactical target, and can result in faster and more effective action selection by a DBC algorithm. 4.",
                "EMT-BASED CONTROL AS A DBC Recently, a control algorithm was introduced called EMT-based Control [13], which instantiates the DBC framework.",
                "Although it provides an approximate greedy solution in the DBC sense, initial experiments using EMT-based control have been encouraging [14, 15].",
                "EMT-based control is based on the Markovian environment definition, as in the case with POMDPs, but its User and Agent Levels are of the Markovian DBC type of optimality. • User Level of EMT-based control defines a limited-case target <br>system dynamics</br> independent of action: qEMT : S → Π(S).",
                "It then utilizes the Kullback-Leibler divergence measure to compose a momentary <br>system dynamics</br> estimator-the Extended Markov Tracking (EMT) algorithm.",
                "The algorithm keeps a <br>system dynamics</br> estimate τt EMT that is capable of explaining recent change in an auxiliary Bayesian system state estimator from pt−1 to pt, and updates it conservatively using Kullback-Leibler divergence.",
                "Since τt EMT and pt−1,t are respectively the conditional and marginal probabilities over the systems state space, explanation simply means that pt(s ) = s τt EMT (s |s)pt−1(s), and the dynamics estimate update is performed by solving a 792 The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) Table 1: Structure of POMDP vs. Dynamics-Based Control in Markovian Environment Level Approach MDP Markovian DBC Environment < S, A, T, O, Ω >,where S - set of states A - set of actions Design T : S × A → Π(S) - transition O - observation set Ω : S × A × S → Π(O) User r : S × A × S → R q : S × A → Π(S) F(π∗ ) → r L(o1, ..., ot) → τ r - reward function q - ideal dynamics F - reward remodeling L - dynamics estimator θ - threshold Agent π∗ = arg max π E[ γt rt] π∗ = arg min π Prob(d(τ q) > θ) minimization problem: τt EMT = H[pt, pt−1, τt−1 EMT ] = arg min τ DKL(τ × pt−1 τt−1 EMT × pt−1) s.t. pt(s ) = s (τ × pt−1)(s , s) pt−1(s) = s (τ × pt−1)(s , s) • Agent Level in EMT-based control is suboptimal with respect to DBC (though it remains within the DBC framework), performing greedy action selection based on prediction of EMTs reaction.",
                "The prediction is based on the environment model provided by the Environment Design level, so that if we denote by Ta the environments transition function limited to action a, and pt−1 is the auxiliary Bayesian system state estimator, then the EMT-based control choice is described by a∗ = arg min a∈A DKL(H[Ta × pt, pt, τt EMT ] qEMT × pt−1) Note that this follows the Markovian DBC framework precisely: the rewarding optimality of POMDPs is discarded, and in its place a dynamics estimator (EMT in this case) is manipulated via action effects on the environment to produce an estimate close to the specified target <br>system dynamics</br>.",
                "Yet as we mentioned, naive EMTbased control is suboptimal in the DBC sense, and has several additional limitations that do not exist in the general DBC framework (discussed in Section 4.2). 4.1 Multi-Target EMT At times, there may exist several behavioral preferences.",
                "For example, in the case of museum guards, some art items are more heavily guarded, requiring that the guards stay more often in their close vicinity.",
                "On the other hand, no corner of the museum is to be left unchecked, which demands constant motion.",
                "Successful museum security would demand that the guards adhere to, and balance, both of these behaviors.",
                "For EMT-based control, this would mean facing several tactical targets {qk}K k=1, and the question becomes how to merge and balance them.",
                "A balancing mechanism can be applied to resolve this issue.",
                "Note that EMT-based control, while selecting an action, creates a preference vector over the set of actions based on their predicted performance with respect to a given target.",
                "If these preference vectors are normalized, they can be combined into a single unified preference.",
                "This requires replacement of standard EMT-based action selection by the algorithm below [15]: • Given: - a set of target dynamics {qk}K k=1, - vector of weights w(k) • Select action as follows - For each action a ∈ A predict the future state distribution ¯pa t+1 = Ta ∗ pt; - For each action, compute Da = H(¯pa t+1, pt, PDt) - For each a ∈ A and qk tactical target, denote V (a, k) = DKL (Da qk) pt .",
                "Let Vk(a) = 1 Zk V (a, k), where Zk = a∈A V (a, k) is a normalization factor. - Select a∗ = arg min a k k=1 w(k)Vk(a) The weights vector w = (w1, ..., wK ) allows the additional tuning of importance among target dynamics without the need to redesign the targets themselves.",
                "This balancing method is also seamlessly integrated into the EMT-based control flow of operation. 4.2 EMT-based Control Limitations EMT-based control is a sub-optimal (in the DBC sense) representative of the DBC structure.",
                "It limits the User by forcing EMT to be its dynamic tracking algorithm, and replaces Agent optimization by greedy action selection.",
                "This kind of combination, however, is common for on-line algorithms.",
                "Although further development of EMT-based controllers is necessary, evidence so far suggests that even the simplest form of the algorithm possesses a great deal of power, and displays trends that are optimal in the DBC sense of the word.",
                "There are two further, EMT-specific, limitations to EMT-based control that are evident at this point.",
                "Both already have partial solutions and are subjects of ongoing research.",
                "The first limitation is the problem of negative preference.",
                "In the POMDP framework for example, this is captured simply, through The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) 793 the appearance of values with different signs within the reward structure.",
                "For EMT-based control, however, negative preference means that one would like to avoid a certain distribution over system development sequences; EMT-based control, however, concentrates on getting as close as possible to a distribution.",
                "Avoidance is thus unnatural in native EMT-based control.",
                "The second limitation comes from the fact that standard environment modeling can create pure sensory actions-actions that do not change the state of the world, and differ only in the way observations are received and the quality of observations received.",
                "Since the world state does not change, EMT-based control would not be able to differentiate between different sensory actions.",
                "Notice that both of these limitations of EMT-based control are absent from the general DBC framework, since it may have a tracking algorithm capable of considering pure sensory actions and, unlike Kullback-Leibler divergence, a distribution deviation measure that is capable of dealing with negative preference. 5.",
                "EMT PLAYING TAG The Game of Tag was first introduced in [11].",
                "It is a single agent problem of capturing a quarry, and belongs to the class of area sweeping problems.",
                "An example domain is shown in Figure 2. 0 51 2 3 4 6 7 8 10 12 13 161514 17 18 19 2221 23 9 11Q A 20 Figure 2: Tag domain; an agent (A) attempts to seek and capture a quarry (Q) The Game of Tag extremely limits the agents perception, so that the agent is able to detect the quarry only if they are co-located in the same cell of the grid world.",
                "In the classical version of the game, co-location leads to a special observation, and the Tag action can be performed.",
                "We slightly modify this setting: the moment that both agents occupy the same cell, the game ends.",
                "As a result, both the agent and its quarry have the same motion capability, which allows them to move in four directions, North, South, East, and West.",
                "These form a formal space of actions within a Markovian environment.",
                "The state space of the formal Markovian environment is described by the cross-product of the agent and quarrys positions.",
                "For Figure 2, it would be S = {s0, ..., s23} × {s0, ..., s23}.",
                "The effects of an action taken by the agent are deterministic, but the environment in general has a stochastic response due to the motion of the quarry.",
                "With probability q0 1 it stays put, and with probability 1 − q0 it moves to an adjacent cell further away from the 1 In our experiments this was taken to be q0 = 0.2. agent.",
                "So for the instance shown in Figure 2 and q0 = 0.1: P(Q = s9|Q = s9, A = s11) = 0.1 P(Q = s2|Q = s9, A = s11) = 0.3 P(Q = s8|Q = s9, A = s11) = 0.3 P(Q = s14|Q = s9, A = s11) = 0.3 Although the evasive behavior of the quarry is known to the agent, the quarrys position is not.",
                "The only sensory information available to the agent is its own location.",
                "We use EMT and directly specify the target dynamics.",
                "For the Game of Tag, we can easily formulate three major trends: catching the quarry, staying mobile, and stalking the quarry.",
                "This results in the following three target dynamics: Tcatch(At+1 = si|Qt = sj, At = sa) ∝ 1 si = sj 0 otherwise Tmobile(At+1 = si|Qt = so, At = sj) ∝ 0 si = sj 1 otherwise Tstalk(At+1 = si|Qt = so, At = sj) ∝ 1 dist(si,so) Note that none of the above targets are directly achievable; for instance, if Qt = s9 and At = s11, there is no action that can move the agent to At+1 = s9 as required by the Tcatch target dynamics.",
                "We ran several experiments to evaluate EMT performance in the Tag Game.",
                "Three configurations of the domain shown in Figure 3 were used, each posing a different challenge to the agent due to partial observability.",
                "In each setting, a set of 1000 runs was performed with a time limit of 100 steps.",
                "In every run, the initial position of both the agent and its quarry was selected at random; this means that as far as the agent was concerned, the quarrys initial position was uniformly distributed over the entire domain cell space.",
                "We also used two variations of the environment observability function.",
                "In the first version, observability function mapped all joint positions of hunter and quarry into the position of the hunter as an observation.",
                "In the second, only those joint positions in which hunter and quarry occupied different locations were mapped into the hunters location.",
                "The second version in fact utilized and expressed the fact that once hunter and quarry occupy the same cell the game ends.",
                "The results of these experiments are shown in Table 2.",
                "Balancing [15] the catch, move, and stalk target dynamics described in the previous section by the weight vector [0.8, 0.1, 0.1], EMT produced stable performance in all three domains.",
                "Although direct comparisons are difficult to make, the EMT performance displayed notable efficiency vis-`a-vis the POMDP approach.",
                "In spite of a simple and inefficient Matlab implementation of the EMT algorithm, the decision time for any given step averaged significantly below 1 second in all experiments.",
                "For the irregular open arena domain, which proved to be the most difficult, 1000 experiment runs bounded by 100 steps each, a total of 42411 steps, were completed in slightly under 6 hours.",
                "That is, over 4 × 104 online steps took an order of magnitude less time than the offline computation of POMDP policy in [11].",
                "The significance of this differential is made even more prominent by the fact that, should the environment model parameters change, the online nature of EMT would allow it to maintain its performance time, while the POMDP policy would need to be recomputed, requiring yet again a large overhead of computation.",
                "We also tested the behavior cell frequency entropy, empirical measures from trial data.",
                "As Figure 4 and Figure 5 show, empir794 The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) A Q Q A 0 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 A Q Figure 3: These configurations of the Tag Game space were used: a) multiple dead-end, b) irregular open arena, c) circular corridor Table 2: Performance of the EMT-based solution in three Tag Game domains and two observability models: I) omniposition quarry, II) quarry is not at hunters position Model Domain Capture% E(Steps) Time/Step I Dead-ends 100 14.8 72(mSec) Arena 80.2 42.4 500(mSec) Circle 91.4 34.6 187(mSec) II Dead-ends 100 13.2 91(mSec) Arena 96.8 28.67 396(mSec) Circle 94.4 31.63 204(mSec) ical entropy grows with the length of interaction.",
                "For runs where the quarry was not captured immediately, the entropy reaches between 0.85 and 0.952 for different runs and scenarios.",
                "As the agent actively seeks the quarry, the entropy never reaches its maximum.",
                "One characteristic of the entropy graph for the open arena scenario particularly caught our attention in the case of the omniposition quarry observation model.",
                "Near the maximum limit of trial length (100 steps), entropy suddenly dropped.",
                "Further analysis of the data showed that under certain circumstances, a fluctuating behavior occurs in which the agent faces equally viable versions of quarry-following behavior.",
                "Since the EMT algorithm has greedy action selection, and the state space does not encode any form of commitment (not even speed or acceleration), the agent is locked within a small portion of cells.",
                "It is essentially attempting to simultaneously follow several courses of action, all of which are consistent with the target dynamics.",
                "This behavior did not occur in our second observation model, since it significantly reduced the set of eligible courses of action-essentially contributing to tie-breaking among them. 6.",
                "DISCUSSION The design of the EMT solution for the Tag Game exposes the core difference in approach to planning and control between EMT or DBC, on the one hand, and the more familiar POMDP approach, on the other.",
                "POMDP defines a reward structure to optimize, and influences <br>system dynamics</br> indirectly through that optimization.",
                "EMT discards any reward scheme, and instead measures and influences <br>system dynamics</br> directly. 2 Entropy was calculated using log base equal to the number of possible locations within the domain; this properly scales entropy expression into the range [0, 1] for all domains.",
                "Thus for the Tag Game, we did not search for a reward function that would encode and express our preference over the agents behavior, but rather directly set three (heuristic) behavior preferences as the basis for target dynamics to be maintained.",
                "Experimental data shows that these targets need not be directly achievable via the agents actions.",
                "However, the ratio between EMT performance and achievability of target dynamics remains to be explored.",
                "The tag game experiment data also revealed the different emphasis DBC and POMDPs place on the formulation of an environment state space.",
                "POMDPs rely entirely on the mechanism of reward accumulation maximization, i.e., formation of the action selection procedure to achieve necessary state sequencing.",
                "DBC, on the other hand, has two sources of sequencing specification: through the properties of an action selection procedure, and through direct specification within the target dynamics.",
                "The importance of the second source was underlined by the Tag Game experiment data, in which the greedy EMT algorithm, applied to a POMDP-type state space specification, failed, since target description over such a state space was incapable of encoding the necessary behavior tendencies, e.g., tie-breaking and commitment to directed motion.",
                "The structural differences between DBC (and EMT in particular), and POMDPs, prohibits direct performance comparison, and places them on complementary tracks, each within a suitable niche.",
                "For instance, POMDPs could be seen as a much more natural formulation of economic sequential decision-making problems, while EMT is a better fit for continual demand for stochastic change, as happens in many robotic or embodied-agent problems.",
                "The complementary properties of POMDPs and EMT can be further exploited.",
                "There is recent interest in using POMDPs in hybrid solutions [17], in which the POMDPs can be used together with other control approaches to provide results not easily achievable with either approach by itself.",
                "DBC can be an effective partner in such a hybrid solution.",
                "For instance, POMDPs have prohibitively large off-line time requirements for policy computation, but can be readily used in simpler settings to expose beneficial behavioral trends; this can serve as a form of target dynamics that are provided to EMT in a larger domain for on-line operation. 7.",
                "CONCLUSIONS AND FUTURE WORK In this paper, we have presented a novel perspective on the process of planning and control in stochastic environments, in the form of the Dynamics Based Control (DBC) framework.",
                "DBC formulates the task of planning as support of a specified target <br>system dynamics</br>, which describes the necessary properties of change within the environment.",
                "Optimality of DBC plans of action are measured The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) 795 0 20 40 60 80 100 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1 Steps Entropy Dead−ends 0 20 40 60 80 100 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1 Steps Entropy Arena 0 20 40 60 80 100 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1 Steps Entropy Circle Figure 4: Observation Model I: Omniposition quarry.",
                "Entropy development with length of Tag Game for the three experiment scenarios: a) multiple dead-end, b) irregular open arena, c) circular corridor. 0 10 20 30 40 50 60 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1 Steps Entropy Dead−ends 0 20 40 60 80 100 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1 Steps Entropy Arena 0 20 40 60 80 100 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1 Steps Entropy Circle Figure 5: Observation Model II: quarry not observed at hunters position.",
                "Entropy development with length of Tag Game for the three experiment scenarios: a) multiple dead-end, b) irregular open arena, c) circular corridor. with respect to the deviation of actual <br>system dynamics</br> from the target dynamics.",
                "We show that a recently developed technique of Extended Markov Tracking (EMT) [13] is an instantiation of DBC.",
                "In fact, EMT can be seen as a specific case of DBC parameterization, which employs a greedy action selection procedure.",
                "Since EMT exhibits the key features of the general DBC framework, as well as polynomial time complexity, we used the multitarget version of EMT [15] to demonstrate that the class of area sweeping problems naturally lends itself to dynamics-based descriptions, as instantiated by our experiments in the Tag Game domain.",
                "As enumerated in Section 4.2, EMT has a number of limitations, such as difficulty in dealing with negative dynamic preference.",
                "This prevents direct application of EMT to such problems as Rendezvous-Evasion Games (e.g., [6]).",
                "However, DBC in general has no such limitations, and readily enables the formulation of evasion games.",
                "In future work, we intend to proceed with the development of dynamics-based controllers for these problems. 8.",
                "ACKNOWLEDGMENT The work of the first two authors was partially supported by Israel Science Foundation grant #898/05, and the third author was partially supported by a grant from Israels Ministry of Science and Technology. 9.",
                "REFERENCES [1] R. C. Arkin.",
                "Behavior-Based Robotics.",
                "MIT Press, 1998. [2] J.",
                "A. Bilmes.",
                "A gentle tutorial of the EM algorithm and its application to parameter estimation for Gaussian mixture and Hidden Markov Models.",
                "Technical Report TR-97-021, Department of Electrical Engeineering and Computer Science, University of California at Berkeley, 1998. [3] T. M. Cover and J.",
                "A. Thomas.",
                "Elements of information theory.",
                "Wiley, 1991. [4] M. E. desJardins, E. H. Durfee, C. L. Ortiz, and M. J. Wolverton.",
                "A survey of research in distributed, continual planning.",
                "AI Magazine, 4:13-22, 1999. [5] V. R. Konda and J. N. Tsitsiklis.",
                "Actor-Critic algorithms.",
                "SIAM Journal on Control and Optimization, 42(4):1143-1166, 2003. [6] W. S. Lim.",
                "A rendezvous-evasion game on discrete locations with joint randomization.",
                "Advances in Applied Probability, 29(4):1004-1017, December 1997. [7] M. L. Littman, T. L. Dean, and L. P. Kaelbling.",
                "On the complexity of solving Markov decision problems.",
                "In Proceedings of the 11th Annual Conference on Uncertainty in Artificial Intelligence (UAI-95), pages 394-402, 1995. [8] O. Madani, S. Hanks, and A. Condon.",
                "On the undecidability of probabilistic planning and related stochastic optimization problems.",
                "Artificial Intelligence Journal, 147(1-2):5-34, July 2003. [9] R. M. Neal and G. E. Hinton.",
                "A view of the EM algorithm 796 The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) that justifies incremental, sparse, and other variants.",
                "In M. I. Jordan, editor, Learning in Graphical Models, pages 355-368.",
                "Kluwer Academic Publishers, 1998. [10] P. Paruchuri, M. Tambe, F. Ordonez, and S. Kraus.",
                "Security in multiagent systems by policy randomization.",
                "In Proceeding of AAMAS 2006, 2006. [11] J. Pineau, G. Gordon, and S. Thrun.",
                "Point-based value iteration: An anytime algorithm for pomdps.",
                "In International Joint Conference on Artificial Intelligence (IJCAI), pages 1025-1032, August 2003. [12] M. L. Puterman.",
                "Markov Decision Processes.",
                "Wiley Series in Probability and Mathematical Statistics: Applied Probability and Statistics Section.",
                "Wiley-Interscience Publication, New York, 1994. [13] Z. Rabinovich and J. S. Rosenschein.",
                "Extended Markov Tracking with an application to control.",
                "In The Workshop on Agent Tracking: Modeling Other Agents from Observations, at the Third International Joint Conference on Autonomous Agents and Multiagent Systems, pages 95-100, New-York, July 2004. [14] Z. Rabinovich and J. S. Rosenschein.",
                "Multiagent coordination by Extended Markov Tracking.",
                "In The Fourth International Joint Conference on Autonomous Agents and Multiagent Systems, pages 431-438, Utrecht, The Netherlands, July 2005. [15] Z. Rabinovich and J. S. Rosenschein.",
                "On the response of EMT-based control to interacting targets and models.",
                "In The Fifth International Joint Conference on Autonomous Agents and Multiagent Systems, pages 465-470, Hakodate, Japan, May 2006. [16] R. F. Stengel.",
                "Optimal Control and Estimation.",
                "Dover Publications, 1994. [17] M. Tambe, E. Bowring, H. Jung, G. Kaminka, R. Maheswaran, J. Marecki, J. Modi, R. Nair, J. Pearce, P. Paruchuri, D. Pynadath, P. Scerri, N. Schurr, and P. Varakantham.",
                "Conflicts in teamwork: Hybrids to the The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) 797"
            ],
            "original_annotated_samples": [],
            "translated_annotated_samples": [
                "A diferencia de los enfoques existentes, que buscan optimizar las recompensas esperadas (por ejemplo, problemas de decisión de Markov parcialmente observables (POMDPS)), DBC optimiza el comportamiento del sistema hacia la \"dinámica del sistema\" especificada.",
                "No utilizamos una función de recompensa (basada en el estado), sino que optimizamos en un criterio diferente, una especificación basada en la transición de la \"dinámica del sistema\" deseada.",
                "De hecho, cualquier problema de búsqueda con movimiento aleatorizado, la clase de problemas de barrido de área, se puede describir mediante la especificación de dicha \"dinámica del sistema\".",
                "Por ejemplo, este nivel especificaría las leyes de la física dentro del sistema y establecería sus parámetros, como la constante de gravitación.• El nivel de usuario a su vez se basa en el modelo de entorno producido por el diseño del entorno para especificar la \"dinámica del sistema\" de destino que desea observar.",
                "El nivel de usuario también especifica la estimación o procedimiento de aprendizaje para la \"dinámica del sistema\" y la medida de la desviación.",
                "En el escenario de la Guardia del Museo anterior, estos corresponderían a un horario estocástico de barrido, y una medida de sorpresa relativa entre el barrido especificado y real.• El nivel de agente a su vez combina el modelo de entorno desde el nivel de diseño del entorno, el procedimiento de estimación de la dinámica, la medida de desviación y la especificación de dinámica de destino desde el nivel de usuario, para producir una secuencia de acciones que crean \"dinámica del sistema\" lo más cerca posible de laEspecificación dirigida.",
                "Es decir, ω (o | s, a, s) es la probabilidad de que uno observe o dado que el entorno se ha movido de los estados a los estados bajo acción a.• El nivel de usuario, en el caso del entorno de Markovian, funciona en el conjunto de \"dinámica del sistema\" descrita por una familia de probabilidades condicionales F = {τ: S × A → π (S)}.",
                "Por lo tanto, la especificación de la dinámica objetivo puede expresarse mediante q ∈ F, y el algoritmo de aprendizaje o seguimiento puede representarse en función L: O × (A × O) ∗ → F;Es decir, mapea secuencias de observaciones y acciones realizadas hasta ahora en una estimación τ ∈ F de \"dinámica del sistema\".",
                "Hay muchas variaciones posibles disponibles a nivel de usuario para definir la divergencia entre la \"dinámica del sistema\";Varios de ellos son: - distancia de traza o distancia de L1 entre dos distribuciones P y Q definidas por d (p (·), q (·)) = 1 2 x | p (x) - q (x) |- Medida de fidelidad de la distancia f (p (·), q (·)) = x p (x) q (x) - kullback -leíbler divergencia dkl (p (·) q (·)) = x p (x) log p (x) Q (x) Observe que los dos últimos no son realmente métricas sobre el espacio de posibles distribuciones, sino que tienen interpretaciones significativas e importantes.",
                "DBC se concentra en el principio subyacente de la secuenciación estatal, la \"dinámica del sistema\".",
                "El control basado en EMT se basa en la definición del entorno de Markovian, como en el caso de POMDPS, pero sus niveles de usuario y agente son del tipo de optimización DBC Markovian.• El nivel de usuario del control basado en EMT define una \"dinámica del sistema\" de objetivos limitados independientemente de la acción: QEMT: S → π (s).",
                "Luego utiliza la medida de divergencia de Kullback-Leibler para componer un momento de estimador de \"Dinámica del sistema\", el algoritmo de seguimiento de Markov (EMT) extendido.",
                "El algoritmo mantiene una estimación de \"dinámica del sistema\" τt EMT que es capaz de explicar un cambio reciente en un estimador de estado del sistema bayesiano auxiliar de PT-1 a PT, y lo actualiza de manera conservadora utilizando la divergencia de Kullback-Leibler.",
                "La predicción se basa en el modelo de entorno proporcionado por el nivel de diseño del entorno, de modo que si denotamos por TA la función de transición de los entornos limitado a la acción A, y PT-1 es el estimador de estado del sistema bayesiano auxiliar, entonces la elección de control basada en EMTse describe por A ∗ = arg min a∈A dkl (h [ta × pt, pt, τt emt] qemt × pt - 1) Tenga en cuenta que esto sigue el marco de DBC de Markovian con precisión: la optimización gratificante de POMDPS se descarta, y enSu lugar donde un estimador de dinámica (EMT en este caso) se manipula a través de los efectos de acción en el entorno para producir una estimación cercana a la \"dinámica del sistema\" de objetivo especificado.",
                "POMDP define una estructura de recompensa para optimizar e influye en la \"dinámica del sistema\" indirectamente a través de esa optimización.",
                "EMT descarta cualquier esquema de recompensa y, en cambio, mide e influye directamente en la \"dinámica del sistema\".2 La entropía se calculó utilizando la base log igual al número de ubicaciones posibles dentro del dominio;Esto escala correctamente la expresión de entropía en el rango [0, 1] para todos los dominios.",
                "DBC formula la tarea de planificar como soporte de un objetivo especificado \"Dinámica del sistema\", que describe las propiedades necesarias del cambio dentro del entorno.",
                "Desarrollo de entropía con longitud del juego de etiqueta para los tres escenarios de experimentos: a) múltiples de fin de vía muerta, b) Arena abierta irregular, c) Corredor circular.con respecto a la desviación de la \"dinámica del sistema\" real de la dinámica de destino."
            ],
            "translated_text": "",
            "candidates": [
                "sistemas dinámicos",
                "dinámica del sistema",
                "sistemas dinámicos",
                "dinámica del sistema",
                "sistemas dinámicos",
                "dinámica del sistema",
                "sistemas dinámicos",
                "dinámica del sistema",
                "sistemas dinámicos",
                "dinámica del sistema",
                "sistemas dinámicos",
                "dinámica del sistema",
                "sistemas dinámicos",
                "dinámica del sistema",
                "sistemas dinámicos",
                "dinámica del sistema",
                "sistemas dinámicos",
                "dinámica del sistema",
                "sistemas dinámicos",
                "dinámica del sistema",
                "sistemas dinámicos",
                "dinámica del sistema",
                "sistemas dinámicos",
                "Dinámica del sistema",
                "sistemas dinámicos",
                "dinámica del sistema",
                "sistemas dinámicos",
                "dinámica del sistema",
                "sistemas dinámicos",
                "dinámica del sistema",
                "sistemas dinámicos",
                "dinámica del sistema",
                "sistemas dinámicos",
                "Dinámica del sistema",
                "sistemas dinámicos",
                "dinámica del sistema"
            ],
            "error": []
        },
        "control": {
            "translated_key": "control",
            "is_in_text": true,
            "original_annotated_sentences": [
                "Dynamics Based <br>control</br> with an Application to Area-Sweeping Problems Zinovi Rabinovich Engineering and Computer Science Hebrew University of Jerusalem Jerusalem, Israel nomad@cs.huji.ac.il Jeffrey S. Rosenschein Engineering and Computer Science Hebrew University of Jerusalem Jerusalem, Israel jeff@cs.huji.ac.il Gal A. Kaminka The MAVERICK Group Department of Computer Science Bar Ilan University, Israel galk@cs.biu.ac.il ABSTRACT In this paper we introduce Dynamics Based <br>control</br> (DBC), an approach to planning and control of an agent in stochastic environments.",
                "Unlike existing approaches, which seek to optimize expected rewards (e.g., in Partially Observable Markov Decision Problems (POMDPs)), DBC optimizes system behavior towards specified system dynamics.",
                "We show that a recently developed planning and <br>control</br> approach, Extended Markov Tracking (EMT) is an instantiation of DBC.",
                "EMT employs greedy action selection to provide an efficient <br>control</br> algorithm in Markovian environments.",
                "We exploit this efficiency in a set of experiments that applied multitarget EMT to a class of area-sweeping problems (searching for moving targets).",
                "We show that such problems can be naturally defined and efficiently solved using the DBC framework, and its EMT instantiation.",
                "Categories and Subject Descriptors I.2.8 [Problem Solving, <br>control</br> Methods, and Search]: <br>control</br> Theory; I.2.9 [Robotics]; I.2.11 [Distributed Artificial Intelligence]: Intelligent Agents General Terms Algorithms, Theory 1.",
                "INTRODUCTION Planning and <br>control</br> constitutes a central research area in multiagent systems and artificial intelligence.",
                "In recent years, Partially Observable Markov Decision Processes (POMDPs) [12] have become a popular formal basis for planning in stochastic environments.",
                "In this framework, the planning and <br>control</br> problem is often addressed by imposing a reward function, and computing a policy (of choosing actions) that is optimal, in the sense that it will result in the highest expected utility.",
                "While theoretically attractive, the complexity of optimally solving a POMDP is prohibitive [8, 7].",
                "We take an alternative view of planning in stochastic environments.",
                "We do not use a (state-based) reward function, but instead optimize over a different criterion, a transition-based specification of the desired system dynamics.",
                "The idea here is to view planexecution as a process that compels a (stochastic) system to change, and a plan as a dynamic process that shapes that change according to desired criteria.",
                "We call this general planning framework Dynamics Based <br>control</br> (DBC).",
                "In DBC, the goal of a planning (or <br>control</br>) process becomes to ensure that the system will change in accordance with specific (potentially stochastic) target dynamics.",
                "As actual system behavior may deviate from that which is specified by target dynamics (due to the stochastic nature of the system), planning in such environments needs to be continual [4], in a manner similar to classical closed-loop controllers [16].",
                "Here, optimality is measured in terms of probability of deviation magnitudes.",
                "In this paper, we present the structure of Dynamics Based <br>control</br>.",
                "We show that the recently developed Extended Markov Tracking (EMT) approach [13, 14, 15] is subsumed by DBC, with EMT employing greedy action selection, which is a specific parameterization among the options possible within DBC.",
                "EMT is an efficient instantiation of DBC.",
                "To evaluate DBC, we carried out a set of experiments applying multi-target EMT to the Tag Game [11]; this is a variant on the area sweeping problem, where an agent is trying to tag a moving target (quarry) whose position is not known with certainty.",
                "Experimental data demonstrates that even with a simple model of the environment and a simple design of target dynamics, high success rates can be produced both in catching the quarry, and in surprising the quarry (as expressed by the observed entropy of the controlled agents position).",
                "The paper is organized as follows.",
                "In Section 2 we motivate DBC using area-sweeping problems, and discuss related work.",
                "Section 3 introduces the Dynamics Based <br>control</br> (DBC) structure, and its specialization to Markovian environments.",
                "This is followed by a review of the Extended Markov Tracking (EMT) approach as a DBC-structured <br>control</br> regimen in Section 4.",
                "That section also discusses the limitations of EMT-based <br>control</br> relative to the general DBC framework.",
                "Experimental settings and results are then presented in Section 5.",
                "Section 6 provides a short discussion of the overall approach, and Section 7 gives some concluding remarks and directions for future work. 790 978-81-904262-7-5 (RPS) c 2007 IFAAMAS 2.",
                "MOTIVATION AND RELATED WORK Many real-life scenarios naturally have a stochastic target dynamics specification, especially those domains where there exists no ultimate goal, but rather system behavior (with specific properties) that has to be continually supported.",
                "For example, security guards perform persistent sweeps of an area to detect any sign of intrusion.",
                "Cunning thieves will attempt to track these sweeps, and time their operation to key points of the guards motion.",
                "It is thus advisable to make the guards motion dynamics appear irregular and random.",
                "Recent work by Paruchuri et al. [10] has addressed such randomization in the context of single-agent and distributed POMDPs.",
                "The goal in that work was to generate policies that provide a measure of action-selection randomization, while maintaining rewards within some acceptable levels.",
                "Our focus differs from this work in that DBC does not optimize expected rewards-indeed we do not consider rewards at all-but instead maintains desired dynamics (including, but not limited to, randomization).",
                "The Game of Tag is another example of the applicability of the approach.",
                "It was introduced in the work by Pineau et al. [11].",
                "There are two agents that can move about an area, which is divided into a grid.",
                "The grid may have blocked cells (holes) into which no agent can move.",
                "One agent (the hunter) seeks to move into a cell occupied by the other (the quarry), such that they are co-located (this is a successful tag).",
                "The quarry seeks to avoid the hunter agent, and is always aware of the hunters position, but does not know how the hunter will behave, which opens up the possibility for a hunter to surprise the prey.",
                "The hunter knows the quarrys probabilistic law of motion, but does not know its current location.",
                "Tag is an instance of a family of area-sweeping (pursuit-evasion) problems.",
                "In [11], the hunter modeled the problem using a POMDP.",
                "A reward function was defined, to reflect the desire to tag the quarry, and an action policy was computed to optimize the reward collected over time.",
                "Due to the intractable complexity of determining the optimal policy, the action policy computed in that paper was essentially an approximation.",
                "In this paper, instead of formulating a reward function, we use EMT to solve the problem, by directly specifying the target dynamics.",
                "In fact, any search problem with randomized motion, the socalled class of area sweeping problems, can be described through specification of such target system dynamics.",
                "Dynamics Based <br>control</br> provides a natural approach to solving these problems. 3.",
                "DYNAMICS BASED <br>control</br> The specification of Dynamics Based <br>control</br> (DBC) can be broken into three interacting levels: Environment Design Level, User Level, and Agent Level. • Environment Design Level is concerned with the formal specification and modeling of the environment.",
                "For example, this level would specify the laws of physics within the system, and set its parameters, such as the gravitation constant. • User Level in turn relies on the environment model produced by Environment Design to specify the target system dynamics it wishes to observe.",
                "The User Level also specifies the estimation or learning procedure for system dynamics, and the measure of deviation.",
                "In the museum guard scenario above, these would correspond to a stochastic sweep schedule, and a measure of relative surprise between the specified and actual sweeping. • Agent Level in turn combines the environment model from the Environment Design level, the dynamics estimation procedure, the deviation measure and the target dynamics specification from User Level, to produce a sequence of actions that create system dynamics as close as possible to the targeted specification.",
                "As we are interested in the continual development of a stochastic system, such as happens in classical <br>control</br> theory [16] and continual planning [4], as well as in our example of museum sweeps, the question becomes how the Agent Level is to treat the deviation measurements over time.",
                "To this end, we use a probability threshold-that is, we would like the Agent Level to maximize the probability that the deviation measure will remain below a certain threshold.",
                "Specific action selection then depends on system formalization.",
                "One possibility would be to create a mixture of available system trends, much like that which happens in Behavior-Based Robotic architectures [1].",
                "The other alternative would be to rely on the estimation procedure provided by the User Level-to utilize the Environment Design Level model of the environment to choose actions, so as to manipulate the dynamics estimator into believing that a certain dynamics has been achieved.",
                "Notice that this manipulation is not direct, but via the environment.",
                "Thus, for strong enough estimator algorithms, successful manipulation would mean a successful simulation of the specified target dynamics (i.e., beyond discerning via the available sensory input).",
                "DBC levels can also have a back-flow of information (see Figure 1).",
                "For instance, the Agent Level could provide data about target dynamics feasibility, allowing the User Level to modify the requirement, perhaps focusing on attainable features of system behavior.",
                "Data would also be available about the system response to different actions performed; combined with a dynamics estimator defined by the User Level, this can provide an important tool for the environment model calibration at the Environment Design Level.",
                "UserEnv.",
                "Design Agent Model Ideal Dynamics Estimator Estimator Dynamics Feasibility System Response Data Figure 1: Data flow of the DBC framework Extending upon the idea of Actor-Critic algorithms [5], DBC data flow can provide a good basis for the design of a learning algorithm.",
                "For example, the User Level can operate as an exploratory device for a learning algorithm, inferring an ideal dynamics target from the environment model at hand that would expose and verify most critical features of system behavior.",
                "In this case, feasibility and system response data from the Agent Level would provide key information for an environment model update.",
                "In fact, the combination of feasibility and response data can provide a basis for the application of strong learning algorithms such as EM [2, 9]. 3.1 DBC for Markovian Environments For a Partially Observable Markovian Environment, DBC can be specified in a more rigorous manner.",
                "Notice how DBC discards rewards, and replaces it by another optimality criterion (structural differences are summarized in Table 1): • Environment Design level is to specify a tuple < S, A, T, O, Ω, s0 >, where: - S is the set of all possible environment states; - s0 is the initial state of the environment (which can also be viewed as a probability distribution over S); The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) 791 - A is the set of all possible actions applicable in the environment; - T is the environments probabilistic transition function: T : S ×A → Π(S).",
                "That is, T(s |a, s) is the probability that the environment will move from state s to state s under action a; - O is the set of all possible observations.",
                "This is what the sensor input would look like for an outside observer; - Ω is the observation probability function: Ω : S × A × S → Π(O).",
                "That is, Ω(o|s , a, s) is the probability that one will observe o given that the environment has moved from state s to state s under action a. • User Level, in the case of Markovian environment, operates on the set of system dynamics described by a family of conditional probabilities F = {τ : S × A → Π(S)}.",
                "Thus specification of target dynamics can be expressed by q ∈ F, and the learning or tracking algorithm can be represented as a function L : O×(A×O)∗ → F; that is, it maps sequences of observations and actions performed so far into an estimate τ ∈ F of system dynamics.",
                "There are many possible variations available at the User Level to define divergence between system dynamics; several of them are: - Trace distance or L1 distance between two distributions p and q defined by D(p(·), q(·)) = 1 2 x |p(x) − q(x)| - Fidelity measure of distance F(p(·), q(·)) = x p(x)q(x) - Kullback-Leibler divergence DKL(p(·) q(·)) = x p(x) log p(x) q(x) Notice that the latter two are not actually metrics over the space of possible distributions, but nevertheless have meaningful and important interpretations.",
                "For instance, KullbackLeibler divergence is an important tool of information theory [3] that allows one to measure the price of encoding an information source governed by q, while assuming that it is governed by p. The User Level also defines the threshold of dynamics deviation probability θ. • Agent Level is then faced with a problem of selecting a <br>control</br> signal function a∗ to satisfy a minimization problem as follows: a∗ = arg min a Pr(d(τa, q) > θ) where d(τa, q) is a random variable describing deviation of the dynamics estimate τa, created by L under <br>control</br> signal a, from the ideal dynamics q.",
                "Implicit in this minimization problem is that L is manipulated via the environment, based on the environment model produced by the Environment Design Level. 3.2 DBC View of the State Space It is important to note the complementary view that DBC and POMDPs take on the state space of the environment.",
                "POMDPs regard state as a stationary snap-shot of the environment; whatever attributes of state sequencing one seeks are reached through properties of the <br>control</br> process, in this case reward accumulation.",
                "This can be viewed as if the sequencing of states and the attributes of that sequencing are only introduced by and for the controlling mechanism-the POMDP policy.",
                "DBC concentrates on the underlying principle of state sequencing, the system dynamics.",
                "DBCs target dynamics specification can use the environments state space as a means to describe, discern, and preserve changes that occur within the system.",
                "As a result, DBC has a greater ability to express state sequencing properties, which are grounded in the environment model and its state space definition.",
                "For example, consider the task of moving through rough terrain towards a goal and reaching it as fast as possible.",
                "POMDPs would encode terrain as state space points, while speed would be ensured by negative reward for every step taken without reaching the goalaccumulating higher reward can be reached only by faster motion.",
                "Alternatively, the state space could directly include the notion of speed.",
                "For POMDPs, this would mean that the same concept is encoded twice, in some sense: directly in the state space, and indirectly within reward accumulation.",
                "Now, even if the reward function would encode more, and finer, details of the properties of motion, the POMDP solution will have to search in a much larger space of policies, while still being guided by the implicit concept of the reward accumulation procedure.",
                "On the other hand, the tactical target expression of variations and correlations between position and speed of motion is now grounded in the state space representation.",
                "In this situation, any further constraints, e.g., smoothness of motion, speed limits in different locations, or speed reductions during sharp turns, are explicitly and uniformly expressed by the tactical target, and can result in faster and more effective action selection by a DBC algorithm. 4.",
                "EMT-BASED <br>control</br> AS A DBC Recently, a <br>control</br> algorithm was introduced called EMT-based Control [13], which instantiates the DBC framework.",
                "Although it provides an approximate greedy solution in the DBC sense, initial experiments using EMT-based <br>control</br> have been encouraging [14, 15].",
                "EMT-based <br>control</br> is based on the Markovian environment definition, as in the case with POMDPs, but its User and Agent Levels are of the Markovian DBC type of optimality. • User Level of EMT-based <br>control</br> defines a limited-case target system dynamics independent of action: qEMT : S → Π(S).",
                "It then utilizes the Kullback-Leibler divergence measure to compose a momentary system dynamics estimator-the Extended Markov Tracking (EMT) algorithm.",
                "The algorithm keeps a system dynamics estimate τt EMT that is capable of explaining recent change in an auxiliary Bayesian system state estimator from pt−1 to pt, and updates it conservatively using Kullback-Leibler divergence.",
                "Since τt EMT and pt−1,t are respectively the conditional and marginal probabilities over the systems state space, explanation simply means that pt(s ) = s τt EMT (s |s)pt−1(s), and the dynamics estimate update is performed by solving a 792 The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) Table 1: Structure of POMDP vs. Dynamics-Based <br>control</br> in Markovian Environment Level Approach MDP Markovian DBC Environment < S, A, T, O, Ω >,where S - set of states A - set of actions Design T : S × A → Π(S) - transition O - observation set Ω : S × A × S → Π(O) User r : S × A × S → R q : S × A → Π(S) F(π∗ ) → r L(o1, ..., ot) → τ r - reward function q - ideal dynamics F - reward remodeling L - dynamics estimator θ - threshold Agent π∗ = arg max π E[ γt rt] π∗ = arg min π Prob(d(τ q) > θ) minimization problem: τt EMT = H[pt, pt−1, τt−1 EMT ] = arg min τ DKL(τ × pt−1 τt−1 EMT × pt−1) s.t. pt(s ) = s (τ × pt−1)(s , s) pt−1(s) = s (τ × pt−1)(s , s) • Agent Level in EMT-based <br>control</br> is suboptimal with respect to DBC (though it remains within the DBC framework), performing greedy action selection based on prediction of EMTs reaction.",
                "The prediction is based on the environment model provided by the Environment Design level, so that if we denote by Ta the environments transition function limited to action a, and pt−1 is the auxiliary Bayesian system state estimator, then the EMT-based <br>control</br> choice is described by a∗ = arg min a∈A DKL(H[Ta × pt, pt, τt EMT ] qEMT × pt−1) Note that this follows the Markovian DBC framework precisely: the rewarding optimality of POMDPs is discarded, and in its place a dynamics estimator (EMT in this case) is manipulated via action effects on the environment to produce an estimate close to the specified target system dynamics.",
                "Yet as we mentioned, naive EMTbased <br>control</br> is suboptimal in the DBC sense, and has several additional limitations that do not exist in the general DBC framework (discussed in Section 4.2). 4.1 Multi-Target EMT At times, there may exist several behavioral preferences.",
                "For example, in the case of museum guards, some art items are more heavily guarded, requiring that the guards stay more often in their close vicinity.",
                "On the other hand, no corner of the museum is to be left unchecked, which demands constant motion.",
                "Successful museum security would demand that the guards adhere to, and balance, both of these behaviors.",
                "For EMT-based <br>control</br>, this would mean facing several tactical targets {qk}K k=1, and the question becomes how to merge and balance them.",
                "A balancing mechanism can be applied to resolve this issue.",
                "Note that EMT-based <br>control</br>, while selecting an action, creates a preference vector over the set of actions based on their predicted performance with respect to a given target.",
                "If these preference vectors are normalized, they can be combined into a single unified preference.",
                "This requires replacement of standard EMT-based action selection by the algorithm below [15]: • Given: - a set of target dynamics {qk}K k=1, - vector of weights w(k) • Select action as follows - For each action a ∈ A predict the future state distribution ¯pa t+1 = Ta ∗ pt; - For each action, compute Da = H(¯pa t+1, pt, PDt) - For each a ∈ A and qk tactical target, denote V (a, k) = DKL (Da qk) pt .",
                "Let Vk(a) = 1 Zk V (a, k), where Zk = a∈A V (a, k) is a normalization factor. - Select a∗ = arg min a k k=1 w(k)Vk(a) The weights vector w = (w1, ..., wK ) allows the additional tuning of importance among target dynamics without the need to redesign the targets themselves.",
                "This balancing method is also seamlessly integrated into the EMT-based <br>control</br> flow of operation. 4.2 EMT-based <br>control</br> Limitations EMT-based control is a sub-optimal (in the DBC sense) representative of the DBC structure.",
                "It limits the User by forcing EMT to be its dynamic tracking algorithm, and replaces Agent optimization by greedy action selection.",
                "This kind of combination, however, is common for on-line algorithms.",
                "Although further development of EMT-based controllers is necessary, evidence so far suggests that even the simplest form of the algorithm possesses a great deal of power, and displays trends that are optimal in the DBC sense of the word.",
                "There are two further, EMT-specific, limitations to EMT-based <br>control</br> that are evident at this point.",
                "Both already have partial solutions and are subjects of ongoing research.",
                "The first limitation is the problem of negative preference.",
                "In the POMDP framework for example, this is captured simply, through The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) 793 the appearance of values with different signs within the reward structure.",
                "For EMT-based <br>control</br>, however, negative preference means that one would like to avoid a certain distribution over system development sequences; EMT-based <br>control</br>, however, concentrates on getting as close as possible to a distribution.",
                "Avoidance is thus unnatural in native EMT-based <br>control</br>.",
                "The second limitation comes from the fact that standard environment modeling can create pure sensory actions-actions that do not change the state of the world, and differ only in the way observations are received and the quality of observations received.",
                "Since the world state does not change, EMT-based <br>control</br> would not be able to differentiate between different sensory actions.",
                "Notice that both of these limitations of EMT-based <br>control</br> are absent from the general DBC framework, since it may have a tracking algorithm capable of considering pure sensory actions and, unlike Kullback-Leibler divergence, a distribution deviation measure that is capable of dealing with negative preference. 5.",
                "EMT PLAYING TAG The Game of Tag was first introduced in [11].",
                "It is a single agent problem of capturing a quarry, and belongs to the class of area sweeping problems.",
                "An example domain is shown in Figure 2. 0 51 2 3 4 6 7 8 10 12 13 161514 17 18 19 2221 23 9 11Q A 20 Figure 2: Tag domain; an agent (A) attempts to seek and capture a quarry (Q) The Game of Tag extremely limits the agents perception, so that the agent is able to detect the quarry only if they are co-located in the same cell of the grid world.",
                "In the classical version of the game, co-location leads to a special observation, and the Tag action can be performed.",
                "We slightly modify this setting: the moment that both agents occupy the same cell, the game ends.",
                "As a result, both the agent and its quarry have the same motion capability, which allows them to move in four directions, North, South, East, and West.",
                "These form a formal space of actions within a Markovian environment.",
                "The state space of the formal Markovian environment is described by the cross-product of the agent and quarrys positions.",
                "For Figure 2, it would be S = {s0, ..., s23} × {s0, ..., s23}.",
                "The effects of an action taken by the agent are deterministic, but the environment in general has a stochastic response due to the motion of the quarry.",
                "With probability q0 1 it stays put, and with probability 1 − q0 it moves to an adjacent cell further away from the 1 In our experiments this was taken to be q0 = 0.2. agent.",
                "So for the instance shown in Figure 2 and q0 = 0.1: P(Q = s9|Q = s9, A = s11) = 0.1 P(Q = s2|Q = s9, A = s11) = 0.3 P(Q = s8|Q = s9, A = s11) = 0.3 P(Q = s14|Q = s9, A = s11) = 0.3 Although the evasive behavior of the quarry is known to the agent, the quarrys position is not.",
                "The only sensory information available to the agent is its own location.",
                "We use EMT and directly specify the target dynamics.",
                "For the Game of Tag, we can easily formulate three major trends: catching the quarry, staying mobile, and stalking the quarry.",
                "This results in the following three target dynamics: Tcatch(At+1 = si|Qt = sj, At = sa) ∝ 1 si = sj 0 otherwise Tmobile(At+1 = si|Qt = so, At = sj) ∝ 0 si = sj 1 otherwise Tstalk(At+1 = si|Qt = so, At = sj) ∝ 1 dist(si,so) Note that none of the above targets are directly achievable; for instance, if Qt = s9 and At = s11, there is no action that can move the agent to At+1 = s9 as required by the Tcatch target dynamics.",
                "We ran several experiments to evaluate EMT performance in the Tag Game.",
                "Three configurations of the domain shown in Figure 3 were used, each posing a different challenge to the agent due to partial observability.",
                "In each setting, a set of 1000 runs was performed with a time limit of 100 steps.",
                "In every run, the initial position of both the agent and its quarry was selected at random; this means that as far as the agent was concerned, the quarrys initial position was uniformly distributed over the entire domain cell space.",
                "We also used two variations of the environment observability function.",
                "In the first version, observability function mapped all joint positions of hunter and quarry into the position of the hunter as an observation.",
                "In the second, only those joint positions in which hunter and quarry occupied different locations were mapped into the hunters location.",
                "The second version in fact utilized and expressed the fact that once hunter and quarry occupy the same cell the game ends.",
                "The results of these experiments are shown in Table 2.",
                "Balancing [15] the catch, move, and stalk target dynamics described in the previous section by the weight vector [0.8, 0.1, 0.1], EMT produced stable performance in all three domains.",
                "Although direct comparisons are difficult to make, the EMT performance displayed notable efficiency vis-`a-vis the POMDP approach.",
                "In spite of a simple and inefficient Matlab implementation of the EMT algorithm, the decision time for any given step averaged significantly below 1 second in all experiments.",
                "For the irregular open arena domain, which proved to be the most difficult, 1000 experiment runs bounded by 100 steps each, a total of 42411 steps, were completed in slightly under 6 hours.",
                "That is, over 4 × 104 online steps took an order of magnitude less time than the offline computation of POMDP policy in [11].",
                "The significance of this differential is made even more prominent by the fact that, should the environment model parameters change, the online nature of EMT would allow it to maintain its performance time, while the POMDP policy would need to be recomputed, requiring yet again a large overhead of computation.",
                "We also tested the behavior cell frequency entropy, empirical measures from trial data.",
                "As Figure 4 and Figure 5 show, empir794 The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) A Q Q A 0 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 A Q Figure 3: These configurations of the Tag Game space were used: a) multiple dead-end, b) irregular open arena, c) circular corridor Table 2: Performance of the EMT-based solution in three Tag Game domains and two observability models: I) omniposition quarry, II) quarry is not at hunters position Model Domain Capture% E(Steps) Time/Step I Dead-ends 100 14.8 72(mSec) Arena 80.2 42.4 500(mSec) Circle 91.4 34.6 187(mSec) II Dead-ends 100 13.2 91(mSec) Arena 96.8 28.67 396(mSec) Circle 94.4 31.63 204(mSec) ical entropy grows with the length of interaction.",
                "For runs where the quarry was not captured immediately, the entropy reaches between 0.85 and 0.952 for different runs and scenarios.",
                "As the agent actively seeks the quarry, the entropy never reaches its maximum.",
                "One characteristic of the entropy graph for the open arena scenario particularly caught our attention in the case of the omniposition quarry observation model.",
                "Near the maximum limit of trial length (100 steps), entropy suddenly dropped.",
                "Further analysis of the data showed that under certain circumstances, a fluctuating behavior occurs in which the agent faces equally viable versions of quarry-following behavior.",
                "Since the EMT algorithm has greedy action selection, and the state space does not encode any form of commitment (not even speed or acceleration), the agent is locked within a small portion of cells.",
                "It is essentially attempting to simultaneously follow several courses of action, all of which are consistent with the target dynamics.",
                "This behavior did not occur in our second observation model, since it significantly reduced the set of eligible courses of action-essentially contributing to tie-breaking among them. 6.",
                "DISCUSSION The design of the EMT solution for the Tag Game exposes the core difference in approach to planning and <br>control</br> between EMT or DBC, on the one hand, and the more familiar POMDP approach, on the other.",
                "POMDP defines a reward structure to optimize, and influences system dynamics indirectly through that optimization.",
                "EMT discards any reward scheme, and instead measures and influences system dynamics directly. 2 Entropy was calculated using log base equal to the number of possible locations within the domain; this properly scales entropy expression into the range [0, 1] for all domains.",
                "Thus for the Tag Game, we did not search for a reward function that would encode and express our preference over the agents behavior, but rather directly set three (heuristic) behavior preferences as the basis for target dynamics to be maintained.",
                "Experimental data shows that these targets need not be directly achievable via the agents actions.",
                "However, the ratio between EMT performance and achievability of target dynamics remains to be explored.",
                "The tag game experiment data also revealed the different emphasis DBC and POMDPs place on the formulation of an environment state space.",
                "POMDPs rely entirely on the mechanism of reward accumulation maximization, i.e., formation of the action selection procedure to achieve necessary state sequencing.",
                "DBC, on the other hand, has two sources of sequencing specification: through the properties of an action selection procedure, and through direct specification within the target dynamics.",
                "The importance of the second source was underlined by the Tag Game experiment data, in which the greedy EMT algorithm, applied to a POMDP-type state space specification, failed, since target description over such a state space was incapable of encoding the necessary behavior tendencies, e.g., tie-breaking and commitment to directed motion.",
                "The structural differences between DBC (and EMT in particular), and POMDPs, prohibits direct performance comparison, and places them on complementary tracks, each within a suitable niche.",
                "For instance, POMDPs could be seen as a much more natural formulation of economic sequential decision-making problems, while EMT is a better fit for continual demand for stochastic change, as happens in many robotic or embodied-agent problems.",
                "The complementary properties of POMDPs and EMT can be further exploited.",
                "There is recent interest in using POMDPs in hybrid solutions [17], in which the POMDPs can be used together with other <br>control</br> approaches to provide results not easily achievable with either approach by itself.",
                "DBC can be an effective partner in such a hybrid solution.",
                "For instance, POMDPs have prohibitively large off-line time requirements for policy computation, but can be readily used in simpler settings to expose beneficial behavioral trends; this can serve as a form of target dynamics that are provided to EMT in a larger domain for on-line operation. 7.",
                "CONCLUSIONS AND FUTURE WORK In this paper, we have presented a novel perspective on the process of planning and <br>control</br> in stochastic environments, in the form of the Dynamics Based <br>control</br> (DBC) framework.",
                "DBC formulates the task of planning as support of a specified target system dynamics, which describes the necessary properties of change within the environment.",
                "Optimality of DBC plans of action are measured The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) 795 0 20 40 60 80 100 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1 Steps Entropy Dead−ends 0 20 40 60 80 100 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1 Steps Entropy Arena 0 20 40 60 80 100 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1 Steps Entropy Circle Figure 4: Observation Model I: Omniposition quarry.",
                "Entropy development with length of Tag Game for the three experiment scenarios: a) multiple dead-end, b) irregular open arena, c) circular corridor. 0 10 20 30 40 50 60 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1 Steps Entropy Dead−ends 0 20 40 60 80 100 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1 Steps Entropy Arena 0 20 40 60 80 100 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1 Steps Entropy Circle Figure 5: Observation Model II: quarry not observed at hunters position.",
                "Entropy development with length of Tag Game for the three experiment scenarios: a) multiple dead-end, b) irregular open arena, c) circular corridor. with respect to the deviation of actual system dynamics from the target dynamics.",
                "We show that a recently developed technique of Extended Markov Tracking (EMT) [13] is an instantiation of DBC.",
                "In fact, EMT can be seen as a specific case of DBC parameterization, which employs a greedy action selection procedure.",
                "Since EMT exhibits the key features of the general DBC framework, as well as polynomial time complexity, we used the multitarget version of EMT [15] to demonstrate that the class of area sweeping problems naturally lends itself to dynamics-based descriptions, as instantiated by our experiments in the Tag Game domain.",
                "As enumerated in Section 4.2, EMT has a number of limitations, such as difficulty in dealing with negative dynamic preference.",
                "This prevents direct application of EMT to such problems as Rendezvous-Evasion Games (e.g., [6]).",
                "However, DBC in general has no such limitations, and readily enables the formulation of evasion games.",
                "In future work, we intend to proceed with the development of dynamics-based controllers for these problems. 8.",
                "ACKNOWLEDGMENT The work of the first two authors was partially supported by Israel Science Foundation grant #898/05, and the third author was partially supported by a grant from Israels Ministry of Science and Technology. 9.",
                "REFERENCES [1] R. C. Arkin.",
                "Behavior-Based Robotics.",
                "MIT Press, 1998. [2] J.",
                "A. Bilmes.",
                "A gentle tutorial of the EM algorithm and its application to parameter estimation for Gaussian mixture and Hidden Markov Models.",
                "Technical Report TR-97-021, Department of Electrical Engeineering and Computer Science, University of California at Berkeley, 1998. [3] T. M. Cover and J.",
                "A. Thomas.",
                "Elements of information theory.",
                "Wiley, 1991. [4] M. E. desJardins, E. H. Durfee, C. L. Ortiz, and M. J. Wolverton.",
                "A survey of research in distributed, continual planning.",
                "AI Magazine, 4:13-22, 1999. [5] V. R. Konda and J. N. Tsitsiklis.",
                "Actor-Critic algorithms.",
                "SIAM Journal on <br>control</br> and Optimization, 42(4):1143-1166, 2003. [6] W. S. Lim.",
                "A rendezvous-evasion game on discrete locations with joint randomization.",
                "Advances in Applied Probability, 29(4):1004-1017, December 1997. [7] M. L. Littman, T. L. Dean, and L. P. Kaelbling.",
                "On the complexity of solving Markov decision problems.",
                "In Proceedings of the 11th Annual Conference on Uncertainty in Artificial Intelligence (UAI-95), pages 394-402, 1995. [8] O. Madani, S. Hanks, and A. Condon.",
                "On the undecidability of probabilistic planning and related stochastic optimization problems.",
                "Artificial Intelligence Journal, 147(1-2):5-34, July 2003. [9] R. M. Neal and G. E. Hinton.",
                "A view of the EM algorithm 796 The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) that justifies incremental, sparse, and other variants.",
                "In M. I. Jordan, editor, Learning in Graphical Models, pages 355-368.",
                "Kluwer Academic Publishers, 1998. [10] P. Paruchuri, M. Tambe, F. Ordonez, and S. Kraus.",
                "Security in multiagent systems by policy randomization.",
                "In Proceeding of AAMAS 2006, 2006. [11] J. Pineau, G. Gordon, and S. Thrun.",
                "Point-based value iteration: An anytime algorithm for pomdps.",
                "In International Joint Conference on Artificial Intelligence (IJCAI), pages 1025-1032, August 2003. [12] M. L. Puterman.",
                "Markov Decision Processes.",
                "Wiley Series in Probability and Mathematical Statistics: Applied Probability and Statistics Section.",
                "Wiley-Interscience Publication, New York, 1994. [13] Z. Rabinovich and J. S. Rosenschein.",
                "Extended Markov Tracking with an application to <br>control</br>.",
                "In The Workshop on Agent Tracking: Modeling Other Agents from Observations, at the Third International Joint Conference on Autonomous Agents and Multiagent Systems, pages 95-100, New-York, July 2004. [14] Z. Rabinovich and J. S. Rosenschein.",
                "Multiagent coordination by Extended Markov Tracking.",
                "In The Fourth International Joint Conference on Autonomous Agents and Multiagent Systems, pages 431-438, Utrecht, The Netherlands, July 2005. [15] Z. Rabinovich and J. S. Rosenschein.",
                "On the response of EMT-based <br>control</br> to interacting targets and models.",
                "In The Fifth International Joint Conference on Autonomous Agents and Multiagent Systems, pages 465-470, Hakodate, Japan, May 2006. [16] R. F. Stengel.",
                "Optimal <br>control</br> and Estimation.",
                "Dover Publications, 1994. [17] M. Tambe, E. Bowring, H. Jung, G. Kaminka, R. Maheswaran, J. Marecki, J. Modi, R. Nair, J. Pearce, P. Paruchuri, D. Pynadath, P. Scerri, N. Schurr, and P. Varakantham.",
                "Conflicts in teamwork: Hybrids to the The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) 797"
            ],
            "original_annotated_samples": [],
            "translated_annotated_samples": [
                "\"Control\" basado en la dinámica con una aplicación a los problemas de barrido del área Zinovi Rabinovich Ingeniería y Ciencias de la Computación Universidad Hebrea de Jerusalén Jerusalén, Israel nomad@cs.huji.ac.il Jeffrey S. Rosenschein Ingeniería y ciencias de la informática Universidad Hebrea de Jerusalén Jerusalem, Israel ISRAELELELELELELjeff@cs.huji.ac.il Gal A. Kaminka El Departamento de Barra de Ciencias de la Computación del Grupo Maverick Ilan University, Israel galk@cs.biu.ac.il Resumen En este documento presentamos \"Control\" basado en la dinámica (DBC), unEnfoque para la planificación y el control de un agente en entornos estocásticos.",
                "Mostramos que un enfoque de planificación y \"control\" recientemente desarrollado, el seguimiento extendido de Markov (EMT) es una instanciación de DBC.",
                "EMT emplea la selección de acción codiciosa para proporcionar un algoritmo eficiente de \"control\" en los entornos de Markovian.",
                "Categorías y descriptores de sujetos I.2.8 [Resolución de problemas, métodos de \"control\" y búsqueda]: teoría de \"control\";I.2.9 [Robótica];I.2.11 [Inteligencia artificial distribuida]: Algoritmos de términos generales de agentes inteligentes, teoría 1.",
                "La planificación de la introducción y el \"control\" constituye un área de investigación central en sistemas múltiples e inteligencia artificial.",
                "En este marco, el problema de planificación y \"control\" a menudo se aborda imponiendo una función de recompensa y calculando una política (de elegir acciones) que sea óptimo, en el sentido de que dará como resultado la utilidad más alta esperada.",
                "Llamamos a este \"control\" (DBC) basado en la dinámica de planificación general.",
                "En DBC, el objetivo de un proceso de planificación (o \"control\") se convierte en garantizar que el sistema cambie de acuerdo con la dinámica objetivo específica (potencialmente estocástica).",
                "En este artículo, presentamos la estructura del \"control\" basado en la dinámica.",
                "La Sección 3 introduce la estructura de \"control\" (DBC) basada en la dinámica, y su especialización a los entornos de Markovian.",
                "Esto es seguido por una revisión del enfoque de seguimiento de Markov (EMT) extendido como un régimen de \"control\" estructurado por DBC en la Sección 4.",
                "Esa sección también analiza las limitaciones del \"control\" basado en EMT en relación con el marco DBC general.",
                "El \"control\" basado en la dinámica proporciona un enfoque natural para resolver estos problemas.3.",
                "El \"control\" basado en la dinámica, la especificación del \"control\" (DBC) basado en la dinámica (DBC) se puede dividir en tres niveles de interacción: nivel de diseño del entorno, nivel de usuario y nivel de agente.• El nivel de diseño del medio ambiente se refiere a la especificación formal y el modelado del entorno.",
                "Como estamos interesados en el desarrollo continuo de un sistema estocástico, como sucede en la teoría clásica de \"control\" [16] y la planificación continua [4], así como en nuestro ejemplo de barridos de museos, la pregunta se convierte en cómo es el nivel de agentePara tratar las mediciones de desviación con el tiempo.",
                "Por ejemplo, la divergencia de Kullbackleibler es una herramienta importante de la teoría de la información [3] que le permite a uno medir el precio de codificar una fuente de información regida por Q, al tiempo que supone que se rige por p.El nivel de usuario también define el umbral de la probabilidad de desviación dinámica θ.• El nivel de agente se enfrenta con el problema de seleccionar una función de señal de \"control\" A ∗ para satisfacer un problema de minimización de la siguiente) es una variable aleatoria que describe la desviación de la estimación de la dinámica τa, creada por L bajo la señal de \"control\" A, a partir de la dinámica ideal q.",
                "Los POMDP consideran el estado estacionario como un instantáneo del entorno;Cualesquiera que sean los atributos de la secuencia de estado que se busca a través de propiedades del proceso de \"control\", en este caso la acumulación de recompensas.",
                "\"Control\" basado en EMT como DBC recientemente, se introdujo un algoritmo de \"control\" llamado control basado en EMT [13], que instancias el marco DBC.",
                "Aunque proporciona una solución codiciosa aproximada en el sentido de DBC, los experimentos iniciales que utilizan \"control\" basado en EMT han sido alentadores [14, 15].",
                "El \"control\" basado en EMT se basa en la definición del entorno de Markovian, como en el caso de POMDPS, pero sus niveles de usuario y agente son del tipo de optimización de Markovian DBC.• El nivel de usuario del \"control\" basado en EMT define una dinámica del sistema de destino de caso limitado independientemente de la acción: QEMT: S → π (S).",
                "Conf.en agentes autónomos y sistemas de múltiples agentes (AAMAS 07) Tabla 1: Estructura de POMDP versus \"Control\" basado en la dinámica en el enfoque de nivel de entorno Markovian MDP Markovian DBC Environment <S, A, T, O, ω>, donde s-conjunto de estados a - conjunto de acciones diseño t: s × a → π (s) - transición o - observación conjunto Ω: s × a × s → π (o) usuario r: s × a × s → r q: s× a → π (s) f (π ∗) → r l (o1, ..., ot) → τ r - función de recompensa q - dinámica ideal f - remodelación de recompensa l - estimador de dinámica θ - agente de umbral π ∗ = arg = argmax π e [γt rt] π ∗ = arg min π prob (d (τ q)> θ) Problema de minimización: τt emt = h [pt, pt - 1, τt - 1 emt] = arg min τ dkl (τ ×PT - 1 τt - 1 EMT × PT - 1) S.T.pt (s) = s (τ × pt-1) (s, s, s) pt-1 (s) = s (τ × pt-1) (s, s) • El nivel de agente en el \"control\" basado en EMT es subóptimoCon respecto a DBC (aunque permanece dentro del marco DBC), realizando una selección de acción codiciosa basada en la predicción de la reacción de EMTS.",
                "La predicción se basa en el modelo de entorno proporcionado por el nivel de diseño del entorno, de modo que si denotamos por TA la función de transición de los entornos limitados a la acción A, y PT-1 es el estimador de estado del sistema bayesiano auxiliar, entonces el \"control de control basado en EMT\"La elección se describe mediante un ∗ = arg min a∈A dkl (h [ta × pt, pt, τt emt] qemt × pt - 1) Tenga en cuenta que esto sigue el marco de DBC de Markovian con precisión: la optimización gratificante de POMDPS se descarta, se descarta,y en su lugar, se manipula un estimador de dinámica (EMT en este caso) mediante efectos de acción en el entorno para producir una estimación cercana a la dinámica del sistema objetivo especificada.",
                "Sin embargo, como mencionamos, el \"control\" basado en EMT ingenuo es subóptimo en el sentido de DBC, y tiene varias limitaciones adicionales que no existen en el marco DBC general (discutido en la Sección 4.2).4.1 EMT multi-objetivo A veces, puede existir varias preferencias de comportamiento.",
                "Para el \"control\" basado en EMT, esto significaría enfrentar varios objetivos tácticos {Qk} k k = 1, y la pregunta es cómo fusionarlos y equilibrarlos.",
                "Tenga en cuenta que el \"control\" basado en EMT, al seleccionar una acción, crea un vector de preferencia sobre el conjunto de acciones basadas en su rendimiento previsto con respecto a un objetivo dado.",
                "Este método de equilibrio también se integra perfectamente en el flujo de operación de \"control\" basado en EMT.4.2 Limitaciones de \"control\" basadas en EMT El control basado en EMT es un representante subóptimo (en el sentido de DBC) de la estructura DBC.",
                "Hay dos limitaciones más específicas de EMT al \"control\" basado en EMT que son evidentes en este momento.",
                "Sin embargo, para el \"control\" basado en EMT, la preferencia negativa significa que uno le gustaría evitar una cierta distribución sobre las secuencias de desarrollo del sistema;Sin embargo, el \"control\" basado en EMT se concentra en acercarse lo más posible a una distribución.",
                "La evitación no es natural en el \"control\" nativo basado en EMT.",
                "Dado que el estado mundial no cambia, el \"control\" basado en EMT no podría diferenciar entre diferentes acciones sensoriales.",
                "Observe que ambas limitaciones del \"control\" basada en EMT están ausentes del marco DBC general, ya que puede tener un algoritmo de seguimiento capaz de considerar las acciones sensoriales puras y, a diferencia de la divergencia de Kullback-Lebler, una medida de desviación de distribución que es capaz de ser capaz delidiar con preferencia negativa.5.",
                "Discusión El diseño de la solución EMT para el juego de etiquetas expone la diferencia central en el enfoque de la planificación y el \"control\" entre EMT o DBC, por un lado, y el enfoque POMDP más familiar, por el otro.",
                "Existe un interés reciente en el uso de POMDPS en soluciones híbridas [17], en el que los POMDP pueden usarse junto con otros enfoques de \"control\" para proporcionar resultados que no se pueden lograr fácilmente con ninguno de los enfoques por sí mismo.",
                "Conclusiones y trabajo futuro En este documento, hemos presentado una perspectiva novedosa sobre el proceso de planificación y \"control\" en entornos estocásticos, en forma del marco \"Control\" (DBC) basado en la dinámica.",
                "Siam Journal on \"Control\" y Optimization, 42 (4): 1143-1166, 2003. [6] W. S. Lim.",
                "Seguimiento extendido de Markov con una aplicación para \"control\".",
                "Sobre la respuesta del \"control\" basado en EMT a los objetivos y modelos interactuantes.",
                "\"Control\" y estimación óptimos."
            ],
            "translated_text": "",
            "candidates": [
                "control",
                "Control",
                "Control",
                "control",
                "control",
                "control",
                "control",
                "Control",
                "control",
                "control",
                "control",
                "control",
                "control",
                "control",
                "control",
                "control",
                "control",
                "control",
                "control",
                "control",
                "control",
                "control",
                "Control",
                "control",
                "control",
                "control",
                "Control",
                "control",
                "control",
                "control",
                "control",
                "control",
                "control",
                "control",
                "control",
                "control",
                "control",
                "control",
                "control",
                "Control",
                "control",
                "control",
                "control",
                "control",
                "control",
                "control",
                "control",
                "Control",
                "control",
                "control",
                "control de control basado en EMT",
                "control",
                "control",
                "control",
                "control",
                "control",
                "control",
                "control",
                "control",
                "control",
                "control",
                "control",
                "control",
                "control",
                "control",
                "control",
                "control",
                "control",
                "control",
                "Control",
                "control",
                "control",
                "control",
                "control",
                "control",
                "control",
                "control",
                "Control",
                "control",
                "Control",
                "control",
                "control",
                "control",
                "control",
                "control",
                "Control"
            ],
            "error": []
        },
        "robotic": {
            "translated_key": "robótico",
            "is_in_text": true,
            "original_annotated_sentences": [
                "Dynamics Based Control with an Application to Area-Sweeping Problems Zinovi Rabinovich Engineering and Computer Science Hebrew University of Jerusalem Jerusalem, Israel nomad@cs.huji.ac.il Jeffrey S. Rosenschein Engineering and Computer Science Hebrew University of Jerusalem Jerusalem, Israel jeff@cs.huji.ac.il Gal A. Kaminka The MAVERICK Group Department of Computer Science Bar Ilan University, Israel galk@cs.biu.ac.il ABSTRACT In this paper we introduce Dynamics Based Control (DBC), an approach to planning and control of an agent in stochastic environments.",
                "Unlike existing approaches, which seek to optimize expected rewards (e.g., in Partially Observable Markov Decision Problems (POMDPs)), DBC optimizes system behavior towards specified system dynamics.",
                "We show that a recently developed planning and control approach, Extended Markov Tracking (EMT) is an instantiation of DBC.",
                "EMT employs greedy action selection to provide an efficient control algorithm in Markovian environments.",
                "We exploit this efficiency in a set of experiments that applied multitarget EMT to a class of area-sweeping problems (searching for moving targets).",
                "We show that such problems can be naturally defined and efficiently solved using the DBC framework, and its EMT instantiation.",
                "Categories and Subject Descriptors I.2.8 [Problem Solving, Control Methods, and Search]: Control Theory; I.2.9 [Robotics]; I.2.11 [Distributed Artificial Intelligence]: Intelligent Agents General Terms Algorithms, Theory 1.",
                "INTRODUCTION Planning and control constitutes a central research area in multiagent systems and artificial intelligence.",
                "In recent years, Partially Observable Markov Decision Processes (POMDPs) [12] have become a popular formal basis for planning in stochastic environments.",
                "In this framework, the planning and control problem is often addressed by imposing a reward function, and computing a policy (of choosing actions) that is optimal, in the sense that it will result in the highest expected utility.",
                "While theoretically attractive, the complexity of optimally solving a POMDP is prohibitive [8, 7].",
                "We take an alternative view of planning in stochastic environments.",
                "We do not use a (state-based) reward function, but instead optimize over a different criterion, a transition-based specification of the desired system dynamics.",
                "The idea here is to view planexecution as a process that compels a (stochastic) system to change, and a plan as a dynamic process that shapes that change according to desired criteria.",
                "We call this general planning framework Dynamics Based Control (DBC).",
                "In DBC, the goal of a planning (or control) process becomes to ensure that the system will change in accordance with specific (potentially stochastic) target dynamics.",
                "As actual system behavior may deviate from that which is specified by target dynamics (due to the stochastic nature of the system), planning in such environments needs to be continual [4], in a manner similar to classical closed-loop controllers [16].",
                "Here, optimality is measured in terms of probability of deviation magnitudes.",
                "In this paper, we present the structure of Dynamics Based Control.",
                "We show that the recently developed Extended Markov Tracking (EMT) approach [13, 14, 15] is subsumed by DBC, with EMT employing greedy action selection, which is a specific parameterization among the options possible within DBC.",
                "EMT is an efficient instantiation of DBC.",
                "To evaluate DBC, we carried out a set of experiments applying multi-target EMT to the Tag Game [11]; this is a variant on the area sweeping problem, where an agent is trying to tag a moving target (quarry) whose position is not known with certainty.",
                "Experimental data demonstrates that even with a simple model of the environment and a simple design of target dynamics, high success rates can be produced both in catching the quarry, and in surprising the quarry (as expressed by the observed entropy of the controlled agents position).",
                "The paper is organized as follows.",
                "In Section 2 we motivate DBC using area-sweeping problems, and discuss related work.",
                "Section 3 introduces the Dynamics Based Control (DBC) structure, and its specialization to Markovian environments.",
                "This is followed by a review of the Extended Markov Tracking (EMT) approach as a DBC-structured control regimen in Section 4.",
                "That section also discusses the limitations of EMT-based control relative to the general DBC framework.",
                "Experimental settings and results are then presented in Section 5.",
                "Section 6 provides a short discussion of the overall approach, and Section 7 gives some concluding remarks and directions for future work. 790 978-81-904262-7-5 (RPS) c 2007 IFAAMAS 2.",
                "MOTIVATION AND RELATED WORK Many real-life scenarios naturally have a stochastic target dynamics specification, especially those domains where there exists no ultimate goal, but rather system behavior (with specific properties) that has to be continually supported.",
                "For example, security guards perform persistent sweeps of an area to detect any sign of intrusion.",
                "Cunning thieves will attempt to track these sweeps, and time their operation to key points of the guards motion.",
                "It is thus advisable to make the guards motion dynamics appear irregular and random.",
                "Recent work by Paruchuri et al. [10] has addressed such randomization in the context of single-agent and distributed POMDPs.",
                "The goal in that work was to generate policies that provide a measure of action-selection randomization, while maintaining rewards within some acceptable levels.",
                "Our focus differs from this work in that DBC does not optimize expected rewards-indeed we do not consider rewards at all-but instead maintains desired dynamics (including, but not limited to, randomization).",
                "The Game of Tag is another example of the applicability of the approach.",
                "It was introduced in the work by Pineau et al. [11].",
                "There are two agents that can move about an area, which is divided into a grid.",
                "The grid may have blocked cells (holes) into which no agent can move.",
                "One agent (the hunter) seeks to move into a cell occupied by the other (the quarry), such that they are co-located (this is a successful tag).",
                "The quarry seeks to avoid the hunter agent, and is always aware of the hunters position, but does not know how the hunter will behave, which opens up the possibility for a hunter to surprise the prey.",
                "The hunter knows the quarrys probabilistic law of motion, but does not know its current location.",
                "Tag is an instance of a family of area-sweeping (pursuit-evasion) problems.",
                "In [11], the hunter modeled the problem using a POMDP.",
                "A reward function was defined, to reflect the desire to tag the quarry, and an action policy was computed to optimize the reward collected over time.",
                "Due to the intractable complexity of determining the optimal policy, the action policy computed in that paper was essentially an approximation.",
                "In this paper, instead of formulating a reward function, we use EMT to solve the problem, by directly specifying the target dynamics.",
                "In fact, any search problem with randomized motion, the socalled class of area sweeping problems, can be described through specification of such target system dynamics.",
                "Dynamics Based Control provides a natural approach to solving these problems. 3.",
                "DYNAMICS BASED CONTROL The specification of Dynamics Based Control (DBC) can be broken into three interacting levels: Environment Design Level, User Level, and Agent Level. • Environment Design Level is concerned with the formal specification and modeling of the environment.",
                "For example, this level would specify the laws of physics within the system, and set its parameters, such as the gravitation constant. • User Level in turn relies on the environment model produced by Environment Design to specify the target system dynamics it wishes to observe.",
                "The User Level also specifies the estimation or learning procedure for system dynamics, and the measure of deviation.",
                "In the museum guard scenario above, these would correspond to a stochastic sweep schedule, and a measure of relative surprise between the specified and actual sweeping. • Agent Level in turn combines the environment model from the Environment Design level, the dynamics estimation procedure, the deviation measure and the target dynamics specification from User Level, to produce a sequence of actions that create system dynamics as close as possible to the targeted specification.",
                "As we are interested in the continual development of a stochastic system, such as happens in classical control theory [16] and continual planning [4], as well as in our example of museum sweeps, the question becomes how the Agent Level is to treat the deviation measurements over time.",
                "To this end, we use a probability threshold-that is, we would like the Agent Level to maximize the probability that the deviation measure will remain below a certain threshold.",
                "Specific action selection then depends on system formalization.",
                "One possibility would be to create a mixture of available system trends, much like that which happens in Behavior-Based <br>robotic</br> architectures [1].",
                "The other alternative would be to rely on the estimation procedure provided by the User Level-to utilize the Environment Design Level model of the environment to choose actions, so as to manipulate the dynamics estimator into believing that a certain dynamics has been achieved.",
                "Notice that this manipulation is not direct, but via the environment.",
                "Thus, for strong enough estimator algorithms, successful manipulation would mean a successful simulation of the specified target dynamics (i.e., beyond discerning via the available sensory input).",
                "DBC levels can also have a back-flow of information (see Figure 1).",
                "For instance, the Agent Level could provide data about target dynamics feasibility, allowing the User Level to modify the requirement, perhaps focusing on attainable features of system behavior.",
                "Data would also be available about the system response to different actions performed; combined with a dynamics estimator defined by the User Level, this can provide an important tool for the environment model calibration at the Environment Design Level.",
                "UserEnv.",
                "Design Agent Model Ideal Dynamics Estimator Estimator Dynamics Feasibility System Response Data Figure 1: Data flow of the DBC framework Extending upon the idea of Actor-Critic algorithms [5], DBC data flow can provide a good basis for the design of a learning algorithm.",
                "For example, the User Level can operate as an exploratory device for a learning algorithm, inferring an ideal dynamics target from the environment model at hand that would expose and verify most critical features of system behavior.",
                "In this case, feasibility and system response data from the Agent Level would provide key information for an environment model update.",
                "In fact, the combination of feasibility and response data can provide a basis for the application of strong learning algorithms such as EM [2, 9]. 3.1 DBC for Markovian Environments For a Partially Observable Markovian Environment, DBC can be specified in a more rigorous manner.",
                "Notice how DBC discards rewards, and replaces it by another optimality criterion (structural differences are summarized in Table 1): • Environment Design level is to specify a tuple < S, A, T, O, Ω, s0 >, where: - S is the set of all possible environment states; - s0 is the initial state of the environment (which can also be viewed as a probability distribution over S); The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) 791 - A is the set of all possible actions applicable in the environment; - T is the environments probabilistic transition function: T : S ×A → Π(S).",
                "That is, T(s |a, s) is the probability that the environment will move from state s to state s under action a; - O is the set of all possible observations.",
                "This is what the sensor input would look like for an outside observer; - Ω is the observation probability function: Ω : S × A × S → Π(O).",
                "That is, Ω(o|s , a, s) is the probability that one will observe o given that the environment has moved from state s to state s under action a. • User Level, in the case of Markovian environment, operates on the set of system dynamics described by a family of conditional probabilities F = {τ : S × A → Π(S)}.",
                "Thus specification of target dynamics can be expressed by q ∈ F, and the learning or tracking algorithm can be represented as a function L : O×(A×O)∗ → F; that is, it maps sequences of observations and actions performed so far into an estimate τ ∈ F of system dynamics.",
                "There are many possible variations available at the User Level to define divergence between system dynamics; several of them are: - Trace distance or L1 distance between two distributions p and q defined by D(p(·), q(·)) = 1 2 x |p(x) − q(x)| - Fidelity measure of distance F(p(·), q(·)) = x p(x)q(x) - Kullback-Leibler divergence DKL(p(·) q(·)) = x p(x) log p(x) q(x) Notice that the latter two are not actually metrics over the space of possible distributions, but nevertheless have meaningful and important interpretations.",
                "For instance, KullbackLeibler divergence is an important tool of information theory [3] that allows one to measure the price of encoding an information source governed by q, while assuming that it is governed by p. The User Level also defines the threshold of dynamics deviation probability θ. • Agent Level is then faced with a problem of selecting a control signal function a∗ to satisfy a minimization problem as follows: a∗ = arg min a Pr(d(τa, q) > θ) where d(τa, q) is a random variable describing deviation of the dynamics estimate τa, created by L under control signal a, from the ideal dynamics q.",
                "Implicit in this minimization problem is that L is manipulated via the environment, based on the environment model produced by the Environment Design Level. 3.2 DBC View of the State Space It is important to note the complementary view that DBC and POMDPs take on the state space of the environment.",
                "POMDPs regard state as a stationary snap-shot of the environment; whatever attributes of state sequencing one seeks are reached through properties of the control process, in this case reward accumulation.",
                "This can be viewed as if the sequencing of states and the attributes of that sequencing are only introduced by and for the controlling mechanism-the POMDP policy.",
                "DBC concentrates on the underlying principle of state sequencing, the system dynamics.",
                "DBCs target dynamics specification can use the environments state space as a means to describe, discern, and preserve changes that occur within the system.",
                "As a result, DBC has a greater ability to express state sequencing properties, which are grounded in the environment model and its state space definition.",
                "For example, consider the task of moving through rough terrain towards a goal and reaching it as fast as possible.",
                "POMDPs would encode terrain as state space points, while speed would be ensured by negative reward for every step taken without reaching the goalaccumulating higher reward can be reached only by faster motion.",
                "Alternatively, the state space could directly include the notion of speed.",
                "For POMDPs, this would mean that the same concept is encoded twice, in some sense: directly in the state space, and indirectly within reward accumulation.",
                "Now, even if the reward function would encode more, and finer, details of the properties of motion, the POMDP solution will have to search in a much larger space of policies, while still being guided by the implicit concept of the reward accumulation procedure.",
                "On the other hand, the tactical target expression of variations and correlations between position and speed of motion is now grounded in the state space representation.",
                "In this situation, any further constraints, e.g., smoothness of motion, speed limits in different locations, or speed reductions during sharp turns, are explicitly and uniformly expressed by the tactical target, and can result in faster and more effective action selection by a DBC algorithm. 4.",
                "EMT-BASED CONTROL AS A DBC Recently, a control algorithm was introduced called EMT-based Control [13], which instantiates the DBC framework.",
                "Although it provides an approximate greedy solution in the DBC sense, initial experiments using EMT-based control have been encouraging [14, 15].",
                "EMT-based control is based on the Markovian environment definition, as in the case with POMDPs, but its User and Agent Levels are of the Markovian DBC type of optimality. • User Level of EMT-based control defines a limited-case target system dynamics independent of action: qEMT : S → Π(S).",
                "It then utilizes the Kullback-Leibler divergence measure to compose a momentary system dynamics estimator-the Extended Markov Tracking (EMT) algorithm.",
                "The algorithm keeps a system dynamics estimate τt EMT that is capable of explaining recent change in an auxiliary Bayesian system state estimator from pt−1 to pt, and updates it conservatively using Kullback-Leibler divergence.",
                "Since τt EMT and pt−1,t are respectively the conditional and marginal probabilities over the systems state space, explanation simply means that pt(s ) = s τt EMT (s |s)pt−1(s), and the dynamics estimate update is performed by solving a 792 The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) Table 1: Structure of POMDP vs. Dynamics-Based Control in Markovian Environment Level Approach MDP Markovian DBC Environment < S, A, T, O, Ω >,where S - set of states A - set of actions Design T : S × A → Π(S) - transition O - observation set Ω : S × A × S → Π(O) User r : S × A × S → R q : S × A → Π(S) F(π∗ ) → r L(o1, ..., ot) → τ r - reward function q - ideal dynamics F - reward remodeling L - dynamics estimator θ - threshold Agent π∗ = arg max π E[ γt rt] π∗ = arg min π Prob(d(τ q) > θ) minimization problem: τt EMT = H[pt, pt−1, τt−1 EMT ] = arg min τ DKL(τ × pt−1 τt−1 EMT × pt−1) s.t. pt(s ) = s (τ × pt−1)(s , s) pt−1(s) = s (τ × pt−1)(s , s) • Agent Level in EMT-based control is suboptimal with respect to DBC (though it remains within the DBC framework), performing greedy action selection based on prediction of EMTs reaction.",
                "The prediction is based on the environment model provided by the Environment Design level, so that if we denote by Ta the environments transition function limited to action a, and pt−1 is the auxiliary Bayesian system state estimator, then the EMT-based control choice is described by a∗ = arg min a∈A DKL(H[Ta × pt, pt, τt EMT ] qEMT × pt−1) Note that this follows the Markovian DBC framework precisely: the rewarding optimality of POMDPs is discarded, and in its place a dynamics estimator (EMT in this case) is manipulated via action effects on the environment to produce an estimate close to the specified target system dynamics.",
                "Yet as we mentioned, naive EMTbased control is suboptimal in the DBC sense, and has several additional limitations that do not exist in the general DBC framework (discussed in Section 4.2). 4.1 Multi-Target EMT At times, there may exist several behavioral preferences.",
                "For example, in the case of museum guards, some art items are more heavily guarded, requiring that the guards stay more often in their close vicinity.",
                "On the other hand, no corner of the museum is to be left unchecked, which demands constant motion.",
                "Successful museum security would demand that the guards adhere to, and balance, both of these behaviors.",
                "For EMT-based control, this would mean facing several tactical targets {qk}K k=1, and the question becomes how to merge and balance them.",
                "A balancing mechanism can be applied to resolve this issue.",
                "Note that EMT-based control, while selecting an action, creates a preference vector over the set of actions based on their predicted performance with respect to a given target.",
                "If these preference vectors are normalized, they can be combined into a single unified preference.",
                "This requires replacement of standard EMT-based action selection by the algorithm below [15]: • Given: - a set of target dynamics {qk}K k=1, - vector of weights w(k) • Select action as follows - For each action a ∈ A predict the future state distribution ¯pa t+1 = Ta ∗ pt; - For each action, compute Da = H(¯pa t+1, pt, PDt) - For each a ∈ A and qk tactical target, denote V (a, k) = DKL (Da qk) pt .",
                "Let Vk(a) = 1 Zk V (a, k), where Zk = a∈A V (a, k) is a normalization factor. - Select a∗ = arg min a k k=1 w(k)Vk(a) The weights vector w = (w1, ..., wK ) allows the additional tuning of importance among target dynamics without the need to redesign the targets themselves.",
                "This balancing method is also seamlessly integrated into the EMT-based control flow of operation. 4.2 EMT-based Control Limitations EMT-based control is a sub-optimal (in the DBC sense) representative of the DBC structure.",
                "It limits the User by forcing EMT to be its dynamic tracking algorithm, and replaces Agent optimization by greedy action selection.",
                "This kind of combination, however, is common for on-line algorithms.",
                "Although further development of EMT-based controllers is necessary, evidence so far suggests that even the simplest form of the algorithm possesses a great deal of power, and displays trends that are optimal in the DBC sense of the word.",
                "There are two further, EMT-specific, limitations to EMT-based control that are evident at this point.",
                "Both already have partial solutions and are subjects of ongoing research.",
                "The first limitation is the problem of negative preference.",
                "In the POMDP framework for example, this is captured simply, through The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) 793 the appearance of values with different signs within the reward structure.",
                "For EMT-based control, however, negative preference means that one would like to avoid a certain distribution over system development sequences; EMT-based control, however, concentrates on getting as close as possible to a distribution.",
                "Avoidance is thus unnatural in native EMT-based control.",
                "The second limitation comes from the fact that standard environment modeling can create pure sensory actions-actions that do not change the state of the world, and differ only in the way observations are received and the quality of observations received.",
                "Since the world state does not change, EMT-based control would not be able to differentiate between different sensory actions.",
                "Notice that both of these limitations of EMT-based control are absent from the general DBC framework, since it may have a tracking algorithm capable of considering pure sensory actions and, unlike Kullback-Leibler divergence, a distribution deviation measure that is capable of dealing with negative preference. 5.",
                "EMT PLAYING TAG The Game of Tag was first introduced in [11].",
                "It is a single agent problem of capturing a quarry, and belongs to the class of area sweeping problems.",
                "An example domain is shown in Figure 2. 0 51 2 3 4 6 7 8 10 12 13 161514 17 18 19 2221 23 9 11Q A 20 Figure 2: Tag domain; an agent (A) attempts to seek and capture a quarry (Q) The Game of Tag extremely limits the agents perception, so that the agent is able to detect the quarry only if they are co-located in the same cell of the grid world.",
                "In the classical version of the game, co-location leads to a special observation, and the Tag action can be performed.",
                "We slightly modify this setting: the moment that both agents occupy the same cell, the game ends.",
                "As a result, both the agent and its quarry have the same motion capability, which allows them to move in four directions, North, South, East, and West.",
                "These form a formal space of actions within a Markovian environment.",
                "The state space of the formal Markovian environment is described by the cross-product of the agent and quarrys positions.",
                "For Figure 2, it would be S = {s0, ..., s23} × {s0, ..., s23}.",
                "The effects of an action taken by the agent are deterministic, but the environment in general has a stochastic response due to the motion of the quarry.",
                "With probability q0 1 it stays put, and with probability 1 − q0 it moves to an adjacent cell further away from the 1 In our experiments this was taken to be q0 = 0.2. agent.",
                "So for the instance shown in Figure 2 and q0 = 0.1: P(Q = s9|Q = s9, A = s11) = 0.1 P(Q = s2|Q = s9, A = s11) = 0.3 P(Q = s8|Q = s9, A = s11) = 0.3 P(Q = s14|Q = s9, A = s11) = 0.3 Although the evasive behavior of the quarry is known to the agent, the quarrys position is not.",
                "The only sensory information available to the agent is its own location.",
                "We use EMT and directly specify the target dynamics.",
                "For the Game of Tag, we can easily formulate three major trends: catching the quarry, staying mobile, and stalking the quarry.",
                "This results in the following three target dynamics: Tcatch(At+1 = si|Qt = sj, At = sa) ∝ 1 si = sj 0 otherwise Tmobile(At+1 = si|Qt = so, At = sj) ∝ 0 si = sj 1 otherwise Tstalk(At+1 = si|Qt = so, At = sj) ∝ 1 dist(si,so) Note that none of the above targets are directly achievable; for instance, if Qt = s9 and At = s11, there is no action that can move the agent to At+1 = s9 as required by the Tcatch target dynamics.",
                "We ran several experiments to evaluate EMT performance in the Tag Game.",
                "Three configurations of the domain shown in Figure 3 were used, each posing a different challenge to the agent due to partial observability.",
                "In each setting, a set of 1000 runs was performed with a time limit of 100 steps.",
                "In every run, the initial position of both the agent and its quarry was selected at random; this means that as far as the agent was concerned, the quarrys initial position was uniformly distributed over the entire domain cell space.",
                "We also used two variations of the environment observability function.",
                "In the first version, observability function mapped all joint positions of hunter and quarry into the position of the hunter as an observation.",
                "In the second, only those joint positions in which hunter and quarry occupied different locations were mapped into the hunters location.",
                "The second version in fact utilized and expressed the fact that once hunter and quarry occupy the same cell the game ends.",
                "The results of these experiments are shown in Table 2.",
                "Balancing [15] the catch, move, and stalk target dynamics described in the previous section by the weight vector [0.8, 0.1, 0.1], EMT produced stable performance in all three domains.",
                "Although direct comparisons are difficult to make, the EMT performance displayed notable efficiency vis-`a-vis the POMDP approach.",
                "In spite of a simple and inefficient Matlab implementation of the EMT algorithm, the decision time for any given step averaged significantly below 1 second in all experiments.",
                "For the irregular open arena domain, which proved to be the most difficult, 1000 experiment runs bounded by 100 steps each, a total of 42411 steps, were completed in slightly under 6 hours.",
                "That is, over 4 × 104 online steps took an order of magnitude less time than the offline computation of POMDP policy in [11].",
                "The significance of this differential is made even more prominent by the fact that, should the environment model parameters change, the online nature of EMT would allow it to maintain its performance time, while the POMDP policy would need to be recomputed, requiring yet again a large overhead of computation.",
                "We also tested the behavior cell frequency entropy, empirical measures from trial data.",
                "As Figure 4 and Figure 5 show, empir794 The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) A Q Q A 0 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 A Q Figure 3: These configurations of the Tag Game space were used: a) multiple dead-end, b) irregular open arena, c) circular corridor Table 2: Performance of the EMT-based solution in three Tag Game domains and two observability models: I) omniposition quarry, II) quarry is not at hunters position Model Domain Capture% E(Steps) Time/Step I Dead-ends 100 14.8 72(mSec) Arena 80.2 42.4 500(mSec) Circle 91.4 34.6 187(mSec) II Dead-ends 100 13.2 91(mSec) Arena 96.8 28.67 396(mSec) Circle 94.4 31.63 204(mSec) ical entropy grows with the length of interaction.",
                "For runs where the quarry was not captured immediately, the entropy reaches between 0.85 and 0.952 for different runs and scenarios.",
                "As the agent actively seeks the quarry, the entropy never reaches its maximum.",
                "One characteristic of the entropy graph for the open arena scenario particularly caught our attention in the case of the omniposition quarry observation model.",
                "Near the maximum limit of trial length (100 steps), entropy suddenly dropped.",
                "Further analysis of the data showed that under certain circumstances, a fluctuating behavior occurs in which the agent faces equally viable versions of quarry-following behavior.",
                "Since the EMT algorithm has greedy action selection, and the state space does not encode any form of commitment (not even speed or acceleration), the agent is locked within a small portion of cells.",
                "It is essentially attempting to simultaneously follow several courses of action, all of which are consistent with the target dynamics.",
                "This behavior did not occur in our second observation model, since it significantly reduced the set of eligible courses of action-essentially contributing to tie-breaking among them. 6.",
                "DISCUSSION The design of the EMT solution for the Tag Game exposes the core difference in approach to planning and control between EMT or DBC, on the one hand, and the more familiar POMDP approach, on the other.",
                "POMDP defines a reward structure to optimize, and influences system dynamics indirectly through that optimization.",
                "EMT discards any reward scheme, and instead measures and influences system dynamics directly. 2 Entropy was calculated using log base equal to the number of possible locations within the domain; this properly scales entropy expression into the range [0, 1] for all domains.",
                "Thus for the Tag Game, we did not search for a reward function that would encode and express our preference over the agents behavior, but rather directly set three (heuristic) behavior preferences as the basis for target dynamics to be maintained.",
                "Experimental data shows that these targets need not be directly achievable via the agents actions.",
                "However, the ratio between EMT performance and achievability of target dynamics remains to be explored.",
                "The tag game experiment data also revealed the different emphasis DBC and POMDPs place on the formulation of an environment state space.",
                "POMDPs rely entirely on the mechanism of reward accumulation maximization, i.e., formation of the action selection procedure to achieve necessary state sequencing.",
                "DBC, on the other hand, has two sources of sequencing specification: through the properties of an action selection procedure, and through direct specification within the target dynamics.",
                "The importance of the second source was underlined by the Tag Game experiment data, in which the greedy EMT algorithm, applied to a POMDP-type state space specification, failed, since target description over such a state space was incapable of encoding the necessary behavior tendencies, e.g., tie-breaking and commitment to directed motion.",
                "The structural differences between DBC (and EMT in particular), and POMDPs, prohibits direct performance comparison, and places them on complementary tracks, each within a suitable niche.",
                "For instance, POMDPs could be seen as a much more natural formulation of economic sequential decision-making problems, while EMT is a better fit for continual demand for stochastic change, as happens in many <br>robotic</br> or embodied-agent problems.",
                "The complementary properties of POMDPs and EMT can be further exploited.",
                "There is recent interest in using POMDPs in hybrid solutions [17], in which the POMDPs can be used together with other control approaches to provide results not easily achievable with either approach by itself.",
                "DBC can be an effective partner in such a hybrid solution.",
                "For instance, POMDPs have prohibitively large off-line time requirements for policy computation, but can be readily used in simpler settings to expose beneficial behavioral trends; this can serve as a form of target dynamics that are provided to EMT in a larger domain for on-line operation. 7.",
                "CONCLUSIONS AND FUTURE WORK In this paper, we have presented a novel perspective on the process of planning and control in stochastic environments, in the form of the Dynamics Based Control (DBC) framework.",
                "DBC formulates the task of planning as support of a specified target system dynamics, which describes the necessary properties of change within the environment.",
                "Optimality of DBC plans of action are measured The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) 795 0 20 40 60 80 100 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1 Steps Entropy Dead−ends 0 20 40 60 80 100 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1 Steps Entropy Arena 0 20 40 60 80 100 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1 Steps Entropy Circle Figure 4: Observation Model I: Omniposition quarry.",
                "Entropy development with length of Tag Game for the three experiment scenarios: a) multiple dead-end, b) irregular open arena, c) circular corridor. 0 10 20 30 40 50 60 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1 Steps Entropy Dead−ends 0 20 40 60 80 100 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1 Steps Entropy Arena 0 20 40 60 80 100 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1 Steps Entropy Circle Figure 5: Observation Model II: quarry not observed at hunters position.",
                "Entropy development with length of Tag Game for the three experiment scenarios: a) multiple dead-end, b) irregular open arena, c) circular corridor. with respect to the deviation of actual system dynamics from the target dynamics.",
                "We show that a recently developed technique of Extended Markov Tracking (EMT) [13] is an instantiation of DBC.",
                "In fact, EMT can be seen as a specific case of DBC parameterization, which employs a greedy action selection procedure.",
                "Since EMT exhibits the key features of the general DBC framework, as well as polynomial time complexity, we used the multitarget version of EMT [15] to demonstrate that the class of area sweeping problems naturally lends itself to dynamics-based descriptions, as instantiated by our experiments in the Tag Game domain.",
                "As enumerated in Section 4.2, EMT has a number of limitations, such as difficulty in dealing with negative dynamic preference.",
                "This prevents direct application of EMT to such problems as Rendezvous-Evasion Games (e.g., [6]).",
                "However, DBC in general has no such limitations, and readily enables the formulation of evasion games.",
                "In future work, we intend to proceed with the development of dynamics-based controllers for these problems. 8.",
                "ACKNOWLEDGMENT The work of the first two authors was partially supported by Israel Science Foundation grant #898/05, and the third author was partially supported by a grant from Israels Ministry of Science and Technology. 9.",
                "REFERENCES [1] R. C. Arkin.",
                "Behavior-Based Robotics.",
                "MIT Press, 1998. [2] J.",
                "A. Bilmes.",
                "A gentle tutorial of the EM algorithm and its application to parameter estimation for Gaussian mixture and Hidden Markov Models.",
                "Technical Report TR-97-021, Department of Electrical Engeineering and Computer Science, University of California at Berkeley, 1998. [3] T. M. Cover and J.",
                "A. Thomas.",
                "Elements of information theory.",
                "Wiley, 1991. [4] M. E. desJardins, E. H. Durfee, C. L. Ortiz, and M. J. Wolverton.",
                "A survey of research in distributed, continual planning.",
                "AI Magazine, 4:13-22, 1999. [5] V. R. Konda and J. N. Tsitsiklis.",
                "Actor-Critic algorithms.",
                "SIAM Journal on Control and Optimization, 42(4):1143-1166, 2003. [6] W. S. Lim.",
                "A rendezvous-evasion game on discrete locations with joint randomization.",
                "Advances in Applied Probability, 29(4):1004-1017, December 1997. [7] M. L. Littman, T. L. Dean, and L. P. Kaelbling.",
                "On the complexity of solving Markov decision problems.",
                "In Proceedings of the 11th Annual Conference on Uncertainty in Artificial Intelligence (UAI-95), pages 394-402, 1995. [8] O. Madani, S. Hanks, and A. Condon.",
                "On the undecidability of probabilistic planning and related stochastic optimization problems.",
                "Artificial Intelligence Journal, 147(1-2):5-34, July 2003. [9] R. M. Neal and G. E. Hinton.",
                "A view of the EM algorithm 796 The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) that justifies incremental, sparse, and other variants.",
                "In M. I. Jordan, editor, Learning in Graphical Models, pages 355-368.",
                "Kluwer Academic Publishers, 1998. [10] P. Paruchuri, M. Tambe, F. Ordonez, and S. Kraus.",
                "Security in multiagent systems by policy randomization.",
                "In Proceeding of AAMAS 2006, 2006. [11] J. Pineau, G. Gordon, and S. Thrun.",
                "Point-based value iteration: An anytime algorithm for pomdps.",
                "In International Joint Conference on Artificial Intelligence (IJCAI), pages 1025-1032, August 2003. [12] M. L. Puterman.",
                "Markov Decision Processes.",
                "Wiley Series in Probability and Mathematical Statistics: Applied Probability and Statistics Section.",
                "Wiley-Interscience Publication, New York, 1994. [13] Z. Rabinovich and J. S. Rosenschein.",
                "Extended Markov Tracking with an application to control.",
                "In The Workshop on Agent Tracking: Modeling Other Agents from Observations, at the Third International Joint Conference on Autonomous Agents and Multiagent Systems, pages 95-100, New-York, July 2004. [14] Z. Rabinovich and J. S. Rosenschein.",
                "Multiagent coordination by Extended Markov Tracking.",
                "In The Fourth International Joint Conference on Autonomous Agents and Multiagent Systems, pages 431-438, Utrecht, The Netherlands, July 2005. [15] Z. Rabinovich and J. S. Rosenschein.",
                "On the response of EMT-based control to interacting targets and models.",
                "In The Fifth International Joint Conference on Autonomous Agents and Multiagent Systems, pages 465-470, Hakodate, Japan, May 2006. [16] R. F. Stengel.",
                "Optimal Control and Estimation.",
                "Dover Publications, 1994. [17] M. Tambe, E. Bowring, H. Jung, G. Kaminka, R. Maheswaran, J. Marecki, J. Modi, R. Nair, J. Pearce, P. Paruchuri, D. Pynadath, P. Scerri, N. Schurr, and P. Varakantham.",
                "Conflicts in teamwork: Hybrids to the The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) 797"
            ],
            "original_annotated_samples": [],
            "translated_annotated_samples": [
                "Una posibilidad sería crear una mezcla de las tendencias del sistema disponibles, muy parecido a la que ocurre en las arquitecturas \"robóticas\" basadas en el comportamiento [1].",
                "Por ejemplo, los POMDP podrían verse como una formulación mucho más natural de los problemas de toma de decisiones secuenciales económicas, mientras que EMT es mejor para la demanda continua de cambio estocástico, como sucede en muchos problemas \"robóticos\" o de agentes incorporados."
            ],
            "translated_text": "",
            "candidates": [
                "robótico",
                "robóticas",
                "robótico",
                "robóticos"
            ],
            "error": []
        },
        "dynamics base control": {
            "translated_key": "control base dinámica",
            "is_in_text": false,
            "original_annotated_sentences": [
                "Dynamics Based Control with an Application to Area-Sweeping Problems Zinovi Rabinovich Engineering and Computer Science Hebrew University of Jerusalem Jerusalem, Israel nomad@cs.huji.ac.il Jeffrey S. Rosenschein Engineering and Computer Science Hebrew University of Jerusalem Jerusalem, Israel jeff@cs.huji.ac.il Gal A. Kaminka The MAVERICK Group Department of Computer Science Bar Ilan University, Israel galk@cs.biu.ac.il ABSTRACT In this paper we introduce Dynamics Based Control (DBC), an approach to planning and control of an agent in stochastic environments.",
                "Unlike existing approaches, which seek to optimize expected rewards (e.g., in Partially Observable Markov Decision Problems (POMDPs)), DBC optimizes system behavior towards specified system dynamics.",
                "We show that a recently developed planning and control approach, Extended Markov Tracking (EMT) is an instantiation of DBC.",
                "EMT employs greedy action selection to provide an efficient control algorithm in Markovian environments.",
                "We exploit this efficiency in a set of experiments that applied multitarget EMT to a class of area-sweeping problems (searching for moving targets).",
                "We show that such problems can be naturally defined and efficiently solved using the DBC framework, and its EMT instantiation.",
                "Categories and Subject Descriptors I.2.8 [Problem Solving, Control Methods, and Search]: Control Theory; I.2.9 [Robotics]; I.2.11 [Distributed Artificial Intelligence]: Intelligent Agents General Terms Algorithms, Theory 1.",
                "INTRODUCTION Planning and control constitutes a central research area in multiagent systems and artificial intelligence.",
                "In recent years, Partially Observable Markov Decision Processes (POMDPs) [12] have become a popular formal basis for planning in stochastic environments.",
                "In this framework, the planning and control problem is often addressed by imposing a reward function, and computing a policy (of choosing actions) that is optimal, in the sense that it will result in the highest expected utility.",
                "While theoretically attractive, the complexity of optimally solving a POMDP is prohibitive [8, 7].",
                "We take an alternative view of planning in stochastic environments.",
                "We do not use a (state-based) reward function, but instead optimize over a different criterion, a transition-based specification of the desired system dynamics.",
                "The idea here is to view planexecution as a process that compels a (stochastic) system to change, and a plan as a dynamic process that shapes that change according to desired criteria.",
                "We call this general planning framework Dynamics Based Control (DBC).",
                "In DBC, the goal of a planning (or control) process becomes to ensure that the system will change in accordance with specific (potentially stochastic) target dynamics.",
                "As actual system behavior may deviate from that which is specified by target dynamics (due to the stochastic nature of the system), planning in such environments needs to be continual [4], in a manner similar to classical closed-loop controllers [16].",
                "Here, optimality is measured in terms of probability of deviation magnitudes.",
                "In this paper, we present the structure of Dynamics Based Control.",
                "We show that the recently developed Extended Markov Tracking (EMT) approach [13, 14, 15] is subsumed by DBC, with EMT employing greedy action selection, which is a specific parameterization among the options possible within DBC.",
                "EMT is an efficient instantiation of DBC.",
                "To evaluate DBC, we carried out a set of experiments applying multi-target EMT to the Tag Game [11]; this is a variant on the area sweeping problem, where an agent is trying to tag a moving target (quarry) whose position is not known with certainty.",
                "Experimental data demonstrates that even with a simple model of the environment and a simple design of target dynamics, high success rates can be produced both in catching the quarry, and in surprising the quarry (as expressed by the observed entropy of the controlled agents position).",
                "The paper is organized as follows.",
                "In Section 2 we motivate DBC using area-sweeping problems, and discuss related work.",
                "Section 3 introduces the Dynamics Based Control (DBC) structure, and its specialization to Markovian environments.",
                "This is followed by a review of the Extended Markov Tracking (EMT) approach as a DBC-structured control regimen in Section 4.",
                "That section also discusses the limitations of EMT-based control relative to the general DBC framework.",
                "Experimental settings and results are then presented in Section 5.",
                "Section 6 provides a short discussion of the overall approach, and Section 7 gives some concluding remarks and directions for future work. 790 978-81-904262-7-5 (RPS) c 2007 IFAAMAS 2.",
                "MOTIVATION AND RELATED WORK Many real-life scenarios naturally have a stochastic target dynamics specification, especially those domains where there exists no ultimate goal, but rather system behavior (with specific properties) that has to be continually supported.",
                "For example, security guards perform persistent sweeps of an area to detect any sign of intrusion.",
                "Cunning thieves will attempt to track these sweeps, and time their operation to key points of the guards motion.",
                "It is thus advisable to make the guards motion dynamics appear irregular and random.",
                "Recent work by Paruchuri et al. [10] has addressed such randomization in the context of single-agent and distributed POMDPs.",
                "The goal in that work was to generate policies that provide a measure of action-selection randomization, while maintaining rewards within some acceptable levels.",
                "Our focus differs from this work in that DBC does not optimize expected rewards-indeed we do not consider rewards at all-but instead maintains desired dynamics (including, but not limited to, randomization).",
                "The Game of Tag is another example of the applicability of the approach.",
                "It was introduced in the work by Pineau et al. [11].",
                "There are two agents that can move about an area, which is divided into a grid.",
                "The grid may have blocked cells (holes) into which no agent can move.",
                "One agent (the hunter) seeks to move into a cell occupied by the other (the quarry), such that they are co-located (this is a successful tag).",
                "The quarry seeks to avoid the hunter agent, and is always aware of the hunters position, but does not know how the hunter will behave, which opens up the possibility for a hunter to surprise the prey.",
                "The hunter knows the quarrys probabilistic law of motion, but does not know its current location.",
                "Tag is an instance of a family of area-sweeping (pursuit-evasion) problems.",
                "In [11], the hunter modeled the problem using a POMDP.",
                "A reward function was defined, to reflect the desire to tag the quarry, and an action policy was computed to optimize the reward collected over time.",
                "Due to the intractable complexity of determining the optimal policy, the action policy computed in that paper was essentially an approximation.",
                "In this paper, instead of formulating a reward function, we use EMT to solve the problem, by directly specifying the target dynamics.",
                "In fact, any search problem with randomized motion, the socalled class of area sweeping problems, can be described through specification of such target system dynamics.",
                "Dynamics Based Control provides a natural approach to solving these problems. 3.",
                "DYNAMICS BASED CONTROL The specification of Dynamics Based Control (DBC) can be broken into three interacting levels: Environment Design Level, User Level, and Agent Level. • Environment Design Level is concerned with the formal specification and modeling of the environment.",
                "For example, this level would specify the laws of physics within the system, and set its parameters, such as the gravitation constant. • User Level in turn relies on the environment model produced by Environment Design to specify the target system dynamics it wishes to observe.",
                "The User Level also specifies the estimation or learning procedure for system dynamics, and the measure of deviation.",
                "In the museum guard scenario above, these would correspond to a stochastic sweep schedule, and a measure of relative surprise between the specified and actual sweeping. • Agent Level in turn combines the environment model from the Environment Design level, the dynamics estimation procedure, the deviation measure and the target dynamics specification from User Level, to produce a sequence of actions that create system dynamics as close as possible to the targeted specification.",
                "As we are interested in the continual development of a stochastic system, such as happens in classical control theory [16] and continual planning [4], as well as in our example of museum sweeps, the question becomes how the Agent Level is to treat the deviation measurements over time.",
                "To this end, we use a probability threshold-that is, we would like the Agent Level to maximize the probability that the deviation measure will remain below a certain threshold.",
                "Specific action selection then depends on system formalization.",
                "One possibility would be to create a mixture of available system trends, much like that which happens in Behavior-Based Robotic architectures [1].",
                "The other alternative would be to rely on the estimation procedure provided by the User Level-to utilize the Environment Design Level model of the environment to choose actions, so as to manipulate the dynamics estimator into believing that a certain dynamics has been achieved.",
                "Notice that this manipulation is not direct, but via the environment.",
                "Thus, for strong enough estimator algorithms, successful manipulation would mean a successful simulation of the specified target dynamics (i.e., beyond discerning via the available sensory input).",
                "DBC levels can also have a back-flow of information (see Figure 1).",
                "For instance, the Agent Level could provide data about target dynamics feasibility, allowing the User Level to modify the requirement, perhaps focusing on attainable features of system behavior.",
                "Data would also be available about the system response to different actions performed; combined with a dynamics estimator defined by the User Level, this can provide an important tool for the environment model calibration at the Environment Design Level.",
                "UserEnv.",
                "Design Agent Model Ideal Dynamics Estimator Estimator Dynamics Feasibility System Response Data Figure 1: Data flow of the DBC framework Extending upon the idea of Actor-Critic algorithms [5], DBC data flow can provide a good basis for the design of a learning algorithm.",
                "For example, the User Level can operate as an exploratory device for a learning algorithm, inferring an ideal dynamics target from the environment model at hand that would expose and verify most critical features of system behavior.",
                "In this case, feasibility and system response data from the Agent Level would provide key information for an environment model update.",
                "In fact, the combination of feasibility and response data can provide a basis for the application of strong learning algorithms such as EM [2, 9]. 3.1 DBC for Markovian Environments For a Partially Observable Markovian Environment, DBC can be specified in a more rigorous manner.",
                "Notice how DBC discards rewards, and replaces it by another optimality criterion (structural differences are summarized in Table 1): • Environment Design level is to specify a tuple < S, A, T, O, Ω, s0 >, where: - S is the set of all possible environment states; - s0 is the initial state of the environment (which can also be viewed as a probability distribution over S); The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) 791 - A is the set of all possible actions applicable in the environment; - T is the environments probabilistic transition function: T : S ×A → Π(S).",
                "That is, T(s |a, s) is the probability that the environment will move from state s to state s under action a; - O is the set of all possible observations.",
                "This is what the sensor input would look like for an outside observer; - Ω is the observation probability function: Ω : S × A × S → Π(O).",
                "That is, Ω(o|s , a, s) is the probability that one will observe o given that the environment has moved from state s to state s under action a. • User Level, in the case of Markovian environment, operates on the set of system dynamics described by a family of conditional probabilities F = {τ : S × A → Π(S)}.",
                "Thus specification of target dynamics can be expressed by q ∈ F, and the learning or tracking algorithm can be represented as a function L : O×(A×O)∗ → F; that is, it maps sequences of observations and actions performed so far into an estimate τ ∈ F of system dynamics.",
                "There are many possible variations available at the User Level to define divergence between system dynamics; several of them are: - Trace distance or L1 distance between two distributions p and q defined by D(p(·), q(·)) = 1 2 x |p(x) − q(x)| - Fidelity measure of distance F(p(·), q(·)) = x p(x)q(x) - Kullback-Leibler divergence DKL(p(·) q(·)) = x p(x) log p(x) q(x) Notice that the latter two are not actually metrics over the space of possible distributions, but nevertheless have meaningful and important interpretations.",
                "For instance, KullbackLeibler divergence is an important tool of information theory [3] that allows one to measure the price of encoding an information source governed by q, while assuming that it is governed by p. The User Level also defines the threshold of dynamics deviation probability θ. • Agent Level is then faced with a problem of selecting a control signal function a∗ to satisfy a minimization problem as follows: a∗ = arg min a Pr(d(τa, q) > θ) where d(τa, q) is a random variable describing deviation of the dynamics estimate τa, created by L under control signal a, from the ideal dynamics q.",
                "Implicit in this minimization problem is that L is manipulated via the environment, based on the environment model produced by the Environment Design Level. 3.2 DBC View of the State Space It is important to note the complementary view that DBC and POMDPs take on the state space of the environment.",
                "POMDPs regard state as a stationary snap-shot of the environment; whatever attributes of state sequencing one seeks are reached through properties of the control process, in this case reward accumulation.",
                "This can be viewed as if the sequencing of states and the attributes of that sequencing are only introduced by and for the controlling mechanism-the POMDP policy.",
                "DBC concentrates on the underlying principle of state sequencing, the system dynamics.",
                "DBCs target dynamics specification can use the environments state space as a means to describe, discern, and preserve changes that occur within the system.",
                "As a result, DBC has a greater ability to express state sequencing properties, which are grounded in the environment model and its state space definition.",
                "For example, consider the task of moving through rough terrain towards a goal and reaching it as fast as possible.",
                "POMDPs would encode terrain as state space points, while speed would be ensured by negative reward for every step taken without reaching the goalaccumulating higher reward can be reached only by faster motion.",
                "Alternatively, the state space could directly include the notion of speed.",
                "For POMDPs, this would mean that the same concept is encoded twice, in some sense: directly in the state space, and indirectly within reward accumulation.",
                "Now, even if the reward function would encode more, and finer, details of the properties of motion, the POMDP solution will have to search in a much larger space of policies, while still being guided by the implicit concept of the reward accumulation procedure.",
                "On the other hand, the tactical target expression of variations and correlations between position and speed of motion is now grounded in the state space representation.",
                "In this situation, any further constraints, e.g., smoothness of motion, speed limits in different locations, or speed reductions during sharp turns, are explicitly and uniformly expressed by the tactical target, and can result in faster and more effective action selection by a DBC algorithm. 4.",
                "EMT-BASED CONTROL AS A DBC Recently, a control algorithm was introduced called EMT-based Control [13], which instantiates the DBC framework.",
                "Although it provides an approximate greedy solution in the DBC sense, initial experiments using EMT-based control have been encouraging [14, 15].",
                "EMT-based control is based on the Markovian environment definition, as in the case with POMDPs, but its User and Agent Levels are of the Markovian DBC type of optimality. • User Level of EMT-based control defines a limited-case target system dynamics independent of action: qEMT : S → Π(S).",
                "It then utilizes the Kullback-Leibler divergence measure to compose a momentary system dynamics estimator-the Extended Markov Tracking (EMT) algorithm.",
                "The algorithm keeps a system dynamics estimate τt EMT that is capable of explaining recent change in an auxiliary Bayesian system state estimator from pt−1 to pt, and updates it conservatively using Kullback-Leibler divergence.",
                "Since τt EMT and pt−1,t are respectively the conditional and marginal probabilities over the systems state space, explanation simply means that pt(s ) = s τt EMT (s |s)pt−1(s), and the dynamics estimate update is performed by solving a 792 The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) Table 1: Structure of POMDP vs. Dynamics-Based Control in Markovian Environment Level Approach MDP Markovian DBC Environment < S, A, T, O, Ω >,where S - set of states A - set of actions Design T : S × A → Π(S) - transition O - observation set Ω : S × A × S → Π(O) User r : S × A × S → R q : S × A → Π(S) F(π∗ ) → r L(o1, ..., ot) → τ r - reward function q - ideal dynamics F - reward remodeling L - dynamics estimator θ - threshold Agent π∗ = arg max π E[ γt rt] π∗ = arg min π Prob(d(τ q) > θ) minimization problem: τt EMT = H[pt, pt−1, τt−1 EMT ] = arg min τ DKL(τ × pt−1 τt−1 EMT × pt−1) s.t. pt(s ) = s (τ × pt−1)(s , s) pt−1(s) = s (τ × pt−1)(s , s) • Agent Level in EMT-based control is suboptimal with respect to DBC (though it remains within the DBC framework), performing greedy action selection based on prediction of EMTs reaction.",
                "The prediction is based on the environment model provided by the Environment Design level, so that if we denote by Ta the environments transition function limited to action a, and pt−1 is the auxiliary Bayesian system state estimator, then the EMT-based control choice is described by a∗ = arg min a∈A DKL(H[Ta × pt, pt, τt EMT ] qEMT × pt−1) Note that this follows the Markovian DBC framework precisely: the rewarding optimality of POMDPs is discarded, and in its place a dynamics estimator (EMT in this case) is manipulated via action effects on the environment to produce an estimate close to the specified target system dynamics.",
                "Yet as we mentioned, naive EMTbased control is suboptimal in the DBC sense, and has several additional limitations that do not exist in the general DBC framework (discussed in Section 4.2). 4.1 Multi-Target EMT At times, there may exist several behavioral preferences.",
                "For example, in the case of museum guards, some art items are more heavily guarded, requiring that the guards stay more often in their close vicinity.",
                "On the other hand, no corner of the museum is to be left unchecked, which demands constant motion.",
                "Successful museum security would demand that the guards adhere to, and balance, both of these behaviors.",
                "For EMT-based control, this would mean facing several tactical targets {qk}K k=1, and the question becomes how to merge and balance them.",
                "A balancing mechanism can be applied to resolve this issue.",
                "Note that EMT-based control, while selecting an action, creates a preference vector over the set of actions based on their predicted performance with respect to a given target.",
                "If these preference vectors are normalized, they can be combined into a single unified preference.",
                "This requires replacement of standard EMT-based action selection by the algorithm below [15]: • Given: - a set of target dynamics {qk}K k=1, - vector of weights w(k) • Select action as follows - For each action a ∈ A predict the future state distribution ¯pa t+1 = Ta ∗ pt; - For each action, compute Da = H(¯pa t+1, pt, PDt) - For each a ∈ A and qk tactical target, denote V (a, k) = DKL (Da qk) pt .",
                "Let Vk(a) = 1 Zk V (a, k), where Zk = a∈A V (a, k) is a normalization factor. - Select a∗ = arg min a k k=1 w(k)Vk(a) The weights vector w = (w1, ..., wK ) allows the additional tuning of importance among target dynamics without the need to redesign the targets themselves.",
                "This balancing method is also seamlessly integrated into the EMT-based control flow of operation. 4.2 EMT-based Control Limitations EMT-based control is a sub-optimal (in the DBC sense) representative of the DBC structure.",
                "It limits the User by forcing EMT to be its dynamic tracking algorithm, and replaces Agent optimization by greedy action selection.",
                "This kind of combination, however, is common for on-line algorithms.",
                "Although further development of EMT-based controllers is necessary, evidence so far suggests that even the simplest form of the algorithm possesses a great deal of power, and displays trends that are optimal in the DBC sense of the word.",
                "There are two further, EMT-specific, limitations to EMT-based control that are evident at this point.",
                "Both already have partial solutions and are subjects of ongoing research.",
                "The first limitation is the problem of negative preference.",
                "In the POMDP framework for example, this is captured simply, through The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) 793 the appearance of values with different signs within the reward structure.",
                "For EMT-based control, however, negative preference means that one would like to avoid a certain distribution over system development sequences; EMT-based control, however, concentrates on getting as close as possible to a distribution.",
                "Avoidance is thus unnatural in native EMT-based control.",
                "The second limitation comes from the fact that standard environment modeling can create pure sensory actions-actions that do not change the state of the world, and differ only in the way observations are received and the quality of observations received.",
                "Since the world state does not change, EMT-based control would not be able to differentiate between different sensory actions.",
                "Notice that both of these limitations of EMT-based control are absent from the general DBC framework, since it may have a tracking algorithm capable of considering pure sensory actions and, unlike Kullback-Leibler divergence, a distribution deviation measure that is capable of dealing with negative preference. 5.",
                "EMT PLAYING TAG The Game of Tag was first introduced in [11].",
                "It is a single agent problem of capturing a quarry, and belongs to the class of area sweeping problems.",
                "An example domain is shown in Figure 2. 0 51 2 3 4 6 7 8 10 12 13 161514 17 18 19 2221 23 9 11Q A 20 Figure 2: Tag domain; an agent (A) attempts to seek and capture a quarry (Q) The Game of Tag extremely limits the agents perception, so that the agent is able to detect the quarry only if they are co-located in the same cell of the grid world.",
                "In the classical version of the game, co-location leads to a special observation, and the Tag action can be performed.",
                "We slightly modify this setting: the moment that both agents occupy the same cell, the game ends.",
                "As a result, both the agent and its quarry have the same motion capability, which allows them to move in four directions, North, South, East, and West.",
                "These form a formal space of actions within a Markovian environment.",
                "The state space of the formal Markovian environment is described by the cross-product of the agent and quarrys positions.",
                "For Figure 2, it would be S = {s0, ..., s23} × {s0, ..., s23}.",
                "The effects of an action taken by the agent are deterministic, but the environment in general has a stochastic response due to the motion of the quarry.",
                "With probability q0 1 it stays put, and with probability 1 − q0 it moves to an adjacent cell further away from the 1 In our experiments this was taken to be q0 = 0.2. agent.",
                "So for the instance shown in Figure 2 and q0 = 0.1: P(Q = s9|Q = s9, A = s11) = 0.1 P(Q = s2|Q = s9, A = s11) = 0.3 P(Q = s8|Q = s9, A = s11) = 0.3 P(Q = s14|Q = s9, A = s11) = 0.3 Although the evasive behavior of the quarry is known to the agent, the quarrys position is not.",
                "The only sensory information available to the agent is its own location.",
                "We use EMT and directly specify the target dynamics.",
                "For the Game of Tag, we can easily formulate three major trends: catching the quarry, staying mobile, and stalking the quarry.",
                "This results in the following three target dynamics: Tcatch(At+1 = si|Qt = sj, At = sa) ∝ 1 si = sj 0 otherwise Tmobile(At+1 = si|Qt = so, At = sj) ∝ 0 si = sj 1 otherwise Tstalk(At+1 = si|Qt = so, At = sj) ∝ 1 dist(si,so) Note that none of the above targets are directly achievable; for instance, if Qt = s9 and At = s11, there is no action that can move the agent to At+1 = s9 as required by the Tcatch target dynamics.",
                "We ran several experiments to evaluate EMT performance in the Tag Game.",
                "Three configurations of the domain shown in Figure 3 were used, each posing a different challenge to the agent due to partial observability.",
                "In each setting, a set of 1000 runs was performed with a time limit of 100 steps.",
                "In every run, the initial position of both the agent and its quarry was selected at random; this means that as far as the agent was concerned, the quarrys initial position was uniformly distributed over the entire domain cell space.",
                "We also used two variations of the environment observability function.",
                "In the first version, observability function mapped all joint positions of hunter and quarry into the position of the hunter as an observation.",
                "In the second, only those joint positions in which hunter and quarry occupied different locations were mapped into the hunters location.",
                "The second version in fact utilized and expressed the fact that once hunter and quarry occupy the same cell the game ends.",
                "The results of these experiments are shown in Table 2.",
                "Balancing [15] the catch, move, and stalk target dynamics described in the previous section by the weight vector [0.8, 0.1, 0.1], EMT produced stable performance in all three domains.",
                "Although direct comparisons are difficult to make, the EMT performance displayed notable efficiency vis-`a-vis the POMDP approach.",
                "In spite of a simple and inefficient Matlab implementation of the EMT algorithm, the decision time for any given step averaged significantly below 1 second in all experiments.",
                "For the irregular open arena domain, which proved to be the most difficult, 1000 experiment runs bounded by 100 steps each, a total of 42411 steps, were completed in slightly under 6 hours.",
                "That is, over 4 × 104 online steps took an order of magnitude less time than the offline computation of POMDP policy in [11].",
                "The significance of this differential is made even more prominent by the fact that, should the environment model parameters change, the online nature of EMT would allow it to maintain its performance time, while the POMDP policy would need to be recomputed, requiring yet again a large overhead of computation.",
                "We also tested the behavior cell frequency entropy, empirical measures from trial data.",
                "As Figure 4 and Figure 5 show, empir794 The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) A Q Q A 0 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 A Q Figure 3: These configurations of the Tag Game space were used: a) multiple dead-end, b) irregular open arena, c) circular corridor Table 2: Performance of the EMT-based solution in three Tag Game domains and two observability models: I) omniposition quarry, II) quarry is not at hunters position Model Domain Capture% E(Steps) Time/Step I Dead-ends 100 14.8 72(mSec) Arena 80.2 42.4 500(mSec) Circle 91.4 34.6 187(mSec) II Dead-ends 100 13.2 91(mSec) Arena 96.8 28.67 396(mSec) Circle 94.4 31.63 204(mSec) ical entropy grows with the length of interaction.",
                "For runs where the quarry was not captured immediately, the entropy reaches between 0.85 and 0.952 for different runs and scenarios.",
                "As the agent actively seeks the quarry, the entropy never reaches its maximum.",
                "One characteristic of the entropy graph for the open arena scenario particularly caught our attention in the case of the omniposition quarry observation model.",
                "Near the maximum limit of trial length (100 steps), entropy suddenly dropped.",
                "Further analysis of the data showed that under certain circumstances, a fluctuating behavior occurs in which the agent faces equally viable versions of quarry-following behavior.",
                "Since the EMT algorithm has greedy action selection, and the state space does not encode any form of commitment (not even speed or acceleration), the agent is locked within a small portion of cells.",
                "It is essentially attempting to simultaneously follow several courses of action, all of which are consistent with the target dynamics.",
                "This behavior did not occur in our second observation model, since it significantly reduced the set of eligible courses of action-essentially contributing to tie-breaking among them. 6.",
                "DISCUSSION The design of the EMT solution for the Tag Game exposes the core difference in approach to planning and control between EMT or DBC, on the one hand, and the more familiar POMDP approach, on the other.",
                "POMDP defines a reward structure to optimize, and influences system dynamics indirectly through that optimization.",
                "EMT discards any reward scheme, and instead measures and influences system dynamics directly. 2 Entropy was calculated using log base equal to the number of possible locations within the domain; this properly scales entropy expression into the range [0, 1] for all domains.",
                "Thus for the Tag Game, we did not search for a reward function that would encode and express our preference over the agents behavior, but rather directly set three (heuristic) behavior preferences as the basis for target dynamics to be maintained.",
                "Experimental data shows that these targets need not be directly achievable via the agents actions.",
                "However, the ratio between EMT performance and achievability of target dynamics remains to be explored.",
                "The tag game experiment data also revealed the different emphasis DBC and POMDPs place on the formulation of an environment state space.",
                "POMDPs rely entirely on the mechanism of reward accumulation maximization, i.e., formation of the action selection procedure to achieve necessary state sequencing.",
                "DBC, on the other hand, has two sources of sequencing specification: through the properties of an action selection procedure, and through direct specification within the target dynamics.",
                "The importance of the second source was underlined by the Tag Game experiment data, in which the greedy EMT algorithm, applied to a POMDP-type state space specification, failed, since target description over such a state space was incapable of encoding the necessary behavior tendencies, e.g., tie-breaking and commitment to directed motion.",
                "The structural differences between DBC (and EMT in particular), and POMDPs, prohibits direct performance comparison, and places them on complementary tracks, each within a suitable niche.",
                "For instance, POMDPs could be seen as a much more natural formulation of economic sequential decision-making problems, while EMT is a better fit for continual demand for stochastic change, as happens in many robotic or embodied-agent problems.",
                "The complementary properties of POMDPs and EMT can be further exploited.",
                "There is recent interest in using POMDPs in hybrid solutions [17], in which the POMDPs can be used together with other control approaches to provide results not easily achievable with either approach by itself.",
                "DBC can be an effective partner in such a hybrid solution.",
                "For instance, POMDPs have prohibitively large off-line time requirements for policy computation, but can be readily used in simpler settings to expose beneficial behavioral trends; this can serve as a form of target dynamics that are provided to EMT in a larger domain for on-line operation. 7.",
                "CONCLUSIONS AND FUTURE WORK In this paper, we have presented a novel perspective on the process of planning and control in stochastic environments, in the form of the Dynamics Based Control (DBC) framework.",
                "DBC formulates the task of planning as support of a specified target system dynamics, which describes the necessary properties of change within the environment.",
                "Optimality of DBC plans of action are measured The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) 795 0 20 40 60 80 100 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1 Steps Entropy Dead−ends 0 20 40 60 80 100 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1 Steps Entropy Arena 0 20 40 60 80 100 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1 Steps Entropy Circle Figure 4: Observation Model I: Omniposition quarry.",
                "Entropy development with length of Tag Game for the three experiment scenarios: a) multiple dead-end, b) irregular open arena, c) circular corridor. 0 10 20 30 40 50 60 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1 Steps Entropy Dead−ends 0 20 40 60 80 100 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1 Steps Entropy Arena 0 20 40 60 80 100 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1 Steps Entropy Circle Figure 5: Observation Model II: quarry not observed at hunters position.",
                "Entropy development with length of Tag Game for the three experiment scenarios: a) multiple dead-end, b) irregular open arena, c) circular corridor. with respect to the deviation of actual system dynamics from the target dynamics.",
                "We show that a recently developed technique of Extended Markov Tracking (EMT) [13] is an instantiation of DBC.",
                "In fact, EMT can be seen as a specific case of DBC parameterization, which employs a greedy action selection procedure.",
                "Since EMT exhibits the key features of the general DBC framework, as well as polynomial time complexity, we used the multitarget version of EMT [15] to demonstrate that the class of area sweeping problems naturally lends itself to dynamics-based descriptions, as instantiated by our experiments in the Tag Game domain.",
                "As enumerated in Section 4.2, EMT has a number of limitations, such as difficulty in dealing with negative dynamic preference.",
                "This prevents direct application of EMT to such problems as Rendezvous-Evasion Games (e.g., [6]).",
                "However, DBC in general has no such limitations, and readily enables the formulation of evasion games.",
                "In future work, we intend to proceed with the development of dynamics-based controllers for these problems. 8.",
                "ACKNOWLEDGMENT The work of the first two authors was partially supported by Israel Science Foundation grant #898/05, and the third author was partially supported by a grant from Israels Ministry of Science and Technology. 9.",
                "REFERENCES [1] R. C. Arkin.",
                "Behavior-Based Robotics.",
                "MIT Press, 1998. [2] J.",
                "A. Bilmes.",
                "A gentle tutorial of the EM algorithm and its application to parameter estimation for Gaussian mixture and Hidden Markov Models.",
                "Technical Report TR-97-021, Department of Electrical Engeineering and Computer Science, University of California at Berkeley, 1998. [3] T. M. Cover and J.",
                "A. Thomas.",
                "Elements of information theory.",
                "Wiley, 1991. [4] M. E. desJardins, E. H. Durfee, C. L. Ortiz, and M. J. Wolverton.",
                "A survey of research in distributed, continual planning.",
                "AI Magazine, 4:13-22, 1999. [5] V. R. Konda and J. N. Tsitsiklis.",
                "Actor-Critic algorithms.",
                "SIAM Journal on Control and Optimization, 42(4):1143-1166, 2003. [6] W. S. Lim.",
                "A rendezvous-evasion game on discrete locations with joint randomization.",
                "Advances in Applied Probability, 29(4):1004-1017, December 1997. [7] M. L. Littman, T. L. Dean, and L. P. Kaelbling.",
                "On the complexity of solving Markov decision problems.",
                "In Proceedings of the 11th Annual Conference on Uncertainty in Artificial Intelligence (UAI-95), pages 394-402, 1995. [8] O. Madani, S. Hanks, and A. Condon.",
                "On the undecidability of probabilistic planning and related stochastic optimization problems.",
                "Artificial Intelligence Journal, 147(1-2):5-34, July 2003. [9] R. M. Neal and G. E. Hinton.",
                "A view of the EM algorithm 796 The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) that justifies incremental, sparse, and other variants.",
                "In M. I. Jordan, editor, Learning in Graphical Models, pages 355-368.",
                "Kluwer Academic Publishers, 1998. [10] P. Paruchuri, M. Tambe, F. Ordonez, and S. Kraus.",
                "Security in multiagent systems by policy randomization.",
                "In Proceeding of AAMAS 2006, 2006. [11] J. Pineau, G. Gordon, and S. Thrun.",
                "Point-based value iteration: An anytime algorithm for pomdps.",
                "In International Joint Conference on Artificial Intelligence (IJCAI), pages 1025-1032, August 2003. [12] M. L. Puterman.",
                "Markov Decision Processes.",
                "Wiley Series in Probability and Mathematical Statistics: Applied Probability and Statistics Section.",
                "Wiley-Interscience Publication, New York, 1994. [13] Z. Rabinovich and J. S. Rosenschein.",
                "Extended Markov Tracking with an application to control.",
                "In The Workshop on Agent Tracking: Modeling Other Agents from Observations, at the Third International Joint Conference on Autonomous Agents and Multiagent Systems, pages 95-100, New-York, July 2004. [14] Z. Rabinovich and J. S. Rosenschein.",
                "Multiagent coordination by Extended Markov Tracking.",
                "In The Fourth International Joint Conference on Autonomous Agents and Multiagent Systems, pages 431-438, Utrecht, The Netherlands, July 2005. [15] Z. Rabinovich and J. S. Rosenschein.",
                "On the response of EMT-based control to interacting targets and models.",
                "In The Fifth International Joint Conference on Autonomous Agents and Multiagent Systems, pages 465-470, Hakodate, Japan, May 2006. [16] R. F. Stengel.",
                "Optimal Control and Estimation.",
                "Dover Publications, 1994. [17] M. Tambe, E. Bowring, H. Jung, G. Kaminka, R. Maheswaran, J. Marecki, J. Modi, R. Nair, J. Pearce, P. Paruchuri, D. Pynadath, P. Scerri, N. Schurr, and P. Varakantham.",
                "Conflicts in teamwork: Hybrids to the The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) 797"
            ],
            "original_annotated_samples": [],
            "translated_annotated_samples": [],
            "translated_text": "",
            "candidates": [],
            "error": []
        }
    }
}