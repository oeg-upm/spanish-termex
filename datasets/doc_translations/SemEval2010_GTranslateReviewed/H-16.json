{
    "id": "H-16",
    "original_text": "The Impact of Caching on Search Engines Ricardo Baeza-Yates1 rbaeza@acm.org Aristides Gionis1 gionis@yahoo-inc.com Flavio Junqueira1 fpj@yahoo-inc.com Vanessa Murdock1 vmurdock@yahoo-inc.com Vassilis Plachouras1 vassilis@yahoo-inc.com Fabrizio Silvestri2 f.silvestri@isti.cnr.it 1 Yahoo! Research Barcelona 2 ISTI - CNR Barcelona, SPAIN Pisa, ITALY ABSTRACT In this paper we study the trade-offs in designing efficient caching systems for Web search engines. We explore the impact of different approaches, such as static vs. dynamic caching, and caching query results vs. caching posting lists. Using a query log spanning a whole year we explore the limitations of caching and we demonstrate that caching posting lists can achieve higher hit rates than caching query answers. We propose a new algorithm for static caching of posting lists, which outperforms previous methods. We also study the problem of finding the optimal way to split the static cache between answers and posting lists. Finally, we measure how the changes in the query log affect the effectiveness of static caching, given our observation that the distribution of the queries changes slowly over time. Our results and observations are applicable to different levels of the data-access hierarchy, for instance, for a memory/disk layer or a broker/remote server layer. Categories and Subject Descriptors H.3.3 [Information Storage and Retrieval]: Information Search and Retrieval - Search process; H.3.4 [Information Storage and Retrieval]: Systems and Software - Distributed systems, Performance evaluation (efficiency and effectiveness) General Terms Algorithms, Experimentation 1. INTRODUCTION Millions of queries are submitted daily to Web search engines, and users have high expectations of the quality and speed of the answers. As the searchable Web becomes larger and larger, with more than 20 billion pages to index, evaluating a single query requires processing large amounts of data. In such a setting, to achieve a fast response time and to increase the query throughput, using a cache is crucial. The primary use of a cache memory is to speedup computation by exploiting frequently or recently used data, although reducing the workload to back-end servers is also a major goal. Caching can be applied at different levels with increasing response latencies or processing requirements. For example, the different levels may correspond to the main memory, the disk, or resources in a local or a wide area network. The decision of what to cache is either off-line (static) or online (dynamic). A static cache is based on historical information and is periodically updated. A dynamic cache replaces entries according to the sequence of requests. When a new request arrives, the cache system decides whether to evict some entry from the cache in the case of a cache miss. Such online decisions are based on a cache policy, and several different policies have been studied in the past. For a search engine, there are two possible ways to use a cache memory: Caching answers: As the engine returns answers to a particular query, it may decide to store these answers to resolve future queries. Caching terms: As the engine evaluates a particular query, it may decide to store in memory the posting lists of the involved query terms. Often the whole set of posting lists does not fit in memory, and consequently, the engine has to select a small set to keep in memory and speed up query processing. Returning an answer to a query that already exists in the cache is more efficient than computing the answer using cached posting lists. On the other hand, previously unseen queries occur more often than previously unseen terms, implying a higher miss rate for cached answers. Caching of posting lists has additional challenges. As posting lists have variable size, caching them dynamically is not very efficient, due to the complexity in terms of efficiency and space, and the skewed distribution of the query stream, as shown later. Static caching of posting lists poses even more challenges: when deciding which terms to cache one faces the trade-off between frequently queried terms and terms with small posting lists that are space efficient. Finally, before deciding to adopt a static caching policy the query stream should be analyzed to verify that its characteristics do not change rapidly over time. Broker Static caching posting lists Dynamic/Static cached answers Local query processor Disk Next caching level Local network access Remote network access Figure 1: One caching level in a distributed search architecture. In this paper we explore the trade-offs in the design of each cache level, showing that the problem is the same and only a few parameters change. In general, we assume that each level of caching in a distributed search architecture is similar to that shown in Figure 1. We use a query log spanning a whole year to explore the limitations of dynamically caching query answers or posting lists for query terms. More concretely, our main conclusions are that: • Caching query answers results in lower hit ratios compared to caching of posting lists for query terms, but it is faster because there is no need for query evaluation. We provide a framework for the analysis of the trade-off between static caching of query answers and posting lists; • Static caching of terms can be more effective than dynamic caching with, for example, LRU. We provide algorithms based on the Knapsack problem for selecting the posting lists to put in a static cache, and we show improvements over previous work, achieving a hit ratio over 90%; • Changes of the query distribution over time have little impact on static caching. The remainder of this paper is organized as follows. Sections 2 and 3 summarize related work and characterize the data sets we use. Section 4 discusses the limitations of dynamic caching. Sections 5 and 6 introduce algorithms for caching posting lists, and a theoretical framework for the analysis of static caching, respectively. Section 7 discusses the impact of changes in the query distribution on static caching, and Section 8 provides concluding remarks. 2. RELATED WORK There is a large body of work devoted to query optimization. Buckley and Lewit [3], in one of the earliest works, take a term-at-a-time approach to deciding when inverted lists need not be further examined. More recent examples demonstrate that the top k documents for a query can be returned without the need for evaluating the complete set of posting lists [1, 4, 15]. Although these approaches seek to improve query processing efficiency, they differ from our current work in that they do not consider caching. They may be considered separate and complementary to a cache-based approach. Raghavan and Sever [12], in one of the first papers on exploiting user query history, propose using a query base, built upon a set of persistent optimal queries submitted in the past, to improve the retrieval effectiveness for similar future queries. Markatos [10] shows the existence of temporal locality in queries, and compares the performance of different caching policies. Based on the observations of Markatos, Lempel and Moran propose a new caching policy, called Probabilistic Driven Caching, by attempting to estimate the probability distribution of all possible queries submitted to a search engine [8]. Fagni et al. follow Markatos work by showing that combining static and dynamic caching policies together with an adaptive prefetching policy achieves a high hit ratio [7]. Different from our work, they consider caching and prefetching of pages of results. As systems are often hierarchical, there has also been some effort on multi-level architectures. Saraiva et al. propose a new architecture for Web search engines using a two-level dynamic caching system [13]. Their goal for such systems has been to improve response time for hierarchical engines. In their architecture, both levels use an LRU eviction policy. They find that the second-level cache can effectively reduce disk traffic, thus increasing the overall throughput. Baeza-Yates and Saint-Jean propose a three-level index organization [2]. Long and Suel propose a caching system structured according to three different levels [9]. The intermediate level contains frequently occurring pairs of terms and stores the intersections of the corresponding inverted lists. These last two papers are related to ours in that they exploit different caching strategies at different levels of the memory hierarchy. Finally, our static caching algorithm for posting lists in Section 5 uses the ratio frequency/size in order to evaluate the goodness of an item to cache. Similar ideas have been used in the context of file caching [17], Web caching [5], and even caching of posting lists [9], but in all cases in a dynamic setting. To the best of our knowledge we are the first to use this approach for static caching of posting lists. 3. DATA CHARACTERIZATION Our data consists of a crawl of documents from the UK domain, and query logs of one year of queries submitted to http://www.yahoo.co.uk from November 2005 to November 2006. In our logs, 50% of the total volume of queries are unique. The average query length is 2.5 terms, with the longest query having 731 terms. 1e-07 1e-06 1e-05 1e-04 0.001 0.01 0.1 1 1e-08 1e-07 1e-06 1e-05 1e-04 0.001 0.01 0.1 1 Frequency(normalized) Frequency rank (normalized) Figure 2: The distribution of queries (bottom curve) and query terms (middle curve) in the query log. The distribution of document frequencies of terms in the UK-2006 dataset (upper curve). Figure 2 shows the distributions of queries (lower curve), and query terms (middle curve). The x-axis represents the normalized frequency rank of the query or term. (The most frequent query appears closest to the y-axis.) The y-axis is Table 1: Statistics of the UK-2006 sample. UK-2006 sample statistics # of documents 2,786,391 # of terms 6,491,374 # of tokens 2,109,512,558 the normalized frequency for a given query (or term). As expected, the distribution of query frequencies and query term frequencies follow power law distributions, with slope of 1.84 and 2.26, respectively. In this figure, the query frequencies were computed as they appear in the logs with no normalization for case or white space. The query terms (middle curve) have been normalized for case, as have the terms in the document collection. The document collection that we use for our experiments is a summary of the UK domain crawled in May 2006.1 This summary corresponds to a maximum of 400 crawled documents per host, using a breadth first crawling strategy, comprising 15GB. The distribution of document frequencies of terms in the collection follows a power law distribution with slope 2.38 (upper curve in Figure 2). The statistics of the collection are shown in Table 1. We measured the correlation between the document frequency of terms in the collection and the number of queries that contain a particular term in the query log to be 0.424. A scatter plot for a random sample of terms is shown in Figure 3. In this experiment, terms have been converted to lower case in both the queries and the documents so that the frequencies will be comparable. 1e-07 1e-06 1e-05 1e-04 0.001 0.01 0.1 1 1e-06 1e-05 1e-04 0.001 0.01 0.1 1 Queryfrequency Document frequency Figure 3: Normalized scatter plot of document-term frequencies vs. query-term frequencies. 4. CACHING OF QUERIES AND TERMS Caching relies upon the assumption that there is locality in the stream of requests. That is, there must be sufficient repetition in the stream of requests and within intervals of time that enable a cache memory of reasonable size to be effective. In the query log we used, 88% of the unique queries are singleton queries, and 44% are singleton queries out of the whole volume. Thus, out of all queries in the stream composing the query log, the upper threshold on hit ratio is 56%. This is because only 56% of all the queries comprise queries that have multiple occurrences. It is important to observe, however, that not all queries in this 56% can be cache hits because of compulsory misses. A compulsory miss 1 The collection is available from the University of Milan: http://law.dsi.unimi.it/. URL retrieved 05/2007. 0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1 240 260 280 300 320 340 360 Numberofelements Bin number Total terms Terms diff Total queries Unique queries Unique terms Query diff Figure 4: Arrival rate for both terms and queries. happens when the cache receives a query for the first time. This is different from capacity misses, which happen due to space constraints on the amount of memory the cache uses. If we consider a cache with infinite memory, then the hit ratio is 50%. Note that for an infinite cache there are no capacity misses. As we mentioned before, another possibility is to cache the posting lists of terms. Intuitively, this gives more freedom in the utilization of the cache content to respond to queries because cached terms might form a new query. On the other hand, they need more space. As opposed to queries, the fraction of singleton terms in the total volume of terms is smaller. In our query log, only 4% of the terms appear once, but this accounts for 73% of the vocabulary of query terms. We show in Section 5 that caching a small fraction of terms, while accounting for terms appearing in many documents, is potentially very effective. Figure 4 shows several graphs corresponding to the normalized arrival rate for different cases using days as bins. That is, we plot the normalized number of elements that appear in a day. This graph shows only a period of 122 days, and we normalize the values by the maximum value observed throughout the whole period of the query log. Total queries and Total terms correspond to the total volume of queries and terms, respectively. Unique queries and Unique terms correspond to the arrival rate of unique queries and terms. Finally, Query diff and Terms diff correspond to the difference between the curves for total and unique. In Figure 4, as expected, the volume of terms is much higher than the volume of queries. The difference between the total number of terms and the number of unique terms is much larger than the difference between the total number of queries and the number of unique queries. This observation implies that terms repeat significantly more than queries. If we use smaller bins, say of one hour, then the ratio of unique to volume is higher for both terms and queries because it leaves less room for repetition. We also estimated the workload using the document frequency of terms as a measure of how much work a query imposes on a search engine. We found that it follows closely the arrival rate for terms shown in Figure 4. To demonstrate the effect of a dynamic cache on the query frequency distribution of Figure 2, we plot the same frequency graph, but now considering the frequency of queries Figure 5: Frequency graph after LRU cache. after going through an LRU cache. On a cache miss, an LRU cache decides upon an entry to evict using the information on the recency of queries. In this graph, the most frequent queries are not the same queries that were most frequent before the cache. It is possible that queries that are most frequent after the cache have different characteristics, and tuning the search engine to queries frequent before the cache may degrade performance for non-cached queries. The maximum frequency after caching is less than 1% of the maximum frequency before the cache, thus showing that the cache is very effective in reducing the load of frequent queries. If we re-rank the queries according to after-cache frequency, the distribution is still a power law, but with a much smaller value for the highest frequency. When discussing the effectiveness of dynamically caching, an important metric is cache miss rate. To analyze the cache miss rate for different memory constraints, we use the working set model [6, 14]. A working set, informally, is the set of references that an application or an operating system is currently working with. The model uses such sets in a strategy that tries to capture the temporal locality of references. The working set strategy then consists in keeping in memory only the elements that are referenced in the previous θ steps of the input sequence, where θ is a configurable parameter corresponding to the window size. Originally, working sets have been used for page replacement algorithms of operating systems, and considering such a strategy in the context of search engines is interesting for three reasons. First, it captures the amount of locality of queries and terms in a sequence of queries. Locality in this case refers to the frequency of queries and terms in a window of time. If many queries appear multiple times in a window, then locality is high. Second, it enables an oﬄine analysis of the expected miss rate given different memory constraints. Third, working sets capture aspects of efficient caching algorithms such as LRU. LRU assumes that references farther in the past are less likely to be referenced in the present, which is implicit in the concept of working sets [14]. Figure 6 plots the miss rate for different working set sizes, and we consider working sets of both queries and terms. The working set sizes are normalized against the total number of queries in the query log. In the graph for queries, there is a sharp decay until approximately 0.01, and the rate at which the miss rate drops decreases as we increase the size of the working set over 0.01. Finally, the minimum value it reaches is 50% miss rate, not shown in the figure as we have cut the tail of the curve for presentation purposes. 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1 0 0.05 0.1 0.15 0.2 Missrate Normalized working set size Queries Terms Figure 6: Miss rate as a function of the working set size. 1 10 100 1000 10000 100000 1e+06 Frequency Distance Figure 7: Distribution of distances expressed in terms of distinct queries. Compared to the query curve, we observe that the minimum miss rate for terms is substantially smaller. The miss rate also drops sharply on values up to 0.01, and it decreases minimally for higher values. The minimum value, however, is slightly over 10%, which is much smaller than the minimum value for the sequence of queries. This implies that with such a policy it is possible to achieve over 80% hit rate, if we consider caching dynamically posting lists for terms as opposed to caching answers for queries. This result does not consider the space required for each unit stored in the cache memory, or the amount of time it takes to put together a response to a user query. We analyze these issues more carefully later in this paper. It is interesting also to observe the histogram of Figure 7, which is an intermediate step in the computation of the miss rate graph. It reports the distribution of distances between repetitions of the same frequent query. The distance in the plot is measured in the number of distinct queries separating a query and its repetition, and it considers only queries appearing at least 10 times. From Figures 6 and 7, we conclude that even if we set the size of the query answers cache to a relatively large number of entries, the miss rate is high. Thus, caching the posting lists of terms has the potential to improve the hit ratio. This is what we explore next. 5. CACHING POSTING LISTS The previous section shows that caching posting lists can obtain a higher hit rate compared to caching query answers. In this section we study the problem of how to select posting lists to place on a certain amount of available memory, assuming that the whole index is larger than the amount of memory available. The posting lists have variable size (in fact, their size distribution follows a power law), so it is beneficial for a caching policy to consider the sizes of the posting lists. We consider both dynamic and static caching. For dynamic caching, we use two well-known policies, LRU and LFU, as well as a modified algorithm that takes posting-list size into account. Before discussing the static caching strategies, we introduce some notation. We use fq(t) to denote the query-term frequency of a term t, that is, the number of queries containing t in the query log, and fd(t) to denote the document frequency of t, that is, the number of documents in the collection in which the term t appears. The first strategy we consider is the algorithm proposed by Baeza-Yates and Saint-Jean [2], which consists in selecting the posting lists of the terms with the highest query-term frequencies fq(t). We call this algorithm Qtf. We observe that there is a trade-off between fq(t) and fd(t). Terms with high fq(t) are useful to keep in the cache because they are queried often. On the other hand, terms with high fd(t) are not good candidates because they correspond to long posting lists and consume a substantial amount of space. In fact, the problem of selecting the best posting lists for the static cache corresponds to the standard Knapsack problem: given a knapsack of fixed capacity, and a set of n items, such as the i-th item has value ci and size si, select the set of items that fit in the knapsack and maximize the overall value. In our case, value corresponds to fq(t) and size corresponds to fd(t). Thus, we employ a simple algorithm for the knapsack problem, which is selecting the posting lists of the terms with the highest values of the ratio fq(t) fd(t) . We call this algorithm QtfDf. We tried other variations considering query frequencies instead of term frequencies, but the gain was minimal compared to the complexity added. In addition to the above two static algorithms we consider the following algorithms for dynamic caching: • LRU: A standard LRU algorithm, but many posting lists might need to be evicted (in order of least-recent usage) until there is enough space in the memory to place the currently accessed posting list; • LFU: A standard LFU algorithm (eviction of the leastfrequently used), with the same modification as the LRU; • Dyn-QtfDf: A dynamic version of the QtfDf algorithm; evict from the cache the term(s) with the lowest fq(t) fd(t) ratio. The performance of all the above algorithms for 15 weeks of the query log and the UK dataset are shown in Figure 8. Performance is measured with hit rate. The cache size is measured as a fraction of the total space required to store the posting lists of all terms. For the dynamic algorithms, we load the cache with terms in order of fq(t) and we let the cache warm up for 1 million queries. For the static algorithms, we assume complete knowledge of the frequencies fq(t), that is, we estimate fq(t) from the whole query stream. As we show in Section 7 the results do not change much if we compute the query-term frequencies using the first 3 or 4 weeks of the query log and measure the hit rate on the rest. 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1 0.1 0.2 0.3 0.4 0.5 0.6 0.7 Hitrate Cache size Caching posting lists static QTF/DF LRU LFU Dyn-QTF/DF QTF Figure 8: Hit rate of different strategies for caching posting lists. The most important observation from our experiments is that the static QtfDf algorithm has a better hit rate than all the dynamic algorithms. An important benefit a static cache is that it requires no eviction and it is hence more efficient when evaluating queries. However, if the characteristics of the query traffic change frequently over time, then it requires re-populating the cache often or there will be a significant impact on hit rate. 6. ANALYSIS OF STATIC CACHING In this section we provide a detailed analysis for the problem of deciding whether it is preferable to cache query answers or cache posting lists. Our analysis takes into account the impact of caching between two levels of the data-access hierarchy. It can either be applied at the memory/disk layer or at a server/remote server layer as in the architecture we discussed in the introduction. Using a particular system model, we obtain estimates for the parameters required by our analysis, which we subsequently use to decide the optimal trade-off between caching query answers and caching posting lists. 6.1 Analytical Model Let M be the size of the cache measured in answer units (the cache can store M query answers). Assume that all posting lists are of the same length L, measured in answer units. We consider the following two cases: (A) a cache that stores only precomputed answers, and (B) a cache that stores only posting lists. In the first case, Nc = M answers fit in the cache, while in the second case Np = M/L posting lists fit in the cache. Thus, Np = Nc/L. Note that although posting lists require more space, we can combine terms to evaluate more queries (or partial queries). For case (A), suppose that a query answer in the cache can be evaluated in 1 time unit. For case (B), assume that if the posting lists of the terms of a query are in the cache then the results can be computed in TR1 time units, while if the posting lists are not in the cache then the results can be computed in TR2 time units. Of course TR2 > TR1. Now we want to compare the time to answer a stream of Q queries in both cases. Let Vc(Nc) be the volume of the most frequent Nc queries. Then, for case (A), we have an overall time TCA = Vc(Nc) + TR2(Q − Vc(Nc)). Similarly, for case (B), let Vp(Np) be the number of computable queries. Then we have overall time TP L = TR1Vp(Np) + TR2(Q − Vp(Np)). We want to check under which conditions we have TP L < TCA. We have TP L − TCA = (TR2 − 1)Vc(Nc) − (TR2 − TR1)Vp(Np) > 0. Figure 9 shows the values of Vp and Vc for our data. We can see that caching answers saturates faster and for this particular data there is no additional benefit from using more than 10% of the index space for caching answers. As the query distribution is a power law with parameter α > 1, the i-th most frequent query appears with probability proportional to 1 iα . Therefore, the volume Vc(n), which is the total number of the n most frequent queries, is Vc(n) = V0 n i=1 Q iα = γnQ (0 < γn < 1). We know that Vp(n) grows faster than Vc(n) and assume, based on experimental results, that the relation is of the form Vp(n) = k Vc(n)β . In the worst case, for a large cache, β → 1. That is, both techniques will cache a constant fraction of the overall query volume. Then caching posting lists makes sense only if L(TR2 − 1) k(TR2 − TR1) > 1. If we use compression, we have L < L and TR1 > TR1. According to the experiments that we show later, compression is always better. For a small cache, we are interested in the transient behavior and then β > 1, as computed from our data. In this case there will always be a point where TP L > TCA for a large number of queries. In reality, instead of filling the cache only with answers or only with posting lists, a better strategy will be to divide the total cache space into cache for answers and cache for posting lists. In such a case, there will be some queries that could be answered by both parts of the cache. As the answer cache is faster, it will be the first choice for answering those queries. Let QNc and QNp be the set of queries that can be answered by the cached answers and the cached posting lists, respectively. Then, the overall time is T = Vc(Nc)+TR1V (QNp −QNc )+TR2(Q−V (QNp ∪QNc )), where Np = (M − Nc)/L. Finding the optimal division of the cache in order to minimize the overall retrieval time is a difficult problem to solve analytically. In Section 6.3 we use simulations to derive optimal cache trade-offs for particular implementation examples. 6.2 Parameter Estimation We now use a particular implementation of a centralized system and the model of a distributed system as examples from which we estimate the parameters of the analysis from the previous section. We perform the experiments using an optimized version of Terrier [11] for both indexing documents and processing queries, on a single machine with a Pentium 4 at 2GHz and 1GB of RAM. We indexed the documents from the UK-2006 dataset, without removing stop words or applying stemming. The posting lists in the inverted file consist of pairs of document identifier and term frequency. We compress the document identifier gaps using Elias gamma encoding, and the 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1 0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1 Queryvolume Space precomputed answers posting lists Figure 9: Cache saturation as a function of size. Table 2: Ratios between the average time to evaluate a query and the average time to return cached answers (centralized and distributed case). Centralized system TR1 TR2 TR1 TR2 Full evaluation 233 1760 707 1140 Partial evaluation 99 1626 493 798 LAN system TRL 1 TRL 2 TR L 1 TR L 2 Full evaluation 242 1769 716 1149 Partial evaluation 108 1635 502 807 WAN system TRW 1 TRW 2 TR W 1 TR W 2 Full evaluation 5001 6528 5475 5908 Partial evaluation 4867 6394 5270 5575 term frequencies in documents using unary encoding [16]. The size of the inverted file is 1,189Mb. A stored answer requires 1264 bytes, and an uncompressed posting takes 8 bytes. From Table 1, we obtain L = (8·# of postings) 1264·# of terms = 0.75 and L = Inverted file size 1264·# of terms = 0.26. We estimate the ratio TR = T/Tc between the average time T it takes to evaluate a query and the average time Tc it takes to return a stored answer for the same query, in the following way. Tc is measured by loading the answers for 100,000 queries in memory, and answering the queries from memory. The average time is Tc = 0.069ms. T is measured by processing the same 100,000 queries (the first 10,000 queries are used to warm-up the system). For each query, we remove stop words, if there are at least three remaining terms. The stop words correspond to the terms with a frequency higher than the number of documents in the index. We use a document-at-a-time approach to retrieve documents containing all query terms. The only disk access required during query processing is for reading compressed posting lists from the inverted file. We perform both full and partial evaluation of answers, because some queries are likely to retrieve a large number of documents, and only a fraction of the retrieved documents will be seen by users. In the partial evaluation of queries, we terminate the processing after matching 10,000 documents. The estimated ratios TR are presented in Table 2. Figure 10 shows for a sample of queries the workload of the system with partial query evaluation and compressed posting lists. The x-axis corresponds to the total time the system spends processing a particular query, and the vertical axis corresponds to the sum t∈q fq · fd(t). Notice that the total number of postings of the query-terms does not necessarily provide an accurate estimate of the workload imposed on the system by a query (which is the case for full evaluation and uncompressed lists). 0 0.2 0.4 0.6 0.8 1 0 0.2 0.4 0.6 0.8 1 Totalpostingstoprocessquery(normalized) Total time to process query (normalized) Partial processing of compressed postings query len = 1 query len in [2,3] query len in [4,8] query len > 8 Figure 10: Workload for partial query evaluation with compressed posting lists. The analysis of the previous section also applies to a distributed retrieval system in one or multiple sites. Suppose that a document partitioned distributed system is running on a cluster of machines interconnected with a local area network (LAN) in one site. The broker receives queries and broadcasts them to the query processors, which answer the queries and return the results to the broker. Finally, the broker merges the received answers and generates the final set of answers (we assume that the time spent on merging results is negligible). The difference between the centralized architecture and the document partition architecture is the extra communication between the broker and the query processors. Using ICMP pings on a 100Mbps LAN, we have measured that sending the query from the broker to the query processors which send an answer of 4,000 bytes back to the broker takes on average 0.615ms. Hence, TRL = TR + 0.615ms/0.069ms = TR + 9. In the case when the broker and the query processors are in different sites connected with a wide area network (WAN), we estimated that broadcasting the query from the broker to the query processors and getting back an answer of 4,000 bytes takes on average 329ms. Hence, TRW = TR + 329ms/0.069ms = TR + 4768. 6.3 Simulation Results We now address the problem of finding the optimal tradeoff between caching query answers and caching posting lists. To make the problem concrete we assume a fixed budget M on the available memory, out of which x units are used for caching query answers and M − x for caching posting lists. We perform simulations and compute the average response time as a function of x. Using a part of the query log as training data, we first allocate in the cache the answers to the most frequent queries that fit in space x, and then we use the rest of the memory to cache posting lists. For selecting posting lists we use the QtfDf algorithm, applied to the training query log but excluding the queries that have already been cached. In Figure 11, we plot the simulated response time for a centralized system as a function of x. For the uncompressed index we use M = 1GB, and for the compressed index we use M = 0.5GB. In the case of the configuration that uses partial query evaluation with compressed posting lists, the lowest response time is achieved when 0.15GB out of the 0.5GB is allocated for storing answers for queries. We obtained similar trends in the results for the LAN setting. Figure 12 shows the simulated workload for a distributed system across a WAN. In this case, the total amount of memory is split between the broker, which holds the cached 400 500 600 700 800 900 1000 1100 1200 0 0.2 0.4 0.6 0.8 1 Averageresponsetime Space (GB) Simulated workload -- single machine full / uncompr / 1 G partial / uncompr / 1 G full / compr / 0.5 G partial / compr / 0.5 G Figure 11: Optimal division of the cache in a server. 3000 3500 4000 4500 5000 5500 6000 0 0.2 0.4 0.6 0.8 1 Averageresponsetime Space (GB) Simulated workload -- WAN full / uncompr / 1 G partial / uncompr / 1 G full / compr / 0.5 G partial / compr / 0.5 G Figure 12: Optimal division of the cache when the next level requires WAN access. answers of queries, and the query processors, which hold the cache of posting lists. According to the figure, the difference between the configurations of the query processors is less important because the network communication overhead increases the response time substantially. When using uncompressed posting lists, the optimal allocation of memory corresponds to using approximately 70% of the memory for caching query answers. This is explained by the fact that there is no need for network communication when the query can be answered by the cache at the broker. 7. EFFECT OF THE QUERY DYNAMICS For our query log, the query distribution and query-term distribution change slowly over time. To support this claim, we first assess how topics change comparing the distribution of queries from the first week in June, 2006, to the distribution of queries for the remainder of 2006 that did not appear in the first week in June. We found that a very small percentage of queries are new queries. The majority of queries that appear in a given week repeat in the following weeks for the next six months. We then compute the hit rate of a static cache of 128, 000 answers trained over a period of two weeks (Figure 13). We report hit rate hourly for 7 days, starting from 5pm. We observe that the hit rate reaches its highest value during the night (around midnight), whereas around 2-3pm it reaches its minimum. After a small decay in hit rate values, the hit rate stabilizes between 0.28, and 0.34 for the entire week, suggesting that the static cache is effective for a whole week after the training period. 0.26 0.27 0.28 0.29 0.3 0.31 0.32 0.33 0.34 0.35 0.36 0.37 0 20 40 60 80 100 120 140 160 Hit-rate Time Hits on the frequent queries of distances Figure 13: Hourly hit rate for a static cache holding 128,000 answers during the period of a week. The static cache of posting lists can be periodically recomputed. To estimate the time interval in which we need to recompute the posting lists on the static cache we need to consider an efficiency/quality trade-off: using too short a time interval might be prohibitively expensive, while recomputing the cache too infrequently might lead to having an obsolete cache not corresponding to the statistical characteristics of the current query stream. We measured the effect on the QtfDf algorithm of the changes in a 15-week query stream (Figure 14). We compute the query term frequencies over the whole stream, select which terms to cache, and then compute the hit rate on the whole query stream. This hit rate is as an upper bound, and it assumes perfect knowledge of the query term frequencies. To simulate a realistic scenario, we use the first 6 (3) weeks of the query stream for computing query term frequencies and the following 9 (12) weeks to estimate the hit rate. As Figure 14 shows, the hit rate decreases by less than 2%. The high correlation among the query term frequencies during different time periods explains the graceful adaptation of the static caching algorithms to the future query stream. Indeed, the pairwise correlation among all possible 3-week periods of the 15-week query stream is over 99.5%. 8. CONCLUSIONS Caching is an effective technique in search engines for improving response time, reducing the load on query processors, and improving network bandwidth utilization. We present results on both dynamic and static caching. Dynamic caching of queries has limited effectiveness due to the high number of compulsory misses caused by the number of unique or infrequent queries. Our results show that in our UK log, the minimum miss rate is 50% using a working set strategy. Caching terms is more effective with respect to miss rate, achieving values as low as 12%. We also propose a new algorithm for static caching of posting lists that outperforms previous static caching algorithms as well as dynamic algorithms such as LRU and LFU, obtaining hit rate values that are over 10% higher compared these strategies. We present a framework for the analysis of the trade-off between caching query results and caching posting lists, and we simulate different types of architectures. Our results show that for centralized and LAN environments, there is an optimal allocation of caching query results and caching of posting lists, while for WAN scenarios in which network time prevails it is more important to cache query results. 0.45 0.5 0.55 0.6 0.65 0.7 0.75 0.8 0.85 0.9 0.95 0.1 0.2 0.3 0.4 0.5 0.6 0.7 Hitrate Cache size Dynamics of static QTF/DF caching policy perfect knowledge 6-week training 3-week training Figure 14: Impact of distribution changes on the static caching of posting lists. 9. REFERENCES [1] V. N. Anh and A. Moffat. Pruned query evaluation using pre-computed impacts. In ACM CIKM, 2006. [2] R. A. Baeza-Yates and F. Saint-Jean. A three level search engine index based in query log distribution. In SPIRE, 2003. [3] C. Buckley and A. F. Lewit. Optimization of inverted vector searches. In ACM SIGIR, 1985. [4] S. B¨uttcher and C. L. A. Clarke. A document-centric approach to static index pruning in text retrieval systems. In ACM CIKM, 2006. [5] P. Cao and S. Irani. Cost-aware WWW proxy caching algorithms. In USITS, 1997. [6] P. Denning. Working sets past and present. IEEE Trans. on Software Engineering, SE-6(1):64-84, 1980. [7] T. Fagni, R. Perego, F. Silvestri, and S. Orlando. Boosting the performance of web search engines: Caching and prefetching query results by exploiting historical usage data. ACM Trans. Inf. Syst., 24(1):51-78, 2006. [8] R. Lempel and S. Moran. Predictive caching and prefetching of query results in search engines. In WWW, 2003. [9] X. Long and T. Suel. Three-level caching for efficient query processing in large web search engines. In WWW, 2005. [10] E. P. Markatos. On caching search engine query results. Computer Communications, 24(2):137-143, 2001. [11] I. Ounis, G. Amati, V. Plachouras, B. He, C. Macdonald, and C. Lioma. Terrier: A High Performance and Scalable Information Retrieval Platform. In SIGIR Workshop on Open Source Information Retrieval, 2006. [12] V. V. Raghavan and H. Sever. On the reuse of past optimal queries. In ACM SIGIR, 1995. [13] P. C. Saraiva, E. S. de Moura, N. Ziviani, W. Meira, R. Fonseca, and B. Riberio-Neto. Rank-preserving two-level caching for scalable search engines. In ACM SIGIR, 2001. [14] D. R. Slutz and I. L. Traiger. A note on the calculation of average working set size. Communications of the ACM, 17(10):563-565, 1974. [15] T. Strohman, H. Turtle, and W. B. Croft. Optimization strategies for complex queries. In ACM SIGIR, 2005. [16] I. H. Witten, T. C. Bell, and A. Moffat. Managing Gigabytes: Compressing and Indexing Documents and Images. John Wiley & Sons, Inc., NY, 1994. [17] N. E. Young. On-line file caching. Algorithmica, 33(3):371-383, 2002.",
    "original_translation": "El impacto del almacenamiento en caché en los motores de búsqueda ricardo baza-yates1 rbaeza@acm.org aristides gionis1 gionis@yahoo-inc.com flavio Junqueira1 fpj@yahoo-inc.com.com Fabrizio Silvestri2 f.silvestri@isti.cnr.it 1 Yahoo! Investigación Barcelona 2 ISTI - CNR Barcelona, España Pisa, Italia Resumen En este documento Estudiamos las compensaciones en el diseño de sistemas de almacenamiento de almacenamiento eficientes para motores de búsqueda web. Exploramos el impacto de diferentes enfoques, como el almacenamiento en caché estático frente a la dinámica y los resultados de la consulta de almacenamiento en caché frente a las listas de publicación de almacenamiento en caché. Utilizando un registro de consultas que abarca todo un año, exploramos las limitaciones del almacenamiento en caché y demostramos que las listas de publicación de almacenamiento en caché pueden lograr tasas de éxito más altas que las respuestas de la consulta en caché. Proponemos un nuevo algoritmo para el almacenamiento en caché estático de las listas de publicación, lo que supera a los métodos anteriores. También estudiamos el problema de encontrar la forma óptima de dividir el caché estático entre respuestas y listas de publicación. Finalmente, medimos cómo los cambios en el registro de consultas afectan la efectividad del almacenamiento en caché estático, dada nuestra observación de que la distribución de las consultas cambia lentamente con el tiempo. Nuestros resultados y observaciones son aplicables a diferentes niveles de la jerarquía de acceso de datos, por ejemplo, para una capa de memoria/disco o una capa de corredor/servidor remoto. Categorías y descriptores de sujetos H.3.3 [Almacenamiento y recuperación de información]: Búsqueda y recuperación de información - Proceso de búsqueda;H.3.4 [Algoritmos de términos generales de almacenamiento y recuperación de la información]: Sistemas y software - Sistemas distribuidos, Evaluación del desempeño (eficiencia y efectividad), Experimentación 1. Introducción Millones de consultas se envían diariamente a los motores de búsqueda web, y los usuarios tienen altas expectativas de la calidad y velocidad de las respuestas. A medida que la red de búsqueda se vuelve cada vez más grande, con más de 20 mil millones de páginas para indexar, evaluar una sola consulta requiere procesar grandes cantidades de datos. En tal entorno, para lograr un tiempo de respuesta rápido y para aumentar el rendimiento de la consulta, usar un caché es crucial. El uso principal de una memoria de caché es acelerar el cálculo explotando datos utilizados con frecuencia o recientemente, aunque reducir la carga de trabajo a los servidores de fondo también es un objetivo importante. El almacenamiento en caché se puede aplicar en diferentes niveles con latencias de respuesta crecientes o requisitos de procesamiento. Por ejemplo, los diferentes niveles pueden corresponder a la memoria principal, el disco o los recursos en una red de área local o amplia. La decisión de qué almacenar en caché es fuera de línea (estático) o en línea (dinámico). Un caché estático se basa en información histórica y se actualiza periódicamente. Un caché dinámico reemplaza las entradas de acuerdo con la secuencia de solicitudes. Cuando llega una nueva solicitud, el sistema de caché decide si desalojar una entrada del caché en el caso de una falla de caché. Dichas decisiones en línea se basan en una política de caché, y se han estudiado varias políticas diferentes en el pasado. Para un motor de búsqueda, hay dos formas posibles de usar una memoria de caché: respuestas de almacenamiento en caché: a medida que el motor devuelve las respuestas a una consulta en particular, puede decidir almacenar estas respuestas para resolver consultas futuras. Términos de almacenamiento en caché: a medida que el motor evalúa una consulta en particular, puede decidir almacenar en la memoria las listas de publicación de los términos de consulta involucrados. A menudo, todo el conjunto de listas de publicación no encaja en la memoria y, en consecuencia, el motor tiene que seleccionar un pequeño conjunto para mantener en la memoria y acelerar el procesamiento de consultas. Devolver una respuesta a una consulta que ya existe en el caché es más eficiente que calcular la respuesta utilizando listas de publicación en caché. Por otro lado, las consultas previamente invisibles ocurren con más frecuencia que los términos no vistos anteriormente, lo que implica una tasa de fallas más alta para las respuestas almacenadas en caché. El almacenamiento en caché de las listas de publicación tiene desafíos adicionales. Como las listas de publicación tienen un tamaño variable, almacenarlos en caché dinámicamente no es muy eficiente, debido a la complejidad en términos de eficiencia y espacio, y la distribución sesgada de la corriente de consulta, como se muestra más adelante. El almacenamiento en caché estático de las listas de publicación plantea aún más desafíos: al decidir qué términos almacenados en caché se enfrenta la compensación entre los términos y términos consultados frecuentemente con pequeñas listas de publicación que son eficientes en el espacio. Finalmente, antes de decidir adoptar una política de almacenamiento en caché estático, el flujo de consulta debe analizarse para verificar que sus características no cambien rápidamente con el tiempo. Broker Caching Static en caché Listas de las respuestas de caché dinámico/estático Procesador de consultas locales Disco Nivel de almacenamiento en caché Acceso a la red remota Acceso a la red remota Figura 1: Un nivel de almacenamiento en caché en una arquitectura de búsqueda distribuida. En este artículo exploramos las compensaciones en el diseño de cada nivel de caché, lo que demuestra que el problema es el mismo y solo cambian unos pocos parámetros. En general, suponemos que cada nivel de almacenamiento en caché en una arquitectura de búsqueda distribuida es similar al que se muestra en la Figura 1. Utilizamos un registro de consultas que abarca todo un año para explorar las limitaciones de las respuestas de consulta de almacenamiento dinámico en caché o publicar listas para términos de consulta. Más concretamente, nuestras principales conclusiones son que: • Las respuestas de consulta de almacenamiento en caché dan como resultado relaciones más bajas en comparación con el almacenamiento en caché de las listas de publicación para los términos de la consulta, pero es más rápido porque no hay necesidad de evaluación de consultas. Proporcionamos un marco para el análisis de la compensación entre el almacenamiento en caché estático de las respuestas de consulta y las listas de publicación;• El almacenamiento en caché estático de los términos puede ser más efectivo que el almacenamiento en caché dinámico con, por ejemplo, LRU. Proporcionamos algoritmos basados en el problema de la mochila para seleccionar las listas de publicación para poner en un caché estático, y mostramos mejoras sobre el trabajo anterior, logrando una relación de éxito durante el 90%;• Los cambios de la distribución de la consulta a lo largo del tiempo tienen poco impacto en el almacenamiento en caché estático. El resto de este documento está organizado de la siguiente manera. Las secciones 2 y 3 resumen el trabajo relacionado y caracterizan los conjuntos de datos que utilizamos. La Sección 4 analiza las limitaciones del almacenamiento en caché dinámico. Las secciones 5 y 6 introducen algoritmos para el almacenamiento en caché de listas de publicación y un marco teórico para el análisis del almacenamiento en caché estático, respectivamente. La Sección 7 analiza el impacto de los cambios en la distribución de la consulta en el almacenamiento en caché estático, y la Sección 8 proporciona comentarios finales.2. Trabajo relacionado Hay un gran trabajo dedicado a la optimización de consultas. Buckley y Lewit [3], en uno de los primeros trabajos, adoptan un enfoque de término a tiempo para decidir cuándo las listas invertidas no necesitan ser examinadas más a fondo. Ejemplos más recientes demuestran que los principales documentos K para una consulta se pueden devolver sin la necesidad de evaluar el conjunto completo de listas de publicación [1, 4, 15]. Aunque estos enfoques buscan mejorar la eficiencia del procesamiento de consultas, difieren de nuestro trabajo actual, ya que no consideran el almacenamiento en caché. Pueden considerarse separados y complementarios a un enfoque basado en caché. Raghavan y Sever [12], en uno de los primeros documentos sobre la explotación del historial de consultas de usuarios, proponen usar una base de consulta, basado en un conjunto de consultas óptimas persistentes presentadas en el pasado, para mejorar la efectividad de la recuperación para consultas futuras similares. Markatos [10] muestra la existencia de localidad temporal en consultas y compara el rendimiento de diferentes políticas de almacenamiento en caché. Basado en las observaciones de Markatos, Lempel y Moran proponen una nueva política de almacenamiento en caché, llamada almacenamiento en caché impulsado probabilístico, al intentar estimar la distribución de probabilidad de todas las consultas posibles presentadas a un motor de búsqueda [8]. Fagni et al.Siga el trabajo de Markatos mostrando que combinar políticas de almacenamiento en caché estático y dinámico junto con una política de captación previa adaptativa logra una alta relación HIT [7]. A diferencia de nuestro trabajo, consideran almacenamiento en caché y captación previa de páginas de resultados. Como los sistemas a menudo son jerárquicos, también ha habido un esfuerzo en arquitecturas de varios niveles. Saraiva et al.Proponga una nueva arquitectura para los motores de búsqueda web utilizando un sistema de almacenamiento de almacenamiento dinámico de dos niveles [13]. Su objetivo para tales sistemas ha sido mejorar el tiempo de respuesta para los motores jerárquicos. En su arquitectura, ambos niveles usan una política de desalojo de LRU. Encuentran que el caché de segundo nivel puede reducir efectivamente el tráfico de disco, lo que aumenta el rendimiento general. Baeza-Yates y Saint-Jean proponen una organización índice de tres niveles [2]. Long y Suel proponen un sistema de almacenamiento en caché estructurado según tres niveles diferentes [9]. El nivel intermedio contiene pares de términos frecuentes y almacena las intersecciones de las listas invertidas correspondientes. Estos dos últimos documentos están relacionados con los nuestros, ya que explotan diferentes estrategias de almacenamiento en caché en diferentes niveles de la jerarquía de memoria. Finalmente, nuestro algoritmo de almacenamiento en caché estático para publicar listas en la Sección 5 utiliza la frecuencia/tamaño de relación para evaluar la bondad de un elemento en caché. Se han utilizado ideas similares en el contexto del almacenamiento en caché de archivos [17], el almacenamiento en caché web [5] e incluso el almacenamiento en caché de las listas de publicación [9], pero en todos los casos en un entorno dinámico. Hasta donde sabemos, somos los primeros en utilizar este enfoque para el almacenamiento en caché estático de las listas de publicación.3. Caracterización de datos Nuestros datos consisten en un rastreo de documentos del dominio del Reino Unido, y los registros de consulta de un año de consultas enviadas a http://www.yahoo.co.uk de noviembre de 2005 a noviembre de 2006. En nuestros registros, el 50% del volumen total de consultas es único. La longitud promedio de la consulta es de 2.5 términos, con la consulta más larga que tiene 731 términos.1E-07 1E-06 1E-05 1E-04 0.001 0.01 0.1 1 1e-08 1e-07 1e-06 1e-05 1e-04 0.001 0.01 0.1 1 Frecuencia (normalizado) Rango de frecuencia (normalizado) Figura 2: La distribución de la distribución de la distribución de la distribución deconsultas (curva inferior) y términos de consulta (curva media) en el registro de consultas. La distribución de frecuencias de documentos de términos en el conjunto de datos del Reino Unido-2006 (curva superior). La Figura 2 muestra las distribuciones de consultas (curva inferior) y términos de consulta (curva media). El eje x representa el rango de frecuencia normalizado de la consulta o término.(La consulta más frecuente aparece más cerca del eje y). El eje Y es la Tabla 1: Estadísticas de la muestra del Reino Unido-2006. UK-2006 Estadísticas de muestra # de documentos 2,786,391 # de términos 6,491,374 # de tokens 2,109,512,558 La frecuencia normalizada para una consulta dada (o término). Como se esperaba, la distribución de frecuencias de consulta y frecuencias de términos de consulta sigue las distribuciones de ley de energía, con pendiente de 1.84 y 2.26, respectivamente. En esta figura, las frecuencias de consulta se calcularon cuando aparecen en los registros sin normalización para el caso o el espacio en blanco. Los términos de consulta (curva media) se han normalizado para el caso, al igual que los términos en la recopilación de documentos. La recopilación de documentos que utilizamos para nuestros experimentos es un resumen del dominio del Reino Unido que se arrastró en mayo de 2006.1 Este resumen corresponde a un máximo de 400 documentos rastreados por host, utilizando una estrategia de rastreo de amplitud, que comprende 15 GB. La distribución de las frecuencias de documentos de los términos en la colección sigue una distribución de la ley de potencia con la pendiente 2.38 (curva superior en la Figura 2). Las estadísticas de la colección se muestran en la Tabla 1. Medimos la correlación entre la frecuencia del documento de los términos en la colección y el número de consultas que contienen un término particular en el registro de consultas para ser 0.424. En la Figura 3 se muestra una gráfica de dispersión para una muestra aleatoria de términos. En este experimento, los términos se han convertido en minúsculas tanto en las consultas como en los documentos para que las frecuencias sean comparables.1E-07 1E-06 1E-05 1E-04 0.001 0.01 0.1 1 1e-06 1E-05 1E-04 0.001 0.01 0.1 1 Frecuencia de documento de consecuencia Figura 3: Gráfico de dispersión normalizado de las frecuencias de documento a término frente a las frecuencias de término de consultas.4. El almacenamiento en caché de consultas y términos almacenado en caché se basa en la suposición de que hay localidad en el flujo de solicitudes. Es decir, debe haber suficiente repetición en el flujo de solicitudes y dentro de los intervalos de tiempo que permiten que una memoria de caché de tamaño razonable sea efectiva. En el registro de consultas que utilizamos, el 88% de las consultas únicas son consultas singleton, y el 44% son consultas singleton fuera de todo el volumen. Por lo tanto, de todas las consultas en la corriente que componen el registro de consultas, el umbral superior en la relación HIT es del 56%. Esto se debe a que solo el 56% de todas las consultas comprenden consultas que tienen múltiples ocurrencias. Sin embargo, es importante observar que no todas las consultas en este 56% pueden ser éxitos de caché debido a las fallas obligatorias. Una misión obligatoria 1 La colección está disponible en la Universidad de Milán: http://law.dsi.unimi.it/. URL recuperada 05/2007.0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1 240 260 280 300 320 340 360 Número de contenedores Número de contenedores Términos Términos Diff Consultas totales de consultas únicas Términos únicos Consulta Diferencia Figura 4: Llegada para términos y consultas.sucede cuando el caché recibe una consulta por primera vez. Esto es diferente de las fallas de capacidad, que ocurren debido a las limitaciones de espacio en la cantidad de memoria que usa el caché. Si consideramos un caché con memoria infinita, entonces la relación HIT es del 50%. Tenga en cuenta que para un caché infinito no hay capacidad de capacidad. Como mencionamos antes, otra posibilidad es almacenar en caché las listas de publicaciones de términos. Intuitivamente, esto da más libertad en la utilización del contenido de caché para responder a las consultas porque los términos almacenados en caché podrían formar una nueva consulta. Por otro lado, necesitan más espacio. A diferencia de las consultas, la fracción de los términos singleton en el volumen total de términos es menor. En nuestro registro de consultas, solo el 4% de los términos aparecen una vez, pero esto representa el 73% del vocabulario de los términos de consulta. Mostramos en la Sección 5 que el almacenamiento en caché de una pequeña fracción de términos, mientras que contabilizar los términos que aparecen en muchos documentos, es potencialmente muy efectivo. La Figura 4 muestra varios gráficos correspondientes a la tasa de llegada normalizada para diferentes casos utilizando días como contenedores. Es decir, trazamos el número normalizado de elementos que aparecen en un día. Este gráfico muestra solo un período de 122 días, y normalizamos los valores por el valor máximo observado durante todo el período del registro de consulta. Las consultas totales y los términos totales corresponden al volumen total de consultas y términos, respectivamente. Las consultas únicas y los términos únicos corresponden a la tasa de llegada de consultas y términos únicos. Finalmente, la diferencia de consulta y los términos diff corresponden a la diferencia entre las curvas para total y único. En la Figura 4, como se esperaba, el volumen de términos es mucho más alto que el volumen de consultas. La diferencia entre el número total de términos y el número de términos únicos es mucho mayor que la diferencia entre el número total de consultas y el número de consultas únicas. Esta observación implica que los términos se repiten significativamente más que consultas. Si usamos contenedores más pequeños, digamos de una hora, entonces la relación de un solo volumen es mayor para los términos y consultas porque deja menos espacio para la repetición. También estimamos la carga de trabajo utilizando la frecuencia de documentos de los términos como una medida de cuánto trabajo impone una consulta a un motor de búsqueda. Encontramos que sigue de cerca la tasa de llegada para los términos que se muestran en la Figura 4. Para demostrar el efecto de un caché dinámico en la distribución de frecuencia de consulta de la Figura 2, trazamos el mismo gráfico de frecuencia, pero ahora considerando la frecuencia de las consultas Figura 5: Gráfico de frecuencia después del caché LRU.Después de pasar por un caché LRU. En una falla de caché, un caché de LRU decide una entrada para desalojar el uso de la información sobre la recuperación de consultas. En este gráfico, las consultas más frecuentes no son las mismas consultas que eran más frecuentes antes del caché. Es posible que las consultas que sean más frecuentes después del caché tengan características diferentes, y ajustar el motor de búsqueda a consultas frecuentes antes de que el caché pueda degradar el rendimiento de las consultas no consultadas. La frecuencia máxima después del almacenamiento en caché es inferior al 1% de la frecuencia máxima antes del caché, lo que demuestra que el caché es muy efectivo para reducir la carga de consultas frecuentes. Si volvemos a clasificar las consultas de acuerdo con la frecuencia posterior a la atención, la distribución sigue siendo una ley de potencia, pero con un valor mucho menor para la frecuencia más alta. Al discutir la efectividad del almacenamiento en caché dinámico, una métrica importante es la tasa de fallas de caché. Para analizar la tasa de fallas de caché para diferentes restricciones de memoria, utilizamos el modelo de conjunto de trabajo [6, 14]. Un conjunto de trabajo, informalmente, es el conjunto de referencias con la que una aplicación o un sistema operativo está trabajando actualmente. El modelo utiliza tales conjuntos en una estrategia que intenta capturar la localidad temporal de las referencias. La estrategia del conjunto de trabajo luego consiste en mantener en la memoria solo los elementos a los que se hace referencia en los pasos θ anteriores de la secuencia de entrada, donde θ es un parámetro configurable correspondiente al tamaño de la ventana. Originalmente, los conjuntos de trabajo se han utilizado para los algoritmos de reemplazo de la página de los sistemas operativos, y considerar tal estrategia en el contexto de los motores de búsqueda es interesante por tres razones. Primero, captura la cantidad de localidad de consultas y términos en una secuencia de consultas. La localidad en este caso se refiere a la frecuencia de consultas y términos en una ventana de tiempo. Si muchas consultas aparecen varias veces en una ventana, entonces la localidad es alta. En segundo lugar, permite un análisis de la tasa de fallas esperado dadas diferentes restricciones de memoria. Tercero, los establecimientos de trabajo capturan aspectos de algoritmos de almacenamiento de almacenamiento eficientes como LRU. LRU supone que las referencias más lejos en el pasado tienen menos probabilidades de ser referenciadas en el presente, lo que está implícito en el concepto de conjuntos de trabajo [14]. La Figura 6 traza la tasa de fallas para diferentes tamaños de conjunto de trabajo, y consideramos conjuntos de trabajo de consultas y términos. Los tamaños del conjunto de trabajo se normalizan contra el número total de consultas en el registro de consultas. En el gráfico para consultas, hay una disminución aguda hasta aproximadamente 0.01, y la velocidad a la que disminuye la tasa de fallas disminuye a medida que aumentamos el tamaño del conjunto de trabajo en más de 0.01. Finalmente, el valor mínimo que alcanza es la tasa de fallas del 50%, no se muestra en la cifra, ya que hemos cortado la cola de la curva para fines de presentación.0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1 0 0.05 0.1 0.15 0.2 Missro Normalización de trabajo de trabajo de trabajo Términos Figura 6: Tasa de fallas en función del tamaño del conjunto de trabajo.1 10 100 1000 10000 100000 1E+06 Distancia de frecuencia Figura 7: Distribución de distancias expresadas en términos de consultas distintas. En comparación con la curva de consulta, observamos que la tasa de fallas mínima para los términos es sustancialmente menor. La tasa de fallas también cae bruscamente en valores de hasta 0.01, y disminuye mínimamente para valores más altos. Sin embargo, el valor mínimo es ligeramente superior al 10%, que es mucho menor que el valor mínimo para la secuencia de consultas. Esto implica que con dicha política es posible alcanzar más del 80% de la tasa de aciertos, si consideramos el almacenamiento en caché de las listas de publicación dinámica para términos en lugar de respuestas de almacenamiento en caché para consultas. Este resultado no considera el espacio requerido para cada unidad almacenada en la memoria de caché, o la cantidad de tiempo que lleva armar una respuesta a una consulta de usuario. Analizamos estos problemas con más cuidado más adelante en este documento. También es interesante observar el histograma de la Figura 7, que es un paso intermedio en el cálculo del gráfico de la tasa de fallecimiento. Reporta la distribución de distancias entre repeticiones de la misma consulta frecuente. La distancia en la gráfica se mide en el número de consultas distintas que separan una consulta y su repetición, y considera solo consultas que aparecen al menos 10 veces. De las Figuras 6 y 7, concluimos que incluso si establecemos el tamaño de la consulta responde al caché a un número relativamente grande de entradas, la tasa de fallas es alta. Por lo tanto, el almacenamiento en caché de las listas de publicaciones de términos tiene el potencial de mejorar la relación HIT. Esto es lo que exploramos a continuación.5. Listas de publicación de almacenamiento en caché La sección anterior muestra que las listas de publicación de almacenamiento en caché pueden obtener una tasa de aciertos más alta en comparación con las respuestas de consulta en caché. En esta sección, estudiamos el problema de cómo seleccionar listas de publicación para colocar en una cierta cantidad de memoria disponible, suponiendo que todo el índice sea mayor que la cantidad de memoria disponible. Las listas de publicación tienen un tamaño variable (de hecho, su distribución de tamaño sigue una ley de energía), por lo que es beneficioso que una política de almacenamiento en caché considere los tamaños de las listas de publicación. Consideramos tanto el almacenamiento en caché dinámico como estático. Para el almacenamiento en caché dinámico, usamos dos políticas bien conocidas, LRU y LFU, así como un algoritmo modificado que tiene en cuenta el tamaño de la lista de publicación. Antes de discutir las estrategias de almacenamiento en caché estático, presentamos cierta notación. Utilizamos FQ (t) para denotar la frecuencia a término de consulta de un término t, es decir, el número de consultas que contienen t en el registro de consulta y FD (t) para denotar la frecuencia del documento de t, es decir, el númerode documentos en la colección en la que aparece el término t. La primera estrategia que consideramos es el algoritmo propuesto por Baeza-Yates y Saint-Jean [2], que consiste en seleccionar las listas de publicación de los términos con las frecuencias de consulta más altas FQ (t). Llamamos a este algoritmo QTF. Observamos que hay una compensación entre FQ (t) y FD (t). Los términos con alto FQ (t) son útiles para mantener en el caché porque se consultan a menudo. Por otro lado, los términos con alto FD (t) no son buenos candidatos porque corresponden a largas listas de publicación y consumen una cantidad sustancial de espacio. De hecho, el problema de seleccionar las mejores listas de publicación para el caché estático corresponde al problema estándar de la mochila: dada una mochila de capacidad fija y un conjunto de n elementos, como el elemento I-Th tiene valor CI y tamaño SI,Seleccione el conjunto de elementos que caben en la mochila y maximicen el valor general. En nuestro caso, el valor corresponde a FQ (t) y el tamaño corresponde a FD (t). Por lo tanto, empleamos un algoritmo simple para el problema de la mochila, que es seleccionar las listas de publicación de los términos con los valores más altos de la relación fq (t) fd (t). Llamamos a este algoritmo QTFDF. Intentamos otras variaciones considerando las frecuencias de consulta en lugar de las frecuencias de término, pero la ganancia fue mínima en comparación con la complejidad agregada. Además de los dos algoritmos estáticos anteriores, consideramos los siguientes algoritmos para el almacenamiento en caché dinámico: • LRU: un algoritmo LRU estándar, pero es posible que deba desalojarse muchas listas de publicación (en orden de uso menos reciente) hasta que haya suficiente espacio en el espacio en el espacio en elmemoria para colocar la lista de publicaciones de acceso actualmente;• LFU: un algoritmo estándar de LFU (desalojo de los menos utilizados), con la misma modificación que la LRU;• Dyn-Qtfdf: una versión dinámica del algoritmo QTFDF;Evice de la memoria caché el término (s) con la relación FQ (t) FD (t) más baja. El rendimiento de todos los algoritmos anteriores durante 15 semanas del registro de consultas y el conjunto de datos del Reino Unido se muestra en la Figura 8. El rendimiento se mide con la tasa de aciertos. El tamaño del caché se mide como una fracción del espacio total requerido para almacenar las listas de publicación de todos los términos. Para los algoritmos dinámicos, cargamos el caché con los términos en orden de FQ (t) y dejamos que el caché se calienta por 1 millón de consultas. Para los algoritmos estáticos, asumimos el conocimiento completo de las frecuencias fq (t), es decir, estimamos FQ (t) de toda la corriente de consulta. Como mostramos en la Sección 7, los resultados no cambian mucho si calculamos las frecuencias a plazo de consulta utilizando las primeras 3 o 4 semanas del registro de la consulta y medimos la tasa de aciertos en el resto.0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1 0.1 0.2 0.3 0.4 0.5 0.6 0.7 HITRATE Tamaño de caché Publicación de caché de la publicación Listas de Publicación estática QTF/DF LRU LFU DYN-QTF/DF QTF Figura 8: Tasa de aciertos de diferentes estrategias para las listas de publicaciones en caché. La observación más importante de nuestros experimentos es que el algoritmo estático QTFDF tiene una mejor tasa de éxito que todos los algoritmos dinámicos. Un beneficio importante que un caché estático es que no requiere desalojo y, por lo tanto, es más eficiente al evaluar las consultas. Sin embargo, si las características del tráfico de consulta cambian con frecuencia con el tiempo, entonces requiere re-populando el caché a menudo o habrá un impacto significativo en la tasa de aciertos.6. Análisis del almacenamiento en caché estático En esta sección, proporcionamos un análisis detallado para el problema de decidir si es preferible a las respuestas de consulta de caché o listas de publicación de caché. Nuestro análisis tiene en cuenta el impacto del almacenamiento en caché entre dos niveles de la jerarquía de acceso de datos. Se puede aplicar en la capa de memoria/disco o en una capa de servidor/servidor remoto como en la arquitectura que discutimos en la introducción. Utilizando un modelo de sistema en particular, obtenemos estimaciones para los parámetros requeridos por nuestro análisis, que posteriormente usamos para decidir la compensación óptima entre las respuestas de consulta en caché y las listas de publicación de almacenamiento en caché.6.1 Modelo analítico Sea m del tamaño del caché medido en las unidades de respuestas (el caché puede almacenar m respuestas de consulta). Suponga que todas las listas de publicación son de la misma longitud L, medidas en unidades de respuesta. Consideramos los siguientes dos casos: (a) un caché que almacena solo respuestas precomputadas, y (b) un caché que almacena solo las listas de publicación. En el primer caso, NC = M respuestas se ajustan en el caché, mientras que en el segundo caso np = m/l de publicaciones de publicaciones se ajustan en el caché. Así, np = nc/l. Tenga en cuenta que aunque las listas de publicación requieren más espacio, podemos combinar términos para evaluar más consultas (o consultas parciales). Para el caso (a), suponga que una respuesta de consulta en el caché puede evaluarse en 1 unidad de tiempo. Para el caso (b), suponga que si las listas de publicación de los términos de una consulta están en la memoria caché, los resultados se pueden calcular en unidades de tiempo TR1, mientras que si las listas de publicación no están en el caché, los resultados se pueden calcular enUnidades de tiempo TR2. Por supuesto TR2> TR1. Ahora queremos comparar el tiempo para responder un flujo de consultas Q en ambos casos. Deje que VC (NC) sea el volumen de las consultas NC más frecuentes. Luego, para el caso (A), tenemos un tiempo general TCA = VC (NC) + TR2 (Q - VC (NC)). Del mismo modo, para el caso (b), deje que VP (NP) sea el número de consultas computables. Luego tenemos tiempo general TP L = TR1VP (NP) + TR2 (Q - VP (NP)). Queremos verificar en qué condiciones tenemos tp l <tca. Tenemos TP L - TCA = (TR2 - 1) VC (NC) - (TR2 - TR1) VP (NP)> 0. La Figura 9 muestra los valores de VP y VC para nuestros datos. Podemos ver que las respuestas de almacenamiento en caché se saturan más rápido y para estos datos particulares no hay un beneficio adicional al usar más del 10% del espacio de índice para el almacenamiento en caché. Como la distribución de la consulta es una ley de potencia con el parámetro α> 1, la consulta más frecuente de I-th aparece con probabilidad proporcional a 1 Iα. Por lo tanto, el volumen VC (n), que es el número total de las N consultas más frecuentes, es VC (N) = V0 N I = 1 Q Iα = γNQ (0 <γn <1). Sabemos que VP (N) crece más rápido que VC (N) y suponemos, basado en resultados experimentales, que la relación es de la forma VP (N) = K VC (N) β. En el peor de los casos, para un caché grande, β → 1. Es decir, ambas técnicas almacenan en caché una fracción constante del volumen general de la consulta. Entonces las listas de publicación de almacenamiento en caché tienen sentido solo si L (TR2 - 1) K (TR2 - TR1)> 1. Si usamos compresión, tenemos L <L y TR1> TR1. Según los experimentos que mostramos más adelante, la compresión siempre es mejor. Para un caché pequeño, estamos interesados en el comportamiento transitorio y luego β> 1, según lo calculado a partir de nuestros datos. En este caso, siempre habrá un punto en el que TP L> TCA para una gran cantidad de consultas. En realidad, en lugar de llenar el caché solo con respuestas o solo con listas de publicación, una mejor estrategia será dividir el espacio total de caché en caché para respuestas y caché para publicar listas. En tal caso, habrá algunas consultas que podrían ser respondidas por ambas partes del caché. Como el caché de respuesta es más rápido, será la primera opción para responder esas consultas. Deje que QNC y QNP sean el conjunto de consultas que pueden responder las respuestas en caché y las listas de publicación en caché, respectivamente. Luego, el tiempo general es t = VC (NC)+TR1V (QNP −QNC)+TR2 (Q - V (QNP ∪QNC)), donde np = (m - nc)/l. Encontrar la división óptima del caché para minimizar el tiempo de recuperación general es un problema difícil de resolver analíticamente. En la Sección 6.3 utilizamos simulaciones para obtener compensaciones óptimas de caché para ejemplos de implementación particulares.6.2 Estimación de parámetros Ahora utilizamos una implementación particular de un sistema centralizado y el modelo de un sistema distribuido como ejemplos de los cuales estimamos los parámetros del análisis de la sección anterior. Realizamos los experimentos utilizando una versión optimizada de Terrier [11] tanto para los documentos de indexación como para las consultas de procesamiento, en una sola máquina con un Pentium 4 a 2GHz y 1 GB de RAM. Indexamos los documentos del conjunto de datos del Reino Unido-2006, sin eliminar las palabras de detención o la aplicación de Stemming. Las listas de publicación en el archivo invertido consisten en pares de identificador de documentos y frecuencia de término. Comprimimos las brechas del identificador del documento utilizando la codificación de Elias gamma, y el 0.1 0.2 0.3 0.3 0.4 0.5 0.6 0.7 0.8 0.8 0.9 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1 Espacio de consulta Respuestas precomputadas Publicado Listas Figura 9: Cache como función de tamaño de tamaño. Tabla 2: Relaciones entre el tiempo promedio para evaluar una consulta y el tiempo promedio para devolver respuestas en caché (caso centralizado y distribuido). Sistema centralizado TR1 TR2 TR1 TR2 Evaluación completa 233 1760 707 1140 Evaluación parcial 99 1626 493 798 Sistema LAN TRL 1 TRL 2 TR L 1 TR L 2 Evaluación completa 242 1769 716 1149 Evaluación parcial 108 1635 502 807 Sistema WAN TRW 1 TRW 2 TR W W1 TR W 2 Evaluación completa 5001 6528 5475 5908 Evaluación parcial 4867 6394 5270 5575 frecuencias a término en documentos que utilizan una codificación unaria [16]. El tamaño del archivo invertido es de 1.189 MB. Una respuesta almacenada requiere 1264 bytes, y una publicación sin comprimir toma 8 bytes. De la Tabla 1, obtenemos l = (8 ·# de publicaciones) 1264 ·# de términos = 0.75 y l = tamaño de archivo invertido 1264 ·# de términos = 0.26. Estimamos la relación tr = t/tc entre el tiempo promedio T Se necesita para evaluar una consulta y el tiempo promedio TC que se necesita para devolver una respuesta almacenada para la misma consulta, de la siguiente manera. TC se mide cargando las respuestas para 100,000 consultas en la memoria y respondiendo las consultas desde la memoria. El tiempo promedio es TC = 0.069ms. T se mide procesando las mismas 100,000 consultas (las primeras 10,000 consultas se utilizan para calentar el sistema). Para cada consulta, eliminamos las palabras de parada, si hay al menos tres términos restantes. Las palabras de parada corresponden a los términos con una frecuencia más alta que el número de documentos en el índice. Utilizamos un enfoque de documento para recuperar documentos que contienen todos los términos de consulta. El único acceso al disco requerido durante el procesamiento de consultas es para leer listas de publicación comprimidas del archivo invertido. Realizamos una evaluación completa y parcial de respuestas, porque es probable que algunas consultas recuperen una gran cantidad de documentos, y solo los usuarios verán solo una fracción de los documentos recuperados. En la evaluación parcial de consultas, terminamos el procesamiento después de igualar 10,000 documentos. Las relaciones estimadas TR se presentan en la Tabla 2. La Figura 10 muestra para una muestra de consultas la carga de trabajo del sistema con evaluación de consultas parcial y listas de publicación comprimidas. El eje x corresponde al tiempo total que el sistema pasa procesando una consulta particular, y el eje vertical corresponde a la suma t∈Q fq · fd (t). Observe que el número total de publicaciones de la consulta-Termos no necesariamente proporciona una estimación precisa de la carga de trabajo impuesta al sistema mediante una consulta (que es el caso de la evaluación completa y las listas sin comprimir).0 0.2 0.4 0.6 0.8 1 0 0.2 0.4 0.6 0.8 1 Total Postingstoprocessquery (normalizado) Tiempo total para procesar la consulta (normalizada) Procesamiento parcial de las publicaciones comprimidas Consulta LEN = 1 Consulta LEN en [2,3] Consulta LEN en [4,8] consultaLEN> 8 Figura 10: Carga de trabajo para evaluación de consultas parcial con listas de publicación comprimidas. El análisis de la sección anterior también se aplica a un sistema de recuperación distribuido en uno o múltiples sitios. Suponga que un sistema distribuido dividido en documento se ejecuta en un clúster de máquinas interconectadas con una red de área local (LAN) en un sitio. El corredor recibe consultas y las transmite a los procesadores de consulta, que responden las consultas y devuelven los resultados al corredor. Finalmente, el corredor fusiona las respuestas recibidas y genera el conjunto final de respuestas (suponemos que el tiempo dedicado a fusionar resultados es insignificante). La diferencia entre la arquitectura centralizada y la arquitectura de partición del documento es la comunicación adicional entre el corredor y los procesadores de consultas. Usando pings ICMP en una LAN de 100 Mbps, hemos medido que enviar la consulta del corredor a los procesadores de consulta que envían una respuesta de 4.000 bytes al corredor lleva en promedio 0.615 ms. Por lo tanto, TRL = TR + 0.615MS/0.069MS = TR + 9. En el caso de que el corredor y los procesadores de consultas se encuentren en diferentes sitios conectados con una red de área amplia (WAN), estimamos que transmitir la consulta del corredor a los procesadores de consulta y recuperar una respuesta de 4.000 bytes toma en promedio 329 ms. Por lo tanto, TRW = TR + 329MS/0.069MS = TR + 4768. 6.3 Resultados de simulación Ahora abordamos el problema de encontrar la compensación óptima entre las respuestas de la consulta de almacenamiento en caché y las listas de publicación de almacenamiento en caché. Para hacer el problema concreto, asumimos un presupuesto fijo en la memoria disponible, de la cual se utilizan X unidades para almacenar en caché las respuestas de consulta y M - x para las listas de publicaciones en caché. Realizamos simulaciones y calculamos el tiempo de respuesta promedio en función de x. Utilizando una parte del registro de consultas como datos de entrenamiento, primero asignamos en la memoria caché las respuestas a las consultas más frecuentes que se ajustan en el espacio X, y luego usamos el resto de la memoria para almacenar en caché las listas de publicación. Para seleccionar listas de publicación, utilizamos el algoritmo QTFDF, aplicado al registro de consultas de capacitación pero excluyendo las consultas que ya se han almacenado en caché. En la Figura 11, trazamos el tiempo de respuesta simulado para un sistema centralizado en función de x. Para el índice sin comprimir usamos M = 1GB, y para el índice comprimido usamos M = 0.5GB. En el caso de la configuración que utiliza evaluación de consultas parcial con listas de publicación comprimidas, el tiempo de respuesta más bajo se logra cuando 0.15 GB de los 0.5 GB se asignan para almacenar respuestas para consultas. Obtuvimos tendencias similares en los resultados para la configuración LAN. La Figura 12 muestra la carga de trabajo simulada para un sistema distribuido a través de una WAN. En este caso, la cantidad total de memoria se divide entre el corredor, que contiene el almacenamiento de trabajo simulado 400 500 600 700 800 900 1000 1100 1200 0 0.2 0.4 0.6 0.8 1 1 1G Parcial / UNCOMPR / 1 g completo / Compr / 0.5 g Parcial / Compr / 0.5 g Figura 11: División óptima del caché en un servidor.3000 3500 4000 4500 5000 5500 6000 0 0.2 0.4 0.6 0.8 1 AveragerSponsetEment Space (GB) Carga de trabajo simulada - WAN Full / Uncomppr / 1 G Parcial / UncompR / 1 G Full / CompR / 0.5 G Parcial / Compr / 0.5 g Figura 12:División óptima del caché Cuando el siguiente nivel requiere acceso WAN.Respuestas de consultas y los procesadores de consulta, que contienen el caché de las listas de publicación. Según la figura, la diferencia entre las configuraciones de los procesadores de consulta es menos importante porque la sobrecarga de comunicación de red aumenta el tiempo de respuesta sustancialmente. Cuando se usa listas de publicación sin comprimir, la asignación óptima de la memoria corresponde a usar aproximadamente el 70% de la memoria para el almacenamiento en caché de las respuestas de consultas. Esto se explica por el hecho de que no hay necesidad de comunicación de red cuando la consulta puede ser respondida por el caché en el corredor.7. Efecto de la dinámica de la consulta para nuestro registro de consultas, la distribución de consultas y la distribución de consultas cambian lentamente con el tiempo. Para respaldar esta afirmación, primero evaluamos cómo los temas cambian la comparación de la distribución de consultas desde la primera semana de junio de 2006, con la distribución de consultas para el resto de 2006 que no apareció en la primera semana de junio. Descubrimos que un porcentaje muy pequeño de consultas son nuevas consultas. La mayoría de las consultas que aparecen en una semana dada se repiten en las siguientes semanas durante los próximos seis meses. Luego calculamos la tasa de aciertos de un caché estático de 128, 000 respuestas entrenadas durante un período de dos semanas (Figura 13). Reportamos la tasa de aciertos por hora durante 7 días, a partir de las 5 p.m. Observamos que la tasa de golpes alcanza su valor más alto durante la noche (alrededor de la medianoche), mientras que alrededor de las 2-3 p.m. alcanza su mínimo. Después de una pequeña descomposición en los valores de la tasa de aciertos, la tasa de aciertos se estabiliza entre 0.28 y 0.34 durante toda la semana, lo que sugiere que el caché estático es efectivo durante una semana completa después del período de entrenamiento.0.26 0.27 0.28 0.29 0.3 0.31 0.32 0.33 0.34 0.35 0.36 0.37 0 20 40 40 60 80 100 120 140 160 Hits de tiempo de golpe de golpe en las consultas frecuentes de distancias Figura 13: Tasa de aciertos por hora para un caché estático que contiene 128,000 respuestas durante el período de un Asemana. El caché estático de las listas de publicación se puede recomputar periódicamente. Para estimar el intervalo de tiempo en el que necesitamos recomputar las listas de publicación en el caché estático, debemos considerar una compensación de eficiencia/calidad: usar un intervalo de tiempo demasiado corto podría ser prohibitivamente costoso, mientras que recomputar el caché con demasiada frecuencia podría conducir aTener un caché obsoleto que no corresponde a las características estadísticas de la corriente de consulta actual. Medimos el efecto sobre el algoritmo QTFDF de los cambios en una corriente de consulta de 15 semanas (Figura 14). Calculamos las frecuencias del término de consulta en toda la secuencia, seleccionamos qué términos almacenan en caché y luego calculamos la tasa de aciertos en todo el flujo de consulta. Esta tasa de éxito es como un límite superior, y asume un conocimiento perfecto de las frecuencias del término de consulta. Para simular un escenario realista, utilizamos las primeras 6 (3) semanas de la corriente de consulta para calcular las frecuencias de los términos de consulta y las siguientes 9 (12) semanas para estimar la tasa de aciertos. Como muestra la Figura 14, la tasa de golpes disminuye en menos del 2%. La alta correlación entre las frecuencias del término de consulta durante diferentes períodos de tiempo explica la elegante adaptación de los algoritmos de almacenamiento en caché estático a la corriente de consulta futura. De hecho, la correlación por pares entre todos los períodos posibles de 3 semanas de la corriente de consulta de 15 semanas es superior al 99.5%.8. Conclusiones El almacenamiento en caché es una técnica efectiva en los motores de búsqueda para mejorar el tiempo de respuesta, reducir la carga en los procesadores de consultas y mejorar la utilización del ancho de banda de la red. Presentamos resultados en el almacenamiento en caché dinámico y estático. El almacenamiento en caché dinámico de consultas tiene una efectividad limitada debido al alto número de fallas obligatorias causadas por el número de consultas únicas o infrecuentes. Nuestros resultados muestran que en nuestro registro del Reino Unido, la tasa mínima de fallas es del 50% utilizando una estrategia de conjunto de trabajo. Los términos de almacenamiento en caché son más efectivos con respecto a la tasa de fallas, logrando valores tan bajos como 12%. También proponemos un nuevo algoritmo para el almacenamiento en caché estático de las listas de publicación que superan a los algoritmos de almacenamiento estático previos, así como algoritmos dinámicos como LRU y LFU, la obtención de valores de tasa de éxitos que son más de un 10% más altos compararon estas estrategias. Presentamos un marco para el análisis de la compensación entre los resultados de las consultas de almacenamiento en caché y las listas de publicación de almacenamiento en caché, y simulamos diferentes tipos de arquitecturas. Nuestros resultados muestran que para los entornos centralizados y LAN, existe una asignación óptima de los resultados de la consulta de almacenamiento en caché y el almacenamiento en caché de las listas de publicación, mientras que para los escenarios WAN en los que prevalece el tiempo de la red, es más importante para cachear los resultados de la consulta.0.45 0.5 0.5 0.5 0.6 0.65 0.7 0.75 0.8 0.85 0.9 0.95 0.1 0.2 0.3 0.4 0.5 0.5 0.6 0.7 Dinámica de tamaño de caché HITRate Dinámica de la caché de caché estático/DF Conocimiento perfecto de 6 semanas Capacitación de 3 semanas Figura 14: Impacto de la distribución Cambios de distribución en el caché estáticode publicar listas.9. Referencias [1] V. N. Anh y A. Moffat. Evaluación de consultas podadas utilizando impactos precomputados. En ACM CIKM, 2006. [2] R. A. Baeza-Yates y F. Saint-Jean. Un índice de motor de búsqueda de tres niveles basado en la distribución del registro de consultas. En Spire, 2003. [3] C. Buckley y A. F. Lewit. Optimización de búsquedas vectoriales invertidas. En ACM Sigir, 1985. [4] S. B¨uttcher y C. L. A. Clarke. Un enfoque centrado en el documento para la poda de índice estático en los sistemas de recuperación de texto. En ACM CIKM, 2006. [5] P. Cao y S. Irani. Algoritmos de almacenamiento en caché del proxy WWW consciente de WWW. En Usits, 1997. [6] P. Denning. Trabajar conjuntos pasados y presentes. IEEE Trans.en Ingeniería de Software, SE-6 (1): 64-84, 1980. [7] T. Fagni, R. Perego, F. Silvestri y S. Orlando. Aumentando el rendimiento de los motores de búsqueda web: almacenamiento en caché y captación de los resultados de consultas mediante la explotación de datos de uso histórico. ACM Trans. Inf. Syst., 24 (1): 51-78, 2006. [8] R. Lempel y S. Moran. El almacenamiento en caché predictivo y la captación previa de la consulta resulta en motores de búsqueda. En www, 2003. [9] X. Long y T. Suel. El almacenamiento en caché de tres niveles para un procesamiento de consultas eficiente en grandes motores de búsqueda web. En www, 2005. [10] E. P. Markatos. En los resultados de la consulta del motor de búsqueda en caché. Computer Communications, 24 (2): 137-143, 2001. [11] I. Ounis, G. Amati, V. Plachouras, B. Él, C. MacDonald y C. Lioma. Terrier: una plataforma de recuperación de información de alto rendimiento y escalable. En el taller Sigir sobre la recuperación de información de código abierto, 2006. [12] V. V. Raghavan y H. Sever. Sobre la reutilización de consultas óptimas pasadas. En ACM Sigir, 1995. [13] P. C. Saraiva, E. S. de Moura, N. Ziviani, W. Meira, R. Fonseca y B. Riberio-Neto. El almacenamiento en caché de dos niveles para preservar el rango para motores de búsqueda escalables. En ACM Sigir, 2001. [14] D. R. Slutz e I. L. Traiger. Una nota sobre el cálculo del tamaño de conjunto de trabajo promedio. Comunicaciones de la ACM, 17 (10): 563-565, 1974. [15] T. Strohman, H. Turtle y W. B. Croft. Estrategias de optimización para consultas complejas. En ACM Sigir, 2005. [16] I. H. Witten, T. C. Bell y A. Moffat. Gestión de gigabytes: compresión e indexación de documentos e imágenes. John Wiley & Sons, Inc., NY, 1994. [17] N. E. Young. Almacenamiento en caché de archivos en línea. Algorithmica, 33 (3): 371-383, 2002.",
    "original_sentences": [
        "The Impact of Caching on Search Engines Ricardo Baeza-Yates1 rbaeza@acm.org Aristides Gionis1 gionis@yahoo-inc.com Flavio Junqueira1 fpj@yahoo-inc.com Vanessa Murdock1 vmurdock@yahoo-inc.com Vassilis Plachouras1 vassilis@yahoo-inc.com Fabrizio Silvestri2 f.silvestri@isti.cnr.it 1 Yahoo!",
        "Research Barcelona 2 ISTI - CNR Barcelona, SPAIN Pisa, ITALY ABSTRACT In this paper we study the trade-offs in designing efficient caching systems for Web search engines.",
        "We explore the impact of different approaches, such as static vs. dynamic caching, and caching query results vs. caching posting lists.",
        "Using a query log spanning a whole year we explore the limitations of caching and we demonstrate that caching posting lists can achieve higher hit rates than caching query answers.",
        "We propose a new algorithm for static caching of posting lists, which outperforms previous methods.",
        "We also study the problem of finding the optimal way to split the static cache between answers and posting lists.",
        "Finally, we measure how the changes in the query log affect the effectiveness of static caching, given our observation that the distribution of the queries changes slowly over time.",
        "Our results and observations are applicable to different levels of the data-access hierarchy, for instance, for a memory/disk layer or a broker/remote server layer.",
        "Categories and Subject Descriptors H.3.3 [Information Storage and Retrieval]: Information Search and Retrieval - Search process; H.3.4 [Information Storage and Retrieval]: Systems and Software - Distributed systems, Performance evaluation (efficiency and effectiveness) General Terms Algorithms, Experimentation 1.",
        "INTRODUCTION Millions of queries are submitted daily to Web search engines, and users have high expectations of the quality and speed of the answers.",
        "As the searchable Web becomes larger and larger, with more than 20 billion pages to index, evaluating a single query requires processing large amounts of data.",
        "In such a setting, to achieve a fast response time and to increase the query throughput, using a cache is crucial.",
        "The primary use of a cache memory is to speedup computation by exploiting frequently or recently used data, although reducing the workload to back-end servers is also a major goal.",
        "Caching can be applied at different levels with increasing response latencies or processing requirements.",
        "For example, the different levels may correspond to the main memory, the disk, or resources in a local or a wide area network.",
        "The decision of what to cache is either off-line (static) or online (dynamic).",
        "A static cache is based on historical information and is periodically updated.",
        "A dynamic cache replaces entries according to the sequence of requests.",
        "When a new request arrives, the cache system decides whether to evict some entry from the cache in the case of a cache miss.",
        "Such online decisions are based on a cache policy, and several different policies have been studied in the past.",
        "For a search engine, there are two possible ways to use a cache memory: Caching answers: As the engine returns answers to a particular query, it may decide to store these answers to resolve future queries.",
        "Caching terms: As the engine evaluates a particular query, it may decide to store in memory the posting lists of the involved query terms.",
        "Often the whole set of posting lists does not fit in memory, and consequently, the engine has to select a small set to keep in memory and speed up query processing.",
        "Returning an answer to a query that already exists in the cache is more efficient than computing the answer using cached posting lists.",
        "On the other hand, previously unseen queries occur more often than previously unseen terms, implying a higher miss rate for cached answers.",
        "Caching of posting lists has additional challenges.",
        "As posting lists have variable size, caching them dynamically is not very efficient, due to the complexity in terms of efficiency and space, and the skewed distribution of the query stream, as shown later.",
        "Static caching of posting lists poses even more challenges: when deciding which terms to cache one faces the trade-off between frequently queried terms and terms with small posting lists that are space efficient.",
        "Finally, before deciding to adopt a static caching policy the query stream should be analyzed to verify that its characteristics do not change rapidly over time.",
        "Broker Static caching posting lists Dynamic/Static cached answers Local query processor Disk Next caching level Local network access Remote network access Figure 1: One caching level in a distributed search architecture.",
        "In this paper we explore the trade-offs in the design of each cache level, showing that the problem is the same and only a few parameters change.",
        "In general, we assume that each level of caching in a distributed search architecture is similar to that shown in Figure 1.",
        "We use a query log spanning a whole year to explore the limitations of dynamically caching query answers or posting lists for query terms.",
        "More concretely, our main conclusions are that: • Caching query answers results in lower hit ratios compared to caching of posting lists for query terms, but it is faster because there is no need for query evaluation.",
        "We provide a framework for the analysis of the trade-off between static caching of query answers and posting lists; • Static caching of terms can be more effective than dynamic caching with, for example, LRU.",
        "We provide algorithms based on the Knapsack problem for selecting the posting lists to put in a static cache, and we show improvements over previous work, achieving a hit ratio over 90%; • Changes of the query distribution over time have little impact on static caching.",
        "The remainder of this paper is organized as follows.",
        "Sections 2 and 3 summarize related work and characterize the data sets we use.",
        "Section 4 discusses the limitations of dynamic caching.",
        "Sections 5 and 6 introduce algorithms for caching posting lists, and a theoretical framework for the analysis of static caching, respectively.",
        "Section 7 discusses the impact of changes in the query distribution on static caching, and Section 8 provides concluding remarks. 2.",
        "RELATED WORK There is a large body of work devoted to query optimization.",
        "Buckley and Lewit [3], in one of the earliest works, take a term-at-a-time approach to deciding when inverted lists need not be further examined.",
        "More recent examples demonstrate that the top k documents for a query can be returned without the need for evaluating the complete set of posting lists [1, 4, 15].",
        "Although these approaches seek to improve query processing efficiency, they differ from our current work in that they do not consider caching.",
        "They may be considered separate and complementary to a cache-based approach.",
        "Raghavan and Sever [12], in one of the first papers on exploiting user query history, propose using a query base, built upon a set of persistent optimal queries submitted in the past, to improve the retrieval effectiveness for similar future queries.",
        "Markatos [10] shows the existence of temporal locality in queries, and compares the performance of different caching policies.",
        "Based on the observations of Markatos, Lempel and Moran propose a new caching policy, called Probabilistic Driven Caching, by attempting to estimate the probability distribution of all possible queries submitted to a search engine [8].",
        "Fagni et al. follow Markatos work by showing that combining static and dynamic caching policies together with an adaptive prefetching policy achieves a high hit ratio [7].",
        "Different from our work, they consider caching and prefetching of pages of results.",
        "As systems are often hierarchical, there has also been some effort on multi-level architectures.",
        "Saraiva et al. propose a new architecture for Web search engines using a two-level dynamic caching system [13].",
        "Their goal for such systems has been to improve response time for hierarchical engines.",
        "In their architecture, both levels use an LRU eviction policy.",
        "They find that the second-level cache can effectively reduce disk traffic, thus increasing the overall throughput.",
        "Baeza-Yates and Saint-Jean propose a three-level index organization [2].",
        "Long and Suel propose a caching system structured according to three different levels [9].",
        "The intermediate level contains frequently occurring pairs of terms and stores the intersections of the corresponding inverted lists.",
        "These last two papers are related to ours in that they exploit different caching strategies at different levels of the memory hierarchy.",
        "Finally, our static caching algorithm for posting lists in Section 5 uses the ratio frequency/size in order to evaluate the goodness of an item to cache.",
        "Similar ideas have been used in the context of file caching [17], Web caching [5], and even caching of posting lists [9], but in all cases in a dynamic setting.",
        "To the best of our knowledge we are the first to use this approach for static caching of posting lists. 3.",
        "DATA CHARACTERIZATION Our data consists of a crawl of documents from the UK domain, and query logs of one year of queries submitted to http://www.yahoo.co.uk from November 2005 to November 2006.",
        "In our logs, 50% of the total volume of queries are unique.",
        "The average query length is 2.5 terms, with the longest query having 731 terms. 1e-07 1e-06 1e-05 1e-04 0.001 0.01 0.1 1 1e-08 1e-07 1e-06 1e-05 1e-04 0.001 0.01 0.1 1 Frequency(normalized) Frequency rank (normalized) Figure 2: The distribution of queries (bottom curve) and query terms (middle curve) in the query log.",
        "The distribution of document frequencies of terms in the UK-2006 dataset (upper curve).",
        "Figure 2 shows the distributions of queries (lower curve), and query terms (middle curve).",
        "The x-axis represents the normalized frequency rank of the query or term. (The most frequent query appears closest to the y-axis.)",
        "The y-axis is Table 1: Statistics of the UK-2006 sample.",
        "UK-2006 sample statistics # of documents 2,786,391 # of terms 6,491,374 # of tokens 2,109,512,558 the normalized frequency for a given query (or term).",
        "As expected, the distribution of query frequencies and query term frequencies follow power law distributions, with slope of 1.84 and 2.26, respectively.",
        "In this figure, the query frequencies were computed as they appear in the logs with no normalization for case or white space.",
        "The query terms (middle curve) have been normalized for case, as have the terms in the document collection.",
        "The document collection that we use for our experiments is a summary of the UK domain crawled in May 2006.1 This summary corresponds to a maximum of 400 crawled documents per host, using a breadth first crawling strategy, comprising 15GB.",
        "The distribution of document frequencies of terms in the collection follows a power law distribution with slope 2.38 (upper curve in Figure 2).",
        "The statistics of the collection are shown in Table 1.",
        "We measured the correlation between the document frequency of terms in the collection and the number of queries that contain a particular term in the query log to be 0.424.",
        "A scatter plot for a random sample of terms is shown in Figure 3.",
        "In this experiment, terms have been converted to lower case in both the queries and the documents so that the frequencies will be comparable. 1e-07 1e-06 1e-05 1e-04 0.001 0.01 0.1 1 1e-06 1e-05 1e-04 0.001 0.01 0.1 1 Queryfrequency Document frequency Figure 3: Normalized scatter plot of document-term frequencies vs. query-term frequencies. 4.",
        "CACHING OF QUERIES AND TERMS Caching relies upon the assumption that there is locality in the stream of requests.",
        "That is, there must be sufficient repetition in the stream of requests and within intervals of time that enable a cache memory of reasonable size to be effective.",
        "In the query log we used, 88% of the unique queries are singleton queries, and 44% are singleton queries out of the whole volume.",
        "Thus, out of all queries in the stream composing the query log, the upper threshold on hit ratio is 56%.",
        "This is because only 56% of all the queries comprise queries that have multiple occurrences.",
        "It is important to observe, however, that not all queries in this 56% can be cache hits because of compulsory misses.",
        "A compulsory miss 1 The collection is available from the University of Milan: http://law.dsi.unimi.it/.",
        "URL retrieved 05/2007. 0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1 240 260 280 300 320 340 360 Numberofelements Bin number Total terms Terms diff Total queries Unique queries Unique terms Query diff Figure 4: Arrival rate for both terms and queries. happens when the cache receives a query for the first time.",
        "This is different from capacity misses, which happen due to space constraints on the amount of memory the cache uses.",
        "If we consider a cache with infinite memory, then the hit ratio is 50%.",
        "Note that for an infinite cache there are no capacity misses.",
        "As we mentioned before, another possibility is to cache the posting lists of terms.",
        "Intuitively, this gives more freedom in the utilization of the cache content to respond to queries because cached terms might form a new query.",
        "On the other hand, they need more space.",
        "As opposed to queries, the fraction of singleton terms in the total volume of terms is smaller.",
        "In our query log, only 4% of the terms appear once, but this accounts for 73% of the vocabulary of query terms.",
        "We show in Section 5 that caching a small fraction of terms, while accounting for terms appearing in many documents, is potentially very effective.",
        "Figure 4 shows several graphs corresponding to the normalized arrival rate for different cases using days as bins.",
        "That is, we plot the normalized number of elements that appear in a day.",
        "This graph shows only a period of 122 days, and we normalize the values by the maximum value observed throughout the whole period of the query log.",
        "Total queries and Total terms correspond to the total volume of queries and terms, respectively.",
        "Unique queries and Unique terms correspond to the arrival rate of unique queries and terms.",
        "Finally, Query diff and Terms diff correspond to the difference between the curves for total and unique.",
        "In Figure 4, as expected, the volume of terms is much higher than the volume of queries.",
        "The difference between the total number of terms and the number of unique terms is much larger than the difference between the total number of queries and the number of unique queries.",
        "This observation implies that terms repeat significantly more than queries.",
        "If we use smaller bins, say of one hour, then the ratio of unique to volume is higher for both terms and queries because it leaves less room for repetition.",
        "We also estimated the workload using the document frequency of terms as a measure of how much work a query imposes on a search engine.",
        "We found that it follows closely the arrival rate for terms shown in Figure 4.",
        "To demonstrate the effect of a dynamic cache on the query frequency distribution of Figure 2, we plot the same frequency graph, but now considering the frequency of queries Figure 5: Frequency graph after LRU cache. after going through an LRU cache.",
        "On a cache miss, an LRU cache decides upon an entry to evict using the information on the recency of queries.",
        "In this graph, the most frequent queries are not the same queries that were most frequent before the cache.",
        "It is possible that queries that are most frequent after the cache have different characteristics, and tuning the search engine to queries frequent before the cache may degrade performance for non-cached queries.",
        "The maximum frequency after caching is less than 1% of the maximum frequency before the cache, thus showing that the cache is very effective in reducing the load of frequent queries.",
        "If we re-rank the queries according to after-cache frequency, the distribution is still a power law, but with a much smaller value for the highest frequency.",
        "When discussing the effectiveness of dynamically caching, an important metric is cache miss rate.",
        "To analyze the cache miss rate for different memory constraints, we use the working set model [6, 14].",
        "A working set, informally, is the set of references that an application or an operating system is currently working with.",
        "The model uses such sets in a strategy that tries to capture the temporal locality of references.",
        "The working set strategy then consists in keeping in memory only the elements that are referenced in the previous θ steps of the input sequence, where θ is a configurable parameter corresponding to the window size.",
        "Originally, working sets have been used for page replacement algorithms of operating systems, and considering such a strategy in the context of search engines is interesting for three reasons.",
        "First, it captures the amount of locality of queries and terms in a sequence of queries.",
        "Locality in this case refers to the frequency of queries and terms in a window of time.",
        "If many queries appear multiple times in a window, then locality is high.",
        "Second, it enables an oﬄine analysis of the expected miss rate given different memory constraints.",
        "Third, working sets capture aspects of efficient caching algorithms such as LRU.",
        "LRU assumes that references farther in the past are less likely to be referenced in the present, which is implicit in the concept of working sets [14].",
        "Figure 6 plots the miss rate for different working set sizes, and we consider working sets of both queries and terms.",
        "The working set sizes are normalized against the total number of queries in the query log.",
        "In the graph for queries, there is a sharp decay until approximately 0.01, and the rate at which the miss rate drops decreases as we increase the size of the working set over 0.01.",
        "Finally, the minimum value it reaches is 50% miss rate, not shown in the figure as we have cut the tail of the curve for presentation purposes. 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1 0 0.05 0.1 0.15 0.2 Missrate Normalized working set size Queries Terms Figure 6: Miss rate as a function of the working set size. 1 10 100 1000 10000 100000 1e+06 Frequency Distance Figure 7: Distribution of distances expressed in terms of distinct queries.",
        "Compared to the query curve, we observe that the minimum miss rate for terms is substantially smaller.",
        "The miss rate also drops sharply on values up to 0.01, and it decreases minimally for higher values.",
        "The minimum value, however, is slightly over 10%, which is much smaller than the minimum value for the sequence of queries.",
        "This implies that with such a policy it is possible to achieve over 80% hit rate, if we consider caching dynamically posting lists for terms as opposed to caching answers for queries.",
        "This result does not consider the space required for each unit stored in the cache memory, or the amount of time it takes to put together a response to a user query.",
        "We analyze these issues more carefully later in this paper.",
        "It is interesting also to observe the histogram of Figure 7, which is an intermediate step in the computation of the miss rate graph.",
        "It reports the distribution of distances between repetitions of the same frequent query.",
        "The distance in the plot is measured in the number of distinct queries separating a query and its repetition, and it considers only queries appearing at least 10 times.",
        "From Figures 6 and 7, we conclude that even if we set the size of the query answers cache to a relatively large number of entries, the miss rate is high.",
        "Thus, caching the posting lists of terms has the potential to improve the hit ratio.",
        "This is what we explore next. 5.",
        "CACHING POSTING LISTS The previous section shows that caching posting lists can obtain a higher hit rate compared to caching query answers.",
        "In this section we study the problem of how to select posting lists to place on a certain amount of available memory, assuming that the whole index is larger than the amount of memory available.",
        "The posting lists have variable size (in fact, their size distribution follows a power law), so it is beneficial for a caching policy to consider the sizes of the posting lists.",
        "We consider both dynamic and static caching.",
        "For dynamic caching, we use two well-known policies, LRU and LFU, as well as a modified algorithm that takes posting-list size into account.",
        "Before discussing the static caching strategies, we introduce some notation.",
        "We use fq(t) to denote the query-term frequency of a term t, that is, the number of queries containing t in the query log, and fd(t) to denote the document frequency of t, that is, the number of documents in the collection in which the term t appears.",
        "The first strategy we consider is the algorithm proposed by Baeza-Yates and Saint-Jean [2], which consists in selecting the posting lists of the terms with the highest query-term frequencies fq(t).",
        "We call this algorithm Qtf.",
        "We observe that there is a trade-off between fq(t) and fd(t).",
        "Terms with high fq(t) are useful to keep in the cache because they are queried often.",
        "On the other hand, terms with high fd(t) are not good candidates because they correspond to long posting lists and consume a substantial amount of space.",
        "In fact, the problem of selecting the best posting lists for the static cache corresponds to the standard Knapsack problem: given a knapsack of fixed capacity, and a set of n items, such as the i-th item has value ci and size si, select the set of items that fit in the knapsack and maximize the overall value.",
        "In our case, value corresponds to fq(t) and size corresponds to fd(t).",
        "Thus, we employ a simple algorithm for the knapsack problem, which is selecting the posting lists of the terms with the highest values of the ratio fq(t) fd(t) .",
        "We call this algorithm QtfDf.",
        "We tried other variations considering query frequencies instead of term frequencies, but the gain was minimal compared to the complexity added.",
        "In addition to the above two static algorithms we consider the following algorithms for dynamic caching: • LRU: A standard LRU algorithm, but many posting lists might need to be evicted (in order of least-recent usage) until there is enough space in the memory to place the currently accessed posting list; • LFU: A standard LFU algorithm (eviction of the leastfrequently used), with the same modification as the LRU; • Dyn-QtfDf: A dynamic version of the QtfDf algorithm; evict from the cache the term(s) with the lowest fq(t) fd(t) ratio.",
        "The performance of all the above algorithms for 15 weeks of the query log and the UK dataset are shown in Figure 8.",
        "Performance is measured with hit rate.",
        "The cache size is measured as a fraction of the total space required to store the posting lists of all terms.",
        "For the dynamic algorithms, we load the cache with terms in order of fq(t) and we let the cache warm up for 1 million queries.",
        "For the static algorithms, we assume complete knowledge of the frequencies fq(t), that is, we estimate fq(t) from the whole query stream.",
        "As we show in Section 7 the results do not change much if we compute the query-term frequencies using the first 3 or 4 weeks of the query log and measure the hit rate on the rest. 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1 0.1 0.2 0.3 0.4 0.5 0.6 0.7 Hitrate Cache size Caching posting lists static QTF/DF LRU LFU Dyn-QTF/DF QTF Figure 8: Hit rate of different strategies for caching posting lists.",
        "The most important observation from our experiments is that the static QtfDf algorithm has a better hit rate than all the dynamic algorithms.",
        "An important benefit a static cache is that it requires no eviction and it is hence more efficient when evaluating queries.",
        "However, if the characteristics of the query traffic change frequently over time, then it requires re-populating the cache often or there will be a significant impact on hit rate. 6.",
        "ANALYSIS OF STATIC CACHING In this section we provide a detailed analysis for the problem of deciding whether it is preferable to cache query answers or cache posting lists.",
        "Our analysis takes into account the impact of caching between two levels of the data-access hierarchy.",
        "It can either be applied at the memory/disk layer or at a server/remote server layer as in the architecture we discussed in the introduction.",
        "Using a particular system model, we obtain estimates for the parameters required by our analysis, which we subsequently use to decide the optimal trade-off between caching query answers and caching posting lists. 6.1 Analytical Model Let M be the size of the cache measured in answer units (the cache can store M query answers).",
        "Assume that all posting lists are of the same length L, measured in answer units.",
        "We consider the following two cases: (A) a cache that stores only precomputed answers, and (B) a cache that stores only posting lists.",
        "In the first case, Nc = M answers fit in the cache, while in the second case Np = M/L posting lists fit in the cache.",
        "Thus, Np = Nc/L.",
        "Note that although posting lists require more space, we can combine terms to evaluate more queries (or partial queries).",
        "For case (A), suppose that a query answer in the cache can be evaluated in 1 time unit.",
        "For case (B), assume that if the posting lists of the terms of a query are in the cache then the results can be computed in TR1 time units, while if the posting lists are not in the cache then the results can be computed in TR2 time units.",
        "Of course TR2 > TR1.",
        "Now we want to compare the time to answer a stream of Q queries in both cases.",
        "Let Vc(Nc) be the volume of the most frequent Nc queries.",
        "Then, for case (A), we have an overall time TCA = Vc(Nc) + TR2(Q − Vc(Nc)).",
        "Similarly, for case (B), let Vp(Np) be the number of computable queries.",
        "Then we have overall time TP L = TR1Vp(Np) + TR2(Q − Vp(Np)).",
        "We want to check under which conditions we have TP L < TCA.",
        "We have TP L − TCA = (TR2 − 1)Vc(Nc) − (TR2 − TR1)Vp(Np) > 0.",
        "Figure 9 shows the values of Vp and Vc for our data.",
        "We can see that caching answers saturates faster and for this particular data there is no additional benefit from using more than 10% of the index space for caching answers.",
        "As the query distribution is a power law with parameter α > 1, the i-th most frequent query appears with probability proportional to 1 iα .",
        "Therefore, the volume Vc(n), which is the total number of the n most frequent queries, is Vc(n) = V0 n i=1 Q iα = γnQ (0 < γn < 1).",
        "We know that Vp(n) grows faster than Vc(n) and assume, based on experimental results, that the relation is of the form Vp(n) = k Vc(n)β .",
        "In the worst case, for a large cache, β → 1.",
        "That is, both techniques will cache a constant fraction of the overall query volume.",
        "Then caching posting lists makes sense only if L(TR2 − 1) k(TR2 − TR1) > 1.",
        "If we use compression, we have L < L and TR1 > TR1.",
        "According to the experiments that we show later, compression is always better.",
        "For a small cache, we are interested in the transient behavior and then β > 1, as computed from our data.",
        "In this case there will always be a point where TP L > TCA for a large number of queries.",
        "In reality, instead of filling the cache only with answers or only with posting lists, a better strategy will be to divide the total cache space into cache for answers and cache for posting lists.",
        "In such a case, there will be some queries that could be answered by both parts of the cache.",
        "As the answer cache is faster, it will be the first choice for answering those queries.",
        "Let QNc and QNp be the set of queries that can be answered by the cached answers and the cached posting lists, respectively.",
        "Then, the overall time is T = Vc(Nc)+TR1V (QNp −QNc )+TR2(Q−V (QNp ∪QNc )), where Np = (M − Nc)/L.",
        "Finding the optimal division of the cache in order to minimize the overall retrieval time is a difficult problem to solve analytically.",
        "In Section 6.3 we use simulations to derive optimal cache trade-offs for particular implementation examples. 6.2 Parameter Estimation We now use a particular implementation of a centralized system and the model of a distributed system as examples from which we estimate the parameters of the analysis from the previous section.",
        "We perform the experiments using an optimized version of Terrier [11] for both indexing documents and processing queries, on a single machine with a Pentium 4 at 2GHz and 1GB of RAM.",
        "We indexed the documents from the UK-2006 dataset, without removing stop words or applying stemming.",
        "The posting lists in the inverted file consist of pairs of document identifier and term frequency.",
        "We compress the document identifier gaps using Elias gamma encoding, and the 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1 0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1 Queryvolume Space precomputed answers posting lists Figure 9: Cache saturation as a function of size.",
        "Table 2: Ratios between the average time to evaluate a query and the average time to return cached answers (centralized and distributed case).",
        "Centralized system TR1 TR2 TR1 TR2 Full evaluation 233 1760 707 1140 Partial evaluation 99 1626 493 798 LAN system TRL 1 TRL 2 TR L 1 TR L 2 Full evaluation 242 1769 716 1149 Partial evaluation 108 1635 502 807 WAN system TRW 1 TRW 2 TR W 1 TR W 2 Full evaluation 5001 6528 5475 5908 Partial evaluation 4867 6394 5270 5575 term frequencies in documents using unary encoding [16].",
        "The size of the inverted file is 1,189Mb.",
        "A stored answer requires 1264 bytes, and an uncompressed posting takes 8 bytes.",
        "From Table 1, we obtain L = (8·# of postings) 1264·# of terms = 0.75 and L = Inverted file size 1264·# of terms = 0.26.",
        "We estimate the ratio TR = T/Tc between the average time T it takes to evaluate a query and the average time Tc it takes to return a stored answer for the same query, in the following way.",
        "Tc is measured by loading the answers for 100,000 queries in memory, and answering the queries from memory.",
        "The average time is Tc = 0.069ms.",
        "T is measured by processing the same 100,000 queries (the first 10,000 queries are used to warm-up the system).",
        "For each query, we remove stop words, if there are at least three remaining terms.",
        "The stop words correspond to the terms with a frequency higher than the number of documents in the index.",
        "We use a document-at-a-time approach to retrieve documents containing all query terms.",
        "The only disk access required during query processing is for reading compressed posting lists from the inverted file.",
        "We perform both full and partial evaluation of answers, because some queries are likely to retrieve a large number of documents, and only a fraction of the retrieved documents will be seen by users.",
        "In the partial evaluation of queries, we terminate the processing after matching 10,000 documents.",
        "The estimated ratios TR are presented in Table 2.",
        "Figure 10 shows for a sample of queries the workload of the system with partial query evaluation and compressed posting lists.",
        "The x-axis corresponds to the total time the system spends processing a particular query, and the vertical axis corresponds to the sum t∈q fq · fd(t).",
        "Notice that the total number of postings of the query-terms does not necessarily provide an accurate estimate of the workload imposed on the system by a query (which is the case for full evaluation and uncompressed lists). 0 0.2 0.4 0.6 0.8 1 0 0.2 0.4 0.6 0.8 1 Totalpostingstoprocessquery(normalized) Total time to process query (normalized) Partial processing of compressed postings query len = 1 query len in [2,3] query len in [4,8] query len > 8 Figure 10: Workload for partial query evaluation with compressed posting lists.",
        "The analysis of the previous section also applies to a distributed retrieval system in one or multiple sites.",
        "Suppose that a document partitioned distributed system is running on a cluster of machines interconnected with a local area network (LAN) in one site.",
        "The broker receives queries and broadcasts them to the query processors, which answer the queries and return the results to the broker.",
        "Finally, the broker merges the received answers and generates the final set of answers (we assume that the time spent on merging results is negligible).",
        "The difference between the centralized architecture and the document partition architecture is the extra communication between the broker and the query processors.",
        "Using ICMP pings on a 100Mbps LAN, we have measured that sending the query from the broker to the query processors which send an answer of 4,000 bytes back to the broker takes on average 0.615ms.",
        "Hence, TRL = TR + 0.615ms/0.069ms = TR + 9.",
        "In the case when the broker and the query processors are in different sites connected with a wide area network (WAN), we estimated that broadcasting the query from the broker to the query processors and getting back an answer of 4,000 bytes takes on average 329ms.",
        "Hence, TRW = TR + 329ms/0.069ms = TR + 4768. 6.3 Simulation Results We now address the problem of finding the optimal tradeoff between caching query answers and caching posting lists.",
        "To make the problem concrete we assume a fixed budget M on the available memory, out of which x units are used for caching query answers and M − x for caching posting lists.",
        "We perform simulations and compute the average response time as a function of x.",
        "Using a part of the query log as training data, we first allocate in the cache the answers to the most frequent queries that fit in space x, and then we use the rest of the memory to cache posting lists.",
        "For selecting posting lists we use the QtfDf algorithm, applied to the training query log but excluding the queries that have already been cached.",
        "In Figure 11, we plot the simulated response time for a centralized system as a function of x.",
        "For the uncompressed index we use M = 1GB, and for the compressed index we use M = 0.5GB.",
        "In the case of the configuration that uses partial query evaluation with compressed posting lists, the lowest response time is achieved when 0.15GB out of the 0.5GB is allocated for storing answers for queries.",
        "We obtained similar trends in the results for the LAN setting.",
        "Figure 12 shows the simulated workload for a distributed system across a WAN.",
        "In this case, the total amount of memory is split between the broker, which holds the cached 400 500 600 700 800 900 1000 1100 1200 0 0.2 0.4 0.6 0.8 1 Averageresponsetime Space (GB) Simulated workload -- single machine full / uncompr / 1 G partial / uncompr / 1 G full / compr / 0.5 G partial / compr / 0.5 G Figure 11: Optimal division of the cache in a server. 3000 3500 4000 4500 5000 5500 6000 0 0.2 0.4 0.6 0.8 1 Averageresponsetime Space (GB) Simulated workload -- WAN full / uncompr / 1 G partial / uncompr / 1 G full / compr / 0.5 G partial / compr / 0.5 G Figure 12: Optimal division of the cache when the next level requires WAN access. answers of queries, and the query processors, which hold the cache of posting lists.",
        "According to the figure, the difference between the configurations of the query processors is less important because the network communication overhead increases the response time substantially.",
        "When using uncompressed posting lists, the optimal allocation of memory corresponds to using approximately 70% of the memory for caching query answers.",
        "This is explained by the fact that there is no need for network communication when the query can be answered by the cache at the broker. 7.",
        "EFFECT OF THE QUERY DYNAMICS For our query log, the query distribution and query-term distribution change slowly over time.",
        "To support this claim, we first assess how topics change comparing the distribution of queries from the first week in June, 2006, to the distribution of queries for the remainder of 2006 that did not appear in the first week in June.",
        "We found that a very small percentage of queries are new queries.",
        "The majority of queries that appear in a given week repeat in the following weeks for the next six months.",
        "We then compute the hit rate of a static cache of 128, 000 answers trained over a period of two weeks (Figure 13).",
        "We report hit rate hourly for 7 days, starting from 5pm.",
        "We observe that the hit rate reaches its highest value during the night (around midnight), whereas around 2-3pm it reaches its minimum.",
        "After a small decay in hit rate values, the hit rate stabilizes between 0.28, and 0.34 for the entire week, suggesting that the static cache is effective for a whole week after the training period. 0.26 0.27 0.28 0.29 0.3 0.31 0.32 0.33 0.34 0.35 0.36 0.37 0 20 40 60 80 100 120 140 160 Hit-rate Time Hits on the frequent queries of distances Figure 13: Hourly hit rate for a static cache holding 128,000 answers during the period of a week.",
        "The static cache of posting lists can be periodically recomputed.",
        "To estimate the time interval in which we need to recompute the posting lists on the static cache we need to consider an efficiency/quality trade-off: using too short a time interval might be prohibitively expensive, while recomputing the cache too infrequently might lead to having an obsolete cache not corresponding to the statistical characteristics of the current query stream.",
        "We measured the effect on the QtfDf algorithm of the changes in a 15-week query stream (Figure 14).",
        "We compute the query term frequencies over the whole stream, select which terms to cache, and then compute the hit rate on the whole query stream.",
        "This hit rate is as an upper bound, and it assumes perfect knowledge of the query term frequencies.",
        "To simulate a realistic scenario, we use the first 6 (3) weeks of the query stream for computing query term frequencies and the following 9 (12) weeks to estimate the hit rate.",
        "As Figure 14 shows, the hit rate decreases by less than 2%.",
        "The high correlation among the query term frequencies during different time periods explains the graceful adaptation of the static caching algorithms to the future query stream.",
        "Indeed, the pairwise correlation among all possible 3-week periods of the 15-week query stream is over 99.5%. 8.",
        "CONCLUSIONS Caching is an effective technique in search engines for improving response time, reducing the load on query processors, and improving network bandwidth utilization.",
        "We present results on both dynamic and static caching.",
        "Dynamic caching of queries has limited effectiveness due to the high number of compulsory misses caused by the number of unique or infrequent queries.",
        "Our results show that in our UK log, the minimum miss rate is 50% using a working set strategy.",
        "Caching terms is more effective with respect to miss rate, achieving values as low as 12%.",
        "We also propose a new algorithm for static caching of posting lists that outperforms previous static caching algorithms as well as dynamic algorithms such as LRU and LFU, obtaining hit rate values that are over 10% higher compared these strategies.",
        "We present a framework for the analysis of the trade-off between caching query results and caching posting lists, and we simulate different types of architectures.",
        "Our results show that for centralized and LAN environments, there is an optimal allocation of caching query results and caching of posting lists, while for WAN scenarios in which network time prevails it is more important to cache query results. 0.45 0.5 0.55 0.6 0.65 0.7 0.75 0.8 0.85 0.9 0.95 0.1 0.2 0.3 0.4 0.5 0.6 0.7 Hitrate Cache size Dynamics of static QTF/DF caching policy perfect knowledge 6-week training 3-week training Figure 14: Impact of distribution changes on the static caching of posting lists. 9.",
        "REFERENCES [1] V. N. Anh and A. Moffat.",
        "Pruned query evaluation using pre-computed impacts.",
        "In ACM CIKM, 2006. [2] R. A. Baeza-Yates and F. Saint-Jean.",
        "A three level search engine index based in query log distribution.",
        "In SPIRE, 2003. [3] C. Buckley and A. F. Lewit.",
        "Optimization of inverted vector searches.",
        "In ACM SIGIR, 1985. [4] S. B¨uttcher and C. L. A. Clarke.",
        "A document-centric approach to static index pruning in text retrieval systems.",
        "In ACM CIKM, 2006. [5] P. Cao and S. Irani.",
        "Cost-aware WWW proxy caching algorithms.",
        "In USITS, 1997. [6] P. Denning.",
        "Working sets past and present.",
        "IEEE Trans. on Software Engineering, SE-6(1):64-84, 1980. [7] T. Fagni, R. Perego, F. Silvestri, and S. Orlando.",
        "Boosting the performance of web search engines: Caching and prefetching query results by exploiting historical usage data.",
        "ACM Trans.",
        "Inf.",
        "Syst., 24(1):51-78, 2006. [8] R. Lempel and S. Moran.",
        "Predictive caching and prefetching of query results in search engines.",
        "In WWW, 2003. [9] X.",
        "Long and T. Suel.",
        "Three-level caching for efficient query processing in large web search engines.",
        "In WWW, 2005. [10] E. P. Markatos.",
        "On caching search engine query results.",
        "Computer Communications, 24(2):137-143, 2001. [11] I. Ounis, G. Amati, V. Plachouras, B.",
        "He, C. Macdonald, and C. Lioma.",
        "Terrier: A High Performance and Scalable Information Retrieval Platform.",
        "In SIGIR Workshop on Open Source Information Retrieval, 2006. [12] V. V. Raghavan and H. Sever.",
        "On the reuse of past optimal queries.",
        "In ACM SIGIR, 1995. [13] P. C. Saraiva, E. S. de Moura, N. Ziviani, W. Meira, R. Fonseca, and B. Riberio-Neto.",
        "Rank-preserving two-level caching for scalable search engines.",
        "In ACM SIGIR, 2001. [14] D. R. Slutz and I. L. Traiger.",
        "A note on the calculation of average working set size.",
        "Communications of the ACM, 17(10):563-565, 1974. [15] T. Strohman, H. Turtle, and W. B. Croft.",
        "Optimization strategies for complex queries.",
        "In ACM SIGIR, 2005. [16] I. H. Witten, T. C. Bell, and A. Moffat.",
        "Managing Gigabytes: Compressing and Indexing Documents and Images.",
        "John Wiley & Sons, Inc., NY, 1994. [17] N. E. Young.",
        "On-line file caching.",
        "Algorithmica, 33(3):371-383, 2002."
    ],
    "error_count": 0,
    "keys": {
        "efficient caching system": {
            "translated_key": "sistema de almacenamiento eficiente",
            "is_in_text": true,
            "original_annotated_sentences": [
                "The Impact of Caching on Search Engines Ricardo Baeza-Yates1 rbaeza@acm.org Aristides Gionis1 gionis@yahoo-inc.com Flavio Junqueira1 fpj@yahoo-inc.com Vanessa Murdock1 vmurdock@yahoo-inc.com Vassilis Plachouras1 vassilis@yahoo-inc.com Fabrizio Silvestri2 f.silvestri@isti.cnr.it 1 Yahoo!",
                "Research Barcelona 2 ISTI - CNR Barcelona, SPAIN Pisa, ITALY ABSTRACT In this paper we study the trade-offs in designing <br>efficient caching system</br>s for Web search engines.",
                "We explore the impact of different approaches, such as static vs. dynamic caching, and caching query results vs. caching posting lists.",
                "Using a query log spanning a whole year we explore the limitations of caching and we demonstrate that caching posting lists can achieve higher hit rates than caching query answers.",
                "We propose a new algorithm for static caching of posting lists, which outperforms previous methods.",
                "We also study the problem of finding the optimal way to split the static cache between answers and posting lists.",
                "Finally, we measure how the changes in the query log affect the effectiveness of static caching, given our observation that the distribution of the queries changes slowly over time.",
                "Our results and observations are applicable to different levels of the data-access hierarchy, for instance, for a memory/disk layer or a broker/remote server layer.",
                "Categories and Subject Descriptors H.3.3 [Information Storage and Retrieval]: Information Search and Retrieval - Search process; H.3.4 [Information Storage and Retrieval]: Systems and Software - Distributed systems, Performance evaluation (efficiency and effectiveness) General Terms Algorithms, Experimentation 1.",
                "INTRODUCTION Millions of queries are submitted daily to Web search engines, and users have high expectations of the quality and speed of the answers.",
                "As the searchable Web becomes larger and larger, with more than 20 billion pages to index, evaluating a single query requires processing large amounts of data.",
                "In such a setting, to achieve a fast response time and to increase the query throughput, using a cache is crucial.",
                "The primary use of a cache memory is to speedup computation by exploiting frequently or recently used data, although reducing the workload to back-end servers is also a major goal.",
                "Caching can be applied at different levels with increasing response latencies or processing requirements.",
                "For example, the different levels may correspond to the main memory, the disk, or resources in a local or a wide area network.",
                "The decision of what to cache is either off-line (static) or online (dynamic).",
                "A static cache is based on historical information and is periodically updated.",
                "A dynamic cache replaces entries according to the sequence of requests.",
                "When a new request arrives, the cache system decides whether to evict some entry from the cache in the case of a cache miss.",
                "Such online decisions are based on a cache policy, and several different policies have been studied in the past.",
                "For a search engine, there are two possible ways to use a cache memory: Caching answers: As the engine returns answers to a particular query, it may decide to store these answers to resolve future queries.",
                "Caching terms: As the engine evaluates a particular query, it may decide to store in memory the posting lists of the involved query terms.",
                "Often the whole set of posting lists does not fit in memory, and consequently, the engine has to select a small set to keep in memory and speed up query processing.",
                "Returning an answer to a query that already exists in the cache is more efficient than computing the answer using cached posting lists.",
                "On the other hand, previously unseen queries occur more often than previously unseen terms, implying a higher miss rate for cached answers.",
                "Caching of posting lists has additional challenges.",
                "As posting lists have variable size, caching them dynamically is not very efficient, due to the complexity in terms of efficiency and space, and the skewed distribution of the query stream, as shown later.",
                "Static caching of posting lists poses even more challenges: when deciding which terms to cache one faces the trade-off between frequently queried terms and terms with small posting lists that are space efficient.",
                "Finally, before deciding to adopt a static caching policy the query stream should be analyzed to verify that its characteristics do not change rapidly over time.",
                "Broker Static caching posting lists Dynamic/Static cached answers Local query processor Disk Next caching level Local network access Remote network access Figure 1: One caching level in a distributed search architecture.",
                "In this paper we explore the trade-offs in the design of each cache level, showing that the problem is the same and only a few parameters change.",
                "In general, we assume that each level of caching in a distributed search architecture is similar to that shown in Figure 1.",
                "We use a query log spanning a whole year to explore the limitations of dynamically caching query answers or posting lists for query terms.",
                "More concretely, our main conclusions are that: • Caching query answers results in lower hit ratios compared to caching of posting lists for query terms, but it is faster because there is no need for query evaluation.",
                "We provide a framework for the analysis of the trade-off between static caching of query answers and posting lists; • Static caching of terms can be more effective than dynamic caching with, for example, LRU.",
                "We provide algorithms based on the Knapsack problem for selecting the posting lists to put in a static cache, and we show improvements over previous work, achieving a hit ratio over 90%; • Changes of the query distribution over time have little impact on static caching.",
                "The remainder of this paper is organized as follows.",
                "Sections 2 and 3 summarize related work and characterize the data sets we use.",
                "Section 4 discusses the limitations of dynamic caching.",
                "Sections 5 and 6 introduce algorithms for caching posting lists, and a theoretical framework for the analysis of static caching, respectively.",
                "Section 7 discusses the impact of changes in the query distribution on static caching, and Section 8 provides concluding remarks. 2.",
                "RELATED WORK There is a large body of work devoted to query optimization.",
                "Buckley and Lewit [3], in one of the earliest works, take a term-at-a-time approach to deciding when inverted lists need not be further examined.",
                "More recent examples demonstrate that the top k documents for a query can be returned without the need for evaluating the complete set of posting lists [1, 4, 15].",
                "Although these approaches seek to improve query processing efficiency, they differ from our current work in that they do not consider caching.",
                "They may be considered separate and complementary to a cache-based approach.",
                "Raghavan and Sever [12], in one of the first papers on exploiting user query history, propose using a query base, built upon a set of persistent optimal queries submitted in the past, to improve the retrieval effectiveness for similar future queries.",
                "Markatos [10] shows the existence of temporal locality in queries, and compares the performance of different caching policies.",
                "Based on the observations of Markatos, Lempel and Moran propose a new caching policy, called Probabilistic Driven Caching, by attempting to estimate the probability distribution of all possible queries submitted to a search engine [8].",
                "Fagni et al. follow Markatos work by showing that combining static and dynamic caching policies together with an adaptive prefetching policy achieves a high hit ratio [7].",
                "Different from our work, they consider caching and prefetching of pages of results.",
                "As systems are often hierarchical, there has also been some effort on multi-level architectures.",
                "Saraiva et al. propose a new architecture for Web search engines using a two-level dynamic caching system [13].",
                "Their goal for such systems has been to improve response time for hierarchical engines.",
                "In their architecture, both levels use an LRU eviction policy.",
                "They find that the second-level cache can effectively reduce disk traffic, thus increasing the overall throughput.",
                "Baeza-Yates and Saint-Jean propose a three-level index organization [2].",
                "Long and Suel propose a caching system structured according to three different levels [9].",
                "The intermediate level contains frequently occurring pairs of terms and stores the intersections of the corresponding inverted lists.",
                "These last two papers are related to ours in that they exploit different caching strategies at different levels of the memory hierarchy.",
                "Finally, our static caching algorithm for posting lists in Section 5 uses the ratio frequency/size in order to evaluate the goodness of an item to cache.",
                "Similar ideas have been used in the context of file caching [17], Web caching [5], and even caching of posting lists [9], but in all cases in a dynamic setting.",
                "To the best of our knowledge we are the first to use this approach for static caching of posting lists. 3.",
                "DATA CHARACTERIZATION Our data consists of a crawl of documents from the UK domain, and query logs of one year of queries submitted to http://www.yahoo.co.uk from November 2005 to November 2006.",
                "In our logs, 50% of the total volume of queries are unique.",
                "The average query length is 2.5 terms, with the longest query having 731 terms. 1e-07 1e-06 1e-05 1e-04 0.001 0.01 0.1 1 1e-08 1e-07 1e-06 1e-05 1e-04 0.001 0.01 0.1 1 Frequency(normalized) Frequency rank (normalized) Figure 2: The distribution of queries (bottom curve) and query terms (middle curve) in the query log.",
                "The distribution of document frequencies of terms in the UK-2006 dataset (upper curve).",
                "Figure 2 shows the distributions of queries (lower curve), and query terms (middle curve).",
                "The x-axis represents the normalized frequency rank of the query or term. (The most frequent query appears closest to the y-axis.)",
                "The y-axis is Table 1: Statistics of the UK-2006 sample.",
                "UK-2006 sample statistics # of documents 2,786,391 # of terms 6,491,374 # of tokens 2,109,512,558 the normalized frequency for a given query (or term).",
                "As expected, the distribution of query frequencies and query term frequencies follow power law distributions, with slope of 1.84 and 2.26, respectively.",
                "In this figure, the query frequencies were computed as they appear in the logs with no normalization for case or white space.",
                "The query terms (middle curve) have been normalized for case, as have the terms in the document collection.",
                "The document collection that we use for our experiments is a summary of the UK domain crawled in May 2006.1 This summary corresponds to a maximum of 400 crawled documents per host, using a breadth first crawling strategy, comprising 15GB.",
                "The distribution of document frequencies of terms in the collection follows a power law distribution with slope 2.38 (upper curve in Figure 2).",
                "The statistics of the collection are shown in Table 1.",
                "We measured the correlation between the document frequency of terms in the collection and the number of queries that contain a particular term in the query log to be 0.424.",
                "A scatter plot for a random sample of terms is shown in Figure 3.",
                "In this experiment, terms have been converted to lower case in both the queries and the documents so that the frequencies will be comparable. 1e-07 1e-06 1e-05 1e-04 0.001 0.01 0.1 1 1e-06 1e-05 1e-04 0.001 0.01 0.1 1 Queryfrequency Document frequency Figure 3: Normalized scatter plot of document-term frequencies vs. query-term frequencies. 4.",
                "CACHING OF QUERIES AND TERMS Caching relies upon the assumption that there is locality in the stream of requests.",
                "That is, there must be sufficient repetition in the stream of requests and within intervals of time that enable a cache memory of reasonable size to be effective.",
                "In the query log we used, 88% of the unique queries are singleton queries, and 44% are singleton queries out of the whole volume.",
                "Thus, out of all queries in the stream composing the query log, the upper threshold on hit ratio is 56%.",
                "This is because only 56% of all the queries comprise queries that have multiple occurrences.",
                "It is important to observe, however, that not all queries in this 56% can be cache hits because of compulsory misses.",
                "A compulsory miss 1 The collection is available from the University of Milan: http://law.dsi.unimi.it/.",
                "URL retrieved 05/2007. 0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1 240 260 280 300 320 340 360 Numberofelements Bin number Total terms Terms diff Total queries Unique queries Unique terms Query diff Figure 4: Arrival rate for both terms and queries. happens when the cache receives a query for the first time.",
                "This is different from capacity misses, which happen due to space constraints on the amount of memory the cache uses.",
                "If we consider a cache with infinite memory, then the hit ratio is 50%.",
                "Note that for an infinite cache there are no capacity misses.",
                "As we mentioned before, another possibility is to cache the posting lists of terms.",
                "Intuitively, this gives more freedom in the utilization of the cache content to respond to queries because cached terms might form a new query.",
                "On the other hand, they need more space.",
                "As opposed to queries, the fraction of singleton terms in the total volume of terms is smaller.",
                "In our query log, only 4% of the terms appear once, but this accounts for 73% of the vocabulary of query terms.",
                "We show in Section 5 that caching a small fraction of terms, while accounting for terms appearing in many documents, is potentially very effective.",
                "Figure 4 shows several graphs corresponding to the normalized arrival rate for different cases using days as bins.",
                "That is, we plot the normalized number of elements that appear in a day.",
                "This graph shows only a period of 122 days, and we normalize the values by the maximum value observed throughout the whole period of the query log.",
                "Total queries and Total terms correspond to the total volume of queries and terms, respectively.",
                "Unique queries and Unique terms correspond to the arrival rate of unique queries and terms.",
                "Finally, Query diff and Terms diff correspond to the difference between the curves for total and unique.",
                "In Figure 4, as expected, the volume of terms is much higher than the volume of queries.",
                "The difference between the total number of terms and the number of unique terms is much larger than the difference between the total number of queries and the number of unique queries.",
                "This observation implies that terms repeat significantly more than queries.",
                "If we use smaller bins, say of one hour, then the ratio of unique to volume is higher for both terms and queries because it leaves less room for repetition.",
                "We also estimated the workload using the document frequency of terms as a measure of how much work a query imposes on a search engine.",
                "We found that it follows closely the arrival rate for terms shown in Figure 4.",
                "To demonstrate the effect of a dynamic cache on the query frequency distribution of Figure 2, we plot the same frequency graph, but now considering the frequency of queries Figure 5: Frequency graph after LRU cache. after going through an LRU cache.",
                "On a cache miss, an LRU cache decides upon an entry to evict using the information on the recency of queries.",
                "In this graph, the most frequent queries are not the same queries that were most frequent before the cache.",
                "It is possible that queries that are most frequent after the cache have different characteristics, and tuning the search engine to queries frequent before the cache may degrade performance for non-cached queries.",
                "The maximum frequency after caching is less than 1% of the maximum frequency before the cache, thus showing that the cache is very effective in reducing the load of frequent queries.",
                "If we re-rank the queries according to after-cache frequency, the distribution is still a power law, but with a much smaller value for the highest frequency.",
                "When discussing the effectiveness of dynamically caching, an important metric is cache miss rate.",
                "To analyze the cache miss rate for different memory constraints, we use the working set model [6, 14].",
                "A working set, informally, is the set of references that an application or an operating system is currently working with.",
                "The model uses such sets in a strategy that tries to capture the temporal locality of references.",
                "The working set strategy then consists in keeping in memory only the elements that are referenced in the previous θ steps of the input sequence, where θ is a configurable parameter corresponding to the window size.",
                "Originally, working sets have been used for page replacement algorithms of operating systems, and considering such a strategy in the context of search engines is interesting for three reasons.",
                "First, it captures the amount of locality of queries and terms in a sequence of queries.",
                "Locality in this case refers to the frequency of queries and terms in a window of time.",
                "If many queries appear multiple times in a window, then locality is high.",
                "Second, it enables an oﬄine analysis of the expected miss rate given different memory constraints.",
                "Third, working sets capture aspects of efficient caching algorithms such as LRU.",
                "LRU assumes that references farther in the past are less likely to be referenced in the present, which is implicit in the concept of working sets [14].",
                "Figure 6 plots the miss rate for different working set sizes, and we consider working sets of both queries and terms.",
                "The working set sizes are normalized against the total number of queries in the query log.",
                "In the graph for queries, there is a sharp decay until approximately 0.01, and the rate at which the miss rate drops decreases as we increase the size of the working set over 0.01.",
                "Finally, the minimum value it reaches is 50% miss rate, not shown in the figure as we have cut the tail of the curve for presentation purposes. 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1 0 0.05 0.1 0.15 0.2 Missrate Normalized working set size Queries Terms Figure 6: Miss rate as a function of the working set size. 1 10 100 1000 10000 100000 1e+06 Frequency Distance Figure 7: Distribution of distances expressed in terms of distinct queries.",
                "Compared to the query curve, we observe that the minimum miss rate for terms is substantially smaller.",
                "The miss rate also drops sharply on values up to 0.01, and it decreases minimally for higher values.",
                "The minimum value, however, is slightly over 10%, which is much smaller than the minimum value for the sequence of queries.",
                "This implies that with such a policy it is possible to achieve over 80% hit rate, if we consider caching dynamically posting lists for terms as opposed to caching answers for queries.",
                "This result does not consider the space required for each unit stored in the cache memory, or the amount of time it takes to put together a response to a user query.",
                "We analyze these issues more carefully later in this paper.",
                "It is interesting also to observe the histogram of Figure 7, which is an intermediate step in the computation of the miss rate graph.",
                "It reports the distribution of distances between repetitions of the same frequent query.",
                "The distance in the plot is measured in the number of distinct queries separating a query and its repetition, and it considers only queries appearing at least 10 times.",
                "From Figures 6 and 7, we conclude that even if we set the size of the query answers cache to a relatively large number of entries, the miss rate is high.",
                "Thus, caching the posting lists of terms has the potential to improve the hit ratio.",
                "This is what we explore next. 5.",
                "CACHING POSTING LISTS The previous section shows that caching posting lists can obtain a higher hit rate compared to caching query answers.",
                "In this section we study the problem of how to select posting lists to place on a certain amount of available memory, assuming that the whole index is larger than the amount of memory available.",
                "The posting lists have variable size (in fact, their size distribution follows a power law), so it is beneficial for a caching policy to consider the sizes of the posting lists.",
                "We consider both dynamic and static caching.",
                "For dynamic caching, we use two well-known policies, LRU and LFU, as well as a modified algorithm that takes posting-list size into account.",
                "Before discussing the static caching strategies, we introduce some notation.",
                "We use fq(t) to denote the query-term frequency of a term t, that is, the number of queries containing t in the query log, and fd(t) to denote the document frequency of t, that is, the number of documents in the collection in which the term t appears.",
                "The first strategy we consider is the algorithm proposed by Baeza-Yates and Saint-Jean [2], which consists in selecting the posting lists of the terms with the highest query-term frequencies fq(t).",
                "We call this algorithm Qtf.",
                "We observe that there is a trade-off between fq(t) and fd(t).",
                "Terms with high fq(t) are useful to keep in the cache because they are queried often.",
                "On the other hand, terms with high fd(t) are not good candidates because they correspond to long posting lists and consume a substantial amount of space.",
                "In fact, the problem of selecting the best posting lists for the static cache corresponds to the standard Knapsack problem: given a knapsack of fixed capacity, and a set of n items, such as the i-th item has value ci and size si, select the set of items that fit in the knapsack and maximize the overall value.",
                "In our case, value corresponds to fq(t) and size corresponds to fd(t).",
                "Thus, we employ a simple algorithm for the knapsack problem, which is selecting the posting lists of the terms with the highest values of the ratio fq(t) fd(t) .",
                "We call this algorithm QtfDf.",
                "We tried other variations considering query frequencies instead of term frequencies, but the gain was minimal compared to the complexity added.",
                "In addition to the above two static algorithms we consider the following algorithms for dynamic caching: • LRU: A standard LRU algorithm, but many posting lists might need to be evicted (in order of least-recent usage) until there is enough space in the memory to place the currently accessed posting list; • LFU: A standard LFU algorithm (eviction of the leastfrequently used), with the same modification as the LRU; • Dyn-QtfDf: A dynamic version of the QtfDf algorithm; evict from the cache the term(s) with the lowest fq(t) fd(t) ratio.",
                "The performance of all the above algorithms for 15 weeks of the query log and the UK dataset are shown in Figure 8.",
                "Performance is measured with hit rate.",
                "The cache size is measured as a fraction of the total space required to store the posting lists of all terms.",
                "For the dynamic algorithms, we load the cache with terms in order of fq(t) and we let the cache warm up for 1 million queries.",
                "For the static algorithms, we assume complete knowledge of the frequencies fq(t), that is, we estimate fq(t) from the whole query stream.",
                "As we show in Section 7 the results do not change much if we compute the query-term frequencies using the first 3 or 4 weeks of the query log and measure the hit rate on the rest. 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1 0.1 0.2 0.3 0.4 0.5 0.6 0.7 Hitrate Cache size Caching posting lists static QTF/DF LRU LFU Dyn-QTF/DF QTF Figure 8: Hit rate of different strategies for caching posting lists.",
                "The most important observation from our experiments is that the static QtfDf algorithm has a better hit rate than all the dynamic algorithms.",
                "An important benefit a static cache is that it requires no eviction and it is hence more efficient when evaluating queries.",
                "However, if the characteristics of the query traffic change frequently over time, then it requires re-populating the cache often or there will be a significant impact on hit rate. 6.",
                "ANALYSIS OF STATIC CACHING In this section we provide a detailed analysis for the problem of deciding whether it is preferable to cache query answers or cache posting lists.",
                "Our analysis takes into account the impact of caching between two levels of the data-access hierarchy.",
                "It can either be applied at the memory/disk layer or at a server/remote server layer as in the architecture we discussed in the introduction.",
                "Using a particular system model, we obtain estimates for the parameters required by our analysis, which we subsequently use to decide the optimal trade-off between caching query answers and caching posting lists. 6.1 Analytical Model Let M be the size of the cache measured in answer units (the cache can store M query answers).",
                "Assume that all posting lists are of the same length L, measured in answer units.",
                "We consider the following two cases: (A) a cache that stores only precomputed answers, and (B) a cache that stores only posting lists.",
                "In the first case, Nc = M answers fit in the cache, while in the second case Np = M/L posting lists fit in the cache.",
                "Thus, Np = Nc/L.",
                "Note that although posting lists require more space, we can combine terms to evaluate more queries (or partial queries).",
                "For case (A), suppose that a query answer in the cache can be evaluated in 1 time unit.",
                "For case (B), assume that if the posting lists of the terms of a query are in the cache then the results can be computed in TR1 time units, while if the posting lists are not in the cache then the results can be computed in TR2 time units.",
                "Of course TR2 > TR1.",
                "Now we want to compare the time to answer a stream of Q queries in both cases.",
                "Let Vc(Nc) be the volume of the most frequent Nc queries.",
                "Then, for case (A), we have an overall time TCA = Vc(Nc) + TR2(Q − Vc(Nc)).",
                "Similarly, for case (B), let Vp(Np) be the number of computable queries.",
                "Then we have overall time TP L = TR1Vp(Np) + TR2(Q − Vp(Np)).",
                "We want to check under which conditions we have TP L < TCA.",
                "We have TP L − TCA = (TR2 − 1)Vc(Nc) − (TR2 − TR1)Vp(Np) > 0.",
                "Figure 9 shows the values of Vp and Vc for our data.",
                "We can see that caching answers saturates faster and for this particular data there is no additional benefit from using more than 10% of the index space for caching answers.",
                "As the query distribution is a power law with parameter α > 1, the i-th most frequent query appears with probability proportional to 1 iα .",
                "Therefore, the volume Vc(n), which is the total number of the n most frequent queries, is Vc(n) = V0 n i=1 Q iα = γnQ (0 < γn < 1).",
                "We know that Vp(n) grows faster than Vc(n) and assume, based on experimental results, that the relation is of the form Vp(n) = k Vc(n)β .",
                "In the worst case, for a large cache, β → 1.",
                "That is, both techniques will cache a constant fraction of the overall query volume.",
                "Then caching posting lists makes sense only if L(TR2 − 1) k(TR2 − TR1) > 1.",
                "If we use compression, we have L < L and TR1 > TR1.",
                "According to the experiments that we show later, compression is always better.",
                "For a small cache, we are interested in the transient behavior and then β > 1, as computed from our data.",
                "In this case there will always be a point where TP L > TCA for a large number of queries.",
                "In reality, instead of filling the cache only with answers or only with posting lists, a better strategy will be to divide the total cache space into cache for answers and cache for posting lists.",
                "In such a case, there will be some queries that could be answered by both parts of the cache.",
                "As the answer cache is faster, it will be the first choice for answering those queries.",
                "Let QNc and QNp be the set of queries that can be answered by the cached answers and the cached posting lists, respectively.",
                "Then, the overall time is T = Vc(Nc)+TR1V (QNp −QNc )+TR2(Q−V (QNp ∪QNc )), where Np = (M − Nc)/L.",
                "Finding the optimal division of the cache in order to minimize the overall retrieval time is a difficult problem to solve analytically.",
                "In Section 6.3 we use simulations to derive optimal cache trade-offs for particular implementation examples. 6.2 Parameter Estimation We now use a particular implementation of a centralized system and the model of a distributed system as examples from which we estimate the parameters of the analysis from the previous section.",
                "We perform the experiments using an optimized version of Terrier [11] for both indexing documents and processing queries, on a single machine with a Pentium 4 at 2GHz and 1GB of RAM.",
                "We indexed the documents from the UK-2006 dataset, without removing stop words or applying stemming.",
                "The posting lists in the inverted file consist of pairs of document identifier and term frequency.",
                "We compress the document identifier gaps using Elias gamma encoding, and the 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1 0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1 Queryvolume Space precomputed answers posting lists Figure 9: Cache saturation as a function of size.",
                "Table 2: Ratios between the average time to evaluate a query and the average time to return cached answers (centralized and distributed case).",
                "Centralized system TR1 TR2 TR1 TR2 Full evaluation 233 1760 707 1140 Partial evaluation 99 1626 493 798 LAN system TRL 1 TRL 2 TR L 1 TR L 2 Full evaluation 242 1769 716 1149 Partial evaluation 108 1635 502 807 WAN system TRW 1 TRW 2 TR W 1 TR W 2 Full evaluation 5001 6528 5475 5908 Partial evaluation 4867 6394 5270 5575 term frequencies in documents using unary encoding [16].",
                "The size of the inverted file is 1,189Mb.",
                "A stored answer requires 1264 bytes, and an uncompressed posting takes 8 bytes.",
                "From Table 1, we obtain L = (8·# of postings) 1264·# of terms = 0.75 and L = Inverted file size 1264·# of terms = 0.26.",
                "We estimate the ratio TR = T/Tc between the average time T it takes to evaluate a query and the average time Tc it takes to return a stored answer for the same query, in the following way.",
                "Tc is measured by loading the answers for 100,000 queries in memory, and answering the queries from memory.",
                "The average time is Tc = 0.069ms.",
                "T is measured by processing the same 100,000 queries (the first 10,000 queries are used to warm-up the system).",
                "For each query, we remove stop words, if there are at least three remaining terms.",
                "The stop words correspond to the terms with a frequency higher than the number of documents in the index.",
                "We use a document-at-a-time approach to retrieve documents containing all query terms.",
                "The only disk access required during query processing is for reading compressed posting lists from the inverted file.",
                "We perform both full and partial evaluation of answers, because some queries are likely to retrieve a large number of documents, and only a fraction of the retrieved documents will be seen by users.",
                "In the partial evaluation of queries, we terminate the processing after matching 10,000 documents.",
                "The estimated ratios TR are presented in Table 2.",
                "Figure 10 shows for a sample of queries the workload of the system with partial query evaluation and compressed posting lists.",
                "The x-axis corresponds to the total time the system spends processing a particular query, and the vertical axis corresponds to the sum t∈q fq · fd(t).",
                "Notice that the total number of postings of the query-terms does not necessarily provide an accurate estimate of the workload imposed on the system by a query (which is the case for full evaluation and uncompressed lists). 0 0.2 0.4 0.6 0.8 1 0 0.2 0.4 0.6 0.8 1 Totalpostingstoprocessquery(normalized) Total time to process query (normalized) Partial processing of compressed postings query len = 1 query len in [2,3] query len in [4,8] query len > 8 Figure 10: Workload for partial query evaluation with compressed posting lists.",
                "The analysis of the previous section also applies to a distributed retrieval system in one or multiple sites.",
                "Suppose that a document partitioned distributed system is running on a cluster of machines interconnected with a local area network (LAN) in one site.",
                "The broker receives queries and broadcasts them to the query processors, which answer the queries and return the results to the broker.",
                "Finally, the broker merges the received answers and generates the final set of answers (we assume that the time spent on merging results is negligible).",
                "The difference between the centralized architecture and the document partition architecture is the extra communication between the broker and the query processors.",
                "Using ICMP pings on a 100Mbps LAN, we have measured that sending the query from the broker to the query processors which send an answer of 4,000 bytes back to the broker takes on average 0.615ms.",
                "Hence, TRL = TR + 0.615ms/0.069ms = TR + 9.",
                "In the case when the broker and the query processors are in different sites connected with a wide area network (WAN), we estimated that broadcasting the query from the broker to the query processors and getting back an answer of 4,000 bytes takes on average 329ms.",
                "Hence, TRW = TR + 329ms/0.069ms = TR + 4768. 6.3 Simulation Results We now address the problem of finding the optimal tradeoff between caching query answers and caching posting lists.",
                "To make the problem concrete we assume a fixed budget M on the available memory, out of which x units are used for caching query answers and M − x for caching posting lists.",
                "We perform simulations and compute the average response time as a function of x.",
                "Using a part of the query log as training data, we first allocate in the cache the answers to the most frequent queries that fit in space x, and then we use the rest of the memory to cache posting lists.",
                "For selecting posting lists we use the QtfDf algorithm, applied to the training query log but excluding the queries that have already been cached.",
                "In Figure 11, we plot the simulated response time for a centralized system as a function of x.",
                "For the uncompressed index we use M = 1GB, and for the compressed index we use M = 0.5GB.",
                "In the case of the configuration that uses partial query evaluation with compressed posting lists, the lowest response time is achieved when 0.15GB out of the 0.5GB is allocated for storing answers for queries.",
                "We obtained similar trends in the results for the LAN setting.",
                "Figure 12 shows the simulated workload for a distributed system across a WAN.",
                "In this case, the total amount of memory is split between the broker, which holds the cached 400 500 600 700 800 900 1000 1100 1200 0 0.2 0.4 0.6 0.8 1 Averageresponsetime Space (GB) Simulated workload -- single machine full / uncompr / 1 G partial / uncompr / 1 G full / compr / 0.5 G partial / compr / 0.5 G Figure 11: Optimal division of the cache in a server. 3000 3500 4000 4500 5000 5500 6000 0 0.2 0.4 0.6 0.8 1 Averageresponsetime Space (GB) Simulated workload -- WAN full / uncompr / 1 G partial / uncompr / 1 G full / compr / 0.5 G partial / compr / 0.5 G Figure 12: Optimal division of the cache when the next level requires WAN access. answers of queries, and the query processors, which hold the cache of posting lists.",
                "According to the figure, the difference between the configurations of the query processors is less important because the network communication overhead increases the response time substantially.",
                "When using uncompressed posting lists, the optimal allocation of memory corresponds to using approximately 70% of the memory for caching query answers.",
                "This is explained by the fact that there is no need for network communication when the query can be answered by the cache at the broker. 7.",
                "EFFECT OF THE QUERY DYNAMICS For our query log, the query distribution and query-term distribution change slowly over time.",
                "To support this claim, we first assess how topics change comparing the distribution of queries from the first week in June, 2006, to the distribution of queries for the remainder of 2006 that did not appear in the first week in June.",
                "We found that a very small percentage of queries are new queries.",
                "The majority of queries that appear in a given week repeat in the following weeks for the next six months.",
                "We then compute the hit rate of a static cache of 128, 000 answers trained over a period of two weeks (Figure 13).",
                "We report hit rate hourly for 7 days, starting from 5pm.",
                "We observe that the hit rate reaches its highest value during the night (around midnight), whereas around 2-3pm it reaches its minimum.",
                "After a small decay in hit rate values, the hit rate stabilizes between 0.28, and 0.34 for the entire week, suggesting that the static cache is effective for a whole week after the training period. 0.26 0.27 0.28 0.29 0.3 0.31 0.32 0.33 0.34 0.35 0.36 0.37 0 20 40 60 80 100 120 140 160 Hit-rate Time Hits on the frequent queries of distances Figure 13: Hourly hit rate for a static cache holding 128,000 answers during the period of a week.",
                "The static cache of posting lists can be periodically recomputed.",
                "To estimate the time interval in which we need to recompute the posting lists on the static cache we need to consider an efficiency/quality trade-off: using too short a time interval might be prohibitively expensive, while recomputing the cache too infrequently might lead to having an obsolete cache not corresponding to the statistical characteristics of the current query stream.",
                "We measured the effect on the QtfDf algorithm of the changes in a 15-week query stream (Figure 14).",
                "We compute the query term frequencies over the whole stream, select which terms to cache, and then compute the hit rate on the whole query stream.",
                "This hit rate is as an upper bound, and it assumes perfect knowledge of the query term frequencies.",
                "To simulate a realistic scenario, we use the first 6 (3) weeks of the query stream for computing query term frequencies and the following 9 (12) weeks to estimate the hit rate.",
                "As Figure 14 shows, the hit rate decreases by less than 2%.",
                "The high correlation among the query term frequencies during different time periods explains the graceful adaptation of the static caching algorithms to the future query stream.",
                "Indeed, the pairwise correlation among all possible 3-week periods of the 15-week query stream is over 99.5%. 8.",
                "CONCLUSIONS Caching is an effective technique in search engines for improving response time, reducing the load on query processors, and improving network bandwidth utilization.",
                "We present results on both dynamic and static caching.",
                "Dynamic caching of queries has limited effectiveness due to the high number of compulsory misses caused by the number of unique or infrequent queries.",
                "Our results show that in our UK log, the minimum miss rate is 50% using a working set strategy.",
                "Caching terms is more effective with respect to miss rate, achieving values as low as 12%.",
                "We also propose a new algorithm for static caching of posting lists that outperforms previous static caching algorithms as well as dynamic algorithms such as LRU and LFU, obtaining hit rate values that are over 10% higher compared these strategies.",
                "We present a framework for the analysis of the trade-off between caching query results and caching posting lists, and we simulate different types of architectures.",
                "Our results show that for centralized and LAN environments, there is an optimal allocation of caching query results and caching of posting lists, while for WAN scenarios in which network time prevails it is more important to cache query results. 0.45 0.5 0.55 0.6 0.65 0.7 0.75 0.8 0.85 0.9 0.95 0.1 0.2 0.3 0.4 0.5 0.6 0.7 Hitrate Cache size Dynamics of static QTF/DF caching policy perfect knowledge 6-week training 3-week training Figure 14: Impact of distribution changes on the static caching of posting lists. 9.",
                "REFERENCES [1] V. N. Anh and A. Moffat.",
                "Pruned query evaluation using pre-computed impacts.",
                "In ACM CIKM, 2006. [2] R. A. Baeza-Yates and F. Saint-Jean.",
                "A three level search engine index based in query log distribution.",
                "In SPIRE, 2003. [3] C. Buckley and A. F. Lewit.",
                "Optimization of inverted vector searches.",
                "In ACM SIGIR, 1985. [4] S. B¨uttcher and C. L. A. Clarke.",
                "A document-centric approach to static index pruning in text retrieval systems.",
                "In ACM CIKM, 2006. [5] P. Cao and S. Irani.",
                "Cost-aware WWW proxy caching algorithms.",
                "In USITS, 1997. [6] P. Denning.",
                "Working sets past and present.",
                "IEEE Trans. on Software Engineering, SE-6(1):64-84, 1980. [7] T. Fagni, R. Perego, F. Silvestri, and S. Orlando.",
                "Boosting the performance of web search engines: Caching and prefetching query results by exploiting historical usage data.",
                "ACM Trans.",
                "Inf.",
                "Syst., 24(1):51-78, 2006. [8] R. Lempel and S. Moran.",
                "Predictive caching and prefetching of query results in search engines.",
                "In WWW, 2003. [9] X.",
                "Long and T. Suel.",
                "Three-level caching for efficient query processing in large web search engines.",
                "In WWW, 2005. [10] E. P. Markatos.",
                "On caching search engine query results.",
                "Computer Communications, 24(2):137-143, 2001. [11] I. Ounis, G. Amati, V. Plachouras, B.",
                "He, C. Macdonald, and C. Lioma.",
                "Terrier: A High Performance and Scalable Information Retrieval Platform.",
                "In SIGIR Workshop on Open Source Information Retrieval, 2006. [12] V. V. Raghavan and H. Sever.",
                "On the reuse of past optimal queries.",
                "In ACM SIGIR, 1995. [13] P. C. Saraiva, E. S. de Moura, N. Ziviani, W. Meira, R. Fonseca, and B. Riberio-Neto.",
                "Rank-preserving two-level caching for scalable search engines.",
                "In ACM SIGIR, 2001. [14] D. R. Slutz and I. L. Traiger.",
                "A note on the calculation of average working set size.",
                "Communications of the ACM, 17(10):563-565, 1974. [15] T. Strohman, H. Turtle, and W. B. Croft.",
                "Optimization strategies for complex queries.",
                "In ACM SIGIR, 2005. [16] I. H. Witten, T. C. Bell, and A. Moffat.",
                "Managing Gigabytes: Compressing and Indexing Documents and Images.",
                "John Wiley & Sons, Inc., NY, 1994. [17] N. E. Young.",
                "On-line file caching.",
                "Algorithmica, 33(3):371-383, 2002."
            ],
            "original_annotated_samples": [],
            "translated_annotated_samples": [
                "Investigación Barcelona 2 ISTI - CNR Barcelona, España Pisa, Italia Resumen En este documento Estudiamos las compensaciones en el diseño de \"sistemas de almacenamiento de almacenamiento eficiente\" para los motores de búsqueda web."
            ],
            "translated_text": "",
            "candidates": [
                "sistema de almacenamiento de almacenamiento eficiente",
                "sistemas de almacenamiento de almacenamiento eficiente"
            ],
            "error": []
        },
        "web search engine": {
            "translated_key": "motor de búsqueda web",
            "is_in_text": true,
            "original_annotated_sentences": [
                "The Impact of Caching on Search Engines Ricardo Baeza-Yates1 rbaeza@acm.org Aristides Gionis1 gionis@yahoo-inc.com Flavio Junqueira1 fpj@yahoo-inc.com Vanessa Murdock1 vmurdock@yahoo-inc.com Vassilis Plachouras1 vassilis@yahoo-inc.com Fabrizio Silvestri2 f.silvestri@isti.cnr.it 1 Yahoo!",
                "Research Barcelona 2 ISTI - CNR Barcelona, SPAIN Pisa, ITALY ABSTRACT In this paper we study the trade-offs in designing efficient caching systems for Web search engines.",
                "We explore the impact of different approaches, such as static vs. dynamic caching, and caching query results vs. caching posting lists.",
                "Using a query log spanning a whole year we explore the limitations of caching and we demonstrate that caching posting lists can achieve higher hit rates than caching query answers.",
                "We propose a new algorithm for static caching of posting lists, which outperforms previous methods.",
                "We also study the problem of finding the optimal way to split the static cache between answers and posting lists.",
                "Finally, we measure how the changes in the query log affect the effectiveness of static caching, given our observation that the distribution of the queries changes slowly over time.",
                "Our results and observations are applicable to different levels of the data-access hierarchy, for instance, for a memory/disk layer or a broker/remote server layer.",
                "Categories and Subject Descriptors H.3.3 [Information Storage and Retrieval]: Information Search and Retrieval - Search process; H.3.4 [Information Storage and Retrieval]: Systems and Software - Distributed systems, Performance evaluation (efficiency and effectiveness) General Terms Algorithms, Experimentation 1.",
                "INTRODUCTION Millions of queries are submitted daily to Web search engines, and users have high expectations of the quality and speed of the answers.",
                "As the searchable Web becomes larger and larger, with more than 20 billion pages to index, evaluating a single query requires processing large amounts of data.",
                "In such a setting, to achieve a fast response time and to increase the query throughput, using a cache is crucial.",
                "The primary use of a cache memory is to speedup computation by exploiting frequently or recently used data, although reducing the workload to back-end servers is also a major goal.",
                "Caching can be applied at different levels with increasing response latencies or processing requirements.",
                "For example, the different levels may correspond to the main memory, the disk, or resources in a local or a wide area network.",
                "The decision of what to cache is either off-line (static) or online (dynamic).",
                "A static cache is based on historical information and is periodically updated.",
                "A dynamic cache replaces entries according to the sequence of requests.",
                "When a new request arrives, the cache system decides whether to evict some entry from the cache in the case of a cache miss.",
                "Such online decisions are based on a cache policy, and several different policies have been studied in the past.",
                "For a search engine, there are two possible ways to use a cache memory: Caching answers: As the engine returns answers to a particular query, it may decide to store these answers to resolve future queries.",
                "Caching terms: As the engine evaluates a particular query, it may decide to store in memory the posting lists of the involved query terms.",
                "Often the whole set of posting lists does not fit in memory, and consequently, the engine has to select a small set to keep in memory and speed up query processing.",
                "Returning an answer to a query that already exists in the cache is more efficient than computing the answer using cached posting lists.",
                "On the other hand, previously unseen queries occur more often than previously unseen terms, implying a higher miss rate for cached answers.",
                "Caching of posting lists has additional challenges.",
                "As posting lists have variable size, caching them dynamically is not very efficient, due to the complexity in terms of efficiency and space, and the skewed distribution of the query stream, as shown later.",
                "Static caching of posting lists poses even more challenges: when deciding which terms to cache one faces the trade-off between frequently queried terms and terms with small posting lists that are space efficient.",
                "Finally, before deciding to adopt a static caching policy the query stream should be analyzed to verify that its characteristics do not change rapidly over time.",
                "Broker Static caching posting lists Dynamic/Static cached answers Local query processor Disk Next caching level Local network access Remote network access Figure 1: One caching level in a distributed search architecture.",
                "In this paper we explore the trade-offs in the design of each cache level, showing that the problem is the same and only a few parameters change.",
                "In general, we assume that each level of caching in a distributed search architecture is similar to that shown in Figure 1.",
                "We use a query log spanning a whole year to explore the limitations of dynamically caching query answers or posting lists for query terms.",
                "More concretely, our main conclusions are that: • Caching query answers results in lower hit ratios compared to caching of posting lists for query terms, but it is faster because there is no need for query evaluation.",
                "We provide a framework for the analysis of the trade-off between static caching of query answers and posting lists; • Static caching of terms can be more effective than dynamic caching with, for example, LRU.",
                "We provide algorithms based on the Knapsack problem for selecting the posting lists to put in a static cache, and we show improvements over previous work, achieving a hit ratio over 90%; • Changes of the query distribution over time have little impact on static caching.",
                "The remainder of this paper is organized as follows.",
                "Sections 2 and 3 summarize related work and characterize the data sets we use.",
                "Section 4 discusses the limitations of dynamic caching.",
                "Sections 5 and 6 introduce algorithms for caching posting lists, and a theoretical framework for the analysis of static caching, respectively.",
                "Section 7 discusses the impact of changes in the query distribution on static caching, and Section 8 provides concluding remarks. 2.",
                "RELATED WORK There is a large body of work devoted to query optimization.",
                "Buckley and Lewit [3], in one of the earliest works, take a term-at-a-time approach to deciding when inverted lists need not be further examined.",
                "More recent examples demonstrate that the top k documents for a query can be returned without the need for evaluating the complete set of posting lists [1, 4, 15].",
                "Although these approaches seek to improve query processing efficiency, they differ from our current work in that they do not consider caching.",
                "They may be considered separate and complementary to a cache-based approach.",
                "Raghavan and Sever [12], in one of the first papers on exploiting user query history, propose using a query base, built upon a set of persistent optimal queries submitted in the past, to improve the retrieval effectiveness for similar future queries.",
                "Markatos [10] shows the existence of temporal locality in queries, and compares the performance of different caching policies.",
                "Based on the observations of Markatos, Lempel and Moran propose a new caching policy, called Probabilistic Driven Caching, by attempting to estimate the probability distribution of all possible queries submitted to a search engine [8].",
                "Fagni et al. follow Markatos work by showing that combining static and dynamic caching policies together with an adaptive prefetching policy achieves a high hit ratio [7].",
                "Different from our work, they consider caching and prefetching of pages of results.",
                "As systems are often hierarchical, there has also been some effort on multi-level architectures.",
                "Saraiva et al. propose a new architecture for Web search engines using a two-level dynamic caching system [13].",
                "Their goal for such systems has been to improve response time for hierarchical engines.",
                "In their architecture, both levels use an LRU eviction policy.",
                "They find that the second-level cache can effectively reduce disk traffic, thus increasing the overall throughput.",
                "Baeza-Yates and Saint-Jean propose a three-level index organization [2].",
                "Long and Suel propose a caching system structured according to three different levels [9].",
                "The intermediate level contains frequently occurring pairs of terms and stores the intersections of the corresponding inverted lists.",
                "These last two papers are related to ours in that they exploit different caching strategies at different levels of the memory hierarchy.",
                "Finally, our static caching algorithm for posting lists in Section 5 uses the ratio frequency/size in order to evaluate the goodness of an item to cache.",
                "Similar ideas have been used in the context of file caching [17], Web caching [5], and even caching of posting lists [9], but in all cases in a dynamic setting.",
                "To the best of our knowledge we are the first to use this approach for static caching of posting lists. 3.",
                "DATA CHARACTERIZATION Our data consists of a crawl of documents from the UK domain, and query logs of one year of queries submitted to http://www.yahoo.co.uk from November 2005 to November 2006.",
                "In our logs, 50% of the total volume of queries are unique.",
                "The average query length is 2.5 terms, with the longest query having 731 terms. 1e-07 1e-06 1e-05 1e-04 0.001 0.01 0.1 1 1e-08 1e-07 1e-06 1e-05 1e-04 0.001 0.01 0.1 1 Frequency(normalized) Frequency rank (normalized) Figure 2: The distribution of queries (bottom curve) and query terms (middle curve) in the query log.",
                "The distribution of document frequencies of terms in the UK-2006 dataset (upper curve).",
                "Figure 2 shows the distributions of queries (lower curve), and query terms (middle curve).",
                "The x-axis represents the normalized frequency rank of the query or term. (The most frequent query appears closest to the y-axis.)",
                "The y-axis is Table 1: Statistics of the UK-2006 sample.",
                "UK-2006 sample statistics # of documents 2,786,391 # of terms 6,491,374 # of tokens 2,109,512,558 the normalized frequency for a given query (or term).",
                "As expected, the distribution of query frequencies and query term frequencies follow power law distributions, with slope of 1.84 and 2.26, respectively.",
                "In this figure, the query frequencies were computed as they appear in the logs with no normalization for case or white space.",
                "The query terms (middle curve) have been normalized for case, as have the terms in the document collection.",
                "The document collection that we use for our experiments is a summary of the UK domain crawled in May 2006.1 This summary corresponds to a maximum of 400 crawled documents per host, using a breadth first crawling strategy, comprising 15GB.",
                "The distribution of document frequencies of terms in the collection follows a power law distribution with slope 2.38 (upper curve in Figure 2).",
                "The statistics of the collection are shown in Table 1.",
                "We measured the correlation between the document frequency of terms in the collection and the number of queries that contain a particular term in the query log to be 0.424.",
                "A scatter plot for a random sample of terms is shown in Figure 3.",
                "In this experiment, terms have been converted to lower case in both the queries and the documents so that the frequencies will be comparable. 1e-07 1e-06 1e-05 1e-04 0.001 0.01 0.1 1 1e-06 1e-05 1e-04 0.001 0.01 0.1 1 Queryfrequency Document frequency Figure 3: Normalized scatter plot of document-term frequencies vs. query-term frequencies. 4.",
                "CACHING OF QUERIES AND TERMS Caching relies upon the assumption that there is locality in the stream of requests.",
                "That is, there must be sufficient repetition in the stream of requests and within intervals of time that enable a cache memory of reasonable size to be effective.",
                "In the query log we used, 88% of the unique queries are singleton queries, and 44% are singleton queries out of the whole volume.",
                "Thus, out of all queries in the stream composing the query log, the upper threshold on hit ratio is 56%.",
                "This is because only 56% of all the queries comprise queries that have multiple occurrences.",
                "It is important to observe, however, that not all queries in this 56% can be cache hits because of compulsory misses.",
                "A compulsory miss 1 The collection is available from the University of Milan: http://law.dsi.unimi.it/.",
                "URL retrieved 05/2007. 0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1 240 260 280 300 320 340 360 Numberofelements Bin number Total terms Terms diff Total queries Unique queries Unique terms Query diff Figure 4: Arrival rate for both terms and queries. happens when the cache receives a query for the first time.",
                "This is different from capacity misses, which happen due to space constraints on the amount of memory the cache uses.",
                "If we consider a cache with infinite memory, then the hit ratio is 50%.",
                "Note that for an infinite cache there are no capacity misses.",
                "As we mentioned before, another possibility is to cache the posting lists of terms.",
                "Intuitively, this gives more freedom in the utilization of the cache content to respond to queries because cached terms might form a new query.",
                "On the other hand, they need more space.",
                "As opposed to queries, the fraction of singleton terms in the total volume of terms is smaller.",
                "In our query log, only 4% of the terms appear once, but this accounts for 73% of the vocabulary of query terms.",
                "We show in Section 5 that caching a small fraction of terms, while accounting for terms appearing in many documents, is potentially very effective.",
                "Figure 4 shows several graphs corresponding to the normalized arrival rate for different cases using days as bins.",
                "That is, we plot the normalized number of elements that appear in a day.",
                "This graph shows only a period of 122 days, and we normalize the values by the maximum value observed throughout the whole period of the query log.",
                "Total queries and Total terms correspond to the total volume of queries and terms, respectively.",
                "Unique queries and Unique terms correspond to the arrival rate of unique queries and terms.",
                "Finally, Query diff and Terms diff correspond to the difference between the curves for total and unique.",
                "In Figure 4, as expected, the volume of terms is much higher than the volume of queries.",
                "The difference between the total number of terms and the number of unique terms is much larger than the difference between the total number of queries and the number of unique queries.",
                "This observation implies that terms repeat significantly more than queries.",
                "If we use smaller bins, say of one hour, then the ratio of unique to volume is higher for both terms and queries because it leaves less room for repetition.",
                "We also estimated the workload using the document frequency of terms as a measure of how much work a query imposes on a search engine.",
                "We found that it follows closely the arrival rate for terms shown in Figure 4.",
                "To demonstrate the effect of a dynamic cache on the query frequency distribution of Figure 2, we plot the same frequency graph, but now considering the frequency of queries Figure 5: Frequency graph after LRU cache. after going through an LRU cache.",
                "On a cache miss, an LRU cache decides upon an entry to evict using the information on the recency of queries.",
                "In this graph, the most frequent queries are not the same queries that were most frequent before the cache.",
                "It is possible that queries that are most frequent after the cache have different characteristics, and tuning the search engine to queries frequent before the cache may degrade performance for non-cached queries.",
                "The maximum frequency after caching is less than 1% of the maximum frequency before the cache, thus showing that the cache is very effective in reducing the load of frequent queries.",
                "If we re-rank the queries according to after-cache frequency, the distribution is still a power law, but with a much smaller value for the highest frequency.",
                "When discussing the effectiveness of dynamically caching, an important metric is cache miss rate.",
                "To analyze the cache miss rate for different memory constraints, we use the working set model [6, 14].",
                "A working set, informally, is the set of references that an application or an operating system is currently working with.",
                "The model uses such sets in a strategy that tries to capture the temporal locality of references.",
                "The working set strategy then consists in keeping in memory only the elements that are referenced in the previous θ steps of the input sequence, where θ is a configurable parameter corresponding to the window size.",
                "Originally, working sets have been used for page replacement algorithms of operating systems, and considering such a strategy in the context of search engines is interesting for three reasons.",
                "First, it captures the amount of locality of queries and terms in a sequence of queries.",
                "Locality in this case refers to the frequency of queries and terms in a window of time.",
                "If many queries appear multiple times in a window, then locality is high.",
                "Second, it enables an oﬄine analysis of the expected miss rate given different memory constraints.",
                "Third, working sets capture aspects of efficient caching algorithms such as LRU.",
                "LRU assumes that references farther in the past are less likely to be referenced in the present, which is implicit in the concept of working sets [14].",
                "Figure 6 plots the miss rate for different working set sizes, and we consider working sets of both queries and terms.",
                "The working set sizes are normalized against the total number of queries in the query log.",
                "In the graph for queries, there is a sharp decay until approximately 0.01, and the rate at which the miss rate drops decreases as we increase the size of the working set over 0.01.",
                "Finally, the minimum value it reaches is 50% miss rate, not shown in the figure as we have cut the tail of the curve for presentation purposes. 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1 0 0.05 0.1 0.15 0.2 Missrate Normalized working set size Queries Terms Figure 6: Miss rate as a function of the working set size. 1 10 100 1000 10000 100000 1e+06 Frequency Distance Figure 7: Distribution of distances expressed in terms of distinct queries.",
                "Compared to the query curve, we observe that the minimum miss rate for terms is substantially smaller.",
                "The miss rate also drops sharply on values up to 0.01, and it decreases minimally for higher values.",
                "The minimum value, however, is slightly over 10%, which is much smaller than the minimum value for the sequence of queries.",
                "This implies that with such a policy it is possible to achieve over 80% hit rate, if we consider caching dynamically posting lists for terms as opposed to caching answers for queries.",
                "This result does not consider the space required for each unit stored in the cache memory, or the amount of time it takes to put together a response to a user query.",
                "We analyze these issues more carefully later in this paper.",
                "It is interesting also to observe the histogram of Figure 7, which is an intermediate step in the computation of the miss rate graph.",
                "It reports the distribution of distances between repetitions of the same frequent query.",
                "The distance in the plot is measured in the number of distinct queries separating a query and its repetition, and it considers only queries appearing at least 10 times.",
                "From Figures 6 and 7, we conclude that even if we set the size of the query answers cache to a relatively large number of entries, the miss rate is high.",
                "Thus, caching the posting lists of terms has the potential to improve the hit ratio.",
                "This is what we explore next. 5.",
                "CACHING POSTING LISTS The previous section shows that caching posting lists can obtain a higher hit rate compared to caching query answers.",
                "In this section we study the problem of how to select posting lists to place on a certain amount of available memory, assuming that the whole index is larger than the amount of memory available.",
                "The posting lists have variable size (in fact, their size distribution follows a power law), so it is beneficial for a caching policy to consider the sizes of the posting lists.",
                "We consider both dynamic and static caching.",
                "For dynamic caching, we use two well-known policies, LRU and LFU, as well as a modified algorithm that takes posting-list size into account.",
                "Before discussing the static caching strategies, we introduce some notation.",
                "We use fq(t) to denote the query-term frequency of a term t, that is, the number of queries containing t in the query log, and fd(t) to denote the document frequency of t, that is, the number of documents in the collection in which the term t appears.",
                "The first strategy we consider is the algorithm proposed by Baeza-Yates and Saint-Jean [2], which consists in selecting the posting lists of the terms with the highest query-term frequencies fq(t).",
                "We call this algorithm Qtf.",
                "We observe that there is a trade-off between fq(t) and fd(t).",
                "Terms with high fq(t) are useful to keep in the cache because they are queried often.",
                "On the other hand, terms with high fd(t) are not good candidates because they correspond to long posting lists and consume a substantial amount of space.",
                "In fact, the problem of selecting the best posting lists for the static cache corresponds to the standard Knapsack problem: given a knapsack of fixed capacity, and a set of n items, such as the i-th item has value ci and size si, select the set of items that fit in the knapsack and maximize the overall value.",
                "In our case, value corresponds to fq(t) and size corresponds to fd(t).",
                "Thus, we employ a simple algorithm for the knapsack problem, which is selecting the posting lists of the terms with the highest values of the ratio fq(t) fd(t) .",
                "We call this algorithm QtfDf.",
                "We tried other variations considering query frequencies instead of term frequencies, but the gain was minimal compared to the complexity added.",
                "In addition to the above two static algorithms we consider the following algorithms for dynamic caching: • LRU: A standard LRU algorithm, but many posting lists might need to be evicted (in order of least-recent usage) until there is enough space in the memory to place the currently accessed posting list; • LFU: A standard LFU algorithm (eviction of the leastfrequently used), with the same modification as the LRU; • Dyn-QtfDf: A dynamic version of the QtfDf algorithm; evict from the cache the term(s) with the lowest fq(t) fd(t) ratio.",
                "The performance of all the above algorithms for 15 weeks of the query log and the UK dataset are shown in Figure 8.",
                "Performance is measured with hit rate.",
                "The cache size is measured as a fraction of the total space required to store the posting lists of all terms.",
                "For the dynamic algorithms, we load the cache with terms in order of fq(t) and we let the cache warm up for 1 million queries.",
                "For the static algorithms, we assume complete knowledge of the frequencies fq(t), that is, we estimate fq(t) from the whole query stream.",
                "As we show in Section 7 the results do not change much if we compute the query-term frequencies using the first 3 or 4 weeks of the query log and measure the hit rate on the rest. 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1 0.1 0.2 0.3 0.4 0.5 0.6 0.7 Hitrate Cache size Caching posting lists static QTF/DF LRU LFU Dyn-QTF/DF QTF Figure 8: Hit rate of different strategies for caching posting lists.",
                "The most important observation from our experiments is that the static QtfDf algorithm has a better hit rate than all the dynamic algorithms.",
                "An important benefit a static cache is that it requires no eviction and it is hence more efficient when evaluating queries.",
                "However, if the characteristics of the query traffic change frequently over time, then it requires re-populating the cache often or there will be a significant impact on hit rate. 6.",
                "ANALYSIS OF STATIC CACHING In this section we provide a detailed analysis for the problem of deciding whether it is preferable to cache query answers or cache posting lists.",
                "Our analysis takes into account the impact of caching between two levels of the data-access hierarchy.",
                "It can either be applied at the memory/disk layer or at a server/remote server layer as in the architecture we discussed in the introduction.",
                "Using a particular system model, we obtain estimates for the parameters required by our analysis, which we subsequently use to decide the optimal trade-off between caching query answers and caching posting lists. 6.1 Analytical Model Let M be the size of the cache measured in answer units (the cache can store M query answers).",
                "Assume that all posting lists are of the same length L, measured in answer units.",
                "We consider the following two cases: (A) a cache that stores only precomputed answers, and (B) a cache that stores only posting lists.",
                "In the first case, Nc = M answers fit in the cache, while in the second case Np = M/L posting lists fit in the cache.",
                "Thus, Np = Nc/L.",
                "Note that although posting lists require more space, we can combine terms to evaluate more queries (or partial queries).",
                "For case (A), suppose that a query answer in the cache can be evaluated in 1 time unit.",
                "For case (B), assume that if the posting lists of the terms of a query are in the cache then the results can be computed in TR1 time units, while if the posting lists are not in the cache then the results can be computed in TR2 time units.",
                "Of course TR2 > TR1.",
                "Now we want to compare the time to answer a stream of Q queries in both cases.",
                "Let Vc(Nc) be the volume of the most frequent Nc queries.",
                "Then, for case (A), we have an overall time TCA = Vc(Nc) + TR2(Q − Vc(Nc)).",
                "Similarly, for case (B), let Vp(Np) be the number of computable queries.",
                "Then we have overall time TP L = TR1Vp(Np) + TR2(Q − Vp(Np)).",
                "We want to check under which conditions we have TP L < TCA.",
                "We have TP L − TCA = (TR2 − 1)Vc(Nc) − (TR2 − TR1)Vp(Np) > 0.",
                "Figure 9 shows the values of Vp and Vc for our data.",
                "We can see that caching answers saturates faster and for this particular data there is no additional benefit from using more than 10% of the index space for caching answers.",
                "As the query distribution is a power law with parameter α > 1, the i-th most frequent query appears with probability proportional to 1 iα .",
                "Therefore, the volume Vc(n), which is the total number of the n most frequent queries, is Vc(n) = V0 n i=1 Q iα = γnQ (0 < γn < 1).",
                "We know that Vp(n) grows faster than Vc(n) and assume, based on experimental results, that the relation is of the form Vp(n) = k Vc(n)β .",
                "In the worst case, for a large cache, β → 1.",
                "That is, both techniques will cache a constant fraction of the overall query volume.",
                "Then caching posting lists makes sense only if L(TR2 − 1) k(TR2 − TR1) > 1.",
                "If we use compression, we have L < L and TR1 > TR1.",
                "According to the experiments that we show later, compression is always better.",
                "For a small cache, we are interested in the transient behavior and then β > 1, as computed from our data.",
                "In this case there will always be a point where TP L > TCA for a large number of queries.",
                "In reality, instead of filling the cache only with answers or only with posting lists, a better strategy will be to divide the total cache space into cache for answers and cache for posting lists.",
                "In such a case, there will be some queries that could be answered by both parts of the cache.",
                "As the answer cache is faster, it will be the first choice for answering those queries.",
                "Let QNc and QNp be the set of queries that can be answered by the cached answers and the cached posting lists, respectively.",
                "Then, the overall time is T = Vc(Nc)+TR1V (QNp −QNc )+TR2(Q−V (QNp ∪QNc )), where Np = (M − Nc)/L.",
                "Finding the optimal division of the cache in order to minimize the overall retrieval time is a difficult problem to solve analytically.",
                "In Section 6.3 we use simulations to derive optimal cache trade-offs for particular implementation examples. 6.2 Parameter Estimation We now use a particular implementation of a centralized system and the model of a distributed system as examples from which we estimate the parameters of the analysis from the previous section.",
                "We perform the experiments using an optimized version of Terrier [11] for both indexing documents and processing queries, on a single machine with a Pentium 4 at 2GHz and 1GB of RAM.",
                "We indexed the documents from the UK-2006 dataset, without removing stop words or applying stemming.",
                "The posting lists in the inverted file consist of pairs of document identifier and term frequency.",
                "We compress the document identifier gaps using Elias gamma encoding, and the 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1 0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1 Queryvolume Space precomputed answers posting lists Figure 9: Cache saturation as a function of size.",
                "Table 2: Ratios between the average time to evaluate a query and the average time to return cached answers (centralized and distributed case).",
                "Centralized system TR1 TR2 TR1 TR2 Full evaluation 233 1760 707 1140 Partial evaluation 99 1626 493 798 LAN system TRL 1 TRL 2 TR L 1 TR L 2 Full evaluation 242 1769 716 1149 Partial evaluation 108 1635 502 807 WAN system TRW 1 TRW 2 TR W 1 TR W 2 Full evaluation 5001 6528 5475 5908 Partial evaluation 4867 6394 5270 5575 term frequencies in documents using unary encoding [16].",
                "The size of the inverted file is 1,189Mb.",
                "A stored answer requires 1264 bytes, and an uncompressed posting takes 8 bytes.",
                "From Table 1, we obtain L = (8·# of postings) 1264·# of terms = 0.75 and L = Inverted file size 1264·# of terms = 0.26.",
                "We estimate the ratio TR = T/Tc between the average time T it takes to evaluate a query and the average time Tc it takes to return a stored answer for the same query, in the following way.",
                "Tc is measured by loading the answers for 100,000 queries in memory, and answering the queries from memory.",
                "The average time is Tc = 0.069ms.",
                "T is measured by processing the same 100,000 queries (the first 10,000 queries are used to warm-up the system).",
                "For each query, we remove stop words, if there are at least three remaining terms.",
                "The stop words correspond to the terms with a frequency higher than the number of documents in the index.",
                "We use a document-at-a-time approach to retrieve documents containing all query terms.",
                "The only disk access required during query processing is for reading compressed posting lists from the inverted file.",
                "We perform both full and partial evaluation of answers, because some queries are likely to retrieve a large number of documents, and only a fraction of the retrieved documents will be seen by users.",
                "In the partial evaluation of queries, we terminate the processing after matching 10,000 documents.",
                "The estimated ratios TR are presented in Table 2.",
                "Figure 10 shows for a sample of queries the workload of the system with partial query evaluation and compressed posting lists.",
                "The x-axis corresponds to the total time the system spends processing a particular query, and the vertical axis corresponds to the sum t∈q fq · fd(t).",
                "Notice that the total number of postings of the query-terms does not necessarily provide an accurate estimate of the workload imposed on the system by a query (which is the case for full evaluation and uncompressed lists). 0 0.2 0.4 0.6 0.8 1 0 0.2 0.4 0.6 0.8 1 Totalpostingstoprocessquery(normalized) Total time to process query (normalized) Partial processing of compressed postings query len = 1 query len in [2,3] query len in [4,8] query len > 8 Figure 10: Workload for partial query evaluation with compressed posting lists.",
                "The analysis of the previous section also applies to a distributed retrieval system in one or multiple sites.",
                "Suppose that a document partitioned distributed system is running on a cluster of machines interconnected with a local area network (LAN) in one site.",
                "The broker receives queries and broadcasts them to the query processors, which answer the queries and return the results to the broker.",
                "Finally, the broker merges the received answers and generates the final set of answers (we assume that the time spent on merging results is negligible).",
                "The difference between the centralized architecture and the document partition architecture is the extra communication between the broker and the query processors.",
                "Using ICMP pings on a 100Mbps LAN, we have measured that sending the query from the broker to the query processors which send an answer of 4,000 bytes back to the broker takes on average 0.615ms.",
                "Hence, TRL = TR + 0.615ms/0.069ms = TR + 9.",
                "In the case when the broker and the query processors are in different sites connected with a wide area network (WAN), we estimated that broadcasting the query from the broker to the query processors and getting back an answer of 4,000 bytes takes on average 329ms.",
                "Hence, TRW = TR + 329ms/0.069ms = TR + 4768. 6.3 Simulation Results We now address the problem of finding the optimal tradeoff between caching query answers and caching posting lists.",
                "To make the problem concrete we assume a fixed budget M on the available memory, out of which x units are used for caching query answers and M − x for caching posting lists.",
                "We perform simulations and compute the average response time as a function of x.",
                "Using a part of the query log as training data, we first allocate in the cache the answers to the most frequent queries that fit in space x, and then we use the rest of the memory to cache posting lists.",
                "For selecting posting lists we use the QtfDf algorithm, applied to the training query log but excluding the queries that have already been cached.",
                "In Figure 11, we plot the simulated response time for a centralized system as a function of x.",
                "For the uncompressed index we use M = 1GB, and for the compressed index we use M = 0.5GB.",
                "In the case of the configuration that uses partial query evaluation with compressed posting lists, the lowest response time is achieved when 0.15GB out of the 0.5GB is allocated for storing answers for queries.",
                "We obtained similar trends in the results for the LAN setting.",
                "Figure 12 shows the simulated workload for a distributed system across a WAN.",
                "In this case, the total amount of memory is split between the broker, which holds the cached 400 500 600 700 800 900 1000 1100 1200 0 0.2 0.4 0.6 0.8 1 Averageresponsetime Space (GB) Simulated workload -- single machine full / uncompr / 1 G partial / uncompr / 1 G full / compr / 0.5 G partial / compr / 0.5 G Figure 11: Optimal division of the cache in a server. 3000 3500 4000 4500 5000 5500 6000 0 0.2 0.4 0.6 0.8 1 Averageresponsetime Space (GB) Simulated workload -- WAN full / uncompr / 1 G partial / uncompr / 1 G full / compr / 0.5 G partial / compr / 0.5 G Figure 12: Optimal division of the cache when the next level requires WAN access. answers of queries, and the query processors, which hold the cache of posting lists.",
                "According to the figure, the difference between the configurations of the query processors is less important because the network communication overhead increases the response time substantially.",
                "When using uncompressed posting lists, the optimal allocation of memory corresponds to using approximately 70% of the memory for caching query answers.",
                "This is explained by the fact that there is no need for network communication when the query can be answered by the cache at the broker. 7.",
                "EFFECT OF THE QUERY DYNAMICS For our query log, the query distribution and query-term distribution change slowly over time.",
                "To support this claim, we first assess how topics change comparing the distribution of queries from the first week in June, 2006, to the distribution of queries for the remainder of 2006 that did not appear in the first week in June.",
                "We found that a very small percentage of queries are new queries.",
                "The majority of queries that appear in a given week repeat in the following weeks for the next six months.",
                "We then compute the hit rate of a static cache of 128, 000 answers trained over a period of two weeks (Figure 13).",
                "We report hit rate hourly for 7 days, starting from 5pm.",
                "We observe that the hit rate reaches its highest value during the night (around midnight), whereas around 2-3pm it reaches its minimum.",
                "After a small decay in hit rate values, the hit rate stabilizes between 0.28, and 0.34 for the entire week, suggesting that the static cache is effective for a whole week after the training period. 0.26 0.27 0.28 0.29 0.3 0.31 0.32 0.33 0.34 0.35 0.36 0.37 0 20 40 60 80 100 120 140 160 Hit-rate Time Hits on the frequent queries of distances Figure 13: Hourly hit rate for a static cache holding 128,000 answers during the period of a week.",
                "The static cache of posting lists can be periodically recomputed.",
                "To estimate the time interval in which we need to recompute the posting lists on the static cache we need to consider an efficiency/quality trade-off: using too short a time interval might be prohibitively expensive, while recomputing the cache too infrequently might lead to having an obsolete cache not corresponding to the statistical characteristics of the current query stream.",
                "We measured the effect on the QtfDf algorithm of the changes in a 15-week query stream (Figure 14).",
                "We compute the query term frequencies over the whole stream, select which terms to cache, and then compute the hit rate on the whole query stream.",
                "This hit rate is as an upper bound, and it assumes perfect knowledge of the query term frequencies.",
                "To simulate a realistic scenario, we use the first 6 (3) weeks of the query stream for computing query term frequencies and the following 9 (12) weeks to estimate the hit rate.",
                "As Figure 14 shows, the hit rate decreases by less than 2%.",
                "The high correlation among the query term frequencies during different time periods explains the graceful adaptation of the static caching algorithms to the future query stream.",
                "Indeed, the pairwise correlation among all possible 3-week periods of the 15-week query stream is over 99.5%. 8.",
                "CONCLUSIONS Caching is an effective technique in search engines for improving response time, reducing the load on query processors, and improving network bandwidth utilization.",
                "We present results on both dynamic and static caching.",
                "Dynamic caching of queries has limited effectiveness due to the high number of compulsory misses caused by the number of unique or infrequent queries.",
                "Our results show that in our UK log, the minimum miss rate is 50% using a working set strategy.",
                "Caching terms is more effective with respect to miss rate, achieving values as low as 12%.",
                "We also propose a new algorithm for static caching of posting lists that outperforms previous static caching algorithms as well as dynamic algorithms such as LRU and LFU, obtaining hit rate values that are over 10% higher compared these strategies.",
                "We present a framework for the analysis of the trade-off between caching query results and caching posting lists, and we simulate different types of architectures.",
                "Our results show that for centralized and LAN environments, there is an optimal allocation of caching query results and caching of posting lists, while for WAN scenarios in which network time prevails it is more important to cache query results. 0.45 0.5 0.55 0.6 0.65 0.7 0.75 0.8 0.85 0.9 0.95 0.1 0.2 0.3 0.4 0.5 0.6 0.7 Hitrate Cache size Dynamics of static QTF/DF caching policy perfect knowledge 6-week training 3-week training Figure 14: Impact of distribution changes on the static caching of posting lists. 9.",
                "REFERENCES [1] V. N. Anh and A. Moffat.",
                "Pruned query evaluation using pre-computed impacts.",
                "In ACM CIKM, 2006. [2] R. A. Baeza-Yates and F. Saint-Jean.",
                "A three level search engine index based in query log distribution.",
                "In SPIRE, 2003. [3] C. Buckley and A. F. Lewit.",
                "Optimization of inverted vector searches.",
                "In ACM SIGIR, 1985. [4] S. B¨uttcher and C. L. A. Clarke.",
                "A document-centric approach to static index pruning in text retrieval systems.",
                "In ACM CIKM, 2006. [5] P. Cao and S. Irani.",
                "Cost-aware WWW proxy caching algorithms.",
                "In USITS, 1997. [6] P. Denning.",
                "Working sets past and present.",
                "IEEE Trans. on Software Engineering, SE-6(1):64-84, 1980. [7] T. Fagni, R. Perego, F. Silvestri, and S. Orlando.",
                "Boosting the performance of <br>web search engine</br>s: Caching and prefetching query results by exploiting historical usage data.",
                "ACM Trans.",
                "Inf.",
                "Syst., 24(1):51-78, 2006. [8] R. Lempel and S. Moran.",
                "Predictive caching and prefetching of query results in search engines.",
                "In WWW, 2003. [9] X.",
                "Long and T. Suel.",
                "Three-level caching for efficient query processing in large <br>web search engine</br>s.",
                "In WWW, 2005. [10] E. P. Markatos.",
                "On caching search engine query results.",
                "Computer Communications, 24(2):137-143, 2001. [11] I. Ounis, G. Amati, V. Plachouras, B.",
                "He, C. Macdonald, and C. Lioma.",
                "Terrier: A High Performance and Scalable Information Retrieval Platform.",
                "In SIGIR Workshop on Open Source Information Retrieval, 2006. [12] V. V. Raghavan and H. Sever.",
                "On the reuse of past optimal queries.",
                "In ACM SIGIR, 1995. [13] P. C. Saraiva, E. S. de Moura, N. Ziviani, W. Meira, R. Fonseca, and B. Riberio-Neto.",
                "Rank-preserving two-level caching for scalable search engines.",
                "In ACM SIGIR, 2001. [14] D. R. Slutz and I. L. Traiger.",
                "A note on the calculation of average working set size.",
                "Communications of the ACM, 17(10):563-565, 1974. [15] T. Strohman, H. Turtle, and W. B. Croft.",
                "Optimization strategies for complex queries.",
                "In ACM SIGIR, 2005. [16] I. H. Witten, T. C. Bell, and A. Moffat.",
                "Managing Gigabytes: Compressing and Indexing Documents and Images.",
                "John Wiley & Sons, Inc., NY, 1994. [17] N. E. Young.",
                "On-line file caching.",
                "Algorithmica, 33(3):371-383, 2002."
            ],
            "original_annotated_samples": [],
            "translated_annotated_samples": [
                "Aumentando el rendimiento del \"motor de búsqueda web\": almacenamiento en caché y captación previa de los resultados de consultas mediante la explotación de datos de uso histórico.",
                "El almacenamiento en caché de tres niveles para un procesamiento eficiente de consultas en un gran \"motor de búsqueda web\" s."
            ],
            "translated_text": "",
            "candidates": [
                "motor de búsqueda web",
                "motor de búsqueda web",
                "motor de búsqueda web",
                "motor de búsqueda web"
            ],
            "error": []
        },
        "static caching": {
            "translated_key": "almacenamiento en caché estático",
            "is_in_text": true,
            "original_annotated_sentences": [
                "The Impact of Caching on Search Engines Ricardo Baeza-Yates1 rbaeza@acm.org Aristides Gionis1 gionis@yahoo-inc.com Flavio Junqueira1 fpj@yahoo-inc.com Vanessa Murdock1 vmurdock@yahoo-inc.com Vassilis Plachouras1 vassilis@yahoo-inc.com Fabrizio Silvestri2 f.silvestri@isti.cnr.it 1 Yahoo!",
                "Research Barcelona 2 ISTI - CNR Barcelona, SPAIN Pisa, ITALY ABSTRACT In this paper we study the trade-offs in designing efficient caching systems for Web search engines.",
                "We explore the impact of different approaches, such as static vs. dynamic caching, and caching query results vs. caching posting lists.",
                "Using a query log spanning a whole year we explore the limitations of caching and we demonstrate that caching posting lists can achieve higher hit rates than caching query answers.",
                "We propose a new algorithm for <br>static caching</br> of posting lists, which outperforms previous methods.",
                "We also study the problem of finding the optimal way to split the static cache between answers and posting lists.",
                "Finally, we measure how the changes in the query log affect the effectiveness of <br>static caching</br>, given our observation that the distribution of the queries changes slowly over time.",
                "Our results and observations are applicable to different levels of the data-access hierarchy, for instance, for a memory/disk layer or a broker/remote server layer.",
                "Categories and Subject Descriptors H.3.3 [Information Storage and Retrieval]: Information Search and Retrieval - Search process; H.3.4 [Information Storage and Retrieval]: Systems and Software - Distributed systems, Performance evaluation (efficiency and effectiveness) General Terms Algorithms, Experimentation 1.",
                "INTRODUCTION Millions of queries are submitted daily to Web search engines, and users have high expectations of the quality and speed of the answers.",
                "As the searchable Web becomes larger and larger, with more than 20 billion pages to index, evaluating a single query requires processing large amounts of data.",
                "In such a setting, to achieve a fast response time and to increase the query throughput, using a cache is crucial.",
                "The primary use of a cache memory is to speedup computation by exploiting frequently or recently used data, although reducing the workload to back-end servers is also a major goal.",
                "Caching can be applied at different levels with increasing response latencies or processing requirements.",
                "For example, the different levels may correspond to the main memory, the disk, or resources in a local or a wide area network.",
                "The decision of what to cache is either off-line (static) or online (dynamic).",
                "A static cache is based on historical information and is periodically updated.",
                "A dynamic cache replaces entries according to the sequence of requests.",
                "When a new request arrives, the cache system decides whether to evict some entry from the cache in the case of a cache miss.",
                "Such online decisions are based on a cache policy, and several different policies have been studied in the past.",
                "For a search engine, there are two possible ways to use a cache memory: Caching answers: As the engine returns answers to a particular query, it may decide to store these answers to resolve future queries.",
                "Caching terms: As the engine evaluates a particular query, it may decide to store in memory the posting lists of the involved query terms.",
                "Often the whole set of posting lists does not fit in memory, and consequently, the engine has to select a small set to keep in memory and speed up query processing.",
                "Returning an answer to a query that already exists in the cache is more efficient than computing the answer using cached posting lists.",
                "On the other hand, previously unseen queries occur more often than previously unseen terms, implying a higher miss rate for cached answers.",
                "Caching of posting lists has additional challenges.",
                "As posting lists have variable size, caching them dynamically is not very efficient, due to the complexity in terms of efficiency and space, and the skewed distribution of the query stream, as shown later.",
                "<br>static caching</br> of posting lists poses even more challenges: when deciding which terms to cache one faces the trade-off between frequently queried terms and terms with small posting lists that are space efficient.",
                "Finally, before deciding to adopt a <br>static caching</br> policy the query stream should be analyzed to verify that its characteristics do not change rapidly over time.",
                "Broker <br>static caching</br> posting lists Dynamic/Static cached answers Local query processor Disk Next caching level Local network access Remote network access Figure 1: One caching level in a distributed search architecture.",
                "In this paper we explore the trade-offs in the design of each cache level, showing that the problem is the same and only a few parameters change.",
                "In general, we assume that each level of caching in a distributed search architecture is similar to that shown in Figure 1.",
                "We use a query log spanning a whole year to explore the limitations of dynamically caching query answers or posting lists for query terms.",
                "More concretely, our main conclusions are that: • Caching query answers results in lower hit ratios compared to caching of posting lists for query terms, but it is faster because there is no need for query evaluation.",
                "We provide a framework for the analysis of the trade-off between <br>static caching</br> of query answers and posting lists; • <br>static caching</br> of terms can be more effective than dynamic caching with, for example, LRU.",
                "We provide algorithms based on the Knapsack problem for selecting the posting lists to put in a static cache, and we show improvements over previous work, achieving a hit ratio over 90%; • Changes of the query distribution over time have little impact on <br>static caching</br>.",
                "The remainder of this paper is organized as follows.",
                "Sections 2 and 3 summarize related work and characterize the data sets we use.",
                "Section 4 discusses the limitations of dynamic caching.",
                "Sections 5 and 6 introduce algorithms for caching posting lists, and a theoretical framework for the analysis of <br>static caching</br>, respectively.",
                "Section 7 discusses the impact of changes in the query distribution on <br>static caching</br>, and Section 8 provides concluding remarks. 2.",
                "RELATED WORK There is a large body of work devoted to query optimization.",
                "Buckley and Lewit [3], in one of the earliest works, take a term-at-a-time approach to deciding when inverted lists need not be further examined.",
                "More recent examples demonstrate that the top k documents for a query can be returned without the need for evaluating the complete set of posting lists [1, 4, 15].",
                "Although these approaches seek to improve query processing efficiency, they differ from our current work in that they do not consider caching.",
                "They may be considered separate and complementary to a cache-based approach.",
                "Raghavan and Sever [12], in one of the first papers on exploiting user query history, propose using a query base, built upon a set of persistent optimal queries submitted in the past, to improve the retrieval effectiveness for similar future queries.",
                "Markatos [10] shows the existence of temporal locality in queries, and compares the performance of different caching policies.",
                "Based on the observations of Markatos, Lempel and Moran propose a new caching policy, called Probabilistic Driven Caching, by attempting to estimate the probability distribution of all possible queries submitted to a search engine [8].",
                "Fagni et al. follow Markatos work by showing that combining static and dynamic caching policies together with an adaptive prefetching policy achieves a high hit ratio [7].",
                "Different from our work, they consider caching and prefetching of pages of results.",
                "As systems are often hierarchical, there has also been some effort on multi-level architectures.",
                "Saraiva et al. propose a new architecture for Web search engines using a two-level dynamic caching system [13].",
                "Their goal for such systems has been to improve response time for hierarchical engines.",
                "In their architecture, both levels use an LRU eviction policy.",
                "They find that the second-level cache can effectively reduce disk traffic, thus increasing the overall throughput.",
                "Baeza-Yates and Saint-Jean propose a three-level index organization [2].",
                "Long and Suel propose a caching system structured according to three different levels [9].",
                "The intermediate level contains frequently occurring pairs of terms and stores the intersections of the corresponding inverted lists.",
                "These last two papers are related to ours in that they exploit different caching strategies at different levels of the memory hierarchy.",
                "Finally, our <br>static caching</br> algorithm for posting lists in Section 5 uses the ratio frequency/size in order to evaluate the goodness of an item to cache.",
                "Similar ideas have been used in the context of file caching [17], Web caching [5], and even caching of posting lists [9], but in all cases in a dynamic setting.",
                "To the best of our knowledge we are the first to use this approach for <br>static caching</br> of posting lists. 3.",
                "DATA CHARACTERIZATION Our data consists of a crawl of documents from the UK domain, and query logs of one year of queries submitted to http://www.yahoo.co.uk from November 2005 to November 2006.",
                "In our logs, 50% of the total volume of queries are unique.",
                "The average query length is 2.5 terms, with the longest query having 731 terms. 1e-07 1e-06 1e-05 1e-04 0.001 0.01 0.1 1 1e-08 1e-07 1e-06 1e-05 1e-04 0.001 0.01 0.1 1 Frequency(normalized) Frequency rank (normalized) Figure 2: The distribution of queries (bottom curve) and query terms (middle curve) in the query log.",
                "The distribution of document frequencies of terms in the UK-2006 dataset (upper curve).",
                "Figure 2 shows the distributions of queries (lower curve), and query terms (middle curve).",
                "The x-axis represents the normalized frequency rank of the query or term. (The most frequent query appears closest to the y-axis.)",
                "The y-axis is Table 1: Statistics of the UK-2006 sample.",
                "UK-2006 sample statistics # of documents 2,786,391 # of terms 6,491,374 # of tokens 2,109,512,558 the normalized frequency for a given query (or term).",
                "As expected, the distribution of query frequencies and query term frequencies follow power law distributions, with slope of 1.84 and 2.26, respectively.",
                "In this figure, the query frequencies were computed as they appear in the logs with no normalization for case or white space.",
                "The query terms (middle curve) have been normalized for case, as have the terms in the document collection.",
                "The document collection that we use for our experiments is a summary of the UK domain crawled in May 2006.1 This summary corresponds to a maximum of 400 crawled documents per host, using a breadth first crawling strategy, comprising 15GB.",
                "The distribution of document frequencies of terms in the collection follows a power law distribution with slope 2.38 (upper curve in Figure 2).",
                "The statistics of the collection are shown in Table 1.",
                "We measured the correlation between the document frequency of terms in the collection and the number of queries that contain a particular term in the query log to be 0.424.",
                "A scatter plot for a random sample of terms is shown in Figure 3.",
                "In this experiment, terms have been converted to lower case in both the queries and the documents so that the frequencies will be comparable. 1e-07 1e-06 1e-05 1e-04 0.001 0.01 0.1 1 1e-06 1e-05 1e-04 0.001 0.01 0.1 1 Queryfrequency Document frequency Figure 3: Normalized scatter plot of document-term frequencies vs. query-term frequencies. 4.",
                "CACHING OF QUERIES AND TERMS Caching relies upon the assumption that there is locality in the stream of requests.",
                "That is, there must be sufficient repetition in the stream of requests and within intervals of time that enable a cache memory of reasonable size to be effective.",
                "In the query log we used, 88% of the unique queries are singleton queries, and 44% are singleton queries out of the whole volume.",
                "Thus, out of all queries in the stream composing the query log, the upper threshold on hit ratio is 56%.",
                "This is because only 56% of all the queries comprise queries that have multiple occurrences.",
                "It is important to observe, however, that not all queries in this 56% can be cache hits because of compulsory misses.",
                "A compulsory miss 1 The collection is available from the University of Milan: http://law.dsi.unimi.it/.",
                "URL retrieved 05/2007. 0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1 240 260 280 300 320 340 360 Numberofelements Bin number Total terms Terms diff Total queries Unique queries Unique terms Query diff Figure 4: Arrival rate for both terms and queries. happens when the cache receives a query for the first time.",
                "This is different from capacity misses, which happen due to space constraints on the amount of memory the cache uses.",
                "If we consider a cache with infinite memory, then the hit ratio is 50%.",
                "Note that for an infinite cache there are no capacity misses.",
                "As we mentioned before, another possibility is to cache the posting lists of terms.",
                "Intuitively, this gives more freedom in the utilization of the cache content to respond to queries because cached terms might form a new query.",
                "On the other hand, they need more space.",
                "As opposed to queries, the fraction of singleton terms in the total volume of terms is smaller.",
                "In our query log, only 4% of the terms appear once, but this accounts for 73% of the vocabulary of query terms.",
                "We show in Section 5 that caching a small fraction of terms, while accounting for terms appearing in many documents, is potentially very effective.",
                "Figure 4 shows several graphs corresponding to the normalized arrival rate for different cases using days as bins.",
                "That is, we plot the normalized number of elements that appear in a day.",
                "This graph shows only a period of 122 days, and we normalize the values by the maximum value observed throughout the whole period of the query log.",
                "Total queries and Total terms correspond to the total volume of queries and terms, respectively.",
                "Unique queries and Unique terms correspond to the arrival rate of unique queries and terms.",
                "Finally, Query diff and Terms diff correspond to the difference between the curves for total and unique.",
                "In Figure 4, as expected, the volume of terms is much higher than the volume of queries.",
                "The difference between the total number of terms and the number of unique terms is much larger than the difference between the total number of queries and the number of unique queries.",
                "This observation implies that terms repeat significantly more than queries.",
                "If we use smaller bins, say of one hour, then the ratio of unique to volume is higher for both terms and queries because it leaves less room for repetition.",
                "We also estimated the workload using the document frequency of terms as a measure of how much work a query imposes on a search engine.",
                "We found that it follows closely the arrival rate for terms shown in Figure 4.",
                "To demonstrate the effect of a dynamic cache on the query frequency distribution of Figure 2, we plot the same frequency graph, but now considering the frequency of queries Figure 5: Frequency graph after LRU cache. after going through an LRU cache.",
                "On a cache miss, an LRU cache decides upon an entry to evict using the information on the recency of queries.",
                "In this graph, the most frequent queries are not the same queries that were most frequent before the cache.",
                "It is possible that queries that are most frequent after the cache have different characteristics, and tuning the search engine to queries frequent before the cache may degrade performance for non-cached queries.",
                "The maximum frequency after caching is less than 1% of the maximum frequency before the cache, thus showing that the cache is very effective in reducing the load of frequent queries.",
                "If we re-rank the queries according to after-cache frequency, the distribution is still a power law, but with a much smaller value for the highest frequency.",
                "When discussing the effectiveness of dynamically caching, an important metric is cache miss rate.",
                "To analyze the cache miss rate for different memory constraints, we use the working set model [6, 14].",
                "A working set, informally, is the set of references that an application or an operating system is currently working with.",
                "The model uses such sets in a strategy that tries to capture the temporal locality of references.",
                "The working set strategy then consists in keeping in memory only the elements that are referenced in the previous θ steps of the input sequence, where θ is a configurable parameter corresponding to the window size.",
                "Originally, working sets have been used for page replacement algorithms of operating systems, and considering such a strategy in the context of search engines is interesting for three reasons.",
                "First, it captures the amount of locality of queries and terms in a sequence of queries.",
                "Locality in this case refers to the frequency of queries and terms in a window of time.",
                "If many queries appear multiple times in a window, then locality is high.",
                "Second, it enables an oﬄine analysis of the expected miss rate given different memory constraints.",
                "Third, working sets capture aspects of efficient caching algorithms such as LRU.",
                "LRU assumes that references farther in the past are less likely to be referenced in the present, which is implicit in the concept of working sets [14].",
                "Figure 6 plots the miss rate for different working set sizes, and we consider working sets of both queries and terms.",
                "The working set sizes are normalized against the total number of queries in the query log.",
                "In the graph for queries, there is a sharp decay until approximately 0.01, and the rate at which the miss rate drops decreases as we increase the size of the working set over 0.01.",
                "Finally, the minimum value it reaches is 50% miss rate, not shown in the figure as we have cut the tail of the curve for presentation purposes. 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1 0 0.05 0.1 0.15 0.2 Missrate Normalized working set size Queries Terms Figure 6: Miss rate as a function of the working set size. 1 10 100 1000 10000 100000 1e+06 Frequency Distance Figure 7: Distribution of distances expressed in terms of distinct queries.",
                "Compared to the query curve, we observe that the minimum miss rate for terms is substantially smaller.",
                "The miss rate also drops sharply on values up to 0.01, and it decreases minimally for higher values.",
                "The minimum value, however, is slightly over 10%, which is much smaller than the minimum value for the sequence of queries.",
                "This implies that with such a policy it is possible to achieve over 80% hit rate, if we consider caching dynamically posting lists for terms as opposed to caching answers for queries.",
                "This result does not consider the space required for each unit stored in the cache memory, or the amount of time it takes to put together a response to a user query.",
                "We analyze these issues more carefully later in this paper.",
                "It is interesting also to observe the histogram of Figure 7, which is an intermediate step in the computation of the miss rate graph.",
                "It reports the distribution of distances between repetitions of the same frequent query.",
                "The distance in the plot is measured in the number of distinct queries separating a query and its repetition, and it considers only queries appearing at least 10 times.",
                "From Figures 6 and 7, we conclude that even if we set the size of the query answers cache to a relatively large number of entries, the miss rate is high.",
                "Thus, caching the posting lists of terms has the potential to improve the hit ratio.",
                "This is what we explore next. 5.",
                "CACHING POSTING LISTS The previous section shows that caching posting lists can obtain a higher hit rate compared to caching query answers.",
                "In this section we study the problem of how to select posting lists to place on a certain amount of available memory, assuming that the whole index is larger than the amount of memory available.",
                "The posting lists have variable size (in fact, their size distribution follows a power law), so it is beneficial for a caching policy to consider the sizes of the posting lists.",
                "We consider both dynamic and <br>static caching</br>.",
                "For dynamic caching, we use two well-known policies, LRU and LFU, as well as a modified algorithm that takes posting-list size into account.",
                "Before discussing the <br>static caching</br> strategies, we introduce some notation.",
                "We use fq(t) to denote the query-term frequency of a term t, that is, the number of queries containing t in the query log, and fd(t) to denote the document frequency of t, that is, the number of documents in the collection in which the term t appears.",
                "The first strategy we consider is the algorithm proposed by Baeza-Yates and Saint-Jean [2], which consists in selecting the posting lists of the terms with the highest query-term frequencies fq(t).",
                "We call this algorithm Qtf.",
                "We observe that there is a trade-off between fq(t) and fd(t).",
                "Terms with high fq(t) are useful to keep in the cache because they are queried often.",
                "On the other hand, terms with high fd(t) are not good candidates because they correspond to long posting lists and consume a substantial amount of space.",
                "In fact, the problem of selecting the best posting lists for the static cache corresponds to the standard Knapsack problem: given a knapsack of fixed capacity, and a set of n items, such as the i-th item has value ci and size si, select the set of items that fit in the knapsack and maximize the overall value.",
                "In our case, value corresponds to fq(t) and size corresponds to fd(t).",
                "Thus, we employ a simple algorithm for the knapsack problem, which is selecting the posting lists of the terms with the highest values of the ratio fq(t) fd(t) .",
                "We call this algorithm QtfDf.",
                "We tried other variations considering query frequencies instead of term frequencies, but the gain was minimal compared to the complexity added.",
                "In addition to the above two static algorithms we consider the following algorithms for dynamic caching: • LRU: A standard LRU algorithm, but many posting lists might need to be evicted (in order of least-recent usage) until there is enough space in the memory to place the currently accessed posting list; • LFU: A standard LFU algorithm (eviction of the leastfrequently used), with the same modification as the LRU; • Dyn-QtfDf: A dynamic version of the QtfDf algorithm; evict from the cache the term(s) with the lowest fq(t) fd(t) ratio.",
                "The performance of all the above algorithms for 15 weeks of the query log and the UK dataset are shown in Figure 8.",
                "Performance is measured with hit rate.",
                "The cache size is measured as a fraction of the total space required to store the posting lists of all terms.",
                "For the dynamic algorithms, we load the cache with terms in order of fq(t) and we let the cache warm up for 1 million queries.",
                "For the static algorithms, we assume complete knowledge of the frequencies fq(t), that is, we estimate fq(t) from the whole query stream.",
                "As we show in Section 7 the results do not change much if we compute the query-term frequencies using the first 3 or 4 weeks of the query log and measure the hit rate on the rest. 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1 0.1 0.2 0.3 0.4 0.5 0.6 0.7 Hitrate Cache size Caching posting lists static QTF/DF LRU LFU Dyn-QTF/DF QTF Figure 8: Hit rate of different strategies for caching posting lists.",
                "The most important observation from our experiments is that the static QtfDf algorithm has a better hit rate than all the dynamic algorithms.",
                "An important benefit a static cache is that it requires no eviction and it is hence more efficient when evaluating queries.",
                "However, if the characteristics of the query traffic change frequently over time, then it requires re-populating the cache often or there will be a significant impact on hit rate. 6.",
                "ANALYSIS OF <br>static caching</br> In this section we provide a detailed analysis for the problem of deciding whether it is preferable to cache query answers or cache posting lists.",
                "Our analysis takes into account the impact of caching between two levels of the data-access hierarchy.",
                "It can either be applied at the memory/disk layer or at a server/remote server layer as in the architecture we discussed in the introduction.",
                "Using a particular system model, we obtain estimates for the parameters required by our analysis, which we subsequently use to decide the optimal trade-off between caching query answers and caching posting lists. 6.1 Analytical Model Let M be the size of the cache measured in answer units (the cache can store M query answers).",
                "Assume that all posting lists are of the same length L, measured in answer units.",
                "We consider the following two cases: (A) a cache that stores only precomputed answers, and (B) a cache that stores only posting lists.",
                "In the first case, Nc = M answers fit in the cache, while in the second case Np = M/L posting lists fit in the cache.",
                "Thus, Np = Nc/L.",
                "Note that although posting lists require more space, we can combine terms to evaluate more queries (or partial queries).",
                "For case (A), suppose that a query answer in the cache can be evaluated in 1 time unit.",
                "For case (B), assume that if the posting lists of the terms of a query are in the cache then the results can be computed in TR1 time units, while if the posting lists are not in the cache then the results can be computed in TR2 time units.",
                "Of course TR2 > TR1.",
                "Now we want to compare the time to answer a stream of Q queries in both cases.",
                "Let Vc(Nc) be the volume of the most frequent Nc queries.",
                "Then, for case (A), we have an overall time TCA = Vc(Nc) + TR2(Q − Vc(Nc)).",
                "Similarly, for case (B), let Vp(Np) be the number of computable queries.",
                "Then we have overall time TP L = TR1Vp(Np) + TR2(Q − Vp(Np)).",
                "We want to check under which conditions we have TP L < TCA.",
                "We have TP L − TCA = (TR2 − 1)Vc(Nc) − (TR2 − TR1)Vp(Np) > 0.",
                "Figure 9 shows the values of Vp and Vc for our data.",
                "We can see that caching answers saturates faster and for this particular data there is no additional benefit from using more than 10% of the index space for caching answers.",
                "As the query distribution is a power law with parameter α > 1, the i-th most frequent query appears with probability proportional to 1 iα .",
                "Therefore, the volume Vc(n), which is the total number of the n most frequent queries, is Vc(n) = V0 n i=1 Q iα = γnQ (0 < γn < 1).",
                "We know that Vp(n) grows faster than Vc(n) and assume, based on experimental results, that the relation is of the form Vp(n) = k Vc(n)β .",
                "In the worst case, for a large cache, β → 1.",
                "That is, both techniques will cache a constant fraction of the overall query volume.",
                "Then caching posting lists makes sense only if L(TR2 − 1) k(TR2 − TR1) > 1.",
                "If we use compression, we have L < L and TR1 > TR1.",
                "According to the experiments that we show later, compression is always better.",
                "For a small cache, we are interested in the transient behavior and then β > 1, as computed from our data.",
                "In this case there will always be a point where TP L > TCA for a large number of queries.",
                "In reality, instead of filling the cache only with answers or only with posting lists, a better strategy will be to divide the total cache space into cache for answers and cache for posting lists.",
                "In such a case, there will be some queries that could be answered by both parts of the cache.",
                "As the answer cache is faster, it will be the first choice for answering those queries.",
                "Let QNc and QNp be the set of queries that can be answered by the cached answers and the cached posting lists, respectively.",
                "Then, the overall time is T = Vc(Nc)+TR1V (QNp −QNc )+TR2(Q−V (QNp ∪QNc )), where Np = (M − Nc)/L.",
                "Finding the optimal division of the cache in order to minimize the overall retrieval time is a difficult problem to solve analytically.",
                "In Section 6.3 we use simulations to derive optimal cache trade-offs for particular implementation examples. 6.2 Parameter Estimation We now use a particular implementation of a centralized system and the model of a distributed system as examples from which we estimate the parameters of the analysis from the previous section.",
                "We perform the experiments using an optimized version of Terrier [11] for both indexing documents and processing queries, on a single machine with a Pentium 4 at 2GHz and 1GB of RAM.",
                "We indexed the documents from the UK-2006 dataset, without removing stop words or applying stemming.",
                "The posting lists in the inverted file consist of pairs of document identifier and term frequency.",
                "We compress the document identifier gaps using Elias gamma encoding, and the 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1 0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1 Queryvolume Space precomputed answers posting lists Figure 9: Cache saturation as a function of size.",
                "Table 2: Ratios between the average time to evaluate a query and the average time to return cached answers (centralized and distributed case).",
                "Centralized system TR1 TR2 TR1 TR2 Full evaluation 233 1760 707 1140 Partial evaluation 99 1626 493 798 LAN system TRL 1 TRL 2 TR L 1 TR L 2 Full evaluation 242 1769 716 1149 Partial evaluation 108 1635 502 807 WAN system TRW 1 TRW 2 TR W 1 TR W 2 Full evaluation 5001 6528 5475 5908 Partial evaluation 4867 6394 5270 5575 term frequencies in documents using unary encoding [16].",
                "The size of the inverted file is 1,189Mb.",
                "A stored answer requires 1264 bytes, and an uncompressed posting takes 8 bytes.",
                "From Table 1, we obtain L = (8·# of postings) 1264·# of terms = 0.75 and L = Inverted file size 1264·# of terms = 0.26.",
                "We estimate the ratio TR = T/Tc between the average time T it takes to evaluate a query and the average time Tc it takes to return a stored answer for the same query, in the following way.",
                "Tc is measured by loading the answers for 100,000 queries in memory, and answering the queries from memory.",
                "The average time is Tc = 0.069ms.",
                "T is measured by processing the same 100,000 queries (the first 10,000 queries are used to warm-up the system).",
                "For each query, we remove stop words, if there are at least three remaining terms.",
                "The stop words correspond to the terms with a frequency higher than the number of documents in the index.",
                "We use a document-at-a-time approach to retrieve documents containing all query terms.",
                "The only disk access required during query processing is for reading compressed posting lists from the inverted file.",
                "We perform both full and partial evaluation of answers, because some queries are likely to retrieve a large number of documents, and only a fraction of the retrieved documents will be seen by users.",
                "In the partial evaluation of queries, we terminate the processing after matching 10,000 documents.",
                "The estimated ratios TR are presented in Table 2.",
                "Figure 10 shows for a sample of queries the workload of the system with partial query evaluation and compressed posting lists.",
                "The x-axis corresponds to the total time the system spends processing a particular query, and the vertical axis corresponds to the sum t∈q fq · fd(t).",
                "Notice that the total number of postings of the query-terms does not necessarily provide an accurate estimate of the workload imposed on the system by a query (which is the case for full evaluation and uncompressed lists). 0 0.2 0.4 0.6 0.8 1 0 0.2 0.4 0.6 0.8 1 Totalpostingstoprocessquery(normalized) Total time to process query (normalized) Partial processing of compressed postings query len = 1 query len in [2,3] query len in [4,8] query len > 8 Figure 10: Workload for partial query evaluation with compressed posting lists.",
                "The analysis of the previous section also applies to a distributed retrieval system in one or multiple sites.",
                "Suppose that a document partitioned distributed system is running on a cluster of machines interconnected with a local area network (LAN) in one site.",
                "The broker receives queries and broadcasts them to the query processors, which answer the queries and return the results to the broker.",
                "Finally, the broker merges the received answers and generates the final set of answers (we assume that the time spent on merging results is negligible).",
                "The difference between the centralized architecture and the document partition architecture is the extra communication between the broker and the query processors.",
                "Using ICMP pings on a 100Mbps LAN, we have measured that sending the query from the broker to the query processors which send an answer of 4,000 bytes back to the broker takes on average 0.615ms.",
                "Hence, TRL = TR + 0.615ms/0.069ms = TR + 9.",
                "In the case when the broker and the query processors are in different sites connected with a wide area network (WAN), we estimated that broadcasting the query from the broker to the query processors and getting back an answer of 4,000 bytes takes on average 329ms.",
                "Hence, TRW = TR + 329ms/0.069ms = TR + 4768. 6.3 Simulation Results We now address the problem of finding the optimal tradeoff between caching query answers and caching posting lists.",
                "To make the problem concrete we assume a fixed budget M on the available memory, out of which x units are used for caching query answers and M − x for caching posting lists.",
                "We perform simulations and compute the average response time as a function of x.",
                "Using a part of the query log as training data, we first allocate in the cache the answers to the most frequent queries that fit in space x, and then we use the rest of the memory to cache posting lists.",
                "For selecting posting lists we use the QtfDf algorithm, applied to the training query log but excluding the queries that have already been cached.",
                "In Figure 11, we plot the simulated response time for a centralized system as a function of x.",
                "For the uncompressed index we use M = 1GB, and for the compressed index we use M = 0.5GB.",
                "In the case of the configuration that uses partial query evaluation with compressed posting lists, the lowest response time is achieved when 0.15GB out of the 0.5GB is allocated for storing answers for queries.",
                "We obtained similar trends in the results for the LAN setting.",
                "Figure 12 shows the simulated workload for a distributed system across a WAN.",
                "In this case, the total amount of memory is split between the broker, which holds the cached 400 500 600 700 800 900 1000 1100 1200 0 0.2 0.4 0.6 0.8 1 Averageresponsetime Space (GB) Simulated workload -- single machine full / uncompr / 1 G partial / uncompr / 1 G full / compr / 0.5 G partial / compr / 0.5 G Figure 11: Optimal division of the cache in a server. 3000 3500 4000 4500 5000 5500 6000 0 0.2 0.4 0.6 0.8 1 Averageresponsetime Space (GB) Simulated workload -- WAN full / uncompr / 1 G partial / uncompr / 1 G full / compr / 0.5 G partial / compr / 0.5 G Figure 12: Optimal division of the cache when the next level requires WAN access. answers of queries, and the query processors, which hold the cache of posting lists.",
                "According to the figure, the difference between the configurations of the query processors is less important because the network communication overhead increases the response time substantially.",
                "When using uncompressed posting lists, the optimal allocation of memory corresponds to using approximately 70% of the memory for caching query answers.",
                "This is explained by the fact that there is no need for network communication when the query can be answered by the cache at the broker. 7.",
                "EFFECT OF THE QUERY DYNAMICS For our query log, the query distribution and query-term distribution change slowly over time.",
                "To support this claim, we first assess how topics change comparing the distribution of queries from the first week in June, 2006, to the distribution of queries for the remainder of 2006 that did not appear in the first week in June.",
                "We found that a very small percentage of queries are new queries.",
                "The majority of queries that appear in a given week repeat in the following weeks for the next six months.",
                "We then compute the hit rate of a static cache of 128, 000 answers trained over a period of two weeks (Figure 13).",
                "We report hit rate hourly for 7 days, starting from 5pm.",
                "We observe that the hit rate reaches its highest value during the night (around midnight), whereas around 2-3pm it reaches its minimum.",
                "After a small decay in hit rate values, the hit rate stabilizes between 0.28, and 0.34 for the entire week, suggesting that the static cache is effective for a whole week after the training period. 0.26 0.27 0.28 0.29 0.3 0.31 0.32 0.33 0.34 0.35 0.36 0.37 0 20 40 60 80 100 120 140 160 Hit-rate Time Hits on the frequent queries of distances Figure 13: Hourly hit rate for a static cache holding 128,000 answers during the period of a week.",
                "The static cache of posting lists can be periodically recomputed.",
                "To estimate the time interval in which we need to recompute the posting lists on the static cache we need to consider an efficiency/quality trade-off: using too short a time interval might be prohibitively expensive, while recomputing the cache too infrequently might lead to having an obsolete cache not corresponding to the statistical characteristics of the current query stream.",
                "We measured the effect on the QtfDf algorithm of the changes in a 15-week query stream (Figure 14).",
                "We compute the query term frequencies over the whole stream, select which terms to cache, and then compute the hit rate on the whole query stream.",
                "This hit rate is as an upper bound, and it assumes perfect knowledge of the query term frequencies.",
                "To simulate a realistic scenario, we use the first 6 (3) weeks of the query stream for computing query term frequencies and the following 9 (12) weeks to estimate the hit rate.",
                "As Figure 14 shows, the hit rate decreases by less than 2%.",
                "The high correlation among the query term frequencies during different time periods explains the graceful adaptation of the <br>static caching</br> algorithms to the future query stream.",
                "Indeed, the pairwise correlation among all possible 3-week periods of the 15-week query stream is over 99.5%. 8.",
                "CONCLUSIONS Caching is an effective technique in search engines for improving response time, reducing the load on query processors, and improving network bandwidth utilization.",
                "We present results on both dynamic and <br>static caching</br>.",
                "Dynamic caching of queries has limited effectiveness due to the high number of compulsory misses caused by the number of unique or infrequent queries.",
                "Our results show that in our UK log, the minimum miss rate is 50% using a working set strategy.",
                "Caching terms is more effective with respect to miss rate, achieving values as low as 12%.",
                "We also propose a new algorithm for <br>static caching</br> of posting lists that outperforms previous <br>static caching</br> algorithms as well as dynamic algorithms such as LRU and LFU, obtaining hit rate values that are over 10% higher compared these strategies.",
                "We present a framework for the analysis of the trade-off between caching query results and caching posting lists, and we simulate different types of architectures.",
                "Our results show that for centralized and LAN environments, there is an optimal allocation of caching query results and caching of posting lists, while for WAN scenarios in which network time prevails it is more important to cache query results. 0.45 0.5 0.55 0.6 0.65 0.7 0.75 0.8 0.85 0.9 0.95 0.1 0.2 0.3 0.4 0.5 0.6 0.7 Hitrate Cache size Dynamics of static QTF/DF caching policy perfect knowledge 6-week training 3-week training Figure 14: Impact of distribution changes on the <br>static caching</br> of posting lists. 9.",
                "REFERENCES [1] V. N. Anh and A. Moffat.",
                "Pruned query evaluation using pre-computed impacts.",
                "In ACM CIKM, 2006. [2] R. A. Baeza-Yates and F. Saint-Jean.",
                "A three level search engine index based in query log distribution.",
                "In SPIRE, 2003. [3] C. Buckley and A. F. Lewit.",
                "Optimization of inverted vector searches.",
                "In ACM SIGIR, 1985. [4] S. B¨uttcher and C. L. A. Clarke.",
                "A document-centric approach to static index pruning in text retrieval systems.",
                "In ACM CIKM, 2006. [5] P. Cao and S. Irani.",
                "Cost-aware WWW proxy caching algorithms.",
                "In USITS, 1997. [6] P. Denning.",
                "Working sets past and present.",
                "IEEE Trans. on Software Engineering, SE-6(1):64-84, 1980. [7] T. Fagni, R. Perego, F. Silvestri, and S. Orlando.",
                "Boosting the performance of web search engines: Caching and prefetching query results by exploiting historical usage data.",
                "ACM Trans.",
                "Inf.",
                "Syst., 24(1):51-78, 2006. [8] R. Lempel and S. Moran.",
                "Predictive caching and prefetching of query results in search engines.",
                "In WWW, 2003. [9] X.",
                "Long and T. Suel.",
                "Three-level caching for efficient query processing in large web search engines.",
                "In WWW, 2005. [10] E. P. Markatos.",
                "On caching search engine query results.",
                "Computer Communications, 24(2):137-143, 2001. [11] I. Ounis, G. Amati, V. Plachouras, B.",
                "He, C. Macdonald, and C. Lioma.",
                "Terrier: A High Performance and Scalable Information Retrieval Platform.",
                "In SIGIR Workshop on Open Source Information Retrieval, 2006. [12] V. V. Raghavan and H. Sever.",
                "On the reuse of past optimal queries.",
                "In ACM SIGIR, 1995. [13] P. C. Saraiva, E. S. de Moura, N. Ziviani, W. Meira, R. Fonseca, and B. Riberio-Neto.",
                "Rank-preserving two-level caching for scalable search engines.",
                "In ACM SIGIR, 2001. [14] D. R. Slutz and I. L. Traiger.",
                "A note on the calculation of average working set size.",
                "Communications of the ACM, 17(10):563-565, 1974. [15] T. Strohman, H. Turtle, and W. B. Croft.",
                "Optimization strategies for complex queries.",
                "In ACM SIGIR, 2005. [16] I. H. Witten, T. C. Bell, and A. Moffat.",
                "Managing Gigabytes: Compressing and Indexing Documents and Images.",
                "John Wiley & Sons, Inc., NY, 1994. [17] N. E. Young.",
                "On-line file caching.",
                "Algorithmica, 33(3):371-383, 2002."
            ],
            "original_annotated_samples": [],
            "translated_annotated_samples": [
                "Proponemos un nuevo algoritmo para el \"almacenamiento en caché estático\" de las listas de publicación, lo que supera a los métodos anteriores.",
                "Finalmente, medimos cómo los cambios en el registro de consultas afectan la efectividad del \"almacenamiento en caché estático\", dada nuestra observación de que la distribución de las consultas cambia lentamente con el tiempo.",
                "El \"almacenamiento en caché estático\" de las listas de publicación plantea aún más desafíos: al decidir qué términos almacenados en caché se enfrenta la compensación entre los términos y términos consultados frecuentemente con pequeñas listas de publicación que son eficientes en el espacio.",
                "Finalmente, antes de decidir adoptar una política de \"almacenamiento en caché estático\", el flujo de consulta debe analizarse para verificar que sus características no cambien rápidamente con el tiempo.",
                "Broker \"Estatico en caché\" listas de publicaciones dinámicas/estáticas Respuestas en caché del procesador de consultas locales DISCO NEVITO DE ALTACIÓN NIVEL ACCESO LOCAL ACCESO RETO RED ACCESO Figura 1: Un nivel de almacenamiento en caché en una arquitectura de búsqueda distribuida.",
                "Proporcionamos un marco para el análisis de la compensación entre el \"almacenamiento en caché estático\" de las respuestas de consulta y las listas de publicación;• El \"almacenamiento en caché estático\" de los términos puede ser más efectivo que el almacenamiento en caché dinámico con, por ejemplo, LRU.",
                "Proporcionamos algoritmos basados en el problema de la mochila para seleccionar las listas de publicación para poner en un caché estático, y mostramos mejoras sobre el trabajo anterior, logrando una relación de éxito durante el 90%;• Los cambios de la distribución de la consulta a lo largo del tiempo tienen poco impacto en el \"almacenamiento en caché estático\".",
                "Las secciones 5 y 6 introducen algoritmos para el almacenamiento en caché de listas de publicación y un marco teórico para el análisis del \"almacenamiento en caché estático\", respectivamente.",
                "La Sección 7 analiza el impacto de los cambios en la distribución de la consulta en el \"almacenamiento en caché estático\", y la Sección 8 proporciona comentarios finales.2.",
                "Finalmente, nuestro algoritmo de \"almacenamiento en caché estático\" para publicar listas en la Sección 5 usa la frecuencia/tamaño de relación para evaluar la bondad de un elemento para caché.",
                "Hasta donde sabemos, somos los primeros en utilizar este enfoque para el \"almacenamiento en caché estático\" de las listas de publicación.3.",
                "Consideramos tanto dinámico como \"almacenamiento en caché estático\".",
                "Antes de discutir las estrategias de \"almacenamiento en caché estático\", presentamos alguna notación.",
                "Análisis del \"almacenamiento en caché estático\" En esta sección, proporcionamos un análisis detallado para el problema de decidir si es preferible a las respuestas de consulta de caché o listas de publicación de caché.",
                "La alta correlación entre las frecuencias del término de consulta durante diferentes períodos de tiempo explica la elegante adaptación de los algoritmos de \"almacenamiento en caché estático\" a la corriente de consulta futura.",
                "Presentamos resultados tanto en el \"almacenamiento en caché estático\".",
                "También proponemos un nuevo algoritmo para el \"almacenamiento en caché estático\" de las listas de publicación que superan a los algoritmos anteriores de \"almacenamiento en caché estático\", así como algoritmos dinámicos como LRU y LFU, obteniendo valores de tasa de aciertos que son más de 10% más altos comparados con estas estrategias.",
                "Nuestros resultados muestran que para los entornos centralizados y LAN, existe una asignación óptima de los resultados de la consulta de almacenamiento en caché y el almacenamiento en caché de las listas de publicación, mientras que para los escenarios WAN en los que prevalece el tiempo de la red, es más importante para cachear los resultados de la consulta.0.45 0.5 0.5 0.55 0.6 0.65 0.7 0.75 0.8 0.85 0.9 0.95 0.1 0.2 0.3 0.4 0.4 0.5 0.6 0.7 Dinámica de tamaño de caché HITRATE Dinámica de la Política de almacenamiento de caché estática/DF Conocimiento perfecto de 6 semanas Capacitación de 3 semanas Figura 14: Impacto de la distribución Cambios de distribución en el \"Estaticocaché \"de las listas de publicación.9."
            ],
            "translated_text": "",
            "candidates": [
                "almacenamiento en caché estático",
                "almacenamiento en caché estático",
                "almacenamiento en caché estático",
                "almacenamiento en caché estático",
                "almacenamiento en caché estático",
                "almacenamiento en caché estático",
                "almacenamiento en caché estático",
                "almacenamiento en caché estático",
                "almacenamiento en caché estático",
                "Estatico en caché",
                "almacenamiento en caché estático",
                "almacenamiento en caché estático",
                "almacenamiento en caché estático",
                "almacenamiento en caché estático",
                "almacenamiento en caché estático",
                "almacenamiento en caché estático",
                "almacenamiento en caché estático",
                "almacenamiento en caché estático",
                "almacenamiento en caché estático",
                "almacenamiento en caché estático",
                "almacenamiento en caché estático",
                "Golpe de almacenamiento en caché estático",
                "almacenamiento en caché estático",
                "almacenamiento en caché estático",
                "almacenamiento en caché estático",
                "almacenamiento en caché estático",
                "almacenamiento en caché estático",
                "almacenamiento en caché estático",
                "almacenamiento en caché estático",
                "almacenamiento en caché estático",
                "almacenamiento en caché estático",
                "almacenamiento en caché estático",
                "almacenamiento en caché estático",
                "almacenamiento en caché estático",
                "almacenamiento en caché estático",
                "almacenamiento en caché estático",
                "Golpeado estático",
                "Estaticocaché "
            ],
            "error": []
        },
        "dynamic caching": {
            "translated_key": "almacenamiento en caché dinámico",
            "is_in_text": true,
            "original_annotated_sentences": [
                "The Impact of Caching on Search Engines Ricardo Baeza-Yates1 rbaeza@acm.org Aristides Gionis1 gionis@yahoo-inc.com Flavio Junqueira1 fpj@yahoo-inc.com Vanessa Murdock1 vmurdock@yahoo-inc.com Vassilis Plachouras1 vassilis@yahoo-inc.com Fabrizio Silvestri2 f.silvestri@isti.cnr.it 1 Yahoo!",
                "Research Barcelona 2 ISTI - CNR Barcelona, SPAIN Pisa, ITALY ABSTRACT In this paper we study the trade-offs in designing efficient caching systems for Web search engines.",
                "We explore the impact of different approaches, such as static vs. <br>dynamic caching</br>, and caching query results vs. caching posting lists.",
                "Using a query log spanning a whole year we explore the limitations of caching and we demonstrate that caching posting lists can achieve higher hit rates than caching query answers.",
                "We propose a new algorithm for static caching of posting lists, which outperforms previous methods.",
                "We also study the problem of finding the optimal way to split the static cache between answers and posting lists.",
                "Finally, we measure how the changes in the query log affect the effectiveness of static caching, given our observation that the distribution of the queries changes slowly over time.",
                "Our results and observations are applicable to different levels of the data-access hierarchy, for instance, for a memory/disk layer or a broker/remote server layer.",
                "Categories and Subject Descriptors H.3.3 [Information Storage and Retrieval]: Information Search and Retrieval - Search process; H.3.4 [Information Storage and Retrieval]: Systems and Software - Distributed systems, Performance evaluation (efficiency and effectiveness) General Terms Algorithms, Experimentation 1.",
                "INTRODUCTION Millions of queries are submitted daily to Web search engines, and users have high expectations of the quality and speed of the answers.",
                "As the searchable Web becomes larger and larger, with more than 20 billion pages to index, evaluating a single query requires processing large amounts of data.",
                "In such a setting, to achieve a fast response time and to increase the query throughput, using a cache is crucial.",
                "The primary use of a cache memory is to speedup computation by exploiting frequently or recently used data, although reducing the workload to back-end servers is also a major goal.",
                "Caching can be applied at different levels with increasing response latencies or processing requirements.",
                "For example, the different levels may correspond to the main memory, the disk, or resources in a local or a wide area network.",
                "The decision of what to cache is either off-line (static) or online (dynamic).",
                "A static cache is based on historical information and is periodically updated.",
                "A dynamic cache replaces entries according to the sequence of requests.",
                "When a new request arrives, the cache system decides whether to evict some entry from the cache in the case of a cache miss.",
                "Such online decisions are based on a cache policy, and several different policies have been studied in the past.",
                "For a search engine, there are two possible ways to use a cache memory: Caching answers: As the engine returns answers to a particular query, it may decide to store these answers to resolve future queries.",
                "Caching terms: As the engine evaluates a particular query, it may decide to store in memory the posting lists of the involved query terms.",
                "Often the whole set of posting lists does not fit in memory, and consequently, the engine has to select a small set to keep in memory and speed up query processing.",
                "Returning an answer to a query that already exists in the cache is more efficient than computing the answer using cached posting lists.",
                "On the other hand, previously unseen queries occur more often than previously unseen terms, implying a higher miss rate for cached answers.",
                "Caching of posting lists has additional challenges.",
                "As posting lists have variable size, caching them dynamically is not very efficient, due to the complexity in terms of efficiency and space, and the skewed distribution of the query stream, as shown later.",
                "Static caching of posting lists poses even more challenges: when deciding which terms to cache one faces the trade-off between frequently queried terms and terms with small posting lists that are space efficient.",
                "Finally, before deciding to adopt a static caching policy the query stream should be analyzed to verify that its characteristics do not change rapidly over time.",
                "Broker Static caching posting lists Dynamic/Static cached answers Local query processor Disk Next caching level Local network access Remote network access Figure 1: One caching level in a distributed search architecture.",
                "In this paper we explore the trade-offs in the design of each cache level, showing that the problem is the same and only a few parameters change.",
                "In general, we assume that each level of caching in a distributed search architecture is similar to that shown in Figure 1.",
                "We use a query log spanning a whole year to explore the limitations of dynamically caching query answers or posting lists for query terms.",
                "More concretely, our main conclusions are that: • Caching query answers results in lower hit ratios compared to caching of posting lists for query terms, but it is faster because there is no need for query evaluation.",
                "We provide a framework for the analysis of the trade-off between static caching of query answers and posting lists; • Static caching of terms can be more effective than <br>dynamic caching</br> with, for example, LRU.",
                "We provide algorithms based on the Knapsack problem for selecting the posting lists to put in a static cache, and we show improvements over previous work, achieving a hit ratio over 90%; • Changes of the query distribution over time have little impact on static caching.",
                "The remainder of this paper is organized as follows.",
                "Sections 2 and 3 summarize related work and characterize the data sets we use.",
                "Section 4 discusses the limitations of <br>dynamic caching</br>.",
                "Sections 5 and 6 introduce algorithms for caching posting lists, and a theoretical framework for the analysis of static caching, respectively.",
                "Section 7 discusses the impact of changes in the query distribution on static caching, and Section 8 provides concluding remarks. 2.",
                "RELATED WORK There is a large body of work devoted to query optimization.",
                "Buckley and Lewit [3], in one of the earliest works, take a term-at-a-time approach to deciding when inverted lists need not be further examined.",
                "More recent examples demonstrate that the top k documents for a query can be returned without the need for evaluating the complete set of posting lists [1, 4, 15].",
                "Although these approaches seek to improve query processing efficiency, they differ from our current work in that they do not consider caching.",
                "They may be considered separate and complementary to a cache-based approach.",
                "Raghavan and Sever [12], in one of the first papers on exploiting user query history, propose using a query base, built upon a set of persistent optimal queries submitted in the past, to improve the retrieval effectiveness for similar future queries.",
                "Markatos [10] shows the existence of temporal locality in queries, and compares the performance of different caching policies.",
                "Based on the observations of Markatos, Lempel and Moran propose a new caching policy, called Probabilistic Driven Caching, by attempting to estimate the probability distribution of all possible queries submitted to a search engine [8].",
                "Fagni et al. follow Markatos work by showing that combining static and <br>dynamic caching</br> policies together with an adaptive prefetching policy achieves a high hit ratio [7].",
                "Different from our work, they consider caching and prefetching of pages of results.",
                "As systems are often hierarchical, there has also been some effort on multi-level architectures.",
                "Saraiva et al. propose a new architecture for Web search engines using a two-level <br>dynamic caching</br> system [13].",
                "Their goal for such systems has been to improve response time for hierarchical engines.",
                "In their architecture, both levels use an LRU eviction policy.",
                "They find that the second-level cache can effectively reduce disk traffic, thus increasing the overall throughput.",
                "Baeza-Yates and Saint-Jean propose a three-level index organization [2].",
                "Long and Suel propose a caching system structured according to three different levels [9].",
                "The intermediate level contains frequently occurring pairs of terms and stores the intersections of the corresponding inverted lists.",
                "These last two papers are related to ours in that they exploit different caching strategies at different levels of the memory hierarchy.",
                "Finally, our static caching algorithm for posting lists in Section 5 uses the ratio frequency/size in order to evaluate the goodness of an item to cache.",
                "Similar ideas have been used in the context of file caching [17], Web caching [5], and even caching of posting lists [9], but in all cases in a dynamic setting.",
                "To the best of our knowledge we are the first to use this approach for static caching of posting lists. 3.",
                "DATA CHARACTERIZATION Our data consists of a crawl of documents from the UK domain, and query logs of one year of queries submitted to http://www.yahoo.co.uk from November 2005 to November 2006.",
                "In our logs, 50% of the total volume of queries are unique.",
                "The average query length is 2.5 terms, with the longest query having 731 terms. 1e-07 1e-06 1e-05 1e-04 0.001 0.01 0.1 1 1e-08 1e-07 1e-06 1e-05 1e-04 0.001 0.01 0.1 1 Frequency(normalized) Frequency rank (normalized) Figure 2: The distribution of queries (bottom curve) and query terms (middle curve) in the query log.",
                "The distribution of document frequencies of terms in the UK-2006 dataset (upper curve).",
                "Figure 2 shows the distributions of queries (lower curve), and query terms (middle curve).",
                "The x-axis represents the normalized frequency rank of the query or term. (The most frequent query appears closest to the y-axis.)",
                "The y-axis is Table 1: Statistics of the UK-2006 sample.",
                "UK-2006 sample statistics # of documents 2,786,391 # of terms 6,491,374 # of tokens 2,109,512,558 the normalized frequency for a given query (or term).",
                "As expected, the distribution of query frequencies and query term frequencies follow power law distributions, with slope of 1.84 and 2.26, respectively.",
                "In this figure, the query frequencies were computed as they appear in the logs with no normalization for case or white space.",
                "The query terms (middle curve) have been normalized for case, as have the terms in the document collection.",
                "The document collection that we use for our experiments is a summary of the UK domain crawled in May 2006.1 This summary corresponds to a maximum of 400 crawled documents per host, using a breadth first crawling strategy, comprising 15GB.",
                "The distribution of document frequencies of terms in the collection follows a power law distribution with slope 2.38 (upper curve in Figure 2).",
                "The statistics of the collection are shown in Table 1.",
                "We measured the correlation between the document frequency of terms in the collection and the number of queries that contain a particular term in the query log to be 0.424.",
                "A scatter plot for a random sample of terms is shown in Figure 3.",
                "In this experiment, terms have been converted to lower case in both the queries and the documents so that the frequencies will be comparable. 1e-07 1e-06 1e-05 1e-04 0.001 0.01 0.1 1 1e-06 1e-05 1e-04 0.001 0.01 0.1 1 Queryfrequency Document frequency Figure 3: Normalized scatter plot of document-term frequencies vs. query-term frequencies. 4.",
                "CACHING OF QUERIES AND TERMS Caching relies upon the assumption that there is locality in the stream of requests.",
                "That is, there must be sufficient repetition in the stream of requests and within intervals of time that enable a cache memory of reasonable size to be effective.",
                "In the query log we used, 88% of the unique queries are singleton queries, and 44% are singleton queries out of the whole volume.",
                "Thus, out of all queries in the stream composing the query log, the upper threshold on hit ratio is 56%.",
                "This is because only 56% of all the queries comprise queries that have multiple occurrences.",
                "It is important to observe, however, that not all queries in this 56% can be cache hits because of compulsory misses.",
                "A compulsory miss 1 The collection is available from the University of Milan: http://law.dsi.unimi.it/.",
                "URL retrieved 05/2007. 0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1 240 260 280 300 320 340 360 Numberofelements Bin number Total terms Terms diff Total queries Unique queries Unique terms Query diff Figure 4: Arrival rate for both terms and queries. happens when the cache receives a query for the first time.",
                "This is different from capacity misses, which happen due to space constraints on the amount of memory the cache uses.",
                "If we consider a cache with infinite memory, then the hit ratio is 50%.",
                "Note that for an infinite cache there are no capacity misses.",
                "As we mentioned before, another possibility is to cache the posting lists of terms.",
                "Intuitively, this gives more freedom in the utilization of the cache content to respond to queries because cached terms might form a new query.",
                "On the other hand, they need more space.",
                "As opposed to queries, the fraction of singleton terms in the total volume of terms is smaller.",
                "In our query log, only 4% of the terms appear once, but this accounts for 73% of the vocabulary of query terms.",
                "We show in Section 5 that caching a small fraction of terms, while accounting for terms appearing in many documents, is potentially very effective.",
                "Figure 4 shows several graphs corresponding to the normalized arrival rate for different cases using days as bins.",
                "That is, we plot the normalized number of elements that appear in a day.",
                "This graph shows only a period of 122 days, and we normalize the values by the maximum value observed throughout the whole period of the query log.",
                "Total queries and Total terms correspond to the total volume of queries and terms, respectively.",
                "Unique queries and Unique terms correspond to the arrival rate of unique queries and terms.",
                "Finally, Query diff and Terms diff correspond to the difference between the curves for total and unique.",
                "In Figure 4, as expected, the volume of terms is much higher than the volume of queries.",
                "The difference between the total number of terms and the number of unique terms is much larger than the difference between the total number of queries and the number of unique queries.",
                "This observation implies that terms repeat significantly more than queries.",
                "If we use smaller bins, say of one hour, then the ratio of unique to volume is higher for both terms and queries because it leaves less room for repetition.",
                "We also estimated the workload using the document frequency of terms as a measure of how much work a query imposes on a search engine.",
                "We found that it follows closely the arrival rate for terms shown in Figure 4.",
                "To demonstrate the effect of a dynamic cache on the query frequency distribution of Figure 2, we plot the same frequency graph, but now considering the frequency of queries Figure 5: Frequency graph after LRU cache. after going through an LRU cache.",
                "On a cache miss, an LRU cache decides upon an entry to evict using the information on the recency of queries.",
                "In this graph, the most frequent queries are not the same queries that were most frequent before the cache.",
                "It is possible that queries that are most frequent after the cache have different characteristics, and tuning the search engine to queries frequent before the cache may degrade performance for non-cached queries.",
                "The maximum frequency after caching is less than 1% of the maximum frequency before the cache, thus showing that the cache is very effective in reducing the load of frequent queries.",
                "If we re-rank the queries according to after-cache frequency, the distribution is still a power law, but with a much smaller value for the highest frequency.",
                "When discussing the effectiveness of dynamically caching, an important metric is cache miss rate.",
                "To analyze the cache miss rate for different memory constraints, we use the working set model [6, 14].",
                "A working set, informally, is the set of references that an application or an operating system is currently working with.",
                "The model uses such sets in a strategy that tries to capture the temporal locality of references.",
                "The working set strategy then consists in keeping in memory only the elements that are referenced in the previous θ steps of the input sequence, where θ is a configurable parameter corresponding to the window size.",
                "Originally, working sets have been used for page replacement algorithms of operating systems, and considering such a strategy in the context of search engines is interesting for three reasons.",
                "First, it captures the amount of locality of queries and terms in a sequence of queries.",
                "Locality in this case refers to the frequency of queries and terms in a window of time.",
                "If many queries appear multiple times in a window, then locality is high.",
                "Second, it enables an oﬄine analysis of the expected miss rate given different memory constraints.",
                "Third, working sets capture aspects of efficient caching algorithms such as LRU.",
                "LRU assumes that references farther in the past are less likely to be referenced in the present, which is implicit in the concept of working sets [14].",
                "Figure 6 plots the miss rate for different working set sizes, and we consider working sets of both queries and terms.",
                "The working set sizes are normalized against the total number of queries in the query log.",
                "In the graph for queries, there is a sharp decay until approximately 0.01, and the rate at which the miss rate drops decreases as we increase the size of the working set over 0.01.",
                "Finally, the minimum value it reaches is 50% miss rate, not shown in the figure as we have cut the tail of the curve for presentation purposes. 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1 0 0.05 0.1 0.15 0.2 Missrate Normalized working set size Queries Terms Figure 6: Miss rate as a function of the working set size. 1 10 100 1000 10000 100000 1e+06 Frequency Distance Figure 7: Distribution of distances expressed in terms of distinct queries.",
                "Compared to the query curve, we observe that the minimum miss rate for terms is substantially smaller.",
                "The miss rate also drops sharply on values up to 0.01, and it decreases minimally for higher values.",
                "The minimum value, however, is slightly over 10%, which is much smaller than the minimum value for the sequence of queries.",
                "This implies that with such a policy it is possible to achieve over 80% hit rate, if we consider caching dynamically posting lists for terms as opposed to caching answers for queries.",
                "This result does not consider the space required for each unit stored in the cache memory, or the amount of time it takes to put together a response to a user query.",
                "We analyze these issues more carefully later in this paper.",
                "It is interesting also to observe the histogram of Figure 7, which is an intermediate step in the computation of the miss rate graph.",
                "It reports the distribution of distances between repetitions of the same frequent query.",
                "The distance in the plot is measured in the number of distinct queries separating a query and its repetition, and it considers only queries appearing at least 10 times.",
                "From Figures 6 and 7, we conclude that even if we set the size of the query answers cache to a relatively large number of entries, the miss rate is high.",
                "Thus, caching the posting lists of terms has the potential to improve the hit ratio.",
                "This is what we explore next. 5.",
                "CACHING POSTING LISTS The previous section shows that caching posting lists can obtain a higher hit rate compared to caching query answers.",
                "In this section we study the problem of how to select posting lists to place on a certain amount of available memory, assuming that the whole index is larger than the amount of memory available.",
                "The posting lists have variable size (in fact, their size distribution follows a power law), so it is beneficial for a caching policy to consider the sizes of the posting lists.",
                "We consider both dynamic and static caching.",
                "For <br>dynamic caching</br>, we use two well-known policies, LRU and LFU, as well as a modified algorithm that takes posting-list size into account.",
                "Before discussing the static caching strategies, we introduce some notation.",
                "We use fq(t) to denote the query-term frequency of a term t, that is, the number of queries containing t in the query log, and fd(t) to denote the document frequency of t, that is, the number of documents in the collection in which the term t appears.",
                "The first strategy we consider is the algorithm proposed by Baeza-Yates and Saint-Jean [2], which consists in selecting the posting lists of the terms with the highest query-term frequencies fq(t).",
                "We call this algorithm Qtf.",
                "We observe that there is a trade-off between fq(t) and fd(t).",
                "Terms with high fq(t) are useful to keep in the cache because they are queried often.",
                "On the other hand, terms with high fd(t) are not good candidates because they correspond to long posting lists and consume a substantial amount of space.",
                "In fact, the problem of selecting the best posting lists for the static cache corresponds to the standard Knapsack problem: given a knapsack of fixed capacity, and a set of n items, such as the i-th item has value ci and size si, select the set of items that fit in the knapsack and maximize the overall value.",
                "In our case, value corresponds to fq(t) and size corresponds to fd(t).",
                "Thus, we employ a simple algorithm for the knapsack problem, which is selecting the posting lists of the terms with the highest values of the ratio fq(t) fd(t) .",
                "We call this algorithm QtfDf.",
                "We tried other variations considering query frequencies instead of term frequencies, but the gain was minimal compared to the complexity added.",
                "In addition to the above two static algorithms we consider the following algorithms for <br>dynamic caching</br>: • LRU: A standard LRU algorithm, but many posting lists might need to be evicted (in order of least-recent usage) until there is enough space in the memory to place the currently accessed posting list; • LFU: A standard LFU algorithm (eviction of the leastfrequently used), with the same modification as the LRU; • Dyn-QtfDf: A dynamic version of the QtfDf algorithm; evict from the cache the term(s) with the lowest fq(t) fd(t) ratio.",
                "The performance of all the above algorithms for 15 weeks of the query log and the UK dataset are shown in Figure 8.",
                "Performance is measured with hit rate.",
                "The cache size is measured as a fraction of the total space required to store the posting lists of all terms.",
                "For the dynamic algorithms, we load the cache with terms in order of fq(t) and we let the cache warm up for 1 million queries.",
                "For the static algorithms, we assume complete knowledge of the frequencies fq(t), that is, we estimate fq(t) from the whole query stream.",
                "As we show in Section 7 the results do not change much if we compute the query-term frequencies using the first 3 or 4 weeks of the query log and measure the hit rate on the rest. 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1 0.1 0.2 0.3 0.4 0.5 0.6 0.7 Hitrate Cache size Caching posting lists static QTF/DF LRU LFU Dyn-QTF/DF QTF Figure 8: Hit rate of different strategies for caching posting lists.",
                "The most important observation from our experiments is that the static QtfDf algorithm has a better hit rate than all the dynamic algorithms.",
                "An important benefit a static cache is that it requires no eviction and it is hence more efficient when evaluating queries.",
                "However, if the characteristics of the query traffic change frequently over time, then it requires re-populating the cache often or there will be a significant impact on hit rate. 6.",
                "ANALYSIS OF STATIC CACHING In this section we provide a detailed analysis for the problem of deciding whether it is preferable to cache query answers or cache posting lists.",
                "Our analysis takes into account the impact of caching between two levels of the data-access hierarchy.",
                "It can either be applied at the memory/disk layer or at a server/remote server layer as in the architecture we discussed in the introduction.",
                "Using a particular system model, we obtain estimates for the parameters required by our analysis, which we subsequently use to decide the optimal trade-off between caching query answers and caching posting lists. 6.1 Analytical Model Let M be the size of the cache measured in answer units (the cache can store M query answers).",
                "Assume that all posting lists are of the same length L, measured in answer units.",
                "We consider the following two cases: (A) a cache that stores only precomputed answers, and (B) a cache that stores only posting lists.",
                "In the first case, Nc = M answers fit in the cache, while in the second case Np = M/L posting lists fit in the cache.",
                "Thus, Np = Nc/L.",
                "Note that although posting lists require more space, we can combine terms to evaluate more queries (or partial queries).",
                "For case (A), suppose that a query answer in the cache can be evaluated in 1 time unit.",
                "For case (B), assume that if the posting lists of the terms of a query are in the cache then the results can be computed in TR1 time units, while if the posting lists are not in the cache then the results can be computed in TR2 time units.",
                "Of course TR2 > TR1.",
                "Now we want to compare the time to answer a stream of Q queries in both cases.",
                "Let Vc(Nc) be the volume of the most frequent Nc queries.",
                "Then, for case (A), we have an overall time TCA = Vc(Nc) + TR2(Q − Vc(Nc)).",
                "Similarly, for case (B), let Vp(Np) be the number of computable queries.",
                "Then we have overall time TP L = TR1Vp(Np) + TR2(Q − Vp(Np)).",
                "We want to check under which conditions we have TP L < TCA.",
                "We have TP L − TCA = (TR2 − 1)Vc(Nc) − (TR2 − TR1)Vp(Np) > 0.",
                "Figure 9 shows the values of Vp and Vc for our data.",
                "We can see that caching answers saturates faster and for this particular data there is no additional benefit from using more than 10% of the index space for caching answers.",
                "As the query distribution is a power law with parameter α > 1, the i-th most frequent query appears with probability proportional to 1 iα .",
                "Therefore, the volume Vc(n), which is the total number of the n most frequent queries, is Vc(n) = V0 n i=1 Q iα = γnQ (0 < γn < 1).",
                "We know that Vp(n) grows faster than Vc(n) and assume, based on experimental results, that the relation is of the form Vp(n) = k Vc(n)β .",
                "In the worst case, for a large cache, β → 1.",
                "That is, both techniques will cache a constant fraction of the overall query volume.",
                "Then caching posting lists makes sense only if L(TR2 − 1) k(TR2 − TR1) > 1.",
                "If we use compression, we have L < L and TR1 > TR1.",
                "According to the experiments that we show later, compression is always better.",
                "For a small cache, we are interested in the transient behavior and then β > 1, as computed from our data.",
                "In this case there will always be a point where TP L > TCA for a large number of queries.",
                "In reality, instead of filling the cache only with answers or only with posting lists, a better strategy will be to divide the total cache space into cache for answers and cache for posting lists.",
                "In such a case, there will be some queries that could be answered by both parts of the cache.",
                "As the answer cache is faster, it will be the first choice for answering those queries.",
                "Let QNc and QNp be the set of queries that can be answered by the cached answers and the cached posting lists, respectively.",
                "Then, the overall time is T = Vc(Nc)+TR1V (QNp −QNc )+TR2(Q−V (QNp ∪QNc )), where Np = (M − Nc)/L.",
                "Finding the optimal division of the cache in order to minimize the overall retrieval time is a difficult problem to solve analytically.",
                "In Section 6.3 we use simulations to derive optimal cache trade-offs for particular implementation examples. 6.2 Parameter Estimation We now use a particular implementation of a centralized system and the model of a distributed system as examples from which we estimate the parameters of the analysis from the previous section.",
                "We perform the experiments using an optimized version of Terrier [11] for both indexing documents and processing queries, on a single machine with a Pentium 4 at 2GHz and 1GB of RAM.",
                "We indexed the documents from the UK-2006 dataset, without removing stop words or applying stemming.",
                "The posting lists in the inverted file consist of pairs of document identifier and term frequency.",
                "We compress the document identifier gaps using Elias gamma encoding, and the 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1 0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1 Queryvolume Space precomputed answers posting lists Figure 9: Cache saturation as a function of size.",
                "Table 2: Ratios between the average time to evaluate a query and the average time to return cached answers (centralized and distributed case).",
                "Centralized system TR1 TR2 TR1 TR2 Full evaluation 233 1760 707 1140 Partial evaluation 99 1626 493 798 LAN system TRL 1 TRL 2 TR L 1 TR L 2 Full evaluation 242 1769 716 1149 Partial evaluation 108 1635 502 807 WAN system TRW 1 TRW 2 TR W 1 TR W 2 Full evaluation 5001 6528 5475 5908 Partial evaluation 4867 6394 5270 5575 term frequencies in documents using unary encoding [16].",
                "The size of the inverted file is 1,189Mb.",
                "A stored answer requires 1264 bytes, and an uncompressed posting takes 8 bytes.",
                "From Table 1, we obtain L = (8·# of postings) 1264·# of terms = 0.75 and L = Inverted file size 1264·# of terms = 0.26.",
                "We estimate the ratio TR = T/Tc between the average time T it takes to evaluate a query and the average time Tc it takes to return a stored answer for the same query, in the following way.",
                "Tc is measured by loading the answers for 100,000 queries in memory, and answering the queries from memory.",
                "The average time is Tc = 0.069ms.",
                "T is measured by processing the same 100,000 queries (the first 10,000 queries are used to warm-up the system).",
                "For each query, we remove stop words, if there are at least three remaining terms.",
                "The stop words correspond to the terms with a frequency higher than the number of documents in the index.",
                "We use a document-at-a-time approach to retrieve documents containing all query terms.",
                "The only disk access required during query processing is for reading compressed posting lists from the inverted file.",
                "We perform both full and partial evaluation of answers, because some queries are likely to retrieve a large number of documents, and only a fraction of the retrieved documents will be seen by users.",
                "In the partial evaluation of queries, we terminate the processing after matching 10,000 documents.",
                "The estimated ratios TR are presented in Table 2.",
                "Figure 10 shows for a sample of queries the workload of the system with partial query evaluation and compressed posting lists.",
                "The x-axis corresponds to the total time the system spends processing a particular query, and the vertical axis corresponds to the sum t∈q fq · fd(t).",
                "Notice that the total number of postings of the query-terms does not necessarily provide an accurate estimate of the workload imposed on the system by a query (which is the case for full evaluation and uncompressed lists). 0 0.2 0.4 0.6 0.8 1 0 0.2 0.4 0.6 0.8 1 Totalpostingstoprocessquery(normalized) Total time to process query (normalized) Partial processing of compressed postings query len = 1 query len in [2,3] query len in [4,8] query len > 8 Figure 10: Workload for partial query evaluation with compressed posting lists.",
                "The analysis of the previous section also applies to a distributed retrieval system in one or multiple sites.",
                "Suppose that a document partitioned distributed system is running on a cluster of machines interconnected with a local area network (LAN) in one site.",
                "The broker receives queries and broadcasts them to the query processors, which answer the queries and return the results to the broker.",
                "Finally, the broker merges the received answers and generates the final set of answers (we assume that the time spent on merging results is negligible).",
                "The difference between the centralized architecture and the document partition architecture is the extra communication between the broker and the query processors.",
                "Using ICMP pings on a 100Mbps LAN, we have measured that sending the query from the broker to the query processors which send an answer of 4,000 bytes back to the broker takes on average 0.615ms.",
                "Hence, TRL = TR + 0.615ms/0.069ms = TR + 9.",
                "In the case when the broker and the query processors are in different sites connected with a wide area network (WAN), we estimated that broadcasting the query from the broker to the query processors and getting back an answer of 4,000 bytes takes on average 329ms.",
                "Hence, TRW = TR + 329ms/0.069ms = TR + 4768. 6.3 Simulation Results We now address the problem of finding the optimal tradeoff between caching query answers and caching posting lists.",
                "To make the problem concrete we assume a fixed budget M on the available memory, out of which x units are used for caching query answers and M − x for caching posting lists.",
                "We perform simulations and compute the average response time as a function of x.",
                "Using a part of the query log as training data, we first allocate in the cache the answers to the most frequent queries that fit in space x, and then we use the rest of the memory to cache posting lists.",
                "For selecting posting lists we use the QtfDf algorithm, applied to the training query log but excluding the queries that have already been cached.",
                "In Figure 11, we plot the simulated response time for a centralized system as a function of x.",
                "For the uncompressed index we use M = 1GB, and for the compressed index we use M = 0.5GB.",
                "In the case of the configuration that uses partial query evaluation with compressed posting lists, the lowest response time is achieved when 0.15GB out of the 0.5GB is allocated for storing answers for queries.",
                "We obtained similar trends in the results for the LAN setting.",
                "Figure 12 shows the simulated workload for a distributed system across a WAN.",
                "In this case, the total amount of memory is split between the broker, which holds the cached 400 500 600 700 800 900 1000 1100 1200 0 0.2 0.4 0.6 0.8 1 Averageresponsetime Space (GB) Simulated workload -- single machine full / uncompr / 1 G partial / uncompr / 1 G full / compr / 0.5 G partial / compr / 0.5 G Figure 11: Optimal division of the cache in a server. 3000 3500 4000 4500 5000 5500 6000 0 0.2 0.4 0.6 0.8 1 Averageresponsetime Space (GB) Simulated workload -- WAN full / uncompr / 1 G partial / uncompr / 1 G full / compr / 0.5 G partial / compr / 0.5 G Figure 12: Optimal division of the cache when the next level requires WAN access. answers of queries, and the query processors, which hold the cache of posting lists.",
                "According to the figure, the difference between the configurations of the query processors is less important because the network communication overhead increases the response time substantially.",
                "When using uncompressed posting lists, the optimal allocation of memory corresponds to using approximately 70% of the memory for caching query answers.",
                "This is explained by the fact that there is no need for network communication when the query can be answered by the cache at the broker. 7.",
                "EFFECT OF THE QUERY DYNAMICS For our query log, the query distribution and query-term distribution change slowly over time.",
                "To support this claim, we first assess how topics change comparing the distribution of queries from the first week in June, 2006, to the distribution of queries for the remainder of 2006 that did not appear in the first week in June.",
                "We found that a very small percentage of queries are new queries.",
                "The majority of queries that appear in a given week repeat in the following weeks for the next six months.",
                "We then compute the hit rate of a static cache of 128, 000 answers trained over a period of two weeks (Figure 13).",
                "We report hit rate hourly for 7 days, starting from 5pm.",
                "We observe that the hit rate reaches its highest value during the night (around midnight), whereas around 2-3pm it reaches its minimum.",
                "After a small decay in hit rate values, the hit rate stabilizes between 0.28, and 0.34 for the entire week, suggesting that the static cache is effective for a whole week after the training period. 0.26 0.27 0.28 0.29 0.3 0.31 0.32 0.33 0.34 0.35 0.36 0.37 0 20 40 60 80 100 120 140 160 Hit-rate Time Hits on the frequent queries of distances Figure 13: Hourly hit rate for a static cache holding 128,000 answers during the period of a week.",
                "The static cache of posting lists can be periodically recomputed.",
                "To estimate the time interval in which we need to recompute the posting lists on the static cache we need to consider an efficiency/quality trade-off: using too short a time interval might be prohibitively expensive, while recomputing the cache too infrequently might lead to having an obsolete cache not corresponding to the statistical characteristics of the current query stream.",
                "We measured the effect on the QtfDf algorithm of the changes in a 15-week query stream (Figure 14).",
                "We compute the query term frequencies over the whole stream, select which terms to cache, and then compute the hit rate on the whole query stream.",
                "This hit rate is as an upper bound, and it assumes perfect knowledge of the query term frequencies.",
                "To simulate a realistic scenario, we use the first 6 (3) weeks of the query stream for computing query term frequencies and the following 9 (12) weeks to estimate the hit rate.",
                "As Figure 14 shows, the hit rate decreases by less than 2%.",
                "The high correlation among the query term frequencies during different time periods explains the graceful adaptation of the static caching algorithms to the future query stream.",
                "Indeed, the pairwise correlation among all possible 3-week periods of the 15-week query stream is over 99.5%. 8.",
                "CONCLUSIONS Caching is an effective technique in search engines for improving response time, reducing the load on query processors, and improving network bandwidth utilization.",
                "We present results on both dynamic and static caching.",
                "<br>dynamic caching</br> of queries has limited effectiveness due to the high number of compulsory misses caused by the number of unique or infrequent queries.",
                "Our results show that in our UK log, the minimum miss rate is 50% using a working set strategy.",
                "Caching terms is more effective with respect to miss rate, achieving values as low as 12%.",
                "We also propose a new algorithm for static caching of posting lists that outperforms previous static caching algorithms as well as dynamic algorithms such as LRU and LFU, obtaining hit rate values that are over 10% higher compared these strategies.",
                "We present a framework for the analysis of the trade-off between caching query results and caching posting lists, and we simulate different types of architectures.",
                "Our results show that for centralized and LAN environments, there is an optimal allocation of caching query results and caching of posting lists, while for WAN scenarios in which network time prevails it is more important to cache query results. 0.45 0.5 0.55 0.6 0.65 0.7 0.75 0.8 0.85 0.9 0.95 0.1 0.2 0.3 0.4 0.5 0.6 0.7 Hitrate Cache size Dynamics of static QTF/DF caching policy perfect knowledge 6-week training 3-week training Figure 14: Impact of distribution changes on the static caching of posting lists. 9.",
                "REFERENCES [1] V. N. Anh and A. Moffat.",
                "Pruned query evaluation using pre-computed impacts.",
                "In ACM CIKM, 2006. [2] R. A. Baeza-Yates and F. Saint-Jean.",
                "A three level search engine index based in query log distribution.",
                "In SPIRE, 2003. [3] C. Buckley and A. F. Lewit.",
                "Optimization of inverted vector searches.",
                "In ACM SIGIR, 1985. [4] S. B¨uttcher and C. L. A. Clarke.",
                "A document-centric approach to static index pruning in text retrieval systems.",
                "In ACM CIKM, 2006. [5] P. Cao and S. Irani.",
                "Cost-aware WWW proxy caching algorithms.",
                "In USITS, 1997. [6] P. Denning.",
                "Working sets past and present.",
                "IEEE Trans. on Software Engineering, SE-6(1):64-84, 1980. [7] T. Fagni, R. Perego, F. Silvestri, and S. Orlando.",
                "Boosting the performance of web search engines: Caching and prefetching query results by exploiting historical usage data.",
                "ACM Trans.",
                "Inf.",
                "Syst., 24(1):51-78, 2006. [8] R. Lempel and S. Moran.",
                "Predictive caching and prefetching of query results in search engines.",
                "In WWW, 2003. [9] X.",
                "Long and T. Suel.",
                "Three-level caching for efficient query processing in large web search engines.",
                "In WWW, 2005. [10] E. P. Markatos.",
                "On caching search engine query results.",
                "Computer Communications, 24(2):137-143, 2001. [11] I. Ounis, G. Amati, V. Plachouras, B.",
                "He, C. Macdonald, and C. Lioma.",
                "Terrier: A High Performance and Scalable Information Retrieval Platform.",
                "In SIGIR Workshop on Open Source Information Retrieval, 2006. [12] V. V. Raghavan and H. Sever.",
                "On the reuse of past optimal queries.",
                "In ACM SIGIR, 1995. [13] P. C. Saraiva, E. S. de Moura, N. Ziviani, W. Meira, R. Fonseca, and B. Riberio-Neto.",
                "Rank-preserving two-level caching for scalable search engines.",
                "In ACM SIGIR, 2001. [14] D. R. Slutz and I. L. Traiger.",
                "A note on the calculation of average working set size.",
                "Communications of the ACM, 17(10):563-565, 1974. [15] T. Strohman, H. Turtle, and W. B. Croft.",
                "Optimization strategies for complex queries.",
                "In ACM SIGIR, 2005. [16] I. H. Witten, T. C. Bell, and A. Moffat.",
                "Managing Gigabytes: Compressing and Indexing Documents and Images.",
                "John Wiley & Sons, Inc., NY, 1994. [17] N. E. Young.",
                "On-line file caching.",
                "Algorithmica, 33(3):371-383, 2002."
            ],
            "original_annotated_samples": [],
            "translated_annotated_samples": [
                "Exploramos el impacto de diferentes enfoques, como estática versus \"almacenamiento en caché dinámico\", y los resultados de la consulta de almacenamiento en caché frente a las listas de publicación de almacenamiento en caché.",
                "Proporcionamos un marco para el análisis de la compensación entre el almacenamiento en caché estático de las respuestas de consulta y las listas de publicación;• El almacenamiento en caché estático de los términos puede ser más efectivo que el \"almacenamiento en caché dinámico\" con, por ejemplo, LRU.",
                "La Sección 4 discute las limitaciones del \"almacenamiento en caché dinámico\".",
                "Fagni et al.Siga el trabajo de Markatos mostrando que la combinación de políticas estáticas y de \"almacenamiento dinámico en caché\" junto con una política de captación previa adaptativa logra una alta relación HIT [7].",
                "Saraiva et al.Proponga una nueva arquitectura para los motores de búsqueda web utilizando un sistema de \"almacenamiento dinámico\" de dos niveles [13].",
                "Para el \"almacenamiento en caché dinámico\", usamos dos políticas bien conocidas, LRU y LFU, así como un algoritmo modificado que tiene en cuenta el tamaño de la lista de publicación.",
                "Además de los dos algoritmos estáticos anteriores, consideramos los siguientes algoritmos para el \"almacenamiento en caché dinámico\": • LRU: un algoritmo LRU estándar, pero es posible que deba desalojarse muchas listas de publicación (en orden de uso menos reciente) hasta que haya suficiente espacioen la memoria para colocar la lista de publicación de acceso actualmente;• LFU: un algoritmo estándar de LFU (desalojo de los menos utilizados), con la misma modificación que la LRU;• Dyn-Qtfdf: una versión dinámica del algoritmo QTFDF;Evice de la memoria caché el término (s) con la relación FQ (t) FD (t) más baja.",
                "El \"almacenamiento en caché dinámico\" de las consultas tiene una efectividad limitada debido al alto número de fallas obligatorias causadas por el número de consultas únicas o infrecuentes."
            ],
            "translated_text": "",
            "candidates": [
                "almacenamiento en caché dinámico",
                "almacenamiento en caché dinámico",
                "almacenamiento en caché dinámico",
                "almacenamiento en caché dinámico",
                "almacenamiento en caché dinámico",
                "almacenamiento en caché dinámico",
                "almacenamiento en caché dinámico",
                "almacenamiento dinámico en caché",
                "almacenamiento en caché dinámico",
                "almacenamiento dinámico",
                "almacenamiento en caché dinámico",
                "almacenamiento en caché dinámico",
                "almacenamiento en caché dinámico",
                "almacenamiento en caché dinámico",
                "almacenamiento en caché dinámico",
                "almacenamiento en caché dinámico"
            ],
            "error": []
        },
        "caching query result": {
            "translated_key": "resultado de la consulta de almacenamiento en caché",
            "is_in_text": true,
            "original_annotated_sentences": [
                "The Impact of Caching on Search Engines Ricardo Baeza-Yates1 rbaeza@acm.org Aristides Gionis1 gionis@yahoo-inc.com Flavio Junqueira1 fpj@yahoo-inc.com Vanessa Murdock1 vmurdock@yahoo-inc.com Vassilis Plachouras1 vassilis@yahoo-inc.com Fabrizio Silvestri2 f.silvestri@isti.cnr.it 1 Yahoo!",
                "Research Barcelona 2 ISTI - CNR Barcelona, SPAIN Pisa, ITALY ABSTRACT In this paper we study the trade-offs in designing efficient caching systems for Web search engines.",
                "We explore the impact of different approaches, such as static vs. dynamic caching, and <br>caching query result</br>s vs. caching posting lists.",
                "Using a query log spanning a whole year we explore the limitations of caching and we demonstrate that caching posting lists can achieve higher hit rates than caching query answers.",
                "We propose a new algorithm for static caching of posting lists, which outperforms previous methods.",
                "We also study the problem of finding the optimal way to split the static cache between answers and posting lists.",
                "Finally, we measure how the changes in the query log affect the effectiveness of static caching, given our observation that the distribution of the queries changes slowly over time.",
                "Our results and observations are applicable to different levels of the data-access hierarchy, for instance, for a memory/disk layer or a broker/remote server layer.",
                "Categories and Subject Descriptors H.3.3 [Information Storage and Retrieval]: Information Search and Retrieval - Search process; H.3.4 [Information Storage and Retrieval]: Systems and Software - Distributed systems, Performance evaluation (efficiency and effectiveness) General Terms Algorithms, Experimentation 1.",
                "INTRODUCTION Millions of queries are submitted daily to Web search engines, and users have high expectations of the quality and speed of the answers.",
                "As the searchable Web becomes larger and larger, with more than 20 billion pages to index, evaluating a single query requires processing large amounts of data.",
                "In such a setting, to achieve a fast response time and to increase the query throughput, using a cache is crucial.",
                "The primary use of a cache memory is to speedup computation by exploiting frequently or recently used data, although reducing the workload to back-end servers is also a major goal.",
                "Caching can be applied at different levels with increasing response latencies or processing requirements.",
                "For example, the different levels may correspond to the main memory, the disk, or resources in a local or a wide area network.",
                "The decision of what to cache is either off-line (static) or online (dynamic).",
                "A static cache is based on historical information and is periodically updated.",
                "A dynamic cache replaces entries according to the sequence of requests.",
                "When a new request arrives, the cache system decides whether to evict some entry from the cache in the case of a cache miss.",
                "Such online decisions are based on a cache policy, and several different policies have been studied in the past.",
                "For a search engine, there are two possible ways to use a cache memory: Caching answers: As the engine returns answers to a particular query, it may decide to store these answers to resolve future queries.",
                "Caching terms: As the engine evaluates a particular query, it may decide to store in memory the posting lists of the involved query terms.",
                "Often the whole set of posting lists does not fit in memory, and consequently, the engine has to select a small set to keep in memory and speed up query processing.",
                "Returning an answer to a query that already exists in the cache is more efficient than computing the answer using cached posting lists.",
                "On the other hand, previously unseen queries occur more often than previously unseen terms, implying a higher miss rate for cached answers.",
                "Caching of posting lists has additional challenges.",
                "As posting lists have variable size, caching them dynamically is not very efficient, due to the complexity in terms of efficiency and space, and the skewed distribution of the query stream, as shown later.",
                "Static caching of posting lists poses even more challenges: when deciding which terms to cache one faces the trade-off between frequently queried terms and terms with small posting lists that are space efficient.",
                "Finally, before deciding to adopt a static caching policy the query stream should be analyzed to verify that its characteristics do not change rapidly over time.",
                "Broker Static caching posting lists Dynamic/Static cached answers Local query processor Disk Next caching level Local network access Remote network access Figure 1: One caching level in a distributed search architecture.",
                "In this paper we explore the trade-offs in the design of each cache level, showing that the problem is the same and only a few parameters change.",
                "In general, we assume that each level of caching in a distributed search architecture is similar to that shown in Figure 1.",
                "We use a query log spanning a whole year to explore the limitations of dynamically caching query answers or posting lists for query terms.",
                "More concretely, our main conclusions are that: • Caching query answers results in lower hit ratios compared to caching of posting lists for query terms, but it is faster because there is no need for query evaluation.",
                "We provide a framework for the analysis of the trade-off between static caching of query answers and posting lists; • Static caching of terms can be more effective than dynamic caching with, for example, LRU.",
                "We provide algorithms based on the Knapsack problem for selecting the posting lists to put in a static cache, and we show improvements over previous work, achieving a hit ratio over 90%; • Changes of the query distribution over time have little impact on static caching.",
                "The remainder of this paper is organized as follows.",
                "Sections 2 and 3 summarize related work and characterize the data sets we use.",
                "Section 4 discusses the limitations of dynamic caching.",
                "Sections 5 and 6 introduce algorithms for caching posting lists, and a theoretical framework for the analysis of static caching, respectively.",
                "Section 7 discusses the impact of changes in the query distribution on static caching, and Section 8 provides concluding remarks. 2.",
                "RELATED WORK There is a large body of work devoted to query optimization.",
                "Buckley and Lewit [3], in one of the earliest works, take a term-at-a-time approach to deciding when inverted lists need not be further examined.",
                "More recent examples demonstrate that the top k documents for a query can be returned without the need for evaluating the complete set of posting lists [1, 4, 15].",
                "Although these approaches seek to improve query processing efficiency, they differ from our current work in that they do not consider caching.",
                "They may be considered separate and complementary to a cache-based approach.",
                "Raghavan and Sever [12], in one of the first papers on exploiting user query history, propose using a query base, built upon a set of persistent optimal queries submitted in the past, to improve the retrieval effectiveness for similar future queries.",
                "Markatos [10] shows the existence of temporal locality in queries, and compares the performance of different caching policies.",
                "Based on the observations of Markatos, Lempel and Moran propose a new caching policy, called Probabilistic Driven Caching, by attempting to estimate the probability distribution of all possible queries submitted to a search engine [8].",
                "Fagni et al. follow Markatos work by showing that combining static and dynamic caching policies together with an adaptive prefetching policy achieves a high hit ratio [7].",
                "Different from our work, they consider caching and prefetching of pages of results.",
                "As systems are often hierarchical, there has also been some effort on multi-level architectures.",
                "Saraiva et al. propose a new architecture for Web search engines using a two-level dynamic caching system [13].",
                "Their goal for such systems has been to improve response time for hierarchical engines.",
                "In their architecture, both levels use an LRU eviction policy.",
                "They find that the second-level cache can effectively reduce disk traffic, thus increasing the overall throughput.",
                "Baeza-Yates and Saint-Jean propose a three-level index organization [2].",
                "Long and Suel propose a caching system structured according to three different levels [9].",
                "The intermediate level contains frequently occurring pairs of terms and stores the intersections of the corresponding inverted lists.",
                "These last two papers are related to ours in that they exploit different caching strategies at different levels of the memory hierarchy.",
                "Finally, our static caching algorithm for posting lists in Section 5 uses the ratio frequency/size in order to evaluate the goodness of an item to cache.",
                "Similar ideas have been used in the context of file caching [17], Web caching [5], and even caching of posting lists [9], but in all cases in a dynamic setting.",
                "To the best of our knowledge we are the first to use this approach for static caching of posting lists. 3.",
                "DATA CHARACTERIZATION Our data consists of a crawl of documents from the UK domain, and query logs of one year of queries submitted to http://www.yahoo.co.uk from November 2005 to November 2006.",
                "In our logs, 50% of the total volume of queries are unique.",
                "The average query length is 2.5 terms, with the longest query having 731 terms. 1e-07 1e-06 1e-05 1e-04 0.001 0.01 0.1 1 1e-08 1e-07 1e-06 1e-05 1e-04 0.001 0.01 0.1 1 Frequency(normalized) Frequency rank (normalized) Figure 2: The distribution of queries (bottom curve) and query terms (middle curve) in the query log.",
                "The distribution of document frequencies of terms in the UK-2006 dataset (upper curve).",
                "Figure 2 shows the distributions of queries (lower curve), and query terms (middle curve).",
                "The x-axis represents the normalized frequency rank of the query or term. (The most frequent query appears closest to the y-axis.)",
                "The y-axis is Table 1: Statistics of the UK-2006 sample.",
                "UK-2006 sample statistics # of documents 2,786,391 # of terms 6,491,374 # of tokens 2,109,512,558 the normalized frequency for a given query (or term).",
                "As expected, the distribution of query frequencies and query term frequencies follow power law distributions, with slope of 1.84 and 2.26, respectively.",
                "In this figure, the query frequencies were computed as they appear in the logs with no normalization for case or white space.",
                "The query terms (middle curve) have been normalized for case, as have the terms in the document collection.",
                "The document collection that we use for our experiments is a summary of the UK domain crawled in May 2006.1 This summary corresponds to a maximum of 400 crawled documents per host, using a breadth first crawling strategy, comprising 15GB.",
                "The distribution of document frequencies of terms in the collection follows a power law distribution with slope 2.38 (upper curve in Figure 2).",
                "The statistics of the collection are shown in Table 1.",
                "We measured the correlation between the document frequency of terms in the collection and the number of queries that contain a particular term in the query log to be 0.424.",
                "A scatter plot for a random sample of terms is shown in Figure 3.",
                "In this experiment, terms have been converted to lower case in both the queries and the documents so that the frequencies will be comparable. 1e-07 1e-06 1e-05 1e-04 0.001 0.01 0.1 1 1e-06 1e-05 1e-04 0.001 0.01 0.1 1 Queryfrequency Document frequency Figure 3: Normalized scatter plot of document-term frequencies vs. query-term frequencies. 4.",
                "CACHING OF QUERIES AND TERMS Caching relies upon the assumption that there is locality in the stream of requests.",
                "That is, there must be sufficient repetition in the stream of requests and within intervals of time that enable a cache memory of reasonable size to be effective.",
                "In the query log we used, 88% of the unique queries are singleton queries, and 44% are singleton queries out of the whole volume.",
                "Thus, out of all queries in the stream composing the query log, the upper threshold on hit ratio is 56%.",
                "This is because only 56% of all the queries comprise queries that have multiple occurrences.",
                "It is important to observe, however, that not all queries in this 56% can be cache hits because of compulsory misses.",
                "A compulsory miss 1 The collection is available from the University of Milan: http://law.dsi.unimi.it/.",
                "URL retrieved 05/2007. 0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1 240 260 280 300 320 340 360 Numberofelements Bin number Total terms Terms diff Total queries Unique queries Unique terms Query diff Figure 4: Arrival rate for both terms and queries. happens when the cache receives a query for the first time.",
                "This is different from capacity misses, which happen due to space constraints on the amount of memory the cache uses.",
                "If we consider a cache with infinite memory, then the hit ratio is 50%.",
                "Note that for an infinite cache there are no capacity misses.",
                "As we mentioned before, another possibility is to cache the posting lists of terms.",
                "Intuitively, this gives more freedom in the utilization of the cache content to respond to queries because cached terms might form a new query.",
                "On the other hand, they need more space.",
                "As opposed to queries, the fraction of singleton terms in the total volume of terms is smaller.",
                "In our query log, only 4% of the terms appear once, but this accounts for 73% of the vocabulary of query terms.",
                "We show in Section 5 that caching a small fraction of terms, while accounting for terms appearing in many documents, is potentially very effective.",
                "Figure 4 shows several graphs corresponding to the normalized arrival rate for different cases using days as bins.",
                "That is, we plot the normalized number of elements that appear in a day.",
                "This graph shows only a period of 122 days, and we normalize the values by the maximum value observed throughout the whole period of the query log.",
                "Total queries and Total terms correspond to the total volume of queries and terms, respectively.",
                "Unique queries and Unique terms correspond to the arrival rate of unique queries and terms.",
                "Finally, Query diff and Terms diff correspond to the difference between the curves for total and unique.",
                "In Figure 4, as expected, the volume of terms is much higher than the volume of queries.",
                "The difference between the total number of terms and the number of unique terms is much larger than the difference between the total number of queries and the number of unique queries.",
                "This observation implies that terms repeat significantly more than queries.",
                "If we use smaller bins, say of one hour, then the ratio of unique to volume is higher for both terms and queries because it leaves less room for repetition.",
                "We also estimated the workload using the document frequency of terms as a measure of how much work a query imposes on a search engine.",
                "We found that it follows closely the arrival rate for terms shown in Figure 4.",
                "To demonstrate the effect of a dynamic cache on the query frequency distribution of Figure 2, we plot the same frequency graph, but now considering the frequency of queries Figure 5: Frequency graph after LRU cache. after going through an LRU cache.",
                "On a cache miss, an LRU cache decides upon an entry to evict using the information on the recency of queries.",
                "In this graph, the most frequent queries are not the same queries that were most frequent before the cache.",
                "It is possible that queries that are most frequent after the cache have different characteristics, and tuning the search engine to queries frequent before the cache may degrade performance for non-cached queries.",
                "The maximum frequency after caching is less than 1% of the maximum frequency before the cache, thus showing that the cache is very effective in reducing the load of frequent queries.",
                "If we re-rank the queries according to after-cache frequency, the distribution is still a power law, but with a much smaller value for the highest frequency.",
                "When discussing the effectiveness of dynamically caching, an important metric is cache miss rate.",
                "To analyze the cache miss rate for different memory constraints, we use the working set model [6, 14].",
                "A working set, informally, is the set of references that an application or an operating system is currently working with.",
                "The model uses such sets in a strategy that tries to capture the temporal locality of references.",
                "The working set strategy then consists in keeping in memory only the elements that are referenced in the previous θ steps of the input sequence, where θ is a configurable parameter corresponding to the window size.",
                "Originally, working sets have been used for page replacement algorithms of operating systems, and considering such a strategy in the context of search engines is interesting for three reasons.",
                "First, it captures the amount of locality of queries and terms in a sequence of queries.",
                "Locality in this case refers to the frequency of queries and terms in a window of time.",
                "If many queries appear multiple times in a window, then locality is high.",
                "Second, it enables an oﬄine analysis of the expected miss rate given different memory constraints.",
                "Third, working sets capture aspects of efficient caching algorithms such as LRU.",
                "LRU assumes that references farther in the past are less likely to be referenced in the present, which is implicit in the concept of working sets [14].",
                "Figure 6 plots the miss rate for different working set sizes, and we consider working sets of both queries and terms.",
                "The working set sizes are normalized against the total number of queries in the query log.",
                "In the graph for queries, there is a sharp decay until approximately 0.01, and the rate at which the miss rate drops decreases as we increase the size of the working set over 0.01.",
                "Finally, the minimum value it reaches is 50% miss rate, not shown in the figure as we have cut the tail of the curve for presentation purposes. 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1 0 0.05 0.1 0.15 0.2 Missrate Normalized working set size Queries Terms Figure 6: Miss rate as a function of the working set size. 1 10 100 1000 10000 100000 1e+06 Frequency Distance Figure 7: Distribution of distances expressed in terms of distinct queries.",
                "Compared to the query curve, we observe that the minimum miss rate for terms is substantially smaller.",
                "The miss rate also drops sharply on values up to 0.01, and it decreases minimally for higher values.",
                "The minimum value, however, is slightly over 10%, which is much smaller than the minimum value for the sequence of queries.",
                "This implies that with such a policy it is possible to achieve over 80% hit rate, if we consider caching dynamically posting lists for terms as opposed to caching answers for queries.",
                "This result does not consider the space required for each unit stored in the cache memory, or the amount of time it takes to put together a response to a user query.",
                "We analyze these issues more carefully later in this paper.",
                "It is interesting also to observe the histogram of Figure 7, which is an intermediate step in the computation of the miss rate graph.",
                "It reports the distribution of distances between repetitions of the same frequent query.",
                "The distance in the plot is measured in the number of distinct queries separating a query and its repetition, and it considers only queries appearing at least 10 times.",
                "From Figures 6 and 7, we conclude that even if we set the size of the query answers cache to a relatively large number of entries, the miss rate is high.",
                "Thus, caching the posting lists of terms has the potential to improve the hit ratio.",
                "This is what we explore next. 5.",
                "CACHING POSTING LISTS The previous section shows that caching posting lists can obtain a higher hit rate compared to caching query answers.",
                "In this section we study the problem of how to select posting lists to place on a certain amount of available memory, assuming that the whole index is larger than the amount of memory available.",
                "The posting lists have variable size (in fact, their size distribution follows a power law), so it is beneficial for a caching policy to consider the sizes of the posting lists.",
                "We consider both dynamic and static caching.",
                "For dynamic caching, we use two well-known policies, LRU and LFU, as well as a modified algorithm that takes posting-list size into account.",
                "Before discussing the static caching strategies, we introduce some notation.",
                "We use fq(t) to denote the query-term frequency of a term t, that is, the number of queries containing t in the query log, and fd(t) to denote the document frequency of t, that is, the number of documents in the collection in which the term t appears.",
                "The first strategy we consider is the algorithm proposed by Baeza-Yates and Saint-Jean [2], which consists in selecting the posting lists of the terms with the highest query-term frequencies fq(t).",
                "We call this algorithm Qtf.",
                "We observe that there is a trade-off between fq(t) and fd(t).",
                "Terms with high fq(t) are useful to keep in the cache because they are queried often.",
                "On the other hand, terms with high fd(t) are not good candidates because they correspond to long posting lists and consume a substantial amount of space.",
                "In fact, the problem of selecting the best posting lists for the static cache corresponds to the standard Knapsack problem: given a knapsack of fixed capacity, and a set of n items, such as the i-th item has value ci and size si, select the set of items that fit in the knapsack and maximize the overall value.",
                "In our case, value corresponds to fq(t) and size corresponds to fd(t).",
                "Thus, we employ a simple algorithm for the knapsack problem, which is selecting the posting lists of the terms with the highest values of the ratio fq(t) fd(t) .",
                "We call this algorithm QtfDf.",
                "We tried other variations considering query frequencies instead of term frequencies, but the gain was minimal compared to the complexity added.",
                "In addition to the above two static algorithms we consider the following algorithms for dynamic caching: • LRU: A standard LRU algorithm, but many posting lists might need to be evicted (in order of least-recent usage) until there is enough space in the memory to place the currently accessed posting list; • LFU: A standard LFU algorithm (eviction of the leastfrequently used), with the same modification as the LRU; • Dyn-QtfDf: A dynamic version of the QtfDf algorithm; evict from the cache the term(s) with the lowest fq(t) fd(t) ratio.",
                "The performance of all the above algorithms for 15 weeks of the query log and the UK dataset are shown in Figure 8.",
                "Performance is measured with hit rate.",
                "The cache size is measured as a fraction of the total space required to store the posting lists of all terms.",
                "For the dynamic algorithms, we load the cache with terms in order of fq(t) and we let the cache warm up for 1 million queries.",
                "For the static algorithms, we assume complete knowledge of the frequencies fq(t), that is, we estimate fq(t) from the whole query stream.",
                "As we show in Section 7 the results do not change much if we compute the query-term frequencies using the first 3 or 4 weeks of the query log and measure the hit rate on the rest. 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1 0.1 0.2 0.3 0.4 0.5 0.6 0.7 Hitrate Cache size Caching posting lists static QTF/DF LRU LFU Dyn-QTF/DF QTF Figure 8: Hit rate of different strategies for caching posting lists.",
                "The most important observation from our experiments is that the static QtfDf algorithm has a better hit rate than all the dynamic algorithms.",
                "An important benefit a static cache is that it requires no eviction and it is hence more efficient when evaluating queries.",
                "However, if the characteristics of the query traffic change frequently over time, then it requires re-populating the cache often or there will be a significant impact on hit rate. 6.",
                "ANALYSIS OF STATIC CACHING In this section we provide a detailed analysis for the problem of deciding whether it is preferable to cache query answers or cache posting lists.",
                "Our analysis takes into account the impact of caching between two levels of the data-access hierarchy.",
                "It can either be applied at the memory/disk layer or at a server/remote server layer as in the architecture we discussed in the introduction.",
                "Using a particular system model, we obtain estimates for the parameters required by our analysis, which we subsequently use to decide the optimal trade-off between caching query answers and caching posting lists. 6.1 Analytical Model Let M be the size of the cache measured in answer units (the cache can store M query answers).",
                "Assume that all posting lists are of the same length L, measured in answer units.",
                "We consider the following two cases: (A) a cache that stores only precomputed answers, and (B) a cache that stores only posting lists.",
                "In the first case, Nc = M answers fit in the cache, while in the second case Np = M/L posting lists fit in the cache.",
                "Thus, Np = Nc/L.",
                "Note that although posting lists require more space, we can combine terms to evaluate more queries (or partial queries).",
                "For case (A), suppose that a query answer in the cache can be evaluated in 1 time unit.",
                "For case (B), assume that if the posting lists of the terms of a query are in the cache then the results can be computed in TR1 time units, while if the posting lists are not in the cache then the results can be computed in TR2 time units.",
                "Of course TR2 > TR1.",
                "Now we want to compare the time to answer a stream of Q queries in both cases.",
                "Let Vc(Nc) be the volume of the most frequent Nc queries.",
                "Then, for case (A), we have an overall time TCA = Vc(Nc) + TR2(Q − Vc(Nc)).",
                "Similarly, for case (B), let Vp(Np) be the number of computable queries.",
                "Then we have overall time TP L = TR1Vp(Np) + TR2(Q − Vp(Np)).",
                "We want to check under which conditions we have TP L < TCA.",
                "We have TP L − TCA = (TR2 − 1)Vc(Nc) − (TR2 − TR1)Vp(Np) > 0.",
                "Figure 9 shows the values of Vp and Vc for our data.",
                "We can see that caching answers saturates faster and for this particular data there is no additional benefit from using more than 10% of the index space for caching answers.",
                "As the query distribution is a power law with parameter α > 1, the i-th most frequent query appears with probability proportional to 1 iα .",
                "Therefore, the volume Vc(n), which is the total number of the n most frequent queries, is Vc(n) = V0 n i=1 Q iα = γnQ (0 < γn < 1).",
                "We know that Vp(n) grows faster than Vc(n) and assume, based on experimental results, that the relation is of the form Vp(n) = k Vc(n)β .",
                "In the worst case, for a large cache, β → 1.",
                "That is, both techniques will cache a constant fraction of the overall query volume.",
                "Then caching posting lists makes sense only if L(TR2 − 1) k(TR2 − TR1) > 1.",
                "If we use compression, we have L < L and TR1 > TR1.",
                "According to the experiments that we show later, compression is always better.",
                "For a small cache, we are interested in the transient behavior and then β > 1, as computed from our data.",
                "In this case there will always be a point where TP L > TCA for a large number of queries.",
                "In reality, instead of filling the cache only with answers or only with posting lists, a better strategy will be to divide the total cache space into cache for answers and cache for posting lists.",
                "In such a case, there will be some queries that could be answered by both parts of the cache.",
                "As the answer cache is faster, it will be the first choice for answering those queries.",
                "Let QNc and QNp be the set of queries that can be answered by the cached answers and the cached posting lists, respectively.",
                "Then, the overall time is T = Vc(Nc)+TR1V (QNp −QNc )+TR2(Q−V (QNp ∪QNc )), where Np = (M − Nc)/L.",
                "Finding the optimal division of the cache in order to minimize the overall retrieval time is a difficult problem to solve analytically.",
                "In Section 6.3 we use simulations to derive optimal cache trade-offs for particular implementation examples. 6.2 Parameter Estimation We now use a particular implementation of a centralized system and the model of a distributed system as examples from which we estimate the parameters of the analysis from the previous section.",
                "We perform the experiments using an optimized version of Terrier [11] for both indexing documents and processing queries, on a single machine with a Pentium 4 at 2GHz and 1GB of RAM.",
                "We indexed the documents from the UK-2006 dataset, without removing stop words or applying stemming.",
                "The posting lists in the inverted file consist of pairs of document identifier and term frequency.",
                "We compress the document identifier gaps using Elias gamma encoding, and the 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1 0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1 Queryvolume Space precomputed answers posting lists Figure 9: Cache saturation as a function of size.",
                "Table 2: Ratios between the average time to evaluate a query and the average time to return cached answers (centralized and distributed case).",
                "Centralized system TR1 TR2 TR1 TR2 Full evaluation 233 1760 707 1140 Partial evaluation 99 1626 493 798 LAN system TRL 1 TRL 2 TR L 1 TR L 2 Full evaluation 242 1769 716 1149 Partial evaluation 108 1635 502 807 WAN system TRW 1 TRW 2 TR W 1 TR W 2 Full evaluation 5001 6528 5475 5908 Partial evaluation 4867 6394 5270 5575 term frequencies in documents using unary encoding [16].",
                "The size of the inverted file is 1,189Mb.",
                "A stored answer requires 1264 bytes, and an uncompressed posting takes 8 bytes.",
                "From Table 1, we obtain L = (8·# of postings) 1264·# of terms = 0.75 and L = Inverted file size 1264·# of terms = 0.26.",
                "We estimate the ratio TR = T/Tc between the average time T it takes to evaluate a query and the average time Tc it takes to return a stored answer for the same query, in the following way.",
                "Tc is measured by loading the answers for 100,000 queries in memory, and answering the queries from memory.",
                "The average time is Tc = 0.069ms.",
                "T is measured by processing the same 100,000 queries (the first 10,000 queries are used to warm-up the system).",
                "For each query, we remove stop words, if there are at least three remaining terms.",
                "The stop words correspond to the terms with a frequency higher than the number of documents in the index.",
                "We use a document-at-a-time approach to retrieve documents containing all query terms.",
                "The only disk access required during query processing is for reading compressed posting lists from the inverted file.",
                "We perform both full and partial evaluation of answers, because some queries are likely to retrieve a large number of documents, and only a fraction of the retrieved documents will be seen by users.",
                "In the partial evaluation of queries, we terminate the processing after matching 10,000 documents.",
                "The estimated ratios TR are presented in Table 2.",
                "Figure 10 shows for a sample of queries the workload of the system with partial query evaluation and compressed posting lists.",
                "The x-axis corresponds to the total time the system spends processing a particular query, and the vertical axis corresponds to the sum t∈q fq · fd(t).",
                "Notice that the total number of postings of the query-terms does not necessarily provide an accurate estimate of the workload imposed on the system by a query (which is the case for full evaluation and uncompressed lists). 0 0.2 0.4 0.6 0.8 1 0 0.2 0.4 0.6 0.8 1 Totalpostingstoprocessquery(normalized) Total time to process query (normalized) Partial processing of compressed postings query len = 1 query len in [2,3] query len in [4,8] query len > 8 Figure 10: Workload for partial query evaluation with compressed posting lists.",
                "The analysis of the previous section also applies to a distributed retrieval system in one or multiple sites.",
                "Suppose that a document partitioned distributed system is running on a cluster of machines interconnected with a local area network (LAN) in one site.",
                "The broker receives queries and broadcasts them to the query processors, which answer the queries and return the results to the broker.",
                "Finally, the broker merges the received answers and generates the final set of answers (we assume that the time spent on merging results is negligible).",
                "The difference between the centralized architecture and the document partition architecture is the extra communication between the broker and the query processors.",
                "Using ICMP pings on a 100Mbps LAN, we have measured that sending the query from the broker to the query processors which send an answer of 4,000 bytes back to the broker takes on average 0.615ms.",
                "Hence, TRL = TR + 0.615ms/0.069ms = TR + 9.",
                "In the case when the broker and the query processors are in different sites connected with a wide area network (WAN), we estimated that broadcasting the query from the broker to the query processors and getting back an answer of 4,000 bytes takes on average 329ms.",
                "Hence, TRW = TR + 329ms/0.069ms = TR + 4768. 6.3 Simulation Results We now address the problem of finding the optimal tradeoff between caching query answers and caching posting lists.",
                "To make the problem concrete we assume a fixed budget M on the available memory, out of which x units are used for caching query answers and M − x for caching posting lists.",
                "We perform simulations and compute the average response time as a function of x.",
                "Using a part of the query log as training data, we first allocate in the cache the answers to the most frequent queries that fit in space x, and then we use the rest of the memory to cache posting lists.",
                "For selecting posting lists we use the QtfDf algorithm, applied to the training query log but excluding the queries that have already been cached.",
                "In Figure 11, we plot the simulated response time for a centralized system as a function of x.",
                "For the uncompressed index we use M = 1GB, and for the compressed index we use M = 0.5GB.",
                "In the case of the configuration that uses partial query evaluation with compressed posting lists, the lowest response time is achieved when 0.15GB out of the 0.5GB is allocated for storing answers for queries.",
                "We obtained similar trends in the results for the LAN setting.",
                "Figure 12 shows the simulated workload for a distributed system across a WAN.",
                "In this case, the total amount of memory is split between the broker, which holds the cached 400 500 600 700 800 900 1000 1100 1200 0 0.2 0.4 0.6 0.8 1 Averageresponsetime Space (GB) Simulated workload -- single machine full / uncompr / 1 G partial / uncompr / 1 G full / compr / 0.5 G partial / compr / 0.5 G Figure 11: Optimal division of the cache in a server. 3000 3500 4000 4500 5000 5500 6000 0 0.2 0.4 0.6 0.8 1 Averageresponsetime Space (GB) Simulated workload -- WAN full / uncompr / 1 G partial / uncompr / 1 G full / compr / 0.5 G partial / compr / 0.5 G Figure 12: Optimal division of the cache when the next level requires WAN access. answers of queries, and the query processors, which hold the cache of posting lists.",
                "According to the figure, the difference between the configurations of the query processors is less important because the network communication overhead increases the response time substantially.",
                "When using uncompressed posting lists, the optimal allocation of memory corresponds to using approximately 70% of the memory for caching query answers.",
                "This is explained by the fact that there is no need for network communication when the query can be answered by the cache at the broker. 7.",
                "EFFECT OF THE QUERY DYNAMICS For our query log, the query distribution and query-term distribution change slowly over time.",
                "To support this claim, we first assess how topics change comparing the distribution of queries from the first week in June, 2006, to the distribution of queries for the remainder of 2006 that did not appear in the first week in June.",
                "We found that a very small percentage of queries are new queries.",
                "The majority of queries that appear in a given week repeat in the following weeks for the next six months.",
                "We then compute the hit rate of a static cache of 128, 000 answers trained over a period of two weeks (Figure 13).",
                "We report hit rate hourly for 7 days, starting from 5pm.",
                "We observe that the hit rate reaches its highest value during the night (around midnight), whereas around 2-3pm it reaches its minimum.",
                "After a small decay in hit rate values, the hit rate stabilizes between 0.28, and 0.34 for the entire week, suggesting that the static cache is effective for a whole week after the training period. 0.26 0.27 0.28 0.29 0.3 0.31 0.32 0.33 0.34 0.35 0.36 0.37 0 20 40 60 80 100 120 140 160 Hit-rate Time Hits on the frequent queries of distances Figure 13: Hourly hit rate for a static cache holding 128,000 answers during the period of a week.",
                "The static cache of posting lists can be periodically recomputed.",
                "To estimate the time interval in which we need to recompute the posting lists on the static cache we need to consider an efficiency/quality trade-off: using too short a time interval might be prohibitively expensive, while recomputing the cache too infrequently might lead to having an obsolete cache not corresponding to the statistical characteristics of the current query stream.",
                "We measured the effect on the QtfDf algorithm of the changes in a 15-week query stream (Figure 14).",
                "We compute the query term frequencies over the whole stream, select which terms to cache, and then compute the hit rate on the whole query stream.",
                "This hit rate is as an upper bound, and it assumes perfect knowledge of the query term frequencies.",
                "To simulate a realistic scenario, we use the first 6 (3) weeks of the query stream for computing query term frequencies and the following 9 (12) weeks to estimate the hit rate.",
                "As Figure 14 shows, the hit rate decreases by less than 2%.",
                "The high correlation among the query term frequencies during different time periods explains the graceful adaptation of the static caching algorithms to the future query stream.",
                "Indeed, the pairwise correlation among all possible 3-week periods of the 15-week query stream is over 99.5%. 8.",
                "CONCLUSIONS Caching is an effective technique in search engines for improving response time, reducing the load on query processors, and improving network bandwidth utilization.",
                "We present results on both dynamic and static caching.",
                "Dynamic caching of queries has limited effectiveness due to the high number of compulsory misses caused by the number of unique or infrequent queries.",
                "Our results show that in our UK log, the minimum miss rate is 50% using a working set strategy.",
                "Caching terms is more effective with respect to miss rate, achieving values as low as 12%.",
                "We also propose a new algorithm for static caching of posting lists that outperforms previous static caching algorithms as well as dynamic algorithms such as LRU and LFU, obtaining hit rate values that are over 10% higher compared these strategies.",
                "We present a framework for the analysis of the trade-off between <br>caching query result</br>s and caching posting lists, and we simulate different types of architectures.",
                "Our results show that for centralized and LAN environments, there is an optimal allocation of <br>caching query result</br>s and caching of posting lists, while for WAN scenarios in which network time prevails it is more important to cache query results. 0.45 0.5 0.55 0.6 0.65 0.7 0.75 0.8 0.85 0.9 0.95 0.1 0.2 0.3 0.4 0.5 0.6 0.7 Hitrate Cache size Dynamics of static QTF/DF caching policy perfect knowledge 6-week training 3-week training Figure 14: Impact of distribution changes on the static caching of posting lists. 9.",
                "REFERENCES [1] V. N. Anh and A. Moffat.",
                "Pruned query evaluation using pre-computed impacts.",
                "In ACM CIKM, 2006. [2] R. A. Baeza-Yates and F. Saint-Jean.",
                "A three level search engine index based in query log distribution.",
                "In SPIRE, 2003. [3] C. Buckley and A. F. Lewit.",
                "Optimization of inverted vector searches.",
                "In ACM SIGIR, 1985. [4] S. B¨uttcher and C. L. A. Clarke.",
                "A document-centric approach to static index pruning in text retrieval systems.",
                "In ACM CIKM, 2006. [5] P. Cao and S. Irani.",
                "Cost-aware WWW proxy caching algorithms.",
                "In USITS, 1997. [6] P. Denning.",
                "Working sets past and present.",
                "IEEE Trans. on Software Engineering, SE-6(1):64-84, 1980. [7] T. Fagni, R. Perego, F. Silvestri, and S. Orlando.",
                "Boosting the performance of web search engines: Caching and prefetching query results by exploiting historical usage data.",
                "ACM Trans.",
                "Inf.",
                "Syst., 24(1):51-78, 2006. [8] R. Lempel and S. Moran.",
                "Predictive caching and prefetching of query results in search engines.",
                "In WWW, 2003. [9] X.",
                "Long and T. Suel.",
                "Three-level caching for efficient query processing in large web search engines.",
                "In WWW, 2005. [10] E. P. Markatos.",
                "On caching search engine query results.",
                "Computer Communications, 24(2):137-143, 2001. [11] I. Ounis, G. Amati, V. Plachouras, B.",
                "He, C. Macdonald, and C. Lioma.",
                "Terrier: A High Performance and Scalable Information Retrieval Platform.",
                "In SIGIR Workshop on Open Source Information Retrieval, 2006. [12] V. V. Raghavan and H. Sever.",
                "On the reuse of past optimal queries.",
                "In ACM SIGIR, 1995. [13] P. C. Saraiva, E. S. de Moura, N. Ziviani, W. Meira, R. Fonseca, and B. Riberio-Neto.",
                "Rank-preserving two-level caching for scalable search engines.",
                "In ACM SIGIR, 2001. [14] D. R. Slutz and I. L. Traiger.",
                "A note on the calculation of average working set size.",
                "Communications of the ACM, 17(10):563-565, 1974. [15] T. Strohman, H. Turtle, and W. B. Croft.",
                "Optimization strategies for complex queries.",
                "In ACM SIGIR, 2005. [16] I. H. Witten, T. C. Bell, and A. Moffat.",
                "Managing Gigabytes: Compressing and Indexing Documents and Images.",
                "John Wiley & Sons, Inc., NY, 1994. [17] N. E. Young.",
                "On-line file caching.",
                "Algorithmica, 33(3):371-383, 2002."
            ],
            "original_annotated_samples": [],
            "translated_annotated_samples": [
                "Exploramos el impacto de diferentes enfoques, como el almacenamiento en caché estático frente a la dinámica, y el \"resultado de la consulta de almacenamiento en caché\" s vs. listas de publicación de almacenamiento en caché.",
                "Presentamos un marco para el análisis de la compensación entre el resultado de la \"consulta de almacenamiento en caché\" S y las listas de publicación de almacenamiento en caché, y simulamos diferentes tipos de arquitecturas.",
                "Nuestros resultados muestran que para los entornos centralizados y LAN, existe una asignación óptima de \"resultados de consulta de almacenamiento en caché\" y almacenamiento en caché de listas de publicación, mientras que para escenarios WAN en los que prevalece el tiempo de red es más importante para los resultados de la consulta de caché.0.45 0.5 0.5 0.5 0.6 0.65 0.7 0.75 0.8 0.85 0.9 0.95 0.1 0.2 0.3 0.4 0.5 0.5 0.6 0.7 Dinámica de tamaño de caché HITRate Dinámica de la caché de caché estático/DF Conocimiento perfecto de 6 semanas Capacitación de 3 semanas Figura 14: Impacto de la distribución Cambios de distribución en el caché estáticode publicar listas.9."
            ],
            "translated_text": "",
            "candidates": [
                "Resultado de la consulta de almacenamiento en caché",
                "resultado de la consulta de almacenamiento en caché",
                "Resultado de la consulta de almacenamiento en caché",
                "consulta de almacenamiento en caché",
                "Resultado de la consulta de almacenamiento en caché",
                "resultados de consulta de almacenamiento en caché"
            ],
            "error": []
        },
        "caching posting list": {
            "translated_key": "lista de publicaciones en caché",
            "is_in_text": true,
            "original_annotated_sentences": [
                "The Impact of Caching on Search Engines Ricardo Baeza-Yates1 rbaeza@acm.org Aristides Gionis1 gionis@yahoo-inc.com Flavio Junqueira1 fpj@yahoo-inc.com Vanessa Murdock1 vmurdock@yahoo-inc.com Vassilis Plachouras1 vassilis@yahoo-inc.com Fabrizio Silvestri2 f.silvestri@isti.cnr.it 1 Yahoo!",
                "Research Barcelona 2 ISTI - CNR Barcelona, SPAIN Pisa, ITALY ABSTRACT In this paper we study the trade-offs in designing efficient caching systems for Web search engines.",
                "We explore the impact of different approaches, such as static vs. dynamic caching, and caching query results vs. <br>caching posting list</br>s.",
                "Using a query log spanning a whole year we explore the limitations of caching and we demonstrate that <br>caching posting list</br>s can achieve higher hit rates than caching query answers.",
                "We propose a new algorithm for static caching of posting lists, which outperforms previous methods.",
                "We also study the problem of finding the optimal way to split the static cache between answers and posting lists.",
                "Finally, we measure how the changes in the query log affect the effectiveness of static caching, given our observation that the distribution of the queries changes slowly over time.",
                "Our results and observations are applicable to different levels of the data-access hierarchy, for instance, for a memory/disk layer or a broker/remote server layer.",
                "Categories and Subject Descriptors H.3.3 [Information Storage and Retrieval]: Information Search and Retrieval - Search process; H.3.4 [Information Storage and Retrieval]: Systems and Software - Distributed systems, Performance evaluation (efficiency and effectiveness) General Terms Algorithms, Experimentation 1.",
                "INTRODUCTION Millions of queries are submitted daily to Web search engines, and users have high expectations of the quality and speed of the answers.",
                "As the searchable Web becomes larger and larger, with more than 20 billion pages to index, evaluating a single query requires processing large amounts of data.",
                "In such a setting, to achieve a fast response time and to increase the query throughput, using a cache is crucial.",
                "The primary use of a cache memory is to speedup computation by exploiting frequently or recently used data, although reducing the workload to back-end servers is also a major goal.",
                "Caching can be applied at different levels with increasing response latencies or processing requirements.",
                "For example, the different levels may correspond to the main memory, the disk, or resources in a local or a wide area network.",
                "The decision of what to cache is either off-line (static) or online (dynamic).",
                "A static cache is based on historical information and is periodically updated.",
                "A dynamic cache replaces entries according to the sequence of requests.",
                "When a new request arrives, the cache system decides whether to evict some entry from the cache in the case of a cache miss.",
                "Such online decisions are based on a cache policy, and several different policies have been studied in the past.",
                "For a search engine, there are two possible ways to use a cache memory: Caching answers: As the engine returns answers to a particular query, it may decide to store these answers to resolve future queries.",
                "Caching terms: As the engine evaluates a particular query, it may decide to store in memory the posting lists of the involved query terms.",
                "Often the whole set of posting lists does not fit in memory, and consequently, the engine has to select a small set to keep in memory and speed up query processing.",
                "Returning an answer to a query that already exists in the cache is more efficient than computing the answer using cached posting lists.",
                "On the other hand, previously unseen queries occur more often than previously unseen terms, implying a higher miss rate for cached answers.",
                "Caching of posting lists has additional challenges.",
                "As posting lists have variable size, caching them dynamically is not very efficient, due to the complexity in terms of efficiency and space, and the skewed distribution of the query stream, as shown later.",
                "Static caching of posting lists poses even more challenges: when deciding which terms to cache one faces the trade-off between frequently queried terms and terms with small posting lists that are space efficient.",
                "Finally, before deciding to adopt a static caching policy the query stream should be analyzed to verify that its characteristics do not change rapidly over time.",
                "Broker Static <br>caching posting list</br>s Dynamic/Static cached answers Local query processor Disk Next caching level Local network access Remote network access Figure 1: One caching level in a distributed search architecture.",
                "In this paper we explore the trade-offs in the design of each cache level, showing that the problem is the same and only a few parameters change.",
                "In general, we assume that each level of caching in a distributed search architecture is similar to that shown in Figure 1.",
                "We use a query log spanning a whole year to explore the limitations of dynamically caching query answers or posting lists for query terms.",
                "More concretely, our main conclusions are that: • Caching query answers results in lower hit ratios compared to caching of posting lists for query terms, but it is faster because there is no need for query evaluation.",
                "We provide a framework for the analysis of the trade-off between static caching of query answers and posting lists; • Static caching of terms can be more effective than dynamic caching with, for example, LRU.",
                "We provide algorithms based on the Knapsack problem for selecting the posting lists to put in a static cache, and we show improvements over previous work, achieving a hit ratio over 90%; • Changes of the query distribution over time have little impact on static caching.",
                "The remainder of this paper is organized as follows.",
                "Sections 2 and 3 summarize related work and characterize the data sets we use.",
                "Section 4 discusses the limitations of dynamic caching.",
                "Sections 5 and 6 introduce algorithms for <br>caching posting list</br>s, and a theoretical framework for the analysis of static caching, respectively.",
                "Section 7 discusses the impact of changes in the query distribution on static caching, and Section 8 provides concluding remarks. 2.",
                "RELATED WORK There is a large body of work devoted to query optimization.",
                "Buckley and Lewit [3], in one of the earliest works, take a term-at-a-time approach to deciding when inverted lists need not be further examined.",
                "More recent examples demonstrate that the top k documents for a query can be returned without the need for evaluating the complete set of posting lists [1, 4, 15].",
                "Although these approaches seek to improve query processing efficiency, they differ from our current work in that they do not consider caching.",
                "They may be considered separate and complementary to a cache-based approach.",
                "Raghavan and Sever [12], in one of the first papers on exploiting user query history, propose using a query base, built upon a set of persistent optimal queries submitted in the past, to improve the retrieval effectiveness for similar future queries.",
                "Markatos [10] shows the existence of temporal locality in queries, and compares the performance of different caching policies.",
                "Based on the observations of Markatos, Lempel and Moran propose a new caching policy, called Probabilistic Driven Caching, by attempting to estimate the probability distribution of all possible queries submitted to a search engine [8].",
                "Fagni et al. follow Markatos work by showing that combining static and dynamic caching policies together with an adaptive prefetching policy achieves a high hit ratio [7].",
                "Different from our work, they consider caching and prefetching of pages of results.",
                "As systems are often hierarchical, there has also been some effort on multi-level architectures.",
                "Saraiva et al. propose a new architecture for Web search engines using a two-level dynamic caching system [13].",
                "Their goal for such systems has been to improve response time for hierarchical engines.",
                "In their architecture, both levels use an LRU eviction policy.",
                "They find that the second-level cache can effectively reduce disk traffic, thus increasing the overall throughput.",
                "Baeza-Yates and Saint-Jean propose a three-level index organization [2].",
                "Long and Suel propose a caching system structured according to three different levels [9].",
                "The intermediate level contains frequently occurring pairs of terms and stores the intersections of the corresponding inverted lists.",
                "These last two papers are related to ours in that they exploit different caching strategies at different levels of the memory hierarchy.",
                "Finally, our static caching algorithm for posting lists in Section 5 uses the ratio frequency/size in order to evaluate the goodness of an item to cache.",
                "Similar ideas have been used in the context of file caching [17], Web caching [5], and even caching of posting lists [9], but in all cases in a dynamic setting.",
                "To the best of our knowledge we are the first to use this approach for static caching of posting lists. 3.",
                "DATA CHARACTERIZATION Our data consists of a crawl of documents from the UK domain, and query logs of one year of queries submitted to http://www.yahoo.co.uk from November 2005 to November 2006.",
                "In our logs, 50% of the total volume of queries are unique.",
                "The average query length is 2.5 terms, with the longest query having 731 terms. 1e-07 1e-06 1e-05 1e-04 0.001 0.01 0.1 1 1e-08 1e-07 1e-06 1e-05 1e-04 0.001 0.01 0.1 1 Frequency(normalized) Frequency rank (normalized) Figure 2: The distribution of queries (bottom curve) and query terms (middle curve) in the query log.",
                "The distribution of document frequencies of terms in the UK-2006 dataset (upper curve).",
                "Figure 2 shows the distributions of queries (lower curve), and query terms (middle curve).",
                "The x-axis represents the normalized frequency rank of the query or term. (The most frequent query appears closest to the y-axis.)",
                "The y-axis is Table 1: Statistics of the UK-2006 sample.",
                "UK-2006 sample statistics # of documents 2,786,391 # of terms 6,491,374 # of tokens 2,109,512,558 the normalized frequency for a given query (or term).",
                "As expected, the distribution of query frequencies and query term frequencies follow power law distributions, with slope of 1.84 and 2.26, respectively.",
                "In this figure, the query frequencies were computed as they appear in the logs with no normalization for case or white space.",
                "The query terms (middle curve) have been normalized for case, as have the terms in the document collection.",
                "The document collection that we use for our experiments is a summary of the UK domain crawled in May 2006.1 This summary corresponds to a maximum of 400 crawled documents per host, using a breadth first crawling strategy, comprising 15GB.",
                "The distribution of document frequencies of terms in the collection follows a power law distribution with slope 2.38 (upper curve in Figure 2).",
                "The statistics of the collection are shown in Table 1.",
                "We measured the correlation between the document frequency of terms in the collection and the number of queries that contain a particular term in the query log to be 0.424.",
                "A scatter plot for a random sample of terms is shown in Figure 3.",
                "In this experiment, terms have been converted to lower case in both the queries and the documents so that the frequencies will be comparable. 1e-07 1e-06 1e-05 1e-04 0.001 0.01 0.1 1 1e-06 1e-05 1e-04 0.001 0.01 0.1 1 Queryfrequency Document frequency Figure 3: Normalized scatter plot of document-term frequencies vs. query-term frequencies. 4.",
                "CACHING OF QUERIES AND TERMS Caching relies upon the assumption that there is locality in the stream of requests.",
                "That is, there must be sufficient repetition in the stream of requests and within intervals of time that enable a cache memory of reasonable size to be effective.",
                "In the query log we used, 88% of the unique queries are singleton queries, and 44% are singleton queries out of the whole volume.",
                "Thus, out of all queries in the stream composing the query log, the upper threshold on hit ratio is 56%.",
                "This is because only 56% of all the queries comprise queries that have multiple occurrences.",
                "It is important to observe, however, that not all queries in this 56% can be cache hits because of compulsory misses.",
                "A compulsory miss 1 The collection is available from the University of Milan: http://law.dsi.unimi.it/.",
                "URL retrieved 05/2007. 0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1 240 260 280 300 320 340 360 Numberofelements Bin number Total terms Terms diff Total queries Unique queries Unique terms Query diff Figure 4: Arrival rate for both terms and queries. happens when the cache receives a query for the first time.",
                "This is different from capacity misses, which happen due to space constraints on the amount of memory the cache uses.",
                "If we consider a cache with infinite memory, then the hit ratio is 50%.",
                "Note that for an infinite cache there are no capacity misses.",
                "As we mentioned before, another possibility is to cache the posting lists of terms.",
                "Intuitively, this gives more freedom in the utilization of the cache content to respond to queries because cached terms might form a new query.",
                "On the other hand, they need more space.",
                "As opposed to queries, the fraction of singleton terms in the total volume of terms is smaller.",
                "In our query log, only 4% of the terms appear once, but this accounts for 73% of the vocabulary of query terms.",
                "We show in Section 5 that caching a small fraction of terms, while accounting for terms appearing in many documents, is potentially very effective.",
                "Figure 4 shows several graphs corresponding to the normalized arrival rate for different cases using days as bins.",
                "That is, we plot the normalized number of elements that appear in a day.",
                "This graph shows only a period of 122 days, and we normalize the values by the maximum value observed throughout the whole period of the query log.",
                "Total queries and Total terms correspond to the total volume of queries and terms, respectively.",
                "Unique queries and Unique terms correspond to the arrival rate of unique queries and terms.",
                "Finally, Query diff and Terms diff correspond to the difference between the curves for total and unique.",
                "In Figure 4, as expected, the volume of terms is much higher than the volume of queries.",
                "The difference between the total number of terms and the number of unique terms is much larger than the difference between the total number of queries and the number of unique queries.",
                "This observation implies that terms repeat significantly more than queries.",
                "If we use smaller bins, say of one hour, then the ratio of unique to volume is higher for both terms and queries because it leaves less room for repetition.",
                "We also estimated the workload using the document frequency of terms as a measure of how much work a query imposes on a search engine.",
                "We found that it follows closely the arrival rate for terms shown in Figure 4.",
                "To demonstrate the effect of a dynamic cache on the query frequency distribution of Figure 2, we plot the same frequency graph, but now considering the frequency of queries Figure 5: Frequency graph after LRU cache. after going through an LRU cache.",
                "On a cache miss, an LRU cache decides upon an entry to evict using the information on the recency of queries.",
                "In this graph, the most frequent queries are not the same queries that were most frequent before the cache.",
                "It is possible that queries that are most frequent after the cache have different characteristics, and tuning the search engine to queries frequent before the cache may degrade performance for non-cached queries.",
                "The maximum frequency after caching is less than 1% of the maximum frequency before the cache, thus showing that the cache is very effective in reducing the load of frequent queries.",
                "If we re-rank the queries according to after-cache frequency, the distribution is still a power law, but with a much smaller value for the highest frequency.",
                "When discussing the effectiveness of dynamically caching, an important metric is cache miss rate.",
                "To analyze the cache miss rate for different memory constraints, we use the working set model [6, 14].",
                "A working set, informally, is the set of references that an application or an operating system is currently working with.",
                "The model uses such sets in a strategy that tries to capture the temporal locality of references.",
                "The working set strategy then consists in keeping in memory only the elements that are referenced in the previous θ steps of the input sequence, where θ is a configurable parameter corresponding to the window size.",
                "Originally, working sets have been used for page replacement algorithms of operating systems, and considering such a strategy in the context of search engines is interesting for three reasons.",
                "First, it captures the amount of locality of queries and terms in a sequence of queries.",
                "Locality in this case refers to the frequency of queries and terms in a window of time.",
                "If many queries appear multiple times in a window, then locality is high.",
                "Second, it enables an oﬄine analysis of the expected miss rate given different memory constraints.",
                "Third, working sets capture aspects of efficient caching algorithms such as LRU.",
                "LRU assumes that references farther in the past are less likely to be referenced in the present, which is implicit in the concept of working sets [14].",
                "Figure 6 plots the miss rate for different working set sizes, and we consider working sets of both queries and terms.",
                "The working set sizes are normalized against the total number of queries in the query log.",
                "In the graph for queries, there is a sharp decay until approximately 0.01, and the rate at which the miss rate drops decreases as we increase the size of the working set over 0.01.",
                "Finally, the minimum value it reaches is 50% miss rate, not shown in the figure as we have cut the tail of the curve for presentation purposes. 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1 0 0.05 0.1 0.15 0.2 Missrate Normalized working set size Queries Terms Figure 6: Miss rate as a function of the working set size. 1 10 100 1000 10000 100000 1e+06 Frequency Distance Figure 7: Distribution of distances expressed in terms of distinct queries.",
                "Compared to the query curve, we observe that the minimum miss rate for terms is substantially smaller.",
                "The miss rate also drops sharply on values up to 0.01, and it decreases minimally for higher values.",
                "The minimum value, however, is slightly over 10%, which is much smaller than the minimum value for the sequence of queries.",
                "This implies that with such a policy it is possible to achieve over 80% hit rate, if we consider caching dynamically posting lists for terms as opposed to caching answers for queries.",
                "This result does not consider the space required for each unit stored in the cache memory, or the amount of time it takes to put together a response to a user query.",
                "We analyze these issues more carefully later in this paper.",
                "It is interesting also to observe the histogram of Figure 7, which is an intermediate step in the computation of the miss rate graph.",
                "It reports the distribution of distances between repetitions of the same frequent query.",
                "The distance in the plot is measured in the number of distinct queries separating a query and its repetition, and it considers only queries appearing at least 10 times.",
                "From Figures 6 and 7, we conclude that even if we set the size of the query answers cache to a relatively large number of entries, the miss rate is high.",
                "Thus, caching the posting lists of terms has the potential to improve the hit ratio.",
                "This is what we explore next. 5.",
                "CACHING POSTING LISTS The previous section shows that <br>caching posting list</br>s can obtain a higher hit rate compared to caching query answers.",
                "In this section we study the problem of how to select posting lists to place on a certain amount of available memory, assuming that the whole index is larger than the amount of memory available.",
                "The posting lists have variable size (in fact, their size distribution follows a power law), so it is beneficial for a caching policy to consider the sizes of the posting lists.",
                "We consider both dynamic and static caching.",
                "For dynamic caching, we use two well-known policies, LRU and LFU, as well as a modified algorithm that takes posting-list size into account.",
                "Before discussing the static caching strategies, we introduce some notation.",
                "We use fq(t) to denote the query-term frequency of a term t, that is, the number of queries containing t in the query log, and fd(t) to denote the document frequency of t, that is, the number of documents in the collection in which the term t appears.",
                "The first strategy we consider is the algorithm proposed by Baeza-Yates and Saint-Jean [2], which consists in selecting the posting lists of the terms with the highest query-term frequencies fq(t).",
                "We call this algorithm Qtf.",
                "We observe that there is a trade-off between fq(t) and fd(t).",
                "Terms with high fq(t) are useful to keep in the cache because they are queried often.",
                "On the other hand, terms with high fd(t) are not good candidates because they correspond to long posting lists and consume a substantial amount of space.",
                "In fact, the problem of selecting the best posting lists for the static cache corresponds to the standard Knapsack problem: given a knapsack of fixed capacity, and a set of n items, such as the i-th item has value ci and size si, select the set of items that fit in the knapsack and maximize the overall value.",
                "In our case, value corresponds to fq(t) and size corresponds to fd(t).",
                "Thus, we employ a simple algorithm for the knapsack problem, which is selecting the posting lists of the terms with the highest values of the ratio fq(t) fd(t) .",
                "We call this algorithm QtfDf.",
                "We tried other variations considering query frequencies instead of term frequencies, but the gain was minimal compared to the complexity added.",
                "In addition to the above two static algorithms we consider the following algorithms for dynamic caching: • LRU: A standard LRU algorithm, but many posting lists might need to be evicted (in order of least-recent usage) until there is enough space in the memory to place the currently accessed posting list; • LFU: A standard LFU algorithm (eviction of the leastfrequently used), with the same modification as the LRU; • Dyn-QtfDf: A dynamic version of the QtfDf algorithm; evict from the cache the term(s) with the lowest fq(t) fd(t) ratio.",
                "The performance of all the above algorithms for 15 weeks of the query log and the UK dataset are shown in Figure 8.",
                "Performance is measured with hit rate.",
                "The cache size is measured as a fraction of the total space required to store the posting lists of all terms.",
                "For the dynamic algorithms, we load the cache with terms in order of fq(t) and we let the cache warm up for 1 million queries.",
                "For the static algorithms, we assume complete knowledge of the frequencies fq(t), that is, we estimate fq(t) from the whole query stream.",
                "As we show in Section 7 the results do not change much if we compute the query-term frequencies using the first 3 or 4 weeks of the query log and measure the hit rate on the rest. 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1 0.1 0.2 0.3 0.4 0.5 0.6 0.7 Hitrate Cache size Caching posting lists static QTF/DF LRU LFU Dyn-QTF/DF QTF Figure 8: Hit rate of different strategies for <br>caching posting list</br>s.",
                "The most important observation from our experiments is that the static QtfDf algorithm has a better hit rate than all the dynamic algorithms.",
                "An important benefit a static cache is that it requires no eviction and it is hence more efficient when evaluating queries.",
                "However, if the characteristics of the query traffic change frequently over time, then it requires re-populating the cache often or there will be a significant impact on hit rate. 6.",
                "ANALYSIS OF STATIC CACHING In this section we provide a detailed analysis for the problem of deciding whether it is preferable to cache query answers or cache posting lists.",
                "Our analysis takes into account the impact of caching between two levels of the data-access hierarchy.",
                "It can either be applied at the memory/disk layer or at a server/remote server layer as in the architecture we discussed in the introduction.",
                "Using a particular system model, we obtain estimates for the parameters required by our analysis, which we subsequently use to decide the optimal trade-off between caching query answers and <br>caching posting list</br>s. 6.1 Analytical Model Let M be the size of the cache measured in answer units (the cache can store M query answers).",
                "Assume that all posting lists are of the same length L, measured in answer units.",
                "We consider the following two cases: (A) a cache that stores only precomputed answers, and (B) a cache that stores only posting lists.",
                "In the first case, Nc = M answers fit in the cache, while in the second case Np = M/L posting lists fit in the cache.",
                "Thus, Np = Nc/L.",
                "Note that although posting lists require more space, we can combine terms to evaluate more queries (or partial queries).",
                "For case (A), suppose that a query answer in the cache can be evaluated in 1 time unit.",
                "For case (B), assume that if the posting lists of the terms of a query are in the cache then the results can be computed in TR1 time units, while if the posting lists are not in the cache then the results can be computed in TR2 time units.",
                "Of course TR2 > TR1.",
                "Now we want to compare the time to answer a stream of Q queries in both cases.",
                "Let Vc(Nc) be the volume of the most frequent Nc queries.",
                "Then, for case (A), we have an overall time TCA = Vc(Nc) + TR2(Q − Vc(Nc)).",
                "Similarly, for case (B), let Vp(Np) be the number of computable queries.",
                "Then we have overall time TP L = TR1Vp(Np) + TR2(Q − Vp(Np)).",
                "We want to check under which conditions we have TP L < TCA.",
                "We have TP L − TCA = (TR2 − 1)Vc(Nc) − (TR2 − TR1)Vp(Np) > 0.",
                "Figure 9 shows the values of Vp and Vc for our data.",
                "We can see that caching answers saturates faster and for this particular data there is no additional benefit from using more than 10% of the index space for caching answers.",
                "As the query distribution is a power law with parameter α > 1, the i-th most frequent query appears with probability proportional to 1 iα .",
                "Therefore, the volume Vc(n), which is the total number of the n most frequent queries, is Vc(n) = V0 n i=1 Q iα = γnQ (0 < γn < 1).",
                "We know that Vp(n) grows faster than Vc(n) and assume, based on experimental results, that the relation is of the form Vp(n) = k Vc(n)β .",
                "In the worst case, for a large cache, β → 1.",
                "That is, both techniques will cache a constant fraction of the overall query volume.",
                "Then <br>caching posting list</br>s makes sense only if L(TR2 − 1) k(TR2 − TR1) > 1.",
                "If we use compression, we have L < L and TR1 > TR1.",
                "According to the experiments that we show later, compression is always better.",
                "For a small cache, we are interested in the transient behavior and then β > 1, as computed from our data.",
                "In this case there will always be a point where TP L > TCA for a large number of queries.",
                "In reality, instead of filling the cache only with answers or only with posting lists, a better strategy will be to divide the total cache space into cache for answers and cache for posting lists.",
                "In such a case, there will be some queries that could be answered by both parts of the cache.",
                "As the answer cache is faster, it will be the first choice for answering those queries.",
                "Let QNc and QNp be the set of queries that can be answered by the cached answers and the cached posting lists, respectively.",
                "Then, the overall time is T = Vc(Nc)+TR1V (QNp −QNc )+TR2(Q−V (QNp ∪QNc )), where Np = (M − Nc)/L.",
                "Finding the optimal division of the cache in order to minimize the overall retrieval time is a difficult problem to solve analytically.",
                "In Section 6.3 we use simulations to derive optimal cache trade-offs for particular implementation examples. 6.2 Parameter Estimation We now use a particular implementation of a centralized system and the model of a distributed system as examples from which we estimate the parameters of the analysis from the previous section.",
                "We perform the experiments using an optimized version of Terrier [11] for both indexing documents and processing queries, on a single machine with a Pentium 4 at 2GHz and 1GB of RAM.",
                "We indexed the documents from the UK-2006 dataset, without removing stop words or applying stemming.",
                "The posting lists in the inverted file consist of pairs of document identifier and term frequency.",
                "We compress the document identifier gaps using Elias gamma encoding, and the 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1 0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1 Queryvolume Space precomputed answers posting lists Figure 9: Cache saturation as a function of size.",
                "Table 2: Ratios between the average time to evaluate a query and the average time to return cached answers (centralized and distributed case).",
                "Centralized system TR1 TR2 TR1 TR2 Full evaluation 233 1760 707 1140 Partial evaluation 99 1626 493 798 LAN system TRL 1 TRL 2 TR L 1 TR L 2 Full evaluation 242 1769 716 1149 Partial evaluation 108 1635 502 807 WAN system TRW 1 TRW 2 TR W 1 TR W 2 Full evaluation 5001 6528 5475 5908 Partial evaluation 4867 6394 5270 5575 term frequencies in documents using unary encoding [16].",
                "The size of the inverted file is 1,189Mb.",
                "A stored answer requires 1264 bytes, and an uncompressed posting takes 8 bytes.",
                "From Table 1, we obtain L = (8·# of postings) 1264·# of terms = 0.75 and L = Inverted file size 1264·# of terms = 0.26.",
                "We estimate the ratio TR = T/Tc between the average time T it takes to evaluate a query and the average time Tc it takes to return a stored answer for the same query, in the following way.",
                "Tc is measured by loading the answers for 100,000 queries in memory, and answering the queries from memory.",
                "The average time is Tc = 0.069ms.",
                "T is measured by processing the same 100,000 queries (the first 10,000 queries are used to warm-up the system).",
                "For each query, we remove stop words, if there are at least three remaining terms.",
                "The stop words correspond to the terms with a frequency higher than the number of documents in the index.",
                "We use a document-at-a-time approach to retrieve documents containing all query terms.",
                "The only disk access required during query processing is for reading compressed posting lists from the inverted file.",
                "We perform both full and partial evaluation of answers, because some queries are likely to retrieve a large number of documents, and only a fraction of the retrieved documents will be seen by users.",
                "In the partial evaluation of queries, we terminate the processing after matching 10,000 documents.",
                "The estimated ratios TR are presented in Table 2.",
                "Figure 10 shows for a sample of queries the workload of the system with partial query evaluation and compressed posting lists.",
                "The x-axis corresponds to the total time the system spends processing a particular query, and the vertical axis corresponds to the sum t∈q fq · fd(t).",
                "Notice that the total number of postings of the query-terms does not necessarily provide an accurate estimate of the workload imposed on the system by a query (which is the case for full evaluation and uncompressed lists). 0 0.2 0.4 0.6 0.8 1 0 0.2 0.4 0.6 0.8 1 Totalpostingstoprocessquery(normalized) Total time to process query (normalized) Partial processing of compressed postings query len = 1 query len in [2,3] query len in [4,8] query len > 8 Figure 10: Workload for partial query evaluation with compressed posting lists.",
                "The analysis of the previous section also applies to a distributed retrieval system in one or multiple sites.",
                "Suppose that a document partitioned distributed system is running on a cluster of machines interconnected with a local area network (LAN) in one site.",
                "The broker receives queries and broadcasts them to the query processors, which answer the queries and return the results to the broker.",
                "Finally, the broker merges the received answers and generates the final set of answers (we assume that the time spent on merging results is negligible).",
                "The difference between the centralized architecture and the document partition architecture is the extra communication between the broker and the query processors.",
                "Using ICMP pings on a 100Mbps LAN, we have measured that sending the query from the broker to the query processors which send an answer of 4,000 bytes back to the broker takes on average 0.615ms.",
                "Hence, TRL = TR + 0.615ms/0.069ms = TR + 9.",
                "In the case when the broker and the query processors are in different sites connected with a wide area network (WAN), we estimated that broadcasting the query from the broker to the query processors and getting back an answer of 4,000 bytes takes on average 329ms.",
                "Hence, TRW = TR + 329ms/0.069ms = TR + 4768. 6.3 Simulation Results We now address the problem of finding the optimal tradeoff between caching query answers and <br>caching posting list</br>s.",
                "To make the problem concrete we assume a fixed budget M on the available memory, out of which x units are used for caching query answers and M − x for <br>caching posting list</br>s.",
                "We perform simulations and compute the average response time as a function of x.",
                "Using a part of the query log as training data, we first allocate in the cache the answers to the most frequent queries that fit in space x, and then we use the rest of the memory to cache posting lists.",
                "For selecting posting lists we use the QtfDf algorithm, applied to the training query log but excluding the queries that have already been cached.",
                "In Figure 11, we plot the simulated response time for a centralized system as a function of x.",
                "For the uncompressed index we use M = 1GB, and for the compressed index we use M = 0.5GB.",
                "In the case of the configuration that uses partial query evaluation with compressed posting lists, the lowest response time is achieved when 0.15GB out of the 0.5GB is allocated for storing answers for queries.",
                "We obtained similar trends in the results for the LAN setting.",
                "Figure 12 shows the simulated workload for a distributed system across a WAN.",
                "In this case, the total amount of memory is split between the broker, which holds the cached 400 500 600 700 800 900 1000 1100 1200 0 0.2 0.4 0.6 0.8 1 Averageresponsetime Space (GB) Simulated workload -- single machine full / uncompr / 1 G partial / uncompr / 1 G full / compr / 0.5 G partial / compr / 0.5 G Figure 11: Optimal division of the cache in a server. 3000 3500 4000 4500 5000 5500 6000 0 0.2 0.4 0.6 0.8 1 Averageresponsetime Space (GB) Simulated workload -- WAN full / uncompr / 1 G partial / uncompr / 1 G full / compr / 0.5 G partial / compr / 0.5 G Figure 12: Optimal division of the cache when the next level requires WAN access. answers of queries, and the query processors, which hold the cache of posting lists.",
                "According to the figure, the difference between the configurations of the query processors is less important because the network communication overhead increases the response time substantially.",
                "When using uncompressed posting lists, the optimal allocation of memory corresponds to using approximately 70% of the memory for caching query answers.",
                "This is explained by the fact that there is no need for network communication when the query can be answered by the cache at the broker. 7.",
                "EFFECT OF THE QUERY DYNAMICS For our query log, the query distribution and query-term distribution change slowly over time.",
                "To support this claim, we first assess how topics change comparing the distribution of queries from the first week in June, 2006, to the distribution of queries for the remainder of 2006 that did not appear in the first week in June.",
                "We found that a very small percentage of queries are new queries.",
                "The majority of queries that appear in a given week repeat in the following weeks for the next six months.",
                "We then compute the hit rate of a static cache of 128, 000 answers trained over a period of two weeks (Figure 13).",
                "We report hit rate hourly for 7 days, starting from 5pm.",
                "We observe that the hit rate reaches its highest value during the night (around midnight), whereas around 2-3pm it reaches its minimum.",
                "After a small decay in hit rate values, the hit rate stabilizes between 0.28, and 0.34 for the entire week, suggesting that the static cache is effective for a whole week after the training period. 0.26 0.27 0.28 0.29 0.3 0.31 0.32 0.33 0.34 0.35 0.36 0.37 0 20 40 60 80 100 120 140 160 Hit-rate Time Hits on the frequent queries of distances Figure 13: Hourly hit rate for a static cache holding 128,000 answers during the period of a week.",
                "The static cache of posting lists can be periodically recomputed.",
                "To estimate the time interval in which we need to recompute the posting lists on the static cache we need to consider an efficiency/quality trade-off: using too short a time interval might be prohibitively expensive, while recomputing the cache too infrequently might lead to having an obsolete cache not corresponding to the statistical characteristics of the current query stream.",
                "We measured the effect on the QtfDf algorithm of the changes in a 15-week query stream (Figure 14).",
                "We compute the query term frequencies over the whole stream, select which terms to cache, and then compute the hit rate on the whole query stream.",
                "This hit rate is as an upper bound, and it assumes perfect knowledge of the query term frequencies.",
                "To simulate a realistic scenario, we use the first 6 (3) weeks of the query stream for computing query term frequencies and the following 9 (12) weeks to estimate the hit rate.",
                "As Figure 14 shows, the hit rate decreases by less than 2%.",
                "The high correlation among the query term frequencies during different time periods explains the graceful adaptation of the static caching algorithms to the future query stream.",
                "Indeed, the pairwise correlation among all possible 3-week periods of the 15-week query stream is over 99.5%. 8.",
                "CONCLUSIONS Caching is an effective technique in search engines for improving response time, reducing the load on query processors, and improving network bandwidth utilization.",
                "We present results on both dynamic and static caching.",
                "Dynamic caching of queries has limited effectiveness due to the high number of compulsory misses caused by the number of unique or infrequent queries.",
                "Our results show that in our UK log, the minimum miss rate is 50% using a working set strategy.",
                "Caching terms is more effective with respect to miss rate, achieving values as low as 12%.",
                "We also propose a new algorithm for static caching of posting lists that outperforms previous static caching algorithms as well as dynamic algorithms such as LRU and LFU, obtaining hit rate values that are over 10% higher compared these strategies.",
                "We present a framework for the analysis of the trade-off between caching query results and <br>caching posting list</br>s, and we simulate different types of architectures.",
                "Our results show that for centralized and LAN environments, there is an optimal allocation of caching query results and caching of posting lists, while for WAN scenarios in which network time prevails it is more important to cache query results. 0.45 0.5 0.55 0.6 0.65 0.7 0.75 0.8 0.85 0.9 0.95 0.1 0.2 0.3 0.4 0.5 0.6 0.7 Hitrate Cache size Dynamics of static QTF/DF caching policy perfect knowledge 6-week training 3-week training Figure 14: Impact of distribution changes on the static caching of posting lists. 9.",
                "REFERENCES [1] V. N. Anh and A. Moffat.",
                "Pruned query evaluation using pre-computed impacts.",
                "In ACM CIKM, 2006. [2] R. A. Baeza-Yates and F. Saint-Jean.",
                "A three level search engine index based in query log distribution.",
                "In SPIRE, 2003. [3] C. Buckley and A. F. Lewit.",
                "Optimization of inverted vector searches.",
                "In ACM SIGIR, 1985. [4] S. B¨uttcher and C. L. A. Clarke.",
                "A document-centric approach to static index pruning in text retrieval systems.",
                "In ACM CIKM, 2006. [5] P. Cao and S. Irani.",
                "Cost-aware WWW proxy caching algorithms.",
                "In USITS, 1997. [6] P. Denning.",
                "Working sets past and present.",
                "IEEE Trans. on Software Engineering, SE-6(1):64-84, 1980. [7] T. Fagni, R. Perego, F. Silvestri, and S. Orlando.",
                "Boosting the performance of web search engines: Caching and prefetching query results by exploiting historical usage data.",
                "ACM Trans.",
                "Inf.",
                "Syst., 24(1):51-78, 2006. [8] R. Lempel and S. Moran.",
                "Predictive caching and prefetching of query results in search engines.",
                "In WWW, 2003. [9] X.",
                "Long and T. Suel.",
                "Three-level caching for efficient query processing in large web search engines.",
                "In WWW, 2005. [10] E. P. Markatos.",
                "On caching search engine query results.",
                "Computer Communications, 24(2):137-143, 2001. [11] I. Ounis, G. Amati, V. Plachouras, B.",
                "He, C. Macdonald, and C. Lioma.",
                "Terrier: A High Performance and Scalable Information Retrieval Platform.",
                "In SIGIR Workshop on Open Source Information Retrieval, 2006. [12] V. V. Raghavan and H. Sever.",
                "On the reuse of past optimal queries.",
                "In ACM SIGIR, 1995. [13] P. C. Saraiva, E. S. de Moura, N. Ziviani, W. Meira, R. Fonseca, and B. Riberio-Neto.",
                "Rank-preserving two-level caching for scalable search engines.",
                "In ACM SIGIR, 2001. [14] D. R. Slutz and I. L. Traiger.",
                "A note on the calculation of average working set size.",
                "Communications of the ACM, 17(10):563-565, 1974. [15] T. Strohman, H. Turtle, and W. B. Croft.",
                "Optimization strategies for complex queries.",
                "In ACM SIGIR, 2005. [16] I. H. Witten, T. C. Bell, and A. Moffat.",
                "Managing Gigabytes: Compressing and Indexing Documents and Images.",
                "John Wiley & Sons, Inc., NY, 1994. [17] N. E. Young.",
                "On-line file caching.",
                "Algorithmica, 33(3):371-383, 2002."
            ],
            "original_annotated_samples": [],
            "translated_annotated_samples": [
                "Exploramos el impacto de diferentes enfoques, como el almacenamiento en caché estático frente a la dinámica, y los resultados de la consulta de almacenamiento en caché frente a la \"lista de publicación de almacenamiento en caché\" s.",
                "Usando un registro de consultas que abarca todo un año, exploramos las limitaciones del almacenamiento en caché y demostramos que la \"lista de publicación de almacenamiento en caché\" puede lograr tasas de aciertos más altas que las respuestas de consulta en caché.",
                "Broker estático \"Lista de publicación de caché\" S Dinámica/estática Respuestas almacenadas en caché Procesador de consultas locales Disco Nivel de almacenamiento en caché Acceso a la red remota Acceso a la red remota Figura 1: Un nivel de almacenamiento en caché en una arquitectura de búsqueda distribuida.",
                "Las secciones 5 y 6 introducen algoritmos para la \"lista de publicaciones en caché\", y un marco teórico para el análisis del almacenamiento en caché estático, respectivamente.",
                "Las listas de publicaciones de almacenamiento en caché La sección anterior muestra que la \"Lista de publicación de almacenamiento en caché\" s puede obtener una tasa de éxito más alta en comparación con las respuestas de la consulta en caché.",
                "Como mostramos en la Sección 7, los resultados no cambian mucho si calculamos las frecuencias a plazo de consulta utilizando las primeras 3 o 4 semanas del registro de la consulta y medimos la tasa de aciertos en el resto.0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1 0.1 0.2 0.3 0.4 0.5 0.6 0.7 HITRADO Tamaño de caché Publicación de caché de la publicación Listas de Publicación estática QTF/DF LRU LFU DYN-QTF/DF QTF Figura 8: Tasa de aciertos de diferentes estrategias para \"Lista de publicaciones en caché\" s.",
                "Usando un modelo de sistema en particular, obtenemos estimaciones para los parámetros requeridos por nuestro análisis, que posteriormente usamos para decidir la compensación óptima entre las respuestas de consulta en caché y la \"lista de publicación de caché\" s.6.1 Modelo analítico Sea m del tamaño del caché medido en las unidades de respuestas (el caché puede almacenar m respuestas de consulta).",
                "Luego, \"Lista de publicación de almacenamiento en caché\" s tiene sentido solo si L (Tr2 - 1) K (TR2 - TR1)> 1.",
                "Por lo tanto, TRW = TR + 329MS/0.069MS = TR + 4768. 6.3 Resultados de simulación Ahora abordamos el problema de encontrar la compensación óptima entre las respuestas de la consulta de almacenamiento en caché y la \"lista de publicaciones de almacenamiento en caché\" s.",
                "Para hacer el problema concreto, asumimos un presupuesto fijo en la memoria disponible, de la cual se utilizan X unidades para almacenar en caché las respuestas de consulta y M - x para la \"lista de publicaciones en caché\" s.",
                "Presentamos un marco para el análisis de la compensación entre los resultados de las consultas de almacenamiento en caché y la \"lista de publicaciones de almacenamiento en caché\", y simulamos diferentes tipos de arquitecturas."
            ],
            "translated_text": "",
            "candidates": [
                "Lista de publicaciones de almacenamiento en caché",
                "lista de publicación de almacenamiento en caché",
                "Lista de publicaciones de almacenamiento en caché",
                "lista de publicación de almacenamiento en caché",
                "Lista de publicaciones de almacenamiento en caché",
                "Lista de publicación de caché",
                "Lista de publicaciones de almacenamiento en caché",
                "lista de publicaciones en caché",
                "Lista de publicaciones de almacenamiento en caché",
                "Lista de publicación de almacenamiento en caché",
                "Lista de publicaciones de almacenamiento en caché",
                "Lista de publicaciones en caché",
                "Lista de publicaciones de almacenamiento en caché",
                "lista de publicación de caché",
                "Lista de publicación de almacenamiento en caché",
                "Lista de publicación de almacenamiento en caché",
                "Lista de publicaciones de almacenamiento en caché",
                "lista de publicaciones de almacenamiento en caché",
                "Lista de publicaciones de almacenamiento en caché",
                "lista de publicaciones en caché",
                "Lista de publicaciones de almacenamiento en caché",
                "lista de publicaciones de almacenamiento en caché"
            ],
            "error": []
        },
        "static cache": {
            "translated_key": "caché estático",
            "is_in_text": true,
            "original_annotated_sentences": [
                "The Impact of Caching on Search Engines Ricardo Baeza-Yates1 rbaeza@acm.org Aristides Gionis1 gionis@yahoo-inc.com Flavio Junqueira1 fpj@yahoo-inc.com Vanessa Murdock1 vmurdock@yahoo-inc.com Vassilis Plachouras1 vassilis@yahoo-inc.com Fabrizio Silvestri2 f.silvestri@isti.cnr.it 1 Yahoo!",
                "Research Barcelona 2 ISTI - CNR Barcelona, SPAIN Pisa, ITALY ABSTRACT In this paper we study the trade-offs in designing efficient caching systems for Web search engines.",
                "We explore the impact of different approaches, such as static vs. dynamic caching, and caching query results vs. caching posting lists.",
                "Using a query log spanning a whole year we explore the limitations of caching and we demonstrate that caching posting lists can achieve higher hit rates than caching query answers.",
                "We propose a new algorithm for static caching of posting lists, which outperforms previous methods.",
                "We also study the problem of finding the optimal way to split the <br>static cache</br> between answers and posting lists.",
                "Finally, we measure how the changes in the query log affect the effectiveness of static caching, given our observation that the distribution of the queries changes slowly over time.",
                "Our results and observations are applicable to different levels of the data-access hierarchy, for instance, for a memory/disk layer or a broker/remote server layer.",
                "Categories and Subject Descriptors H.3.3 [Information Storage and Retrieval]: Information Search and Retrieval - Search process; H.3.4 [Information Storage and Retrieval]: Systems and Software - Distributed systems, Performance evaluation (efficiency and effectiveness) General Terms Algorithms, Experimentation 1.",
                "INTRODUCTION Millions of queries are submitted daily to Web search engines, and users have high expectations of the quality and speed of the answers.",
                "As the searchable Web becomes larger and larger, with more than 20 billion pages to index, evaluating a single query requires processing large amounts of data.",
                "In such a setting, to achieve a fast response time and to increase the query throughput, using a cache is crucial.",
                "The primary use of a cache memory is to speedup computation by exploiting frequently or recently used data, although reducing the workload to back-end servers is also a major goal.",
                "Caching can be applied at different levels with increasing response latencies or processing requirements.",
                "For example, the different levels may correspond to the main memory, the disk, or resources in a local or a wide area network.",
                "The decision of what to cache is either off-line (static) or online (dynamic).",
                "A <br>static cache</br> is based on historical information and is periodically updated.",
                "A dynamic cache replaces entries according to the sequence of requests.",
                "When a new request arrives, the cache system decides whether to evict some entry from the cache in the case of a cache miss.",
                "Such online decisions are based on a cache policy, and several different policies have been studied in the past.",
                "For a search engine, there are two possible ways to use a cache memory: Caching answers: As the engine returns answers to a particular query, it may decide to store these answers to resolve future queries.",
                "Caching terms: As the engine evaluates a particular query, it may decide to store in memory the posting lists of the involved query terms.",
                "Often the whole set of posting lists does not fit in memory, and consequently, the engine has to select a small set to keep in memory and speed up query processing.",
                "Returning an answer to a query that already exists in the cache is more efficient than computing the answer using cached posting lists.",
                "On the other hand, previously unseen queries occur more often than previously unseen terms, implying a higher miss rate for cached answers.",
                "Caching of posting lists has additional challenges.",
                "As posting lists have variable size, caching them dynamically is not very efficient, due to the complexity in terms of efficiency and space, and the skewed distribution of the query stream, as shown later.",
                "Static caching of posting lists poses even more challenges: when deciding which terms to cache one faces the trade-off between frequently queried terms and terms with small posting lists that are space efficient.",
                "Finally, before deciding to adopt a static caching policy the query stream should be analyzed to verify that its characteristics do not change rapidly over time.",
                "Broker Static caching posting lists Dynamic/Static cached answers Local query processor Disk Next caching level Local network access Remote network access Figure 1: One caching level in a distributed search architecture.",
                "In this paper we explore the trade-offs in the design of each cache level, showing that the problem is the same and only a few parameters change.",
                "In general, we assume that each level of caching in a distributed search architecture is similar to that shown in Figure 1.",
                "We use a query log spanning a whole year to explore the limitations of dynamically caching query answers or posting lists for query terms.",
                "More concretely, our main conclusions are that: • Caching query answers results in lower hit ratios compared to caching of posting lists for query terms, but it is faster because there is no need for query evaluation.",
                "We provide a framework for the analysis of the trade-off between static caching of query answers and posting lists; • Static caching of terms can be more effective than dynamic caching with, for example, LRU.",
                "We provide algorithms based on the Knapsack problem for selecting the posting lists to put in a <br>static cache</br>, and we show improvements over previous work, achieving a hit ratio over 90%; • Changes of the query distribution over time have little impact on static caching.",
                "The remainder of this paper is organized as follows.",
                "Sections 2 and 3 summarize related work and characterize the data sets we use.",
                "Section 4 discusses the limitations of dynamic caching.",
                "Sections 5 and 6 introduce algorithms for caching posting lists, and a theoretical framework for the analysis of static caching, respectively.",
                "Section 7 discusses the impact of changes in the query distribution on static caching, and Section 8 provides concluding remarks. 2.",
                "RELATED WORK There is a large body of work devoted to query optimization.",
                "Buckley and Lewit [3], in one of the earliest works, take a term-at-a-time approach to deciding when inverted lists need not be further examined.",
                "More recent examples demonstrate that the top k documents for a query can be returned without the need for evaluating the complete set of posting lists [1, 4, 15].",
                "Although these approaches seek to improve query processing efficiency, they differ from our current work in that they do not consider caching.",
                "They may be considered separate and complementary to a cache-based approach.",
                "Raghavan and Sever [12], in one of the first papers on exploiting user query history, propose using a query base, built upon a set of persistent optimal queries submitted in the past, to improve the retrieval effectiveness for similar future queries.",
                "Markatos [10] shows the existence of temporal locality in queries, and compares the performance of different caching policies.",
                "Based on the observations of Markatos, Lempel and Moran propose a new caching policy, called Probabilistic Driven Caching, by attempting to estimate the probability distribution of all possible queries submitted to a search engine [8].",
                "Fagni et al. follow Markatos work by showing that combining static and dynamic caching policies together with an adaptive prefetching policy achieves a high hit ratio [7].",
                "Different from our work, they consider caching and prefetching of pages of results.",
                "As systems are often hierarchical, there has also been some effort on multi-level architectures.",
                "Saraiva et al. propose a new architecture for Web search engines using a two-level dynamic caching system [13].",
                "Their goal for such systems has been to improve response time for hierarchical engines.",
                "In their architecture, both levels use an LRU eviction policy.",
                "They find that the second-level cache can effectively reduce disk traffic, thus increasing the overall throughput.",
                "Baeza-Yates and Saint-Jean propose a three-level index organization [2].",
                "Long and Suel propose a caching system structured according to three different levels [9].",
                "The intermediate level contains frequently occurring pairs of terms and stores the intersections of the corresponding inverted lists.",
                "These last two papers are related to ours in that they exploit different caching strategies at different levels of the memory hierarchy.",
                "Finally, our static caching algorithm for posting lists in Section 5 uses the ratio frequency/size in order to evaluate the goodness of an item to cache.",
                "Similar ideas have been used in the context of file caching [17], Web caching [5], and even caching of posting lists [9], but in all cases in a dynamic setting.",
                "To the best of our knowledge we are the first to use this approach for static caching of posting lists. 3.",
                "DATA CHARACTERIZATION Our data consists of a crawl of documents from the UK domain, and query logs of one year of queries submitted to http://www.yahoo.co.uk from November 2005 to November 2006.",
                "In our logs, 50% of the total volume of queries are unique.",
                "The average query length is 2.5 terms, with the longest query having 731 terms. 1e-07 1e-06 1e-05 1e-04 0.001 0.01 0.1 1 1e-08 1e-07 1e-06 1e-05 1e-04 0.001 0.01 0.1 1 Frequency(normalized) Frequency rank (normalized) Figure 2: The distribution of queries (bottom curve) and query terms (middle curve) in the query log.",
                "The distribution of document frequencies of terms in the UK-2006 dataset (upper curve).",
                "Figure 2 shows the distributions of queries (lower curve), and query terms (middle curve).",
                "The x-axis represents the normalized frequency rank of the query or term. (The most frequent query appears closest to the y-axis.)",
                "The y-axis is Table 1: Statistics of the UK-2006 sample.",
                "UK-2006 sample statistics # of documents 2,786,391 # of terms 6,491,374 # of tokens 2,109,512,558 the normalized frequency for a given query (or term).",
                "As expected, the distribution of query frequencies and query term frequencies follow power law distributions, with slope of 1.84 and 2.26, respectively.",
                "In this figure, the query frequencies were computed as they appear in the logs with no normalization for case or white space.",
                "The query terms (middle curve) have been normalized for case, as have the terms in the document collection.",
                "The document collection that we use for our experiments is a summary of the UK domain crawled in May 2006.1 This summary corresponds to a maximum of 400 crawled documents per host, using a breadth first crawling strategy, comprising 15GB.",
                "The distribution of document frequencies of terms in the collection follows a power law distribution with slope 2.38 (upper curve in Figure 2).",
                "The statistics of the collection are shown in Table 1.",
                "We measured the correlation between the document frequency of terms in the collection and the number of queries that contain a particular term in the query log to be 0.424.",
                "A scatter plot for a random sample of terms is shown in Figure 3.",
                "In this experiment, terms have been converted to lower case in both the queries and the documents so that the frequencies will be comparable. 1e-07 1e-06 1e-05 1e-04 0.001 0.01 0.1 1 1e-06 1e-05 1e-04 0.001 0.01 0.1 1 Queryfrequency Document frequency Figure 3: Normalized scatter plot of document-term frequencies vs. query-term frequencies. 4.",
                "CACHING OF QUERIES AND TERMS Caching relies upon the assumption that there is locality in the stream of requests.",
                "That is, there must be sufficient repetition in the stream of requests and within intervals of time that enable a cache memory of reasonable size to be effective.",
                "In the query log we used, 88% of the unique queries are singleton queries, and 44% are singleton queries out of the whole volume.",
                "Thus, out of all queries in the stream composing the query log, the upper threshold on hit ratio is 56%.",
                "This is because only 56% of all the queries comprise queries that have multiple occurrences.",
                "It is important to observe, however, that not all queries in this 56% can be cache hits because of compulsory misses.",
                "A compulsory miss 1 The collection is available from the University of Milan: http://law.dsi.unimi.it/.",
                "URL retrieved 05/2007. 0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1 240 260 280 300 320 340 360 Numberofelements Bin number Total terms Terms diff Total queries Unique queries Unique terms Query diff Figure 4: Arrival rate for both terms and queries. happens when the cache receives a query for the first time.",
                "This is different from capacity misses, which happen due to space constraints on the amount of memory the cache uses.",
                "If we consider a cache with infinite memory, then the hit ratio is 50%.",
                "Note that for an infinite cache there are no capacity misses.",
                "As we mentioned before, another possibility is to cache the posting lists of terms.",
                "Intuitively, this gives more freedom in the utilization of the cache content to respond to queries because cached terms might form a new query.",
                "On the other hand, they need more space.",
                "As opposed to queries, the fraction of singleton terms in the total volume of terms is smaller.",
                "In our query log, only 4% of the terms appear once, but this accounts for 73% of the vocabulary of query terms.",
                "We show in Section 5 that caching a small fraction of terms, while accounting for terms appearing in many documents, is potentially very effective.",
                "Figure 4 shows several graphs corresponding to the normalized arrival rate for different cases using days as bins.",
                "That is, we plot the normalized number of elements that appear in a day.",
                "This graph shows only a period of 122 days, and we normalize the values by the maximum value observed throughout the whole period of the query log.",
                "Total queries and Total terms correspond to the total volume of queries and terms, respectively.",
                "Unique queries and Unique terms correspond to the arrival rate of unique queries and terms.",
                "Finally, Query diff and Terms diff correspond to the difference between the curves for total and unique.",
                "In Figure 4, as expected, the volume of terms is much higher than the volume of queries.",
                "The difference between the total number of terms and the number of unique terms is much larger than the difference between the total number of queries and the number of unique queries.",
                "This observation implies that terms repeat significantly more than queries.",
                "If we use smaller bins, say of one hour, then the ratio of unique to volume is higher for both terms and queries because it leaves less room for repetition.",
                "We also estimated the workload using the document frequency of terms as a measure of how much work a query imposes on a search engine.",
                "We found that it follows closely the arrival rate for terms shown in Figure 4.",
                "To demonstrate the effect of a dynamic cache on the query frequency distribution of Figure 2, we plot the same frequency graph, but now considering the frequency of queries Figure 5: Frequency graph after LRU cache. after going through an LRU cache.",
                "On a cache miss, an LRU cache decides upon an entry to evict using the information on the recency of queries.",
                "In this graph, the most frequent queries are not the same queries that were most frequent before the cache.",
                "It is possible that queries that are most frequent after the cache have different characteristics, and tuning the search engine to queries frequent before the cache may degrade performance for non-cached queries.",
                "The maximum frequency after caching is less than 1% of the maximum frequency before the cache, thus showing that the cache is very effective in reducing the load of frequent queries.",
                "If we re-rank the queries according to after-cache frequency, the distribution is still a power law, but with a much smaller value for the highest frequency.",
                "When discussing the effectiveness of dynamically caching, an important metric is cache miss rate.",
                "To analyze the cache miss rate for different memory constraints, we use the working set model [6, 14].",
                "A working set, informally, is the set of references that an application or an operating system is currently working with.",
                "The model uses such sets in a strategy that tries to capture the temporal locality of references.",
                "The working set strategy then consists in keeping in memory only the elements that are referenced in the previous θ steps of the input sequence, where θ is a configurable parameter corresponding to the window size.",
                "Originally, working sets have been used for page replacement algorithms of operating systems, and considering such a strategy in the context of search engines is interesting for three reasons.",
                "First, it captures the amount of locality of queries and terms in a sequence of queries.",
                "Locality in this case refers to the frequency of queries and terms in a window of time.",
                "If many queries appear multiple times in a window, then locality is high.",
                "Second, it enables an oﬄine analysis of the expected miss rate given different memory constraints.",
                "Third, working sets capture aspects of efficient caching algorithms such as LRU.",
                "LRU assumes that references farther in the past are less likely to be referenced in the present, which is implicit in the concept of working sets [14].",
                "Figure 6 plots the miss rate for different working set sizes, and we consider working sets of both queries and terms.",
                "The working set sizes are normalized against the total number of queries in the query log.",
                "In the graph for queries, there is a sharp decay until approximately 0.01, and the rate at which the miss rate drops decreases as we increase the size of the working set over 0.01.",
                "Finally, the minimum value it reaches is 50% miss rate, not shown in the figure as we have cut the tail of the curve for presentation purposes. 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1 0 0.05 0.1 0.15 0.2 Missrate Normalized working set size Queries Terms Figure 6: Miss rate as a function of the working set size. 1 10 100 1000 10000 100000 1e+06 Frequency Distance Figure 7: Distribution of distances expressed in terms of distinct queries.",
                "Compared to the query curve, we observe that the minimum miss rate for terms is substantially smaller.",
                "The miss rate also drops sharply on values up to 0.01, and it decreases minimally for higher values.",
                "The minimum value, however, is slightly over 10%, which is much smaller than the minimum value for the sequence of queries.",
                "This implies that with such a policy it is possible to achieve over 80% hit rate, if we consider caching dynamically posting lists for terms as opposed to caching answers for queries.",
                "This result does not consider the space required for each unit stored in the cache memory, or the amount of time it takes to put together a response to a user query.",
                "We analyze these issues more carefully later in this paper.",
                "It is interesting also to observe the histogram of Figure 7, which is an intermediate step in the computation of the miss rate graph.",
                "It reports the distribution of distances between repetitions of the same frequent query.",
                "The distance in the plot is measured in the number of distinct queries separating a query and its repetition, and it considers only queries appearing at least 10 times.",
                "From Figures 6 and 7, we conclude that even if we set the size of the query answers cache to a relatively large number of entries, the miss rate is high.",
                "Thus, caching the posting lists of terms has the potential to improve the hit ratio.",
                "This is what we explore next. 5.",
                "CACHING POSTING LISTS The previous section shows that caching posting lists can obtain a higher hit rate compared to caching query answers.",
                "In this section we study the problem of how to select posting lists to place on a certain amount of available memory, assuming that the whole index is larger than the amount of memory available.",
                "The posting lists have variable size (in fact, their size distribution follows a power law), so it is beneficial for a caching policy to consider the sizes of the posting lists.",
                "We consider both dynamic and static caching.",
                "For dynamic caching, we use two well-known policies, LRU and LFU, as well as a modified algorithm that takes posting-list size into account.",
                "Before discussing the static caching strategies, we introduce some notation.",
                "We use fq(t) to denote the query-term frequency of a term t, that is, the number of queries containing t in the query log, and fd(t) to denote the document frequency of t, that is, the number of documents in the collection in which the term t appears.",
                "The first strategy we consider is the algorithm proposed by Baeza-Yates and Saint-Jean [2], which consists in selecting the posting lists of the terms with the highest query-term frequencies fq(t).",
                "We call this algorithm Qtf.",
                "We observe that there is a trade-off between fq(t) and fd(t).",
                "Terms with high fq(t) are useful to keep in the cache because they are queried often.",
                "On the other hand, terms with high fd(t) are not good candidates because they correspond to long posting lists and consume a substantial amount of space.",
                "In fact, the problem of selecting the best posting lists for the <br>static cache</br> corresponds to the standard Knapsack problem: given a knapsack of fixed capacity, and a set of n items, such as the i-th item has value ci and size si, select the set of items that fit in the knapsack and maximize the overall value.",
                "In our case, value corresponds to fq(t) and size corresponds to fd(t).",
                "Thus, we employ a simple algorithm for the knapsack problem, which is selecting the posting lists of the terms with the highest values of the ratio fq(t) fd(t) .",
                "We call this algorithm QtfDf.",
                "We tried other variations considering query frequencies instead of term frequencies, but the gain was minimal compared to the complexity added.",
                "In addition to the above two static algorithms we consider the following algorithms for dynamic caching: • LRU: A standard LRU algorithm, but many posting lists might need to be evicted (in order of least-recent usage) until there is enough space in the memory to place the currently accessed posting list; • LFU: A standard LFU algorithm (eviction of the leastfrequently used), with the same modification as the LRU; • Dyn-QtfDf: A dynamic version of the QtfDf algorithm; evict from the cache the term(s) with the lowest fq(t) fd(t) ratio.",
                "The performance of all the above algorithms for 15 weeks of the query log and the UK dataset are shown in Figure 8.",
                "Performance is measured with hit rate.",
                "The cache size is measured as a fraction of the total space required to store the posting lists of all terms.",
                "For the dynamic algorithms, we load the cache with terms in order of fq(t) and we let the cache warm up for 1 million queries.",
                "For the static algorithms, we assume complete knowledge of the frequencies fq(t), that is, we estimate fq(t) from the whole query stream.",
                "As we show in Section 7 the results do not change much if we compute the query-term frequencies using the first 3 or 4 weeks of the query log and measure the hit rate on the rest. 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1 0.1 0.2 0.3 0.4 0.5 0.6 0.7 Hitrate Cache size Caching posting lists static QTF/DF LRU LFU Dyn-QTF/DF QTF Figure 8: Hit rate of different strategies for caching posting lists.",
                "The most important observation from our experiments is that the static QtfDf algorithm has a better hit rate than all the dynamic algorithms.",
                "An important benefit a <br>static cache</br> is that it requires no eviction and it is hence more efficient when evaluating queries.",
                "However, if the characteristics of the query traffic change frequently over time, then it requires re-populating the cache often or there will be a significant impact on hit rate. 6.",
                "ANALYSIS OF STATIC CACHING In this section we provide a detailed analysis for the problem of deciding whether it is preferable to cache query answers or cache posting lists.",
                "Our analysis takes into account the impact of caching between two levels of the data-access hierarchy.",
                "It can either be applied at the memory/disk layer or at a server/remote server layer as in the architecture we discussed in the introduction.",
                "Using a particular system model, we obtain estimates for the parameters required by our analysis, which we subsequently use to decide the optimal trade-off between caching query answers and caching posting lists. 6.1 Analytical Model Let M be the size of the cache measured in answer units (the cache can store M query answers).",
                "Assume that all posting lists are of the same length L, measured in answer units.",
                "We consider the following two cases: (A) a cache that stores only precomputed answers, and (B) a cache that stores only posting lists.",
                "In the first case, Nc = M answers fit in the cache, while in the second case Np = M/L posting lists fit in the cache.",
                "Thus, Np = Nc/L.",
                "Note that although posting lists require more space, we can combine terms to evaluate more queries (or partial queries).",
                "For case (A), suppose that a query answer in the cache can be evaluated in 1 time unit.",
                "For case (B), assume that if the posting lists of the terms of a query are in the cache then the results can be computed in TR1 time units, while if the posting lists are not in the cache then the results can be computed in TR2 time units.",
                "Of course TR2 > TR1.",
                "Now we want to compare the time to answer a stream of Q queries in both cases.",
                "Let Vc(Nc) be the volume of the most frequent Nc queries.",
                "Then, for case (A), we have an overall time TCA = Vc(Nc) + TR2(Q − Vc(Nc)).",
                "Similarly, for case (B), let Vp(Np) be the number of computable queries.",
                "Then we have overall time TP L = TR1Vp(Np) + TR2(Q − Vp(Np)).",
                "We want to check under which conditions we have TP L < TCA.",
                "We have TP L − TCA = (TR2 − 1)Vc(Nc) − (TR2 − TR1)Vp(Np) > 0.",
                "Figure 9 shows the values of Vp and Vc for our data.",
                "We can see that caching answers saturates faster and for this particular data there is no additional benefit from using more than 10% of the index space for caching answers.",
                "As the query distribution is a power law with parameter α > 1, the i-th most frequent query appears with probability proportional to 1 iα .",
                "Therefore, the volume Vc(n), which is the total number of the n most frequent queries, is Vc(n) = V0 n i=1 Q iα = γnQ (0 < γn < 1).",
                "We know that Vp(n) grows faster than Vc(n) and assume, based on experimental results, that the relation is of the form Vp(n) = k Vc(n)β .",
                "In the worst case, for a large cache, β → 1.",
                "That is, both techniques will cache a constant fraction of the overall query volume.",
                "Then caching posting lists makes sense only if L(TR2 − 1) k(TR2 − TR1) > 1.",
                "If we use compression, we have L < L and TR1 > TR1.",
                "According to the experiments that we show later, compression is always better.",
                "For a small cache, we are interested in the transient behavior and then β > 1, as computed from our data.",
                "In this case there will always be a point where TP L > TCA for a large number of queries.",
                "In reality, instead of filling the cache only with answers or only with posting lists, a better strategy will be to divide the total cache space into cache for answers and cache for posting lists.",
                "In such a case, there will be some queries that could be answered by both parts of the cache.",
                "As the answer cache is faster, it will be the first choice for answering those queries.",
                "Let QNc and QNp be the set of queries that can be answered by the cached answers and the cached posting lists, respectively.",
                "Then, the overall time is T = Vc(Nc)+TR1V (QNp −QNc )+TR2(Q−V (QNp ∪QNc )), where Np = (M − Nc)/L.",
                "Finding the optimal division of the cache in order to minimize the overall retrieval time is a difficult problem to solve analytically.",
                "In Section 6.3 we use simulations to derive optimal cache trade-offs for particular implementation examples. 6.2 Parameter Estimation We now use a particular implementation of a centralized system and the model of a distributed system as examples from which we estimate the parameters of the analysis from the previous section.",
                "We perform the experiments using an optimized version of Terrier [11] for both indexing documents and processing queries, on a single machine with a Pentium 4 at 2GHz and 1GB of RAM.",
                "We indexed the documents from the UK-2006 dataset, without removing stop words or applying stemming.",
                "The posting lists in the inverted file consist of pairs of document identifier and term frequency.",
                "We compress the document identifier gaps using Elias gamma encoding, and the 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1 0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1 Queryvolume Space precomputed answers posting lists Figure 9: Cache saturation as a function of size.",
                "Table 2: Ratios between the average time to evaluate a query and the average time to return cached answers (centralized and distributed case).",
                "Centralized system TR1 TR2 TR1 TR2 Full evaluation 233 1760 707 1140 Partial evaluation 99 1626 493 798 LAN system TRL 1 TRL 2 TR L 1 TR L 2 Full evaluation 242 1769 716 1149 Partial evaluation 108 1635 502 807 WAN system TRW 1 TRW 2 TR W 1 TR W 2 Full evaluation 5001 6528 5475 5908 Partial evaluation 4867 6394 5270 5575 term frequencies in documents using unary encoding [16].",
                "The size of the inverted file is 1,189Mb.",
                "A stored answer requires 1264 bytes, and an uncompressed posting takes 8 bytes.",
                "From Table 1, we obtain L = (8·# of postings) 1264·# of terms = 0.75 and L = Inverted file size 1264·# of terms = 0.26.",
                "We estimate the ratio TR = T/Tc between the average time T it takes to evaluate a query and the average time Tc it takes to return a stored answer for the same query, in the following way.",
                "Tc is measured by loading the answers for 100,000 queries in memory, and answering the queries from memory.",
                "The average time is Tc = 0.069ms.",
                "T is measured by processing the same 100,000 queries (the first 10,000 queries are used to warm-up the system).",
                "For each query, we remove stop words, if there are at least three remaining terms.",
                "The stop words correspond to the terms with a frequency higher than the number of documents in the index.",
                "We use a document-at-a-time approach to retrieve documents containing all query terms.",
                "The only disk access required during query processing is for reading compressed posting lists from the inverted file.",
                "We perform both full and partial evaluation of answers, because some queries are likely to retrieve a large number of documents, and only a fraction of the retrieved documents will be seen by users.",
                "In the partial evaluation of queries, we terminate the processing after matching 10,000 documents.",
                "The estimated ratios TR are presented in Table 2.",
                "Figure 10 shows for a sample of queries the workload of the system with partial query evaluation and compressed posting lists.",
                "The x-axis corresponds to the total time the system spends processing a particular query, and the vertical axis corresponds to the sum t∈q fq · fd(t).",
                "Notice that the total number of postings of the query-terms does not necessarily provide an accurate estimate of the workload imposed on the system by a query (which is the case for full evaluation and uncompressed lists). 0 0.2 0.4 0.6 0.8 1 0 0.2 0.4 0.6 0.8 1 Totalpostingstoprocessquery(normalized) Total time to process query (normalized) Partial processing of compressed postings query len = 1 query len in [2,3] query len in [4,8] query len > 8 Figure 10: Workload for partial query evaluation with compressed posting lists.",
                "The analysis of the previous section also applies to a distributed retrieval system in one or multiple sites.",
                "Suppose that a document partitioned distributed system is running on a cluster of machines interconnected with a local area network (LAN) in one site.",
                "The broker receives queries and broadcasts them to the query processors, which answer the queries and return the results to the broker.",
                "Finally, the broker merges the received answers and generates the final set of answers (we assume that the time spent on merging results is negligible).",
                "The difference between the centralized architecture and the document partition architecture is the extra communication between the broker and the query processors.",
                "Using ICMP pings on a 100Mbps LAN, we have measured that sending the query from the broker to the query processors which send an answer of 4,000 bytes back to the broker takes on average 0.615ms.",
                "Hence, TRL = TR + 0.615ms/0.069ms = TR + 9.",
                "In the case when the broker and the query processors are in different sites connected with a wide area network (WAN), we estimated that broadcasting the query from the broker to the query processors and getting back an answer of 4,000 bytes takes on average 329ms.",
                "Hence, TRW = TR + 329ms/0.069ms = TR + 4768. 6.3 Simulation Results We now address the problem of finding the optimal tradeoff between caching query answers and caching posting lists.",
                "To make the problem concrete we assume a fixed budget M on the available memory, out of which x units are used for caching query answers and M − x for caching posting lists.",
                "We perform simulations and compute the average response time as a function of x.",
                "Using a part of the query log as training data, we first allocate in the cache the answers to the most frequent queries that fit in space x, and then we use the rest of the memory to cache posting lists.",
                "For selecting posting lists we use the QtfDf algorithm, applied to the training query log but excluding the queries that have already been cached.",
                "In Figure 11, we plot the simulated response time for a centralized system as a function of x.",
                "For the uncompressed index we use M = 1GB, and for the compressed index we use M = 0.5GB.",
                "In the case of the configuration that uses partial query evaluation with compressed posting lists, the lowest response time is achieved when 0.15GB out of the 0.5GB is allocated for storing answers for queries.",
                "We obtained similar trends in the results for the LAN setting.",
                "Figure 12 shows the simulated workload for a distributed system across a WAN.",
                "In this case, the total amount of memory is split between the broker, which holds the cached 400 500 600 700 800 900 1000 1100 1200 0 0.2 0.4 0.6 0.8 1 Averageresponsetime Space (GB) Simulated workload -- single machine full / uncompr / 1 G partial / uncompr / 1 G full / compr / 0.5 G partial / compr / 0.5 G Figure 11: Optimal division of the cache in a server. 3000 3500 4000 4500 5000 5500 6000 0 0.2 0.4 0.6 0.8 1 Averageresponsetime Space (GB) Simulated workload -- WAN full / uncompr / 1 G partial / uncompr / 1 G full / compr / 0.5 G partial / compr / 0.5 G Figure 12: Optimal division of the cache when the next level requires WAN access. answers of queries, and the query processors, which hold the cache of posting lists.",
                "According to the figure, the difference between the configurations of the query processors is less important because the network communication overhead increases the response time substantially.",
                "When using uncompressed posting lists, the optimal allocation of memory corresponds to using approximately 70% of the memory for caching query answers.",
                "This is explained by the fact that there is no need for network communication when the query can be answered by the cache at the broker. 7.",
                "EFFECT OF THE QUERY DYNAMICS For our query log, the query distribution and query-term distribution change slowly over time.",
                "To support this claim, we first assess how topics change comparing the distribution of queries from the first week in June, 2006, to the distribution of queries for the remainder of 2006 that did not appear in the first week in June.",
                "We found that a very small percentage of queries are new queries.",
                "The majority of queries that appear in a given week repeat in the following weeks for the next six months.",
                "We then compute the hit rate of a <br>static cache</br> of 128, 000 answers trained over a period of two weeks (Figure 13).",
                "We report hit rate hourly for 7 days, starting from 5pm.",
                "We observe that the hit rate reaches its highest value during the night (around midnight), whereas around 2-3pm it reaches its minimum.",
                "After a small decay in hit rate values, the hit rate stabilizes between 0.28, and 0.34 for the entire week, suggesting that the <br>static cache</br> is effective for a whole week after the training period. 0.26 0.27 0.28 0.29 0.3 0.31 0.32 0.33 0.34 0.35 0.36 0.37 0 20 40 60 80 100 120 140 160 Hit-rate Time Hits on the frequent queries of distances Figure 13: Hourly hit rate for a <br>static cache</br> holding 128,000 answers during the period of a week.",
                "The <br>static cache</br> of posting lists can be periodically recomputed.",
                "To estimate the time interval in which we need to recompute the posting lists on the <br>static cache</br> we need to consider an efficiency/quality trade-off: using too short a time interval might be prohibitively expensive, while recomputing the cache too infrequently might lead to having an obsolete cache not corresponding to the statistical characteristics of the current query stream.",
                "We measured the effect on the QtfDf algorithm of the changes in a 15-week query stream (Figure 14).",
                "We compute the query term frequencies over the whole stream, select which terms to cache, and then compute the hit rate on the whole query stream.",
                "This hit rate is as an upper bound, and it assumes perfect knowledge of the query term frequencies.",
                "To simulate a realistic scenario, we use the first 6 (3) weeks of the query stream for computing query term frequencies and the following 9 (12) weeks to estimate the hit rate.",
                "As Figure 14 shows, the hit rate decreases by less than 2%.",
                "The high correlation among the query term frequencies during different time periods explains the graceful adaptation of the static caching algorithms to the future query stream.",
                "Indeed, the pairwise correlation among all possible 3-week periods of the 15-week query stream is over 99.5%. 8.",
                "CONCLUSIONS Caching is an effective technique in search engines for improving response time, reducing the load on query processors, and improving network bandwidth utilization.",
                "We present results on both dynamic and static caching.",
                "Dynamic caching of queries has limited effectiveness due to the high number of compulsory misses caused by the number of unique or infrequent queries.",
                "Our results show that in our UK log, the minimum miss rate is 50% using a working set strategy.",
                "Caching terms is more effective with respect to miss rate, achieving values as low as 12%.",
                "We also propose a new algorithm for static caching of posting lists that outperforms previous static caching algorithms as well as dynamic algorithms such as LRU and LFU, obtaining hit rate values that are over 10% higher compared these strategies.",
                "We present a framework for the analysis of the trade-off between caching query results and caching posting lists, and we simulate different types of architectures.",
                "Our results show that for centralized and LAN environments, there is an optimal allocation of caching query results and caching of posting lists, while for WAN scenarios in which network time prevails it is more important to cache query results. 0.45 0.5 0.55 0.6 0.65 0.7 0.75 0.8 0.85 0.9 0.95 0.1 0.2 0.3 0.4 0.5 0.6 0.7 Hitrate Cache size Dynamics of static QTF/DF caching policy perfect knowledge 6-week training 3-week training Figure 14: Impact of distribution changes on the static caching of posting lists. 9.",
                "REFERENCES [1] V. N. Anh and A. Moffat.",
                "Pruned query evaluation using pre-computed impacts.",
                "In ACM CIKM, 2006. [2] R. A. Baeza-Yates and F. Saint-Jean.",
                "A three level search engine index based in query log distribution.",
                "In SPIRE, 2003. [3] C. Buckley and A. F. Lewit.",
                "Optimization of inverted vector searches.",
                "In ACM SIGIR, 1985. [4] S. B¨uttcher and C. L. A. Clarke.",
                "A document-centric approach to static index pruning in text retrieval systems.",
                "In ACM CIKM, 2006. [5] P. Cao and S. Irani.",
                "Cost-aware WWW proxy caching algorithms.",
                "In USITS, 1997. [6] P. Denning.",
                "Working sets past and present.",
                "IEEE Trans. on Software Engineering, SE-6(1):64-84, 1980. [7] T. Fagni, R. Perego, F. Silvestri, and S. Orlando.",
                "Boosting the performance of web search engines: Caching and prefetching query results by exploiting historical usage data.",
                "ACM Trans.",
                "Inf.",
                "Syst., 24(1):51-78, 2006. [8] R. Lempel and S. Moran.",
                "Predictive caching and prefetching of query results in search engines.",
                "In WWW, 2003. [9] X.",
                "Long and T. Suel.",
                "Three-level caching for efficient query processing in large web search engines.",
                "In WWW, 2005. [10] E. P. Markatos.",
                "On caching search engine query results.",
                "Computer Communications, 24(2):137-143, 2001. [11] I. Ounis, G. Amati, V. Plachouras, B.",
                "He, C. Macdonald, and C. Lioma.",
                "Terrier: A High Performance and Scalable Information Retrieval Platform.",
                "In SIGIR Workshop on Open Source Information Retrieval, 2006. [12] V. V. Raghavan and H. Sever.",
                "On the reuse of past optimal queries.",
                "In ACM SIGIR, 1995. [13] P. C. Saraiva, E. S. de Moura, N. Ziviani, W. Meira, R. Fonseca, and B. Riberio-Neto.",
                "Rank-preserving two-level caching for scalable search engines.",
                "In ACM SIGIR, 2001. [14] D. R. Slutz and I. L. Traiger.",
                "A note on the calculation of average working set size.",
                "Communications of the ACM, 17(10):563-565, 1974. [15] T. Strohman, H. Turtle, and W. B. Croft.",
                "Optimization strategies for complex queries.",
                "In ACM SIGIR, 2005. [16] I. H. Witten, T. C. Bell, and A. Moffat.",
                "Managing Gigabytes: Compressing and Indexing Documents and Images.",
                "John Wiley & Sons, Inc., NY, 1994. [17] N. E. Young.",
                "On-line file caching.",
                "Algorithmica, 33(3):371-383, 2002."
            ],
            "original_annotated_samples": [],
            "translated_annotated_samples": [
                "También estudiamos el problema de encontrar la forma óptima de dividir el \"caché estático\" entre respuestas y listas de publicación.",
                "Un \"caché estático\" se basa en información histórica y se actualiza periódicamente.",
                "Proporcionamos algoritmos basados en el problema de la mochila para seleccionar las listas de publicación para poner en un \"caché estático\", y mostramos mejoras sobre el trabajo anterior, logrando una relación de éxito en el 90%;• Los cambios de la distribución de la consulta a lo largo del tiempo tienen poco impacto en el almacenamiento en caché estático.",
                "De hecho, el problema de seleccionar las mejores listas de publicación para el \"caché estático\" corresponde al problema estándar de la mochila: dada una mochila de capacidad fija y un conjunto de n elementos, como el elemento i-th tiene valor CI y tamañoSI, seleccione el conjunto de elementos que caben en la mochila y maximicen el valor general.",
                "Un beneficio importante, un \"caché estático\" es que no requiere desalojo y, por lo tanto, es más eficiente al evaluar las consultas.",
                "Luego calculamos la tasa de aciertos de un \"caché estático\" de 128, 000 respuestas entrenadas durante un período de dos semanas (Figura 13).",
                "Después de una pequeña descomposición en los valores de la tasa de aciertos, la tasa de aciertos se estabiliza entre 0.28 y 0.34 durante toda la semana, lo que sugiere que el \"caché estático\" es efectivo durante toda una semana después del período de entrenamiento.0.26 0.27 0.28 0.29 0.3 0.31 0.32 0.33 0.34 0.35 0.36 0.37 0 20 40 40 60 80 100 120 140 160 Hits de tiempo de tasas de golpe en las consultas frecuentes de distancias Figura 13: Tasa de aciertos por hora para un \"caché estático\" que contiene 128,000 respuestas durante el período durante el períodode una semana.",
                "El \"caché estático\" de las listas de publicación se puede recomputar periódicamente.",
                "Para estimar el intervalo de tiempo en el que necesitamos recomputar las listas de publicación en el \"caché estático\", debemos considerar una compensación de eficiencia/calidad: usar un intervalo de tiempo demasiado corto podría ser prohibitivamente costoso, mientras que recomputar el caché con demasiada frecuencia podríaconducir a tener un caché obsoleto que no corresponde a las características estadísticas de la corriente de consulta actual."
            ],
            "translated_text": "",
            "candidates": [
                "caché estático",
                "caché estático",
                "caché estático",
                "caché estático",
                "caché estático",
                "caché estático",
                "caché estático",
                "caché estático",
                "caché estático",
                "caché estático",
                "caché estático",
                "caché estático",
                "caché estático",
                "caché estático",
                "caché estático",
                "caché estático",
                "caché estático",
                "caché estático",
                "caché estático"
            ],
            "error": []
        },
        "answer and posting list": {
            "translated_key": "Lista de respuestas y publicaciones",
            "is_in_text": false,
            "original_annotated_sentences": [
                "The Impact of Caching on Search Engines Ricardo Baeza-Yates1 rbaeza@acm.org Aristides Gionis1 gionis@yahoo-inc.com Flavio Junqueira1 fpj@yahoo-inc.com Vanessa Murdock1 vmurdock@yahoo-inc.com Vassilis Plachouras1 vassilis@yahoo-inc.com Fabrizio Silvestri2 f.silvestri@isti.cnr.it 1 Yahoo!",
                "Research Barcelona 2 ISTI - CNR Barcelona, SPAIN Pisa, ITALY ABSTRACT In this paper we study the trade-offs in designing efficient caching systems for Web search engines.",
                "We explore the impact of different approaches, such as static vs. dynamic caching, and caching query results vs. caching posting lists.",
                "Using a query log spanning a whole year we explore the limitations of caching and we demonstrate that caching posting lists can achieve higher hit rates than caching query answers.",
                "We propose a new algorithm for static caching of posting lists, which outperforms previous methods.",
                "We also study the problem of finding the optimal way to split the static cache between answers and posting lists.",
                "Finally, we measure how the changes in the query log affect the effectiveness of static caching, given our observation that the distribution of the queries changes slowly over time.",
                "Our results and observations are applicable to different levels of the data-access hierarchy, for instance, for a memory/disk layer or a broker/remote server layer.",
                "Categories and Subject Descriptors H.3.3 [Information Storage and Retrieval]: Information Search and Retrieval - Search process; H.3.4 [Information Storage and Retrieval]: Systems and Software - Distributed systems, Performance evaluation (efficiency and effectiveness) General Terms Algorithms, Experimentation 1.",
                "INTRODUCTION Millions of queries are submitted daily to Web search engines, and users have high expectations of the quality and speed of the answers.",
                "As the searchable Web becomes larger and larger, with more than 20 billion pages to index, evaluating a single query requires processing large amounts of data.",
                "In such a setting, to achieve a fast response time and to increase the query throughput, using a cache is crucial.",
                "The primary use of a cache memory is to speedup computation by exploiting frequently or recently used data, although reducing the workload to back-end servers is also a major goal.",
                "Caching can be applied at different levels with increasing response latencies or processing requirements.",
                "For example, the different levels may correspond to the main memory, the disk, or resources in a local or a wide area network.",
                "The decision of what to cache is either off-line (static) or online (dynamic).",
                "A static cache is based on historical information and is periodically updated.",
                "A dynamic cache replaces entries according to the sequence of requests.",
                "When a new request arrives, the cache system decides whether to evict some entry from the cache in the case of a cache miss.",
                "Such online decisions are based on a cache policy, and several different policies have been studied in the past.",
                "For a search engine, there are two possible ways to use a cache memory: Caching answers: As the engine returns answers to a particular query, it may decide to store these answers to resolve future queries.",
                "Caching terms: As the engine evaluates a particular query, it may decide to store in memory the posting lists of the involved query terms.",
                "Often the whole set of posting lists does not fit in memory, and consequently, the engine has to select a small set to keep in memory and speed up query processing.",
                "Returning an answer to a query that already exists in the cache is more efficient than computing the answer using cached posting lists.",
                "On the other hand, previously unseen queries occur more often than previously unseen terms, implying a higher miss rate for cached answers.",
                "Caching of posting lists has additional challenges.",
                "As posting lists have variable size, caching them dynamically is not very efficient, due to the complexity in terms of efficiency and space, and the skewed distribution of the query stream, as shown later.",
                "Static caching of posting lists poses even more challenges: when deciding which terms to cache one faces the trade-off between frequently queried terms and terms with small posting lists that are space efficient.",
                "Finally, before deciding to adopt a static caching policy the query stream should be analyzed to verify that its characteristics do not change rapidly over time.",
                "Broker Static caching posting lists Dynamic/Static cached answers Local query processor Disk Next caching level Local network access Remote network access Figure 1: One caching level in a distributed search architecture.",
                "In this paper we explore the trade-offs in the design of each cache level, showing that the problem is the same and only a few parameters change.",
                "In general, we assume that each level of caching in a distributed search architecture is similar to that shown in Figure 1.",
                "We use a query log spanning a whole year to explore the limitations of dynamically caching query answers or posting lists for query terms.",
                "More concretely, our main conclusions are that: • Caching query answers results in lower hit ratios compared to caching of posting lists for query terms, but it is faster because there is no need for query evaluation.",
                "We provide a framework for the analysis of the trade-off between static caching of query answers and posting lists; • Static caching of terms can be more effective than dynamic caching with, for example, LRU.",
                "We provide algorithms based on the Knapsack problem for selecting the posting lists to put in a static cache, and we show improvements over previous work, achieving a hit ratio over 90%; • Changes of the query distribution over time have little impact on static caching.",
                "The remainder of this paper is organized as follows.",
                "Sections 2 and 3 summarize related work and characterize the data sets we use.",
                "Section 4 discusses the limitations of dynamic caching.",
                "Sections 5 and 6 introduce algorithms for caching posting lists, and a theoretical framework for the analysis of static caching, respectively.",
                "Section 7 discusses the impact of changes in the query distribution on static caching, and Section 8 provides concluding remarks. 2.",
                "RELATED WORK There is a large body of work devoted to query optimization.",
                "Buckley and Lewit [3], in one of the earliest works, take a term-at-a-time approach to deciding when inverted lists need not be further examined.",
                "More recent examples demonstrate that the top k documents for a query can be returned without the need for evaluating the complete set of posting lists [1, 4, 15].",
                "Although these approaches seek to improve query processing efficiency, they differ from our current work in that they do not consider caching.",
                "They may be considered separate and complementary to a cache-based approach.",
                "Raghavan and Sever [12], in one of the first papers on exploiting user query history, propose using a query base, built upon a set of persistent optimal queries submitted in the past, to improve the retrieval effectiveness for similar future queries.",
                "Markatos [10] shows the existence of temporal locality in queries, and compares the performance of different caching policies.",
                "Based on the observations of Markatos, Lempel and Moran propose a new caching policy, called Probabilistic Driven Caching, by attempting to estimate the probability distribution of all possible queries submitted to a search engine [8].",
                "Fagni et al. follow Markatos work by showing that combining static and dynamic caching policies together with an adaptive prefetching policy achieves a high hit ratio [7].",
                "Different from our work, they consider caching and prefetching of pages of results.",
                "As systems are often hierarchical, there has also been some effort on multi-level architectures.",
                "Saraiva et al. propose a new architecture for Web search engines using a two-level dynamic caching system [13].",
                "Their goal for such systems has been to improve response time for hierarchical engines.",
                "In their architecture, both levels use an LRU eviction policy.",
                "They find that the second-level cache can effectively reduce disk traffic, thus increasing the overall throughput.",
                "Baeza-Yates and Saint-Jean propose a three-level index organization [2].",
                "Long and Suel propose a caching system structured according to three different levels [9].",
                "The intermediate level contains frequently occurring pairs of terms and stores the intersections of the corresponding inverted lists.",
                "These last two papers are related to ours in that they exploit different caching strategies at different levels of the memory hierarchy.",
                "Finally, our static caching algorithm for posting lists in Section 5 uses the ratio frequency/size in order to evaluate the goodness of an item to cache.",
                "Similar ideas have been used in the context of file caching [17], Web caching [5], and even caching of posting lists [9], but in all cases in a dynamic setting.",
                "To the best of our knowledge we are the first to use this approach for static caching of posting lists. 3.",
                "DATA CHARACTERIZATION Our data consists of a crawl of documents from the UK domain, and query logs of one year of queries submitted to http://www.yahoo.co.uk from November 2005 to November 2006.",
                "In our logs, 50% of the total volume of queries are unique.",
                "The average query length is 2.5 terms, with the longest query having 731 terms. 1e-07 1e-06 1e-05 1e-04 0.001 0.01 0.1 1 1e-08 1e-07 1e-06 1e-05 1e-04 0.001 0.01 0.1 1 Frequency(normalized) Frequency rank (normalized) Figure 2: The distribution of queries (bottom curve) and query terms (middle curve) in the query log.",
                "The distribution of document frequencies of terms in the UK-2006 dataset (upper curve).",
                "Figure 2 shows the distributions of queries (lower curve), and query terms (middle curve).",
                "The x-axis represents the normalized frequency rank of the query or term. (The most frequent query appears closest to the y-axis.)",
                "The y-axis is Table 1: Statistics of the UK-2006 sample.",
                "UK-2006 sample statistics # of documents 2,786,391 # of terms 6,491,374 # of tokens 2,109,512,558 the normalized frequency for a given query (or term).",
                "As expected, the distribution of query frequencies and query term frequencies follow power law distributions, with slope of 1.84 and 2.26, respectively.",
                "In this figure, the query frequencies were computed as they appear in the logs with no normalization for case or white space.",
                "The query terms (middle curve) have been normalized for case, as have the terms in the document collection.",
                "The document collection that we use for our experiments is a summary of the UK domain crawled in May 2006.1 This summary corresponds to a maximum of 400 crawled documents per host, using a breadth first crawling strategy, comprising 15GB.",
                "The distribution of document frequencies of terms in the collection follows a power law distribution with slope 2.38 (upper curve in Figure 2).",
                "The statistics of the collection are shown in Table 1.",
                "We measured the correlation between the document frequency of terms in the collection and the number of queries that contain a particular term in the query log to be 0.424.",
                "A scatter plot for a random sample of terms is shown in Figure 3.",
                "In this experiment, terms have been converted to lower case in both the queries and the documents so that the frequencies will be comparable. 1e-07 1e-06 1e-05 1e-04 0.001 0.01 0.1 1 1e-06 1e-05 1e-04 0.001 0.01 0.1 1 Queryfrequency Document frequency Figure 3: Normalized scatter plot of document-term frequencies vs. query-term frequencies. 4.",
                "CACHING OF QUERIES AND TERMS Caching relies upon the assumption that there is locality in the stream of requests.",
                "That is, there must be sufficient repetition in the stream of requests and within intervals of time that enable a cache memory of reasonable size to be effective.",
                "In the query log we used, 88% of the unique queries are singleton queries, and 44% are singleton queries out of the whole volume.",
                "Thus, out of all queries in the stream composing the query log, the upper threshold on hit ratio is 56%.",
                "This is because only 56% of all the queries comprise queries that have multiple occurrences.",
                "It is important to observe, however, that not all queries in this 56% can be cache hits because of compulsory misses.",
                "A compulsory miss 1 The collection is available from the University of Milan: http://law.dsi.unimi.it/.",
                "URL retrieved 05/2007. 0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1 240 260 280 300 320 340 360 Numberofelements Bin number Total terms Terms diff Total queries Unique queries Unique terms Query diff Figure 4: Arrival rate for both terms and queries. happens when the cache receives a query for the first time.",
                "This is different from capacity misses, which happen due to space constraints on the amount of memory the cache uses.",
                "If we consider a cache with infinite memory, then the hit ratio is 50%.",
                "Note that for an infinite cache there are no capacity misses.",
                "As we mentioned before, another possibility is to cache the posting lists of terms.",
                "Intuitively, this gives more freedom in the utilization of the cache content to respond to queries because cached terms might form a new query.",
                "On the other hand, they need more space.",
                "As opposed to queries, the fraction of singleton terms in the total volume of terms is smaller.",
                "In our query log, only 4% of the terms appear once, but this accounts for 73% of the vocabulary of query terms.",
                "We show in Section 5 that caching a small fraction of terms, while accounting for terms appearing in many documents, is potentially very effective.",
                "Figure 4 shows several graphs corresponding to the normalized arrival rate for different cases using days as bins.",
                "That is, we plot the normalized number of elements that appear in a day.",
                "This graph shows only a period of 122 days, and we normalize the values by the maximum value observed throughout the whole period of the query log.",
                "Total queries and Total terms correspond to the total volume of queries and terms, respectively.",
                "Unique queries and Unique terms correspond to the arrival rate of unique queries and terms.",
                "Finally, Query diff and Terms diff correspond to the difference between the curves for total and unique.",
                "In Figure 4, as expected, the volume of terms is much higher than the volume of queries.",
                "The difference between the total number of terms and the number of unique terms is much larger than the difference between the total number of queries and the number of unique queries.",
                "This observation implies that terms repeat significantly more than queries.",
                "If we use smaller bins, say of one hour, then the ratio of unique to volume is higher for both terms and queries because it leaves less room for repetition.",
                "We also estimated the workload using the document frequency of terms as a measure of how much work a query imposes on a search engine.",
                "We found that it follows closely the arrival rate for terms shown in Figure 4.",
                "To demonstrate the effect of a dynamic cache on the query frequency distribution of Figure 2, we plot the same frequency graph, but now considering the frequency of queries Figure 5: Frequency graph after LRU cache. after going through an LRU cache.",
                "On a cache miss, an LRU cache decides upon an entry to evict using the information on the recency of queries.",
                "In this graph, the most frequent queries are not the same queries that were most frequent before the cache.",
                "It is possible that queries that are most frequent after the cache have different characteristics, and tuning the search engine to queries frequent before the cache may degrade performance for non-cached queries.",
                "The maximum frequency after caching is less than 1% of the maximum frequency before the cache, thus showing that the cache is very effective in reducing the load of frequent queries.",
                "If we re-rank the queries according to after-cache frequency, the distribution is still a power law, but with a much smaller value for the highest frequency.",
                "When discussing the effectiveness of dynamically caching, an important metric is cache miss rate.",
                "To analyze the cache miss rate for different memory constraints, we use the working set model [6, 14].",
                "A working set, informally, is the set of references that an application or an operating system is currently working with.",
                "The model uses such sets in a strategy that tries to capture the temporal locality of references.",
                "The working set strategy then consists in keeping in memory only the elements that are referenced in the previous θ steps of the input sequence, where θ is a configurable parameter corresponding to the window size.",
                "Originally, working sets have been used for page replacement algorithms of operating systems, and considering such a strategy in the context of search engines is interesting for three reasons.",
                "First, it captures the amount of locality of queries and terms in a sequence of queries.",
                "Locality in this case refers to the frequency of queries and terms in a window of time.",
                "If many queries appear multiple times in a window, then locality is high.",
                "Second, it enables an oﬄine analysis of the expected miss rate given different memory constraints.",
                "Third, working sets capture aspects of efficient caching algorithms such as LRU.",
                "LRU assumes that references farther in the past are less likely to be referenced in the present, which is implicit in the concept of working sets [14].",
                "Figure 6 plots the miss rate for different working set sizes, and we consider working sets of both queries and terms.",
                "The working set sizes are normalized against the total number of queries in the query log.",
                "In the graph for queries, there is a sharp decay until approximately 0.01, and the rate at which the miss rate drops decreases as we increase the size of the working set over 0.01.",
                "Finally, the minimum value it reaches is 50% miss rate, not shown in the figure as we have cut the tail of the curve for presentation purposes. 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1 0 0.05 0.1 0.15 0.2 Missrate Normalized working set size Queries Terms Figure 6: Miss rate as a function of the working set size. 1 10 100 1000 10000 100000 1e+06 Frequency Distance Figure 7: Distribution of distances expressed in terms of distinct queries.",
                "Compared to the query curve, we observe that the minimum miss rate for terms is substantially smaller.",
                "The miss rate also drops sharply on values up to 0.01, and it decreases minimally for higher values.",
                "The minimum value, however, is slightly over 10%, which is much smaller than the minimum value for the sequence of queries.",
                "This implies that with such a policy it is possible to achieve over 80% hit rate, if we consider caching dynamically posting lists for terms as opposed to caching answers for queries.",
                "This result does not consider the space required for each unit stored in the cache memory, or the amount of time it takes to put together a response to a user query.",
                "We analyze these issues more carefully later in this paper.",
                "It is interesting also to observe the histogram of Figure 7, which is an intermediate step in the computation of the miss rate graph.",
                "It reports the distribution of distances between repetitions of the same frequent query.",
                "The distance in the plot is measured in the number of distinct queries separating a query and its repetition, and it considers only queries appearing at least 10 times.",
                "From Figures 6 and 7, we conclude that even if we set the size of the query answers cache to a relatively large number of entries, the miss rate is high.",
                "Thus, caching the posting lists of terms has the potential to improve the hit ratio.",
                "This is what we explore next. 5.",
                "CACHING POSTING LISTS The previous section shows that caching posting lists can obtain a higher hit rate compared to caching query answers.",
                "In this section we study the problem of how to select posting lists to place on a certain amount of available memory, assuming that the whole index is larger than the amount of memory available.",
                "The posting lists have variable size (in fact, their size distribution follows a power law), so it is beneficial for a caching policy to consider the sizes of the posting lists.",
                "We consider both dynamic and static caching.",
                "For dynamic caching, we use two well-known policies, LRU and LFU, as well as a modified algorithm that takes posting-list size into account.",
                "Before discussing the static caching strategies, we introduce some notation.",
                "We use fq(t) to denote the query-term frequency of a term t, that is, the number of queries containing t in the query log, and fd(t) to denote the document frequency of t, that is, the number of documents in the collection in which the term t appears.",
                "The first strategy we consider is the algorithm proposed by Baeza-Yates and Saint-Jean [2], which consists in selecting the posting lists of the terms with the highest query-term frequencies fq(t).",
                "We call this algorithm Qtf.",
                "We observe that there is a trade-off between fq(t) and fd(t).",
                "Terms with high fq(t) are useful to keep in the cache because they are queried often.",
                "On the other hand, terms with high fd(t) are not good candidates because they correspond to long posting lists and consume a substantial amount of space.",
                "In fact, the problem of selecting the best posting lists for the static cache corresponds to the standard Knapsack problem: given a knapsack of fixed capacity, and a set of n items, such as the i-th item has value ci and size si, select the set of items that fit in the knapsack and maximize the overall value.",
                "In our case, value corresponds to fq(t) and size corresponds to fd(t).",
                "Thus, we employ a simple algorithm for the knapsack problem, which is selecting the posting lists of the terms with the highest values of the ratio fq(t) fd(t) .",
                "We call this algorithm QtfDf.",
                "We tried other variations considering query frequencies instead of term frequencies, but the gain was minimal compared to the complexity added.",
                "In addition to the above two static algorithms we consider the following algorithms for dynamic caching: • LRU: A standard LRU algorithm, but many posting lists might need to be evicted (in order of least-recent usage) until there is enough space in the memory to place the currently accessed posting list; • LFU: A standard LFU algorithm (eviction of the leastfrequently used), with the same modification as the LRU; • Dyn-QtfDf: A dynamic version of the QtfDf algorithm; evict from the cache the term(s) with the lowest fq(t) fd(t) ratio.",
                "The performance of all the above algorithms for 15 weeks of the query log and the UK dataset are shown in Figure 8.",
                "Performance is measured with hit rate.",
                "The cache size is measured as a fraction of the total space required to store the posting lists of all terms.",
                "For the dynamic algorithms, we load the cache with terms in order of fq(t) and we let the cache warm up for 1 million queries.",
                "For the static algorithms, we assume complete knowledge of the frequencies fq(t), that is, we estimate fq(t) from the whole query stream.",
                "As we show in Section 7 the results do not change much if we compute the query-term frequencies using the first 3 or 4 weeks of the query log and measure the hit rate on the rest. 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1 0.1 0.2 0.3 0.4 0.5 0.6 0.7 Hitrate Cache size Caching posting lists static QTF/DF LRU LFU Dyn-QTF/DF QTF Figure 8: Hit rate of different strategies for caching posting lists.",
                "The most important observation from our experiments is that the static QtfDf algorithm has a better hit rate than all the dynamic algorithms.",
                "An important benefit a static cache is that it requires no eviction and it is hence more efficient when evaluating queries.",
                "However, if the characteristics of the query traffic change frequently over time, then it requires re-populating the cache often or there will be a significant impact on hit rate. 6.",
                "ANALYSIS OF STATIC CACHING In this section we provide a detailed analysis for the problem of deciding whether it is preferable to cache query answers or cache posting lists.",
                "Our analysis takes into account the impact of caching between two levels of the data-access hierarchy.",
                "It can either be applied at the memory/disk layer or at a server/remote server layer as in the architecture we discussed in the introduction.",
                "Using a particular system model, we obtain estimates for the parameters required by our analysis, which we subsequently use to decide the optimal trade-off between caching query answers and caching posting lists. 6.1 Analytical Model Let M be the size of the cache measured in answer units (the cache can store M query answers).",
                "Assume that all posting lists are of the same length L, measured in answer units.",
                "We consider the following two cases: (A) a cache that stores only precomputed answers, and (B) a cache that stores only posting lists.",
                "In the first case, Nc = M answers fit in the cache, while in the second case Np = M/L posting lists fit in the cache.",
                "Thus, Np = Nc/L.",
                "Note that although posting lists require more space, we can combine terms to evaluate more queries (or partial queries).",
                "For case (A), suppose that a query answer in the cache can be evaluated in 1 time unit.",
                "For case (B), assume that if the posting lists of the terms of a query are in the cache then the results can be computed in TR1 time units, while if the posting lists are not in the cache then the results can be computed in TR2 time units.",
                "Of course TR2 > TR1.",
                "Now we want to compare the time to answer a stream of Q queries in both cases.",
                "Let Vc(Nc) be the volume of the most frequent Nc queries.",
                "Then, for case (A), we have an overall time TCA = Vc(Nc) + TR2(Q − Vc(Nc)).",
                "Similarly, for case (B), let Vp(Np) be the number of computable queries.",
                "Then we have overall time TP L = TR1Vp(Np) + TR2(Q − Vp(Np)).",
                "We want to check under which conditions we have TP L < TCA.",
                "We have TP L − TCA = (TR2 − 1)Vc(Nc) − (TR2 − TR1)Vp(Np) > 0.",
                "Figure 9 shows the values of Vp and Vc for our data.",
                "We can see that caching answers saturates faster and for this particular data there is no additional benefit from using more than 10% of the index space for caching answers.",
                "As the query distribution is a power law with parameter α > 1, the i-th most frequent query appears with probability proportional to 1 iα .",
                "Therefore, the volume Vc(n), which is the total number of the n most frequent queries, is Vc(n) = V0 n i=1 Q iα = γnQ (0 < γn < 1).",
                "We know that Vp(n) grows faster than Vc(n) and assume, based on experimental results, that the relation is of the form Vp(n) = k Vc(n)β .",
                "In the worst case, for a large cache, β → 1.",
                "That is, both techniques will cache a constant fraction of the overall query volume.",
                "Then caching posting lists makes sense only if L(TR2 − 1) k(TR2 − TR1) > 1.",
                "If we use compression, we have L < L and TR1 > TR1.",
                "According to the experiments that we show later, compression is always better.",
                "For a small cache, we are interested in the transient behavior and then β > 1, as computed from our data.",
                "In this case there will always be a point where TP L > TCA for a large number of queries.",
                "In reality, instead of filling the cache only with answers or only with posting lists, a better strategy will be to divide the total cache space into cache for answers and cache for posting lists.",
                "In such a case, there will be some queries that could be answered by both parts of the cache.",
                "As the answer cache is faster, it will be the first choice for answering those queries.",
                "Let QNc and QNp be the set of queries that can be answered by the cached answers and the cached posting lists, respectively.",
                "Then, the overall time is T = Vc(Nc)+TR1V (QNp −QNc )+TR2(Q−V (QNp ∪QNc )), where Np = (M − Nc)/L.",
                "Finding the optimal division of the cache in order to minimize the overall retrieval time is a difficult problem to solve analytically.",
                "In Section 6.3 we use simulations to derive optimal cache trade-offs for particular implementation examples. 6.2 Parameter Estimation We now use a particular implementation of a centralized system and the model of a distributed system as examples from which we estimate the parameters of the analysis from the previous section.",
                "We perform the experiments using an optimized version of Terrier [11] for both indexing documents and processing queries, on a single machine with a Pentium 4 at 2GHz and 1GB of RAM.",
                "We indexed the documents from the UK-2006 dataset, without removing stop words or applying stemming.",
                "The posting lists in the inverted file consist of pairs of document identifier and term frequency.",
                "We compress the document identifier gaps using Elias gamma encoding, and the 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1 0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1 Queryvolume Space precomputed answers posting lists Figure 9: Cache saturation as a function of size.",
                "Table 2: Ratios between the average time to evaluate a query and the average time to return cached answers (centralized and distributed case).",
                "Centralized system TR1 TR2 TR1 TR2 Full evaluation 233 1760 707 1140 Partial evaluation 99 1626 493 798 LAN system TRL 1 TRL 2 TR L 1 TR L 2 Full evaluation 242 1769 716 1149 Partial evaluation 108 1635 502 807 WAN system TRW 1 TRW 2 TR W 1 TR W 2 Full evaluation 5001 6528 5475 5908 Partial evaluation 4867 6394 5270 5575 term frequencies in documents using unary encoding [16].",
                "The size of the inverted file is 1,189Mb.",
                "A stored answer requires 1264 bytes, and an uncompressed posting takes 8 bytes.",
                "From Table 1, we obtain L = (8·# of postings) 1264·# of terms = 0.75 and L = Inverted file size 1264·# of terms = 0.26.",
                "We estimate the ratio TR = T/Tc between the average time T it takes to evaluate a query and the average time Tc it takes to return a stored answer for the same query, in the following way.",
                "Tc is measured by loading the answers for 100,000 queries in memory, and answering the queries from memory.",
                "The average time is Tc = 0.069ms.",
                "T is measured by processing the same 100,000 queries (the first 10,000 queries are used to warm-up the system).",
                "For each query, we remove stop words, if there are at least three remaining terms.",
                "The stop words correspond to the terms with a frequency higher than the number of documents in the index.",
                "We use a document-at-a-time approach to retrieve documents containing all query terms.",
                "The only disk access required during query processing is for reading compressed posting lists from the inverted file.",
                "We perform both full and partial evaluation of answers, because some queries are likely to retrieve a large number of documents, and only a fraction of the retrieved documents will be seen by users.",
                "In the partial evaluation of queries, we terminate the processing after matching 10,000 documents.",
                "The estimated ratios TR are presented in Table 2.",
                "Figure 10 shows for a sample of queries the workload of the system with partial query evaluation and compressed posting lists.",
                "The x-axis corresponds to the total time the system spends processing a particular query, and the vertical axis corresponds to the sum t∈q fq · fd(t).",
                "Notice that the total number of postings of the query-terms does not necessarily provide an accurate estimate of the workload imposed on the system by a query (which is the case for full evaluation and uncompressed lists). 0 0.2 0.4 0.6 0.8 1 0 0.2 0.4 0.6 0.8 1 Totalpostingstoprocessquery(normalized) Total time to process query (normalized) Partial processing of compressed postings query len = 1 query len in [2,3] query len in [4,8] query len > 8 Figure 10: Workload for partial query evaluation with compressed posting lists.",
                "The analysis of the previous section also applies to a distributed retrieval system in one or multiple sites.",
                "Suppose that a document partitioned distributed system is running on a cluster of machines interconnected with a local area network (LAN) in one site.",
                "The broker receives queries and broadcasts them to the query processors, which answer the queries and return the results to the broker.",
                "Finally, the broker merges the received answers and generates the final set of answers (we assume that the time spent on merging results is negligible).",
                "The difference between the centralized architecture and the document partition architecture is the extra communication between the broker and the query processors.",
                "Using ICMP pings on a 100Mbps LAN, we have measured that sending the query from the broker to the query processors which send an answer of 4,000 bytes back to the broker takes on average 0.615ms.",
                "Hence, TRL = TR + 0.615ms/0.069ms = TR + 9.",
                "In the case when the broker and the query processors are in different sites connected with a wide area network (WAN), we estimated that broadcasting the query from the broker to the query processors and getting back an answer of 4,000 bytes takes on average 329ms.",
                "Hence, TRW = TR + 329ms/0.069ms = TR + 4768. 6.3 Simulation Results We now address the problem of finding the optimal tradeoff between caching query answers and caching posting lists.",
                "To make the problem concrete we assume a fixed budget M on the available memory, out of which x units are used for caching query answers and M − x for caching posting lists.",
                "We perform simulations and compute the average response time as a function of x.",
                "Using a part of the query log as training data, we first allocate in the cache the answers to the most frequent queries that fit in space x, and then we use the rest of the memory to cache posting lists.",
                "For selecting posting lists we use the QtfDf algorithm, applied to the training query log but excluding the queries that have already been cached.",
                "In Figure 11, we plot the simulated response time for a centralized system as a function of x.",
                "For the uncompressed index we use M = 1GB, and for the compressed index we use M = 0.5GB.",
                "In the case of the configuration that uses partial query evaluation with compressed posting lists, the lowest response time is achieved when 0.15GB out of the 0.5GB is allocated for storing answers for queries.",
                "We obtained similar trends in the results for the LAN setting.",
                "Figure 12 shows the simulated workload for a distributed system across a WAN.",
                "In this case, the total amount of memory is split between the broker, which holds the cached 400 500 600 700 800 900 1000 1100 1200 0 0.2 0.4 0.6 0.8 1 Averageresponsetime Space (GB) Simulated workload -- single machine full / uncompr / 1 G partial / uncompr / 1 G full / compr / 0.5 G partial / compr / 0.5 G Figure 11: Optimal division of the cache in a server. 3000 3500 4000 4500 5000 5500 6000 0 0.2 0.4 0.6 0.8 1 Averageresponsetime Space (GB) Simulated workload -- WAN full / uncompr / 1 G partial / uncompr / 1 G full / compr / 0.5 G partial / compr / 0.5 G Figure 12: Optimal division of the cache when the next level requires WAN access. answers of queries, and the query processors, which hold the cache of posting lists.",
                "According to the figure, the difference between the configurations of the query processors is less important because the network communication overhead increases the response time substantially.",
                "When using uncompressed posting lists, the optimal allocation of memory corresponds to using approximately 70% of the memory for caching query answers.",
                "This is explained by the fact that there is no need for network communication when the query can be answered by the cache at the broker. 7.",
                "EFFECT OF THE QUERY DYNAMICS For our query log, the query distribution and query-term distribution change slowly over time.",
                "To support this claim, we first assess how topics change comparing the distribution of queries from the first week in June, 2006, to the distribution of queries for the remainder of 2006 that did not appear in the first week in June.",
                "We found that a very small percentage of queries are new queries.",
                "The majority of queries that appear in a given week repeat in the following weeks for the next six months.",
                "We then compute the hit rate of a static cache of 128, 000 answers trained over a period of two weeks (Figure 13).",
                "We report hit rate hourly for 7 days, starting from 5pm.",
                "We observe that the hit rate reaches its highest value during the night (around midnight), whereas around 2-3pm it reaches its minimum.",
                "After a small decay in hit rate values, the hit rate stabilizes between 0.28, and 0.34 for the entire week, suggesting that the static cache is effective for a whole week after the training period. 0.26 0.27 0.28 0.29 0.3 0.31 0.32 0.33 0.34 0.35 0.36 0.37 0 20 40 60 80 100 120 140 160 Hit-rate Time Hits on the frequent queries of distances Figure 13: Hourly hit rate for a static cache holding 128,000 answers during the period of a week.",
                "The static cache of posting lists can be periodically recomputed.",
                "To estimate the time interval in which we need to recompute the posting lists on the static cache we need to consider an efficiency/quality trade-off: using too short a time interval might be prohibitively expensive, while recomputing the cache too infrequently might lead to having an obsolete cache not corresponding to the statistical characteristics of the current query stream.",
                "We measured the effect on the QtfDf algorithm of the changes in a 15-week query stream (Figure 14).",
                "We compute the query term frequencies over the whole stream, select which terms to cache, and then compute the hit rate on the whole query stream.",
                "This hit rate is as an upper bound, and it assumes perfect knowledge of the query term frequencies.",
                "To simulate a realistic scenario, we use the first 6 (3) weeks of the query stream for computing query term frequencies and the following 9 (12) weeks to estimate the hit rate.",
                "As Figure 14 shows, the hit rate decreases by less than 2%.",
                "The high correlation among the query term frequencies during different time periods explains the graceful adaptation of the static caching algorithms to the future query stream.",
                "Indeed, the pairwise correlation among all possible 3-week periods of the 15-week query stream is over 99.5%. 8.",
                "CONCLUSIONS Caching is an effective technique in search engines for improving response time, reducing the load on query processors, and improving network bandwidth utilization.",
                "We present results on both dynamic and static caching.",
                "Dynamic caching of queries has limited effectiveness due to the high number of compulsory misses caused by the number of unique or infrequent queries.",
                "Our results show that in our UK log, the minimum miss rate is 50% using a working set strategy.",
                "Caching terms is more effective with respect to miss rate, achieving values as low as 12%.",
                "We also propose a new algorithm for static caching of posting lists that outperforms previous static caching algorithms as well as dynamic algorithms such as LRU and LFU, obtaining hit rate values that are over 10% higher compared these strategies.",
                "We present a framework for the analysis of the trade-off between caching query results and caching posting lists, and we simulate different types of architectures.",
                "Our results show that for centralized and LAN environments, there is an optimal allocation of caching query results and caching of posting lists, while for WAN scenarios in which network time prevails it is more important to cache query results. 0.45 0.5 0.55 0.6 0.65 0.7 0.75 0.8 0.85 0.9 0.95 0.1 0.2 0.3 0.4 0.5 0.6 0.7 Hitrate Cache size Dynamics of static QTF/DF caching policy perfect knowledge 6-week training 3-week training Figure 14: Impact of distribution changes on the static caching of posting lists. 9.",
                "REFERENCES [1] V. N. Anh and A. Moffat.",
                "Pruned query evaluation using pre-computed impacts.",
                "In ACM CIKM, 2006. [2] R. A. Baeza-Yates and F. Saint-Jean.",
                "A three level search engine index based in query log distribution.",
                "In SPIRE, 2003. [3] C. Buckley and A. F. Lewit.",
                "Optimization of inverted vector searches.",
                "In ACM SIGIR, 1985. [4] S. B¨uttcher and C. L. A. Clarke.",
                "A document-centric approach to static index pruning in text retrieval systems.",
                "In ACM CIKM, 2006. [5] P. Cao and S. Irani.",
                "Cost-aware WWW proxy caching algorithms.",
                "In USITS, 1997. [6] P. Denning.",
                "Working sets past and present.",
                "IEEE Trans. on Software Engineering, SE-6(1):64-84, 1980. [7] T. Fagni, R. Perego, F. Silvestri, and S. Orlando.",
                "Boosting the performance of web search engines: Caching and prefetching query results by exploiting historical usage data.",
                "ACM Trans.",
                "Inf.",
                "Syst., 24(1):51-78, 2006. [8] R. Lempel and S. Moran.",
                "Predictive caching and prefetching of query results in search engines.",
                "In WWW, 2003. [9] X.",
                "Long and T. Suel.",
                "Three-level caching for efficient query processing in large web search engines.",
                "In WWW, 2005. [10] E. P. Markatos.",
                "On caching search engine query results.",
                "Computer Communications, 24(2):137-143, 2001. [11] I. Ounis, G. Amati, V. Plachouras, B.",
                "He, C. Macdonald, and C. Lioma.",
                "Terrier: A High Performance and Scalable Information Retrieval Platform.",
                "In SIGIR Workshop on Open Source Information Retrieval, 2006. [12] V. V. Raghavan and H. Sever.",
                "On the reuse of past optimal queries.",
                "In ACM SIGIR, 1995. [13] P. C. Saraiva, E. S. de Moura, N. Ziviani, W. Meira, R. Fonseca, and B. Riberio-Neto.",
                "Rank-preserving two-level caching for scalable search engines.",
                "In ACM SIGIR, 2001. [14] D. R. Slutz and I. L. Traiger.",
                "A note on the calculation of average working set size.",
                "Communications of the ACM, 17(10):563-565, 1974. [15] T. Strohman, H. Turtle, and W. B. Croft.",
                "Optimization strategies for complex queries.",
                "In ACM SIGIR, 2005. [16] I. H. Witten, T. C. Bell, and A. Moffat.",
                "Managing Gigabytes: Compressing and Indexing Documents and Images.",
                "John Wiley & Sons, Inc., NY, 1994. [17] N. E. Young.",
                "On-line file caching.",
                "Algorithmica, 33(3):371-383, 2002."
            ],
            "original_annotated_samples": [],
            "translated_annotated_samples": [],
            "translated_text": "",
            "candidates": [],
            "error": []
        },
        "query log": {
            "translated_key": "registro de consultas",
            "is_in_text": true,
            "original_annotated_sentences": [
                "The Impact of Caching on Search Engines Ricardo Baeza-Yates1 rbaeza@acm.org Aristides Gionis1 gionis@yahoo-inc.com Flavio Junqueira1 fpj@yahoo-inc.com Vanessa Murdock1 vmurdock@yahoo-inc.com Vassilis Plachouras1 vassilis@yahoo-inc.com Fabrizio Silvestri2 f.silvestri@isti.cnr.it 1 Yahoo!",
                "Research Barcelona 2 ISTI - CNR Barcelona, SPAIN Pisa, ITALY ABSTRACT In this paper we study the trade-offs in designing efficient caching systems for Web search engines.",
                "We explore the impact of different approaches, such as static vs. dynamic caching, and caching query results vs. caching posting lists.",
                "Using a <br>query log</br> spanning a whole year we explore the limitations of caching and we demonstrate that caching posting lists can achieve higher hit rates than caching query answers.",
                "We propose a new algorithm for static caching of posting lists, which outperforms previous methods.",
                "We also study the problem of finding the optimal way to split the static cache between answers and posting lists.",
                "Finally, we measure how the changes in the <br>query log</br> affect the effectiveness of static caching, given our observation that the distribution of the queries changes slowly over time.",
                "Our results and observations are applicable to different levels of the data-access hierarchy, for instance, for a memory/disk layer or a broker/remote server layer.",
                "Categories and Subject Descriptors H.3.3 [Information Storage and Retrieval]: Information Search and Retrieval - Search process; H.3.4 [Information Storage and Retrieval]: Systems and Software - Distributed systems, Performance evaluation (efficiency and effectiveness) General Terms Algorithms, Experimentation 1.",
                "INTRODUCTION Millions of queries are submitted daily to Web search engines, and users have high expectations of the quality and speed of the answers.",
                "As the searchable Web becomes larger and larger, with more than 20 billion pages to index, evaluating a single query requires processing large amounts of data.",
                "In such a setting, to achieve a fast response time and to increase the query throughput, using a cache is crucial.",
                "The primary use of a cache memory is to speedup computation by exploiting frequently or recently used data, although reducing the workload to back-end servers is also a major goal.",
                "Caching can be applied at different levels with increasing response latencies or processing requirements.",
                "For example, the different levels may correspond to the main memory, the disk, or resources in a local or a wide area network.",
                "The decision of what to cache is either off-line (static) or online (dynamic).",
                "A static cache is based on historical information and is periodically updated.",
                "A dynamic cache replaces entries according to the sequence of requests.",
                "When a new request arrives, the cache system decides whether to evict some entry from the cache in the case of a cache miss.",
                "Such online decisions are based on a cache policy, and several different policies have been studied in the past.",
                "For a search engine, there are two possible ways to use a cache memory: Caching answers: As the engine returns answers to a particular query, it may decide to store these answers to resolve future queries.",
                "Caching terms: As the engine evaluates a particular query, it may decide to store in memory the posting lists of the involved query terms.",
                "Often the whole set of posting lists does not fit in memory, and consequently, the engine has to select a small set to keep in memory and speed up query processing.",
                "Returning an answer to a query that already exists in the cache is more efficient than computing the answer using cached posting lists.",
                "On the other hand, previously unseen queries occur more often than previously unseen terms, implying a higher miss rate for cached answers.",
                "Caching of posting lists has additional challenges.",
                "As posting lists have variable size, caching them dynamically is not very efficient, due to the complexity in terms of efficiency and space, and the skewed distribution of the query stream, as shown later.",
                "Static caching of posting lists poses even more challenges: when deciding which terms to cache one faces the trade-off between frequently queried terms and terms with small posting lists that are space efficient.",
                "Finally, before deciding to adopt a static caching policy the query stream should be analyzed to verify that its characteristics do not change rapidly over time.",
                "Broker Static caching posting lists Dynamic/Static cached answers Local query processor Disk Next caching level Local network access Remote network access Figure 1: One caching level in a distributed search architecture.",
                "In this paper we explore the trade-offs in the design of each cache level, showing that the problem is the same and only a few parameters change.",
                "In general, we assume that each level of caching in a distributed search architecture is similar to that shown in Figure 1.",
                "We use a <br>query log</br> spanning a whole year to explore the limitations of dynamically caching query answers or posting lists for query terms.",
                "More concretely, our main conclusions are that: • Caching query answers results in lower hit ratios compared to caching of posting lists for query terms, but it is faster because there is no need for query evaluation.",
                "We provide a framework for the analysis of the trade-off between static caching of query answers and posting lists; • Static caching of terms can be more effective than dynamic caching with, for example, LRU.",
                "We provide algorithms based on the Knapsack problem for selecting the posting lists to put in a static cache, and we show improvements over previous work, achieving a hit ratio over 90%; • Changes of the query distribution over time have little impact on static caching.",
                "The remainder of this paper is organized as follows.",
                "Sections 2 and 3 summarize related work and characterize the data sets we use.",
                "Section 4 discusses the limitations of dynamic caching.",
                "Sections 5 and 6 introduce algorithms for caching posting lists, and a theoretical framework for the analysis of static caching, respectively.",
                "Section 7 discusses the impact of changes in the query distribution on static caching, and Section 8 provides concluding remarks. 2.",
                "RELATED WORK There is a large body of work devoted to query optimization.",
                "Buckley and Lewit [3], in one of the earliest works, take a term-at-a-time approach to deciding when inverted lists need not be further examined.",
                "More recent examples demonstrate that the top k documents for a query can be returned without the need for evaluating the complete set of posting lists [1, 4, 15].",
                "Although these approaches seek to improve query processing efficiency, they differ from our current work in that they do not consider caching.",
                "They may be considered separate and complementary to a cache-based approach.",
                "Raghavan and Sever [12], in one of the first papers on exploiting user query history, propose using a query base, built upon a set of persistent optimal queries submitted in the past, to improve the retrieval effectiveness for similar future queries.",
                "Markatos [10] shows the existence of temporal locality in queries, and compares the performance of different caching policies.",
                "Based on the observations of Markatos, Lempel and Moran propose a new caching policy, called Probabilistic Driven Caching, by attempting to estimate the probability distribution of all possible queries submitted to a search engine [8].",
                "Fagni et al. follow Markatos work by showing that combining static and dynamic caching policies together with an adaptive prefetching policy achieves a high hit ratio [7].",
                "Different from our work, they consider caching and prefetching of pages of results.",
                "As systems are often hierarchical, there has also been some effort on multi-level architectures.",
                "Saraiva et al. propose a new architecture for Web search engines using a two-level dynamic caching system [13].",
                "Their goal for such systems has been to improve response time for hierarchical engines.",
                "In their architecture, both levels use an LRU eviction policy.",
                "They find that the second-level cache can effectively reduce disk traffic, thus increasing the overall throughput.",
                "Baeza-Yates and Saint-Jean propose a three-level index organization [2].",
                "Long and Suel propose a caching system structured according to three different levels [9].",
                "The intermediate level contains frequently occurring pairs of terms and stores the intersections of the corresponding inverted lists.",
                "These last two papers are related to ours in that they exploit different caching strategies at different levels of the memory hierarchy.",
                "Finally, our static caching algorithm for posting lists in Section 5 uses the ratio frequency/size in order to evaluate the goodness of an item to cache.",
                "Similar ideas have been used in the context of file caching [17], Web caching [5], and even caching of posting lists [9], but in all cases in a dynamic setting.",
                "To the best of our knowledge we are the first to use this approach for static caching of posting lists. 3.",
                "DATA CHARACTERIZATION Our data consists of a crawl of documents from the UK domain, and query logs of one year of queries submitted to http://www.yahoo.co.uk from November 2005 to November 2006.",
                "In our logs, 50% of the total volume of queries are unique.",
                "The average query length is 2.5 terms, with the longest query having 731 terms. 1e-07 1e-06 1e-05 1e-04 0.001 0.01 0.1 1 1e-08 1e-07 1e-06 1e-05 1e-04 0.001 0.01 0.1 1 Frequency(normalized) Frequency rank (normalized) Figure 2: The distribution of queries (bottom curve) and query terms (middle curve) in the <br>query log</br>.",
                "The distribution of document frequencies of terms in the UK-2006 dataset (upper curve).",
                "Figure 2 shows the distributions of queries (lower curve), and query terms (middle curve).",
                "The x-axis represents the normalized frequency rank of the query or term. (The most frequent query appears closest to the y-axis.)",
                "The y-axis is Table 1: Statistics of the UK-2006 sample.",
                "UK-2006 sample statistics # of documents 2,786,391 # of terms 6,491,374 # of tokens 2,109,512,558 the normalized frequency for a given query (or term).",
                "As expected, the distribution of query frequencies and query term frequencies follow power law distributions, with slope of 1.84 and 2.26, respectively.",
                "In this figure, the query frequencies were computed as they appear in the logs with no normalization for case or white space.",
                "The query terms (middle curve) have been normalized for case, as have the terms in the document collection.",
                "The document collection that we use for our experiments is a summary of the UK domain crawled in May 2006.1 This summary corresponds to a maximum of 400 crawled documents per host, using a breadth first crawling strategy, comprising 15GB.",
                "The distribution of document frequencies of terms in the collection follows a power law distribution with slope 2.38 (upper curve in Figure 2).",
                "The statistics of the collection are shown in Table 1.",
                "We measured the correlation between the document frequency of terms in the collection and the number of queries that contain a particular term in the <br>query log</br> to be 0.424.",
                "A scatter plot for a random sample of terms is shown in Figure 3.",
                "In this experiment, terms have been converted to lower case in both the queries and the documents so that the frequencies will be comparable. 1e-07 1e-06 1e-05 1e-04 0.001 0.01 0.1 1 1e-06 1e-05 1e-04 0.001 0.01 0.1 1 Queryfrequency Document frequency Figure 3: Normalized scatter plot of document-term frequencies vs. query-term frequencies. 4.",
                "CACHING OF QUERIES AND TERMS Caching relies upon the assumption that there is locality in the stream of requests.",
                "That is, there must be sufficient repetition in the stream of requests and within intervals of time that enable a cache memory of reasonable size to be effective.",
                "In the <br>query log</br> we used, 88% of the unique queries are singleton queries, and 44% are singleton queries out of the whole volume.",
                "Thus, out of all queries in the stream composing the <br>query log</br>, the upper threshold on hit ratio is 56%.",
                "This is because only 56% of all the queries comprise queries that have multiple occurrences.",
                "It is important to observe, however, that not all queries in this 56% can be cache hits because of compulsory misses.",
                "A compulsory miss 1 The collection is available from the University of Milan: http://law.dsi.unimi.it/.",
                "URL retrieved 05/2007. 0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1 240 260 280 300 320 340 360 Numberofelements Bin number Total terms Terms diff Total queries Unique queries Unique terms Query diff Figure 4: Arrival rate for both terms and queries. happens when the cache receives a query for the first time.",
                "This is different from capacity misses, which happen due to space constraints on the amount of memory the cache uses.",
                "If we consider a cache with infinite memory, then the hit ratio is 50%.",
                "Note that for an infinite cache there are no capacity misses.",
                "As we mentioned before, another possibility is to cache the posting lists of terms.",
                "Intuitively, this gives more freedom in the utilization of the cache content to respond to queries because cached terms might form a new query.",
                "On the other hand, they need more space.",
                "As opposed to queries, the fraction of singleton terms in the total volume of terms is smaller.",
                "In our <br>query log</br>, only 4% of the terms appear once, but this accounts for 73% of the vocabulary of query terms.",
                "We show in Section 5 that caching a small fraction of terms, while accounting for terms appearing in many documents, is potentially very effective.",
                "Figure 4 shows several graphs corresponding to the normalized arrival rate for different cases using days as bins.",
                "That is, we plot the normalized number of elements that appear in a day.",
                "This graph shows only a period of 122 days, and we normalize the values by the maximum value observed throughout the whole period of the <br>query log</br>.",
                "Total queries and Total terms correspond to the total volume of queries and terms, respectively.",
                "Unique queries and Unique terms correspond to the arrival rate of unique queries and terms.",
                "Finally, Query diff and Terms diff correspond to the difference between the curves for total and unique.",
                "In Figure 4, as expected, the volume of terms is much higher than the volume of queries.",
                "The difference between the total number of terms and the number of unique terms is much larger than the difference between the total number of queries and the number of unique queries.",
                "This observation implies that terms repeat significantly more than queries.",
                "If we use smaller bins, say of one hour, then the ratio of unique to volume is higher for both terms and queries because it leaves less room for repetition.",
                "We also estimated the workload using the document frequency of terms as a measure of how much work a query imposes on a search engine.",
                "We found that it follows closely the arrival rate for terms shown in Figure 4.",
                "To demonstrate the effect of a dynamic cache on the query frequency distribution of Figure 2, we plot the same frequency graph, but now considering the frequency of queries Figure 5: Frequency graph after LRU cache. after going through an LRU cache.",
                "On a cache miss, an LRU cache decides upon an entry to evict using the information on the recency of queries.",
                "In this graph, the most frequent queries are not the same queries that were most frequent before the cache.",
                "It is possible that queries that are most frequent after the cache have different characteristics, and tuning the search engine to queries frequent before the cache may degrade performance for non-cached queries.",
                "The maximum frequency after caching is less than 1% of the maximum frequency before the cache, thus showing that the cache is very effective in reducing the load of frequent queries.",
                "If we re-rank the queries according to after-cache frequency, the distribution is still a power law, but with a much smaller value for the highest frequency.",
                "When discussing the effectiveness of dynamically caching, an important metric is cache miss rate.",
                "To analyze the cache miss rate for different memory constraints, we use the working set model [6, 14].",
                "A working set, informally, is the set of references that an application or an operating system is currently working with.",
                "The model uses such sets in a strategy that tries to capture the temporal locality of references.",
                "The working set strategy then consists in keeping in memory only the elements that are referenced in the previous θ steps of the input sequence, where θ is a configurable parameter corresponding to the window size.",
                "Originally, working sets have been used for page replacement algorithms of operating systems, and considering such a strategy in the context of search engines is interesting for three reasons.",
                "First, it captures the amount of locality of queries and terms in a sequence of queries.",
                "Locality in this case refers to the frequency of queries and terms in a window of time.",
                "If many queries appear multiple times in a window, then locality is high.",
                "Second, it enables an oﬄine analysis of the expected miss rate given different memory constraints.",
                "Third, working sets capture aspects of efficient caching algorithms such as LRU.",
                "LRU assumes that references farther in the past are less likely to be referenced in the present, which is implicit in the concept of working sets [14].",
                "Figure 6 plots the miss rate for different working set sizes, and we consider working sets of both queries and terms.",
                "The working set sizes are normalized against the total number of queries in the <br>query log</br>.",
                "In the graph for queries, there is a sharp decay until approximately 0.01, and the rate at which the miss rate drops decreases as we increase the size of the working set over 0.01.",
                "Finally, the minimum value it reaches is 50% miss rate, not shown in the figure as we have cut the tail of the curve for presentation purposes. 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1 0 0.05 0.1 0.15 0.2 Missrate Normalized working set size Queries Terms Figure 6: Miss rate as a function of the working set size. 1 10 100 1000 10000 100000 1e+06 Frequency Distance Figure 7: Distribution of distances expressed in terms of distinct queries.",
                "Compared to the query curve, we observe that the minimum miss rate for terms is substantially smaller.",
                "The miss rate also drops sharply on values up to 0.01, and it decreases minimally for higher values.",
                "The minimum value, however, is slightly over 10%, which is much smaller than the minimum value for the sequence of queries.",
                "This implies that with such a policy it is possible to achieve over 80% hit rate, if we consider caching dynamically posting lists for terms as opposed to caching answers for queries.",
                "This result does not consider the space required for each unit stored in the cache memory, or the amount of time it takes to put together a response to a user query.",
                "We analyze these issues more carefully later in this paper.",
                "It is interesting also to observe the histogram of Figure 7, which is an intermediate step in the computation of the miss rate graph.",
                "It reports the distribution of distances between repetitions of the same frequent query.",
                "The distance in the plot is measured in the number of distinct queries separating a query and its repetition, and it considers only queries appearing at least 10 times.",
                "From Figures 6 and 7, we conclude that even if we set the size of the query answers cache to a relatively large number of entries, the miss rate is high.",
                "Thus, caching the posting lists of terms has the potential to improve the hit ratio.",
                "This is what we explore next. 5.",
                "CACHING POSTING LISTS The previous section shows that caching posting lists can obtain a higher hit rate compared to caching query answers.",
                "In this section we study the problem of how to select posting lists to place on a certain amount of available memory, assuming that the whole index is larger than the amount of memory available.",
                "The posting lists have variable size (in fact, their size distribution follows a power law), so it is beneficial for a caching policy to consider the sizes of the posting lists.",
                "We consider both dynamic and static caching.",
                "For dynamic caching, we use two well-known policies, LRU and LFU, as well as a modified algorithm that takes posting-list size into account.",
                "Before discussing the static caching strategies, we introduce some notation.",
                "We use fq(t) to denote the query-term frequency of a term t, that is, the number of queries containing t in the <br>query log</br>, and fd(t) to denote the document frequency of t, that is, the number of documents in the collection in which the term t appears.",
                "The first strategy we consider is the algorithm proposed by Baeza-Yates and Saint-Jean [2], which consists in selecting the posting lists of the terms with the highest query-term frequencies fq(t).",
                "We call this algorithm Qtf.",
                "We observe that there is a trade-off between fq(t) and fd(t).",
                "Terms with high fq(t) are useful to keep in the cache because they are queried often.",
                "On the other hand, terms with high fd(t) are not good candidates because they correspond to long posting lists and consume a substantial amount of space.",
                "In fact, the problem of selecting the best posting lists for the static cache corresponds to the standard Knapsack problem: given a knapsack of fixed capacity, and a set of n items, such as the i-th item has value ci and size si, select the set of items that fit in the knapsack and maximize the overall value.",
                "In our case, value corresponds to fq(t) and size corresponds to fd(t).",
                "Thus, we employ a simple algorithm for the knapsack problem, which is selecting the posting lists of the terms with the highest values of the ratio fq(t) fd(t) .",
                "We call this algorithm QtfDf.",
                "We tried other variations considering query frequencies instead of term frequencies, but the gain was minimal compared to the complexity added.",
                "In addition to the above two static algorithms we consider the following algorithms for dynamic caching: • LRU: A standard LRU algorithm, but many posting lists might need to be evicted (in order of least-recent usage) until there is enough space in the memory to place the currently accessed posting list; • LFU: A standard LFU algorithm (eviction of the leastfrequently used), with the same modification as the LRU; • Dyn-QtfDf: A dynamic version of the QtfDf algorithm; evict from the cache the term(s) with the lowest fq(t) fd(t) ratio.",
                "The performance of all the above algorithms for 15 weeks of the <br>query log</br> and the UK dataset are shown in Figure 8.",
                "Performance is measured with hit rate.",
                "The cache size is measured as a fraction of the total space required to store the posting lists of all terms.",
                "For the dynamic algorithms, we load the cache with terms in order of fq(t) and we let the cache warm up for 1 million queries.",
                "For the static algorithms, we assume complete knowledge of the frequencies fq(t), that is, we estimate fq(t) from the whole query stream.",
                "As we show in Section 7 the results do not change much if we compute the query-term frequencies using the first 3 or 4 weeks of the <br>query log</br> and measure the hit rate on the rest. 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1 0.1 0.2 0.3 0.4 0.5 0.6 0.7 Hitrate Cache size Caching posting lists static QTF/DF LRU LFU Dyn-QTF/DF QTF Figure 8: Hit rate of different strategies for caching posting lists.",
                "The most important observation from our experiments is that the static QtfDf algorithm has a better hit rate than all the dynamic algorithms.",
                "An important benefit a static cache is that it requires no eviction and it is hence more efficient when evaluating queries.",
                "However, if the characteristics of the query traffic change frequently over time, then it requires re-populating the cache often or there will be a significant impact on hit rate. 6.",
                "ANALYSIS OF STATIC CACHING In this section we provide a detailed analysis for the problem of deciding whether it is preferable to cache query answers or cache posting lists.",
                "Our analysis takes into account the impact of caching between two levels of the data-access hierarchy.",
                "It can either be applied at the memory/disk layer or at a server/remote server layer as in the architecture we discussed in the introduction.",
                "Using a particular system model, we obtain estimates for the parameters required by our analysis, which we subsequently use to decide the optimal trade-off between caching query answers and caching posting lists. 6.1 Analytical Model Let M be the size of the cache measured in answer units (the cache can store M query answers).",
                "Assume that all posting lists are of the same length L, measured in answer units.",
                "We consider the following two cases: (A) a cache that stores only precomputed answers, and (B) a cache that stores only posting lists.",
                "In the first case, Nc = M answers fit in the cache, while in the second case Np = M/L posting lists fit in the cache.",
                "Thus, Np = Nc/L.",
                "Note that although posting lists require more space, we can combine terms to evaluate more queries (or partial queries).",
                "For case (A), suppose that a query answer in the cache can be evaluated in 1 time unit.",
                "For case (B), assume that if the posting lists of the terms of a query are in the cache then the results can be computed in TR1 time units, while if the posting lists are not in the cache then the results can be computed in TR2 time units.",
                "Of course TR2 > TR1.",
                "Now we want to compare the time to answer a stream of Q queries in both cases.",
                "Let Vc(Nc) be the volume of the most frequent Nc queries.",
                "Then, for case (A), we have an overall time TCA = Vc(Nc) + TR2(Q − Vc(Nc)).",
                "Similarly, for case (B), let Vp(Np) be the number of computable queries.",
                "Then we have overall time TP L = TR1Vp(Np) + TR2(Q − Vp(Np)).",
                "We want to check under which conditions we have TP L < TCA.",
                "We have TP L − TCA = (TR2 − 1)Vc(Nc) − (TR2 − TR1)Vp(Np) > 0.",
                "Figure 9 shows the values of Vp and Vc for our data.",
                "We can see that caching answers saturates faster and for this particular data there is no additional benefit from using more than 10% of the index space for caching answers.",
                "As the query distribution is a power law with parameter α > 1, the i-th most frequent query appears with probability proportional to 1 iα .",
                "Therefore, the volume Vc(n), which is the total number of the n most frequent queries, is Vc(n) = V0 n i=1 Q iα = γnQ (0 < γn < 1).",
                "We know that Vp(n) grows faster than Vc(n) and assume, based on experimental results, that the relation is of the form Vp(n) = k Vc(n)β .",
                "In the worst case, for a large cache, β → 1.",
                "That is, both techniques will cache a constant fraction of the overall query volume.",
                "Then caching posting lists makes sense only if L(TR2 − 1) k(TR2 − TR1) > 1.",
                "If we use compression, we have L < L and TR1 > TR1.",
                "According to the experiments that we show later, compression is always better.",
                "For a small cache, we are interested in the transient behavior and then β > 1, as computed from our data.",
                "In this case there will always be a point where TP L > TCA for a large number of queries.",
                "In reality, instead of filling the cache only with answers or only with posting lists, a better strategy will be to divide the total cache space into cache for answers and cache for posting lists.",
                "In such a case, there will be some queries that could be answered by both parts of the cache.",
                "As the answer cache is faster, it will be the first choice for answering those queries.",
                "Let QNc and QNp be the set of queries that can be answered by the cached answers and the cached posting lists, respectively.",
                "Then, the overall time is T = Vc(Nc)+TR1V (QNp −QNc )+TR2(Q−V (QNp ∪QNc )), where Np = (M − Nc)/L.",
                "Finding the optimal division of the cache in order to minimize the overall retrieval time is a difficult problem to solve analytically.",
                "In Section 6.3 we use simulations to derive optimal cache trade-offs for particular implementation examples. 6.2 Parameter Estimation We now use a particular implementation of a centralized system and the model of a distributed system as examples from which we estimate the parameters of the analysis from the previous section.",
                "We perform the experiments using an optimized version of Terrier [11] for both indexing documents and processing queries, on a single machine with a Pentium 4 at 2GHz and 1GB of RAM.",
                "We indexed the documents from the UK-2006 dataset, without removing stop words or applying stemming.",
                "The posting lists in the inverted file consist of pairs of document identifier and term frequency.",
                "We compress the document identifier gaps using Elias gamma encoding, and the 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1 0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1 Queryvolume Space precomputed answers posting lists Figure 9: Cache saturation as a function of size.",
                "Table 2: Ratios between the average time to evaluate a query and the average time to return cached answers (centralized and distributed case).",
                "Centralized system TR1 TR2 TR1 TR2 Full evaluation 233 1760 707 1140 Partial evaluation 99 1626 493 798 LAN system TRL 1 TRL 2 TR L 1 TR L 2 Full evaluation 242 1769 716 1149 Partial evaluation 108 1635 502 807 WAN system TRW 1 TRW 2 TR W 1 TR W 2 Full evaluation 5001 6528 5475 5908 Partial evaluation 4867 6394 5270 5575 term frequencies in documents using unary encoding [16].",
                "The size of the inverted file is 1,189Mb.",
                "A stored answer requires 1264 bytes, and an uncompressed posting takes 8 bytes.",
                "From Table 1, we obtain L = (8·# of postings) 1264·# of terms = 0.75 and L = Inverted file size 1264·# of terms = 0.26.",
                "We estimate the ratio TR = T/Tc between the average time T it takes to evaluate a query and the average time Tc it takes to return a stored answer for the same query, in the following way.",
                "Tc is measured by loading the answers for 100,000 queries in memory, and answering the queries from memory.",
                "The average time is Tc = 0.069ms.",
                "T is measured by processing the same 100,000 queries (the first 10,000 queries are used to warm-up the system).",
                "For each query, we remove stop words, if there are at least three remaining terms.",
                "The stop words correspond to the terms with a frequency higher than the number of documents in the index.",
                "We use a document-at-a-time approach to retrieve documents containing all query terms.",
                "The only disk access required during query processing is for reading compressed posting lists from the inverted file.",
                "We perform both full and partial evaluation of answers, because some queries are likely to retrieve a large number of documents, and only a fraction of the retrieved documents will be seen by users.",
                "In the partial evaluation of queries, we terminate the processing after matching 10,000 documents.",
                "The estimated ratios TR are presented in Table 2.",
                "Figure 10 shows for a sample of queries the workload of the system with partial query evaluation and compressed posting lists.",
                "The x-axis corresponds to the total time the system spends processing a particular query, and the vertical axis corresponds to the sum t∈q fq · fd(t).",
                "Notice that the total number of postings of the query-terms does not necessarily provide an accurate estimate of the workload imposed on the system by a query (which is the case for full evaluation and uncompressed lists). 0 0.2 0.4 0.6 0.8 1 0 0.2 0.4 0.6 0.8 1 Totalpostingstoprocessquery(normalized) Total time to process query (normalized) Partial processing of compressed postings query len = 1 query len in [2,3] query len in [4,8] query len > 8 Figure 10: Workload for partial query evaluation with compressed posting lists.",
                "The analysis of the previous section also applies to a distributed retrieval system in one or multiple sites.",
                "Suppose that a document partitioned distributed system is running on a cluster of machines interconnected with a local area network (LAN) in one site.",
                "The broker receives queries and broadcasts them to the query processors, which answer the queries and return the results to the broker.",
                "Finally, the broker merges the received answers and generates the final set of answers (we assume that the time spent on merging results is negligible).",
                "The difference between the centralized architecture and the document partition architecture is the extra communication between the broker and the query processors.",
                "Using ICMP pings on a 100Mbps LAN, we have measured that sending the query from the broker to the query processors which send an answer of 4,000 bytes back to the broker takes on average 0.615ms.",
                "Hence, TRL = TR + 0.615ms/0.069ms = TR + 9.",
                "In the case when the broker and the query processors are in different sites connected with a wide area network (WAN), we estimated that broadcasting the query from the broker to the query processors and getting back an answer of 4,000 bytes takes on average 329ms.",
                "Hence, TRW = TR + 329ms/0.069ms = TR + 4768. 6.3 Simulation Results We now address the problem of finding the optimal tradeoff between caching query answers and caching posting lists.",
                "To make the problem concrete we assume a fixed budget M on the available memory, out of which x units are used for caching query answers and M − x for caching posting lists.",
                "We perform simulations and compute the average response time as a function of x.",
                "Using a part of the <br>query log</br> as training data, we first allocate in the cache the answers to the most frequent queries that fit in space x, and then we use the rest of the memory to cache posting lists.",
                "For selecting posting lists we use the QtfDf algorithm, applied to the training <br>query log</br> but excluding the queries that have already been cached.",
                "In Figure 11, we plot the simulated response time for a centralized system as a function of x.",
                "For the uncompressed index we use M = 1GB, and for the compressed index we use M = 0.5GB.",
                "In the case of the configuration that uses partial query evaluation with compressed posting lists, the lowest response time is achieved when 0.15GB out of the 0.5GB is allocated for storing answers for queries.",
                "We obtained similar trends in the results for the LAN setting.",
                "Figure 12 shows the simulated workload for a distributed system across a WAN.",
                "In this case, the total amount of memory is split between the broker, which holds the cached 400 500 600 700 800 900 1000 1100 1200 0 0.2 0.4 0.6 0.8 1 Averageresponsetime Space (GB) Simulated workload -- single machine full / uncompr / 1 G partial / uncompr / 1 G full / compr / 0.5 G partial / compr / 0.5 G Figure 11: Optimal division of the cache in a server. 3000 3500 4000 4500 5000 5500 6000 0 0.2 0.4 0.6 0.8 1 Averageresponsetime Space (GB) Simulated workload -- WAN full / uncompr / 1 G partial / uncompr / 1 G full / compr / 0.5 G partial / compr / 0.5 G Figure 12: Optimal division of the cache when the next level requires WAN access. answers of queries, and the query processors, which hold the cache of posting lists.",
                "According to the figure, the difference between the configurations of the query processors is less important because the network communication overhead increases the response time substantially.",
                "When using uncompressed posting lists, the optimal allocation of memory corresponds to using approximately 70% of the memory for caching query answers.",
                "This is explained by the fact that there is no need for network communication when the query can be answered by the cache at the broker. 7.",
                "EFFECT OF THE QUERY DYNAMICS For our <br>query log</br>, the query distribution and query-term distribution change slowly over time.",
                "To support this claim, we first assess how topics change comparing the distribution of queries from the first week in June, 2006, to the distribution of queries for the remainder of 2006 that did not appear in the first week in June.",
                "We found that a very small percentage of queries are new queries.",
                "The majority of queries that appear in a given week repeat in the following weeks for the next six months.",
                "We then compute the hit rate of a static cache of 128, 000 answers trained over a period of two weeks (Figure 13).",
                "We report hit rate hourly for 7 days, starting from 5pm.",
                "We observe that the hit rate reaches its highest value during the night (around midnight), whereas around 2-3pm it reaches its minimum.",
                "After a small decay in hit rate values, the hit rate stabilizes between 0.28, and 0.34 for the entire week, suggesting that the static cache is effective for a whole week after the training period. 0.26 0.27 0.28 0.29 0.3 0.31 0.32 0.33 0.34 0.35 0.36 0.37 0 20 40 60 80 100 120 140 160 Hit-rate Time Hits on the frequent queries of distances Figure 13: Hourly hit rate for a static cache holding 128,000 answers during the period of a week.",
                "The static cache of posting lists can be periodically recomputed.",
                "To estimate the time interval in which we need to recompute the posting lists on the static cache we need to consider an efficiency/quality trade-off: using too short a time interval might be prohibitively expensive, while recomputing the cache too infrequently might lead to having an obsolete cache not corresponding to the statistical characteristics of the current query stream.",
                "We measured the effect on the QtfDf algorithm of the changes in a 15-week query stream (Figure 14).",
                "We compute the query term frequencies over the whole stream, select which terms to cache, and then compute the hit rate on the whole query stream.",
                "This hit rate is as an upper bound, and it assumes perfect knowledge of the query term frequencies.",
                "To simulate a realistic scenario, we use the first 6 (3) weeks of the query stream for computing query term frequencies and the following 9 (12) weeks to estimate the hit rate.",
                "As Figure 14 shows, the hit rate decreases by less than 2%.",
                "The high correlation among the query term frequencies during different time periods explains the graceful adaptation of the static caching algorithms to the future query stream.",
                "Indeed, the pairwise correlation among all possible 3-week periods of the 15-week query stream is over 99.5%. 8.",
                "CONCLUSIONS Caching is an effective technique in search engines for improving response time, reducing the load on query processors, and improving network bandwidth utilization.",
                "We present results on both dynamic and static caching.",
                "Dynamic caching of queries has limited effectiveness due to the high number of compulsory misses caused by the number of unique or infrequent queries.",
                "Our results show that in our UK log, the minimum miss rate is 50% using a working set strategy.",
                "Caching terms is more effective with respect to miss rate, achieving values as low as 12%.",
                "We also propose a new algorithm for static caching of posting lists that outperforms previous static caching algorithms as well as dynamic algorithms such as LRU and LFU, obtaining hit rate values that are over 10% higher compared these strategies.",
                "We present a framework for the analysis of the trade-off between caching query results and caching posting lists, and we simulate different types of architectures.",
                "Our results show that for centralized and LAN environments, there is an optimal allocation of caching query results and caching of posting lists, while for WAN scenarios in which network time prevails it is more important to cache query results. 0.45 0.5 0.55 0.6 0.65 0.7 0.75 0.8 0.85 0.9 0.95 0.1 0.2 0.3 0.4 0.5 0.6 0.7 Hitrate Cache size Dynamics of static QTF/DF caching policy perfect knowledge 6-week training 3-week training Figure 14: Impact of distribution changes on the static caching of posting lists. 9.",
                "REFERENCES [1] V. N. Anh and A. Moffat.",
                "Pruned query evaluation using pre-computed impacts.",
                "In ACM CIKM, 2006. [2] R. A. Baeza-Yates and F. Saint-Jean.",
                "A three level search engine index based in <br>query log</br> distribution.",
                "In SPIRE, 2003. [3] C. Buckley and A. F. Lewit.",
                "Optimization of inverted vector searches.",
                "In ACM SIGIR, 1985. [4] S. B¨uttcher and C. L. A. Clarke.",
                "A document-centric approach to static index pruning in text retrieval systems.",
                "In ACM CIKM, 2006. [5] P. Cao and S. Irani.",
                "Cost-aware WWW proxy caching algorithms.",
                "In USITS, 1997. [6] P. Denning.",
                "Working sets past and present.",
                "IEEE Trans. on Software Engineering, SE-6(1):64-84, 1980. [7] T. Fagni, R. Perego, F. Silvestri, and S. Orlando.",
                "Boosting the performance of web search engines: Caching and prefetching query results by exploiting historical usage data.",
                "ACM Trans.",
                "Inf.",
                "Syst., 24(1):51-78, 2006. [8] R. Lempel and S. Moran.",
                "Predictive caching and prefetching of query results in search engines.",
                "In WWW, 2003. [9] X.",
                "Long and T. Suel.",
                "Three-level caching for efficient query processing in large web search engines.",
                "In WWW, 2005. [10] E. P. Markatos.",
                "On caching search engine query results.",
                "Computer Communications, 24(2):137-143, 2001. [11] I. Ounis, G. Amati, V. Plachouras, B.",
                "He, C. Macdonald, and C. Lioma.",
                "Terrier: A High Performance and Scalable Information Retrieval Platform.",
                "In SIGIR Workshop on Open Source Information Retrieval, 2006. [12] V. V. Raghavan and H. Sever.",
                "On the reuse of past optimal queries.",
                "In ACM SIGIR, 1995. [13] P. C. Saraiva, E. S. de Moura, N. Ziviani, W. Meira, R. Fonseca, and B. Riberio-Neto.",
                "Rank-preserving two-level caching for scalable search engines.",
                "In ACM SIGIR, 2001. [14] D. R. Slutz and I. L. Traiger.",
                "A note on the calculation of average working set size.",
                "Communications of the ACM, 17(10):563-565, 1974. [15] T. Strohman, H. Turtle, and W. B. Croft.",
                "Optimization strategies for complex queries.",
                "In ACM SIGIR, 2005. [16] I. H. Witten, T. C. Bell, and A. Moffat.",
                "Managing Gigabytes: Compressing and Indexing Documents and Images.",
                "John Wiley & Sons, Inc., NY, 1994. [17] N. E. Young.",
                "On-line file caching.",
                "Algorithmica, 33(3):371-383, 2002."
            ],
            "original_annotated_samples": [],
            "translated_annotated_samples": [
                "Usando un \"registro de consultas\" que abarca todo un año, exploramos las limitaciones del almacenamiento en caché y demostramos que las listas de publicación de almacenamiento en caché pueden lograr tasas de aciertos más altas que las respuestas de consulta en caché.",
                "Finalmente, medimos cómo los cambios en el \"registro de consultas\" afectan la efectividad del almacenamiento en caché estático, dada nuestra observación de que la distribución de las consultas cambia lentamente con el tiempo.",
                "Utilizamos un \"registro de consultas\" que abarca un año entero para explorar las limitaciones de las respuestas de consulta de almacenamiento dinámico o publicar listas para términos de consulta.",
                "La longitud promedio de la consulta es de 2.5 términos, con la consulta más larga que tiene 731 términos.1E-07 1E-06 1E-05 1E-04 0.001 0.01 0.1 1 1e-08 1e-07 1e-06 1e-05 1e-04 0.001 0.01 0.1 1 Frecuencia (normalizado) Rango de frecuencia (normalizado) Figura 2: La distribución de la distribución de la distribución de la distribución deconsultas (curva inferior) y términos de consulta (curva media) en el \"registro de consultas\".",
                "Medimos la correlación entre la frecuencia del documento de los términos en la colección y el número de consultas que contienen un término particular en el \"registro de consultas\" para ser 0.424.",
                "En el \"registro de consultas\" que utilizamos, el 88% de las consultas únicas son consultas singleton, y el 44% son consultas singleton fuera de todo el volumen.",
                "Por lo tanto, de todas las consultas en la corriente que componen el \"registro de consultas\", el umbral superior en la relación HIT es del 56%.",
                "En nuestro \"registro de consultas\", solo el 4% de los términos aparecen una vez, pero esto representa el 73% del vocabulario de los términos de consulta.",
                "Este gráfico muestra solo un período de 122 días, y normalizamos los valores por el valor máximo observado durante todo el período del \"registro de consultas\".",
                "Los tamaños del conjunto de trabajo se normalizan contra el número total de consultas en el \"registro de consultas\".",
                "Utilizamos FQ (t) para denotar la frecuencia a plazo de consulta de un término t, es decir, el número de consultas que contienen t en el \"registro de consultas\" y fd (t) para denotar la frecuencia del documento de t, es decir, es, es decir,El número de documentos en la colección en la que aparece el término t.",
                "El rendimiento de todos los algoritmos anteriores durante 15 semanas del \"registro de consultas\" y el conjunto de datos del Reino Unido se muestran en la Figura 8.",
                "Como mostramos en la Sección 7, los resultados no cambian mucho si calculamos las frecuencias a plazo de consulta utilizando las primeras 3 o 4 semanas del \"registro de consultas\" y medimos la tasa de aciertos en el resto.0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1 0.1 0.2 0.3 0.4 0.5 0.6 0.7 HITRATE Tamaño de caché Publicación de caché de la publicación Listas de Publicación estática QTF/DF LRU LFU DYN-QTF/DF QTF Figura 8: Tasa de aciertos de diferentes estrategias para las listas de publicaciones en caché.",
                "Utilizando una parte del \"registro de consultas\" como datos de entrenamiento, primero asignamos en el caché las respuestas a las consultas más frecuentes que se ajustan en el espacio X, y luego usamos el resto de la memoria para caché listas de publicación.",
                "Para seleccionar listas de publicación, utilizamos el algoritmo QTFDF, aplicado al \"registro de consultas\" de capacitación pero excluyendo las consultas que ya se han almacenado en caché.",
                "Efecto de la dinámica de la consulta para nuestro \"registro de consultas\", la distribución de consultas y la distribución de consultas cambian lentamente con el tiempo.",
                "Un índice de motor de búsqueda de tres niveles basado en la distribución de \"registro de consultas\"."
            ],
            "translated_text": "",
            "candidates": [
                "registro de consultas",
                "registro de consultas",
                "registro de consultas",
                "registro de consultas",
                "registro de consultas",
                "registro de consultas",
                "registro de consultas",
                "registro de consultas",
                "registro de consultas",
                "registro de consultas",
                "registro de consultas",
                "registro de consultas",
                "registro de consultas",
                "registro de consultas",
                "registro de consultas",
                "registro de consultas",
                "registro de consultas",
                "registro de consultas",
                "registro de consultas",
                "registro de consultas",
                "registro de consultas",
                "registro de consultas",
                "Registro de consultas",
                "registro de consultas",
                "registro de consultas",
                "registro de consultas",
                "registro de consultas",
                "registro de consultas",
                "registro de consultas",
                "registro de consultas",
                "registro de consultas",
                "registro de consultas",
                "registro de consultas",
                "registro de consultas"
            ],
            "error": []
        },
        "effectiveness of static caching": {
            "translated_key": "efectividad del almacenamiento en caché estático",
            "is_in_text": true,
            "original_annotated_sentences": [
                "The Impact of Caching on Search Engines Ricardo Baeza-Yates1 rbaeza@acm.org Aristides Gionis1 gionis@yahoo-inc.com Flavio Junqueira1 fpj@yahoo-inc.com Vanessa Murdock1 vmurdock@yahoo-inc.com Vassilis Plachouras1 vassilis@yahoo-inc.com Fabrizio Silvestri2 f.silvestri@isti.cnr.it 1 Yahoo!",
                "Research Barcelona 2 ISTI - CNR Barcelona, SPAIN Pisa, ITALY ABSTRACT In this paper we study the trade-offs in designing efficient caching systems for Web search engines.",
                "We explore the impact of different approaches, such as static vs. dynamic caching, and caching query results vs. caching posting lists.",
                "Using a query log spanning a whole year we explore the limitations of caching and we demonstrate that caching posting lists can achieve higher hit rates than caching query answers.",
                "We propose a new algorithm for static caching of posting lists, which outperforms previous methods.",
                "We also study the problem of finding the optimal way to split the static cache between answers and posting lists.",
                "Finally, we measure how the changes in the query log affect the <br>effectiveness of static caching</br>, given our observation that the distribution of the queries changes slowly over time.",
                "Our results and observations are applicable to different levels of the data-access hierarchy, for instance, for a memory/disk layer or a broker/remote server layer.",
                "Categories and Subject Descriptors H.3.3 [Information Storage and Retrieval]: Information Search and Retrieval - Search process; H.3.4 [Information Storage and Retrieval]: Systems and Software - Distributed systems, Performance evaluation (efficiency and effectiveness) General Terms Algorithms, Experimentation 1.",
                "INTRODUCTION Millions of queries are submitted daily to Web search engines, and users have high expectations of the quality and speed of the answers.",
                "As the searchable Web becomes larger and larger, with more than 20 billion pages to index, evaluating a single query requires processing large amounts of data.",
                "In such a setting, to achieve a fast response time and to increase the query throughput, using a cache is crucial.",
                "The primary use of a cache memory is to speedup computation by exploiting frequently or recently used data, although reducing the workload to back-end servers is also a major goal.",
                "Caching can be applied at different levels with increasing response latencies or processing requirements.",
                "For example, the different levels may correspond to the main memory, the disk, or resources in a local or a wide area network.",
                "The decision of what to cache is either off-line (static) or online (dynamic).",
                "A static cache is based on historical information and is periodically updated.",
                "A dynamic cache replaces entries according to the sequence of requests.",
                "When a new request arrives, the cache system decides whether to evict some entry from the cache in the case of a cache miss.",
                "Such online decisions are based on a cache policy, and several different policies have been studied in the past.",
                "For a search engine, there are two possible ways to use a cache memory: Caching answers: As the engine returns answers to a particular query, it may decide to store these answers to resolve future queries.",
                "Caching terms: As the engine evaluates a particular query, it may decide to store in memory the posting lists of the involved query terms.",
                "Often the whole set of posting lists does not fit in memory, and consequently, the engine has to select a small set to keep in memory and speed up query processing.",
                "Returning an answer to a query that already exists in the cache is more efficient than computing the answer using cached posting lists.",
                "On the other hand, previously unseen queries occur more often than previously unseen terms, implying a higher miss rate for cached answers.",
                "Caching of posting lists has additional challenges.",
                "As posting lists have variable size, caching them dynamically is not very efficient, due to the complexity in terms of efficiency and space, and the skewed distribution of the query stream, as shown later.",
                "Static caching of posting lists poses even more challenges: when deciding which terms to cache one faces the trade-off between frequently queried terms and terms with small posting lists that are space efficient.",
                "Finally, before deciding to adopt a static caching policy the query stream should be analyzed to verify that its characteristics do not change rapidly over time.",
                "Broker Static caching posting lists Dynamic/Static cached answers Local query processor Disk Next caching level Local network access Remote network access Figure 1: One caching level in a distributed search architecture.",
                "In this paper we explore the trade-offs in the design of each cache level, showing that the problem is the same and only a few parameters change.",
                "In general, we assume that each level of caching in a distributed search architecture is similar to that shown in Figure 1.",
                "We use a query log spanning a whole year to explore the limitations of dynamically caching query answers or posting lists for query terms.",
                "More concretely, our main conclusions are that: • Caching query answers results in lower hit ratios compared to caching of posting lists for query terms, but it is faster because there is no need for query evaluation.",
                "We provide a framework for the analysis of the trade-off between static caching of query answers and posting lists; • Static caching of terms can be more effective than dynamic caching with, for example, LRU.",
                "We provide algorithms based on the Knapsack problem for selecting the posting lists to put in a static cache, and we show improvements over previous work, achieving a hit ratio over 90%; • Changes of the query distribution over time have little impact on static caching.",
                "The remainder of this paper is organized as follows.",
                "Sections 2 and 3 summarize related work and characterize the data sets we use.",
                "Section 4 discusses the limitations of dynamic caching.",
                "Sections 5 and 6 introduce algorithms for caching posting lists, and a theoretical framework for the analysis of static caching, respectively.",
                "Section 7 discusses the impact of changes in the query distribution on static caching, and Section 8 provides concluding remarks. 2.",
                "RELATED WORK There is a large body of work devoted to query optimization.",
                "Buckley and Lewit [3], in one of the earliest works, take a term-at-a-time approach to deciding when inverted lists need not be further examined.",
                "More recent examples demonstrate that the top k documents for a query can be returned without the need for evaluating the complete set of posting lists [1, 4, 15].",
                "Although these approaches seek to improve query processing efficiency, they differ from our current work in that they do not consider caching.",
                "They may be considered separate and complementary to a cache-based approach.",
                "Raghavan and Sever [12], in one of the first papers on exploiting user query history, propose using a query base, built upon a set of persistent optimal queries submitted in the past, to improve the retrieval effectiveness for similar future queries.",
                "Markatos [10] shows the existence of temporal locality in queries, and compares the performance of different caching policies.",
                "Based on the observations of Markatos, Lempel and Moran propose a new caching policy, called Probabilistic Driven Caching, by attempting to estimate the probability distribution of all possible queries submitted to a search engine [8].",
                "Fagni et al. follow Markatos work by showing that combining static and dynamic caching policies together with an adaptive prefetching policy achieves a high hit ratio [7].",
                "Different from our work, they consider caching and prefetching of pages of results.",
                "As systems are often hierarchical, there has also been some effort on multi-level architectures.",
                "Saraiva et al. propose a new architecture for Web search engines using a two-level dynamic caching system [13].",
                "Their goal for such systems has been to improve response time for hierarchical engines.",
                "In their architecture, both levels use an LRU eviction policy.",
                "They find that the second-level cache can effectively reduce disk traffic, thus increasing the overall throughput.",
                "Baeza-Yates and Saint-Jean propose a three-level index organization [2].",
                "Long and Suel propose a caching system structured according to three different levels [9].",
                "The intermediate level contains frequently occurring pairs of terms and stores the intersections of the corresponding inverted lists.",
                "These last two papers are related to ours in that they exploit different caching strategies at different levels of the memory hierarchy.",
                "Finally, our static caching algorithm for posting lists in Section 5 uses the ratio frequency/size in order to evaluate the goodness of an item to cache.",
                "Similar ideas have been used in the context of file caching [17], Web caching [5], and even caching of posting lists [9], but in all cases in a dynamic setting.",
                "To the best of our knowledge we are the first to use this approach for static caching of posting lists. 3.",
                "DATA CHARACTERIZATION Our data consists of a crawl of documents from the UK domain, and query logs of one year of queries submitted to http://www.yahoo.co.uk from November 2005 to November 2006.",
                "In our logs, 50% of the total volume of queries are unique.",
                "The average query length is 2.5 terms, with the longest query having 731 terms. 1e-07 1e-06 1e-05 1e-04 0.001 0.01 0.1 1 1e-08 1e-07 1e-06 1e-05 1e-04 0.001 0.01 0.1 1 Frequency(normalized) Frequency rank (normalized) Figure 2: The distribution of queries (bottom curve) and query terms (middle curve) in the query log.",
                "The distribution of document frequencies of terms in the UK-2006 dataset (upper curve).",
                "Figure 2 shows the distributions of queries (lower curve), and query terms (middle curve).",
                "The x-axis represents the normalized frequency rank of the query or term. (The most frequent query appears closest to the y-axis.)",
                "The y-axis is Table 1: Statistics of the UK-2006 sample.",
                "UK-2006 sample statistics # of documents 2,786,391 # of terms 6,491,374 # of tokens 2,109,512,558 the normalized frequency for a given query (or term).",
                "As expected, the distribution of query frequencies and query term frequencies follow power law distributions, with slope of 1.84 and 2.26, respectively.",
                "In this figure, the query frequencies were computed as they appear in the logs with no normalization for case or white space.",
                "The query terms (middle curve) have been normalized for case, as have the terms in the document collection.",
                "The document collection that we use for our experiments is a summary of the UK domain crawled in May 2006.1 This summary corresponds to a maximum of 400 crawled documents per host, using a breadth first crawling strategy, comprising 15GB.",
                "The distribution of document frequencies of terms in the collection follows a power law distribution with slope 2.38 (upper curve in Figure 2).",
                "The statistics of the collection are shown in Table 1.",
                "We measured the correlation between the document frequency of terms in the collection and the number of queries that contain a particular term in the query log to be 0.424.",
                "A scatter plot for a random sample of terms is shown in Figure 3.",
                "In this experiment, terms have been converted to lower case in both the queries and the documents so that the frequencies will be comparable. 1e-07 1e-06 1e-05 1e-04 0.001 0.01 0.1 1 1e-06 1e-05 1e-04 0.001 0.01 0.1 1 Queryfrequency Document frequency Figure 3: Normalized scatter plot of document-term frequencies vs. query-term frequencies. 4.",
                "CACHING OF QUERIES AND TERMS Caching relies upon the assumption that there is locality in the stream of requests.",
                "That is, there must be sufficient repetition in the stream of requests and within intervals of time that enable a cache memory of reasonable size to be effective.",
                "In the query log we used, 88% of the unique queries are singleton queries, and 44% are singleton queries out of the whole volume.",
                "Thus, out of all queries in the stream composing the query log, the upper threshold on hit ratio is 56%.",
                "This is because only 56% of all the queries comprise queries that have multiple occurrences.",
                "It is important to observe, however, that not all queries in this 56% can be cache hits because of compulsory misses.",
                "A compulsory miss 1 The collection is available from the University of Milan: http://law.dsi.unimi.it/.",
                "URL retrieved 05/2007. 0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1 240 260 280 300 320 340 360 Numberofelements Bin number Total terms Terms diff Total queries Unique queries Unique terms Query diff Figure 4: Arrival rate for both terms and queries. happens when the cache receives a query for the first time.",
                "This is different from capacity misses, which happen due to space constraints on the amount of memory the cache uses.",
                "If we consider a cache with infinite memory, then the hit ratio is 50%.",
                "Note that for an infinite cache there are no capacity misses.",
                "As we mentioned before, another possibility is to cache the posting lists of terms.",
                "Intuitively, this gives more freedom in the utilization of the cache content to respond to queries because cached terms might form a new query.",
                "On the other hand, they need more space.",
                "As opposed to queries, the fraction of singleton terms in the total volume of terms is smaller.",
                "In our query log, only 4% of the terms appear once, but this accounts for 73% of the vocabulary of query terms.",
                "We show in Section 5 that caching a small fraction of terms, while accounting for terms appearing in many documents, is potentially very effective.",
                "Figure 4 shows several graphs corresponding to the normalized arrival rate for different cases using days as bins.",
                "That is, we plot the normalized number of elements that appear in a day.",
                "This graph shows only a period of 122 days, and we normalize the values by the maximum value observed throughout the whole period of the query log.",
                "Total queries and Total terms correspond to the total volume of queries and terms, respectively.",
                "Unique queries and Unique terms correspond to the arrival rate of unique queries and terms.",
                "Finally, Query diff and Terms diff correspond to the difference between the curves for total and unique.",
                "In Figure 4, as expected, the volume of terms is much higher than the volume of queries.",
                "The difference between the total number of terms and the number of unique terms is much larger than the difference between the total number of queries and the number of unique queries.",
                "This observation implies that terms repeat significantly more than queries.",
                "If we use smaller bins, say of one hour, then the ratio of unique to volume is higher for both terms and queries because it leaves less room for repetition.",
                "We also estimated the workload using the document frequency of terms as a measure of how much work a query imposes on a search engine.",
                "We found that it follows closely the arrival rate for terms shown in Figure 4.",
                "To demonstrate the effect of a dynamic cache on the query frequency distribution of Figure 2, we plot the same frequency graph, but now considering the frequency of queries Figure 5: Frequency graph after LRU cache. after going through an LRU cache.",
                "On a cache miss, an LRU cache decides upon an entry to evict using the information on the recency of queries.",
                "In this graph, the most frequent queries are not the same queries that were most frequent before the cache.",
                "It is possible that queries that are most frequent after the cache have different characteristics, and tuning the search engine to queries frequent before the cache may degrade performance for non-cached queries.",
                "The maximum frequency after caching is less than 1% of the maximum frequency before the cache, thus showing that the cache is very effective in reducing the load of frequent queries.",
                "If we re-rank the queries according to after-cache frequency, the distribution is still a power law, but with a much smaller value for the highest frequency.",
                "When discussing the effectiveness of dynamically caching, an important metric is cache miss rate.",
                "To analyze the cache miss rate for different memory constraints, we use the working set model [6, 14].",
                "A working set, informally, is the set of references that an application or an operating system is currently working with.",
                "The model uses such sets in a strategy that tries to capture the temporal locality of references.",
                "The working set strategy then consists in keeping in memory only the elements that are referenced in the previous θ steps of the input sequence, where θ is a configurable parameter corresponding to the window size.",
                "Originally, working sets have been used for page replacement algorithms of operating systems, and considering such a strategy in the context of search engines is interesting for three reasons.",
                "First, it captures the amount of locality of queries and terms in a sequence of queries.",
                "Locality in this case refers to the frequency of queries and terms in a window of time.",
                "If many queries appear multiple times in a window, then locality is high.",
                "Second, it enables an oﬄine analysis of the expected miss rate given different memory constraints.",
                "Third, working sets capture aspects of efficient caching algorithms such as LRU.",
                "LRU assumes that references farther in the past are less likely to be referenced in the present, which is implicit in the concept of working sets [14].",
                "Figure 6 plots the miss rate for different working set sizes, and we consider working sets of both queries and terms.",
                "The working set sizes are normalized against the total number of queries in the query log.",
                "In the graph for queries, there is a sharp decay until approximately 0.01, and the rate at which the miss rate drops decreases as we increase the size of the working set over 0.01.",
                "Finally, the minimum value it reaches is 50% miss rate, not shown in the figure as we have cut the tail of the curve for presentation purposes. 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1 0 0.05 0.1 0.15 0.2 Missrate Normalized working set size Queries Terms Figure 6: Miss rate as a function of the working set size. 1 10 100 1000 10000 100000 1e+06 Frequency Distance Figure 7: Distribution of distances expressed in terms of distinct queries.",
                "Compared to the query curve, we observe that the minimum miss rate for terms is substantially smaller.",
                "The miss rate also drops sharply on values up to 0.01, and it decreases minimally for higher values.",
                "The minimum value, however, is slightly over 10%, which is much smaller than the minimum value for the sequence of queries.",
                "This implies that with such a policy it is possible to achieve over 80% hit rate, if we consider caching dynamically posting lists for terms as opposed to caching answers for queries.",
                "This result does not consider the space required for each unit stored in the cache memory, or the amount of time it takes to put together a response to a user query.",
                "We analyze these issues more carefully later in this paper.",
                "It is interesting also to observe the histogram of Figure 7, which is an intermediate step in the computation of the miss rate graph.",
                "It reports the distribution of distances between repetitions of the same frequent query.",
                "The distance in the plot is measured in the number of distinct queries separating a query and its repetition, and it considers only queries appearing at least 10 times.",
                "From Figures 6 and 7, we conclude that even if we set the size of the query answers cache to a relatively large number of entries, the miss rate is high.",
                "Thus, caching the posting lists of terms has the potential to improve the hit ratio.",
                "This is what we explore next. 5.",
                "CACHING POSTING LISTS The previous section shows that caching posting lists can obtain a higher hit rate compared to caching query answers.",
                "In this section we study the problem of how to select posting lists to place on a certain amount of available memory, assuming that the whole index is larger than the amount of memory available.",
                "The posting lists have variable size (in fact, their size distribution follows a power law), so it is beneficial for a caching policy to consider the sizes of the posting lists.",
                "We consider both dynamic and static caching.",
                "For dynamic caching, we use two well-known policies, LRU and LFU, as well as a modified algorithm that takes posting-list size into account.",
                "Before discussing the static caching strategies, we introduce some notation.",
                "We use fq(t) to denote the query-term frequency of a term t, that is, the number of queries containing t in the query log, and fd(t) to denote the document frequency of t, that is, the number of documents in the collection in which the term t appears.",
                "The first strategy we consider is the algorithm proposed by Baeza-Yates and Saint-Jean [2], which consists in selecting the posting lists of the terms with the highest query-term frequencies fq(t).",
                "We call this algorithm Qtf.",
                "We observe that there is a trade-off between fq(t) and fd(t).",
                "Terms with high fq(t) are useful to keep in the cache because they are queried often.",
                "On the other hand, terms with high fd(t) are not good candidates because they correspond to long posting lists and consume a substantial amount of space.",
                "In fact, the problem of selecting the best posting lists for the static cache corresponds to the standard Knapsack problem: given a knapsack of fixed capacity, and a set of n items, such as the i-th item has value ci and size si, select the set of items that fit in the knapsack and maximize the overall value.",
                "In our case, value corresponds to fq(t) and size corresponds to fd(t).",
                "Thus, we employ a simple algorithm for the knapsack problem, which is selecting the posting lists of the terms with the highest values of the ratio fq(t) fd(t) .",
                "We call this algorithm QtfDf.",
                "We tried other variations considering query frequencies instead of term frequencies, but the gain was minimal compared to the complexity added.",
                "In addition to the above two static algorithms we consider the following algorithms for dynamic caching: • LRU: A standard LRU algorithm, but many posting lists might need to be evicted (in order of least-recent usage) until there is enough space in the memory to place the currently accessed posting list; • LFU: A standard LFU algorithm (eviction of the leastfrequently used), with the same modification as the LRU; • Dyn-QtfDf: A dynamic version of the QtfDf algorithm; evict from the cache the term(s) with the lowest fq(t) fd(t) ratio.",
                "The performance of all the above algorithms for 15 weeks of the query log and the UK dataset are shown in Figure 8.",
                "Performance is measured with hit rate.",
                "The cache size is measured as a fraction of the total space required to store the posting lists of all terms.",
                "For the dynamic algorithms, we load the cache with terms in order of fq(t) and we let the cache warm up for 1 million queries.",
                "For the static algorithms, we assume complete knowledge of the frequencies fq(t), that is, we estimate fq(t) from the whole query stream.",
                "As we show in Section 7 the results do not change much if we compute the query-term frequencies using the first 3 or 4 weeks of the query log and measure the hit rate on the rest. 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1 0.1 0.2 0.3 0.4 0.5 0.6 0.7 Hitrate Cache size Caching posting lists static QTF/DF LRU LFU Dyn-QTF/DF QTF Figure 8: Hit rate of different strategies for caching posting lists.",
                "The most important observation from our experiments is that the static QtfDf algorithm has a better hit rate than all the dynamic algorithms.",
                "An important benefit a static cache is that it requires no eviction and it is hence more efficient when evaluating queries.",
                "However, if the characteristics of the query traffic change frequently over time, then it requires re-populating the cache often or there will be a significant impact on hit rate. 6.",
                "ANALYSIS OF STATIC CACHING In this section we provide a detailed analysis for the problem of deciding whether it is preferable to cache query answers or cache posting lists.",
                "Our analysis takes into account the impact of caching between two levels of the data-access hierarchy.",
                "It can either be applied at the memory/disk layer or at a server/remote server layer as in the architecture we discussed in the introduction.",
                "Using a particular system model, we obtain estimates for the parameters required by our analysis, which we subsequently use to decide the optimal trade-off between caching query answers and caching posting lists. 6.1 Analytical Model Let M be the size of the cache measured in answer units (the cache can store M query answers).",
                "Assume that all posting lists are of the same length L, measured in answer units.",
                "We consider the following two cases: (A) a cache that stores only precomputed answers, and (B) a cache that stores only posting lists.",
                "In the first case, Nc = M answers fit in the cache, while in the second case Np = M/L posting lists fit in the cache.",
                "Thus, Np = Nc/L.",
                "Note that although posting lists require more space, we can combine terms to evaluate more queries (or partial queries).",
                "For case (A), suppose that a query answer in the cache can be evaluated in 1 time unit.",
                "For case (B), assume that if the posting lists of the terms of a query are in the cache then the results can be computed in TR1 time units, while if the posting lists are not in the cache then the results can be computed in TR2 time units.",
                "Of course TR2 > TR1.",
                "Now we want to compare the time to answer a stream of Q queries in both cases.",
                "Let Vc(Nc) be the volume of the most frequent Nc queries.",
                "Then, for case (A), we have an overall time TCA = Vc(Nc) + TR2(Q − Vc(Nc)).",
                "Similarly, for case (B), let Vp(Np) be the number of computable queries.",
                "Then we have overall time TP L = TR1Vp(Np) + TR2(Q − Vp(Np)).",
                "We want to check under which conditions we have TP L < TCA.",
                "We have TP L − TCA = (TR2 − 1)Vc(Nc) − (TR2 − TR1)Vp(Np) > 0.",
                "Figure 9 shows the values of Vp and Vc for our data.",
                "We can see that caching answers saturates faster and for this particular data there is no additional benefit from using more than 10% of the index space for caching answers.",
                "As the query distribution is a power law with parameter α > 1, the i-th most frequent query appears with probability proportional to 1 iα .",
                "Therefore, the volume Vc(n), which is the total number of the n most frequent queries, is Vc(n) = V0 n i=1 Q iα = γnQ (0 < γn < 1).",
                "We know that Vp(n) grows faster than Vc(n) and assume, based on experimental results, that the relation is of the form Vp(n) = k Vc(n)β .",
                "In the worst case, for a large cache, β → 1.",
                "That is, both techniques will cache a constant fraction of the overall query volume.",
                "Then caching posting lists makes sense only if L(TR2 − 1) k(TR2 − TR1) > 1.",
                "If we use compression, we have L < L and TR1 > TR1.",
                "According to the experiments that we show later, compression is always better.",
                "For a small cache, we are interested in the transient behavior and then β > 1, as computed from our data.",
                "In this case there will always be a point where TP L > TCA for a large number of queries.",
                "In reality, instead of filling the cache only with answers or only with posting lists, a better strategy will be to divide the total cache space into cache for answers and cache for posting lists.",
                "In such a case, there will be some queries that could be answered by both parts of the cache.",
                "As the answer cache is faster, it will be the first choice for answering those queries.",
                "Let QNc and QNp be the set of queries that can be answered by the cached answers and the cached posting lists, respectively.",
                "Then, the overall time is T = Vc(Nc)+TR1V (QNp −QNc )+TR2(Q−V (QNp ∪QNc )), where Np = (M − Nc)/L.",
                "Finding the optimal division of the cache in order to minimize the overall retrieval time is a difficult problem to solve analytically.",
                "In Section 6.3 we use simulations to derive optimal cache trade-offs for particular implementation examples. 6.2 Parameter Estimation We now use a particular implementation of a centralized system and the model of a distributed system as examples from which we estimate the parameters of the analysis from the previous section.",
                "We perform the experiments using an optimized version of Terrier [11] for both indexing documents and processing queries, on a single machine with a Pentium 4 at 2GHz and 1GB of RAM.",
                "We indexed the documents from the UK-2006 dataset, without removing stop words or applying stemming.",
                "The posting lists in the inverted file consist of pairs of document identifier and term frequency.",
                "We compress the document identifier gaps using Elias gamma encoding, and the 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1 0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1 Queryvolume Space precomputed answers posting lists Figure 9: Cache saturation as a function of size.",
                "Table 2: Ratios between the average time to evaluate a query and the average time to return cached answers (centralized and distributed case).",
                "Centralized system TR1 TR2 TR1 TR2 Full evaluation 233 1760 707 1140 Partial evaluation 99 1626 493 798 LAN system TRL 1 TRL 2 TR L 1 TR L 2 Full evaluation 242 1769 716 1149 Partial evaluation 108 1635 502 807 WAN system TRW 1 TRW 2 TR W 1 TR W 2 Full evaluation 5001 6528 5475 5908 Partial evaluation 4867 6394 5270 5575 term frequencies in documents using unary encoding [16].",
                "The size of the inverted file is 1,189Mb.",
                "A stored answer requires 1264 bytes, and an uncompressed posting takes 8 bytes.",
                "From Table 1, we obtain L = (8·# of postings) 1264·# of terms = 0.75 and L = Inverted file size 1264·# of terms = 0.26.",
                "We estimate the ratio TR = T/Tc between the average time T it takes to evaluate a query and the average time Tc it takes to return a stored answer for the same query, in the following way.",
                "Tc is measured by loading the answers for 100,000 queries in memory, and answering the queries from memory.",
                "The average time is Tc = 0.069ms.",
                "T is measured by processing the same 100,000 queries (the first 10,000 queries are used to warm-up the system).",
                "For each query, we remove stop words, if there are at least three remaining terms.",
                "The stop words correspond to the terms with a frequency higher than the number of documents in the index.",
                "We use a document-at-a-time approach to retrieve documents containing all query terms.",
                "The only disk access required during query processing is for reading compressed posting lists from the inverted file.",
                "We perform both full and partial evaluation of answers, because some queries are likely to retrieve a large number of documents, and only a fraction of the retrieved documents will be seen by users.",
                "In the partial evaluation of queries, we terminate the processing after matching 10,000 documents.",
                "The estimated ratios TR are presented in Table 2.",
                "Figure 10 shows for a sample of queries the workload of the system with partial query evaluation and compressed posting lists.",
                "The x-axis corresponds to the total time the system spends processing a particular query, and the vertical axis corresponds to the sum t∈q fq · fd(t).",
                "Notice that the total number of postings of the query-terms does not necessarily provide an accurate estimate of the workload imposed on the system by a query (which is the case for full evaluation and uncompressed lists). 0 0.2 0.4 0.6 0.8 1 0 0.2 0.4 0.6 0.8 1 Totalpostingstoprocessquery(normalized) Total time to process query (normalized) Partial processing of compressed postings query len = 1 query len in [2,3] query len in [4,8] query len > 8 Figure 10: Workload for partial query evaluation with compressed posting lists.",
                "The analysis of the previous section also applies to a distributed retrieval system in one or multiple sites.",
                "Suppose that a document partitioned distributed system is running on a cluster of machines interconnected with a local area network (LAN) in one site.",
                "The broker receives queries and broadcasts them to the query processors, which answer the queries and return the results to the broker.",
                "Finally, the broker merges the received answers and generates the final set of answers (we assume that the time spent on merging results is negligible).",
                "The difference between the centralized architecture and the document partition architecture is the extra communication between the broker and the query processors.",
                "Using ICMP pings on a 100Mbps LAN, we have measured that sending the query from the broker to the query processors which send an answer of 4,000 bytes back to the broker takes on average 0.615ms.",
                "Hence, TRL = TR + 0.615ms/0.069ms = TR + 9.",
                "In the case when the broker and the query processors are in different sites connected with a wide area network (WAN), we estimated that broadcasting the query from the broker to the query processors and getting back an answer of 4,000 bytes takes on average 329ms.",
                "Hence, TRW = TR + 329ms/0.069ms = TR + 4768. 6.3 Simulation Results We now address the problem of finding the optimal tradeoff between caching query answers and caching posting lists.",
                "To make the problem concrete we assume a fixed budget M on the available memory, out of which x units are used for caching query answers and M − x for caching posting lists.",
                "We perform simulations and compute the average response time as a function of x.",
                "Using a part of the query log as training data, we first allocate in the cache the answers to the most frequent queries that fit in space x, and then we use the rest of the memory to cache posting lists.",
                "For selecting posting lists we use the QtfDf algorithm, applied to the training query log but excluding the queries that have already been cached.",
                "In Figure 11, we plot the simulated response time for a centralized system as a function of x.",
                "For the uncompressed index we use M = 1GB, and for the compressed index we use M = 0.5GB.",
                "In the case of the configuration that uses partial query evaluation with compressed posting lists, the lowest response time is achieved when 0.15GB out of the 0.5GB is allocated for storing answers for queries.",
                "We obtained similar trends in the results for the LAN setting.",
                "Figure 12 shows the simulated workload for a distributed system across a WAN.",
                "In this case, the total amount of memory is split between the broker, which holds the cached 400 500 600 700 800 900 1000 1100 1200 0 0.2 0.4 0.6 0.8 1 Averageresponsetime Space (GB) Simulated workload -- single machine full / uncompr / 1 G partial / uncompr / 1 G full / compr / 0.5 G partial / compr / 0.5 G Figure 11: Optimal division of the cache in a server. 3000 3500 4000 4500 5000 5500 6000 0 0.2 0.4 0.6 0.8 1 Averageresponsetime Space (GB) Simulated workload -- WAN full / uncompr / 1 G partial / uncompr / 1 G full / compr / 0.5 G partial / compr / 0.5 G Figure 12: Optimal division of the cache when the next level requires WAN access. answers of queries, and the query processors, which hold the cache of posting lists.",
                "According to the figure, the difference between the configurations of the query processors is less important because the network communication overhead increases the response time substantially.",
                "When using uncompressed posting lists, the optimal allocation of memory corresponds to using approximately 70% of the memory for caching query answers.",
                "This is explained by the fact that there is no need for network communication when the query can be answered by the cache at the broker. 7.",
                "EFFECT OF THE QUERY DYNAMICS For our query log, the query distribution and query-term distribution change slowly over time.",
                "To support this claim, we first assess how topics change comparing the distribution of queries from the first week in June, 2006, to the distribution of queries for the remainder of 2006 that did not appear in the first week in June.",
                "We found that a very small percentage of queries are new queries.",
                "The majority of queries that appear in a given week repeat in the following weeks for the next six months.",
                "We then compute the hit rate of a static cache of 128, 000 answers trained over a period of two weeks (Figure 13).",
                "We report hit rate hourly for 7 days, starting from 5pm.",
                "We observe that the hit rate reaches its highest value during the night (around midnight), whereas around 2-3pm it reaches its minimum.",
                "After a small decay in hit rate values, the hit rate stabilizes between 0.28, and 0.34 for the entire week, suggesting that the static cache is effective for a whole week after the training period. 0.26 0.27 0.28 0.29 0.3 0.31 0.32 0.33 0.34 0.35 0.36 0.37 0 20 40 60 80 100 120 140 160 Hit-rate Time Hits on the frequent queries of distances Figure 13: Hourly hit rate for a static cache holding 128,000 answers during the period of a week.",
                "The static cache of posting lists can be periodically recomputed.",
                "To estimate the time interval in which we need to recompute the posting lists on the static cache we need to consider an efficiency/quality trade-off: using too short a time interval might be prohibitively expensive, while recomputing the cache too infrequently might lead to having an obsolete cache not corresponding to the statistical characteristics of the current query stream.",
                "We measured the effect on the QtfDf algorithm of the changes in a 15-week query stream (Figure 14).",
                "We compute the query term frequencies over the whole stream, select which terms to cache, and then compute the hit rate on the whole query stream.",
                "This hit rate is as an upper bound, and it assumes perfect knowledge of the query term frequencies.",
                "To simulate a realistic scenario, we use the first 6 (3) weeks of the query stream for computing query term frequencies and the following 9 (12) weeks to estimate the hit rate.",
                "As Figure 14 shows, the hit rate decreases by less than 2%.",
                "The high correlation among the query term frequencies during different time periods explains the graceful adaptation of the static caching algorithms to the future query stream.",
                "Indeed, the pairwise correlation among all possible 3-week periods of the 15-week query stream is over 99.5%. 8.",
                "CONCLUSIONS Caching is an effective technique in search engines for improving response time, reducing the load on query processors, and improving network bandwidth utilization.",
                "We present results on both dynamic and static caching.",
                "Dynamic caching of queries has limited effectiveness due to the high number of compulsory misses caused by the number of unique or infrequent queries.",
                "Our results show that in our UK log, the minimum miss rate is 50% using a working set strategy.",
                "Caching terms is more effective with respect to miss rate, achieving values as low as 12%.",
                "We also propose a new algorithm for static caching of posting lists that outperforms previous static caching algorithms as well as dynamic algorithms such as LRU and LFU, obtaining hit rate values that are over 10% higher compared these strategies.",
                "We present a framework for the analysis of the trade-off between caching query results and caching posting lists, and we simulate different types of architectures.",
                "Our results show that for centralized and LAN environments, there is an optimal allocation of caching query results and caching of posting lists, while for WAN scenarios in which network time prevails it is more important to cache query results. 0.45 0.5 0.55 0.6 0.65 0.7 0.75 0.8 0.85 0.9 0.95 0.1 0.2 0.3 0.4 0.5 0.6 0.7 Hitrate Cache size Dynamics of static QTF/DF caching policy perfect knowledge 6-week training 3-week training Figure 14: Impact of distribution changes on the static caching of posting lists. 9.",
                "REFERENCES [1] V. N. Anh and A. Moffat.",
                "Pruned query evaluation using pre-computed impacts.",
                "In ACM CIKM, 2006. [2] R. A. Baeza-Yates and F. Saint-Jean.",
                "A three level search engine index based in query log distribution.",
                "In SPIRE, 2003. [3] C. Buckley and A. F. Lewit.",
                "Optimization of inverted vector searches.",
                "In ACM SIGIR, 1985. [4] S. B¨uttcher and C. L. A. Clarke.",
                "A document-centric approach to static index pruning in text retrieval systems.",
                "In ACM CIKM, 2006. [5] P. Cao and S. Irani.",
                "Cost-aware WWW proxy caching algorithms.",
                "In USITS, 1997. [6] P. Denning.",
                "Working sets past and present.",
                "IEEE Trans. on Software Engineering, SE-6(1):64-84, 1980. [7] T. Fagni, R. Perego, F. Silvestri, and S. Orlando.",
                "Boosting the performance of web search engines: Caching and prefetching query results by exploiting historical usage data.",
                "ACM Trans.",
                "Inf.",
                "Syst., 24(1):51-78, 2006. [8] R. Lempel and S. Moran.",
                "Predictive caching and prefetching of query results in search engines.",
                "In WWW, 2003. [9] X.",
                "Long and T. Suel.",
                "Three-level caching for efficient query processing in large web search engines.",
                "In WWW, 2005. [10] E. P. Markatos.",
                "On caching search engine query results.",
                "Computer Communications, 24(2):137-143, 2001. [11] I. Ounis, G. Amati, V. Plachouras, B.",
                "He, C. Macdonald, and C. Lioma.",
                "Terrier: A High Performance and Scalable Information Retrieval Platform.",
                "In SIGIR Workshop on Open Source Information Retrieval, 2006. [12] V. V. Raghavan and H. Sever.",
                "On the reuse of past optimal queries.",
                "In ACM SIGIR, 1995. [13] P. C. Saraiva, E. S. de Moura, N. Ziviani, W. Meira, R. Fonseca, and B. Riberio-Neto.",
                "Rank-preserving two-level caching for scalable search engines.",
                "In ACM SIGIR, 2001. [14] D. R. Slutz and I. L. Traiger.",
                "A note on the calculation of average working set size.",
                "Communications of the ACM, 17(10):563-565, 1974. [15] T. Strohman, H. Turtle, and W. B. Croft.",
                "Optimization strategies for complex queries.",
                "In ACM SIGIR, 2005. [16] I. H. Witten, T. C. Bell, and A. Moffat.",
                "Managing Gigabytes: Compressing and Indexing Documents and Images.",
                "John Wiley & Sons, Inc., NY, 1994. [17] N. E. Young.",
                "On-line file caching.",
                "Algorithmica, 33(3):371-383, 2002."
            ],
            "original_annotated_samples": [],
            "translated_annotated_samples": [
                "Finalmente, medimos cómo los cambios en el registro de consultas afectan la \"efectividad del almacenamiento en caché estático\", dada nuestra observación de que la distribución de las consultas cambia lentamente con el tiempo."
            ],
            "translated_text": "",
            "candidates": [
                "efectividad del almacenamiento en caché estático",
                "efectividad del almacenamiento en caché estático"
            ],
            "error": []
        },
        "static caching effectiveness": {
            "translated_key": "efectividad del almacenamiento en caché estático",
            "is_in_text": false,
            "original_annotated_sentences": [
                "The Impact of Caching on Search Engines Ricardo Baeza-Yates1 rbaeza@acm.org Aristides Gionis1 gionis@yahoo-inc.com Flavio Junqueira1 fpj@yahoo-inc.com Vanessa Murdock1 vmurdock@yahoo-inc.com Vassilis Plachouras1 vassilis@yahoo-inc.com Fabrizio Silvestri2 f.silvestri@isti.cnr.it 1 Yahoo!",
                "Research Barcelona 2 ISTI - CNR Barcelona, SPAIN Pisa, ITALY ABSTRACT In this paper we study the trade-offs in designing efficient caching systems for Web search engines.",
                "We explore the impact of different approaches, such as static vs. dynamic caching, and caching query results vs. caching posting lists.",
                "Using a query log spanning a whole year we explore the limitations of caching and we demonstrate that caching posting lists can achieve higher hit rates than caching query answers.",
                "We propose a new algorithm for static caching of posting lists, which outperforms previous methods.",
                "We also study the problem of finding the optimal way to split the static cache between answers and posting lists.",
                "Finally, we measure how the changes in the query log affect the effectiveness of static caching, given our observation that the distribution of the queries changes slowly over time.",
                "Our results and observations are applicable to different levels of the data-access hierarchy, for instance, for a memory/disk layer or a broker/remote server layer.",
                "Categories and Subject Descriptors H.3.3 [Information Storage and Retrieval]: Information Search and Retrieval - Search process; H.3.4 [Information Storage and Retrieval]: Systems and Software - Distributed systems, Performance evaluation (efficiency and effectiveness) General Terms Algorithms, Experimentation 1.",
                "INTRODUCTION Millions of queries are submitted daily to Web search engines, and users have high expectations of the quality and speed of the answers.",
                "As the searchable Web becomes larger and larger, with more than 20 billion pages to index, evaluating a single query requires processing large amounts of data.",
                "In such a setting, to achieve a fast response time and to increase the query throughput, using a cache is crucial.",
                "The primary use of a cache memory is to speedup computation by exploiting frequently or recently used data, although reducing the workload to back-end servers is also a major goal.",
                "Caching can be applied at different levels with increasing response latencies or processing requirements.",
                "For example, the different levels may correspond to the main memory, the disk, or resources in a local or a wide area network.",
                "The decision of what to cache is either off-line (static) or online (dynamic).",
                "A static cache is based on historical information and is periodically updated.",
                "A dynamic cache replaces entries according to the sequence of requests.",
                "When a new request arrives, the cache system decides whether to evict some entry from the cache in the case of a cache miss.",
                "Such online decisions are based on a cache policy, and several different policies have been studied in the past.",
                "For a search engine, there are two possible ways to use a cache memory: Caching answers: As the engine returns answers to a particular query, it may decide to store these answers to resolve future queries.",
                "Caching terms: As the engine evaluates a particular query, it may decide to store in memory the posting lists of the involved query terms.",
                "Often the whole set of posting lists does not fit in memory, and consequently, the engine has to select a small set to keep in memory and speed up query processing.",
                "Returning an answer to a query that already exists in the cache is more efficient than computing the answer using cached posting lists.",
                "On the other hand, previously unseen queries occur more often than previously unseen terms, implying a higher miss rate for cached answers.",
                "Caching of posting lists has additional challenges.",
                "As posting lists have variable size, caching them dynamically is not very efficient, due to the complexity in terms of efficiency and space, and the skewed distribution of the query stream, as shown later.",
                "Static caching of posting lists poses even more challenges: when deciding which terms to cache one faces the trade-off between frequently queried terms and terms with small posting lists that are space efficient.",
                "Finally, before deciding to adopt a static caching policy the query stream should be analyzed to verify that its characteristics do not change rapidly over time.",
                "Broker Static caching posting lists Dynamic/Static cached answers Local query processor Disk Next caching level Local network access Remote network access Figure 1: One caching level in a distributed search architecture.",
                "In this paper we explore the trade-offs in the design of each cache level, showing that the problem is the same and only a few parameters change.",
                "In general, we assume that each level of caching in a distributed search architecture is similar to that shown in Figure 1.",
                "We use a query log spanning a whole year to explore the limitations of dynamically caching query answers or posting lists for query terms.",
                "More concretely, our main conclusions are that: • Caching query answers results in lower hit ratios compared to caching of posting lists for query terms, but it is faster because there is no need for query evaluation.",
                "We provide a framework for the analysis of the trade-off between static caching of query answers and posting lists; • Static caching of terms can be more effective than dynamic caching with, for example, LRU.",
                "We provide algorithms based on the Knapsack problem for selecting the posting lists to put in a static cache, and we show improvements over previous work, achieving a hit ratio over 90%; • Changes of the query distribution over time have little impact on static caching.",
                "The remainder of this paper is organized as follows.",
                "Sections 2 and 3 summarize related work and characterize the data sets we use.",
                "Section 4 discusses the limitations of dynamic caching.",
                "Sections 5 and 6 introduce algorithms for caching posting lists, and a theoretical framework for the analysis of static caching, respectively.",
                "Section 7 discusses the impact of changes in the query distribution on static caching, and Section 8 provides concluding remarks. 2.",
                "RELATED WORK There is a large body of work devoted to query optimization.",
                "Buckley and Lewit [3], in one of the earliest works, take a term-at-a-time approach to deciding when inverted lists need not be further examined.",
                "More recent examples demonstrate that the top k documents for a query can be returned without the need for evaluating the complete set of posting lists [1, 4, 15].",
                "Although these approaches seek to improve query processing efficiency, they differ from our current work in that they do not consider caching.",
                "They may be considered separate and complementary to a cache-based approach.",
                "Raghavan and Sever [12], in one of the first papers on exploiting user query history, propose using a query base, built upon a set of persistent optimal queries submitted in the past, to improve the retrieval effectiveness for similar future queries.",
                "Markatos [10] shows the existence of temporal locality in queries, and compares the performance of different caching policies.",
                "Based on the observations of Markatos, Lempel and Moran propose a new caching policy, called Probabilistic Driven Caching, by attempting to estimate the probability distribution of all possible queries submitted to a search engine [8].",
                "Fagni et al. follow Markatos work by showing that combining static and dynamic caching policies together with an adaptive prefetching policy achieves a high hit ratio [7].",
                "Different from our work, they consider caching and prefetching of pages of results.",
                "As systems are often hierarchical, there has also been some effort on multi-level architectures.",
                "Saraiva et al. propose a new architecture for Web search engines using a two-level dynamic caching system [13].",
                "Their goal for such systems has been to improve response time for hierarchical engines.",
                "In their architecture, both levels use an LRU eviction policy.",
                "They find that the second-level cache can effectively reduce disk traffic, thus increasing the overall throughput.",
                "Baeza-Yates and Saint-Jean propose a three-level index organization [2].",
                "Long and Suel propose a caching system structured according to three different levels [9].",
                "The intermediate level contains frequently occurring pairs of terms and stores the intersections of the corresponding inverted lists.",
                "These last two papers are related to ours in that they exploit different caching strategies at different levels of the memory hierarchy.",
                "Finally, our static caching algorithm for posting lists in Section 5 uses the ratio frequency/size in order to evaluate the goodness of an item to cache.",
                "Similar ideas have been used in the context of file caching [17], Web caching [5], and even caching of posting lists [9], but in all cases in a dynamic setting.",
                "To the best of our knowledge we are the first to use this approach for static caching of posting lists. 3.",
                "DATA CHARACTERIZATION Our data consists of a crawl of documents from the UK domain, and query logs of one year of queries submitted to http://www.yahoo.co.uk from November 2005 to November 2006.",
                "In our logs, 50% of the total volume of queries are unique.",
                "The average query length is 2.5 terms, with the longest query having 731 terms. 1e-07 1e-06 1e-05 1e-04 0.001 0.01 0.1 1 1e-08 1e-07 1e-06 1e-05 1e-04 0.001 0.01 0.1 1 Frequency(normalized) Frequency rank (normalized) Figure 2: The distribution of queries (bottom curve) and query terms (middle curve) in the query log.",
                "The distribution of document frequencies of terms in the UK-2006 dataset (upper curve).",
                "Figure 2 shows the distributions of queries (lower curve), and query terms (middle curve).",
                "The x-axis represents the normalized frequency rank of the query or term. (The most frequent query appears closest to the y-axis.)",
                "The y-axis is Table 1: Statistics of the UK-2006 sample.",
                "UK-2006 sample statistics # of documents 2,786,391 # of terms 6,491,374 # of tokens 2,109,512,558 the normalized frequency for a given query (or term).",
                "As expected, the distribution of query frequencies and query term frequencies follow power law distributions, with slope of 1.84 and 2.26, respectively.",
                "In this figure, the query frequencies were computed as they appear in the logs with no normalization for case or white space.",
                "The query terms (middle curve) have been normalized for case, as have the terms in the document collection.",
                "The document collection that we use for our experiments is a summary of the UK domain crawled in May 2006.1 This summary corresponds to a maximum of 400 crawled documents per host, using a breadth first crawling strategy, comprising 15GB.",
                "The distribution of document frequencies of terms in the collection follows a power law distribution with slope 2.38 (upper curve in Figure 2).",
                "The statistics of the collection are shown in Table 1.",
                "We measured the correlation between the document frequency of terms in the collection and the number of queries that contain a particular term in the query log to be 0.424.",
                "A scatter plot for a random sample of terms is shown in Figure 3.",
                "In this experiment, terms have been converted to lower case in both the queries and the documents so that the frequencies will be comparable. 1e-07 1e-06 1e-05 1e-04 0.001 0.01 0.1 1 1e-06 1e-05 1e-04 0.001 0.01 0.1 1 Queryfrequency Document frequency Figure 3: Normalized scatter plot of document-term frequencies vs. query-term frequencies. 4.",
                "CACHING OF QUERIES AND TERMS Caching relies upon the assumption that there is locality in the stream of requests.",
                "That is, there must be sufficient repetition in the stream of requests and within intervals of time that enable a cache memory of reasonable size to be effective.",
                "In the query log we used, 88% of the unique queries are singleton queries, and 44% are singleton queries out of the whole volume.",
                "Thus, out of all queries in the stream composing the query log, the upper threshold on hit ratio is 56%.",
                "This is because only 56% of all the queries comprise queries that have multiple occurrences.",
                "It is important to observe, however, that not all queries in this 56% can be cache hits because of compulsory misses.",
                "A compulsory miss 1 The collection is available from the University of Milan: http://law.dsi.unimi.it/.",
                "URL retrieved 05/2007. 0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1 240 260 280 300 320 340 360 Numberofelements Bin number Total terms Terms diff Total queries Unique queries Unique terms Query diff Figure 4: Arrival rate for both terms and queries. happens when the cache receives a query for the first time.",
                "This is different from capacity misses, which happen due to space constraints on the amount of memory the cache uses.",
                "If we consider a cache with infinite memory, then the hit ratio is 50%.",
                "Note that for an infinite cache there are no capacity misses.",
                "As we mentioned before, another possibility is to cache the posting lists of terms.",
                "Intuitively, this gives more freedom in the utilization of the cache content to respond to queries because cached terms might form a new query.",
                "On the other hand, they need more space.",
                "As opposed to queries, the fraction of singleton terms in the total volume of terms is smaller.",
                "In our query log, only 4% of the terms appear once, but this accounts for 73% of the vocabulary of query terms.",
                "We show in Section 5 that caching a small fraction of terms, while accounting for terms appearing in many documents, is potentially very effective.",
                "Figure 4 shows several graphs corresponding to the normalized arrival rate for different cases using days as bins.",
                "That is, we plot the normalized number of elements that appear in a day.",
                "This graph shows only a period of 122 days, and we normalize the values by the maximum value observed throughout the whole period of the query log.",
                "Total queries and Total terms correspond to the total volume of queries and terms, respectively.",
                "Unique queries and Unique terms correspond to the arrival rate of unique queries and terms.",
                "Finally, Query diff and Terms diff correspond to the difference between the curves for total and unique.",
                "In Figure 4, as expected, the volume of terms is much higher than the volume of queries.",
                "The difference between the total number of terms and the number of unique terms is much larger than the difference between the total number of queries and the number of unique queries.",
                "This observation implies that terms repeat significantly more than queries.",
                "If we use smaller bins, say of one hour, then the ratio of unique to volume is higher for both terms and queries because it leaves less room for repetition.",
                "We also estimated the workload using the document frequency of terms as a measure of how much work a query imposes on a search engine.",
                "We found that it follows closely the arrival rate for terms shown in Figure 4.",
                "To demonstrate the effect of a dynamic cache on the query frequency distribution of Figure 2, we plot the same frequency graph, but now considering the frequency of queries Figure 5: Frequency graph after LRU cache. after going through an LRU cache.",
                "On a cache miss, an LRU cache decides upon an entry to evict using the information on the recency of queries.",
                "In this graph, the most frequent queries are not the same queries that were most frequent before the cache.",
                "It is possible that queries that are most frequent after the cache have different characteristics, and tuning the search engine to queries frequent before the cache may degrade performance for non-cached queries.",
                "The maximum frequency after caching is less than 1% of the maximum frequency before the cache, thus showing that the cache is very effective in reducing the load of frequent queries.",
                "If we re-rank the queries according to after-cache frequency, the distribution is still a power law, but with a much smaller value for the highest frequency.",
                "When discussing the effectiveness of dynamically caching, an important metric is cache miss rate.",
                "To analyze the cache miss rate for different memory constraints, we use the working set model [6, 14].",
                "A working set, informally, is the set of references that an application or an operating system is currently working with.",
                "The model uses such sets in a strategy that tries to capture the temporal locality of references.",
                "The working set strategy then consists in keeping in memory only the elements that are referenced in the previous θ steps of the input sequence, where θ is a configurable parameter corresponding to the window size.",
                "Originally, working sets have been used for page replacement algorithms of operating systems, and considering such a strategy in the context of search engines is interesting for three reasons.",
                "First, it captures the amount of locality of queries and terms in a sequence of queries.",
                "Locality in this case refers to the frequency of queries and terms in a window of time.",
                "If many queries appear multiple times in a window, then locality is high.",
                "Second, it enables an oﬄine analysis of the expected miss rate given different memory constraints.",
                "Third, working sets capture aspects of efficient caching algorithms such as LRU.",
                "LRU assumes that references farther in the past are less likely to be referenced in the present, which is implicit in the concept of working sets [14].",
                "Figure 6 plots the miss rate for different working set sizes, and we consider working sets of both queries and terms.",
                "The working set sizes are normalized against the total number of queries in the query log.",
                "In the graph for queries, there is a sharp decay until approximately 0.01, and the rate at which the miss rate drops decreases as we increase the size of the working set over 0.01.",
                "Finally, the minimum value it reaches is 50% miss rate, not shown in the figure as we have cut the tail of the curve for presentation purposes. 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1 0 0.05 0.1 0.15 0.2 Missrate Normalized working set size Queries Terms Figure 6: Miss rate as a function of the working set size. 1 10 100 1000 10000 100000 1e+06 Frequency Distance Figure 7: Distribution of distances expressed in terms of distinct queries.",
                "Compared to the query curve, we observe that the minimum miss rate for terms is substantially smaller.",
                "The miss rate also drops sharply on values up to 0.01, and it decreases minimally for higher values.",
                "The minimum value, however, is slightly over 10%, which is much smaller than the minimum value for the sequence of queries.",
                "This implies that with such a policy it is possible to achieve over 80% hit rate, if we consider caching dynamically posting lists for terms as opposed to caching answers for queries.",
                "This result does not consider the space required for each unit stored in the cache memory, or the amount of time it takes to put together a response to a user query.",
                "We analyze these issues more carefully later in this paper.",
                "It is interesting also to observe the histogram of Figure 7, which is an intermediate step in the computation of the miss rate graph.",
                "It reports the distribution of distances between repetitions of the same frequent query.",
                "The distance in the plot is measured in the number of distinct queries separating a query and its repetition, and it considers only queries appearing at least 10 times.",
                "From Figures 6 and 7, we conclude that even if we set the size of the query answers cache to a relatively large number of entries, the miss rate is high.",
                "Thus, caching the posting lists of terms has the potential to improve the hit ratio.",
                "This is what we explore next. 5.",
                "CACHING POSTING LISTS The previous section shows that caching posting lists can obtain a higher hit rate compared to caching query answers.",
                "In this section we study the problem of how to select posting lists to place on a certain amount of available memory, assuming that the whole index is larger than the amount of memory available.",
                "The posting lists have variable size (in fact, their size distribution follows a power law), so it is beneficial for a caching policy to consider the sizes of the posting lists.",
                "We consider both dynamic and static caching.",
                "For dynamic caching, we use two well-known policies, LRU and LFU, as well as a modified algorithm that takes posting-list size into account.",
                "Before discussing the static caching strategies, we introduce some notation.",
                "We use fq(t) to denote the query-term frequency of a term t, that is, the number of queries containing t in the query log, and fd(t) to denote the document frequency of t, that is, the number of documents in the collection in which the term t appears.",
                "The first strategy we consider is the algorithm proposed by Baeza-Yates and Saint-Jean [2], which consists in selecting the posting lists of the terms with the highest query-term frequencies fq(t).",
                "We call this algorithm Qtf.",
                "We observe that there is a trade-off between fq(t) and fd(t).",
                "Terms with high fq(t) are useful to keep in the cache because they are queried often.",
                "On the other hand, terms with high fd(t) are not good candidates because they correspond to long posting lists and consume a substantial amount of space.",
                "In fact, the problem of selecting the best posting lists for the static cache corresponds to the standard Knapsack problem: given a knapsack of fixed capacity, and a set of n items, such as the i-th item has value ci and size si, select the set of items that fit in the knapsack and maximize the overall value.",
                "In our case, value corresponds to fq(t) and size corresponds to fd(t).",
                "Thus, we employ a simple algorithm for the knapsack problem, which is selecting the posting lists of the terms with the highest values of the ratio fq(t) fd(t) .",
                "We call this algorithm QtfDf.",
                "We tried other variations considering query frequencies instead of term frequencies, but the gain was minimal compared to the complexity added.",
                "In addition to the above two static algorithms we consider the following algorithms for dynamic caching: • LRU: A standard LRU algorithm, but many posting lists might need to be evicted (in order of least-recent usage) until there is enough space in the memory to place the currently accessed posting list; • LFU: A standard LFU algorithm (eviction of the leastfrequently used), with the same modification as the LRU; • Dyn-QtfDf: A dynamic version of the QtfDf algorithm; evict from the cache the term(s) with the lowest fq(t) fd(t) ratio.",
                "The performance of all the above algorithms for 15 weeks of the query log and the UK dataset are shown in Figure 8.",
                "Performance is measured with hit rate.",
                "The cache size is measured as a fraction of the total space required to store the posting lists of all terms.",
                "For the dynamic algorithms, we load the cache with terms in order of fq(t) and we let the cache warm up for 1 million queries.",
                "For the static algorithms, we assume complete knowledge of the frequencies fq(t), that is, we estimate fq(t) from the whole query stream.",
                "As we show in Section 7 the results do not change much if we compute the query-term frequencies using the first 3 or 4 weeks of the query log and measure the hit rate on the rest. 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1 0.1 0.2 0.3 0.4 0.5 0.6 0.7 Hitrate Cache size Caching posting lists static QTF/DF LRU LFU Dyn-QTF/DF QTF Figure 8: Hit rate of different strategies for caching posting lists.",
                "The most important observation from our experiments is that the static QtfDf algorithm has a better hit rate than all the dynamic algorithms.",
                "An important benefit a static cache is that it requires no eviction and it is hence more efficient when evaluating queries.",
                "However, if the characteristics of the query traffic change frequently over time, then it requires re-populating the cache often or there will be a significant impact on hit rate. 6.",
                "ANALYSIS OF STATIC CACHING In this section we provide a detailed analysis for the problem of deciding whether it is preferable to cache query answers or cache posting lists.",
                "Our analysis takes into account the impact of caching between two levels of the data-access hierarchy.",
                "It can either be applied at the memory/disk layer or at a server/remote server layer as in the architecture we discussed in the introduction.",
                "Using a particular system model, we obtain estimates for the parameters required by our analysis, which we subsequently use to decide the optimal trade-off between caching query answers and caching posting lists. 6.1 Analytical Model Let M be the size of the cache measured in answer units (the cache can store M query answers).",
                "Assume that all posting lists are of the same length L, measured in answer units.",
                "We consider the following two cases: (A) a cache that stores only precomputed answers, and (B) a cache that stores only posting lists.",
                "In the first case, Nc = M answers fit in the cache, while in the second case Np = M/L posting lists fit in the cache.",
                "Thus, Np = Nc/L.",
                "Note that although posting lists require more space, we can combine terms to evaluate more queries (or partial queries).",
                "For case (A), suppose that a query answer in the cache can be evaluated in 1 time unit.",
                "For case (B), assume that if the posting lists of the terms of a query are in the cache then the results can be computed in TR1 time units, while if the posting lists are not in the cache then the results can be computed in TR2 time units.",
                "Of course TR2 > TR1.",
                "Now we want to compare the time to answer a stream of Q queries in both cases.",
                "Let Vc(Nc) be the volume of the most frequent Nc queries.",
                "Then, for case (A), we have an overall time TCA = Vc(Nc) + TR2(Q − Vc(Nc)).",
                "Similarly, for case (B), let Vp(Np) be the number of computable queries.",
                "Then we have overall time TP L = TR1Vp(Np) + TR2(Q − Vp(Np)).",
                "We want to check under which conditions we have TP L < TCA.",
                "We have TP L − TCA = (TR2 − 1)Vc(Nc) − (TR2 − TR1)Vp(Np) > 0.",
                "Figure 9 shows the values of Vp and Vc for our data.",
                "We can see that caching answers saturates faster and for this particular data there is no additional benefit from using more than 10% of the index space for caching answers.",
                "As the query distribution is a power law with parameter α > 1, the i-th most frequent query appears with probability proportional to 1 iα .",
                "Therefore, the volume Vc(n), which is the total number of the n most frequent queries, is Vc(n) = V0 n i=1 Q iα = γnQ (0 < γn < 1).",
                "We know that Vp(n) grows faster than Vc(n) and assume, based on experimental results, that the relation is of the form Vp(n) = k Vc(n)β .",
                "In the worst case, for a large cache, β → 1.",
                "That is, both techniques will cache a constant fraction of the overall query volume.",
                "Then caching posting lists makes sense only if L(TR2 − 1) k(TR2 − TR1) > 1.",
                "If we use compression, we have L < L and TR1 > TR1.",
                "According to the experiments that we show later, compression is always better.",
                "For a small cache, we are interested in the transient behavior and then β > 1, as computed from our data.",
                "In this case there will always be a point where TP L > TCA for a large number of queries.",
                "In reality, instead of filling the cache only with answers or only with posting lists, a better strategy will be to divide the total cache space into cache for answers and cache for posting lists.",
                "In such a case, there will be some queries that could be answered by both parts of the cache.",
                "As the answer cache is faster, it will be the first choice for answering those queries.",
                "Let QNc and QNp be the set of queries that can be answered by the cached answers and the cached posting lists, respectively.",
                "Then, the overall time is T = Vc(Nc)+TR1V (QNp −QNc )+TR2(Q−V (QNp ∪QNc )), where Np = (M − Nc)/L.",
                "Finding the optimal division of the cache in order to minimize the overall retrieval time is a difficult problem to solve analytically.",
                "In Section 6.3 we use simulations to derive optimal cache trade-offs for particular implementation examples. 6.2 Parameter Estimation We now use a particular implementation of a centralized system and the model of a distributed system as examples from which we estimate the parameters of the analysis from the previous section.",
                "We perform the experiments using an optimized version of Terrier [11] for both indexing documents and processing queries, on a single machine with a Pentium 4 at 2GHz and 1GB of RAM.",
                "We indexed the documents from the UK-2006 dataset, without removing stop words or applying stemming.",
                "The posting lists in the inverted file consist of pairs of document identifier and term frequency.",
                "We compress the document identifier gaps using Elias gamma encoding, and the 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1 0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1 Queryvolume Space precomputed answers posting lists Figure 9: Cache saturation as a function of size.",
                "Table 2: Ratios between the average time to evaluate a query and the average time to return cached answers (centralized and distributed case).",
                "Centralized system TR1 TR2 TR1 TR2 Full evaluation 233 1760 707 1140 Partial evaluation 99 1626 493 798 LAN system TRL 1 TRL 2 TR L 1 TR L 2 Full evaluation 242 1769 716 1149 Partial evaluation 108 1635 502 807 WAN system TRW 1 TRW 2 TR W 1 TR W 2 Full evaluation 5001 6528 5475 5908 Partial evaluation 4867 6394 5270 5575 term frequencies in documents using unary encoding [16].",
                "The size of the inverted file is 1,189Mb.",
                "A stored answer requires 1264 bytes, and an uncompressed posting takes 8 bytes.",
                "From Table 1, we obtain L = (8·# of postings) 1264·# of terms = 0.75 and L = Inverted file size 1264·# of terms = 0.26.",
                "We estimate the ratio TR = T/Tc between the average time T it takes to evaluate a query and the average time Tc it takes to return a stored answer for the same query, in the following way.",
                "Tc is measured by loading the answers for 100,000 queries in memory, and answering the queries from memory.",
                "The average time is Tc = 0.069ms.",
                "T is measured by processing the same 100,000 queries (the first 10,000 queries are used to warm-up the system).",
                "For each query, we remove stop words, if there are at least three remaining terms.",
                "The stop words correspond to the terms with a frequency higher than the number of documents in the index.",
                "We use a document-at-a-time approach to retrieve documents containing all query terms.",
                "The only disk access required during query processing is for reading compressed posting lists from the inverted file.",
                "We perform both full and partial evaluation of answers, because some queries are likely to retrieve a large number of documents, and only a fraction of the retrieved documents will be seen by users.",
                "In the partial evaluation of queries, we terminate the processing after matching 10,000 documents.",
                "The estimated ratios TR are presented in Table 2.",
                "Figure 10 shows for a sample of queries the workload of the system with partial query evaluation and compressed posting lists.",
                "The x-axis corresponds to the total time the system spends processing a particular query, and the vertical axis corresponds to the sum t∈q fq · fd(t).",
                "Notice that the total number of postings of the query-terms does not necessarily provide an accurate estimate of the workload imposed on the system by a query (which is the case for full evaluation and uncompressed lists). 0 0.2 0.4 0.6 0.8 1 0 0.2 0.4 0.6 0.8 1 Totalpostingstoprocessquery(normalized) Total time to process query (normalized) Partial processing of compressed postings query len = 1 query len in [2,3] query len in [4,8] query len > 8 Figure 10: Workload for partial query evaluation with compressed posting lists.",
                "The analysis of the previous section also applies to a distributed retrieval system in one or multiple sites.",
                "Suppose that a document partitioned distributed system is running on a cluster of machines interconnected with a local area network (LAN) in one site.",
                "The broker receives queries and broadcasts them to the query processors, which answer the queries and return the results to the broker.",
                "Finally, the broker merges the received answers and generates the final set of answers (we assume that the time spent on merging results is negligible).",
                "The difference between the centralized architecture and the document partition architecture is the extra communication between the broker and the query processors.",
                "Using ICMP pings on a 100Mbps LAN, we have measured that sending the query from the broker to the query processors which send an answer of 4,000 bytes back to the broker takes on average 0.615ms.",
                "Hence, TRL = TR + 0.615ms/0.069ms = TR + 9.",
                "In the case when the broker and the query processors are in different sites connected with a wide area network (WAN), we estimated that broadcasting the query from the broker to the query processors and getting back an answer of 4,000 bytes takes on average 329ms.",
                "Hence, TRW = TR + 329ms/0.069ms = TR + 4768. 6.3 Simulation Results We now address the problem of finding the optimal tradeoff between caching query answers and caching posting lists.",
                "To make the problem concrete we assume a fixed budget M on the available memory, out of which x units are used for caching query answers and M − x for caching posting lists.",
                "We perform simulations and compute the average response time as a function of x.",
                "Using a part of the query log as training data, we first allocate in the cache the answers to the most frequent queries that fit in space x, and then we use the rest of the memory to cache posting lists.",
                "For selecting posting lists we use the QtfDf algorithm, applied to the training query log but excluding the queries that have already been cached.",
                "In Figure 11, we plot the simulated response time for a centralized system as a function of x.",
                "For the uncompressed index we use M = 1GB, and for the compressed index we use M = 0.5GB.",
                "In the case of the configuration that uses partial query evaluation with compressed posting lists, the lowest response time is achieved when 0.15GB out of the 0.5GB is allocated for storing answers for queries.",
                "We obtained similar trends in the results for the LAN setting.",
                "Figure 12 shows the simulated workload for a distributed system across a WAN.",
                "In this case, the total amount of memory is split between the broker, which holds the cached 400 500 600 700 800 900 1000 1100 1200 0 0.2 0.4 0.6 0.8 1 Averageresponsetime Space (GB) Simulated workload -- single machine full / uncompr / 1 G partial / uncompr / 1 G full / compr / 0.5 G partial / compr / 0.5 G Figure 11: Optimal division of the cache in a server. 3000 3500 4000 4500 5000 5500 6000 0 0.2 0.4 0.6 0.8 1 Averageresponsetime Space (GB) Simulated workload -- WAN full / uncompr / 1 G partial / uncompr / 1 G full / compr / 0.5 G partial / compr / 0.5 G Figure 12: Optimal division of the cache when the next level requires WAN access. answers of queries, and the query processors, which hold the cache of posting lists.",
                "According to the figure, the difference between the configurations of the query processors is less important because the network communication overhead increases the response time substantially.",
                "When using uncompressed posting lists, the optimal allocation of memory corresponds to using approximately 70% of the memory for caching query answers.",
                "This is explained by the fact that there is no need for network communication when the query can be answered by the cache at the broker. 7.",
                "EFFECT OF THE QUERY DYNAMICS For our query log, the query distribution and query-term distribution change slowly over time.",
                "To support this claim, we first assess how topics change comparing the distribution of queries from the first week in June, 2006, to the distribution of queries for the remainder of 2006 that did not appear in the first week in June.",
                "We found that a very small percentage of queries are new queries.",
                "The majority of queries that appear in a given week repeat in the following weeks for the next six months.",
                "We then compute the hit rate of a static cache of 128, 000 answers trained over a period of two weeks (Figure 13).",
                "We report hit rate hourly for 7 days, starting from 5pm.",
                "We observe that the hit rate reaches its highest value during the night (around midnight), whereas around 2-3pm it reaches its minimum.",
                "After a small decay in hit rate values, the hit rate stabilizes between 0.28, and 0.34 for the entire week, suggesting that the static cache is effective for a whole week after the training period. 0.26 0.27 0.28 0.29 0.3 0.31 0.32 0.33 0.34 0.35 0.36 0.37 0 20 40 60 80 100 120 140 160 Hit-rate Time Hits on the frequent queries of distances Figure 13: Hourly hit rate for a static cache holding 128,000 answers during the period of a week.",
                "The static cache of posting lists can be periodically recomputed.",
                "To estimate the time interval in which we need to recompute the posting lists on the static cache we need to consider an efficiency/quality trade-off: using too short a time interval might be prohibitively expensive, while recomputing the cache too infrequently might lead to having an obsolete cache not corresponding to the statistical characteristics of the current query stream.",
                "We measured the effect on the QtfDf algorithm of the changes in a 15-week query stream (Figure 14).",
                "We compute the query term frequencies over the whole stream, select which terms to cache, and then compute the hit rate on the whole query stream.",
                "This hit rate is as an upper bound, and it assumes perfect knowledge of the query term frequencies.",
                "To simulate a realistic scenario, we use the first 6 (3) weeks of the query stream for computing query term frequencies and the following 9 (12) weeks to estimate the hit rate.",
                "As Figure 14 shows, the hit rate decreases by less than 2%.",
                "The high correlation among the query term frequencies during different time periods explains the graceful adaptation of the static caching algorithms to the future query stream.",
                "Indeed, the pairwise correlation among all possible 3-week periods of the 15-week query stream is over 99.5%. 8.",
                "CONCLUSIONS Caching is an effective technique in search engines for improving response time, reducing the load on query processors, and improving network bandwidth utilization.",
                "We present results on both dynamic and static caching.",
                "Dynamic caching of queries has limited effectiveness due to the high number of compulsory misses caused by the number of unique or infrequent queries.",
                "Our results show that in our UK log, the minimum miss rate is 50% using a working set strategy.",
                "Caching terms is more effective with respect to miss rate, achieving values as low as 12%.",
                "We also propose a new algorithm for static caching of posting lists that outperforms previous static caching algorithms as well as dynamic algorithms such as LRU and LFU, obtaining hit rate values that are over 10% higher compared these strategies.",
                "We present a framework for the analysis of the trade-off between caching query results and caching posting lists, and we simulate different types of architectures.",
                "Our results show that for centralized and LAN environments, there is an optimal allocation of caching query results and caching of posting lists, while for WAN scenarios in which network time prevails it is more important to cache query results. 0.45 0.5 0.55 0.6 0.65 0.7 0.75 0.8 0.85 0.9 0.95 0.1 0.2 0.3 0.4 0.5 0.6 0.7 Hitrate Cache size Dynamics of static QTF/DF caching policy perfect knowledge 6-week training 3-week training Figure 14: Impact of distribution changes on the static caching of posting lists. 9.",
                "REFERENCES [1] V. N. Anh and A. Moffat.",
                "Pruned query evaluation using pre-computed impacts.",
                "In ACM CIKM, 2006. [2] R. A. Baeza-Yates and F. Saint-Jean.",
                "A three level search engine index based in query log distribution.",
                "In SPIRE, 2003. [3] C. Buckley and A. F. Lewit.",
                "Optimization of inverted vector searches.",
                "In ACM SIGIR, 1985. [4] S. B¨uttcher and C. L. A. Clarke.",
                "A document-centric approach to static index pruning in text retrieval systems.",
                "In ACM CIKM, 2006. [5] P. Cao and S. Irani.",
                "Cost-aware WWW proxy caching algorithms.",
                "In USITS, 1997. [6] P. Denning.",
                "Working sets past and present.",
                "IEEE Trans. on Software Engineering, SE-6(1):64-84, 1980. [7] T. Fagni, R. Perego, F. Silvestri, and S. Orlando.",
                "Boosting the performance of web search engines: Caching and prefetching query results by exploiting historical usage data.",
                "ACM Trans.",
                "Inf.",
                "Syst., 24(1):51-78, 2006. [8] R. Lempel and S. Moran.",
                "Predictive caching and prefetching of query results in search engines.",
                "In WWW, 2003. [9] X.",
                "Long and T. Suel.",
                "Three-level caching for efficient query processing in large web search engines.",
                "In WWW, 2005. [10] E. P. Markatos.",
                "On caching search engine query results.",
                "Computer Communications, 24(2):137-143, 2001. [11] I. Ounis, G. Amati, V. Plachouras, B.",
                "He, C. Macdonald, and C. Lioma.",
                "Terrier: A High Performance and Scalable Information Retrieval Platform.",
                "In SIGIR Workshop on Open Source Information Retrieval, 2006. [12] V. V. Raghavan and H. Sever.",
                "On the reuse of past optimal queries.",
                "In ACM SIGIR, 1995. [13] P. C. Saraiva, E. S. de Moura, N. Ziviani, W. Meira, R. Fonseca, and B. Riberio-Neto.",
                "Rank-preserving two-level caching for scalable search engines.",
                "In ACM SIGIR, 2001. [14] D. R. Slutz and I. L. Traiger.",
                "A note on the calculation of average working set size.",
                "Communications of the ACM, 17(10):563-565, 1974. [15] T. Strohman, H. Turtle, and W. B. Croft.",
                "Optimization strategies for complex queries.",
                "In ACM SIGIR, 2005. [16] I. H. Witten, T. C. Bell, and A. Moffat.",
                "Managing Gigabytes: Compressing and Indexing Documents and Images.",
                "John Wiley & Sons, Inc., NY, 1994. [17] N. E. Young.",
                "On-line file caching.",
                "Algorithmica, 33(3):371-383, 2002."
            ],
            "original_annotated_samples": [],
            "translated_annotated_samples": [],
            "translated_text": "",
            "candidates": [],
            "error": []
        },
        "distribution of the query": {
            "translated_key": "Distribución de la consulta",
            "is_in_text": true,
            "original_annotated_sentences": [
                "The Impact of Caching on Search Engines Ricardo Baeza-Yates1 rbaeza@acm.org Aristides Gionis1 gionis@yahoo-inc.com Flavio Junqueira1 fpj@yahoo-inc.com Vanessa Murdock1 vmurdock@yahoo-inc.com Vassilis Plachouras1 vassilis@yahoo-inc.com Fabrizio Silvestri2 f.silvestri@isti.cnr.it 1 Yahoo!",
                "Research Barcelona 2 ISTI - CNR Barcelona, SPAIN Pisa, ITALY ABSTRACT In this paper we study the trade-offs in designing efficient caching systems for Web search engines.",
                "We explore the impact of different approaches, such as static vs. dynamic caching, and caching query results vs. caching posting lists.",
                "Using a query log spanning a whole year we explore the limitations of caching and we demonstrate that caching posting lists can achieve higher hit rates than caching query answers.",
                "We propose a new algorithm for static caching of posting lists, which outperforms previous methods.",
                "We also study the problem of finding the optimal way to split the static cache between answers and posting lists.",
                "Finally, we measure how the changes in the query log affect the effectiveness of static caching, given our observation that the distribution of the queries changes slowly over time.",
                "Our results and observations are applicable to different levels of the data-access hierarchy, for instance, for a memory/disk layer or a broker/remote server layer.",
                "Categories and Subject Descriptors H.3.3 [Information Storage and Retrieval]: Information Search and Retrieval - Search process; H.3.4 [Information Storage and Retrieval]: Systems and Software - Distributed systems, Performance evaluation (efficiency and effectiveness) General Terms Algorithms, Experimentation 1.",
                "INTRODUCTION Millions of queries are submitted daily to Web search engines, and users have high expectations of the quality and speed of the answers.",
                "As the searchable Web becomes larger and larger, with more than 20 billion pages to index, evaluating a single query requires processing large amounts of data.",
                "In such a setting, to achieve a fast response time and to increase the query throughput, using a cache is crucial.",
                "The primary use of a cache memory is to speedup computation by exploiting frequently or recently used data, although reducing the workload to back-end servers is also a major goal.",
                "Caching can be applied at different levels with increasing response latencies or processing requirements.",
                "For example, the different levels may correspond to the main memory, the disk, or resources in a local or a wide area network.",
                "The decision of what to cache is either off-line (static) or online (dynamic).",
                "A static cache is based on historical information and is periodically updated.",
                "A dynamic cache replaces entries according to the sequence of requests.",
                "When a new request arrives, the cache system decides whether to evict some entry from the cache in the case of a cache miss.",
                "Such online decisions are based on a cache policy, and several different policies have been studied in the past.",
                "For a search engine, there are two possible ways to use a cache memory: Caching answers: As the engine returns answers to a particular query, it may decide to store these answers to resolve future queries.",
                "Caching terms: As the engine evaluates a particular query, it may decide to store in memory the posting lists of the involved query terms.",
                "Often the whole set of posting lists does not fit in memory, and consequently, the engine has to select a small set to keep in memory and speed up query processing.",
                "Returning an answer to a query that already exists in the cache is more efficient than computing the answer using cached posting lists.",
                "On the other hand, previously unseen queries occur more often than previously unseen terms, implying a higher miss rate for cached answers.",
                "Caching of posting lists has additional challenges.",
                "As posting lists have variable size, caching them dynamically is not very efficient, due to the complexity in terms of efficiency and space, and the skewed <br>distribution of the query</br> stream, as shown later.",
                "Static caching of posting lists poses even more challenges: when deciding which terms to cache one faces the trade-off between frequently queried terms and terms with small posting lists that are space efficient.",
                "Finally, before deciding to adopt a static caching policy the query stream should be analyzed to verify that its characteristics do not change rapidly over time.",
                "Broker Static caching posting lists Dynamic/Static cached answers Local query processor Disk Next caching level Local network access Remote network access Figure 1: One caching level in a distributed search architecture.",
                "In this paper we explore the trade-offs in the design of each cache level, showing that the problem is the same and only a few parameters change.",
                "In general, we assume that each level of caching in a distributed search architecture is similar to that shown in Figure 1.",
                "We use a query log spanning a whole year to explore the limitations of dynamically caching query answers or posting lists for query terms.",
                "More concretely, our main conclusions are that: • Caching query answers results in lower hit ratios compared to caching of posting lists for query terms, but it is faster because there is no need for query evaluation.",
                "We provide a framework for the analysis of the trade-off between static caching of query answers and posting lists; • Static caching of terms can be more effective than dynamic caching with, for example, LRU.",
                "We provide algorithms based on the Knapsack problem for selecting the posting lists to put in a static cache, and we show improvements over previous work, achieving a hit ratio over 90%; • Changes of the query distribution over time have little impact on static caching.",
                "The remainder of this paper is organized as follows.",
                "Sections 2 and 3 summarize related work and characterize the data sets we use.",
                "Section 4 discusses the limitations of dynamic caching.",
                "Sections 5 and 6 introduce algorithms for caching posting lists, and a theoretical framework for the analysis of static caching, respectively.",
                "Section 7 discusses the impact of changes in the query distribution on static caching, and Section 8 provides concluding remarks. 2.",
                "RELATED WORK There is a large body of work devoted to query optimization.",
                "Buckley and Lewit [3], in one of the earliest works, take a term-at-a-time approach to deciding when inverted lists need not be further examined.",
                "More recent examples demonstrate that the top k documents for a query can be returned without the need for evaluating the complete set of posting lists [1, 4, 15].",
                "Although these approaches seek to improve query processing efficiency, they differ from our current work in that they do not consider caching.",
                "They may be considered separate and complementary to a cache-based approach.",
                "Raghavan and Sever [12], in one of the first papers on exploiting user query history, propose using a query base, built upon a set of persistent optimal queries submitted in the past, to improve the retrieval effectiveness for similar future queries.",
                "Markatos [10] shows the existence of temporal locality in queries, and compares the performance of different caching policies.",
                "Based on the observations of Markatos, Lempel and Moran propose a new caching policy, called Probabilistic Driven Caching, by attempting to estimate the probability distribution of all possible queries submitted to a search engine [8].",
                "Fagni et al. follow Markatos work by showing that combining static and dynamic caching policies together with an adaptive prefetching policy achieves a high hit ratio [7].",
                "Different from our work, they consider caching and prefetching of pages of results.",
                "As systems are often hierarchical, there has also been some effort on multi-level architectures.",
                "Saraiva et al. propose a new architecture for Web search engines using a two-level dynamic caching system [13].",
                "Their goal for such systems has been to improve response time for hierarchical engines.",
                "In their architecture, both levels use an LRU eviction policy.",
                "They find that the second-level cache can effectively reduce disk traffic, thus increasing the overall throughput.",
                "Baeza-Yates and Saint-Jean propose a three-level index organization [2].",
                "Long and Suel propose a caching system structured according to three different levels [9].",
                "The intermediate level contains frequently occurring pairs of terms and stores the intersections of the corresponding inverted lists.",
                "These last two papers are related to ours in that they exploit different caching strategies at different levels of the memory hierarchy.",
                "Finally, our static caching algorithm for posting lists in Section 5 uses the ratio frequency/size in order to evaluate the goodness of an item to cache.",
                "Similar ideas have been used in the context of file caching [17], Web caching [5], and even caching of posting lists [9], but in all cases in a dynamic setting.",
                "To the best of our knowledge we are the first to use this approach for static caching of posting lists. 3.",
                "DATA CHARACTERIZATION Our data consists of a crawl of documents from the UK domain, and query logs of one year of queries submitted to http://www.yahoo.co.uk from November 2005 to November 2006.",
                "In our logs, 50% of the total volume of queries are unique.",
                "The average query length is 2.5 terms, with the longest query having 731 terms. 1e-07 1e-06 1e-05 1e-04 0.001 0.01 0.1 1 1e-08 1e-07 1e-06 1e-05 1e-04 0.001 0.01 0.1 1 Frequency(normalized) Frequency rank (normalized) Figure 2: The distribution of queries (bottom curve) and query terms (middle curve) in the query log.",
                "The distribution of document frequencies of terms in the UK-2006 dataset (upper curve).",
                "Figure 2 shows the distributions of queries (lower curve), and query terms (middle curve).",
                "The x-axis represents the normalized frequency rank of the query or term. (The most frequent query appears closest to the y-axis.)",
                "The y-axis is Table 1: Statistics of the UK-2006 sample.",
                "UK-2006 sample statistics # of documents 2,786,391 # of terms 6,491,374 # of tokens 2,109,512,558 the normalized frequency for a given query (or term).",
                "As expected, the distribution of query frequencies and query term frequencies follow power law distributions, with slope of 1.84 and 2.26, respectively.",
                "In this figure, the query frequencies were computed as they appear in the logs with no normalization for case or white space.",
                "The query terms (middle curve) have been normalized for case, as have the terms in the document collection.",
                "The document collection that we use for our experiments is a summary of the UK domain crawled in May 2006.1 This summary corresponds to a maximum of 400 crawled documents per host, using a breadth first crawling strategy, comprising 15GB.",
                "The distribution of document frequencies of terms in the collection follows a power law distribution with slope 2.38 (upper curve in Figure 2).",
                "The statistics of the collection are shown in Table 1.",
                "We measured the correlation between the document frequency of terms in the collection and the number of queries that contain a particular term in the query log to be 0.424.",
                "A scatter plot for a random sample of terms is shown in Figure 3.",
                "In this experiment, terms have been converted to lower case in both the queries and the documents so that the frequencies will be comparable. 1e-07 1e-06 1e-05 1e-04 0.001 0.01 0.1 1 1e-06 1e-05 1e-04 0.001 0.01 0.1 1 Queryfrequency Document frequency Figure 3: Normalized scatter plot of document-term frequencies vs. query-term frequencies. 4.",
                "CACHING OF QUERIES AND TERMS Caching relies upon the assumption that there is locality in the stream of requests.",
                "That is, there must be sufficient repetition in the stream of requests and within intervals of time that enable a cache memory of reasonable size to be effective.",
                "In the query log we used, 88% of the unique queries are singleton queries, and 44% are singleton queries out of the whole volume.",
                "Thus, out of all queries in the stream composing the query log, the upper threshold on hit ratio is 56%.",
                "This is because only 56% of all the queries comprise queries that have multiple occurrences.",
                "It is important to observe, however, that not all queries in this 56% can be cache hits because of compulsory misses.",
                "A compulsory miss 1 The collection is available from the University of Milan: http://law.dsi.unimi.it/.",
                "URL retrieved 05/2007. 0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1 240 260 280 300 320 340 360 Numberofelements Bin number Total terms Terms diff Total queries Unique queries Unique terms Query diff Figure 4: Arrival rate for both terms and queries. happens when the cache receives a query for the first time.",
                "This is different from capacity misses, which happen due to space constraints on the amount of memory the cache uses.",
                "If we consider a cache with infinite memory, then the hit ratio is 50%.",
                "Note that for an infinite cache there are no capacity misses.",
                "As we mentioned before, another possibility is to cache the posting lists of terms.",
                "Intuitively, this gives more freedom in the utilization of the cache content to respond to queries because cached terms might form a new query.",
                "On the other hand, they need more space.",
                "As opposed to queries, the fraction of singleton terms in the total volume of terms is smaller.",
                "In our query log, only 4% of the terms appear once, but this accounts for 73% of the vocabulary of query terms.",
                "We show in Section 5 that caching a small fraction of terms, while accounting for terms appearing in many documents, is potentially very effective.",
                "Figure 4 shows several graphs corresponding to the normalized arrival rate for different cases using days as bins.",
                "That is, we plot the normalized number of elements that appear in a day.",
                "This graph shows only a period of 122 days, and we normalize the values by the maximum value observed throughout the whole period of the query log.",
                "Total queries and Total terms correspond to the total volume of queries and terms, respectively.",
                "Unique queries and Unique terms correspond to the arrival rate of unique queries and terms.",
                "Finally, Query diff and Terms diff correspond to the difference between the curves for total and unique.",
                "In Figure 4, as expected, the volume of terms is much higher than the volume of queries.",
                "The difference between the total number of terms and the number of unique terms is much larger than the difference between the total number of queries and the number of unique queries.",
                "This observation implies that terms repeat significantly more than queries.",
                "If we use smaller bins, say of one hour, then the ratio of unique to volume is higher for both terms and queries because it leaves less room for repetition.",
                "We also estimated the workload using the document frequency of terms as a measure of how much work a query imposes on a search engine.",
                "We found that it follows closely the arrival rate for terms shown in Figure 4.",
                "To demonstrate the effect of a dynamic cache on the query frequency distribution of Figure 2, we plot the same frequency graph, but now considering the frequency of queries Figure 5: Frequency graph after LRU cache. after going through an LRU cache.",
                "On a cache miss, an LRU cache decides upon an entry to evict using the information on the recency of queries.",
                "In this graph, the most frequent queries are not the same queries that were most frequent before the cache.",
                "It is possible that queries that are most frequent after the cache have different characteristics, and tuning the search engine to queries frequent before the cache may degrade performance for non-cached queries.",
                "The maximum frequency after caching is less than 1% of the maximum frequency before the cache, thus showing that the cache is very effective in reducing the load of frequent queries.",
                "If we re-rank the queries according to after-cache frequency, the distribution is still a power law, but with a much smaller value for the highest frequency.",
                "When discussing the effectiveness of dynamically caching, an important metric is cache miss rate.",
                "To analyze the cache miss rate for different memory constraints, we use the working set model [6, 14].",
                "A working set, informally, is the set of references that an application or an operating system is currently working with.",
                "The model uses such sets in a strategy that tries to capture the temporal locality of references.",
                "The working set strategy then consists in keeping in memory only the elements that are referenced in the previous θ steps of the input sequence, where θ is a configurable parameter corresponding to the window size.",
                "Originally, working sets have been used for page replacement algorithms of operating systems, and considering such a strategy in the context of search engines is interesting for three reasons.",
                "First, it captures the amount of locality of queries and terms in a sequence of queries.",
                "Locality in this case refers to the frequency of queries and terms in a window of time.",
                "If many queries appear multiple times in a window, then locality is high.",
                "Second, it enables an oﬄine analysis of the expected miss rate given different memory constraints.",
                "Third, working sets capture aspects of efficient caching algorithms such as LRU.",
                "LRU assumes that references farther in the past are less likely to be referenced in the present, which is implicit in the concept of working sets [14].",
                "Figure 6 plots the miss rate for different working set sizes, and we consider working sets of both queries and terms.",
                "The working set sizes are normalized against the total number of queries in the query log.",
                "In the graph for queries, there is a sharp decay until approximately 0.01, and the rate at which the miss rate drops decreases as we increase the size of the working set over 0.01.",
                "Finally, the minimum value it reaches is 50% miss rate, not shown in the figure as we have cut the tail of the curve for presentation purposes. 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1 0 0.05 0.1 0.15 0.2 Missrate Normalized working set size Queries Terms Figure 6: Miss rate as a function of the working set size. 1 10 100 1000 10000 100000 1e+06 Frequency Distance Figure 7: Distribution of distances expressed in terms of distinct queries.",
                "Compared to the query curve, we observe that the minimum miss rate for terms is substantially smaller.",
                "The miss rate also drops sharply on values up to 0.01, and it decreases minimally for higher values.",
                "The minimum value, however, is slightly over 10%, which is much smaller than the minimum value for the sequence of queries.",
                "This implies that with such a policy it is possible to achieve over 80% hit rate, if we consider caching dynamically posting lists for terms as opposed to caching answers for queries.",
                "This result does not consider the space required for each unit stored in the cache memory, or the amount of time it takes to put together a response to a user query.",
                "We analyze these issues more carefully later in this paper.",
                "It is interesting also to observe the histogram of Figure 7, which is an intermediate step in the computation of the miss rate graph.",
                "It reports the distribution of distances between repetitions of the same frequent query.",
                "The distance in the plot is measured in the number of distinct queries separating a query and its repetition, and it considers only queries appearing at least 10 times.",
                "From Figures 6 and 7, we conclude that even if we set the size of the query answers cache to a relatively large number of entries, the miss rate is high.",
                "Thus, caching the posting lists of terms has the potential to improve the hit ratio.",
                "This is what we explore next. 5.",
                "CACHING POSTING LISTS The previous section shows that caching posting lists can obtain a higher hit rate compared to caching query answers.",
                "In this section we study the problem of how to select posting lists to place on a certain amount of available memory, assuming that the whole index is larger than the amount of memory available.",
                "The posting lists have variable size (in fact, their size distribution follows a power law), so it is beneficial for a caching policy to consider the sizes of the posting lists.",
                "We consider both dynamic and static caching.",
                "For dynamic caching, we use two well-known policies, LRU and LFU, as well as a modified algorithm that takes posting-list size into account.",
                "Before discussing the static caching strategies, we introduce some notation.",
                "We use fq(t) to denote the query-term frequency of a term t, that is, the number of queries containing t in the query log, and fd(t) to denote the document frequency of t, that is, the number of documents in the collection in which the term t appears.",
                "The first strategy we consider is the algorithm proposed by Baeza-Yates and Saint-Jean [2], which consists in selecting the posting lists of the terms with the highest query-term frequencies fq(t).",
                "We call this algorithm Qtf.",
                "We observe that there is a trade-off between fq(t) and fd(t).",
                "Terms with high fq(t) are useful to keep in the cache because they are queried often.",
                "On the other hand, terms with high fd(t) are not good candidates because they correspond to long posting lists and consume a substantial amount of space.",
                "In fact, the problem of selecting the best posting lists for the static cache corresponds to the standard Knapsack problem: given a knapsack of fixed capacity, and a set of n items, such as the i-th item has value ci and size si, select the set of items that fit in the knapsack and maximize the overall value.",
                "In our case, value corresponds to fq(t) and size corresponds to fd(t).",
                "Thus, we employ a simple algorithm for the knapsack problem, which is selecting the posting lists of the terms with the highest values of the ratio fq(t) fd(t) .",
                "We call this algorithm QtfDf.",
                "We tried other variations considering query frequencies instead of term frequencies, but the gain was minimal compared to the complexity added.",
                "In addition to the above two static algorithms we consider the following algorithms for dynamic caching: • LRU: A standard LRU algorithm, but many posting lists might need to be evicted (in order of least-recent usage) until there is enough space in the memory to place the currently accessed posting list; • LFU: A standard LFU algorithm (eviction of the leastfrequently used), with the same modification as the LRU; • Dyn-QtfDf: A dynamic version of the QtfDf algorithm; evict from the cache the term(s) with the lowest fq(t) fd(t) ratio.",
                "The performance of all the above algorithms for 15 weeks of the query log and the UK dataset are shown in Figure 8.",
                "Performance is measured with hit rate.",
                "The cache size is measured as a fraction of the total space required to store the posting lists of all terms.",
                "For the dynamic algorithms, we load the cache with terms in order of fq(t) and we let the cache warm up for 1 million queries.",
                "For the static algorithms, we assume complete knowledge of the frequencies fq(t), that is, we estimate fq(t) from the whole query stream.",
                "As we show in Section 7 the results do not change much if we compute the query-term frequencies using the first 3 or 4 weeks of the query log and measure the hit rate on the rest. 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1 0.1 0.2 0.3 0.4 0.5 0.6 0.7 Hitrate Cache size Caching posting lists static QTF/DF LRU LFU Dyn-QTF/DF QTF Figure 8: Hit rate of different strategies for caching posting lists.",
                "The most important observation from our experiments is that the static QtfDf algorithm has a better hit rate than all the dynamic algorithms.",
                "An important benefit a static cache is that it requires no eviction and it is hence more efficient when evaluating queries.",
                "However, if the characteristics of the query traffic change frequently over time, then it requires re-populating the cache often or there will be a significant impact on hit rate. 6.",
                "ANALYSIS OF STATIC CACHING In this section we provide a detailed analysis for the problem of deciding whether it is preferable to cache query answers or cache posting lists.",
                "Our analysis takes into account the impact of caching between two levels of the data-access hierarchy.",
                "It can either be applied at the memory/disk layer or at a server/remote server layer as in the architecture we discussed in the introduction.",
                "Using a particular system model, we obtain estimates for the parameters required by our analysis, which we subsequently use to decide the optimal trade-off between caching query answers and caching posting lists. 6.1 Analytical Model Let M be the size of the cache measured in answer units (the cache can store M query answers).",
                "Assume that all posting lists are of the same length L, measured in answer units.",
                "We consider the following two cases: (A) a cache that stores only precomputed answers, and (B) a cache that stores only posting lists.",
                "In the first case, Nc = M answers fit in the cache, while in the second case Np = M/L posting lists fit in the cache.",
                "Thus, Np = Nc/L.",
                "Note that although posting lists require more space, we can combine terms to evaluate more queries (or partial queries).",
                "For case (A), suppose that a query answer in the cache can be evaluated in 1 time unit.",
                "For case (B), assume that if the posting lists of the terms of a query are in the cache then the results can be computed in TR1 time units, while if the posting lists are not in the cache then the results can be computed in TR2 time units.",
                "Of course TR2 > TR1.",
                "Now we want to compare the time to answer a stream of Q queries in both cases.",
                "Let Vc(Nc) be the volume of the most frequent Nc queries.",
                "Then, for case (A), we have an overall time TCA = Vc(Nc) + TR2(Q − Vc(Nc)).",
                "Similarly, for case (B), let Vp(Np) be the number of computable queries.",
                "Then we have overall time TP L = TR1Vp(Np) + TR2(Q − Vp(Np)).",
                "We want to check under which conditions we have TP L < TCA.",
                "We have TP L − TCA = (TR2 − 1)Vc(Nc) − (TR2 − TR1)Vp(Np) > 0.",
                "Figure 9 shows the values of Vp and Vc for our data.",
                "We can see that caching answers saturates faster and for this particular data there is no additional benefit from using more than 10% of the index space for caching answers.",
                "As the query distribution is a power law with parameter α > 1, the i-th most frequent query appears with probability proportional to 1 iα .",
                "Therefore, the volume Vc(n), which is the total number of the n most frequent queries, is Vc(n) = V0 n i=1 Q iα = γnQ (0 < γn < 1).",
                "We know that Vp(n) grows faster than Vc(n) and assume, based on experimental results, that the relation is of the form Vp(n) = k Vc(n)β .",
                "In the worst case, for a large cache, β → 1.",
                "That is, both techniques will cache a constant fraction of the overall query volume.",
                "Then caching posting lists makes sense only if L(TR2 − 1) k(TR2 − TR1) > 1.",
                "If we use compression, we have L < L and TR1 > TR1.",
                "According to the experiments that we show later, compression is always better.",
                "For a small cache, we are interested in the transient behavior and then β > 1, as computed from our data.",
                "In this case there will always be a point where TP L > TCA for a large number of queries.",
                "In reality, instead of filling the cache only with answers or only with posting lists, a better strategy will be to divide the total cache space into cache for answers and cache for posting lists.",
                "In such a case, there will be some queries that could be answered by both parts of the cache.",
                "As the answer cache is faster, it will be the first choice for answering those queries.",
                "Let QNc and QNp be the set of queries that can be answered by the cached answers and the cached posting lists, respectively.",
                "Then, the overall time is T = Vc(Nc)+TR1V (QNp −QNc )+TR2(Q−V (QNp ∪QNc )), where Np = (M − Nc)/L.",
                "Finding the optimal division of the cache in order to minimize the overall retrieval time is a difficult problem to solve analytically.",
                "In Section 6.3 we use simulations to derive optimal cache trade-offs for particular implementation examples. 6.2 Parameter Estimation We now use a particular implementation of a centralized system and the model of a distributed system as examples from which we estimate the parameters of the analysis from the previous section.",
                "We perform the experiments using an optimized version of Terrier [11] for both indexing documents and processing queries, on a single machine with a Pentium 4 at 2GHz and 1GB of RAM.",
                "We indexed the documents from the UK-2006 dataset, without removing stop words or applying stemming.",
                "The posting lists in the inverted file consist of pairs of document identifier and term frequency.",
                "We compress the document identifier gaps using Elias gamma encoding, and the 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1 0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1 Queryvolume Space precomputed answers posting lists Figure 9: Cache saturation as a function of size.",
                "Table 2: Ratios between the average time to evaluate a query and the average time to return cached answers (centralized and distributed case).",
                "Centralized system TR1 TR2 TR1 TR2 Full evaluation 233 1760 707 1140 Partial evaluation 99 1626 493 798 LAN system TRL 1 TRL 2 TR L 1 TR L 2 Full evaluation 242 1769 716 1149 Partial evaluation 108 1635 502 807 WAN system TRW 1 TRW 2 TR W 1 TR W 2 Full evaluation 5001 6528 5475 5908 Partial evaluation 4867 6394 5270 5575 term frequencies in documents using unary encoding [16].",
                "The size of the inverted file is 1,189Mb.",
                "A stored answer requires 1264 bytes, and an uncompressed posting takes 8 bytes.",
                "From Table 1, we obtain L = (8·# of postings) 1264·# of terms = 0.75 and L = Inverted file size 1264·# of terms = 0.26.",
                "We estimate the ratio TR = T/Tc between the average time T it takes to evaluate a query and the average time Tc it takes to return a stored answer for the same query, in the following way.",
                "Tc is measured by loading the answers for 100,000 queries in memory, and answering the queries from memory.",
                "The average time is Tc = 0.069ms.",
                "T is measured by processing the same 100,000 queries (the first 10,000 queries are used to warm-up the system).",
                "For each query, we remove stop words, if there are at least three remaining terms.",
                "The stop words correspond to the terms with a frequency higher than the number of documents in the index.",
                "We use a document-at-a-time approach to retrieve documents containing all query terms.",
                "The only disk access required during query processing is for reading compressed posting lists from the inverted file.",
                "We perform both full and partial evaluation of answers, because some queries are likely to retrieve a large number of documents, and only a fraction of the retrieved documents will be seen by users.",
                "In the partial evaluation of queries, we terminate the processing after matching 10,000 documents.",
                "The estimated ratios TR are presented in Table 2.",
                "Figure 10 shows for a sample of queries the workload of the system with partial query evaluation and compressed posting lists.",
                "The x-axis corresponds to the total time the system spends processing a particular query, and the vertical axis corresponds to the sum t∈q fq · fd(t).",
                "Notice that the total number of postings of the query-terms does not necessarily provide an accurate estimate of the workload imposed on the system by a query (which is the case for full evaluation and uncompressed lists). 0 0.2 0.4 0.6 0.8 1 0 0.2 0.4 0.6 0.8 1 Totalpostingstoprocessquery(normalized) Total time to process query (normalized) Partial processing of compressed postings query len = 1 query len in [2,3] query len in [4,8] query len > 8 Figure 10: Workload for partial query evaluation with compressed posting lists.",
                "The analysis of the previous section also applies to a distributed retrieval system in one or multiple sites.",
                "Suppose that a document partitioned distributed system is running on a cluster of machines interconnected with a local area network (LAN) in one site.",
                "The broker receives queries and broadcasts them to the query processors, which answer the queries and return the results to the broker.",
                "Finally, the broker merges the received answers and generates the final set of answers (we assume that the time spent on merging results is negligible).",
                "The difference between the centralized architecture and the document partition architecture is the extra communication between the broker and the query processors.",
                "Using ICMP pings on a 100Mbps LAN, we have measured that sending the query from the broker to the query processors which send an answer of 4,000 bytes back to the broker takes on average 0.615ms.",
                "Hence, TRL = TR + 0.615ms/0.069ms = TR + 9.",
                "In the case when the broker and the query processors are in different sites connected with a wide area network (WAN), we estimated that broadcasting the query from the broker to the query processors and getting back an answer of 4,000 bytes takes on average 329ms.",
                "Hence, TRW = TR + 329ms/0.069ms = TR + 4768. 6.3 Simulation Results We now address the problem of finding the optimal tradeoff between caching query answers and caching posting lists.",
                "To make the problem concrete we assume a fixed budget M on the available memory, out of which x units are used for caching query answers and M − x for caching posting lists.",
                "We perform simulations and compute the average response time as a function of x.",
                "Using a part of the query log as training data, we first allocate in the cache the answers to the most frequent queries that fit in space x, and then we use the rest of the memory to cache posting lists.",
                "For selecting posting lists we use the QtfDf algorithm, applied to the training query log but excluding the queries that have already been cached.",
                "In Figure 11, we plot the simulated response time for a centralized system as a function of x.",
                "For the uncompressed index we use M = 1GB, and for the compressed index we use M = 0.5GB.",
                "In the case of the configuration that uses partial query evaluation with compressed posting lists, the lowest response time is achieved when 0.15GB out of the 0.5GB is allocated for storing answers for queries.",
                "We obtained similar trends in the results for the LAN setting.",
                "Figure 12 shows the simulated workload for a distributed system across a WAN.",
                "In this case, the total amount of memory is split between the broker, which holds the cached 400 500 600 700 800 900 1000 1100 1200 0 0.2 0.4 0.6 0.8 1 Averageresponsetime Space (GB) Simulated workload -- single machine full / uncompr / 1 G partial / uncompr / 1 G full / compr / 0.5 G partial / compr / 0.5 G Figure 11: Optimal division of the cache in a server. 3000 3500 4000 4500 5000 5500 6000 0 0.2 0.4 0.6 0.8 1 Averageresponsetime Space (GB) Simulated workload -- WAN full / uncompr / 1 G partial / uncompr / 1 G full / compr / 0.5 G partial / compr / 0.5 G Figure 12: Optimal division of the cache when the next level requires WAN access. answers of queries, and the query processors, which hold the cache of posting lists.",
                "According to the figure, the difference between the configurations of the query processors is less important because the network communication overhead increases the response time substantially.",
                "When using uncompressed posting lists, the optimal allocation of memory corresponds to using approximately 70% of the memory for caching query answers.",
                "This is explained by the fact that there is no need for network communication when the query can be answered by the cache at the broker. 7.",
                "EFFECT OF THE QUERY DYNAMICS For our query log, the query distribution and query-term distribution change slowly over time.",
                "To support this claim, we first assess how topics change comparing the distribution of queries from the first week in June, 2006, to the distribution of queries for the remainder of 2006 that did not appear in the first week in June.",
                "We found that a very small percentage of queries are new queries.",
                "The majority of queries that appear in a given week repeat in the following weeks for the next six months.",
                "We then compute the hit rate of a static cache of 128, 000 answers trained over a period of two weeks (Figure 13).",
                "We report hit rate hourly for 7 days, starting from 5pm.",
                "We observe that the hit rate reaches its highest value during the night (around midnight), whereas around 2-3pm it reaches its minimum.",
                "After a small decay in hit rate values, the hit rate stabilizes between 0.28, and 0.34 for the entire week, suggesting that the static cache is effective for a whole week after the training period. 0.26 0.27 0.28 0.29 0.3 0.31 0.32 0.33 0.34 0.35 0.36 0.37 0 20 40 60 80 100 120 140 160 Hit-rate Time Hits on the frequent queries of distances Figure 13: Hourly hit rate for a static cache holding 128,000 answers during the period of a week.",
                "The static cache of posting lists can be periodically recomputed.",
                "To estimate the time interval in which we need to recompute the posting lists on the static cache we need to consider an efficiency/quality trade-off: using too short a time interval might be prohibitively expensive, while recomputing the cache too infrequently might lead to having an obsolete cache not corresponding to the statistical characteristics of the current query stream.",
                "We measured the effect on the QtfDf algorithm of the changes in a 15-week query stream (Figure 14).",
                "We compute the query term frequencies over the whole stream, select which terms to cache, and then compute the hit rate on the whole query stream.",
                "This hit rate is as an upper bound, and it assumes perfect knowledge of the query term frequencies.",
                "To simulate a realistic scenario, we use the first 6 (3) weeks of the query stream for computing query term frequencies and the following 9 (12) weeks to estimate the hit rate.",
                "As Figure 14 shows, the hit rate decreases by less than 2%.",
                "The high correlation among the query term frequencies during different time periods explains the graceful adaptation of the static caching algorithms to the future query stream.",
                "Indeed, the pairwise correlation among all possible 3-week periods of the 15-week query stream is over 99.5%. 8.",
                "CONCLUSIONS Caching is an effective technique in search engines for improving response time, reducing the load on query processors, and improving network bandwidth utilization.",
                "We present results on both dynamic and static caching.",
                "Dynamic caching of queries has limited effectiveness due to the high number of compulsory misses caused by the number of unique or infrequent queries.",
                "Our results show that in our UK log, the minimum miss rate is 50% using a working set strategy.",
                "Caching terms is more effective with respect to miss rate, achieving values as low as 12%.",
                "We also propose a new algorithm for static caching of posting lists that outperforms previous static caching algorithms as well as dynamic algorithms such as LRU and LFU, obtaining hit rate values that are over 10% higher compared these strategies.",
                "We present a framework for the analysis of the trade-off between caching query results and caching posting lists, and we simulate different types of architectures.",
                "Our results show that for centralized and LAN environments, there is an optimal allocation of caching query results and caching of posting lists, while for WAN scenarios in which network time prevails it is more important to cache query results. 0.45 0.5 0.55 0.6 0.65 0.7 0.75 0.8 0.85 0.9 0.95 0.1 0.2 0.3 0.4 0.5 0.6 0.7 Hitrate Cache size Dynamics of static QTF/DF caching policy perfect knowledge 6-week training 3-week training Figure 14: Impact of distribution changes on the static caching of posting lists. 9.",
                "REFERENCES [1] V. N. Anh and A. Moffat.",
                "Pruned query evaluation using pre-computed impacts.",
                "In ACM CIKM, 2006. [2] R. A. Baeza-Yates and F. Saint-Jean.",
                "A three level search engine index based in query log distribution.",
                "In SPIRE, 2003. [3] C. Buckley and A. F. Lewit.",
                "Optimization of inverted vector searches.",
                "In ACM SIGIR, 1985. [4] S. B¨uttcher and C. L. A. Clarke.",
                "A document-centric approach to static index pruning in text retrieval systems.",
                "In ACM CIKM, 2006. [5] P. Cao and S. Irani.",
                "Cost-aware WWW proxy caching algorithms.",
                "In USITS, 1997. [6] P. Denning.",
                "Working sets past and present.",
                "IEEE Trans. on Software Engineering, SE-6(1):64-84, 1980. [7] T. Fagni, R. Perego, F. Silvestri, and S. Orlando.",
                "Boosting the performance of web search engines: Caching and prefetching query results by exploiting historical usage data.",
                "ACM Trans.",
                "Inf.",
                "Syst., 24(1):51-78, 2006. [8] R. Lempel and S. Moran.",
                "Predictive caching and prefetching of query results in search engines.",
                "In WWW, 2003. [9] X.",
                "Long and T. Suel.",
                "Three-level caching for efficient query processing in large web search engines.",
                "In WWW, 2005. [10] E. P. Markatos.",
                "On caching search engine query results.",
                "Computer Communications, 24(2):137-143, 2001. [11] I. Ounis, G. Amati, V. Plachouras, B.",
                "He, C. Macdonald, and C. Lioma.",
                "Terrier: A High Performance and Scalable Information Retrieval Platform.",
                "In SIGIR Workshop on Open Source Information Retrieval, 2006. [12] V. V. Raghavan and H. Sever.",
                "On the reuse of past optimal queries.",
                "In ACM SIGIR, 1995. [13] P. C. Saraiva, E. S. de Moura, N. Ziviani, W. Meira, R. Fonseca, and B. Riberio-Neto.",
                "Rank-preserving two-level caching for scalable search engines.",
                "In ACM SIGIR, 2001. [14] D. R. Slutz and I. L. Traiger.",
                "A note on the calculation of average working set size.",
                "Communications of the ACM, 17(10):563-565, 1974. [15] T. Strohman, H. Turtle, and W. B. Croft.",
                "Optimization strategies for complex queries.",
                "In ACM SIGIR, 2005. [16] I. H. Witten, T. C. Bell, and A. Moffat.",
                "Managing Gigabytes: Compressing and Indexing Documents and Images.",
                "John Wiley & Sons, Inc., NY, 1994. [17] N. E. Young.",
                "On-line file caching.",
                "Algorithmica, 33(3):371-383, 2002."
            ],
            "original_annotated_samples": [],
            "translated_annotated_samples": [
                "Como las listas de publicación tienen un tamaño variable, almacenarlos en caché dinámicamente no es muy eficiente, debido a la complejidad en términos de eficiencia y espacio, y la \"distribución de la consulta\" sesgada, como se muestra más adelante."
            ],
            "translated_text": "",
            "candidates": [
                "Distribución de la consulta",
                "distribución de la consulta"
            ],
            "error": []
        },
        "the query distribution": {
            "translated_key": "la distribución de la consulta",
            "is_in_text": true,
            "original_annotated_sentences": [
                "The Impact of Caching on Search Engines Ricardo Baeza-Yates1 rbaeza@acm.org Aristides Gionis1 gionis@yahoo-inc.com Flavio Junqueira1 fpj@yahoo-inc.com Vanessa Murdock1 vmurdock@yahoo-inc.com Vassilis Plachouras1 vassilis@yahoo-inc.com Fabrizio Silvestri2 f.silvestri@isti.cnr.it 1 Yahoo!",
                "Research Barcelona 2 ISTI - CNR Barcelona, SPAIN Pisa, ITALY ABSTRACT In this paper we study the trade-offs in designing efficient caching systems for Web search engines.",
                "We explore the impact of different approaches, such as static vs. dynamic caching, and caching query results vs. caching posting lists.",
                "Using a query log spanning a whole year we explore the limitations of caching and we demonstrate that caching posting lists can achieve higher hit rates than caching query answers.",
                "We propose a new algorithm for static caching of posting lists, which outperforms previous methods.",
                "We also study the problem of finding the optimal way to split the static cache between answers and posting lists.",
                "Finally, we measure how the changes in the query log affect the effectiveness of static caching, given our observation that the distribution of the queries changes slowly over time.",
                "Our results and observations are applicable to different levels of the data-access hierarchy, for instance, for a memory/disk layer or a broker/remote server layer.",
                "Categories and Subject Descriptors H.3.3 [Information Storage and Retrieval]: Information Search and Retrieval - Search process; H.3.4 [Information Storage and Retrieval]: Systems and Software - Distributed systems, Performance evaluation (efficiency and effectiveness) General Terms Algorithms, Experimentation 1.",
                "INTRODUCTION Millions of queries are submitted daily to Web search engines, and users have high expectations of the quality and speed of the answers.",
                "As the searchable Web becomes larger and larger, with more than 20 billion pages to index, evaluating a single query requires processing large amounts of data.",
                "In such a setting, to achieve a fast response time and to increase the query throughput, using a cache is crucial.",
                "The primary use of a cache memory is to speedup computation by exploiting frequently or recently used data, although reducing the workload to back-end servers is also a major goal.",
                "Caching can be applied at different levels with increasing response latencies or processing requirements.",
                "For example, the different levels may correspond to the main memory, the disk, or resources in a local or a wide area network.",
                "The decision of what to cache is either off-line (static) or online (dynamic).",
                "A static cache is based on historical information and is periodically updated.",
                "A dynamic cache replaces entries according to the sequence of requests.",
                "When a new request arrives, the cache system decides whether to evict some entry from the cache in the case of a cache miss.",
                "Such online decisions are based on a cache policy, and several different policies have been studied in the past.",
                "For a search engine, there are two possible ways to use a cache memory: Caching answers: As the engine returns answers to a particular query, it may decide to store these answers to resolve future queries.",
                "Caching terms: As the engine evaluates a particular query, it may decide to store in memory the posting lists of the involved query terms.",
                "Often the whole set of posting lists does not fit in memory, and consequently, the engine has to select a small set to keep in memory and speed up query processing.",
                "Returning an answer to a query that already exists in the cache is more efficient than computing the answer using cached posting lists.",
                "On the other hand, previously unseen queries occur more often than previously unseen terms, implying a higher miss rate for cached answers.",
                "Caching of posting lists has additional challenges.",
                "As posting lists have variable size, caching them dynamically is not very efficient, due to the complexity in terms of efficiency and space, and the skewed distribution of the query stream, as shown later.",
                "Static caching of posting lists poses even more challenges: when deciding which terms to cache one faces the trade-off between frequently queried terms and terms with small posting lists that are space efficient.",
                "Finally, before deciding to adopt a static caching policy the query stream should be analyzed to verify that its characteristics do not change rapidly over time.",
                "Broker Static caching posting lists Dynamic/Static cached answers Local query processor Disk Next caching level Local network access Remote network access Figure 1: One caching level in a distributed search architecture.",
                "In this paper we explore the trade-offs in the design of each cache level, showing that the problem is the same and only a few parameters change.",
                "In general, we assume that each level of caching in a distributed search architecture is similar to that shown in Figure 1.",
                "We use a query log spanning a whole year to explore the limitations of dynamically caching query answers or posting lists for query terms.",
                "More concretely, our main conclusions are that: • Caching query answers results in lower hit ratios compared to caching of posting lists for query terms, but it is faster because there is no need for query evaluation.",
                "We provide a framework for the analysis of the trade-off between static caching of query answers and posting lists; • Static caching of terms can be more effective than dynamic caching with, for example, LRU.",
                "We provide algorithms based on the Knapsack problem for selecting the posting lists to put in a static cache, and we show improvements over previous work, achieving a hit ratio over 90%; • Changes of <br>the query distribution</br> over time have little impact on static caching.",
                "The remainder of this paper is organized as follows.",
                "Sections 2 and 3 summarize related work and characterize the data sets we use.",
                "Section 4 discusses the limitations of dynamic caching.",
                "Sections 5 and 6 introduce algorithms for caching posting lists, and a theoretical framework for the analysis of static caching, respectively.",
                "Section 7 discusses the impact of changes in <br>the query distribution</br> on static caching, and Section 8 provides concluding remarks. 2.",
                "RELATED WORK There is a large body of work devoted to query optimization.",
                "Buckley and Lewit [3], in one of the earliest works, take a term-at-a-time approach to deciding when inverted lists need not be further examined.",
                "More recent examples demonstrate that the top k documents for a query can be returned without the need for evaluating the complete set of posting lists [1, 4, 15].",
                "Although these approaches seek to improve query processing efficiency, they differ from our current work in that they do not consider caching.",
                "They may be considered separate and complementary to a cache-based approach.",
                "Raghavan and Sever [12], in one of the first papers on exploiting user query history, propose using a query base, built upon a set of persistent optimal queries submitted in the past, to improve the retrieval effectiveness for similar future queries.",
                "Markatos [10] shows the existence of temporal locality in queries, and compares the performance of different caching policies.",
                "Based on the observations of Markatos, Lempel and Moran propose a new caching policy, called Probabilistic Driven Caching, by attempting to estimate the probability distribution of all possible queries submitted to a search engine [8].",
                "Fagni et al. follow Markatos work by showing that combining static and dynamic caching policies together with an adaptive prefetching policy achieves a high hit ratio [7].",
                "Different from our work, they consider caching and prefetching of pages of results.",
                "As systems are often hierarchical, there has also been some effort on multi-level architectures.",
                "Saraiva et al. propose a new architecture for Web search engines using a two-level dynamic caching system [13].",
                "Their goal for such systems has been to improve response time for hierarchical engines.",
                "In their architecture, both levels use an LRU eviction policy.",
                "They find that the second-level cache can effectively reduce disk traffic, thus increasing the overall throughput.",
                "Baeza-Yates and Saint-Jean propose a three-level index organization [2].",
                "Long and Suel propose a caching system structured according to three different levels [9].",
                "The intermediate level contains frequently occurring pairs of terms and stores the intersections of the corresponding inverted lists.",
                "These last two papers are related to ours in that they exploit different caching strategies at different levels of the memory hierarchy.",
                "Finally, our static caching algorithm for posting lists in Section 5 uses the ratio frequency/size in order to evaluate the goodness of an item to cache.",
                "Similar ideas have been used in the context of file caching [17], Web caching [5], and even caching of posting lists [9], but in all cases in a dynamic setting.",
                "To the best of our knowledge we are the first to use this approach for static caching of posting lists. 3.",
                "DATA CHARACTERIZATION Our data consists of a crawl of documents from the UK domain, and query logs of one year of queries submitted to http://www.yahoo.co.uk from November 2005 to November 2006.",
                "In our logs, 50% of the total volume of queries are unique.",
                "The average query length is 2.5 terms, with the longest query having 731 terms. 1e-07 1e-06 1e-05 1e-04 0.001 0.01 0.1 1 1e-08 1e-07 1e-06 1e-05 1e-04 0.001 0.01 0.1 1 Frequency(normalized) Frequency rank (normalized) Figure 2: The distribution of queries (bottom curve) and query terms (middle curve) in the query log.",
                "The distribution of document frequencies of terms in the UK-2006 dataset (upper curve).",
                "Figure 2 shows the distributions of queries (lower curve), and query terms (middle curve).",
                "The x-axis represents the normalized frequency rank of the query or term. (The most frequent query appears closest to the y-axis.)",
                "The y-axis is Table 1: Statistics of the UK-2006 sample.",
                "UK-2006 sample statistics # of documents 2,786,391 # of terms 6,491,374 # of tokens 2,109,512,558 the normalized frequency for a given query (or term).",
                "As expected, the distribution of query frequencies and query term frequencies follow power law distributions, with slope of 1.84 and 2.26, respectively.",
                "In this figure, the query frequencies were computed as they appear in the logs with no normalization for case or white space.",
                "The query terms (middle curve) have been normalized for case, as have the terms in the document collection.",
                "The document collection that we use for our experiments is a summary of the UK domain crawled in May 2006.1 This summary corresponds to a maximum of 400 crawled documents per host, using a breadth first crawling strategy, comprising 15GB.",
                "The distribution of document frequencies of terms in the collection follows a power law distribution with slope 2.38 (upper curve in Figure 2).",
                "The statistics of the collection are shown in Table 1.",
                "We measured the correlation between the document frequency of terms in the collection and the number of queries that contain a particular term in the query log to be 0.424.",
                "A scatter plot for a random sample of terms is shown in Figure 3.",
                "In this experiment, terms have been converted to lower case in both the queries and the documents so that the frequencies will be comparable. 1e-07 1e-06 1e-05 1e-04 0.001 0.01 0.1 1 1e-06 1e-05 1e-04 0.001 0.01 0.1 1 Queryfrequency Document frequency Figure 3: Normalized scatter plot of document-term frequencies vs. query-term frequencies. 4.",
                "CACHING OF QUERIES AND TERMS Caching relies upon the assumption that there is locality in the stream of requests.",
                "That is, there must be sufficient repetition in the stream of requests and within intervals of time that enable a cache memory of reasonable size to be effective.",
                "In the query log we used, 88% of the unique queries are singleton queries, and 44% are singleton queries out of the whole volume.",
                "Thus, out of all queries in the stream composing the query log, the upper threshold on hit ratio is 56%.",
                "This is because only 56% of all the queries comprise queries that have multiple occurrences.",
                "It is important to observe, however, that not all queries in this 56% can be cache hits because of compulsory misses.",
                "A compulsory miss 1 The collection is available from the University of Milan: http://law.dsi.unimi.it/.",
                "URL retrieved 05/2007. 0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1 240 260 280 300 320 340 360 Numberofelements Bin number Total terms Terms diff Total queries Unique queries Unique terms Query diff Figure 4: Arrival rate for both terms and queries. happens when the cache receives a query for the first time.",
                "This is different from capacity misses, which happen due to space constraints on the amount of memory the cache uses.",
                "If we consider a cache with infinite memory, then the hit ratio is 50%.",
                "Note that for an infinite cache there are no capacity misses.",
                "As we mentioned before, another possibility is to cache the posting lists of terms.",
                "Intuitively, this gives more freedom in the utilization of the cache content to respond to queries because cached terms might form a new query.",
                "On the other hand, they need more space.",
                "As opposed to queries, the fraction of singleton terms in the total volume of terms is smaller.",
                "In our query log, only 4% of the terms appear once, but this accounts for 73% of the vocabulary of query terms.",
                "We show in Section 5 that caching a small fraction of terms, while accounting for terms appearing in many documents, is potentially very effective.",
                "Figure 4 shows several graphs corresponding to the normalized arrival rate for different cases using days as bins.",
                "That is, we plot the normalized number of elements that appear in a day.",
                "This graph shows only a period of 122 days, and we normalize the values by the maximum value observed throughout the whole period of the query log.",
                "Total queries and Total terms correspond to the total volume of queries and terms, respectively.",
                "Unique queries and Unique terms correspond to the arrival rate of unique queries and terms.",
                "Finally, Query diff and Terms diff correspond to the difference between the curves for total and unique.",
                "In Figure 4, as expected, the volume of terms is much higher than the volume of queries.",
                "The difference between the total number of terms and the number of unique terms is much larger than the difference between the total number of queries and the number of unique queries.",
                "This observation implies that terms repeat significantly more than queries.",
                "If we use smaller bins, say of one hour, then the ratio of unique to volume is higher for both terms and queries because it leaves less room for repetition.",
                "We also estimated the workload using the document frequency of terms as a measure of how much work a query imposes on a search engine.",
                "We found that it follows closely the arrival rate for terms shown in Figure 4.",
                "To demonstrate the effect of a dynamic cache on the query frequency distribution of Figure 2, we plot the same frequency graph, but now considering the frequency of queries Figure 5: Frequency graph after LRU cache. after going through an LRU cache.",
                "On a cache miss, an LRU cache decides upon an entry to evict using the information on the recency of queries.",
                "In this graph, the most frequent queries are not the same queries that were most frequent before the cache.",
                "It is possible that queries that are most frequent after the cache have different characteristics, and tuning the search engine to queries frequent before the cache may degrade performance for non-cached queries.",
                "The maximum frequency after caching is less than 1% of the maximum frequency before the cache, thus showing that the cache is very effective in reducing the load of frequent queries.",
                "If we re-rank the queries according to after-cache frequency, the distribution is still a power law, but with a much smaller value for the highest frequency.",
                "When discussing the effectiveness of dynamically caching, an important metric is cache miss rate.",
                "To analyze the cache miss rate for different memory constraints, we use the working set model [6, 14].",
                "A working set, informally, is the set of references that an application or an operating system is currently working with.",
                "The model uses such sets in a strategy that tries to capture the temporal locality of references.",
                "The working set strategy then consists in keeping in memory only the elements that are referenced in the previous θ steps of the input sequence, where θ is a configurable parameter corresponding to the window size.",
                "Originally, working sets have been used for page replacement algorithms of operating systems, and considering such a strategy in the context of search engines is interesting for three reasons.",
                "First, it captures the amount of locality of queries and terms in a sequence of queries.",
                "Locality in this case refers to the frequency of queries and terms in a window of time.",
                "If many queries appear multiple times in a window, then locality is high.",
                "Second, it enables an oﬄine analysis of the expected miss rate given different memory constraints.",
                "Third, working sets capture aspects of efficient caching algorithms such as LRU.",
                "LRU assumes that references farther in the past are less likely to be referenced in the present, which is implicit in the concept of working sets [14].",
                "Figure 6 plots the miss rate for different working set sizes, and we consider working sets of both queries and terms.",
                "The working set sizes are normalized against the total number of queries in the query log.",
                "In the graph for queries, there is a sharp decay until approximately 0.01, and the rate at which the miss rate drops decreases as we increase the size of the working set over 0.01.",
                "Finally, the minimum value it reaches is 50% miss rate, not shown in the figure as we have cut the tail of the curve for presentation purposes. 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1 0 0.05 0.1 0.15 0.2 Missrate Normalized working set size Queries Terms Figure 6: Miss rate as a function of the working set size. 1 10 100 1000 10000 100000 1e+06 Frequency Distance Figure 7: Distribution of distances expressed in terms of distinct queries.",
                "Compared to the query curve, we observe that the minimum miss rate for terms is substantially smaller.",
                "The miss rate also drops sharply on values up to 0.01, and it decreases minimally for higher values.",
                "The minimum value, however, is slightly over 10%, which is much smaller than the minimum value for the sequence of queries.",
                "This implies that with such a policy it is possible to achieve over 80% hit rate, if we consider caching dynamically posting lists for terms as opposed to caching answers for queries.",
                "This result does not consider the space required for each unit stored in the cache memory, or the amount of time it takes to put together a response to a user query.",
                "We analyze these issues more carefully later in this paper.",
                "It is interesting also to observe the histogram of Figure 7, which is an intermediate step in the computation of the miss rate graph.",
                "It reports the distribution of distances between repetitions of the same frequent query.",
                "The distance in the plot is measured in the number of distinct queries separating a query and its repetition, and it considers only queries appearing at least 10 times.",
                "From Figures 6 and 7, we conclude that even if we set the size of the query answers cache to a relatively large number of entries, the miss rate is high.",
                "Thus, caching the posting lists of terms has the potential to improve the hit ratio.",
                "This is what we explore next. 5.",
                "CACHING POSTING LISTS The previous section shows that caching posting lists can obtain a higher hit rate compared to caching query answers.",
                "In this section we study the problem of how to select posting lists to place on a certain amount of available memory, assuming that the whole index is larger than the amount of memory available.",
                "The posting lists have variable size (in fact, their size distribution follows a power law), so it is beneficial for a caching policy to consider the sizes of the posting lists.",
                "We consider both dynamic and static caching.",
                "For dynamic caching, we use two well-known policies, LRU and LFU, as well as a modified algorithm that takes posting-list size into account.",
                "Before discussing the static caching strategies, we introduce some notation.",
                "We use fq(t) to denote the query-term frequency of a term t, that is, the number of queries containing t in the query log, and fd(t) to denote the document frequency of t, that is, the number of documents in the collection in which the term t appears.",
                "The first strategy we consider is the algorithm proposed by Baeza-Yates and Saint-Jean [2], which consists in selecting the posting lists of the terms with the highest query-term frequencies fq(t).",
                "We call this algorithm Qtf.",
                "We observe that there is a trade-off between fq(t) and fd(t).",
                "Terms with high fq(t) are useful to keep in the cache because they are queried often.",
                "On the other hand, terms with high fd(t) are not good candidates because they correspond to long posting lists and consume a substantial amount of space.",
                "In fact, the problem of selecting the best posting lists for the static cache corresponds to the standard Knapsack problem: given a knapsack of fixed capacity, and a set of n items, such as the i-th item has value ci and size si, select the set of items that fit in the knapsack and maximize the overall value.",
                "In our case, value corresponds to fq(t) and size corresponds to fd(t).",
                "Thus, we employ a simple algorithm for the knapsack problem, which is selecting the posting lists of the terms with the highest values of the ratio fq(t) fd(t) .",
                "We call this algorithm QtfDf.",
                "We tried other variations considering query frequencies instead of term frequencies, but the gain was minimal compared to the complexity added.",
                "In addition to the above two static algorithms we consider the following algorithms for dynamic caching: • LRU: A standard LRU algorithm, but many posting lists might need to be evicted (in order of least-recent usage) until there is enough space in the memory to place the currently accessed posting list; • LFU: A standard LFU algorithm (eviction of the leastfrequently used), with the same modification as the LRU; • Dyn-QtfDf: A dynamic version of the QtfDf algorithm; evict from the cache the term(s) with the lowest fq(t) fd(t) ratio.",
                "The performance of all the above algorithms for 15 weeks of the query log and the UK dataset are shown in Figure 8.",
                "Performance is measured with hit rate.",
                "The cache size is measured as a fraction of the total space required to store the posting lists of all terms.",
                "For the dynamic algorithms, we load the cache with terms in order of fq(t) and we let the cache warm up for 1 million queries.",
                "For the static algorithms, we assume complete knowledge of the frequencies fq(t), that is, we estimate fq(t) from the whole query stream.",
                "As we show in Section 7 the results do not change much if we compute the query-term frequencies using the first 3 or 4 weeks of the query log and measure the hit rate on the rest. 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1 0.1 0.2 0.3 0.4 0.5 0.6 0.7 Hitrate Cache size Caching posting lists static QTF/DF LRU LFU Dyn-QTF/DF QTF Figure 8: Hit rate of different strategies for caching posting lists.",
                "The most important observation from our experiments is that the static QtfDf algorithm has a better hit rate than all the dynamic algorithms.",
                "An important benefit a static cache is that it requires no eviction and it is hence more efficient when evaluating queries.",
                "However, if the characteristics of the query traffic change frequently over time, then it requires re-populating the cache often or there will be a significant impact on hit rate. 6.",
                "ANALYSIS OF STATIC CACHING In this section we provide a detailed analysis for the problem of deciding whether it is preferable to cache query answers or cache posting lists.",
                "Our analysis takes into account the impact of caching between two levels of the data-access hierarchy.",
                "It can either be applied at the memory/disk layer or at a server/remote server layer as in the architecture we discussed in the introduction.",
                "Using a particular system model, we obtain estimates for the parameters required by our analysis, which we subsequently use to decide the optimal trade-off between caching query answers and caching posting lists. 6.1 Analytical Model Let M be the size of the cache measured in answer units (the cache can store M query answers).",
                "Assume that all posting lists are of the same length L, measured in answer units.",
                "We consider the following two cases: (A) a cache that stores only precomputed answers, and (B) a cache that stores only posting lists.",
                "In the first case, Nc = M answers fit in the cache, while in the second case Np = M/L posting lists fit in the cache.",
                "Thus, Np = Nc/L.",
                "Note that although posting lists require more space, we can combine terms to evaluate more queries (or partial queries).",
                "For case (A), suppose that a query answer in the cache can be evaluated in 1 time unit.",
                "For case (B), assume that if the posting lists of the terms of a query are in the cache then the results can be computed in TR1 time units, while if the posting lists are not in the cache then the results can be computed in TR2 time units.",
                "Of course TR2 > TR1.",
                "Now we want to compare the time to answer a stream of Q queries in both cases.",
                "Let Vc(Nc) be the volume of the most frequent Nc queries.",
                "Then, for case (A), we have an overall time TCA = Vc(Nc) + TR2(Q − Vc(Nc)).",
                "Similarly, for case (B), let Vp(Np) be the number of computable queries.",
                "Then we have overall time TP L = TR1Vp(Np) + TR2(Q − Vp(Np)).",
                "We want to check under which conditions we have TP L < TCA.",
                "We have TP L − TCA = (TR2 − 1)Vc(Nc) − (TR2 − TR1)Vp(Np) > 0.",
                "Figure 9 shows the values of Vp and Vc for our data.",
                "We can see that caching answers saturates faster and for this particular data there is no additional benefit from using more than 10% of the index space for caching answers.",
                "As <br>the query distribution</br> is a power law with parameter α > 1, the i-th most frequent query appears with probability proportional to 1 iα .",
                "Therefore, the volume Vc(n), which is the total number of the n most frequent queries, is Vc(n) = V0 n i=1 Q iα = γnQ (0 < γn < 1).",
                "We know that Vp(n) grows faster than Vc(n) and assume, based on experimental results, that the relation is of the form Vp(n) = k Vc(n)β .",
                "In the worst case, for a large cache, β → 1.",
                "That is, both techniques will cache a constant fraction of the overall query volume.",
                "Then caching posting lists makes sense only if L(TR2 − 1) k(TR2 − TR1) > 1.",
                "If we use compression, we have L < L and TR1 > TR1.",
                "According to the experiments that we show later, compression is always better.",
                "For a small cache, we are interested in the transient behavior and then β > 1, as computed from our data.",
                "In this case there will always be a point where TP L > TCA for a large number of queries.",
                "In reality, instead of filling the cache only with answers or only with posting lists, a better strategy will be to divide the total cache space into cache for answers and cache for posting lists.",
                "In such a case, there will be some queries that could be answered by both parts of the cache.",
                "As the answer cache is faster, it will be the first choice for answering those queries.",
                "Let QNc and QNp be the set of queries that can be answered by the cached answers and the cached posting lists, respectively.",
                "Then, the overall time is T = Vc(Nc)+TR1V (QNp −QNc )+TR2(Q−V (QNp ∪QNc )), where Np = (M − Nc)/L.",
                "Finding the optimal division of the cache in order to minimize the overall retrieval time is a difficult problem to solve analytically.",
                "In Section 6.3 we use simulations to derive optimal cache trade-offs for particular implementation examples. 6.2 Parameter Estimation We now use a particular implementation of a centralized system and the model of a distributed system as examples from which we estimate the parameters of the analysis from the previous section.",
                "We perform the experiments using an optimized version of Terrier [11] for both indexing documents and processing queries, on a single machine with a Pentium 4 at 2GHz and 1GB of RAM.",
                "We indexed the documents from the UK-2006 dataset, without removing stop words or applying stemming.",
                "The posting lists in the inverted file consist of pairs of document identifier and term frequency.",
                "We compress the document identifier gaps using Elias gamma encoding, and the 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1 0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1 Queryvolume Space precomputed answers posting lists Figure 9: Cache saturation as a function of size.",
                "Table 2: Ratios between the average time to evaluate a query and the average time to return cached answers (centralized and distributed case).",
                "Centralized system TR1 TR2 TR1 TR2 Full evaluation 233 1760 707 1140 Partial evaluation 99 1626 493 798 LAN system TRL 1 TRL 2 TR L 1 TR L 2 Full evaluation 242 1769 716 1149 Partial evaluation 108 1635 502 807 WAN system TRW 1 TRW 2 TR W 1 TR W 2 Full evaluation 5001 6528 5475 5908 Partial evaluation 4867 6394 5270 5575 term frequencies in documents using unary encoding [16].",
                "The size of the inverted file is 1,189Mb.",
                "A stored answer requires 1264 bytes, and an uncompressed posting takes 8 bytes.",
                "From Table 1, we obtain L = (8·# of postings) 1264·# of terms = 0.75 and L = Inverted file size 1264·# of terms = 0.26.",
                "We estimate the ratio TR = T/Tc between the average time T it takes to evaluate a query and the average time Tc it takes to return a stored answer for the same query, in the following way.",
                "Tc is measured by loading the answers for 100,000 queries in memory, and answering the queries from memory.",
                "The average time is Tc = 0.069ms.",
                "T is measured by processing the same 100,000 queries (the first 10,000 queries are used to warm-up the system).",
                "For each query, we remove stop words, if there are at least three remaining terms.",
                "The stop words correspond to the terms with a frequency higher than the number of documents in the index.",
                "We use a document-at-a-time approach to retrieve documents containing all query terms.",
                "The only disk access required during query processing is for reading compressed posting lists from the inverted file.",
                "We perform both full and partial evaluation of answers, because some queries are likely to retrieve a large number of documents, and only a fraction of the retrieved documents will be seen by users.",
                "In the partial evaluation of queries, we terminate the processing after matching 10,000 documents.",
                "The estimated ratios TR are presented in Table 2.",
                "Figure 10 shows for a sample of queries the workload of the system with partial query evaluation and compressed posting lists.",
                "The x-axis corresponds to the total time the system spends processing a particular query, and the vertical axis corresponds to the sum t∈q fq · fd(t).",
                "Notice that the total number of postings of the query-terms does not necessarily provide an accurate estimate of the workload imposed on the system by a query (which is the case for full evaluation and uncompressed lists). 0 0.2 0.4 0.6 0.8 1 0 0.2 0.4 0.6 0.8 1 Totalpostingstoprocessquery(normalized) Total time to process query (normalized) Partial processing of compressed postings query len = 1 query len in [2,3] query len in [4,8] query len > 8 Figure 10: Workload for partial query evaluation with compressed posting lists.",
                "The analysis of the previous section also applies to a distributed retrieval system in one or multiple sites.",
                "Suppose that a document partitioned distributed system is running on a cluster of machines interconnected with a local area network (LAN) in one site.",
                "The broker receives queries and broadcasts them to the query processors, which answer the queries and return the results to the broker.",
                "Finally, the broker merges the received answers and generates the final set of answers (we assume that the time spent on merging results is negligible).",
                "The difference between the centralized architecture and the document partition architecture is the extra communication between the broker and the query processors.",
                "Using ICMP pings on a 100Mbps LAN, we have measured that sending the query from the broker to the query processors which send an answer of 4,000 bytes back to the broker takes on average 0.615ms.",
                "Hence, TRL = TR + 0.615ms/0.069ms = TR + 9.",
                "In the case when the broker and the query processors are in different sites connected with a wide area network (WAN), we estimated that broadcasting the query from the broker to the query processors and getting back an answer of 4,000 bytes takes on average 329ms.",
                "Hence, TRW = TR + 329ms/0.069ms = TR + 4768. 6.3 Simulation Results We now address the problem of finding the optimal tradeoff between caching query answers and caching posting lists.",
                "To make the problem concrete we assume a fixed budget M on the available memory, out of which x units are used for caching query answers and M − x for caching posting lists.",
                "We perform simulations and compute the average response time as a function of x.",
                "Using a part of the query log as training data, we first allocate in the cache the answers to the most frequent queries that fit in space x, and then we use the rest of the memory to cache posting lists.",
                "For selecting posting lists we use the QtfDf algorithm, applied to the training query log but excluding the queries that have already been cached.",
                "In Figure 11, we plot the simulated response time for a centralized system as a function of x.",
                "For the uncompressed index we use M = 1GB, and for the compressed index we use M = 0.5GB.",
                "In the case of the configuration that uses partial query evaluation with compressed posting lists, the lowest response time is achieved when 0.15GB out of the 0.5GB is allocated for storing answers for queries.",
                "We obtained similar trends in the results for the LAN setting.",
                "Figure 12 shows the simulated workload for a distributed system across a WAN.",
                "In this case, the total amount of memory is split between the broker, which holds the cached 400 500 600 700 800 900 1000 1100 1200 0 0.2 0.4 0.6 0.8 1 Averageresponsetime Space (GB) Simulated workload -- single machine full / uncompr / 1 G partial / uncompr / 1 G full / compr / 0.5 G partial / compr / 0.5 G Figure 11: Optimal division of the cache in a server. 3000 3500 4000 4500 5000 5500 6000 0 0.2 0.4 0.6 0.8 1 Averageresponsetime Space (GB) Simulated workload -- WAN full / uncompr / 1 G partial / uncompr / 1 G full / compr / 0.5 G partial / compr / 0.5 G Figure 12: Optimal division of the cache when the next level requires WAN access. answers of queries, and the query processors, which hold the cache of posting lists.",
                "According to the figure, the difference between the configurations of the query processors is less important because the network communication overhead increases the response time substantially.",
                "When using uncompressed posting lists, the optimal allocation of memory corresponds to using approximately 70% of the memory for caching query answers.",
                "This is explained by the fact that there is no need for network communication when the query can be answered by the cache at the broker. 7.",
                "EFFECT OF THE QUERY DYNAMICS For our query log, <br>the query distribution</br> and query-term distribution change slowly over time.",
                "To support this claim, we first assess how topics change comparing the distribution of queries from the first week in June, 2006, to the distribution of queries for the remainder of 2006 that did not appear in the first week in June.",
                "We found that a very small percentage of queries are new queries.",
                "The majority of queries that appear in a given week repeat in the following weeks for the next six months.",
                "We then compute the hit rate of a static cache of 128, 000 answers trained over a period of two weeks (Figure 13).",
                "We report hit rate hourly for 7 days, starting from 5pm.",
                "We observe that the hit rate reaches its highest value during the night (around midnight), whereas around 2-3pm it reaches its minimum.",
                "After a small decay in hit rate values, the hit rate stabilizes between 0.28, and 0.34 for the entire week, suggesting that the static cache is effective for a whole week after the training period. 0.26 0.27 0.28 0.29 0.3 0.31 0.32 0.33 0.34 0.35 0.36 0.37 0 20 40 60 80 100 120 140 160 Hit-rate Time Hits on the frequent queries of distances Figure 13: Hourly hit rate for a static cache holding 128,000 answers during the period of a week.",
                "The static cache of posting lists can be periodically recomputed.",
                "To estimate the time interval in which we need to recompute the posting lists on the static cache we need to consider an efficiency/quality trade-off: using too short a time interval might be prohibitively expensive, while recomputing the cache too infrequently might lead to having an obsolete cache not corresponding to the statistical characteristics of the current query stream.",
                "We measured the effect on the QtfDf algorithm of the changes in a 15-week query stream (Figure 14).",
                "We compute the query term frequencies over the whole stream, select which terms to cache, and then compute the hit rate on the whole query stream.",
                "This hit rate is as an upper bound, and it assumes perfect knowledge of the query term frequencies.",
                "To simulate a realistic scenario, we use the first 6 (3) weeks of the query stream for computing query term frequencies and the following 9 (12) weeks to estimate the hit rate.",
                "As Figure 14 shows, the hit rate decreases by less than 2%.",
                "The high correlation among the query term frequencies during different time periods explains the graceful adaptation of the static caching algorithms to the future query stream.",
                "Indeed, the pairwise correlation among all possible 3-week periods of the 15-week query stream is over 99.5%. 8.",
                "CONCLUSIONS Caching is an effective technique in search engines for improving response time, reducing the load on query processors, and improving network bandwidth utilization.",
                "We present results on both dynamic and static caching.",
                "Dynamic caching of queries has limited effectiveness due to the high number of compulsory misses caused by the number of unique or infrequent queries.",
                "Our results show that in our UK log, the minimum miss rate is 50% using a working set strategy.",
                "Caching terms is more effective with respect to miss rate, achieving values as low as 12%.",
                "We also propose a new algorithm for static caching of posting lists that outperforms previous static caching algorithms as well as dynamic algorithms such as LRU and LFU, obtaining hit rate values that are over 10% higher compared these strategies.",
                "We present a framework for the analysis of the trade-off between caching query results and caching posting lists, and we simulate different types of architectures.",
                "Our results show that for centralized and LAN environments, there is an optimal allocation of caching query results and caching of posting lists, while for WAN scenarios in which network time prevails it is more important to cache query results. 0.45 0.5 0.55 0.6 0.65 0.7 0.75 0.8 0.85 0.9 0.95 0.1 0.2 0.3 0.4 0.5 0.6 0.7 Hitrate Cache size Dynamics of static QTF/DF caching policy perfect knowledge 6-week training 3-week training Figure 14: Impact of distribution changes on the static caching of posting lists. 9.",
                "REFERENCES [1] V. N. Anh and A. Moffat.",
                "Pruned query evaluation using pre-computed impacts.",
                "In ACM CIKM, 2006. [2] R. A. Baeza-Yates and F. Saint-Jean.",
                "A three level search engine index based in query log distribution.",
                "In SPIRE, 2003. [3] C. Buckley and A. F. Lewit.",
                "Optimization of inverted vector searches.",
                "In ACM SIGIR, 1985. [4] S. B¨uttcher and C. L. A. Clarke.",
                "A document-centric approach to static index pruning in text retrieval systems.",
                "In ACM CIKM, 2006. [5] P. Cao and S. Irani.",
                "Cost-aware WWW proxy caching algorithms.",
                "In USITS, 1997. [6] P. Denning.",
                "Working sets past and present.",
                "IEEE Trans. on Software Engineering, SE-6(1):64-84, 1980. [7] T. Fagni, R. Perego, F. Silvestri, and S. Orlando.",
                "Boosting the performance of web search engines: Caching and prefetching query results by exploiting historical usage data.",
                "ACM Trans.",
                "Inf.",
                "Syst., 24(1):51-78, 2006. [8] R. Lempel and S. Moran.",
                "Predictive caching and prefetching of query results in search engines.",
                "In WWW, 2003. [9] X.",
                "Long and T. Suel.",
                "Three-level caching for efficient query processing in large web search engines.",
                "In WWW, 2005. [10] E. P. Markatos.",
                "On caching search engine query results.",
                "Computer Communications, 24(2):137-143, 2001. [11] I. Ounis, G. Amati, V. Plachouras, B.",
                "He, C. Macdonald, and C. Lioma.",
                "Terrier: A High Performance and Scalable Information Retrieval Platform.",
                "In SIGIR Workshop on Open Source Information Retrieval, 2006. [12] V. V. Raghavan and H. Sever.",
                "On the reuse of past optimal queries.",
                "In ACM SIGIR, 1995. [13] P. C. Saraiva, E. S. de Moura, N. Ziviani, W. Meira, R. Fonseca, and B. Riberio-Neto.",
                "Rank-preserving two-level caching for scalable search engines.",
                "In ACM SIGIR, 2001. [14] D. R. Slutz and I. L. Traiger.",
                "A note on the calculation of average working set size.",
                "Communications of the ACM, 17(10):563-565, 1974. [15] T. Strohman, H. Turtle, and W. B. Croft.",
                "Optimization strategies for complex queries.",
                "In ACM SIGIR, 2005. [16] I. H. Witten, T. C. Bell, and A. Moffat.",
                "Managing Gigabytes: Compressing and Indexing Documents and Images.",
                "John Wiley & Sons, Inc., NY, 1994. [17] N. E. Young.",
                "On-line file caching.",
                "Algorithmica, 33(3):371-383, 2002."
            ],
            "original_annotated_samples": [],
            "translated_annotated_samples": [
                "Proporcionamos algoritmos basados en el problema de la mochila para seleccionar las listas de publicación para poner en un caché estático, y mostramos mejoras sobre el trabajo anterior, logrando una relación de éxito durante el 90%;• Los cambios de \"la distribución de consultas\" a lo largo del tiempo tienen poco impacto en el almacenamiento en caché estático.",
                "La Sección 7 analiza el impacto de los cambios en \"la distribución de consultas\" en el almacenamiento en caché estático, y la Sección 8 proporciona comentarios finales.2.",
                "Como \"la distribución de consultas\" es una ley de potencia con el parámetro α> 1, la consulta más frecuente de I-th aparece con probabilidad proporcional a 1 Iα.",
                "Efecto de la dinámica de la consulta para nuestro registro de consultas, \"La distribución de consultas\" y la distribución de la consulta cambian lentamente con el tiempo."
            ],
            "translated_text": "",
            "candidates": [
                "la distribución de la consulta",
                "la distribución de consultas",
                "La distribución de consultas",
                "la distribución de consultas",
                "la distribución de la consulta",
                "la distribución de consultas",
                "la distribución de la consulta",
                "La distribución de consultas"
            ],
            "error": []
        },
        "data-access hierarchy": {
            "translated_key": "jerarquía de acceso de datos",
            "is_in_text": true,
            "original_annotated_sentences": [
                "The Impact of Caching on Search Engines Ricardo Baeza-Yates1 rbaeza@acm.org Aristides Gionis1 gionis@yahoo-inc.com Flavio Junqueira1 fpj@yahoo-inc.com Vanessa Murdock1 vmurdock@yahoo-inc.com Vassilis Plachouras1 vassilis@yahoo-inc.com Fabrizio Silvestri2 f.silvestri@isti.cnr.it 1 Yahoo!",
                "Research Barcelona 2 ISTI - CNR Barcelona, SPAIN Pisa, ITALY ABSTRACT In this paper we study the trade-offs in designing efficient caching systems for Web search engines.",
                "We explore the impact of different approaches, such as static vs. dynamic caching, and caching query results vs. caching posting lists.",
                "Using a query log spanning a whole year we explore the limitations of caching and we demonstrate that caching posting lists can achieve higher hit rates than caching query answers.",
                "We propose a new algorithm for static caching of posting lists, which outperforms previous methods.",
                "We also study the problem of finding the optimal way to split the static cache between answers and posting lists.",
                "Finally, we measure how the changes in the query log affect the effectiveness of static caching, given our observation that the distribution of the queries changes slowly over time.",
                "Our results and observations are applicable to different levels of the <br>data-access hierarchy</br>, for instance, for a memory/disk layer or a broker/remote server layer.",
                "Categories and Subject Descriptors H.3.3 [Information Storage and Retrieval]: Information Search and Retrieval - Search process; H.3.4 [Information Storage and Retrieval]: Systems and Software - Distributed systems, Performance evaluation (efficiency and effectiveness) General Terms Algorithms, Experimentation 1.",
                "INTRODUCTION Millions of queries are submitted daily to Web search engines, and users have high expectations of the quality and speed of the answers.",
                "As the searchable Web becomes larger and larger, with more than 20 billion pages to index, evaluating a single query requires processing large amounts of data.",
                "In such a setting, to achieve a fast response time and to increase the query throughput, using a cache is crucial.",
                "The primary use of a cache memory is to speedup computation by exploiting frequently or recently used data, although reducing the workload to back-end servers is also a major goal.",
                "Caching can be applied at different levels with increasing response latencies or processing requirements.",
                "For example, the different levels may correspond to the main memory, the disk, or resources in a local or a wide area network.",
                "The decision of what to cache is either off-line (static) or online (dynamic).",
                "A static cache is based on historical information and is periodically updated.",
                "A dynamic cache replaces entries according to the sequence of requests.",
                "When a new request arrives, the cache system decides whether to evict some entry from the cache in the case of a cache miss.",
                "Such online decisions are based on a cache policy, and several different policies have been studied in the past.",
                "For a search engine, there are two possible ways to use a cache memory: Caching answers: As the engine returns answers to a particular query, it may decide to store these answers to resolve future queries.",
                "Caching terms: As the engine evaluates a particular query, it may decide to store in memory the posting lists of the involved query terms.",
                "Often the whole set of posting lists does not fit in memory, and consequently, the engine has to select a small set to keep in memory and speed up query processing.",
                "Returning an answer to a query that already exists in the cache is more efficient than computing the answer using cached posting lists.",
                "On the other hand, previously unseen queries occur more often than previously unseen terms, implying a higher miss rate for cached answers.",
                "Caching of posting lists has additional challenges.",
                "As posting lists have variable size, caching them dynamically is not very efficient, due to the complexity in terms of efficiency and space, and the skewed distribution of the query stream, as shown later.",
                "Static caching of posting lists poses even more challenges: when deciding which terms to cache one faces the trade-off between frequently queried terms and terms with small posting lists that are space efficient.",
                "Finally, before deciding to adopt a static caching policy the query stream should be analyzed to verify that its characteristics do not change rapidly over time.",
                "Broker Static caching posting lists Dynamic/Static cached answers Local query processor Disk Next caching level Local network access Remote network access Figure 1: One caching level in a distributed search architecture.",
                "In this paper we explore the trade-offs in the design of each cache level, showing that the problem is the same and only a few parameters change.",
                "In general, we assume that each level of caching in a distributed search architecture is similar to that shown in Figure 1.",
                "We use a query log spanning a whole year to explore the limitations of dynamically caching query answers or posting lists for query terms.",
                "More concretely, our main conclusions are that: • Caching query answers results in lower hit ratios compared to caching of posting lists for query terms, but it is faster because there is no need for query evaluation.",
                "We provide a framework for the analysis of the trade-off between static caching of query answers and posting lists; • Static caching of terms can be more effective than dynamic caching with, for example, LRU.",
                "We provide algorithms based on the Knapsack problem for selecting the posting lists to put in a static cache, and we show improvements over previous work, achieving a hit ratio over 90%; • Changes of the query distribution over time have little impact on static caching.",
                "The remainder of this paper is organized as follows.",
                "Sections 2 and 3 summarize related work and characterize the data sets we use.",
                "Section 4 discusses the limitations of dynamic caching.",
                "Sections 5 and 6 introduce algorithms for caching posting lists, and a theoretical framework for the analysis of static caching, respectively.",
                "Section 7 discusses the impact of changes in the query distribution on static caching, and Section 8 provides concluding remarks. 2.",
                "RELATED WORK There is a large body of work devoted to query optimization.",
                "Buckley and Lewit [3], in one of the earliest works, take a term-at-a-time approach to deciding when inverted lists need not be further examined.",
                "More recent examples demonstrate that the top k documents for a query can be returned without the need for evaluating the complete set of posting lists [1, 4, 15].",
                "Although these approaches seek to improve query processing efficiency, they differ from our current work in that they do not consider caching.",
                "They may be considered separate and complementary to a cache-based approach.",
                "Raghavan and Sever [12], in one of the first papers on exploiting user query history, propose using a query base, built upon a set of persistent optimal queries submitted in the past, to improve the retrieval effectiveness for similar future queries.",
                "Markatos [10] shows the existence of temporal locality in queries, and compares the performance of different caching policies.",
                "Based on the observations of Markatos, Lempel and Moran propose a new caching policy, called Probabilistic Driven Caching, by attempting to estimate the probability distribution of all possible queries submitted to a search engine [8].",
                "Fagni et al. follow Markatos work by showing that combining static and dynamic caching policies together with an adaptive prefetching policy achieves a high hit ratio [7].",
                "Different from our work, they consider caching and prefetching of pages of results.",
                "As systems are often hierarchical, there has also been some effort on multi-level architectures.",
                "Saraiva et al. propose a new architecture for Web search engines using a two-level dynamic caching system [13].",
                "Their goal for such systems has been to improve response time for hierarchical engines.",
                "In their architecture, both levels use an LRU eviction policy.",
                "They find that the second-level cache can effectively reduce disk traffic, thus increasing the overall throughput.",
                "Baeza-Yates and Saint-Jean propose a three-level index organization [2].",
                "Long and Suel propose a caching system structured according to three different levels [9].",
                "The intermediate level contains frequently occurring pairs of terms and stores the intersections of the corresponding inverted lists.",
                "These last two papers are related to ours in that they exploit different caching strategies at different levels of the memory hierarchy.",
                "Finally, our static caching algorithm for posting lists in Section 5 uses the ratio frequency/size in order to evaluate the goodness of an item to cache.",
                "Similar ideas have been used in the context of file caching [17], Web caching [5], and even caching of posting lists [9], but in all cases in a dynamic setting.",
                "To the best of our knowledge we are the first to use this approach for static caching of posting lists. 3.",
                "DATA CHARACTERIZATION Our data consists of a crawl of documents from the UK domain, and query logs of one year of queries submitted to http://www.yahoo.co.uk from November 2005 to November 2006.",
                "In our logs, 50% of the total volume of queries are unique.",
                "The average query length is 2.5 terms, with the longest query having 731 terms. 1e-07 1e-06 1e-05 1e-04 0.001 0.01 0.1 1 1e-08 1e-07 1e-06 1e-05 1e-04 0.001 0.01 0.1 1 Frequency(normalized) Frequency rank (normalized) Figure 2: The distribution of queries (bottom curve) and query terms (middle curve) in the query log.",
                "The distribution of document frequencies of terms in the UK-2006 dataset (upper curve).",
                "Figure 2 shows the distributions of queries (lower curve), and query terms (middle curve).",
                "The x-axis represents the normalized frequency rank of the query or term. (The most frequent query appears closest to the y-axis.)",
                "The y-axis is Table 1: Statistics of the UK-2006 sample.",
                "UK-2006 sample statistics # of documents 2,786,391 # of terms 6,491,374 # of tokens 2,109,512,558 the normalized frequency for a given query (or term).",
                "As expected, the distribution of query frequencies and query term frequencies follow power law distributions, with slope of 1.84 and 2.26, respectively.",
                "In this figure, the query frequencies were computed as they appear in the logs with no normalization for case or white space.",
                "The query terms (middle curve) have been normalized for case, as have the terms in the document collection.",
                "The document collection that we use for our experiments is a summary of the UK domain crawled in May 2006.1 This summary corresponds to a maximum of 400 crawled documents per host, using a breadth first crawling strategy, comprising 15GB.",
                "The distribution of document frequencies of terms in the collection follows a power law distribution with slope 2.38 (upper curve in Figure 2).",
                "The statistics of the collection are shown in Table 1.",
                "We measured the correlation between the document frequency of terms in the collection and the number of queries that contain a particular term in the query log to be 0.424.",
                "A scatter plot for a random sample of terms is shown in Figure 3.",
                "In this experiment, terms have been converted to lower case in both the queries and the documents so that the frequencies will be comparable. 1e-07 1e-06 1e-05 1e-04 0.001 0.01 0.1 1 1e-06 1e-05 1e-04 0.001 0.01 0.1 1 Queryfrequency Document frequency Figure 3: Normalized scatter plot of document-term frequencies vs. query-term frequencies. 4.",
                "CACHING OF QUERIES AND TERMS Caching relies upon the assumption that there is locality in the stream of requests.",
                "That is, there must be sufficient repetition in the stream of requests and within intervals of time that enable a cache memory of reasonable size to be effective.",
                "In the query log we used, 88% of the unique queries are singleton queries, and 44% are singleton queries out of the whole volume.",
                "Thus, out of all queries in the stream composing the query log, the upper threshold on hit ratio is 56%.",
                "This is because only 56% of all the queries comprise queries that have multiple occurrences.",
                "It is important to observe, however, that not all queries in this 56% can be cache hits because of compulsory misses.",
                "A compulsory miss 1 The collection is available from the University of Milan: http://law.dsi.unimi.it/.",
                "URL retrieved 05/2007. 0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1 240 260 280 300 320 340 360 Numberofelements Bin number Total terms Terms diff Total queries Unique queries Unique terms Query diff Figure 4: Arrival rate for both terms and queries. happens when the cache receives a query for the first time.",
                "This is different from capacity misses, which happen due to space constraints on the amount of memory the cache uses.",
                "If we consider a cache with infinite memory, then the hit ratio is 50%.",
                "Note that for an infinite cache there are no capacity misses.",
                "As we mentioned before, another possibility is to cache the posting lists of terms.",
                "Intuitively, this gives more freedom in the utilization of the cache content to respond to queries because cached terms might form a new query.",
                "On the other hand, they need more space.",
                "As opposed to queries, the fraction of singleton terms in the total volume of terms is smaller.",
                "In our query log, only 4% of the terms appear once, but this accounts for 73% of the vocabulary of query terms.",
                "We show in Section 5 that caching a small fraction of terms, while accounting for terms appearing in many documents, is potentially very effective.",
                "Figure 4 shows several graphs corresponding to the normalized arrival rate for different cases using days as bins.",
                "That is, we plot the normalized number of elements that appear in a day.",
                "This graph shows only a period of 122 days, and we normalize the values by the maximum value observed throughout the whole period of the query log.",
                "Total queries and Total terms correspond to the total volume of queries and terms, respectively.",
                "Unique queries and Unique terms correspond to the arrival rate of unique queries and terms.",
                "Finally, Query diff and Terms diff correspond to the difference between the curves for total and unique.",
                "In Figure 4, as expected, the volume of terms is much higher than the volume of queries.",
                "The difference between the total number of terms and the number of unique terms is much larger than the difference between the total number of queries and the number of unique queries.",
                "This observation implies that terms repeat significantly more than queries.",
                "If we use smaller bins, say of one hour, then the ratio of unique to volume is higher for both terms and queries because it leaves less room for repetition.",
                "We also estimated the workload using the document frequency of terms as a measure of how much work a query imposes on a search engine.",
                "We found that it follows closely the arrival rate for terms shown in Figure 4.",
                "To demonstrate the effect of a dynamic cache on the query frequency distribution of Figure 2, we plot the same frequency graph, but now considering the frequency of queries Figure 5: Frequency graph after LRU cache. after going through an LRU cache.",
                "On a cache miss, an LRU cache decides upon an entry to evict using the information on the recency of queries.",
                "In this graph, the most frequent queries are not the same queries that were most frequent before the cache.",
                "It is possible that queries that are most frequent after the cache have different characteristics, and tuning the search engine to queries frequent before the cache may degrade performance for non-cached queries.",
                "The maximum frequency after caching is less than 1% of the maximum frequency before the cache, thus showing that the cache is very effective in reducing the load of frequent queries.",
                "If we re-rank the queries according to after-cache frequency, the distribution is still a power law, but with a much smaller value for the highest frequency.",
                "When discussing the effectiveness of dynamically caching, an important metric is cache miss rate.",
                "To analyze the cache miss rate for different memory constraints, we use the working set model [6, 14].",
                "A working set, informally, is the set of references that an application or an operating system is currently working with.",
                "The model uses such sets in a strategy that tries to capture the temporal locality of references.",
                "The working set strategy then consists in keeping in memory only the elements that are referenced in the previous θ steps of the input sequence, where θ is a configurable parameter corresponding to the window size.",
                "Originally, working sets have been used for page replacement algorithms of operating systems, and considering such a strategy in the context of search engines is interesting for three reasons.",
                "First, it captures the amount of locality of queries and terms in a sequence of queries.",
                "Locality in this case refers to the frequency of queries and terms in a window of time.",
                "If many queries appear multiple times in a window, then locality is high.",
                "Second, it enables an oﬄine analysis of the expected miss rate given different memory constraints.",
                "Third, working sets capture aspects of efficient caching algorithms such as LRU.",
                "LRU assumes that references farther in the past are less likely to be referenced in the present, which is implicit in the concept of working sets [14].",
                "Figure 6 plots the miss rate for different working set sizes, and we consider working sets of both queries and terms.",
                "The working set sizes are normalized against the total number of queries in the query log.",
                "In the graph for queries, there is a sharp decay until approximately 0.01, and the rate at which the miss rate drops decreases as we increase the size of the working set over 0.01.",
                "Finally, the minimum value it reaches is 50% miss rate, not shown in the figure as we have cut the tail of the curve for presentation purposes. 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1 0 0.05 0.1 0.15 0.2 Missrate Normalized working set size Queries Terms Figure 6: Miss rate as a function of the working set size. 1 10 100 1000 10000 100000 1e+06 Frequency Distance Figure 7: Distribution of distances expressed in terms of distinct queries.",
                "Compared to the query curve, we observe that the minimum miss rate for terms is substantially smaller.",
                "The miss rate also drops sharply on values up to 0.01, and it decreases minimally for higher values.",
                "The minimum value, however, is slightly over 10%, which is much smaller than the minimum value for the sequence of queries.",
                "This implies that with such a policy it is possible to achieve over 80% hit rate, if we consider caching dynamically posting lists for terms as opposed to caching answers for queries.",
                "This result does not consider the space required for each unit stored in the cache memory, or the amount of time it takes to put together a response to a user query.",
                "We analyze these issues more carefully later in this paper.",
                "It is interesting also to observe the histogram of Figure 7, which is an intermediate step in the computation of the miss rate graph.",
                "It reports the distribution of distances between repetitions of the same frequent query.",
                "The distance in the plot is measured in the number of distinct queries separating a query and its repetition, and it considers only queries appearing at least 10 times.",
                "From Figures 6 and 7, we conclude that even if we set the size of the query answers cache to a relatively large number of entries, the miss rate is high.",
                "Thus, caching the posting lists of terms has the potential to improve the hit ratio.",
                "This is what we explore next. 5.",
                "CACHING POSTING LISTS The previous section shows that caching posting lists can obtain a higher hit rate compared to caching query answers.",
                "In this section we study the problem of how to select posting lists to place on a certain amount of available memory, assuming that the whole index is larger than the amount of memory available.",
                "The posting lists have variable size (in fact, their size distribution follows a power law), so it is beneficial for a caching policy to consider the sizes of the posting lists.",
                "We consider both dynamic and static caching.",
                "For dynamic caching, we use two well-known policies, LRU and LFU, as well as a modified algorithm that takes posting-list size into account.",
                "Before discussing the static caching strategies, we introduce some notation.",
                "We use fq(t) to denote the query-term frequency of a term t, that is, the number of queries containing t in the query log, and fd(t) to denote the document frequency of t, that is, the number of documents in the collection in which the term t appears.",
                "The first strategy we consider is the algorithm proposed by Baeza-Yates and Saint-Jean [2], which consists in selecting the posting lists of the terms with the highest query-term frequencies fq(t).",
                "We call this algorithm Qtf.",
                "We observe that there is a trade-off between fq(t) and fd(t).",
                "Terms with high fq(t) are useful to keep in the cache because they are queried often.",
                "On the other hand, terms with high fd(t) are not good candidates because they correspond to long posting lists and consume a substantial amount of space.",
                "In fact, the problem of selecting the best posting lists for the static cache corresponds to the standard Knapsack problem: given a knapsack of fixed capacity, and a set of n items, such as the i-th item has value ci and size si, select the set of items that fit in the knapsack and maximize the overall value.",
                "In our case, value corresponds to fq(t) and size corresponds to fd(t).",
                "Thus, we employ a simple algorithm for the knapsack problem, which is selecting the posting lists of the terms with the highest values of the ratio fq(t) fd(t) .",
                "We call this algorithm QtfDf.",
                "We tried other variations considering query frequencies instead of term frequencies, but the gain was minimal compared to the complexity added.",
                "In addition to the above two static algorithms we consider the following algorithms for dynamic caching: • LRU: A standard LRU algorithm, but many posting lists might need to be evicted (in order of least-recent usage) until there is enough space in the memory to place the currently accessed posting list; • LFU: A standard LFU algorithm (eviction of the leastfrequently used), with the same modification as the LRU; • Dyn-QtfDf: A dynamic version of the QtfDf algorithm; evict from the cache the term(s) with the lowest fq(t) fd(t) ratio.",
                "The performance of all the above algorithms for 15 weeks of the query log and the UK dataset are shown in Figure 8.",
                "Performance is measured with hit rate.",
                "The cache size is measured as a fraction of the total space required to store the posting lists of all terms.",
                "For the dynamic algorithms, we load the cache with terms in order of fq(t) and we let the cache warm up for 1 million queries.",
                "For the static algorithms, we assume complete knowledge of the frequencies fq(t), that is, we estimate fq(t) from the whole query stream.",
                "As we show in Section 7 the results do not change much if we compute the query-term frequencies using the first 3 or 4 weeks of the query log and measure the hit rate on the rest. 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1 0.1 0.2 0.3 0.4 0.5 0.6 0.7 Hitrate Cache size Caching posting lists static QTF/DF LRU LFU Dyn-QTF/DF QTF Figure 8: Hit rate of different strategies for caching posting lists.",
                "The most important observation from our experiments is that the static QtfDf algorithm has a better hit rate than all the dynamic algorithms.",
                "An important benefit a static cache is that it requires no eviction and it is hence more efficient when evaluating queries.",
                "However, if the characteristics of the query traffic change frequently over time, then it requires re-populating the cache often or there will be a significant impact on hit rate. 6.",
                "ANALYSIS OF STATIC CACHING In this section we provide a detailed analysis for the problem of deciding whether it is preferable to cache query answers or cache posting lists.",
                "Our analysis takes into account the impact of caching between two levels of the <br>data-access hierarchy</br>.",
                "It can either be applied at the memory/disk layer or at a server/remote server layer as in the architecture we discussed in the introduction.",
                "Using a particular system model, we obtain estimates for the parameters required by our analysis, which we subsequently use to decide the optimal trade-off between caching query answers and caching posting lists. 6.1 Analytical Model Let M be the size of the cache measured in answer units (the cache can store M query answers).",
                "Assume that all posting lists are of the same length L, measured in answer units.",
                "We consider the following two cases: (A) a cache that stores only precomputed answers, and (B) a cache that stores only posting lists.",
                "In the first case, Nc = M answers fit in the cache, while in the second case Np = M/L posting lists fit in the cache.",
                "Thus, Np = Nc/L.",
                "Note that although posting lists require more space, we can combine terms to evaluate more queries (or partial queries).",
                "For case (A), suppose that a query answer in the cache can be evaluated in 1 time unit.",
                "For case (B), assume that if the posting lists of the terms of a query are in the cache then the results can be computed in TR1 time units, while if the posting lists are not in the cache then the results can be computed in TR2 time units.",
                "Of course TR2 > TR1.",
                "Now we want to compare the time to answer a stream of Q queries in both cases.",
                "Let Vc(Nc) be the volume of the most frequent Nc queries.",
                "Then, for case (A), we have an overall time TCA = Vc(Nc) + TR2(Q − Vc(Nc)).",
                "Similarly, for case (B), let Vp(Np) be the number of computable queries.",
                "Then we have overall time TP L = TR1Vp(Np) + TR2(Q − Vp(Np)).",
                "We want to check under which conditions we have TP L < TCA.",
                "We have TP L − TCA = (TR2 − 1)Vc(Nc) − (TR2 − TR1)Vp(Np) > 0.",
                "Figure 9 shows the values of Vp and Vc for our data.",
                "We can see that caching answers saturates faster and for this particular data there is no additional benefit from using more than 10% of the index space for caching answers.",
                "As the query distribution is a power law with parameter α > 1, the i-th most frequent query appears with probability proportional to 1 iα .",
                "Therefore, the volume Vc(n), which is the total number of the n most frequent queries, is Vc(n) = V0 n i=1 Q iα = γnQ (0 < γn < 1).",
                "We know that Vp(n) grows faster than Vc(n) and assume, based on experimental results, that the relation is of the form Vp(n) = k Vc(n)β .",
                "In the worst case, for a large cache, β → 1.",
                "That is, both techniques will cache a constant fraction of the overall query volume.",
                "Then caching posting lists makes sense only if L(TR2 − 1) k(TR2 − TR1) > 1.",
                "If we use compression, we have L < L and TR1 > TR1.",
                "According to the experiments that we show later, compression is always better.",
                "For a small cache, we are interested in the transient behavior and then β > 1, as computed from our data.",
                "In this case there will always be a point where TP L > TCA for a large number of queries.",
                "In reality, instead of filling the cache only with answers or only with posting lists, a better strategy will be to divide the total cache space into cache for answers and cache for posting lists.",
                "In such a case, there will be some queries that could be answered by both parts of the cache.",
                "As the answer cache is faster, it will be the first choice for answering those queries.",
                "Let QNc and QNp be the set of queries that can be answered by the cached answers and the cached posting lists, respectively.",
                "Then, the overall time is T = Vc(Nc)+TR1V (QNp −QNc )+TR2(Q−V (QNp ∪QNc )), where Np = (M − Nc)/L.",
                "Finding the optimal division of the cache in order to minimize the overall retrieval time is a difficult problem to solve analytically.",
                "In Section 6.3 we use simulations to derive optimal cache trade-offs for particular implementation examples. 6.2 Parameter Estimation We now use a particular implementation of a centralized system and the model of a distributed system as examples from which we estimate the parameters of the analysis from the previous section.",
                "We perform the experiments using an optimized version of Terrier [11] for both indexing documents and processing queries, on a single machine with a Pentium 4 at 2GHz and 1GB of RAM.",
                "We indexed the documents from the UK-2006 dataset, without removing stop words or applying stemming.",
                "The posting lists in the inverted file consist of pairs of document identifier and term frequency.",
                "We compress the document identifier gaps using Elias gamma encoding, and the 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1 0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1 Queryvolume Space precomputed answers posting lists Figure 9: Cache saturation as a function of size.",
                "Table 2: Ratios between the average time to evaluate a query and the average time to return cached answers (centralized and distributed case).",
                "Centralized system TR1 TR2 TR1 TR2 Full evaluation 233 1760 707 1140 Partial evaluation 99 1626 493 798 LAN system TRL 1 TRL 2 TR L 1 TR L 2 Full evaluation 242 1769 716 1149 Partial evaluation 108 1635 502 807 WAN system TRW 1 TRW 2 TR W 1 TR W 2 Full evaluation 5001 6528 5475 5908 Partial evaluation 4867 6394 5270 5575 term frequencies in documents using unary encoding [16].",
                "The size of the inverted file is 1,189Mb.",
                "A stored answer requires 1264 bytes, and an uncompressed posting takes 8 bytes.",
                "From Table 1, we obtain L = (8·# of postings) 1264·# of terms = 0.75 and L = Inverted file size 1264·# of terms = 0.26.",
                "We estimate the ratio TR = T/Tc between the average time T it takes to evaluate a query and the average time Tc it takes to return a stored answer for the same query, in the following way.",
                "Tc is measured by loading the answers for 100,000 queries in memory, and answering the queries from memory.",
                "The average time is Tc = 0.069ms.",
                "T is measured by processing the same 100,000 queries (the first 10,000 queries are used to warm-up the system).",
                "For each query, we remove stop words, if there are at least three remaining terms.",
                "The stop words correspond to the terms with a frequency higher than the number of documents in the index.",
                "We use a document-at-a-time approach to retrieve documents containing all query terms.",
                "The only disk access required during query processing is for reading compressed posting lists from the inverted file.",
                "We perform both full and partial evaluation of answers, because some queries are likely to retrieve a large number of documents, and only a fraction of the retrieved documents will be seen by users.",
                "In the partial evaluation of queries, we terminate the processing after matching 10,000 documents.",
                "The estimated ratios TR are presented in Table 2.",
                "Figure 10 shows for a sample of queries the workload of the system with partial query evaluation and compressed posting lists.",
                "The x-axis corresponds to the total time the system spends processing a particular query, and the vertical axis corresponds to the sum t∈q fq · fd(t).",
                "Notice that the total number of postings of the query-terms does not necessarily provide an accurate estimate of the workload imposed on the system by a query (which is the case for full evaluation and uncompressed lists). 0 0.2 0.4 0.6 0.8 1 0 0.2 0.4 0.6 0.8 1 Totalpostingstoprocessquery(normalized) Total time to process query (normalized) Partial processing of compressed postings query len = 1 query len in [2,3] query len in [4,8] query len > 8 Figure 10: Workload for partial query evaluation with compressed posting lists.",
                "The analysis of the previous section also applies to a distributed retrieval system in one or multiple sites.",
                "Suppose that a document partitioned distributed system is running on a cluster of machines interconnected with a local area network (LAN) in one site.",
                "The broker receives queries and broadcasts them to the query processors, which answer the queries and return the results to the broker.",
                "Finally, the broker merges the received answers and generates the final set of answers (we assume that the time spent on merging results is negligible).",
                "The difference between the centralized architecture and the document partition architecture is the extra communication between the broker and the query processors.",
                "Using ICMP pings on a 100Mbps LAN, we have measured that sending the query from the broker to the query processors which send an answer of 4,000 bytes back to the broker takes on average 0.615ms.",
                "Hence, TRL = TR + 0.615ms/0.069ms = TR + 9.",
                "In the case when the broker and the query processors are in different sites connected with a wide area network (WAN), we estimated that broadcasting the query from the broker to the query processors and getting back an answer of 4,000 bytes takes on average 329ms.",
                "Hence, TRW = TR + 329ms/0.069ms = TR + 4768. 6.3 Simulation Results We now address the problem of finding the optimal tradeoff between caching query answers and caching posting lists.",
                "To make the problem concrete we assume a fixed budget M on the available memory, out of which x units are used for caching query answers and M − x for caching posting lists.",
                "We perform simulations and compute the average response time as a function of x.",
                "Using a part of the query log as training data, we first allocate in the cache the answers to the most frequent queries that fit in space x, and then we use the rest of the memory to cache posting lists.",
                "For selecting posting lists we use the QtfDf algorithm, applied to the training query log but excluding the queries that have already been cached.",
                "In Figure 11, we plot the simulated response time for a centralized system as a function of x.",
                "For the uncompressed index we use M = 1GB, and for the compressed index we use M = 0.5GB.",
                "In the case of the configuration that uses partial query evaluation with compressed posting lists, the lowest response time is achieved when 0.15GB out of the 0.5GB is allocated for storing answers for queries.",
                "We obtained similar trends in the results for the LAN setting.",
                "Figure 12 shows the simulated workload for a distributed system across a WAN.",
                "In this case, the total amount of memory is split between the broker, which holds the cached 400 500 600 700 800 900 1000 1100 1200 0 0.2 0.4 0.6 0.8 1 Averageresponsetime Space (GB) Simulated workload -- single machine full / uncompr / 1 G partial / uncompr / 1 G full / compr / 0.5 G partial / compr / 0.5 G Figure 11: Optimal division of the cache in a server. 3000 3500 4000 4500 5000 5500 6000 0 0.2 0.4 0.6 0.8 1 Averageresponsetime Space (GB) Simulated workload -- WAN full / uncompr / 1 G partial / uncompr / 1 G full / compr / 0.5 G partial / compr / 0.5 G Figure 12: Optimal division of the cache when the next level requires WAN access. answers of queries, and the query processors, which hold the cache of posting lists.",
                "According to the figure, the difference between the configurations of the query processors is less important because the network communication overhead increases the response time substantially.",
                "When using uncompressed posting lists, the optimal allocation of memory corresponds to using approximately 70% of the memory for caching query answers.",
                "This is explained by the fact that there is no need for network communication when the query can be answered by the cache at the broker. 7.",
                "EFFECT OF THE QUERY DYNAMICS For our query log, the query distribution and query-term distribution change slowly over time.",
                "To support this claim, we first assess how topics change comparing the distribution of queries from the first week in June, 2006, to the distribution of queries for the remainder of 2006 that did not appear in the first week in June.",
                "We found that a very small percentage of queries are new queries.",
                "The majority of queries that appear in a given week repeat in the following weeks for the next six months.",
                "We then compute the hit rate of a static cache of 128, 000 answers trained over a period of two weeks (Figure 13).",
                "We report hit rate hourly for 7 days, starting from 5pm.",
                "We observe that the hit rate reaches its highest value during the night (around midnight), whereas around 2-3pm it reaches its minimum.",
                "After a small decay in hit rate values, the hit rate stabilizes between 0.28, and 0.34 for the entire week, suggesting that the static cache is effective for a whole week after the training period. 0.26 0.27 0.28 0.29 0.3 0.31 0.32 0.33 0.34 0.35 0.36 0.37 0 20 40 60 80 100 120 140 160 Hit-rate Time Hits on the frequent queries of distances Figure 13: Hourly hit rate for a static cache holding 128,000 answers during the period of a week.",
                "The static cache of posting lists can be periodically recomputed.",
                "To estimate the time interval in which we need to recompute the posting lists on the static cache we need to consider an efficiency/quality trade-off: using too short a time interval might be prohibitively expensive, while recomputing the cache too infrequently might lead to having an obsolete cache not corresponding to the statistical characteristics of the current query stream.",
                "We measured the effect on the QtfDf algorithm of the changes in a 15-week query stream (Figure 14).",
                "We compute the query term frequencies over the whole stream, select which terms to cache, and then compute the hit rate on the whole query stream.",
                "This hit rate is as an upper bound, and it assumes perfect knowledge of the query term frequencies.",
                "To simulate a realistic scenario, we use the first 6 (3) weeks of the query stream for computing query term frequencies and the following 9 (12) weeks to estimate the hit rate.",
                "As Figure 14 shows, the hit rate decreases by less than 2%.",
                "The high correlation among the query term frequencies during different time periods explains the graceful adaptation of the static caching algorithms to the future query stream.",
                "Indeed, the pairwise correlation among all possible 3-week periods of the 15-week query stream is over 99.5%. 8.",
                "CONCLUSIONS Caching is an effective technique in search engines for improving response time, reducing the load on query processors, and improving network bandwidth utilization.",
                "We present results on both dynamic and static caching.",
                "Dynamic caching of queries has limited effectiveness due to the high number of compulsory misses caused by the number of unique or infrequent queries.",
                "Our results show that in our UK log, the minimum miss rate is 50% using a working set strategy.",
                "Caching terms is more effective with respect to miss rate, achieving values as low as 12%.",
                "We also propose a new algorithm for static caching of posting lists that outperforms previous static caching algorithms as well as dynamic algorithms such as LRU and LFU, obtaining hit rate values that are over 10% higher compared these strategies.",
                "We present a framework for the analysis of the trade-off between caching query results and caching posting lists, and we simulate different types of architectures.",
                "Our results show that for centralized and LAN environments, there is an optimal allocation of caching query results and caching of posting lists, while for WAN scenarios in which network time prevails it is more important to cache query results. 0.45 0.5 0.55 0.6 0.65 0.7 0.75 0.8 0.85 0.9 0.95 0.1 0.2 0.3 0.4 0.5 0.6 0.7 Hitrate Cache size Dynamics of static QTF/DF caching policy perfect knowledge 6-week training 3-week training Figure 14: Impact of distribution changes on the static caching of posting lists. 9.",
                "REFERENCES [1] V. N. Anh and A. Moffat.",
                "Pruned query evaluation using pre-computed impacts.",
                "In ACM CIKM, 2006. [2] R. A. Baeza-Yates and F. Saint-Jean.",
                "A three level search engine index based in query log distribution.",
                "In SPIRE, 2003. [3] C. Buckley and A. F. Lewit.",
                "Optimization of inverted vector searches.",
                "In ACM SIGIR, 1985. [4] S. B¨uttcher and C. L. A. Clarke.",
                "A document-centric approach to static index pruning in text retrieval systems.",
                "In ACM CIKM, 2006. [5] P. Cao and S. Irani.",
                "Cost-aware WWW proxy caching algorithms.",
                "In USITS, 1997. [6] P. Denning.",
                "Working sets past and present.",
                "IEEE Trans. on Software Engineering, SE-6(1):64-84, 1980. [7] T. Fagni, R. Perego, F. Silvestri, and S. Orlando.",
                "Boosting the performance of web search engines: Caching and prefetching query results by exploiting historical usage data.",
                "ACM Trans.",
                "Inf.",
                "Syst., 24(1):51-78, 2006. [8] R. Lempel and S. Moran.",
                "Predictive caching and prefetching of query results in search engines.",
                "In WWW, 2003. [9] X.",
                "Long and T. Suel.",
                "Three-level caching for efficient query processing in large web search engines.",
                "In WWW, 2005. [10] E. P. Markatos.",
                "On caching search engine query results.",
                "Computer Communications, 24(2):137-143, 2001. [11] I. Ounis, G. Amati, V. Plachouras, B.",
                "He, C. Macdonald, and C. Lioma.",
                "Terrier: A High Performance and Scalable Information Retrieval Platform.",
                "In SIGIR Workshop on Open Source Information Retrieval, 2006. [12] V. V. Raghavan and H. Sever.",
                "On the reuse of past optimal queries.",
                "In ACM SIGIR, 1995. [13] P. C. Saraiva, E. S. de Moura, N. Ziviani, W. Meira, R. Fonseca, and B. Riberio-Neto.",
                "Rank-preserving two-level caching for scalable search engines.",
                "In ACM SIGIR, 2001. [14] D. R. Slutz and I. L. Traiger.",
                "A note on the calculation of average working set size.",
                "Communications of the ACM, 17(10):563-565, 1974. [15] T. Strohman, H. Turtle, and W. B. Croft.",
                "Optimization strategies for complex queries.",
                "In ACM SIGIR, 2005. [16] I. H. Witten, T. C. Bell, and A. Moffat.",
                "Managing Gigabytes: Compressing and Indexing Documents and Images.",
                "John Wiley & Sons, Inc., NY, 1994. [17] N. E. Young.",
                "On-line file caching.",
                "Algorithmica, 33(3):371-383, 2002."
            ],
            "original_annotated_samples": [],
            "translated_annotated_samples": [
                "Nuestros resultados y observaciones son aplicables a diferentes niveles de la \"jerarquía de acceso de datos\", por ejemplo, para una capa de memoria/disco o una capa de corredor/servidor remoto.",
                "Nuestro análisis tiene en cuenta el impacto del almacenamiento en caché entre dos niveles de la \"jerarquía de acceso de datos\"."
            ],
            "translated_text": "",
            "candidates": [
                "jerarquía de acceso de datos",
                "jerarquía de acceso de datos",
                "jerarquía de acceso de datos",
                "jerarquía de acceso de datos"
            ],
            "error": []
        },
        "disk layer": {
            "translated_key": "capa de disco",
            "is_in_text": true,
            "original_annotated_sentences": [
                "The Impact of Caching on Search Engines Ricardo Baeza-Yates1 rbaeza@acm.org Aristides Gionis1 gionis@yahoo-inc.com Flavio Junqueira1 fpj@yahoo-inc.com Vanessa Murdock1 vmurdock@yahoo-inc.com Vassilis Plachouras1 vassilis@yahoo-inc.com Fabrizio Silvestri2 f.silvestri@isti.cnr.it 1 Yahoo!",
                "Research Barcelona 2 ISTI - CNR Barcelona, SPAIN Pisa, ITALY ABSTRACT In this paper we study the trade-offs in designing efficient caching systems for Web search engines.",
                "We explore the impact of different approaches, such as static vs. dynamic caching, and caching query results vs. caching posting lists.",
                "Using a query log spanning a whole year we explore the limitations of caching and we demonstrate that caching posting lists can achieve higher hit rates than caching query answers.",
                "We propose a new algorithm for static caching of posting lists, which outperforms previous methods.",
                "We also study the problem of finding the optimal way to split the static cache between answers and posting lists.",
                "Finally, we measure how the changes in the query log affect the effectiveness of static caching, given our observation that the distribution of the queries changes slowly over time.",
                "Our results and observations are applicable to different levels of the data-access hierarchy, for instance, for a memory/<br>disk layer</br> or a broker/remote server layer.",
                "Categories and Subject Descriptors H.3.3 [Information Storage and Retrieval]: Information Search and Retrieval - Search process; H.3.4 [Information Storage and Retrieval]: Systems and Software - Distributed systems, Performance evaluation (efficiency and effectiveness) General Terms Algorithms, Experimentation 1.",
                "INTRODUCTION Millions of queries are submitted daily to Web search engines, and users have high expectations of the quality and speed of the answers.",
                "As the searchable Web becomes larger and larger, with more than 20 billion pages to index, evaluating a single query requires processing large amounts of data.",
                "In such a setting, to achieve a fast response time and to increase the query throughput, using a cache is crucial.",
                "The primary use of a cache memory is to speedup computation by exploiting frequently or recently used data, although reducing the workload to back-end servers is also a major goal.",
                "Caching can be applied at different levels with increasing response latencies or processing requirements.",
                "For example, the different levels may correspond to the main memory, the disk, or resources in a local or a wide area network.",
                "The decision of what to cache is either off-line (static) or online (dynamic).",
                "A static cache is based on historical information and is periodically updated.",
                "A dynamic cache replaces entries according to the sequence of requests.",
                "When a new request arrives, the cache system decides whether to evict some entry from the cache in the case of a cache miss.",
                "Such online decisions are based on a cache policy, and several different policies have been studied in the past.",
                "For a search engine, there are two possible ways to use a cache memory: Caching answers: As the engine returns answers to a particular query, it may decide to store these answers to resolve future queries.",
                "Caching terms: As the engine evaluates a particular query, it may decide to store in memory the posting lists of the involved query terms.",
                "Often the whole set of posting lists does not fit in memory, and consequently, the engine has to select a small set to keep in memory and speed up query processing.",
                "Returning an answer to a query that already exists in the cache is more efficient than computing the answer using cached posting lists.",
                "On the other hand, previously unseen queries occur more often than previously unseen terms, implying a higher miss rate for cached answers.",
                "Caching of posting lists has additional challenges.",
                "As posting lists have variable size, caching them dynamically is not very efficient, due to the complexity in terms of efficiency and space, and the skewed distribution of the query stream, as shown later.",
                "Static caching of posting lists poses even more challenges: when deciding which terms to cache one faces the trade-off between frequently queried terms and terms with small posting lists that are space efficient.",
                "Finally, before deciding to adopt a static caching policy the query stream should be analyzed to verify that its characteristics do not change rapidly over time.",
                "Broker Static caching posting lists Dynamic/Static cached answers Local query processor Disk Next caching level Local network access Remote network access Figure 1: One caching level in a distributed search architecture.",
                "In this paper we explore the trade-offs in the design of each cache level, showing that the problem is the same and only a few parameters change.",
                "In general, we assume that each level of caching in a distributed search architecture is similar to that shown in Figure 1.",
                "We use a query log spanning a whole year to explore the limitations of dynamically caching query answers or posting lists for query terms.",
                "More concretely, our main conclusions are that: • Caching query answers results in lower hit ratios compared to caching of posting lists for query terms, but it is faster because there is no need for query evaluation.",
                "We provide a framework for the analysis of the trade-off between static caching of query answers and posting lists; • Static caching of terms can be more effective than dynamic caching with, for example, LRU.",
                "We provide algorithms based on the Knapsack problem for selecting the posting lists to put in a static cache, and we show improvements over previous work, achieving a hit ratio over 90%; • Changes of the query distribution over time have little impact on static caching.",
                "The remainder of this paper is organized as follows.",
                "Sections 2 and 3 summarize related work and characterize the data sets we use.",
                "Section 4 discusses the limitations of dynamic caching.",
                "Sections 5 and 6 introduce algorithms for caching posting lists, and a theoretical framework for the analysis of static caching, respectively.",
                "Section 7 discusses the impact of changes in the query distribution on static caching, and Section 8 provides concluding remarks. 2.",
                "RELATED WORK There is a large body of work devoted to query optimization.",
                "Buckley and Lewit [3], in one of the earliest works, take a term-at-a-time approach to deciding when inverted lists need not be further examined.",
                "More recent examples demonstrate that the top k documents for a query can be returned without the need for evaluating the complete set of posting lists [1, 4, 15].",
                "Although these approaches seek to improve query processing efficiency, they differ from our current work in that they do not consider caching.",
                "They may be considered separate and complementary to a cache-based approach.",
                "Raghavan and Sever [12], in one of the first papers on exploiting user query history, propose using a query base, built upon a set of persistent optimal queries submitted in the past, to improve the retrieval effectiveness for similar future queries.",
                "Markatos [10] shows the existence of temporal locality in queries, and compares the performance of different caching policies.",
                "Based on the observations of Markatos, Lempel and Moran propose a new caching policy, called Probabilistic Driven Caching, by attempting to estimate the probability distribution of all possible queries submitted to a search engine [8].",
                "Fagni et al. follow Markatos work by showing that combining static and dynamic caching policies together with an adaptive prefetching policy achieves a high hit ratio [7].",
                "Different from our work, they consider caching and prefetching of pages of results.",
                "As systems are often hierarchical, there has also been some effort on multi-level architectures.",
                "Saraiva et al. propose a new architecture for Web search engines using a two-level dynamic caching system [13].",
                "Their goal for such systems has been to improve response time for hierarchical engines.",
                "In their architecture, both levels use an LRU eviction policy.",
                "They find that the second-level cache can effectively reduce disk traffic, thus increasing the overall throughput.",
                "Baeza-Yates and Saint-Jean propose a three-level index organization [2].",
                "Long and Suel propose a caching system structured according to three different levels [9].",
                "The intermediate level contains frequently occurring pairs of terms and stores the intersections of the corresponding inverted lists.",
                "These last two papers are related to ours in that they exploit different caching strategies at different levels of the memory hierarchy.",
                "Finally, our static caching algorithm for posting lists in Section 5 uses the ratio frequency/size in order to evaluate the goodness of an item to cache.",
                "Similar ideas have been used in the context of file caching [17], Web caching [5], and even caching of posting lists [9], but in all cases in a dynamic setting.",
                "To the best of our knowledge we are the first to use this approach for static caching of posting lists. 3.",
                "DATA CHARACTERIZATION Our data consists of a crawl of documents from the UK domain, and query logs of one year of queries submitted to http://www.yahoo.co.uk from November 2005 to November 2006.",
                "In our logs, 50% of the total volume of queries are unique.",
                "The average query length is 2.5 terms, with the longest query having 731 terms. 1e-07 1e-06 1e-05 1e-04 0.001 0.01 0.1 1 1e-08 1e-07 1e-06 1e-05 1e-04 0.001 0.01 0.1 1 Frequency(normalized) Frequency rank (normalized) Figure 2: The distribution of queries (bottom curve) and query terms (middle curve) in the query log.",
                "The distribution of document frequencies of terms in the UK-2006 dataset (upper curve).",
                "Figure 2 shows the distributions of queries (lower curve), and query terms (middle curve).",
                "The x-axis represents the normalized frequency rank of the query or term. (The most frequent query appears closest to the y-axis.)",
                "The y-axis is Table 1: Statistics of the UK-2006 sample.",
                "UK-2006 sample statistics # of documents 2,786,391 # of terms 6,491,374 # of tokens 2,109,512,558 the normalized frequency for a given query (or term).",
                "As expected, the distribution of query frequencies and query term frequencies follow power law distributions, with slope of 1.84 and 2.26, respectively.",
                "In this figure, the query frequencies were computed as they appear in the logs with no normalization for case or white space.",
                "The query terms (middle curve) have been normalized for case, as have the terms in the document collection.",
                "The document collection that we use for our experiments is a summary of the UK domain crawled in May 2006.1 This summary corresponds to a maximum of 400 crawled documents per host, using a breadth first crawling strategy, comprising 15GB.",
                "The distribution of document frequencies of terms in the collection follows a power law distribution with slope 2.38 (upper curve in Figure 2).",
                "The statistics of the collection are shown in Table 1.",
                "We measured the correlation between the document frequency of terms in the collection and the number of queries that contain a particular term in the query log to be 0.424.",
                "A scatter plot for a random sample of terms is shown in Figure 3.",
                "In this experiment, terms have been converted to lower case in both the queries and the documents so that the frequencies will be comparable. 1e-07 1e-06 1e-05 1e-04 0.001 0.01 0.1 1 1e-06 1e-05 1e-04 0.001 0.01 0.1 1 Queryfrequency Document frequency Figure 3: Normalized scatter plot of document-term frequencies vs. query-term frequencies. 4.",
                "CACHING OF QUERIES AND TERMS Caching relies upon the assumption that there is locality in the stream of requests.",
                "That is, there must be sufficient repetition in the stream of requests and within intervals of time that enable a cache memory of reasonable size to be effective.",
                "In the query log we used, 88% of the unique queries are singleton queries, and 44% are singleton queries out of the whole volume.",
                "Thus, out of all queries in the stream composing the query log, the upper threshold on hit ratio is 56%.",
                "This is because only 56% of all the queries comprise queries that have multiple occurrences.",
                "It is important to observe, however, that not all queries in this 56% can be cache hits because of compulsory misses.",
                "A compulsory miss 1 The collection is available from the University of Milan: http://law.dsi.unimi.it/.",
                "URL retrieved 05/2007. 0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1 240 260 280 300 320 340 360 Numberofelements Bin number Total terms Terms diff Total queries Unique queries Unique terms Query diff Figure 4: Arrival rate for both terms and queries. happens when the cache receives a query for the first time.",
                "This is different from capacity misses, which happen due to space constraints on the amount of memory the cache uses.",
                "If we consider a cache with infinite memory, then the hit ratio is 50%.",
                "Note that for an infinite cache there are no capacity misses.",
                "As we mentioned before, another possibility is to cache the posting lists of terms.",
                "Intuitively, this gives more freedom in the utilization of the cache content to respond to queries because cached terms might form a new query.",
                "On the other hand, they need more space.",
                "As opposed to queries, the fraction of singleton terms in the total volume of terms is smaller.",
                "In our query log, only 4% of the terms appear once, but this accounts for 73% of the vocabulary of query terms.",
                "We show in Section 5 that caching a small fraction of terms, while accounting for terms appearing in many documents, is potentially very effective.",
                "Figure 4 shows several graphs corresponding to the normalized arrival rate for different cases using days as bins.",
                "That is, we plot the normalized number of elements that appear in a day.",
                "This graph shows only a period of 122 days, and we normalize the values by the maximum value observed throughout the whole period of the query log.",
                "Total queries and Total terms correspond to the total volume of queries and terms, respectively.",
                "Unique queries and Unique terms correspond to the arrival rate of unique queries and terms.",
                "Finally, Query diff and Terms diff correspond to the difference between the curves for total and unique.",
                "In Figure 4, as expected, the volume of terms is much higher than the volume of queries.",
                "The difference between the total number of terms and the number of unique terms is much larger than the difference between the total number of queries and the number of unique queries.",
                "This observation implies that terms repeat significantly more than queries.",
                "If we use smaller bins, say of one hour, then the ratio of unique to volume is higher for both terms and queries because it leaves less room for repetition.",
                "We also estimated the workload using the document frequency of terms as a measure of how much work a query imposes on a search engine.",
                "We found that it follows closely the arrival rate for terms shown in Figure 4.",
                "To demonstrate the effect of a dynamic cache on the query frequency distribution of Figure 2, we plot the same frequency graph, but now considering the frequency of queries Figure 5: Frequency graph after LRU cache. after going through an LRU cache.",
                "On a cache miss, an LRU cache decides upon an entry to evict using the information on the recency of queries.",
                "In this graph, the most frequent queries are not the same queries that were most frequent before the cache.",
                "It is possible that queries that are most frequent after the cache have different characteristics, and tuning the search engine to queries frequent before the cache may degrade performance for non-cached queries.",
                "The maximum frequency after caching is less than 1% of the maximum frequency before the cache, thus showing that the cache is very effective in reducing the load of frequent queries.",
                "If we re-rank the queries according to after-cache frequency, the distribution is still a power law, but with a much smaller value for the highest frequency.",
                "When discussing the effectiveness of dynamically caching, an important metric is cache miss rate.",
                "To analyze the cache miss rate for different memory constraints, we use the working set model [6, 14].",
                "A working set, informally, is the set of references that an application or an operating system is currently working with.",
                "The model uses such sets in a strategy that tries to capture the temporal locality of references.",
                "The working set strategy then consists in keeping in memory only the elements that are referenced in the previous θ steps of the input sequence, where θ is a configurable parameter corresponding to the window size.",
                "Originally, working sets have been used for page replacement algorithms of operating systems, and considering such a strategy in the context of search engines is interesting for three reasons.",
                "First, it captures the amount of locality of queries and terms in a sequence of queries.",
                "Locality in this case refers to the frequency of queries and terms in a window of time.",
                "If many queries appear multiple times in a window, then locality is high.",
                "Second, it enables an oﬄine analysis of the expected miss rate given different memory constraints.",
                "Third, working sets capture aspects of efficient caching algorithms such as LRU.",
                "LRU assumes that references farther in the past are less likely to be referenced in the present, which is implicit in the concept of working sets [14].",
                "Figure 6 plots the miss rate for different working set sizes, and we consider working sets of both queries and terms.",
                "The working set sizes are normalized against the total number of queries in the query log.",
                "In the graph for queries, there is a sharp decay until approximately 0.01, and the rate at which the miss rate drops decreases as we increase the size of the working set over 0.01.",
                "Finally, the minimum value it reaches is 50% miss rate, not shown in the figure as we have cut the tail of the curve for presentation purposes. 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1 0 0.05 0.1 0.15 0.2 Missrate Normalized working set size Queries Terms Figure 6: Miss rate as a function of the working set size. 1 10 100 1000 10000 100000 1e+06 Frequency Distance Figure 7: Distribution of distances expressed in terms of distinct queries.",
                "Compared to the query curve, we observe that the minimum miss rate for terms is substantially smaller.",
                "The miss rate also drops sharply on values up to 0.01, and it decreases minimally for higher values.",
                "The minimum value, however, is slightly over 10%, which is much smaller than the minimum value for the sequence of queries.",
                "This implies that with such a policy it is possible to achieve over 80% hit rate, if we consider caching dynamically posting lists for terms as opposed to caching answers for queries.",
                "This result does not consider the space required for each unit stored in the cache memory, or the amount of time it takes to put together a response to a user query.",
                "We analyze these issues more carefully later in this paper.",
                "It is interesting also to observe the histogram of Figure 7, which is an intermediate step in the computation of the miss rate graph.",
                "It reports the distribution of distances between repetitions of the same frequent query.",
                "The distance in the plot is measured in the number of distinct queries separating a query and its repetition, and it considers only queries appearing at least 10 times.",
                "From Figures 6 and 7, we conclude that even if we set the size of the query answers cache to a relatively large number of entries, the miss rate is high.",
                "Thus, caching the posting lists of terms has the potential to improve the hit ratio.",
                "This is what we explore next. 5.",
                "CACHING POSTING LISTS The previous section shows that caching posting lists can obtain a higher hit rate compared to caching query answers.",
                "In this section we study the problem of how to select posting lists to place on a certain amount of available memory, assuming that the whole index is larger than the amount of memory available.",
                "The posting lists have variable size (in fact, their size distribution follows a power law), so it is beneficial for a caching policy to consider the sizes of the posting lists.",
                "We consider both dynamic and static caching.",
                "For dynamic caching, we use two well-known policies, LRU and LFU, as well as a modified algorithm that takes posting-list size into account.",
                "Before discussing the static caching strategies, we introduce some notation.",
                "We use fq(t) to denote the query-term frequency of a term t, that is, the number of queries containing t in the query log, and fd(t) to denote the document frequency of t, that is, the number of documents in the collection in which the term t appears.",
                "The first strategy we consider is the algorithm proposed by Baeza-Yates and Saint-Jean [2], which consists in selecting the posting lists of the terms with the highest query-term frequencies fq(t).",
                "We call this algorithm Qtf.",
                "We observe that there is a trade-off between fq(t) and fd(t).",
                "Terms with high fq(t) are useful to keep in the cache because they are queried often.",
                "On the other hand, terms with high fd(t) are not good candidates because they correspond to long posting lists and consume a substantial amount of space.",
                "In fact, the problem of selecting the best posting lists for the static cache corresponds to the standard Knapsack problem: given a knapsack of fixed capacity, and a set of n items, such as the i-th item has value ci and size si, select the set of items that fit in the knapsack and maximize the overall value.",
                "In our case, value corresponds to fq(t) and size corresponds to fd(t).",
                "Thus, we employ a simple algorithm for the knapsack problem, which is selecting the posting lists of the terms with the highest values of the ratio fq(t) fd(t) .",
                "We call this algorithm QtfDf.",
                "We tried other variations considering query frequencies instead of term frequencies, but the gain was minimal compared to the complexity added.",
                "In addition to the above two static algorithms we consider the following algorithms for dynamic caching: • LRU: A standard LRU algorithm, but many posting lists might need to be evicted (in order of least-recent usage) until there is enough space in the memory to place the currently accessed posting list; • LFU: A standard LFU algorithm (eviction of the leastfrequently used), with the same modification as the LRU; • Dyn-QtfDf: A dynamic version of the QtfDf algorithm; evict from the cache the term(s) with the lowest fq(t) fd(t) ratio.",
                "The performance of all the above algorithms for 15 weeks of the query log and the UK dataset are shown in Figure 8.",
                "Performance is measured with hit rate.",
                "The cache size is measured as a fraction of the total space required to store the posting lists of all terms.",
                "For the dynamic algorithms, we load the cache with terms in order of fq(t) and we let the cache warm up for 1 million queries.",
                "For the static algorithms, we assume complete knowledge of the frequencies fq(t), that is, we estimate fq(t) from the whole query stream.",
                "As we show in Section 7 the results do not change much if we compute the query-term frequencies using the first 3 or 4 weeks of the query log and measure the hit rate on the rest. 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1 0.1 0.2 0.3 0.4 0.5 0.6 0.7 Hitrate Cache size Caching posting lists static QTF/DF LRU LFU Dyn-QTF/DF QTF Figure 8: Hit rate of different strategies for caching posting lists.",
                "The most important observation from our experiments is that the static QtfDf algorithm has a better hit rate than all the dynamic algorithms.",
                "An important benefit a static cache is that it requires no eviction and it is hence more efficient when evaluating queries.",
                "However, if the characteristics of the query traffic change frequently over time, then it requires re-populating the cache often or there will be a significant impact on hit rate. 6.",
                "ANALYSIS OF STATIC CACHING In this section we provide a detailed analysis for the problem of deciding whether it is preferable to cache query answers or cache posting lists.",
                "Our analysis takes into account the impact of caching between two levels of the data-access hierarchy.",
                "It can either be applied at the memory/<br>disk layer</br> or at a server/remote server layer as in the architecture we discussed in the introduction.",
                "Using a particular system model, we obtain estimates for the parameters required by our analysis, which we subsequently use to decide the optimal trade-off between caching query answers and caching posting lists. 6.1 Analytical Model Let M be the size of the cache measured in answer units (the cache can store M query answers).",
                "Assume that all posting lists are of the same length L, measured in answer units.",
                "We consider the following two cases: (A) a cache that stores only precomputed answers, and (B) a cache that stores only posting lists.",
                "In the first case, Nc = M answers fit in the cache, while in the second case Np = M/L posting lists fit in the cache.",
                "Thus, Np = Nc/L.",
                "Note that although posting lists require more space, we can combine terms to evaluate more queries (or partial queries).",
                "For case (A), suppose that a query answer in the cache can be evaluated in 1 time unit.",
                "For case (B), assume that if the posting lists of the terms of a query are in the cache then the results can be computed in TR1 time units, while if the posting lists are not in the cache then the results can be computed in TR2 time units.",
                "Of course TR2 > TR1.",
                "Now we want to compare the time to answer a stream of Q queries in both cases.",
                "Let Vc(Nc) be the volume of the most frequent Nc queries.",
                "Then, for case (A), we have an overall time TCA = Vc(Nc) + TR2(Q − Vc(Nc)).",
                "Similarly, for case (B), let Vp(Np) be the number of computable queries.",
                "Then we have overall time TP L = TR1Vp(Np) + TR2(Q − Vp(Np)).",
                "We want to check under which conditions we have TP L < TCA.",
                "We have TP L − TCA = (TR2 − 1)Vc(Nc) − (TR2 − TR1)Vp(Np) > 0.",
                "Figure 9 shows the values of Vp and Vc for our data.",
                "We can see that caching answers saturates faster and for this particular data there is no additional benefit from using more than 10% of the index space for caching answers.",
                "As the query distribution is a power law with parameter α > 1, the i-th most frequent query appears with probability proportional to 1 iα .",
                "Therefore, the volume Vc(n), which is the total number of the n most frequent queries, is Vc(n) = V0 n i=1 Q iα = γnQ (0 < γn < 1).",
                "We know that Vp(n) grows faster than Vc(n) and assume, based on experimental results, that the relation is of the form Vp(n) = k Vc(n)β .",
                "In the worst case, for a large cache, β → 1.",
                "That is, both techniques will cache a constant fraction of the overall query volume.",
                "Then caching posting lists makes sense only if L(TR2 − 1) k(TR2 − TR1) > 1.",
                "If we use compression, we have L < L and TR1 > TR1.",
                "According to the experiments that we show later, compression is always better.",
                "For a small cache, we are interested in the transient behavior and then β > 1, as computed from our data.",
                "In this case there will always be a point where TP L > TCA for a large number of queries.",
                "In reality, instead of filling the cache only with answers or only with posting lists, a better strategy will be to divide the total cache space into cache for answers and cache for posting lists.",
                "In such a case, there will be some queries that could be answered by both parts of the cache.",
                "As the answer cache is faster, it will be the first choice for answering those queries.",
                "Let QNc and QNp be the set of queries that can be answered by the cached answers and the cached posting lists, respectively.",
                "Then, the overall time is T = Vc(Nc)+TR1V (QNp −QNc )+TR2(Q−V (QNp ∪QNc )), where Np = (M − Nc)/L.",
                "Finding the optimal division of the cache in order to minimize the overall retrieval time is a difficult problem to solve analytically.",
                "In Section 6.3 we use simulations to derive optimal cache trade-offs for particular implementation examples. 6.2 Parameter Estimation We now use a particular implementation of a centralized system and the model of a distributed system as examples from which we estimate the parameters of the analysis from the previous section.",
                "We perform the experiments using an optimized version of Terrier [11] for both indexing documents and processing queries, on a single machine with a Pentium 4 at 2GHz and 1GB of RAM.",
                "We indexed the documents from the UK-2006 dataset, without removing stop words or applying stemming.",
                "The posting lists in the inverted file consist of pairs of document identifier and term frequency.",
                "We compress the document identifier gaps using Elias gamma encoding, and the 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1 0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1 Queryvolume Space precomputed answers posting lists Figure 9: Cache saturation as a function of size.",
                "Table 2: Ratios between the average time to evaluate a query and the average time to return cached answers (centralized and distributed case).",
                "Centralized system TR1 TR2 TR1 TR2 Full evaluation 233 1760 707 1140 Partial evaluation 99 1626 493 798 LAN system TRL 1 TRL 2 TR L 1 TR L 2 Full evaluation 242 1769 716 1149 Partial evaluation 108 1635 502 807 WAN system TRW 1 TRW 2 TR W 1 TR W 2 Full evaluation 5001 6528 5475 5908 Partial evaluation 4867 6394 5270 5575 term frequencies in documents using unary encoding [16].",
                "The size of the inverted file is 1,189Mb.",
                "A stored answer requires 1264 bytes, and an uncompressed posting takes 8 bytes.",
                "From Table 1, we obtain L = (8·# of postings) 1264·# of terms = 0.75 and L = Inverted file size 1264·# of terms = 0.26.",
                "We estimate the ratio TR = T/Tc between the average time T it takes to evaluate a query and the average time Tc it takes to return a stored answer for the same query, in the following way.",
                "Tc is measured by loading the answers for 100,000 queries in memory, and answering the queries from memory.",
                "The average time is Tc = 0.069ms.",
                "T is measured by processing the same 100,000 queries (the first 10,000 queries are used to warm-up the system).",
                "For each query, we remove stop words, if there are at least three remaining terms.",
                "The stop words correspond to the terms with a frequency higher than the number of documents in the index.",
                "We use a document-at-a-time approach to retrieve documents containing all query terms.",
                "The only disk access required during query processing is for reading compressed posting lists from the inverted file.",
                "We perform both full and partial evaluation of answers, because some queries are likely to retrieve a large number of documents, and only a fraction of the retrieved documents will be seen by users.",
                "In the partial evaluation of queries, we terminate the processing after matching 10,000 documents.",
                "The estimated ratios TR are presented in Table 2.",
                "Figure 10 shows for a sample of queries the workload of the system with partial query evaluation and compressed posting lists.",
                "The x-axis corresponds to the total time the system spends processing a particular query, and the vertical axis corresponds to the sum t∈q fq · fd(t).",
                "Notice that the total number of postings of the query-terms does not necessarily provide an accurate estimate of the workload imposed on the system by a query (which is the case for full evaluation and uncompressed lists). 0 0.2 0.4 0.6 0.8 1 0 0.2 0.4 0.6 0.8 1 Totalpostingstoprocessquery(normalized) Total time to process query (normalized) Partial processing of compressed postings query len = 1 query len in [2,3] query len in [4,8] query len > 8 Figure 10: Workload for partial query evaluation with compressed posting lists.",
                "The analysis of the previous section also applies to a distributed retrieval system in one or multiple sites.",
                "Suppose that a document partitioned distributed system is running on a cluster of machines interconnected with a local area network (LAN) in one site.",
                "The broker receives queries and broadcasts them to the query processors, which answer the queries and return the results to the broker.",
                "Finally, the broker merges the received answers and generates the final set of answers (we assume that the time spent on merging results is negligible).",
                "The difference between the centralized architecture and the document partition architecture is the extra communication between the broker and the query processors.",
                "Using ICMP pings on a 100Mbps LAN, we have measured that sending the query from the broker to the query processors which send an answer of 4,000 bytes back to the broker takes on average 0.615ms.",
                "Hence, TRL = TR + 0.615ms/0.069ms = TR + 9.",
                "In the case when the broker and the query processors are in different sites connected with a wide area network (WAN), we estimated that broadcasting the query from the broker to the query processors and getting back an answer of 4,000 bytes takes on average 329ms.",
                "Hence, TRW = TR + 329ms/0.069ms = TR + 4768. 6.3 Simulation Results We now address the problem of finding the optimal tradeoff between caching query answers and caching posting lists.",
                "To make the problem concrete we assume a fixed budget M on the available memory, out of which x units are used for caching query answers and M − x for caching posting lists.",
                "We perform simulations and compute the average response time as a function of x.",
                "Using a part of the query log as training data, we first allocate in the cache the answers to the most frequent queries that fit in space x, and then we use the rest of the memory to cache posting lists.",
                "For selecting posting lists we use the QtfDf algorithm, applied to the training query log but excluding the queries that have already been cached.",
                "In Figure 11, we plot the simulated response time for a centralized system as a function of x.",
                "For the uncompressed index we use M = 1GB, and for the compressed index we use M = 0.5GB.",
                "In the case of the configuration that uses partial query evaluation with compressed posting lists, the lowest response time is achieved when 0.15GB out of the 0.5GB is allocated for storing answers for queries.",
                "We obtained similar trends in the results for the LAN setting.",
                "Figure 12 shows the simulated workload for a distributed system across a WAN.",
                "In this case, the total amount of memory is split between the broker, which holds the cached 400 500 600 700 800 900 1000 1100 1200 0 0.2 0.4 0.6 0.8 1 Averageresponsetime Space (GB) Simulated workload -- single machine full / uncompr / 1 G partial / uncompr / 1 G full / compr / 0.5 G partial / compr / 0.5 G Figure 11: Optimal division of the cache in a server. 3000 3500 4000 4500 5000 5500 6000 0 0.2 0.4 0.6 0.8 1 Averageresponsetime Space (GB) Simulated workload -- WAN full / uncompr / 1 G partial / uncompr / 1 G full / compr / 0.5 G partial / compr / 0.5 G Figure 12: Optimal division of the cache when the next level requires WAN access. answers of queries, and the query processors, which hold the cache of posting lists.",
                "According to the figure, the difference between the configurations of the query processors is less important because the network communication overhead increases the response time substantially.",
                "When using uncompressed posting lists, the optimal allocation of memory corresponds to using approximately 70% of the memory for caching query answers.",
                "This is explained by the fact that there is no need for network communication when the query can be answered by the cache at the broker. 7.",
                "EFFECT OF THE QUERY DYNAMICS For our query log, the query distribution and query-term distribution change slowly over time.",
                "To support this claim, we first assess how topics change comparing the distribution of queries from the first week in June, 2006, to the distribution of queries for the remainder of 2006 that did not appear in the first week in June.",
                "We found that a very small percentage of queries are new queries.",
                "The majority of queries that appear in a given week repeat in the following weeks for the next six months.",
                "We then compute the hit rate of a static cache of 128, 000 answers trained over a period of two weeks (Figure 13).",
                "We report hit rate hourly for 7 days, starting from 5pm.",
                "We observe that the hit rate reaches its highest value during the night (around midnight), whereas around 2-3pm it reaches its minimum.",
                "After a small decay in hit rate values, the hit rate stabilizes between 0.28, and 0.34 for the entire week, suggesting that the static cache is effective for a whole week after the training period. 0.26 0.27 0.28 0.29 0.3 0.31 0.32 0.33 0.34 0.35 0.36 0.37 0 20 40 60 80 100 120 140 160 Hit-rate Time Hits on the frequent queries of distances Figure 13: Hourly hit rate for a static cache holding 128,000 answers during the period of a week.",
                "The static cache of posting lists can be periodically recomputed.",
                "To estimate the time interval in which we need to recompute the posting lists on the static cache we need to consider an efficiency/quality trade-off: using too short a time interval might be prohibitively expensive, while recomputing the cache too infrequently might lead to having an obsolete cache not corresponding to the statistical characteristics of the current query stream.",
                "We measured the effect on the QtfDf algorithm of the changes in a 15-week query stream (Figure 14).",
                "We compute the query term frequencies over the whole stream, select which terms to cache, and then compute the hit rate on the whole query stream.",
                "This hit rate is as an upper bound, and it assumes perfect knowledge of the query term frequencies.",
                "To simulate a realistic scenario, we use the first 6 (3) weeks of the query stream for computing query term frequencies and the following 9 (12) weeks to estimate the hit rate.",
                "As Figure 14 shows, the hit rate decreases by less than 2%.",
                "The high correlation among the query term frequencies during different time periods explains the graceful adaptation of the static caching algorithms to the future query stream.",
                "Indeed, the pairwise correlation among all possible 3-week periods of the 15-week query stream is over 99.5%. 8.",
                "CONCLUSIONS Caching is an effective technique in search engines for improving response time, reducing the load on query processors, and improving network bandwidth utilization.",
                "We present results on both dynamic and static caching.",
                "Dynamic caching of queries has limited effectiveness due to the high number of compulsory misses caused by the number of unique or infrequent queries.",
                "Our results show that in our UK log, the minimum miss rate is 50% using a working set strategy.",
                "Caching terms is more effective with respect to miss rate, achieving values as low as 12%.",
                "We also propose a new algorithm for static caching of posting lists that outperforms previous static caching algorithms as well as dynamic algorithms such as LRU and LFU, obtaining hit rate values that are over 10% higher compared these strategies.",
                "We present a framework for the analysis of the trade-off between caching query results and caching posting lists, and we simulate different types of architectures.",
                "Our results show that for centralized and LAN environments, there is an optimal allocation of caching query results and caching of posting lists, while for WAN scenarios in which network time prevails it is more important to cache query results. 0.45 0.5 0.55 0.6 0.65 0.7 0.75 0.8 0.85 0.9 0.95 0.1 0.2 0.3 0.4 0.5 0.6 0.7 Hitrate Cache size Dynamics of static QTF/DF caching policy perfect knowledge 6-week training 3-week training Figure 14: Impact of distribution changes on the static caching of posting lists. 9.",
                "REFERENCES [1] V. N. Anh and A. Moffat.",
                "Pruned query evaluation using pre-computed impacts.",
                "In ACM CIKM, 2006. [2] R. A. Baeza-Yates and F. Saint-Jean.",
                "A three level search engine index based in query log distribution.",
                "In SPIRE, 2003. [3] C. Buckley and A. F. Lewit.",
                "Optimization of inverted vector searches.",
                "In ACM SIGIR, 1985. [4] S. B¨uttcher and C. L. A. Clarke.",
                "A document-centric approach to static index pruning in text retrieval systems.",
                "In ACM CIKM, 2006. [5] P. Cao and S. Irani.",
                "Cost-aware WWW proxy caching algorithms.",
                "In USITS, 1997. [6] P. Denning.",
                "Working sets past and present.",
                "IEEE Trans. on Software Engineering, SE-6(1):64-84, 1980. [7] T. Fagni, R. Perego, F. Silvestri, and S. Orlando.",
                "Boosting the performance of web search engines: Caching and prefetching query results by exploiting historical usage data.",
                "ACM Trans.",
                "Inf.",
                "Syst., 24(1):51-78, 2006. [8] R. Lempel and S. Moran.",
                "Predictive caching and prefetching of query results in search engines.",
                "In WWW, 2003. [9] X.",
                "Long and T. Suel.",
                "Three-level caching for efficient query processing in large web search engines.",
                "In WWW, 2005. [10] E. P. Markatos.",
                "On caching search engine query results.",
                "Computer Communications, 24(2):137-143, 2001. [11] I. Ounis, G. Amati, V. Plachouras, B.",
                "He, C. Macdonald, and C. Lioma.",
                "Terrier: A High Performance and Scalable Information Retrieval Platform.",
                "In SIGIR Workshop on Open Source Information Retrieval, 2006. [12] V. V. Raghavan and H. Sever.",
                "On the reuse of past optimal queries.",
                "In ACM SIGIR, 1995. [13] P. C. Saraiva, E. S. de Moura, N. Ziviani, W. Meira, R. Fonseca, and B. Riberio-Neto.",
                "Rank-preserving two-level caching for scalable search engines.",
                "In ACM SIGIR, 2001. [14] D. R. Slutz and I. L. Traiger.",
                "A note on the calculation of average working set size.",
                "Communications of the ACM, 17(10):563-565, 1974. [15] T. Strohman, H. Turtle, and W. B. Croft.",
                "Optimization strategies for complex queries.",
                "In ACM SIGIR, 2005. [16] I. H. Witten, T. C. Bell, and A. Moffat.",
                "Managing Gigabytes: Compressing and Indexing Documents and Images.",
                "John Wiley & Sons, Inc., NY, 1994. [17] N. E. Young.",
                "On-line file caching.",
                "Algorithmica, 33(3):371-383, 2002."
            ],
            "original_annotated_samples": [],
            "translated_annotated_samples": [
                "Nuestros resultados y observaciones son aplicables a diferentes niveles de la jerarquía de acceso de datos, por ejemplo, para una memoria/\"capa de disco\" o una capa de corredor/servidor remoto.",
                "Se puede aplicar en la memoria/\"capa de disco\" o en una capa de servidor/servidor remoto como en la arquitectura que discutimos en la introducción."
            ],
            "translated_text": "",
            "candidates": [
                "capa de disco",
                "capa de disco",
                "capa de disco",
                "capa de disco"
            ],
            "error": []
        },
        "remote server layer": {
            "translated_key": "capa de servidor remoto",
            "is_in_text": true,
            "original_annotated_sentences": [
                "The Impact of Caching on Search Engines Ricardo Baeza-Yates1 rbaeza@acm.org Aristides Gionis1 gionis@yahoo-inc.com Flavio Junqueira1 fpj@yahoo-inc.com Vanessa Murdock1 vmurdock@yahoo-inc.com Vassilis Plachouras1 vassilis@yahoo-inc.com Fabrizio Silvestri2 f.silvestri@isti.cnr.it 1 Yahoo!",
                "Research Barcelona 2 ISTI - CNR Barcelona, SPAIN Pisa, ITALY ABSTRACT In this paper we study the trade-offs in designing efficient caching systems for Web search engines.",
                "We explore the impact of different approaches, such as static vs. dynamic caching, and caching query results vs. caching posting lists.",
                "Using a query log spanning a whole year we explore the limitations of caching and we demonstrate that caching posting lists can achieve higher hit rates than caching query answers.",
                "We propose a new algorithm for static caching of posting lists, which outperforms previous methods.",
                "We also study the problem of finding the optimal way to split the static cache between answers and posting lists.",
                "Finally, we measure how the changes in the query log affect the effectiveness of static caching, given our observation that the distribution of the queries changes slowly over time.",
                "Our results and observations are applicable to different levels of the data-access hierarchy, for instance, for a memory/disk layer or a broker/<br>remote server layer</br>.",
                "Categories and Subject Descriptors H.3.3 [Information Storage and Retrieval]: Information Search and Retrieval - Search process; H.3.4 [Information Storage and Retrieval]: Systems and Software - Distributed systems, Performance evaluation (efficiency and effectiveness) General Terms Algorithms, Experimentation 1.",
                "INTRODUCTION Millions of queries are submitted daily to Web search engines, and users have high expectations of the quality and speed of the answers.",
                "As the searchable Web becomes larger and larger, with more than 20 billion pages to index, evaluating a single query requires processing large amounts of data.",
                "In such a setting, to achieve a fast response time and to increase the query throughput, using a cache is crucial.",
                "The primary use of a cache memory is to speedup computation by exploiting frequently or recently used data, although reducing the workload to back-end servers is also a major goal.",
                "Caching can be applied at different levels with increasing response latencies or processing requirements.",
                "For example, the different levels may correspond to the main memory, the disk, or resources in a local or a wide area network.",
                "The decision of what to cache is either off-line (static) or online (dynamic).",
                "A static cache is based on historical information and is periodically updated.",
                "A dynamic cache replaces entries according to the sequence of requests.",
                "When a new request arrives, the cache system decides whether to evict some entry from the cache in the case of a cache miss.",
                "Such online decisions are based on a cache policy, and several different policies have been studied in the past.",
                "For a search engine, there are two possible ways to use a cache memory: Caching answers: As the engine returns answers to a particular query, it may decide to store these answers to resolve future queries.",
                "Caching terms: As the engine evaluates a particular query, it may decide to store in memory the posting lists of the involved query terms.",
                "Often the whole set of posting lists does not fit in memory, and consequently, the engine has to select a small set to keep in memory and speed up query processing.",
                "Returning an answer to a query that already exists in the cache is more efficient than computing the answer using cached posting lists.",
                "On the other hand, previously unseen queries occur more often than previously unseen terms, implying a higher miss rate for cached answers.",
                "Caching of posting lists has additional challenges.",
                "As posting lists have variable size, caching them dynamically is not very efficient, due to the complexity in terms of efficiency and space, and the skewed distribution of the query stream, as shown later.",
                "Static caching of posting lists poses even more challenges: when deciding which terms to cache one faces the trade-off between frequently queried terms and terms with small posting lists that are space efficient.",
                "Finally, before deciding to adopt a static caching policy the query stream should be analyzed to verify that its characteristics do not change rapidly over time.",
                "Broker Static caching posting lists Dynamic/Static cached answers Local query processor Disk Next caching level Local network access Remote network access Figure 1: One caching level in a distributed search architecture.",
                "In this paper we explore the trade-offs in the design of each cache level, showing that the problem is the same and only a few parameters change.",
                "In general, we assume that each level of caching in a distributed search architecture is similar to that shown in Figure 1.",
                "We use a query log spanning a whole year to explore the limitations of dynamically caching query answers or posting lists for query terms.",
                "More concretely, our main conclusions are that: • Caching query answers results in lower hit ratios compared to caching of posting lists for query terms, but it is faster because there is no need for query evaluation.",
                "We provide a framework for the analysis of the trade-off between static caching of query answers and posting lists; • Static caching of terms can be more effective than dynamic caching with, for example, LRU.",
                "We provide algorithms based on the Knapsack problem for selecting the posting lists to put in a static cache, and we show improvements over previous work, achieving a hit ratio over 90%; • Changes of the query distribution over time have little impact on static caching.",
                "The remainder of this paper is organized as follows.",
                "Sections 2 and 3 summarize related work and characterize the data sets we use.",
                "Section 4 discusses the limitations of dynamic caching.",
                "Sections 5 and 6 introduce algorithms for caching posting lists, and a theoretical framework for the analysis of static caching, respectively.",
                "Section 7 discusses the impact of changes in the query distribution on static caching, and Section 8 provides concluding remarks. 2.",
                "RELATED WORK There is a large body of work devoted to query optimization.",
                "Buckley and Lewit [3], in one of the earliest works, take a term-at-a-time approach to deciding when inverted lists need not be further examined.",
                "More recent examples demonstrate that the top k documents for a query can be returned without the need for evaluating the complete set of posting lists [1, 4, 15].",
                "Although these approaches seek to improve query processing efficiency, they differ from our current work in that they do not consider caching.",
                "They may be considered separate and complementary to a cache-based approach.",
                "Raghavan and Sever [12], in one of the first papers on exploiting user query history, propose using a query base, built upon a set of persistent optimal queries submitted in the past, to improve the retrieval effectiveness for similar future queries.",
                "Markatos [10] shows the existence of temporal locality in queries, and compares the performance of different caching policies.",
                "Based on the observations of Markatos, Lempel and Moran propose a new caching policy, called Probabilistic Driven Caching, by attempting to estimate the probability distribution of all possible queries submitted to a search engine [8].",
                "Fagni et al. follow Markatos work by showing that combining static and dynamic caching policies together with an adaptive prefetching policy achieves a high hit ratio [7].",
                "Different from our work, they consider caching and prefetching of pages of results.",
                "As systems are often hierarchical, there has also been some effort on multi-level architectures.",
                "Saraiva et al. propose a new architecture for Web search engines using a two-level dynamic caching system [13].",
                "Their goal for such systems has been to improve response time for hierarchical engines.",
                "In their architecture, both levels use an LRU eviction policy.",
                "They find that the second-level cache can effectively reduce disk traffic, thus increasing the overall throughput.",
                "Baeza-Yates and Saint-Jean propose a three-level index organization [2].",
                "Long and Suel propose a caching system structured according to three different levels [9].",
                "The intermediate level contains frequently occurring pairs of terms and stores the intersections of the corresponding inverted lists.",
                "These last two papers are related to ours in that they exploit different caching strategies at different levels of the memory hierarchy.",
                "Finally, our static caching algorithm for posting lists in Section 5 uses the ratio frequency/size in order to evaluate the goodness of an item to cache.",
                "Similar ideas have been used in the context of file caching [17], Web caching [5], and even caching of posting lists [9], but in all cases in a dynamic setting.",
                "To the best of our knowledge we are the first to use this approach for static caching of posting lists. 3.",
                "DATA CHARACTERIZATION Our data consists of a crawl of documents from the UK domain, and query logs of one year of queries submitted to http://www.yahoo.co.uk from November 2005 to November 2006.",
                "In our logs, 50% of the total volume of queries are unique.",
                "The average query length is 2.5 terms, with the longest query having 731 terms. 1e-07 1e-06 1e-05 1e-04 0.001 0.01 0.1 1 1e-08 1e-07 1e-06 1e-05 1e-04 0.001 0.01 0.1 1 Frequency(normalized) Frequency rank (normalized) Figure 2: The distribution of queries (bottom curve) and query terms (middle curve) in the query log.",
                "The distribution of document frequencies of terms in the UK-2006 dataset (upper curve).",
                "Figure 2 shows the distributions of queries (lower curve), and query terms (middle curve).",
                "The x-axis represents the normalized frequency rank of the query or term. (The most frequent query appears closest to the y-axis.)",
                "The y-axis is Table 1: Statistics of the UK-2006 sample.",
                "UK-2006 sample statistics # of documents 2,786,391 # of terms 6,491,374 # of tokens 2,109,512,558 the normalized frequency for a given query (or term).",
                "As expected, the distribution of query frequencies and query term frequencies follow power law distributions, with slope of 1.84 and 2.26, respectively.",
                "In this figure, the query frequencies were computed as they appear in the logs with no normalization for case or white space.",
                "The query terms (middle curve) have been normalized for case, as have the terms in the document collection.",
                "The document collection that we use for our experiments is a summary of the UK domain crawled in May 2006.1 This summary corresponds to a maximum of 400 crawled documents per host, using a breadth first crawling strategy, comprising 15GB.",
                "The distribution of document frequencies of terms in the collection follows a power law distribution with slope 2.38 (upper curve in Figure 2).",
                "The statistics of the collection are shown in Table 1.",
                "We measured the correlation between the document frequency of terms in the collection and the number of queries that contain a particular term in the query log to be 0.424.",
                "A scatter plot for a random sample of terms is shown in Figure 3.",
                "In this experiment, terms have been converted to lower case in both the queries and the documents so that the frequencies will be comparable. 1e-07 1e-06 1e-05 1e-04 0.001 0.01 0.1 1 1e-06 1e-05 1e-04 0.001 0.01 0.1 1 Queryfrequency Document frequency Figure 3: Normalized scatter plot of document-term frequencies vs. query-term frequencies. 4.",
                "CACHING OF QUERIES AND TERMS Caching relies upon the assumption that there is locality in the stream of requests.",
                "That is, there must be sufficient repetition in the stream of requests and within intervals of time that enable a cache memory of reasonable size to be effective.",
                "In the query log we used, 88% of the unique queries are singleton queries, and 44% are singleton queries out of the whole volume.",
                "Thus, out of all queries in the stream composing the query log, the upper threshold on hit ratio is 56%.",
                "This is because only 56% of all the queries comprise queries that have multiple occurrences.",
                "It is important to observe, however, that not all queries in this 56% can be cache hits because of compulsory misses.",
                "A compulsory miss 1 The collection is available from the University of Milan: http://law.dsi.unimi.it/.",
                "URL retrieved 05/2007. 0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1 240 260 280 300 320 340 360 Numberofelements Bin number Total terms Terms diff Total queries Unique queries Unique terms Query diff Figure 4: Arrival rate for both terms and queries. happens when the cache receives a query for the first time.",
                "This is different from capacity misses, which happen due to space constraints on the amount of memory the cache uses.",
                "If we consider a cache with infinite memory, then the hit ratio is 50%.",
                "Note that for an infinite cache there are no capacity misses.",
                "As we mentioned before, another possibility is to cache the posting lists of terms.",
                "Intuitively, this gives more freedom in the utilization of the cache content to respond to queries because cached terms might form a new query.",
                "On the other hand, they need more space.",
                "As opposed to queries, the fraction of singleton terms in the total volume of terms is smaller.",
                "In our query log, only 4% of the terms appear once, but this accounts for 73% of the vocabulary of query terms.",
                "We show in Section 5 that caching a small fraction of terms, while accounting for terms appearing in many documents, is potentially very effective.",
                "Figure 4 shows several graphs corresponding to the normalized arrival rate for different cases using days as bins.",
                "That is, we plot the normalized number of elements that appear in a day.",
                "This graph shows only a period of 122 days, and we normalize the values by the maximum value observed throughout the whole period of the query log.",
                "Total queries and Total terms correspond to the total volume of queries and terms, respectively.",
                "Unique queries and Unique terms correspond to the arrival rate of unique queries and terms.",
                "Finally, Query diff and Terms diff correspond to the difference between the curves for total and unique.",
                "In Figure 4, as expected, the volume of terms is much higher than the volume of queries.",
                "The difference between the total number of terms and the number of unique terms is much larger than the difference between the total number of queries and the number of unique queries.",
                "This observation implies that terms repeat significantly more than queries.",
                "If we use smaller bins, say of one hour, then the ratio of unique to volume is higher for both terms and queries because it leaves less room for repetition.",
                "We also estimated the workload using the document frequency of terms as a measure of how much work a query imposes on a search engine.",
                "We found that it follows closely the arrival rate for terms shown in Figure 4.",
                "To demonstrate the effect of a dynamic cache on the query frequency distribution of Figure 2, we plot the same frequency graph, but now considering the frequency of queries Figure 5: Frequency graph after LRU cache. after going through an LRU cache.",
                "On a cache miss, an LRU cache decides upon an entry to evict using the information on the recency of queries.",
                "In this graph, the most frequent queries are not the same queries that were most frequent before the cache.",
                "It is possible that queries that are most frequent after the cache have different characteristics, and tuning the search engine to queries frequent before the cache may degrade performance for non-cached queries.",
                "The maximum frequency after caching is less than 1% of the maximum frequency before the cache, thus showing that the cache is very effective in reducing the load of frequent queries.",
                "If we re-rank the queries according to after-cache frequency, the distribution is still a power law, but with a much smaller value for the highest frequency.",
                "When discussing the effectiveness of dynamically caching, an important metric is cache miss rate.",
                "To analyze the cache miss rate for different memory constraints, we use the working set model [6, 14].",
                "A working set, informally, is the set of references that an application or an operating system is currently working with.",
                "The model uses such sets in a strategy that tries to capture the temporal locality of references.",
                "The working set strategy then consists in keeping in memory only the elements that are referenced in the previous θ steps of the input sequence, where θ is a configurable parameter corresponding to the window size.",
                "Originally, working sets have been used for page replacement algorithms of operating systems, and considering such a strategy in the context of search engines is interesting for three reasons.",
                "First, it captures the amount of locality of queries and terms in a sequence of queries.",
                "Locality in this case refers to the frequency of queries and terms in a window of time.",
                "If many queries appear multiple times in a window, then locality is high.",
                "Second, it enables an oﬄine analysis of the expected miss rate given different memory constraints.",
                "Third, working sets capture aspects of efficient caching algorithms such as LRU.",
                "LRU assumes that references farther in the past are less likely to be referenced in the present, which is implicit in the concept of working sets [14].",
                "Figure 6 plots the miss rate for different working set sizes, and we consider working sets of both queries and terms.",
                "The working set sizes are normalized against the total number of queries in the query log.",
                "In the graph for queries, there is a sharp decay until approximately 0.01, and the rate at which the miss rate drops decreases as we increase the size of the working set over 0.01.",
                "Finally, the minimum value it reaches is 50% miss rate, not shown in the figure as we have cut the tail of the curve for presentation purposes. 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1 0 0.05 0.1 0.15 0.2 Missrate Normalized working set size Queries Terms Figure 6: Miss rate as a function of the working set size. 1 10 100 1000 10000 100000 1e+06 Frequency Distance Figure 7: Distribution of distances expressed in terms of distinct queries.",
                "Compared to the query curve, we observe that the minimum miss rate for terms is substantially smaller.",
                "The miss rate also drops sharply on values up to 0.01, and it decreases minimally for higher values.",
                "The minimum value, however, is slightly over 10%, which is much smaller than the minimum value for the sequence of queries.",
                "This implies that with such a policy it is possible to achieve over 80% hit rate, if we consider caching dynamically posting lists for terms as opposed to caching answers for queries.",
                "This result does not consider the space required for each unit stored in the cache memory, or the amount of time it takes to put together a response to a user query.",
                "We analyze these issues more carefully later in this paper.",
                "It is interesting also to observe the histogram of Figure 7, which is an intermediate step in the computation of the miss rate graph.",
                "It reports the distribution of distances between repetitions of the same frequent query.",
                "The distance in the plot is measured in the number of distinct queries separating a query and its repetition, and it considers only queries appearing at least 10 times.",
                "From Figures 6 and 7, we conclude that even if we set the size of the query answers cache to a relatively large number of entries, the miss rate is high.",
                "Thus, caching the posting lists of terms has the potential to improve the hit ratio.",
                "This is what we explore next. 5.",
                "CACHING POSTING LISTS The previous section shows that caching posting lists can obtain a higher hit rate compared to caching query answers.",
                "In this section we study the problem of how to select posting lists to place on a certain amount of available memory, assuming that the whole index is larger than the amount of memory available.",
                "The posting lists have variable size (in fact, their size distribution follows a power law), so it is beneficial for a caching policy to consider the sizes of the posting lists.",
                "We consider both dynamic and static caching.",
                "For dynamic caching, we use two well-known policies, LRU and LFU, as well as a modified algorithm that takes posting-list size into account.",
                "Before discussing the static caching strategies, we introduce some notation.",
                "We use fq(t) to denote the query-term frequency of a term t, that is, the number of queries containing t in the query log, and fd(t) to denote the document frequency of t, that is, the number of documents in the collection in which the term t appears.",
                "The first strategy we consider is the algorithm proposed by Baeza-Yates and Saint-Jean [2], which consists in selecting the posting lists of the terms with the highest query-term frequencies fq(t).",
                "We call this algorithm Qtf.",
                "We observe that there is a trade-off between fq(t) and fd(t).",
                "Terms with high fq(t) are useful to keep in the cache because they are queried often.",
                "On the other hand, terms with high fd(t) are not good candidates because they correspond to long posting lists and consume a substantial amount of space.",
                "In fact, the problem of selecting the best posting lists for the static cache corresponds to the standard Knapsack problem: given a knapsack of fixed capacity, and a set of n items, such as the i-th item has value ci and size si, select the set of items that fit in the knapsack and maximize the overall value.",
                "In our case, value corresponds to fq(t) and size corresponds to fd(t).",
                "Thus, we employ a simple algorithm for the knapsack problem, which is selecting the posting lists of the terms with the highest values of the ratio fq(t) fd(t) .",
                "We call this algorithm QtfDf.",
                "We tried other variations considering query frequencies instead of term frequencies, but the gain was minimal compared to the complexity added.",
                "In addition to the above two static algorithms we consider the following algorithms for dynamic caching: • LRU: A standard LRU algorithm, but many posting lists might need to be evicted (in order of least-recent usage) until there is enough space in the memory to place the currently accessed posting list; • LFU: A standard LFU algorithm (eviction of the leastfrequently used), with the same modification as the LRU; • Dyn-QtfDf: A dynamic version of the QtfDf algorithm; evict from the cache the term(s) with the lowest fq(t) fd(t) ratio.",
                "The performance of all the above algorithms for 15 weeks of the query log and the UK dataset are shown in Figure 8.",
                "Performance is measured with hit rate.",
                "The cache size is measured as a fraction of the total space required to store the posting lists of all terms.",
                "For the dynamic algorithms, we load the cache with terms in order of fq(t) and we let the cache warm up for 1 million queries.",
                "For the static algorithms, we assume complete knowledge of the frequencies fq(t), that is, we estimate fq(t) from the whole query stream.",
                "As we show in Section 7 the results do not change much if we compute the query-term frequencies using the first 3 or 4 weeks of the query log and measure the hit rate on the rest. 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1 0.1 0.2 0.3 0.4 0.5 0.6 0.7 Hitrate Cache size Caching posting lists static QTF/DF LRU LFU Dyn-QTF/DF QTF Figure 8: Hit rate of different strategies for caching posting lists.",
                "The most important observation from our experiments is that the static QtfDf algorithm has a better hit rate than all the dynamic algorithms.",
                "An important benefit a static cache is that it requires no eviction and it is hence more efficient when evaluating queries.",
                "However, if the characteristics of the query traffic change frequently over time, then it requires re-populating the cache often or there will be a significant impact on hit rate. 6.",
                "ANALYSIS OF STATIC CACHING In this section we provide a detailed analysis for the problem of deciding whether it is preferable to cache query answers or cache posting lists.",
                "Our analysis takes into account the impact of caching between two levels of the data-access hierarchy.",
                "It can either be applied at the memory/disk layer or at a server/<br>remote server layer</br> as in the architecture we discussed in the introduction.",
                "Using a particular system model, we obtain estimates for the parameters required by our analysis, which we subsequently use to decide the optimal trade-off between caching query answers and caching posting lists. 6.1 Analytical Model Let M be the size of the cache measured in answer units (the cache can store M query answers).",
                "Assume that all posting lists are of the same length L, measured in answer units.",
                "We consider the following two cases: (A) a cache that stores only precomputed answers, and (B) a cache that stores only posting lists.",
                "In the first case, Nc = M answers fit in the cache, while in the second case Np = M/L posting lists fit in the cache.",
                "Thus, Np = Nc/L.",
                "Note that although posting lists require more space, we can combine terms to evaluate more queries (or partial queries).",
                "For case (A), suppose that a query answer in the cache can be evaluated in 1 time unit.",
                "For case (B), assume that if the posting lists of the terms of a query are in the cache then the results can be computed in TR1 time units, while if the posting lists are not in the cache then the results can be computed in TR2 time units.",
                "Of course TR2 > TR1.",
                "Now we want to compare the time to answer a stream of Q queries in both cases.",
                "Let Vc(Nc) be the volume of the most frequent Nc queries.",
                "Then, for case (A), we have an overall time TCA = Vc(Nc) + TR2(Q − Vc(Nc)).",
                "Similarly, for case (B), let Vp(Np) be the number of computable queries.",
                "Then we have overall time TP L = TR1Vp(Np) + TR2(Q − Vp(Np)).",
                "We want to check under which conditions we have TP L < TCA.",
                "We have TP L − TCA = (TR2 − 1)Vc(Nc) − (TR2 − TR1)Vp(Np) > 0.",
                "Figure 9 shows the values of Vp and Vc for our data.",
                "We can see that caching answers saturates faster and for this particular data there is no additional benefit from using more than 10% of the index space for caching answers.",
                "As the query distribution is a power law with parameter α > 1, the i-th most frequent query appears with probability proportional to 1 iα .",
                "Therefore, the volume Vc(n), which is the total number of the n most frequent queries, is Vc(n) = V0 n i=1 Q iα = γnQ (0 < γn < 1).",
                "We know that Vp(n) grows faster than Vc(n) and assume, based on experimental results, that the relation is of the form Vp(n) = k Vc(n)β .",
                "In the worst case, for a large cache, β → 1.",
                "That is, both techniques will cache a constant fraction of the overall query volume.",
                "Then caching posting lists makes sense only if L(TR2 − 1) k(TR2 − TR1) > 1.",
                "If we use compression, we have L < L and TR1 > TR1.",
                "According to the experiments that we show later, compression is always better.",
                "For a small cache, we are interested in the transient behavior and then β > 1, as computed from our data.",
                "In this case there will always be a point where TP L > TCA for a large number of queries.",
                "In reality, instead of filling the cache only with answers or only with posting lists, a better strategy will be to divide the total cache space into cache for answers and cache for posting lists.",
                "In such a case, there will be some queries that could be answered by both parts of the cache.",
                "As the answer cache is faster, it will be the first choice for answering those queries.",
                "Let QNc and QNp be the set of queries that can be answered by the cached answers and the cached posting lists, respectively.",
                "Then, the overall time is T = Vc(Nc)+TR1V (QNp −QNc )+TR2(Q−V (QNp ∪QNc )), where Np = (M − Nc)/L.",
                "Finding the optimal division of the cache in order to minimize the overall retrieval time is a difficult problem to solve analytically.",
                "In Section 6.3 we use simulations to derive optimal cache trade-offs for particular implementation examples. 6.2 Parameter Estimation We now use a particular implementation of a centralized system and the model of a distributed system as examples from which we estimate the parameters of the analysis from the previous section.",
                "We perform the experiments using an optimized version of Terrier [11] for both indexing documents and processing queries, on a single machine with a Pentium 4 at 2GHz and 1GB of RAM.",
                "We indexed the documents from the UK-2006 dataset, without removing stop words or applying stemming.",
                "The posting lists in the inverted file consist of pairs of document identifier and term frequency.",
                "We compress the document identifier gaps using Elias gamma encoding, and the 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1 0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1 Queryvolume Space precomputed answers posting lists Figure 9: Cache saturation as a function of size.",
                "Table 2: Ratios between the average time to evaluate a query and the average time to return cached answers (centralized and distributed case).",
                "Centralized system TR1 TR2 TR1 TR2 Full evaluation 233 1760 707 1140 Partial evaluation 99 1626 493 798 LAN system TRL 1 TRL 2 TR L 1 TR L 2 Full evaluation 242 1769 716 1149 Partial evaluation 108 1635 502 807 WAN system TRW 1 TRW 2 TR W 1 TR W 2 Full evaluation 5001 6528 5475 5908 Partial evaluation 4867 6394 5270 5575 term frequencies in documents using unary encoding [16].",
                "The size of the inverted file is 1,189Mb.",
                "A stored answer requires 1264 bytes, and an uncompressed posting takes 8 bytes.",
                "From Table 1, we obtain L = (8·# of postings) 1264·# of terms = 0.75 and L = Inverted file size 1264·# of terms = 0.26.",
                "We estimate the ratio TR = T/Tc between the average time T it takes to evaluate a query and the average time Tc it takes to return a stored answer for the same query, in the following way.",
                "Tc is measured by loading the answers for 100,000 queries in memory, and answering the queries from memory.",
                "The average time is Tc = 0.069ms.",
                "T is measured by processing the same 100,000 queries (the first 10,000 queries are used to warm-up the system).",
                "For each query, we remove stop words, if there are at least three remaining terms.",
                "The stop words correspond to the terms with a frequency higher than the number of documents in the index.",
                "We use a document-at-a-time approach to retrieve documents containing all query terms.",
                "The only disk access required during query processing is for reading compressed posting lists from the inverted file.",
                "We perform both full and partial evaluation of answers, because some queries are likely to retrieve a large number of documents, and only a fraction of the retrieved documents will be seen by users.",
                "In the partial evaluation of queries, we terminate the processing after matching 10,000 documents.",
                "The estimated ratios TR are presented in Table 2.",
                "Figure 10 shows for a sample of queries the workload of the system with partial query evaluation and compressed posting lists.",
                "The x-axis corresponds to the total time the system spends processing a particular query, and the vertical axis corresponds to the sum t∈q fq · fd(t).",
                "Notice that the total number of postings of the query-terms does not necessarily provide an accurate estimate of the workload imposed on the system by a query (which is the case for full evaluation and uncompressed lists). 0 0.2 0.4 0.6 0.8 1 0 0.2 0.4 0.6 0.8 1 Totalpostingstoprocessquery(normalized) Total time to process query (normalized) Partial processing of compressed postings query len = 1 query len in [2,3] query len in [4,8] query len > 8 Figure 10: Workload for partial query evaluation with compressed posting lists.",
                "The analysis of the previous section also applies to a distributed retrieval system in one or multiple sites.",
                "Suppose that a document partitioned distributed system is running on a cluster of machines interconnected with a local area network (LAN) in one site.",
                "The broker receives queries and broadcasts them to the query processors, which answer the queries and return the results to the broker.",
                "Finally, the broker merges the received answers and generates the final set of answers (we assume that the time spent on merging results is negligible).",
                "The difference between the centralized architecture and the document partition architecture is the extra communication between the broker and the query processors.",
                "Using ICMP pings on a 100Mbps LAN, we have measured that sending the query from the broker to the query processors which send an answer of 4,000 bytes back to the broker takes on average 0.615ms.",
                "Hence, TRL = TR + 0.615ms/0.069ms = TR + 9.",
                "In the case when the broker and the query processors are in different sites connected with a wide area network (WAN), we estimated that broadcasting the query from the broker to the query processors and getting back an answer of 4,000 bytes takes on average 329ms.",
                "Hence, TRW = TR + 329ms/0.069ms = TR + 4768. 6.3 Simulation Results We now address the problem of finding the optimal tradeoff between caching query answers and caching posting lists.",
                "To make the problem concrete we assume a fixed budget M on the available memory, out of which x units are used for caching query answers and M − x for caching posting lists.",
                "We perform simulations and compute the average response time as a function of x.",
                "Using a part of the query log as training data, we first allocate in the cache the answers to the most frequent queries that fit in space x, and then we use the rest of the memory to cache posting lists.",
                "For selecting posting lists we use the QtfDf algorithm, applied to the training query log but excluding the queries that have already been cached.",
                "In Figure 11, we plot the simulated response time for a centralized system as a function of x.",
                "For the uncompressed index we use M = 1GB, and for the compressed index we use M = 0.5GB.",
                "In the case of the configuration that uses partial query evaluation with compressed posting lists, the lowest response time is achieved when 0.15GB out of the 0.5GB is allocated for storing answers for queries.",
                "We obtained similar trends in the results for the LAN setting.",
                "Figure 12 shows the simulated workload for a distributed system across a WAN.",
                "In this case, the total amount of memory is split between the broker, which holds the cached 400 500 600 700 800 900 1000 1100 1200 0 0.2 0.4 0.6 0.8 1 Averageresponsetime Space (GB) Simulated workload -- single machine full / uncompr / 1 G partial / uncompr / 1 G full / compr / 0.5 G partial / compr / 0.5 G Figure 11: Optimal division of the cache in a server. 3000 3500 4000 4500 5000 5500 6000 0 0.2 0.4 0.6 0.8 1 Averageresponsetime Space (GB) Simulated workload -- WAN full / uncompr / 1 G partial / uncompr / 1 G full / compr / 0.5 G partial / compr / 0.5 G Figure 12: Optimal division of the cache when the next level requires WAN access. answers of queries, and the query processors, which hold the cache of posting lists.",
                "According to the figure, the difference between the configurations of the query processors is less important because the network communication overhead increases the response time substantially.",
                "When using uncompressed posting lists, the optimal allocation of memory corresponds to using approximately 70% of the memory for caching query answers.",
                "This is explained by the fact that there is no need for network communication when the query can be answered by the cache at the broker. 7.",
                "EFFECT OF THE QUERY DYNAMICS For our query log, the query distribution and query-term distribution change slowly over time.",
                "To support this claim, we first assess how topics change comparing the distribution of queries from the first week in June, 2006, to the distribution of queries for the remainder of 2006 that did not appear in the first week in June.",
                "We found that a very small percentage of queries are new queries.",
                "The majority of queries that appear in a given week repeat in the following weeks for the next six months.",
                "We then compute the hit rate of a static cache of 128, 000 answers trained over a period of two weeks (Figure 13).",
                "We report hit rate hourly for 7 days, starting from 5pm.",
                "We observe that the hit rate reaches its highest value during the night (around midnight), whereas around 2-3pm it reaches its minimum.",
                "After a small decay in hit rate values, the hit rate stabilizes between 0.28, and 0.34 for the entire week, suggesting that the static cache is effective for a whole week after the training period. 0.26 0.27 0.28 0.29 0.3 0.31 0.32 0.33 0.34 0.35 0.36 0.37 0 20 40 60 80 100 120 140 160 Hit-rate Time Hits on the frequent queries of distances Figure 13: Hourly hit rate for a static cache holding 128,000 answers during the period of a week.",
                "The static cache of posting lists can be periodically recomputed.",
                "To estimate the time interval in which we need to recompute the posting lists on the static cache we need to consider an efficiency/quality trade-off: using too short a time interval might be prohibitively expensive, while recomputing the cache too infrequently might lead to having an obsolete cache not corresponding to the statistical characteristics of the current query stream.",
                "We measured the effect on the QtfDf algorithm of the changes in a 15-week query stream (Figure 14).",
                "We compute the query term frequencies over the whole stream, select which terms to cache, and then compute the hit rate on the whole query stream.",
                "This hit rate is as an upper bound, and it assumes perfect knowledge of the query term frequencies.",
                "To simulate a realistic scenario, we use the first 6 (3) weeks of the query stream for computing query term frequencies and the following 9 (12) weeks to estimate the hit rate.",
                "As Figure 14 shows, the hit rate decreases by less than 2%.",
                "The high correlation among the query term frequencies during different time periods explains the graceful adaptation of the static caching algorithms to the future query stream.",
                "Indeed, the pairwise correlation among all possible 3-week periods of the 15-week query stream is over 99.5%. 8.",
                "CONCLUSIONS Caching is an effective technique in search engines for improving response time, reducing the load on query processors, and improving network bandwidth utilization.",
                "We present results on both dynamic and static caching.",
                "Dynamic caching of queries has limited effectiveness due to the high number of compulsory misses caused by the number of unique or infrequent queries.",
                "Our results show that in our UK log, the minimum miss rate is 50% using a working set strategy.",
                "Caching terms is more effective with respect to miss rate, achieving values as low as 12%.",
                "We also propose a new algorithm for static caching of posting lists that outperforms previous static caching algorithms as well as dynamic algorithms such as LRU and LFU, obtaining hit rate values that are over 10% higher compared these strategies.",
                "We present a framework for the analysis of the trade-off between caching query results and caching posting lists, and we simulate different types of architectures.",
                "Our results show that for centralized and LAN environments, there is an optimal allocation of caching query results and caching of posting lists, while for WAN scenarios in which network time prevails it is more important to cache query results. 0.45 0.5 0.55 0.6 0.65 0.7 0.75 0.8 0.85 0.9 0.95 0.1 0.2 0.3 0.4 0.5 0.6 0.7 Hitrate Cache size Dynamics of static QTF/DF caching policy perfect knowledge 6-week training 3-week training Figure 14: Impact of distribution changes on the static caching of posting lists. 9.",
                "REFERENCES [1] V. N. Anh and A. Moffat.",
                "Pruned query evaluation using pre-computed impacts.",
                "In ACM CIKM, 2006. [2] R. A. Baeza-Yates and F. Saint-Jean.",
                "A three level search engine index based in query log distribution.",
                "In SPIRE, 2003. [3] C. Buckley and A. F. Lewit.",
                "Optimization of inverted vector searches.",
                "In ACM SIGIR, 1985. [4] S. B¨uttcher and C. L. A. Clarke.",
                "A document-centric approach to static index pruning in text retrieval systems.",
                "In ACM CIKM, 2006. [5] P. Cao and S. Irani.",
                "Cost-aware WWW proxy caching algorithms.",
                "In USITS, 1997. [6] P. Denning.",
                "Working sets past and present.",
                "IEEE Trans. on Software Engineering, SE-6(1):64-84, 1980. [7] T. Fagni, R. Perego, F. Silvestri, and S. Orlando.",
                "Boosting the performance of web search engines: Caching and prefetching query results by exploiting historical usage data.",
                "ACM Trans.",
                "Inf.",
                "Syst., 24(1):51-78, 2006. [8] R. Lempel and S. Moran.",
                "Predictive caching and prefetching of query results in search engines.",
                "In WWW, 2003. [9] X.",
                "Long and T. Suel.",
                "Three-level caching for efficient query processing in large web search engines.",
                "In WWW, 2005. [10] E. P. Markatos.",
                "On caching search engine query results.",
                "Computer Communications, 24(2):137-143, 2001. [11] I. Ounis, G. Amati, V. Plachouras, B.",
                "He, C. Macdonald, and C. Lioma.",
                "Terrier: A High Performance and Scalable Information Retrieval Platform.",
                "In SIGIR Workshop on Open Source Information Retrieval, 2006. [12] V. V. Raghavan and H. Sever.",
                "On the reuse of past optimal queries.",
                "In ACM SIGIR, 1995. [13] P. C. Saraiva, E. S. de Moura, N. Ziviani, W. Meira, R. Fonseca, and B. Riberio-Neto.",
                "Rank-preserving two-level caching for scalable search engines.",
                "In ACM SIGIR, 2001. [14] D. R. Slutz and I. L. Traiger.",
                "A note on the calculation of average working set size.",
                "Communications of the ACM, 17(10):563-565, 1974. [15] T. Strohman, H. Turtle, and W. B. Croft.",
                "Optimization strategies for complex queries.",
                "In ACM SIGIR, 2005. [16] I. H. Witten, T. C. Bell, and A. Moffat.",
                "Managing Gigabytes: Compressing and Indexing Documents and Images.",
                "John Wiley & Sons, Inc., NY, 1994. [17] N. E. Young.",
                "On-line file caching.",
                "Algorithmica, 33(3):371-383, 2002."
            ],
            "original_annotated_samples": [],
            "translated_annotated_samples": [
                "Nuestros resultados y observaciones son aplicables a diferentes niveles de la jerarquía de acceso de datos, por ejemplo, para una capa de memoria/disco o una \"capa de servidor remoto\".",
                "Se puede aplicar en la capa de memoria/disco o en un servidor/\"capa de servidor remoto\" como en la arquitectura que discutimos en la introducción."
            ],
            "translated_text": "",
            "candidates": [
                "Capa del servidor remoto",
                "capa de servidor remoto",
                "Capa del servidor remoto",
                "capa de servidor remoto"
            ],
            "error": []
        },
        "cache": {
            "translated_key": "caché",
            "is_in_text": true,
            "original_annotated_sentences": [
                "The Impact of Caching on Search Engines Ricardo Baeza-Yates1 rbaeza@acm.org Aristides Gionis1 gionis@yahoo-inc.com Flavio Junqueira1 fpj@yahoo-inc.com Vanessa Murdock1 vmurdock@yahoo-inc.com Vassilis Plachouras1 vassilis@yahoo-inc.com Fabrizio Silvestri2 f.silvestri@isti.cnr.it 1 Yahoo!",
                "Research Barcelona 2 ISTI - CNR Barcelona, SPAIN Pisa, ITALY ABSTRACT In this paper we study the trade-offs in designing efficient caching systems for Web search engines.",
                "We explore the impact of different approaches, such as static vs. dynamic caching, and caching query results vs. caching posting lists.",
                "Using a query log spanning a whole year we explore the limitations of caching and we demonstrate that caching posting lists can achieve higher hit rates than caching query answers.",
                "We propose a new algorithm for static caching of posting lists, which outperforms previous methods.",
                "We also study the problem of finding the optimal way to split the static <br>cache</br> between answers and posting lists.",
                "Finally, we measure how the changes in the query log affect the effectiveness of static caching, given our observation that the distribution of the queries changes slowly over time.",
                "Our results and observations are applicable to different levels of the data-access hierarchy, for instance, for a memory/disk layer or a broker/remote server layer.",
                "Categories and Subject Descriptors H.3.3 [Information Storage and Retrieval]: Information Search and Retrieval - Search process; H.3.4 [Information Storage and Retrieval]: Systems and Software - Distributed systems, Performance evaluation (efficiency and effectiveness) General Terms Algorithms, Experimentation 1.",
                "INTRODUCTION Millions of queries are submitted daily to Web search engines, and users have high expectations of the quality and speed of the answers.",
                "As the searchable Web becomes larger and larger, with more than 20 billion pages to index, evaluating a single query requires processing large amounts of data.",
                "In such a setting, to achieve a fast response time and to increase the query throughput, using a <br>cache</br> is crucial.",
                "The primary use of a <br>cache</br> memory is to speedup computation by exploiting frequently or recently used data, although reducing the workload to back-end servers is also a major goal.",
                "Caching can be applied at different levels with increasing response latencies or processing requirements.",
                "For example, the different levels may correspond to the main memory, the disk, or resources in a local or a wide area network.",
                "The decision of what to <br>cache</br> is either off-line (static) or online (dynamic).",
                "A static <br>cache</br> is based on historical information and is periodically updated.",
                "A dynamic <br>cache</br> replaces entries according to the sequence of requests.",
                "When a new request arrives, the <br>cache</br> system decides whether to evict some entry from the <br>cache</br> in the case of a cache miss.",
                "Such online decisions are based on a <br>cache</br> policy, and several different policies have been studied in the past.",
                "For a search engine, there are two possible ways to use a <br>cache</br> memory: Caching answers: As the engine returns answers to a particular query, it may decide to store these answers to resolve future queries.",
                "Caching terms: As the engine evaluates a particular query, it may decide to store in memory the posting lists of the involved query terms.",
                "Often the whole set of posting lists does not fit in memory, and consequently, the engine has to select a small set to keep in memory and speed up query processing.",
                "Returning an answer to a query that already exists in the <br>cache</br> is more efficient than computing the answer using cached posting lists.",
                "On the other hand, previously unseen queries occur more often than previously unseen terms, implying a higher miss rate for cached answers.",
                "Caching of posting lists has additional challenges.",
                "As posting lists have variable size, caching them dynamically is not very efficient, due to the complexity in terms of efficiency and space, and the skewed distribution of the query stream, as shown later.",
                "Static caching of posting lists poses even more challenges: when deciding which terms to <br>cache</br> one faces the trade-off between frequently queried terms and terms with small posting lists that are space efficient.",
                "Finally, before deciding to adopt a static caching policy the query stream should be analyzed to verify that its characteristics do not change rapidly over time.",
                "Broker Static caching posting lists Dynamic/Static cached answers Local query processor Disk Next caching level Local network access Remote network access Figure 1: One caching level in a distributed search architecture.",
                "In this paper we explore the trade-offs in the design of each <br>cache</br> level, showing that the problem is the same and only a few parameters change.",
                "In general, we assume that each level of caching in a distributed search architecture is similar to that shown in Figure 1.",
                "We use a query log spanning a whole year to explore the limitations of dynamically caching query answers or posting lists for query terms.",
                "More concretely, our main conclusions are that: • Caching query answers results in lower hit ratios compared to caching of posting lists for query terms, but it is faster because there is no need for query evaluation.",
                "We provide a framework for the analysis of the trade-off between static caching of query answers and posting lists; • Static caching of terms can be more effective than dynamic caching with, for example, LRU.",
                "We provide algorithms based on the Knapsack problem for selecting the posting lists to put in a static <br>cache</br>, and we show improvements over previous work, achieving a hit ratio over 90%; • Changes of the query distribution over time have little impact on static caching.",
                "The remainder of this paper is organized as follows.",
                "Sections 2 and 3 summarize related work and characterize the data sets we use.",
                "Section 4 discusses the limitations of dynamic caching.",
                "Sections 5 and 6 introduce algorithms for caching posting lists, and a theoretical framework for the analysis of static caching, respectively.",
                "Section 7 discusses the impact of changes in the query distribution on static caching, and Section 8 provides concluding remarks. 2.",
                "RELATED WORK There is a large body of work devoted to query optimization.",
                "Buckley and Lewit [3], in one of the earliest works, take a term-at-a-time approach to deciding when inverted lists need not be further examined.",
                "More recent examples demonstrate that the top k documents for a query can be returned without the need for evaluating the complete set of posting lists [1, 4, 15].",
                "Although these approaches seek to improve query processing efficiency, they differ from our current work in that they do not consider caching.",
                "They may be considered separate and complementary to a <br>cache</br>-based approach.",
                "Raghavan and Sever [12], in one of the first papers on exploiting user query history, propose using a query base, built upon a set of persistent optimal queries submitted in the past, to improve the retrieval effectiveness for similar future queries.",
                "Markatos [10] shows the existence of temporal locality in queries, and compares the performance of different caching policies.",
                "Based on the observations of Markatos, Lempel and Moran propose a new caching policy, called Probabilistic Driven Caching, by attempting to estimate the probability distribution of all possible queries submitted to a search engine [8].",
                "Fagni et al. follow Markatos work by showing that combining static and dynamic caching policies together with an adaptive prefetching policy achieves a high hit ratio [7].",
                "Different from our work, they consider caching and prefetching of pages of results.",
                "As systems are often hierarchical, there has also been some effort on multi-level architectures.",
                "Saraiva et al. propose a new architecture for Web search engines using a two-level dynamic caching system [13].",
                "Their goal for such systems has been to improve response time for hierarchical engines.",
                "In their architecture, both levels use an LRU eviction policy.",
                "They find that the second-level <br>cache</br> can effectively reduce disk traffic, thus increasing the overall throughput.",
                "Baeza-Yates and Saint-Jean propose a three-level index organization [2].",
                "Long and Suel propose a caching system structured according to three different levels [9].",
                "The intermediate level contains frequently occurring pairs of terms and stores the intersections of the corresponding inverted lists.",
                "These last two papers are related to ours in that they exploit different caching strategies at different levels of the memory hierarchy.",
                "Finally, our static caching algorithm for posting lists in Section 5 uses the ratio frequency/size in order to evaluate the goodness of an item to <br>cache</br>.",
                "Similar ideas have been used in the context of file caching [17], Web caching [5], and even caching of posting lists [9], but in all cases in a dynamic setting.",
                "To the best of our knowledge we are the first to use this approach for static caching of posting lists. 3.",
                "DATA CHARACTERIZATION Our data consists of a crawl of documents from the UK domain, and query logs of one year of queries submitted to http://www.yahoo.co.uk from November 2005 to November 2006.",
                "In our logs, 50% of the total volume of queries are unique.",
                "The average query length is 2.5 terms, with the longest query having 731 terms. 1e-07 1e-06 1e-05 1e-04 0.001 0.01 0.1 1 1e-08 1e-07 1e-06 1e-05 1e-04 0.001 0.01 0.1 1 Frequency(normalized) Frequency rank (normalized) Figure 2: The distribution of queries (bottom curve) and query terms (middle curve) in the query log.",
                "The distribution of document frequencies of terms in the UK-2006 dataset (upper curve).",
                "Figure 2 shows the distributions of queries (lower curve), and query terms (middle curve).",
                "The x-axis represents the normalized frequency rank of the query or term. (The most frequent query appears closest to the y-axis.)",
                "The y-axis is Table 1: Statistics of the UK-2006 sample.",
                "UK-2006 sample statistics # of documents 2,786,391 # of terms 6,491,374 # of tokens 2,109,512,558 the normalized frequency for a given query (or term).",
                "As expected, the distribution of query frequencies and query term frequencies follow power law distributions, with slope of 1.84 and 2.26, respectively.",
                "In this figure, the query frequencies were computed as they appear in the logs with no normalization for case or white space.",
                "The query terms (middle curve) have been normalized for case, as have the terms in the document collection.",
                "The document collection that we use for our experiments is a summary of the UK domain crawled in May 2006.1 This summary corresponds to a maximum of 400 crawled documents per host, using a breadth first crawling strategy, comprising 15GB.",
                "The distribution of document frequencies of terms in the collection follows a power law distribution with slope 2.38 (upper curve in Figure 2).",
                "The statistics of the collection are shown in Table 1.",
                "We measured the correlation between the document frequency of terms in the collection and the number of queries that contain a particular term in the query log to be 0.424.",
                "A scatter plot for a random sample of terms is shown in Figure 3.",
                "In this experiment, terms have been converted to lower case in both the queries and the documents so that the frequencies will be comparable. 1e-07 1e-06 1e-05 1e-04 0.001 0.01 0.1 1 1e-06 1e-05 1e-04 0.001 0.01 0.1 1 Queryfrequency Document frequency Figure 3: Normalized scatter plot of document-term frequencies vs. query-term frequencies. 4.",
                "CACHING OF QUERIES AND TERMS Caching relies upon the assumption that there is locality in the stream of requests.",
                "That is, there must be sufficient repetition in the stream of requests and within intervals of time that enable a <br>cache</br> memory of reasonable size to be effective.",
                "In the query log we used, 88% of the unique queries are singleton queries, and 44% are singleton queries out of the whole volume.",
                "Thus, out of all queries in the stream composing the query log, the upper threshold on hit ratio is 56%.",
                "This is because only 56% of all the queries comprise queries that have multiple occurrences.",
                "It is important to observe, however, that not all queries in this 56% can be <br>cache</br> hits because of compulsory misses.",
                "A compulsory miss 1 The collection is available from the University of Milan: http://law.dsi.unimi.it/.",
                "URL retrieved 05/2007. 0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1 240 260 280 300 320 340 360 Numberofelements Bin number Total terms Terms diff Total queries Unique queries Unique terms Query diff Figure 4: Arrival rate for both terms and queries. happens when the <br>cache</br> receives a query for the first time.",
                "This is different from capacity misses, which happen due to space constraints on the amount of memory the <br>cache</br> uses.",
                "If we consider a <br>cache</br> with infinite memory, then the hit ratio is 50%.",
                "Note that for an infinite <br>cache</br> there are no capacity misses.",
                "As we mentioned before, another possibility is to <br>cache</br> the posting lists of terms.",
                "Intuitively, this gives more freedom in the utilization of the <br>cache</br> content to respond to queries because cached terms might form a new query.",
                "On the other hand, they need more space.",
                "As opposed to queries, the fraction of singleton terms in the total volume of terms is smaller.",
                "In our query log, only 4% of the terms appear once, but this accounts for 73% of the vocabulary of query terms.",
                "We show in Section 5 that caching a small fraction of terms, while accounting for terms appearing in many documents, is potentially very effective.",
                "Figure 4 shows several graphs corresponding to the normalized arrival rate for different cases using days as bins.",
                "That is, we plot the normalized number of elements that appear in a day.",
                "This graph shows only a period of 122 days, and we normalize the values by the maximum value observed throughout the whole period of the query log.",
                "Total queries and Total terms correspond to the total volume of queries and terms, respectively.",
                "Unique queries and Unique terms correspond to the arrival rate of unique queries and terms.",
                "Finally, Query diff and Terms diff correspond to the difference between the curves for total and unique.",
                "In Figure 4, as expected, the volume of terms is much higher than the volume of queries.",
                "The difference between the total number of terms and the number of unique terms is much larger than the difference between the total number of queries and the number of unique queries.",
                "This observation implies that terms repeat significantly more than queries.",
                "If we use smaller bins, say of one hour, then the ratio of unique to volume is higher for both terms and queries because it leaves less room for repetition.",
                "We also estimated the workload using the document frequency of terms as a measure of how much work a query imposes on a search engine.",
                "We found that it follows closely the arrival rate for terms shown in Figure 4.",
                "To demonstrate the effect of a dynamic <br>cache</br> on the query frequency distribution of Figure 2, we plot the same frequency graph, but now considering the frequency of queries Figure 5: Frequency graph after LRU <br>cache</br>. after going through an LRU cache.",
                "On a <br>cache</br> miss, an LRU <br>cache</br> decides upon an entry to evict using the information on the recency of queries.",
                "In this graph, the most frequent queries are not the same queries that were most frequent before the <br>cache</br>.",
                "It is possible that queries that are most frequent after the <br>cache</br> have different characteristics, and tuning the search engine to queries frequent before the <br>cache</br> may degrade performance for non-cached queries.",
                "The maximum frequency after caching is less than 1% of the maximum frequency before the <br>cache</br>, thus showing that the <br>cache</br> is very effective in reducing the load of frequent queries.",
                "If we re-rank the queries according to after-<br>cache</br> frequency, the distribution is still a power law, but with a much smaller value for the highest frequency.",
                "When discussing the effectiveness of dynamically caching, an important metric is <br>cache</br> miss rate.",
                "To analyze the <br>cache</br> miss rate for different memory constraints, we use the working set model [6, 14].",
                "A working set, informally, is the set of references that an application or an operating system is currently working with.",
                "The model uses such sets in a strategy that tries to capture the temporal locality of references.",
                "The working set strategy then consists in keeping in memory only the elements that are referenced in the previous θ steps of the input sequence, where θ is a configurable parameter corresponding to the window size.",
                "Originally, working sets have been used for page replacement algorithms of operating systems, and considering such a strategy in the context of search engines is interesting for three reasons.",
                "First, it captures the amount of locality of queries and terms in a sequence of queries.",
                "Locality in this case refers to the frequency of queries and terms in a window of time.",
                "If many queries appear multiple times in a window, then locality is high.",
                "Second, it enables an oﬄine analysis of the expected miss rate given different memory constraints.",
                "Third, working sets capture aspects of efficient caching algorithms such as LRU.",
                "LRU assumes that references farther in the past are less likely to be referenced in the present, which is implicit in the concept of working sets [14].",
                "Figure 6 plots the miss rate for different working set sizes, and we consider working sets of both queries and terms.",
                "The working set sizes are normalized against the total number of queries in the query log.",
                "In the graph for queries, there is a sharp decay until approximately 0.01, and the rate at which the miss rate drops decreases as we increase the size of the working set over 0.01.",
                "Finally, the minimum value it reaches is 50% miss rate, not shown in the figure as we have cut the tail of the curve for presentation purposes. 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1 0 0.05 0.1 0.15 0.2 Missrate Normalized working set size Queries Terms Figure 6: Miss rate as a function of the working set size. 1 10 100 1000 10000 100000 1e+06 Frequency Distance Figure 7: Distribution of distances expressed in terms of distinct queries.",
                "Compared to the query curve, we observe that the minimum miss rate for terms is substantially smaller.",
                "The miss rate also drops sharply on values up to 0.01, and it decreases minimally for higher values.",
                "The minimum value, however, is slightly over 10%, which is much smaller than the minimum value for the sequence of queries.",
                "This implies that with such a policy it is possible to achieve over 80% hit rate, if we consider caching dynamically posting lists for terms as opposed to caching answers for queries.",
                "This result does not consider the space required for each unit stored in the <br>cache</br> memory, or the amount of time it takes to put together a response to a user query.",
                "We analyze these issues more carefully later in this paper.",
                "It is interesting also to observe the histogram of Figure 7, which is an intermediate step in the computation of the miss rate graph.",
                "It reports the distribution of distances between repetitions of the same frequent query.",
                "The distance in the plot is measured in the number of distinct queries separating a query and its repetition, and it considers only queries appearing at least 10 times.",
                "From Figures 6 and 7, we conclude that even if we set the size of the query answers <br>cache</br> to a relatively large number of entries, the miss rate is high.",
                "Thus, caching the posting lists of terms has the potential to improve the hit ratio.",
                "This is what we explore next. 5.",
                "CACHING POSTING LISTS The previous section shows that caching posting lists can obtain a higher hit rate compared to caching query answers.",
                "In this section we study the problem of how to select posting lists to place on a certain amount of available memory, assuming that the whole index is larger than the amount of memory available.",
                "The posting lists have variable size (in fact, their size distribution follows a power law), so it is beneficial for a caching policy to consider the sizes of the posting lists.",
                "We consider both dynamic and static caching.",
                "For dynamic caching, we use two well-known policies, LRU and LFU, as well as a modified algorithm that takes posting-list size into account.",
                "Before discussing the static caching strategies, we introduce some notation.",
                "We use fq(t) to denote the query-term frequency of a term t, that is, the number of queries containing t in the query log, and fd(t) to denote the document frequency of t, that is, the number of documents in the collection in which the term t appears.",
                "The first strategy we consider is the algorithm proposed by Baeza-Yates and Saint-Jean [2], which consists in selecting the posting lists of the terms with the highest query-term frequencies fq(t).",
                "We call this algorithm Qtf.",
                "We observe that there is a trade-off between fq(t) and fd(t).",
                "Terms with high fq(t) are useful to keep in the <br>cache</br> because they are queried often.",
                "On the other hand, terms with high fd(t) are not good candidates because they correspond to long posting lists and consume a substantial amount of space.",
                "In fact, the problem of selecting the best posting lists for the static <br>cache</br> corresponds to the standard Knapsack problem: given a knapsack of fixed capacity, and a set of n items, such as the i-th item has value ci and size si, select the set of items that fit in the knapsack and maximize the overall value.",
                "In our case, value corresponds to fq(t) and size corresponds to fd(t).",
                "Thus, we employ a simple algorithm for the knapsack problem, which is selecting the posting lists of the terms with the highest values of the ratio fq(t) fd(t) .",
                "We call this algorithm QtfDf.",
                "We tried other variations considering query frequencies instead of term frequencies, but the gain was minimal compared to the complexity added.",
                "In addition to the above two static algorithms we consider the following algorithms for dynamic caching: • LRU: A standard LRU algorithm, but many posting lists might need to be evicted (in order of least-recent usage) until there is enough space in the memory to place the currently accessed posting list; • LFU: A standard LFU algorithm (eviction of the leastfrequently used), with the same modification as the LRU; • Dyn-QtfDf: A dynamic version of the QtfDf algorithm; evict from the <br>cache</br> the term(s) with the lowest fq(t) fd(t) ratio.",
                "The performance of all the above algorithms for 15 weeks of the query log and the UK dataset are shown in Figure 8.",
                "Performance is measured with hit rate.",
                "The <br>cache</br> size is measured as a fraction of the total space required to store the posting lists of all terms.",
                "For the dynamic algorithms, we load the <br>cache</br> with terms in order of fq(t) and we let the <br>cache</br> warm up for 1 million queries.",
                "For the static algorithms, we assume complete knowledge of the frequencies fq(t), that is, we estimate fq(t) from the whole query stream.",
                "As we show in Section 7 the results do not change much if we compute the query-term frequencies using the first 3 or 4 weeks of the query log and measure the hit rate on the rest. 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1 0.1 0.2 0.3 0.4 0.5 0.6 0.7 Hitrate <br>cache</br> size Caching posting lists static QTF/DF LRU LFU Dyn-QTF/DF QTF Figure 8: Hit rate of different strategies for caching posting lists.",
                "The most important observation from our experiments is that the static QtfDf algorithm has a better hit rate than all the dynamic algorithms.",
                "An important benefit a static <br>cache</br> is that it requires no eviction and it is hence more efficient when evaluating queries.",
                "However, if the characteristics of the query traffic change frequently over time, then it requires re-populating the <br>cache</br> often or there will be a significant impact on hit rate. 6.",
                "ANALYSIS OF STATIC CACHING In this section we provide a detailed analysis for the problem of deciding whether it is preferable to <br>cache</br> query answers or <br>cache</br> posting lists.",
                "Our analysis takes into account the impact of caching between two levels of the data-access hierarchy.",
                "It can either be applied at the memory/disk layer or at a server/remote server layer as in the architecture we discussed in the introduction.",
                "Using a particular system model, we obtain estimates for the parameters required by our analysis, which we subsequently use to decide the optimal trade-off between caching query answers and caching posting lists. 6.1 Analytical Model Let M be the size of the <br>cache</br> measured in answer units (the <br>cache</br> can store M query answers).",
                "Assume that all posting lists are of the same length L, measured in answer units.",
                "We consider the following two cases: (A) a <br>cache</br> that stores only precomputed answers, and (B) a <br>cache</br> that stores only posting lists.",
                "In the first case, Nc = M answers fit in the <br>cache</br>, while in the second case Np = M/L posting lists fit in the <br>cache</br>.",
                "Thus, Np = Nc/L.",
                "Note that although posting lists require more space, we can combine terms to evaluate more queries (or partial queries).",
                "For case (A), suppose that a query answer in the <br>cache</br> can be evaluated in 1 time unit.",
                "For case (B), assume that if the posting lists of the terms of a query are in the <br>cache</br> then the results can be computed in TR1 time units, while if the posting lists are not in the <br>cache</br> then the results can be computed in TR2 time units.",
                "Of course TR2 > TR1.",
                "Now we want to compare the time to answer a stream of Q queries in both cases.",
                "Let Vc(Nc) be the volume of the most frequent Nc queries.",
                "Then, for case (A), we have an overall time TCA = Vc(Nc) + TR2(Q − Vc(Nc)).",
                "Similarly, for case (B), let Vp(Np) be the number of computable queries.",
                "Then we have overall time TP L = TR1Vp(Np) + TR2(Q − Vp(Np)).",
                "We want to check under which conditions we have TP L < TCA.",
                "We have TP L − TCA = (TR2 − 1)Vc(Nc) − (TR2 − TR1)Vp(Np) > 0.",
                "Figure 9 shows the values of Vp and Vc for our data.",
                "We can see that caching answers saturates faster and for this particular data there is no additional benefit from using more than 10% of the index space for caching answers.",
                "As the query distribution is a power law with parameter α > 1, the i-th most frequent query appears with probability proportional to 1 iα .",
                "Therefore, the volume Vc(n), which is the total number of the n most frequent queries, is Vc(n) = V0 n i=1 Q iα = γnQ (0 < γn < 1).",
                "We know that Vp(n) grows faster than Vc(n) and assume, based on experimental results, that the relation is of the form Vp(n) = k Vc(n)β .",
                "In the worst case, for a large <br>cache</br>, β → 1.",
                "That is, both techniques will <br>cache</br> a constant fraction of the overall query volume.",
                "Then caching posting lists makes sense only if L(TR2 − 1) k(TR2 − TR1) > 1.",
                "If we use compression, we have L < L and TR1 > TR1.",
                "According to the experiments that we show later, compression is always better.",
                "For a small <br>cache</br>, we are interested in the transient behavior and then β > 1, as computed from our data.",
                "In this case there will always be a point where TP L > TCA for a large number of queries.",
                "In reality, instead of filling the <br>cache</br> only with answers or only with posting lists, a better strategy will be to divide the total <br>cache</br> space into cache for answers and cache for posting lists.",
                "In such a case, there will be some queries that could be answered by both parts of the <br>cache</br>.",
                "As the answer <br>cache</br> is faster, it will be the first choice for answering those queries.",
                "Let QNc and QNp be the set of queries that can be answered by the cached answers and the cached posting lists, respectively.",
                "Then, the overall time is T = Vc(Nc)+TR1V (QNp −QNc )+TR2(Q−V (QNp ∪QNc )), where Np = (M − Nc)/L.",
                "Finding the optimal division of the <br>cache</br> in order to minimize the overall retrieval time is a difficult problem to solve analytically.",
                "In Section 6.3 we use simulations to derive optimal <br>cache</br> trade-offs for particular implementation examples. 6.2 Parameter Estimation We now use a particular implementation of a centralized system and the model of a distributed system as examples from which we estimate the parameters of the analysis from the previous section.",
                "We perform the experiments using an optimized version of Terrier [11] for both indexing documents and processing queries, on a single machine with a Pentium 4 at 2GHz and 1GB of RAM.",
                "We indexed the documents from the UK-2006 dataset, without removing stop words or applying stemming.",
                "The posting lists in the inverted file consist of pairs of document identifier and term frequency.",
                "We compress the document identifier gaps using Elias gamma encoding, and the 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1 0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1 Queryvolume Space precomputed answers posting lists Figure 9: <br>cache</br> saturation as a function of size.",
                "Table 2: Ratios between the average time to evaluate a query and the average time to return cached answers (centralized and distributed case).",
                "Centralized system TR1 TR2 TR1 TR2 Full evaluation 233 1760 707 1140 Partial evaluation 99 1626 493 798 LAN system TRL 1 TRL 2 TR L 1 TR L 2 Full evaluation 242 1769 716 1149 Partial evaluation 108 1635 502 807 WAN system TRW 1 TRW 2 TR W 1 TR W 2 Full evaluation 5001 6528 5475 5908 Partial evaluation 4867 6394 5270 5575 term frequencies in documents using unary encoding [16].",
                "The size of the inverted file is 1,189Mb.",
                "A stored answer requires 1264 bytes, and an uncompressed posting takes 8 bytes.",
                "From Table 1, we obtain L = (8·# of postings) 1264·# of terms = 0.75 and L = Inverted file size 1264·# of terms = 0.26.",
                "We estimate the ratio TR = T/Tc between the average time T it takes to evaluate a query and the average time Tc it takes to return a stored answer for the same query, in the following way.",
                "Tc is measured by loading the answers for 100,000 queries in memory, and answering the queries from memory.",
                "The average time is Tc = 0.069ms.",
                "T is measured by processing the same 100,000 queries (the first 10,000 queries are used to warm-up the system).",
                "For each query, we remove stop words, if there are at least three remaining terms.",
                "The stop words correspond to the terms with a frequency higher than the number of documents in the index.",
                "We use a document-at-a-time approach to retrieve documents containing all query terms.",
                "The only disk access required during query processing is for reading compressed posting lists from the inverted file.",
                "We perform both full and partial evaluation of answers, because some queries are likely to retrieve a large number of documents, and only a fraction of the retrieved documents will be seen by users.",
                "In the partial evaluation of queries, we terminate the processing after matching 10,000 documents.",
                "The estimated ratios TR are presented in Table 2.",
                "Figure 10 shows for a sample of queries the workload of the system with partial query evaluation and compressed posting lists.",
                "The x-axis corresponds to the total time the system spends processing a particular query, and the vertical axis corresponds to the sum t∈q fq · fd(t).",
                "Notice that the total number of postings of the query-terms does not necessarily provide an accurate estimate of the workload imposed on the system by a query (which is the case for full evaluation and uncompressed lists). 0 0.2 0.4 0.6 0.8 1 0 0.2 0.4 0.6 0.8 1 Totalpostingstoprocessquery(normalized) Total time to process query (normalized) Partial processing of compressed postings query len = 1 query len in [2,3] query len in [4,8] query len > 8 Figure 10: Workload for partial query evaluation with compressed posting lists.",
                "The analysis of the previous section also applies to a distributed retrieval system in one or multiple sites.",
                "Suppose that a document partitioned distributed system is running on a cluster of machines interconnected with a local area network (LAN) in one site.",
                "The broker receives queries and broadcasts them to the query processors, which answer the queries and return the results to the broker.",
                "Finally, the broker merges the received answers and generates the final set of answers (we assume that the time spent on merging results is negligible).",
                "The difference between the centralized architecture and the document partition architecture is the extra communication between the broker and the query processors.",
                "Using ICMP pings on a 100Mbps LAN, we have measured that sending the query from the broker to the query processors which send an answer of 4,000 bytes back to the broker takes on average 0.615ms.",
                "Hence, TRL = TR + 0.615ms/0.069ms = TR + 9.",
                "In the case when the broker and the query processors are in different sites connected with a wide area network (WAN), we estimated that broadcasting the query from the broker to the query processors and getting back an answer of 4,000 bytes takes on average 329ms.",
                "Hence, TRW = TR + 329ms/0.069ms = TR + 4768. 6.3 Simulation Results We now address the problem of finding the optimal tradeoff between caching query answers and caching posting lists.",
                "To make the problem concrete we assume a fixed budget M on the available memory, out of which x units are used for caching query answers and M − x for caching posting lists.",
                "We perform simulations and compute the average response time as a function of x.",
                "Using a part of the query log as training data, we first allocate in the <br>cache</br> the answers to the most frequent queries that fit in space x, and then we use the rest of the memory to <br>cache</br> posting lists.",
                "For selecting posting lists we use the QtfDf algorithm, applied to the training query log but excluding the queries that have already been cached.",
                "In Figure 11, we plot the simulated response time for a centralized system as a function of x.",
                "For the uncompressed index we use M = 1GB, and for the compressed index we use M = 0.5GB.",
                "In the case of the configuration that uses partial query evaluation with compressed posting lists, the lowest response time is achieved when 0.15GB out of the 0.5GB is allocated for storing answers for queries.",
                "We obtained similar trends in the results for the LAN setting.",
                "Figure 12 shows the simulated workload for a distributed system across a WAN.",
                "In this case, the total amount of memory is split between the broker, which holds the cached 400 500 600 700 800 900 1000 1100 1200 0 0.2 0.4 0.6 0.8 1 Averageresponsetime Space (GB) Simulated workload -- single machine full / uncompr / 1 G partial / uncompr / 1 G full / compr / 0.5 G partial / compr / 0.5 G Figure 11: Optimal division of the <br>cache</br> in a server. 3000 3500 4000 4500 5000 5500 6000 0 0.2 0.4 0.6 0.8 1 Averageresponsetime Space (GB) Simulated workload -- WAN full / uncompr / 1 G partial / uncompr / 1 G full / compr / 0.5 G partial / compr / 0.5 G Figure 12: Optimal division of the <br>cache</br> when the next level requires WAN access. answers of queries, and the query processors, which hold the cache of posting lists.",
                "According to the figure, the difference between the configurations of the query processors is less important because the network communication overhead increases the response time substantially.",
                "When using uncompressed posting lists, the optimal allocation of memory corresponds to using approximately 70% of the memory for caching query answers.",
                "This is explained by the fact that there is no need for network communication when the query can be answered by the <br>cache</br> at the broker. 7.",
                "EFFECT OF THE QUERY DYNAMICS For our query log, the query distribution and query-term distribution change slowly over time.",
                "To support this claim, we first assess how topics change comparing the distribution of queries from the first week in June, 2006, to the distribution of queries for the remainder of 2006 that did not appear in the first week in June.",
                "We found that a very small percentage of queries are new queries.",
                "The majority of queries that appear in a given week repeat in the following weeks for the next six months.",
                "We then compute the hit rate of a static <br>cache</br> of 128, 000 answers trained over a period of two weeks (Figure 13).",
                "We report hit rate hourly for 7 days, starting from 5pm.",
                "We observe that the hit rate reaches its highest value during the night (around midnight), whereas around 2-3pm it reaches its minimum.",
                "After a small decay in hit rate values, the hit rate stabilizes between 0.28, and 0.34 for the entire week, suggesting that the static <br>cache</br> is effective for a whole week after the training period. 0.26 0.27 0.28 0.29 0.3 0.31 0.32 0.33 0.34 0.35 0.36 0.37 0 20 40 60 80 100 120 140 160 Hit-rate Time Hits on the frequent queries of distances Figure 13: Hourly hit rate for a static <br>cache</br> holding 128,000 answers during the period of a week.",
                "The static <br>cache</br> of posting lists can be periodically recomputed.",
                "To estimate the time interval in which we need to recompute the posting lists on the static <br>cache</br> we need to consider an efficiency/quality trade-off: using too short a time interval might be prohibitively expensive, while recomputing the <br>cache</br> too infrequently might lead to having an obsolete cache not corresponding to the statistical characteristics of the current query stream.",
                "We measured the effect on the QtfDf algorithm of the changes in a 15-week query stream (Figure 14).",
                "We compute the query term frequencies over the whole stream, select which terms to <br>cache</br>, and then compute the hit rate on the whole query stream.",
                "This hit rate is as an upper bound, and it assumes perfect knowledge of the query term frequencies.",
                "To simulate a realistic scenario, we use the first 6 (3) weeks of the query stream for computing query term frequencies and the following 9 (12) weeks to estimate the hit rate.",
                "As Figure 14 shows, the hit rate decreases by less than 2%.",
                "The high correlation among the query term frequencies during different time periods explains the graceful adaptation of the static caching algorithms to the future query stream.",
                "Indeed, the pairwise correlation among all possible 3-week periods of the 15-week query stream is over 99.5%. 8.",
                "CONCLUSIONS Caching is an effective technique in search engines for improving response time, reducing the load on query processors, and improving network bandwidth utilization.",
                "We present results on both dynamic and static caching.",
                "Dynamic caching of queries has limited effectiveness due to the high number of compulsory misses caused by the number of unique or infrequent queries.",
                "Our results show that in our UK log, the minimum miss rate is 50% using a working set strategy.",
                "Caching terms is more effective with respect to miss rate, achieving values as low as 12%.",
                "We also propose a new algorithm for static caching of posting lists that outperforms previous static caching algorithms as well as dynamic algorithms such as LRU and LFU, obtaining hit rate values that are over 10% higher compared these strategies.",
                "We present a framework for the analysis of the trade-off between caching query results and caching posting lists, and we simulate different types of architectures.",
                "Our results show that for centralized and LAN environments, there is an optimal allocation of caching query results and caching of posting lists, while for WAN scenarios in which network time prevails it is more important to <br>cache</br> query results. 0.45 0.5 0.55 0.6 0.65 0.7 0.75 0.8 0.85 0.9 0.95 0.1 0.2 0.3 0.4 0.5 0.6 0.7 Hitrate <br>cache</br> size Dynamics of static QTF/DF caching policy perfect knowledge 6-week training 3-week training Figure 14: Impact of distribution changes on the static caching of posting lists. 9.",
                "REFERENCES [1] V. N. Anh and A. Moffat.",
                "Pruned query evaluation using pre-computed impacts.",
                "In ACM CIKM, 2006. [2] R. A. Baeza-Yates and F. Saint-Jean.",
                "A three level search engine index based in query log distribution.",
                "In SPIRE, 2003. [3] C. Buckley and A. F. Lewit.",
                "Optimization of inverted vector searches.",
                "In ACM SIGIR, 1985. [4] S. B¨uttcher and C. L. A. Clarke.",
                "A document-centric approach to static index pruning in text retrieval systems.",
                "In ACM CIKM, 2006. [5] P. Cao and S. Irani.",
                "Cost-aware WWW proxy caching algorithms.",
                "In USITS, 1997. [6] P. Denning.",
                "Working sets past and present.",
                "IEEE Trans. on Software Engineering, SE-6(1):64-84, 1980. [7] T. Fagni, R. Perego, F. Silvestri, and S. Orlando.",
                "Boosting the performance of web search engines: Caching and prefetching query results by exploiting historical usage data.",
                "ACM Trans.",
                "Inf.",
                "Syst., 24(1):51-78, 2006. [8] R. Lempel and S. Moran.",
                "Predictive caching and prefetching of query results in search engines.",
                "In WWW, 2003. [9] X.",
                "Long and T. Suel.",
                "Three-level caching for efficient query processing in large web search engines.",
                "In WWW, 2005. [10] E. P. Markatos.",
                "On caching search engine query results.",
                "Computer Communications, 24(2):137-143, 2001. [11] I. Ounis, G. Amati, V. Plachouras, B.",
                "He, C. Macdonald, and C. Lioma.",
                "Terrier: A High Performance and Scalable Information Retrieval Platform.",
                "In SIGIR Workshop on Open Source Information Retrieval, 2006. [12] V. V. Raghavan and H. Sever.",
                "On the reuse of past optimal queries.",
                "In ACM SIGIR, 1995. [13] P. C. Saraiva, E. S. de Moura, N. Ziviani, W. Meira, R. Fonseca, and B. Riberio-Neto.",
                "Rank-preserving two-level caching for scalable search engines.",
                "In ACM SIGIR, 2001. [14] D. R. Slutz and I. L. Traiger.",
                "A note on the calculation of average working set size.",
                "Communications of the ACM, 17(10):563-565, 1974. [15] T. Strohman, H. Turtle, and W. B. Croft.",
                "Optimization strategies for complex queries.",
                "In ACM SIGIR, 2005. [16] I. H. Witten, T. C. Bell, and A. Moffat.",
                "Managing Gigabytes: Compressing and Indexing Documents and Images.",
                "John Wiley & Sons, Inc., NY, 1994. [17] N. E. Young.",
                "On-line file caching.",
                "Algorithmica, 33(3):371-383, 2002."
            ],
            "original_annotated_samples": [],
            "translated_annotated_samples": [
                "También estudiamos el problema de encontrar la forma óptima de dividir el \"caché\" estático entre respuestas y listas de publicación.",
                "En tal entorno, para lograr un tiempo de respuesta rápido y aumentar el rendimiento de la consulta, es crucial usar un \"caché\".",
                "El uso principal de una memoria de \"caché\" es acelerar el cálculo explotando los datos utilizados con frecuencia o recientemente, aunque reducir la carga de trabajo a los servidores de fondo también es un objetivo importante.",
                "La decisión de qué \"almacenar en caché\" es fuera de línea (estático) o en línea (dinámico).",
                "Un \"caché\" estático se basa en información histórica y se actualiza periódicamente.",
                "Un \"caché\" dinámico reemplaza las entradas de acuerdo con la secuencia de solicitudes.",
                "Cuando llega una nueva solicitud, el sistema \"caché\" decide si desalojar una entrada del \"caché\" en el caso de una falla de caché.",
                "Dichas decisiones en línea se basan en una política de \"caché\", y se han estudiado varias políticas diferentes en el pasado.",
                "Para un motor de búsqueda, hay dos formas posibles de usar una memoria de \"caché\": respuestas de almacenamiento en caché: a medida que el motor devuelve las respuestas a una consulta en particular, puede decidir almacenar estas respuestas para resolver consultas futuras.",
                "Devolver una respuesta a una consulta que ya existe en el \"caché\" es más eficiente que calcular la respuesta utilizando listas de publicación en caché.",
                "El almacenamiento en caché estático de las listas de publicación plantea aún más desafíos: al decidir qué términos \"almacenar en caché\" se enfrenta a la compensación entre los términos y términos consultados frecuentemente con pequeñas listas de publicación que son eficientes en el espacio.",
                "En este artículo exploramos las compensaciones en el diseño de cada nivel de \"caché\", lo que demuestra que el problema es el mismo y solo cambian unos pocos parámetros.",
                "Proporcionamos algoritmos basados en el problema de la mochila para seleccionar las listas de publicación para poner en un \"caché\" estático, y mostramos mejoras sobre el trabajo anterior, logrando una relación de éxito durante el 90%;• Los cambios de la distribución de la consulta a lo largo del tiempo tienen poco impacto en el almacenamiento en caché estático.",
                "Se pueden considerar separados y complementarios a un enfoque basado en \"caché\".",
                "Encuentran que el \"caché\" de segundo nivel puede reducir efectivamente el tráfico de disco, lo que aumenta el rendimiento general.",
                "Finalmente, nuestro algoritmo de almacenamiento en caché estático para publicar listas en la Sección 5 usa la frecuencia/tamaño de relación para evaluar la bondad de un elemento para \"caché\".",
                "Es decir, debe haber suficiente repetición en el flujo de solicitudes y dentro de los intervalos de tiempo que permiten que una memoria de \"caché\" de tamaño razonable sea efectiva.",
                "Sin embargo, es importante observar que no todas las consultas en este 56% pueden ser golpes de \"caché\" debido a las fallas obligatorias.",
                "URL recuperada 05/2007.0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1 240 260 280 300 320 340 360 Número de contenedores Número de contenedores Términos Términos Diff Consultas totales de consultas únicas Términos únicos Consulta Diferencia Figura 4: Llegada para términos y consultas.sucede cuando el \"caché\" recibe una consulta por primera vez.",
                "Esto es diferente de las fallas de capacidad, que ocurren debido a las limitaciones de espacio en la cantidad de memoria que usa el \"caché\".",
                "Si consideramos un \"caché\" con memoria infinita, entonces la relación HIT es del 50%.",
                "Tenga en cuenta que para un \"caché\" infinito no hay fallas de capacidad.",
                "Como mencionamos anteriormente, otra posibilidad es \"almacenar en caché\" las listas de publicaciones de términos.",
                "Intuitivamente, esto da más libertad en la utilización del contenido de \"caché\" para responder a las consultas porque los términos almacenados en caché podrían formar una nueva consulta.",
                "Para demostrar el efecto de un \"caché\" dinámico en la distribución de frecuencia de consulta de la Figura 2, trazamos el mismo gráfico de frecuencia, pero ahora considerando la frecuencia de las consultas Figura 5: Gráfico de frecuencia después de LRU \"Cache\".Después de pasar por un caché LRU.",
                "En una falla de \"caché\", un \"caché\" de LRU decide una entrada para desalojar el uso de la información sobre la recuperación de consultas.",
                "En este gráfico, las consultas más frecuentes no son las mismas consultas que eran más frecuentes antes del \"caché\".",
                "Es posible que las consultas que sean más frecuentes después del \"caché\" tengan características diferentes, y ajustar el motor de búsqueda a consultas frecuentes antes de que el \"caché\" pueda degradar el rendimiento de las consultas no consultadas.",
                "La frecuencia máxima después del almacenamiento en caché es inferior al 1% de la frecuencia máxima antes del \"caché\", lo que demuestra que el \"caché\" es muy efectivo para reducir la carga de consultas frecuentes.",
                "Si volvemos a clasificar las consultas de acuerdo con la frecuencia de \"caché\", la distribución sigue siendo una ley de potencia, pero con un valor mucho menor para la frecuencia más alta.",
                "Al discutir la efectividad del almacenamiento en caché dinámico, una métrica importante es la tasa de fallas de \"caché\".",
                "Para analizar la tasa de fallas de \"caché\" para diferentes restricciones de memoria, utilizamos el modelo de conjunto de trabajo [6, 14].",
                "Este resultado no considera el espacio requerido para cada unidad almacenada en la memoria \"caché\", o la cantidad de tiempo que lleva armar una respuesta a una consulta de usuario.",
                "De las Figuras 6 y 7, concluimos que incluso si establecemos el tamaño de las respuestas de consulta \"caché\" a un número relativamente grande de entradas, la tasa de fallas es alta.",
                "Los términos con alto FQ (t) son útiles para mantener en el \"caché\" porque se consultan a menudo.",
                "De hecho, el problema de seleccionar las mejores listas de publicación para el \"caché\" estático corresponde al problema estándar de la mochila: dada una mochila de capacidad fija y un conjunto de n elementos, como el elemento I-Th tiene valor CI y tamañoSI, seleccione el conjunto de elementos que caben en la mochila y maximicen el valor general.",
                "Además de los dos algoritmos estáticos anteriores, consideramos los siguientes algoritmos para el almacenamiento en caché dinámico: • LRU: un algoritmo LRU estándar, pero es posible que deba desalojarse muchas listas de publicación (en orden de uso menos reciente) hasta que haya suficiente espacio en el espacio en el espacio en elmemoria para colocar la lista de publicaciones de acceso actualmente;• LFU: un algoritmo estándar de LFU (desalojo de los menos utilizados), con la misma modificación que la LRU;• Dyn-Qtfdf: una versión dinámica del algoritmo QTFDF;Desaldadero del \"caché\" el término (s) con la relación FQ (t) FD (t) más baja.",
                "El tamaño de \"caché\" se mide como una fracción del espacio total requerido para almacenar las listas de publicación de todos los términos.",
                "Para los algoritmos dinámicos, cargamos el \"caché\" con los términos en orden de FQ (t) y dejamos que el \"caché\" se calienta por 1 millón de consultas.",
                "Como mostramos en la Sección 7, los resultados no cambian mucho si calculamos las frecuencias a plazo de consulta utilizando las primeras 3 o 4 semanas del registro de la consulta y medimos la tasa de aciertos en el resto.0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1 0.1 0.2 0.3 0.4 0.5 0.6 0.7 HITRATE \"Cache\" Tamaño de caché de tachado Listas de publicaciones estáticas QTF/DF LRU LFU DYN-QTF/DF QTF Figura 8: Tasa de aciertos de diferentes estrategias para listas de publicaciones de caché.",
                "Un beneficio importante, un \"caché\" estático es que no requiere desalojo y, por lo tanto, es más eficiente al evaluar las consultas.",
                "Sin embargo, si las características del tráfico de la consulta cambian con frecuencia con el tiempo, entonces requiere re-popular el \"caché\" a menudo o habrá un impacto significativo en la tasa de aciertos.6.",
                "Análisis del almacenamiento en caché estático En esta sección, proporcionamos un análisis detallado para el problema de decidir si es preferible a las respuestas de consulta de \"caché\" o listas de publicación de \"caché\".",
                "Utilizando un modelo de sistema en particular, obtenemos estimaciones para los parámetros requeridos por nuestro análisis, que posteriormente usamos para decidir la compensación óptima entre las respuestas de consulta en caché y las listas de publicación de almacenamiento en caché.6.1 Modelo analítico Sea m del tamaño del \"caché\" medido en las unidades de respuestas (el \"caché\" puede almacenar m respuestas de consulta).",
                "Consideramos los siguientes dos casos: (a) un \"caché\" que almacena solo respuestas precomputadas, y (b) un \"caché\" que almacena solo las listas de publicación.",
                "En el primer caso, NC = M respuestas se ajustan en el \"caché\", mientras que en el segundo caso np = m/l de publicaciones de publicaciones se ajustan en el \"caché\".",
                "Para el caso (a), suponga que una respuesta de consulta en el \"caché\" puede evaluarse en 1 unidad de tiempo.",
                "Para el caso (b), suponga que si las listas de publicación de los términos de una consulta están en el \"caché\", los resultados se pueden calcular en unidades de tiempo TR1, mientras que si las listas de publicación no están en el \"caché\", entonces los resultadosse puede calcular en unidades de tiempo TR2.",
                "En el peor de los casos, para un gran \"caché\", β → 1.",
                "Es decir, ambas técnicas \"almacenan\" una fracción constante del volumen general de consulta.",
                "Para un pequeño \"caché\", estamos interesados en el comportamiento transitorio y luego β> 1, según lo calculado a partir de nuestros datos.",
                "En realidad, en lugar de llenar el \"caché\" solo con respuestas o solo con listas de publicación, una mejor estrategia será dividir el espacio total de \"caché\" en caché para respuestas y caché para publicar listas.",
                "En tal caso, habrá algunas consultas que podrían ser respondidas por ambas partes del \"caché\".",
                "Como la respuesta \"caché\" es más rápida, será la primera opción para responder esas consultas.",
                "Encontrar la división óptima del \"caché\" para minimizar el tiempo de recuperación general es un problema difícil de resolver analíticamente.",
                "En la Sección 6.3 usamos simulaciones para obtener compensaciones óptimas de \"caché\" para ejemplos de implementación particulares.6.2 Estimación de parámetros Ahora utilizamos una implementación particular de un sistema centralizado y el modelo de un sistema distribuido como ejemplos de los cuales estimamos los parámetros del análisis de la sección anterior.",
                "Comprimimos las brechas del identificador de documento usando la codificación de Elias gamma, y el 0.1 0.2 0.2 0.3 0.4 0.5 0.6 0.7 0.7 0.8 0.9 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1 Espacio de consulta Respuestas precomputadas Publicado Listas Figura 9: \"Cache\" Saturación como función como funciónde tamaño.",
                "Utilizando una parte del registro de consultas como datos de capacitación, primero asignamos en el \"caché\" las respuestas a las consultas más frecuentes que se ajustan al espacio X, y luego usamos el resto de la memoria para \"caché\" listas de publicación.",
                "En este caso, la cantidad total de memoria se divide entre el corredor, que contiene el almacenamiento de trabajo simulado 400 500 600 700 800 900 1000 1100 1200 0 0.2 0.4 0.6 0.8 1 1 1G Parcial / UNCOMPR / 1 g completo / Compr / 0.5 g Parcial / Compr / 0.5 g Figura 11: División óptima del \"caché\" en un servidor.3000 3500 4000 4500 5000 5500 6000 0 0.2 0.4 0.6 0.8 1 AveragerSponsetEment Space (GB) Carga de trabajo simulada - WAN Full / Uncomppr / 1 G Parcial / UncompR / 1 G Full / CompR / 0.5 G Parcial / Compr / 0.5 g Figura 12:División óptima del \"caché\" cuando el siguiente nivel requiere acceso WAN.Respuestas de consultas y los procesadores de consulta, que contienen el caché de las listas de publicación.",
                "Esto se explica por el hecho de que no hay necesidad de comunicación de red cuando la consulta puede ser respondida por el \"caché\" en el corredor.7.",
                "Luego calculamos la tasa de aciertos de un \"caché\" estático de 128, 000 respuestas entrenadas durante un período de dos semanas (Figura 13).",
                "Después de una pequeña descomposición en los valores de la tasa de aciertos, la tasa de aciertos se estabiliza entre 0.28 y 0.34 durante toda la semana, lo que sugiere que el \"caché\" estático es efectivo durante toda una semana después del período de entrenamiento.0.26 0.27 0.28 0.29 0.3 0.31 0.32 0.33 0.34 0.35 0.36 0.37 0 20 40 60 60 80 100 120 140 160 Hits de tiempo de tasas de golpe en las consultas frecuentes de distancias Figura 13: Tasa de aciertos por hora para un \"caché\" estático que contiene 128,000 respuestas durante el período durante el períodode una semana.",
                "El \"caché\" estático de las listas de publicación se puede recomputar periódicamente.",
                "Para estimar el intervalo de tiempo en el que necesitamos recomputar las listas de publicación en el \"caché\" estático, debemos considerar una compensación de eficiencia/calidad: usar un intervalo de tiempo demasiado corto podría ser prohibitivamente costoso, al tiempo que recomputamos el \"caché\" tambiénLa con poca frecuencia podría llevar a tener un caché obsoleto que no se corresponda con las características estadísticas de la corriente de consulta actual.",
                "Calculamos las frecuencias del término de consulta en toda la transmisión, seleccionamos qué términos \"caché\" y luego calculamos la tasa de aciertos en toda la corriente de consulta.",
                "Nuestros resultados muestran que para los entornos centralizados y LAN, existe una asignación óptima de los resultados de la consulta de almacenamiento en caché y el almacenamiento en caché de las listas de publicación, mientras que para los escenarios WAN en los que prevalece el tiempo de red, es más importante \"almacenarse\" los resultados de la consulta.0.45 0.5 0.55 0.6 0.65 0.7 0.75 0.8 0.85 0.9 0.95 0.1 0.2 0.3 0.4 0.5 0.5 0.6 0.7 Dinámica de tamaño \"caché\" de la política de almacenamiento de caché estático Capacitación de 6 semanas 3 semanas Figura 14: Impacto de los cambios de distribución en los cambios de distribución en elalmacenamiento en caché estático de listas de publicación.9."
            ],
            "translated_text": "",
            "candidates": [
                "cache",
                "caché",
                "cache",
                "caché",
                "cache",
                "caché",
                "cache",
                "almacenar en caché",
                "cache",
                "caché",
                "cache",
                "caché",
                "cache",
                "caché",
                "caché",
                "cache",
                "caché",
                "cache",
                "caché",
                "cache",
                "caché",
                "cache",
                "almacenar en caché",
                "cache",
                "caché",
                "cache",
                "caché",
                "cache",
                "caché",
                "cache",
                "caché",
                "cache",
                "caché",
                "cache",
                "caché",
                "cache",
                "caché",
                "cache",
                "caché",
                "cache",
                "caché",
                "cache",
                "caché",
                "cache",
                "caché",
                "cache",
                "almacenar en caché",
                "cache",
                "caché",
                "cache",
                "caché",
                "Cache",
                "cache",
                "caché",
                "caché",
                "cache",
                "caché",
                "cache",
                "caché",
                "caché",
                "cache",
                "caché",
                "caché",
                "cache",
                "caché",
                "cache",
                "caché",
                "cache",
                "caché",
                "cache",
                "caché",
                "cache",
                "caché",
                "cache",
                "caché",
                "cache",
                "caché",
                "cache",
                "caché",
                "cache",
                "caché",
                "cache",
                "caché",
                "caché",
                "cache",
                "Cache",
                "cache",
                "caché",
                "caché",
                "caché",
                "cache",
                "caché",
                "caché",
                "cache",
                "caché",
                "caché",
                "cache",
                "caché",
                "caché",
                "cache",
                "caché",
                "caché",
                "cache",
                "caché",
                "cache",
                "caché",
                "caché",
                "caché",
                "caché",
                "cache",
                "almacenan",
                "cache",
                "caché",
                "cache",
                "caché",
                "caché",
                "cache",
                "caché",
                "cache",
                "caché",
                "cache",
                "caché",
                "cache",
                "caché",
                "cache",
                "Cache",
                "cache",
                "caché",
                "caché",
                "cache",
                "caché",
                "caché",
                "Cache",
                "caché",
                "cache",
                "caché",
                "cache",
                "caché",
                "caché",
                "cache",
                "caché",
                "cache",
                "caché",
                "caché",
                "cache",
                "caché",
                "caché",
                "almacenarse",
                "caché"
            ],
            "error": []
        },
        "web search": {
            "translated_key": "búsqueda Web",
            "is_in_text": true,
            "original_annotated_sentences": [
                "The Impact of Caching on Search Engines Ricardo Baeza-Yates1 rbaeza@acm.org Aristides Gionis1 gionis@yahoo-inc.com Flavio Junqueira1 fpj@yahoo-inc.com Vanessa Murdock1 vmurdock@yahoo-inc.com Vassilis Plachouras1 vassilis@yahoo-inc.com Fabrizio Silvestri2 f.silvestri@isti.cnr.it 1 Yahoo!",
                "Research Barcelona 2 ISTI - CNR Barcelona, SPAIN Pisa, ITALY ABSTRACT In this paper we study the trade-offs in designing efficient caching systems for <br>web search</br> engines.",
                "We explore the impact of different approaches, such as static vs. dynamic caching, and caching query results vs. caching posting lists.",
                "Using a query log spanning a whole year we explore the limitations of caching and we demonstrate that caching posting lists can achieve higher hit rates than caching query answers.",
                "We propose a new algorithm for static caching of posting lists, which outperforms previous methods.",
                "We also study the problem of finding the optimal way to split the static cache between answers and posting lists.",
                "Finally, we measure how the changes in the query log affect the effectiveness of static caching, given our observation that the distribution of the queries changes slowly over time.",
                "Our results and observations are applicable to different levels of the data-access hierarchy, for instance, for a memory/disk layer or a broker/remote server layer.",
                "Categories and Subject Descriptors H.3.3 [Information Storage and Retrieval]: Information Search and Retrieval - Search process; H.3.4 [Information Storage and Retrieval]: Systems and Software - Distributed systems, Performance evaluation (efficiency and effectiveness) General Terms Algorithms, Experimentation 1.",
                "INTRODUCTION Millions of queries are submitted daily to <br>web search</br> engines, and users have high expectations of the quality and speed of the answers.",
                "As the searchable Web becomes larger and larger, with more than 20 billion pages to index, evaluating a single query requires processing large amounts of data.",
                "In such a setting, to achieve a fast response time and to increase the query throughput, using a cache is crucial.",
                "The primary use of a cache memory is to speedup computation by exploiting frequently or recently used data, although reducing the workload to back-end servers is also a major goal.",
                "Caching can be applied at different levels with increasing response latencies or processing requirements.",
                "For example, the different levels may correspond to the main memory, the disk, or resources in a local or a wide area network.",
                "The decision of what to cache is either off-line (static) or online (dynamic).",
                "A static cache is based on historical information and is periodically updated.",
                "A dynamic cache replaces entries according to the sequence of requests.",
                "When a new request arrives, the cache system decides whether to evict some entry from the cache in the case of a cache miss.",
                "Such online decisions are based on a cache policy, and several different policies have been studied in the past.",
                "For a search engine, there are two possible ways to use a cache memory: Caching answers: As the engine returns answers to a particular query, it may decide to store these answers to resolve future queries.",
                "Caching terms: As the engine evaluates a particular query, it may decide to store in memory the posting lists of the involved query terms.",
                "Often the whole set of posting lists does not fit in memory, and consequently, the engine has to select a small set to keep in memory and speed up query processing.",
                "Returning an answer to a query that already exists in the cache is more efficient than computing the answer using cached posting lists.",
                "On the other hand, previously unseen queries occur more often than previously unseen terms, implying a higher miss rate for cached answers.",
                "Caching of posting lists has additional challenges.",
                "As posting lists have variable size, caching them dynamically is not very efficient, due to the complexity in terms of efficiency and space, and the skewed distribution of the query stream, as shown later.",
                "Static caching of posting lists poses even more challenges: when deciding which terms to cache one faces the trade-off between frequently queried terms and terms with small posting lists that are space efficient.",
                "Finally, before deciding to adopt a static caching policy the query stream should be analyzed to verify that its characteristics do not change rapidly over time.",
                "Broker Static caching posting lists Dynamic/Static cached answers Local query processor Disk Next caching level Local network access Remote network access Figure 1: One caching level in a distributed search architecture.",
                "In this paper we explore the trade-offs in the design of each cache level, showing that the problem is the same and only a few parameters change.",
                "In general, we assume that each level of caching in a distributed search architecture is similar to that shown in Figure 1.",
                "We use a query log spanning a whole year to explore the limitations of dynamically caching query answers or posting lists for query terms.",
                "More concretely, our main conclusions are that: • Caching query answers results in lower hit ratios compared to caching of posting lists for query terms, but it is faster because there is no need for query evaluation.",
                "We provide a framework for the analysis of the trade-off between static caching of query answers and posting lists; • Static caching of terms can be more effective than dynamic caching with, for example, LRU.",
                "We provide algorithms based on the Knapsack problem for selecting the posting lists to put in a static cache, and we show improvements over previous work, achieving a hit ratio over 90%; • Changes of the query distribution over time have little impact on static caching.",
                "The remainder of this paper is organized as follows.",
                "Sections 2 and 3 summarize related work and characterize the data sets we use.",
                "Section 4 discusses the limitations of dynamic caching.",
                "Sections 5 and 6 introduce algorithms for caching posting lists, and a theoretical framework for the analysis of static caching, respectively.",
                "Section 7 discusses the impact of changes in the query distribution on static caching, and Section 8 provides concluding remarks. 2.",
                "RELATED WORK There is a large body of work devoted to query optimization.",
                "Buckley and Lewit [3], in one of the earliest works, take a term-at-a-time approach to deciding when inverted lists need not be further examined.",
                "More recent examples demonstrate that the top k documents for a query can be returned without the need for evaluating the complete set of posting lists [1, 4, 15].",
                "Although these approaches seek to improve query processing efficiency, they differ from our current work in that they do not consider caching.",
                "They may be considered separate and complementary to a cache-based approach.",
                "Raghavan and Sever [12], in one of the first papers on exploiting user query history, propose using a query base, built upon a set of persistent optimal queries submitted in the past, to improve the retrieval effectiveness for similar future queries.",
                "Markatos [10] shows the existence of temporal locality in queries, and compares the performance of different caching policies.",
                "Based on the observations of Markatos, Lempel and Moran propose a new caching policy, called Probabilistic Driven Caching, by attempting to estimate the probability distribution of all possible queries submitted to a search engine [8].",
                "Fagni et al. follow Markatos work by showing that combining static and dynamic caching policies together with an adaptive prefetching policy achieves a high hit ratio [7].",
                "Different from our work, they consider caching and prefetching of pages of results.",
                "As systems are often hierarchical, there has also been some effort on multi-level architectures.",
                "Saraiva et al. propose a new architecture for <br>web search</br> engines using a two-level dynamic caching system [13].",
                "Their goal for such systems has been to improve response time for hierarchical engines.",
                "In their architecture, both levels use an LRU eviction policy.",
                "They find that the second-level cache can effectively reduce disk traffic, thus increasing the overall throughput.",
                "Baeza-Yates and Saint-Jean propose a three-level index organization [2].",
                "Long and Suel propose a caching system structured according to three different levels [9].",
                "The intermediate level contains frequently occurring pairs of terms and stores the intersections of the corresponding inverted lists.",
                "These last two papers are related to ours in that they exploit different caching strategies at different levels of the memory hierarchy.",
                "Finally, our static caching algorithm for posting lists in Section 5 uses the ratio frequency/size in order to evaluate the goodness of an item to cache.",
                "Similar ideas have been used in the context of file caching [17], Web caching [5], and even caching of posting lists [9], but in all cases in a dynamic setting.",
                "To the best of our knowledge we are the first to use this approach for static caching of posting lists. 3.",
                "DATA CHARACTERIZATION Our data consists of a crawl of documents from the UK domain, and query logs of one year of queries submitted to http://www.yahoo.co.uk from November 2005 to November 2006.",
                "In our logs, 50% of the total volume of queries are unique.",
                "The average query length is 2.5 terms, with the longest query having 731 terms. 1e-07 1e-06 1e-05 1e-04 0.001 0.01 0.1 1 1e-08 1e-07 1e-06 1e-05 1e-04 0.001 0.01 0.1 1 Frequency(normalized) Frequency rank (normalized) Figure 2: The distribution of queries (bottom curve) and query terms (middle curve) in the query log.",
                "The distribution of document frequencies of terms in the UK-2006 dataset (upper curve).",
                "Figure 2 shows the distributions of queries (lower curve), and query terms (middle curve).",
                "The x-axis represents the normalized frequency rank of the query or term. (The most frequent query appears closest to the y-axis.)",
                "The y-axis is Table 1: Statistics of the UK-2006 sample.",
                "UK-2006 sample statistics # of documents 2,786,391 # of terms 6,491,374 # of tokens 2,109,512,558 the normalized frequency for a given query (or term).",
                "As expected, the distribution of query frequencies and query term frequencies follow power law distributions, with slope of 1.84 and 2.26, respectively.",
                "In this figure, the query frequencies were computed as they appear in the logs with no normalization for case or white space.",
                "The query terms (middle curve) have been normalized for case, as have the terms in the document collection.",
                "The document collection that we use for our experiments is a summary of the UK domain crawled in May 2006.1 This summary corresponds to a maximum of 400 crawled documents per host, using a breadth first crawling strategy, comprising 15GB.",
                "The distribution of document frequencies of terms in the collection follows a power law distribution with slope 2.38 (upper curve in Figure 2).",
                "The statistics of the collection are shown in Table 1.",
                "We measured the correlation between the document frequency of terms in the collection and the number of queries that contain a particular term in the query log to be 0.424.",
                "A scatter plot for a random sample of terms is shown in Figure 3.",
                "In this experiment, terms have been converted to lower case in both the queries and the documents so that the frequencies will be comparable. 1e-07 1e-06 1e-05 1e-04 0.001 0.01 0.1 1 1e-06 1e-05 1e-04 0.001 0.01 0.1 1 Queryfrequency Document frequency Figure 3: Normalized scatter plot of document-term frequencies vs. query-term frequencies. 4.",
                "CACHING OF QUERIES AND TERMS Caching relies upon the assumption that there is locality in the stream of requests.",
                "That is, there must be sufficient repetition in the stream of requests and within intervals of time that enable a cache memory of reasonable size to be effective.",
                "In the query log we used, 88% of the unique queries are singleton queries, and 44% are singleton queries out of the whole volume.",
                "Thus, out of all queries in the stream composing the query log, the upper threshold on hit ratio is 56%.",
                "This is because only 56% of all the queries comprise queries that have multiple occurrences.",
                "It is important to observe, however, that not all queries in this 56% can be cache hits because of compulsory misses.",
                "A compulsory miss 1 The collection is available from the University of Milan: http://law.dsi.unimi.it/.",
                "URL retrieved 05/2007. 0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1 240 260 280 300 320 340 360 Numberofelements Bin number Total terms Terms diff Total queries Unique queries Unique terms Query diff Figure 4: Arrival rate for both terms and queries. happens when the cache receives a query for the first time.",
                "This is different from capacity misses, which happen due to space constraints on the amount of memory the cache uses.",
                "If we consider a cache with infinite memory, then the hit ratio is 50%.",
                "Note that for an infinite cache there are no capacity misses.",
                "As we mentioned before, another possibility is to cache the posting lists of terms.",
                "Intuitively, this gives more freedom in the utilization of the cache content to respond to queries because cached terms might form a new query.",
                "On the other hand, they need more space.",
                "As opposed to queries, the fraction of singleton terms in the total volume of terms is smaller.",
                "In our query log, only 4% of the terms appear once, but this accounts for 73% of the vocabulary of query terms.",
                "We show in Section 5 that caching a small fraction of terms, while accounting for terms appearing in many documents, is potentially very effective.",
                "Figure 4 shows several graphs corresponding to the normalized arrival rate for different cases using days as bins.",
                "That is, we plot the normalized number of elements that appear in a day.",
                "This graph shows only a period of 122 days, and we normalize the values by the maximum value observed throughout the whole period of the query log.",
                "Total queries and Total terms correspond to the total volume of queries and terms, respectively.",
                "Unique queries and Unique terms correspond to the arrival rate of unique queries and terms.",
                "Finally, Query diff and Terms diff correspond to the difference between the curves for total and unique.",
                "In Figure 4, as expected, the volume of terms is much higher than the volume of queries.",
                "The difference between the total number of terms and the number of unique terms is much larger than the difference between the total number of queries and the number of unique queries.",
                "This observation implies that terms repeat significantly more than queries.",
                "If we use smaller bins, say of one hour, then the ratio of unique to volume is higher for both terms and queries because it leaves less room for repetition.",
                "We also estimated the workload using the document frequency of terms as a measure of how much work a query imposes on a search engine.",
                "We found that it follows closely the arrival rate for terms shown in Figure 4.",
                "To demonstrate the effect of a dynamic cache on the query frequency distribution of Figure 2, we plot the same frequency graph, but now considering the frequency of queries Figure 5: Frequency graph after LRU cache. after going through an LRU cache.",
                "On a cache miss, an LRU cache decides upon an entry to evict using the information on the recency of queries.",
                "In this graph, the most frequent queries are not the same queries that were most frequent before the cache.",
                "It is possible that queries that are most frequent after the cache have different characteristics, and tuning the search engine to queries frequent before the cache may degrade performance for non-cached queries.",
                "The maximum frequency after caching is less than 1% of the maximum frequency before the cache, thus showing that the cache is very effective in reducing the load of frequent queries.",
                "If we re-rank the queries according to after-cache frequency, the distribution is still a power law, but with a much smaller value for the highest frequency.",
                "When discussing the effectiveness of dynamically caching, an important metric is cache miss rate.",
                "To analyze the cache miss rate for different memory constraints, we use the working set model [6, 14].",
                "A working set, informally, is the set of references that an application or an operating system is currently working with.",
                "The model uses such sets in a strategy that tries to capture the temporal locality of references.",
                "The working set strategy then consists in keeping in memory only the elements that are referenced in the previous θ steps of the input sequence, where θ is a configurable parameter corresponding to the window size.",
                "Originally, working sets have been used for page replacement algorithms of operating systems, and considering such a strategy in the context of search engines is interesting for three reasons.",
                "First, it captures the amount of locality of queries and terms in a sequence of queries.",
                "Locality in this case refers to the frequency of queries and terms in a window of time.",
                "If many queries appear multiple times in a window, then locality is high.",
                "Second, it enables an oﬄine analysis of the expected miss rate given different memory constraints.",
                "Third, working sets capture aspects of efficient caching algorithms such as LRU.",
                "LRU assumes that references farther in the past are less likely to be referenced in the present, which is implicit in the concept of working sets [14].",
                "Figure 6 plots the miss rate for different working set sizes, and we consider working sets of both queries and terms.",
                "The working set sizes are normalized against the total number of queries in the query log.",
                "In the graph for queries, there is a sharp decay until approximately 0.01, and the rate at which the miss rate drops decreases as we increase the size of the working set over 0.01.",
                "Finally, the minimum value it reaches is 50% miss rate, not shown in the figure as we have cut the tail of the curve for presentation purposes. 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1 0 0.05 0.1 0.15 0.2 Missrate Normalized working set size Queries Terms Figure 6: Miss rate as a function of the working set size. 1 10 100 1000 10000 100000 1e+06 Frequency Distance Figure 7: Distribution of distances expressed in terms of distinct queries.",
                "Compared to the query curve, we observe that the minimum miss rate for terms is substantially smaller.",
                "The miss rate also drops sharply on values up to 0.01, and it decreases minimally for higher values.",
                "The minimum value, however, is slightly over 10%, which is much smaller than the minimum value for the sequence of queries.",
                "This implies that with such a policy it is possible to achieve over 80% hit rate, if we consider caching dynamically posting lists for terms as opposed to caching answers for queries.",
                "This result does not consider the space required for each unit stored in the cache memory, or the amount of time it takes to put together a response to a user query.",
                "We analyze these issues more carefully later in this paper.",
                "It is interesting also to observe the histogram of Figure 7, which is an intermediate step in the computation of the miss rate graph.",
                "It reports the distribution of distances between repetitions of the same frequent query.",
                "The distance in the plot is measured in the number of distinct queries separating a query and its repetition, and it considers only queries appearing at least 10 times.",
                "From Figures 6 and 7, we conclude that even if we set the size of the query answers cache to a relatively large number of entries, the miss rate is high.",
                "Thus, caching the posting lists of terms has the potential to improve the hit ratio.",
                "This is what we explore next. 5.",
                "CACHING POSTING LISTS The previous section shows that caching posting lists can obtain a higher hit rate compared to caching query answers.",
                "In this section we study the problem of how to select posting lists to place on a certain amount of available memory, assuming that the whole index is larger than the amount of memory available.",
                "The posting lists have variable size (in fact, their size distribution follows a power law), so it is beneficial for a caching policy to consider the sizes of the posting lists.",
                "We consider both dynamic and static caching.",
                "For dynamic caching, we use two well-known policies, LRU and LFU, as well as a modified algorithm that takes posting-list size into account.",
                "Before discussing the static caching strategies, we introduce some notation.",
                "We use fq(t) to denote the query-term frequency of a term t, that is, the number of queries containing t in the query log, and fd(t) to denote the document frequency of t, that is, the number of documents in the collection in which the term t appears.",
                "The first strategy we consider is the algorithm proposed by Baeza-Yates and Saint-Jean [2], which consists in selecting the posting lists of the terms with the highest query-term frequencies fq(t).",
                "We call this algorithm Qtf.",
                "We observe that there is a trade-off between fq(t) and fd(t).",
                "Terms with high fq(t) are useful to keep in the cache because they are queried often.",
                "On the other hand, terms with high fd(t) are not good candidates because they correspond to long posting lists and consume a substantial amount of space.",
                "In fact, the problem of selecting the best posting lists for the static cache corresponds to the standard Knapsack problem: given a knapsack of fixed capacity, and a set of n items, such as the i-th item has value ci and size si, select the set of items that fit in the knapsack and maximize the overall value.",
                "In our case, value corresponds to fq(t) and size corresponds to fd(t).",
                "Thus, we employ a simple algorithm for the knapsack problem, which is selecting the posting lists of the terms with the highest values of the ratio fq(t) fd(t) .",
                "We call this algorithm QtfDf.",
                "We tried other variations considering query frequencies instead of term frequencies, but the gain was minimal compared to the complexity added.",
                "In addition to the above two static algorithms we consider the following algorithms for dynamic caching: • LRU: A standard LRU algorithm, but many posting lists might need to be evicted (in order of least-recent usage) until there is enough space in the memory to place the currently accessed posting list; • LFU: A standard LFU algorithm (eviction of the leastfrequently used), with the same modification as the LRU; • Dyn-QtfDf: A dynamic version of the QtfDf algorithm; evict from the cache the term(s) with the lowest fq(t) fd(t) ratio.",
                "The performance of all the above algorithms for 15 weeks of the query log and the UK dataset are shown in Figure 8.",
                "Performance is measured with hit rate.",
                "The cache size is measured as a fraction of the total space required to store the posting lists of all terms.",
                "For the dynamic algorithms, we load the cache with terms in order of fq(t) and we let the cache warm up for 1 million queries.",
                "For the static algorithms, we assume complete knowledge of the frequencies fq(t), that is, we estimate fq(t) from the whole query stream.",
                "As we show in Section 7 the results do not change much if we compute the query-term frequencies using the first 3 or 4 weeks of the query log and measure the hit rate on the rest. 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1 0.1 0.2 0.3 0.4 0.5 0.6 0.7 Hitrate Cache size Caching posting lists static QTF/DF LRU LFU Dyn-QTF/DF QTF Figure 8: Hit rate of different strategies for caching posting lists.",
                "The most important observation from our experiments is that the static QtfDf algorithm has a better hit rate than all the dynamic algorithms.",
                "An important benefit a static cache is that it requires no eviction and it is hence more efficient when evaluating queries.",
                "However, if the characteristics of the query traffic change frequently over time, then it requires re-populating the cache often or there will be a significant impact on hit rate. 6.",
                "ANALYSIS OF STATIC CACHING In this section we provide a detailed analysis for the problem of deciding whether it is preferable to cache query answers or cache posting lists.",
                "Our analysis takes into account the impact of caching between two levels of the data-access hierarchy.",
                "It can either be applied at the memory/disk layer or at a server/remote server layer as in the architecture we discussed in the introduction.",
                "Using a particular system model, we obtain estimates for the parameters required by our analysis, which we subsequently use to decide the optimal trade-off between caching query answers and caching posting lists. 6.1 Analytical Model Let M be the size of the cache measured in answer units (the cache can store M query answers).",
                "Assume that all posting lists are of the same length L, measured in answer units.",
                "We consider the following two cases: (A) a cache that stores only precomputed answers, and (B) a cache that stores only posting lists.",
                "In the first case, Nc = M answers fit in the cache, while in the second case Np = M/L posting lists fit in the cache.",
                "Thus, Np = Nc/L.",
                "Note that although posting lists require more space, we can combine terms to evaluate more queries (or partial queries).",
                "For case (A), suppose that a query answer in the cache can be evaluated in 1 time unit.",
                "For case (B), assume that if the posting lists of the terms of a query are in the cache then the results can be computed in TR1 time units, while if the posting lists are not in the cache then the results can be computed in TR2 time units.",
                "Of course TR2 > TR1.",
                "Now we want to compare the time to answer a stream of Q queries in both cases.",
                "Let Vc(Nc) be the volume of the most frequent Nc queries.",
                "Then, for case (A), we have an overall time TCA = Vc(Nc) + TR2(Q − Vc(Nc)).",
                "Similarly, for case (B), let Vp(Np) be the number of computable queries.",
                "Then we have overall time TP L = TR1Vp(Np) + TR2(Q − Vp(Np)).",
                "We want to check under which conditions we have TP L < TCA.",
                "We have TP L − TCA = (TR2 − 1)Vc(Nc) − (TR2 − TR1)Vp(Np) > 0.",
                "Figure 9 shows the values of Vp and Vc for our data.",
                "We can see that caching answers saturates faster and for this particular data there is no additional benefit from using more than 10% of the index space for caching answers.",
                "As the query distribution is a power law with parameter α > 1, the i-th most frequent query appears with probability proportional to 1 iα .",
                "Therefore, the volume Vc(n), which is the total number of the n most frequent queries, is Vc(n) = V0 n i=1 Q iα = γnQ (0 < γn < 1).",
                "We know that Vp(n) grows faster than Vc(n) and assume, based on experimental results, that the relation is of the form Vp(n) = k Vc(n)β .",
                "In the worst case, for a large cache, β → 1.",
                "That is, both techniques will cache a constant fraction of the overall query volume.",
                "Then caching posting lists makes sense only if L(TR2 − 1) k(TR2 − TR1) > 1.",
                "If we use compression, we have L < L and TR1 > TR1.",
                "According to the experiments that we show later, compression is always better.",
                "For a small cache, we are interested in the transient behavior and then β > 1, as computed from our data.",
                "In this case there will always be a point where TP L > TCA for a large number of queries.",
                "In reality, instead of filling the cache only with answers or only with posting lists, a better strategy will be to divide the total cache space into cache for answers and cache for posting lists.",
                "In such a case, there will be some queries that could be answered by both parts of the cache.",
                "As the answer cache is faster, it will be the first choice for answering those queries.",
                "Let QNc and QNp be the set of queries that can be answered by the cached answers and the cached posting lists, respectively.",
                "Then, the overall time is T = Vc(Nc)+TR1V (QNp −QNc )+TR2(Q−V (QNp ∪QNc )), where Np = (M − Nc)/L.",
                "Finding the optimal division of the cache in order to minimize the overall retrieval time is a difficult problem to solve analytically.",
                "In Section 6.3 we use simulations to derive optimal cache trade-offs for particular implementation examples. 6.2 Parameter Estimation We now use a particular implementation of a centralized system and the model of a distributed system as examples from which we estimate the parameters of the analysis from the previous section.",
                "We perform the experiments using an optimized version of Terrier [11] for both indexing documents and processing queries, on a single machine with a Pentium 4 at 2GHz and 1GB of RAM.",
                "We indexed the documents from the UK-2006 dataset, without removing stop words or applying stemming.",
                "The posting lists in the inverted file consist of pairs of document identifier and term frequency.",
                "We compress the document identifier gaps using Elias gamma encoding, and the 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1 0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1 Queryvolume Space precomputed answers posting lists Figure 9: Cache saturation as a function of size.",
                "Table 2: Ratios between the average time to evaluate a query and the average time to return cached answers (centralized and distributed case).",
                "Centralized system TR1 TR2 TR1 TR2 Full evaluation 233 1760 707 1140 Partial evaluation 99 1626 493 798 LAN system TRL 1 TRL 2 TR L 1 TR L 2 Full evaluation 242 1769 716 1149 Partial evaluation 108 1635 502 807 WAN system TRW 1 TRW 2 TR W 1 TR W 2 Full evaluation 5001 6528 5475 5908 Partial evaluation 4867 6394 5270 5575 term frequencies in documents using unary encoding [16].",
                "The size of the inverted file is 1,189Mb.",
                "A stored answer requires 1264 bytes, and an uncompressed posting takes 8 bytes.",
                "From Table 1, we obtain L = (8·# of postings) 1264·# of terms = 0.75 and L = Inverted file size 1264·# of terms = 0.26.",
                "We estimate the ratio TR = T/Tc between the average time T it takes to evaluate a query and the average time Tc it takes to return a stored answer for the same query, in the following way.",
                "Tc is measured by loading the answers for 100,000 queries in memory, and answering the queries from memory.",
                "The average time is Tc = 0.069ms.",
                "T is measured by processing the same 100,000 queries (the first 10,000 queries are used to warm-up the system).",
                "For each query, we remove stop words, if there are at least three remaining terms.",
                "The stop words correspond to the terms with a frequency higher than the number of documents in the index.",
                "We use a document-at-a-time approach to retrieve documents containing all query terms.",
                "The only disk access required during query processing is for reading compressed posting lists from the inverted file.",
                "We perform both full and partial evaluation of answers, because some queries are likely to retrieve a large number of documents, and only a fraction of the retrieved documents will be seen by users.",
                "In the partial evaluation of queries, we terminate the processing after matching 10,000 documents.",
                "The estimated ratios TR are presented in Table 2.",
                "Figure 10 shows for a sample of queries the workload of the system with partial query evaluation and compressed posting lists.",
                "The x-axis corresponds to the total time the system spends processing a particular query, and the vertical axis corresponds to the sum t∈q fq · fd(t).",
                "Notice that the total number of postings of the query-terms does not necessarily provide an accurate estimate of the workload imposed on the system by a query (which is the case for full evaluation and uncompressed lists). 0 0.2 0.4 0.6 0.8 1 0 0.2 0.4 0.6 0.8 1 Totalpostingstoprocessquery(normalized) Total time to process query (normalized) Partial processing of compressed postings query len = 1 query len in [2,3] query len in [4,8] query len > 8 Figure 10: Workload for partial query evaluation with compressed posting lists.",
                "The analysis of the previous section also applies to a distributed retrieval system in one or multiple sites.",
                "Suppose that a document partitioned distributed system is running on a cluster of machines interconnected with a local area network (LAN) in one site.",
                "The broker receives queries and broadcasts them to the query processors, which answer the queries and return the results to the broker.",
                "Finally, the broker merges the received answers and generates the final set of answers (we assume that the time spent on merging results is negligible).",
                "The difference between the centralized architecture and the document partition architecture is the extra communication between the broker and the query processors.",
                "Using ICMP pings on a 100Mbps LAN, we have measured that sending the query from the broker to the query processors which send an answer of 4,000 bytes back to the broker takes on average 0.615ms.",
                "Hence, TRL = TR + 0.615ms/0.069ms = TR + 9.",
                "In the case when the broker and the query processors are in different sites connected with a wide area network (WAN), we estimated that broadcasting the query from the broker to the query processors and getting back an answer of 4,000 bytes takes on average 329ms.",
                "Hence, TRW = TR + 329ms/0.069ms = TR + 4768. 6.3 Simulation Results We now address the problem of finding the optimal tradeoff between caching query answers and caching posting lists.",
                "To make the problem concrete we assume a fixed budget M on the available memory, out of which x units are used for caching query answers and M − x for caching posting lists.",
                "We perform simulations and compute the average response time as a function of x.",
                "Using a part of the query log as training data, we first allocate in the cache the answers to the most frequent queries that fit in space x, and then we use the rest of the memory to cache posting lists.",
                "For selecting posting lists we use the QtfDf algorithm, applied to the training query log but excluding the queries that have already been cached.",
                "In Figure 11, we plot the simulated response time for a centralized system as a function of x.",
                "For the uncompressed index we use M = 1GB, and for the compressed index we use M = 0.5GB.",
                "In the case of the configuration that uses partial query evaluation with compressed posting lists, the lowest response time is achieved when 0.15GB out of the 0.5GB is allocated for storing answers for queries.",
                "We obtained similar trends in the results for the LAN setting.",
                "Figure 12 shows the simulated workload for a distributed system across a WAN.",
                "In this case, the total amount of memory is split between the broker, which holds the cached 400 500 600 700 800 900 1000 1100 1200 0 0.2 0.4 0.6 0.8 1 Averageresponsetime Space (GB) Simulated workload -- single machine full / uncompr / 1 G partial / uncompr / 1 G full / compr / 0.5 G partial / compr / 0.5 G Figure 11: Optimal division of the cache in a server. 3000 3500 4000 4500 5000 5500 6000 0 0.2 0.4 0.6 0.8 1 Averageresponsetime Space (GB) Simulated workload -- WAN full / uncompr / 1 G partial / uncompr / 1 G full / compr / 0.5 G partial / compr / 0.5 G Figure 12: Optimal division of the cache when the next level requires WAN access. answers of queries, and the query processors, which hold the cache of posting lists.",
                "According to the figure, the difference between the configurations of the query processors is less important because the network communication overhead increases the response time substantially.",
                "When using uncompressed posting lists, the optimal allocation of memory corresponds to using approximately 70% of the memory for caching query answers.",
                "This is explained by the fact that there is no need for network communication when the query can be answered by the cache at the broker. 7.",
                "EFFECT OF THE QUERY DYNAMICS For our query log, the query distribution and query-term distribution change slowly over time.",
                "To support this claim, we first assess how topics change comparing the distribution of queries from the first week in June, 2006, to the distribution of queries for the remainder of 2006 that did not appear in the first week in June.",
                "We found that a very small percentage of queries are new queries.",
                "The majority of queries that appear in a given week repeat in the following weeks for the next six months.",
                "We then compute the hit rate of a static cache of 128, 000 answers trained over a period of two weeks (Figure 13).",
                "We report hit rate hourly for 7 days, starting from 5pm.",
                "We observe that the hit rate reaches its highest value during the night (around midnight), whereas around 2-3pm it reaches its minimum.",
                "After a small decay in hit rate values, the hit rate stabilizes between 0.28, and 0.34 for the entire week, suggesting that the static cache is effective for a whole week after the training period. 0.26 0.27 0.28 0.29 0.3 0.31 0.32 0.33 0.34 0.35 0.36 0.37 0 20 40 60 80 100 120 140 160 Hit-rate Time Hits on the frequent queries of distances Figure 13: Hourly hit rate for a static cache holding 128,000 answers during the period of a week.",
                "The static cache of posting lists can be periodically recomputed.",
                "To estimate the time interval in which we need to recompute the posting lists on the static cache we need to consider an efficiency/quality trade-off: using too short a time interval might be prohibitively expensive, while recomputing the cache too infrequently might lead to having an obsolete cache not corresponding to the statistical characteristics of the current query stream.",
                "We measured the effect on the QtfDf algorithm of the changes in a 15-week query stream (Figure 14).",
                "We compute the query term frequencies over the whole stream, select which terms to cache, and then compute the hit rate on the whole query stream.",
                "This hit rate is as an upper bound, and it assumes perfect knowledge of the query term frequencies.",
                "To simulate a realistic scenario, we use the first 6 (3) weeks of the query stream for computing query term frequencies and the following 9 (12) weeks to estimate the hit rate.",
                "As Figure 14 shows, the hit rate decreases by less than 2%.",
                "The high correlation among the query term frequencies during different time periods explains the graceful adaptation of the static caching algorithms to the future query stream.",
                "Indeed, the pairwise correlation among all possible 3-week periods of the 15-week query stream is over 99.5%. 8.",
                "CONCLUSIONS Caching is an effective technique in search engines for improving response time, reducing the load on query processors, and improving network bandwidth utilization.",
                "We present results on both dynamic and static caching.",
                "Dynamic caching of queries has limited effectiveness due to the high number of compulsory misses caused by the number of unique or infrequent queries.",
                "Our results show that in our UK log, the minimum miss rate is 50% using a working set strategy.",
                "Caching terms is more effective with respect to miss rate, achieving values as low as 12%.",
                "We also propose a new algorithm for static caching of posting lists that outperforms previous static caching algorithms as well as dynamic algorithms such as LRU and LFU, obtaining hit rate values that are over 10% higher compared these strategies.",
                "We present a framework for the analysis of the trade-off between caching query results and caching posting lists, and we simulate different types of architectures.",
                "Our results show that for centralized and LAN environments, there is an optimal allocation of caching query results and caching of posting lists, while for WAN scenarios in which network time prevails it is more important to cache query results. 0.45 0.5 0.55 0.6 0.65 0.7 0.75 0.8 0.85 0.9 0.95 0.1 0.2 0.3 0.4 0.5 0.6 0.7 Hitrate Cache size Dynamics of static QTF/DF caching policy perfect knowledge 6-week training 3-week training Figure 14: Impact of distribution changes on the static caching of posting lists. 9.",
                "REFERENCES [1] V. N. Anh and A. Moffat.",
                "Pruned query evaluation using pre-computed impacts.",
                "In ACM CIKM, 2006. [2] R. A. Baeza-Yates and F. Saint-Jean.",
                "A three level search engine index based in query log distribution.",
                "In SPIRE, 2003. [3] C. Buckley and A. F. Lewit.",
                "Optimization of inverted vector searches.",
                "In ACM SIGIR, 1985. [4] S. B¨uttcher and C. L. A. Clarke.",
                "A document-centric approach to static index pruning in text retrieval systems.",
                "In ACM CIKM, 2006. [5] P. Cao and S. Irani.",
                "Cost-aware WWW proxy caching algorithms.",
                "In USITS, 1997. [6] P. Denning.",
                "Working sets past and present.",
                "IEEE Trans. on Software Engineering, SE-6(1):64-84, 1980. [7] T. Fagni, R. Perego, F. Silvestri, and S. Orlando.",
                "Boosting the performance of <br>web search</br> engines: Caching and prefetching query results by exploiting historical usage data.",
                "ACM Trans.",
                "Inf.",
                "Syst., 24(1):51-78, 2006. [8] R. Lempel and S. Moran.",
                "Predictive caching and prefetching of query results in search engines.",
                "In WWW, 2003. [9] X.",
                "Long and T. Suel.",
                "Three-level caching for efficient query processing in large <br>web search</br> engines.",
                "In WWW, 2005. [10] E. P. Markatos.",
                "On caching search engine query results.",
                "Computer Communications, 24(2):137-143, 2001. [11] I. Ounis, G. Amati, V. Plachouras, B.",
                "He, C. Macdonald, and C. Lioma.",
                "Terrier: A High Performance and Scalable Information Retrieval Platform.",
                "In SIGIR Workshop on Open Source Information Retrieval, 2006. [12] V. V. Raghavan and H. Sever.",
                "On the reuse of past optimal queries.",
                "In ACM SIGIR, 1995. [13] P. C. Saraiva, E. S. de Moura, N. Ziviani, W. Meira, R. Fonseca, and B. Riberio-Neto.",
                "Rank-preserving two-level caching for scalable search engines.",
                "In ACM SIGIR, 2001. [14] D. R. Slutz and I. L. Traiger.",
                "A note on the calculation of average working set size.",
                "Communications of the ACM, 17(10):563-565, 1974. [15] T. Strohman, H. Turtle, and W. B. Croft.",
                "Optimization strategies for complex queries.",
                "In ACM SIGIR, 2005. [16] I. H. Witten, T. C. Bell, and A. Moffat.",
                "Managing Gigabytes: Compressing and Indexing Documents and Images.",
                "John Wiley & Sons, Inc., NY, 1994. [17] N. E. Young.",
                "On-line file caching.",
                "Algorithmica, 33(3):371-383, 2002."
            ],
            "original_annotated_samples": [],
            "translated_annotated_samples": [
                "Investigación Barcelona 2 ISTI - CNR Barcelona, España Pisa, Italia Resumen En este documento Estudiamos las compensaciones en el diseño de sistemas de almacenamiento de almacenamiento eficientes para motores de \"búsqueda web\".",
                "Introducción Millones de consultas se envían diariamente a motores de \"búsqueda web\", y los usuarios tienen altas expectativas de la calidad y velocidad de las respuestas.",
                "Saraiva et al.Proponga una nueva arquitectura para los motores \"Búsqueda web\" utilizando un sistema de almacenamiento dinámico de dos niveles [13].",
                "Aumentando el rendimiento de los motores de \"búsqueda web\": almacenamiento en caché y captación de los resultados de consultas mediante la explotación de datos de uso histórico.",
                "El almacenamiento en caché de tres niveles para el procesamiento eficiente de consultas en motores grandes de \"búsqueda web\"."
            ],
            "translated_text": "",
            "candidates": [
                "búsqueda Web",
                "búsqueda web",
                "búsqueda Web",
                "búsqueda web",
                "búsqueda Web",
                "Búsqueda web",
                "búsqueda Web",
                "búsqueda web",
                "búsqueda Web",
                "búsqueda web"
            ],
            "error": []
        },
        "information retrieval system": {
            "translated_key": "sistema de recuperación de información",
            "is_in_text": false,
            "original_annotated_sentences": [
                "The Impact of Caching on Search Engines Ricardo Baeza-Yates1 rbaeza@acm.org Aristides Gionis1 gionis@yahoo-inc.com Flavio Junqueira1 fpj@yahoo-inc.com Vanessa Murdock1 vmurdock@yahoo-inc.com Vassilis Plachouras1 vassilis@yahoo-inc.com Fabrizio Silvestri2 f.silvestri@isti.cnr.it 1 Yahoo!",
                "Research Barcelona 2 ISTI - CNR Barcelona, SPAIN Pisa, ITALY ABSTRACT In this paper we study the trade-offs in designing efficient caching systems for Web search engines.",
                "We explore the impact of different approaches, such as static vs. dynamic caching, and caching query results vs. caching posting lists.",
                "Using a query log spanning a whole year we explore the limitations of caching and we demonstrate that caching posting lists can achieve higher hit rates than caching query answers.",
                "We propose a new algorithm for static caching of posting lists, which outperforms previous methods.",
                "We also study the problem of finding the optimal way to split the static cache between answers and posting lists.",
                "Finally, we measure how the changes in the query log affect the effectiveness of static caching, given our observation that the distribution of the queries changes slowly over time.",
                "Our results and observations are applicable to different levels of the data-access hierarchy, for instance, for a memory/disk layer or a broker/remote server layer.",
                "Categories and Subject Descriptors H.3.3 [Information Storage and Retrieval]: Information Search and Retrieval - Search process; H.3.4 [Information Storage and Retrieval]: Systems and Software - Distributed systems, Performance evaluation (efficiency and effectiveness) General Terms Algorithms, Experimentation 1.",
                "INTRODUCTION Millions of queries are submitted daily to Web search engines, and users have high expectations of the quality and speed of the answers.",
                "As the searchable Web becomes larger and larger, with more than 20 billion pages to index, evaluating a single query requires processing large amounts of data.",
                "In such a setting, to achieve a fast response time and to increase the query throughput, using a cache is crucial.",
                "The primary use of a cache memory is to speedup computation by exploiting frequently or recently used data, although reducing the workload to back-end servers is also a major goal.",
                "Caching can be applied at different levels with increasing response latencies or processing requirements.",
                "For example, the different levels may correspond to the main memory, the disk, or resources in a local or a wide area network.",
                "The decision of what to cache is either off-line (static) or online (dynamic).",
                "A static cache is based on historical information and is periodically updated.",
                "A dynamic cache replaces entries according to the sequence of requests.",
                "When a new request arrives, the cache system decides whether to evict some entry from the cache in the case of a cache miss.",
                "Such online decisions are based on a cache policy, and several different policies have been studied in the past.",
                "For a search engine, there are two possible ways to use a cache memory: Caching answers: As the engine returns answers to a particular query, it may decide to store these answers to resolve future queries.",
                "Caching terms: As the engine evaluates a particular query, it may decide to store in memory the posting lists of the involved query terms.",
                "Often the whole set of posting lists does not fit in memory, and consequently, the engine has to select a small set to keep in memory and speed up query processing.",
                "Returning an answer to a query that already exists in the cache is more efficient than computing the answer using cached posting lists.",
                "On the other hand, previously unseen queries occur more often than previously unseen terms, implying a higher miss rate for cached answers.",
                "Caching of posting lists has additional challenges.",
                "As posting lists have variable size, caching them dynamically is not very efficient, due to the complexity in terms of efficiency and space, and the skewed distribution of the query stream, as shown later.",
                "Static caching of posting lists poses even more challenges: when deciding which terms to cache one faces the trade-off between frequently queried terms and terms with small posting lists that are space efficient.",
                "Finally, before deciding to adopt a static caching policy the query stream should be analyzed to verify that its characteristics do not change rapidly over time.",
                "Broker Static caching posting lists Dynamic/Static cached answers Local query processor Disk Next caching level Local network access Remote network access Figure 1: One caching level in a distributed search architecture.",
                "In this paper we explore the trade-offs in the design of each cache level, showing that the problem is the same and only a few parameters change.",
                "In general, we assume that each level of caching in a distributed search architecture is similar to that shown in Figure 1.",
                "We use a query log spanning a whole year to explore the limitations of dynamically caching query answers or posting lists for query terms.",
                "More concretely, our main conclusions are that: • Caching query answers results in lower hit ratios compared to caching of posting lists for query terms, but it is faster because there is no need for query evaluation.",
                "We provide a framework for the analysis of the trade-off between static caching of query answers and posting lists; • Static caching of terms can be more effective than dynamic caching with, for example, LRU.",
                "We provide algorithms based on the Knapsack problem for selecting the posting lists to put in a static cache, and we show improvements over previous work, achieving a hit ratio over 90%; • Changes of the query distribution over time have little impact on static caching.",
                "The remainder of this paper is organized as follows.",
                "Sections 2 and 3 summarize related work and characterize the data sets we use.",
                "Section 4 discusses the limitations of dynamic caching.",
                "Sections 5 and 6 introduce algorithms for caching posting lists, and a theoretical framework for the analysis of static caching, respectively.",
                "Section 7 discusses the impact of changes in the query distribution on static caching, and Section 8 provides concluding remarks. 2.",
                "RELATED WORK There is a large body of work devoted to query optimization.",
                "Buckley and Lewit [3], in one of the earliest works, take a term-at-a-time approach to deciding when inverted lists need not be further examined.",
                "More recent examples demonstrate that the top k documents for a query can be returned without the need for evaluating the complete set of posting lists [1, 4, 15].",
                "Although these approaches seek to improve query processing efficiency, they differ from our current work in that they do not consider caching.",
                "They may be considered separate and complementary to a cache-based approach.",
                "Raghavan and Sever [12], in one of the first papers on exploiting user query history, propose using a query base, built upon a set of persistent optimal queries submitted in the past, to improve the retrieval effectiveness for similar future queries.",
                "Markatos [10] shows the existence of temporal locality in queries, and compares the performance of different caching policies.",
                "Based on the observations of Markatos, Lempel and Moran propose a new caching policy, called Probabilistic Driven Caching, by attempting to estimate the probability distribution of all possible queries submitted to a search engine [8].",
                "Fagni et al. follow Markatos work by showing that combining static and dynamic caching policies together with an adaptive prefetching policy achieves a high hit ratio [7].",
                "Different from our work, they consider caching and prefetching of pages of results.",
                "As systems are often hierarchical, there has also been some effort on multi-level architectures.",
                "Saraiva et al. propose a new architecture for Web search engines using a two-level dynamic caching system [13].",
                "Their goal for such systems has been to improve response time for hierarchical engines.",
                "In their architecture, both levels use an LRU eviction policy.",
                "They find that the second-level cache can effectively reduce disk traffic, thus increasing the overall throughput.",
                "Baeza-Yates and Saint-Jean propose a three-level index organization [2].",
                "Long and Suel propose a caching system structured according to three different levels [9].",
                "The intermediate level contains frequently occurring pairs of terms and stores the intersections of the corresponding inverted lists.",
                "These last two papers are related to ours in that they exploit different caching strategies at different levels of the memory hierarchy.",
                "Finally, our static caching algorithm for posting lists in Section 5 uses the ratio frequency/size in order to evaluate the goodness of an item to cache.",
                "Similar ideas have been used in the context of file caching [17], Web caching [5], and even caching of posting lists [9], but in all cases in a dynamic setting.",
                "To the best of our knowledge we are the first to use this approach for static caching of posting lists. 3.",
                "DATA CHARACTERIZATION Our data consists of a crawl of documents from the UK domain, and query logs of one year of queries submitted to http://www.yahoo.co.uk from November 2005 to November 2006.",
                "In our logs, 50% of the total volume of queries are unique.",
                "The average query length is 2.5 terms, with the longest query having 731 terms. 1e-07 1e-06 1e-05 1e-04 0.001 0.01 0.1 1 1e-08 1e-07 1e-06 1e-05 1e-04 0.001 0.01 0.1 1 Frequency(normalized) Frequency rank (normalized) Figure 2: The distribution of queries (bottom curve) and query terms (middle curve) in the query log.",
                "The distribution of document frequencies of terms in the UK-2006 dataset (upper curve).",
                "Figure 2 shows the distributions of queries (lower curve), and query terms (middle curve).",
                "The x-axis represents the normalized frequency rank of the query or term. (The most frequent query appears closest to the y-axis.)",
                "The y-axis is Table 1: Statistics of the UK-2006 sample.",
                "UK-2006 sample statistics # of documents 2,786,391 # of terms 6,491,374 # of tokens 2,109,512,558 the normalized frequency for a given query (or term).",
                "As expected, the distribution of query frequencies and query term frequencies follow power law distributions, with slope of 1.84 and 2.26, respectively.",
                "In this figure, the query frequencies were computed as they appear in the logs with no normalization for case or white space.",
                "The query terms (middle curve) have been normalized for case, as have the terms in the document collection.",
                "The document collection that we use for our experiments is a summary of the UK domain crawled in May 2006.1 This summary corresponds to a maximum of 400 crawled documents per host, using a breadth first crawling strategy, comprising 15GB.",
                "The distribution of document frequencies of terms in the collection follows a power law distribution with slope 2.38 (upper curve in Figure 2).",
                "The statistics of the collection are shown in Table 1.",
                "We measured the correlation between the document frequency of terms in the collection and the number of queries that contain a particular term in the query log to be 0.424.",
                "A scatter plot for a random sample of terms is shown in Figure 3.",
                "In this experiment, terms have been converted to lower case in both the queries and the documents so that the frequencies will be comparable. 1e-07 1e-06 1e-05 1e-04 0.001 0.01 0.1 1 1e-06 1e-05 1e-04 0.001 0.01 0.1 1 Queryfrequency Document frequency Figure 3: Normalized scatter plot of document-term frequencies vs. query-term frequencies. 4.",
                "CACHING OF QUERIES AND TERMS Caching relies upon the assumption that there is locality in the stream of requests.",
                "That is, there must be sufficient repetition in the stream of requests and within intervals of time that enable a cache memory of reasonable size to be effective.",
                "In the query log we used, 88% of the unique queries are singleton queries, and 44% are singleton queries out of the whole volume.",
                "Thus, out of all queries in the stream composing the query log, the upper threshold on hit ratio is 56%.",
                "This is because only 56% of all the queries comprise queries that have multiple occurrences.",
                "It is important to observe, however, that not all queries in this 56% can be cache hits because of compulsory misses.",
                "A compulsory miss 1 The collection is available from the University of Milan: http://law.dsi.unimi.it/.",
                "URL retrieved 05/2007. 0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1 240 260 280 300 320 340 360 Numberofelements Bin number Total terms Terms diff Total queries Unique queries Unique terms Query diff Figure 4: Arrival rate for both terms and queries. happens when the cache receives a query for the first time.",
                "This is different from capacity misses, which happen due to space constraints on the amount of memory the cache uses.",
                "If we consider a cache with infinite memory, then the hit ratio is 50%.",
                "Note that for an infinite cache there are no capacity misses.",
                "As we mentioned before, another possibility is to cache the posting lists of terms.",
                "Intuitively, this gives more freedom in the utilization of the cache content to respond to queries because cached terms might form a new query.",
                "On the other hand, they need more space.",
                "As opposed to queries, the fraction of singleton terms in the total volume of terms is smaller.",
                "In our query log, only 4% of the terms appear once, but this accounts for 73% of the vocabulary of query terms.",
                "We show in Section 5 that caching a small fraction of terms, while accounting for terms appearing in many documents, is potentially very effective.",
                "Figure 4 shows several graphs corresponding to the normalized arrival rate for different cases using days as bins.",
                "That is, we plot the normalized number of elements that appear in a day.",
                "This graph shows only a period of 122 days, and we normalize the values by the maximum value observed throughout the whole period of the query log.",
                "Total queries and Total terms correspond to the total volume of queries and terms, respectively.",
                "Unique queries and Unique terms correspond to the arrival rate of unique queries and terms.",
                "Finally, Query diff and Terms diff correspond to the difference between the curves for total and unique.",
                "In Figure 4, as expected, the volume of terms is much higher than the volume of queries.",
                "The difference between the total number of terms and the number of unique terms is much larger than the difference between the total number of queries and the number of unique queries.",
                "This observation implies that terms repeat significantly more than queries.",
                "If we use smaller bins, say of one hour, then the ratio of unique to volume is higher for both terms and queries because it leaves less room for repetition.",
                "We also estimated the workload using the document frequency of terms as a measure of how much work a query imposes on a search engine.",
                "We found that it follows closely the arrival rate for terms shown in Figure 4.",
                "To demonstrate the effect of a dynamic cache on the query frequency distribution of Figure 2, we plot the same frequency graph, but now considering the frequency of queries Figure 5: Frequency graph after LRU cache. after going through an LRU cache.",
                "On a cache miss, an LRU cache decides upon an entry to evict using the information on the recency of queries.",
                "In this graph, the most frequent queries are not the same queries that were most frequent before the cache.",
                "It is possible that queries that are most frequent after the cache have different characteristics, and tuning the search engine to queries frequent before the cache may degrade performance for non-cached queries.",
                "The maximum frequency after caching is less than 1% of the maximum frequency before the cache, thus showing that the cache is very effective in reducing the load of frequent queries.",
                "If we re-rank the queries according to after-cache frequency, the distribution is still a power law, but with a much smaller value for the highest frequency.",
                "When discussing the effectiveness of dynamically caching, an important metric is cache miss rate.",
                "To analyze the cache miss rate for different memory constraints, we use the working set model [6, 14].",
                "A working set, informally, is the set of references that an application or an operating system is currently working with.",
                "The model uses such sets in a strategy that tries to capture the temporal locality of references.",
                "The working set strategy then consists in keeping in memory only the elements that are referenced in the previous θ steps of the input sequence, where θ is a configurable parameter corresponding to the window size.",
                "Originally, working sets have been used for page replacement algorithms of operating systems, and considering such a strategy in the context of search engines is interesting for three reasons.",
                "First, it captures the amount of locality of queries and terms in a sequence of queries.",
                "Locality in this case refers to the frequency of queries and terms in a window of time.",
                "If many queries appear multiple times in a window, then locality is high.",
                "Second, it enables an oﬄine analysis of the expected miss rate given different memory constraints.",
                "Third, working sets capture aspects of efficient caching algorithms such as LRU.",
                "LRU assumes that references farther in the past are less likely to be referenced in the present, which is implicit in the concept of working sets [14].",
                "Figure 6 plots the miss rate for different working set sizes, and we consider working sets of both queries and terms.",
                "The working set sizes are normalized against the total number of queries in the query log.",
                "In the graph for queries, there is a sharp decay until approximately 0.01, and the rate at which the miss rate drops decreases as we increase the size of the working set over 0.01.",
                "Finally, the minimum value it reaches is 50% miss rate, not shown in the figure as we have cut the tail of the curve for presentation purposes. 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1 0 0.05 0.1 0.15 0.2 Missrate Normalized working set size Queries Terms Figure 6: Miss rate as a function of the working set size. 1 10 100 1000 10000 100000 1e+06 Frequency Distance Figure 7: Distribution of distances expressed in terms of distinct queries.",
                "Compared to the query curve, we observe that the minimum miss rate for terms is substantially smaller.",
                "The miss rate also drops sharply on values up to 0.01, and it decreases minimally for higher values.",
                "The minimum value, however, is slightly over 10%, which is much smaller than the minimum value for the sequence of queries.",
                "This implies that with such a policy it is possible to achieve over 80% hit rate, if we consider caching dynamically posting lists for terms as opposed to caching answers for queries.",
                "This result does not consider the space required for each unit stored in the cache memory, or the amount of time it takes to put together a response to a user query.",
                "We analyze these issues more carefully later in this paper.",
                "It is interesting also to observe the histogram of Figure 7, which is an intermediate step in the computation of the miss rate graph.",
                "It reports the distribution of distances between repetitions of the same frequent query.",
                "The distance in the plot is measured in the number of distinct queries separating a query and its repetition, and it considers only queries appearing at least 10 times.",
                "From Figures 6 and 7, we conclude that even if we set the size of the query answers cache to a relatively large number of entries, the miss rate is high.",
                "Thus, caching the posting lists of terms has the potential to improve the hit ratio.",
                "This is what we explore next. 5.",
                "CACHING POSTING LISTS The previous section shows that caching posting lists can obtain a higher hit rate compared to caching query answers.",
                "In this section we study the problem of how to select posting lists to place on a certain amount of available memory, assuming that the whole index is larger than the amount of memory available.",
                "The posting lists have variable size (in fact, their size distribution follows a power law), so it is beneficial for a caching policy to consider the sizes of the posting lists.",
                "We consider both dynamic and static caching.",
                "For dynamic caching, we use two well-known policies, LRU and LFU, as well as a modified algorithm that takes posting-list size into account.",
                "Before discussing the static caching strategies, we introduce some notation.",
                "We use fq(t) to denote the query-term frequency of a term t, that is, the number of queries containing t in the query log, and fd(t) to denote the document frequency of t, that is, the number of documents in the collection in which the term t appears.",
                "The first strategy we consider is the algorithm proposed by Baeza-Yates and Saint-Jean [2], which consists in selecting the posting lists of the terms with the highest query-term frequencies fq(t).",
                "We call this algorithm Qtf.",
                "We observe that there is a trade-off between fq(t) and fd(t).",
                "Terms with high fq(t) are useful to keep in the cache because they are queried often.",
                "On the other hand, terms with high fd(t) are not good candidates because they correspond to long posting lists and consume a substantial amount of space.",
                "In fact, the problem of selecting the best posting lists for the static cache corresponds to the standard Knapsack problem: given a knapsack of fixed capacity, and a set of n items, such as the i-th item has value ci and size si, select the set of items that fit in the knapsack and maximize the overall value.",
                "In our case, value corresponds to fq(t) and size corresponds to fd(t).",
                "Thus, we employ a simple algorithm for the knapsack problem, which is selecting the posting lists of the terms with the highest values of the ratio fq(t) fd(t) .",
                "We call this algorithm QtfDf.",
                "We tried other variations considering query frequencies instead of term frequencies, but the gain was minimal compared to the complexity added.",
                "In addition to the above two static algorithms we consider the following algorithms for dynamic caching: • LRU: A standard LRU algorithm, but many posting lists might need to be evicted (in order of least-recent usage) until there is enough space in the memory to place the currently accessed posting list; • LFU: A standard LFU algorithm (eviction of the leastfrequently used), with the same modification as the LRU; • Dyn-QtfDf: A dynamic version of the QtfDf algorithm; evict from the cache the term(s) with the lowest fq(t) fd(t) ratio.",
                "The performance of all the above algorithms for 15 weeks of the query log and the UK dataset are shown in Figure 8.",
                "Performance is measured with hit rate.",
                "The cache size is measured as a fraction of the total space required to store the posting lists of all terms.",
                "For the dynamic algorithms, we load the cache with terms in order of fq(t) and we let the cache warm up for 1 million queries.",
                "For the static algorithms, we assume complete knowledge of the frequencies fq(t), that is, we estimate fq(t) from the whole query stream.",
                "As we show in Section 7 the results do not change much if we compute the query-term frequencies using the first 3 or 4 weeks of the query log and measure the hit rate on the rest. 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1 0.1 0.2 0.3 0.4 0.5 0.6 0.7 Hitrate Cache size Caching posting lists static QTF/DF LRU LFU Dyn-QTF/DF QTF Figure 8: Hit rate of different strategies for caching posting lists.",
                "The most important observation from our experiments is that the static QtfDf algorithm has a better hit rate than all the dynamic algorithms.",
                "An important benefit a static cache is that it requires no eviction and it is hence more efficient when evaluating queries.",
                "However, if the characteristics of the query traffic change frequently over time, then it requires re-populating the cache often or there will be a significant impact on hit rate. 6.",
                "ANALYSIS OF STATIC CACHING In this section we provide a detailed analysis for the problem of deciding whether it is preferable to cache query answers or cache posting lists.",
                "Our analysis takes into account the impact of caching between two levels of the data-access hierarchy.",
                "It can either be applied at the memory/disk layer or at a server/remote server layer as in the architecture we discussed in the introduction.",
                "Using a particular system model, we obtain estimates for the parameters required by our analysis, which we subsequently use to decide the optimal trade-off between caching query answers and caching posting lists. 6.1 Analytical Model Let M be the size of the cache measured in answer units (the cache can store M query answers).",
                "Assume that all posting lists are of the same length L, measured in answer units.",
                "We consider the following two cases: (A) a cache that stores only precomputed answers, and (B) a cache that stores only posting lists.",
                "In the first case, Nc = M answers fit in the cache, while in the second case Np = M/L posting lists fit in the cache.",
                "Thus, Np = Nc/L.",
                "Note that although posting lists require more space, we can combine terms to evaluate more queries (or partial queries).",
                "For case (A), suppose that a query answer in the cache can be evaluated in 1 time unit.",
                "For case (B), assume that if the posting lists of the terms of a query are in the cache then the results can be computed in TR1 time units, while if the posting lists are not in the cache then the results can be computed in TR2 time units.",
                "Of course TR2 > TR1.",
                "Now we want to compare the time to answer a stream of Q queries in both cases.",
                "Let Vc(Nc) be the volume of the most frequent Nc queries.",
                "Then, for case (A), we have an overall time TCA = Vc(Nc) + TR2(Q − Vc(Nc)).",
                "Similarly, for case (B), let Vp(Np) be the number of computable queries.",
                "Then we have overall time TP L = TR1Vp(Np) + TR2(Q − Vp(Np)).",
                "We want to check under which conditions we have TP L < TCA.",
                "We have TP L − TCA = (TR2 − 1)Vc(Nc) − (TR2 − TR1)Vp(Np) > 0.",
                "Figure 9 shows the values of Vp and Vc for our data.",
                "We can see that caching answers saturates faster and for this particular data there is no additional benefit from using more than 10% of the index space for caching answers.",
                "As the query distribution is a power law with parameter α > 1, the i-th most frequent query appears with probability proportional to 1 iα .",
                "Therefore, the volume Vc(n), which is the total number of the n most frequent queries, is Vc(n) = V0 n i=1 Q iα = γnQ (0 < γn < 1).",
                "We know that Vp(n) grows faster than Vc(n) and assume, based on experimental results, that the relation is of the form Vp(n) = k Vc(n)β .",
                "In the worst case, for a large cache, β → 1.",
                "That is, both techniques will cache a constant fraction of the overall query volume.",
                "Then caching posting lists makes sense only if L(TR2 − 1) k(TR2 − TR1) > 1.",
                "If we use compression, we have L < L and TR1 > TR1.",
                "According to the experiments that we show later, compression is always better.",
                "For a small cache, we are interested in the transient behavior and then β > 1, as computed from our data.",
                "In this case there will always be a point where TP L > TCA for a large number of queries.",
                "In reality, instead of filling the cache only with answers or only with posting lists, a better strategy will be to divide the total cache space into cache for answers and cache for posting lists.",
                "In such a case, there will be some queries that could be answered by both parts of the cache.",
                "As the answer cache is faster, it will be the first choice for answering those queries.",
                "Let QNc and QNp be the set of queries that can be answered by the cached answers and the cached posting lists, respectively.",
                "Then, the overall time is T = Vc(Nc)+TR1V (QNp −QNc )+TR2(Q−V (QNp ∪QNc )), where Np = (M − Nc)/L.",
                "Finding the optimal division of the cache in order to minimize the overall retrieval time is a difficult problem to solve analytically.",
                "In Section 6.3 we use simulations to derive optimal cache trade-offs for particular implementation examples. 6.2 Parameter Estimation We now use a particular implementation of a centralized system and the model of a distributed system as examples from which we estimate the parameters of the analysis from the previous section.",
                "We perform the experiments using an optimized version of Terrier [11] for both indexing documents and processing queries, on a single machine with a Pentium 4 at 2GHz and 1GB of RAM.",
                "We indexed the documents from the UK-2006 dataset, without removing stop words or applying stemming.",
                "The posting lists in the inverted file consist of pairs of document identifier and term frequency.",
                "We compress the document identifier gaps using Elias gamma encoding, and the 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1 0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1 Queryvolume Space precomputed answers posting lists Figure 9: Cache saturation as a function of size.",
                "Table 2: Ratios between the average time to evaluate a query and the average time to return cached answers (centralized and distributed case).",
                "Centralized system TR1 TR2 TR1 TR2 Full evaluation 233 1760 707 1140 Partial evaluation 99 1626 493 798 LAN system TRL 1 TRL 2 TR L 1 TR L 2 Full evaluation 242 1769 716 1149 Partial evaluation 108 1635 502 807 WAN system TRW 1 TRW 2 TR W 1 TR W 2 Full evaluation 5001 6528 5475 5908 Partial evaluation 4867 6394 5270 5575 term frequencies in documents using unary encoding [16].",
                "The size of the inverted file is 1,189Mb.",
                "A stored answer requires 1264 bytes, and an uncompressed posting takes 8 bytes.",
                "From Table 1, we obtain L = (8·# of postings) 1264·# of terms = 0.75 and L = Inverted file size 1264·# of terms = 0.26.",
                "We estimate the ratio TR = T/Tc between the average time T it takes to evaluate a query and the average time Tc it takes to return a stored answer for the same query, in the following way.",
                "Tc is measured by loading the answers for 100,000 queries in memory, and answering the queries from memory.",
                "The average time is Tc = 0.069ms.",
                "T is measured by processing the same 100,000 queries (the first 10,000 queries are used to warm-up the system).",
                "For each query, we remove stop words, if there are at least three remaining terms.",
                "The stop words correspond to the terms with a frequency higher than the number of documents in the index.",
                "We use a document-at-a-time approach to retrieve documents containing all query terms.",
                "The only disk access required during query processing is for reading compressed posting lists from the inverted file.",
                "We perform both full and partial evaluation of answers, because some queries are likely to retrieve a large number of documents, and only a fraction of the retrieved documents will be seen by users.",
                "In the partial evaluation of queries, we terminate the processing after matching 10,000 documents.",
                "The estimated ratios TR are presented in Table 2.",
                "Figure 10 shows for a sample of queries the workload of the system with partial query evaluation and compressed posting lists.",
                "The x-axis corresponds to the total time the system spends processing a particular query, and the vertical axis corresponds to the sum t∈q fq · fd(t).",
                "Notice that the total number of postings of the query-terms does not necessarily provide an accurate estimate of the workload imposed on the system by a query (which is the case for full evaluation and uncompressed lists). 0 0.2 0.4 0.6 0.8 1 0 0.2 0.4 0.6 0.8 1 Totalpostingstoprocessquery(normalized) Total time to process query (normalized) Partial processing of compressed postings query len = 1 query len in [2,3] query len in [4,8] query len > 8 Figure 10: Workload for partial query evaluation with compressed posting lists.",
                "The analysis of the previous section also applies to a distributed retrieval system in one or multiple sites.",
                "Suppose that a document partitioned distributed system is running on a cluster of machines interconnected with a local area network (LAN) in one site.",
                "The broker receives queries and broadcasts them to the query processors, which answer the queries and return the results to the broker.",
                "Finally, the broker merges the received answers and generates the final set of answers (we assume that the time spent on merging results is negligible).",
                "The difference between the centralized architecture and the document partition architecture is the extra communication between the broker and the query processors.",
                "Using ICMP pings on a 100Mbps LAN, we have measured that sending the query from the broker to the query processors which send an answer of 4,000 bytes back to the broker takes on average 0.615ms.",
                "Hence, TRL = TR + 0.615ms/0.069ms = TR + 9.",
                "In the case when the broker and the query processors are in different sites connected with a wide area network (WAN), we estimated that broadcasting the query from the broker to the query processors and getting back an answer of 4,000 bytes takes on average 329ms.",
                "Hence, TRW = TR + 329ms/0.069ms = TR + 4768. 6.3 Simulation Results We now address the problem of finding the optimal tradeoff between caching query answers and caching posting lists.",
                "To make the problem concrete we assume a fixed budget M on the available memory, out of which x units are used for caching query answers and M − x for caching posting lists.",
                "We perform simulations and compute the average response time as a function of x.",
                "Using a part of the query log as training data, we first allocate in the cache the answers to the most frequent queries that fit in space x, and then we use the rest of the memory to cache posting lists.",
                "For selecting posting lists we use the QtfDf algorithm, applied to the training query log but excluding the queries that have already been cached.",
                "In Figure 11, we plot the simulated response time for a centralized system as a function of x.",
                "For the uncompressed index we use M = 1GB, and for the compressed index we use M = 0.5GB.",
                "In the case of the configuration that uses partial query evaluation with compressed posting lists, the lowest response time is achieved when 0.15GB out of the 0.5GB is allocated for storing answers for queries.",
                "We obtained similar trends in the results for the LAN setting.",
                "Figure 12 shows the simulated workload for a distributed system across a WAN.",
                "In this case, the total amount of memory is split between the broker, which holds the cached 400 500 600 700 800 900 1000 1100 1200 0 0.2 0.4 0.6 0.8 1 Averageresponsetime Space (GB) Simulated workload -- single machine full / uncompr / 1 G partial / uncompr / 1 G full / compr / 0.5 G partial / compr / 0.5 G Figure 11: Optimal division of the cache in a server. 3000 3500 4000 4500 5000 5500 6000 0 0.2 0.4 0.6 0.8 1 Averageresponsetime Space (GB) Simulated workload -- WAN full / uncompr / 1 G partial / uncompr / 1 G full / compr / 0.5 G partial / compr / 0.5 G Figure 12: Optimal division of the cache when the next level requires WAN access. answers of queries, and the query processors, which hold the cache of posting lists.",
                "According to the figure, the difference between the configurations of the query processors is less important because the network communication overhead increases the response time substantially.",
                "When using uncompressed posting lists, the optimal allocation of memory corresponds to using approximately 70% of the memory for caching query answers.",
                "This is explained by the fact that there is no need for network communication when the query can be answered by the cache at the broker. 7.",
                "EFFECT OF THE QUERY DYNAMICS For our query log, the query distribution and query-term distribution change slowly over time.",
                "To support this claim, we first assess how topics change comparing the distribution of queries from the first week in June, 2006, to the distribution of queries for the remainder of 2006 that did not appear in the first week in June.",
                "We found that a very small percentage of queries are new queries.",
                "The majority of queries that appear in a given week repeat in the following weeks for the next six months.",
                "We then compute the hit rate of a static cache of 128, 000 answers trained over a period of two weeks (Figure 13).",
                "We report hit rate hourly for 7 days, starting from 5pm.",
                "We observe that the hit rate reaches its highest value during the night (around midnight), whereas around 2-3pm it reaches its minimum.",
                "After a small decay in hit rate values, the hit rate stabilizes between 0.28, and 0.34 for the entire week, suggesting that the static cache is effective for a whole week after the training period. 0.26 0.27 0.28 0.29 0.3 0.31 0.32 0.33 0.34 0.35 0.36 0.37 0 20 40 60 80 100 120 140 160 Hit-rate Time Hits on the frequent queries of distances Figure 13: Hourly hit rate for a static cache holding 128,000 answers during the period of a week.",
                "The static cache of posting lists can be periodically recomputed.",
                "To estimate the time interval in which we need to recompute the posting lists on the static cache we need to consider an efficiency/quality trade-off: using too short a time interval might be prohibitively expensive, while recomputing the cache too infrequently might lead to having an obsolete cache not corresponding to the statistical characteristics of the current query stream.",
                "We measured the effect on the QtfDf algorithm of the changes in a 15-week query stream (Figure 14).",
                "We compute the query term frequencies over the whole stream, select which terms to cache, and then compute the hit rate on the whole query stream.",
                "This hit rate is as an upper bound, and it assumes perfect knowledge of the query term frequencies.",
                "To simulate a realistic scenario, we use the first 6 (3) weeks of the query stream for computing query term frequencies and the following 9 (12) weeks to estimate the hit rate.",
                "As Figure 14 shows, the hit rate decreases by less than 2%.",
                "The high correlation among the query term frequencies during different time periods explains the graceful adaptation of the static caching algorithms to the future query stream.",
                "Indeed, the pairwise correlation among all possible 3-week periods of the 15-week query stream is over 99.5%. 8.",
                "CONCLUSIONS Caching is an effective technique in search engines for improving response time, reducing the load on query processors, and improving network bandwidth utilization.",
                "We present results on both dynamic and static caching.",
                "Dynamic caching of queries has limited effectiveness due to the high number of compulsory misses caused by the number of unique or infrequent queries.",
                "Our results show that in our UK log, the minimum miss rate is 50% using a working set strategy.",
                "Caching terms is more effective with respect to miss rate, achieving values as low as 12%.",
                "We also propose a new algorithm for static caching of posting lists that outperforms previous static caching algorithms as well as dynamic algorithms such as LRU and LFU, obtaining hit rate values that are over 10% higher compared these strategies.",
                "We present a framework for the analysis of the trade-off between caching query results and caching posting lists, and we simulate different types of architectures.",
                "Our results show that for centralized and LAN environments, there is an optimal allocation of caching query results and caching of posting lists, while for WAN scenarios in which network time prevails it is more important to cache query results. 0.45 0.5 0.55 0.6 0.65 0.7 0.75 0.8 0.85 0.9 0.95 0.1 0.2 0.3 0.4 0.5 0.6 0.7 Hitrate Cache size Dynamics of static QTF/DF caching policy perfect knowledge 6-week training 3-week training Figure 14: Impact of distribution changes on the static caching of posting lists. 9.",
                "REFERENCES [1] V. N. Anh and A. Moffat.",
                "Pruned query evaluation using pre-computed impacts.",
                "In ACM CIKM, 2006. [2] R. A. Baeza-Yates and F. Saint-Jean.",
                "A three level search engine index based in query log distribution.",
                "In SPIRE, 2003. [3] C. Buckley and A. F. Lewit.",
                "Optimization of inverted vector searches.",
                "In ACM SIGIR, 1985. [4] S. B¨uttcher and C. L. A. Clarke.",
                "A document-centric approach to static index pruning in text retrieval systems.",
                "In ACM CIKM, 2006. [5] P. Cao and S. Irani.",
                "Cost-aware WWW proxy caching algorithms.",
                "In USITS, 1997. [6] P. Denning.",
                "Working sets past and present.",
                "IEEE Trans. on Software Engineering, SE-6(1):64-84, 1980. [7] T. Fagni, R. Perego, F. Silvestri, and S. Orlando.",
                "Boosting the performance of web search engines: Caching and prefetching query results by exploiting historical usage data.",
                "ACM Trans.",
                "Inf.",
                "Syst., 24(1):51-78, 2006. [8] R. Lempel and S. Moran.",
                "Predictive caching and prefetching of query results in search engines.",
                "In WWW, 2003. [9] X.",
                "Long and T. Suel.",
                "Three-level caching for efficient query processing in large web search engines.",
                "In WWW, 2005. [10] E. P. Markatos.",
                "On caching search engine query results.",
                "Computer Communications, 24(2):137-143, 2001. [11] I. Ounis, G. Amati, V. Plachouras, B.",
                "He, C. Macdonald, and C. Lioma.",
                "Terrier: A High Performance and Scalable Information Retrieval Platform.",
                "In SIGIR Workshop on Open Source Information Retrieval, 2006. [12] V. V. Raghavan and H. Sever.",
                "On the reuse of past optimal queries.",
                "In ACM SIGIR, 1995. [13] P. C. Saraiva, E. S. de Moura, N. Ziviani, W. Meira, R. Fonseca, and B. Riberio-Neto.",
                "Rank-preserving two-level caching for scalable search engines.",
                "In ACM SIGIR, 2001. [14] D. R. Slutz and I. L. Traiger.",
                "A note on the calculation of average working set size.",
                "Communications of the ACM, 17(10):563-565, 1974. [15] T. Strohman, H. Turtle, and W. B. Croft.",
                "Optimization strategies for complex queries.",
                "In ACM SIGIR, 2005. [16] I. H. Witten, T. C. Bell, and A. Moffat.",
                "Managing Gigabytes: Compressing and Indexing Documents and Images.",
                "John Wiley & Sons, Inc., NY, 1994. [17] N. E. Young.",
                "On-line file caching.",
                "Algorithmica, 33(3):371-383, 2002."
            ],
            "original_annotated_samples": [],
            "translated_annotated_samples": [],
            "translated_text": "",
            "candidates": [],
            "error": []
        }
    }
}