{
    "id": "H-7",
    "original_text": "Efficient Bayesian Hierarchical User Modeling for Recommendation Systems Yi Zhang, Jonathan Koren School of Engineering University of California Santa Cruz Santa Cruz, CA, USA {yiz, jonathan}@soe.ucsc.edu ABSTRACT A content-based personalized recommendation system learns user specific profiles from user feedback so that it can deliver information tailored to each individual users interest. A system serving millions of users can learn a better user profile for a new user, or a user with little feedback, by borrowing information from other users through the use of a Bayesian hierarchical model. Learning the model parameters to optimize the joint data likelihood from millions of users is very computationally expensive. The commonly used EM algorithm converges very slowly due to the sparseness of the data in IR applications. This paper proposes a new fast learning technique to learn a large number of individual user profiles. The efficacy and efficiency of the proposed algorithm are justified by theory and demonstrated on actual user data from Netflix and MovieLens. Categories and Subject Descriptors: B.3.3 [Information Search and Retrieval]: Information filtering General Terms: Algorithms 1. INTRODUCTION Personalization is the future of the Web, and it has achieved great success in industrial applications. For example, online stores, such as Amazon and Netflix, provide customized recommendations for additional products or services based on a users history. Recent offerings such as My MSN, My Yahoo!, My Google, and Google News have attracted much attention due to their potential ability to infer a users interests from his/her history. One major personalization topic studied in the information retrieval community is content-based personal recommendation systems1 . These systems learn user-specific profiles from user feedback so that they can recommend information tailored to each individual users interest without requiring the user to make an explicit query. Learning the user profiles is the core problem for these systems. A user profile is usually a classifier that can identify whether a document is relevant to the user or not, or a regression model that tells how relevant a document is to the user. One major challenge of building a recommendation or personalization system is that the profile learned for a particular user is usually of low quality when the amount of data from that particular user is small. This is known as the cold start problem. This means that any new user must endure poor initial performance until sufficient feedback from that user is provided to learn a reliable user profile. There has been much research on improving classification accuracy when the amount of labeled training data is small. The semi-supervised learning approach combines unlabeled and labeled data together to achieve this goal [26]. Another approach is using domain knowledge. Researchers have modified different learning algorithms, such as Na¨ıveBayes [17], logistic regression [7], and SVMs [22], to integrate domain knowledge into a text classifier. The third approach is borrowing training data from other resources [5][7]. The effectiveness of these different approaches is mixed, due to how well the underlying model assumption fits the data. One well-received approach to improve recommendation system performance for a particular user is borrowing information from other users through a Bayesian hierarchical modeling approach. Several researchers have demonstrated that this approach effectively trades off between shared and user-specific information, thus alleviating poor initial performance for each user[27][25]. In order to learn a Bayesian hierarchical model, the system usually tries to find the most likely model parameters for the given data. A mature recommendation system usually works for millions of users. It is well known that learning the optimal parameters of a Bayesian hierarchical model is computationally expensive when there are thousands or millions of users. The EM algorithm is a commonly used technique for parameter learning due to its simplicity and convergence guarantee. However, a content based recommendation system often handles documents in a very high dimensional space, in which each document is represented by a very sparse vector. With careful analysis of the EM algorithm in this scenario (Section 4), we find that the EM tering, or item-based collaborative filtering. In this paper, the words filtering and recommendation are used interchangeably. algorithm converges very slowly due to the sparseness of the input variables. We also find that updating the model parameter at each EM iteration is also expensive with computational complexity of O(MK), where M is the number of users and K is the number of dimensions. This paper modifies the standard EM algorithm to create an improved learning algorithm, which we call the Modified EM algorithm. The basic idea is that instead of calculating the numerical solution for all the user profile parameters, we derive the analytical solution of the parameters for some feature dimensions, and at the M step use the analytical solution instead of the numerical solution estimated at E step for those parameters. This greatly reduces the computation at a single EM iteration, and also has the benefit of increasing the convergence speed of the learning algorithm. The proposed technique is not only well supported by theory, but also by experimental results. The organization of the remaining parts of this paper is as follows: Section 3 describes the Bayesian hierarchical linear regression modeling framework used for content-based recommendations. Section 4 describes how to learn the model parameters using the standard EM algorithm, along with using the new technique proposed in this paper. The experimental setting and results used to validate the proposed learning technique are reported in Sections 5 and 6. Section 7 summarizes and offers concluding remarks. 2. RELATED WORK Providing personalized recommendations to users has been identified as a very important problem in the IR community since the 1970s. The approaches that have been used to solve this problem can be roughly classified into two major categories: content based filtering versus collaborative filtering. Content-based filtering studies the scenario where a recommendation system monitors a document stream and pushes documents that match a user profile to the corresponding user. The user may read the delivered documents and provide explicit relevance feedback, which the filtering system then uses to update the users profile using relevance feedback retrieval models (e.g. Boolean models, vector space models, traditional probabilistic models [20] , inference networks [3] and language models [6]) or machine learning algorithms (e.g. Support Vector Machines (SVM), K nearest neighbors (K-NN) clustering, neural networks, logistic regression, or Winnow [16] [4] [23]). Collaborative filtering goes beyond merely using document content to recommend items to a user by leveraging information from other users with similar tastes and preferences in the past. Memorybased heuristics and model based approaches have been used in collaborative filtering task [15] [8] [2] [14] [12] [11]. This paper contributes to the content-based recommendation research by improving the efficiency and effectiveness of Bayesian hierarchical linear models, which have a strong theoretical basis and good empirical performance on recommendation tasks[27][25]. This paper does not intend to compare content-based filtering with collaborative filtering or claim which one is a better. We think each complements the other, and that content-based filtering is extremely useful for handling new documents/items with little or no user feedback. Similar to some other researchers[18][1][21], we found that a recommendation system will be more effective when both techniques are combined. However, this is beyond the scope of this paper and thus not discussed here. 3. BAYESIAN HIERARCHICAL LINEAR REGRESSION Assume there are M users in the system. The task of the system is to recommend documents that are relevant to each user. For each user, the system learns a user model from the users history. In the rest of this paper, we will use the following notations to represent the variables in the system. m = 1, 2, ..., M: The index for each individual user. M is the total number of users. wm: The user model parameter associated with user m. wm is a K dimensional vector. j = 1, 2, ..., Jm: The index for a set of data for user m. Jm is the number of training data for user m. Dm = {(xm,j, ym,j)}: A set of data associated with user m. xm,j is a K dimensional vector that represents the mth users jth training document.2 ym,j is a scalar that represents the label of document xm,j. k = 1, 2, ..., K: The dimensional index of input variable x. The Bayesian hierarchical modeling approach has been widely used in real-world information retrieval applications. Generalized Bayesian hierarchical linear models, one of the simplest Bayesian hierarchical models, are commonly used and have achieved good performance on collaborative filtering [25] and content-based adaptive filtering [27] tasks. Figure 1 shows the graphical representation of a Bayesian hierarchical model. In this graph, each user model is represented by a random vector wm. We assume a user model is sampled randomly from a prior distribution P(w|Φ). The system can predict the user label y of a document x given an estimation of wm (or wms distribution) using a function y = f(x, w). The model is called generalized Bayesian hierarchical linear model when y = f(wT x) is any generalized linear model such as logistic regression, SVM, and linear regression. To reliably estimate the user model wm, the system can borrow information from other users through the prior Φ = (µ, Σ). Now we look at one commonly used model where y = wT x + , where ∼ N(0, σ2 ) is a random noise [25][27]. Assume that each user model wm is an independent draw from a population distribution P(w|Φ), which is governed by some unknown hyperparameter Φ. Let the prior distribution of user model w be a Gaussian distribution with parameter Φ = (µ, Σ), which is the commonly used prior for linear models. µ = (µ1, µ2, ..., µK ) is a K dimensional vector that represents the mean of the Gaussian distribution, and Σ is the covariance matrix of the Gaussian. Usually, a Normal distribution N(0, aI) and an Inverse Wishart distribution P(Σ) ∝ |Σ|− 1 2 b exp(−1 2 ctr(Σ−1 )) are used as hyperprior to model the prior distribution of µ and Σ respectively. I is the K dimensional identity matrix, and a, b, and c are real numbers. With these settings, we have the following model for the system: 1. µ and Σ are sampled from N(0, aI) and IWν (aI), respectively. 2 The first dimension of x is a dummy variable that always equals to 1. Figure 1: Illustration of dependencies of variables in the hierarchical model. The rating, y, for a document, x, is conditioned on the document and the user model, wm, associated with the user m. Users share information about their models through the prior, Φ = (µ, Σ). 2. For each user m, wm is sampled randomly from a Normal distribution: wm ∼ N(µ, Σ2 ) 3. For each item xm,j, ym,j is sampled randomly from a Normal distribution: ym,j ∼ N(wT mxm,j, σ2 ). Let θ = (Φ, w1, w2, ..., wM ) represent the parameters of this system that needs to be estimated. The joint likelihood for all the variables in the probabilistic model, which includes the data and the parameters, is: P(D, θ) = P(Φ) m P(wm|Φ) j P(ym,j|xm,j, wm) (1) For simplicity, we assume a, b, c, and σ are provided to the system. 4. MODEL PARAMETER LEARNING If the prior Φ is known, finding the optimal wm is straightforward: it is a simple linear regression. Therefore, we will focus on estimating Φ. The maximum a priori solution of Φ is given by ΦMAP = arg max Φ P(Φ|D) (2) = arg max Φ P(Φ, D) P(D) (3) = arg max Φ P(D|Φ)P(Φ) (4) = arg max Φ w P(D|w, Φ)P(w|Φ)P(Φ)dw (5) Finding the optimal solution for the above problem is challenging, since we need to integrate over all w = (w1, w2, ..., wM ), which are unobserved hidden variables. 4.1 EM Algorithm for Bayesian Hierarchical Linear Models In Equation 5, Φ is the parameter needs to be estimated, and the result depends on unobserved latent variables w. This kind of optimization problem is usually solved by the EM algorithm. Applying EM to the above problem, the set of user models w are the unobservable hidden variables and we have: Q = w P(w|µ, Σ2 , Dm) log P(µ, Σ2 , w, D)dw Based on the derivation of the EM formulas presented in [24], we have the following Expectation-Maximization steps for finding the optimal hyperparameters. For space considerations, we omit the derivation in this paper since it is not the focus of our work. E step: For each user m, estimate the user model distribution P(wm|Dm, Φ) = N(wm; ¯wm, Σ2 m) based on the current estimation of the prior Φ = (µ, Σ2 ). ¯wm = ((Σ2 )−1 + Sxx,m σ2 )−1 ( Sxy,m σ2 + (Σ2 )−1 µ) (6) Σ2 m = ((Σ2 )−1 + Sxx,m σ2 )−1 (7) where Sxx,m = j xm,jxT m,j Sxy,m = j xm,jym,j M step: Optimize the prior Φ = (µ, Σ2 ) based on the estimation from the last E step. µ = 1 M m ¯wm (8) Σ2 = 1 M m Σ2 m + ( ¯wm − µ)( ¯wm − µ)T (9) Many machine learning driven IR systems use a point estimate of the parameters at different stages in the system. However, we are estimating the posterior distribution of the variables at the E step. This avoids overfitting wm to a particular users data, which may be small and noisy. A detailed discussion about this subject appears in [10]. 4.2 New Algorithm: Modified EM Although the EM algorithm is widely studied and used in machine learning applications, using the above EM process to solve Bayesian hierarchical linear models in large-scale information retrieval systems is still too computationally expensive. In this section, we describe why the learning rate of the EM algorithm is slow in our application and introduce a new technique to make the learning of the Bayesian hierarchical linear model scalable. The derivation of the new learning algorithm will be based on the EM algorithm described in the previous section. First, the covariance matrices Σ2 , Σ2 m are usually too large to be computationally feasible. For simplicity, and as a common practice in IR, we do not model the correlation between features. Thus we approximate these matrices with K dimensional diagonal matrices. In the rest of the paper, we use these symbols to represent their diagonal approximations: Σ2 =     σ2 1 0 .. 0 0 σ2 2 .. 0 .. .. .. .. 0 0 .. σ2 K     Σ2 m =     σ2 m,1 0 .. 0 0 σ2 m,2 .. 0 .. .. .. .. 0 0 .. σ2 m,K     Secondly, and most importantly, the input space is very sparse and there are many dimensions that are not related to a particular user in a real IR application. For example, let us consider a movie recommendation system, with the input variable x representing a particular movie. For the jth movie that the user m has seen, let xm,j,k = 1 if the director of the movie is Jean-Pierre Jeunet (indexed by k). Here we assume that whether or not that this director directed a specific movie is represented by the kth dimension. If the user m has never seen a movie directed by Jean-Pierre Jeunet, then the corresponding dimension is always zero (xm,j,k = 0 for all j) . One major drawback of the EM algorithm is that the importance of a feature, µk, may be greatly dominated by users who have never encountered this feature (i.e. j xm,j,k = 0) at the M step (Equation 8). Assume that 100 out of 1 million users have viewed the movie directed by Jean-Pierre Jeunet, and that the viewers have rated all of his movies as excellent. Intuitively, he is a good director and the weight for him (µk) should be high. Before the EM iteration, the initial value of µ is usually set to 0. Since the other 999,900 users have not seen this movie, their corresponding weights (w1,k, w2,k, ..., wm,k..., w999900,k) for that director would be very small initially. Thus the corresponding weight of the director in the prior µk at the first M step would be very low , and the variance σm,k will be large (Equations 8 and 7). It is undesirable that users who have never seen any movie produced by the director influence the importance of the director so much. This makes the convergence of the standard EM algorithm very slow. Now lets look at whether we can improve the learning speed of the algorithm. Without a loss of generality, let us assume that the kth dimension of the input variable x is not related to a particular user m. By which we mean, xm,j,k = 0 for all j = 1, ..., Jm. It is straightforward to prove that the kth row and kth column of Sxx,m are completely filled with zeros, and that the kth dimension of Sxy,m is zeroed as well. Thus the corresponding kth dimension of the user models mean, ¯wm, should be equal to that of the prior: ¯wm,k = µk, with the corresponding covariance of σm,k = σk. At the M step, the standard EM algorithm uses the numerical solution of the distribution P(wm|Dm, Φ) estimated at E step (Equation 8 and Equation 7). However, the numerical solutions are very unreliable for ¯wm,k and σm,k when the kth dimension is not related to the mth user. A better approach is using the analytical solutions ¯wm,k = µk, and σm,k = σk for the unrelated (m, k) pairs, along with the numerical solution estimated at E step for the other (m, k) pairs. Thus we get the following new EM-like algorithm: Modified E step: For each user m, estimate the user model distribution P(wm|Dm, Φ) = N(wm; ¯wm, Σ2 m) based on the current estimation of σ , µ, Σ2 . ¯wm = ((Σ2 )−1 + Sxx,m σ2 )−1 ( Sxy,m σ2 + (Σ2 )−1 µ)(10) σ2 m,k = ((σ2 k)−1 + sxx,m,k σ2 )−1 (11) where sxx,m,k = j x2 m,j,k and sxy,m,k = j xm,j,kym,j Modified M Step Optimize the prior Φ = (µ, Σ2 ) based on the estimation from the last E step for related userfeature pairs. The M step implicitly uses the analytical solution for unrelated user-feature pairs. µk = 1 Mk m:related ¯wm,k (12) σ2 k = 1 Mk m:related σ2 m,k +( ¯wm,k − µk)( ¯wm,k − µk)T (13) where Mk is the number of users that are related to feature k We only estimate the diagonal of Σ2 m and Σ since we are using the diagonal approximation of the covariance matrices. To estimate ¯wm, we only need to calculate the numerical solutions for dimensions that are related to user m. To estimate σ2 k and µk, we only sum over users that are related to the kth feature. There are two major benefits of the new algorithm. First, because only the related (m, k) pairs are needed at the modified M step, the computational complexity in a single EM iteration is much smaller when the data is sparse, and many of (m, k) pairs are unrelated. Second, the parameters estimated at the modified M step (Equations 12 - 13) are more accurate than the standard M step described in Section 4.1 because the exact analytical solutions ¯wm,k = µk and σm,k = σk for the unrelated (m, k) pairs were used in the new algorithm instead of an approximate solution as in the standard algorithm. 5. EXPERIMENTAL METHODOLOGY 5.1 Evaluation Data Set To evaluate the proposed technique, we used the following three major data sets (Table 1): MovieLens Data: This data set was created by combining the relevance judgments from the MovieLens[9] data set with documents from the Internet Movie Database (IMDB). MovieLens allows users to rank how much he/she enjoyed a specific movie on a scale from 1 to 5. This likeability rating was used as a measurement of how relevant the document representing the corresponding movie is to the user. We considered documents with likeability scores of 4 or 5 as relevant, and documents with a score of 1 to 3 as irrelevant to the user. MovieLens provided relevance judgments on 3,057 documents from 6,040 separate users. On average, each user rated 151 movies, of these 87 were judged to be relevant. The average score for a document was 3.58. Documents representing each movie were constructed from the portion of the IMDB database that is available for public download[13]. Based on this database, we created one document per movie that contained the relevant information about it (e.g. directors, actors, etc.). Table 1: Data Set Statistics. On Reuters, the number of rating for a simulated user is the number of documents relevant to the corresponding topic. Data Users Docs Ratings per User MovieLens 6,040 3,057 151 Netflix-all 480,189 17,770 208 Netflix-1000 1000 17,770 127 Reuters-C 34 100,000 3949 Reuters-E 26 100,000 1632 Reuters-G 33 100,000 2222 Reuters-M 10 100,000 6529 Netflix Data: This data set was constructed by combining documents about movies crawled from the web with a set of actual movie rental customer relevance judgments from Netflix[19]. Netflix publicly provides the relevance judgments of 480,189 anonymous customers. There are around 100 million rating on a scale of 1 to 5 for 17,770 documents. Similar to MovieLens, we considered documents with likeability scores of 4 or 5 as relevant. This number was reduced to 1000 customers through random sampling. The average customer on the reduced data set provided 127 judgments, with 70 being deemed relevant. The average score for documents is 3.55. Reuters Data: This is the Reuters Corpus, Volume 1. It covers 810,000 Reuters English language news stories from August 20, 1996 to August 19, 1997. Only the first 100,000 news were used in our experiments. The Reuters corpus comes with a topic hierarchy. Each document is assigned to one of several locations on the hierarchical tree. The first level of the tree contains four topics, denoted as C, E, M, and G. For the experiments in this paper, the tree was cut at level 1 to create four smaller trees, each of which corresponds to one smaller data set: Reuters-E Reuters-C, ReutersM and Reuters-G. For each small data set, we created several profiles, one profile for each node in a sub-tree, to simulate multiple users, each with a related, yet separate definition of relevance. All the user profiles on a sub-tree are supposed to share the same prior model distribution. Since this corpus explicitly indicates only the relevant documents for a topic(user), all other documents are considered irrelevant. 5.2 Evaluation We designed the experiments to answer the following three questions: 1. Do we need to take the effort to use a Bayesian approach and learn a prior from other users? 2. Does the new algorithm work better than the standard EM algorithm for learning the Bayesian hierarchical linear model? 3. Can the new algorithm quickly learn many user models? To answer the first question, we compared the Bayesian hierarchical models with commonly used Norm-2 regularized linear regression models. In fact, the commonly used approach is equivalent to the model learned at the end of the first EM iteration. To answer the second question, we compared the proposed new algorithm with the standard EM algorithm to see whether the new learning algorithm is better. To answer the third question, we tested the efficiency of the new algorithm on the entire Netflix data set where about half a million user models need to be learned together. For the MovieLens and Netflix data sets, algorithm effectiveness was measured by mean square error, while on the Reuters data set classification error was used because it was more informative. We first evaluated the performance on each individual user, and then estimated the macro average over all users. Statistical tests (t-tests) were carried out to see whether the results are significant. For the experiments on the MovieLens and Netflix data sets, we used a random sample of 90% of each user for training, and the rest for testing. On Reuters data set, because there are too many relevant documents for each topic in the corpus, we used a random sample of 10% of each topic for training, and 10% of the remaining documents for testing. For all runs, we set (a, b, c, Σ ) = (0.1, 10, 0.1, 1) manually. 6. EXPERIMENTAL RESULTS Figure 2, Figure 3, and Figure 4 show that on all data sets, the Bayesian hierarchical modeling approach has a statistical significant improvement over the regularized linear regression model, which is equivalent to the Bayesian hierarchical models learned at the first iteration. Further analysis shows a negative correlation between the number of training data for a user and the improvement the system gets. This suggests that the borrowing information from other users has more significant improvements for users with less training data, which is as expected. However, the strength of the correlation differs over data sets, and the amount of training data is not the only characteristics that will influence the final performance. Figure 2 and Figure 3 show that the proposed new algorithm works better than the standard EM algorithm on the Netflix and MovieLens data sets. This is not surprising since the number of related feature-users pairs is much smaller than the number of unrelated feature-user pairs on these two data sets, and thus the proposed new algorithm is expected to work better. Figure 4 shows that the two algorithms work similarly on the Reuters-E data set. The accuracy of the new algorithm is similar to that of the standard EM algorithm at each iteration. The general patterns are very similar on other Reuters subsets. Further analysis shows that only 58% of the user-feature pairs are unrelated on this data set. Since the number of unrelated user-feature pairs is not extremely large, the sparseness is not a serious problem on the Reuters data set. Thus the two learning algorithms perform similarly. The results suggest that only on a corpus where the number of unrelated user-feature pairs is much larger than the number of related pairs, such as on the Netflix data set, the proposed technique will get a significant improvement over standard EM. However, the experiments also show that when the assumption does not hold, the new algorithm does not hurt performance. Although the proposed technique is faster than standard Figure 2: Performance on a Netflix subset with 1,000 users. The new algorithm is statistical significantly better than EM algorithm at iterations 2 - 10. Norm-2 regularized linear models are equivalent to the Bayesian hierarchical models learned at the first iteration, and are statistical significantly worse than the Bayesian hierarchical models. 0 2 4 6 8 10 1 1.05 1.1 1.15 1.2 1.25 1.3 1.35 1.4 Iterations MeanSquareError New Algorithm Traditional EM 1 2 3 4 5 6 7 8 9 10 0.33 0.34 0.35 0.36 0.37 0.38 0.39 Iterations ClassificationError New Algorithm Traditional EM Figure 3: Performance on a MovieLens subset with 1,000 users. The new algorithm is statistical significantly better than EM algorithm at iteration 2 to 17 (evaluated with mean square error). 1 6 11 16 21 0.5 1 1.5 2 2.5 3 3.5 Iterations MeanSquareError New Algorithm Traditional EM 1 6 11 16 21 0.35 0.4 0.45 0.5 0.55 0.6 0.65 Iterations ClassificationError New Algorithm Traditional EM Figure 4: Performance on a Reuters-E subset with 26 profiles. Performances on Reuters-C, Reuters-M, Reuters-G are similar. 1 2 3 4 5 0.011 0.0115 0.012 0.0125 0.013 0.0135 0.014 Iterations MeanSquareError New Algorithm Traditional EM 1 2 3 4 5 0.0102 0.0104 0.0106 0.0108 0.011 0.0112 0.0114 Iterations ClassificationError New Algorithm Traditional EM EM, can it really learn millions of user models quickly? Our results show that the modified EM algorithm converges quickly, and 2 - 3 modified EM iterations would result in a reliable estimation. We evaluated the algorithm on the whole Netflix data set (480,189 users, 159,836 features, and 100 million ratings) running on a single CPU PC (2GB memory, P4 3GHz). The system finished one modified EM iteration in about 4 hours. This demonstrates that the proposed technique can efficiently handle large-scale system like Netflix. 7. CONCLUSION Content-based user profile learning is an important problem and is the key to providing personal recommendations to a user, especially for recommending new items with a small number of ratings. The Bayesian hierarchical modeling approach is becoming an important user profile learning approach due to its theoretically justified ability to help one user through information transfer from the other users by way of hyperpriors. This paper examined the weakness of the popular EM based learning approach for Bayesian hierarchical linear models and proposed a better learning technique called Modified EM. We showed that the new technique is theoretically more computationally efficient than the standard EM algorithm. Evaluation on the MovieLens and Netflix data sets demonstrated the effectiveness of the new technique when the data is sparse, by which we mean the ratio of related user-feature pairs to unrelated pairs is small. Evaluation on the Reuters data set showed that the new technique performed similar to the standard EM algorithm when the sparseness condition does not hold. In general, it is better to use the new algorithm since it is as simple as standard EM, the performance is either better or similar to EM, and the computation complexity is lower at each iteration. It is worth mentioning that even if the original problem space is not sparse, sparseness can be created artificially when a recommendation system uses user-specific feature selection techniques to reduce the noise and user model complexity. The proposed technique can also be adapted to improve the learning in such a scenario. We also demonstrated that the proposed technique can learn half a million user profiles from 100 million ratings in a few hours with a single CPU. The research is important because scalability is a major concern for researchers when using the Bayesian hierarchical linear modeling approach to build a practical large scale system, even though the literature have demonstrated the effectiveness of the models in many applications. Our work is one major step on the road to make Bayesian hierarchical linear models more practical. The proposed new technique can be easily adapted to run on a cluster of machines, and thus further speed up the learning process to handle a larger scale system with hundreds of millions of users. The research has much potential to benefit people using EM algorithm on many other IR problems as well as machine learning problems. EM algorithm is a commonly used machine learning technique. It is used to find model parameters in many IR problems where the training data is very sparse. Although we are focusing on the Bayesian hierarchical linear models for recommendation and filtering, the new idea of using analytical solution instead of numerical solution for unrelated user-feature pairs at the M step could be adapted to many other problems. 8. ACKNOWLEDGMENTS We thank Wei Xu, David Lewis and anonymous reviewers for valuable feedback on the work described in this paper. Part of the work was supported by Yahoo, Google, the Petascale Data Storage Institute and the Institute for Scalable Scientific Data Management. Any opinions, findings, conclusions, or recommendations expressed in this material are those of the authors, and do not necessarily reflect those of the sponsors. 9. REFERENCES [1] C. Basu, H. Hirsh, and W. Cohen. Recommendation as classification: Using social and content-based information in recommendation. In Proceedings of the Fifteenth National Conference on Artificial Intelligence, 1998. [2] J. S. Breese, D. Heckerman, and C. Kadie. Empirical analysis of predictive algorithms for collaborative filtering. Technical report, Microsoft Research, One Microsoft Way, Redmond, WA 98052, 1998. [3] J. Callan. Document filtering with inference networks. In Proceedings of the Nineteenth Annual International ACM SIGIR Conference on Research and Development in Information Retrieval, pages 262-269, 1996. [4] N. Cancedda, N. Cesa-Bianchi, A. Conconi, C. Gentile, C. Goutte, T. Graepel, Y. Li, J. M. Renders, J. S. Taylor, and A. Vinokourov. Kernel method for document filtering. In The Eleventh Text REtrieval Conference (TREC11). National Institute of Standards and Technology, special publication 500-249, 2003. [5] C. Chelba and A. Acero. Adaptation of maximum entropy capitalizer: Little data can help a lot. In D. Lin and D. Wu, editors, Proceedings of EMNLP 2004, pages 285-292, Barcelona, Spain, July 2004. Association for Computational Linguistics. [6] B. Croft and J. Lafferty, editors. Language Modeling for Information Retrieval. Kluwer, 2002. [7] A. Dayanik, D. D. Lewis, D. Madigan, V. Menkov, and A. Genkin. Constructing informative prior distributions from domain knowledge in text classification. In SIGIR 06: Proceedings of the 29th annual international ACM SIGIR conference on Research and development in information retrieval, pages 493-500, New York, NY, USA, 2006. ACM Press. [8] J. Delgado and N. Ishii. Memory-based weightedmajority prediction for recommender systems. In ACM SIGIR99 Workshop on Recommender Systems, 1999. [9] GroupLens. Movielens. http://www.grouplens.org/taxonomy/term/14, 2006. [10] D. Heckerman. A tutorial on learning with bayesian networks. In M. Jordan, editor, Learning in Graphical Models. Kluwer Academic, 1998. [11] J. L. Herlocker, J. A. Konstan, A. Borchers, and J. Riedl. An algorithmic framework for performing collaborative filtering. In SIGIR 99: Proceedings of the 22nd annual international ACM SIGIR conference on Research and development in information retrieval, pages 230-237, New York, NY, USA, 1999. ACM Press. [12] T. Hofmann and J. Puzicha. Latent class models for collaborative filtering. In IJCAI 99: Proceedings of the Sixteenth International Joint Conference on Artificial Intelligence, pages 688-693, San Francisco, CA, USA, 1999. Morgan Kaufmann Publishers Inc. [13] I. M. D. (IMDB). Internet movie database. http://www.imdb.com/interfaces/, 2006. [14] R. Jin, J. Y. Chai, and L. Si. An automatic weighting scheme for collaborative filtering. In SIGIR 04: Proceedings of the 27th annual international ACM SIGIR conference on Research and development in information retrieval, pages 337-344, New York, NY, USA, 2004. ACM Press. [15] J. A. Konstan, B. N. Miller, D. Maltz, J. L. Herlocker, L. R. Gordon, and J. Riedl. GroupLens: Applying collaborative filtering to Usenet news. Communications of the ACM, 40(3):77-87, 1997. [16] D. Lewis. Applying support vector machines to the TREC-2001 batch filtering and routing tasks. In Proceedings of the Eleventh Text REtrieval Conference (TREC-11), 2002. [17] B. Liu, X. Li, W. S. Lee, , and P. Yu. Text classification by labeling words. In Proceedings of The Nineteenth National Conference on Artificial Intelligence (AAAI-2004), July 25-29, 2004. [18] P. Melville, R. J. Mooney, and R. Nagarajan. Content-boosted collaborative filtering for improved recommendations. In Proceedings of the Eighteenth National Conference on Artificial Intelligence (AAAI-2002), Edmonton, Canada, 2002. [19] Netflix. Netflix prize. http://www.netflixprize.com (visited on Nov. 30, 2006), 2006. [20] S. Robertson and K. Sparck-Jones. Relevance weighting of search terms. In Journal of the American Society for Information Science, volume 27, pages 129-146, 1976. [21] J. Wang, A. P. de Vries, and M. J. T. Reinders. Unifying user-based and item-based collaborative filtering approaches by similarity fusion. In SIGIR 06: Proceedings of the 29th annual international ACM SIGIR conference on Research and development in information retrieval, pages 501-508, New York, NY, USA, 2006. ACM Press. [22] X. Wu and R. K. Srihari. Incorporating prior knowledge with weighted margin support vector machines. In Proc. ACM Knowledge Discovery Data Mining Conf.(ACM SIGKDD 2004), Aug. 2004. [23] Y. Yang, S. Yoo, J. Zhang, and B. Kisiel. Robustness of adaptive filtering methods in a cross-benchmark evaluation. In Proceedings of the 28th Annual International ACM SIGIR Conference on Research and Development in Information Retrieval, 2005. [24] K. Yu, V. Tresp, and A. Schwaighofer. Learning gaussian processes from multiple tasks. In ICML 05: Proceedings of the 22nd international conference on Machine learning, pages 1012-1019, New York, NY, USA, 2005. ACM Press. [25] K. Yu, V. Tresp, and S. Yu. A nonparametric hierarchical bayesian framework for information filtering. In SIGIR 04: Proceedings of the 27th annual international ACM SIGIR conference on Research and development in information retrieval, pages 353-360. ACM Press, 2004. [26] X. Zhu. Semi-supervised learning literature survey. Technical report, University of Wisconsin - Madison, December 9, 2006. [27] P. Zigoris and Y. Zhang. Bayesian adaptive user profiling with explicit & implicit feedback. In Conference on Information and Knowledge Mangement 2006, 2006.",
    "original_translation": "Modelado de usuarios jerárquicos bayesianos eficientes para sistemas de recomendaciones Yi Zhang, Jonathan Koren School of Engineering University of California Santa Cruz Santa Cruz, CA, EE. UU. {Yiz, jonathanh.@soe.ucsc.edu Resumen Un sistema de recomendación personalizado basado en el contenido Aprende perfiles específicos del usuario del usuario Aprende usuarios de los usuarios.desde los comentarios de los usuarios para que pueda entregar información adaptada al interés de cada usuarios individuales. Un sistema que sirve a millones de usuarios puede aprender un mejor perfil de usuario para un nuevo usuario, o un usuario con pocos comentarios, tomando prestada información de otros usuarios mediante el uso de un modelo jerárquico bayesiano. Aprender los parámetros del modelo para optimizar la probabilidad de datos conjuntos de millones de usuarios es muy costoso computacionalmente. El algoritmo EM comúnmente utilizado converge muy lentamente debido a la escasez de los datos en aplicaciones IR. Este documento propone una nueva técnica de aprendizaje rápido para aprender una gran cantidad de perfiles de usuarios individuales. La teoría de la eficacia y eficiencia del algoritmo propuesto se justifican por la teoría y se demuestran en los datos reales del usuario de Netflix y Movielens. Categorías y descriptores de asignaturas: B.3.3 [Búsqueda y recuperación de información]: Filtrado de información Términos generales: Algoritmos 1. La personalización de la introducción es el futuro de la web, y ha logrado un gran éxito en las aplicaciones industriales. Por ejemplo, las tiendas en línea, como Amazon y Netflix, proporcionan recomendaciones personalizadas para productos o servicios adicionales basados en el historial de un usuario. Ofertas recientes como My MSN, My Yahoo!, My Google y Google News han atraído mucha atención debido a su capacidad potencial para inferir los intereses de los usuarios de su historial. Un tema de personalización importante estudiado en la comunidad de recuperación de información son los sistemas de recomendación personal basados en contenido1. Estos sistemas aprenden perfiles específicos del usuario de los comentarios de los usuarios para que puedan recomendar información adaptada al interés de cada usuarios individuales sin requerir que el usuario haga una consulta explícita. Aprender los perfiles del usuario es el problema central para estos sistemas. Un perfil de usuario suele ser un clasificador que puede identificar si un documento es relevante para el usuario o no, o un modelo de regresión que indica cuán relevante es un documento para el usuario. Un desafío importante de construir un sistema de recomendación o personalización es que el perfil aprendido para un usuario en particular suele ser de baja calidad cuando la cantidad de datos de ese usuario en particular es pequeña. Esto se conoce como el problema de inicio en frío. Esto significa que cualquier usuario nuevo debe soportar un rendimiento inicial deficiente hasta que se proporcione comentarios suficientes de ese usuario para aprender un perfil de usuario confiable. Ha habido mucha investigación sobre la mejora de la precisión de la clasificación cuando la cantidad de datos de capacitación etiquetados es pequeña. El enfoque de aprendizaje semi-supervisado combina datos no etiquetados y etiquetados juntos para lograr este objetivo [26]. Otro enfoque es usar el conocimiento del dominio. Los investigadores han modificado diferentes algoritmos de aprendizaje, como Na¨ıvebayes [17], regresión logística [7] y SVM [22], para integrar el conocimiento del dominio en un clasificador de texto. El tercer enfoque es tomar prestados datos de capacitación de otros recursos [5] [7]. La efectividad de estos diferentes enfoques es mixta, debido a qué tan bien, la suposición del modelo subyacente se ajusta a los datos. Un enfoque bien recibido para mejorar el rendimiento del sistema de recomendación para un usuario en particular es tomar prestada información de otros usuarios a través de un enfoque de modelado jerárquico bayesiano. Varios investigadores han demostrado que este enfoque se cotiza efectivamente entre la información compartida y específica del usuario, aliviando así el bajo rendimiento inicial para cada usuario [27] [25]. Para aprender un modelo jerárquico bayesiano, el sistema generalmente trata de encontrar los parámetros del modelo más probables para los datos dados. Un sistema de recomendación maduro generalmente funciona para millones de usuarios. Es bien sabido que aprender los parámetros óptimos de un modelo jerárquico bayesiano es computacionalmente costoso cuando hay miles o millones de usuarios. El algoritmo EM es una técnica de uso común para el aprendizaje de parámetros debido a su garantía de simplicidad y convergencia. Sin embargo, un sistema de recomendación basado en el contenido a menudo maneja documentos en un espacio dimensional muy alto, en el que cada documento está representado por un vector muy escaso. Con un análisis cuidadoso del algoritmo EM en este escenario (Sección 4), encontramos que el EM Tering o el filtrado colaborativo basado en elementos. En este artículo, las palabras filtrado y recomendación se utilizan indistintamente.El algoritmo converge muy lentamente debido a la escasez de las variables de entrada. También encontramos que actualizar el parámetro del modelo en cada iteración EM también es costosa con la complejidad computacional de O (MK), donde M es el número de usuarios y K es el número de dimensiones. Este documento modifica el algoritmo EM estándar para crear un algoritmo de aprendizaje mejorado, que llamamos el algoritmo EM modificado. La idea básica es que en lugar de calcular la solución numérica para todos los parámetros del perfil de usuario, derivamos la solución analítica de los parámetros para algunas dimensiones de características, y en el paso M usamos la solución analítica en lugar de la solución numérica estimada para Eesos parámetros. Esto reduce en gran medida el cálculo en una sola iteración EM, y también tiene el beneficio de aumentar la velocidad de convergencia del algoritmo de aprendizaje. La técnica propuesta no solo está bien respaldada por la teoría, sino también por los resultados experimentales. La organización de las partes restantes de este documento es la siguiente: la Sección 3 describe el marco de modelado de regresión lineal jerárquica bayesiana utilizada para recomendaciones basadas en contenido. La Sección 4 describe cómo aprender los parámetros del modelo utilizando el algoritmo EM estándar, junto con el uso de la nueva técnica propuesta en este documento. La configuración y los resultados experimentales utilizados para validar la técnica de aprendizaje propuesta se informan en las Secciones 5 y 6. La Sección 7 resume y ofrece comentarios finales.2. El trabajo relacionado que proporciona recomendaciones personalizadas a los usuarios se ha identificado como un problema muy importante en la comunidad IR desde la década de 1970. Los enfoques que se han utilizado para resolver este problema pueden clasificarse aproximadamente en dos categorías principales: filtrado basado en contenido versus filtrado colaborativo. El filtrado basado en contenido estudia el escenario en el que un sistema de recomendación monitorea una secuencia de documentos y presiona documentos que coinciden con un perfil de usuario con el usuario correspondiente. El usuario puede leer los documentos entregados y proporcionar comentarios de relevancia explícita, que el sistema de filtrado utiliza para actualizar el perfil de los usuarios utilizando modelos de recuperación de retroalimentación relevante (p. Ej. Modelos booleanos, modelos de espacio vectorial, modelos probabilísticos tradicionales [20], redes de inferencia [3] y modelos de lenguaje [6]) o algoritmos de aprendizaje automático (p. Ej. Soporte de máquinas vectoriales (SVM), K Vecinos más cercanos (K-NN) Agrupación, redes neuronales, regresión logística o Winnow [16] [4] [23]). El filtrado colaborativo va más allá del simplemente usar contenido de documento para recomendar elementos a un usuario aprovechando la información de otros usuarios con gustos y preferencias similares en el pasado. La heurística basada en la memoria y los enfoques basados en modelos se han utilizado en la tarea de filtrado colaborativo [15] [8] [2] [14] [12] [11]. Este documento contribuye a la investigación de recomendación basada en el contenido al mejorar la eficiencia y la efectividad de los modelos lineales jerárquicos bayesianos, que tienen una sólida base teórica y un buen rendimiento empírico en las tareas de recomendación [27] [25]. Este documento no tiene la intención de comparar el filtrado basado en contenido con filtrado colaborativo o afirmar cuál es mejor. Creemos que cada uno complementa al otro, y ese filtrado basado en contenido es extremadamente útil para manejar nuevos documentos/elementos con poco o ningún comentario de los usuarios. Similar a algunos otros investigadores [18] [1] [21], encontramos que un sistema de recomendación será más efectivo cuando se combinen ambas técnicas. Sin embargo, esto está más allá del alcance de este documento y, por lo tanto, no se discute aquí.3. La regresión lineal jerárquica bayesiana supone que hay M usuarios en el sistema. La tarea del sistema es recomendar documentos que sean relevantes para cada usuario. Para cada usuario, el sistema aprende un modelo de usuario del historial de usuarios. En el resto de este documento, utilizaremos las siguientes anotaciones para representar las variables en el sistema.M = 1, 2, ..., M: El índice para cada usuario individual. M es el número total de usuarios.WM: El parámetro del modelo de usuario asociado con el usuario m.WM es un vector dimensional K.J = 1, 2, ..., JM: el índice para un conjunto de datos para el usuario m.JM es el número de datos de capacitación para el usuario m.Dm = {(xm, j, ym, j)}: un conjunto de datos asociados con el usuario m.XM, J es un vector dimensional K que representa el MTH Usuarios JTH Training Document.2 ym, J es un escalar que representa la etiqueta del documento XM, j.k = 1, 2, ..., k: el índice dimensional de la variable de entrada x. El enfoque de modelado jerárquico bayesiano se ha utilizado ampliamente en aplicaciones de recuperación de información del mundo real. Los modelos lineales jerárquicos bayesianos generalizados, uno de los modelos jerárquicos bayesianos más simples, se usan comúnmente y han logrado un buen rendimiento en el filtrado colaborativo [25] y las tareas de filtrado adaptativo basado en contenido [27]. La Figura 1 muestra la representación gráfica de un modelo jerárquico bayesiano. En este gráfico, cada modelo de usuario está representado por un vector aleatorio WM. Suponemos que un modelo de usuario se muestrea aleatoriamente de una distribución previa P (W | φ). El sistema puede predecir la etiqueta del usuario y de un documento X dada una estimación de WM (o distribución de WMS) utilizando una función y = f (x, w). El modelo se llama modelo lineal jerárquico bayesiano generalizado cuando Y = F (WT x) es cualquier modelo lineal generalizado como regresión logística, SVM y regresión lineal. Para estimar de manera confiable el modelo de usuario WM, el sistema puede pedir prestada información de otros usuarios a través del anterior φ = (µ, σ). Ahora observamos un modelo comúnmente utilizado donde y = wt x +, donde ∼ n (0, σ2) es un ruido aleatorio [25] [27]. Suponga que cada modelo de usuario WM es un sorteo independiente de una distribución de población P (W | φ), que se rige por algún hiperparámetro desconocido φ. Deje que la distribución previa del modelo de usuario W sea una distribución gaussiana con el parámetro φ = (µ, σ), que es el previo comúnmente utilizado para modelos lineales.µ = (µ1, µ2, ..., µK) es un vector k dimensional que representa la media de la distribución gaussiana, y σ es la matriz de covarianza del gaussiano. Por lo general, una distribución normal n (0, ai) y una distribución inversa de wishart p (σ) ∝ | σ | - 1 2 b exp (−1 2 ctr (σ - 1)) se usan como hiperprior para modelar la distribución previa deµ y σ respectivamente. I es la matriz de identidad dimensional K, y A, B y C son números reales. Con estas configuraciones, tenemos el siguiente modelo para el sistema: 1. µ y σ se muestrean a partir de n (0, ai) e iwν (ai), respectivamente.2 La primera dimensión de X es una variable ficticia que siempre es igual a 1. Figura 1: Ilustración de dependencias de variables en el modelo jerárquico. La calificación, y, para un documento, x, está condicionado en el documento y el modelo de usuario, WM, asociado con el usuario m.Los usuarios comparten información sobre sus modelos a través del anterior, φ = (µ, σ).2. Para cada usuario M, WM se muestrean aleatoriamente a partir de una distribución normal: WM ∼ N (µ, σ2) 3. Para cada ítem XM, J, YM, J se muestrean aleatoriamente de una distribución normal: YM, J ∼ N (WT MXM, J, σ2). Deje θ = (φ, w1, w2, ..., wm) representan los parámetros de este sistema que deben estimarse. La probabilidad conjunta para todas las variables en el modelo probabilístico, que incluye los datos y los parámetros, es: P (D, θ) = P (φ) M P (Wm | φ) J P (YM, J | XM, J, wm) (1) Por simplicidad, suponemos que A, B, C y σ se proporcionan al sistema.4. Aprendizaje de parámetros del modelo Si se conoce el φ anterior, encontrar el WM óptimo es sencillo: es una regresión lineal simple. Por lo tanto, nos centraremos en estimar φ. La solución a priori máxima de φ viene dada por φmap = arg max φ p (φ | d) (2) = arg max φ p (φ, d) p (d) (3) = arg max φ p (d | φ) P (φ) (4) = arg max φ w p (d | w, φ) p (w | φ) p (φ) dw (5) Encontrar la solución óptima para el problema anterior es un desafío, ya que necesitamosIntegrar sobre todo W = (W1, W2, ..., WM), que son variables ocultas no observadas.4.1 Algoritmo EM para modelos lineales jerárquicos bayesianos en la ecuación 5, φ es el parámetro que debe estimarse, y el resultado depende de variables latentes no observadas w.Este tipo de problema de optimización generalmente se resuelve mediante el algoritmo EM. Aplicando Em al problema anterior, el conjunto de modelos de usuario W son las variables ocultas no observables y tenemos: Q = W P (W | µ, σ2, DM) log P (µ, σ2, W, D) DW basado en elDerivación de las fórmulas EM presentadas en [24], tenemos los siguientes pasos de maximización de expectativas para encontrar los hiperparámetros óptimos. Para las consideraciones de espacio, omitimos la derivación en este documento, ya que no es el foco de nuestro trabajo. E PASO: Para cada usuario m, estime la distribución del modelo de usuario P (WM | DM, φ) = N (Wm; ¯wm, σ2 m) basado en la estimación actual del anterior φ = (µ, σ2).¯WM = (((σ2) −1 + sxx, m σ2) −1 (sxy, m σ2 + (σ2) −1 µ)(7) Donde SXX, M = J XM, JXT M, J SXY, M = J XM, JYM, J M Paso: Optimice el anterior φ = (µ, σ2) en función de la estimación del último paso E.µ = 1 m m ¯wm (8) σ2 = 1 m m σ2 m + (¯wm - µ) (¯wm - µ) t (9) Muchos sistemas IR impulsados por el aprendizaje automático usan una estimación puntual de los parámetros en diferentes etapasen el sistema. Sin embargo, estamos estimando la distribución posterior de las variables en el paso E. Esto evita el sobreajuste de WM a los datos de los usuarios en particular, que pueden ser pequeños y ruidosos. Una discusión detallada sobre este tema aparece en [10].4.2 Nuevo algoritmo: EM modificado Aunque el algoritmo EM está ampliamente estudiado y utilizado en aplicaciones de aprendizaje automático, utilizando el proceso EM anterior para resolver modelos lineales jerárquicos bayesianos en los sistemas de recuperación de información a gran escala todavía es demasiado costoso. En esta sección, describimos por qué la tasa de aprendizaje del algoritmo EM es lenta en nuestra aplicación e introduce una nueva técnica para hacer que el aprendizaje del modelo lineal jerárquico bayesiano sea escalable. La derivación del nuevo algoritmo de aprendizaje se basará en el algoritmo EM descrito en la sección anterior. Primero, las matrices de covarianza σ2, σ2 m generalmente son demasiado grandes para ser computacionalmente factibles. Por simplicidad, y como práctica común en IR, no modelamos la correlación entre las características. Por lo tanto, aproximamos estas matrices con k dimensionales matrices diagonales. En el resto del documento, usamos estos símbolos para representar sus aproximaciones diagonales: σ2 =     σ2 1 0 .. 0 0 σ2 2 .. 0 .. .. .. .. 0 0 .. σ2 k    σ2 m =     σ2 m, 1 0 .. 0 0 σ2 m, 2 .. 0 .. .. .. 0 0 .. σ2 m, k    en segundo lugar, y la mayoríaEs importante destacar que el espacio de entrada es muy escaso y hay muchas dimensiones que no están relacionadas con un usuario en particular en una aplicación IR real. Por ejemplo, consideremos un sistema de recomendación de películas, con la variable de entrada X que representa una película en particular. Para la película JTH que el usuario M ha visto, Let XM, J, K = 1 si el director de la película es Jean-Pierre Jeunet (indexado por K). Aquí suponemos que si este director dirigió o no una película específica está representada por la dimensión KTH. Si el usuario M nunca ha visto una película dirigida por Jean-Pierre Jeunet, entonces la dimensión correspondiente es siempre cero (xm, j, k = 0 para todo j). Un gran inconveniente del algoritmo EM es que la importancia de una característica, µK, puede estar muy dominada por usuarios que nunca han encontrado esta característica (es decir, J XM, J, K = 0) en el paso M (Ecuación 8). Suponga que 100 de 1 millón de usuarios han visto la película dirigida por Jean-Pierre Jeunet, y que los espectadores han calificado todas sus películas como excelentes. Intuitivamente, es un buen director y el peso para él (µK) debería ser alto. Antes de la iteración EM, el valor inicial de µ generalmente se establece en 0. Dado que los otros 999,900 usuarios no han visto esta película, sus pesos correspondientes (W1, K, W2, K, ..., Wm, K ..., W999900, k) para ese director sería muy pequeño inicialmente. Por lo tanto, el peso correspondiente del director en el µK anterior en el primer paso m sería muy bajo, y la varianza σm, K será grande (Ecuaciones 8 y 7). Es indeseable que los usuarios que nunca han visto ninguna película producida por el director influyen tanto en la importancia del director. Esto hace que la convergencia del algoritmo EM estándar sea muy lento. Ahora veamos si podemos mejorar la velocidad de aprendizaje del algoritmo. Sin una pérdida de generalidad, supongamos que la dimensión KTH de la variable de entrada x no está relacionada con un usuario en particular m.Por lo que queremos decir, xm, j, k = 0 para todos j = 1, ..., jm. Es sencillo demostrar que la fila KTH y la columna KTH de SXX, M están completamente llenas de ceros, y que la dimensión KTH de SXY, M también está a cero. Por lo tanto, la dimensión KTH correspondiente de los modelos de usuario significa, ¯wm, debe ser igual a la del anterior: ¯wm, k = µK, con la covarianza correspondiente de σm, k = σk. En el paso M, el algoritmo EM estándar utiliza la solución numérica de la distribución P (WM | DM, φ) estimada en E PASO (Ecuación 8 y Ecuación 7). Sin embargo, las soluciones numéricas son muy poco confiables para ¯wm, k y σm, k cuando la dimensión KTH no está relacionada con el usuario MTH. Un mejor enfoque es utilizar las soluciones analíticas ¯wm, k = µK y σm, k = σk para los pares no relacionados (M, K), junto con la solución numérica estimada en el paso E para los otros pares (M, K). Por lo tanto, obtenemos el siguiente nuevo algoritmo similar a EM: paso E modificado: para cada usuario m, estimar la distribución del modelo de usuario P (WM | DM, φ) = n (Wm; ¯wm, σ2 m) basado en la estimación actual deσ, µ, σ2.¯wm = ((σ2) −1 + sxx, m σ2) −1 (sxy, m σ2 + (σ2) −1 µ) (10) σ2 m, k = ((σ2 k) −1 + sxx, m,k σ2) −1 (11) donde sxx, m, k = j x2 m, j, k y sxy, m, k = j xm, j, kym, j paso modificado M paso optimizar el anterior φ = (µ, σ2)Basado en la estimación del último paso E para pares de información de usuario relacionados. El paso M utiliza implícitamente la solución analítica para pares de características de usuario no relacionadas.µk = 1 mk m: ¯wm relacionado, k (12) σ2 k = 1 mk m: relacionado σ2 m, k +(¯wm, k - µk) (¯wm, k - µK) t (13) donde mk esEl número de usuarios relacionados con la característica K solo estimamos la diagonal de σ2 m y σ ya que estamos utilizando la aproximación diagonal de las matrices de covarianza. Para estimar ¯wm, solo necesitamos calcular las soluciones numéricas para las dimensiones relacionadas con el usuario m.Para estimar σ2 K y µK, solo resumimos a los usuarios relacionados con la función KTH. Hay dos beneficios principales del nuevo algoritmo. Primero, debido a que solo se necesitan los pares relacionados (M, K) en el paso M modificado, la complejidad computacional en una sola iteración EM es mucho menor cuando los datos son escasos, y muchos de los pares (M, K) no están relacionados. En segundo lugar, los parámetros estimados en el paso M modificado (Ecuaciones 12 - 13) son más precisos que el paso M estándar descrito en la Sección 4.1 porque las soluciones analíticas exactas ¯wm, k = µK y σm, k = σk para los no relacionados (M, k) Se usaron pares en el nuevo algoritmo en lugar de una solución aproximada como en el algoritmo estándar.5. Metodología experimental 5.1 Conjunto de datos de evaluación Para evaluar la técnica propuesta, utilizamos los siguientes tres conjuntos de datos principales (Tabla 1): Datos de Movielens: este conjunto de datos se creó combinando los juicios de relevancia del conjunto de datos de Movielens [9] con documentos delBase de datos de películas de Internet (IMDB). Movielens permite a los usuarios clasificar cuánto disfrutó de una película específica en una escala del 1 al 5. Esta calificación de simpatía se utilizó como una medición de cuán relevante es el documento que representa la película correspondiente para el usuario. Consideramos documentos con puntajes de simpatía de 4 o 5 como relevantes, y documentos con un puntaje de 1 a 3 como irrelevante para el usuario. Movielens proporcionó juicios de relevancia en 3.057 documentos de 6.040 usuarios separados. En promedio, cada usuario calificó 151 películas, de estas 87 se consideró relevante. El puntaje promedio para un documento fue de 3.58. Los documentos que representan cada película se construyeron a partir de la parte de la base de datos IMDB que está disponible para la descarga pública [13]. Según esta base de datos, creamos un documento por película que contenía la información relevante al respecto (por ejemplo, directores, actores, etc.). Tabla 1: Estadísticas del conjunto de datos. En Reuters, el número de calificación para un usuario simulado es el número de documentos relevantes para el tema correspondiente. Usuarios de datos Documentos de documentos por usuario MOVIELENS 6,040 3,057 151 Netflix-All 480,189 17,770 208 Netflix-1000 1000 17,770 127 Reuters-C 34 100,000 3949 Reuters-E 26 100,000 1632 Reuters-G 33 100,000 2222 Reuters-M 10 100,000 6529 Datos de Netflix: Esto: esto: ESTO: ESTO: ESTO INTERIOR DE LA VEUTERS-1632 REUTERS-G 33 100,000 2222 REUTERS-M 10 100,000 6529 DATOS NETFLIX: ESTO: ESTO: ESTO ESTO: ESTO ESTO: ESTO ESTA CONTERECTORES DELEl conjunto de datos se construyó combinando documentos sobre películas que se arrastran desde la web con un conjunto de juicios de relevancia del cliente de alquiler de películas reales de Netflix [19]. Netflix proporciona públicamente los juicios de relevancia de 480,189 clientes anónimos. Hay alrededor de 100 millones de calificaciones en una escala de 1 a 5 para 17,770 documentos. Similar a Movielens, consideramos documentos con puntajes de simpatía de 4 o 5 como relevantes. Este número se redujo a 1000 clientes a través de un muestreo aleatorio. El cliente promedio en el conjunto de datos reducido proporcionó 127 juicios, y 70 se consideran relevantes. El puntaje promedio de los documentos es de 3.55. Datos de Reuters: Este es el Corpus de Reuters, Volumen 1. Cubre 810,000 historias de noticias en inglés Reuters del 20 de agosto de 1996 al 19 de agosto de 1997. Solo se usaron las primeras 100,000 noticias en nuestros experimentos. El Corpus de Reuters viene con una jerarquía de temas. Cada documento se asigna a uno de varios lugares en el árbol jerárquico. El primer nivel del árbol contiene cuatro temas, denotados como C, E, M y G. Para los experimentos en este documento, el árbol se cortó en el nivel 1 para crear cuatro árboles más pequeños, cada uno de los cuales corresponde a un conjunto de datos más pequeño.: Reuters-e Reuters-C, Reutersm y Reuters-G.Para cada pequeño conjunto de datos, creamos varios perfiles, un perfil para cada nodo en un sub-árbol, para simular múltiples usuarios, cada uno con una definición de relevancia relacionada pero separada. Se supone que todos los perfiles de usuario en un sub-árbol comparten la misma distribución del modelo anterior. Dado que este corpus indica explícitamente solo los documentos relevantes para un tema (usuario), todos los demás documentos se consideran irrelevantes.5.2 Evaluación diseñamos los experimentos para responder las siguientes tres preguntas: 1. ¿Necesitamos hacer el esfuerzo para usar un enfoque bayesiano y aprender un anterior de otros usuarios?2. ¿El nuevo algoritmo funciona mejor que el algoritmo EM estándar para aprender el modelo lineal jerárquico bayesiano?3. ¿Puede el nuevo algoritmo aprender rápidamente muchos modelos de usuarios? Para responder a la primera pregunta, comparamos los modelos jerárquicos bayesianos con modelos de regresión lineal regularizados de normas-2 comúnmente utilizados. De hecho, el enfoque comúnmente utilizado es equivalente al modelo aprendido al final de la primera iteración em. Para responder a la segunda pregunta, comparamos el nuevo algoritmo propuesto con el algoritmo EM estándar para ver si el nuevo algoritmo de aprendizaje es mejor. Para responder a la tercera pregunta, probamos la eficiencia del nuevo algoritmo en todo el conjunto de datos de Netflix, donde se deben aprender juntos aproximadamente medio millón de modelos de usuarios. Para los conjuntos de datos de Movielens y Netflix, la efectividad del algoritmo se midió por error medio cuadrado, mientras que en el error de clasificación de datos de Reuters se utilizó porque era más informativo. Primero evaluamos el rendimiento en cada usuario individual y luego estimamos el promedio macro en todos los usuarios. Las pruebas estadísticas (pruebas t) se llevaron a cabo para ver si los resultados son significativos. Para los experimentos en los conjuntos de datos de Movielens y Netflix, utilizamos una muestra aleatoria de 90% de cada usuario para capacitación y el resto para pruebas. En el conjunto de datos de Reuters, debido a que hay demasiados documentos relevantes para cada tema en el corpus, utilizamos una muestra aleatoria del 10% de cada tema para la capacitación y el 10% de los documentos restantes para las pruebas. Para todas las ejecuciones, establecemos (A, B, C, σ) = (0.1, 10, 0.1, 1) manualmente.6. Resultados experimentales La Figura 2, la Figura 3 y la Figura 4 muestran que en todos los conjuntos de datos, el enfoque de modelado jerárquico bayesiano tiene una mejora estadística significativa sobre el modelo de regresión lineal regularizado, que es equivalente a los modelos jerárquicos bayesianos aprendidos en la primera iteración. El análisis posterior muestra una correlación negativa entre el número de datos de capacitación para un usuario y la mejora que obtiene el sistema. Esto sugiere que la información de préstamo de otros usuarios tiene mejoras más significativas para los usuarios con menos datos de capacitación, lo cual es el esperado. Sin embargo, la fuerza de la correlación difiere sobre los conjuntos de datos, y la cantidad de datos de entrenamiento no son las únicas características que influirán en el rendimiento final. La Figura 2 y la Figura 3 muestran que el nuevo algoritmo propuesto funciona mejor que el algoritmo EM estándar en los conjuntos de datos de Netflix y Movielens. Esto no es sorprendente ya que el número de pares de usuarios de características relacionados es mucho menor que el número de pares de funciones no relacionados en estos dos conjuntos de datos, y por lo tanto se espera que el nuevo algoritmo propuesto funcione mejor. La Figura 4 muestra que los dos algoritmos funcionan de manera similar en el conjunto de datos Reuters-E. La precisión del nuevo algoritmo es similar a la del algoritmo EM estándar en cada iteración. Los patrones generales son muy similares en otros subconjuntos de Reuters. Un análisis posterior muestra que solo el 58% de los pares de características de usuario no están relacionados con este conjunto de datos. Dado que el número de pares de funciones de usuario no relacionados no es extremadamente grande, la escasez no es un problema grave en el conjunto de datos de Reuters. Así, los dos algoritmos de aprendizaje funcionan de manera similar. Los resultados sugieren que solo en un corpus donde el número de pares de funciones de usuario no relacionados es mucho mayor que el número de pares relacionados, como en el conjunto de datos de Netflix, la técnica propuesta obtendrá una mejora significativa sobre el EM estándar. Sin embargo, los experimentos también muestran que cuando la suposición no se mantiene, el nuevo algoritmo no perjudica el rendimiento. Aunque la técnica propuesta es más rápida que la Figura 2 estándar: rendimiento en un subconjunto de Netflix con 1,000 usuarios. El nuevo algoritmo es estadístico significativamente mejor que el algoritmo EM en las iteraciones 2 - 10. Los modelos lineales regularizados de Norm-2 son equivalentes a los modelos jerárquicos bayesianos aprendidos en la primera iteración, y son estadísticas significativamente peores que los modelos jerárquicos bayesianos.0 2 4 6 8 10 1 1.05 1.1 1.15 1.2 1.25 1.3 1.35 1.4 iteraciones Subles Significador Nuevo algoritmo tradicional EM 1 2 3 4 5 6 7 8 9 10 0.33 0.34 0.35 0.36 0.37 0.38 0.39 Clasificación de iteraciones Sorror Nuevo Algoritmo tradicional EM Figura 3: rendimiento en unMOVIELENS subconjunto con 1,000 usuarios. El nuevo algoritmo es estadístico significativamente mejor que el algoritmo EM en la iteración 2 a 17 (evaluado con un error cuadrado medio).1 6 11 16 21 0.5 1 1.5 2 2.5 3 3.5 Iterations significa Curar algoritmo nuevo tradicional EM 1 6 11 16 21 21 0.35 0.4 0.45 0.5 0.55 0.6 0.65 Clasificación de iteraciones El nuevo algoritmo tradicional EM Figura 4: rendimiento en un subconjunto de reuters-E con 26 perfiles. Las actuaciones en Reuters-C, Reuters-M, Reuters-G son similares.1 2 3 4 5 0.011 0.0115 0.012 0.0125 0.013 0.0135 0.014 iteraciones significa QuarareRor nuevo algoritmo tradicional EM 1 2 3 4 5 0.0102 0.0104 0.0106 0.0108 0.011 0.0112 0.0114 clasificaciones de iteraciones Strensor New algoritmo EMM tradicional EM, ¿puede realmente aprender? Nuestros resultados muestran que el algoritmo EM modificado converge rápidamente, y 2 a 3 iteraciones EM modificadas darían como resultado una estimación confiable. Evaluamos el algoritmo en todo el conjunto de datos de Netflix (480,189 usuarios, 159,836 características y 100 millones de calificaciones) que se ejecutan en una sola PC de CPU (memoria de 2 GB, P4 3GHz). El sistema terminó una iteración EM modificada en aproximadamente 4 horas. Esto demuestra que la técnica propuesta puede manejar eficientemente un sistema a gran escala como Netflix.7. Conclusión El aprendizaje de perfil de usuario basado en contenido es un problema importante y es la clave para proporcionar recomendaciones personales a un usuario, especialmente para recomendar nuevos elementos con un pequeño número de calificaciones. El enfoque de modelado jerárquico bayesiano se está convirtiendo en un importante enfoque de aprendizaje de perfil de usuario debido a su capacidad teóricamente justificada para ayudar a un usuario a través de la transferencia de información de los otros usuarios a través de hiperpriors. Este documento examinó la debilidad del popular enfoque de aprendizaje basado en EM para los modelos lineales jerárquicos bayesianos y propuso una mejor técnica de aprendizaje llamada Modified EM. Mostramos que la nueva técnica es teóricamente más computacionalmente eficiente que el algoritmo EM estándar. La evaluación en los conjuntos de datos de Movielens y Netflix demostró la efectividad de la nueva técnica cuando los datos son escasos, por lo que nos referimos a la relación de pares de características de usuario relacionadas con pares no relacionados es pequeña. La evaluación en el conjunto de datos de Reuters mostró que la nueva técnica se realizó de manera similar al algoritmo EM estándar cuando la condición de escasez no se mantiene. En general, es mejor usar el nuevo algoritmo ya que es tan simple como EM estándar, el rendimiento es mejor o similar al EM, y la complejidad del cálculo es menor en cada iteración. Vale la pena mencionar que incluso si el espacio de problemas original no es escaso, la escasez se puede crear artificialmente cuando un sistema de recomendación utiliza técnicas de selección de características específicas del usuario para reducir el ruido y la complejidad del modelo de usuario. La técnica propuesta también se puede adaptar para mejorar el aprendizaje en dicho escenario. También demostramos que la técnica propuesta puede aprender medio millón de perfiles de usuario de 100 millones de calificaciones en unas pocas horas con una sola CPU. La investigación es importante porque la escalabilidad es una preocupación importante para los investigadores cuando utilizan el enfoque de modelado lineal jerárquico bayesiano para construir un sistema práctico de gran escala, a pesar de que la literatura ha demostrado la efectividad de los modelos en muchas aplicaciones. Nuestro trabajo es un paso importante en el camino para hacer que los modelos lineales jerárquicos bayesianos sean más prácticos. La nueva técnica propuesta se puede adaptar fácilmente para ejecutarse en un clúster de máquinas y, por lo tanto, acelerar aún más el proceso de aprendizaje para manejar un sistema de mayor escala con cientos de millones de usuarios. La investigación tiene mucho potencial para beneficiar a las personas que usan el algoritmo EM en muchos otros problemas de IR, así como problemas de aprendizaje automático. El algoritmo EM es una técnica de aprendizaje automático de uso común. Se utiliza para encontrar parámetros del modelo en muchos problemas IR donde los datos de entrenamiento son muy escasos. Aunque nos estamos centrando en los modelos lineales jerárquicos bayesianos para la recomendación y el filtrado, la nueva idea de usar una solución analítica en lugar de una solución numérica para pares de características de usuario no relacionadas en el paso M podría adaptarse a muchos otros problemas.8. Agradecimientos Agradecemos a Wei Xu, David Lewis y revisores anónimos por sus valiosos comentarios sobre el trabajo descrito en este documento. Parte del trabajo fue apoyado por Yahoo, Google, el Instituto de Almacenamiento de Datos de Petascale y el Instituto de Gestión de Datos Científicos Escalables. Cualquier opinión, hallazgos, conclusiones o recomendaciones expresadas en este material son las de los autores, y no reflejan necesariamente las de los patrocinadores.9. Referencias [1] C. Basu, H. Hirsh y W. Cohen. Recomendación como clasificación: utilizando información social y basada en el contenido en recomendación. En Actas de la Decimocuarta Conferencia Nacional sobre Inteligencia Artificial, 1998. [2] J. S. Breese, D. Heckerman y C. Kadie. Análisis empírico de algoritmos predictivos para el filtrado colaborativo. Informe técnico, Microsoft Research, One Microsoft Way, Redmond, WA 98052, 1998. [3] J. Callan. Filtrado de documentos con redes de inferencia. En Actas de la Décima Décima Conferencia Internacional Anual de ACM Sigir sobre investigación y desarrollo en recuperación de información, páginas 262-269, 1996. [4] N. Cancedda, N. Cesa-Bianchi, A. Conconi, C. Gentile, C. Goutte,T. Graepel, Y. Li, J. M. Renders, J. S. Taylor y A. Vinokourov. Método del núcleo para el filtrado de documentos. En la Undécima Conferencia de Recuperación de Textos (TREC11). Instituto Nacional de Normas y Tecnología, Publicación Especial 500-249, 2003. [5] C. Chelba y A. Acero. Adaptación del capitalizador de entropía máxima: pequeños datos pueden ayudar mucho. En D. Lin y D. Wu, editores, Actas de EMNLP 2004, páginas 285-292, Barcelona, España, julio de 2004. Asociación de Lingüística Computacional.[6] B. Croft y J. Lafferty, editores. Modelado de idiomas para la recuperación de información. Kluwer, 2002. [7] A. Dayanik, D. D. Lewis, D. Madigan, V. Menkov y A. Genkin. Construcción de distribuciones previas informativas a partir del conocimiento del dominio en la clasificación de texto. En Sigir 06: Actas de la 29a Conferencia Internacional ACM Sigir sobre investigación y desarrollo en recuperación de información, páginas 493-500, Nueva York, NY, EE. UU., 2006. ACM Press.[8] J. Delgado y N. Ishii. Predicción de modificación ponderada basada en la memoria para sistemas de recomendación. En el taller ACM SIGIR99 en Sistemas de recomendación, 1999. [9] Grouplens. Movielens.http://www.grouplens.org/taxonomy/term/14, 2006. [10] D. Heckerman. Un tutorial sobre aprendizaje con redes bayesianas. En M. Jordan, editor, aprendizaje en modelos gráficos. Kluwer Academic, 1998. [11] J. L. Herlocker, J. A. Konstan, A. Borchers y J. Riedl. Un marco algorítmico para realizar un filtrado colaborativo. En Sigir 99: Actas de la 22a Conferencia Internacional ACM Sigir sobre investigación y desarrollo en recuperación de información, páginas 230-237, Nueva York, NY, EE. UU., 1999. ACM Press.[12] T. Hofmann y J. Puzicha. Modelos de clase latente para filtrado colaborativo. En IJCAI 99: Actas de la Decimosexta Conferencia Internacional Conjunta sobre Inteligencia Artificial, páginas 688-693, San Francisco, CA, EE. UU., 1999. Morgan Kaufmann Publishers Inc. [13] I. M. D. (IMDB). Base de datos de películas de Internet.http://www.imdb.com/interfaces/, 2006. [14] R. Jin, J. Y. Chai y L. Si. Un esquema de ponderación automática para el filtrado colaborativo. En Sigir 04: Actas de la 27ª Conferencia Anual Internacional de ACM Sigir sobre investigación y desarrollo en recuperación de información, páginas 337-344, Nueva York, NY, EE. UU., 2004. ACM Press.[15] J. A. Konstan, B. N. Miller, D. Maltz, J. L. Herlocker, L. R. Gordon y J. Riedl. Grouplens: aplicando filtrado colaborativo a Usenet News. Comunicaciones de la ACM, 40 (3): 77-87, 1997. [16] D. Lewis. Aplicación de máquinas de vectores de soporte a las tareas de filtrado y enrutamiento de lotes TREC-2001. En Actas de la Undécima Conferencia de Recuperación de Textos (TREC-11), 2002. [17] B. Liu, X. Li, W. S. Lee y P. Yu. Clasificación de texto etiquetando palabras. En Actas de la Decimonovena Conferencia Nacional sobre Inteligencia Artificial (AAAI-2004), 25-29 de julio de 2004. [18] P. Melville, R. J. Mooney y R. Nagarajan. Filtrado colaborativo estimado por contenido para obtener recomendaciones mejoradas. En Actas de la Decimura Conferencia Nacional sobre Inteligencia Artificial (AAAI-2002), Edmonton, Canadá, 2002. [19] Netflix. Premio Netflix.http://www.netflixprize.com (visitado el 30 de noviembre de 2006), 2006. [20] S. Robertson y K. Sparck-Jones. Ponderación de relevancia de los términos de búsqueda. En Journal of the American Society for Information Science, Volumen 27, páginas 129-146, 1976. [21] J. Wang, A. P. de Vries y M. J. T. Reinders. Unificación de enfoques de filtrado colaborativo basados en el usuario y basados en elementos por fusión de similitud. En Sigir 06: Actas de la 29a Conferencia Internacional ACM Sigir sobre investigación y desarrollo en recuperación de información, páginas 501-508, Nueva York, NY, EE. UU., 2006. ACM Press.[22] X. Wu y R. K. Srihari. Incorporando conocimiento previo con máquinas de vectores de soporte de margen ponderado. En Proc. ACM Knowledge Discovery Data Mining Conf.(ACM Sigkdd 2004), agosto de 2004. [23] Y. Yang, S. Yoo, J. Zhang y B. Kisiel. Robustez de los métodos de filtrado adaptativo en una evaluación de bencillo. En Actas de la 28ª Conferencia Internacional de ACM Sigir sobre investigación y desarrollo en recuperación de información, 2005. [24] K. Yu, V. Tresp y A. Schwaighfer. Aprender procesos gaussianos de múltiples tareas. En ICML 05: Actas de la 22ª Conferencia Internacional sobre Aprendizaje Autor, páginas 1012-1019, Nueva York, NY, EE. UU., 2005. ACM Press.[25] K. Yu, V. Tresp y S. Yu. Un marco bayesiano jerárquico no paramétrico para el filtrado de información. En Sigir 04: Actas de la 27ª Conferencia Internacional de ACM Sigir sobre investigación y desarrollo en recuperación de información, páginas 353-360. ACM Press, 2004. [26] X. Zhu. Encuesta de literatura de aprendizaje semi-supervisado. Informe técnico, Universidad de Wisconsin - Madison, 9 de diciembre de 2006. [27] P. zigoris e Y. Zhang. Profiles de usuarios adaptativos bayesianos con retroalimentación explícita e implícita. En Conferencias sobre Información y Mangment de Conocimiento 2006, 2006.",
    "original_sentences": [
        "Efficient Bayesian Hierarchical User Modeling for Recommendation Systems Yi Zhang, Jonathan Koren School of Engineering University of California Santa Cruz Santa Cruz, CA, USA {yiz, jonathan}@soe.ucsc.edu ABSTRACT A content-based personalized recommendation system learns user specific profiles from user feedback so that it can deliver information tailored to each individual users interest.",
        "A system serving millions of users can learn a better user profile for a new user, or a user with little feedback, by borrowing information from other users through the use of a Bayesian hierarchical model.",
        "Learning the model parameters to optimize the joint data likelihood from millions of users is very computationally expensive.",
        "The commonly used EM algorithm converges very slowly due to the sparseness of the data in IR applications.",
        "This paper proposes a new fast learning technique to learn a large number of individual user profiles.",
        "The efficacy and efficiency of the proposed algorithm are justified by theory and demonstrated on actual user data from Netflix and MovieLens.",
        "Categories and Subject Descriptors: B.3.3 [Information Search and Retrieval]: Information filtering General Terms: Algorithms 1.",
        "INTRODUCTION Personalization is the future of the Web, and it has achieved great success in industrial applications.",
        "For example, online stores, such as Amazon and Netflix, provide customized recommendations for additional products or services based on a users history.",
        "Recent offerings such as My MSN, My Yahoo!, My Google, and Google News have attracted much attention due to their potential ability to infer a users interests from his/her history.",
        "One major personalization topic studied in the information retrieval community is content-based personal recommendation systems1 .",
        "These systems learn user-specific profiles from user feedback so that they can recommend information tailored to each individual users interest without requiring the user to make an explicit query.",
        "Learning the user profiles is the core problem for these systems.",
        "A user profile is usually a classifier that can identify whether a document is relevant to the user or not, or a regression model that tells how relevant a document is to the user.",
        "One major challenge of building a recommendation or personalization system is that the profile learned for a particular user is usually of low quality when the amount of data from that particular user is small.",
        "This is known as the cold start problem.",
        "This means that any new user must endure poor initial performance until sufficient feedback from that user is provided to learn a reliable user profile.",
        "There has been much research on improving classification accuracy when the amount of labeled training data is small.",
        "The semi-supervised learning approach combines unlabeled and labeled data together to achieve this goal [26].",
        "Another approach is using domain knowledge.",
        "Researchers have modified different learning algorithms, such as Na¨ıveBayes [17], logistic regression [7], and SVMs [22], to integrate domain knowledge into a text classifier.",
        "The third approach is borrowing training data from other resources [5][7].",
        "The effectiveness of these different approaches is mixed, due to how well the underlying model assumption fits the data.",
        "One well-received approach to improve recommendation system performance for a particular user is borrowing information from other users through a Bayesian hierarchical modeling approach.",
        "Several researchers have demonstrated that this approach effectively trades off between shared and user-specific information, thus alleviating poor initial performance for each user[27][25].",
        "In order to learn a Bayesian hierarchical model, the system usually tries to find the most likely model parameters for the given data.",
        "A mature recommendation system usually works for millions of users.",
        "It is well known that learning the optimal parameters of a Bayesian hierarchical model is computationally expensive when there are thousands or millions of users.",
        "The EM algorithm is a commonly used technique for parameter learning due to its simplicity and convergence guarantee.",
        "However, a content based recommendation system often handles documents in a very high dimensional space, in which each document is represented by a very sparse vector.",
        "With careful analysis of the EM algorithm in this scenario (Section 4), we find that the EM tering, or item-based collaborative filtering.",
        "In this paper, the words filtering and recommendation are used interchangeably. algorithm converges very slowly due to the sparseness of the input variables.",
        "We also find that updating the model parameter at each EM iteration is also expensive with computational complexity of O(MK), where M is the number of users and K is the number of dimensions.",
        "This paper modifies the standard EM algorithm to create an improved learning algorithm, which we call the Modified EM algorithm.",
        "The basic idea is that instead of calculating the numerical solution for all the user profile parameters, we derive the analytical solution of the parameters for some feature dimensions, and at the M step use the analytical solution instead of the numerical solution estimated at E step for those parameters.",
        "This greatly reduces the computation at a single EM iteration, and also has the benefit of increasing the convergence speed of the learning algorithm.",
        "The proposed technique is not only well supported by theory, but also by experimental results.",
        "The organization of the remaining parts of this paper is as follows: Section 3 describes the Bayesian hierarchical linear regression modeling framework used for content-based recommendations.",
        "Section 4 describes how to learn the model parameters using the standard EM algorithm, along with using the new technique proposed in this paper.",
        "The experimental setting and results used to validate the proposed learning technique are reported in Sections 5 and 6.",
        "Section 7 summarizes and offers concluding remarks. 2.",
        "RELATED WORK Providing personalized recommendations to users has been identified as a very important problem in the IR community since the 1970s.",
        "The approaches that have been used to solve this problem can be roughly classified into two major categories: content based filtering versus collaborative filtering.",
        "Content-based filtering studies the scenario where a recommendation system monitors a document stream and pushes documents that match a user profile to the corresponding user.",
        "The user may read the delivered documents and provide explicit relevance feedback, which the filtering system then uses to update the users profile using relevance feedback retrieval models (e.g.",
        "Boolean models, vector space models, traditional probabilistic models [20] , inference networks [3] and language models [6]) or machine learning algorithms (e.g.",
        "Support Vector Machines (SVM), K nearest neighbors (K-NN) clustering, neural networks, logistic regression, or Winnow [16] [4] [23]).",
        "Collaborative filtering goes beyond merely using document content to recommend items to a user by leveraging information from other users with similar tastes and preferences in the past.",
        "Memorybased heuristics and model based approaches have been used in collaborative filtering task [15] [8] [2] [14] [12] [11].",
        "This paper contributes to the content-based recommendation research by improving the efficiency and effectiveness of Bayesian hierarchical linear models, which have a strong theoretical basis and good empirical performance on recommendation tasks[27][25].",
        "This paper does not intend to compare content-based filtering with collaborative filtering or claim which one is a better.",
        "We think each complements the other, and that content-based filtering is extremely useful for handling new documents/items with little or no user feedback.",
        "Similar to some other researchers[18][1][21], we found that a recommendation system will be more effective when both techniques are combined.",
        "However, this is beyond the scope of this paper and thus not discussed here. 3.",
        "BAYESIAN HIERARCHICAL LINEAR REGRESSION Assume there are M users in the system.",
        "The task of the system is to recommend documents that are relevant to each user.",
        "For each user, the system learns a user model from the users history.",
        "In the rest of this paper, we will use the following notations to represent the variables in the system. m = 1, 2, ..., M: The index for each individual user.",
        "M is the total number of users. wm: The user model parameter associated with user m. wm is a K dimensional vector. j = 1, 2, ..., Jm: The index for a set of data for user m. Jm is the number of training data for user m. Dm = {(xm,j, ym,j)}: A set of data associated with user m. xm,j is a K dimensional vector that represents the mth users jth training document.2 ym,j is a scalar that represents the label of document xm,j. k = 1, 2, ..., K: The dimensional index of input variable x.",
        "The Bayesian hierarchical modeling approach has been widely used in real-world information retrieval applications.",
        "Generalized Bayesian hierarchical linear models, one of the simplest Bayesian hierarchical models, are commonly used and have achieved good performance on collaborative filtering [25] and content-based adaptive filtering [27] tasks.",
        "Figure 1 shows the graphical representation of a Bayesian hierarchical model.",
        "In this graph, each user model is represented by a random vector wm.",
        "We assume a user model is sampled randomly from a prior distribution P(w|Φ).",
        "The system can predict the user label y of a document x given an estimation of wm (or wms distribution) using a function y = f(x, w).",
        "The model is called generalized Bayesian hierarchical linear model when y = f(wT x) is any generalized linear model such as logistic regression, SVM, and linear regression.",
        "To reliably estimate the user model wm, the system can borrow information from other users through the prior Φ = (µ, Σ).",
        "Now we look at one commonly used model where y = wT x + , where ∼ N(0, σ2 ) is a random noise [25][27].",
        "Assume that each user model wm is an independent draw from a population distribution P(w|Φ), which is governed by some unknown hyperparameter Φ.",
        "Let the prior distribution of user model w be a Gaussian distribution with parameter Φ = (µ, Σ), which is the commonly used prior for linear models. µ = (µ1, µ2, ..., µK ) is a K dimensional vector that represents the mean of the Gaussian distribution, and Σ is the covariance matrix of the Gaussian.",
        "Usually, a Normal distribution N(0, aI) and an Inverse Wishart distribution P(Σ) ∝ |Σ|− 1 2 b exp(−1 2 ctr(Σ−1 )) are used as hyperprior to model the prior distribution of µ and Σ respectively.",
        "I is the K dimensional identity matrix, and a, b, and c are real numbers.",
        "With these settings, we have the following model for the system: 1. µ and Σ are sampled from N(0, aI) and IWν (aI), respectively. 2 The first dimension of x is a dummy variable that always equals to 1.",
        "Figure 1: Illustration of dependencies of variables in the hierarchical model.",
        "The rating, y, for a document, x, is conditioned on the document and the user model, wm, associated with the user m. Users share information about their models through the prior, Φ = (µ, Σ). 2.",
        "For each user m, wm is sampled randomly from a Normal distribution: wm ∼ N(µ, Σ2 ) 3.",
        "For each item xm,j, ym,j is sampled randomly from a Normal distribution: ym,j ∼ N(wT mxm,j, σ2 ).",
        "Let θ = (Φ, w1, w2, ..., wM ) represent the parameters of this system that needs to be estimated.",
        "The joint likelihood for all the variables in the probabilistic model, which includes the data and the parameters, is: P(D, θ) = P(Φ) m P(wm|Φ) j P(ym,j|xm,j, wm) (1) For simplicity, we assume a, b, c, and σ are provided to the system. 4.",
        "MODEL PARAMETER LEARNING If the prior Φ is known, finding the optimal wm is straightforward: it is a simple linear regression.",
        "Therefore, we will focus on estimating Φ.",
        "The maximum a priori solution of Φ is given by ΦMAP = arg max Φ P(Φ|D) (2) = arg max Φ P(Φ, D) P(D) (3) = arg max Φ P(D|Φ)P(Φ) (4) = arg max Φ w P(D|w, Φ)P(w|Φ)P(Φ)dw (5) Finding the optimal solution for the above problem is challenging, since we need to integrate over all w = (w1, w2, ..., wM ), which are unobserved hidden variables. 4.1 EM Algorithm for Bayesian Hierarchical Linear Models In Equation 5, Φ is the parameter needs to be estimated, and the result depends on unobserved latent variables w. This kind of optimization problem is usually solved by the EM algorithm.",
        "Applying EM to the above problem, the set of user models w are the unobservable hidden variables and we have: Q = w P(w|µ, Σ2 , Dm) log P(µ, Σ2 , w, D)dw Based on the derivation of the EM formulas presented in [24], we have the following Expectation-Maximization steps for finding the optimal hyperparameters.",
        "For space considerations, we omit the derivation in this paper since it is not the focus of our work.",
        "E step: For each user m, estimate the user model distribution P(wm|Dm, Φ) = N(wm; ¯wm, Σ2 m) based on the current estimation of the prior Φ = (µ, Σ2 ). ¯wm = ((Σ2 )−1 + Sxx,m σ2 )−1 ( Sxy,m σ2 + (Σ2 )−1 µ) (6) Σ2 m = ((Σ2 )−1 + Sxx,m σ2 )−1 (7) where Sxx,m = j xm,jxT m,j Sxy,m = j xm,jym,j M step: Optimize the prior Φ = (µ, Σ2 ) based on the estimation from the last E step. µ = 1 M m ¯wm (8) Σ2 = 1 M m Σ2 m + ( ¯wm − µ)( ¯wm − µ)T (9) Many machine learning driven IR systems use a point estimate of the parameters at different stages in the system.",
        "However, we are estimating the posterior distribution of the variables at the E step.",
        "This avoids overfitting wm to a particular users data, which may be small and noisy.",
        "A detailed discussion about this subject appears in [10]. 4.2 New Algorithm: Modified EM Although the EM algorithm is widely studied and used in machine learning applications, using the above EM process to solve Bayesian hierarchical linear models in large-scale information retrieval systems is still too computationally expensive.",
        "In this section, we describe why the learning rate of the EM algorithm is slow in our application and introduce a new technique to make the learning of the Bayesian hierarchical linear model scalable.",
        "The derivation of the new learning algorithm will be based on the EM algorithm described in the previous section.",
        "First, the covariance matrices Σ2 , Σ2 m are usually too large to be computationally feasible.",
        "For simplicity, and as a common practice in IR, we do not model the correlation between features.",
        "Thus we approximate these matrices with K dimensional diagonal matrices.",
        "In the rest of the paper, we use these symbols to represent their diagonal approximations: Σ2 =     σ2 1 0 .. 0 0 σ2 2 .. 0 .. .. .. .. 0 0 .. σ2 K     Σ2 m =     σ2 m,1 0 .. 0 0 σ2 m,2 .. 0 .. .. .. .. 0 0 .. σ2 m,K     Secondly, and most importantly, the input space is very sparse and there are many dimensions that are not related to a particular user in a real IR application.",
        "For example, let us consider a movie recommendation system, with the input variable x representing a particular movie.",
        "For the jth movie that the user m has seen, let xm,j,k = 1 if the director of the movie is Jean-Pierre Jeunet (indexed by k).",
        "Here we assume that whether or not that this director directed a specific movie is represented by the kth dimension.",
        "If the user m has never seen a movie directed by Jean-Pierre Jeunet, then the corresponding dimension is always zero (xm,j,k = 0 for all j) .",
        "One major drawback of the EM algorithm is that the importance of a feature, µk, may be greatly dominated by users who have never encountered this feature (i.e. j xm,j,k = 0) at the M step (Equation 8).",
        "Assume that 100 out of 1 million users have viewed the movie directed by Jean-Pierre Jeunet, and that the viewers have rated all of his movies as excellent.",
        "Intuitively, he is a good director and the weight for him (µk) should be high.",
        "Before the EM iteration, the initial value of µ is usually set to 0.",
        "Since the other 999,900 users have not seen this movie, their corresponding weights (w1,k, w2,k, ..., wm,k..., w999900,k) for that director would be very small initially.",
        "Thus the corresponding weight of the director in the prior µk at the first M step would be very low , and the variance σm,k will be large (Equations 8 and 7).",
        "It is undesirable that users who have never seen any movie produced by the director influence the importance of the director so much.",
        "This makes the convergence of the standard EM algorithm very slow.",
        "Now lets look at whether we can improve the learning speed of the algorithm.",
        "Without a loss of generality, let us assume that the kth dimension of the input variable x is not related to a particular user m. By which we mean, xm,j,k = 0 for all j = 1, ..., Jm.",
        "It is straightforward to prove that the kth row and kth column of Sxx,m are completely filled with zeros, and that the kth dimension of Sxy,m is zeroed as well.",
        "Thus the corresponding kth dimension of the user models mean, ¯wm, should be equal to that of the prior: ¯wm,k = µk, with the corresponding covariance of σm,k = σk.",
        "At the M step, the standard EM algorithm uses the numerical solution of the distribution P(wm|Dm, Φ) estimated at E step (Equation 8 and Equation 7).",
        "However, the numerical solutions are very unreliable for ¯wm,k and σm,k when the kth dimension is not related to the mth user.",
        "A better approach is using the analytical solutions ¯wm,k = µk, and σm,k = σk for the unrelated (m, k) pairs, along with the numerical solution estimated at E step for the other (m, k) pairs.",
        "Thus we get the following new EM-like algorithm: Modified E step: For each user m, estimate the user model distribution P(wm|Dm, Φ) = N(wm; ¯wm, Σ2 m) based on the current estimation of σ , µ, Σ2 . ¯wm = ((Σ2 )−1 + Sxx,m σ2 )−1 ( Sxy,m σ2 + (Σ2 )−1 µ)(10) σ2 m,k = ((σ2 k)−1 + sxx,m,k σ2 )−1 (11) where sxx,m,k = j x2 m,j,k and sxy,m,k = j xm,j,kym,j Modified M Step Optimize the prior Φ = (µ, Σ2 ) based on the estimation from the last E step for related userfeature pairs.",
        "The M step implicitly uses the analytical solution for unrelated user-feature pairs. µk = 1 Mk m:related ¯wm,k (12) σ2 k = 1 Mk m:related σ2 m,k +( ¯wm,k − µk)( ¯wm,k − µk)T (13) where Mk is the number of users that are related to feature k We only estimate the diagonal of Σ2 m and Σ since we are using the diagonal approximation of the covariance matrices.",
        "To estimate ¯wm, we only need to calculate the numerical solutions for dimensions that are related to user m. To estimate σ2 k and µk, we only sum over users that are related to the kth feature.",
        "There are two major benefits of the new algorithm.",
        "First, because only the related (m, k) pairs are needed at the modified M step, the computational complexity in a single EM iteration is much smaller when the data is sparse, and many of (m, k) pairs are unrelated.",
        "Second, the parameters estimated at the modified M step (Equations 12 - 13) are more accurate than the standard M step described in Section 4.1 because the exact analytical solutions ¯wm,k = µk and σm,k = σk for the unrelated (m, k) pairs were used in the new algorithm instead of an approximate solution as in the standard algorithm. 5.",
        "EXPERIMENTAL METHODOLOGY 5.1 Evaluation Data Set To evaluate the proposed technique, we used the following three major data sets (Table 1): MovieLens Data: This data set was created by combining the relevance judgments from the MovieLens[9] data set with documents from the Internet Movie Database (IMDB).",
        "MovieLens allows users to rank how much he/she enjoyed a specific movie on a scale from 1 to 5.",
        "This likeability rating was used as a measurement of how relevant the document representing the corresponding movie is to the user.",
        "We considered documents with likeability scores of 4 or 5 as relevant, and documents with a score of 1 to 3 as irrelevant to the user.",
        "MovieLens provided relevance judgments on 3,057 documents from 6,040 separate users.",
        "On average, each user rated 151 movies, of these 87 were judged to be relevant.",
        "The average score for a document was 3.58.",
        "Documents representing each movie were constructed from the portion of the IMDB database that is available for public download[13].",
        "Based on this database, we created one document per movie that contained the relevant information about it (e.g. directors, actors, etc.).",
        "Table 1: Data Set Statistics.",
        "On Reuters, the number of rating for a simulated user is the number of documents relevant to the corresponding topic.",
        "Data Users Docs Ratings per User MovieLens 6,040 3,057 151 Netflix-all 480,189 17,770 208 Netflix-1000 1000 17,770 127 Reuters-C 34 100,000 3949 Reuters-E 26 100,000 1632 Reuters-G 33 100,000 2222 Reuters-M 10 100,000 6529 Netflix Data: This data set was constructed by combining documents about movies crawled from the web with a set of actual movie rental customer relevance judgments from Netflix[19].",
        "Netflix publicly provides the relevance judgments of 480,189 anonymous customers.",
        "There are around 100 million rating on a scale of 1 to 5 for 17,770 documents.",
        "Similar to MovieLens, we considered documents with likeability scores of 4 or 5 as relevant.",
        "This number was reduced to 1000 customers through random sampling.",
        "The average customer on the reduced data set provided 127 judgments, with 70 being deemed relevant.",
        "The average score for documents is 3.55.",
        "Reuters Data: This is the Reuters Corpus, Volume 1.",
        "It covers 810,000 Reuters English language news stories from August 20, 1996 to August 19, 1997.",
        "Only the first 100,000 news were used in our experiments.",
        "The Reuters corpus comes with a topic hierarchy.",
        "Each document is assigned to one of several locations on the hierarchical tree.",
        "The first level of the tree contains four topics, denoted as C, E, M, and G. For the experiments in this paper, the tree was cut at level 1 to create four smaller trees, each of which corresponds to one smaller data set: Reuters-E Reuters-C, ReutersM and Reuters-G. For each small data set, we created several profiles, one profile for each node in a sub-tree, to simulate multiple users, each with a related, yet separate definition of relevance.",
        "All the user profiles on a sub-tree are supposed to share the same prior model distribution.",
        "Since this corpus explicitly indicates only the relevant documents for a topic(user), all other documents are considered irrelevant. 5.2 Evaluation We designed the experiments to answer the following three questions: 1.",
        "Do we need to take the effort to use a Bayesian approach and learn a prior from other users? 2.",
        "Does the new algorithm work better than the standard EM algorithm for learning the Bayesian hierarchical linear model? 3.",
        "Can the new algorithm quickly learn many user models?",
        "To answer the first question, we compared the Bayesian hierarchical models with commonly used Norm-2 regularized linear regression models.",
        "In fact, the commonly used approach is equivalent to the model learned at the end of the first EM iteration.",
        "To answer the second question, we compared the proposed new algorithm with the standard EM algorithm to see whether the new learning algorithm is better.",
        "To answer the third question, we tested the efficiency of the new algorithm on the entire Netflix data set where about half a million user models need to be learned together.",
        "For the MovieLens and Netflix data sets, algorithm effectiveness was measured by mean square error, while on the Reuters data set classification error was used because it was more informative.",
        "We first evaluated the performance on each individual user, and then estimated the macro average over all users.",
        "Statistical tests (t-tests) were carried out to see whether the results are significant.",
        "For the experiments on the MovieLens and Netflix data sets, we used a random sample of 90% of each user for training, and the rest for testing.",
        "On Reuters data set, because there are too many relevant documents for each topic in the corpus, we used a random sample of 10% of each topic for training, and 10% of the remaining documents for testing.",
        "For all runs, we set (a, b, c, Σ ) = (0.1, 10, 0.1, 1) manually. 6.",
        "EXPERIMENTAL RESULTS Figure 2, Figure 3, and Figure 4 show that on all data sets, the Bayesian hierarchical modeling approach has a statistical significant improvement over the regularized linear regression model, which is equivalent to the Bayesian hierarchical models learned at the first iteration.",
        "Further analysis shows a negative correlation between the number of training data for a user and the improvement the system gets.",
        "This suggests that the borrowing information from other users has more significant improvements for users with less training data, which is as expected.",
        "However, the strength of the correlation differs over data sets, and the amount of training data is not the only characteristics that will influence the final performance.",
        "Figure 2 and Figure 3 show that the proposed new algorithm works better than the standard EM algorithm on the Netflix and MovieLens data sets.",
        "This is not surprising since the number of related feature-users pairs is much smaller than the number of unrelated feature-user pairs on these two data sets, and thus the proposed new algorithm is expected to work better.",
        "Figure 4 shows that the two algorithms work similarly on the Reuters-E data set.",
        "The accuracy of the new algorithm is similar to that of the standard EM algorithm at each iteration.",
        "The general patterns are very similar on other Reuters subsets.",
        "Further analysis shows that only 58% of the user-feature pairs are unrelated on this data set.",
        "Since the number of unrelated user-feature pairs is not extremely large, the sparseness is not a serious problem on the Reuters data set.",
        "Thus the two learning algorithms perform similarly.",
        "The results suggest that only on a corpus where the number of unrelated user-feature pairs is much larger than the number of related pairs, such as on the Netflix data set, the proposed technique will get a significant improvement over standard EM.",
        "However, the experiments also show that when the assumption does not hold, the new algorithm does not hurt performance.",
        "Although the proposed technique is faster than standard Figure 2: Performance on a Netflix subset with 1,000 users.",
        "The new algorithm is statistical significantly better than EM algorithm at iterations 2 - 10.",
        "Norm-2 regularized linear models are equivalent to the Bayesian hierarchical models learned at the first iteration, and are statistical significantly worse than the Bayesian hierarchical models. 0 2 4 6 8 10 1 1.05 1.1 1.15 1.2 1.25 1.3 1.35 1.4 Iterations MeanSquareError New Algorithm Traditional EM 1 2 3 4 5 6 7 8 9 10 0.33 0.34 0.35 0.36 0.37 0.38 0.39 Iterations ClassificationError New Algorithm Traditional EM Figure 3: Performance on a MovieLens subset with 1,000 users.",
        "The new algorithm is statistical significantly better than EM algorithm at iteration 2 to 17 (evaluated with mean square error). 1 6 11 16 21 0.5 1 1.5 2 2.5 3 3.5 Iterations MeanSquareError New Algorithm Traditional EM 1 6 11 16 21 0.35 0.4 0.45 0.5 0.55 0.6 0.65 Iterations ClassificationError New Algorithm Traditional EM Figure 4: Performance on a Reuters-E subset with 26 profiles.",
        "Performances on Reuters-C, Reuters-M, Reuters-G are similar. 1 2 3 4 5 0.011 0.0115 0.012 0.0125 0.013 0.0135 0.014 Iterations MeanSquareError New Algorithm Traditional EM 1 2 3 4 5 0.0102 0.0104 0.0106 0.0108 0.011 0.0112 0.0114 Iterations ClassificationError New Algorithm Traditional EM EM, can it really learn millions of user models quickly?",
        "Our results show that the modified EM algorithm converges quickly, and 2 - 3 modified EM iterations would result in a reliable estimation.",
        "We evaluated the algorithm on the whole Netflix data set (480,189 users, 159,836 features, and 100 million ratings) running on a single CPU PC (2GB memory, P4 3GHz).",
        "The system finished one modified EM iteration in about 4 hours.",
        "This demonstrates that the proposed technique can efficiently handle large-scale system like Netflix. 7.",
        "CONCLUSION Content-based user profile learning is an important problem and is the key to providing personal recommendations to a user, especially for recommending new items with a small number of ratings.",
        "The Bayesian hierarchical modeling approach is becoming an important user profile learning approach due to its theoretically justified ability to help one user through information transfer from the other users by way of hyperpriors.",
        "This paper examined the weakness of the popular EM based learning approach for Bayesian hierarchical linear models and proposed a better learning technique called Modified EM.",
        "We showed that the new technique is theoretically more computationally efficient than the standard EM algorithm.",
        "Evaluation on the MovieLens and Netflix data sets demonstrated the effectiveness of the new technique when the data is sparse, by which we mean the ratio of related user-feature pairs to unrelated pairs is small.",
        "Evaluation on the Reuters data set showed that the new technique performed similar to the standard EM algorithm when the sparseness condition does not hold.",
        "In general, it is better to use the new algorithm since it is as simple as standard EM, the performance is either better or similar to EM, and the computation complexity is lower at each iteration.",
        "It is worth mentioning that even if the original problem space is not sparse, sparseness can be created artificially when a recommendation system uses user-specific feature selection techniques to reduce the noise and user model complexity.",
        "The proposed technique can also be adapted to improve the learning in such a scenario.",
        "We also demonstrated that the proposed technique can learn half a million user profiles from 100 million ratings in a few hours with a single CPU.",
        "The research is important because scalability is a major concern for researchers when using the Bayesian hierarchical linear modeling approach to build a practical large scale system, even though the literature have demonstrated the effectiveness of the models in many applications.",
        "Our work is one major step on the road to make Bayesian hierarchical linear models more practical.",
        "The proposed new technique can be easily adapted to run on a cluster of machines, and thus further speed up the learning process to handle a larger scale system with hundreds of millions of users.",
        "The research has much potential to benefit people using EM algorithm on many other IR problems as well as machine learning problems.",
        "EM algorithm is a commonly used machine learning technique.",
        "It is used to find model parameters in many IR problems where the training data is very sparse.",
        "Although we are focusing on the Bayesian hierarchical linear models for recommendation and filtering, the new idea of using analytical solution instead of numerical solution for unrelated user-feature pairs at the M step could be adapted to many other problems. 8.",
        "ACKNOWLEDGMENTS We thank Wei Xu, David Lewis and anonymous reviewers for valuable feedback on the work described in this paper.",
        "Part of the work was supported by Yahoo, Google, the Petascale Data Storage Institute and the Institute for Scalable Scientific Data Management.",
        "Any opinions, findings, conclusions, or recommendations expressed in this material are those of the authors, and do not necessarily reflect those of the sponsors. 9.",
        "REFERENCES [1] C. Basu, H. Hirsh, and W. Cohen.",
        "Recommendation as classification: Using social and content-based information in recommendation.",
        "In Proceedings of the Fifteenth National Conference on Artificial Intelligence, 1998. [2] J. S. Breese, D. Heckerman, and C. Kadie.",
        "Empirical analysis of predictive algorithms for collaborative filtering.",
        "Technical report, Microsoft Research, One Microsoft Way, Redmond, WA 98052, 1998. [3] J. Callan.",
        "Document filtering with inference networks.",
        "In Proceedings of the Nineteenth Annual International ACM SIGIR Conference on Research and Development in Information Retrieval, pages 262-269, 1996. [4] N. Cancedda, N. Cesa-Bianchi, A. Conconi, C. Gentile, C. Goutte, T. Graepel, Y. Li, J. M. Renders, J. S. Taylor, and A. Vinokourov.",
        "Kernel method for document filtering.",
        "In The Eleventh Text REtrieval Conference (TREC11).",
        "National Institute of Standards and Technology, special publication 500-249, 2003. [5] C. Chelba and A. Acero.",
        "Adaptation of maximum entropy capitalizer: Little data can help a lot.",
        "In D. Lin and D. Wu, editors, Proceedings of EMNLP 2004, pages 285-292, Barcelona, Spain, July 2004.",
        "Association for Computational Linguistics. [6] B. Croft and J. Lafferty, editors.",
        "Language Modeling for Information Retrieval.",
        "Kluwer, 2002. [7] A. Dayanik, D. D. Lewis, D. Madigan, V. Menkov, and A. Genkin.",
        "Constructing informative prior distributions from domain knowledge in text classification.",
        "In SIGIR 06: Proceedings of the 29th annual international ACM SIGIR conference on Research and development in information retrieval, pages 493-500, New York, NY, USA, 2006.",
        "ACM Press. [8] J. Delgado and N. Ishii.",
        "Memory-based weightedmajority prediction for recommender systems.",
        "In ACM SIGIR99 Workshop on Recommender Systems, 1999. [9] GroupLens.",
        "Movielens. http://www.grouplens.org/taxonomy/term/14, 2006. [10] D. Heckerman.",
        "A tutorial on learning with bayesian networks.",
        "In M. Jordan, editor, Learning in Graphical Models.",
        "Kluwer Academic, 1998. [11] J. L. Herlocker, J.",
        "A. Konstan, A. Borchers, and J. Riedl.",
        "An algorithmic framework for performing collaborative filtering.",
        "In SIGIR 99: Proceedings of the 22nd annual international ACM SIGIR conference on Research and development in information retrieval, pages 230-237, New York, NY, USA, 1999.",
        "ACM Press. [12] T. Hofmann and J. Puzicha.",
        "Latent class models for collaborative filtering.",
        "In IJCAI 99: Proceedings of the Sixteenth International Joint Conference on Artificial Intelligence, pages 688-693, San Francisco, CA, USA, 1999.",
        "Morgan Kaufmann Publishers Inc. [13] I. M. D. (IMDB).",
        "Internet movie database. http://www.imdb.com/interfaces/, 2006. [14] R. Jin, J. Y. Chai, and L. Si.",
        "An automatic weighting scheme for collaborative filtering.",
        "In SIGIR 04: Proceedings of the 27th annual international ACM SIGIR conference on Research and development in information retrieval, pages 337-344, New York, NY, USA, 2004.",
        "ACM Press. [15] J.",
        "A. Konstan, B. N. Miller, D. Maltz, J. L. Herlocker, L. R. Gordon, and J. Riedl.",
        "GroupLens: Applying collaborative filtering to Usenet news.",
        "Communications of the ACM, 40(3):77-87, 1997. [16] D. Lewis.",
        "Applying support vector machines to the TREC-2001 batch filtering and routing tasks.",
        "In Proceedings of the Eleventh Text REtrieval Conference (TREC-11), 2002. [17] B. Liu, X. Li, W. S. Lee, , and P. Yu.",
        "Text classification by labeling words.",
        "In Proceedings of The Nineteenth National Conference on Artificial Intelligence (AAAI-2004), July 25-29, 2004. [18] P. Melville, R. J. Mooney, and R. Nagarajan.",
        "Content-boosted collaborative filtering for improved recommendations.",
        "In Proceedings of the Eighteenth National Conference on Artificial Intelligence (AAAI-2002), Edmonton, Canada, 2002. [19] Netflix.",
        "Netflix prize. http://www.netflixprize.com (visited on Nov. 30, 2006), 2006. [20] S. Robertson and K. Sparck-Jones.",
        "Relevance weighting of search terms.",
        "In Journal of the American Society for Information Science, volume 27, pages 129-146, 1976. [21] J. Wang, A. P. de Vries, and M. J. T. Reinders.",
        "Unifying user-based and item-based collaborative filtering approaches by similarity fusion.",
        "In SIGIR 06: Proceedings of the 29th annual international ACM SIGIR conference on Research and development in information retrieval, pages 501-508, New York, NY, USA, 2006.",
        "ACM Press. [22] X. Wu and R. K. Srihari.",
        "Incorporating prior knowledge with weighted margin support vector machines.",
        "In Proc.",
        "ACM Knowledge Discovery Data Mining Conf. (ACM SIGKDD 2004), Aug. 2004. [23] Y. Yang, S. Yoo, J. Zhang, and B. Kisiel.",
        "Robustness of adaptive filtering methods in a cross-benchmark evaluation.",
        "In Proceedings of the 28th Annual International ACM SIGIR Conference on Research and Development in Information Retrieval, 2005. [24] K. Yu, V. Tresp, and A. Schwaighofer.",
        "Learning gaussian processes from multiple tasks.",
        "In ICML 05: Proceedings of the 22nd international conference on Machine learning, pages 1012-1019, New York, NY, USA, 2005.",
        "ACM Press. [25] K. Yu, V. Tresp, and S. Yu.",
        "A nonparametric hierarchical bayesian framework for information filtering.",
        "In SIGIR 04: Proceedings of the 27th annual international ACM SIGIR conference on Research and development in information retrieval, pages 353-360.",
        "ACM Press, 2004. [26] X. Zhu.",
        "Semi-supervised learning literature survey.",
        "Technical report, University of Wisconsin - Madison, December 9, 2006. [27] P. Zigoris and Y. Zhang.",
        "Bayesian adaptive user profiling with explicit & implicit feedback.",
        "In Conference on Information and Knowledge Mangement 2006, 2006."
    ],
    "error_count": 0,
    "keys": {
        "modeling": {
            "translated_key": "modelado",
            "is_in_text": true,
            "original_annotated_sentences": [
                "Efficient Bayesian Hierarchical User <br>modeling</br> for Recommendation Systems Yi Zhang, Jonathan Koren School of Engineering University of California Santa Cruz Santa Cruz, CA, USA {yiz, jonathan}@soe.ucsc.edu ABSTRACT A content-based personalized recommendation system learns user specific profiles from user feedback so that it can deliver information tailored to each individual users interest.",
                "A system serving millions of users can learn a better user profile for a new user, or a user with little feedback, by borrowing information from other users through the use of a Bayesian hierarchical model.",
                "Learning the model parameters to optimize the joint data likelihood from millions of users is very computationally expensive.",
                "The commonly used EM algorithm converges very slowly due to the sparseness of the data in IR applications.",
                "This paper proposes a new fast learning technique to learn a large number of individual user profiles.",
                "The efficacy and efficiency of the proposed algorithm are justified by theory and demonstrated on actual user data from Netflix and MovieLens.",
                "Categories and Subject Descriptors: B.3.3 [Information Search and Retrieval]: Information filtering General Terms: Algorithms 1.",
                "INTRODUCTION Personalization is the future of the Web, and it has achieved great success in industrial applications.",
                "For example, online stores, such as Amazon and Netflix, provide customized recommendations for additional products or services based on a users history.",
                "Recent offerings such as My MSN, My Yahoo!, My Google, and Google News have attracted much attention due to their potential ability to infer a users interests from his/her history.",
                "One major personalization topic studied in the information retrieval community is content-based personal recommendation systems1 .",
                "These systems learn user-specific profiles from user feedback so that they can recommend information tailored to each individual users interest without requiring the user to make an explicit query.",
                "Learning the user profiles is the core problem for these systems.",
                "A user profile is usually a classifier that can identify whether a document is relevant to the user or not, or a regression model that tells how relevant a document is to the user.",
                "One major challenge of building a recommendation or personalization system is that the profile learned for a particular user is usually of low quality when the amount of data from that particular user is small.",
                "This is known as the cold start problem.",
                "This means that any new user must endure poor initial performance until sufficient feedback from that user is provided to learn a reliable user profile.",
                "There has been much research on improving classification accuracy when the amount of labeled training data is small.",
                "The semi-supervised learning approach combines unlabeled and labeled data together to achieve this goal [26].",
                "Another approach is using domain knowledge.",
                "Researchers have modified different learning algorithms, such as Na¨ıveBayes [17], logistic regression [7], and SVMs [22], to integrate domain knowledge into a text classifier.",
                "The third approach is borrowing training data from other resources [5][7].",
                "The effectiveness of these different approaches is mixed, due to how well the underlying model assumption fits the data.",
                "One well-received approach to improve recommendation system performance for a particular user is borrowing information from other users through a Bayesian hierarchical <br>modeling</br> approach.",
                "Several researchers have demonstrated that this approach effectively trades off between shared and user-specific information, thus alleviating poor initial performance for each user[27][25].",
                "In order to learn a Bayesian hierarchical model, the system usually tries to find the most likely model parameters for the given data.",
                "A mature recommendation system usually works for millions of users.",
                "It is well known that learning the optimal parameters of a Bayesian hierarchical model is computationally expensive when there are thousands or millions of users.",
                "The EM algorithm is a commonly used technique for parameter learning due to its simplicity and convergence guarantee.",
                "However, a content based recommendation system often handles documents in a very high dimensional space, in which each document is represented by a very sparse vector.",
                "With careful analysis of the EM algorithm in this scenario (Section 4), we find that the EM tering, or item-based collaborative filtering.",
                "In this paper, the words filtering and recommendation are used interchangeably. algorithm converges very slowly due to the sparseness of the input variables.",
                "We also find that updating the model parameter at each EM iteration is also expensive with computational complexity of O(MK), where M is the number of users and K is the number of dimensions.",
                "This paper modifies the standard EM algorithm to create an improved learning algorithm, which we call the Modified EM algorithm.",
                "The basic idea is that instead of calculating the numerical solution for all the user profile parameters, we derive the analytical solution of the parameters for some feature dimensions, and at the M step use the analytical solution instead of the numerical solution estimated at E step for those parameters.",
                "This greatly reduces the computation at a single EM iteration, and also has the benefit of increasing the convergence speed of the learning algorithm.",
                "The proposed technique is not only well supported by theory, but also by experimental results.",
                "The organization of the remaining parts of this paper is as follows: Section 3 describes the Bayesian hierarchical linear regression <br>modeling</br> framework used for content-based recommendations.",
                "Section 4 describes how to learn the model parameters using the standard EM algorithm, along with using the new technique proposed in this paper.",
                "The experimental setting and results used to validate the proposed learning technique are reported in Sections 5 and 6.",
                "Section 7 summarizes and offers concluding remarks. 2.",
                "RELATED WORK Providing personalized recommendations to users has been identified as a very important problem in the IR community since the 1970s.",
                "The approaches that have been used to solve this problem can be roughly classified into two major categories: content based filtering versus collaborative filtering.",
                "Content-based filtering studies the scenario where a recommendation system monitors a document stream and pushes documents that match a user profile to the corresponding user.",
                "The user may read the delivered documents and provide explicit relevance feedback, which the filtering system then uses to update the users profile using relevance feedback retrieval models (e.g.",
                "Boolean models, vector space models, traditional probabilistic models [20] , inference networks [3] and language models [6]) or machine learning algorithms (e.g.",
                "Support Vector Machines (SVM), K nearest neighbors (K-NN) clustering, neural networks, logistic regression, or Winnow [16] [4] [23]).",
                "Collaborative filtering goes beyond merely using document content to recommend items to a user by leveraging information from other users with similar tastes and preferences in the past.",
                "Memorybased heuristics and model based approaches have been used in collaborative filtering task [15] [8] [2] [14] [12] [11].",
                "This paper contributes to the content-based recommendation research by improving the efficiency and effectiveness of Bayesian hierarchical linear models, which have a strong theoretical basis and good empirical performance on recommendation tasks[27][25].",
                "This paper does not intend to compare content-based filtering with collaborative filtering or claim which one is a better.",
                "We think each complements the other, and that content-based filtering is extremely useful for handling new documents/items with little or no user feedback.",
                "Similar to some other researchers[18][1][21], we found that a recommendation system will be more effective when both techniques are combined.",
                "However, this is beyond the scope of this paper and thus not discussed here. 3.",
                "BAYESIAN HIERARCHICAL LINEAR REGRESSION Assume there are M users in the system.",
                "The task of the system is to recommend documents that are relevant to each user.",
                "For each user, the system learns a user model from the users history.",
                "In the rest of this paper, we will use the following notations to represent the variables in the system. m = 1, 2, ..., M: The index for each individual user.",
                "M is the total number of users. wm: The user model parameter associated with user m. wm is a K dimensional vector. j = 1, 2, ..., Jm: The index for a set of data for user m. Jm is the number of training data for user m. Dm = {(xm,j, ym,j)}: A set of data associated with user m. xm,j is a K dimensional vector that represents the mth users jth training document.2 ym,j is a scalar that represents the label of document xm,j. k = 1, 2, ..., K: The dimensional index of input variable x.",
                "The Bayesian hierarchical <br>modeling</br> approach has been widely used in real-world information retrieval applications.",
                "Generalized Bayesian hierarchical linear models, one of the simplest Bayesian hierarchical models, are commonly used and have achieved good performance on collaborative filtering [25] and content-based adaptive filtering [27] tasks.",
                "Figure 1 shows the graphical representation of a Bayesian hierarchical model.",
                "In this graph, each user model is represented by a random vector wm.",
                "We assume a user model is sampled randomly from a prior distribution P(w|Φ).",
                "The system can predict the user label y of a document x given an estimation of wm (or wms distribution) using a function y = f(x, w).",
                "The model is called generalized Bayesian hierarchical linear model when y = f(wT x) is any generalized linear model such as logistic regression, SVM, and linear regression.",
                "To reliably estimate the user model wm, the system can borrow information from other users through the prior Φ = (µ, Σ).",
                "Now we look at one commonly used model where y = wT x + , where ∼ N(0, σ2 ) is a random noise [25][27].",
                "Assume that each user model wm is an independent draw from a population distribution P(w|Φ), which is governed by some unknown hyperparameter Φ.",
                "Let the prior distribution of user model w be a Gaussian distribution with parameter Φ = (µ, Σ), which is the commonly used prior for linear models. µ = (µ1, µ2, ..., µK ) is a K dimensional vector that represents the mean of the Gaussian distribution, and Σ is the covariance matrix of the Gaussian.",
                "Usually, a Normal distribution N(0, aI) and an Inverse Wishart distribution P(Σ) ∝ |Σ|− 1 2 b exp(−1 2 ctr(Σ−1 )) are used as hyperprior to model the prior distribution of µ and Σ respectively.",
                "I is the K dimensional identity matrix, and a, b, and c are real numbers.",
                "With these settings, we have the following model for the system: 1. µ and Σ are sampled from N(0, aI) and IWν (aI), respectively. 2 The first dimension of x is a dummy variable that always equals to 1.",
                "Figure 1: Illustration of dependencies of variables in the hierarchical model.",
                "The rating, y, for a document, x, is conditioned on the document and the user model, wm, associated with the user m. Users share information about their models through the prior, Φ = (µ, Σ). 2.",
                "For each user m, wm is sampled randomly from a Normal distribution: wm ∼ N(µ, Σ2 ) 3.",
                "For each item xm,j, ym,j is sampled randomly from a Normal distribution: ym,j ∼ N(wT mxm,j, σ2 ).",
                "Let θ = (Φ, w1, w2, ..., wM ) represent the parameters of this system that needs to be estimated.",
                "The joint likelihood for all the variables in the probabilistic model, which includes the data and the parameters, is: P(D, θ) = P(Φ) m P(wm|Φ) j P(ym,j|xm,j, wm) (1) For simplicity, we assume a, b, c, and σ are provided to the system. 4.",
                "MODEL PARAMETER LEARNING If the prior Φ is known, finding the optimal wm is straightforward: it is a simple linear regression.",
                "Therefore, we will focus on estimating Φ.",
                "The maximum a priori solution of Φ is given by ΦMAP = arg max Φ P(Φ|D) (2) = arg max Φ P(Φ, D) P(D) (3) = arg max Φ P(D|Φ)P(Φ) (4) = arg max Φ w P(D|w, Φ)P(w|Φ)P(Φ)dw (5) Finding the optimal solution for the above problem is challenging, since we need to integrate over all w = (w1, w2, ..., wM ), which are unobserved hidden variables. 4.1 EM Algorithm for Bayesian Hierarchical Linear Models In Equation 5, Φ is the parameter needs to be estimated, and the result depends on unobserved latent variables w. This kind of optimization problem is usually solved by the EM algorithm.",
                "Applying EM to the above problem, the set of user models w are the unobservable hidden variables and we have: Q = w P(w|µ, Σ2 , Dm) log P(µ, Σ2 , w, D)dw Based on the derivation of the EM formulas presented in [24], we have the following Expectation-Maximization steps for finding the optimal hyperparameters.",
                "For space considerations, we omit the derivation in this paper since it is not the focus of our work.",
                "E step: For each user m, estimate the user model distribution P(wm|Dm, Φ) = N(wm; ¯wm, Σ2 m) based on the current estimation of the prior Φ = (µ, Σ2 ). ¯wm = ((Σ2 )−1 + Sxx,m σ2 )−1 ( Sxy,m σ2 + (Σ2 )−1 µ) (6) Σ2 m = ((Σ2 )−1 + Sxx,m σ2 )−1 (7) where Sxx,m = j xm,jxT m,j Sxy,m = j xm,jym,j M step: Optimize the prior Φ = (µ, Σ2 ) based on the estimation from the last E step. µ = 1 M m ¯wm (8) Σ2 = 1 M m Σ2 m + ( ¯wm − µ)( ¯wm − µ)T (9) Many machine learning driven IR systems use a point estimate of the parameters at different stages in the system.",
                "However, we are estimating the posterior distribution of the variables at the E step.",
                "This avoids overfitting wm to a particular users data, which may be small and noisy.",
                "A detailed discussion about this subject appears in [10]. 4.2 New Algorithm: Modified EM Although the EM algorithm is widely studied and used in machine learning applications, using the above EM process to solve Bayesian hierarchical linear models in large-scale information retrieval systems is still too computationally expensive.",
                "In this section, we describe why the learning rate of the EM algorithm is slow in our application and introduce a new technique to make the learning of the Bayesian hierarchical linear model scalable.",
                "The derivation of the new learning algorithm will be based on the EM algorithm described in the previous section.",
                "First, the covariance matrices Σ2 , Σ2 m are usually too large to be computationally feasible.",
                "For simplicity, and as a common practice in IR, we do not model the correlation between features.",
                "Thus we approximate these matrices with K dimensional diagonal matrices.",
                "In the rest of the paper, we use these symbols to represent their diagonal approximations: Σ2 =     σ2 1 0 .. 0 0 σ2 2 .. 0 .. .. .. .. 0 0 .. σ2 K     Σ2 m =     σ2 m,1 0 .. 0 0 σ2 m,2 .. 0 .. .. .. .. 0 0 .. σ2 m,K     Secondly, and most importantly, the input space is very sparse and there are many dimensions that are not related to a particular user in a real IR application.",
                "For example, let us consider a movie recommendation system, with the input variable x representing a particular movie.",
                "For the jth movie that the user m has seen, let xm,j,k = 1 if the director of the movie is Jean-Pierre Jeunet (indexed by k).",
                "Here we assume that whether or not that this director directed a specific movie is represented by the kth dimension.",
                "If the user m has never seen a movie directed by Jean-Pierre Jeunet, then the corresponding dimension is always zero (xm,j,k = 0 for all j) .",
                "One major drawback of the EM algorithm is that the importance of a feature, µk, may be greatly dominated by users who have never encountered this feature (i.e. j xm,j,k = 0) at the M step (Equation 8).",
                "Assume that 100 out of 1 million users have viewed the movie directed by Jean-Pierre Jeunet, and that the viewers have rated all of his movies as excellent.",
                "Intuitively, he is a good director and the weight for him (µk) should be high.",
                "Before the EM iteration, the initial value of µ is usually set to 0.",
                "Since the other 999,900 users have not seen this movie, their corresponding weights (w1,k, w2,k, ..., wm,k..., w999900,k) for that director would be very small initially.",
                "Thus the corresponding weight of the director in the prior µk at the first M step would be very low , and the variance σm,k will be large (Equations 8 and 7).",
                "It is undesirable that users who have never seen any movie produced by the director influence the importance of the director so much.",
                "This makes the convergence of the standard EM algorithm very slow.",
                "Now lets look at whether we can improve the learning speed of the algorithm.",
                "Without a loss of generality, let us assume that the kth dimension of the input variable x is not related to a particular user m. By which we mean, xm,j,k = 0 for all j = 1, ..., Jm.",
                "It is straightforward to prove that the kth row and kth column of Sxx,m are completely filled with zeros, and that the kth dimension of Sxy,m is zeroed as well.",
                "Thus the corresponding kth dimension of the user models mean, ¯wm, should be equal to that of the prior: ¯wm,k = µk, with the corresponding covariance of σm,k = σk.",
                "At the M step, the standard EM algorithm uses the numerical solution of the distribution P(wm|Dm, Φ) estimated at E step (Equation 8 and Equation 7).",
                "However, the numerical solutions are very unreliable for ¯wm,k and σm,k when the kth dimension is not related to the mth user.",
                "A better approach is using the analytical solutions ¯wm,k = µk, and σm,k = σk for the unrelated (m, k) pairs, along with the numerical solution estimated at E step for the other (m, k) pairs.",
                "Thus we get the following new EM-like algorithm: Modified E step: For each user m, estimate the user model distribution P(wm|Dm, Φ) = N(wm; ¯wm, Σ2 m) based on the current estimation of σ , µ, Σ2 . ¯wm = ((Σ2 )−1 + Sxx,m σ2 )−1 ( Sxy,m σ2 + (Σ2 )−1 µ)(10) σ2 m,k = ((σ2 k)−1 + sxx,m,k σ2 )−1 (11) where sxx,m,k = j x2 m,j,k and sxy,m,k = j xm,j,kym,j Modified M Step Optimize the prior Φ = (µ, Σ2 ) based on the estimation from the last E step for related userfeature pairs.",
                "The M step implicitly uses the analytical solution for unrelated user-feature pairs. µk = 1 Mk m:related ¯wm,k (12) σ2 k = 1 Mk m:related σ2 m,k +( ¯wm,k − µk)( ¯wm,k − µk)T (13) where Mk is the number of users that are related to feature k We only estimate the diagonal of Σ2 m and Σ since we are using the diagonal approximation of the covariance matrices.",
                "To estimate ¯wm, we only need to calculate the numerical solutions for dimensions that are related to user m. To estimate σ2 k and µk, we only sum over users that are related to the kth feature.",
                "There are two major benefits of the new algorithm.",
                "First, because only the related (m, k) pairs are needed at the modified M step, the computational complexity in a single EM iteration is much smaller when the data is sparse, and many of (m, k) pairs are unrelated.",
                "Second, the parameters estimated at the modified M step (Equations 12 - 13) are more accurate than the standard M step described in Section 4.1 because the exact analytical solutions ¯wm,k = µk and σm,k = σk for the unrelated (m, k) pairs were used in the new algorithm instead of an approximate solution as in the standard algorithm. 5.",
                "EXPERIMENTAL METHODOLOGY 5.1 Evaluation Data Set To evaluate the proposed technique, we used the following three major data sets (Table 1): MovieLens Data: This data set was created by combining the relevance judgments from the MovieLens[9] data set with documents from the Internet Movie Database (IMDB).",
                "MovieLens allows users to rank how much he/she enjoyed a specific movie on a scale from 1 to 5.",
                "This likeability rating was used as a measurement of how relevant the document representing the corresponding movie is to the user.",
                "We considered documents with likeability scores of 4 or 5 as relevant, and documents with a score of 1 to 3 as irrelevant to the user.",
                "MovieLens provided relevance judgments on 3,057 documents from 6,040 separate users.",
                "On average, each user rated 151 movies, of these 87 were judged to be relevant.",
                "The average score for a document was 3.58.",
                "Documents representing each movie were constructed from the portion of the IMDB database that is available for public download[13].",
                "Based on this database, we created one document per movie that contained the relevant information about it (e.g. directors, actors, etc.).",
                "Table 1: Data Set Statistics.",
                "On Reuters, the number of rating for a simulated user is the number of documents relevant to the corresponding topic.",
                "Data Users Docs Ratings per User MovieLens 6,040 3,057 151 Netflix-all 480,189 17,770 208 Netflix-1000 1000 17,770 127 Reuters-C 34 100,000 3949 Reuters-E 26 100,000 1632 Reuters-G 33 100,000 2222 Reuters-M 10 100,000 6529 Netflix Data: This data set was constructed by combining documents about movies crawled from the web with a set of actual movie rental customer relevance judgments from Netflix[19].",
                "Netflix publicly provides the relevance judgments of 480,189 anonymous customers.",
                "There are around 100 million rating on a scale of 1 to 5 for 17,770 documents.",
                "Similar to MovieLens, we considered documents with likeability scores of 4 or 5 as relevant.",
                "This number was reduced to 1000 customers through random sampling.",
                "The average customer on the reduced data set provided 127 judgments, with 70 being deemed relevant.",
                "The average score for documents is 3.55.",
                "Reuters Data: This is the Reuters Corpus, Volume 1.",
                "It covers 810,000 Reuters English language news stories from August 20, 1996 to August 19, 1997.",
                "Only the first 100,000 news were used in our experiments.",
                "The Reuters corpus comes with a topic hierarchy.",
                "Each document is assigned to one of several locations on the hierarchical tree.",
                "The first level of the tree contains four topics, denoted as C, E, M, and G. For the experiments in this paper, the tree was cut at level 1 to create four smaller trees, each of which corresponds to one smaller data set: Reuters-E Reuters-C, ReutersM and Reuters-G. For each small data set, we created several profiles, one profile for each node in a sub-tree, to simulate multiple users, each with a related, yet separate definition of relevance.",
                "All the user profiles on a sub-tree are supposed to share the same prior model distribution.",
                "Since this corpus explicitly indicates only the relevant documents for a topic(user), all other documents are considered irrelevant. 5.2 Evaluation We designed the experiments to answer the following three questions: 1.",
                "Do we need to take the effort to use a Bayesian approach and learn a prior from other users? 2.",
                "Does the new algorithm work better than the standard EM algorithm for learning the Bayesian hierarchical linear model? 3.",
                "Can the new algorithm quickly learn many user models?",
                "To answer the first question, we compared the Bayesian hierarchical models with commonly used Norm-2 regularized linear regression models.",
                "In fact, the commonly used approach is equivalent to the model learned at the end of the first EM iteration.",
                "To answer the second question, we compared the proposed new algorithm with the standard EM algorithm to see whether the new learning algorithm is better.",
                "To answer the third question, we tested the efficiency of the new algorithm on the entire Netflix data set where about half a million user models need to be learned together.",
                "For the MovieLens and Netflix data sets, algorithm effectiveness was measured by mean square error, while on the Reuters data set classification error was used because it was more informative.",
                "We first evaluated the performance on each individual user, and then estimated the macro average over all users.",
                "Statistical tests (t-tests) were carried out to see whether the results are significant.",
                "For the experiments on the MovieLens and Netflix data sets, we used a random sample of 90% of each user for training, and the rest for testing.",
                "On Reuters data set, because there are too many relevant documents for each topic in the corpus, we used a random sample of 10% of each topic for training, and 10% of the remaining documents for testing.",
                "For all runs, we set (a, b, c, Σ ) = (0.1, 10, 0.1, 1) manually. 6.",
                "EXPERIMENTAL RESULTS Figure 2, Figure 3, and Figure 4 show that on all data sets, the Bayesian hierarchical <br>modeling</br> approach has a statistical significant improvement over the regularized linear regression model, which is equivalent to the Bayesian hierarchical models learned at the first iteration.",
                "Further analysis shows a negative correlation between the number of training data for a user and the improvement the system gets.",
                "This suggests that the borrowing information from other users has more significant improvements for users with less training data, which is as expected.",
                "However, the strength of the correlation differs over data sets, and the amount of training data is not the only characteristics that will influence the final performance.",
                "Figure 2 and Figure 3 show that the proposed new algorithm works better than the standard EM algorithm on the Netflix and MovieLens data sets.",
                "This is not surprising since the number of related feature-users pairs is much smaller than the number of unrelated feature-user pairs on these two data sets, and thus the proposed new algorithm is expected to work better.",
                "Figure 4 shows that the two algorithms work similarly on the Reuters-E data set.",
                "The accuracy of the new algorithm is similar to that of the standard EM algorithm at each iteration.",
                "The general patterns are very similar on other Reuters subsets.",
                "Further analysis shows that only 58% of the user-feature pairs are unrelated on this data set.",
                "Since the number of unrelated user-feature pairs is not extremely large, the sparseness is not a serious problem on the Reuters data set.",
                "Thus the two learning algorithms perform similarly.",
                "The results suggest that only on a corpus where the number of unrelated user-feature pairs is much larger than the number of related pairs, such as on the Netflix data set, the proposed technique will get a significant improvement over standard EM.",
                "However, the experiments also show that when the assumption does not hold, the new algorithm does not hurt performance.",
                "Although the proposed technique is faster than standard Figure 2: Performance on a Netflix subset with 1,000 users.",
                "The new algorithm is statistical significantly better than EM algorithm at iterations 2 - 10.",
                "Norm-2 regularized linear models are equivalent to the Bayesian hierarchical models learned at the first iteration, and are statistical significantly worse than the Bayesian hierarchical models. 0 2 4 6 8 10 1 1.05 1.1 1.15 1.2 1.25 1.3 1.35 1.4 Iterations MeanSquareError New Algorithm Traditional EM 1 2 3 4 5 6 7 8 9 10 0.33 0.34 0.35 0.36 0.37 0.38 0.39 Iterations ClassificationError New Algorithm Traditional EM Figure 3: Performance on a MovieLens subset with 1,000 users.",
                "The new algorithm is statistical significantly better than EM algorithm at iteration 2 to 17 (evaluated with mean square error). 1 6 11 16 21 0.5 1 1.5 2 2.5 3 3.5 Iterations MeanSquareError New Algorithm Traditional EM 1 6 11 16 21 0.35 0.4 0.45 0.5 0.55 0.6 0.65 Iterations ClassificationError New Algorithm Traditional EM Figure 4: Performance on a Reuters-E subset with 26 profiles.",
                "Performances on Reuters-C, Reuters-M, Reuters-G are similar. 1 2 3 4 5 0.011 0.0115 0.012 0.0125 0.013 0.0135 0.014 Iterations MeanSquareError New Algorithm Traditional EM 1 2 3 4 5 0.0102 0.0104 0.0106 0.0108 0.011 0.0112 0.0114 Iterations ClassificationError New Algorithm Traditional EM EM, can it really learn millions of user models quickly?",
                "Our results show that the modified EM algorithm converges quickly, and 2 - 3 modified EM iterations would result in a reliable estimation.",
                "We evaluated the algorithm on the whole Netflix data set (480,189 users, 159,836 features, and 100 million ratings) running on a single CPU PC (2GB memory, P4 3GHz).",
                "The system finished one modified EM iteration in about 4 hours.",
                "This demonstrates that the proposed technique can efficiently handle large-scale system like Netflix. 7.",
                "CONCLUSION Content-based user profile learning is an important problem and is the key to providing personal recommendations to a user, especially for recommending new items with a small number of ratings.",
                "The Bayesian hierarchical <br>modeling</br> approach is becoming an important user profile learning approach due to its theoretically justified ability to help one user through information transfer from the other users by way of hyperpriors.",
                "This paper examined the weakness of the popular EM based learning approach for Bayesian hierarchical linear models and proposed a better learning technique called Modified EM.",
                "We showed that the new technique is theoretically more computationally efficient than the standard EM algorithm.",
                "Evaluation on the MovieLens and Netflix data sets demonstrated the effectiveness of the new technique when the data is sparse, by which we mean the ratio of related user-feature pairs to unrelated pairs is small.",
                "Evaluation on the Reuters data set showed that the new technique performed similar to the standard EM algorithm when the sparseness condition does not hold.",
                "In general, it is better to use the new algorithm since it is as simple as standard EM, the performance is either better or similar to EM, and the computation complexity is lower at each iteration.",
                "It is worth mentioning that even if the original problem space is not sparse, sparseness can be created artificially when a recommendation system uses user-specific feature selection techniques to reduce the noise and user model complexity.",
                "The proposed technique can also be adapted to improve the learning in such a scenario.",
                "We also demonstrated that the proposed technique can learn half a million user profiles from 100 million ratings in a few hours with a single CPU.",
                "The research is important because scalability is a major concern for researchers when using the Bayesian hierarchical linear <br>modeling</br> approach to build a practical large scale system, even though the literature have demonstrated the effectiveness of the models in many applications.",
                "Our work is one major step on the road to make Bayesian hierarchical linear models more practical.",
                "The proposed new technique can be easily adapted to run on a cluster of machines, and thus further speed up the learning process to handle a larger scale system with hundreds of millions of users.",
                "The research has much potential to benefit people using EM algorithm on many other IR problems as well as machine learning problems.",
                "EM algorithm is a commonly used machine learning technique.",
                "It is used to find model parameters in many IR problems where the training data is very sparse.",
                "Although we are focusing on the Bayesian hierarchical linear models for recommendation and filtering, the new idea of using analytical solution instead of numerical solution for unrelated user-feature pairs at the M step could be adapted to many other problems. 8.",
                "ACKNOWLEDGMENTS We thank Wei Xu, David Lewis and anonymous reviewers for valuable feedback on the work described in this paper.",
                "Part of the work was supported by Yahoo, Google, the Petascale Data Storage Institute and the Institute for Scalable Scientific Data Management.",
                "Any opinions, findings, conclusions, or recommendations expressed in this material are those of the authors, and do not necessarily reflect those of the sponsors. 9.",
                "REFERENCES [1] C. Basu, H. Hirsh, and W. Cohen.",
                "Recommendation as classification: Using social and content-based information in recommendation.",
                "In Proceedings of the Fifteenth National Conference on Artificial Intelligence, 1998. [2] J. S. Breese, D. Heckerman, and C. Kadie.",
                "Empirical analysis of predictive algorithms for collaborative filtering.",
                "Technical report, Microsoft Research, One Microsoft Way, Redmond, WA 98052, 1998. [3] J. Callan.",
                "Document filtering with inference networks.",
                "In Proceedings of the Nineteenth Annual International ACM SIGIR Conference on Research and Development in Information Retrieval, pages 262-269, 1996. [4] N. Cancedda, N. Cesa-Bianchi, A. Conconi, C. Gentile, C. Goutte, T. Graepel, Y. Li, J. M. Renders, J. S. Taylor, and A. Vinokourov.",
                "Kernel method for document filtering.",
                "In The Eleventh Text REtrieval Conference (TREC11).",
                "National Institute of Standards and Technology, special publication 500-249, 2003. [5] C. Chelba and A. Acero.",
                "Adaptation of maximum entropy capitalizer: Little data can help a lot.",
                "In D. Lin and D. Wu, editors, Proceedings of EMNLP 2004, pages 285-292, Barcelona, Spain, July 2004.",
                "Association for Computational Linguistics. [6] B. Croft and J. Lafferty, editors.",
                "Language <br>modeling</br> for Information Retrieval.",
                "Kluwer, 2002. [7] A. Dayanik, D. D. Lewis, D. Madigan, V. Menkov, and A. Genkin.",
                "Constructing informative prior distributions from domain knowledge in text classification.",
                "In SIGIR 06: Proceedings of the 29th annual international ACM SIGIR conference on Research and development in information retrieval, pages 493-500, New York, NY, USA, 2006.",
                "ACM Press. [8] J. Delgado and N. Ishii.",
                "Memory-based weightedmajority prediction for recommender systems.",
                "In ACM SIGIR99 Workshop on Recommender Systems, 1999. [9] GroupLens.",
                "Movielens. http://www.grouplens.org/taxonomy/term/14, 2006. [10] D. Heckerman.",
                "A tutorial on learning with bayesian networks.",
                "In M. Jordan, editor, Learning in Graphical Models.",
                "Kluwer Academic, 1998. [11] J. L. Herlocker, J.",
                "A. Konstan, A. Borchers, and J. Riedl.",
                "An algorithmic framework for performing collaborative filtering.",
                "In SIGIR 99: Proceedings of the 22nd annual international ACM SIGIR conference on Research and development in information retrieval, pages 230-237, New York, NY, USA, 1999.",
                "ACM Press. [12] T. Hofmann and J. Puzicha.",
                "Latent class models for collaborative filtering.",
                "In IJCAI 99: Proceedings of the Sixteenth International Joint Conference on Artificial Intelligence, pages 688-693, San Francisco, CA, USA, 1999.",
                "Morgan Kaufmann Publishers Inc. [13] I. M. D. (IMDB).",
                "Internet movie database. http://www.imdb.com/interfaces/, 2006. [14] R. Jin, J. Y. Chai, and L. Si.",
                "An automatic weighting scheme for collaborative filtering.",
                "In SIGIR 04: Proceedings of the 27th annual international ACM SIGIR conference on Research and development in information retrieval, pages 337-344, New York, NY, USA, 2004.",
                "ACM Press. [15] J.",
                "A. Konstan, B. N. Miller, D. Maltz, J. L. Herlocker, L. R. Gordon, and J. Riedl.",
                "GroupLens: Applying collaborative filtering to Usenet news.",
                "Communications of the ACM, 40(3):77-87, 1997. [16] D. Lewis.",
                "Applying support vector machines to the TREC-2001 batch filtering and routing tasks.",
                "In Proceedings of the Eleventh Text REtrieval Conference (TREC-11), 2002. [17] B. Liu, X. Li, W. S. Lee, , and P. Yu.",
                "Text classification by labeling words.",
                "In Proceedings of The Nineteenth National Conference on Artificial Intelligence (AAAI-2004), July 25-29, 2004. [18] P. Melville, R. J. Mooney, and R. Nagarajan.",
                "Content-boosted collaborative filtering for improved recommendations.",
                "In Proceedings of the Eighteenth National Conference on Artificial Intelligence (AAAI-2002), Edmonton, Canada, 2002. [19] Netflix.",
                "Netflix prize. http://www.netflixprize.com (visited on Nov. 30, 2006), 2006. [20] S. Robertson and K. Sparck-Jones.",
                "Relevance weighting of search terms.",
                "In Journal of the American Society for Information Science, volume 27, pages 129-146, 1976. [21] J. Wang, A. P. de Vries, and M. J. T. Reinders.",
                "Unifying user-based and item-based collaborative filtering approaches by similarity fusion.",
                "In SIGIR 06: Proceedings of the 29th annual international ACM SIGIR conference on Research and development in information retrieval, pages 501-508, New York, NY, USA, 2006.",
                "ACM Press. [22] X. Wu and R. K. Srihari.",
                "Incorporating prior knowledge with weighted margin support vector machines.",
                "In Proc.",
                "ACM Knowledge Discovery Data Mining Conf. (ACM SIGKDD 2004), Aug. 2004. [23] Y. Yang, S. Yoo, J. Zhang, and B. Kisiel.",
                "Robustness of adaptive filtering methods in a cross-benchmark evaluation.",
                "In Proceedings of the 28th Annual International ACM SIGIR Conference on Research and Development in Information Retrieval, 2005. [24] K. Yu, V. Tresp, and A. Schwaighofer.",
                "Learning gaussian processes from multiple tasks.",
                "In ICML 05: Proceedings of the 22nd international conference on Machine learning, pages 1012-1019, New York, NY, USA, 2005.",
                "ACM Press. [25] K. Yu, V. Tresp, and S. Yu.",
                "A nonparametric hierarchical bayesian framework for information filtering.",
                "In SIGIR 04: Proceedings of the 27th annual international ACM SIGIR conference on Research and development in information retrieval, pages 353-360.",
                "ACM Press, 2004. [26] X. Zhu.",
                "Semi-supervised learning literature survey.",
                "Technical report, University of Wisconsin - Madison, December 9, 2006. [27] P. Zigoris and Y. Zhang.",
                "Bayesian adaptive user profiling with explicit & implicit feedback.",
                "In Conference on Information and Knowledge Mangement 2006, 2006."
            ],
            "original_annotated_samples": [],
            "translated_annotated_samples": [
                "Usuario jerárquico bayesiano eficiente \"modelado\" para sistemas de recomendaciones Yi Zhang, Jonathan Koren School of Engineering University of California Santa Cruz Santa Cruz, CA, EE. UU.Perfiles específicos de los comentarios de los usuarios para que pueda entregar información adaptada al interés de cada usuarios individuales.",
                "Un enfoque bien recibido para mejorar el rendimiento del sistema de recomendación para un usuario en particular es tomar prestada información de otros usuarios a través de un enfoque de \"modelado\" jerárquico bayesiano.",
                "La organización de las partes restantes de este documento es la siguiente: la Sección 3 describe el marco de \"modelado\" de regresión lineal jerárquica bayesiana utilizada para recomendaciones basadas en contenido.",
                "El enfoque jerárquico de \"modelado\" bayesiano se ha utilizado ampliamente en aplicaciones de recuperación de información del mundo real.",
                "Resultados experimentales La Figura 2, la Figura 3 y la Figura 4 muestran que en todos los conjuntos de datos, el enfoque jerárquico de \"modelado\" bayesiano tiene una mejora estadística significativa sobre el modelo de regresión lineal regularizado, que es equivalente a los modelos jerárquicos bayesianos aprendidos en la primera iteración.",
                "El enfoque de \"modelado\" bayesiano se está convirtiendo en un importante enfoque de aprendizaje de perfil de usuario debido a su capacidad teóricamente justificada para ayudar a un usuario a través de la transferencia de información de los otros usuarios a través de hiperpriors.",
                "La investigación es importante porque la escalabilidad es una preocupación importante para los investigadores cuando utilizan el enfoque de \"modelado\" jerárquico bayesiano lineal para construir un sistema práctico a gran escala, a pesar de que la literatura ha demostrado la efectividad de los modelos en muchas aplicaciones.",
                "Lenguaje \"modelado\" para la recuperación de información."
            ],
            "translated_text": "",
            "candidates": [
                "modelado",
                "modelado",
                "modelado",
                "modelado",
                "modelado",
                "modelado",
                "modelado",
                "modelado",
                "modelado",
                "modelado",
                "modelado",
                "modelado",
                "modelado",
                "modelado",
                "modelado",
                "modelado"
            ],
            "error": []
        },
        "content-based": {
            "translated_key": "basado en el contenido",
            "is_in_text": true,
            "original_annotated_sentences": [
                "Efficient Bayesian Hierarchical User Modeling for Recommendation Systems Yi Zhang, Jonathan Koren School of Engineering University of California Santa Cruz Santa Cruz, CA, USA {yiz, jonathan}@soe.ucsc.edu ABSTRACT A <br>content-based</br> personalized recommendation system learns user specific profiles from user feedback so that it can deliver information tailored to each individual users interest.",
                "A system serving millions of users can learn a better user profile for a new user, or a user with little feedback, by borrowing information from other users through the use of a Bayesian hierarchical model.",
                "Learning the model parameters to optimize the joint data likelihood from millions of users is very computationally expensive.",
                "The commonly used EM algorithm converges very slowly due to the sparseness of the data in IR applications.",
                "This paper proposes a new fast learning technique to learn a large number of individual user profiles.",
                "The efficacy and efficiency of the proposed algorithm are justified by theory and demonstrated on actual user data from Netflix and MovieLens.",
                "Categories and Subject Descriptors: B.3.3 [Information Search and Retrieval]: Information filtering General Terms: Algorithms 1.",
                "INTRODUCTION Personalization is the future of the Web, and it has achieved great success in industrial applications.",
                "For example, online stores, such as Amazon and Netflix, provide customized recommendations for additional products or services based on a users history.",
                "Recent offerings such as My MSN, My Yahoo!, My Google, and Google News have attracted much attention due to their potential ability to infer a users interests from his/her history.",
                "One major personalization topic studied in the information retrieval community is <br>content-based</br> personal recommendation systems1 .",
                "These systems learn user-specific profiles from user feedback so that they can recommend information tailored to each individual users interest without requiring the user to make an explicit query.",
                "Learning the user profiles is the core problem for these systems.",
                "A user profile is usually a classifier that can identify whether a document is relevant to the user or not, or a regression model that tells how relevant a document is to the user.",
                "One major challenge of building a recommendation or personalization system is that the profile learned for a particular user is usually of low quality when the amount of data from that particular user is small.",
                "This is known as the cold start problem.",
                "This means that any new user must endure poor initial performance until sufficient feedback from that user is provided to learn a reliable user profile.",
                "There has been much research on improving classification accuracy when the amount of labeled training data is small.",
                "The semi-supervised learning approach combines unlabeled and labeled data together to achieve this goal [26].",
                "Another approach is using domain knowledge.",
                "Researchers have modified different learning algorithms, such as Na¨ıveBayes [17], logistic regression [7], and SVMs [22], to integrate domain knowledge into a text classifier.",
                "The third approach is borrowing training data from other resources [5][7].",
                "The effectiveness of these different approaches is mixed, due to how well the underlying model assumption fits the data.",
                "One well-received approach to improve recommendation system performance for a particular user is borrowing information from other users through a Bayesian hierarchical modeling approach.",
                "Several researchers have demonstrated that this approach effectively trades off between shared and user-specific information, thus alleviating poor initial performance for each user[27][25].",
                "In order to learn a Bayesian hierarchical model, the system usually tries to find the most likely model parameters for the given data.",
                "A mature recommendation system usually works for millions of users.",
                "It is well known that learning the optimal parameters of a Bayesian hierarchical model is computationally expensive when there are thousands or millions of users.",
                "The EM algorithm is a commonly used technique for parameter learning due to its simplicity and convergence guarantee.",
                "However, a content based recommendation system often handles documents in a very high dimensional space, in which each document is represented by a very sparse vector.",
                "With careful analysis of the EM algorithm in this scenario (Section 4), we find that the EM tering, or item-based collaborative filtering.",
                "In this paper, the words filtering and recommendation are used interchangeably. algorithm converges very slowly due to the sparseness of the input variables.",
                "We also find that updating the model parameter at each EM iteration is also expensive with computational complexity of O(MK), where M is the number of users and K is the number of dimensions.",
                "This paper modifies the standard EM algorithm to create an improved learning algorithm, which we call the Modified EM algorithm.",
                "The basic idea is that instead of calculating the numerical solution for all the user profile parameters, we derive the analytical solution of the parameters for some feature dimensions, and at the M step use the analytical solution instead of the numerical solution estimated at E step for those parameters.",
                "This greatly reduces the computation at a single EM iteration, and also has the benefit of increasing the convergence speed of the learning algorithm.",
                "The proposed technique is not only well supported by theory, but also by experimental results.",
                "The organization of the remaining parts of this paper is as follows: Section 3 describes the Bayesian hierarchical linear regression modeling framework used for <br>content-based</br> recommendations.",
                "Section 4 describes how to learn the model parameters using the standard EM algorithm, along with using the new technique proposed in this paper.",
                "The experimental setting and results used to validate the proposed learning technique are reported in Sections 5 and 6.",
                "Section 7 summarizes and offers concluding remarks. 2.",
                "RELATED WORK Providing personalized recommendations to users has been identified as a very important problem in the IR community since the 1970s.",
                "The approaches that have been used to solve this problem can be roughly classified into two major categories: content based filtering versus collaborative filtering.",
                "<br>content-based</br> filtering studies the scenario where a recommendation system monitors a document stream and pushes documents that match a user profile to the corresponding user.",
                "The user may read the delivered documents and provide explicit relevance feedback, which the filtering system then uses to update the users profile using relevance feedback retrieval models (e.g.",
                "Boolean models, vector space models, traditional probabilistic models [20] , inference networks [3] and language models [6]) or machine learning algorithms (e.g.",
                "Support Vector Machines (SVM), K nearest neighbors (K-NN) clustering, neural networks, logistic regression, or Winnow [16] [4] [23]).",
                "Collaborative filtering goes beyond merely using document content to recommend items to a user by leveraging information from other users with similar tastes and preferences in the past.",
                "Memorybased heuristics and model based approaches have been used in collaborative filtering task [15] [8] [2] [14] [12] [11].",
                "This paper contributes to the <br>content-based</br> recommendation research by improving the efficiency and effectiveness of Bayesian hierarchical linear models, which have a strong theoretical basis and good empirical performance on recommendation tasks[27][25].",
                "This paper does not intend to compare <br>content-based</br> filtering with collaborative filtering or claim which one is a better.",
                "We think each complements the other, and that <br>content-based</br> filtering is extremely useful for handling new documents/items with little or no user feedback.",
                "Similar to some other researchers[18][1][21], we found that a recommendation system will be more effective when both techniques are combined.",
                "However, this is beyond the scope of this paper and thus not discussed here. 3.",
                "BAYESIAN HIERARCHICAL LINEAR REGRESSION Assume there are M users in the system.",
                "The task of the system is to recommend documents that are relevant to each user.",
                "For each user, the system learns a user model from the users history.",
                "In the rest of this paper, we will use the following notations to represent the variables in the system. m = 1, 2, ..., M: The index for each individual user.",
                "M is the total number of users. wm: The user model parameter associated with user m. wm is a K dimensional vector. j = 1, 2, ..., Jm: The index for a set of data for user m. Jm is the number of training data for user m. Dm = {(xm,j, ym,j)}: A set of data associated with user m. xm,j is a K dimensional vector that represents the mth users jth training document.2 ym,j is a scalar that represents the label of document xm,j. k = 1, 2, ..., K: The dimensional index of input variable x.",
                "The Bayesian hierarchical modeling approach has been widely used in real-world information retrieval applications.",
                "Generalized Bayesian hierarchical linear models, one of the simplest Bayesian hierarchical models, are commonly used and have achieved good performance on collaborative filtering [25] and <br>content-based</br> adaptive filtering [27] tasks.",
                "Figure 1 shows the graphical representation of a Bayesian hierarchical model.",
                "In this graph, each user model is represented by a random vector wm.",
                "We assume a user model is sampled randomly from a prior distribution P(w|Φ).",
                "The system can predict the user label y of a document x given an estimation of wm (or wms distribution) using a function y = f(x, w).",
                "The model is called generalized Bayesian hierarchical linear model when y = f(wT x) is any generalized linear model such as logistic regression, SVM, and linear regression.",
                "To reliably estimate the user model wm, the system can borrow information from other users through the prior Φ = (µ, Σ).",
                "Now we look at one commonly used model where y = wT x + , where ∼ N(0, σ2 ) is a random noise [25][27].",
                "Assume that each user model wm is an independent draw from a population distribution P(w|Φ), which is governed by some unknown hyperparameter Φ.",
                "Let the prior distribution of user model w be a Gaussian distribution with parameter Φ = (µ, Σ), which is the commonly used prior for linear models. µ = (µ1, µ2, ..., µK ) is a K dimensional vector that represents the mean of the Gaussian distribution, and Σ is the covariance matrix of the Gaussian.",
                "Usually, a Normal distribution N(0, aI) and an Inverse Wishart distribution P(Σ) ∝ |Σ|− 1 2 b exp(−1 2 ctr(Σ−1 )) are used as hyperprior to model the prior distribution of µ and Σ respectively.",
                "I is the K dimensional identity matrix, and a, b, and c are real numbers.",
                "With these settings, we have the following model for the system: 1. µ and Σ are sampled from N(0, aI) and IWν (aI), respectively. 2 The first dimension of x is a dummy variable that always equals to 1.",
                "Figure 1: Illustration of dependencies of variables in the hierarchical model.",
                "The rating, y, for a document, x, is conditioned on the document and the user model, wm, associated with the user m. Users share information about their models through the prior, Φ = (µ, Σ). 2.",
                "For each user m, wm is sampled randomly from a Normal distribution: wm ∼ N(µ, Σ2 ) 3.",
                "For each item xm,j, ym,j is sampled randomly from a Normal distribution: ym,j ∼ N(wT mxm,j, σ2 ).",
                "Let θ = (Φ, w1, w2, ..., wM ) represent the parameters of this system that needs to be estimated.",
                "The joint likelihood for all the variables in the probabilistic model, which includes the data and the parameters, is: P(D, θ) = P(Φ) m P(wm|Φ) j P(ym,j|xm,j, wm) (1) For simplicity, we assume a, b, c, and σ are provided to the system. 4.",
                "MODEL PARAMETER LEARNING If the prior Φ is known, finding the optimal wm is straightforward: it is a simple linear regression.",
                "Therefore, we will focus on estimating Φ.",
                "The maximum a priori solution of Φ is given by ΦMAP = arg max Φ P(Φ|D) (2) = arg max Φ P(Φ, D) P(D) (3) = arg max Φ P(D|Φ)P(Φ) (4) = arg max Φ w P(D|w, Φ)P(w|Φ)P(Φ)dw (5) Finding the optimal solution for the above problem is challenging, since we need to integrate over all w = (w1, w2, ..., wM ), which are unobserved hidden variables. 4.1 EM Algorithm for Bayesian Hierarchical Linear Models In Equation 5, Φ is the parameter needs to be estimated, and the result depends on unobserved latent variables w. This kind of optimization problem is usually solved by the EM algorithm.",
                "Applying EM to the above problem, the set of user models w are the unobservable hidden variables and we have: Q = w P(w|µ, Σ2 , Dm) log P(µ, Σ2 , w, D)dw Based on the derivation of the EM formulas presented in [24], we have the following Expectation-Maximization steps for finding the optimal hyperparameters.",
                "For space considerations, we omit the derivation in this paper since it is not the focus of our work.",
                "E step: For each user m, estimate the user model distribution P(wm|Dm, Φ) = N(wm; ¯wm, Σ2 m) based on the current estimation of the prior Φ = (µ, Σ2 ). ¯wm = ((Σ2 )−1 + Sxx,m σ2 )−1 ( Sxy,m σ2 + (Σ2 )−1 µ) (6) Σ2 m = ((Σ2 )−1 + Sxx,m σ2 )−1 (7) where Sxx,m = j xm,jxT m,j Sxy,m = j xm,jym,j M step: Optimize the prior Φ = (µ, Σ2 ) based on the estimation from the last E step. µ = 1 M m ¯wm (8) Σ2 = 1 M m Σ2 m + ( ¯wm − µ)( ¯wm − µ)T (9) Many machine learning driven IR systems use a point estimate of the parameters at different stages in the system.",
                "However, we are estimating the posterior distribution of the variables at the E step.",
                "This avoids overfitting wm to a particular users data, which may be small and noisy.",
                "A detailed discussion about this subject appears in [10]. 4.2 New Algorithm: Modified EM Although the EM algorithm is widely studied and used in machine learning applications, using the above EM process to solve Bayesian hierarchical linear models in large-scale information retrieval systems is still too computationally expensive.",
                "In this section, we describe why the learning rate of the EM algorithm is slow in our application and introduce a new technique to make the learning of the Bayesian hierarchical linear model scalable.",
                "The derivation of the new learning algorithm will be based on the EM algorithm described in the previous section.",
                "First, the covariance matrices Σ2 , Σ2 m are usually too large to be computationally feasible.",
                "For simplicity, and as a common practice in IR, we do not model the correlation between features.",
                "Thus we approximate these matrices with K dimensional diagonal matrices.",
                "In the rest of the paper, we use these symbols to represent their diagonal approximations: Σ2 =     σ2 1 0 .. 0 0 σ2 2 .. 0 .. .. .. .. 0 0 .. σ2 K     Σ2 m =     σ2 m,1 0 .. 0 0 σ2 m,2 .. 0 .. .. .. .. 0 0 .. σ2 m,K     Secondly, and most importantly, the input space is very sparse and there are many dimensions that are not related to a particular user in a real IR application.",
                "For example, let us consider a movie recommendation system, with the input variable x representing a particular movie.",
                "For the jth movie that the user m has seen, let xm,j,k = 1 if the director of the movie is Jean-Pierre Jeunet (indexed by k).",
                "Here we assume that whether or not that this director directed a specific movie is represented by the kth dimension.",
                "If the user m has never seen a movie directed by Jean-Pierre Jeunet, then the corresponding dimension is always zero (xm,j,k = 0 for all j) .",
                "One major drawback of the EM algorithm is that the importance of a feature, µk, may be greatly dominated by users who have never encountered this feature (i.e. j xm,j,k = 0) at the M step (Equation 8).",
                "Assume that 100 out of 1 million users have viewed the movie directed by Jean-Pierre Jeunet, and that the viewers have rated all of his movies as excellent.",
                "Intuitively, he is a good director and the weight for him (µk) should be high.",
                "Before the EM iteration, the initial value of µ is usually set to 0.",
                "Since the other 999,900 users have not seen this movie, their corresponding weights (w1,k, w2,k, ..., wm,k..., w999900,k) for that director would be very small initially.",
                "Thus the corresponding weight of the director in the prior µk at the first M step would be very low , and the variance σm,k will be large (Equations 8 and 7).",
                "It is undesirable that users who have never seen any movie produced by the director influence the importance of the director so much.",
                "This makes the convergence of the standard EM algorithm very slow.",
                "Now lets look at whether we can improve the learning speed of the algorithm.",
                "Without a loss of generality, let us assume that the kth dimension of the input variable x is not related to a particular user m. By which we mean, xm,j,k = 0 for all j = 1, ..., Jm.",
                "It is straightforward to prove that the kth row and kth column of Sxx,m are completely filled with zeros, and that the kth dimension of Sxy,m is zeroed as well.",
                "Thus the corresponding kth dimension of the user models mean, ¯wm, should be equal to that of the prior: ¯wm,k = µk, with the corresponding covariance of σm,k = σk.",
                "At the M step, the standard EM algorithm uses the numerical solution of the distribution P(wm|Dm, Φ) estimated at E step (Equation 8 and Equation 7).",
                "However, the numerical solutions are very unreliable for ¯wm,k and σm,k when the kth dimension is not related to the mth user.",
                "A better approach is using the analytical solutions ¯wm,k = µk, and σm,k = σk for the unrelated (m, k) pairs, along with the numerical solution estimated at E step for the other (m, k) pairs.",
                "Thus we get the following new EM-like algorithm: Modified E step: For each user m, estimate the user model distribution P(wm|Dm, Φ) = N(wm; ¯wm, Σ2 m) based on the current estimation of σ , µ, Σ2 . ¯wm = ((Σ2 )−1 + Sxx,m σ2 )−1 ( Sxy,m σ2 + (Σ2 )−1 µ)(10) σ2 m,k = ((σ2 k)−1 + sxx,m,k σ2 )−1 (11) where sxx,m,k = j x2 m,j,k and sxy,m,k = j xm,j,kym,j Modified M Step Optimize the prior Φ = (µ, Σ2 ) based on the estimation from the last E step for related userfeature pairs.",
                "The M step implicitly uses the analytical solution for unrelated user-feature pairs. µk = 1 Mk m:related ¯wm,k (12) σ2 k = 1 Mk m:related σ2 m,k +( ¯wm,k − µk)( ¯wm,k − µk)T (13) where Mk is the number of users that are related to feature k We only estimate the diagonal of Σ2 m and Σ since we are using the diagonal approximation of the covariance matrices.",
                "To estimate ¯wm, we only need to calculate the numerical solutions for dimensions that are related to user m. To estimate σ2 k and µk, we only sum over users that are related to the kth feature.",
                "There are two major benefits of the new algorithm.",
                "First, because only the related (m, k) pairs are needed at the modified M step, the computational complexity in a single EM iteration is much smaller when the data is sparse, and many of (m, k) pairs are unrelated.",
                "Second, the parameters estimated at the modified M step (Equations 12 - 13) are more accurate than the standard M step described in Section 4.1 because the exact analytical solutions ¯wm,k = µk and σm,k = σk for the unrelated (m, k) pairs were used in the new algorithm instead of an approximate solution as in the standard algorithm. 5.",
                "EXPERIMENTAL METHODOLOGY 5.1 Evaluation Data Set To evaluate the proposed technique, we used the following three major data sets (Table 1): MovieLens Data: This data set was created by combining the relevance judgments from the MovieLens[9] data set with documents from the Internet Movie Database (IMDB).",
                "MovieLens allows users to rank how much he/she enjoyed a specific movie on a scale from 1 to 5.",
                "This likeability rating was used as a measurement of how relevant the document representing the corresponding movie is to the user.",
                "We considered documents with likeability scores of 4 or 5 as relevant, and documents with a score of 1 to 3 as irrelevant to the user.",
                "MovieLens provided relevance judgments on 3,057 documents from 6,040 separate users.",
                "On average, each user rated 151 movies, of these 87 were judged to be relevant.",
                "The average score for a document was 3.58.",
                "Documents representing each movie were constructed from the portion of the IMDB database that is available for public download[13].",
                "Based on this database, we created one document per movie that contained the relevant information about it (e.g. directors, actors, etc.).",
                "Table 1: Data Set Statistics.",
                "On Reuters, the number of rating for a simulated user is the number of documents relevant to the corresponding topic.",
                "Data Users Docs Ratings per User MovieLens 6,040 3,057 151 Netflix-all 480,189 17,770 208 Netflix-1000 1000 17,770 127 Reuters-C 34 100,000 3949 Reuters-E 26 100,000 1632 Reuters-G 33 100,000 2222 Reuters-M 10 100,000 6529 Netflix Data: This data set was constructed by combining documents about movies crawled from the web with a set of actual movie rental customer relevance judgments from Netflix[19].",
                "Netflix publicly provides the relevance judgments of 480,189 anonymous customers.",
                "There are around 100 million rating on a scale of 1 to 5 for 17,770 documents.",
                "Similar to MovieLens, we considered documents with likeability scores of 4 or 5 as relevant.",
                "This number was reduced to 1000 customers through random sampling.",
                "The average customer on the reduced data set provided 127 judgments, with 70 being deemed relevant.",
                "The average score for documents is 3.55.",
                "Reuters Data: This is the Reuters Corpus, Volume 1.",
                "It covers 810,000 Reuters English language news stories from August 20, 1996 to August 19, 1997.",
                "Only the first 100,000 news were used in our experiments.",
                "The Reuters corpus comes with a topic hierarchy.",
                "Each document is assigned to one of several locations on the hierarchical tree.",
                "The first level of the tree contains four topics, denoted as C, E, M, and G. For the experiments in this paper, the tree was cut at level 1 to create four smaller trees, each of which corresponds to one smaller data set: Reuters-E Reuters-C, ReutersM and Reuters-G. For each small data set, we created several profiles, one profile for each node in a sub-tree, to simulate multiple users, each with a related, yet separate definition of relevance.",
                "All the user profiles on a sub-tree are supposed to share the same prior model distribution.",
                "Since this corpus explicitly indicates only the relevant documents for a topic(user), all other documents are considered irrelevant. 5.2 Evaluation We designed the experiments to answer the following three questions: 1.",
                "Do we need to take the effort to use a Bayesian approach and learn a prior from other users? 2.",
                "Does the new algorithm work better than the standard EM algorithm for learning the Bayesian hierarchical linear model? 3.",
                "Can the new algorithm quickly learn many user models?",
                "To answer the first question, we compared the Bayesian hierarchical models with commonly used Norm-2 regularized linear regression models.",
                "In fact, the commonly used approach is equivalent to the model learned at the end of the first EM iteration.",
                "To answer the second question, we compared the proposed new algorithm with the standard EM algorithm to see whether the new learning algorithm is better.",
                "To answer the third question, we tested the efficiency of the new algorithm on the entire Netflix data set where about half a million user models need to be learned together.",
                "For the MovieLens and Netflix data sets, algorithm effectiveness was measured by mean square error, while on the Reuters data set classification error was used because it was more informative.",
                "We first evaluated the performance on each individual user, and then estimated the macro average over all users.",
                "Statistical tests (t-tests) were carried out to see whether the results are significant.",
                "For the experiments on the MovieLens and Netflix data sets, we used a random sample of 90% of each user for training, and the rest for testing.",
                "On Reuters data set, because there are too many relevant documents for each topic in the corpus, we used a random sample of 10% of each topic for training, and 10% of the remaining documents for testing.",
                "For all runs, we set (a, b, c, Σ ) = (0.1, 10, 0.1, 1) manually. 6.",
                "EXPERIMENTAL RESULTS Figure 2, Figure 3, and Figure 4 show that on all data sets, the Bayesian hierarchical modeling approach has a statistical significant improvement over the regularized linear regression model, which is equivalent to the Bayesian hierarchical models learned at the first iteration.",
                "Further analysis shows a negative correlation between the number of training data for a user and the improvement the system gets.",
                "This suggests that the borrowing information from other users has more significant improvements for users with less training data, which is as expected.",
                "However, the strength of the correlation differs over data sets, and the amount of training data is not the only characteristics that will influence the final performance.",
                "Figure 2 and Figure 3 show that the proposed new algorithm works better than the standard EM algorithm on the Netflix and MovieLens data sets.",
                "This is not surprising since the number of related feature-users pairs is much smaller than the number of unrelated feature-user pairs on these two data sets, and thus the proposed new algorithm is expected to work better.",
                "Figure 4 shows that the two algorithms work similarly on the Reuters-E data set.",
                "The accuracy of the new algorithm is similar to that of the standard EM algorithm at each iteration.",
                "The general patterns are very similar on other Reuters subsets.",
                "Further analysis shows that only 58% of the user-feature pairs are unrelated on this data set.",
                "Since the number of unrelated user-feature pairs is not extremely large, the sparseness is not a serious problem on the Reuters data set.",
                "Thus the two learning algorithms perform similarly.",
                "The results suggest that only on a corpus where the number of unrelated user-feature pairs is much larger than the number of related pairs, such as on the Netflix data set, the proposed technique will get a significant improvement over standard EM.",
                "However, the experiments also show that when the assumption does not hold, the new algorithm does not hurt performance.",
                "Although the proposed technique is faster than standard Figure 2: Performance on a Netflix subset with 1,000 users.",
                "The new algorithm is statistical significantly better than EM algorithm at iterations 2 - 10.",
                "Norm-2 regularized linear models are equivalent to the Bayesian hierarchical models learned at the first iteration, and are statistical significantly worse than the Bayesian hierarchical models. 0 2 4 6 8 10 1 1.05 1.1 1.15 1.2 1.25 1.3 1.35 1.4 Iterations MeanSquareError New Algorithm Traditional EM 1 2 3 4 5 6 7 8 9 10 0.33 0.34 0.35 0.36 0.37 0.38 0.39 Iterations ClassificationError New Algorithm Traditional EM Figure 3: Performance on a MovieLens subset with 1,000 users.",
                "The new algorithm is statistical significantly better than EM algorithm at iteration 2 to 17 (evaluated with mean square error). 1 6 11 16 21 0.5 1 1.5 2 2.5 3 3.5 Iterations MeanSquareError New Algorithm Traditional EM 1 6 11 16 21 0.35 0.4 0.45 0.5 0.55 0.6 0.65 Iterations ClassificationError New Algorithm Traditional EM Figure 4: Performance on a Reuters-E subset with 26 profiles.",
                "Performances on Reuters-C, Reuters-M, Reuters-G are similar. 1 2 3 4 5 0.011 0.0115 0.012 0.0125 0.013 0.0135 0.014 Iterations MeanSquareError New Algorithm Traditional EM 1 2 3 4 5 0.0102 0.0104 0.0106 0.0108 0.011 0.0112 0.0114 Iterations ClassificationError New Algorithm Traditional EM EM, can it really learn millions of user models quickly?",
                "Our results show that the modified EM algorithm converges quickly, and 2 - 3 modified EM iterations would result in a reliable estimation.",
                "We evaluated the algorithm on the whole Netflix data set (480,189 users, 159,836 features, and 100 million ratings) running on a single CPU PC (2GB memory, P4 3GHz).",
                "The system finished one modified EM iteration in about 4 hours.",
                "This demonstrates that the proposed technique can efficiently handle large-scale system like Netflix. 7.",
                "CONCLUSION <br>content-based</br> user profile learning is an important problem and is the key to providing personal recommendations to a user, especially for recommending new items with a small number of ratings.",
                "The Bayesian hierarchical modeling approach is becoming an important user profile learning approach due to its theoretically justified ability to help one user through information transfer from the other users by way of hyperpriors.",
                "This paper examined the weakness of the popular EM based learning approach for Bayesian hierarchical linear models and proposed a better learning technique called Modified EM.",
                "We showed that the new technique is theoretically more computationally efficient than the standard EM algorithm.",
                "Evaluation on the MovieLens and Netflix data sets demonstrated the effectiveness of the new technique when the data is sparse, by which we mean the ratio of related user-feature pairs to unrelated pairs is small.",
                "Evaluation on the Reuters data set showed that the new technique performed similar to the standard EM algorithm when the sparseness condition does not hold.",
                "In general, it is better to use the new algorithm since it is as simple as standard EM, the performance is either better or similar to EM, and the computation complexity is lower at each iteration.",
                "It is worth mentioning that even if the original problem space is not sparse, sparseness can be created artificially when a recommendation system uses user-specific feature selection techniques to reduce the noise and user model complexity.",
                "The proposed technique can also be adapted to improve the learning in such a scenario.",
                "We also demonstrated that the proposed technique can learn half a million user profiles from 100 million ratings in a few hours with a single CPU.",
                "The research is important because scalability is a major concern for researchers when using the Bayesian hierarchical linear modeling approach to build a practical large scale system, even though the literature have demonstrated the effectiveness of the models in many applications.",
                "Our work is one major step on the road to make Bayesian hierarchical linear models more practical.",
                "The proposed new technique can be easily adapted to run on a cluster of machines, and thus further speed up the learning process to handle a larger scale system with hundreds of millions of users.",
                "The research has much potential to benefit people using EM algorithm on many other IR problems as well as machine learning problems.",
                "EM algorithm is a commonly used machine learning technique.",
                "It is used to find model parameters in many IR problems where the training data is very sparse.",
                "Although we are focusing on the Bayesian hierarchical linear models for recommendation and filtering, the new idea of using analytical solution instead of numerical solution for unrelated user-feature pairs at the M step could be adapted to many other problems. 8.",
                "ACKNOWLEDGMENTS We thank Wei Xu, David Lewis and anonymous reviewers for valuable feedback on the work described in this paper.",
                "Part of the work was supported by Yahoo, Google, the Petascale Data Storage Institute and the Institute for Scalable Scientific Data Management.",
                "Any opinions, findings, conclusions, or recommendations expressed in this material are those of the authors, and do not necessarily reflect those of the sponsors. 9.",
                "REFERENCES [1] C. Basu, H. Hirsh, and W. Cohen.",
                "Recommendation as classification: Using social and <br>content-based</br> information in recommendation.",
                "In Proceedings of the Fifteenth National Conference on Artificial Intelligence, 1998. [2] J. S. Breese, D. Heckerman, and C. Kadie.",
                "Empirical analysis of predictive algorithms for collaborative filtering.",
                "Technical report, Microsoft Research, One Microsoft Way, Redmond, WA 98052, 1998. [3] J. Callan.",
                "Document filtering with inference networks.",
                "In Proceedings of the Nineteenth Annual International ACM SIGIR Conference on Research and Development in Information Retrieval, pages 262-269, 1996. [4] N. Cancedda, N. Cesa-Bianchi, A. Conconi, C. Gentile, C. Goutte, T. Graepel, Y. Li, J. M. Renders, J. S. Taylor, and A. Vinokourov.",
                "Kernel method for document filtering.",
                "In The Eleventh Text REtrieval Conference (TREC11).",
                "National Institute of Standards and Technology, special publication 500-249, 2003. [5] C. Chelba and A. Acero.",
                "Adaptation of maximum entropy capitalizer: Little data can help a lot.",
                "In D. Lin and D. Wu, editors, Proceedings of EMNLP 2004, pages 285-292, Barcelona, Spain, July 2004.",
                "Association for Computational Linguistics. [6] B. Croft and J. Lafferty, editors.",
                "Language Modeling for Information Retrieval.",
                "Kluwer, 2002. [7] A. Dayanik, D. D. Lewis, D. Madigan, V. Menkov, and A. Genkin.",
                "Constructing informative prior distributions from domain knowledge in text classification.",
                "In SIGIR 06: Proceedings of the 29th annual international ACM SIGIR conference on Research and development in information retrieval, pages 493-500, New York, NY, USA, 2006.",
                "ACM Press. [8] J. Delgado and N. Ishii.",
                "Memory-based weightedmajority prediction for recommender systems.",
                "In ACM SIGIR99 Workshop on Recommender Systems, 1999. [9] GroupLens.",
                "Movielens. http://www.grouplens.org/taxonomy/term/14, 2006. [10] D. Heckerman.",
                "A tutorial on learning with bayesian networks.",
                "In M. Jordan, editor, Learning in Graphical Models.",
                "Kluwer Academic, 1998. [11] J. L. Herlocker, J.",
                "A. Konstan, A. Borchers, and J. Riedl.",
                "An algorithmic framework for performing collaborative filtering.",
                "In SIGIR 99: Proceedings of the 22nd annual international ACM SIGIR conference on Research and development in information retrieval, pages 230-237, New York, NY, USA, 1999.",
                "ACM Press. [12] T. Hofmann and J. Puzicha.",
                "Latent class models for collaborative filtering.",
                "In IJCAI 99: Proceedings of the Sixteenth International Joint Conference on Artificial Intelligence, pages 688-693, San Francisco, CA, USA, 1999.",
                "Morgan Kaufmann Publishers Inc. [13] I. M. D. (IMDB).",
                "Internet movie database. http://www.imdb.com/interfaces/, 2006. [14] R. Jin, J. Y. Chai, and L. Si.",
                "An automatic weighting scheme for collaborative filtering.",
                "In SIGIR 04: Proceedings of the 27th annual international ACM SIGIR conference on Research and development in information retrieval, pages 337-344, New York, NY, USA, 2004.",
                "ACM Press. [15] J.",
                "A. Konstan, B. N. Miller, D. Maltz, J. L. Herlocker, L. R. Gordon, and J. Riedl.",
                "GroupLens: Applying collaborative filtering to Usenet news.",
                "Communications of the ACM, 40(3):77-87, 1997. [16] D. Lewis.",
                "Applying support vector machines to the TREC-2001 batch filtering and routing tasks.",
                "In Proceedings of the Eleventh Text REtrieval Conference (TREC-11), 2002. [17] B. Liu, X. Li, W. S. Lee, , and P. Yu.",
                "Text classification by labeling words.",
                "In Proceedings of The Nineteenth National Conference on Artificial Intelligence (AAAI-2004), July 25-29, 2004. [18] P. Melville, R. J. Mooney, and R. Nagarajan.",
                "Content-boosted collaborative filtering for improved recommendations.",
                "In Proceedings of the Eighteenth National Conference on Artificial Intelligence (AAAI-2002), Edmonton, Canada, 2002. [19] Netflix.",
                "Netflix prize. http://www.netflixprize.com (visited on Nov. 30, 2006), 2006. [20] S. Robertson and K. Sparck-Jones.",
                "Relevance weighting of search terms.",
                "In Journal of the American Society for Information Science, volume 27, pages 129-146, 1976. [21] J. Wang, A. P. de Vries, and M. J. T. Reinders.",
                "Unifying user-based and item-based collaborative filtering approaches by similarity fusion.",
                "In SIGIR 06: Proceedings of the 29th annual international ACM SIGIR conference on Research and development in information retrieval, pages 501-508, New York, NY, USA, 2006.",
                "ACM Press. [22] X. Wu and R. K. Srihari.",
                "Incorporating prior knowledge with weighted margin support vector machines.",
                "In Proc.",
                "ACM Knowledge Discovery Data Mining Conf. (ACM SIGKDD 2004), Aug. 2004. [23] Y. Yang, S. Yoo, J. Zhang, and B. Kisiel.",
                "Robustness of adaptive filtering methods in a cross-benchmark evaluation.",
                "In Proceedings of the 28th Annual International ACM SIGIR Conference on Research and Development in Information Retrieval, 2005. [24] K. Yu, V. Tresp, and A. Schwaighofer.",
                "Learning gaussian processes from multiple tasks.",
                "In ICML 05: Proceedings of the 22nd international conference on Machine learning, pages 1012-1019, New York, NY, USA, 2005.",
                "ACM Press. [25] K. Yu, V. Tresp, and S. Yu.",
                "A nonparametric hierarchical bayesian framework for information filtering.",
                "In SIGIR 04: Proceedings of the 27th annual international ACM SIGIR conference on Research and development in information retrieval, pages 353-360.",
                "ACM Press, 2004. [26] X. Zhu.",
                "Semi-supervised learning literature survey.",
                "Technical report, University of Wisconsin - Madison, December 9, 2006. [27] P. Zigoris and Y. Zhang.",
                "Bayesian adaptive user profiling with explicit & implicit feedback.",
                "In Conference on Information and Knowledge Mangement 2006, 2006."
            ],
            "original_annotated_samples": [],
            "translated_annotated_samples": [
                "Modelado de usuarios jerárquicos bayesianos eficientes para sistemas de recomendación Yi Zhang, Jonathan Koren School of Engineering University of California Santa Cruz Santa Cruz, CA, EE. UU. {Yiz, jonathan}@soe.ucsc.edu Resumen un sistema de recomendación \"basado en el contenido\" aprende el usuario de los usuariosPerfiles específicos de los comentarios de los usuarios para que pueda entregar información adaptada al interés de cada usuarios individuales.",
                "Un tema de personalización importante estudiado en la comunidad de recuperación de información son los sistemas de recomendación personal \"basados en el contenido \"1.",
                "La organización de las partes restantes de este documento es la siguiente: la Sección 3 describe el marco de modelado de regresión lineal jerárquica bayesiana utilizada para recomendaciones \"basadas en contenido\".",
                "El filtrado \"basado en el contenido\" estudia el escenario en el que un sistema de recomendación monitorea una secuencia de documentos y empuja documentos que coinciden con un perfil de usuario con el usuario correspondiente.",
                "Este documento contribuye a la investigación de recomendación \"basada en el contenido\" al mejorar la eficiencia y la efectividad de los modelos lineales jerárquicos bayesianos, que tienen una sólida base teórica y un buen rendimiento empírico en las tareas de recomendación [27] [25].",
                "Este documento no tiene la intención de comparar el filtrado \"basado en el contenido\" con filtrado colaborativo o reclamar cuál es mejor.",
                "Creemos que cada uno complementa al otro, y que el filtrado \"basado en el contenido\" es extremadamente útil para manejar nuevos documentos/elementos con poco o ningún comentario de los usuarios.",
                "Los modelos lineales jerárquicos bayesianos generalizados, uno de los modelos jerárquicos bayesianos más simples, se usan comúnmente y ha logrado un buen rendimiento en las tareas de filtrado colaborativo [25] y filtrado adaptativo \"27]\" 27].",
                "Conclusión El aprendizaje de perfil de usuario \"basado en contenido\" es un problema importante y es la clave para proporcionar recomendaciones personales a un usuario, especialmente para recomendar nuevos elementos con un pequeño número de calificaciones.",
                "Recomendación como clasificación: utilizando información social y \"basada en el contenido\" en recomendación."
            ],
            "translated_text": "",
            "candidates": [
                "basado en contenido",
                "basado en el contenido",
                "basado en contenido",
                "basados en el contenido ",
                "basado en contenido",
                "basadas en contenido",
                "basado en contenido",
                "basado en el contenido",
                "basado en contenido",
                "basada en el contenido",
                "basado en contenido",
                "basado en el contenido",
                "basado en contenido",
                "basado en el contenido",
                "basado en contenido",
                "27]",
                "basado en contenido",
                "basado en contenido",
                "basado en contenido",
                "basada en el contenido"
            ],
            "error": []
        },
        "recommendation system": {
            "translated_key": "sistema de recomendación",
            "is_in_text": true,
            "original_annotated_sentences": [
                "Efficient Bayesian Hierarchical User Modeling for Recommendation Systems Yi Zhang, Jonathan Koren School of Engineering University of California Santa Cruz Santa Cruz, CA, USA {yiz, jonathan}@soe.ucsc.edu ABSTRACT A content-based personalized <br>recommendation system</br> learns user specific profiles from user feedback so that it can deliver information tailored to each individual users interest.",
                "A system serving millions of users can learn a better user profile for a new user, or a user with little feedback, by borrowing information from other users through the use of a Bayesian hierarchical model.",
                "Learning the model parameters to optimize the joint data likelihood from millions of users is very computationally expensive.",
                "The commonly used EM algorithm converges very slowly due to the sparseness of the data in IR applications.",
                "This paper proposes a new fast learning technique to learn a large number of individual user profiles.",
                "The efficacy and efficiency of the proposed algorithm are justified by theory and demonstrated on actual user data from Netflix and MovieLens.",
                "Categories and Subject Descriptors: B.3.3 [Information Search and Retrieval]: Information filtering General Terms: Algorithms 1.",
                "INTRODUCTION Personalization is the future of the Web, and it has achieved great success in industrial applications.",
                "For example, online stores, such as Amazon and Netflix, provide customized recommendations for additional products or services based on a users history.",
                "Recent offerings such as My MSN, My Yahoo!, My Google, and Google News have attracted much attention due to their potential ability to infer a users interests from his/her history.",
                "One major personalization topic studied in the information retrieval community is content-based personal recommendation systems1 .",
                "These systems learn user-specific profiles from user feedback so that they can recommend information tailored to each individual users interest without requiring the user to make an explicit query.",
                "Learning the user profiles is the core problem for these systems.",
                "A user profile is usually a classifier that can identify whether a document is relevant to the user or not, or a regression model that tells how relevant a document is to the user.",
                "One major challenge of building a recommendation or personalization system is that the profile learned for a particular user is usually of low quality when the amount of data from that particular user is small.",
                "This is known as the cold start problem.",
                "This means that any new user must endure poor initial performance until sufficient feedback from that user is provided to learn a reliable user profile.",
                "There has been much research on improving classification accuracy when the amount of labeled training data is small.",
                "The semi-supervised learning approach combines unlabeled and labeled data together to achieve this goal [26].",
                "Another approach is using domain knowledge.",
                "Researchers have modified different learning algorithms, such as Na¨ıveBayes [17], logistic regression [7], and SVMs [22], to integrate domain knowledge into a text classifier.",
                "The third approach is borrowing training data from other resources [5][7].",
                "The effectiveness of these different approaches is mixed, due to how well the underlying model assumption fits the data.",
                "One well-received approach to improve <br>recommendation system</br> performance for a particular user is borrowing information from other users through a Bayesian hierarchical modeling approach.",
                "Several researchers have demonstrated that this approach effectively trades off between shared and user-specific information, thus alleviating poor initial performance for each user[27][25].",
                "In order to learn a Bayesian hierarchical model, the system usually tries to find the most likely model parameters for the given data.",
                "A mature <br>recommendation system</br> usually works for millions of users.",
                "It is well known that learning the optimal parameters of a Bayesian hierarchical model is computationally expensive when there are thousands or millions of users.",
                "The EM algorithm is a commonly used technique for parameter learning due to its simplicity and convergence guarantee.",
                "However, a content based <br>recommendation system</br> often handles documents in a very high dimensional space, in which each document is represented by a very sparse vector.",
                "With careful analysis of the EM algorithm in this scenario (Section 4), we find that the EM tering, or item-based collaborative filtering.",
                "In this paper, the words filtering and recommendation are used interchangeably. algorithm converges very slowly due to the sparseness of the input variables.",
                "We also find that updating the model parameter at each EM iteration is also expensive with computational complexity of O(MK), where M is the number of users and K is the number of dimensions.",
                "This paper modifies the standard EM algorithm to create an improved learning algorithm, which we call the Modified EM algorithm.",
                "The basic idea is that instead of calculating the numerical solution for all the user profile parameters, we derive the analytical solution of the parameters for some feature dimensions, and at the M step use the analytical solution instead of the numerical solution estimated at E step for those parameters.",
                "This greatly reduces the computation at a single EM iteration, and also has the benefit of increasing the convergence speed of the learning algorithm.",
                "The proposed technique is not only well supported by theory, but also by experimental results.",
                "The organization of the remaining parts of this paper is as follows: Section 3 describes the Bayesian hierarchical linear regression modeling framework used for content-based recommendations.",
                "Section 4 describes how to learn the model parameters using the standard EM algorithm, along with using the new technique proposed in this paper.",
                "The experimental setting and results used to validate the proposed learning technique are reported in Sections 5 and 6.",
                "Section 7 summarizes and offers concluding remarks. 2.",
                "RELATED WORK Providing personalized recommendations to users has been identified as a very important problem in the IR community since the 1970s.",
                "The approaches that have been used to solve this problem can be roughly classified into two major categories: content based filtering versus collaborative filtering.",
                "Content-based filtering studies the scenario where a <br>recommendation system</br> monitors a document stream and pushes documents that match a user profile to the corresponding user.",
                "The user may read the delivered documents and provide explicit relevance feedback, which the filtering system then uses to update the users profile using relevance feedback retrieval models (e.g.",
                "Boolean models, vector space models, traditional probabilistic models [20] , inference networks [3] and language models [6]) or machine learning algorithms (e.g.",
                "Support Vector Machines (SVM), K nearest neighbors (K-NN) clustering, neural networks, logistic regression, or Winnow [16] [4] [23]).",
                "Collaborative filtering goes beyond merely using document content to recommend items to a user by leveraging information from other users with similar tastes and preferences in the past.",
                "Memorybased heuristics and model based approaches have been used in collaborative filtering task [15] [8] [2] [14] [12] [11].",
                "This paper contributes to the content-based recommendation research by improving the efficiency and effectiveness of Bayesian hierarchical linear models, which have a strong theoretical basis and good empirical performance on recommendation tasks[27][25].",
                "This paper does not intend to compare content-based filtering with collaborative filtering or claim which one is a better.",
                "We think each complements the other, and that content-based filtering is extremely useful for handling new documents/items with little or no user feedback.",
                "Similar to some other researchers[18][1][21], we found that a <br>recommendation system</br> will be more effective when both techniques are combined.",
                "However, this is beyond the scope of this paper and thus not discussed here. 3.",
                "BAYESIAN HIERARCHICAL LINEAR REGRESSION Assume there are M users in the system.",
                "The task of the system is to recommend documents that are relevant to each user.",
                "For each user, the system learns a user model from the users history.",
                "In the rest of this paper, we will use the following notations to represent the variables in the system. m = 1, 2, ..., M: The index for each individual user.",
                "M is the total number of users. wm: The user model parameter associated with user m. wm is a K dimensional vector. j = 1, 2, ..., Jm: The index for a set of data for user m. Jm is the number of training data for user m. Dm = {(xm,j, ym,j)}: A set of data associated with user m. xm,j is a K dimensional vector that represents the mth users jth training document.2 ym,j is a scalar that represents the label of document xm,j. k = 1, 2, ..., K: The dimensional index of input variable x.",
                "The Bayesian hierarchical modeling approach has been widely used in real-world information retrieval applications.",
                "Generalized Bayesian hierarchical linear models, one of the simplest Bayesian hierarchical models, are commonly used and have achieved good performance on collaborative filtering [25] and content-based adaptive filtering [27] tasks.",
                "Figure 1 shows the graphical representation of a Bayesian hierarchical model.",
                "In this graph, each user model is represented by a random vector wm.",
                "We assume a user model is sampled randomly from a prior distribution P(w|Φ).",
                "The system can predict the user label y of a document x given an estimation of wm (or wms distribution) using a function y = f(x, w).",
                "The model is called generalized Bayesian hierarchical linear model when y = f(wT x) is any generalized linear model such as logistic regression, SVM, and linear regression.",
                "To reliably estimate the user model wm, the system can borrow information from other users through the prior Φ = (µ, Σ).",
                "Now we look at one commonly used model where y = wT x + , where ∼ N(0, σ2 ) is a random noise [25][27].",
                "Assume that each user model wm is an independent draw from a population distribution P(w|Φ), which is governed by some unknown hyperparameter Φ.",
                "Let the prior distribution of user model w be a Gaussian distribution with parameter Φ = (µ, Σ), which is the commonly used prior for linear models. µ = (µ1, µ2, ..., µK ) is a K dimensional vector that represents the mean of the Gaussian distribution, and Σ is the covariance matrix of the Gaussian.",
                "Usually, a Normal distribution N(0, aI) and an Inverse Wishart distribution P(Σ) ∝ |Σ|− 1 2 b exp(−1 2 ctr(Σ−1 )) are used as hyperprior to model the prior distribution of µ and Σ respectively.",
                "I is the K dimensional identity matrix, and a, b, and c are real numbers.",
                "With these settings, we have the following model for the system: 1. µ and Σ are sampled from N(0, aI) and IWν (aI), respectively. 2 The first dimension of x is a dummy variable that always equals to 1.",
                "Figure 1: Illustration of dependencies of variables in the hierarchical model.",
                "The rating, y, for a document, x, is conditioned on the document and the user model, wm, associated with the user m. Users share information about their models through the prior, Φ = (µ, Σ). 2.",
                "For each user m, wm is sampled randomly from a Normal distribution: wm ∼ N(µ, Σ2 ) 3.",
                "For each item xm,j, ym,j is sampled randomly from a Normal distribution: ym,j ∼ N(wT mxm,j, σ2 ).",
                "Let θ = (Φ, w1, w2, ..., wM ) represent the parameters of this system that needs to be estimated.",
                "The joint likelihood for all the variables in the probabilistic model, which includes the data and the parameters, is: P(D, θ) = P(Φ) m P(wm|Φ) j P(ym,j|xm,j, wm) (1) For simplicity, we assume a, b, c, and σ are provided to the system. 4.",
                "MODEL PARAMETER LEARNING If the prior Φ is known, finding the optimal wm is straightforward: it is a simple linear regression.",
                "Therefore, we will focus on estimating Φ.",
                "The maximum a priori solution of Φ is given by ΦMAP = arg max Φ P(Φ|D) (2) = arg max Φ P(Φ, D) P(D) (3) = arg max Φ P(D|Φ)P(Φ) (4) = arg max Φ w P(D|w, Φ)P(w|Φ)P(Φ)dw (5) Finding the optimal solution for the above problem is challenging, since we need to integrate over all w = (w1, w2, ..., wM ), which are unobserved hidden variables. 4.1 EM Algorithm for Bayesian Hierarchical Linear Models In Equation 5, Φ is the parameter needs to be estimated, and the result depends on unobserved latent variables w. This kind of optimization problem is usually solved by the EM algorithm.",
                "Applying EM to the above problem, the set of user models w are the unobservable hidden variables and we have: Q = w P(w|µ, Σ2 , Dm) log P(µ, Σ2 , w, D)dw Based on the derivation of the EM formulas presented in [24], we have the following Expectation-Maximization steps for finding the optimal hyperparameters.",
                "For space considerations, we omit the derivation in this paper since it is not the focus of our work.",
                "E step: For each user m, estimate the user model distribution P(wm|Dm, Φ) = N(wm; ¯wm, Σ2 m) based on the current estimation of the prior Φ = (µ, Σ2 ). ¯wm = ((Σ2 )−1 + Sxx,m σ2 )−1 ( Sxy,m σ2 + (Σ2 )−1 µ) (6) Σ2 m = ((Σ2 )−1 + Sxx,m σ2 )−1 (7) where Sxx,m = j xm,jxT m,j Sxy,m = j xm,jym,j M step: Optimize the prior Φ = (µ, Σ2 ) based on the estimation from the last E step. µ = 1 M m ¯wm (8) Σ2 = 1 M m Σ2 m + ( ¯wm − µ)( ¯wm − µ)T (9) Many machine learning driven IR systems use a point estimate of the parameters at different stages in the system.",
                "However, we are estimating the posterior distribution of the variables at the E step.",
                "This avoids overfitting wm to a particular users data, which may be small and noisy.",
                "A detailed discussion about this subject appears in [10]. 4.2 New Algorithm: Modified EM Although the EM algorithm is widely studied and used in machine learning applications, using the above EM process to solve Bayesian hierarchical linear models in large-scale information retrieval systems is still too computationally expensive.",
                "In this section, we describe why the learning rate of the EM algorithm is slow in our application and introduce a new technique to make the learning of the Bayesian hierarchical linear model scalable.",
                "The derivation of the new learning algorithm will be based on the EM algorithm described in the previous section.",
                "First, the covariance matrices Σ2 , Σ2 m are usually too large to be computationally feasible.",
                "For simplicity, and as a common practice in IR, we do not model the correlation between features.",
                "Thus we approximate these matrices with K dimensional diagonal matrices.",
                "In the rest of the paper, we use these symbols to represent their diagonal approximations: Σ2 =     σ2 1 0 .. 0 0 σ2 2 .. 0 .. .. .. .. 0 0 .. σ2 K     Σ2 m =     σ2 m,1 0 .. 0 0 σ2 m,2 .. 0 .. .. .. .. 0 0 .. σ2 m,K     Secondly, and most importantly, the input space is very sparse and there are many dimensions that are not related to a particular user in a real IR application.",
                "For example, let us consider a movie <br>recommendation system</br>, with the input variable x representing a particular movie.",
                "For the jth movie that the user m has seen, let xm,j,k = 1 if the director of the movie is Jean-Pierre Jeunet (indexed by k).",
                "Here we assume that whether or not that this director directed a specific movie is represented by the kth dimension.",
                "If the user m has never seen a movie directed by Jean-Pierre Jeunet, then the corresponding dimension is always zero (xm,j,k = 0 for all j) .",
                "One major drawback of the EM algorithm is that the importance of a feature, µk, may be greatly dominated by users who have never encountered this feature (i.e. j xm,j,k = 0) at the M step (Equation 8).",
                "Assume that 100 out of 1 million users have viewed the movie directed by Jean-Pierre Jeunet, and that the viewers have rated all of his movies as excellent.",
                "Intuitively, he is a good director and the weight for him (µk) should be high.",
                "Before the EM iteration, the initial value of µ is usually set to 0.",
                "Since the other 999,900 users have not seen this movie, their corresponding weights (w1,k, w2,k, ..., wm,k..., w999900,k) for that director would be very small initially.",
                "Thus the corresponding weight of the director in the prior µk at the first M step would be very low , and the variance σm,k will be large (Equations 8 and 7).",
                "It is undesirable that users who have never seen any movie produced by the director influence the importance of the director so much.",
                "This makes the convergence of the standard EM algorithm very slow.",
                "Now lets look at whether we can improve the learning speed of the algorithm.",
                "Without a loss of generality, let us assume that the kth dimension of the input variable x is not related to a particular user m. By which we mean, xm,j,k = 0 for all j = 1, ..., Jm.",
                "It is straightforward to prove that the kth row and kth column of Sxx,m are completely filled with zeros, and that the kth dimension of Sxy,m is zeroed as well.",
                "Thus the corresponding kth dimension of the user models mean, ¯wm, should be equal to that of the prior: ¯wm,k = µk, with the corresponding covariance of σm,k = σk.",
                "At the M step, the standard EM algorithm uses the numerical solution of the distribution P(wm|Dm, Φ) estimated at E step (Equation 8 and Equation 7).",
                "However, the numerical solutions are very unreliable for ¯wm,k and σm,k when the kth dimension is not related to the mth user.",
                "A better approach is using the analytical solutions ¯wm,k = µk, and σm,k = σk for the unrelated (m, k) pairs, along with the numerical solution estimated at E step for the other (m, k) pairs.",
                "Thus we get the following new EM-like algorithm: Modified E step: For each user m, estimate the user model distribution P(wm|Dm, Φ) = N(wm; ¯wm, Σ2 m) based on the current estimation of σ , µ, Σ2 . ¯wm = ((Σ2 )−1 + Sxx,m σ2 )−1 ( Sxy,m σ2 + (Σ2 )−1 µ)(10) σ2 m,k = ((σ2 k)−1 + sxx,m,k σ2 )−1 (11) where sxx,m,k = j x2 m,j,k and sxy,m,k = j xm,j,kym,j Modified M Step Optimize the prior Φ = (µ, Σ2 ) based on the estimation from the last E step for related userfeature pairs.",
                "The M step implicitly uses the analytical solution for unrelated user-feature pairs. µk = 1 Mk m:related ¯wm,k (12) σ2 k = 1 Mk m:related σ2 m,k +( ¯wm,k − µk)( ¯wm,k − µk)T (13) where Mk is the number of users that are related to feature k We only estimate the diagonal of Σ2 m and Σ since we are using the diagonal approximation of the covariance matrices.",
                "To estimate ¯wm, we only need to calculate the numerical solutions for dimensions that are related to user m. To estimate σ2 k and µk, we only sum over users that are related to the kth feature.",
                "There are two major benefits of the new algorithm.",
                "First, because only the related (m, k) pairs are needed at the modified M step, the computational complexity in a single EM iteration is much smaller when the data is sparse, and many of (m, k) pairs are unrelated.",
                "Second, the parameters estimated at the modified M step (Equations 12 - 13) are more accurate than the standard M step described in Section 4.1 because the exact analytical solutions ¯wm,k = µk and σm,k = σk for the unrelated (m, k) pairs were used in the new algorithm instead of an approximate solution as in the standard algorithm. 5.",
                "EXPERIMENTAL METHODOLOGY 5.1 Evaluation Data Set To evaluate the proposed technique, we used the following three major data sets (Table 1): MovieLens Data: This data set was created by combining the relevance judgments from the MovieLens[9] data set with documents from the Internet Movie Database (IMDB).",
                "MovieLens allows users to rank how much he/she enjoyed a specific movie on a scale from 1 to 5.",
                "This likeability rating was used as a measurement of how relevant the document representing the corresponding movie is to the user.",
                "We considered documents with likeability scores of 4 or 5 as relevant, and documents with a score of 1 to 3 as irrelevant to the user.",
                "MovieLens provided relevance judgments on 3,057 documents from 6,040 separate users.",
                "On average, each user rated 151 movies, of these 87 were judged to be relevant.",
                "The average score for a document was 3.58.",
                "Documents representing each movie were constructed from the portion of the IMDB database that is available for public download[13].",
                "Based on this database, we created one document per movie that contained the relevant information about it (e.g. directors, actors, etc.).",
                "Table 1: Data Set Statistics.",
                "On Reuters, the number of rating for a simulated user is the number of documents relevant to the corresponding topic.",
                "Data Users Docs Ratings per User MovieLens 6,040 3,057 151 Netflix-all 480,189 17,770 208 Netflix-1000 1000 17,770 127 Reuters-C 34 100,000 3949 Reuters-E 26 100,000 1632 Reuters-G 33 100,000 2222 Reuters-M 10 100,000 6529 Netflix Data: This data set was constructed by combining documents about movies crawled from the web with a set of actual movie rental customer relevance judgments from Netflix[19].",
                "Netflix publicly provides the relevance judgments of 480,189 anonymous customers.",
                "There are around 100 million rating on a scale of 1 to 5 for 17,770 documents.",
                "Similar to MovieLens, we considered documents with likeability scores of 4 or 5 as relevant.",
                "This number was reduced to 1000 customers through random sampling.",
                "The average customer on the reduced data set provided 127 judgments, with 70 being deemed relevant.",
                "The average score for documents is 3.55.",
                "Reuters Data: This is the Reuters Corpus, Volume 1.",
                "It covers 810,000 Reuters English language news stories from August 20, 1996 to August 19, 1997.",
                "Only the first 100,000 news were used in our experiments.",
                "The Reuters corpus comes with a topic hierarchy.",
                "Each document is assigned to one of several locations on the hierarchical tree.",
                "The first level of the tree contains four topics, denoted as C, E, M, and G. For the experiments in this paper, the tree was cut at level 1 to create four smaller trees, each of which corresponds to one smaller data set: Reuters-E Reuters-C, ReutersM and Reuters-G. For each small data set, we created several profiles, one profile for each node in a sub-tree, to simulate multiple users, each with a related, yet separate definition of relevance.",
                "All the user profiles on a sub-tree are supposed to share the same prior model distribution.",
                "Since this corpus explicitly indicates only the relevant documents for a topic(user), all other documents are considered irrelevant. 5.2 Evaluation We designed the experiments to answer the following three questions: 1.",
                "Do we need to take the effort to use a Bayesian approach and learn a prior from other users? 2.",
                "Does the new algorithm work better than the standard EM algorithm for learning the Bayesian hierarchical linear model? 3.",
                "Can the new algorithm quickly learn many user models?",
                "To answer the first question, we compared the Bayesian hierarchical models with commonly used Norm-2 regularized linear regression models.",
                "In fact, the commonly used approach is equivalent to the model learned at the end of the first EM iteration.",
                "To answer the second question, we compared the proposed new algorithm with the standard EM algorithm to see whether the new learning algorithm is better.",
                "To answer the third question, we tested the efficiency of the new algorithm on the entire Netflix data set where about half a million user models need to be learned together.",
                "For the MovieLens and Netflix data sets, algorithm effectiveness was measured by mean square error, while on the Reuters data set classification error was used because it was more informative.",
                "We first evaluated the performance on each individual user, and then estimated the macro average over all users.",
                "Statistical tests (t-tests) were carried out to see whether the results are significant.",
                "For the experiments on the MovieLens and Netflix data sets, we used a random sample of 90% of each user for training, and the rest for testing.",
                "On Reuters data set, because there are too many relevant documents for each topic in the corpus, we used a random sample of 10% of each topic for training, and 10% of the remaining documents for testing.",
                "For all runs, we set (a, b, c, Σ ) = (0.1, 10, 0.1, 1) manually. 6.",
                "EXPERIMENTAL RESULTS Figure 2, Figure 3, and Figure 4 show that on all data sets, the Bayesian hierarchical modeling approach has a statistical significant improvement over the regularized linear regression model, which is equivalent to the Bayesian hierarchical models learned at the first iteration.",
                "Further analysis shows a negative correlation between the number of training data for a user and the improvement the system gets.",
                "This suggests that the borrowing information from other users has more significant improvements for users with less training data, which is as expected.",
                "However, the strength of the correlation differs over data sets, and the amount of training data is not the only characteristics that will influence the final performance.",
                "Figure 2 and Figure 3 show that the proposed new algorithm works better than the standard EM algorithm on the Netflix and MovieLens data sets.",
                "This is not surprising since the number of related feature-users pairs is much smaller than the number of unrelated feature-user pairs on these two data sets, and thus the proposed new algorithm is expected to work better.",
                "Figure 4 shows that the two algorithms work similarly on the Reuters-E data set.",
                "The accuracy of the new algorithm is similar to that of the standard EM algorithm at each iteration.",
                "The general patterns are very similar on other Reuters subsets.",
                "Further analysis shows that only 58% of the user-feature pairs are unrelated on this data set.",
                "Since the number of unrelated user-feature pairs is not extremely large, the sparseness is not a serious problem on the Reuters data set.",
                "Thus the two learning algorithms perform similarly.",
                "The results suggest that only on a corpus where the number of unrelated user-feature pairs is much larger than the number of related pairs, such as on the Netflix data set, the proposed technique will get a significant improvement over standard EM.",
                "However, the experiments also show that when the assumption does not hold, the new algorithm does not hurt performance.",
                "Although the proposed technique is faster than standard Figure 2: Performance on a Netflix subset with 1,000 users.",
                "The new algorithm is statistical significantly better than EM algorithm at iterations 2 - 10.",
                "Norm-2 regularized linear models are equivalent to the Bayesian hierarchical models learned at the first iteration, and are statistical significantly worse than the Bayesian hierarchical models. 0 2 4 6 8 10 1 1.05 1.1 1.15 1.2 1.25 1.3 1.35 1.4 Iterations MeanSquareError New Algorithm Traditional EM 1 2 3 4 5 6 7 8 9 10 0.33 0.34 0.35 0.36 0.37 0.38 0.39 Iterations ClassificationError New Algorithm Traditional EM Figure 3: Performance on a MovieLens subset with 1,000 users.",
                "The new algorithm is statistical significantly better than EM algorithm at iteration 2 to 17 (evaluated with mean square error). 1 6 11 16 21 0.5 1 1.5 2 2.5 3 3.5 Iterations MeanSquareError New Algorithm Traditional EM 1 6 11 16 21 0.35 0.4 0.45 0.5 0.55 0.6 0.65 Iterations ClassificationError New Algorithm Traditional EM Figure 4: Performance on a Reuters-E subset with 26 profiles.",
                "Performances on Reuters-C, Reuters-M, Reuters-G are similar. 1 2 3 4 5 0.011 0.0115 0.012 0.0125 0.013 0.0135 0.014 Iterations MeanSquareError New Algorithm Traditional EM 1 2 3 4 5 0.0102 0.0104 0.0106 0.0108 0.011 0.0112 0.0114 Iterations ClassificationError New Algorithm Traditional EM EM, can it really learn millions of user models quickly?",
                "Our results show that the modified EM algorithm converges quickly, and 2 - 3 modified EM iterations would result in a reliable estimation.",
                "We evaluated the algorithm on the whole Netflix data set (480,189 users, 159,836 features, and 100 million ratings) running on a single CPU PC (2GB memory, P4 3GHz).",
                "The system finished one modified EM iteration in about 4 hours.",
                "This demonstrates that the proposed technique can efficiently handle large-scale system like Netflix. 7.",
                "CONCLUSION Content-based user profile learning is an important problem and is the key to providing personal recommendations to a user, especially for recommending new items with a small number of ratings.",
                "The Bayesian hierarchical modeling approach is becoming an important user profile learning approach due to its theoretically justified ability to help one user through information transfer from the other users by way of hyperpriors.",
                "This paper examined the weakness of the popular EM based learning approach for Bayesian hierarchical linear models and proposed a better learning technique called Modified EM.",
                "We showed that the new technique is theoretically more computationally efficient than the standard EM algorithm.",
                "Evaluation on the MovieLens and Netflix data sets demonstrated the effectiveness of the new technique when the data is sparse, by which we mean the ratio of related user-feature pairs to unrelated pairs is small.",
                "Evaluation on the Reuters data set showed that the new technique performed similar to the standard EM algorithm when the sparseness condition does not hold.",
                "In general, it is better to use the new algorithm since it is as simple as standard EM, the performance is either better or similar to EM, and the computation complexity is lower at each iteration.",
                "It is worth mentioning that even if the original problem space is not sparse, sparseness can be created artificially when a <br>recommendation system</br> uses user-specific feature selection techniques to reduce the noise and user model complexity.",
                "The proposed technique can also be adapted to improve the learning in such a scenario.",
                "We also demonstrated that the proposed technique can learn half a million user profiles from 100 million ratings in a few hours with a single CPU.",
                "The research is important because scalability is a major concern for researchers when using the Bayesian hierarchical linear modeling approach to build a practical large scale system, even though the literature have demonstrated the effectiveness of the models in many applications.",
                "Our work is one major step on the road to make Bayesian hierarchical linear models more practical.",
                "The proposed new technique can be easily adapted to run on a cluster of machines, and thus further speed up the learning process to handle a larger scale system with hundreds of millions of users.",
                "The research has much potential to benefit people using EM algorithm on many other IR problems as well as machine learning problems.",
                "EM algorithm is a commonly used machine learning technique.",
                "It is used to find model parameters in many IR problems where the training data is very sparse.",
                "Although we are focusing on the Bayesian hierarchical linear models for recommendation and filtering, the new idea of using analytical solution instead of numerical solution for unrelated user-feature pairs at the M step could be adapted to many other problems. 8.",
                "ACKNOWLEDGMENTS We thank Wei Xu, David Lewis and anonymous reviewers for valuable feedback on the work described in this paper.",
                "Part of the work was supported by Yahoo, Google, the Petascale Data Storage Institute and the Institute for Scalable Scientific Data Management.",
                "Any opinions, findings, conclusions, or recommendations expressed in this material are those of the authors, and do not necessarily reflect those of the sponsors. 9.",
                "REFERENCES [1] C. Basu, H. Hirsh, and W. Cohen.",
                "Recommendation as classification: Using social and content-based information in recommendation.",
                "In Proceedings of the Fifteenth National Conference on Artificial Intelligence, 1998. [2] J. S. Breese, D. Heckerman, and C. Kadie.",
                "Empirical analysis of predictive algorithms for collaborative filtering.",
                "Technical report, Microsoft Research, One Microsoft Way, Redmond, WA 98052, 1998. [3] J. Callan.",
                "Document filtering with inference networks.",
                "In Proceedings of the Nineteenth Annual International ACM SIGIR Conference on Research and Development in Information Retrieval, pages 262-269, 1996. [4] N. Cancedda, N. Cesa-Bianchi, A. Conconi, C. Gentile, C. Goutte, T. Graepel, Y. Li, J. M. Renders, J. S. Taylor, and A. Vinokourov.",
                "Kernel method for document filtering.",
                "In The Eleventh Text REtrieval Conference (TREC11).",
                "National Institute of Standards and Technology, special publication 500-249, 2003. [5] C. Chelba and A. Acero.",
                "Adaptation of maximum entropy capitalizer: Little data can help a lot.",
                "In D. Lin and D. Wu, editors, Proceedings of EMNLP 2004, pages 285-292, Barcelona, Spain, July 2004.",
                "Association for Computational Linguistics. [6] B. Croft and J. Lafferty, editors.",
                "Language Modeling for Information Retrieval.",
                "Kluwer, 2002. [7] A. Dayanik, D. D. Lewis, D. Madigan, V. Menkov, and A. Genkin.",
                "Constructing informative prior distributions from domain knowledge in text classification.",
                "In SIGIR 06: Proceedings of the 29th annual international ACM SIGIR conference on Research and development in information retrieval, pages 493-500, New York, NY, USA, 2006.",
                "ACM Press. [8] J. Delgado and N. Ishii.",
                "Memory-based weightedmajority prediction for recommender systems.",
                "In ACM SIGIR99 Workshop on Recommender Systems, 1999. [9] GroupLens.",
                "Movielens. http://www.grouplens.org/taxonomy/term/14, 2006. [10] D. Heckerman.",
                "A tutorial on learning with bayesian networks.",
                "In M. Jordan, editor, Learning in Graphical Models.",
                "Kluwer Academic, 1998. [11] J. L. Herlocker, J.",
                "A. Konstan, A. Borchers, and J. Riedl.",
                "An algorithmic framework for performing collaborative filtering.",
                "In SIGIR 99: Proceedings of the 22nd annual international ACM SIGIR conference on Research and development in information retrieval, pages 230-237, New York, NY, USA, 1999.",
                "ACM Press. [12] T. Hofmann and J. Puzicha.",
                "Latent class models for collaborative filtering.",
                "In IJCAI 99: Proceedings of the Sixteenth International Joint Conference on Artificial Intelligence, pages 688-693, San Francisco, CA, USA, 1999.",
                "Morgan Kaufmann Publishers Inc. [13] I. M. D. (IMDB).",
                "Internet movie database. http://www.imdb.com/interfaces/, 2006. [14] R. Jin, J. Y. Chai, and L. Si.",
                "An automatic weighting scheme for collaborative filtering.",
                "In SIGIR 04: Proceedings of the 27th annual international ACM SIGIR conference on Research and development in information retrieval, pages 337-344, New York, NY, USA, 2004.",
                "ACM Press. [15] J.",
                "A. Konstan, B. N. Miller, D. Maltz, J. L. Herlocker, L. R. Gordon, and J. Riedl.",
                "GroupLens: Applying collaborative filtering to Usenet news.",
                "Communications of the ACM, 40(3):77-87, 1997. [16] D. Lewis.",
                "Applying support vector machines to the TREC-2001 batch filtering and routing tasks.",
                "In Proceedings of the Eleventh Text REtrieval Conference (TREC-11), 2002. [17] B. Liu, X. Li, W. S. Lee, , and P. Yu.",
                "Text classification by labeling words.",
                "In Proceedings of The Nineteenth National Conference on Artificial Intelligence (AAAI-2004), July 25-29, 2004. [18] P. Melville, R. J. Mooney, and R. Nagarajan.",
                "Content-boosted collaborative filtering for improved recommendations.",
                "In Proceedings of the Eighteenth National Conference on Artificial Intelligence (AAAI-2002), Edmonton, Canada, 2002. [19] Netflix.",
                "Netflix prize. http://www.netflixprize.com (visited on Nov. 30, 2006), 2006. [20] S. Robertson and K. Sparck-Jones.",
                "Relevance weighting of search terms.",
                "In Journal of the American Society for Information Science, volume 27, pages 129-146, 1976. [21] J. Wang, A. P. de Vries, and M. J. T. Reinders.",
                "Unifying user-based and item-based collaborative filtering approaches by similarity fusion.",
                "In SIGIR 06: Proceedings of the 29th annual international ACM SIGIR conference on Research and development in information retrieval, pages 501-508, New York, NY, USA, 2006.",
                "ACM Press. [22] X. Wu and R. K. Srihari.",
                "Incorporating prior knowledge with weighted margin support vector machines.",
                "In Proc.",
                "ACM Knowledge Discovery Data Mining Conf. (ACM SIGKDD 2004), Aug. 2004. [23] Y. Yang, S. Yoo, J. Zhang, and B. Kisiel.",
                "Robustness of adaptive filtering methods in a cross-benchmark evaluation.",
                "In Proceedings of the 28th Annual International ACM SIGIR Conference on Research and Development in Information Retrieval, 2005. [24] K. Yu, V. Tresp, and A. Schwaighofer.",
                "Learning gaussian processes from multiple tasks.",
                "In ICML 05: Proceedings of the 22nd international conference on Machine learning, pages 1012-1019, New York, NY, USA, 2005.",
                "ACM Press. [25] K. Yu, V. Tresp, and S. Yu.",
                "A nonparametric hierarchical bayesian framework for information filtering.",
                "In SIGIR 04: Proceedings of the 27th annual international ACM SIGIR conference on Research and development in information retrieval, pages 353-360.",
                "ACM Press, 2004. [26] X. Zhu.",
                "Semi-supervised learning literature survey.",
                "Technical report, University of Wisconsin - Madison, December 9, 2006. [27] P. Zigoris and Y. Zhang.",
                "Bayesian adaptive user profiling with explicit & implicit feedback.",
                "In Conference on Information and Knowledge Mangement 2006, 2006."
            ],
            "original_annotated_samples": [],
            "translated_annotated_samples": [
                "Modelado de usuarios jerárquicos bayesianos eficientes para sistemas de recomendación Yi Zhang, Jonathan Koren School of Engineering University of California Santa Cruz Santa Cruz, CA, EE. UU. {Yiz, jonathan}@soe.ucsc.edu Resumen un \"sistema de recomendación\" basado en el contenido \"Aprende el usuario de los usuarios de usuariosPerfiles específicos de los comentarios de los usuarios para que pueda entregar información adaptada al interés de cada usuarios individuales.",
                "Un enfoque bien recibido para mejorar el rendimiento del \"sistema de recomendación\" para un usuario en particular es tomar prestada información de otros usuarios a través de un enfoque de modelado jerárquico bayesiano.",
                "Un \"sistema de recomendación\" maduro generalmente funciona para millones de usuarios.",
                "Sin embargo, un \"sistema de recomendación\" basado en el contenido a menudo maneja documentos en un espacio de muy alta dimensión, en el que cada documento está representado por un vector muy escaso.",
                "El filtrado basado en el contenido estudia el escenario en el que un \"sistema de recomendación\" monitorea una secuencia de documentos y empuja documentos que coinciden con un perfil de usuario con el usuario correspondiente.",
                "Similar a algunos otros investigadores [18] [1] [21], encontramos que un \"sistema de recomendación\" será más efectivo cuando ambas técnicas se combinen.",
                "Por ejemplo, consideremos una película \"Sistema de recomendación\", con la variable de entrada X que representa una película en particular.",
                "Vale la pena mencionar que incluso si el espacio de problemas original no es escaso, la escasez se puede crear artificialmente cuando un \"sistema de recomendación\" utiliza técnicas de selección de características específicas del usuario para reducir el ruido y la complejidad del modelo de usuario."
            ],
            "translated_text": "",
            "candidates": [
                "sistema de recomendación",
                "sistema de recomendación",
                "sistema de recomendación",
                "sistema de recomendación",
                "sistema de recomendación",
                "sistema de recomendación",
                "sistema de recomendación",
                "sistema de recomendación",
                "sistema de recomendación",
                "sistema de recomendación",
                "sistema de recomendación",
                "sistema de recomendación",
                "sistema de recomendación",
                "Sistema de recomendación",
                "sistema de recomendación",
                "sistema de recomendación"
            ],
            "error": []
        },
        "linear regression": {
            "translated_key": "regresión lineal",
            "is_in_text": true,
            "original_annotated_sentences": [
                "Efficient Bayesian Hierarchical User Modeling for Recommendation Systems Yi Zhang, Jonathan Koren School of Engineering University of California Santa Cruz Santa Cruz, CA, USA {yiz, jonathan}@soe.ucsc.edu ABSTRACT A content-based personalized recommendation system learns user specific profiles from user feedback so that it can deliver information tailored to each individual users interest.",
                "A system serving millions of users can learn a better user profile for a new user, or a user with little feedback, by borrowing information from other users through the use of a Bayesian hierarchical model.",
                "Learning the model parameters to optimize the joint data likelihood from millions of users is very computationally expensive.",
                "The commonly used EM algorithm converges very slowly due to the sparseness of the data in IR applications.",
                "This paper proposes a new fast learning technique to learn a large number of individual user profiles.",
                "The efficacy and efficiency of the proposed algorithm are justified by theory and demonstrated on actual user data from Netflix and MovieLens.",
                "Categories and Subject Descriptors: B.3.3 [Information Search and Retrieval]: Information filtering General Terms: Algorithms 1.",
                "INTRODUCTION Personalization is the future of the Web, and it has achieved great success in industrial applications.",
                "For example, online stores, such as Amazon and Netflix, provide customized recommendations for additional products or services based on a users history.",
                "Recent offerings such as My MSN, My Yahoo!, My Google, and Google News have attracted much attention due to their potential ability to infer a users interests from his/her history.",
                "One major personalization topic studied in the information retrieval community is content-based personal recommendation systems1 .",
                "These systems learn user-specific profiles from user feedback so that they can recommend information tailored to each individual users interest without requiring the user to make an explicit query.",
                "Learning the user profiles is the core problem for these systems.",
                "A user profile is usually a classifier that can identify whether a document is relevant to the user or not, or a regression model that tells how relevant a document is to the user.",
                "One major challenge of building a recommendation or personalization system is that the profile learned for a particular user is usually of low quality when the amount of data from that particular user is small.",
                "This is known as the cold start problem.",
                "This means that any new user must endure poor initial performance until sufficient feedback from that user is provided to learn a reliable user profile.",
                "There has been much research on improving classification accuracy when the amount of labeled training data is small.",
                "The semi-supervised learning approach combines unlabeled and labeled data together to achieve this goal [26].",
                "Another approach is using domain knowledge.",
                "Researchers have modified different learning algorithms, such as Na¨ıveBayes [17], logistic regression [7], and SVMs [22], to integrate domain knowledge into a text classifier.",
                "The third approach is borrowing training data from other resources [5][7].",
                "The effectiveness of these different approaches is mixed, due to how well the underlying model assumption fits the data.",
                "One well-received approach to improve recommendation system performance for a particular user is borrowing information from other users through a Bayesian hierarchical modeling approach.",
                "Several researchers have demonstrated that this approach effectively trades off between shared and user-specific information, thus alleviating poor initial performance for each user[27][25].",
                "In order to learn a Bayesian hierarchical model, the system usually tries to find the most likely model parameters for the given data.",
                "A mature recommendation system usually works for millions of users.",
                "It is well known that learning the optimal parameters of a Bayesian hierarchical model is computationally expensive when there are thousands or millions of users.",
                "The EM algorithm is a commonly used technique for parameter learning due to its simplicity and convergence guarantee.",
                "However, a content based recommendation system often handles documents in a very high dimensional space, in which each document is represented by a very sparse vector.",
                "With careful analysis of the EM algorithm in this scenario (Section 4), we find that the EM tering, or item-based collaborative filtering.",
                "In this paper, the words filtering and recommendation are used interchangeably. algorithm converges very slowly due to the sparseness of the input variables.",
                "We also find that updating the model parameter at each EM iteration is also expensive with computational complexity of O(MK), where M is the number of users and K is the number of dimensions.",
                "This paper modifies the standard EM algorithm to create an improved learning algorithm, which we call the Modified EM algorithm.",
                "The basic idea is that instead of calculating the numerical solution for all the user profile parameters, we derive the analytical solution of the parameters for some feature dimensions, and at the M step use the analytical solution instead of the numerical solution estimated at E step for those parameters.",
                "This greatly reduces the computation at a single EM iteration, and also has the benefit of increasing the convergence speed of the learning algorithm.",
                "The proposed technique is not only well supported by theory, but also by experimental results.",
                "The organization of the remaining parts of this paper is as follows: Section 3 describes the Bayesian hierarchical <br>linear regression</br> modeling framework used for content-based recommendations.",
                "Section 4 describes how to learn the model parameters using the standard EM algorithm, along with using the new technique proposed in this paper.",
                "The experimental setting and results used to validate the proposed learning technique are reported in Sections 5 and 6.",
                "Section 7 summarizes and offers concluding remarks. 2.",
                "RELATED WORK Providing personalized recommendations to users has been identified as a very important problem in the IR community since the 1970s.",
                "The approaches that have been used to solve this problem can be roughly classified into two major categories: content based filtering versus collaborative filtering.",
                "Content-based filtering studies the scenario where a recommendation system monitors a document stream and pushes documents that match a user profile to the corresponding user.",
                "The user may read the delivered documents and provide explicit relevance feedback, which the filtering system then uses to update the users profile using relevance feedback retrieval models (e.g.",
                "Boolean models, vector space models, traditional probabilistic models [20] , inference networks [3] and language models [6]) or machine learning algorithms (e.g.",
                "Support Vector Machines (SVM), K nearest neighbors (K-NN) clustering, neural networks, logistic regression, or Winnow [16] [4] [23]).",
                "Collaborative filtering goes beyond merely using document content to recommend items to a user by leveraging information from other users with similar tastes and preferences in the past.",
                "Memorybased heuristics and model based approaches have been used in collaborative filtering task [15] [8] [2] [14] [12] [11].",
                "This paper contributes to the content-based recommendation research by improving the efficiency and effectiveness of Bayesian hierarchical linear models, which have a strong theoretical basis and good empirical performance on recommendation tasks[27][25].",
                "This paper does not intend to compare content-based filtering with collaborative filtering or claim which one is a better.",
                "We think each complements the other, and that content-based filtering is extremely useful for handling new documents/items with little or no user feedback.",
                "Similar to some other researchers[18][1][21], we found that a recommendation system will be more effective when both techniques are combined.",
                "However, this is beyond the scope of this paper and thus not discussed here. 3.",
                "BAYESIAN HIERARCHICAL <br>linear regression</br> Assume there are M users in the system.",
                "The task of the system is to recommend documents that are relevant to each user.",
                "For each user, the system learns a user model from the users history.",
                "In the rest of this paper, we will use the following notations to represent the variables in the system. m = 1, 2, ..., M: The index for each individual user.",
                "M is the total number of users. wm: The user model parameter associated with user m. wm is a K dimensional vector. j = 1, 2, ..., Jm: The index for a set of data for user m. Jm is the number of training data for user m. Dm = {(xm,j, ym,j)}: A set of data associated with user m. xm,j is a K dimensional vector that represents the mth users jth training document.2 ym,j is a scalar that represents the label of document xm,j. k = 1, 2, ..., K: The dimensional index of input variable x.",
                "The Bayesian hierarchical modeling approach has been widely used in real-world information retrieval applications.",
                "Generalized Bayesian hierarchical linear models, one of the simplest Bayesian hierarchical models, are commonly used and have achieved good performance on collaborative filtering [25] and content-based adaptive filtering [27] tasks.",
                "Figure 1 shows the graphical representation of a Bayesian hierarchical model.",
                "In this graph, each user model is represented by a random vector wm.",
                "We assume a user model is sampled randomly from a prior distribution P(w|Φ).",
                "The system can predict the user label y of a document x given an estimation of wm (or wms distribution) using a function y = f(x, w).",
                "The model is called generalized Bayesian hierarchical linear model when y = f(wT x) is any generalized linear model such as logistic regression, SVM, and <br>linear regression</br>.",
                "To reliably estimate the user model wm, the system can borrow information from other users through the prior Φ = (µ, Σ).",
                "Now we look at one commonly used model where y = wT x + , where ∼ N(0, σ2 ) is a random noise [25][27].",
                "Assume that each user model wm is an independent draw from a population distribution P(w|Φ), which is governed by some unknown hyperparameter Φ.",
                "Let the prior distribution of user model w be a Gaussian distribution with parameter Φ = (µ, Σ), which is the commonly used prior for linear models. µ = (µ1, µ2, ..., µK ) is a K dimensional vector that represents the mean of the Gaussian distribution, and Σ is the covariance matrix of the Gaussian.",
                "Usually, a Normal distribution N(0, aI) and an Inverse Wishart distribution P(Σ) ∝ |Σ|− 1 2 b exp(−1 2 ctr(Σ−1 )) are used as hyperprior to model the prior distribution of µ and Σ respectively.",
                "I is the K dimensional identity matrix, and a, b, and c are real numbers.",
                "With these settings, we have the following model for the system: 1. µ and Σ are sampled from N(0, aI) and IWν (aI), respectively. 2 The first dimension of x is a dummy variable that always equals to 1.",
                "Figure 1: Illustration of dependencies of variables in the hierarchical model.",
                "The rating, y, for a document, x, is conditioned on the document and the user model, wm, associated with the user m. Users share information about their models through the prior, Φ = (µ, Σ). 2.",
                "For each user m, wm is sampled randomly from a Normal distribution: wm ∼ N(µ, Σ2 ) 3.",
                "For each item xm,j, ym,j is sampled randomly from a Normal distribution: ym,j ∼ N(wT mxm,j, σ2 ).",
                "Let θ = (Φ, w1, w2, ..., wM ) represent the parameters of this system that needs to be estimated.",
                "The joint likelihood for all the variables in the probabilistic model, which includes the data and the parameters, is: P(D, θ) = P(Φ) m P(wm|Φ) j P(ym,j|xm,j, wm) (1) For simplicity, we assume a, b, c, and σ are provided to the system. 4.",
                "MODEL PARAMETER LEARNING If the prior Φ is known, finding the optimal wm is straightforward: it is a simple <br>linear regression</br>.",
                "Therefore, we will focus on estimating Φ.",
                "The maximum a priori solution of Φ is given by ΦMAP = arg max Φ P(Φ|D) (2) = arg max Φ P(Φ, D) P(D) (3) = arg max Φ P(D|Φ)P(Φ) (4) = arg max Φ w P(D|w, Φ)P(w|Φ)P(Φ)dw (5) Finding the optimal solution for the above problem is challenging, since we need to integrate over all w = (w1, w2, ..., wM ), which are unobserved hidden variables. 4.1 EM Algorithm for Bayesian Hierarchical Linear Models In Equation 5, Φ is the parameter needs to be estimated, and the result depends on unobserved latent variables w. This kind of optimization problem is usually solved by the EM algorithm.",
                "Applying EM to the above problem, the set of user models w are the unobservable hidden variables and we have: Q = w P(w|µ, Σ2 , Dm) log P(µ, Σ2 , w, D)dw Based on the derivation of the EM formulas presented in [24], we have the following Expectation-Maximization steps for finding the optimal hyperparameters.",
                "For space considerations, we omit the derivation in this paper since it is not the focus of our work.",
                "E step: For each user m, estimate the user model distribution P(wm|Dm, Φ) = N(wm; ¯wm, Σ2 m) based on the current estimation of the prior Φ = (µ, Σ2 ). ¯wm = ((Σ2 )−1 + Sxx,m σ2 )−1 ( Sxy,m σ2 + (Σ2 )−1 µ) (6) Σ2 m = ((Σ2 )−1 + Sxx,m σ2 )−1 (7) where Sxx,m = j xm,jxT m,j Sxy,m = j xm,jym,j M step: Optimize the prior Φ = (µ, Σ2 ) based on the estimation from the last E step. µ = 1 M m ¯wm (8) Σ2 = 1 M m Σ2 m + ( ¯wm − µ)( ¯wm − µ)T (9) Many machine learning driven IR systems use a point estimate of the parameters at different stages in the system.",
                "However, we are estimating the posterior distribution of the variables at the E step.",
                "This avoids overfitting wm to a particular users data, which may be small and noisy.",
                "A detailed discussion about this subject appears in [10]. 4.2 New Algorithm: Modified EM Although the EM algorithm is widely studied and used in machine learning applications, using the above EM process to solve Bayesian hierarchical linear models in large-scale information retrieval systems is still too computationally expensive.",
                "In this section, we describe why the learning rate of the EM algorithm is slow in our application and introduce a new technique to make the learning of the Bayesian hierarchical linear model scalable.",
                "The derivation of the new learning algorithm will be based on the EM algorithm described in the previous section.",
                "First, the covariance matrices Σ2 , Σ2 m are usually too large to be computationally feasible.",
                "For simplicity, and as a common practice in IR, we do not model the correlation between features.",
                "Thus we approximate these matrices with K dimensional diagonal matrices.",
                "In the rest of the paper, we use these symbols to represent their diagonal approximations: Σ2 =     σ2 1 0 .. 0 0 σ2 2 .. 0 .. .. .. .. 0 0 .. σ2 K     Σ2 m =     σ2 m,1 0 .. 0 0 σ2 m,2 .. 0 .. .. .. .. 0 0 .. σ2 m,K     Secondly, and most importantly, the input space is very sparse and there are many dimensions that are not related to a particular user in a real IR application.",
                "For example, let us consider a movie recommendation system, with the input variable x representing a particular movie.",
                "For the jth movie that the user m has seen, let xm,j,k = 1 if the director of the movie is Jean-Pierre Jeunet (indexed by k).",
                "Here we assume that whether or not that this director directed a specific movie is represented by the kth dimension.",
                "If the user m has never seen a movie directed by Jean-Pierre Jeunet, then the corresponding dimension is always zero (xm,j,k = 0 for all j) .",
                "One major drawback of the EM algorithm is that the importance of a feature, µk, may be greatly dominated by users who have never encountered this feature (i.e. j xm,j,k = 0) at the M step (Equation 8).",
                "Assume that 100 out of 1 million users have viewed the movie directed by Jean-Pierre Jeunet, and that the viewers have rated all of his movies as excellent.",
                "Intuitively, he is a good director and the weight for him (µk) should be high.",
                "Before the EM iteration, the initial value of µ is usually set to 0.",
                "Since the other 999,900 users have not seen this movie, their corresponding weights (w1,k, w2,k, ..., wm,k..., w999900,k) for that director would be very small initially.",
                "Thus the corresponding weight of the director in the prior µk at the first M step would be very low , and the variance σm,k will be large (Equations 8 and 7).",
                "It is undesirable that users who have never seen any movie produced by the director influence the importance of the director so much.",
                "This makes the convergence of the standard EM algorithm very slow.",
                "Now lets look at whether we can improve the learning speed of the algorithm.",
                "Without a loss of generality, let us assume that the kth dimension of the input variable x is not related to a particular user m. By which we mean, xm,j,k = 0 for all j = 1, ..., Jm.",
                "It is straightforward to prove that the kth row and kth column of Sxx,m are completely filled with zeros, and that the kth dimension of Sxy,m is zeroed as well.",
                "Thus the corresponding kth dimension of the user models mean, ¯wm, should be equal to that of the prior: ¯wm,k = µk, with the corresponding covariance of σm,k = σk.",
                "At the M step, the standard EM algorithm uses the numerical solution of the distribution P(wm|Dm, Φ) estimated at E step (Equation 8 and Equation 7).",
                "However, the numerical solutions are very unreliable for ¯wm,k and σm,k when the kth dimension is not related to the mth user.",
                "A better approach is using the analytical solutions ¯wm,k = µk, and σm,k = σk for the unrelated (m, k) pairs, along with the numerical solution estimated at E step for the other (m, k) pairs.",
                "Thus we get the following new EM-like algorithm: Modified E step: For each user m, estimate the user model distribution P(wm|Dm, Φ) = N(wm; ¯wm, Σ2 m) based on the current estimation of σ , µ, Σ2 . ¯wm = ((Σ2 )−1 + Sxx,m σ2 )−1 ( Sxy,m σ2 + (Σ2 )−1 µ)(10) σ2 m,k = ((σ2 k)−1 + sxx,m,k σ2 )−1 (11) where sxx,m,k = j x2 m,j,k and sxy,m,k = j xm,j,kym,j Modified M Step Optimize the prior Φ = (µ, Σ2 ) based on the estimation from the last E step for related userfeature pairs.",
                "The M step implicitly uses the analytical solution for unrelated user-feature pairs. µk = 1 Mk m:related ¯wm,k (12) σ2 k = 1 Mk m:related σ2 m,k +( ¯wm,k − µk)( ¯wm,k − µk)T (13) where Mk is the number of users that are related to feature k We only estimate the diagonal of Σ2 m and Σ since we are using the diagonal approximation of the covariance matrices.",
                "To estimate ¯wm, we only need to calculate the numerical solutions for dimensions that are related to user m. To estimate σ2 k and µk, we only sum over users that are related to the kth feature.",
                "There are two major benefits of the new algorithm.",
                "First, because only the related (m, k) pairs are needed at the modified M step, the computational complexity in a single EM iteration is much smaller when the data is sparse, and many of (m, k) pairs are unrelated.",
                "Second, the parameters estimated at the modified M step (Equations 12 - 13) are more accurate than the standard M step described in Section 4.1 because the exact analytical solutions ¯wm,k = µk and σm,k = σk for the unrelated (m, k) pairs were used in the new algorithm instead of an approximate solution as in the standard algorithm. 5.",
                "EXPERIMENTAL METHODOLOGY 5.1 Evaluation Data Set To evaluate the proposed technique, we used the following three major data sets (Table 1): MovieLens Data: This data set was created by combining the relevance judgments from the MovieLens[9] data set with documents from the Internet Movie Database (IMDB).",
                "MovieLens allows users to rank how much he/she enjoyed a specific movie on a scale from 1 to 5.",
                "This likeability rating was used as a measurement of how relevant the document representing the corresponding movie is to the user.",
                "We considered documents with likeability scores of 4 or 5 as relevant, and documents with a score of 1 to 3 as irrelevant to the user.",
                "MovieLens provided relevance judgments on 3,057 documents from 6,040 separate users.",
                "On average, each user rated 151 movies, of these 87 were judged to be relevant.",
                "The average score for a document was 3.58.",
                "Documents representing each movie were constructed from the portion of the IMDB database that is available for public download[13].",
                "Based on this database, we created one document per movie that contained the relevant information about it (e.g. directors, actors, etc.).",
                "Table 1: Data Set Statistics.",
                "On Reuters, the number of rating for a simulated user is the number of documents relevant to the corresponding topic.",
                "Data Users Docs Ratings per User MovieLens 6,040 3,057 151 Netflix-all 480,189 17,770 208 Netflix-1000 1000 17,770 127 Reuters-C 34 100,000 3949 Reuters-E 26 100,000 1632 Reuters-G 33 100,000 2222 Reuters-M 10 100,000 6529 Netflix Data: This data set was constructed by combining documents about movies crawled from the web with a set of actual movie rental customer relevance judgments from Netflix[19].",
                "Netflix publicly provides the relevance judgments of 480,189 anonymous customers.",
                "There are around 100 million rating on a scale of 1 to 5 for 17,770 documents.",
                "Similar to MovieLens, we considered documents with likeability scores of 4 or 5 as relevant.",
                "This number was reduced to 1000 customers through random sampling.",
                "The average customer on the reduced data set provided 127 judgments, with 70 being deemed relevant.",
                "The average score for documents is 3.55.",
                "Reuters Data: This is the Reuters Corpus, Volume 1.",
                "It covers 810,000 Reuters English language news stories from August 20, 1996 to August 19, 1997.",
                "Only the first 100,000 news were used in our experiments.",
                "The Reuters corpus comes with a topic hierarchy.",
                "Each document is assigned to one of several locations on the hierarchical tree.",
                "The first level of the tree contains four topics, denoted as C, E, M, and G. For the experiments in this paper, the tree was cut at level 1 to create four smaller trees, each of which corresponds to one smaller data set: Reuters-E Reuters-C, ReutersM and Reuters-G. For each small data set, we created several profiles, one profile for each node in a sub-tree, to simulate multiple users, each with a related, yet separate definition of relevance.",
                "All the user profiles on a sub-tree are supposed to share the same prior model distribution.",
                "Since this corpus explicitly indicates only the relevant documents for a topic(user), all other documents are considered irrelevant. 5.2 Evaluation We designed the experiments to answer the following three questions: 1.",
                "Do we need to take the effort to use a Bayesian approach and learn a prior from other users? 2.",
                "Does the new algorithm work better than the standard EM algorithm for learning the Bayesian hierarchical linear model? 3.",
                "Can the new algorithm quickly learn many user models?",
                "To answer the first question, we compared the Bayesian hierarchical models with commonly used Norm-2 regularized <br>linear regression</br> models.",
                "In fact, the commonly used approach is equivalent to the model learned at the end of the first EM iteration.",
                "To answer the second question, we compared the proposed new algorithm with the standard EM algorithm to see whether the new learning algorithm is better.",
                "To answer the third question, we tested the efficiency of the new algorithm on the entire Netflix data set where about half a million user models need to be learned together.",
                "For the MovieLens and Netflix data sets, algorithm effectiveness was measured by mean square error, while on the Reuters data set classification error was used because it was more informative.",
                "We first evaluated the performance on each individual user, and then estimated the macro average over all users.",
                "Statistical tests (t-tests) were carried out to see whether the results are significant.",
                "For the experiments on the MovieLens and Netflix data sets, we used a random sample of 90% of each user for training, and the rest for testing.",
                "On Reuters data set, because there are too many relevant documents for each topic in the corpus, we used a random sample of 10% of each topic for training, and 10% of the remaining documents for testing.",
                "For all runs, we set (a, b, c, Σ ) = (0.1, 10, 0.1, 1) manually. 6.",
                "EXPERIMENTAL RESULTS Figure 2, Figure 3, and Figure 4 show that on all data sets, the Bayesian hierarchical modeling approach has a statistical significant improvement over the regularized <br>linear regression</br> model, which is equivalent to the Bayesian hierarchical models learned at the first iteration.",
                "Further analysis shows a negative correlation between the number of training data for a user and the improvement the system gets.",
                "This suggests that the borrowing information from other users has more significant improvements for users with less training data, which is as expected.",
                "However, the strength of the correlation differs over data sets, and the amount of training data is not the only characteristics that will influence the final performance.",
                "Figure 2 and Figure 3 show that the proposed new algorithm works better than the standard EM algorithm on the Netflix and MovieLens data sets.",
                "This is not surprising since the number of related feature-users pairs is much smaller than the number of unrelated feature-user pairs on these two data sets, and thus the proposed new algorithm is expected to work better.",
                "Figure 4 shows that the two algorithms work similarly on the Reuters-E data set.",
                "The accuracy of the new algorithm is similar to that of the standard EM algorithm at each iteration.",
                "The general patterns are very similar on other Reuters subsets.",
                "Further analysis shows that only 58% of the user-feature pairs are unrelated on this data set.",
                "Since the number of unrelated user-feature pairs is not extremely large, the sparseness is not a serious problem on the Reuters data set.",
                "Thus the two learning algorithms perform similarly.",
                "The results suggest that only on a corpus where the number of unrelated user-feature pairs is much larger than the number of related pairs, such as on the Netflix data set, the proposed technique will get a significant improvement over standard EM.",
                "However, the experiments also show that when the assumption does not hold, the new algorithm does not hurt performance.",
                "Although the proposed technique is faster than standard Figure 2: Performance on a Netflix subset with 1,000 users.",
                "The new algorithm is statistical significantly better than EM algorithm at iterations 2 - 10.",
                "Norm-2 regularized linear models are equivalent to the Bayesian hierarchical models learned at the first iteration, and are statistical significantly worse than the Bayesian hierarchical models. 0 2 4 6 8 10 1 1.05 1.1 1.15 1.2 1.25 1.3 1.35 1.4 Iterations MeanSquareError New Algorithm Traditional EM 1 2 3 4 5 6 7 8 9 10 0.33 0.34 0.35 0.36 0.37 0.38 0.39 Iterations ClassificationError New Algorithm Traditional EM Figure 3: Performance on a MovieLens subset with 1,000 users.",
                "The new algorithm is statistical significantly better than EM algorithm at iteration 2 to 17 (evaluated with mean square error). 1 6 11 16 21 0.5 1 1.5 2 2.5 3 3.5 Iterations MeanSquareError New Algorithm Traditional EM 1 6 11 16 21 0.35 0.4 0.45 0.5 0.55 0.6 0.65 Iterations ClassificationError New Algorithm Traditional EM Figure 4: Performance on a Reuters-E subset with 26 profiles.",
                "Performances on Reuters-C, Reuters-M, Reuters-G are similar. 1 2 3 4 5 0.011 0.0115 0.012 0.0125 0.013 0.0135 0.014 Iterations MeanSquareError New Algorithm Traditional EM 1 2 3 4 5 0.0102 0.0104 0.0106 0.0108 0.011 0.0112 0.0114 Iterations ClassificationError New Algorithm Traditional EM EM, can it really learn millions of user models quickly?",
                "Our results show that the modified EM algorithm converges quickly, and 2 - 3 modified EM iterations would result in a reliable estimation.",
                "We evaluated the algorithm on the whole Netflix data set (480,189 users, 159,836 features, and 100 million ratings) running on a single CPU PC (2GB memory, P4 3GHz).",
                "The system finished one modified EM iteration in about 4 hours.",
                "This demonstrates that the proposed technique can efficiently handle large-scale system like Netflix. 7.",
                "CONCLUSION Content-based user profile learning is an important problem and is the key to providing personal recommendations to a user, especially for recommending new items with a small number of ratings.",
                "The Bayesian hierarchical modeling approach is becoming an important user profile learning approach due to its theoretically justified ability to help one user through information transfer from the other users by way of hyperpriors.",
                "This paper examined the weakness of the popular EM based learning approach for Bayesian hierarchical linear models and proposed a better learning technique called Modified EM.",
                "We showed that the new technique is theoretically more computationally efficient than the standard EM algorithm.",
                "Evaluation on the MovieLens and Netflix data sets demonstrated the effectiveness of the new technique when the data is sparse, by which we mean the ratio of related user-feature pairs to unrelated pairs is small.",
                "Evaluation on the Reuters data set showed that the new technique performed similar to the standard EM algorithm when the sparseness condition does not hold.",
                "In general, it is better to use the new algorithm since it is as simple as standard EM, the performance is either better or similar to EM, and the computation complexity is lower at each iteration.",
                "It is worth mentioning that even if the original problem space is not sparse, sparseness can be created artificially when a recommendation system uses user-specific feature selection techniques to reduce the noise and user model complexity.",
                "The proposed technique can also be adapted to improve the learning in such a scenario.",
                "We also demonstrated that the proposed technique can learn half a million user profiles from 100 million ratings in a few hours with a single CPU.",
                "The research is important because scalability is a major concern for researchers when using the Bayesian hierarchical linear modeling approach to build a practical large scale system, even though the literature have demonstrated the effectiveness of the models in many applications.",
                "Our work is one major step on the road to make Bayesian hierarchical linear models more practical.",
                "The proposed new technique can be easily adapted to run on a cluster of machines, and thus further speed up the learning process to handle a larger scale system with hundreds of millions of users.",
                "The research has much potential to benefit people using EM algorithm on many other IR problems as well as machine learning problems.",
                "EM algorithm is a commonly used machine learning technique.",
                "It is used to find model parameters in many IR problems where the training data is very sparse.",
                "Although we are focusing on the Bayesian hierarchical linear models for recommendation and filtering, the new idea of using analytical solution instead of numerical solution for unrelated user-feature pairs at the M step could be adapted to many other problems. 8.",
                "ACKNOWLEDGMENTS We thank Wei Xu, David Lewis and anonymous reviewers for valuable feedback on the work described in this paper.",
                "Part of the work was supported by Yahoo, Google, the Petascale Data Storage Institute and the Institute for Scalable Scientific Data Management.",
                "Any opinions, findings, conclusions, or recommendations expressed in this material are those of the authors, and do not necessarily reflect those of the sponsors. 9.",
                "REFERENCES [1] C. Basu, H. Hirsh, and W. Cohen.",
                "Recommendation as classification: Using social and content-based information in recommendation.",
                "In Proceedings of the Fifteenth National Conference on Artificial Intelligence, 1998. [2] J. S. Breese, D. Heckerman, and C. Kadie.",
                "Empirical analysis of predictive algorithms for collaborative filtering.",
                "Technical report, Microsoft Research, One Microsoft Way, Redmond, WA 98052, 1998. [3] J. Callan.",
                "Document filtering with inference networks.",
                "In Proceedings of the Nineteenth Annual International ACM SIGIR Conference on Research and Development in Information Retrieval, pages 262-269, 1996. [4] N. Cancedda, N. Cesa-Bianchi, A. Conconi, C. Gentile, C. Goutte, T. Graepel, Y. Li, J. M. Renders, J. S. Taylor, and A. Vinokourov.",
                "Kernel method for document filtering.",
                "In The Eleventh Text REtrieval Conference (TREC11).",
                "National Institute of Standards and Technology, special publication 500-249, 2003. [5] C. Chelba and A. Acero.",
                "Adaptation of maximum entropy capitalizer: Little data can help a lot.",
                "In D. Lin and D. Wu, editors, Proceedings of EMNLP 2004, pages 285-292, Barcelona, Spain, July 2004.",
                "Association for Computational Linguistics. [6] B. Croft and J. Lafferty, editors.",
                "Language Modeling for Information Retrieval.",
                "Kluwer, 2002. [7] A. Dayanik, D. D. Lewis, D. Madigan, V. Menkov, and A. Genkin.",
                "Constructing informative prior distributions from domain knowledge in text classification.",
                "In SIGIR 06: Proceedings of the 29th annual international ACM SIGIR conference on Research and development in information retrieval, pages 493-500, New York, NY, USA, 2006.",
                "ACM Press. [8] J. Delgado and N. Ishii.",
                "Memory-based weightedmajority prediction for recommender systems.",
                "In ACM SIGIR99 Workshop on Recommender Systems, 1999. [9] GroupLens.",
                "Movielens. http://www.grouplens.org/taxonomy/term/14, 2006. [10] D. Heckerman.",
                "A tutorial on learning with bayesian networks.",
                "In M. Jordan, editor, Learning in Graphical Models.",
                "Kluwer Academic, 1998. [11] J. L. Herlocker, J.",
                "A. Konstan, A. Borchers, and J. Riedl.",
                "An algorithmic framework for performing collaborative filtering.",
                "In SIGIR 99: Proceedings of the 22nd annual international ACM SIGIR conference on Research and development in information retrieval, pages 230-237, New York, NY, USA, 1999.",
                "ACM Press. [12] T. Hofmann and J. Puzicha.",
                "Latent class models for collaborative filtering.",
                "In IJCAI 99: Proceedings of the Sixteenth International Joint Conference on Artificial Intelligence, pages 688-693, San Francisco, CA, USA, 1999.",
                "Morgan Kaufmann Publishers Inc. [13] I. M. D. (IMDB).",
                "Internet movie database. http://www.imdb.com/interfaces/, 2006. [14] R. Jin, J. Y. Chai, and L. Si.",
                "An automatic weighting scheme for collaborative filtering.",
                "In SIGIR 04: Proceedings of the 27th annual international ACM SIGIR conference on Research and development in information retrieval, pages 337-344, New York, NY, USA, 2004.",
                "ACM Press. [15] J.",
                "A. Konstan, B. N. Miller, D. Maltz, J. L. Herlocker, L. R. Gordon, and J. Riedl.",
                "GroupLens: Applying collaborative filtering to Usenet news.",
                "Communications of the ACM, 40(3):77-87, 1997. [16] D. Lewis.",
                "Applying support vector machines to the TREC-2001 batch filtering and routing tasks.",
                "In Proceedings of the Eleventh Text REtrieval Conference (TREC-11), 2002. [17] B. Liu, X. Li, W. S. Lee, , and P. Yu.",
                "Text classification by labeling words.",
                "In Proceedings of The Nineteenth National Conference on Artificial Intelligence (AAAI-2004), July 25-29, 2004. [18] P. Melville, R. J. Mooney, and R. Nagarajan.",
                "Content-boosted collaborative filtering for improved recommendations.",
                "In Proceedings of the Eighteenth National Conference on Artificial Intelligence (AAAI-2002), Edmonton, Canada, 2002. [19] Netflix.",
                "Netflix prize. http://www.netflixprize.com (visited on Nov. 30, 2006), 2006. [20] S. Robertson and K. Sparck-Jones.",
                "Relevance weighting of search terms.",
                "In Journal of the American Society for Information Science, volume 27, pages 129-146, 1976. [21] J. Wang, A. P. de Vries, and M. J. T. Reinders.",
                "Unifying user-based and item-based collaborative filtering approaches by similarity fusion.",
                "In SIGIR 06: Proceedings of the 29th annual international ACM SIGIR conference on Research and development in information retrieval, pages 501-508, New York, NY, USA, 2006.",
                "ACM Press. [22] X. Wu and R. K. Srihari.",
                "Incorporating prior knowledge with weighted margin support vector machines.",
                "In Proc.",
                "ACM Knowledge Discovery Data Mining Conf. (ACM SIGKDD 2004), Aug. 2004. [23] Y. Yang, S. Yoo, J. Zhang, and B. Kisiel.",
                "Robustness of adaptive filtering methods in a cross-benchmark evaluation.",
                "In Proceedings of the 28th Annual International ACM SIGIR Conference on Research and Development in Information Retrieval, 2005. [24] K. Yu, V. Tresp, and A. Schwaighofer.",
                "Learning gaussian processes from multiple tasks.",
                "In ICML 05: Proceedings of the 22nd international conference on Machine learning, pages 1012-1019, New York, NY, USA, 2005.",
                "ACM Press. [25] K. Yu, V. Tresp, and S. Yu.",
                "A nonparametric hierarchical bayesian framework for information filtering.",
                "In SIGIR 04: Proceedings of the 27th annual international ACM SIGIR conference on Research and development in information retrieval, pages 353-360.",
                "ACM Press, 2004. [26] X. Zhu.",
                "Semi-supervised learning literature survey.",
                "Technical report, University of Wisconsin - Madison, December 9, 2006. [27] P. Zigoris and Y. Zhang.",
                "Bayesian adaptive user profiling with explicit & implicit feedback.",
                "In Conference on Information and Knowledge Mangement 2006, 2006."
            ],
            "original_annotated_samples": [],
            "translated_annotated_samples": [
                "La organización de las partes restantes de este documento es la siguiente: la Sección 3 describe el marco de modelado jerárquico bayesiano de \"regresión lineal\" utilizado para recomendaciones basadas en contenido.",
                "La \"regresión lineal\" jerárquica bayesiana supone que hay M usuarios en el sistema.",
                "El modelo se llama modelo lineal jerárquico bayesiano generalizado cuando Y = F (WT x) es cualquier modelo lineal generalizado como regresión logística, SVM y \"regresión lineal\".",
                "Aprendizaje de parámetros del modelo Si se conoce el φ anterior, encontrar el WM óptimo es sencillo: es una simple \"regresión lineal\".",
                "Para responder a la primera pregunta, comparamos los modelos jerárquicos bayesianos con los modelos de \"regresión lineal\" regularizados de Norm-2 comúnmente utilizados.",
                "Resultados experimentales La Figura 2, la Figura 3 y la Figura 4 muestran que en todos los conjuntos de datos, el enfoque de modelado jerárquico bayesiano tiene una mejora estadística significativa sobre el modelo regularizado de \"regresión lineal\", que es equivalente a los modelos jerárquicos bayesianos aprendidos en la primera iteración."
            ],
            "translated_text": "",
            "candidates": [
                "regresión lineal",
                "regresión lineal",
                "regresión lineal",
                "regresión lineal",
                "regresión lineal",
                "regresión lineal",
                "regresión lineal",
                "regresión lineal",
                "regresión lineal",
                "regresión lineal",
                "regresión lineal",
                "regresión lineal"
            ],
            "error": []
        },
        "collaborative filtering": {
            "translated_key": "filtración colaborativa",
            "is_in_text": true,
            "original_annotated_sentences": [
                "Efficient Bayesian Hierarchical User Modeling for Recommendation Systems Yi Zhang, Jonathan Koren School of Engineering University of California Santa Cruz Santa Cruz, CA, USA {yiz, jonathan}@soe.ucsc.edu ABSTRACT A content-based personalized recommendation system learns user specific profiles from user feedback so that it can deliver information tailored to each individual users interest.",
                "A system serving millions of users can learn a better user profile for a new user, or a user with little feedback, by borrowing information from other users through the use of a Bayesian hierarchical model.",
                "Learning the model parameters to optimize the joint data likelihood from millions of users is very computationally expensive.",
                "The commonly used EM algorithm converges very slowly due to the sparseness of the data in IR applications.",
                "This paper proposes a new fast learning technique to learn a large number of individual user profiles.",
                "The efficacy and efficiency of the proposed algorithm are justified by theory and demonstrated on actual user data from Netflix and MovieLens.",
                "Categories and Subject Descriptors: B.3.3 [Information Search and Retrieval]: Information filtering General Terms: Algorithms 1.",
                "INTRODUCTION Personalization is the future of the Web, and it has achieved great success in industrial applications.",
                "For example, online stores, such as Amazon and Netflix, provide customized recommendations for additional products or services based on a users history.",
                "Recent offerings such as My MSN, My Yahoo!, My Google, and Google News have attracted much attention due to their potential ability to infer a users interests from his/her history.",
                "One major personalization topic studied in the information retrieval community is content-based personal recommendation systems1 .",
                "These systems learn user-specific profiles from user feedback so that they can recommend information tailored to each individual users interest without requiring the user to make an explicit query.",
                "Learning the user profiles is the core problem for these systems.",
                "A user profile is usually a classifier that can identify whether a document is relevant to the user or not, or a regression model that tells how relevant a document is to the user.",
                "One major challenge of building a recommendation or personalization system is that the profile learned for a particular user is usually of low quality when the amount of data from that particular user is small.",
                "This is known as the cold start problem.",
                "This means that any new user must endure poor initial performance until sufficient feedback from that user is provided to learn a reliable user profile.",
                "There has been much research on improving classification accuracy when the amount of labeled training data is small.",
                "The semi-supervised learning approach combines unlabeled and labeled data together to achieve this goal [26].",
                "Another approach is using domain knowledge.",
                "Researchers have modified different learning algorithms, such as Na¨ıveBayes [17], logistic regression [7], and SVMs [22], to integrate domain knowledge into a text classifier.",
                "The third approach is borrowing training data from other resources [5][7].",
                "The effectiveness of these different approaches is mixed, due to how well the underlying model assumption fits the data.",
                "One well-received approach to improve recommendation system performance for a particular user is borrowing information from other users through a Bayesian hierarchical modeling approach.",
                "Several researchers have demonstrated that this approach effectively trades off between shared and user-specific information, thus alleviating poor initial performance for each user[27][25].",
                "In order to learn a Bayesian hierarchical model, the system usually tries to find the most likely model parameters for the given data.",
                "A mature recommendation system usually works for millions of users.",
                "It is well known that learning the optimal parameters of a Bayesian hierarchical model is computationally expensive when there are thousands or millions of users.",
                "The EM algorithm is a commonly used technique for parameter learning due to its simplicity and convergence guarantee.",
                "However, a content based recommendation system often handles documents in a very high dimensional space, in which each document is represented by a very sparse vector.",
                "With careful analysis of the EM algorithm in this scenario (Section 4), we find that the EM tering, or item-based <br>collaborative filtering</br>.",
                "In this paper, the words filtering and recommendation are used interchangeably. algorithm converges very slowly due to the sparseness of the input variables.",
                "We also find that updating the model parameter at each EM iteration is also expensive with computational complexity of O(MK), where M is the number of users and K is the number of dimensions.",
                "This paper modifies the standard EM algorithm to create an improved learning algorithm, which we call the Modified EM algorithm.",
                "The basic idea is that instead of calculating the numerical solution for all the user profile parameters, we derive the analytical solution of the parameters for some feature dimensions, and at the M step use the analytical solution instead of the numerical solution estimated at E step for those parameters.",
                "This greatly reduces the computation at a single EM iteration, and also has the benefit of increasing the convergence speed of the learning algorithm.",
                "The proposed technique is not only well supported by theory, but also by experimental results.",
                "The organization of the remaining parts of this paper is as follows: Section 3 describes the Bayesian hierarchical linear regression modeling framework used for content-based recommendations.",
                "Section 4 describes how to learn the model parameters using the standard EM algorithm, along with using the new technique proposed in this paper.",
                "The experimental setting and results used to validate the proposed learning technique are reported in Sections 5 and 6.",
                "Section 7 summarizes and offers concluding remarks. 2.",
                "RELATED WORK Providing personalized recommendations to users has been identified as a very important problem in the IR community since the 1970s.",
                "The approaches that have been used to solve this problem can be roughly classified into two major categories: content based filtering versus <br>collaborative filtering</br>.",
                "Content-based filtering studies the scenario where a recommendation system monitors a document stream and pushes documents that match a user profile to the corresponding user.",
                "The user may read the delivered documents and provide explicit relevance feedback, which the filtering system then uses to update the users profile using relevance feedback retrieval models (e.g.",
                "Boolean models, vector space models, traditional probabilistic models [20] , inference networks [3] and language models [6]) or machine learning algorithms (e.g.",
                "Support Vector Machines (SVM), K nearest neighbors (K-NN) clustering, neural networks, logistic regression, or Winnow [16] [4] [23]).",
                "<br>collaborative filtering</br> goes beyond merely using document content to recommend items to a user by leveraging information from other users with similar tastes and preferences in the past.",
                "Memorybased heuristics and model based approaches have been used in <br>collaborative filtering</br> task [15] [8] [2] [14] [12] [11].",
                "This paper contributes to the content-based recommendation research by improving the efficiency and effectiveness of Bayesian hierarchical linear models, which have a strong theoretical basis and good empirical performance on recommendation tasks[27][25].",
                "This paper does not intend to compare content-based filtering with <br>collaborative filtering</br> or claim which one is a better.",
                "We think each complements the other, and that content-based filtering is extremely useful for handling new documents/items with little or no user feedback.",
                "Similar to some other researchers[18][1][21], we found that a recommendation system will be more effective when both techniques are combined.",
                "However, this is beyond the scope of this paper and thus not discussed here. 3.",
                "BAYESIAN HIERARCHICAL LINEAR REGRESSION Assume there are M users in the system.",
                "The task of the system is to recommend documents that are relevant to each user.",
                "For each user, the system learns a user model from the users history.",
                "In the rest of this paper, we will use the following notations to represent the variables in the system. m = 1, 2, ..., M: The index for each individual user.",
                "M is the total number of users. wm: The user model parameter associated with user m. wm is a K dimensional vector. j = 1, 2, ..., Jm: The index for a set of data for user m. Jm is the number of training data for user m. Dm = {(xm,j, ym,j)}: A set of data associated with user m. xm,j is a K dimensional vector that represents the mth users jth training document.2 ym,j is a scalar that represents the label of document xm,j. k = 1, 2, ..., K: The dimensional index of input variable x.",
                "The Bayesian hierarchical modeling approach has been widely used in real-world information retrieval applications.",
                "Generalized Bayesian hierarchical linear models, one of the simplest Bayesian hierarchical models, are commonly used and have achieved good performance on <br>collaborative filtering</br> [25] and content-based adaptive filtering [27] tasks.",
                "Figure 1 shows the graphical representation of a Bayesian hierarchical model.",
                "In this graph, each user model is represented by a random vector wm.",
                "We assume a user model is sampled randomly from a prior distribution P(w|Φ).",
                "The system can predict the user label y of a document x given an estimation of wm (or wms distribution) using a function y = f(x, w).",
                "The model is called generalized Bayesian hierarchical linear model when y = f(wT x) is any generalized linear model such as logistic regression, SVM, and linear regression.",
                "To reliably estimate the user model wm, the system can borrow information from other users through the prior Φ = (µ, Σ).",
                "Now we look at one commonly used model where y = wT x + , where ∼ N(0, σ2 ) is a random noise [25][27].",
                "Assume that each user model wm is an independent draw from a population distribution P(w|Φ), which is governed by some unknown hyperparameter Φ.",
                "Let the prior distribution of user model w be a Gaussian distribution with parameter Φ = (µ, Σ), which is the commonly used prior for linear models. µ = (µ1, µ2, ..., µK ) is a K dimensional vector that represents the mean of the Gaussian distribution, and Σ is the covariance matrix of the Gaussian.",
                "Usually, a Normal distribution N(0, aI) and an Inverse Wishart distribution P(Σ) ∝ |Σ|− 1 2 b exp(−1 2 ctr(Σ−1 )) are used as hyperprior to model the prior distribution of µ and Σ respectively.",
                "I is the K dimensional identity matrix, and a, b, and c are real numbers.",
                "With these settings, we have the following model for the system: 1. µ and Σ are sampled from N(0, aI) and IWν (aI), respectively. 2 The first dimension of x is a dummy variable that always equals to 1.",
                "Figure 1: Illustration of dependencies of variables in the hierarchical model.",
                "The rating, y, for a document, x, is conditioned on the document and the user model, wm, associated with the user m. Users share information about their models through the prior, Φ = (µ, Σ). 2.",
                "For each user m, wm is sampled randomly from a Normal distribution: wm ∼ N(µ, Σ2 ) 3.",
                "For each item xm,j, ym,j is sampled randomly from a Normal distribution: ym,j ∼ N(wT mxm,j, σ2 ).",
                "Let θ = (Φ, w1, w2, ..., wM ) represent the parameters of this system that needs to be estimated.",
                "The joint likelihood for all the variables in the probabilistic model, which includes the data and the parameters, is: P(D, θ) = P(Φ) m P(wm|Φ) j P(ym,j|xm,j, wm) (1) For simplicity, we assume a, b, c, and σ are provided to the system. 4.",
                "MODEL PARAMETER LEARNING If the prior Φ is known, finding the optimal wm is straightforward: it is a simple linear regression.",
                "Therefore, we will focus on estimating Φ.",
                "The maximum a priori solution of Φ is given by ΦMAP = arg max Φ P(Φ|D) (2) = arg max Φ P(Φ, D) P(D) (3) = arg max Φ P(D|Φ)P(Φ) (4) = arg max Φ w P(D|w, Φ)P(w|Φ)P(Φ)dw (5) Finding the optimal solution for the above problem is challenging, since we need to integrate over all w = (w1, w2, ..., wM ), which are unobserved hidden variables. 4.1 EM Algorithm for Bayesian Hierarchical Linear Models In Equation 5, Φ is the parameter needs to be estimated, and the result depends on unobserved latent variables w. This kind of optimization problem is usually solved by the EM algorithm.",
                "Applying EM to the above problem, the set of user models w are the unobservable hidden variables and we have: Q = w P(w|µ, Σ2 , Dm) log P(µ, Σ2 , w, D)dw Based on the derivation of the EM formulas presented in [24], we have the following Expectation-Maximization steps for finding the optimal hyperparameters.",
                "For space considerations, we omit the derivation in this paper since it is not the focus of our work.",
                "E step: For each user m, estimate the user model distribution P(wm|Dm, Φ) = N(wm; ¯wm, Σ2 m) based on the current estimation of the prior Φ = (µ, Σ2 ). ¯wm = ((Σ2 )−1 + Sxx,m σ2 )−1 ( Sxy,m σ2 + (Σ2 )−1 µ) (6) Σ2 m = ((Σ2 )−1 + Sxx,m σ2 )−1 (7) where Sxx,m = j xm,jxT m,j Sxy,m = j xm,jym,j M step: Optimize the prior Φ = (µ, Σ2 ) based on the estimation from the last E step. µ = 1 M m ¯wm (8) Σ2 = 1 M m Σ2 m + ( ¯wm − µ)( ¯wm − µ)T (9) Many machine learning driven IR systems use a point estimate of the parameters at different stages in the system.",
                "However, we are estimating the posterior distribution of the variables at the E step.",
                "This avoids overfitting wm to a particular users data, which may be small and noisy.",
                "A detailed discussion about this subject appears in [10]. 4.2 New Algorithm: Modified EM Although the EM algorithm is widely studied and used in machine learning applications, using the above EM process to solve Bayesian hierarchical linear models in large-scale information retrieval systems is still too computationally expensive.",
                "In this section, we describe why the learning rate of the EM algorithm is slow in our application and introduce a new technique to make the learning of the Bayesian hierarchical linear model scalable.",
                "The derivation of the new learning algorithm will be based on the EM algorithm described in the previous section.",
                "First, the covariance matrices Σ2 , Σ2 m are usually too large to be computationally feasible.",
                "For simplicity, and as a common practice in IR, we do not model the correlation between features.",
                "Thus we approximate these matrices with K dimensional diagonal matrices.",
                "In the rest of the paper, we use these symbols to represent their diagonal approximations: Σ2 =     σ2 1 0 .. 0 0 σ2 2 .. 0 .. .. .. .. 0 0 .. σ2 K     Σ2 m =     σ2 m,1 0 .. 0 0 σ2 m,2 .. 0 .. .. .. .. 0 0 .. σ2 m,K     Secondly, and most importantly, the input space is very sparse and there are many dimensions that are not related to a particular user in a real IR application.",
                "For example, let us consider a movie recommendation system, with the input variable x representing a particular movie.",
                "For the jth movie that the user m has seen, let xm,j,k = 1 if the director of the movie is Jean-Pierre Jeunet (indexed by k).",
                "Here we assume that whether or not that this director directed a specific movie is represented by the kth dimension.",
                "If the user m has never seen a movie directed by Jean-Pierre Jeunet, then the corresponding dimension is always zero (xm,j,k = 0 for all j) .",
                "One major drawback of the EM algorithm is that the importance of a feature, µk, may be greatly dominated by users who have never encountered this feature (i.e. j xm,j,k = 0) at the M step (Equation 8).",
                "Assume that 100 out of 1 million users have viewed the movie directed by Jean-Pierre Jeunet, and that the viewers have rated all of his movies as excellent.",
                "Intuitively, he is a good director and the weight for him (µk) should be high.",
                "Before the EM iteration, the initial value of µ is usually set to 0.",
                "Since the other 999,900 users have not seen this movie, their corresponding weights (w1,k, w2,k, ..., wm,k..., w999900,k) for that director would be very small initially.",
                "Thus the corresponding weight of the director in the prior µk at the first M step would be very low , and the variance σm,k will be large (Equations 8 and 7).",
                "It is undesirable that users who have never seen any movie produced by the director influence the importance of the director so much.",
                "This makes the convergence of the standard EM algorithm very slow.",
                "Now lets look at whether we can improve the learning speed of the algorithm.",
                "Without a loss of generality, let us assume that the kth dimension of the input variable x is not related to a particular user m. By which we mean, xm,j,k = 0 for all j = 1, ..., Jm.",
                "It is straightforward to prove that the kth row and kth column of Sxx,m are completely filled with zeros, and that the kth dimension of Sxy,m is zeroed as well.",
                "Thus the corresponding kth dimension of the user models mean, ¯wm, should be equal to that of the prior: ¯wm,k = µk, with the corresponding covariance of σm,k = σk.",
                "At the M step, the standard EM algorithm uses the numerical solution of the distribution P(wm|Dm, Φ) estimated at E step (Equation 8 and Equation 7).",
                "However, the numerical solutions are very unreliable for ¯wm,k and σm,k when the kth dimension is not related to the mth user.",
                "A better approach is using the analytical solutions ¯wm,k = µk, and σm,k = σk for the unrelated (m, k) pairs, along with the numerical solution estimated at E step for the other (m, k) pairs.",
                "Thus we get the following new EM-like algorithm: Modified E step: For each user m, estimate the user model distribution P(wm|Dm, Φ) = N(wm; ¯wm, Σ2 m) based on the current estimation of σ , µ, Σ2 . ¯wm = ((Σ2 )−1 + Sxx,m σ2 )−1 ( Sxy,m σ2 + (Σ2 )−1 µ)(10) σ2 m,k = ((σ2 k)−1 + sxx,m,k σ2 )−1 (11) where sxx,m,k = j x2 m,j,k and sxy,m,k = j xm,j,kym,j Modified M Step Optimize the prior Φ = (µ, Σ2 ) based on the estimation from the last E step for related userfeature pairs.",
                "The M step implicitly uses the analytical solution for unrelated user-feature pairs. µk = 1 Mk m:related ¯wm,k (12) σ2 k = 1 Mk m:related σ2 m,k +( ¯wm,k − µk)( ¯wm,k − µk)T (13) where Mk is the number of users that are related to feature k We only estimate the diagonal of Σ2 m and Σ since we are using the diagonal approximation of the covariance matrices.",
                "To estimate ¯wm, we only need to calculate the numerical solutions for dimensions that are related to user m. To estimate σ2 k and µk, we only sum over users that are related to the kth feature.",
                "There are two major benefits of the new algorithm.",
                "First, because only the related (m, k) pairs are needed at the modified M step, the computational complexity in a single EM iteration is much smaller when the data is sparse, and many of (m, k) pairs are unrelated.",
                "Second, the parameters estimated at the modified M step (Equations 12 - 13) are more accurate than the standard M step described in Section 4.1 because the exact analytical solutions ¯wm,k = µk and σm,k = σk for the unrelated (m, k) pairs were used in the new algorithm instead of an approximate solution as in the standard algorithm. 5.",
                "EXPERIMENTAL METHODOLOGY 5.1 Evaluation Data Set To evaluate the proposed technique, we used the following three major data sets (Table 1): MovieLens Data: This data set was created by combining the relevance judgments from the MovieLens[9] data set with documents from the Internet Movie Database (IMDB).",
                "MovieLens allows users to rank how much he/she enjoyed a specific movie on a scale from 1 to 5.",
                "This likeability rating was used as a measurement of how relevant the document representing the corresponding movie is to the user.",
                "We considered documents with likeability scores of 4 or 5 as relevant, and documents with a score of 1 to 3 as irrelevant to the user.",
                "MovieLens provided relevance judgments on 3,057 documents from 6,040 separate users.",
                "On average, each user rated 151 movies, of these 87 were judged to be relevant.",
                "The average score for a document was 3.58.",
                "Documents representing each movie were constructed from the portion of the IMDB database that is available for public download[13].",
                "Based on this database, we created one document per movie that contained the relevant information about it (e.g. directors, actors, etc.).",
                "Table 1: Data Set Statistics.",
                "On Reuters, the number of rating for a simulated user is the number of documents relevant to the corresponding topic.",
                "Data Users Docs Ratings per User MovieLens 6,040 3,057 151 Netflix-all 480,189 17,770 208 Netflix-1000 1000 17,770 127 Reuters-C 34 100,000 3949 Reuters-E 26 100,000 1632 Reuters-G 33 100,000 2222 Reuters-M 10 100,000 6529 Netflix Data: This data set was constructed by combining documents about movies crawled from the web with a set of actual movie rental customer relevance judgments from Netflix[19].",
                "Netflix publicly provides the relevance judgments of 480,189 anonymous customers.",
                "There are around 100 million rating on a scale of 1 to 5 for 17,770 documents.",
                "Similar to MovieLens, we considered documents with likeability scores of 4 or 5 as relevant.",
                "This number was reduced to 1000 customers through random sampling.",
                "The average customer on the reduced data set provided 127 judgments, with 70 being deemed relevant.",
                "The average score for documents is 3.55.",
                "Reuters Data: This is the Reuters Corpus, Volume 1.",
                "It covers 810,000 Reuters English language news stories from August 20, 1996 to August 19, 1997.",
                "Only the first 100,000 news were used in our experiments.",
                "The Reuters corpus comes with a topic hierarchy.",
                "Each document is assigned to one of several locations on the hierarchical tree.",
                "The first level of the tree contains four topics, denoted as C, E, M, and G. For the experiments in this paper, the tree was cut at level 1 to create four smaller trees, each of which corresponds to one smaller data set: Reuters-E Reuters-C, ReutersM and Reuters-G. For each small data set, we created several profiles, one profile for each node in a sub-tree, to simulate multiple users, each with a related, yet separate definition of relevance.",
                "All the user profiles on a sub-tree are supposed to share the same prior model distribution.",
                "Since this corpus explicitly indicates only the relevant documents for a topic(user), all other documents are considered irrelevant. 5.2 Evaluation We designed the experiments to answer the following three questions: 1.",
                "Do we need to take the effort to use a Bayesian approach and learn a prior from other users? 2.",
                "Does the new algorithm work better than the standard EM algorithm for learning the Bayesian hierarchical linear model? 3.",
                "Can the new algorithm quickly learn many user models?",
                "To answer the first question, we compared the Bayesian hierarchical models with commonly used Norm-2 regularized linear regression models.",
                "In fact, the commonly used approach is equivalent to the model learned at the end of the first EM iteration.",
                "To answer the second question, we compared the proposed new algorithm with the standard EM algorithm to see whether the new learning algorithm is better.",
                "To answer the third question, we tested the efficiency of the new algorithm on the entire Netflix data set where about half a million user models need to be learned together.",
                "For the MovieLens and Netflix data sets, algorithm effectiveness was measured by mean square error, while on the Reuters data set classification error was used because it was more informative.",
                "We first evaluated the performance on each individual user, and then estimated the macro average over all users.",
                "Statistical tests (t-tests) were carried out to see whether the results are significant.",
                "For the experiments on the MovieLens and Netflix data sets, we used a random sample of 90% of each user for training, and the rest for testing.",
                "On Reuters data set, because there are too many relevant documents for each topic in the corpus, we used a random sample of 10% of each topic for training, and 10% of the remaining documents for testing.",
                "For all runs, we set (a, b, c, Σ ) = (0.1, 10, 0.1, 1) manually. 6.",
                "EXPERIMENTAL RESULTS Figure 2, Figure 3, and Figure 4 show that on all data sets, the Bayesian hierarchical modeling approach has a statistical significant improvement over the regularized linear regression model, which is equivalent to the Bayesian hierarchical models learned at the first iteration.",
                "Further analysis shows a negative correlation between the number of training data for a user and the improvement the system gets.",
                "This suggests that the borrowing information from other users has more significant improvements for users with less training data, which is as expected.",
                "However, the strength of the correlation differs over data sets, and the amount of training data is not the only characteristics that will influence the final performance.",
                "Figure 2 and Figure 3 show that the proposed new algorithm works better than the standard EM algorithm on the Netflix and MovieLens data sets.",
                "This is not surprising since the number of related feature-users pairs is much smaller than the number of unrelated feature-user pairs on these two data sets, and thus the proposed new algorithm is expected to work better.",
                "Figure 4 shows that the two algorithms work similarly on the Reuters-E data set.",
                "The accuracy of the new algorithm is similar to that of the standard EM algorithm at each iteration.",
                "The general patterns are very similar on other Reuters subsets.",
                "Further analysis shows that only 58% of the user-feature pairs are unrelated on this data set.",
                "Since the number of unrelated user-feature pairs is not extremely large, the sparseness is not a serious problem on the Reuters data set.",
                "Thus the two learning algorithms perform similarly.",
                "The results suggest that only on a corpus where the number of unrelated user-feature pairs is much larger than the number of related pairs, such as on the Netflix data set, the proposed technique will get a significant improvement over standard EM.",
                "However, the experiments also show that when the assumption does not hold, the new algorithm does not hurt performance.",
                "Although the proposed technique is faster than standard Figure 2: Performance on a Netflix subset with 1,000 users.",
                "The new algorithm is statistical significantly better than EM algorithm at iterations 2 - 10.",
                "Norm-2 regularized linear models are equivalent to the Bayesian hierarchical models learned at the first iteration, and are statistical significantly worse than the Bayesian hierarchical models. 0 2 4 6 8 10 1 1.05 1.1 1.15 1.2 1.25 1.3 1.35 1.4 Iterations MeanSquareError New Algorithm Traditional EM 1 2 3 4 5 6 7 8 9 10 0.33 0.34 0.35 0.36 0.37 0.38 0.39 Iterations ClassificationError New Algorithm Traditional EM Figure 3: Performance on a MovieLens subset with 1,000 users.",
                "The new algorithm is statistical significantly better than EM algorithm at iteration 2 to 17 (evaluated with mean square error). 1 6 11 16 21 0.5 1 1.5 2 2.5 3 3.5 Iterations MeanSquareError New Algorithm Traditional EM 1 6 11 16 21 0.35 0.4 0.45 0.5 0.55 0.6 0.65 Iterations ClassificationError New Algorithm Traditional EM Figure 4: Performance on a Reuters-E subset with 26 profiles.",
                "Performances on Reuters-C, Reuters-M, Reuters-G are similar. 1 2 3 4 5 0.011 0.0115 0.012 0.0125 0.013 0.0135 0.014 Iterations MeanSquareError New Algorithm Traditional EM 1 2 3 4 5 0.0102 0.0104 0.0106 0.0108 0.011 0.0112 0.0114 Iterations ClassificationError New Algorithm Traditional EM EM, can it really learn millions of user models quickly?",
                "Our results show that the modified EM algorithm converges quickly, and 2 - 3 modified EM iterations would result in a reliable estimation.",
                "We evaluated the algorithm on the whole Netflix data set (480,189 users, 159,836 features, and 100 million ratings) running on a single CPU PC (2GB memory, P4 3GHz).",
                "The system finished one modified EM iteration in about 4 hours.",
                "This demonstrates that the proposed technique can efficiently handle large-scale system like Netflix. 7.",
                "CONCLUSION Content-based user profile learning is an important problem and is the key to providing personal recommendations to a user, especially for recommending new items with a small number of ratings.",
                "The Bayesian hierarchical modeling approach is becoming an important user profile learning approach due to its theoretically justified ability to help one user through information transfer from the other users by way of hyperpriors.",
                "This paper examined the weakness of the popular EM based learning approach for Bayesian hierarchical linear models and proposed a better learning technique called Modified EM.",
                "We showed that the new technique is theoretically more computationally efficient than the standard EM algorithm.",
                "Evaluation on the MovieLens and Netflix data sets demonstrated the effectiveness of the new technique when the data is sparse, by which we mean the ratio of related user-feature pairs to unrelated pairs is small.",
                "Evaluation on the Reuters data set showed that the new technique performed similar to the standard EM algorithm when the sparseness condition does not hold.",
                "In general, it is better to use the new algorithm since it is as simple as standard EM, the performance is either better or similar to EM, and the computation complexity is lower at each iteration.",
                "It is worth mentioning that even if the original problem space is not sparse, sparseness can be created artificially when a recommendation system uses user-specific feature selection techniques to reduce the noise and user model complexity.",
                "The proposed technique can also be adapted to improve the learning in such a scenario.",
                "We also demonstrated that the proposed technique can learn half a million user profiles from 100 million ratings in a few hours with a single CPU.",
                "The research is important because scalability is a major concern for researchers when using the Bayesian hierarchical linear modeling approach to build a practical large scale system, even though the literature have demonstrated the effectiveness of the models in many applications.",
                "Our work is one major step on the road to make Bayesian hierarchical linear models more practical.",
                "The proposed new technique can be easily adapted to run on a cluster of machines, and thus further speed up the learning process to handle a larger scale system with hundreds of millions of users.",
                "The research has much potential to benefit people using EM algorithm on many other IR problems as well as machine learning problems.",
                "EM algorithm is a commonly used machine learning technique.",
                "It is used to find model parameters in many IR problems where the training data is very sparse.",
                "Although we are focusing on the Bayesian hierarchical linear models for recommendation and filtering, the new idea of using analytical solution instead of numerical solution for unrelated user-feature pairs at the M step could be adapted to many other problems. 8.",
                "ACKNOWLEDGMENTS We thank Wei Xu, David Lewis and anonymous reviewers for valuable feedback on the work described in this paper.",
                "Part of the work was supported by Yahoo, Google, the Petascale Data Storage Institute and the Institute for Scalable Scientific Data Management.",
                "Any opinions, findings, conclusions, or recommendations expressed in this material are those of the authors, and do not necessarily reflect those of the sponsors. 9.",
                "REFERENCES [1] C. Basu, H. Hirsh, and W. Cohen.",
                "Recommendation as classification: Using social and content-based information in recommendation.",
                "In Proceedings of the Fifteenth National Conference on Artificial Intelligence, 1998. [2] J. S. Breese, D. Heckerman, and C. Kadie.",
                "Empirical analysis of predictive algorithms for <br>collaborative filtering</br>.",
                "Technical report, Microsoft Research, One Microsoft Way, Redmond, WA 98052, 1998. [3] J. Callan.",
                "Document filtering with inference networks.",
                "In Proceedings of the Nineteenth Annual International ACM SIGIR Conference on Research and Development in Information Retrieval, pages 262-269, 1996. [4] N. Cancedda, N. Cesa-Bianchi, A. Conconi, C. Gentile, C. Goutte, T. Graepel, Y. Li, J. M. Renders, J. S. Taylor, and A. Vinokourov.",
                "Kernel method for document filtering.",
                "In The Eleventh Text REtrieval Conference (TREC11).",
                "National Institute of Standards and Technology, special publication 500-249, 2003. [5] C. Chelba and A. Acero.",
                "Adaptation of maximum entropy capitalizer: Little data can help a lot.",
                "In D. Lin and D. Wu, editors, Proceedings of EMNLP 2004, pages 285-292, Barcelona, Spain, July 2004.",
                "Association for Computational Linguistics. [6] B. Croft and J. Lafferty, editors.",
                "Language Modeling for Information Retrieval.",
                "Kluwer, 2002. [7] A. Dayanik, D. D. Lewis, D. Madigan, V. Menkov, and A. Genkin.",
                "Constructing informative prior distributions from domain knowledge in text classification.",
                "In SIGIR 06: Proceedings of the 29th annual international ACM SIGIR conference on Research and development in information retrieval, pages 493-500, New York, NY, USA, 2006.",
                "ACM Press. [8] J. Delgado and N. Ishii.",
                "Memory-based weightedmajority prediction for recommender systems.",
                "In ACM SIGIR99 Workshop on Recommender Systems, 1999. [9] GroupLens.",
                "Movielens. http://www.grouplens.org/taxonomy/term/14, 2006. [10] D. Heckerman.",
                "A tutorial on learning with bayesian networks.",
                "In M. Jordan, editor, Learning in Graphical Models.",
                "Kluwer Academic, 1998. [11] J. L. Herlocker, J.",
                "A. Konstan, A. Borchers, and J. Riedl.",
                "An algorithmic framework for performing <br>collaborative filtering</br>.",
                "In SIGIR 99: Proceedings of the 22nd annual international ACM SIGIR conference on Research and development in information retrieval, pages 230-237, New York, NY, USA, 1999.",
                "ACM Press. [12] T. Hofmann and J. Puzicha.",
                "Latent class models for <br>collaborative filtering</br>.",
                "In IJCAI 99: Proceedings of the Sixteenth International Joint Conference on Artificial Intelligence, pages 688-693, San Francisco, CA, USA, 1999.",
                "Morgan Kaufmann Publishers Inc. [13] I. M. D. (IMDB).",
                "Internet movie database. http://www.imdb.com/interfaces/, 2006. [14] R. Jin, J. Y. Chai, and L. Si.",
                "An automatic weighting scheme for <br>collaborative filtering</br>.",
                "In SIGIR 04: Proceedings of the 27th annual international ACM SIGIR conference on Research and development in information retrieval, pages 337-344, New York, NY, USA, 2004.",
                "ACM Press. [15] J.",
                "A. Konstan, B. N. Miller, D. Maltz, J. L. Herlocker, L. R. Gordon, and J. Riedl.",
                "GroupLens: Applying <br>collaborative filtering</br> to Usenet news.",
                "Communications of the ACM, 40(3):77-87, 1997. [16] D. Lewis.",
                "Applying support vector machines to the TREC-2001 batch filtering and routing tasks.",
                "In Proceedings of the Eleventh Text REtrieval Conference (TREC-11), 2002. [17] B. Liu, X. Li, W. S. Lee, , and P. Yu.",
                "Text classification by labeling words.",
                "In Proceedings of The Nineteenth National Conference on Artificial Intelligence (AAAI-2004), July 25-29, 2004. [18] P. Melville, R. J. Mooney, and R. Nagarajan.",
                "Content-boosted <br>collaborative filtering</br> for improved recommendations.",
                "In Proceedings of the Eighteenth National Conference on Artificial Intelligence (AAAI-2002), Edmonton, Canada, 2002. [19] Netflix.",
                "Netflix prize. http://www.netflixprize.com (visited on Nov. 30, 2006), 2006. [20] S. Robertson and K. Sparck-Jones.",
                "Relevance weighting of search terms.",
                "In Journal of the American Society for Information Science, volume 27, pages 129-146, 1976. [21] J. Wang, A. P. de Vries, and M. J. T. Reinders.",
                "Unifying user-based and item-based <br>collaborative filtering</br> approaches by similarity fusion.",
                "In SIGIR 06: Proceedings of the 29th annual international ACM SIGIR conference on Research and development in information retrieval, pages 501-508, New York, NY, USA, 2006.",
                "ACM Press. [22] X. Wu and R. K. Srihari.",
                "Incorporating prior knowledge with weighted margin support vector machines.",
                "In Proc.",
                "ACM Knowledge Discovery Data Mining Conf. (ACM SIGKDD 2004), Aug. 2004. [23] Y. Yang, S. Yoo, J. Zhang, and B. Kisiel.",
                "Robustness of adaptive filtering methods in a cross-benchmark evaluation.",
                "In Proceedings of the 28th Annual International ACM SIGIR Conference on Research and Development in Information Retrieval, 2005. [24] K. Yu, V. Tresp, and A. Schwaighofer.",
                "Learning gaussian processes from multiple tasks.",
                "In ICML 05: Proceedings of the 22nd international conference on Machine learning, pages 1012-1019, New York, NY, USA, 2005.",
                "ACM Press. [25] K. Yu, V. Tresp, and S. Yu.",
                "A nonparametric hierarchical bayesian framework for information filtering.",
                "In SIGIR 04: Proceedings of the 27th annual international ACM SIGIR conference on Research and development in information retrieval, pages 353-360.",
                "ACM Press, 2004. [26] X. Zhu.",
                "Semi-supervised learning literature survey.",
                "Technical report, University of Wisconsin - Madison, December 9, 2006. [27] P. Zigoris and Y. Zhang.",
                "Bayesian adaptive user profiling with explicit & implicit feedback.",
                "In Conference on Information and Knowledge Mangement 2006, 2006."
            ],
            "original_annotated_samples": [],
            "translated_annotated_samples": [
                "Con un análisis cuidadoso del algoritmo EM en este escenario (Sección 4), encontramos que la emergencia EM, o \"filtrado colaborativo\" basado en elementos.",
                "Los enfoques que se han utilizado para resolver este problema pueden clasificarse aproximadamente en dos categorías principales: filtrado basado en contenido versus \"filtrado colaborativo\".",
                "El \"filtrado colaborativo\" va más allá del simplemente usar contenido de documento para recomendar elementos a un usuario aprovechando la información de otros usuarios con gustos y preferencias similares en el pasado.",
                "La heurística basada en la memoria y los enfoques basados en modelos se han utilizado en la tarea de \"filtrado colaborativo\" [15] [8] [2] [14] [12] [11].",
                "Este documento no tiene la intención de comparar el filtrado basado en el contenido con \"filtrado colaborativo\" o afirmar cuál es mejor.",
                "Los modelos lineales jerárquicos bayesianos generalizados, uno de los modelos jerárquicos bayesianos más simples, se usan comúnmente y ha logrado un buen rendimiento en las tareas de \"filtrado colaborativo\" [25] y filtrado adaptativo basado en contenido [27].",
                "Análisis empírico de algoritmos predictivos para el \"filtrado colaborativo\".",
                "Un marco algorítmico para realizar un \"filtrado colaborativo\".",
                "Modelos de clase latente para \"filtrado colaborativo\".",
                "Un esquema de ponderación automática para \"filtrado colaborativo\"."
            ],
            "translated_text": "",
            "candidates": [
                "filtración colaborativa",
                "filtrado colaborativo",
                "filtración colaborativa",
                "filtrado colaborativo",
                "filtración colaborativa",
                "filtrado colaborativo",
                "filtración colaborativa",
                "filtrado colaborativo",
                "filtración colaborativa",
                "filtrado colaborativo",
                "filtración colaborativa",
                "filtrado colaborativo",
                "filtración colaborativa",
                "filtrado colaborativo",
                "filtración colaborativa",
                "filtrado colaborativo",
                "filtración colaborativa",
                "filtrado colaborativo",
                "filtración colaborativa",
                "filtrado colaborativo"
            ],
            "error": []
        },
        "parameter": {
            "translated_key": "parámetro",
            "is_in_text": true,
            "original_annotated_sentences": [
                "Efficient Bayesian Hierarchical User Modeling for Recommendation Systems Yi Zhang, Jonathan Koren School of Engineering University of California Santa Cruz Santa Cruz, CA, USA {yiz, jonathan}@soe.ucsc.edu ABSTRACT A content-based personalized recommendation system learns user specific profiles from user feedback so that it can deliver information tailored to each individual users interest.",
                "A system serving millions of users can learn a better user profile for a new user, or a user with little feedback, by borrowing information from other users through the use of a Bayesian hierarchical model.",
                "Learning the model parameters to optimize the joint data likelihood from millions of users is very computationally expensive.",
                "The commonly used EM algorithm converges very slowly due to the sparseness of the data in IR applications.",
                "This paper proposes a new fast learning technique to learn a large number of individual user profiles.",
                "The efficacy and efficiency of the proposed algorithm are justified by theory and demonstrated on actual user data from Netflix and MovieLens.",
                "Categories and Subject Descriptors: B.3.3 [Information Search and Retrieval]: Information filtering General Terms: Algorithms 1.",
                "INTRODUCTION Personalization is the future of the Web, and it has achieved great success in industrial applications.",
                "For example, online stores, such as Amazon and Netflix, provide customized recommendations for additional products or services based on a users history.",
                "Recent offerings such as My MSN, My Yahoo!, My Google, and Google News have attracted much attention due to their potential ability to infer a users interests from his/her history.",
                "One major personalization topic studied in the information retrieval community is content-based personal recommendation systems1 .",
                "These systems learn user-specific profiles from user feedback so that they can recommend information tailored to each individual users interest without requiring the user to make an explicit query.",
                "Learning the user profiles is the core problem for these systems.",
                "A user profile is usually a classifier that can identify whether a document is relevant to the user or not, or a regression model that tells how relevant a document is to the user.",
                "One major challenge of building a recommendation or personalization system is that the profile learned for a particular user is usually of low quality when the amount of data from that particular user is small.",
                "This is known as the cold start problem.",
                "This means that any new user must endure poor initial performance until sufficient feedback from that user is provided to learn a reliable user profile.",
                "There has been much research on improving classification accuracy when the amount of labeled training data is small.",
                "The semi-supervised learning approach combines unlabeled and labeled data together to achieve this goal [26].",
                "Another approach is using domain knowledge.",
                "Researchers have modified different learning algorithms, such as Na¨ıveBayes [17], logistic regression [7], and SVMs [22], to integrate domain knowledge into a text classifier.",
                "The third approach is borrowing training data from other resources [5][7].",
                "The effectiveness of these different approaches is mixed, due to how well the underlying model assumption fits the data.",
                "One well-received approach to improve recommendation system performance for a particular user is borrowing information from other users through a Bayesian hierarchical modeling approach.",
                "Several researchers have demonstrated that this approach effectively trades off between shared and user-specific information, thus alleviating poor initial performance for each user[27][25].",
                "In order to learn a Bayesian hierarchical model, the system usually tries to find the most likely model parameters for the given data.",
                "A mature recommendation system usually works for millions of users.",
                "It is well known that learning the optimal parameters of a Bayesian hierarchical model is computationally expensive when there are thousands or millions of users.",
                "The EM algorithm is a commonly used technique for <br>parameter</br> learning due to its simplicity and convergence guarantee.",
                "However, a content based recommendation system often handles documents in a very high dimensional space, in which each document is represented by a very sparse vector.",
                "With careful analysis of the EM algorithm in this scenario (Section 4), we find that the EM tering, or item-based collaborative filtering.",
                "In this paper, the words filtering and recommendation are used interchangeably. algorithm converges very slowly due to the sparseness of the input variables.",
                "We also find that updating the model <br>parameter</br> at each EM iteration is also expensive with computational complexity of O(MK), where M is the number of users and K is the number of dimensions.",
                "This paper modifies the standard EM algorithm to create an improved learning algorithm, which we call the Modified EM algorithm.",
                "The basic idea is that instead of calculating the numerical solution for all the user profile parameters, we derive the analytical solution of the parameters for some feature dimensions, and at the M step use the analytical solution instead of the numerical solution estimated at E step for those parameters.",
                "This greatly reduces the computation at a single EM iteration, and also has the benefit of increasing the convergence speed of the learning algorithm.",
                "The proposed technique is not only well supported by theory, but also by experimental results.",
                "The organization of the remaining parts of this paper is as follows: Section 3 describes the Bayesian hierarchical linear regression modeling framework used for content-based recommendations.",
                "Section 4 describes how to learn the model parameters using the standard EM algorithm, along with using the new technique proposed in this paper.",
                "The experimental setting and results used to validate the proposed learning technique are reported in Sections 5 and 6.",
                "Section 7 summarizes and offers concluding remarks. 2.",
                "RELATED WORK Providing personalized recommendations to users has been identified as a very important problem in the IR community since the 1970s.",
                "The approaches that have been used to solve this problem can be roughly classified into two major categories: content based filtering versus collaborative filtering.",
                "Content-based filtering studies the scenario where a recommendation system monitors a document stream and pushes documents that match a user profile to the corresponding user.",
                "The user may read the delivered documents and provide explicit relevance feedback, which the filtering system then uses to update the users profile using relevance feedback retrieval models (e.g.",
                "Boolean models, vector space models, traditional probabilistic models [20] , inference networks [3] and language models [6]) or machine learning algorithms (e.g.",
                "Support Vector Machines (SVM), K nearest neighbors (K-NN) clustering, neural networks, logistic regression, or Winnow [16] [4] [23]).",
                "Collaborative filtering goes beyond merely using document content to recommend items to a user by leveraging information from other users with similar tastes and preferences in the past.",
                "Memorybased heuristics and model based approaches have been used in collaborative filtering task [15] [8] [2] [14] [12] [11].",
                "This paper contributes to the content-based recommendation research by improving the efficiency and effectiveness of Bayesian hierarchical linear models, which have a strong theoretical basis and good empirical performance on recommendation tasks[27][25].",
                "This paper does not intend to compare content-based filtering with collaborative filtering or claim which one is a better.",
                "We think each complements the other, and that content-based filtering is extremely useful for handling new documents/items with little or no user feedback.",
                "Similar to some other researchers[18][1][21], we found that a recommendation system will be more effective when both techniques are combined.",
                "However, this is beyond the scope of this paper and thus not discussed here. 3.",
                "BAYESIAN HIERARCHICAL LINEAR REGRESSION Assume there are M users in the system.",
                "The task of the system is to recommend documents that are relevant to each user.",
                "For each user, the system learns a user model from the users history.",
                "In the rest of this paper, we will use the following notations to represent the variables in the system. m = 1, 2, ..., M: The index for each individual user.",
                "M is the total number of users. wm: The user model <br>parameter</br> associated with user m. wm is a K dimensional vector. j = 1, 2, ..., Jm: The index for a set of data for user m. Jm is the number of training data for user m. Dm = {(xm,j, ym,j)}: A set of data associated with user m. xm,j is a K dimensional vector that represents the mth users jth training document.2 ym,j is a scalar that represents the label of document xm,j. k = 1, 2, ..., K: The dimensional index of input variable x.",
                "The Bayesian hierarchical modeling approach has been widely used in real-world information retrieval applications.",
                "Generalized Bayesian hierarchical linear models, one of the simplest Bayesian hierarchical models, are commonly used and have achieved good performance on collaborative filtering [25] and content-based adaptive filtering [27] tasks.",
                "Figure 1 shows the graphical representation of a Bayesian hierarchical model.",
                "In this graph, each user model is represented by a random vector wm.",
                "We assume a user model is sampled randomly from a prior distribution P(w|Φ).",
                "The system can predict the user label y of a document x given an estimation of wm (or wms distribution) using a function y = f(x, w).",
                "The model is called generalized Bayesian hierarchical linear model when y = f(wT x) is any generalized linear model such as logistic regression, SVM, and linear regression.",
                "To reliably estimate the user model wm, the system can borrow information from other users through the prior Φ = (µ, Σ).",
                "Now we look at one commonly used model where y = wT x + , where ∼ N(0, σ2 ) is a random noise [25][27].",
                "Assume that each user model wm is an independent draw from a population distribution P(w|Φ), which is governed by some unknown hyperparameter Φ.",
                "Let the prior distribution of user model w be a Gaussian distribution with <br>parameter</br> Φ = (µ, Σ), which is the commonly used prior for linear models. µ = (µ1, µ2, ..., µK ) is a K dimensional vector that represents the mean of the Gaussian distribution, and Σ is the covariance matrix of the Gaussian.",
                "Usually, a Normal distribution N(0, aI) and an Inverse Wishart distribution P(Σ) ∝ |Σ|− 1 2 b exp(−1 2 ctr(Σ−1 )) are used as hyperprior to model the prior distribution of µ and Σ respectively.",
                "I is the K dimensional identity matrix, and a, b, and c are real numbers.",
                "With these settings, we have the following model for the system: 1. µ and Σ are sampled from N(0, aI) and IWν (aI), respectively. 2 The first dimension of x is a dummy variable that always equals to 1.",
                "Figure 1: Illustration of dependencies of variables in the hierarchical model.",
                "The rating, y, for a document, x, is conditioned on the document and the user model, wm, associated with the user m. Users share information about their models through the prior, Φ = (µ, Σ). 2.",
                "For each user m, wm is sampled randomly from a Normal distribution: wm ∼ N(µ, Σ2 ) 3.",
                "For each item xm,j, ym,j is sampled randomly from a Normal distribution: ym,j ∼ N(wT mxm,j, σ2 ).",
                "Let θ = (Φ, w1, w2, ..., wM ) represent the parameters of this system that needs to be estimated.",
                "The joint likelihood for all the variables in the probabilistic model, which includes the data and the parameters, is: P(D, θ) = P(Φ) m P(wm|Φ) j P(ym,j|xm,j, wm) (1) For simplicity, we assume a, b, c, and σ are provided to the system. 4.",
                "MODEL <br>parameter</br> LEARNING If the prior Φ is known, finding the optimal wm is straightforward: it is a simple linear regression.",
                "Therefore, we will focus on estimating Φ.",
                "The maximum a priori solution of Φ is given by ΦMAP = arg max Φ P(Φ|D) (2) = arg max Φ P(Φ, D) P(D) (3) = arg max Φ P(D|Φ)P(Φ) (4) = arg max Φ w P(D|w, Φ)P(w|Φ)P(Φ)dw (5) Finding the optimal solution for the above problem is challenging, since we need to integrate over all w = (w1, w2, ..., wM ), which are unobserved hidden variables. 4.1 EM Algorithm for Bayesian Hierarchical Linear Models In Equation 5, Φ is the <br>parameter</br> needs to be estimated, and the result depends on unobserved latent variables w. This kind of optimization problem is usually solved by the EM algorithm.",
                "Applying EM to the above problem, the set of user models w are the unobservable hidden variables and we have: Q = w P(w|µ, Σ2 , Dm) log P(µ, Σ2 , w, D)dw Based on the derivation of the EM formulas presented in [24], we have the following Expectation-Maximization steps for finding the optimal hyperparameters.",
                "For space considerations, we omit the derivation in this paper since it is not the focus of our work.",
                "E step: For each user m, estimate the user model distribution P(wm|Dm, Φ) = N(wm; ¯wm, Σ2 m) based on the current estimation of the prior Φ = (µ, Σ2 ). ¯wm = ((Σ2 )−1 + Sxx,m σ2 )−1 ( Sxy,m σ2 + (Σ2 )−1 µ) (6) Σ2 m = ((Σ2 )−1 + Sxx,m σ2 )−1 (7) where Sxx,m = j xm,jxT m,j Sxy,m = j xm,jym,j M step: Optimize the prior Φ = (µ, Σ2 ) based on the estimation from the last E step. µ = 1 M m ¯wm (8) Σ2 = 1 M m Σ2 m + ( ¯wm − µ)( ¯wm − µ)T (9) Many machine learning driven IR systems use a point estimate of the parameters at different stages in the system.",
                "However, we are estimating the posterior distribution of the variables at the E step.",
                "This avoids overfitting wm to a particular users data, which may be small and noisy.",
                "A detailed discussion about this subject appears in [10]. 4.2 New Algorithm: Modified EM Although the EM algorithm is widely studied and used in machine learning applications, using the above EM process to solve Bayesian hierarchical linear models in large-scale information retrieval systems is still too computationally expensive.",
                "In this section, we describe why the learning rate of the EM algorithm is slow in our application and introduce a new technique to make the learning of the Bayesian hierarchical linear model scalable.",
                "The derivation of the new learning algorithm will be based on the EM algorithm described in the previous section.",
                "First, the covariance matrices Σ2 , Σ2 m are usually too large to be computationally feasible.",
                "For simplicity, and as a common practice in IR, we do not model the correlation between features.",
                "Thus we approximate these matrices with K dimensional diagonal matrices.",
                "In the rest of the paper, we use these symbols to represent their diagonal approximations: Σ2 =     σ2 1 0 .. 0 0 σ2 2 .. 0 .. .. .. .. 0 0 .. σ2 K     Σ2 m =     σ2 m,1 0 .. 0 0 σ2 m,2 .. 0 .. .. .. .. 0 0 .. σ2 m,K     Secondly, and most importantly, the input space is very sparse and there are many dimensions that are not related to a particular user in a real IR application.",
                "For example, let us consider a movie recommendation system, with the input variable x representing a particular movie.",
                "For the jth movie that the user m has seen, let xm,j,k = 1 if the director of the movie is Jean-Pierre Jeunet (indexed by k).",
                "Here we assume that whether or not that this director directed a specific movie is represented by the kth dimension.",
                "If the user m has never seen a movie directed by Jean-Pierre Jeunet, then the corresponding dimension is always zero (xm,j,k = 0 for all j) .",
                "One major drawback of the EM algorithm is that the importance of a feature, µk, may be greatly dominated by users who have never encountered this feature (i.e. j xm,j,k = 0) at the M step (Equation 8).",
                "Assume that 100 out of 1 million users have viewed the movie directed by Jean-Pierre Jeunet, and that the viewers have rated all of his movies as excellent.",
                "Intuitively, he is a good director and the weight for him (µk) should be high.",
                "Before the EM iteration, the initial value of µ is usually set to 0.",
                "Since the other 999,900 users have not seen this movie, their corresponding weights (w1,k, w2,k, ..., wm,k..., w999900,k) for that director would be very small initially.",
                "Thus the corresponding weight of the director in the prior µk at the first M step would be very low , and the variance σm,k will be large (Equations 8 and 7).",
                "It is undesirable that users who have never seen any movie produced by the director influence the importance of the director so much.",
                "This makes the convergence of the standard EM algorithm very slow.",
                "Now lets look at whether we can improve the learning speed of the algorithm.",
                "Without a loss of generality, let us assume that the kth dimension of the input variable x is not related to a particular user m. By which we mean, xm,j,k = 0 for all j = 1, ..., Jm.",
                "It is straightforward to prove that the kth row and kth column of Sxx,m are completely filled with zeros, and that the kth dimension of Sxy,m is zeroed as well.",
                "Thus the corresponding kth dimension of the user models mean, ¯wm, should be equal to that of the prior: ¯wm,k = µk, with the corresponding covariance of σm,k = σk.",
                "At the M step, the standard EM algorithm uses the numerical solution of the distribution P(wm|Dm, Φ) estimated at E step (Equation 8 and Equation 7).",
                "However, the numerical solutions are very unreliable for ¯wm,k and σm,k when the kth dimension is not related to the mth user.",
                "A better approach is using the analytical solutions ¯wm,k = µk, and σm,k = σk for the unrelated (m, k) pairs, along with the numerical solution estimated at E step for the other (m, k) pairs.",
                "Thus we get the following new EM-like algorithm: Modified E step: For each user m, estimate the user model distribution P(wm|Dm, Φ) = N(wm; ¯wm, Σ2 m) based on the current estimation of σ , µ, Σ2 . ¯wm = ((Σ2 )−1 + Sxx,m σ2 )−1 ( Sxy,m σ2 + (Σ2 )−1 µ)(10) σ2 m,k = ((σ2 k)−1 + sxx,m,k σ2 )−1 (11) where sxx,m,k = j x2 m,j,k and sxy,m,k = j xm,j,kym,j Modified M Step Optimize the prior Φ = (µ, Σ2 ) based on the estimation from the last E step for related userfeature pairs.",
                "The M step implicitly uses the analytical solution for unrelated user-feature pairs. µk = 1 Mk m:related ¯wm,k (12) σ2 k = 1 Mk m:related σ2 m,k +( ¯wm,k − µk)( ¯wm,k − µk)T (13) where Mk is the number of users that are related to feature k We only estimate the diagonal of Σ2 m and Σ since we are using the diagonal approximation of the covariance matrices.",
                "To estimate ¯wm, we only need to calculate the numerical solutions for dimensions that are related to user m. To estimate σ2 k and µk, we only sum over users that are related to the kth feature.",
                "There are two major benefits of the new algorithm.",
                "First, because only the related (m, k) pairs are needed at the modified M step, the computational complexity in a single EM iteration is much smaller when the data is sparse, and many of (m, k) pairs are unrelated.",
                "Second, the parameters estimated at the modified M step (Equations 12 - 13) are more accurate than the standard M step described in Section 4.1 because the exact analytical solutions ¯wm,k = µk and σm,k = σk for the unrelated (m, k) pairs were used in the new algorithm instead of an approximate solution as in the standard algorithm. 5.",
                "EXPERIMENTAL METHODOLOGY 5.1 Evaluation Data Set To evaluate the proposed technique, we used the following three major data sets (Table 1): MovieLens Data: This data set was created by combining the relevance judgments from the MovieLens[9] data set with documents from the Internet Movie Database (IMDB).",
                "MovieLens allows users to rank how much he/she enjoyed a specific movie on a scale from 1 to 5.",
                "This likeability rating was used as a measurement of how relevant the document representing the corresponding movie is to the user.",
                "We considered documents with likeability scores of 4 or 5 as relevant, and documents with a score of 1 to 3 as irrelevant to the user.",
                "MovieLens provided relevance judgments on 3,057 documents from 6,040 separate users.",
                "On average, each user rated 151 movies, of these 87 were judged to be relevant.",
                "The average score for a document was 3.58.",
                "Documents representing each movie were constructed from the portion of the IMDB database that is available for public download[13].",
                "Based on this database, we created one document per movie that contained the relevant information about it (e.g. directors, actors, etc.).",
                "Table 1: Data Set Statistics.",
                "On Reuters, the number of rating for a simulated user is the number of documents relevant to the corresponding topic.",
                "Data Users Docs Ratings per User MovieLens 6,040 3,057 151 Netflix-all 480,189 17,770 208 Netflix-1000 1000 17,770 127 Reuters-C 34 100,000 3949 Reuters-E 26 100,000 1632 Reuters-G 33 100,000 2222 Reuters-M 10 100,000 6529 Netflix Data: This data set was constructed by combining documents about movies crawled from the web with a set of actual movie rental customer relevance judgments from Netflix[19].",
                "Netflix publicly provides the relevance judgments of 480,189 anonymous customers.",
                "There are around 100 million rating on a scale of 1 to 5 for 17,770 documents.",
                "Similar to MovieLens, we considered documents with likeability scores of 4 or 5 as relevant.",
                "This number was reduced to 1000 customers through random sampling.",
                "The average customer on the reduced data set provided 127 judgments, with 70 being deemed relevant.",
                "The average score for documents is 3.55.",
                "Reuters Data: This is the Reuters Corpus, Volume 1.",
                "It covers 810,000 Reuters English language news stories from August 20, 1996 to August 19, 1997.",
                "Only the first 100,000 news were used in our experiments.",
                "The Reuters corpus comes with a topic hierarchy.",
                "Each document is assigned to one of several locations on the hierarchical tree.",
                "The first level of the tree contains four topics, denoted as C, E, M, and G. For the experiments in this paper, the tree was cut at level 1 to create four smaller trees, each of which corresponds to one smaller data set: Reuters-E Reuters-C, ReutersM and Reuters-G. For each small data set, we created several profiles, one profile for each node in a sub-tree, to simulate multiple users, each with a related, yet separate definition of relevance.",
                "All the user profiles on a sub-tree are supposed to share the same prior model distribution.",
                "Since this corpus explicitly indicates only the relevant documents for a topic(user), all other documents are considered irrelevant. 5.2 Evaluation We designed the experiments to answer the following three questions: 1.",
                "Do we need to take the effort to use a Bayesian approach and learn a prior from other users? 2.",
                "Does the new algorithm work better than the standard EM algorithm for learning the Bayesian hierarchical linear model? 3.",
                "Can the new algorithm quickly learn many user models?",
                "To answer the first question, we compared the Bayesian hierarchical models with commonly used Norm-2 regularized linear regression models.",
                "In fact, the commonly used approach is equivalent to the model learned at the end of the first EM iteration.",
                "To answer the second question, we compared the proposed new algorithm with the standard EM algorithm to see whether the new learning algorithm is better.",
                "To answer the third question, we tested the efficiency of the new algorithm on the entire Netflix data set where about half a million user models need to be learned together.",
                "For the MovieLens and Netflix data sets, algorithm effectiveness was measured by mean square error, while on the Reuters data set classification error was used because it was more informative.",
                "We first evaluated the performance on each individual user, and then estimated the macro average over all users.",
                "Statistical tests (t-tests) were carried out to see whether the results are significant.",
                "For the experiments on the MovieLens and Netflix data sets, we used a random sample of 90% of each user for training, and the rest for testing.",
                "On Reuters data set, because there are too many relevant documents for each topic in the corpus, we used a random sample of 10% of each topic for training, and 10% of the remaining documents for testing.",
                "For all runs, we set (a, b, c, Σ ) = (0.1, 10, 0.1, 1) manually. 6.",
                "EXPERIMENTAL RESULTS Figure 2, Figure 3, and Figure 4 show that on all data sets, the Bayesian hierarchical modeling approach has a statistical significant improvement over the regularized linear regression model, which is equivalent to the Bayesian hierarchical models learned at the first iteration.",
                "Further analysis shows a negative correlation between the number of training data for a user and the improvement the system gets.",
                "This suggests that the borrowing information from other users has more significant improvements for users with less training data, which is as expected.",
                "However, the strength of the correlation differs over data sets, and the amount of training data is not the only characteristics that will influence the final performance.",
                "Figure 2 and Figure 3 show that the proposed new algorithm works better than the standard EM algorithm on the Netflix and MovieLens data sets.",
                "This is not surprising since the number of related feature-users pairs is much smaller than the number of unrelated feature-user pairs on these two data sets, and thus the proposed new algorithm is expected to work better.",
                "Figure 4 shows that the two algorithms work similarly on the Reuters-E data set.",
                "The accuracy of the new algorithm is similar to that of the standard EM algorithm at each iteration.",
                "The general patterns are very similar on other Reuters subsets.",
                "Further analysis shows that only 58% of the user-feature pairs are unrelated on this data set.",
                "Since the number of unrelated user-feature pairs is not extremely large, the sparseness is not a serious problem on the Reuters data set.",
                "Thus the two learning algorithms perform similarly.",
                "The results suggest that only on a corpus where the number of unrelated user-feature pairs is much larger than the number of related pairs, such as on the Netflix data set, the proposed technique will get a significant improvement over standard EM.",
                "However, the experiments also show that when the assumption does not hold, the new algorithm does not hurt performance.",
                "Although the proposed technique is faster than standard Figure 2: Performance on a Netflix subset with 1,000 users.",
                "The new algorithm is statistical significantly better than EM algorithm at iterations 2 - 10.",
                "Norm-2 regularized linear models are equivalent to the Bayesian hierarchical models learned at the first iteration, and are statistical significantly worse than the Bayesian hierarchical models. 0 2 4 6 8 10 1 1.05 1.1 1.15 1.2 1.25 1.3 1.35 1.4 Iterations MeanSquareError New Algorithm Traditional EM 1 2 3 4 5 6 7 8 9 10 0.33 0.34 0.35 0.36 0.37 0.38 0.39 Iterations ClassificationError New Algorithm Traditional EM Figure 3: Performance on a MovieLens subset with 1,000 users.",
                "The new algorithm is statistical significantly better than EM algorithm at iteration 2 to 17 (evaluated with mean square error). 1 6 11 16 21 0.5 1 1.5 2 2.5 3 3.5 Iterations MeanSquareError New Algorithm Traditional EM 1 6 11 16 21 0.35 0.4 0.45 0.5 0.55 0.6 0.65 Iterations ClassificationError New Algorithm Traditional EM Figure 4: Performance on a Reuters-E subset with 26 profiles.",
                "Performances on Reuters-C, Reuters-M, Reuters-G are similar. 1 2 3 4 5 0.011 0.0115 0.012 0.0125 0.013 0.0135 0.014 Iterations MeanSquareError New Algorithm Traditional EM 1 2 3 4 5 0.0102 0.0104 0.0106 0.0108 0.011 0.0112 0.0114 Iterations ClassificationError New Algorithm Traditional EM EM, can it really learn millions of user models quickly?",
                "Our results show that the modified EM algorithm converges quickly, and 2 - 3 modified EM iterations would result in a reliable estimation.",
                "We evaluated the algorithm on the whole Netflix data set (480,189 users, 159,836 features, and 100 million ratings) running on a single CPU PC (2GB memory, P4 3GHz).",
                "The system finished one modified EM iteration in about 4 hours.",
                "This demonstrates that the proposed technique can efficiently handle large-scale system like Netflix. 7.",
                "CONCLUSION Content-based user profile learning is an important problem and is the key to providing personal recommendations to a user, especially for recommending new items with a small number of ratings.",
                "The Bayesian hierarchical modeling approach is becoming an important user profile learning approach due to its theoretically justified ability to help one user through information transfer from the other users by way of hyperpriors.",
                "This paper examined the weakness of the popular EM based learning approach for Bayesian hierarchical linear models and proposed a better learning technique called Modified EM.",
                "We showed that the new technique is theoretically more computationally efficient than the standard EM algorithm.",
                "Evaluation on the MovieLens and Netflix data sets demonstrated the effectiveness of the new technique when the data is sparse, by which we mean the ratio of related user-feature pairs to unrelated pairs is small.",
                "Evaluation on the Reuters data set showed that the new technique performed similar to the standard EM algorithm when the sparseness condition does not hold.",
                "In general, it is better to use the new algorithm since it is as simple as standard EM, the performance is either better or similar to EM, and the computation complexity is lower at each iteration.",
                "It is worth mentioning that even if the original problem space is not sparse, sparseness can be created artificially when a recommendation system uses user-specific feature selection techniques to reduce the noise and user model complexity.",
                "The proposed technique can also be adapted to improve the learning in such a scenario.",
                "We also demonstrated that the proposed technique can learn half a million user profiles from 100 million ratings in a few hours with a single CPU.",
                "The research is important because scalability is a major concern for researchers when using the Bayesian hierarchical linear modeling approach to build a practical large scale system, even though the literature have demonstrated the effectiveness of the models in many applications.",
                "Our work is one major step on the road to make Bayesian hierarchical linear models more practical.",
                "The proposed new technique can be easily adapted to run on a cluster of machines, and thus further speed up the learning process to handle a larger scale system with hundreds of millions of users.",
                "The research has much potential to benefit people using EM algorithm on many other IR problems as well as machine learning problems.",
                "EM algorithm is a commonly used machine learning technique.",
                "It is used to find model parameters in many IR problems where the training data is very sparse.",
                "Although we are focusing on the Bayesian hierarchical linear models for recommendation and filtering, the new idea of using analytical solution instead of numerical solution for unrelated user-feature pairs at the M step could be adapted to many other problems. 8.",
                "ACKNOWLEDGMENTS We thank Wei Xu, David Lewis and anonymous reviewers for valuable feedback on the work described in this paper.",
                "Part of the work was supported by Yahoo, Google, the Petascale Data Storage Institute and the Institute for Scalable Scientific Data Management.",
                "Any opinions, findings, conclusions, or recommendations expressed in this material are those of the authors, and do not necessarily reflect those of the sponsors. 9.",
                "REFERENCES [1] C. Basu, H. Hirsh, and W. Cohen.",
                "Recommendation as classification: Using social and content-based information in recommendation.",
                "In Proceedings of the Fifteenth National Conference on Artificial Intelligence, 1998. [2] J. S. Breese, D. Heckerman, and C. Kadie.",
                "Empirical analysis of predictive algorithms for collaborative filtering.",
                "Technical report, Microsoft Research, One Microsoft Way, Redmond, WA 98052, 1998. [3] J. Callan.",
                "Document filtering with inference networks.",
                "In Proceedings of the Nineteenth Annual International ACM SIGIR Conference on Research and Development in Information Retrieval, pages 262-269, 1996. [4] N. Cancedda, N. Cesa-Bianchi, A. Conconi, C. Gentile, C. Goutte, T. Graepel, Y. Li, J. M. Renders, J. S. Taylor, and A. Vinokourov.",
                "Kernel method for document filtering.",
                "In The Eleventh Text REtrieval Conference (TREC11).",
                "National Institute of Standards and Technology, special publication 500-249, 2003. [5] C. Chelba and A. Acero.",
                "Adaptation of maximum entropy capitalizer: Little data can help a lot.",
                "In D. Lin and D. Wu, editors, Proceedings of EMNLP 2004, pages 285-292, Barcelona, Spain, July 2004.",
                "Association for Computational Linguistics. [6] B. Croft and J. Lafferty, editors.",
                "Language Modeling for Information Retrieval.",
                "Kluwer, 2002. [7] A. Dayanik, D. D. Lewis, D. Madigan, V. Menkov, and A. Genkin.",
                "Constructing informative prior distributions from domain knowledge in text classification.",
                "In SIGIR 06: Proceedings of the 29th annual international ACM SIGIR conference on Research and development in information retrieval, pages 493-500, New York, NY, USA, 2006.",
                "ACM Press. [8] J. Delgado and N. Ishii.",
                "Memory-based weightedmajority prediction for recommender systems.",
                "In ACM SIGIR99 Workshop on Recommender Systems, 1999. [9] GroupLens.",
                "Movielens. http://www.grouplens.org/taxonomy/term/14, 2006. [10] D. Heckerman.",
                "A tutorial on learning with bayesian networks.",
                "In M. Jordan, editor, Learning in Graphical Models.",
                "Kluwer Academic, 1998. [11] J. L. Herlocker, J.",
                "A. Konstan, A. Borchers, and J. Riedl.",
                "An algorithmic framework for performing collaborative filtering.",
                "In SIGIR 99: Proceedings of the 22nd annual international ACM SIGIR conference on Research and development in information retrieval, pages 230-237, New York, NY, USA, 1999.",
                "ACM Press. [12] T. Hofmann and J. Puzicha.",
                "Latent class models for collaborative filtering.",
                "In IJCAI 99: Proceedings of the Sixteenth International Joint Conference on Artificial Intelligence, pages 688-693, San Francisco, CA, USA, 1999.",
                "Morgan Kaufmann Publishers Inc. [13] I. M. D. (IMDB).",
                "Internet movie database. http://www.imdb.com/interfaces/, 2006. [14] R. Jin, J. Y. Chai, and L. Si.",
                "An automatic weighting scheme for collaborative filtering.",
                "In SIGIR 04: Proceedings of the 27th annual international ACM SIGIR conference on Research and development in information retrieval, pages 337-344, New York, NY, USA, 2004.",
                "ACM Press. [15] J.",
                "A. Konstan, B. N. Miller, D. Maltz, J. L. Herlocker, L. R. Gordon, and J. Riedl.",
                "GroupLens: Applying collaborative filtering to Usenet news.",
                "Communications of the ACM, 40(3):77-87, 1997. [16] D. Lewis.",
                "Applying support vector machines to the TREC-2001 batch filtering and routing tasks.",
                "In Proceedings of the Eleventh Text REtrieval Conference (TREC-11), 2002. [17] B. Liu, X. Li, W. S. Lee, , and P. Yu.",
                "Text classification by labeling words.",
                "In Proceedings of The Nineteenth National Conference on Artificial Intelligence (AAAI-2004), July 25-29, 2004. [18] P. Melville, R. J. Mooney, and R. Nagarajan.",
                "Content-boosted collaborative filtering for improved recommendations.",
                "In Proceedings of the Eighteenth National Conference on Artificial Intelligence (AAAI-2002), Edmonton, Canada, 2002. [19] Netflix.",
                "Netflix prize. http://www.netflixprize.com (visited on Nov. 30, 2006), 2006. [20] S. Robertson and K. Sparck-Jones.",
                "Relevance weighting of search terms.",
                "In Journal of the American Society for Information Science, volume 27, pages 129-146, 1976. [21] J. Wang, A. P. de Vries, and M. J. T. Reinders.",
                "Unifying user-based and item-based collaborative filtering approaches by similarity fusion.",
                "In SIGIR 06: Proceedings of the 29th annual international ACM SIGIR conference on Research and development in information retrieval, pages 501-508, New York, NY, USA, 2006.",
                "ACM Press. [22] X. Wu and R. K. Srihari.",
                "Incorporating prior knowledge with weighted margin support vector machines.",
                "In Proc.",
                "ACM Knowledge Discovery Data Mining Conf. (ACM SIGKDD 2004), Aug. 2004. [23] Y. Yang, S. Yoo, J. Zhang, and B. Kisiel.",
                "Robustness of adaptive filtering methods in a cross-benchmark evaluation.",
                "In Proceedings of the 28th Annual International ACM SIGIR Conference on Research and Development in Information Retrieval, 2005. [24] K. Yu, V. Tresp, and A. Schwaighofer.",
                "Learning gaussian processes from multiple tasks.",
                "In ICML 05: Proceedings of the 22nd international conference on Machine learning, pages 1012-1019, New York, NY, USA, 2005.",
                "ACM Press. [25] K. Yu, V. Tresp, and S. Yu.",
                "A nonparametric hierarchical bayesian framework for information filtering.",
                "In SIGIR 04: Proceedings of the 27th annual international ACM SIGIR conference on Research and development in information retrieval, pages 353-360.",
                "ACM Press, 2004. [26] X. Zhu.",
                "Semi-supervised learning literature survey.",
                "Technical report, University of Wisconsin - Madison, December 9, 2006. [27] P. Zigoris and Y. Zhang.",
                "Bayesian adaptive user profiling with explicit & implicit feedback.",
                "In Conference on Information and Knowledge Mangement 2006, 2006."
            ],
            "original_annotated_samples": [],
            "translated_annotated_samples": [
                "El algoritmo EM es una técnica de uso común para el aprendizaje de \"parámetros\" debido a su garantía de simplicidad y convergencia.",
                "También encontramos que actualizar el \"parámetro\" del modelo en cada iteración EM también es costosa con la complejidad computacional de O (MK), donde M es el número de usuarios y K es el número de dimensiones.",
                "M es el número total de usuarios.WM: El \"parámetro\" del modelo de usuario asociado con el usuario m.WM es un vector dimensional K.J = 1, 2, ..., JM: el índice para un conjunto de datos para el usuario m.JM es el número de datos de capacitación para el usuario m.Dm = {(xm, j, ym, j)}: un conjunto de datos asociados con el usuario m.XM, J es un vector dimensional K que representa el MTH Usuarios JTH Training Document.2 ym, J es un escalar que representa la etiqueta del documento XM, j.k = 1, 2, ..., k: el índice dimensional de la variable de entrada x.",
                "Deje que la distribución previa del modelo de usuario W sea una distribución gaussiana con \"parámetro\" φ = (µ, σ), que es el previo comúnmente utilizado para modelos lineales.µ = (µ1, µ2, ..., µK) es un vector k dimensional que representa la media de la distribución gaussiana, y σ es la matriz de covarianza del gaussiano.",
                "Aprendizaje del modelo \"parámetro\" Si se conoce el φ anterior, encontrar el WM óptimo es sencillo: es una regresión lineal simple.",
                "La solución a priori máxima de φ viene dada por φmap = arg max φ p (φ | d) (2) = arg max φ p (φ, d) p (d) (3) = arg max φ p (d | φ) P (φ) (4) = arg max φ w p (d | w, φ) p (w | φ) p (φ) dw (5) Encontrar la solución óptima para el problema anterior es un desafío, ya que necesitamosIntegrar sobre todo W = (W1, W2, ..., WM), que son variables ocultas no observadas.4.1 Algoritmo EM para modelos lineales jerárquicos bayesianos en la ecuación 5, φ es el \"parámetro\" que debe estimarse, y el resultado depende de variables latentes no observadas w.Este tipo de problema de optimización generalmente se resuelve mediante el algoritmo EM."
            ],
            "translated_text": "",
            "candidates": [
                "parámetro",
                "parámetros",
                "parámetro",
                "parámetro",
                "parámetro",
                "parámetro",
                "parámetro",
                "parámetro",
                "parámetro",
                "parámetro",
                "parámetro",
                "parámetro"
            ],
            "error": []
        },
        "learning technique": {
            "translated_key": "técnica de aprendizaje",
            "is_in_text": true,
            "original_annotated_sentences": [
                "Efficient Bayesian Hierarchical User Modeling for Recommendation Systems Yi Zhang, Jonathan Koren School of Engineering University of California Santa Cruz Santa Cruz, CA, USA {yiz, jonathan}@soe.ucsc.edu ABSTRACT A content-based personalized recommendation system learns user specific profiles from user feedback so that it can deliver information tailored to each individual users interest.",
                "A system serving millions of users can learn a better user profile for a new user, or a user with little feedback, by borrowing information from other users through the use of a Bayesian hierarchical model.",
                "Learning the model parameters to optimize the joint data likelihood from millions of users is very computationally expensive.",
                "The commonly used EM algorithm converges very slowly due to the sparseness of the data in IR applications.",
                "This paper proposes a new fast <br>learning technique</br> to learn a large number of individual user profiles.",
                "The efficacy and efficiency of the proposed algorithm are justified by theory and demonstrated on actual user data from Netflix and MovieLens.",
                "Categories and Subject Descriptors: B.3.3 [Information Search and Retrieval]: Information filtering General Terms: Algorithms 1.",
                "INTRODUCTION Personalization is the future of the Web, and it has achieved great success in industrial applications.",
                "For example, online stores, such as Amazon and Netflix, provide customized recommendations for additional products or services based on a users history.",
                "Recent offerings such as My MSN, My Yahoo!, My Google, and Google News have attracted much attention due to their potential ability to infer a users interests from his/her history.",
                "One major personalization topic studied in the information retrieval community is content-based personal recommendation systems1 .",
                "These systems learn user-specific profiles from user feedback so that they can recommend information tailored to each individual users interest without requiring the user to make an explicit query.",
                "Learning the user profiles is the core problem for these systems.",
                "A user profile is usually a classifier that can identify whether a document is relevant to the user or not, or a regression model that tells how relevant a document is to the user.",
                "One major challenge of building a recommendation or personalization system is that the profile learned for a particular user is usually of low quality when the amount of data from that particular user is small.",
                "This is known as the cold start problem.",
                "This means that any new user must endure poor initial performance until sufficient feedback from that user is provided to learn a reliable user profile.",
                "There has been much research on improving classification accuracy when the amount of labeled training data is small.",
                "The semi-supervised learning approach combines unlabeled and labeled data together to achieve this goal [26].",
                "Another approach is using domain knowledge.",
                "Researchers have modified different learning algorithms, such as Na¨ıveBayes [17], logistic regression [7], and SVMs [22], to integrate domain knowledge into a text classifier.",
                "The third approach is borrowing training data from other resources [5][7].",
                "The effectiveness of these different approaches is mixed, due to how well the underlying model assumption fits the data.",
                "One well-received approach to improve recommendation system performance for a particular user is borrowing information from other users through a Bayesian hierarchical modeling approach.",
                "Several researchers have demonstrated that this approach effectively trades off between shared and user-specific information, thus alleviating poor initial performance for each user[27][25].",
                "In order to learn a Bayesian hierarchical model, the system usually tries to find the most likely model parameters for the given data.",
                "A mature recommendation system usually works for millions of users.",
                "It is well known that learning the optimal parameters of a Bayesian hierarchical model is computationally expensive when there are thousands or millions of users.",
                "The EM algorithm is a commonly used technique for parameter learning due to its simplicity and convergence guarantee.",
                "However, a content based recommendation system often handles documents in a very high dimensional space, in which each document is represented by a very sparse vector.",
                "With careful analysis of the EM algorithm in this scenario (Section 4), we find that the EM tering, or item-based collaborative filtering.",
                "In this paper, the words filtering and recommendation are used interchangeably. algorithm converges very slowly due to the sparseness of the input variables.",
                "We also find that updating the model parameter at each EM iteration is also expensive with computational complexity of O(MK), where M is the number of users and K is the number of dimensions.",
                "This paper modifies the standard EM algorithm to create an improved learning algorithm, which we call the Modified EM algorithm.",
                "The basic idea is that instead of calculating the numerical solution for all the user profile parameters, we derive the analytical solution of the parameters for some feature dimensions, and at the M step use the analytical solution instead of the numerical solution estimated at E step for those parameters.",
                "This greatly reduces the computation at a single EM iteration, and also has the benefit of increasing the convergence speed of the learning algorithm.",
                "The proposed technique is not only well supported by theory, but also by experimental results.",
                "The organization of the remaining parts of this paper is as follows: Section 3 describes the Bayesian hierarchical linear regression modeling framework used for content-based recommendations.",
                "Section 4 describes how to learn the model parameters using the standard EM algorithm, along with using the new technique proposed in this paper.",
                "The experimental setting and results used to validate the proposed <br>learning technique</br> are reported in Sections 5 and 6.",
                "Section 7 summarizes and offers concluding remarks. 2.",
                "RELATED WORK Providing personalized recommendations to users has been identified as a very important problem in the IR community since the 1970s.",
                "The approaches that have been used to solve this problem can be roughly classified into two major categories: content based filtering versus collaborative filtering.",
                "Content-based filtering studies the scenario where a recommendation system monitors a document stream and pushes documents that match a user profile to the corresponding user.",
                "The user may read the delivered documents and provide explicit relevance feedback, which the filtering system then uses to update the users profile using relevance feedback retrieval models (e.g.",
                "Boolean models, vector space models, traditional probabilistic models [20] , inference networks [3] and language models [6]) or machine learning algorithms (e.g.",
                "Support Vector Machines (SVM), K nearest neighbors (K-NN) clustering, neural networks, logistic regression, or Winnow [16] [4] [23]).",
                "Collaborative filtering goes beyond merely using document content to recommend items to a user by leveraging information from other users with similar tastes and preferences in the past.",
                "Memorybased heuristics and model based approaches have been used in collaborative filtering task [15] [8] [2] [14] [12] [11].",
                "This paper contributes to the content-based recommendation research by improving the efficiency and effectiveness of Bayesian hierarchical linear models, which have a strong theoretical basis and good empirical performance on recommendation tasks[27][25].",
                "This paper does not intend to compare content-based filtering with collaborative filtering or claim which one is a better.",
                "We think each complements the other, and that content-based filtering is extremely useful for handling new documents/items with little or no user feedback.",
                "Similar to some other researchers[18][1][21], we found that a recommendation system will be more effective when both techniques are combined.",
                "However, this is beyond the scope of this paper and thus not discussed here. 3.",
                "BAYESIAN HIERARCHICAL LINEAR REGRESSION Assume there are M users in the system.",
                "The task of the system is to recommend documents that are relevant to each user.",
                "For each user, the system learns a user model from the users history.",
                "In the rest of this paper, we will use the following notations to represent the variables in the system. m = 1, 2, ..., M: The index for each individual user.",
                "M is the total number of users. wm: The user model parameter associated with user m. wm is a K dimensional vector. j = 1, 2, ..., Jm: The index for a set of data for user m. Jm is the number of training data for user m. Dm = {(xm,j, ym,j)}: A set of data associated with user m. xm,j is a K dimensional vector that represents the mth users jth training document.2 ym,j is a scalar that represents the label of document xm,j. k = 1, 2, ..., K: The dimensional index of input variable x.",
                "The Bayesian hierarchical modeling approach has been widely used in real-world information retrieval applications.",
                "Generalized Bayesian hierarchical linear models, one of the simplest Bayesian hierarchical models, are commonly used and have achieved good performance on collaborative filtering [25] and content-based adaptive filtering [27] tasks.",
                "Figure 1 shows the graphical representation of a Bayesian hierarchical model.",
                "In this graph, each user model is represented by a random vector wm.",
                "We assume a user model is sampled randomly from a prior distribution P(w|Φ).",
                "The system can predict the user label y of a document x given an estimation of wm (or wms distribution) using a function y = f(x, w).",
                "The model is called generalized Bayesian hierarchical linear model when y = f(wT x) is any generalized linear model such as logistic regression, SVM, and linear regression.",
                "To reliably estimate the user model wm, the system can borrow information from other users through the prior Φ = (µ, Σ).",
                "Now we look at one commonly used model where y = wT x + , where ∼ N(0, σ2 ) is a random noise [25][27].",
                "Assume that each user model wm is an independent draw from a population distribution P(w|Φ), which is governed by some unknown hyperparameter Φ.",
                "Let the prior distribution of user model w be a Gaussian distribution with parameter Φ = (µ, Σ), which is the commonly used prior for linear models. µ = (µ1, µ2, ..., µK ) is a K dimensional vector that represents the mean of the Gaussian distribution, and Σ is the covariance matrix of the Gaussian.",
                "Usually, a Normal distribution N(0, aI) and an Inverse Wishart distribution P(Σ) ∝ |Σ|− 1 2 b exp(−1 2 ctr(Σ−1 )) are used as hyperprior to model the prior distribution of µ and Σ respectively.",
                "I is the K dimensional identity matrix, and a, b, and c are real numbers.",
                "With these settings, we have the following model for the system: 1. µ and Σ are sampled from N(0, aI) and IWν (aI), respectively. 2 The first dimension of x is a dummy variable that always equals to 1.",
                "Figure 1: Illustration of dependencies of variables in the hierarchical model.",
                "The rating, y, for a document, x, is conditioned on the document and the user model, wm, associated with the user m. Users share information about their models through the prior, Φ = (µ, Σ). 2.",
                "For each user m, wm is sampled randomly from a Normal distribution: wm ∼ N(µ, Σ2 ) 3.",
                "For each item xm,j, ym,j is sampled randomly from a Normal distribution: ym,j ∼ N(wT mxm,j, σ2 ).",
                "Let θ = (Φ, w1, w2, ..., wM ) represent the parameters of this system that needs to be estimated.",
                "The joint likelihood for all the variables in the probabilistic model, which includes the data and the parameters, is: P(D, θ) = P(Φ) m P(wm|Φ) j P(ym,j|xm,j, wm) (1) For simplicity, we assume a, b, c, and σ are provided to the system. 4.",
                "MODEL PARAMETER LEARNING If the prior Φ is known, finding the optimal wm is straightforward: it is a simple linear regression.",
                "Therefore, we will focus on estimating Φ.",
                "The maximum a priori solution of Φ is given by ΦMAP = arg max Φ P(Φ|D) (2) = arg max Φ P(Φ, D) P(D) (3) = arg max Φ P(D|Φ)P(Φ) (4) = arg max Φ w P(D|w, Φ)P(w|Φ)P(Φ)dw (5) Finding the optimal solution for the above problem is challenging, since we need to integrate over all w = (w1, w2, ..., wM ), which are unobserved hidden variables. 4.1 EM Algorithm for Bayesian Hierarchical Linear Models In Equation 5, Φ is the parameter needs to be estimated, and the result depends on unobserved latent variables w. This kind of optimization problem is usually solved by the EM algorithm.",
                "Applying EM to the above problem, the set of user models w are the unobservable hidden variables and we have: Q = w P(w|µ, Σ2 , Dm) log P(µ, Σ2 , w, D)dw Based on the derivation of the EM formulas presented in [24], we have the following Expectation-Maximization steps for finding the optimal hyperparameters.",
                "For space considerations, we omit the derivation in this paper since it is not the focus of our work.",
                "E step: For each user m, estimate the user model distribution P(wm|Dm, Φ) = N(wm; ¯wm, Σ2 m) based on the current estimation of the prior Φ = (µ, Σ2 ). ¯wm = ((Σ2 )−1 + Sxx,m σ2 )−1 ( Sxy,m σ2 + (Σ2 )−1 µ) (6) Σ2 m = ((Σ2 )−1 + Sxx,m σ2 )−1 (7) where Sxx,m = j xm,jxT m,j Sxy,m = j xm,jym,j M step: Optimize the prior Φ = (µ, Σ2 ) based on the estimation from the last E step. µ = 1 M m ¯wm (8) Σ2 = 1 M m Σ2 m + ( ¯wm − µ)( ¯wm − µ)T (9) Many machine learning driven IR systems use a point estimate of the parameters at different stages in the system.",
                "However, we are estimating the posterior distribution of the variables at the E step.",
                "This avoids overfitting wm to a particular users data, which may be small and noisy.",
                "A detailed discussion about this subject appears in [10]. 4.2 New Algorithm: Modified EM Although the EM algorithm is widely studied and used in machine learning applications, using the above EM process to solve Bayesian hierarchical linear models in large-scale information retrieval systems is still too computationally expensive.",
                "In this section, we describe why the learning rate of the EM algorithm is slow in our application and introduce a new technique to make the learning of the Bayesian hierarchical linear model scalable.",
                "The derivation of the new learning algorithm will be based on the EM algorithm described in the previous section.",
                "First, the covariance matrices Σ2 , Σ2 m are usually too large to be computationally feasible.",
                "For simplicity, and as a common practice in IR, we do not model the correlation between features.",
                "Thus we approximate these matrices with K dimensional diagonal matrices.",
                "In the rest of the paper, we use these symbols to represent their diagonal approximations: Σ2 =     σ2 1 0 .. 0 0 σ2 2 .. 0 .. .. .. .. 0 0 .. σ2 K     Σ2 m =     σ2 m,1 0 .. 0 0 σ2 m,2 .. 0 .. .. .. .. 0 0 .. σ2 m,K     Secondly, and most importantly, the input space is very sparse and there are many dimensions that are not related to a particular user in a real IR application.",
                "For example, let us consider a movie recommendation system, with the input variable x representing a particular movie.",
                "For the jth movie that the user m has seen, let xm,j,k = 1 if the director of the movie is Jean-Pierre Jeunet (indexed by k).",
                "Here we assume that whether or not that this director directed a specific movie is represented by the kth dimension.",
                "If the user m has never seen a movie directed by Jean-Pierre Jeunet, then the corresponding dimension is always zero (xm,j,k = 0 for all j) .",
                "One major drawback of the EM algorithm is that the importance of a feature, µk, may be greatly dominated by users who have never encountered this feature (i.e. j xm,j,k = 0) at the M step (Equation 8).",
                "Assume that 100 out of 1 million users have viewed the movie directed by Jean-Pierre Jeunet, and that the viewers have rated all of his movies as excellent.",
                "Intuitively, he is a good director and the weight for him (µk) should be high.",
                "Before the EM iteration, the initial value of µ is usually set to 0.",
                "Since the other 999,900 users have not seen this movie, their corresponding weights (w1,k, w2,k, ..., wm,k..., w999900,k) for that director would be very small initially.",
                "Thus the corresponding weight of the director in the prior µk at the first M step would be very low , and the variance σm,k will be large (Equations 8 and 7).",
                "It is undesirable that users who have never seen any movie produced by the director influence the importance of the director so much.",
                "This makes the convergence of the standard EM algorithm very slow.",
                "Now lets look at whether we can improve the learning speed of the algorithm.",
                "Without a loss of generality, let us assume that the kth dimension of the input variable x is not related to a particular user m. By which we mean, xm,j,k = 0 for all j = 1, ..., Jm.",
                "It is straightforward to prove that the kth row and kth column of Sxx,m are completely filled with zeros, and that the kth dimension of Sxy,m is zeroed as well.",
                "Thus the corresponding kth dimension of the user models mean, ¯wm, should be equal to that of the prior: ¯wm,k = µk, with the corresponding covariance of σm,k = σk.",
                "At the M step, the standard EM algorithm uses the numerical solution of the distribution P(wm|Dm, Φ) estimated at E step (Equation 8 and Equation 7).",
                "However, the numerical solutions are very unreliable for ¯wm,k and σm,k when the kth dimension is not related to the mth user.",
                "A better approach is using the analytical solutions ¯wm,k = µk, and σm,k = σk for the unrelated (m, k) pairs, along with the numerical solution estimated at E step for the other (m, k) pairs.",
                "Thus we get the following new EM-like algorithm: Modified E step: For each user m, estimate the user model distribution P(wm|Dm, Φ) = N(wm; ¯wm, Σ2 m) based on the current estimation of σ , µ, Σ2 . ¯wm = ((Σ2 )−1 + Sxx,m σ2 )−1 ( Sxy,m σ2 + (Σ2 )−1 µ)(10) σ2 m,k = ((σ2 k)−1 + sxx,m,k σ2 )−1 (11) where sxx,m,k = j x2 m,j,k and sxy,m,k = j xm,j,kym,j Modified M Step Optimize the prior Φ = (µ, Σ2 ) based on the estimation from the last E step for related userfeature pairs.",
                "The M step implicitly uses the analytical solution for unrelated user-feature pairs. µk = 1 Mk m:related ¯wm,k (12) σ2 k = 1 Mk m:related σ2 m,k +( ¯wm,k − µk)( ¯wm,k − µk)T (13) where Mk is the number of users that are related to feature k We only estimate the diagonal of Σ2 m and Σ since we are using the diagonal approximation of the covariance matrices.",
                "To estimate ¯wm, we only need to calculate the numerical solutions for dimensions that are related to user m. To estimate σ2 k and µk, we only sum over users that are related to the kth feature.",
                "There are two major benefits of the new algorithm.",
                "First, because only the related (m, k) pairs are needed at the modified M step, the computational complexity in a single EM iteration is much smaller when the data is sparse, and many of (m, k) pairs are unrelated.",
                "Second, the parameters estimated at the modified M step (Equations 12 - 13) are more accurate than the standard M step described in Section 4.1 because the exact analytical solutions ¯wm,k = µk and σm,k = σk for the unrelated (m, k) pairs were used in the new algorithm instead of an approximate solution as in the standard algorithm. 5.",
                "EXPERIMENTAL METHODOLOGY 5.1 Evaluation Data Set To evaluate the proposed technique, we used the following three major data sets (Table 1): MovieLens Data: This data set was created by combining the relevance judgments from the MovieLens[9] data set with documents from the Internet Movie Database (IMDB).",
                "MovieLens allows users to rank how much he/she enjoyed a specific movie on a scale from 1 to 5.",
                "This likeability rating was used as a measurement of how relevant the document representing the corresponding movie is to the user.",
                "We considered documents with likeability scores of 4 or 5 as relevant, and documents with a score of 1 to 3 as irrelevant to the user.",
                "MovieLens provided relevance judgments on 3,057 documents from 6,040 separate users.",
                "On average, each user rated 151 movies, of these 87 were judged to be relevant.",
                "The average score for a document was 3.58.",
                "Documents representing each movie were constructed from the portion of the IMDB database that is available for public download[13].",
                "Based on this database, we created one document per movie that contained the relevant information about it (e.g. directors, actors, etc.).",
                "Table 1: Data Set Statistics.",
                "On Reuters, the number of rating for a simulated user is the number of documents relevant to the corresponding topic.",
                "Data Users Docs Ratings per User MovieLens 6,040 3,057 151 Netflix-all 480,189 17,770 208 Netflix-1000 1000 17,770 127 Reuters-C 34 100,000 3949 Reuters-E 26 100,000 1632 Reuters-G 33 100,000 2222 Reuters-M 10 100,000 6529 Netflix Data: This data set was constructed by combining documents about movies crawled from the web with a set of actual movie rental customer relevance judgments from Netflix[19].",
                "Netflix publicly provides the relevance judgments of 480,189 anonymous customers.",
                "There are around 100 million rating on a scale of 1 to 5 for 17,770 documents.",
                "Similar to MovieLens, we considered documents with likeability scores of 4 or 5 as relevant.",
                "This number was reduced to 1000 customers through random sampling.",
                "The average customer on the reduced data set provided 127 judgments, with 70 being deemed relevant.",
                "The average score for documents is 3.55.",
                "Reuters Data: This is the Reuters Corpus, Volume 1.",
                "It covers 810,000 Reuters English language news stories from August 20, 1996 to August 19, 1997.",
                "Only the first 100,000 news were used in our experiments.",
                "The Reuters corpus comes with a topic hierarchy.",
                "Each document is assigned to one of several locations on the hierarchical tree.",
                "The first level of the tree contains four topics, denoted as C, E, M, and G. For the experiments in this paper, the tree was cut at level 1 to create four smaller trees, each of which corresponds to one smaller data set: Reuters-E Reuters-C, ReutersM and Reuters-G. For each small data set, we created several profiles, one profile for each node in a sub-tree, to simulate multiple users, each with a related, yet separate definition of relevance.",
                "All the user profiles on a sub-tree are supposed to share the same prior model distribution.",
                "Since this corpus explicitly indicates only the relevant documents for a topic(user), all other documents are considered irrelevant. 5.2 Evaluation We designed the experiments to answer the following three questions: 1.",
                "Do we need to take the effort to use a Bayesian approach and learn a prior from other users? 2.",
                "Does the new algorithm work better than the standard EM algorithm for learning the Bayesian hierarchical linear model? 3.",
                "Can the new algorithm quickly learn many user models?",
                "To answer the first question, we compared the Bayesian hierarchical models with commonly used Norm-2 regularized linear regression models.",
                "In fact, the commonly used approach is equivalent to the model learned at the end of the first EM iteration.",
                "To answer the second question, we compared the proposed new algorithm with the standard EM algorithm to see whether the new learning algorithm is better.",
                "To answer the third question, we tested the efficiency of the new algorithm on the entire Netflix data set where about half a million user models need to be learned together.",
                "For the MovieLens and Netflix data sets, algorithm effectiveness was measured by mean square error, while on the Reuters data set classification error was used because it was more informative.",
                "We first evaluated the performance on each individual user, and then estimated the macro average over all users.",
                "Statistical tests (t-tests) were carried out to see whether the results are significant.",
                "For the experiments on the MovieLens and Netflix data sets, we used a random sample of 90% of each user for training, and the rest for testing.",
                "On Reuters data set, because there are too many relevant documents for each topic in the corpus, we used a random sample of 10% of each topic for training, and 10% of the remaining documents for testing.",
                "For all runs, we set (a, b, c, Σ ) = (0.1, 10, 0.1, 1) manually. 6.",
                "EXPERIMENTAL RESULTS Figure 2, Figure 3, and Figure 4 show that on all data sets, the Bayesian hierarchical modeling approach has a statistical significant improvement over the regularized linear regression model, which is equivalent to the Bayesian hierarchical models learned at the first iteration.",
                "Further analysis shows a negative correlation between the number of training data for a user and the improvement the system gets.",
                "This suggests that the borrowing information from other users has more significant improvements for users with less training data, which is as expected.",
                "However, the strength of the correlation differs over data sets, and the amount of training data is not the only characteristics that will influence the final performance.",
                "Figure 2 and Figure 3 show that the proposed new algorithm works better than the standard EM algorithm on the Netflix and MovieLens data sets.",
                "This is not surprising since the number of related feature-users pairs is much smaller than the number of unrelated feature-user pairs on these two data sets, and thus the proposed new algorithm is expected to work better.",
                "Figure 4 shows that the two algorithms work similarly on the Reuters-E data set.",
                "The accuracy of the new algorithm is similar to that of the standard EM algorithm at each iteration.",
                "The general patterns are very similar on other Reuters subsets.",
                "Further analysis shows that only 58% of the user-feature pairs are unrelated on this data set.",
                "Since the number of unrelated user-feature pairs is not extremely large, the sparseness is not a serious problem on the Reuters data set.",
                "Thus the two learning algorithms perform similarly.",
                "The results suggest that only on a corpus where the number of unrelated user-feature pairs is much larger than the number of related pairs, such as on the Netflix data set, the proposed technique will get a significant improvement over standard EM.",
                "However, the experiments also show that when the assumption does not hold, the new algorithm does not hurt performance.",
                "Although the proposed technique is faster than standard Figure 2: Performance on a Netflix subset with 1,000 users.",
                "The new algorithm is statistical significantly better than EM algorithm at iterations 2 - 10.",
                "Norm-2 regularized linear models are equivalent to the Bayesian hierarchical models learned at the first iteration, and are statistical significantly worse than the Bayesian hierarchical models. 0 2 4 6 8 10 1 1.05 1.1 1.15 1.2 1.25 1.3 1.35 1.4 Iterations MeanSquareError New Algorithm Traditional EM 1 2 3 4 5 6 7 8 9 10 0.33 0.34 0.35 0.36 0.37 0.38 0.39 Iterations ClassificationError New Algorithm Traditional EM Figure 3: Performance on a MovieLens subset with 1,000 users.",
                "The new algorithm is statistical significantly better than EM algorithm at iteration 2 to 17 (evaluated with mean square error). 1 6 11 16 21 0.5 1 1.5 2 2.5 3 3.5 Iterations MeanSquareError New Algorithm Traditional EM 1 6 11 16 21 0.35 0.4 0.45 0.5 0.55 0.6 0.65 Iterations ClassificationError New Algorithm Traditional EM Figure 4: Performance on a Reuters-E subset with 26 profiles.",
                "Performances on Reuters-C, Reuters-M, Reuters-G are similar. 1 2 3 4 5 0.011 0.0115 0.012 0.0125 0.013 0.0135 0.014 Iterations MeanSquareError New Algorithm Traditional EM 1 2 3 4 5 0.0102 0.0104 0.0106 0.0108 0.011 0.0112 0.0114 Iterations ClassificationError New Algorithm Traditional EM EM, can it really learn millions of user models quickly?",
                "Our results show that the modified EM algorithm converges quickly, and 2 - 3 modified EM iterations would result in a reliable estimation.",
                "We evaluated the algorithm on the whole Netflix data set (480,189 users, 159,836 features, and 100 million ratings) running on a single CPU PC (2GB memory, P4 3GHz).",
                "The system finished one modified EM iteration in about 4 hours.",
                "This demonstrates that the proposed technique can efficiently handle large-scale system like Netflix. 7.",
                "CONCLUSION Content-based user profile learning is an important problem and is the key to providing personal recommendations to a user, especially for recommending new items with a small number of ratings.",
                "The Bayesian hierarchical modeling approach is becoming an important user profile learning approach due to its theoretically justified ability to help one user through information transfer from the other users by way of hyperpriors.",
                "This paper examined the weakness of the popular EM based learning approach for Bayesian hierarchical linear models and proposed a better <br>learning technique</br> called Modified EM.",
                "We showed that the new technique is theoretically more computationally efficient than the standard EM algorithm.",
                "Evaluation on the MovieLens and Netflix data sets demonstrated the effectiveness of the new technique when the data is sparse, by which we mean the ratio of related user-feature pairs to unrelated pairs is small.",
                "Evaluation on the Reuters data set showed that the new technique performed similar to the standard EM algorithm when the sparseness condition does not hold.",
                "In general, it is better to use the new algorithm since it is as simple as standard EM, the performance is either better or similar to EM, and the computation complexity is lower at each iteration.",
                "It is worth mentioning that even if the original problem space is not sparse, sparseness can be created artificially when a recommendation system uses user-specific feature selection techniques to reduce the noise and user model complexity.",
                "The proposed technique can also be adapted to improve the learning in such a scenario.",
                "We also demonstrated that the proposed technique can learn half a million user profiles from 100 million ratings in a few hours with a single CPU.",
                "The research is important because scalability is a major concern for researchers when using the Bayesian hierarchical linear modeling approach to build a practical large scale system, even though the literature have demonstrated the effectiveness of the models in many applications.",
                "Our work is one major step on the road to make Bayesian hierarchical linear models more practical.",
                "The proposed new technique can be easily adapted to run on a cluster of machines, and thus further speed up the learning process to handle a larger scale system with hundreds of millions of users.",
                "The research has much potential to benefit people using EM algorithm on many other IR problems as well as machine learning problems.",
                "EM algorithm is a commonly used machine <br>learning technique</br>.",
                "It is used to find model parameters in many IR problems where the training data is very sparse.",
                "Although we are focusing on the Bayesian hierarchical linear models for recommendation and filtering, the new idea of using analytical solution instead of numerical solution for unrelated user-feature pairs at the M step could be adapted to many other problems. 8.",
                "ACKNOWLEDGMENTS We thank Wei Xu, David Lewis and anonymous reviewers for valuable feedback on the work described in this paper.",
                "Part of the work was supported by Yahoo, Google, the Petascale Data Storage Institute and the Institute for Scalable Scientific Data Management.",
                "Any opinions, findings, conclusions, or recommendations expressed in this material are those of the authors, and do not necessarily reflect those of the sponsors. 9.",
                "REFERENCES [1] C. Basu, H. Hirsh, and W. Cohen.",
                "Recommendation as classification: Using social and content-based information in recommendation.",
                "In Proceedings of the Fifteenth National Conference on Artificial Intelligence, 1998. [2] J. S. Breese, D. Heckerman, and C. Kadie.",
                "Empirical analysis of predictive algorithms for collaborative filtering.",
                "Technical report, Microsoft Research, One Microsoft Way, Redmond, WA 98052, 1998. [3] J. Callan.",
                "Document filtering with inference networks.",
                "In Proceedings of the Nineteenth Annual International ACM SIGIR Conference on Research and Development in Information Retrieval, pages 262-269, 1996. [4] N. Cancedda, N. Cesa-Bianchi, A. Conconi, C. Gentile, C. Goutte, T. Graepel, Y. Li, J. M. Renders, J. S. Taylor, and A. Vinokourov.",
                "Kernel method for document filtering.",
                "In The Eleventh Text REtrieval Conference (TREC11).",
                "National Institute of Standards and Technology, special publication 500-249, 2003. [5] C. Chelba and A. Acero.",
                "Adaptation of maximum entropy capitalizer: Little data can help a lot.",
                "In D. Lin and D. Wu, editors, Proceedings of EMNLP 2004, pages 285-292, Barcelona, Spain, July 2004.",
                "Association for Computational Linguistics. [6] B. Croft and J. Lafferty, editors.",
                "Language Modeling for Information Retrieval.",
                "Kluwer, 2002. [7] A. Dayanik, D. D. Lewis, D. Madigan, V. Menkov, and A. Genkin.",
                "Constructing informative prior distributions from domain knowledge in text classification.",
                "In SIGIR 06: Proceedings of the 29th annual international ACM SIGIR conference on Research and development in information retrieval, pages 493-500, New York, NY, USA, 2006.",
                "ACM Press. [8] J. Delgado and N. Ishii.",
                "Memory-based weightedmajority prediction for recommender systems.",
                "In ACM SIGIR99 Workshop on Recommender Systems, 1999. [9] GroupLens.",
                "Movielens. http://www.grouplens.org/taxonomy/term/14, 2006. [10] D. Heckerman.",
                "A tutorial on learning with bayesian networks.",
                "In M. Jordan, editor, Learning in Graphical Models.",
                "Kluwer Academic, 1998. [11] J. L. Herlocker, J.",
                "A. Konstan, A. Borchers, and J. Riedl.",
                "An algorithmic framework for performing collaborative filtering.",
                "In SIGIR 99: Proceedings of the 22nd annual international ACM SIGIR conference on Research and development in information retrieval, pages 230-237, New York, NY, USA, 1999.",
                "ACM Press. [12] T. Hofmann and J. Puzicha.",
                "Latent class models for collaborative filtering.",
                "In IJCAI 99: Proceedings of the Sixteenth International Joint Conference on Artificial Intelligence, pages 688-693, San Francisco, CA, USA, 1999.",
                "Morgan Kaufmann Publishers Inc. [13] I. M. D. (IMDB).",
                "Internet movie database. http://www.imdb.com/interfaces/, 2006. [14] R. Jin, J. Y. Chai, and L. Si.",
                "An automatic weighting scheme for collaborative filtering.",
                "In SIGIR 04: Proceedings of the 27th annual international ACM SIGIR conference on Research and development in information retrieval, pages 337-344, New York, NY, USA, 2004.",
                "ACM Press. [15] J.",
                "A. Konstan, B. N. Miller, D. Maltz, J. L. Herlocker, L. R. Gordon, and J. Riedl.",
                "GroupLens: Applying collaborative filtering to Usenet news.",
                "Communications of the ACM, 40(3):77-87, 1997. [16] D. Lewis.",
                "Applying support vector machines to the TREC-2001 batch filtering and routing tasks.",
                "In Proceedings of the Eleventh Text REtrieval Conference (TREC-11), 2002. [17] B. Liu, X. Li, W. S. Lee, , and P. Yu.",
                "Text classification by labeling words.",
                "In Proceedings of The Nineteenth National Conference on Artificial Intelligence (AAAI-2004), July 25-29, 2004. [18] P. Melville, R. J. Mooney, and R. Nagarajan.",
                "Content-boosted collaborative filtering for improved recommendations.",
                "In Proceedings of the Eighteenth National Conference on Artificial Intelligence (AAAI-2002), Edmonton, Canada, 2002. [19] Netflix.",
                "Netflix prize. http://www.netflixprize.com (visited on Nov. 30, 2006), 2006. [20] S. Robertson and K. Sparck-Jones.",
                "Relevance weighting of search terms.",
                "In Journal of the American Society for Information Science, volume 27, pages 129-146, 1976. [21] J. Wang, A. P. de Vries, and M. J. T. Reinders.",
                "Unifying user-based and item-based collaborative filtering approaches by similarity fusion.",
                "In SIGIR 06: Proceedings of the 29th annual international ACM SIGIR conference on Research and development in information retrieval, pages 501-508, New York, NY, USA, 2006.",
                "ACM Press. [22] X. Wu and R. K. Srihari.",
                "Incorporating prior knowledge with weighted margin support vector machines.",
                "In Proc.",
                "ACM Knowledge Discovery Data Mining Conf. (ACM SIGKDD 2004), Aug. 2004. [23] Y. Yang, S. Yoo, J. Zhang, and B. Kisiel.",
                "Robustness of adaptive filtering methods in a cross-benchmark evaluation.",
                "In Proceedings of the 28th Annual International ACM SIGIR Conference on Research and Development in Information Retrieval, 2005. [24] K. Yu, V. Tresp, and A. Schwaighofer.",
                "Learning gaussian processes from multiple tasks.",
                "In ICML 05: Proceedings of the 22nd international conference on Machine learning, pages 1012-1019, New York, NY, USA, 2005.",
                "ACM Press. [25] K. Yu, V. Tresp, and S. Yu.",
                "A nonparametric hierarchical bayesian framework for information filtering.",
                "In SIGIR 04: Proceedings of the 27th annual international ACM SIGIR conference on Research and development in information retrieval, pages 353-360.",
                "ACM Press, 2004. [26] X. Zhu.",
                "Semi-supervised learning literature survey.",
                "Technical report, University of Wisconsin - Madison, December 9, 2006. [27] P. Zigoris and Y. Zhang.",
                "Bayesian adaptive user profiling with explicit & implicit feedback.",
                "In Conference on Information and Knowledge Mangement 2006, 2006."
            ],
            "original_annotated_samples": [],
            "translated_annotated_samples": [
                "Este documento propone una nueva \"técnica de aprendizaje\" rápida para aprender una gran cantidad de perfiles de usuarios individuales.",
                "La configuración y los resultados experimentales utilizados para validar la \"técnica de aprendizaje\" propuesta se informan en las Secciones 5 y 6.",
                "Este artículo examinó la debilidad del popular enfoque de aprendizaje basado en EM para los modelos lineales jerárquicos bayesianos y propuso una mejor \"técnica de aprendizaje\" llamada Modified EM.",
                "El algoritmo EM es una máquina \"técnica de aprendizaje\" de máquina comúnmente utilizada."
            ],
            "translated_text": "",
            "candidates": [
                "técnica de aprendizaje",
                "técnica de aprendizaje",
                "Técnica de aprendizaje",
                "técnica de aprendizaje",
                "técnica de aprendizaje",
                "técnica de aprendizaje",
                "técnica de aprendizaje",
                "técnica de aprendizaje"
            ],
            "error": []
        },
        "ir": {
            "translated_key": "IR",
            "is_in_text": true,
            "original_annotated_sentences": [
                "Efficient Bayesian Hierarchical User Modeling for Recommendation Systems Yi Zhang, Jonathan Koren School of Engineering University of California Santa Cruz Santa Cruz, CA, USA {yiz, jonathan}@soe.ucsc.edu ABSTRACT A content-based personalized recommendation system learns user specific profiles from user feedback so that it can deliver information tailored to each individual users interest.",
                "A system serving millions of users can learn a better user profile for a new user, or a user with little feedback, by borrowing information from other users through the use of a Bayesian hierarchical model.",
                "Learning the model parameters to optimize the joint data likelihood from millions of users is very computationally expensive.",
                "The commonly used EM algorithm converges very slowly due to the sparseness of the data in <br>ir</br> applications.",
                "This paper proposes a new fast learning technique to learn a large number of individual user profiles.",
                "The efficacy and efficiency of the proposed algorithm are justified by theory and demonstrated on actual user data from Netflix and MovieLens.",
                "Categories and Subject Descriptors: B.3.3 [Information Search and Retrieval]: Information filtering General Terms: Algorithms 1.",
                "INTRODUCTION Personalization is the future of the Web, and it has achieved great success in industrial applications.",
                "For example, online stores, such as Amazon and Netflix, provide customized recommendations for additional products or services based on a users history.",
                "Recent offerings such as My MSN, My Yahoo!, My Google, and Google News have attracted much attention due to their potential ability to infer a users interests from his/her history.",
                "One major personalization topic studied in the information retrieval community is content-based personal recommendation systems1 .",
                "These systems learn user-specific profiles from user feedback so that they can recommend information tailored to each individual users interest without requiring the user to make an explicit query.",
                "Learning the user profiles is the core problem for these systems.",
                "A user profile is usually a classifier that can identify whether a document is relevant to the user or not, or a regression model that tells how relevant a document is to the user.",
                "One major challenge of building a recommendation or personalization system is that the profile learned for a particular user is usually of low quality when the amount of data from that particular user is small.",
                "This is known as the cold start problem.",
                "This means that any new user must endure poor initial performance until sufficient feedback from that user is provided to learn a reliable user profile.",
                "There has been much research on improving classification accuracy when the amount of labeled training data is small.",
                "The semi-supervised learning approach combines unlabeled and labeled data together to achieve this goal [26].",
                "Another approach is using domain knowledge.",
                "Researchers have modified different learning algorithms, such as Na¨ıveBayes [17], logistic regression [7], and SVMs [22], to integrate domain knowledge into a text classifier.",
                "The third approach is borrowing training data from other resources [5][7].",
                "The effectiveness of these different approaches is mixed, due to how well the underlying model assumption fits the data.",
                "One well-received approach to improve recommendation system performance for a particular user is borrowing information from other users through a Bayesian hierarchical modeling approach.",
                "Several researchers have demonstrated that this approach effectively trades off between shared and user-specific information, thus alleviating poor initial performance for each user[27][25].",
                "In order to learn a Bayesian hierarchical model, the system usually tries to find the most likely model parameters for the given data.",
                "A mature recommendation system usually works for millions of users.",
                "It is well known that learning the optimal parameters of a Bayesian hierarchical model is computationally expensive when there are thousands or millions of users.",
                "The EM algorithm is a commonly used technique for parameter learning due to its simplicity and convergence guarantee.",
                "However, a content based recommendation system often handles documents in a very high dimensional space, in which each document is represented by a very sparse vector.",
                "With careful analysis of the EM algorithm in this scenario (Section 4), we find that the EM tering, or item-based collaborative filtering.",
                "In this paper, the words filtering and recommendation are used interchangeably. algorithm converges very slowly due to the sparseness of the input variables.",
                "We also find that updating the model parameter at each EM iteration is also expensive with computational complexity of O(MK), where M is the number of users and K is the number of dimensions.",
                "This paper modifies the standard EM algorithm to create an improved learning algorithm, which we call the Modified EM algorithm.",
                "The basic idea is that instead of calculating the numerical solution for all the user profile parameters, we derive the analytical solution of the parameters for some feature dimensions, and at the M step use the analytical solution instead of the numerical solution estimated at E step for those parameters.",
                "This greatly reduces the computation at a single EM iteration, and also has the benefit of increasing the convergence speed of the learning algorithm.",
                "The proposed technique is not only well supported by theory, but also by experimental results.",
                "The organization of the remaining parts of this paper is as follows: Section 3 describes the Bayesian hierarchical linear regression modeling framework used for content-based recommendations.",
                "Section 4 describes how to learn the model parameters using the standard EM algorithm, along with using the new technique proposed in this paper.",
                "The experimental setting and results used to validate the proposed learning technique are reported in Sections 5 and 6.",
                "Section 7 summarizes and offers concluding remarks. 2.",
                "RELATED WORK Providing personalized recommendations to users has been identified as a very important problem in the <br>ir</br> community since the 1970s.",
                "The approaches that have been used to solve this problem can be roughly classified into two major categories: content based filtering versus collaborative filtering.",
                "Content-based filtering studies the scenario where a recommendation system monitors a document stream and pushes documents that match a user profile to the corresponding user.",
                "The user may read the delivered documents and provide explicit relevance feedback, which the filtering system then uses to update the users profile using relevance feedback retrieval models (e.g.",
                "Boolean models, vector space models, traditional probabilistic models [20] , inference networks [3] and language models [6]) or machine learning algorithms (e.g.",
                "Support Vector Machines (SVM), K nearest neighbors (K-NN) clustering, neural networks, logistic regression, or Winnow [16] [4] [23]).",
                "Collaborative filtering goes beyond merely using document content to recommend items to a user by leveraging information from other users with similar tastes and preferences in the past.",
                "Memorybased heuristics and model based approaches have been used in collaborative filtering task [15] [8] [2] [14] [12] [11].",
                "This paper contributes to the content-based recommendation research by improving the efficiency and effectiveness of Bayesian hierarchical linear models, which have a strong theoretical basis and good empirical performance on recommendation tasks[27][25].",
                "This paper does not intend to compare content-based filtering with collaborative filtering or claim which one is a better.",
                "We think each complements the other, and that content-based filtering is extremely useful for handling new documents/items with little or no user feedback.",
                "Similar to some other researchers[18][1][21], we found that a recommendation system will be more effective when both techniques are combined.",
                "However, this is beyond the scope of this paper and thus not discussed here. 3.",
                "BAYESIAN HIERARCHICAL LINEAR REGRESSION Assume there are M users in the system.",
                "The task of the system is to recommend documents that are relevant to each user.",
                "For each user, the system learns a user model from the users history.",
                "In the rest of this paper, we will use the following notations to represent the variables in the system. m = 1, 2, ..., M: The index for each individual user.",
                "M is the total number of users. wm: The user model parameter associated with user m. wm is a K dimensional vector. j = 1, 2, ..., Jm: The index for a set of data for user m. Jm is the number of training data for user m. Dm = {(xm,j, ym,j)}: A set of data associated with user m. xm,j is a K dimensional vector that represents the mth users jth training document.2 ym,j is a scalar that represents the label of document xm,j. k = 1, 2, ..., K: The dimensional index of input variable x.",
                "The Bayesian hierarchical modeling approach has been widely used in real-world information retrieval applications.",
                "Generalized Bayesian hierarchical linear models, one of the simplest Bayesian hierarchical models, are commonly used and have achieved good performance on collaborative filtering [25] and content-based adaptive filtering [27] tasks.",
                "Figure 1 shows the graphical representation of a Bayesian hierarchical model.",
                "In this graph, each user model is represented by a random vector wm.",
                "We assume a user model is sampled randomly from a prior distribution P(w|Φ).",
                "The system can predict the user label y of a document x given an estimation of wm (or wms distribution) using a function y = f(x, w).",
                "The model is called generalized Bayesian hierarchical linear model when y = f(wT x) is any generalized linear model such as logistic regression, SVM, and linear regression.",
                "To reliably estimate the user model wm, the system can borrow information from other users through the prior Φ = (µ, Σ).",
                "Now we look at one commonly used model where y = wT x + , where ∼ N(0, σ2 ) is a random noise [25][27].",
                "Assume that each user model wm is an independent draw from a population distribution P(w|Φ), which is governed by some unknown hyperparameter Φ.",
                "Let the prior distribution of user model w be a Gaussian distribution with parameter Φ = (µ, Σ), which is the commonly used prior for linear models. µ = (µ1, µ2, ..., µK ) is a K dimensional vector that represents the mean of the Gaussian distribution, and Σ is the covariance matrix of the Gaussian.",
                "Usually, a Normal distribution N(0, aI) and an Inverse Wishart distribution P(Σ) ∝ |Σ|− 1 2 b exp(−1 2 ctr(Σ−1 )) are used as hyperprior to model the prior distribution of µ and Σ respectively.",
                "I is the K dimensional identity matrix, and a, b, and c are real numbers.",
                "With these settings, we have the following model for the system: 1. µ and Σ are sampled from N(0, aI) and IWν (aI), respectively. 2 The first dimension of x is a dummy variable that always equals to 1.",
                "Figure 1: Illustration of dependencies of variables in the hierarchical model.",
                "The rating, y, for a document, x, is conditioned on the document and the user model, wm, associated with the user m. Users share information about their models through the prior, Φ = (µ, Σ). 2.",
                "For each user m, wm is sampled randomly from a Normal distribution: wm ∼ N(µ, Σ2 ) 3.",
                "For each item xm,j, ym,j is sampled randomly from a Normal distribution: ym,j ∼ N(wT mxm,j, σ2 ).",
                "Let θ = (Φ, w1, w2, ..., wM ) represent the parameters of this system that needs to be estimated.",
                "The joint likelihood for all the variables in the probabilistic model, which includes the data and the parameters, is: P(D, θ) = P(Φ) m P(wm|Φ) j P(ym,j|xm,j, wm) (1) For simplicity, we assume a, b, c, and σ are provided to the system. 4.",
                "MODEL PARAMETER LEARNING If the prior Φ is known, finding the optimal wm is straightforward: it is a simple linear regression.",
                "Therefore, we will focus on estimating Φ.",
                "The maximum a priori solution of Φ is given by ΦMAP = arg max Φ P(Φ|D) (2) = arg max Φ P(Φ, D) P(D) (3) = arg max Φ P(D|Φ)P(Φ) (4) = arg max Φ w P(D|w, Φ)P(w|Φ)P(Φ)dw (5) Finding the optimal solution for the above problem is challenging, since we need to integrate over all w = (w1, w2, ..., wM ), which are unobserved hidden variables. 4.1 EM Algorithm for Bayesian Hierarchical Linear Models In Equation 5, Φ is the parameter needs to be estimated, and the result depends on unobserved latent variables w. This kind of optimization problem is usually solved by the EM algorithm.",
                "Applying EM to the above problem, the set of user models w are the unobservable hidden variables and we have: Q = w P(w|µ, Σ2 , Dm) log P(µ, Σ2 , w, D)dw Based on the derivation of the EM formulas presented in [24], we have the following Expectation-Maximization steps for finding the optimal hyperparameters.",
                "For space considerations, we omit the derivation in this paper since it is not the focus of our work.",
                "E step: For each user m, estimate the user model distribution P(wm|Dm, Φ) = N(wm; ¯wm, Σ2 m) based on the current estimation of the prior Φ = (µ, Σ2 ). ¯wm = ((Σ2 )−1 + Sxx,m σ2 )−1 ( Sxy,m σ2 + (Σ2 )−1 µ) (6) Σ2 m = ((Σ2 )−1 + Sxx,m σ2 )−1 (7) where Sxx,m = j xm,jxT m,j Sxy,m = j xm,jym,j M step: Optimize the prior Φ = (µ, Σ2 ) based on the estimation from the last E step. µ = 1 M m ¯wm (8) Σ2 = 1 M m Σ2 m + ( ¯wm − µ)( ¯wm − µ)T (9) Many machine learning driven <br>ir</br> systems use a point estimate of the parameters at different stages in the system.",
                "However, we are estimating the posterior distribution of the variables at the E step.",
                "This avoids overfitting wm to a particular users data, which may be small and noisy.",
                "A detailed discussion about this subject appears in [10]. 4.2 New Algorithm: Modified EM Although the EM algorithm is widely studied and used in machine learning applications, using the above EM process to solve Bayesian hierarchical linear models in large-scale information retrieval systems is still too computationally expensive.",
                "In this section, we describe why the learning rate of the EM algorithm is slow in our application and introduce a new technique to make the learning of the Bayesian hierarchical linear model scalable.",
                "The derivation of the new learning algorithm will be based on the EM algorithm described in the previous section.",
                "First, the covariance matrices Σ2 , Σ2 m are usually too large to be computationally feasible.",
                "For simplicity, and as a common practice in <br>ir</br>, we do not model the correlation between features.",
                "Thus we approximate these matrices with K dimensional diagonal matrices.",
                "In the rest of the paper, we use these symbols to represent their diagonal approximations: Σ2 =     σ2 1 0 .. 0 0 σ2 2 .. 0 .. .. .. .. 0 0 .. σ2 K     Σ2 m =     σ2 m,1 0 .. 0 0 σ2 m,2 .. 0 .. .. .. .. 0 0 .. σ2 m,K     Secondly, and most importantly, the input space is very sparse and there are many dimensions that are not related to a particular user in a real <br>ir</br> application.",
                "For example, let us consider a movie recommendation system, with the input variable x representing a particular movie.",
                "For the jth movie that the user m has seen, let xm,j,k = 1 if the director of the movie is Jean-Pierre Jeunet (indexed by k).",
                "Here we assume that whether or not that this director directed a specific movie is represented by the kth dimension.",
                "If the user m has never seen a movie directed by Jean-Pierre Jeunet, then the corresponding dimension is always zero (xm,j,k = 0 for all j) .",
                "One major drawback of the EM algorithm is that the importance of a feature, µk, may be greatly dominated by users who have never encountered this feature (i.e. j xm,j,k = 0) at the M step (Equation 8).",
                "Assume that 100 out of 1 million users have viewed the movie directed by Jean-Pierre Jeunet, and that the viewers have rated all of his movies as excellent.",
                "Intuitively, he is a good director and the weight for him (µk) should be high.",
                "Before the EM iteration, the initial value of µ is usually set to 0.",
                "Since the other 999,900 users have not seen this movie, their corresponding weights (w1,k, w2,k, ..., wm,k..., w999900,k) for that director would be very small initially.",
                "Thus the corresponding weight of the director in the prior µk at the first M step would be very low , and the variance σm,k will be large (Equations 8 and 7).",
                "It is undesirable that users who have never seen any movie produced by the director influence the importance of the director so much.",
                "This makes the convergence of the standard EM algorithm very slow.",
                "Now lets look at whether we can improve the learning speed of the algorithm.",
                "Without a loss of generality, let us assume that the kth dimension of the input variable x is not related to a particular user m. By which we mean, xm,j,k = 0 for all j = 1, ..., Jm.",
                "It is straightforward to prove that the kth row and kth column of Sxx,m are completely filled with zeros, and that the kth dimension of Sxy,m is zeroed as well.",
                "Thus the corresponding kth dimension of the user models mean, ¯wm, should be equal to that of the prior: ¯wm,k = µk, with the corresponding covariance of σm,k = σk.",
                "At the M step, the standard EM algorithm uses the numerical solution of the distribution P(wm|Dm, Φ) estimated at E step (Equation 8 and Equation 7).",
                "However, the numerical solutions are very unreliable for ¯wm,k and σm,k when the kth dimension is not related to the mth user.",
                "A better approach is using the analytical solutions ¯wm,k = µk, and σm,k = σk for the unrelated (m, k) pairs, along with the numerical solution estimated at E step for the other (m, k) pairs.",
                "Thus we get the following new EM-like algorithm: Modified E step: For each user m, estimate the user model distribution P(wm|Dm, Φ) = N(wm; ¯wm, Σ2 m) based on the current estimation of σ , µ, Σ2 . ¯wm = ((Σ2 )−1 + Sxx,m σ2 )−1 ( Sxy,m σ2 + (Σ2 )−1 µ)(10) σ2 m,k = ((σ2 k)−1 + sxx,m,k σ2 )−1 (11) where sxx,m,k = j x2 m,j,k and sxy,m,k = j xm,j,kym,j Modified M Step Optimize the prior Φ = (µ, Σ2 ) based on the estimation from the last E step for related userfeature pairs.",
                "The M step implicitly uses the analytical solution for unrelated user-feature pairs. µk = 1 Mk m:related ¯wm,k (12) σ2 k = 1 Mk m:related σ2 m,k +( ¯wm,k − µk)( ¯wm,k − µk)T (13) where Mk is the number of users that are related to feature k We only estimate the diagonal of Σ2 m and Σ since we are using the diagonal approximation of the covariance matrices.",
                "To estimate ¯wm, we only need to calculate the numerical solutions for dimensions that are related to user m. To estimate σ2 k and µk, we only sum over users that are related to the kth feature.",
                "There are two major benefits of the new algorithm.",
                "First, because only the related (m, k) pairs are needed at the modified M step, the computational complexity in a single EM iteration is much smaller when the data is sparse, and many of (m, k) pairs are unrelated.",
                "Second, the parameters estimated at the modified M step (Equations 12 - 13) are more accurate than the standard M step described in Section 4.1 because the exact analytical solutions ¯wm,k = µk and σm,k = σk for the unrelated (m, k) pairs were used in the new algorithm instead of an approximate solution as in the standard algorithm. 5.",
                "EXPERIMENTAL METHODOLOGY 5.1 Evaluation Data Set To evaluate the proposed technique, we used the following three major data sets (Table 1): MovieLens Data: This data set was created by combining the relevance judgments from the MovieLens[9] data set with documents from the Internet Movie Database (IMDB).",
                "MovieLens allows users to rank how much he/she enjoyed a specific movie on a scale from 1 to 5.",
                "This likeability rating was used as a measurement of how relevant the document representing the corresponding movie is to the user.",
                "We considered documents with likeability scores of 4 or 5 as relevant, and documents with a score of 1 to 3 as irrelevant to the user.",
                "MovieLens provided relevance judgments on 3,057 documents from 6,040 separate users.",
                "On average, each user rated 151 movies, of these 87 were judged to be relevant.",
                "The average score for a document was 3.58.",
                "Documents representing each movie were constructed from the portion of the IMDB database that is available for public download[13].",
                "Based on this database, we created one document per movie that contained the relevant information about it (e.g. directors, actors, etc.).",
                "Table 1: Data Set Statistics.",
                "On Reuters, the number of rating for a simulated user is the number of documents relevant to the corresponding topic.",
                "Data Users Docs Ratings per User MovieLens 6,040 3,057 151 Netflix-all 480,189 17,770 208 Netflix-1000 1000 17,770 127 Reuters-C 34 100,000 3949 Reuters-E 26 100,000 1632 Reuters-G 33 100,000 2222 Reuters-M 10 100,000 6529 Netflix Data: This data set was constructed by combining documents about movies crawled from the web with a set of actual movie rental customer relevance judgments from Netflix[19].",
                "Netflix publicly provides the relevance judgments of 480,189 anonymous customers.",
                "There are around 100 million rating on a scale of 1 to 5 for 17,770 documents.",
                "Similar to MovieLens, we considered documents with likeability scores of 4 or 5 as relevant.",
                "This number was reduced to 1000 customers through random sampling.",
                "The average customer on the reduced data set provided 127 judgments, with 70 being deemed relevant.",
                "The average score for documents is 3.55.",
                "Reuters Data: This is the Reuters Corpus, Volume 1.",
                "It covers 810,000 Reuters English language news stories from August 20, 1996 to August 19, 1997.",
                "Only the first 100,000 news were used in our experiments.",
                "The Reuters corpus comes with a topic hierarchy.",
                "Each document is assigned to one of several locations on the hierarchical tree.",
                "The first level of the tree contains four topics, denoted as C, E, M, and G. For the experiments in this paper, the tree was cut at level 1 to create four smaller trees, each of which corresponds to one smaller data set: Reuters-E Reuters-C, ReutersM and Reuters-G. For each small data set, we created several profiles, one profile for each node in a sub-tree, to simulate multiple users, each with a related, yet separate definition of relevance.",
                "All the user profiles on a sub-tree are supposed to share the same prior model distribution.",
                "Since this corpus explicitly indicates only the relevant documents for a topic(user), all other documents are considered irrelevant. 5.2 Evaluation We designed the experiments to answer the following three questions: 1.",
                "Do we need to take the effort to use a Bayesian approach and learn a prior from other users? 2.",
                "Does the new algorithm work better than the standard EM algorithm for learning the Bayesian hierarchical linear model? 3.",
                "Can the new algorithm quickly learn many user models?",
                "To answer the first question, we compared the Bayesian hierarchical models with commonly used Norm-2 regularized linear regression models.",
                "In fact, the commonly used approach is equivalent to the model learned at the end of the first EM iteration.",
                "To answer the second question, we compared the proposed new algorithm with the standard EM algorithm to see whether the new learning algorithm is better.",
                "To answer the third question, we tested the efficiency of the new algorithm on the entire Netflix data set where about half a million user models need to be learned together.",
                "For the MovieLens and Netflix data sets, algorithm effectiveness was measured by mean square error, while on the Reuters data set classification error was used because it was more informative.",
                "We first evaluated the performance on each individual user, and then estimated the macro average over all users.",
                "Statistical tests (t-tests) were carried out to see whether the results are significant.",
                "For the experiments on the MovieLens and Netflix data sets, we used a random sample of 90% of each user for training, and the rest for testing.",
                "On Reuters data set, because there are too many relevant documents for each topic in the corpus, we used a random sample of 10% of each topic for training, and 10% of the remaining documents for testing.",
                "For all runs, we set (a, b, c, Σ ) = (0.1, 10, 0.1, 1) manually. 6.",
                "EXPERIMENTAL RESULTS Figure 2, Figure 3, and Figure 4 show that on all data sets, the Bayesian hierarchical modeling approach has a statistical significant improvement over the regularized linear regression model, which is equivalent to the Bayesian hierarchical models learned at the first iteration.",
                "Further analysis shows a negative correlation between the number of training data for a user and the improvement the system gets.",
                "This suggests that the borrowing information from other users has more significant improvements for users with less training data, which is as expected.",
                "However, the strength of the correlation differs over data sets, and the amount of training data is not the only characteristics that will influence the final performance.",
                "Figure 2 and Figure 3 show that the proposed new algorithm works better than the standard EM algorithm on the Netflix and MovieLens data sets.",
                "This is not surprising since the number of related feature-users pairs is much smaller than the number of unrelated feature-user pairs on these two data sets, and thus the proposed new algorithm is expected to work better.",
                "Figure 4 shows that the two algorithms work similarly on the Reuters-E data set.",
                "The accuracy of the new algorithm is similar to that of the standard EM algorithm at each iteration.",
                "The general patterns are very similar on other Reuters subsets.",
                "Further analysis shows that only 58% of the user-feature pairs are unrelated on this data set.",
                "Since the number of unrelated user-feature pairs is not extremely large, the sparseness is not a serious problem on the Reuters data set.",
                "Thus the two learning algorithms perform similarly.",
                "The results suggest that only on a corpus where the number of unrelated user-feature pairs is much larger than the number of related pairs, such as on the Netflix data set, the proposed technique will get a significant improvement over standard EM.",
                "However, the experiments also show that when the assumption does not hold, the new algorithm does not hurt performance.",
                "Although the proposed technique is faster than standard Figure 2: Performance on a Netflix subset with 1,000 users.",
                "The new algorithm is statistical significantly better than EM algorithm at iterations 2 - 10.",
                "Norm-2 regularized linear models are equivalent to the Bayesian hierarchical models learned at the first iteration, and are statistical significantly worse than the Bayesian hierarchical models. 0 2 4 6 8 10 1 1.05 1.1 1.15 1.2 1.25 1.3 1.35 1.4 Iterations MeanSquareError New Algorithm Traditional EM 1 2 3 4 5 6 7 8 9 10 0.33 0.34 0.35 0.36 0.37 0.38 0.39 Iterations ClassificationError New Algorithm Traditional EM Figure 3: Performance on a MovieLens subset with 1,000 users.",
                "The new algorithm is statistical significantly better than EM algorithm at iteration 2 to 17 (evaluated with mean square error). 1 6 11 16 21 0.5 1 1.5 2 2.5 3 3.5 Iterations MeanSquareError New Algorithm Traditional EM 1 6 11 16 21 0.35 0.4 0.45 0.5 0.55 0.6 0.65 Iterations ClassificationError New Algorithm Traditional EM Figure 4: Performance on a Reuters-E subset with 26 profiles.",
                "Performances on Reuters-C, Reuters-M, Reuters-G are similar. 1 2 3 4 5 0.011 0.0115 0.012 0.0125 0.013 0.0135 0.014 Iterations MeanSquareError New Algorithm Traditional EM 1 2 3 4 5 0.0102 0.0104 0.0106 0.0108 0.011 0.0112 0.0114 Iterations ClassificationError New Algorithm Traditional EM EM, can it really learn millions of user models quickly?",
                "Our results show that the modified EM algorithm converges quickly, and 2 - 3 modified EM iterations would result in a reliable estimation.",
                "We evaluated the algorithm on the whole Netflix data set (480,189 users, 159,836 features, and 100 million ratings) running on a single CPU PC (2GB memory, P4 3GHz).",
                "The system finished one modified EM iteration in about 4 hours.",
                "This demonstrates that the proposed technique can efficiently handle large-scale system like Netflix. 7.",
                "CONCLUSION Content-based user profile learning is an important problem and is the key to providing personal recommendations to a user, especially for recommending new items with a small number of ratings.",
                "The Bayesian hierarchical modeling approach is becoming an important user profile learning approach due to its theoretically justified ability to help one user through information transfer from the other users by way of hyperpriors.",
                "This paper examined the weakness of the popular EM based learning approach for Bayesian hierarchical linear models and proposed a better learning technique called Modified EM.",
                "We showed that the new technique is theoretically more computationally efficient than the standard EM algorithm.",
                "Evaluation on the MovieLens and Netflix data sets demonstrated the effectiveness of the new technique when the data is sparse, by which we mean the ratio of related user-feature pairs to unrelated pairs is small.",
                "Evaluation on the Reuters data set showed that the new technique performed similar to the standard EM algorithm when the sparseness condition does not hold.",
                "In general, it is better to use the new algorithm since it is as simple as standard EM, the performance is either better or similar to EM, and the computation complexity is lower at each iteration.",
                "It is worth mentioning that even if the original problem space is not sparse, sparseness can be created artificially when a recommendation system uses user-specific feature selection techniques to reduce the noise and user model complexity.",
                "The proposed technique can also be adapted to improve the learning in such a scenario.",
                "We also demonstrated that the proposed technique can learn half a million user profiles from 100 million ratings in a few hours with a single CPU.",
                "The research is important because scalability is a major concern for researchers when using the Bayesian hierarchical linear modeling approach to build a practical large scale system, even though the literature have demonstrated the effectiveness of the models in many applications.",
                "Our work is one major step on the road to make Bayesian hierarchical linear models more practical.",
                "The proposed new technique can be easily adapted to run on a cluster of machines, and thus further speed up the learning process to handle a larger scale system with hundreds of millions of users.",
                "The research has much potential to benefit people using EM algorithm on many other <br>ir</br> problems as well as machine learning problems.",
                "EM algorithm is a commonly used machine learning technique.",
                "It is used to find model parameters in many <br>ir</br> problems where the training data is very sparse.",
                "Although we are focusing on the Bayesian hierarchical linear models for recommendation and filtering, the new idea of using analytical solution instead of numerical solution for unrelated user-feature pairs at the M step could be adapted to many other problems. 8.",
                "ACKNOWLEDGMENTS We thank Wei Xu, David Lewis and anonymous reviewers for valuable feedback on the work described in this paper.",
                "Part of the work was supported by Yahoo, Google, the Petascale Data Storage Institute and the Institute for Scalable Scientific Data Management.",
                "Any opinions, findings, conclusions, or recommendations expressed in this material are those of the authors, and do not necessarily reflect those of the sponsors. 9.",
                "REFERENCES [1] C. Basu, H. Hirsh, and W. Cohen.",
                "Recommendation as classification: Using social and content-based information in recommendation.",
                "In Proceedings of the Fifteenth National Conference on Artificial Intelligence, 1998. [2] J. S. Breese, D. Heckerman, and C. Kadie.",
                "Empirical analysis of predictive algorithms for collaborative filtering.",
                "Technical report, Microsoft Research, One Microsoft Way, Redmond, WA 98052, 1998. [3] J. Callan.",
                "Document filtering with inference networks.",
                "In Proceedings of the Nineteenth Annual International ACM SIGIR Conference on Research and Development in Information Retrieval, pages 262-269, 1996. [4] N. Cancedda, N. Cesa-Bianchi, A. Conconi, C. Gentile, C. Goutte, T. Graepel, Y. Li, J. M. Renders, J. S. Taylor, and A. Vinokourov.",
                "Kernel method for document filtering.",
                "In The Eleventh Text REtrieval Conference (TREC11).",
                "National Institute of Standards and Technology, special publication 500-249, 2003. [5] C. Chelba and A. Acero.",
                "Adaptation of maximum entropy capitalizer: Little data can help a lot.",
                "In D. Lin and D. Wu, editors, Proceedings of EMNLP 2004, pages 285-292, Barcelona, Spain, July 2004.",
                "Association for Computational Linguistics. [6] B. Croft and J. Lafferty, editors.",
                "Language Modeling for Information Retrieval.",
                "Kluwer, 2002. [7] A. Dayanik, D. D. Lewis, D. Madigan, V. Menkov, and A. Genkin.",
                "Constructing informative prior distributions from domain knowledge in text classification.",
                "In SIGIR 06: Proceedings of the 29th annual international ACM SIGIR conference on Research and development in information retrieval, pages 493-500, New York, NY, USA, 2006.",
                "ACM Press. [8] J. Delgado and N. Ishii.",
                "Memory-based weightedmajority prediction for recommender systems.",
                "In ACM SIGIR99 Workshop on Recommender Systems, 1999. [9] GroupLens.",
                "Movielens. http://www.grouplens.org/taxonomy/term/14, 2006. [10] D. Heckerman.",
                "A tutorial on learning with bayesian networks.",
                "In M. Jordan, editor, Learning in Graphical Models.",
                "Kluwer Academic, 1998. [11] J. L. Herlocker, J.",
                "A. Konstan, A. Borchers, and J. Riedl.",
                "An algorithmic framework for performing collaborative filtering.",
                "In SIGIR 99: Proceedings of the 22nd annual international ACM SIGIR conference on Research and development in information retrieval, pages 230-237, New York, NY, USA, 1999.",
                "ACM Press. [12] T. Hofmann and J. Puzicha.",
                "Latent class models for collaborative filtering.",
                "In IJCAI 99: Proceedings of the Sixteenth International Joint Conference on Artificial Intelligence, pages 688-693, San Francisco, CA, USA, 1999.",
                "Morgan Kaufmann Publishers Inc. [13] I. M. D. (IMDB).",
                "Internet movie database. http://www.imdb.com/interfaces/, 2006. [14] R. Jin, J. Y. Chai, and L. Si.",
                "An automatic weighting scheme for collaborative filtering.",
                "In SIGIR 04: Proceedings of the 27th annual international ACM SIGIR conference on Research and development in information retrieval, pages 337-344, New York, NY, USA, 2004.",
                "ACM Press. [15] J.",
                "A. Konstan, B. N. Miller, D. Maltz, J. L. Herlocker, L. R. Gordon, and J. Riedl.",
                "GroupLens: Applying collaborative filtering to Usenet news.",
                "Communications of the ACM, 40(3):77-87, 1997. [16] D. Lewis.",
                "Applying support vector machines to the TREC-2001 batch filtering and routing tasks.",
                "In Proceedings of the Eleventh Text REtrieval Conference (TREC-11), 2002. [17] B. Liu, X. Li, W. S. Lee, , and P. Yu.",
                "Text classification by labeling words.",
                "In Proceedings of The Nineteenth National Conference on Artificial Intelligence (AAAI-2004), July 25-29, 2004. [18] P. Melville, R. J. Mooney, and R. Nagarajan.",
                "Content-boosted collaborative filtering for improved recommendations.",
                "In Proceedings of the Eighteenth National Conference on Artificial Intelligence (AAAI-2002), Edmonton, Canada, 2002. [19] Netflix.",
                "Netflix prize. http://www.netflixprize.com (visited on Nov. 30, 2006), 2006. [20] S. Robertson and K. Sparck-Jones.",
                "Relevance weighting of search terms.",
                "In Journal of the American Society for Information Science, volume 27, pages 129-146, 1976. [21] J. Wang, A. P. de Vries, and M. J. T. Reinders.",
                "Unifying user-based and item-based collaborative filtering approaches by similarity fusion.",
                "In SIGIR 06: Proceedings of the 29th annual international ACM SIGIR conference on Research and development in information retrieval, pages 501-508, New York, NY, USA, 2006.",
                "ACM Press. [22] X. Wu and R. K. Srihari.",
                "Incorporating prior knowledge with weighted margin support vector machines.",
                "In Proc.",
                "ACM Knowledge Discovery Data Mining Conf. (ACM SIGKDD 2004), Aug. 2004. [23] Y. Yang, S. Yoo, J. Zhang, and B. Kisiel.",
                "Robustness of adaptive filtering methods in a cross-benchmark evaluation.",
                "In Proceedings of the 28th Annual International ACM SIGIR Conference on Research and Development in Information Retrieval, 2005. [24] K. Yu, V. Tresp, and A. Schwaighofer.",
                "Learning gaussian processes from multiple tasks.",
                "In ICML 05: Proceedings of the 22nd international conference on Machine learning, pages 1012-1019, New York, NY, USA, 2005.",
                "ACM Press. [25] K. Yu, V. Tresp, and S. Yu.",
                "A nonparametric hierarchical bayesian framework for information filtering.",
                "In SIGIR 04: Proceedings of the 27th annual international ACM SIGIR conference on Research and development in information retrieval, pages 353-360.",
                "ACM Press, 2004. [26] X. Zhu.",
                "Semi-supervised learning literature survey.",
                "Technical report, University of Wisconsin - Madison, December 9, 2006. [27] P. Zigoris and Y. Zhang.",
                "Bayesian adaptive user profiling with explicit & implicit feedback.",
                "In Conference on Information and Knowledge Mangement 2006, 2006."
            ],
            "original_annotated_samples": [],
            "translated_annotated_samples": [
                "El algoritmo EM comúnmente utilizado converge muy lentamente debido a la escasez de los datos en aplicaciones \"IR\".",
                "El trabajo relacionado que proporciona recomendaciones personalizadas a los usuarios se ha identificado como un problema muy importante en la comunidad \"IR\" desde la década de 1970.",
                "E PASO: Para cada usuario m, estime la distribución del modelo de usuario P (WM | DM, φ) = N (Wm; ¯wm, σ2 m) basado en la estimación actual del anterior φ = (µ, σ2).¯WM = (((σ2) −1 + sxx, m σ2) −1 (sxy, m σ2 + (σ2) −1 µ)(7) Donde SXX, M = J XM, JXT M, J SXY, M = J XM, JYM, J M Paso: Optimice el anterior φ = (µ, σ2) en función de la estimación del último paso E.µ = 1 m m ¯wm (8) σ2 = 1 m m σ2 m + (¯wm - µ) (¯wm - µ) t (9) Muchos sistemas \"IR\" impulsados por el aprendizaje automático usan una estimación puntual de los parámetros aDiferentes etapas en el sistema.",
                "Por simplicidad, y como una práctica común en \"IR\", no modelamos la correlación entre las características.",
                "En el resto del documento, usamos estos símbolos para representar sus aproximaciones diagonales: σ2 =     σ2 1 0 .. 0 0 σ2 2 .. 0 .. .. .. .. 0 0 .. σ2 k    σ2 m =     σ2 m, 1 0 .. 0 0 σ2 m, 2 .. 0 .. .. .. 0 0 .. σ2 m, k    en segundo lugar, y la mayoríaEs importante destacar que el espacio de entrada es muy escaso y hay muchas dimensiones que no están relacionadas con un usuario en particular en una aplicación real \"IR\".",
                "La investigación tiene mucho potencial para beneficiar a las personas que usan el algoritmo EM en muchos otros problemas de \"IR\", así como problemas de aprendizaje automático.",
                "Se utiliza para encontrar parámetros del modelo en muchos problemas \"IR\" donde los datos de entrenamiento son muy escasos."
            ],
            "translated_text": "",
            "candidates": [
                "IR",
                "IR",
                "IR",
                "IR",
                "IR",
                "IR",
                "IR",
                "IR",
                "IR",
                "IR",
                "IR",
                "IR",
                "IR",
                "IR"
            ],
            "error": []
        },
        "em algorithm": {
            "translated_key": "algoritmo EM",
            "is_in_text": true,
            "original_annotated_sentences": [
                "Efficient Bayesian Hierarchical User Modeling for Recommendation Systems Yi Zhang, Jonathan Koren School of Engineering University of California Santa Cruz Santa Cruz, CA, USA {yiz, jonathan}@soe.ucsc.edu ABSTRACT A content-based personalized recommendation system learns user specific profiles from user feedback so that it can deliver information tailored to each individual users interest.",
                "A system serving millions of users can learn a better user profile for a new user, or a user with little feedback, by borrowing information from other users through the use of a Bayesian hierarchical model.",
                "Learning the model parameters to optimize the joint data likelihood from millions of users is very computationally expensive.",
                "The commonly used <br>em algorithm</br> converges very slowly due to the sparseness of the data in IR applications.",
                "This paper proposes a new fast learning technique to learn a large number of individual user profiles.",
                "The efficacy and efficiency of the proposed algorithm are justified by theory and demonstrated on actual user data from Netflix and MovieLens.",
                "Categories and Subject Descriptors: B.3.3 [Information Search and Retrieval]: Information filtering General Terms: Algorithms 1.",
                "INTRODUCTION Personalization is the future of the Web, and it has achieved great success in industrial applications.",
                "For example, online stores, such as Amazon and Netflix, provide customized recommendations for additional products or services based on a users history.",
                "Recent offerings such as My MSN, My Yahoo!, My Google, and Google News have attracted much attention due to their potential ability to infer a users interests from his/her history.",
                "One major personalization topic studied in the information retrieval community is content-based personal recommendation systems1 .",
                "These systems learn user-specific profiles from user feedback so that they can recommend information tailored to each individual users interest without requiring the user to make an explicit query.",
                "Learning the user profiles is the core problem for these systems.",
                "A user profile is usually a classifier that can identify whether a document is relevant to the user or not, or a regression model that tells how relevant a document is to the user.",
                "One major challenge of building a recommendation or personalization system is that the profile learned for a particular user is usually of low quality when the amount of data from that particular user is small.",
                "This is known as the cold start problem.",
                "This means that any new user must endure poor initial performance until sufficient feedback from that user is provided to learn a reliable user profile.",
                "There has been much research on improving classification accuracy when the amount of labeled training data is small.",
                "The semi-supervised learning approach combines unlabeled and labeled data together to achieve this goal [26].",
                "Another approach is using domain knowledge.",
                "Researchers have modified different learning algorithms, such as Na¨ıveBayes [17], logistic regression [7], and SVMs [22], to integrate domain knowledge into a text classifier.",
                "The third approach is borrowing training data from other resources [5][7].",
                "The effectiveness of these different approaches is mixed, due to how well the underlying model assumption fits the data.",
                "One well-received approach to improve recommendation system performance for a particular user is borrowing information from other users through a Bayesian hierarchical modeling approach.",
                "Several researchers have demonstrated that this approach effectively trades off between shared and user-specific information, thus alleviating poor initial performance for each user[27][25].",
                "In order to learn a Bayesian hierarchical model, the system usually tries to find the most likely model parameters for the given data.",
                "A mature recommendation system usually works for millions of users.",
                "It is well known that learning the optimal parameters of a Bayesian hierarchical model is computationally expensive when there are thousands or millions of users.",
                "The <br>em algorithm</br> is a commonly used technique for parameter learning due to its simplicity and convergence guarantee.",
                "However, a content based recommendation system often handles documents in a very high dimensional space, in which each document is represented by a very sparse vector.",
                "With careful analysis of the <br>em algorithm</br> in this scenario (Section 4), we find that the EM tering, or item-based collaborative filtering.",
                "In this paper, the words filtering and recommendation are used interchangeably. algorithm converges very slowly due to the sparseness of the input variables.",
                "We also find that updating the model parameter at each EM iteration is also expensive with computational complexity of O(MK), where M is the number of users and K is the number of dimensions.",
                "This paper modifies the standard <br>em algorithm</br> to create an improved learning algorithm, which we call the Modified <br>em algorithm</br>.",
                "The basic idea is that instead of calculating the numerical solution for all the user profile parameters, we derive the analytical solution of the parameters for some feature dimensions, and at the M step use the analytical solution instead of the numerical solution estimated at E step for those parameters.",
                "This greatly reduces the computation at a single EM iteration, and also has the benefit of increasing the convergence speed of the learning algorithm.",
                "The proposed technique is not only well supported by theory, but also by experimental results.",
                "The organization of the remaining parts of this paper is as follows: Section 3 describes the Bayesian hierarchical linear regression modeling framework used for content-based recommendations.",
                "Section 4 describes how to learn the model parameters using the standard <br>em algorithm</br>, along with using the new technique proposed in this paper.",
                "The experimental setting and results used to validate the proposed learning technique are reported in Sections 5 and 6.",
                "Section 7 summarizes and offers concluding remarks. 2.",
                "RELATED WORK Providing personalized recommendations to users has been identified as a very important problem in the IR community since the 1970s.",
                "The approaches that have been used to solve this problem can be roughly classified into two major categories: content based filtering versus collaborative filtering.",
                "Content-based filtering studies the scenario where a recommendation system monitors a document stream and pushes documents that match a user profile to the corresponding user.",
                "The user may read the delivered documents and provide explicit relevance feedback, which the filtering system then uses to update the users profile using relevance feedback retrieval models (e.g.",
                "Boolean models, vector space models, traditional probabilistic models [20] , inference networks [3] and language models [6]) or machine learning algorithms (e.g.",
                "Support Vector Machines (SVM), K nearest neighbors (K-NN) clustering, neural networks, logistic regression, or Winnow [16] [4] [23]).",
                "Collaborative filtering goes beyond merely using document content to recommend items to a user by leveraging information from other users with similar tastes and preferences in the past.",
                "Memorybased heuristics and model based approaches have been used in collaborative filtering task [15] [8] [2] [14] [12] [11].",
                "This paper contributes to the content-based recommendation research by improving the efficiency and effectiveness of Bayesian hierarchical linear models, which have a strong theoretical basis and good empirical performance on recommendation tasks[27][25].",
                "This paper does not intend to compare content-based filtering with collaborative filtering or claim which one is a better.",
                "We think each complements the other, and that content-based filtering is extremely useful for handling new documents/items with little or no user feedback.",
                "Similar to some other researchers[18][1][21], we found that a recommendation system will be more effective when both techniques are combined.",
                "However, this is beyond the scope of this paper and thus not discussed here. 3.",
                "BAYESIAN HIERARCHICAL LINEAR REGRESSION Assume there are M users in the system.",
                "The task of the system is to recommend documents that are relevant to each user.",
                "For each user, the system learns a user model from the users history.",
                "In the rest of this paper, we will use the following notations to represent the variables in the system. m = 1, 2, ..., M: The index for each individual user.",
                "M is the total number of users. wm: The user model parameter associated with user m. wm is a K dimensional vector. j = 1, 2, ..., Jm: The index for a set of data for user m. Jm is the number of training data for user m. Dm = {(xm,j, ym,j)}: A set of data associated with user m. xm,j is a K dimensional vector that represents the mth users jth training document.2 ym,j is a scalar that represents the label of document xm,j. k = 1, 2, ..., K: The dimensional index of input variable x.",
                "The Bayesian hierarchical modeling approach has been widely used in real-world information retrieval applications.",
                "Generalized Bayesian hierarchical linear models, one of the simplest Bayesian hierarchical models, are commonly used and have achieved good performance on collaborative filtering [25] and content-based adaptive filtering [27] tasks.",
                "Figure 1 shows the graphical representation of a Bayesian hierarchical model.",
                "In this graph, each user model is represented by a random vector wm.",
                "We assume a user model is sampled randomly from a prior distribution P(w|Φ).",
                "The system can predict the user label y of a document x given an estimation of wm (or wms distribution) using a function y = f(x, w).",
                "The model is called generalized Bayesian hierarchical linear model when y = f(wT x) is any generalized linear model such as logistic regression, SVM, and linear regression.",
                "To reliably estimate the user model wm, the system can borrow information from other users through the prior Φ = (µ, Σ).",
                "Now we look at one commonly used model where y = wT x + , where ∼ N(0, σ2 ) is a random noise [25][27].",
                "Assume that each user model wm is an independent draw from a population distribution P(w|Φ), which is governed by some unknown hyperparameter Φ.",
                "Let the prior distribution of user model w be a Gaussian distribution with parameter Φ = (µ, Σ), which is the commonly used prior for linear models. µ = (µ1, µ2, ..., µK ) is a K dimensional vector that represents the mean of the Gaussian distribution, and Σ is the covariance matrix of the Gaussian.",
                "Usually, a Normal distribution N(0, aI) and an Inverse Wishart distribution P(Σ) ∝ |Σ|− 1 2 b exp(−1 2 ctr(Σ−1 )) are used as hyperprior to model the prior distribution of µ and Σ respectively.",
                "I is the K dimensional identity matrix, and a, b, and c are real numbers.",
                "With these settings, we have the following model for the system: 1. µ and Σ are sampled from N(0, aI) and IWν (aI), respectively. 2 The first dimension of x is a dummy variable that always equals to 1.",
                "Figure 1: Illustration of dependencies of variables in the hierarchical model.",
                "The rating, y, for a document, x, is conditioned on the document and the user model, wm, associated with the user m. Users share information about their models through the prior, Φ = (µ, Σ). 2.",
                "For each user m, wm is sampled randomly from a Normal distribution: wm ∼ N(µ, Σ2 ) 3.",
                "For each item xm,j, ym,j is sampled randomly from a Normal distribution: ym,j ∼ N(wT mxm,j, σ2 ).",
                "Let θ = (Φ, w1, w2, ..., wM ) represent the parameters of this system that needs to be estimated.",
                "The joint likelihood for all the variables in the probabilistic model, which includes the data and the parameters, is: P(D, θ) = P(Φ) m P(wm|Φ) j P(ym,j|xm,j, wm) (1) For simplicity, we assume a, b, c, and σ are provided to the system. 4.",
                "MODEL PARAMETER LEARNING If the prior Φ is known, finding the optimal wm is straightforward: it is a simple linear regression.",
                "Therefore, we will focus on estimating Φ.",
                "The maximum a priori solution of Φ is given by ΦMAP = arg max Φ P(Φ|D) (2) = arg max Φ P(Φ, D) P(D) (3) = arg max Φ P(D|Φ)P(Φ) (4) = arg max Φ w P(D|w, Φ)P(w|Φ)P(Φ)dw (5) Finding the optimal solution for the above problem is challenging, since we need to integrate over all w = (w1, w2, ..., wM ), which are unobserved hidden variables. 4.1 <br>em algorithm</br> for Bayesian Hierarchical Linear Models In Equation 5, Φ is the parameter needs to be estimated, and the result depends on unobserved latent variables w. This kind of optimization problem is usually solved by the <br>em algorithm</br>.",
                "Applying EM to the above problem, the set of user models w are the unobservable hidden variables and we have: Q = w P(w|µ, Σ2 , Dm) log P(µ, Σ2 , w, D)dw Based on the derivation of the EM formulas presented in [24], we have the following Expectation-Maximization steps for finding the optimal hyperparameters.",
                "For space considerations, we omit the derivation in this paper since it is not the focus of our work.",
                "E step: For each user m, estimate the user model distribution P(wm|Dm, Φ) = N(wm; ¯wm, Σ2 m) based on the current estimation of the prior Φ = (µ, Σ2 ). ¯wm = ((Σ2 )−1 + Sxx,m σ2 )−1 ( Sxy,m σ2 + (Σ2 )−1 µ) (6) Σ2 m = ((Σ2 )−1 + Sxx,m σ2 )−1 (7) where Sxx,m = j xm,jxT m,j Sxy,m = j xm,jym,j M step: Optimize the prior Φ = (µ, Σ2 ) based on the estimation from the last E step. µ = 1 M m ¯wm (8) Σ2 = 1 M m Σ2 m + ( ¯wm − µ)( ¯wm − µ)T (9) Many machine learning driven IR systems use a point estimate of the parameters at different stages in the system.",
                "However, we are estimating the posterior distribution of the variables at the E step.",
                "This avoids overfitting wm to a particular users data, which may be small and noisy.",
                "A detailed discussion about this subject appears in [10]. 4.2 New Algorithm: Modified EM Although the <br>em algorithm</br> is widely studied and used in machine learning applications, using the above EM process to solve Bayesian hierarchical linear models in large-scale information retrieval systems is still too computationally expensive.",
                "In this section, we describe why the learning rate of the <br>em algorithm</br> is slow in our application and introduce a new technique to make the learning of the Bayesian hierarchical linear model scalable.",
                "The derivation of the new learning algorithm will be based on the <br>em algorithm</br> described in the previous section.",
                "First, the covariance matrices Σ2 , Σ2 m are usually too large to be computationally feasible.",
                "For simplicity, and as a common practice in IR, we do not model the correlation between features.",
                "Thus we approximate these matrices with K dimensional diagonal matrices.",
                "In the rest of the paper, we use these symbols to represent their diagonal approximations: Σ2 =     σ2 1 0 .. 0 0 σ2 2 .. 0 .. .. .. .. 0 0 .. σ2 K     Σ2 m =     σ2 m,1 0 .. 0 0 σ2 m,2 .. 0 .. .. .. .. 0 0 .. σ2 m,K     Secondly, and most importantly, the input space is very sparse and there are many dimensions that are not related to a particular user in a real IR application.",
                "For example, let us consider a movie recommendation system, with the input variable x representing a particular movie.",
                "For the jth movie that the user m has seen, let xm,j,k = 1 if the director of the movie is Jean-Pierre Jeunet (indexed by k).",
                "Here we assume that whether or not that this director directed a specific movie is represented by the kth dimension.",
                "If the user m has never seen a movie directed by Jean-Pierre Jeunet, then the corresponding dimension is always zero (xm,j,k = 0 for all j) .",
                "One major drawback of the <br>em algorithm</br> is that the importance of a feature, µk, may be greatly dominated by users who have never encountered this feature (i.e. j xm,j,k = 0) at the M step (Equation 8).",
                "Assume that 100 out of 1 million users have viewed the movie directed by Jean-Pierre Jeunet, and that the viewers have rated all of his movies as excellent.",
                "Intuitively, he is a good director and the weight for him (µk) should be high.",
                "Before the EM iteration, the initial value of µ is usually set to 0.",
                "Since the other 999,900 users have not seen this movie, their corresponding weights (w1,k, w2,k, ..., wm,k..., w999900,k) for that director would be very small initially.",
                "Thus the corresponding weight of the director in the prior µk at the first M step would be very low , and the variance σm,k will be large (Equations 8 and 7).",
                "It is undesirable that users who have never seen any movie produced by the director influence the importance of the director so much.",
                "This makes the convergence of the standard <br>em algorithm</br> very slow.",
                "Now lets look at whether we can improve the learning speed of the algorithm.",
                "Without a loss of generality, let us assume that the kth dimension of the input variable x is not related to a particular user m. By which we mean, xm,j,k = 0 for all j = 1, ..., Jm.",
                "It is straightforward to prove that the kth row and kth column of Sxx,m are completely filled with zeros, and that the kth dimension of Sxy,m is zeroed as well.",
                "Thus the corresponding kth dimension of the user models mean, ¯wm, should be equal to that of the prior: ¯wm,k = µk, with the corresponding covariance of σm,k = σk.",
                "At the M step, the standard <br>em algorithm</br> uses the numerical solution of the distribution P(wm|Dm, Φ) estimated at E step (Equation 8 and Equation 7).",
                "However, the numerical solutions are very unreliable for ¯wm,k and σm,k when the kth dimension is not related to the mth user.",
                "A better approach is using the analytical solutions ¯wm,k = µk, and σm,k = σk for the unrelated (m, k) pairs, along with the numerical solution estimated at E step for the other (m, k) pairs.",
                "Thus we get the following new EM-like algorithm: Modified E step: For each user m, estimate the user model distribution P(wm|Dm, Φ) = N(wm; ¯wm, Σ2 m) based on the current estimation of σ , µ, Σ2 . ¯wm = ((Σ2 )−1 + Sxx,m σ2 )−1 ( Sxy,m σ2 + (Σ2 )−1 µ)(10) σ2 m,k = ((σ2 k)−1 + sxx,m,k σ2 )−1 (11) where sxx,m,k = j x2 m,j,k and sxy,m,k = j xm,j,kym,j Modified M Step Optimize the prior Φ = (µ, Σ2 ) based on the estimation from the last E step for related userfeature pairs.",
                "The M step implicitly uses the analytical solution for unrelated user-feature pairs. µk = 1 Mk m:related ¯wm,k (12) σ2 k = 1 Mk m:related σ2 m,k +( ¯wm,k − µk)( ¯wm,k − µk)T (13) where Mk is the number of users that are related to feature k We only estimate the diagonal of Σ2 m and Σ since we are using the diagonal approximation of the covariance matrices.",
                "To estimate ¯wm, we only need to calculate the numerical solutions for dimensions that are related to user m. To estimate σ2 k and µk, we only sum over users that are related to the kth feature.",
                "There are two major benefits of the new algorithm.",
                "First, because only the related (m, k) pairs are needed at the modified M step, the computational complexity in a single EM iteration is much smaller when the data is sparse, and many of (m, k) pairs are unrelated.",
                "Second, the parameters estimated at the modified M step (Equations 12 - 13) are more accurate than the standard M step described in Section 4.1 because the exact analytical solutions ¯wm,k = µk and σm,k = σk for the unrelated (m, k) pairs were used in the new algorithm instead of an approximate solution as in the standard algorithm. 5.",
                "EXPERIMENTAL METHODOLOGY 5.1 Evaluation Data Set To evaluate the proposed technique, we used the following three major data sets (Table 1): MovieLens Data: This data set was created by combining the relevance judgments from the MovieLens[9] data set with documents from the Internet Movie Database (IMDB).",
                "MovieLens allows users to rank how much he/she enjoyed a specific movie on a scale from 1 to 5.",
                "This likeability rating was used as a measurement of how relevant the document representing the corresponding movie is to the user.",
                "We considered documents with likeability scores of 4 or 5 as relevant, and documents with a score of 1 to 3 as irrelevant to the user.",
                "MovieLens provided relevance judgments on 3,057 documents from 6,040 separate users.",
                "On average, each user rated 151 movies, of these 87 were judged to be relevant.",
                "The average score for a document was 3.58.",
                "Documents representing each movie were constructed from the portion of the IMDB database that is available for public download[13].",
                "Based on this database, we created one document per movie that contained the relevant information about it (e.g. directors, actors, etc.).",
                "Table 1: Data Set Statistics.",
                "On Reuters, the number of rating for a simulated user is the number of documents relevant to the corresponding topic.",
                "Data Users Docs Ratings per User MovieLens 6,040 3,057 151 Netflix-all 480,189 17,770 208 Netflix-1000 1000 17,770 127 Reuters-C 34 100,000 3949 Reuters-E 26 100,000 1632 Reuters-G 33 100,000 2222 Reuters-M 10 100,000 6529 Netflix Data: This data set was constructed by combining documents about movies crawled from the web with a set of actual movie rental customer relevance judgments from Netflix[19].",
                "Netflix publicly provides the relevance judgments of 480,189 anonymous customers.",
                "There are around 100 million rating on a scale of 1 to 5 for 17,770 documents.",
                "Similar to MovieLens, we considered documents with likeability scores of 4 or 5 as relevant.",
                "This number was reduced to 1000 customers through random sampling.",
                "The average customer on the reduced data set provided 127 judgments, with 70 being deemed relevant.",
                "The average score for documents is 3.55.",
                "Reuters Data: This is the Reuters Corpus, Volume 1.",
                "It covers 810,000 Reuters English language news stories from August 20, 1996 to August 19, 1997.",
                "Only the first 100,000 news were used in our experiments.",
                "The Reuters corpus comes with a topic hierarchy.",
                "Each document is assigned to one of several locations on the hierarchical tree.",
                "The first level of the tree contains four topics, denoted as C, E, M, and G. For the experiments in this paper, the tree was cut at level 1 to create four smaller trees, each of which corresponds to one smaller data set: Reuters-E Reuters-C, ReutersM and Reuters-G. For each small data set, we created several profiles, one profile for each node in a sub-tree, to simulate multiple users, each with a related, yet separate definition of relevance.",
                "All the user profiles on a sub-tree are supposed to share the same prior model distribution.",
                "Since this corpus explicitly indicates only the relevant documents for a topic(user), all other documents are considered irrelevant. 5.2 Evaluation We designed the experiments to answer the following three questions: 1.",
                "Do we need to take the effort to use a Bayesian approach and learn a prior from other users? 2.",
                "Does the new algorithm work better than the standard <br>em algorithm</br> for learning the Bayesian hierarchical linear model? 3.",
                "Can the new algorithm quickly learn many user models?",
                "To answer the first question, we compared the Bayesian hierarchical models with commonly used Norm-2 regularized linear regression models.",
                "In fact, the commonly used approach is equivalent to the model learned at the end of the first EM iteration.",
                "To answer the second question, we compared the proposed new algorithm with the standard <br>em algorithm</br> to see whether the new learning algorithm is better.",
                "To answer the third question, we tested the efficiency of the new algorithm on the entire Netflix data set where about half a million user models need to be learned together.",
                "For the MovieLens and Netflix data sets, algorithm effectiveness was measured by mean square error, while on the Reuters data set classification error was used because it was more informative.",
                "We first evaluated the performance on each individual user, and then estimated the macro average over all users.",
                "Statistical tests (t-tests) were carried out to see whether the results are significant.",
                "For the experiments on the MovieLens and Netflix data sets, we used a random sample of 90% of each user for training, and the rest for testing.",
                "On Reuters data set, because there are too many relevant documents for each topic in the corpus, we used a random sample of 10% of each topic for training, and 10% of the remaining documents for testing.",
                "For all runs, we set (a, b, c, Σ ) = (0.1, 10, 0.1, 1) manually. 6.",
                "EXPERIMENTAL RESULTS Figure 2, Figure 3, and Figure 4 show that on all data sets, the Bayesian hierarchical modeling approach has a statistical significant improvement over the regularized linear regression model, which is equivalent to the Bayesian hierarchical models learned at the first iteration.",
                "Further analysis shows a negative correlation between the number of training data for a user and the improvement the system gets.",
                "This suggests that the borrowing information from other users has more significant improvements for users with less training data, which is as expected.",
                "However, the strength of the correlation differs over data sets, and the amount of training data is not the only characteristics that will influence the final performance.",
                "Figure 2 and Figure 3 show that the proposed new algorithm works better than the standard <br>em algorithm</br> on the Netflix and MovieLens data sets.",
                "This is not surprising since the number of related feature-users pairs is much smaller than the number of unrelated feature-user pairs on these two data sets, and thus the proposed new algorithm is expected to work better.",
                "Figure 4 shows that the two algorithms work similarly on the Reuters-E data set.",
                "The accuracy of the new algorithm is similar to that of the standard <br>em algorithm</br> at each iteration.",
                "The general patterns are very similar on other Reuters subsets.",
                "Further analysis shows that only 58% of the user-feature pairs are unrelated on this data set.",
                "Since the number of unrelated user-feature pairs is not extremely large, the sparseness is not a serious problem on the Reuters data set.",
                "Thus the two learning algorithms perform similarly.",
                "The results suggest that only on a corpus where the number of unrelated user-feature pairs is much larger than the number of related pairs, such as on the Netflix data set, the proposed technique will get a significant improvement over standard EM.",
                "However, the experiments also show that when the assumption does not hold, the new algorithm does not hurt performance.",
                "Although the proposed technique is faster than standard Figure 2: Performance on a Netflix subset with 1,000 users.",
                "The new algorithm is statistical significantly better than <br>em algorithm</br> at iterations 2 - 10.",
                "Norm-2 regularized linear models are equivalent to the Bayesian hierarchical models learned at the first iteration, and are statistical significantly worse than the Bayesian hierarchical models. 0 2 4 6 8 10 1 1.05 1.1 1.15 1.2 1.25 1.3 1.35 1.4 Iterations MeanSquareError New Algorithm Traditional EM 1 2 3 4 5 6 7 8 9 10 0.33 0.34 0.35 0.36 0.37 0.38 0.39 Iterations ClassificationError New Algorithm Traditional EM Figure 3: Performance on a MovieLens subset with 1,000 users.",
                "The new algorithm is statistical significantly better than <br>em algorithm</br> at iteration 2 to 17 (evaluated with mean square error). 1 6 11 16 21 0.5 1 1.5 2 2.5 3 3.5 Iterations MeanSquareError New Algorithm Traditional EM 1 6 11 16 21 0.35 0.4 0.45 0.5 0.55 0.6 0.65 Iterations ClassificationError New Algorithm Traditional EM Figure 4: Performance on a Reuters-E subset with 26 profiles.",
                "Performances on Reuters-C, Reuters-M, Reuters-G are similar. 1 2 3 4 5 0.011 0.0115 0.012 0.0125 0.013 0.0135 0.014 Iterations MeanSquareError New Algorithm Traditional EM 1 2 3 4 5 0.0102 0.0104 0.0106 0.0108 0.011 0.0112 0.0114 Iterations ClassificationError New Algorithm Traditional EM EM, can it really learn millions of user models quickly?",
                "Our results show that the modified <br>em algorithm</br> converges quickly, and 2 - 3 modified EM iterations would result in a reliable estimation.",
                "We evaluated the algorithm on the whole Netflix data set (480,189 users, 159,836 features, and 100 million ratings) running on a single CPU PC (2GB memory, P4 3GHz).",
                "The system finished one modified EM iteration in about 4 hours.",
                "This demonstrates that the proposed technique can efficiently handle large-scale system like Netflix. 7.",
                "CONCLUSION Content-based user profile learning is an important problem and is the key to providing personal recommendations to a user, especially for recommending new items with a small number of ratings.",
                "The Bayesian hierarchical modeling approach is becoming an important user profile learning approach due to its theoretically justified ability to help one user through information transfer from the other users by way of hyperpriors.",
                "This paper examined the weakness of the popular EM based learning approach for Bayesian hierarchical linear models and proposed a better learning technique called Modified EM.",
                "We showed that the new technique is theoretically more computationally efficient than the standard <br>em algorithm</br>.",
                "Evaluation on the MovieLens and Netflix data sets demonstrated the effectiveness of the new technique when the data is sparse, by which we mean the ratio of related user-feature pairs to unrelated pairs is small.",
                "Evaluation on the Reuters data set showed that the new technique performed similar to the standard <br>em algorithm</br> when the sparseness condition does not hold.",
                "In general, it is better to use the new algorithm since it is as simple as standard EM, the performance is either better or similar to EM, and the computation complexity is lower at each iteration.",
                "It is worth mentioning that even if the original problem space is not sparse, sparseness can be created artificially when a recommendation system uses user-specific feature selection techniques to reduce the noise and user model complexity.",
                "The proposed technique can also be adapted to improve the learning in such a scenario.",
                "We also demonstrated that the proposed technique can learn half a million user profiles from 100 million ratings in a few hours with a single CPU.",
                "The research is important because scalability is a major concern for researchers when using the Bayesian hierarchical linear modeling approach to build a practical large scale system, even though the literature have demonstrated the effectiveness of the models in many applications.",
                "Our work is one major step on the road to make Bayesian hierarchical linear models more practical.",
                "The proposed new technique can be easily adapted to run on a cluster of machines, and thus further speed up the learning process to handle a larger scale system with hundreds of millions of users.",
                "The research has much potential to benefit people using <br>em algorithm</br> on many other IR problems as well as machine learning problems.",
                "<br>em algorithm</br> is a commonly used machine learning technique.",
                "It is used to find model parameters in many IR problems where the training data is very sparse.",
                "Although we are focusing on the Bayesian hierarchical linear models for recommendation and filtering, the new idea of using analytical solution instead of numerical solution for unrelated user-feature pairs at the M step could be adapted to many other problems. 8.",
                "ACKNOWLEDGMENTS We thank Wei Xu, David Lewis and anonymous reviewers for valuable feedback on the work described in this paper.",
                "Part of the work was supported by Yahoo, Google, the Petascale Data Storage Institute and the Institute for Scalable Scientific Data Management.",
                "Any opinions, findings, conclusions, or recommendations expressed in this material are those of the authors, and do not necessarily reflect those of the sponsors. 9.",
                "REFERENCES [1] C. Basu, H. Hirsh, and W. Cohen.",
                "Recommendation as classification: Using social and content-based information in recommendation.",
                "In Proceedings of the Fifteenth National Conference on Artificial Intelligence, 1998. [2] J. S. Breese, D. Heckerman, and C. Kadie.",
                "Empirical analysis of predictive algorithms for collaborative filtering.",
                "Technical report, Microsoft Research, One Microsoft Way, Redmond, WA 98052, 1998. [3] J. Callan.",
                "Document filtering with inference networks.",
                "In Proceedings of the Nineteenth Annual International ACM SIGIR Conference on Research and Development in Information Retrieval, pages 262-269, 1996. [4] N. Cancedda, N. Cesa-Bianchi, A. Conconi, C. Gentile, C. Goutte, T. Graepel, Y. Li, J. M. Renders, J. S. Taylor, and A. Vinokourov.",
                "Kernel method for document filtering.",
                "In The Eleventh Text REtrieval Conference (TREC11).",
                "National Institute of Standards and Technology, special publication 500-249, 2003. [5] C. Chelba and A. Acero.",
                "Adaptation of maximum entropy capitalizer: Little data can help a lot.",
                "In D. Lin and D. Wu, editors, Proceedings of EMNLP 2004, pages 285-292, Barcelona, Spain, July 2004.",
                "Association for Computational Linguistics. [6] B. Croft and J. Lafferty, editors.",
                "Language Modeling for Information Retrieval.",
                "Kluwer, 2002. [7] A. Dayanik, D. D. Lewis, D. Madigan, V. Menkov, and A. Genkin.",
                "Constructing informative prior distributions from domain knowledge in text classification.",
                "In SIGIR 06: Proceedings of the 29th annual international ACM SIGIR conference on Research and development in information retrieval, pages 493-500, New York, NY, USA, 2006.",
                "ACM Press. [8] J. Delgado and N. Ishii.",
                "Memory-based weightedmajority prediction for recommender systems.",
                "In ACM SIGIR99 Workshop on Recommender Systems, 1999. [9] GroupLens.",
                "Movielens. http://www.grouplens.org/taxonomy/term/14, 2006. [10] D. Heckerman.",
                "A tutorial on learning with bayesian networks.",
                "In M. Jordan, editor, Learning in Graphical Models.",
                "Kluwer Academic, 1998. [11] J. L. Herlocker, J.",
                "A. Konstan, A. Borchers, and J. Riedl.",
                "An algorithmic framework for performing collaborative filtering.",
                "In SIGIR 99: Proceedings of the 22nd annual international ACM SIGIR conference on Research and development in information retrieval, pages 230-237, New York, NY, USA, 1999.",
                "ACM Press. [12] T. Hofmann and J. Puzicha.",
                "Latent class models for collaborative filtering.",
                "In IJCAI 99: Proceedings of the Sixteenth International Joint Conference on Artificial Intelligence, pages 688-693, San Francisco, CA, USA, 1999.",
                "Morgan Kaufmann Publishers Inc. [13] I. M. D. (IMDB).",
                "Internet movie database. http://www.imdb.com/interfaces/, 2006. [14] R. Jin, J. Y. Chai, and L. Si.",
                "An automatic weighting scheme for collaborative filtering.",
                "In SIGIR 04: Proceedings of the 27th annual international ACM SIGIR conference on Research and development in information retrieval, pages 337-344, New York, NY, USA, 2004.",
                "ACM Press. [15] J.",
                "A. Konstan, B. N. Miller, D. Maltz, J. L. Herlocker, L. R. Gordon, and J. Riedl.",
                "GroupLens: Applying collaborative filtering to Usenet news.",
                "Communications of the ACM, 40(3):77-87, 1997. [16] D. Lewis.",
                "Applying support vector machines to the TREC-2001 batch filtering and routing tasks.",
                "In Proceedings of the Eleventh Text REtrieval Conference (TREC-11), 2002. [17] B. Liu, X. Li, W. S. Lee, , and P. Yu.",
                "Text classification by labeling words.",
                "In Proceedings of The Nineteenth National Conference on Artificial Intelligence (AAAI-2004), July 25-29, 2004. [18] P. Melville, R. J. Mooney, and R. Nagarajan.",
                "Content-boosted collaborative filtering for improved recommendations.",
                "In Proceedings of the Eighteenth National Conference on Artificial Intelligence (AAAI-2002), Edmonton, Canada, 2002. [19] Netflix.",
                "Netflix prize. http://www.netflixprize.com (visited on Nov. 30, 2006), 2006. [20] S. Robertson and K. Sparck-Jones.",
                "Relevance weighting of search terms.",
                "In Journal of the American Society for Information Science, volume 27, pages 129-146, 1976. [21] J. Wang, A. P. de Vries, and M. J. T. Reinders.",
                "Unifying user-based and item-based collaborative filtering approaches by similarity fusion.",
                "In SIGIR 06: Proceedings of the 29th annual international ACM SIGIR conference on Research and development in information retrieval, pages 501-508, New York, NY, USA, 2006.",
                "ACM Press. [22] X. Wu and R. K. Srihari.",
                "Incorporating prior knowledge with weighted margin support vector machines.",
                "In Proc.",
                "ACM Knowledge Discovery Data Mining Conf. (ACM SIGKDD 2004), Aug. 2004. [23] Y. Yang, S. Yoo, J. Zhang, and B. Kisiel.",
                "Robustness of adaptive filtering methods in a cross-benchmark evaluation.",
                "In Proceedings of the 28th Annual International ACM SIGIR Conference on Research and Development in Information Retrieval, 2005. [24] K. Yu, V. Tresp, and A. Schwaighofer.",
                "Learning gaussian processes from multiple tasks.",
                "In ICML 05: Proceedings of the 22nd international conference on Machine learning, pages 1012-1019, New York, NY, USA, 2005.",
                "ACM Press. [25] K. Yu, V. Tresp, and S. Yu.",
                "A nonparametric hierarchical bayesian framework for information filtering.",
                "In SIGIR 04: Proceedings of the 27th annual international ACM SIGIR conference on Research and development in information retrieval, pages 353-360.",
                "ACM Press, 2004. [26] X. Zhu.",
                "Semi-supervised learning literature survey.",
                "Technical report, University of Wisconsin - Madison, December 9, 2006. [27] P. Zigoris and Y. Zhang.",
                "Bayesian adaptive user profiling with explicit & implicit feedback.",
                "In Conference on Information and Knowledge Mangement 2006, 2006."
            ],
            "original_annotated_samples": [],
            "translated_annotated_samples": [
                "El \"algoritmo EM\" comúnmente utilizado converge muy lentamente debido a la escasez de los datos en aplicaciones IR.",
                "El \"Algoritmo EM\" es una técnica de uso común para el aprendizaje de parámetros debido a su garantía de simplicidad y convergencia.",
                "Con un análisis cuidadoso del \"algoritmo EM\" en este escenario (Sección 4), encontramos que la emergencia o filtrado colaborativo basado en elementos.",
                "Este documento modifica el \"algoritmo EM\" estándar para crear un algoritmo de aprendizaje mejorado, que llamamos el \"algoritmo EM\" modificado.",
                "La Sección 4 describe cómo aprender los parámetros del modelo utilizando el \"algoritmo EM\" estándar, junto con el uso de la nueva técnica propuesta en este documento.",
                "La solución a priori máxima de φ viene dada por φmap = arg max φ p (φ | d) (2) = arg max φ p (φ, d) p (d) (3) = arg max φ p (d | φ) P (φ) (4) = arg max φ w p (d | w, φ) p (w | φ) p (φ) dw (5) Encontrar la solución óptima para el problema anterior es un desafío, ya que necesitamosIntegrar sobre todo W = (W1, W2, ..., WM), que son variables ocultas no observadas.4.1 \"Algoritmo EM\" para modelos lineales jerárquicos bayesianos en la Ecuación 5, φ es el parámetro que debe estimarse, y el resultado depende de variables latentes no observadas w.Este tipo de problema de optimización generalmente se resuelve mediante el \"algoritmo EM\".",
                "Una discusión detallada sobre este tema aparece en [10].4.2 Nuevo algoritmo: EM modificado Aunque el \"algoritmo EM\" está ampliamente estudiado y utilizado en aplicaciones de aprendizaje automático, utilizando el proceso EM anterior para resolver modelos lineales jerárquicos bayesianos en los sistemas de recuperación de información a gran escala todavía es demasiado costoso.",
                "En esta sección, describimos por qué la tasa de aprendizaje del \"Algoritmo EM\" es lenta en nuestra aplicación e introduce una nueva técnica para hacer que el aprendizaje del modelo lineal jerárquico bayesiano sea escalable.",
                "La derivación del nuevo algoritmo de aprendizaje se basará en el \"algoritmo EM\" descrito en la sección anterior.",
                "Un inconveniente importante del \"Algoritmo EM\" es que la importancia de una característica, µK, puede estar muy dominada por usuarios que nunca han encontrado esta característica (es decir, J XM, J, K = 0) en el paso M (Ecuación 8)."
            ],
            "translated_text": "",
            "candidates": [
                "algoritmo",
                "algoritmo EM",
                "algoritmo",
                "Algoritmo EM",
                "algoritmo",
                "algoritmo EM",
                "algoritmo",
                "algoritmo EM",
                "algoritmo EM",
                "algoritmo",
                "algoritmo EM",
                "algoritmo",
                "Algoritmo EM",
                "algoritmo EM",
                "algoritmo",
                "algoritmo EM",
                "algoritmo",
                "Algoritmo EM",
                "algoritmo",
                "algoritmo EM",
                "algoritmo",
                "Algoritmo EM"
            ],
            "error": []
        },
        "classification": {
            "translated_key": "clasificación",
            "is_in_text": true,
            "original_annotated_sentences": [
                "Efficient Bayesian Hierarchical User Modeling for Recommendation Systems Yi Zhang, Jonathan Koren School of Engineering University of California Santa Cruz Santa Cruz, CA, USA {yiz, jonathan}@soe.ucsc.edu ABSTRACT A content-based personalized recommendation system learns user specific profiles from user feedback so that it can deliver information tailored to each individual users interest.",
                "A system serving millions of users can learn a better user profile for a new user, or a user with little feedback, by borrowing information from other users through the use of a Bayesian hierarchical model.",
                "Learning the model parameters to optimize the joint data likelihood from millions of users is very computationally expensive.",
                "The commonly used EM algorithm converges very slowly due to the sparseness of the data in IR applications.",
                "This paper proposes a new fast learning technique to learn a large number of individual user profiles.",
                "The efficacy and efficiency of the proposed algorithm are justified by theory and demonstrated on actual user data from Netflix and MovieLens.",
                "Categories and Subject Descriptors: B.3.3 [Information Search and Retrieval]: Information filtering General Terms: Algorithms 1.",
                "INTRODUCTION Personalization is the future of the Web, and it has achieved great success in industrial applications.",
                "For example, online stores, such as Amazon and Netflix, provide customized recommendations for additional products or services based on a users history.",
                "Recent offerings such as My MSN, My Yahoo!, My Google, and Google News have attracted much attention due to their potential ability to infer a users interests from his/her history.",
                "One major personalization topic studied in the information retrieval community is content-based personal recommendation systems1 .",
                "These systems learn user-specific profiles from user feedback so that they can recommend information tailored to each individual users interest without requiring the user to make an explicit query.",
                "Learning the user profiles is the core problem for these systems.",
                "A user profile is usually a classifier that can identify whether a document is relevant to the user or not, or a regression model that tells how relevant a document is to the user.",
                "One major challenge of building a recommendation or personalization system is that the profile learned for a particular user is usually of low quality when the amount of data from that particular user is small.",
                "This is known as the cold start problem.",
                "This means that any new user must endure poor initial performance until sufficient feedback from that user is provided to learn a reliable user profile.",
                "There has been much research on improving <br>classification</br> accuracy when the amount of labeled training data is small.",
                "The semi-supervised learning approach combines unlabeled and labeled data together to achieve this goal [26].",
                "Another approach is using domain knowledge.",
                "Researchers have modified different learning algorithms, such as Na¨ıveBayes [17], logistic regression [7], and SVMs [22], to integrate domain knowledge into a text classifier.",
                "The third approach is borrowing training data from other resources [5][7].",
                "The effectiveness of these different approaches is mixed, due to how well the underlying model assumption fits the data.",
                "One well-received approach to improve recommendation system performance for a particular user is borrowing information from other users through a Bayesian hierarchical modeling approach.",
                "Several researchers have demonstrated that this approach effectively trades off between shared and user-specific information, thus alleviating poor initial performance for each user[27][25].",
                "In order to learn a Bayesian hierarchical model, the system usually tries to find the most likely model parameters for the given data.",
                "A mature recommendation system usually works for millions of users.",
                "It is well known that learning the optimal parameters of a Bayesian hierarchical model is computationally expensive when there are thousands or millions of users.",
                "The EM algorithm is a commonly used technique for parameter learning due to its simplicity and convergence guarantee.",
                "However, a content based recommendation system often handles documents in a very high dimensional space, in which each document is represented by a very sparse vector.",
                "With careful analysis of the EM algorithm in this scenario (Section 4), we find that the EM tering, or item-based collaborative filtering.",
                "In this paper, the words filtering and recommendation are used interchangeably. algorithm converges very slowly due to the sparseness of the input variables.",
                "We also find that updating the model parameter at each EM iteration is also expensive with computational complexity of O(MK), where M is the number of users and K is the number of dimensions.",
                "This paper modifies the standard EM algorithm to create an improved learning algorithm, which we call the Modified EM algorithm.",
                "The basic idea is that instead of calculating the numerical solution for all the user profile parameters, we derive the analytical solution of the parameters for some feature dimensions, and at the M step use the analytical solution instead of the numerical solution estimated at E step for those parameters.",
                "This greatly reduces the computation at a single EM iteration, and also has the benefit of increasing the convergence speed of the learning algorithm.",
                "The proposed technique is not only well supported by theory, but also by experimental results.",
                "The organization of the remaining parts of this paper is as follows: Section 3 describes the Bayesian hierarchical linear regression modeling framework used for content-based recommendations.",
                "Section 4 describes how to learn the model parameters using the standard EM algorithm, along with using the new technique proposed in this paper.",
                "The experimental setting and results used to validate the proposed learning technique are reported in Sections 5 and 6.",
                "Section 7 summarizes and offers concluding remarks. 2.",
                "RELATED WORK Providing personalized recommendations to users has been identified as a very important problem in the IR community since the 1970s.",
                "The approaches that have been used to solve this problem can be roughly classified into two major categories: content based filtering versus collaborative filtering.",
                "Content-based filtering studies the scenario where a recommendation system monitors a document stream and pushes documents that match a user profile to the corresponding user.",
                "The user may read the delivered documents and provide explicit relevance feedback, which the filtering system then uses to update the users profile using relevance feedback retrieval models (e.g.",
                "Boolean models, vector space models, traditional probabilistic models [20] , inference networks [3] and language models [6]) or machine learning algorithms (e.g.",
                "Support Vector Machines (SVM), K nearest neighbors (K-NN) clustering, neural networks, logistic regression, or Winnow [16] [4] [23]).",
                "Collaborative filtering goes beyond merely using document content to recommend items to a user by leveraging information from other users with similar tastes and preferences in the past.",
                "Memorybased heuristics and model based approaches have been used in collaborative filtering task [15] [8] [2] [14] [12] [11].",
                "This paper contributes to the content-based recommendation research by improving the efficiency and effectiveness of Bayesian hierarchical linear models, which have a strong theoretical basis and good empirical performance on recommendation tasks[27][25].",
                "This paper does not intend to compare content-based filtering with collaborative filtering or claim which one is a better.",
                "We think each complements the other, and that content-based filtering is extremely useful for handling new documents/items with little or no user feedback.",
                "Similar to some other researchers[18][1][21], we found that a recommendation system will be more effective when both techniques are combined.",
                "However, this is beyond the scope of this paper and thus not discussed here. 3.",
                "BAYESIAN HIERARCHICAL LINEAR REGRESSION Assume there are M users in the system.",
                "The task of the system is to recommend documents that are relevant to each user.",
                "For each user, the system learns a user model from the users history.",
                "In the rest of this paper, we will use the following notations to represent the variables in the system. m = 1, 2, ..., M: The index for each individual user.",
                "M is the total number of users. wm: The user model parameter associated with user m. wm is a K dimensional vector. j = 1, 2, ..., Jm: The index for a set of data for user m. Jm is the number of training data for user m. Dm = {(xm,j, ym,j)}: A set of data associated with user m. xm,j is a K dimensional vector that represents the mth users jth training document.2 ym,j is a scalar that represents the label of document xm,j. k = 1, 2, ..., K: The dimensional index of input variable x.",
                "The Bayesian hierarchical modeling approach has been widely used in real-world information retrieval applications.",
                "Generalized Bayesian hierarchical linear models, one of the simplest Bayesian hierarchical models, are commonly used and have achieved good performance on collaborative filtering [25] and content-based adaptive filtering [27] tasks.",
                "Figure 1 shows the graphical representation of a Bayesian hierarchical model.",
                "In this graph, each user model is represented by a random vector wm.",
                "We assume a user model is sampled randomly from a prior distribution P(w|Φ).",
                "The system can predict the user label y of a document x given an estimation of wm (or wms distribution) using a function y = f(x, w).",
                "The model is called generalized Bayesian hierarchical linear model when y = f(wT x) is any generalized linear model such as logistic regression, SVM, and linear regression.",
                "To reliably estimate the user model wm, the system can borrow information from other users through the prior Φ = (µ, Σ).",
                "Now we look at one commonly used model where y = wT x + , where ∼ N(0, σ2 ) is a random noise [25][27].",
                "Assume that each user model wm is an independent draw from a population distribution P(w|Φ), which is governed by some unknown hyperparameter Φ.",
                "Let the prior distribution of user model w be a Gaussian distribution with parameter Φ = (µ, Σ), which is the commonly used prior for linear models. µ = (µ1, µ2, ..., µK ) is a K dimensional vector that represents the mean of the Gaussian distribution, and Σ is the covariance matrix of the Gaussian.",
                "Usually, a Normal distribution N(0, aI) and an Inverse Wishart distribution P(Σ) ∝ |Σ|− 1 2 b exp(−1 2 ctr(Σ−1 )) are used as hyperprior to model the prior distribution of µ and Σ respectively.",
                "I is the K dimensional identity matrix, and a, b, and c are real numbers.",
                "With these settings, we have the following model for the system: 1. µ and Σ are sampled from N(0, aI) and IWν (aI), respectively. 2 The first dimension of x is a dummy variable that always equals to 1.",
                "Figure 1: Illustration of dependencies of variables in the hierarchical model.",
                "The rating, y, for a document, x, is conditioned on the document and the user model, wm, associated with the user m. Users share information about their models through the prior, Φ = (µ, Σ). 2.",
                "For each user m, wm is sampled randomly from a Normal distribution: wm ∼ N(µ, Σ2 ) 3.",
                "For each item xm,j, ym,j is sampled randomly from a Normal distribution: ym,j ∼ N(wT mxm,j, σ2 ).",
                "Let θ = (Φ, w1, w2, ..., wM ) represent the parameters of this system that needs to be estimated.",
                "The joint likelihood for all the variables in the probabilistic model, which includes the data and the parameters, is: P(D, θ) = P(Φ) m P(wm|Φ) j P(ym,j|xm,j, wm) (1) For simplicity, we assume a, b, c, and σ are provided to the system. 4.",
                "MODEL PARAMETER LEARNING If the prior Φ is known, finding the optimal wm is straightforward: it is a simple linear regression.",
                "Therefore, we will focus on estimating Φ.",
                "The maximum a priori solution of Φ is given by ΦMAP = arg max Φ P(Φ|D) (2) = arg max Φ P(Φ, D) P(D) (3) = arg max Φ P(D|Φ)P(Φ) (4) = arg max Φ w P(D|w, Φ)P(w|Φ)P(Φ)dw (5) Finding the optimal solution for the above problem is challenging, since we need to integrate over all w = (w1, w2, ..., wM ), which are unobserved hidden variables. 4.1 EM Algorithm for Bayesian Hierarchical Linear Models In Equation 5, Φ is the parameter needs to be estimated, and the result depends on unobserved latent variables w. This kind of optimization problem is usually solved by the EM algorithm.",
                "Applying EM to the above problem, the set of user models w are the unobservable hidden variables and we have: Q = w P(w|µ, Σ2 , Dm) log P(µ, Σ2 , w, D)dw Based on the derivation of the EM formulas presented in [24], we have the following Expectation-Maximization steps for finding the optimal hyperparameters.",
                "For space considerations, we omit the derivation in this paper since it is not the focus of our work.",
                "E step: For each user m, estimate the user model distribution P(wm|Dm, Φ) = N(wm; ¯wm, Σ2 m) based on the current estimation of the prior Φ = (µ, Σ2 ). ¯wm = ((Σ2 )−1 + Sxx,m σ2 )−1 ( Sxy,m σ2 + (Σ2 )−1 µ) (6) Σ2 m = ((Σ2 )−1 + Sxx,m σ2 )−1 (7) where Sxx,m = j xm,jxT m,j Sxy,m = j xm,jym,j M step: Optimize the prior Φ = (µ, Σ2 ) based on the estimation from the last E step. µ = 1 M m ¯wm (8) Σ2 = 1 M m Σ2 m + ( ¯wm − µ)( ¯wm − µ)T (9) Many machine learning driven IR systems use a point estimate of the parameters at different stages in the system.",
                "However, we are estimating the posterior distribution of the variables at the E step.",
                "This avoids overfitting wm to a particular users data, which may be small and noisy.",
                "A detailed discussion about this subject appears in [10]. 4.2 New Algorithm: Modified EM Although the EM algorithm is widely studied and used in machine learning applications, using the above EM process to solve Bayesian hierarchical linear models in large-scale information retrieval systems is still too computationally expensive.",
                "In this section, we describe why the learning rate of the EM algorithm is slow in our application and introduce a new technique to make the learning of the Bayesian hierarchical linear model scalable.",
                "The derivation of the new learning algorithm will be based on the EM algorithm described in the previous section.",
                "First, the covariance matrices Σ2 , Σ2 m are usually too large to be computationally feasible.",
                "For simplicity, and as a common practice in IR, we do not model the correlation between features.",
                "Thus we approximate these matrices with K dimensional diagonal matrices.",
                "In the rest of the paper, we use these symbols to represent their diagonal approximations: Σ2 =     σ2 1 0 .. 0 0 σ2 2 .. 0 .. .. .. .. 0 0 .. σ2 K     Σ2 m =     σ2 m,1 0 .. 0 0 σ2 m,2 .. 0 .. .. .. .. 0 0 .. σ2 m,K     Secondly, and most importantly, the input space is very sparse and there are many dimensions that are not related to a particular user in a real IR application.",
                "For example, let us consider a movie recommendation system, with the input variable x representing a particular movie.",
                "For the jth movie that the user m has seen, let xm,j,k = 1 if the director of the movie is Jean-Pierre Jeunet (indexed by k).",
                "Here we assume that whether or not that this director directed a specific movie is represented by the kth dimension.",
                "If the user m has never seen a movie directed by Jean-Pierre Jeunet, then the corresponding dimension is always zero (xm,j,k = 0 for all j) .",
                "One major drawback of the EM algorithm is that the importance of a feature, µk, may be greatly dominated by users who have never encountered this feature (i.e. j xm,j,k = 0) at the M step (Equation 8).",
                "Assume that 100 out of 1 million users have viewed the movie directed by Jean-Pierre Jeunet, and that the viewers have rated all of his movies as excellent.",
                "Intuitively, he is a good director and the weight for him (µk) should be high.",
                "Before the EM iteration, the initial value of µ is usually set to 0.",
                "Since the other 999,900 users have not seen this movie, their corresponding weights (w1,k, w2,k, ..., wm,k..., w999900,k) for that director would be very small initially.",
                "Thus the corresponding weight of the director in the prior µk at the first M step would be very low , and the variance σm,k will be large (Equations 8 and 7).",
                "It is undesirable that users who have never seen any movie produced by the director influence the importance of the director so much.",
                "This makes the convergence of the standard EM algorithm very slow.",
                "Now lets look at whether we can improve the learning speed of the algorithm.",
                "Without a loss of generality, let us assume that the kth dimension of the input variable x is not related to a particular user m. By which we mean, xm,j,k = 0 for all j = 1, ..., Jm.",
                "It is straightforward to prove that the kth row and kth column of Sxx,m are completely filled with zeros, and that the kth dimension of Sxy,m is zeroed as well.",
                "Thus the corresponding kth dimension of the user models mean, ¯wm, should be equal to that of the prior: ¯wm,k = µk, with the corresponding covariance of σm,k = σk.",
                "At the M step, the standard EM algorithm uses the numerical solution of the distribution P(wm|Dm, Φ) estimated at E step (Equation 8 and Equation 7).",
                "However, the numerical solutions are very unreliable for ¯wm,k and σm,k when the kth dimension is not related to the mth user.",
                "A better approach is using the analytical solutions ¯wm,k = µk, and σm,k = σk for the unrelated (m, k) pairs, along with the numerical solution estimated at E step for the other (m, k) pairs.",
                "Thus we get the following new EM-like algorithm: Modified E step: For each user m, estimate the user model distribution P(wm|Dm, Φ) = N(wm; ¯wm, Σ2 m) based on the current estimation of σ , µ, Σ2 . ¯wm = ((Σ2 )−1 + Sxx,m σ2 )−1 ( Sxy,m σ2 + (Σ2 )−1 µ)(10) σ2 m,k = ((σ2 k)−1 + sxx,m,k σ2 )−1 (11) where sxx,m,k = j x2 m,j,k and sxy,m,k = j xm,j,kym,j Modified M Step Optimize the prior Φ = (µ, Σ2 ) based on the estimation from the last E step for related userfeature pairs.",
                "The M step implicitly uses the analytical solution for unrelated user-feature pairs. µk = 1 Mk m:related ¯wm,k (12) σ2 k = 1 Mk m:related σ2 m,k +( ¯wm,k − µk)( ¯wm,k − µk)T (13) where Mk is the number of users that are related to feature k We only estimate the diagonal of Σ2 m and Σ since we are using the diagonal approximation of the covariance matrices.",
                "To estimate ¯wm, we only need to calculate the numerical solutions for dimensions that are related to user m. To estimate σ2 k and µk, we only sum over users that are related to the kth feature.",
                "There are two major benefits of the new algorithm.",
                "First, because only the related (m, k) pairs are needed at the modified M step, the computational complexity in a single EM iteration is much smaller when the data is sparse, and many of (m, k) pairs are unrelated.",
                "Second, the parameters estimated at the modified M step (Equations 12 - 13) are more accurate than the standard M step described in Section 4.1 because the exact analytical solutions ¯wm,k = µk and σm,k = σk for the unrelated (m, k) pairs were used in the new algorithm instead of an approximate solution as in the standard algorithm. 5.",
                "EXPERIMENTAL METHODOLOGY 5.1 Evaluation Data Set To evaluate the proposed technique, we used the following three major data sets (Table 1): MovieLens Data: This data set was created by combining the relevance judgments from the MovieLens[9] data set with documents from the Internet Movie Database (IMDB).",
                "MovieLens allows users to rank how much he/she enjoyed a specific movie on a scale from 1 to 5.",
                "This likeability rating was used as a measurement of how relevant the document representing the corresponding movie is to the user.",
                "We considered documents with likeability scores of 4 or 5 as relevant, and documents with a score of 1 to 3 as irrelevant to the user.",
                "MovieLens provided relevance judgments on 3,057 documents from 6,040 separate users.",
                "On average, each user rated 151 movies, of these 87 were judged to be relevant.",
                "The average score for a document was 3.58.",
                "Documents representing each movie were constructed from the portion of the IMDB database that is available for public download[13].",
                "Based on this database, we created one document per movie that contained the relevant information about it (e.g. directors, actors, etc.).",
                "Table 1: Data Set Statistics.",
                "On Reuters, the number of rating for a simulated user is the number of documents relevant to the corresponding topic.",
                "Data Users Docs Ratings per User MovieLens 6,040 3,057 151 Netflix-all 480,189 17,770 208 Netflix-1000 1000 17,770 127 Reuters-C 34 100,000 3949 Reuters-E 26 100,000 1632 Reuters-G 33 100,000 2222 Reuters-M 10 100,000 6529 Netflix Data: This data set was constructed by combining documents about movies crawled from the web with a set of actual movie rental customer relevance judgments from Netflix[19].",
                "Netflix publicly provides the relevance judgments of 480,189 anonymous customers.",
                "There are around 100 million rating on a scale of 1 to 5 for 17,770 documents.",
                "Similar to MovieLens, we considered documents with likeability scores of 4 or 5 as relevant.",
                "This number was reduced to 1000 customers through random sampling.",
                "The average customer on the reduced data set provided 127 judgments, with 70 being deemed relevant.",
                "The average score for documents is 3.55.",
                "Reuters Data: This is the Reuters Corpus, Volume 1.",
                "It covers 810,000 Reuters English language news stories from August 20, 1996 to August 19, 1997.",
                "Only the first 100,000 news were used in our experiments.",
                "The Reuters corpus comes with a topic hierarchy.",
                "Each document is assigned to one of several locations on the hierarchical tree.",
                "The first level of the tree contains four topics, denoted as C, E, M, and G. For the experiments in this paper, the tree was cut at level 1 to create four smaller trees, each of which corresponds to one smaller data set: Reuters-E Reuters-C, ReutersM and Reuters-G. For each small data set, we created several profiles, one profile for each node in a sub-tree, to simulate multiple users, each with a related, yet separate definition of relevance.",
                "All the user profiles on a sub-tree are supposed to share the same prior model distribution.",
                "Since this corpus explicitly indicates only the relevant documents for a topic(user), all other documents are considered irrelevant. 5.2 Evaluation We designed the experiments to answer the following three questions: 1.",
                "Do we need to take the effort to use a Bayesian approach and learn a prior from other users? 2.",
                "Does the new algorithm work better than the standard EM algorithm for learning the Bayesian hierarchical linear model? 3.",
                "Can the new algorithm quickly learn many user models?",
                "To answer the first question, we compared the Bayesian hierarchical models with commonly used Norm-2 regularized linear regression models.",
                "In fact, the commonly used approach is equivalent to the model learned at the end of the first EM iteration.",
                "To answer the second question, we compared the proposed new algorithm with the standard EM algorithm to see whether the new learning algorithm is better.",
                "To answer the third question, we tested the efficiency of the new algorithm on the entire Netflix data set where about half a million user models need to be learned together.",
                "For the MovieLens and Netflix data sets, algorithm effectiveness was measured by mean square error, while on the Reuters data set <br>classification</br> error was used because it was more informative.",
                "We first evaluated the performance on each individual user, and then estimated the macro average over all users.",
                "Statistical tests (t-tests) were carried out to see whether the results are significant.",
                "For the experiments on the MovieLens and Netflix data sets, we used a random sample of 90% of each user for training, and the rest for testing.",
                "On Reuters data set, because there are too many relevant documents for each topic in the corpus, we used a random sample of 10% of each topic for training, and 10% of the remaining documents for testing.",
                "For all runs, we set (a, b, c, Σ ) = (0.1, 10, 0.1, 1) manually. 6.",
                "EXPERIMENTAL RESULTS Figure 2, Figure 3, and Figure 4 show that on all data sets, the Bayesian hierarchical modeling approach has a statistical significant improvement over the regularized linear regression model, which is equivalent to the Bayesian hierarchical models learned at the first iteration.",
                "Further analysis shows a negative correlation between the number of training data for a user and the improvement the system gets.",
                "This suggests that the borrowing information from other users has more significant improvements for users with less training data, which is as expected.",
                "However, the strength of the correlation differs over data sets, and the amount of training data is not the only characteristics that will influence the final performance.",
                "Figure 2 and Figure 3 show that the proposed new algorithm works better than the standard EM algorithm on the Netflix and MovieLens data sets.",
                "This is not surprising since the number of related feature-users pairs is much smaller than the number of unrelated feature-user pairs on these two data sets, and thus the proposed new algorithm is expected to work better.",
                "Figure 4 shows that the two algorithms work similarly on the Reuters-E data set.",
                "The accuracy of the new algorithm is similar to that of the standard EM algorithm at each iteration.",
                "The general patterns are very similar on other Reuters subsets.",
                "Further analysis shows that only 58% of the user-feature pairs are unrelated on this data set.",
                "Since the number of unrelated user-feature pairs is not extremely large, the sparseness is not a serious problem on the Reuters data set.",
                "Thus the two learning algorithms perform similarly.",
                "The results suggest that only on a corpus where the number of unrelated user-feature pairs is much larger than the number of related pairs, such as on the Netflix data set, the proposed technique will get a significant improvement over standard EM.",
                "However, the experiments also show that when the assumption does not hold, the new algorithm does not hurt performance.",
                "Although the proposed technique is faster than standard Figure 2: Performance on a Netflix subset with 1,000 users.",
                "The new algorithm is statistical significantly better than EM algorithm at iterations 2 - 10.",
                "Norm-2 regularized linear models are equivalent to the Bayesian hierarchical models learned at the first iteration, and are statistical significantly worse than the Bayesian hierarchical models. 0 2 4 6 8 10 1 1.05 1.1 1.15 1.2 1.25 1.3 1.35 1.4 Iterations MeanSquareError New Algorithm Traditional EM 1 2 3 4 5 6 7 8 9 10 0.33 0.34 0.35 0.36 0.37 0.38 0.39 Iterations ClassificationError New Algorithm Traditional EM Figure 3: Performance on a MovieLens subset with 1,000 users.",
                "The new algorithm is statistical significantly better than EM algorithm at iteration 2 to 17 (evaluated with mean square error). 1 6 11 16 21 0.5 1 1.5 2 2.5 3 3.5 Iterations MeanSquareError New Algorithm Traditional EM 1 6 11 16 21 0.35 0.4 0.45 0.5 0.55 0.6 0.65 Iterations ClassificationError New Algorithm Traditional EM Figure 4: Performance on a Reuters-E subset with 26 profiles.",
                "Performances on Reuters-C, Reuters-M, Reuters-G are similar. 1 2 3 4 5 0.011 0.0115 0.012 0.0125 0.013 0.0135 0.014 Iterations MeanSquareError New Algorithm Traditional EM 1 2 3 4 5 0.0102 0.0104 0.0106 0.0108 0.011 0.0112 0.0114 Iterations ClassificationError New Algorithm Traditional EM EM, can it really learn millions of user models quickly?",
                "Our results show that the modified EM algorithm converges quickly, and 2 - 3 modified EM iterations would result in a reliable estimation.",
                "We evaluated the algorithm on the whole Netflix data set (480,189 users, 159,836 features, and 100 million ratings) running on a single CPU PC (2GB memory, P4 3GHz).",
                "The system finished one modified EM iteration in about 4 hours.",
                "This demonstrates that the proposed technique can efficiently handle large-scale system like Netflix. 7.",
                "CONCLUSION Content-based user profile learning is an important problem and is the key to providing personal recommendations to a user, especially for recommending new items with a small number of ratings.",
                "The Bayesian hierarchical modeling approach is becoming an important user profile learning approach due to its theoretically justified ability to help one user through information transfer from the other users by way of hyperpriors.",
                "This paper examined the weakness of the popular EM based learning approach for Bayesian hierarchical linear models and proposed a better learning technique called Modified EM.",
                "We showed that the new technique is theoretically more computationally efficient than the standard EM algorithm.",
                "Evaluation on the MovieLens and Netflix data sets demonstrated the effectiveness of the new technique when the data is sparse, by which we mean the ratio of related user-feature pairs to unrelated pairs is small.",
                "Evaluation on the Reuters data set showed that the new technique performed similar to the standard EM algorithm when the sparseness condition does not hold.",
                "In general, it is better to use the new algorithm since it is as simple as standard EM, the performance is either better or similar to EM, and the computation complexity is lower at each iteration.",
                "It is worth mentioning that even if the original problem space is not sparse, sparseness can be created artificially when a recommendation system uses user-specific feature selection techniques to reduce the noise and user model complexity.",
                "The proposed technique can also be adapted to improve the learning in such a scenario.",
                "We also demonstrated that the proposed technique can learn half a million user profiles from 100 million ratings in a few hours with a single CPU.",
                "The research is important because scalability is a major concern for researchers when using the Bayesian hierarchical linear modeling approach to build a practical large scale system, even though the literature have demonstrated the effectiveness of the models in many applications.",
                "Our work is one major step on the road to make Bayesian hierarchical linear models more practical.",
                "The proposed new technique can be easily adapted to run on a cluster of machines, and thus further speed up the learning process to handle a larger scale system with hundreds of millions of users.",
                "The research has much potential to benefit people using EM algorithm on many other IR problems as well as machine learning problems.",
                "EM algorithm is a commonly used machine learning technique.",
                "It is used to find model parameters in many IR problems where the training data is very sparse.",
                "Although we are focusing on the Bayesian hierarchical linear models for recommendation and filtering, the new idea of using analytical solution instead of numerical solution for unrelated user-feature pairs at the M step could be adapted to many other problems. 8.",
                "ACKNOWLEDGMENTS We thank Wei Xu, David Lewis and anonymous reviewers for valuable feedback on the work described in this paper.",
                "Part of the work was supported by Yahoo, Google, the Petascale Data Storage Institute and the Institute for Scalable Scientific Data Management.",
                "Any opinions, findings, conclusions, or recommendations expressed in this material are those of the authors, and do not necessarily reflect those of the sponsors. 9.",
                "REFERENCES [1] C. Basu, H. Hirsh, and W. Cohen.",
                "Recommendation as <br>classification</br>: Using social and content-based information in recommendation.",
                "In Proceedings of the Fifteenth National Conference on Artificial Intelligence, 1998. [2] J. S. Breese, D. Heckerman, and C. Kadie.",
                "Empirical analysis of predictive algorithms for collaborative filtering.",
                "Technical report, Microsoft Research, One Microsoft Way, Redmond, WA 98052, 1998. [3] J. Callan.",
                "Document filtering with inference networks.",
                "In Proceedings of the Nineteenth Annual International ACM SIGIR Conference on Research and Development in Information Retrieval, pages 262-269, 1996. [4] N. Cancedda, N. Cesa-Bianchi, A. Conconi, C. Gentile, C. Goutte, T. Graepel, Y. Li, J. M. Renders, J. S. Taylor, and A. Vinokourov.",
                "Kernel method for document filtering.",
                "In The Eleventh Text REtrieval Conference (TREC11).",
                "National Institute of Standards and Technology, special publication 500-249, 2003. [5] C. Chelba and A. Acero.",
                "Adaptation of maximum entropy capitalizer: Little data can help a lot.",
                "In D. Lin and D. Wu, editors, Proceedings of EMNLP 2004, pages 285-292, Barcelona, Spain, July 2004.",
                "Association for Computational Linguistics. [6] B. Croft and J. Lafferty, editors.",
                "Language Modeling for Information Retrieval.",
                "Kluwer, 2002. [7] A. Dayanik, D. D. Lewis, D. Madigan, V. Menkov, and A. Genkin.",
                "Constructing informative prior distributions from domain knowledge in text <br>classification</br>.",
                "In SIGIR 06: Proceedings of the 29th annual international ACM SIGIR conference on Research and development in information retrieval, pages 493-500, New York, NY, USA, 2006.",
                "ACM Press. [8] J. Delgado and N. Ishii.",
                "Memory-based weightedmajority prediction for recommender systems.",
                "In ACM SIGIR99 Workshop on Recommender Systems, 1999. [9] GroupLens.",
                "Movielens. http://www.grouplens.org/taxonomy/term/14, 2006. [10] D. Heckerman.",
                "A tutorial on learning with bayesian networks.",
                "In M. Jordan, editor, Learning in Graphical Models.",
                "Kluwer Academic, 1998. [11] J. L. Herlocker, J.",
                "A. Konstan, A. Borchers, and J. Riedl.",
                "An algorithmic framework for performing collaborative filtering.",
                "In SIGIR 99: Proceedings of the 22nd annual international ACM SIGIR conference on Research and development in information retrieval, pages 230-237, New York, NY, USA, 1999.",
                "ACM Press. [12] T. Hofmann and J. Puzicha.",
                "Latent class models for collaborative filtering.",
                "In IJCAI 99: Proceedings of the Sixteenth International Joint Conference on Artificial Intelligence, pages 688-693, San Francisco, CA, USA, 1999.",
                "Morgan Kaufmann Publishers Inc. [13] I. M. D. (IMDB).",
                "Internet movie database. http://www.imdb.com/interfaces/, 2006. [14] R. Jin, J. Y. Chai, and L. Si.",
                "An automatic weighting scheme for collaborative filtering.",
                "In SIGIR 04: Proceedings of the 27th annual international ACM SIGIR conference on Research and development in information retrieval, pages 337-344, New York, NY, USA, 2004.",
                "ACM Press. [15] J.",
                "A. Konstan, B. N. Miller, D. Maltz, J. L. Herlocker, L. R. Gordon, and J. Riedl.",
                "GroupLens: Applying collaborative filtering to Usenet news.",
                "Communications of the ACM, 40(3):77-87, 1997. [16] D. Lewis.",
                "Applying support vector machines to the TREC-2001 batch filtering and routing tasks.",
                "In Proceedings of the Eleventh Text REtrieval Conference (TREC-11), 2002. [17] B. Liu, X. Li, W. S. Lee, , and P. Yu.",
                "Text <br>classification</br> by labeling words.",
                "In Proceedings of The Nineteenth National Conference on Artificial Intelligence (AAAI-2004), July 25-29, 2004. [18] P. Melville, R. J. Mooney, and R. Nagarajan.",
                "Content-boosted collaborative filtering for improved recommendations.",
                "In Proceedings of the Eighteenth National Conference on Artificial Intelligence (AAAI-2002), Edmonton, Canada, 2002. [19] Netflix.",
                "Netflix prize. http://www.netflixprize.com (visited on Nov. 30, 2006), 2006. [20] S. Robertson and K. Sparck-Jones.",
                "Relevance weighting of search terms.",
                "In Journal of the American Society for Information Science, volume 27, pages 129-146, 1976. [21] J. Wang, A. P. de Vries, and M. J. T. Reinders.",
                "Unifying user-based and item-based collaborative filtering approaches by similarity fusion.",
                "In SIGIR 06: Proceedings of the 29th annual international ACM SIGIR conference on Research and development in information retrieval, pages 501-508, New York, NY, USA, 2006.",
                "ACM Press. [22] X. Wu and R. K. Srihari.",
                "Incorporating prior knowledge with weighted margin support vector machines.",
                "In Proc.",
                "ACM Knowledge Discovery Data Mining Conf. (ACM SIGKDD 2004), Aug. 2004. [23] Y. Yang, S. Yoo, J. Zhang, and B. Kisiel.",
                "Robustness of adaptive filtering methods in a cross-benchmark evaluation.",
                "In Proceedings of the 28th Annual International ACM SIGIR Conference on Research and Development in Information Retrieval, 2005. [24] K. Yu, V. Tresp, and A. Schwaighofer.",
                "Learning gaussian processes from multiple tasks.",
                "In ICML 05: Proceedings of the 22nd international conference on Machine learning, pages 1012-1019, New York, NY, USA, 2005.",
                "ACM Press. [25] K. Yu, V. Tresp, and S. Yu.",
                "A nonparametric hierarchical bayesian framework for information filtering.",
                "In SIGIR 04: Proceedings of the 27th annual international ACM SIGIR conference on Research and development in information retrieval, pages 353-360.",
                "ACM Press, 2004. [26] X. Zhu.",
                "Semi-supervised learning literature survey.",
                "Technical report, University of Wisconsin - Madison, December 9, 2006. [27] P. Zigoris and Y. Zhang.",
                "Bayesian adaptive user profiling with explicit & implicit feedback.",
                "In Conference on Information and Knowledge Mangement 2006, 2006."
            ],
            "original_annotated_samples": [],
            "translated_annotated_samples": [
                "Ha habido mucha investigación sobre la mejora de la precisión de la \"clasificación\" cuando la cantidad de datos de capacitación etiquetados es pequeña.",
                "Para los conjuntos de datos de Movielens y Netflix, la efectividad del algoritmo se midió por error cuadrado medio, mientras que en el conjunto de datos de datos del conjunto de datos de Reuters se usó porque era más informativo.",
                "Recomendación como \"clasificación\": uso de información social y basada en contenido en recomendación.",
                "Construcción de distribuciones previas informativas a partir del conocimiento del dominio en el texto \"Clasificación\".",
                "Texto \"Clasificación\" etiquetando palabras."
            ],
            "translated_text": "",
            "candidates": [
                "clasificación",
                "clasificación",
                "clasificación",
                "clasificación",
                "clasificación",
                "clasificación",
                "Clasificación",
                "clasificación",
                "Clasificación"
            ],
            "error": []
        },
        "rating": {
            "translated_key": "calificación",
            "is_in_text": true,
            "original_annotated_sentences": [
                "Efficient Bayesian Hierarchical User Modeling for Recommendation Systems Yi Zhang, Jonathan Koren School of Engineering University of California Santa Cruz Santa Cruz, CA, USA {yiz, jonathan}@soe.ucsc.edu ABSTRACT A content-based personalized recommendation system learns user specific profiles from user feedback so that it can deliver information tailored to each individual users interest.",
                "A system serving millions of users can learn a better user profile for a new user, or a user with little feedback, by borrowing information from other users through the use of a Bayesian hierarchical model.",
                "Learning the model parameters to optimize the joint data likelihood from millions of users is very computationally expensive.",
                "The commonly used EM algorithm converges very slowly due to the sparseness of the data in IR applications.",
                "This paper proposes a new fast learning technique to learn a large number of individual user profiles.",
                "The efficacy and efficiency of the proposed algorithm are justified by theory and demonstrated on actual user data from Netflix and MovieLens.",
                "Categories and Subject Descriptors: B.3.3 [Information Search and Retrieval]: Information filtering General Terms: Algorithms 1.",
                "INTRODUCTION Personalization is the future of the Web, and it has achieved great success in industrial applications.",
                "For example, online stores, such as Amazon and Netflix, provide customized recommendations for additional products or services based on a users history.",
                "Recent offerings such as My MSN, My Yahoo!, My Google, and Google News have attracted much attention due to their potential ability to infer a users interests from his/her history.",
                "One major personalization topic studied in the information retrieval community is content-based personal recommendation systems1 .",
                "These systems learn user-specific profiles from user feedback so that they can recommend information tailored to each individual users interest without requiring the user to make an explicit query.",
                "Learning the user profiles is the core problem for these systems.",
                "A user profile is usually a classifier that can identify whether a document is relevant to the user or not, or a regression model that tells how relevant a document is to the user.",
                "One major challenge of building a recommendation or personalization system is that the profile learned for a particular user is usually of low quality when the amount of data from that particular user is small.",
                "This is known as the cold start problem.",
                "This means that any new user must endure poor initial performance until sufficient feedback from that user is provided to learn a reliable user profile.",
                "There has been much research on improving classification accuracy when the amount of labeled training data is small.",
                "The semi-supervised learning approach combines unlabeled and labeled data together to achieve this goal [26].",
                "Another approach is using domain knowledge.",
                "Researchers have modified different learning algorithms, such as Na¨ıveBayes [17], logistic regression [7], and SVMs [22], to integrate domain knowledge into a text classifier.",
                "The third approach is borrowing training data from other resources [5][7].",
                "The effectiveness of these different approaches is mixed, due to how well the underlying model assumption fits the data.",
                "One well-received approach to improve recommendation system performance for a particular user is borrowing information from other users through a Bayesian hierarchical modeling approach.",
                "Several researchers have demonstrated that this approach effectively trades off between shared and user-specific information, thus alleviating poor initial performance for each user[27][25].",
                "In order to learn a Bayesian hierarchical model, the system usually tries to find the most likely model parameters for the given data.",
                "A mature recommendation system usually works for millions of users.",
                "It is well known that learning the optimal parameters of a Bayesian hierarchical model is computationally expensive when there are thousands or millions of users.",
                "The EM algorithm is a commonly used technique for parameter learning due to its simplicity and convergence guarantee.",
                "However, a content based recommendation system often handles documents in a very high dimensional space, in which each document is represented by a very sparse vector.",
                "With careful analysis of the EM algorithm in this scenario (Section 4), we find that the EM tering, or item-based collaborative filtering.",
                "In this paper, the words filtering and recommendation are used interchangeably. algorithm converges very slowly due to the sparseness of the input variables.",
                "We also find that updating the model parameter at each EM iteration is also expensive with computational complexity of O(MK), where M is the number of users and K is the number of dimensions.",
                "This paper modifies the standard EM algorithm to create an improved learning algorithm, which we call the Modified EM algorithm.",
                "The basic idea is that instead of calculating the numerical solution for all the user profile parameters, we derive the analytical solution of the parameters for some feature dimensions, and at the M step use the analytical solution instead of the numerical solution estimated at E step for those parameters.",
                "This greatly reduces the computation at a single EM iteration, and also has the benefit of increasing the convergence speed of the learning algorithm.",
                "The proposed technique is not only well supported by theory, but also by experimental results.",
                "The organization of the remaining parts of this paper is as follows: Section 3 describes the Bayesian hierarchical linear regression modeling framework used for content-based recommendations.",
                "Section 4 describes how to learn the model parameters using the standard EM algorithm, along with using the new technique proposed in this paper.",
                "The experimental setting and results used to validate the proposed learning technique are reported in Sections 5 and 6.",
                "Section 7 summarizes and offers concluding remarks. 2.",
                "RELATED WORK Providing personalized recommendations to users has been identified as a very important problem in the IR community since the 1970s.",
                "The approaches that have been used to solve this problem can be roughly classified into two major categories: content based filtering versus collaborative filtering.",
                "Content-based filtering studies the scenario where a recommendation system monitors a document stream and pushes documents that match a user profile to the corresponding user.",
                "The user may read the delivered documents and provide explicit relevance feedback, which the filtering system then uses to update the users profile using relevance feedback retrieval models (e.g.",
                "Boolean models, vector space models, traditional probabilistic models [20] , inference networks [3] and language models [6]) or machine learning algorithms (e.g.",
                "Support Vector Machines (SVM), K nearest neighbors (K-NN) clustering, neural networks, logistic regression, or Winnow [16] [4] [23]).",
                "Collaborative filtering goes beyond merely using document content to recommend items to a user by leveraging information from other users with similar tastes and preferences in the past.",
                "Memorybased heuristics and model based approaches have been used in collaborative filtering task [15] [8] [2] [14] [12] [11].",
                "This paper contributes to the content-based recommendation research by improving the efficiency and effectiveness of Bayesian hierarchical linear models, which have a strong theoretical basis and good empirical performance on recommendation tasks[27][25].",
                "This paper does not intend to compare content-based filtering with collaborative filtering or claim which one is a better.",
                "We think each complements the other, and that content-based filtering is extremely useful for handling new documents/items with little or no user feedback.",
                "Similar to some other researchers[18][1][21], we found that a recommendation system will be more effective when both techniques are combined.",
                "However, this is beyond the scope of this paper and thus not discussed here. 3.",
                "BAYESIAN HIERARCHICAL LINEAR REGRESSION Assume there are M users in the system.",
                "The task of the system is to recommend documents that are relevant to each user.",
                "For each user, the system learns a user model from the users history.",
                "In the rest of this paper, we will use the following notations to represent the variables in the system. m = 1, 2, ..., M: The index for each individual user.",
                "M is the total number of users. wm: The user model parameter associated with user m. wm is a K dimensional vector. j = 1, 2, ..., Jm: The index for a set of data for user m. Jm is the number of training data for user m. Dm = {(xm,j, ym,j)}: A set of data associated with user m. xm,j is a K dimensional vector that represents the mth users jth training document.2 ym,j is a scalar that represents the label of document xm,j. k = 1, 2, ..., K: The dimensional index of input variable x.",
                "The Bayesian hierarchical modeling approach has been widely used in real-world information retrieval applications.",
                "Generalized Bayesian hierarchical linear models, one of the simplest Bayesian hierarchical models, are commonly used and have achieved good performance on collaborative filtering [25] and content-based adaptive filtering [27] tasks.",
                "Figure 1 shows the graphical representation of a Bayesian hierarchical model.",
                "In this graph, each user model is represented by a random vector wm.",
                "We assume a user model is sampled randomly from a prior distribution P(w|Φ).",
                "The system can predict the user label y of a document x given an estimation of wm (or wms distribution) using a function y = f(x, w).",
                "The model is called generalized Bayesian hierarchical linear model when y = f(wT x) is any generalized linear model such as logistic regression, SVM, and linear regression.",
                "To reliably estimate the user model wm, the system can borrow information from other users through the prior Φ = (µ, Σ).",
                "Now we look at one commonly used model where y = wT x + , where ∼ N(0, σ2 ) is a random noise [25][27].",
                "Assume that each user model wm is an independent draw from a population distribution P(w|Φ), which is governed by some unknown hyperparameter Φ.",
                "Let the prior distribution of user model w be a Gaussian distribution with parameter Φ = (µ, Σ), which is the commonly used prior for linear models. µ = (µ1, µ2, ..., µK ) is a K dimensional vector that represents the mean of the Gaussian distribution, and Σ is the covariance matrix of the Gaussian.",
                "Usually, a Normal distribution N(0, aI) and an Inverse Wishart distribution P(Σ) ∝ |Σ|− 1 2 b exp(−1 2 ctr(Σ−1 )) are used as hyperprior to model the prior distribution of µ and Σ respectively.",
                "I is the K dimensional identity matrix, and a, b, and c are real numbers.",
                "With these settings, we have the following model for the system: 1. µ and Σ are sampled from N(0, aI) and IWν (aI), respectively. 2 The first dimension of x is a dummy variable that always equals to 1.",
                "Figure 1: Illustration of dependencies of variables in the hierarchical model.",
                "The <br>rating</br>, y, for a document, x, is conditioned on the document and the user model, wm, associated with the user m. Users share information about their models through the prior, Φ = (µ, Σ). 2.",
                "For each user m, wm is sampled randomly from a Normal distribution: wm ∼ N(µ, Σ2 ) 3.",
                "For each item xm,j, ym,j is sampled randomly from a Normal distribution: ym,j ∼ N(wT mxm,j, σ2 ).",
                "Let θ = (Φ, w1, w2, ..., wM ) represent the parameters of this system that needs to be estimated.",
                "The joint likelihood for all the variables in the probabilistic model, which includes the data and the parameters, is: P(D, θ) = P(Φ) m P(wm|Φ) j P(ym,j|xm,j, wm) (1) For simplicity, we assume a, b, c, and σ are provided to the system. 4.",
                "MODEL PARAMETER LEARNING If the prior Φ is known, finding the optimal wm is straightforward: it is a simple linear regression.",
                "Therefore, we will focus on estimating Φ.",
                "The maximum a priori solution of Φ is given by ΦMAP = arg max Φ P(Φ|D) (2) = arg max Φ P(Φ, D) P(D) (3) = arg max Φ P(D|Φ)P(Φ) (4) = arg max Φ w P(D|w, Φ)P(w|Φ)P(Φ)dw (5) Finding the optimal solution for the above problem is challenging, since we need to integrate over all w = (w1, w2, ..., wM ), which are unobserved hidden variables. 4.1 EM Algorithm for Bayesian Hierarchical Linear Models In Equation 5, Φ is the parameter needs to be estimated, and the result depends on unobserved latent variables w. This kind of optimization problem is usually solved by the EM algorithm.",
                "Applying EM to the above problem, the set of user models w are the unobservable hidden variables and we have: Q = w P(w|µ, Σ2 , Dm) log P(µ, Σ2 , w, D)dw Based on the derivation of the EM formulas presented in [24], we have the following Expectation-Maximization steps for finding the optimal hyperparameters.",
                "For space considerations, we omit the derivation in this paper since it is not the focus of our work.",
                "E step: For each user m, estimate the user model distribution P(wm|Dm, Φ) = N(wm; ¯wm, Σ2 m) based on the current estimation of the prior Φ = (µ, Σ2 ). ¯wm = ((Σ2 )−1 + Sxx,m σ2 )−1 ( Sxy,m σ2 + (Σ2 )−1 µ) (6) Σ2 m = ((Σ2 )−1 + Sxx,m σ2 )−1 (7) where Sxx,m = j xm,jxT m,j Sxy,m = j xm,jym,j M step: Optimize the prior Φ = (µ, Σ2 ) based on the estimation from the last E step. µ = 1 M m ¯wm (8) Σ2 = 1 M m Σ2 m + ( ¯wm − µ)( ¯wm − µ)T (9) Many machine learning driven IR systems use a point estimate of the parameters at different stages in the system.",
                "However, we are estimating the posterior distribution of the variables at the E step.",
                "This avoids overfitting wm to a particular users data, which may be small and noisy.",
                "A detailed discussion about this subject appears in [10]. 4.2 New Algorithm: Modified EM Although the EM algorithm is widely studied and used in machine learning applications, using the above EM process to solve Bayesian hierarchical linear models in large-scale information retrieval systems is still too computationally expensive.",
                "In this section, we describe why the learning rate of the EM algorithm is slow in our application and introduce a new technique to make the learning of the Bayesian hierarchical linear model scalable.",
                "The derivation of the new learning algorithm will be based on the EM algorithm described in the previous section.",
                "First, the covariance matrices Σ2 , Σ2 m are usually too large to be computationally feasible.",
                "For simplicity, and as a common practice in IR, we do not model the correlation between features.",
                "Thus we approximate these matrices with K dimensional diagonal matrices.",
                "In the rest of the paper, we use these symbols to represent their diagonal approximations: Σ2 =     σ2 1 0 .. 0 0 σ2 2 .. 0 .. .. .. .. 0 0 .. σ2 K     Σ2 m =     σ2 m,1 0 .. 0 0 σ2 m,2 .. 0 .. .. .. .. 0 0 .. σ2 m,K     Secondly, and most importantly, the input space is very sparse and there are many dimensions that are not related to a particular user in a real IR application.",
                "For example, let us consider a movie recommendation system, with the input variable x representing a particular movie.",
                "For the jth movie that the user m has seen, let xm,j,k = 1 if the director of the movie is Jean-Pierre Jeunet (indexed by k).",
                "Here we assume that whether or not that this director directed a specific movie is represented by the kth dimension.",
                "If the user m has never seen a movie directed by Jean-Pierre Jeunet, then the corresponding dimension is always zero (xm,j,k = 0 for all j) .",
                "One major drawback of the EM algorithm is that the importance of a feature, µk, may be greatly dominated by users who have never encountered this feature (i.e. j xm,j,k = 0) at the M step (Equation 8).",
                "Assume that 100 out of 1 million users have viewed the movie directed by Jean-Pierre Jeunet, and that the viewers have rated all of his movies as excellent.",
                "Intuitively, he is a good director and the weight for him (µk) should be high.",
                "Before the EM iteration, the initial value of µ is usually set to 0.",
                "Since the other 999,900 users have not seen this movie, their corresponding weights (w1,k, w2,k, ..., wm,k..., w999900,k) for that director would be very small initially.",
                "Thus the corresponding weight of the director in the prior µk at the first M step would be very low , and the variance σm,k will be large (Equations 8 and 7).",
                "It is undesirable that users who have never seen any movie produced by the director influence the importance of the director so much.",
                "This makes the convergence of the standard EM algorithm very slow.",
                "Now lets look at whether we can improve the learning speed of the algorithm.",
                "Without a loss of generality, let us assume that the kth dimension of the input variable x is not related to a particular user m. By which we mean, xm,j,k = 0 for all j = 1, ..., Jm.",
                "It is straightforward to prove that the kth row and kth column of Sxx,m are completely filled with zeros, and that the kth dimension of Sxy,m is zeroed as well.",
                "Thus the corresponding kth dimension of the user models mean, ¯wm, should be equal to that of the prior: ¯wm,k = µk, with the corresponding covariance of σm,k = σk.",
                "At the M step, the standard EM algorithm uses the numerical solution of the distribution P(wm|Dm, Φ) estimated at E step (Equation 8 and Equation 7).",
                "However, the numerical solutions are very unreliable for ¯wm,k and σm,k when the kth dimension is not related to the mth user.",
                "A better approach is using the analytical solutions ¯wm,k = µk, and σm,k = σk for the unrelated (m, k) pairs, along with the numerical solution estimated at E step for the other (m, k) pairs.",
                "Thus we get the following new EM-like algorithm: Modified E step: For each user m, estimate the user model distribution P(wm|Dm, Φ) = N(wm; ¯wm, Σ2 m) based on the current estimation of σ , µ, Σ2 . ¯wm = ((Σ2 )−1 + Sxx,m σ2 )−1 ( Sxy,m σ2 + (Σ2 )−1 µ)(10) σ2 m,k = ((σ2 k)−1 + sxx,m,k σ2 )−1 (11) where sxx,m,k = j x2 m,j,k and sxy,m,k = j xm,j,kym,j Modified M Step Optimize the prior Φ = (µ, Σ2 ) based on the estimation from the last E step for related userfeature pairs.",
                "The M step implicitly uses the analytical solution for unrelated user-feature pairs. µk = 1 Mk m:related ¯wm,k (12) σ2 k = 1 Mk m:related σ2 m,k +( ¯wm,k − µk)( ¯wm,k − µk)T (13) where Mk is the number of users that are related to feature k We only estimate the diagonal of Σ2 m and Σ since we are using the diagonal approximation of the covariance matrices.",
                "To estimate ¯wm, we only need to calculate the numerical solutions for dimensions that are related to user m. To estimate σ2 k and µk, we only sum over users that are related to the kth feature.",
                "There are two major benefits of the new algorithm.",
                "First, because only the related (m, k) pairs are needed at the modified M step, the computational complexity in a single EM iteration is much smaller when the data is sparse, and many of (m, k) pairs are unrelated.",
                "Second, the parameters estimated at the modified M step (Equations 12 - 13) are more accurate than the standard M step described in Section 4.1 because the exact analytical solutions ¯wm,k = µk and σm,k = σk for the unrelated (m, k) pairs were used in the new algorithm instead of an approximate solution as in the standard algorithm. 5.",
                "EXPERIMENTAL METHODOLOGY 5.1 Evaluation Data Set To evaluate the proposed technique, we used the following three major data sets (Table 1): MovieLens Data: This data set was created by combining the relevance judgments from the MovieLens[9] data set with documents from the Internet Movie Database (IMDB).",
                "MovieLens allows users to rank how much he/she enjoyed a specific movie on a scale from 1 to 5.",
                "This likeability <br>rating</br> was used as a measurement of how relevant the document representing the corresponding movie is to the user.",
                "We considered documents with likeability scores of 4 or 5 as relevant, and documents with a score of 1 to 3 as irrelevant to the user.",
                "MovieLens provided relevance judgments on 3,057 documents from 6,040 separate users.",
                "On average, each user rated 151 movies, of these 87 were judged to be relevant.",
                "The average score for a document was 3.58.",
                "Documents representing each movie were constructed from the portion of the IMDB database that is available for public download[13].",
                "Based on this database, we created one document per movie that contained the relevant information about it (e.g. directors, actors, etc.).",
                "Table 1: Data Set Statistics.",
                "On Reuters, the number of <br>rating</br> for a simulated user is the number of documents relevant to the corresponding topic.",
                "Data Users Docs Ratings per User MovieLens 6,040 3,057 151 Netflix-all 480,189 17,770 208 Netflix-1000 1000 17,770 127 Reuters-C 34 100,000 3949 Reuters-E 26 100,000 1632 Reuters-G 33 100,000 2222 Reuters-M 10 100,000 6529 Netflix Data: This data set was constructed by combining documents about movies crawled from the web with a set of actual movie rental customer relevance judgments from Netflix[19].",
                "Netflix publicly provides the relevance judgments of 480,189 anonymous customers.",
                "There are around 100 million <br>rating</br> on a scale of 1 to 5 for 17,770 documents.",
                "Similar to MovieLens, we considered documents with likeability scores of 4 or 5 as relevant.",
                "This number was reduced to 1000 customers through random sampling.",
                "The average customer on the reduced data set provided 127 judgments, with 70 being deemed relevant.",
                "The average score for documents is 3.55.",
                "Reuters Data: This is the Reuters Corpus, Volume 1.",
                "It covers 810,000 Reuters English language news stories from August 20, 1996 to August 19, 1997.",
                "Only the first 100,000 news were used in our experiments.",
                "The Reuters corpus comes with a topic hierarchy.",
                "Each document is assigned to one of several locations on the hierarchical tree.",
                "The first level of the tree contains four topics, denoted as C, E, M, and G. For the experiments in this paper, the tree was cut at level 1 to create four smaller trees, each of which corresponds to one smaller data set: Reuters-E Reuters-C, ReutersM and Reuters-G. For each small data set, we created several profiles, one profile for each node in a sub-tree, to simulate multiple users, each with a related, yet separate definition of relevance.",
                "All the user profiles on a sub-tree are supposed to share the same prior model distribution.",
                "Since this corpus explicitly indicates only the relevant documents for a topic(user), all other documents are considered irrelevant. 5.2 Evaluation We designed the experiments to answer the following three questions: 1.",
                "Do we need to take the effort to use a Bayesian approach and learn a prior from other users? 2.",
                "Does the new algorithm work better than the standard EM algorithm for learning the Bayesian hierarchical linear model? 3.",
                "Can the new algorithm quickly learn many user models?",
                "To answer the first question, we compared the Bayesian hierarchical models with commonly used Norm-2 regularized linear regression models.",
                "In fact, the commonly used approach is equivalent to the model learned at the end of the first EM iteration.",
                "To answer the second question, we compared the proposed new algorithm with the standard EM algorithm to see whether the new learning algorithm is better.",
                "To answer the third question, we tested the efficiency of the new algorithm on the entire Netflix data set where about half a million user models need to be learned together.",
                "For the MovieLens and Netflix data sets, algorithm effectiveness was measured by mean square error, while on the Reuters data set classification error was used because it was more informative.",
                "We first evaluated the performance on each individual user, and then estimated the macro average over all users.",
                "Statistical tests (t-tests) were carried out to see whether the results are significant.",
                "For the experiments on the MovieLens and Netflix data sets, we used a random sample of 90% of each user for training, and the rest for testing.",
                "On Reuters data set, because there are too many relevant documents for each topic in the corpus, we used a random sample of 10% of each topic for training, and 10% of the remaining documents for testing.",
                "For all runs, we set (a, b, c, Σ ) = (0.1, 10, 0.1, 1) manually. 6.",
                "EXPERIMENTAL RESULTS Figure 2, Figure 3, and Figure 4 show that on all data sets, the Bayesian hierarchical modeling approach has a statistical significant improvement over the regularized linear regression model, which is equivalent to the Bayesian hierarchical models learned at the first iteration.",
                "Further analysis shows a negative correlation between the number of training data for a user and the improvement the system gets.",
                "This suggests that the borrowing information from other users has more significant improvements for users with less training data, which is as expected.",
                "However, the strength of the correlation differs over data sets, and the amount of training data is not the only characteristics that will influence the final performance.",
                "Figure 2 and Figure 3 show that the proposed new algorithm works better than the standard EM algorithm on the Netflix and MovieLens data sets.",
                "This is not surprising since the number of related feature-users pairs is much smaller than the number of unrelated feature-user pairs on these two data sets, and thus the proposed new algorithm is expected to work better.",
                "Figure 4 shows that the two algorithms work similarly on the Reuters-E data set.",
                "The accuracy of the new algorithm is similar to that of the standard EM algorithm at each iteration.",
                "The general patterns are very similar on other Reuters subsets.",
                "Further analysis shows that only 58% of the user-feature pairs are unrelated on this data set.",
                "Since the number of unrelated user-feature pairs is not extremely large, the sparseness is not a serious problem on the Reuters data set.",
                "Thus the two learning algorithms perform similarly.",
                "The results suggest that only on a corpus where the number of unrelated user-feature pairs is much larger than the number of related pairs, such as on the Netflix data set, the proposed technique will get a significant improvement over standard EM.",
                "However, the experiments also show that when the assumption does not hold, the new algorithm does not hurt performance.",
                "Although the proposed technique is faster than standard Figure 2: Performance on a Netflix subset with 1,000 users.",
                "The new algorithm is statistical significantly better than EM algorithm at iterations 2 - 10.",
                "Norm-2 regularized linear models are equivalent to the Bayesian hierarchical models learned at the first iteration, and are statistical significantly worse than the Bayesian hierarchical models. 0 2 4 6 8 10 1 1.05 1.1 1.15 1.2 1.25 1.3 1.35 1.4 Iterations MeanSquareError New Algorithm Traditional EM 1 2 3 4 5 6 7 8 9 10 0.33 0.34 0.35 0.36 0.37 0.38 0.39 Iterations ClassificationError New Algorithm Traditional EM Figure 3: Performance on a MovieLens subset with 1,000 users.",
                "The new algorithm is statistical significantly better than EM algorithm at iteration 2 to 17 (evaluated with mean square error). 1 6 11 16 21 0.5 1 1.5 2 2.5 3 3.5 Iterations MeanSquareError New Algorithm Traditional EM 1 6 11 16 21 0.35 0.4 0.45 0.5 0.55 0.6 0.65 Iterations ClassificationError New Algorithm Traditional EM Figure 4: Performance on a Reuters-E subset with 26 profiles.",
                "Performances on Reuters-C, Reuters-M, Reuters-G are similar. 1 2 3 4 5 0.011 0.0115 0.012 0.0125 0.013 0.0135 0.014 Iterations MeanSquareError New Algorithm Traditional EM 1 2 3 4 5 0.0102 0.0104 0.0106 0.0108 0.011 0.0112 0.0114 Iterations ClassificationError New Algorithm Traditional EM EM, can it really learn millions of user models quickly?",
                "Our results show that the modified EM algorithm converges quickly, and 2 - 3 modified EM iterations would result in a reliable estimation.",
                "We evaluated the algorithm on the whole Netflix data set (480,189 users, 159,836 features, and 100 million ratings) running on a single CPU PC (2GB memory, P4 3GHz).",
                "The system finished one modified EM iteration in about 4 hours.",
                "This demonstrates that the proposed technique can efficiently handle large-scale system like Netflix. 7.",
                "CONCLUSION Content-based user profile learning is an important problem and is the key to providing personal recommendations to a user, especially for recommending new items with a small number of ratings.",
                "The Bayesian hierarchical modeling approach is becoming an important user profile learning approach due to its theoretically justified ability to help one user through information transfer from the other users by way of hyperpriors.",
                "This paper examined the weakness of the popular EM based learning approach for Bayesian hierarchical linear models and proposed a better learning technique called Modified EM.",
                "We showed that the new technique is theoretically more computationally efficient than the standard EM algorithm.",
                "Evaluation on the MovieLens and Netflix data sets demonstrated the effectiveness of the new technique when the data is sparse, by which we mean the ratio of related user-feature pairs to unrelated pairs is small.",
                "Evaluation on the Reuters data set showed that the new technique performed similar to the standard EM algorithm when the sparseness condition does not hold.",
                "In general, it is better to use the new algorithm since it is as simple as standard EM, the performance is either better or similar to EM, and the computation complexity is lower at each iteration.",
                "It is worth mentioning that even if the original problem space is not sparse, sparseness can be created artificially when a recommendation system uses user-specific feature selection techniques to reduce the noise and user model complexity.",
                "The proposed technique can also be adapted to improve the learning in such a scenario.",
                "We also demonstrated that the proposed technique can learn half a million user profiles from 100 million ratings in a few hours with a single CPU.",
                "The research is important because scalability is a major concern for researchers when using the Bayesian hierarchical linear modeling approach to build a practical large scale system, even though the literature have demonstrated the effectiveness of the models in many applications.",
                "Our work is one major step on the road to make Bayesian hierarchical linear models more practical.",
                "The proposed new technique can be easily adapted to run on a cluster of machines, and thus further speed up the learning process to handle a larger scale system with hundreds of millions of users.",
                "The research has much potential to benefit people using EM algorithm on many other IR problems as well as machine learning problems.",
                "EM algorithm is a commonly used machine learning technique.",
                "It is used to find model parameters in many IR problems where the training data is very sparse.",
                "Although we are focusing on the Bayesian hierarchical linear models for recommendation and filtering, the new idea of using analytical solution instead of numerical solution for unrelated user-feature pairs at the M step could be adapted to many other problems. 8.",
                "ACKNOWLEDGMENTS We thank Wei Xu, David Lewis and anonymous reviewers for valuable feedback on the work described in this paper.",
                "Part of the work was supported by Yahoo, Google, the Petascale Data Storage Institute and the Institute for Scalable Scientific Data Management.",
                "Any opinions, findings, conclusions, or recommendations expressed in this material are those of the authors, and do not necessarily reflect those of the sponsors. 9.",
                "REFERENCES [1] C. Basu, H. Hirsh, and W. Cohen.",
                "Recommendation as classification: Using social and content-based information in recommendation.",
                "In Proceedings of the Fifteenth National Conference on Artificial Intelligence, 1998. [2] J. S. Breese, D. Heckerman, and C. Kadie.",
                "Empirical analysis of predictive algorithms for collaborative filtering.",
                "Technical report, Microsoft Research, One Microsoft Way, Redmond, WA 98052, 1998. [3] J. Callan.",
                "Document filtering with inference networks.",
                "In Proceedings of the Nineteenth Annual International ACM SIGIR Conference on Research and Development in Information Retrieval, pages 262-269, 1996. [4] N. Cancedda, N. Cesa-Bianchi, A. Conconi, C. Gentile, C. Goutte, T. Graepel, Y. Li, J. M. Renders, J. S. Taylor, and A. Vinokourov.",
                "Kernel method for document filtering.",
                "In The Eleventh Text REtrieval Conference (TREC11).",
                "National Institute of Standards and Technology, special publication 500-249, 2003. [5] C. Chelba and A. Acero.",
                "Adaptation of maximum entropy capitalizer: Little data can help a lot.",
                "In D. Lin and D. Wu, editors, Proceedings of EMNLP 2004, pages 285-292, Barcelona, Spain, July 2004.",
                "Association for Computational Linguistics. [6] B. Croft and J. Lafferty, editors.",
                "Language Modeling for Information Retrieval.",
                "Kluwer, 2002. [7] A. Dayanik, D. D. Lewis, D. Madigan, V. Menkov, and A. Genkin.",
                "Constructing informative prior distributions from domain knowledge in text classification.",
                "In SIGIR 06: Proceedings of the 29th annual international ACM SIGIR conference on Research and development in information retrieval, pages 493-500, New York, NY, USA, 2006.",
                "ACM Press. [8] J. Delgado and N. Ishii.",
                "Memory-based weightedmajority prediction for recommender systems.",
                "In ACM SIGIR99 Workshop on Recommender Systems, 1999. [9] GroupLens.",
                "Movielens. http://www.grouplens.org/taxonomy/term/14, 2006. [10] D. Heckerman.",
                "A tutorial on learning with bayesian networks.",
                "In M. Jordan, editor, Learning in Graphical Models.",
                "Kluwer Academic, 1998. [11] J. L. Herlocker, J.",
                "A. Konstan, A. Borchers, and J. Riedl.",
                "An algorithmic framework for performing collaborative filtering.",
                "In SIGIR 99: Proceedings of the 22nd annual international ACM SIGIR conference on Research and development in information retrieval, pages 230-237, New York, NY, USA, 1999.",
                "ACM Press. [12] T. Hofmann and J. Puzicha.",
                "Latent class models for collaborative filtering.",
                "In IJCAI 99: Proceedings of the Sixteenth International Joint Conference on Artificial Intelligence, pages 688-693, San Francisco, CA, USA, 1999.",
                "Morgan Kaufmann Publishers Inc. [13] I. M. D. (IMDB).",
                "Internet movie database. http://www.imdb.com/interfaces/, 2006. [14] R. Jin, J. Y. Chai, and L. Si.",
                "An automatic weighting scheme for collaborative filtering.",
                "In SIGIR 04: Proceedings of the 27th annual international ACM SIGIR conference on Research and development in information retrieval, pages 337-344, New York, NY, USA, 2004.",
                "ACM Press. [15] J.",
                "A. Konstan, B. N. Miller, D. Maltz, J. L. Herlocker, L. R. Gordon, and J. Riedl.",
                "GroupLens: Applying collaborative filtering to Usenet news.",
                "Communications of the ACM, 40(3):77-87, 1997. [16] D. Lewis.",
                "Applying support vector machines to the TREC-2001 batch filtering and routing tasks.",
                "In Proceedings of the Eleventh Text REtrieval Conference (TREC-11), 2002. [17] B. Liu, X. Li, W. S. Lee, , and P. Yu.",
                "Text classification by labeling words.",
                "In Proceedings of The Nineteenth National Conference on Artificial Intelligence (AAAI-2004), July 25-29, 2004. [18] P. Melville, R. J. Mooney, and R. Nagarajan.",
                "Content-boosted collaborative filtering for improved recommendations.",
                "In Proceedings of the Eighteenth National Conference on Artificial Intelligence (AAAI-2002), Edmonton, Canada, 2002. [19] Netflix.",
                "Netflix prize. http://www.netflixprize.com (visited on Nov. 30, 2006), 2006. [20] S. Robertson and K. Sparck-Jones.",
                "Relevance weighting of search terms.",
                "In Journal of the American Society for Information Science, volume 27, pages 129-146, 1976. [21] J. Wang, A. P. de Vries, and M. J. T. Reinders.",
                "Unifying user-based and item-based collaborative filtering approaches by similarity fusion.",
                "In SIGIR 06: Proceedings of the 29th annual international ACM SIGIR conference on Research and development in information retrieval, pages 501-508, New York, NY, USA, 2006.",
                "ACM Press. [22] X. Wu and R. K. Srihari.",
                "Incorporating prior knowledge with weighted margin support vector machines.",
                "In Proc.",
                "ACM Knowledge Discovery Data Mining Conf. (ACM SIGKDD 2004), Aug. 2004. [23] Y. Yang, S. Yoo, J. Zhang, and B. Kisiel.",
                "Robustness of adaptive filtering methods in a cross-benchmark evaluation.",
                "In Proceedings of the 28th Annual International ACM SIGIR Conference on Research and Development in Information Retrieval, 2005. [24] K. Yu, V. Tresp, and A. Schwaighofer.",
                "Learning gaussian processes from multiple tasks.",
                "In ICML 05: Proceedings of the 22nd international conference on Machine learning, pages 1012-1019, New York, NY, USA, 2005.",
                "ACM Press. [25] K. Yu, V. Tresp, and S. Yu.",
                "A nonparametric hierarchical bayesian framework for information filtering.",
                "In SIGIR 04: Proceedings of the 27th annual international ACM SIGIR conference on Research and development in information retrieval, pages 353-360.",
                "ACM Press, 2004. [26] X. Zhu.",
                "Semi-supervised learning literature survey.",
                "Technical report, University of Wisconsin - Madison, December 9, 2006. [27] P. Zigoris and Y. Zhang.",
                "Bayesian adaptive user profiling with explicit & implicit feedback.",
                "In Conference on Information and Knowledge Mangement 2006, 2006."
            ],
            "original_annotated_samples": [],
            "translated_annotated_samples": [
                "La \"calificación\", y, para un documento, x, está condicionado en el documento y el modelo de usuario, WM, asociado con el usuario m.Los usuarios comparten información sobre sus modelos a través del anterior, φ = (µ, σ).2.",
                "Esta \"calificación\" de simpatía se utilizó como una medida de cuán relevante es el documento que representa la película correspondiente para el usuario.",
                "En Reuters, el número de \"calificación\" para un usuario simulado es el número de documentos relevantes para el tema correspondiente.",
                "Hay alrededor de 100 millones de \"calificación\" en una escala de 1 a 5 para 17,770 documentos."
            ],
            "translated_text": "",
            "candidates": [
                "Calificación",
                "calificación",
                "clasificación",
                "calificación",
                "clasificación",
                "calificación",
                "clasificación",
                "calificación"
            ],
            "error": []
        },
        "recommender system": {
            "translated_key": "sistema de recomendación",
            "is_in_text": true,
            "original_annotated_sentences": [
                "Efficient Bayesian Hierarchical User Modeling for Recommendation Systems Yi Zhang, Jonathan Koren School of Engineering University of California Santa Cruz Santa Cruz, CA, USA {yiz, jonathan}@soe.ucsc.edu ABSTRACT A content-based personalized recommendation system learns user specific profiles from user feedback so that it can deliver information tailored to each individual users interest.",
                "A system serving millions of users can learn a better user profile for a new user, or a user with little feedback, by borrowing information from other users through the use of a Bayesian hierarchical model.",
                "Learning the model parameters to optimize the joint data likelihood from millions of users is very computationally expensive.",
                "The commonly used EM algorithm converges very slowly due to the sparseness of the data in IR applications.",
                "This paper proposes a new fast learning technique to learn a large number of individual user profiles.",
                "The efficacy and efficiency of the proposed algorithm are justified by theory and demonstrated on actual user data from Netflix and MovieLens.",
                "Categories and Subject Descriptors: B.3.3 [Information Search and Retrieval]: Information filtering General Terms: Algorithms 1.",
                "INTRODUCTION Personalization is the future of the Web, and it has achieved great success in industrial applications.",
                "For example, online stores, such as Amazon and Netflix, provide customized recommendations for additional products or services based on a users history.",
                "Recent offerings such as My MSN, My Yahoo!, My Google, and Google News have attracted much attention due to their potential ability to infer a users interests from his/her history.",
                "One major personalization topic studied in the information retrieval community is content-based personal recommendation systems1 .",
                "These systems learn user-specific profiles from user feedback so that they can recommend information tailored to each individual users interest without requiring the user to make an explicit query.",
                "Learning the user profiles is the core problem for these systems.",
                "A user profile is usually a classifier that can identify whether a document is relevant to the user or not, or a regression model that tells how relevant a document is to the user.",
                "One major challenge of building a recommendation or personalization system is that the profile learned for a particular user is usually of low quality when the amount of data from that particular user is small.",
                "This is known as the cold start problem.",
                "This means that any new user must endure poor initial performance until sufficient feedback from that user is provided to learn a reliable user profile.",
                "There has been much research on improving classification accuracy when the amount of labeled training data is small.",
                "The semi-supervised learning approach combines unlabeled and labeled data together to achieve this goal [26].",
                "Another approach is using domain knowledge.",
                "Researchers have modified different learning algorithms, such as Na¨ıveBayes [17], logistic regression [7], and SVMs [22], to integrate domain knowledge into a text classifier.",
                "The third approach is borrowing training data from other resources [5][7].",
                "The effectiveness of these different approaches is mixed, due to how well the underlying model assumption fits the data.",
                "One well-received approach to improve recommendation system performance for a particular user is borrowing information from other users through a Bayesian hierarchical modeling approach.",
                "Several researchers have demonstrated that this approach effectively trades off between shared and user-specific information, thus alleviating poor initial performance for each user[27][25].",
                "In order to learn a Bayesian hierarchical model, the system usually tries to find the most likely model parameters for the given data.",
                "A mature recommendation system usually works for millions of users.",
                "It is well known that learning the optimal parameters of a Bayesian hierarchical model is computationally expensive when there are thousands or millions of users.",
                "The EM algorithm is a commonly used technique for parameter learning due to its simplicity and convergence guarantee.",
                "However, a content based recommendation system often handles documents in a very high dimensional space, in which each document is represented by a very sparse vector.",
                "With careful analysis of the EM algorithm in this scenario (Section 4), we find that the EM tering, or item-based collaborative filtering.",
                "In this paper, the words filtering and recommendation are used interchangeably. algorithm converges very slowly due to the sparseness of the input variables.",
                "We also find that updating the model parameter at each EM iteration is also expensive with computational complexity of O(MK), where M is the number of users and K is the number of dimensions.",
                "This paper modifies the standard EM algorithm to create an improved learning algorithm, which we call the Modified EM algorithm.",
                "The basic idea is that instead of calculating the numerical solution for all the user profile parameters, we derive the analytical solution of the parameters for some feature dimensions, and at the M step use the analytical solution instead of the numerical solution estimated at E step for those parameters.",
                "This greatly reduces the computation at a single EM iteration, and also has the benefit of increasing the convergence speed of the learning algorithm.",
                "The proposed technique is not only well supported by theory, but also by experimental results.",
                "The organization of the remaining parts of this paper is as follows: Section 3 describes the Bayesian hierarchical linear regression modeling framework used for content-based recommendations.",
                "Section 4 describes how to learn the model parameters using the standard EM algorithm, along with using the new technique proposed in this paper.",
                "The experimental setting and results used to validate the proposed learning technique are reported in Sections 5 and 6.",
                "Section 7 summarizes and offers concluding remarks. 2.",
                "RELATED WORK Providing personalized recommendations to users has been identified as a very important problem in the IR community since the 1970s.",
                "The approaches that have been used to solve this problem can be roughly classified into two major categories: content based filtering versus collaborative filtering.",
                "Content-based filtering studies the scenario where a recommendation system monitors a document stream and pushes documents that match a user profile to the corresponding user.",
                "The user may read the delivered documents and provide explicit relevance feedback, which the filtering system then uses to update the users profile using relevance feedback retrieval models (e.g.",
                "Boolean models, vector space models, traditional probabilistic models [20] , inference networks [3] and language models [6]) or machine learning algorithms (e.g.",
                "Support Vector Machines (SVM), K nearest neighbors (K-NN) clustering, neural networks, logistic regression, or Winnow [16] [4] [23]).",
                "Collaborative filtering goes beyond merely using document content to recommend items to a user by leveraging information from other users with similar tastes and preferences in the past.",
                "Memorybased heuristics and model based approaches have been used in collaborative filtering task [15] [8] [2] [14] [12] [11].",
                "This paper contributes to the content-based recommendation research by improving the efficiency and effectiveness of Bayesian hierarchical linear models, which have a strong theoretical basis and good empirical performance on recommendation tasks[27][25].",
                "This paper does not intend to compare content-based filtering with collaborative filtering or claim which one is a better.",
                "We think each complements the other, and that content-based filtering is extremely useful for handling new documents/items with little or no user feedback.",
                "Similar to some other researchers[18][1][21], we found that a recommendation system will be more effective when both techniques are combined.",
                "However, this is beyond the scope of this paper and thus not discussed here. 3.",
                "BAYESIAN HIERARCHICAL LINEAR REGRESSION Assume there are M users in the system.",
                "The task of the system is to recommend documents that are relevant to each user.",
                "For each user, the system learns a user model from the users history.",
                "In the rest of this paper, we will use the following notations to represent the variables in the system. m = 1, 2, ..., M: The index for each individual user.",
                "M is the total number of users. wm: The user model parameter associated with user m. wm is a K dimensional vector. j = 1, 2, ..., Jm: The index for a set of data for user m. Jm is the number of training data for user m. Dm = {(xm,j, ym,j)}: A set of data associated with user m. xm,j is a K dimensional vector that represents the mth users jth training document.2 ym,j is a scalar that represents the label of document xm,j. k = 1, 2, ..., K: The dimensional index of input variable x.",
                "The Bayesian hierarchical modeling approach has been widely used in real-world information retrieval applications.",
                "Generalized Bayesian hierarchical linear models, one of the simplest Bayesian hierarchical models, are commonly used and have achieved good performance on collaborative filtering [25] and content-based adaptive filtering [27] tasks.",
                "Figure 1 shows the graphical representation of a Bayesian hierarchical model.",
                "In this graph, each user model is represented by a random vector wm.",
                "We assume a user model is sampled randomly from a prior distribution P(w|Φ).",
                "The system can predict the user label y of a document x given an estimation of wm (or wms distribution) using a function y = f(x, w).",
                "The model is called generalized Bayesian hierarchical linear model when y = f(wT x) is any generalized linear model such as logistic regression, SVM, and linear regression.",
                "To reliably estimate the user model wm, the system can borrow information from other users through the prior Φ = (µ, Σ).",
                "Now we look at one commonly used model where y = wT x + , where ∼ N(0, σ2 ) is a random noise [25][27].",
                "Assume that each user model wm is an independent draw from a population distribution P(w|Φ), which is governed by some unknown hyperparameter Φ.",
                "Let the prior distribution of user model w be a Gaussian distribution with parameter Φ = (µ, Σ), which is the commonly used prior for linear models. µ = (µ1, µ2, ..., µK ) is a K dimensional vector that represents the mean of the Gaussian distribution, and Σ is the covariance matrix of the Gaussian.",
                "Usually, a Normal distribution N(0, aI) and an Inverse Wishart distribution P(Σ) ∝ |Σ|− 1 2 b exp(−1 2 ctr(Σ−1 )) are used as hyperprior to model the prior distribution of µ and Σ respectively.",
                "I is the K dimensional identity matrix, and a, b, and c are real numbers.",
                "With these settings, we have the following model for the system: 1. µ and Σ are sampled from N(0, aI) and IWν (aI), respectively. 2 The first dimension of x is a dummy variable that always equals to 1.",
                "Figure 1: Illustration of dependencies of variables in the hierarchical model.",
                "The rating, y, for a document, x, is conditioned on the document and the user model, wm, associated with the user m. Users share information about their models through the prior, Φ = (µ, Σ). 2.",
                "For each user m, wm is sampled randomly from a Normal distribution: wm ∼ N(µ, Σ2 ) 3.",
                "For each item xm,j, ym,j is sampled randomly from a Normal distribution: ym,j ∼ N(wT mxm,j, σ2 ).",
                "Let θ = (Φ, w1, w2, ..., wM ) represent the parameters of this system that needs to be estimated.",
                "The joint likelihood for all the variables in the probabilistic model, which includes the data and the parameters, is: P(D, θ) = P(Φ) m P(wm|Φ) j P(ym,j|xm,j, wm) (1) For simplicity, we assume a, b, c, and σ are provided to the system. 4.",
                "MODEL PARAMETER LEARNING If the prior Φ is known, finding the optimal wm is straightforward: it is a simple linear regression.",
                "Therefore, we will focus on estimating Φ.",
                "The maximum a priori solution of Φ is given by ΦMAP = arg max Φ P(Φ|D) (2) = arg max Φ P(Φ, D) P(D) (3) = arg max Φ P(D|Φ)P(Φ) (4) = arg max Φ w P(D|w, Φ)P(w|Φ)P(Φ)dw (5) Finding the optimal solution for the above problem is challenging, since we need to integrate over all w = (w1, w2, ..., wM ), which are unobserved hidden variables. 4.1 EM Algorithm for Bayesian Hierarchical Linear Models In Equation 5, Φ is the parameter needs to be estimated, and the result depends on unobserved latent variables w. This kind of optimization problem is usually solved by the EM algorithm.",
                "Applying EM to the above problem, the set of user models w are the unobservable hidden variables and we have: Q = w P(w|µ, Σ2 , Dm) log P(µ, Σ2 , w, D)dw Based on the derivation of the EM formulas presented in [24], we have the following Expectation-Maximization steps for finding the optimal hyperparameters.",
                "For space considerations, we omit the derivation in this paper since it is not the focus of our work.",
                "E step: For each user m, estimate the user model distribution P(wm|Dm, Φ) = N(wm; ¯wm, Σ2 m) based on the current estimation of the prior Φ = (µ, Σ2 ). ¯wm = ((Σ2 )−1 + Sxx,m σ2 )−1 ( Sxy,m σ2 + (Σ2 )−1 µ) (6) Σ2 m = ((Σ2 )−1 + Sxx,m σ2 )−1 (7) where Sxx,m = j xm,jxT m,j Sxy,m = j xm,jym,j M step: Optimize the prior Φ = (µ, Σ2 ) based on the estimation from the last E step. µ = 1 M m ¯wm (8) Σ2 = 1 M m Σ2 m + ( ¯wm − µ)( ¯wm − µ)T (9) Many machine learning driven IR systems use a point estimate of the parameters at different stages in the system.",
                "However, we are estimating the posterior distribution of the variables at the E step.",
                "This avoids overfitting wm to a particular users data, which may be small and noisy.",
                "A detailed discussion about this subject appears in [10]. 4.2 New Algorithm: Modified EM Although the EM algorithm is widely studied and used in machine learning applications, using the above EM process to solve Bayesian hierarchical linear models in large-scale information retrieval systems is still too computationally expensive.",
                "In this section, we describe why the learning rate of the EM algorithm is slow in our application and introduce a new technique to make the learning of the Bayesian hierarchical linear model scalable.",
                "The derivation of the new learning algorithm will be based on the EM algorithm described in the previous section.",
                "First, the covariance matrices Σ2 , Σ2 m are usually too large to be computationally feasible.",
                "For simplicity, and as a common practice in IR, we do not model the correlation between features.",
                "Thus we approximate these matrices with K dimensional diagonal matrices.",
                "In the rest of the paper, we use these symbols to represent their diagonal approximations: Σ2 =     σ2 1 0 .. 0 0 σ2 2 .. 0 .. .. .. .. 0 0 .. σ2 K     Σ2 m =     σ2 m,1 0 .. 0 0 σ2 m,2 .. 0 .. .. .. .. 0 0 .. σ2 m,K     Secondly, and most importantly, the input space is very sparse and there are many dimensions that are not related to a particular user in a real IR application.",
                "For example, let us consider a movie recommendation system, with the input variable x representing a particular movie.",
                "For the jth movie that the user m has seen, let xm,j,k = 1 if the director of the movie is Jean-Pierre Jeunet (indexed by k).",
                "Here we assume that whether or not that this director directed a specific movie is represented by the kth dimension.",
                "If the user m has never seen a movie directed by Jean-Pierre Jeunet, then the corresponding dimension is always zero (xm,j,k = 0 for all j) .",
                "One major drawback of the EM algorithm is that the importance of a feature, µk, may be greatly dominated by users who have never encountered this feature (i.e. j xm,j,k = 0) at the M step (Equation 8).",
                "Assume that 100 out of 1 million users have viewed the movie directed by Jean-Pierre Jeunet, and that the viewers have rated all of his movies as excellent.",
                "Intuitively, he is a good director and the weight for him (µk) should be high.",
                "Before the EM iteration, the initial value of µ is usually set to 0.",
                "Since the other 999,900 users have not seen this movie, their corresponding weights (w1,k, w2,k, ..., wm,k..., w999900,k) for that director would be very small initially.",
                "Thus the corresponding weight of the director in the prior µk at the first M step would be very low , and the variance σm,k will be large (Equations 8 and 7).",
                "It is undesirable that users who have never seen any movie produced by the director influence the importance of the director so much.",
                "This makes the convergence of the standard EM algorithm very slow.",
                "Now lets look at whether we can improve the learning speed of the algorithm.",
                "Without a loss of generality, let us assume that the kth dimension of the input variable x is not related to a particular user m. By which we mean, xm,j,k = 0 for all j = 1, ..., Jm.",
                "It is straightforward to prove that the kth row and kth column of Sxx,m are completely filled with zeros, and that the kth dimension of Sxy,m is zeroed as well.",
                "Thus the corresponding kth dimension of the user models mean, ¯wm, should be equal to that of the prior: ¯wm,k = µk, with the corresponding covariance of σm,k = σk.",
                "At the M step, the standard EM algorithm uses the numerical solution of the distribution P(wm|Dm, Φ) estimated at E step (Equation 8 and Equation 7).",
                "However, the numerical solutions are very unreliable for ¯wm,k and σm,k when the kth dimension is not related to the mth user.",
                "A better approach is using the analytical solutions ¯wm,k = µk, and σm,k = σk for the unrelated (m, k) pairs, along with the numerical solution estimated at E step for the other (m, k) pairs.",
                "Thus we get the following new EM-like algorithm: Modified E step: For each user m, estimate the user model distribution P(wm|Dm, Φ) = N(wm; ¯wm, Σ2 m) based on the current estimation of σ , µ, Σ2 . ¯wm = ((Σ2 )−1 + Sxx,m σ2 )−1 ( Sxy,m σ2 + (Σ2 )−1 µ)(10) σ2 m,k = ((σ2 k)−1 + sxx,m,k σ2 )−1 (11) where sxx,m,k = j x2 m,j,k and sxy,m,k = j xm,j,kym,j Modified M Step Optimize the prior Φ = (µ, Σ2 ) based on the estimation from the last E step for related userfeature pairs.",
                "The M step implicitly uses the analytical solution for unrelated user-feature pairs. µk = 1 Mk m:related ¯wm,k (12) σ2 k = 1 Mk m:related σ2 m,k +( ¯wm,k − µk)( ¯wm,k − µk)T (13) where Mk is the number of users that are related to feature k We only estimate the diagonal of Σ2 m and Σ since we are using the diagonal approximation of the covariance matrices.",
                "To estimate ¯wm, we only need to calculate the numerical solutions for dimensions that are related to user m. To estimate σ2 k and µk, we only sum over users that are related to the kth feature.",
                "There are two major benefits of the new algorithm.",
                "First, because only the related (m, k) pairs are needed at the modified M step, the computational complexity in a single EM iteration is much smaller when the data is sparse, and many of (m, k) pairs are unrelated.",
                "Second, the parameters estimated at the modified M step (Equations 12 - 13) are more accurate than the standard M step described in Section 4.1 because the exact analytical solutions ¯wm,k = µk and σm,k = σk for the unrelated (m, k) pairs were used in the new algorithm instead of an approximate solution as in the standard algorithm. 5.",
                "EXPERIMENTAL METHODOLOGY 5.1 Evaluation Data Set To evaluate the proposed technique, we used the following three major data sets (Table 1): MovieLens Data: This data set was created by combining the relevance judgments from the MovieLens[9] data set with documents from the Internet Movie Database (IMDB).",
                "MovieLens allows users to rank how much he/she enjoyed a specific movie on a scale from 1 to 5.",
                "This likeability rating was used as a measurement of how relevant the document representing the corresponding movie is to the user.",
                "We considered documents with likeability scores of 4 or 5 as relevant, and documents with a score of 1 to 3 as irrelevant to the user.",
                "MovieLens provided relevance judgments on 3,057 documents from 6,040 separate users.",
                "On average, each user rated 151 movies, of these 87 were judged to be relevant.",
                "The average score for a document was 3.58.",
                "Documents representing each movie were constructed from the portion of the IMDB database that is available for public download[13].",
                "Based on this database, we created one document per movie that contained the relevant information about it (e.g. directors, actors, etc.).",
                "Table 1: Data Set Statistics.",
                "On Reuters, the number of rating for a simulated user is the number of documents relevant to the corresponding topic.",
                "Data Users Docs Ratings per User MovieLens 6,040 3,057 151 Netflix-all 480,189 17,770 208 Netflix-1000 1000 17,770 127 Reuters-C 34 100,000 3949 Reuters-E 26 100,000 1632 Reuters-G 33 100,000 2222 Reuters-M 10 100,000 6529 Netflix Data: This data set was constructed by combining documents about movies crawled from the web with a set of actual movie rental customer relevance judgments from Netflix[19].",
                "Netflix publicly provides the relevance judgments of 480,189 anonymous customers.",
                "There are around 100 million rating on a scale of 1 to 5 for 17,770 documents.",
                "Similar to MovieLens, we considered documents with likeability scores of 4 or 5 as relevant.",
                "This number was reduced to 1000 customers through random sampling.",
                "The average customer on the reduced data set provided 127 judgments, with 70 being deemed relevant.",
                "The average score for documents is 3.55.",
                "Reuters Data: This is the Reuters Corpus, Volume 1.",
                "It covers 810,000 Reuters English language news stories from August 20, 1996 to August 19, 1997.",
                "Only the first 100,000 news were used in our experiments.",
                "The Reuters corpus comes with a topic hierarchy.",
                "Each document is assigned to one of several locations on the hierarchical tree.",
                "The first level of the tree contains four topics, denoted as C, E, M, and G. For the experiments in this paper, the tree was cut at level 1 to create four smaller trees, each of which corresponds to one smaller data set: Reuters-E Reuters-C, ReutersM and Reuters-G. For each small data set, we created several profiles, one profile for each node in a sub-tree, to simulate multiple users, each with a related, yet separate definition of relevance.",
                "All the user profiles on a sub-tree are supposed to share the same prior model distribution.",
                "Since this corpus explicitly indicates only the relevant documents for a topic(user), all other documents are considered irrelevant. 5.2 Evaluation We designed the experiments to answer the following three questions: 1.",
                "Do we need to take the effort to use a Bayesian approach and learn a prior from other users? 2.",
                "Does the new algorithm work better than the standard EM algorithm for learning the Bayesian hierarchical linear model? 3.",
                "Can the new algorithm quickly learn many user models?",
                "To answer the first question, we compared the Bayesian hierarchical models with commonly used Norm-2 regularized linear regression models.",
                "In fact, the commonly used approach is equivalent to the model learned at the end of the first EM iteration.",
                "To answer the second question, we compared the proposed new algorithm with the standard EM algorithm to see whether the new learning algorithm is better.",
                "To answer the third question, we tested the efficiency of the new algorithm on the entire Netflix data set where about half a million user models need to be learned together.",
                "For the MovieLens and Netflix data sets, algorithm effectiveness was measured by mean square error, while on the Reuters data set classification error was used because it was more informative.",
                "We first evaluated the performance on each individual user, and then estimated the macro average over all users.",
                "Statistical tests (t-tests) were carried out to see whether the results are significant.",
                "For the experiments on the MovieLens and Netflix data sets, we used a random sample of 90% of each user for training, and the rest for testing.",
                "On Reuters data set, because there are too many relevant documents for each topic in the corpus, we used a random sample of 10% of each topic for training, and 10% of the remaining documents for testing.",
                "For all runs, we set (a, b, c, Σ ) = (0.1, 10, 0.1, 1) manually. 6.",
                "EXPERIMENTAL RESULTS Figure 2, Figure 3, and Figure 4 show that on all data sets, the Bayesian hierarchical modeling approach has a statistical significant improvement over the regularized linear regression model, which is equivalent to the Bayesian hierarchical models learned at the first iteration.",
                "Further analysis shows a negative correlation between the number of training data for a user and the improvement the system gets.",
                "This suggests that the borrowing information from other users has more significant improvements for users with less training data, which is as expected.",
                "However, the strength of the correlation differs over data sets, and the amount of training data is not the only characteristics that will influence the final performance.",
                "Figure 2 and Figure 3 show that the proposed new algorithm works better than the standard EM algorithm on the Netflix and MovieLens data sets.",
                "This is not surprising since the number of related feature-users pairs is much smaller than the number of unrelated feature-user pairs on these two data sets, and thus the proposed new algorithm is expected to work better.",
                "Figure 4 shows that the two algorithms work similarly on the Reuters-E data set.",
                "The accuracy of the new algorithm is similar to that of the standard EM algorithm at each iteration.",
                "The general patterns are very similar on other Reuters subsets.",
                "Further analysis shows that only 58% of the user-feature pairs are unrelated on this data set.",
                "Since the number of unrelated user-feature pairs is not extremely large, the sparseness is not a serious problem on the Reuters data set.",
                "Thus the two learning algorithms perform similarly.",
                "The results suggest that only on a corpus where the number of unrelated user-feature pairs is much larger than the number of related pairs, such as on the Netflix data set, the proposed technique will get a significant improvement over standard EM.",
                "However, the experiments also show that when the assumption does not hold, the new algorithm does not hurt performance.",
                "Although the proposed technique is faster than standard Figure 2: Performance on a Netflix subset with 1,000 users.",
                "The new algorithm is statistical significantly better than EM algorithm at iterations 2 - 10.",
                "Norm-2 regularized linear models are equivalent to the Bayesian hierarchical models learned at the first iteration, and are statistical significantly worse than the Bayesian hierarchical models. 0 2 4 6 8 10 1 1.05 1.1 1.15 1.2 1.25 1.3 1.35 1.4 Iterations MeanSquareError New Algorithm Traditional EM 1 2 3 4 5 6 7 8 9 10 0.33 0.34 0.35 0.36 0.37 0.38 0.39 Iterations ClassificationError New Algorithm Traditional EM Figure 3: Performance on a MovieLens subset with 1,000 users.",
                "The new algorithm is statistical significantly better than EM algorithm at iteration 2 to 17 (evaluated with mean square error). 1 6 11 16 21 0.5 1 1.5 2 2.5 3 3.5 Iterations MeanSquareError New Algorithm Traditional EM 1 6 11 16 21 0.35 0.4 0.45 0.5 0.55 0.6 0.65 Iterations ClassificationError New Algorithm Traditional EM Figure 4: Performance on a Reuters-E subset with 26 profiles.",
                "Performances on Reuters-C, Reuters-M, Reuters-G are similar. 1 2 3 4 5 0.011 0.0115 0.012 0.0125 0.013 0.0135 0.014 Iterations MeanSquareError New Algorithm Traditional EM 1 2 3 4 5 0.0102 0.0104 0.0106 0.0108 0.011 0.0112 0.0114 Iterations ClassificationError New Algorithm Traditional EM EM, can it really learn millions of user models quickly?",
                "Our results show that the modified EM algorithm converges quickly, and 2 - 3 modified EM iterations would result in a reliable estimation.",
                "We evaluated the algorithm on the whole Netflix data set (480,189 users, 159,836 features, and 100 million ratings) running on a single CPU PC (2GB memory, P4 3GHz).",
                "The system finished one modified EM iteration in about 4 hours.",
                "This demonstrates that the proposed technique can efficiently handle large-scale system like Netflix. 7.",
                "CONCLUSION Content-based user profile learning is an important problem and is the key to providing personal recommendations to a user, especially for recommending new items with a small number of ratings.",
                "The Bayesian hierarchical modeling approach is becoming an important user profile learning approach due to its theoretically justified ability to help one user through information transfer from the other users by way of hyperpriors.",
                "This paper examined the weakness of the popular EM based learning approach for Bayesian hierarchical linear models and proposed a better learning technique called Modified EM.",
                "We showed that the new technique is theoretically more computationally efficient than the standard EM algorithm.",
                "Evaluation on the MovieLens and Netflix data sets demonstrated the effectiveness of the new technique when the data is sparse, by which we mean the ratio of related user-feature pairs to unrelated pairs is small.",
                "Evaluation on the Reuters data set showed that the new technique performed similar to the standard EM algorithm when the sparseness condition does not hold.",
                "In general, it is better to use the new algorithm since it is as simple as standard EM, the performance is either better or similar to EM, and the computation complexity is lower at each iteration.",
                "It is worth mentioning that even if the original problem space is not sparse, sparseness can be created artificially when a recommendation system uses user-specific feature selection techniques to reduce the noise and user model complexity.",
                "The proposed technique can also be adapted to improve the learning in such a scenario.",
                "We also demonstrated that the proposed technique can learn half a million user profiles from 100 million ratings in a few hours with a single CPU.",
                "The research is important because scalability is a major concern for researchers when using the Bayesian hierarchical linear modeling approach to build a practical large scale system, even though the literature have demonstrated the effectiveness of the models in many applications.",
                "Our work is one major step on the road to make Bayesian hierarchical linear models more practical.",
                "The proposed new technique can be easily adapted to run on a cluster of machines, and thus further speed up the learning process to handle a larger scale system with hundreds of millions of users.",
                "The research has much potential to benefit people using EM algorithm on many other IR problems as well as machine learning problems.",
                "EM algorithm is a commonly used machine learning technique.",
                "It is used to find model parameters in many IR problems where the training data is very sparse.",
                "Although we are focusing on the Bayesian hierarchical linear models for recommendation and filtering, the new idea of using analytical solution instead of numerical solution for unrelated user-feature pairs at the M step could be adapted to many other problems. 8.",
                "ACKNOWLEDGMENTS We thank Wei Xu, David Lewis and anonymous reviewers for valuable feedback on the work described in this paper.",
                "Part of the work was supported by Yahoo, Google, the Petascale Data Storage Institute and the Institute for Scalable Scientific Data Management.",
                "Any opinions, findings, conclusions, or recommendations expressed in this material are those of the authors, and do not necessarily reflect those of the sponsors. 9.",
                "REFERENCES [1] C. Basu, H. Hirsh, and W. Cohen.",
                "Recommendation as classification: Using social and content-based information in recommendation.",
                "In Proceedings of the Fifteenth National Conference on Artificial Intelligence, 1998. [2] J. S. Breese, D. Heckerman, and C. Kadie.",
                "Empirical analysis of predictive algorithms for collaborative filtering.",
                "Technical report, Microsoft Research, One Microsoft Way, Redmond, WA 98052, 1998. [3] J. Callan.",
                "Document filtering with inference networks.",
                "In Proceedings of the Nineteenth Annual International ACM SIGIR Conference on Research and Development in Information Retrieval, pages 262-269, 1996. [4] N. Cancedda, N. Cesa-Bianchi, A. Conconi, C. Gentile, C. Goutte, T. Graepel, Y. Li, J. M. Renders, J. S. Taylor, and A. Vinokourov.",
                "Kernel method for document filtering.",
                "In The Eleventh Text REtrieval Conference (TREC11).",
                "National Institute of Standards and Technology, special publication 500-249, 2003. [5] C. Chelba and A. Acero.",
                "Adaptation of maximum entropy capitalizer: Little data can help a lot.",
                "In D. Lin and D. Wu, editors, Proceedings of EMNLP 2004, pages 285-292, Barcelona, Spain, July 2004.",
                "Association for Computational Linguistics. [6] B. Croft and J. Lafferty, editors.",
                "Language Modeling for Information Retrieval.",
                "Kluwer, 2002. [7] A. Dayanik, D. D. Lewis, D. Madigan, V. Menkov, and A. Genkin.",
                "Constructing informative prior distributions from domain knowledge in text classification.",
                "In SIGIR 06: Proceedings of the 29th annual international ACM SIGIR conference on Research and development in information retrieval, pages 493-500, New York, NY, USA, 2006.",
                "ACM Press. [8] J. Delgado and N. Ishii.",
                "Memory-based weightedmajority prediction for <br>recommender system</br>s.",
                "In ACM SIGIR99 Workshop on Recommender Systems, 1999. [9] GroupLens.",
                "Movielens. http://www.grouplens.org/taxonomy/term/14, 2006. [10] D. Heckerman.",
                "A tutorial on learning with bayesian networks.",
                "In M. Jordan, editor, Learning in Graphical Models.",
                "Kluwer Academic, 1998. [11] J. L. Herlocker, J.",
                "A. Konstan, A. Borchers, and J. Riedl.",
                "An algorithmic framework for performing collaborative filtering.",
                "In SIGIR 99: Proceedings of the 22nd annual international ACM SIGIR conference on Research and development in information retrieval, pages 230-237, New York, NY, USA, 1999.",
                "ACM Press. [12] T. Hofmann and J. Puzicha.",
                "Latent class models for collaborative filtering.",
                "In IJCAI 99: Proceedings of the Sixteenth International Joint Conference on Artificial Intelligence, pages 688-693, San Francisco, CA, USA, 1999.",
                "Morgan Kaufmann Publishers Inc. [13] I. M. D. (IMDB).",
                "Internet movie database. http://www.imdb.com/interfaces/, 2006. [14] R. Jin, J. Y. Chai, and L. Si.",
                "An automatic weighting scheme for collaborative filtering.",
                "In SIGIR 04: Proceedings of the 27th annual international ACM SIGIR conference on Research and development in information retrieval, pages 337-344, New York, NY, USA, 2004.",
                "ACM Press. [15] J.",
                "A. Konstan, B. N. Miller, D. Maltz, J. L. Herlocker, L. R. Gordon, and J. Riedl.",
                "GroupLens: Applying collaborative filtering to Usenet news.",
                "Communications of the ACM, 40(3):77-87, 1997. [16] D. Lewis.",
                "Applying support vector machines to the TREC-2001 batch filtering and routing tasks.",
                "In Proceedings of the Eleventh Text REtrieval Conference (TREC-11), 2002. [17] B. Liu, X. Li, W. S. Lee, , and P. Yu.",
                "Text classification by labeling words.",
                "In Proceedings of The Nineteenth National Conference on Artificial Intelligence (AAAI-2004), July 25-29, 2004. [18] P. Melville, R. J. Mooney, and R. Nagarajan.",
                "Content-boosted collaborative filtering for improved recommendations.",
                "In Proceedings of the Eighteenth National Conference on Artificial Intelligence (AAAI-2002), Edmonton, Canada, 2002. [19] Netflix.",
                "Netflix prize. http://www.netflixprize.com (visited on Nov. 30, 2006), 2006. [20] S. Robertson and K. Sparck-Jones.",
                "Relevance weighting of search terms.",
                "In Journal of the American Society for Information Science, volume 27, pages 129-146, 1976. [21] J. Wang, A. P. de Vries, and M. J. T. Reinders.",
                "Unifying user-based and item-based collaborative filtering approaches by similarity fusion.",
                "In SIGIR 06: Proceedings of the 29th annual international ACM SIGIR conference on Research and development in information retrieval, pages 501-508, New York, NY, USA, 2006.",
                "ACM Press. [22] X. Wu and R. K. Srihari.",
                "Incorporating prior knowledge with weighted margin support vector machines.",
                "In Proc.",
                "ACM Knowledge Discovery Data Mining Conf. (ACM SIGKDD 2004), Aug. 2004. [23] Y. Yang, S. Yoo, J. Zhang, and B. Kisiel.",
                "Robustness of adaptive filtering methods in a cross-benchmark evaluation.",
                "In Proceedings of the 28th Annual International ACM SIGIR Conference on Research and Development in Information Retrieval, 2005. [24] K. Yu, V. Tresp, and A. Schwaighofer.",
                "Learning gaussian processes from multiple tasks.",
                "In ICML 05: Proceedings of the 22nd international conference on Machine learning, pages 1012-1019, New York, NY, USA, 2005.",
                "ACM Press. [25] K. Yu, V. Tresp, and S. Yu.",
                "A nonparametric hierarchical bayesian framework for information filtering.",
                "In SIGIR 04: Proceedings of the 27th annual international ACM SIGIR conference on Research and development in information retrieval, pages 353-360.",
                "ACM Press, 2004. [26] X. Zhu.",
                "Semi-supervised learning literature survey.",
                "Technical report, University of Wisconsin - Madison, December 9, 2006. [27] P. Zigoris and Y. Zhang.",
                "Bayesian adaptive user profiling with explicit & implicit feedback.",
                "In Conference on Information and Knowledge Mangement 2006, 2006."
            ],
            "original_annotated_samples": [],
            "translated_annotated_samples": [
                "Predicción de modificación de peso basada en la memoria para \"Sistema de recomendación\" s."
            ],
            "translated_text": "",
            "candidates": [
                "sistema de recomendación",
                "Sistema de recomendación"
            ],
            "error": []
        },
        "information filter": {
            "translated_key": "filtro de información",
            "is_in_text": true,
            "original_annotated_sentences": [
                "Efficient Bayesian Hierarchical User Modeling for Recommendation Systems Yi Zhang, Jonathan Koren School of Engineering University of California Santa Cruz Santa Cruz, CA, USA {yiz, jonathan}@soe.ucsc.edu ABSTRACT A content-based personalized recommendation system learns user specific profiles from user feedback so that it can deliver information tailored to each individual users interest.",
                "A system serving millions of users can learn a better user profile for a new user, or a user with little feedback, by borrowing information from other users through the use of a Bayesian hierarchical model.",
                "Learning the model parameters to optimize the joint data likelihood from millions of users is very computationally expensive.",
                "The commonly used EM algorithm converges very slowly due to the sparseness of the data in IR applications.",
                "This paper proposes a new fast learning technique to learn a large number of individual user profiles.",
                "The efficacy and efficiency of the proposed algorithm are justified by theory and demonstrated on actual user data from Netflix and MovieLens.",
                "Categories and Subject Descriptors: B.3.3 [Information Search and Retrieval]: Information filtering General Terms: Algorithms 1.",
                "INTRODUCTION Personalization is the future of the Web, and it has achieved great success in industrial applications.",
                "For example, online stores, such as Amazon and Netflix, provide customized recommendations for additional products or services based on a users history.",
                "Recent offerings such as My MSN, My Yahoo!, My Google, and Google News have attracted much attention due to their potential ability to infer a users interests from his/her history.",
                "One major personalization topic studied in the information retrieval community is content-based personal recommendation systems1 .",
                "These systems learn user-specific profiles from user feedback so that they can recommend information tailored to each individual users interest without requiring the user to make an explicit query.",
                "Learning the user profiles is the core problem for these systems.",
                "A user profile is usually a classifier that can identify whether a document is relevant to the user or not, or a regression model that tells how relevant a document is to the user.",
                "One major challenge of building a recommendation or personalization system is that the profile learned for a particular user is usually of low quality when the amount of data from that particular user is small.",
                "This is known as the cold start problem.",
                "This means that any new user must endure poor initial performance until sufficient feedback from that user is provided to learn a reliable user profile.",
                "There has been much research on improving classification accuracy when the amount of labeled training data is small.",
                "The semi-supervised learning approach combines unlabeled and labeled data together to achieve this goal [26].",
                "Another approach is using domain knowledge.",
                "Researchers have modified different learning algorithms, such as Na¨ıveBayes [17], logistic regression [7], and SVMs [22], to integrate domain knowledge into a text classifier.",
                "The third approach is borrowing training data from other resources [5][7].",
                "The effectiveness of these different approaches is mixed, due to how well the underlying model assumption fits the data.",
                "One well-received approach to improve recommendation system performance for a particular user is borrowing information from other users through a Bayesian hierarchical modeling approach.",
                "Several researchers have demonstrated that this approach effectively trades off between shared and user-specific information, thus alleviating poor initial performance for each user[27][25].",
                "In order to learn a Bayesian hierarchical model, the system usually tries to find the most likely model parameters for the given data.",
                "A mature recommendation system usually works for millions of users.",
                "It is well known that learning the optimal parameters of a Bayesian hierarchical model is computationally expensive when there are thousands or millions of users.",
                "The EM algorithm is a commonly used technique for parameter learning due to its simplicity and convergence guarantee.",
                "However, a content based recommendation system often handles documents in a very high dimensional space, in which each document is represented by a very sparse vector.",
                "With careful analysis of the EM algorithm in this scenario (Section 4), we find that the EM tering, or item-based collaborative filtering.",
                "In this paper, the words filtering and recommendation are used interchangeably. algorithm converges very slowly due to the sparseness of the input variables.",
                "We also find that updating the model parameter at each EM iteration is also expensive with computational complexity of O(MK), where M is the number of users and K is the number of dimensions.",
                "This paper modifies the standard EM algorithm to create an improved learning algorithm, which we call the Modified EM algorithm.",
                "The basic idea is that instead of calculating the numerical solution for all the user profile parameters, we derive the analytical solution of the parameters for some feature dimensions, and at the M step use the analytical solution instead of the numerical solution estimated at E step for those parameters.",
                "This greatly reduces the computation at a single EM iteration, and also has the benefit of increasing the convergence speed of the learning algorithm.",
                "The proposed technique is not only well supported by theory, but also by experimental results.",
                "The organization of the remaining parts of this paper is as follows: Section 3 describes the Bayesian hierarchical linear regression modeling framework used for content-based recommendations.",
                "Section 4 describes how to learn the model parameters using the standard EM algorithm, along with using the new technique proposed in this paper.",
                "The experimental setting and results used to validate the proposed learning technique are reported in Sections 5 and 6.",
                "Section 7 summarizes and offers concluding remarks. 2.",
                "RELATED WORK Providing personalized recommendations to users has been identified as a very important problem in the IR community since the 1970s.",
                "The approaches that have been used to solve this problem can be roughly classified into two major categories: content based filtering versus collaborative filtering.",
                "Content-based filtering studies the scenario where a recommendation system monitors a document stream and pushes documents that match a user profile to the corresponding user.",
                "The user may read the delivered documents and provide explicit relevance feedback, which the filtering system then uses to update the users profile using relevance feedback retrieval models (e.g.",
                "Boolean models, vector space models, traditional probabilistic models [20] , inference networks [3] and language models [6]) or machine learning algorithms (e.g.",
                "Support Vector Machines (SVM), K nearest neighbors (K-NN) clustering, neural networks, logistic regression, or Winnow [16] [4] [23]).",
                "Collaborative filtering goes beyond merely using document content to recommend items to a user by leveraging information from other users with similar tastes and preferences in the past.",
                "Memorybased heuristics and model based approaches have been used in collaborative filtering task [15] [8] [2] [14] [12] [11].",
                "This paper contributes to the content-based recommendation research by improving the efficiency and effectiveness of Bayesian hierarchical linear models, which have a strong theoretical basis and good empirical performance on recommendation tasks[27][25].",
                "This paper does not intend to compare content-based filtering with collaborative filtering or claim which one is a better.",
                "We think each complements the other, and that content-based filtering is extremely useful for handling new documents/items with little or no user feedback.",
                "Similar to some other researchers[18][1][21], we found that a recommendation system will be more effective when both techniques are combined.",
                "However, this is beyond the scope of this paper and thus not discussed here. 3.",
                "BAYESIAN HIERARCHICAL LINEAR REGRESSION Assume there are M users in the system.",
                "The task of the system is to recommend documents that are relevant to each user.",
                "For each user, the system learns a user model from the users history.",
                "In the rest of this paper, we will use the following notations to represent the variables in the system. m = 1, 2, ..., M: The index for each individual user.",
                "M is the total number of users. wm: The user model parameter associated with user m. wm is a K dimensional vector. j = 1, 2, ..., Jm: The index for a set of data for user m. Jm is the number of training data for user m. Dm = {(xm,j, ym,j)}: A set of data associated with user m. xm,j is a K dimensional vector that represents the mth users jth training document.2 ym,j is a scalar that represents the label of document xm,j. k = 1, 2, ..., K: The dimensional index of input variable x.",
                "The Bayesian hierarchical modeling approach has been widely used in real-world information retrieval applications.",
                "Generalized Bayesian hierarchical linear models, one of the simplest Bayesian hierarchical models, are commonly used and have achieved good performance on collaborative filtering [25] and content-based adaptive filtering [27] tasks.",
                "Figure 1 shows the graphical representation of a Bayesian hierarchical model.",
                "In this graph, each user model is represented by a random vector wm.",
                "We assume a user model is sampled randomly from a prior distribution P(w|Φ).",
                "The system can predict the user label y of a document x given an estimation of wm (or wms distribution) using a function y = f(x, w).",
                "The model is called generalized Bayesian hierarchical linear model when y = f(wT x) is any generalized linear model such as logistic regression, SVM, and linear regression.",
                "To reliably estimate the user model wm, the system can borrow information from other users through the prior Φ = (µ, Σ).",
                "Now we look at one commonly used model where y = wT x + , where ∼ N(0, σ2 ) is a random noise [25][27].",
                "Assume that each user model wm is an independent draw from a population distribution P(w|Φ), which is governed by some unknown hyperparameter Φ.",
                "Let the prior distribution of user model w be a Gaussian distribution with parameter Φ = (µ, Σ), which is the commonly used prior for linear models. µ = (µ1, µ2, ..., µK ) is a K dimensional vector that represents the mean of the Gaussian distribution, and Σ is the covariance matrix of the Gaussian.",
                "Usually, a Normal distribution N(0, aI) and an Inverse Wishart distribution P(Σ) ∝ |Σ|− 1 2 b exp(−1 2 ctr(Σ−1 )) are used as hyperprior to model the prior distribution of µ and Σ respectively.",
                "I is the K dimensional identity matrix, and a, b, and c are real numbers.",
                "With these settings, we have the following model for the system: 1. µ and Σ are sampled from N(0, aI) and IWν (aI), respectively. 2 The first dimension of x is a dummy variable that always equals to 1.",
                "Figure 1: Illustration of dependencies of variables in the hierarchical model.",
                "The rating, y, for a document, x, is conditioned on the document and the user model, wm, associated with the user m. Users share information about their models through the prior, Φ = (µ, Σ). 2.",
                "For each user m, wm is sampled randomly from a Normal distribution: wm ∼ N(µ, Σ2 ) 3.",
                "For each item xm,j, ym,j is sampled randomly from a Normal distribution: ym,j ∼ N(wT mxm,j, σ2 ).",
                "Let θ = (Φ, w1, w2, ..., wM ) represent the parameters of this system that needs to be estimated.",
                "The joint likelihood for all the variables in the probabilistic model, which includes the data and the parameters, is: P(D, θ) = P(Φ) m P(wm|Φ) j P(ym,j|xm,j, wm) (1) For simplicity, we assume a, b, c, and σ are provided to the system. 4.",
                "MODEL PARAMETER LEARNING If the prior Φ is known, finding the optimal wm is straightforward: it is a simple linear regression.",
                "Therefore, we will focus on estimating Φ.",
                "The maximum a priori solution of Φ is given by ΦMAP = arg max Φ P(Φ|D) (2) = arg max Φ P(Φ, D) P(D) (3) = arg max Φ P(D|Φ)P(Φ) (4) = arg max Φ w P(D|w, Φ)P(w|Φ)P(Φ)dw (5) Finding the optimal solution for the above problem is challenging, since we need to integrate over all w = (w1, w2, ..., wM ), which are unobserved hidden variables. 4.1 EM Algorithm for Bayesian Hierarchical Linear Models In Equation 5, Φ is the parameter needs to be estimated, and the result depends on unobserved latent variables w. This kind of optimization problem is usually solved by the EM algorithm.",
                "Applying EM to the above problem, the set of user models w are the unobservable hidden variables and we have: Q = w P(w|µ, Σ2 , Dm) log P(µ, Σ2 , w, D)dw Based on the derivation of the EM formulas presented in [24], we have the following Expectation-Maximization steps for finding the optimal hyperparameters.",
                "For space considerations, we omit the derivation in this paper since it is not the focus of our work.",
                "E step: For each user m, estimate the user model distribution P(wm|Dm, Φ) = N(wm; ¯wm, Σ2 m) based on the current estimation of the prior Φ = (µ, Σ2 ). ¯wm = ((Σ2 )−1 + Sxx,m σ2 )−1 ( Sxy,m σ2 + (Σ2 )−1 µ) (6) Σ2 m = ((Σ2 )−1 + Sxx,m σ2 )−1 (7) where Sxx,m = j xm,jxT m,j Sxy,m = j xm,jym,j M step: Optimize the prior Φ = (µ, Σ2 ) based on the estimation from the last E step. µ = 1 M m ¯wm (8) Σ2 = 1 M m Σ2 m + ( ¯wm − µ)( ¯wm − µ)T (9) Many machine learning driven IR systems use a point estimate of the parameters at different stages in the system.",
                "However, we are estimating the posterior distribution of the variables at the E step.",
                "This avoids overfitting wm to a particular users data, which may be small and noisy.",
                "A detailed discussion about this subject appears in [10]. 4.2 New Algorithm: Modified EM Although the EM algorithm is widely studied and used in machine learning applications, using the above EM process to solve Bayesian hierarchical linear models in large-scale information retrieval systems is still too computationally expensive.",
                "In this section, we describe why the learning rate of the EM algorithm is slow in our application and introduce a new technique to make the learning of the Bayesian hierarchical linear model scalable.",
                "The derivation of the new learning algorithm will be based on the EM algorithm described in the previous section.",
                "First, the covariance matrices Σ2 , Σ2 m are usually too large to be computationally feasible.",
                "For simplicity, and as a common practice in IR, we do not model the correlation between features.",
                "Thus we approximate these matrices with K dimensional diagonal matrices.",
                "In the rest of the paper, we use these symbols to represent their diagonal approximations: Σ2 =     σ2 1 0 .. 0 0 σ2 2 .. 0 .. .. .. .. 0 0 .. σ2 K     Σ2 m =     σ2 m,1 0 .. 0 0 σ2 m,2 .. 0 .. .. .. .. 0 0 .. σ2 m,K     Secondly, and most importantly, the input space is very sparse and there are many dimensions that are not related to a particular user in a real IR application.",
                "For example, let us consider a movie recommendation system, with the input variable x representing a particular movie.",
                "For the jth movie that the user m has seen, let xm,j,k = 1 if the director of the movie is Jean-Pierre Jeunet (indexed by k).",
                "Here we assume that whether or not that this director directed a specific movie is represented by the kth dimension.",
                "If the user m has never seen a movie directed by Jean-Pierre Jeunet, then the corresponding dimension is always zero (xm,j,k = 0 for all j) .",
                "One major drawback of the EM algorithm is that the importance of a feature, µk, may be greatly dominated by users who have never encountered this feature (i.e. j xm,j,k = 0) at the M step (Equation 8).",
                "Assume that 100 out of 1 million users have viewed the movie directed by Jean-Pierre Jeunet, and that the viewers have rated all of his movies as excellent.",
                "Intuitively, he is a good director and the weight for him (µk) should be high.",
                "Before the EM iteration, the initial value of µ is usually set to 0.",
                "Since the other 999,900 users have not seen this movie, their corresponding weights (w1,k, w2,k, ..., wm,k..., w999900,k) for that director would be very small initially.",
                "Thus the corresponding weight of the director in the prior µk at the first M step would be very low , and the variance σm,k will be large (Equations 8 and 7).",
                "It is undesirable that users who have never seen any movie produced by the director influence the importance of the director so much.",
                "This makes the convergence of the standard EM algorithm very slow.",
                "Now lets look at whether we can improve the learning speed of the algorithm.",
                "Without a loss of generality, let us assume that the kth dimension of the input variable x is not related to a particular user m. By which we mean, xm,j,k = 0 for all j = 1, ..., Jm.",
                "It is straightforward to prove that the kth row and kth column of Sxx,m are completely filled with zeros, and that the kth dimension of Sxy,m is zeroed as well.",
                "Thus the corresponding kth dimension of the user models mean, ¯wm, should be equal to that of the prior: ¯wm,k = µk, with the corresponding covariance of σm,k = σk.",
                "At the M step, the standard EM algorithm uses the numerical solution of the distribution P(wm|Dm, Φ) estimated at E step (Equation 8 and Equation 7).",
                "However, the numerical solutions are very unreliable for ¯wm,k and σm,k when the kth dimension is not related to the mth user.",
                "A better approach is using the analytical solutions ¯wm,k = µk, and σm,k = σk for the unrelated (m, k) pairs, along with the numerical solution estimated at E step for the other (m, k) pairs.",
                "Thus we get the following new EM-like algorithm: Modified E step: For each user m, estimate the user model distribution P(wm|Dm, Φ) = N(wm; ¯wm, Σ2 m) based on the current estimation of σ , µ, Σ2 . ¯wm = ((Σ2 )−1 + Sxx,m σ2 )−1 ( Sxy,m σ2 + (Σ2 )−1 µ)(10) σ2 m,k = ((σ2 k)−1 + sxx,m,k σ2 )−1 (11) where sxx,m,k = j x2 m,j,k and sxy,m,k = j xm,j,kym,j Modified M Step Optimize the prior Φ = (µ, Σ2 ) based on the estimation from the last E step for related userfeature pairs.",
                "The M step implicitly uses the analytical solution for unrelated user-feature pairs. µk = 1 Mk m:related ¯wm,k (12) σ2 k = 1 Mk m:related σ2 m,k +( ¯wm,k − µk)( ¯wm,k − µk)T (13) where Mk is the number of users that are related to feature k We only estimate the diagonal of Σ2 m and Σ since we are using the diagonal approximation of the covariance matrices.",
                "To estimate ¯wm, we only need to calculate the numerical solutions for dimensions that are related to user m. To estimate σ2 k and µk, we only sum over users that are related to the kth feature.",
                "There are two major benefits of the new algorithm.",
                "First, because only the related (m, k) pairs are needed at the modified M step, the computational complexity in a single EM iteration is much smaller when the data is sparse, and many of (m, k) pairs are unrelated.",
                "Second, the parameters estimated at the modified M step (Equations 12 - 13) are more accurate than the standard M step described in Section 4.1 because the exact analytical solutions ¯wm,k = µk and σm,k = σk for the unrelated (m, k) pairs were used in the new algorithm instead of an approximate solution as in the standard algorithm. 5.",
                "EXPERIMENTAL METHODOLOGY 5.1 Evaluation Data Set To evaluate the proposed technique, we used the following three major data sets (Table 1): MovieLens Data: This data set was created by combining the relevance judgments from the MovieLens[9] data set with documents from the Internet Movie Database (IMDB).",
                "MovieLens allows users to rank how much he/she enjoyed a specific movie on a scale from 1 to 5.",
                "This likeability rating was used as a measurement of how relevant the document representing the corresponding movie is to the user.",
                "We considered documents with likeability scores of 4 or 5 as relevant, and documents with a score of 1 to 3 as irrelevant to the user.",
                "MovieLens provided relevance judgments on 3,057 documents from 6,040 separate users.",
                "On average, each user rated 151 movies, of these 87 were judged to be relevant.",
                "The average score for a document was 3.58.",
                "Documents representing each movie were constructed from the portion of the IMDB database that is available for public download[13].",
                "Based on this database, we created one document per movie that contained the relevant information about it (e.g. directors, actors, etc.).",
                "Table 1: Data Set Statistics.",
                "On Reuters, the number of rating for a simulated user is the number of documents relevant to the corresponding topic.",
                "Data Users Docs Ratings per User MovieLens 6,040 3,057 151 Netflix-all 480,189 17,770 208 Netflix-1000 1000 17,770 127 Reuters-C 34 100,000 3949 Reuters-E 26 100,000 1632 Reuters-G 33 100,000 2222 Reuters-M 10 100,000 6529 Netflix Data: This data set was constructed by combining documents about movies crawled from the web with a set of actual movie rental customer relevance judgments from Netflix[19].",
                "Netflix publicly provides the relevance judgments of 480,189 anonymous customers.",
                "There are around 100 million rating on a scale of 1 to 5 for 17,770 documents.",
                "Similar to MovieLens, we considered documents with likeability scores of 4 or 5 as relevant.",
                "This number was reduced to 1000 customers through random sampling.",
                "The average customer on the reduced data set provided 127 judgments, with 70 being deemed relevant.",
                "The average score for documents is 3.55.",
                "Reuters Data: This is the Reuters Corpus, Volume 1.",
                "It covers 810,000 Reuters English language news stories from August 20, 1996 to August 19, 1997.",
                "Only the first 100,000 news were used in our experiments.",
                "The Reuters corpus comes with a topic hierarchy.",
                "Each document is assigned to one of several locations on the hierarchical tree.",
                "The first level of the tree contains four topics, denoted as C, E, M, and G. For the experiments in this paper, the tree was cut at level 1 to create four smaller trees, each of which corresponds to one smaller data set: Reuters-E Reuters-C, ReutersM and Reuters-G. For each small data set, we created several profiles, one profile for each node in a sub-tree, to simulate multiple users, each with a related, yet separate definition of relevance.",
                "All the user profiles on a sub-tree are supposed to share the same prior model distribution.",
                "Since this corpus explicitly indicates only the relevant documents for a topic(user), all other documents are considered irrelevant. 5.2 Evaluation We designed the experiments to answer the following three questions: 1.",
                "Do we need to take the effort to use a Bayesian approach and learn a prior from other users? 2.",
                "Does the new algorithm work better than the standard EM algorithm for learning the Bayesian hierarchical linear model? 3.",
                "Can the new algorithm quickly learn many user models?",
                "To answer the first question, we compared the Bayesian hierarchical models with commonly used Norm-2 regularized linear regression models.",
                "In fact, the commonly used approach is equivalent to the model learned at the end of the first EM iteration.",
                "To answer the second question, we compared the proposed new algorithm with the standard EM algorithm to see whether the new learning algorithm is better.",
                "To answer the third question, we tested the efficiency of the new algorithm on the entire Netflix data set where about half a million user models need to be learned together.",
                "For the MovieLens and Netflix data sets, algorithm effectiveness was measured by mean square error, while on the Reuters data set classification error was used because it was more informative.",
                "We first evaluated the performance on each individual user, and then estimated the macro average over all users.",
                "Statistical tests (t-tests) were carried out to see whether the results are significant.",
                "For the experiments on the MovieLens and Netflix data sets, we used a random sample of 90% of each user for training, and the rest for testing.",
                "On Reuters data set, because there are too many relevant documents for each topic in the corpus, we used a random sample of 10% of each topic for training, and 10% of the remaining documents for testing.",
                "For all runs, we set (a, b, c, Σ ) = (0.1, 10, 0.1, 1) manually. 6.",
                "EXPERIMENTAL RESULTS Figure 2, Figure 3, and Figure 4 show that on all data sets, the Bayesian hierarchical modeling approach has a statistical significant improvement over the regularized linear regression model, which is equivalent to the Bayesian hierarchical models learned at the first iteration.",
                "Further analysis shows a negative correlation between the number of training data for a user and the improvement the system gets.",
                "This suggests that the borrowing information from other users has more significant improvements for users with less training data, which is as expected.",
                "However, the strength of the correlation differs over data sets, and the amount of training data is not the only characteristics that will influence the final performance.",
                "Figure 2 and Figure 3 show that the proposed new algorithm works better than the standard EM algorithm on the Netflix and MovieLens data sets.",
                "This is not surprising since the number of related feature-users pairs is much smaller than the number of unrelated feature-user pairs on these two data sets, and thus the proposed new algorithm is expected to work better.",
                "Figure 4 shows that the two algorithms work similarly on the Reuters-E data set.",
                "The accuracy of the new algorithm is similar to that of the standard EM algorithm at each iteration.",
                "The general patterns are very similar on other Reuters subsets.",
                "Further analysis shows that only 58% of the user-feature pairs are unrelated on this data set.",
                "Since the number of unrelated user-feature pairs is not extremely large, the sparseness is not a serious problem on the Reuters data set.",
                "Thus the two learning algorithms perform similarly.",
                "The results suggest that only on a corpus where the number of unrelated user-feature pairs is much larger than the number of related pairs, such as on the Netflix data set, the proposed technique will get a significant improvement over standard EM.",
                "However, the experiments also show that when the assumption does not hold, the new algorithm does not hurt performance.",
                "Although the proposed technique is faster than standard Figure 2: Performance on a Netflix subset with 1,000 users.",
                "The new algorithm is statistical significantly better than EM algorithm at iterations 2 - 10.",
                "Norm-2 regularized linear models are equivalent to the Bayesian hierarchical models learned at the first iteration, and are statistical significantly worse than the Bayesian hierarchical models. 0 2 4 6 8 10 1 1.05 1.1 1.15 1.2 1.25 1.3 1.35 1.4 Iterations MeanSquareError New Algorithm Traditional EM 1 2 3 4 5 6 7 8 9 10 0.33 0.34 0.35 0.36 0.37 0.38 0.39 Iterations ClassificationError New Algorithm Traditional EM Figure 3: Performance on a MovieLens subset with 1,000 users.",
                "The new algorithm is statistical significantly better than EM algorithm at iteration 2 to 17 (evaluated with mean square error). 1 6 11 16 21 0.5 1 1.5 2 2.5 3 3.5 Iterations MeanSquareError New Algorithm Traditional EM 1 6 11 16 21 0.35 0.4 0.45 0.5 0.55 0.6 0.65 Iterations ClassificationError New Algorithm Traditional EM Figure 4: Performance on a Reuters-E subset with 26 profiles.",
                "Performances on Reuters-C, Reuters-M, Reuters-G are similar. 1 2 3 4 5 0.011 0.0115 0.012 0.0125 0.013 0.0135 0.014 Iterations MeanSquareError New Algorithm Traditional EM 1 2 3 4 5 0.0102 0.0104 0.0106 0.0108 0.011 0.0112 0.0114 Iterations ClassificationError New Algorithm Traditional EM EM, can it really learn millions of user models quickly?",
                "Our results show that the modified EM algorithm converges quickly, and 2 - 3 modified EM iterations would result in a reliable estimation.",
                "We evaluated the algorithm on the whole Netflix data set (480,189 users, 159,836 features, and 100 million ratings) running on a single CPU PC (2GB memory, P4 3GHz).",
                "The system finished one modified EM iteration in about 4 hours.",
                "This demonstrates that the proposed technique can efficiently handle large-scale system like Netflix. 7.",
                "CONCLUSION Content-based user profile learning is an important problem and is the key to providing personal recommendations to a user, especially for recommending new items with a small number of ratings.",
                "The Bayesian hierarchical modeling approach is becoming an important user profile learning approach due to its theoretically justified ability to help one user through information transfer from the other users by way of hyperpriors.",
                "This paper examined the weakness of the popular EM based learning approach for Bayesian hierarchical linear models and proposed a better learning technique called Modified EM.",
                "We showed that the new technique is theoretically more computationally efficient than the standard EM algorithm.",
                "Evaluation on the MovieLens and Netflix data sets demonstrated the effectiveness of the new technique when the data is sparse, by which we mean the ratio of related user-feature pairs to unrelated pairs is small.",
                "Evaluation on the Reuters data set showed that the new technique performed similar to the standard EM algorithm when the sparseness condition does not hold.",
                "In general, it is better to use the new algorithm since it is as simple as standard EM, the performance is either better or similar to EM, and the computation complexity is lower at each iteration.",
                "It is worth mentioning that even if the original problem space is not sparse, sparseness can be created artificially when a recommendation system uses user-specific feature selection techniques to reduce the noise and user model complexity.",
                "The proposed technique can also be adapted to improve the learning in such a scenario.",
                "We also demonstrated that the proposed technique can learn half a million user profiles from 100 million ratings in a few hours with a single CPU.",
                "The research is important because scalability is a major concern for researchers when using the Bayesian hierarchical linear modeling approach to build a practical large scale system, even though the literature have demonstrated the effectiveness of the models in many applications.",
                "Our work is one major step on the road to make Bayesian hierarchical linear models more practical.",
                "The proposed new technique can be easily adapted to run on a cluster of machines, and thus further speed up the learning process to handle a larger scale system with hundreds of millions of users.",
                "The research has much potential to benefit people using EM algorithm on many other IR problems as well as machine learning problems.",
                "EM algorithm is a commonly used machine learning technique.",
                "It is used to find model parameters in many IR problems where the training data is very sparse.",
                "Although we are focusing on the Bayesian hierarchical linear models for recommendation and filtering, the new idea of using analytical solution instead of numerical solution for unrelated user-feature pairs at the M step could be adapted to many other problems. 8.",
                "ACKNOWLEDGMENTS We thank Wei Xu, David Lewis and anonymous reviewers for valuable feedback on the work described in this paper.",
                "Part of the work was supported by Yahoo, Google, the Petascale Data Storage Institute and the Institute for Scalable Scientific Data Management.",
                "Any opinions, findings, conclusions, or recommendations expressed in this material are those of the authors, and do not necessarily reflect those of the sponsors. 9.",
                "REFERENCES [1] C. Basu, H. Hirsh, and W. Cohen.",
                "Recommendation as classification: Using social and content-based information in recommendation.",
                "In Proceedings of the Fifteenth National Conference on Artificial Intelligence, 1998. [2] J. S. Breese, D. Heckerman, and C. Kadie.",
                "Empirical analysis of predictive algorithms for collaborative filtering.",
                "Technical report, Microsoft Research, One Microsoft Way, Redmond, WA 98052, 1998. [3] J. Callan.",
                "Document filtering with inference networks.",
                "In Proceedings of the Nineteenth Annual International ACM SIGIR Conference on Research and Development in Information Retrieval, pages 262-269, 1996. [4] N. Cancedda, N. Cesa-Bianchi, A. Conconi, C. Gentile, C. Goutte, T. Graepel, Y. Li, J. M. Renders, J. S. Taylor, and A. Vinokourov.",
                "Kernel method for document filtering.",
                "In The Eleventh Text REtrieval Conference (TREC11).",
                "National Institute of Standards and Technology, special publication 500-249, 2003. [5] C. Chelba and A. Acero.",
                "Adaptation of maximum entropy capitalizer: Little data can help a lot.",
                "In D. Lin and D. Wu, editors, Proceedings of EMNLP 2004, pages 285-292, Barcelona, Spain, July 2004.",
                "Association for Computational Linguistics. [6] B. Croft and J. Lafferty, editors.",
                "Language Modeling for Information Retrieval.",
                "Kluwer, 2002. [7] A. Dayanik, D. D. Lewis, D. Madigan, V. Menkov, and A. Genkin.",
                "Constructing informative prior distributions from domain knowledge in text classification.",
                "In SIGIR 06: Proceedings of the 29th annual international ACM SIGIR conference on Research and development in information retrieval, pages 493-500, New York, NY, USA, 2006.",
                "ACM Press. [8] J. Delgado and N. Ishii.",
                "Memory-based weightedmajority prediction for recommender systems.",
                "In ACM SIGIR99 Workshop on Recommender Systems, 1999. [9] GroupLens.",
                "Movielens. http://www.grouplens.org/taxonomy/term/14, 2006. [10] D. Heckerman.",
                "A tutorial on learning with bayesian networks.",
                "In M. Jordan, editor, Learning in Graphical Models.",
                "Kluwer Academic, 1998. [11] J. L. Herlocker, J.",
                "A. Konstan, A. Borchers, and J. Riedl.",
                "An algorithmic framework for performing collaborative filtering.",
                "In SIGIR 99: Proceedings of the 22nd annual international ACM SIGIR conference on Research and development in information retrieval, pages 230-237, New York, NY, USA, 1999.",
                "ACM Press. [12] T. Hofmann and J. Puzicha.",
                "Latent class models for collaborative filtering.",
                "In IJCAI 99: Proceedings of the Sixteenth International Joint Conference on Artificial Intelligence, pages 688-693, San Francisco, CA, USA, 1999.",
                "Morgan Kaufmann Publishers Inc. [13] I. M. D. (IMDB).",
                "Internet movie database. http://www.imdb.com/interfaces/, 2006. [14] R. Jin, J. Y. Chai, and L. Si.",
                "An automatic weighting scheme for collaborative filtering.",
                "In SIGIR 04: Proceedings of the 27th annual international ACM SIGIR conference on Research and development in information retrieval, pages 337-344, New York, NY, USA, 2004.",
                "ACM Press. [15] J.",
                "A. Konstan, B. N. Miller, D. Maltz, J. L. Herlocker, L. R. Gordon, and J. Riedl.",
                "GroupLens: Applying collaborative filtering to Usenet news.",
                "Communications of the ACM, 40(3):77-87, 1997. [16] D. Lewis.",
                "Applying support vector machines to the TREC-2001 batch filtering and routing tasks.",
                "In Proceedings of the Eleventh Text REtrieval Conference (TREC-11), 2002. [17] B. Liu, X. Li, W. S. Lee, , and P. Yu.",
                "Text classification by labeling words.",
                "In Proceedings of The Nineteenth National Conference on Artificial Intelligence (AAAI-2004), July 25-29, 2004. [18] P. Melville, R. J. Mooney, and R. Nagarajan.",
                "Content-boosted collaborative filtering for improved recommendations.",
                "In Proceedings of the Eighteenth National Conference on Artificial Intelligence (AAAI-2002), Edmonton, Canada, 2002. [19] Netflix.",
                "Netflix prize. http://www.netflixprize.com (visited on Nov. 30, 2006), 2006. [20] S. Robertson and K. Sparck-Jones.",
                "Relevance weighting of search terms.",
                "In Journal of the American Society for Information Science, volume 27, pages 129-146, 1976. [21] J. Wang, A. P. de Vries, and M. J. T. Reinders.",
                "Unifying user-based and item-based collaborative filtering approaches by similarity fusion.",
                "In SIGIR 06: Proceedings of the 29th annual international ACM SIGIR conference on Research and development in information retrieval, pages 501-508, New York, NY, USA, 2006.",
                "ACM Press. [22] X. Wu and R. K. Srihari.",
                "Incorporating prior knowledge with weighted margin support vector machines.",
                "In Proc.",
                "ACM Knowledge Discovery Data Mining Conf. (ACM SIGKDD 2004), Aug. 2004. [23] Y. Yang, S. Yoo, J. Zhang, and B. Kisiel.",
                "Robustness of adaptive filtering methods in a cross-benchmark evaluation.",
                "In Proceedings of the 28th Annual International ACM SIGIR Conference on Research and Development in Information Retrieval, 2005. [24] K. Yu, V. Tresp, and A. Schwaighofer.",
                "Learning gaussian processes from multiple tasks.",
                "In ICML 05: Proceedings of the 22nd international conference on Machine learning, pages 1012-1019, New York, NY, USA, 2005.",
                "ACM Press. [25] K. Yu, V. Tresp, and S. Yu.",
                "A nonparametric hierarchical bayesian framework for <br>information filter</br>ing.",
                "In SIGIR 04: Proceedings of the 27th annual international ACM SIGIR conference on Research and development in information retrieval, pages 353-360.",
                "ACM Press, 2004. [26] X. Zhu.",
                "Semi-supervised learning literature survey.",
                "Technical report, University of Wisconsin - Madison, December 9, 2006. [27] P. Zigoris and Y. Zhang.",
                "Bayesian adaptive user profiling with explicit & implicit feedback.",
                "In Conference on Information and Knowledge Mangement 2006, 2006."
            ],
            "original_annotated_samples": [],
            "translated_annotated_samples": [
                "Un marco bayesiano jerárquico no paramétrico para el \"filtro de información\"."
            ],
            "translated_text": "",
            "candidates": [
                "filtro de información",
                "filtro de información"
            ],
            "error": []
        },
        "personalization": {
            "translated_key": "personalización",
            "is_in_text": true,
            "original_annotated_sentences": [
                "Efficient Bayesian Hierarchical User Modeling for Recommendation Systems Yi Zhang, Jonathan Koren School of Engineering University of California Santa Cruz Santa Cruz, CA, USA {yiz, jonathan}@soe.ucsc.edu ABSTRACT A content-based personalized recommendation system learns user specific profiles from user feedback so that it can deliver information tailored to each individual users interest.",
                "A system serving millions of users can learn a better user profile for a new user, or a user with little feedback, by borrowing information from other users through the use of a Bayesian hierarchical model.",
                "Learning the model parameters to optimize the joint data likelihood from millions of users is very computationally expensive.",
                "The commonly used EM algorithm converges very slowly due to the sparseness of the data in IR applications.",
                "This paper proposes a new fast learning technique to learn a large number of individual user profiles.",
                "The efficacy and efficiency of the proposed algorithm are justified by theory and demonstrated on actual user data from Netflix and MovieLens.",
                "Categories and Subject Descriptors: B.3.3 [Information Search and Retrieval]: Information filtering General Terms: Algorithms 1.",
                "INTRODUCTION <br>personalization</br> is the future of the Web, and it has achieved great success in industrial applications.",
                "For example, online stores, such as Amazon and Netflix, provide customized recommendations for additional products or services based on a users history.",
                "Recent offerings such as My MSN, My Yahoo!, My Google, and Google News have attracted much attention due to their potential ability to infer a users interests from his/her history.",
                "One major <br>personalization</br> topic studied in the information retrieval community is content-based personal recommendation systems1 .",
                "These systems learn user-specific profiles from user feedback so that they can recommend information tailored to each individual users interest without requiring the user to make an explicit query.",
                "Learning the user profiles is the core problem for these systems.",
                "A user profile is usually a classifier that can identify whether a document is relevant to the user or not, or a regression model that tells how relevant a document is to the user.",
                "One major challenge of building a recommendation or <br>personalization</br> system is that the profile learned for a particular user is usually of low quality when the amount of data from that particular user is small.",
                "This is known as the cold start problem.",
                "This means that any new user must endure poor initial performance until sufficient feedback from that user is provided to learn a reliable user profile.",
                "There has been much research on improving classification accuracy when the amount of labeled training data is small.",
                "The semi-supervised learning approach combines unlabeled and labeled data together to achieve this goal [26].",
                "Another approach is using domain knowledge.",
                "Researchers have modified different learning algorithms, such as Na¨ıveBayes [17], logistic regression [7], and SVMs [22], to integrate domain knowledge into a text classifier.",
                "The third approach is borrowing training data from other resources [5][7].",
                "The effectiveness of these different approaches is mixed, due to how well the underlying model assumption fits the data.",
                "One well-received approach to improve recommendation system performance for a particular user is borrowing information from other users through a Bayesian hierarchical modeling approach.",
                "Several researchers have demonstrated that this approach effectively trades off between shared and user-specific information, thus alleviating poor initial performance for each user[27][25].",
                "In order to learn a Bayesian hierarchical model, the system usually tries to find the most likely model parameters for the given data.",
                "A mature recommendation system usually works for millions of users.",
                "It is well known that learning the optimal parameters of a Bayesian hierarchical model is computationally expensive when there are thousands or millions of users.",
                "The EM algorithm is a commonly used technique for parameter learning due to its simplicity and convergence guarantee.",
                "However, a content based recommendation system often handles documents in a very high dimensional space, in which each document is represented by a very sparse vector.",
                "With careful analysis of the EM algorithm in this scenario (Section 4), we find that the EM tering, or item-based collaborative filtering.",
                "In this paper, the words filtering and recommendation are used interchangeably. algorithm converges very slowly due to the sparseness of the input variables.",
                "We also find that updating the model parameter at each EM iteration is also expensive with computational complexity of O(MK), where M is the number of users and K is the number of dimensions.",
                "This paper modifies the standard EM algorithm to create an improved learning algorithm, which we call the Modified EM algorithm.",
                "The basic idea is that instead of calculating the numerical solution for all the user profile parameters, we derive the analytical solution of the parameters for some feature dimensions, and at the M step use the analytical solution instead of the numerical solution estimated at E step for those parameters.",
                "This greatly reduces the computation at a single EM iteration, and also has the benefit of increasing the convergence speed of the learning algorithm.",
                "The proposed technique is not only well supported by theory, but also by experimental results.",
                "The organization of the remaining parts of this paper is as follows: Section 3 describes the Bayesian hierarchical linear regression modeling framework used for content-based recommendations.",
                "Section 4 describes how to learn the model parameters using the standard EM algorithm, along with using the new technique proposed in this paper.",
                "The experimental setting and results used to validate the proposed learning technique are reported in Sections 5 and 6.",
                "Section 7 summarizes and offers concluding remarks. 2.",
                "RELATED WORK Providing personalized recommendations to users has been identified as a very important problem in the IR community since the 1970s.",
                "The approaches that have been used to solve this problem can be roughly classified into two major categories: content based filtering versus collaborative filtering.",
                "Content-based filtering studies the scenario where a recommendation system monitors a document stream and pushes documents that match a user profile to the corresponding user.",
                "The user may read the delivered documents and provide explicit relevance feedback, which the filtering system then uses to update the users profile using relevance feedback retrieval models (e.g.",
                "Boolean models, vector space models, traditional probabilistic models [20] , inference networks [3] and language models [6]) or machine learning algorithms (e.g.",
                "Support Vector Machines (SVM), K nearest neighbors (K-NN) clustering, neural networks, logistic regression, or Winnow [16] [4] [23]).",
                "Collaborative filtering goes beyond merely using document content to recommend items to a user by leveraging information from other users with similar tastes and preferences in the past.",
                "Memorybased heuristics and model based approaches have been used in collaborative filtering task [15] [8] [2] [14] [12] [11].",
                "This paper contributes to the content-based recommendation research by improving the efficiency and effectiveness of Bayesian hierarchical linear models, which have a strong theoretical basis and good empirical performance on recommendation tasks[27][25].",
                "This paper does not intend to compare content-based filtering with collaborative filtering or claim which one is a better.",
                "We think each complements the other, and that content-based filtering is extremely useful for handling new documents/items with little or no user feedback.",
                "Similar to some other researchers[18][1][21], we found that a recommendation system will be more effective when both techniques are combined.",
                "However, this is beyond the scope of this paper and thus not discussed here. 3.",
                "BAYESIAN HIERARCHICAL LINEAR REGRESSION Assume there are M users in the system.",
                "The task of the system is to recommend documents that are relevant to each user.",
                "For each user, the system learns a user model from the users history.",
                "In the rest of this paper, we will use the following notations to represent the variables in the system. m = 1, 2, ..., M: The index for each individual user.",
                "M is the total number of users. wm: The user model parameter associated with user m. wm is a K dimensional vector. j = 1, 2, ..., Jm: The index for a set of data for user m. Jm is the number of training data for user m. Dm = {(xm,j, ym,j)}: A set of data associated with user m. xm,j is a K dimensional vector that represents the mth users jth training document.2 ym,j is a scalar that represents the label of document xm,j. k = 1, 2, ..., K: The dimensional index of input variable x.",
                "The Bayesian hierarchical modeling approach has been widely used in real-world information retrieval applications.",
                "Generalized Bayesian hierarchical linear models, one of the simplest Bayesian hierarchical models, are commonly used and have achieved good performance on collaborative filtering [25] and content-based adaptive filtering [27] tasks.",
                "Figure 1 shows the graphical representation of a Bayesian hierarchical model.",
                "In this graph, each user model is represented by a random vector wm.",
                "We assume a user model is sampled randomly from a prior distribution P(w|Φ).",
                "The system can predict the user label y of a document x given an estimation of wm (or wms distribution) using a function y = f(x, w).",
                "The model is called generalized Bayesian hierarchical linear model when y = f(wT x) is any generalized linear model such as logistic regression, SVM, and linear regression.",
                "To reliably estimate the user model wm, the system can borrow information from other users through the prior Φ = (µ, Σ).",
                "Now we look at one commonly used model where y = wT x + , where ∼ N(0, σ2 ) is a random noise [25][27].",
                "Assume that each user model wm is an independent draw from a population distribution P(w|Φ), which is governed by some unknown hyperparameter Φ.",
                "Let the prior distribution of user model w be a Gaussian distribution with parameter Φ = (µ, Σ), which is the commonly used prior for linear models. µ = (µ1, µ2, ..., µK ) is a K dimensional vector that represents the mean of the Gaussian distribution, and Σ is the covariance matrix of the Gaussian.",
                "Usually, a Normal distribution N(0, aI) and an Inverse Wishart distribution P(Σ) ∝ |Σ|− 1 2 b exp(−1 2 ctr(Σ−1 )) are used as hyperprior to model the prior distribution of µ and Σ respectively.",
                "I is the K dimensional identity matrix, and a, b, and c are real numbers.",
                "With these settings, we have the following model for the system: 1. µ and Σ are sampled from N(0, aI) and IWν (aI), respectively. 2 The first dimension of x is a dummy variable that always equals to 1.",
                "Figure 1: Illustration of dependencies of variables in the hierarchical model.",
                "The rating, y, for a document, x, is conditioned on the document and the user model, wm, associated with the user m. Users share information about their models through the prior, Φ = (µ, Σ). 2.",
                "For each user m, wm is sampled randomly from a Normal distribution: wm ∼ N(µ, Σ2 ) 3.",
                "For each item xm,j, ym,j is sampled randomly from a Normal distribution: ym,j ∼ N(wT mxm,j, σ2 ).",
                "Let θ = (Φ, w1, w2, ..., wM ) represent the parameters of this system that needs to be estimated.",
                "The joint likelihood for all the variables in the probabilistic model, which includes the data and the parameters, is: P(D, θ) = P(Φ) m P(wm|Φ) j P(ym,j|xm,j, wm) (1) For simplicity, we assume a, b, c, and σ are provided to the system. 4.",
                "MODEL PARAMETER LEARNING If the prior Φ is known, finding the optimal wm is straightforward: it is a simple linear regression.",
                "Therefore, we will focus on estimating Φ.",
                "The maximum a priori solution of Φ is given by ΦMAP = arg max Φ P(Φ|D) (2) = arg max Φ P(Φ, D) P(D) (3) = arg max Φ P(D|Φ)P(Φ) (4) = arg max Φ w P(D|w, Φ)P(w|Φ)P(Φ)dw (5) Finding the optimal solution for the above problem is challenging, since we need to integrate over all w = (w1, w2, ..., wM ), which are unobserved hidden variables. 4.1 EM Algorithm for Bayesian Hierarchical Linear Models In Equation 5, Φ is the parameter needs to be estimated, and the result depends on unobserved latent variables w. This kind of optimization problem is usually solved by the EM algorithm.",
                "Applying EM to the above problem, the set of user models w are the unobservable hidden variables and we have: Q = w P(w|µ, Σ2 , Dm) log P(µ, Σ2 , w, D)dw Based on the derivation of the EM formulas presented in [24], we have the following Expectation-Maximization steps for finding the optimal hyperparameters.",
                "For space considerations, we omit the derivation in this paper since it is not the focus of our work.",
                "E step: For each user m, estimate the user model distribution P(wm|Dm, Φ) = N(wm; ¯wm, Σ2 m) based on the current estimation of the prior Φ = (µ, Σ2 ). ¯wm = ((Σ2 )−1 + Sxx,m σ2 )−1 ( Sxy,m σ2 + (Σ2 )−1 µ) (6) Σ2 m = ((Σ2 )−1 + Sxx,m σ2 )−1 (7) where Sxx,m = j xm,jxT m,j Sxy,m = j xm,jym,j M step: Optimize the prior Φ = (µ, Σ2 ) based on the estimation from the last E step. µ = 1 M m ¯wm (8) Σ2 = 1 M m Σ2 m + ( ¯wm − µ)( ¯wm − µ)T (9) Many machine learning driven IR systems use a point estimate of the parameters at different stages in the system.",
                "However, we are estimating the posterior distribution of the variables at the E step.",
                "This avoids overfitting wm to a particular users data, which may be small and noisy.",
                "A detailed discussion about this subject appears in [10]. 4.2 New Algorithm: Modified EM Although the EM algorithm is widely studied and used in machine learning applications, using the above EM process to solve Bayesian hierarchical linear models in large-scale information retrieval systems is still too computationally expensive.",
                "In this section, we describe why the learning rate of the EM algorithm is slow in our application and introduce a new technique to make the learning of the Bayesian hierarchical linear model scalable.",
                "The derivation of the new learning algorithm will be based on the EM algorithm described in the previous section.",
                "First, the covariance matrices Σ2 , Σ2 m are usually too large to be computationally feasible.",
                "For simplicity, and as a common practice in IR, we do not model the correlation between features.",
                "Thus we approximate these matrices with K dimensional diagonal matrices.",
                "In the rest of the paper, we use these symbols to represent their diagonal approximations: Σ2 =     σ2 1 0 .. 0 0 σ2 2 .. 0 .. .. .. .. 0 0 .. σ2 K     Σ2 m =     σ2 m,1 0 .. 0 0 σ2 m,2 .. 0 .. .. .. .. 0 0 .. σ2 m,K     Secondly, and most importantly, the input space is very sparse and there are many dimensions that are not related to a particular user in a real IR application.",
                "For example, let us consider a movie recommendation system, with the input variable x representing a particular movie.",
                "For the jth movie that the user m has seen, let xm,j,k = 1 if the director of the movie is Jean-Pierre Jeunet (indexed by k).",
                "Here we assume that whether or not that this director directed a specific movie is represented by the kth dimension.",
                "If the user m has never seen a movie directed by Jean-Pierre Jeunet, then the corresponding dimension is always zero (xm,j,k = 0 for all j) .",
                "One major drawback of the EM algorithm is that the importance of a feature, µk, may be greatly dominated by users who have never encountered this feature (i.e. j xm,j,k = 0) at the M step (Equation 8).",
                "Assume that 100 out of 1 million users have viewed the movie directed by Jean-Pierre Jeunet, and that the viewers have rated all of his movies as excellent.",
                "Intuitively, he is a good director and the weight for him (µk) should be high.",
                "Before the EM iteration, the initial value of µ is usually set to 0.",
                "Since the other 999,900 users have not seen this movie, their corresponding weights (w1,k, w2,k, ..., wm,k..., w999900,k) for that director would be very small initially.",
                "Thus the corresponding weight of the director in the prior µk at the first M step would be very low , and the variance σm,k will be large (Equations 8 and 7).",
                "It is undesirable that users who have never seen any movie produced by the director influence the importance of the director so much.",
                "This makes the convergence of the standard EM algorithm very slow.",
                "Now lets look at whether we can improve the learning speed of the algorithm.",
                "Without a loss of generality, let us assume that the kth dimension of the input variable x is not related to a particular user m. By which we mean, xm,j,k = 0 for all j = 1, ..., Jm.",
                "It is straightforward to prove that the kth row and kth column of Sxx,m are completely filled with zeros, and that the kth dimension of Sxy,m is zeroed as well.",
                "Thus the corresponding kth dimension of the user models mean, ¯wm, should be equal to that of the prior: ¯wm,k = µk, with the corresponding covariance of σm,k = σk.",
                "At the M step, the standard EM algorithm uses the numerical solution of the distribution P(wm|Dm, Φ) estimated at E step (Equation 8 and Equation 7).",
                "However, the numerical solutions are very unreliable for ¯wm,k and σm,k when the kth dimension is not related to the mth user.",
                "A better approach is using the analytical solutions ¯wm,k = µk, and σm,k = σk for the unrelated (m, k) pairs, along with the numerical solution estimated at E step for the other (m, k) pairs.",
                "Thus we get the following new EM-like algorithm: Modified E step: For each user m, estimate the user model distribution P(wm|Dm, Φ) = N(wm; ¯wm, Σ2 m) based on the current estimation of σ , µ, Σ2 . ¯wm = ((Σ2 )−1 + Sxx,m σ2 )−1 ( Sxy,m σ2 + (Σ2 )−1 µ)(10) σ2 m,k = ((σ2 k)−1 + sxx,m,k σ2 )−1 (11) where sxx,m,k = j x2 m,j,k and sxy,m,k = j xm,j,kym,j Modified M Step Optimize the prior Φ = (µ, Σ2 ) based on the estimation from the last E step for related userfeature pairs.",
                "The M step implicitly uses the analytical solution for unrelated user-feature pairs. µk = 1 Mk m:related ¯wm,k (12) σ2 k = 1 Mk m:related σ2 m,k +( ¯wm,k − µk)( ¯wm,k − µk)T (13) where Mk is the number of users that are related to feature k We only estimate the diagonal of Σ2 m and Σ since we are using the diagonal approximation of the covariance matrices.",
                "To estimate ¯wm, we only need to calculate the numerical solutions for dimensions that are related to user m. To estimate σ2 k and µk, we only sum over users that are related to the kth feature.",
                "There are two major benefits of the new algorithm.",
                "First, because only the related (m, k) pairs are needed at the modified M step, the computational complexity in a single EM iteration is much smaller when the data is sparse, and many of (m, k) pairs are unrelated.",
                "Second, the parameters estimated at the modified M step (Equations 12 - 13) are more accurate than the standard M step described in Section 4.1 because the exact analytical solutions ¯wm,k = µk and σm,k = σk for the unrelated (m, k) pairs were used in the new algorithm instead of an approximate solution as in the standard algorithm. 5.",
                "EXPERIMENTAL METHODOLOGY 5.1 Evaluation Data Set To evaluate the proposed technique, we used the following three major data sets (Table 1): MovieLens Data: This data set was created by combining the relevance judgments from the MovieLens[9] data set with documents from the Internet Movie Database (IMDB).",
                "MovieLens allows users to rank how much he/she enjoyed a specific movie on a scale from 1 to 5.",
                "This likeability rating was used as a measurement of how relevant the document representing the corresponding movie is to the user.",
                "We considered documents with likeability scores of 4 or 5 as relevant, and documents with a score of 1 to 3 as irrelevant to the user.",
                "MovieLens provided relevance judgments on 3,057 documents from 6,040 separate users.",
                "On average, each user rated 151 movies, of these 87 were judged to be relevant.",
                "The average score for a document was 3.58.",
                "Documents representing each movie were constructed from the portion of the IMDB database that is available for public download[13].",
                "Based on this database, we created one document per movie that contained the relevant information about it (e.g. directors, actors, etc.).",
                "Table 1: Data Set Statistics.",
                "On Reuters, the number of rating for a simulated user is the number of documents relevant to the corresponding topic.",
                "Data Users Docs Ratings per User MovieLens 6,040 3,057 151 Netflix-all 480,189 17,770 208 Netflix-1000 1000 17,770 127 Reuters-C 34 100,000 3949 Reuters-E 26 100,000 1632 Reuters-G 33 100,000 2222 Reuters-M 10 100,000 6529 Netflix Data: This data set was constructed by combining documents about movies crawled from the web with a set of actual movie rental customer relevance judgments from Netflix[19].",
                "Netflix publicly provides the relevance judgments of 480,189 anonymous customers.",
                "There are around 100 million rating on a scale of 1 to 5 for 17,770 documents.",
                "Similar to MovieLens, we considered documents with likeability scores of 4 or 5 as relevant.",
                "This number was reduced to 1000 customers through random sampling.",
                "The average customer on the reduced data set provided 127 judgments, with 70 being deemed relevant.",
                "The average score for documents is 3.55.",
                "Reuters Data: This is the Reuters Corpus, Volume 1.",
                "It covers 810,000 Reuters English language news stories from August 20, 1996 to August 19, 1997.",
                "Only the first 100,000 news were used in our experiments.",
                "The Reuters corpus comes with a topic hierarchy.",
                "Each document is assigned to one of several locations on the hierarchical tree.",
                "The first level of the tree contains four topics, denoted as C, E, M, and G. For the experiments in this paper, the tree was cut at level 1 to create four smaller trees, each of which corresponds to one smaller data set: Reuters-E Reuters-C, ReutersM and Reuters-G. For each small data set, we created several profiles, one profile for each node in a sub-tree, to simulate multiple users, each with a related, yet separate definition of relevance.",
                "All the user profiles on a sub-tree are supposed to share the same prior model distribution.",
                "Since this corpus explicitly indicates only the relevant documents for a topic(user), all other documents are considered irrelevant. 5.2 Evaluation We designed the experiments to answer the following three questions: 1.",
                "Do we need to take the effort to use a Bayesian approach and learn a prior from other users? 2.",
                "Does the new algorithm work better than the standard EM algorithm for learning the Bayesian hierarchical linear model? 3.",
                "Can the new algorithm quickly learn many user models?",
                "To answer the first question, we compared the Bayesian hierarchical models with commonly used Norm-2 regularized linear regression models.",
                "In fact, the commonly used approach is equivalent to the model learned at the end of the first EM iteration.",
                "To answer the second question, we compared the proposed new algorithm with the standard EM algorithm to see whether the new learning algorithm is better.",
                "To answer the third question, we tested the efficiency of the new algorithm on the entire Netflix data set where about half a million user models need to be learned together.",
                "For the MovieLens and Netflix data sets, algorithm effectiveness was measured by mean square error, while on the Reuters data set classification error was used because it was more informative.",
                "We first evaluated the performance on each individual user, and then estimated the macro average over all users.",
                "Statistical tests (t-tests) were carried out to see whether the results are significant.",
                "For the experiments on the MovieLens and Netflix data sets, we used a random sample of 90% of each user for training, and the rest for testing.",
                "On Reuters data set, because there are too many relevant documents for each topic in the corpus, we used a random sample of 10% of each topic for training, and 10% of the remaining documents for testing.",
                "For all runs, we set (a, b, c, Σ ) = (0.1, 10, 0.1, 1) manually. 6.",
                "EXPERIMENTAL RESULTS Figure 2, Figure 3, and Figure 4 show that on all data sets, the Bayesian hierarchical modeling approach has a statistical significant improvement over the regularized linear regression model, which is equivalent to the Bayesian hierarchical models learned at the first iteration.",
                "Further analysis shows a negative correlation between the number of training data for a user and the improvement the system gets.",
                "This suggests that the borrowing information from other users has more significant improvements for users with less training data, which is as expected.",
                "However, the strength of the correlation differs over data sets, and the amount of training data is not the only characteristics that will influence the final performance.",
                "Figure 2 and Figure 3 show that the proposed new algorithm works better than the standard EM algorithm on the Netflix and MovieLens data sets.",
                "This is not surprising since the number of related feature-users pairs is much smaller than the number of unrelated feature-user pairs on these two data sets, and thus the proposed new algorithm is expected to work better.",
                "Figure 4 shows that the two algorithms work similarly on the Reuters-E data set.",
                "The accuracy of the new algorithm is similar to that of the standard EM algorithm at each iteration.",
                "The general patterns are very similar on other Reuters subsets.",
                "Further analysis shows that only 58% of the user-feature pairs are unrelated on this data set.",
                "Since the number of unrelated user-feature pairs is not extremely large, the sparseness is not a serious problem on the Reuters data set.",
                "Thus the two learning algorithms perform similarly.",
                "The results suggest that only on a corpus where the number of unrelated user-feature pairs is much larger than the number of related pairs, such as on the Netflix data set, the proposed technique will get a significant improvement over standard EM.",
                "However, the experiments also show that when the assumption does not hold, the new algorithm does not hurt performance.",
                "Although the proposed technique is faster than standard Figure 2: Performance on a Netflix subset with 1,000 users.",
                "The new algorithm is statistical significantly better than EM algorithm at iterations 2 - 10.",
                "Norm-2 regularized linear models are equivalent to the Bayesian hierarchical models learned at the first iteration, and are statistical significantly worse than the Bayesian hierarchical models. 0 2 4 6 8 10 1 1.05 1.1 1.15 1.2 1.25 1.3 1.35 1.4 Iterations MeanSquareError New Algorithm Traditional EM 1 2 3 4 5 6 7 8 9 10 0.33 0.34 0.35 0.36 0.37 0.38 0.39 Iterations ClassificationError New Algorithm Traditional EM Figure 3: Performance on a MovieLens subset with 1,000 users.",
                "The new algorithm is statistical significantly better than EM algorithm at iteration 2 to 17 (evaluated with mean square error). 1 6 11 16 21 0.5 1 1.5 2 2.5 3 3.5 Iterations MeanSquareError New Algorithm Traditional EM 1 6 11 16 21 0.35 0.4 0.45 0.5 0.55 0.6 0.65 Iterations ClassificationError New Algorithm Traditional EM Figure 4: Performance on a Reuters-E subset with 26 profiles.",
                "Performances on Reuters-C, Reuters-M, Reuters-G are similar. 1 2 3 4 5 0.011 0.0115 0.012 0.0125 0.013 0.0135 0.014 Iterations MeanSquareError New Algorithm Traditional EM 1 2 3 4 5 0.0102 0.0104 0.0106 0.0108 0.011 0.0112 0.0114 Iterations ClassificationError New Algorithm Traditional EM EM, can it really learn millions of user models quickly?",
                "Our results show that the modified EM algorithm converges quickly, and 2 - 3 modified EM iterations would result in a reliable estimation.",
                "We evaluated the algorithm on the whole Netflix data set (480,189 users, 159,836 features, and 100 million ratings) running on a single CPU PC (2GB memory, P4 3GHz).",
                "The system finished one modified EM iteration in about 4 hours.",
                "This demonstrates that the proposed technique can efficiently handle large-scale system like Netflix. 7.",
                "CONCLUSION Content-based user profile learning is an important problem and is the key to providing personal recommendations to a user, especially for recommending new items with a small number of ratings.",
                "The Bayesian hierarchical modeling approach is becoming an important user profile learning approach due to its theoretically justified ability to help one user through information transfer from the other users by way of hyperpriors.",
                "This paper examined the weakness of the popular EM based learning approach for Bayesian hierarchical linear models and proposed a better learning technique called Modified EM.",
                "We showed that the new technique is theoretically more computationally efficient than the standard EM algorithm.",
                "Evaluation on the MovieLens and Netflix data sets demonstrated the effectiveness of the new technique when the data is sparse, by which we mean the ratio of related user-feature pairs to unrelated pairs is small.",
                "Evaluation on the Reuters data set showed that the new technique performed similar to the standard EM algorithm when the sparseness condition does not hold.",
                "In general, it is better to use the new algorithm since it is as simple as standard EM, the performance is either better or similar to EM, and the computation complexity is lower at each iteration.",
                "It is worth mentioning that even if the original problem space is not sparse, sparseness can be created artificially when a recommendation system uses user-specific feature selection techniques to reduce the noise and user model complexity.",
                "The proposed technique can also be adapted to improve the learning in such a scenario.",
                "We also demonstrated that the proposed technique can learn half a million user profiles from 100 million ratings in a few hours with a single CPU.",
                "The research is important because scalability is a major concern for researchers when using the Bayesian hierarchical linear modeling approach to build a practical large scale system, even though the literature have demonstrated the effectiveness of the models in many applications.",
                "Our work is one major step on the road to make Bayesian hierarchical linear models more practical.",
                "The proposed new technique can be easily adapted to run on a cluster of machines, and thus further speed up the learning process to handle a larger scale system with hundreds of millions of users.",
                "The research has much potential to benefit people using EM algorithm on many other IR problems as well as machine learning problems.",
                "EM algorithm is a commonly used machine learning technique.",
                "It is used to find model parameters in many IR problems where the training data is very sparse.",
                "Although we are focusing on the Bayesian hierarchical linear models for recommendation and filtering, the new idea of using analytical solution instead of numerical solution for unrelated user-feature pairs at the M step could be adapted to many other problems. 8.",
                "ACKNOWLEDGMENTS We thank Wei Xu, David Lewis and anonymous reviewers for valuable feedback on the work described in this paper.",
                "Part of the work was supported by Yahoo, Google, the Petascale Data Storage Institute and the Institute for Scalable Scientific Data Management.",
                "Any opinions, findings, conclusions, or recommendations expressed in this material are those of the authors, and do not necessarily reflect those of the sponsors. 9.",
                "REFERENCES [1] C. Basu, H. Hirsh, and W. Cohen.",
                "Recommendation as classification: Using social and content-based information in recommendation.",
                "In Proceedings of the Fifteenth National Conference on Artificial Intelligence, 1998. [2] J. S. Breese, D. Heckerman, and C. Kadie.",
                "Empirical analysis of predictive algorithms for collaborative filtering.",
                "Technical report, Microsoft Research, One Microsoft Way, Redmond, WA 98052, 1998. [3] J. Callan.",
                "Document filtering with inference networks.",
                "In Proceedings of the Nineteenth Annual International ACM SIGIR Conference on Research and Development in Information Retrieval, pages 262-269, 1996. [4] N. Cancedda, N. Cesa-Bianchi, A. Conconi, C. Gentile, C. Goutte, T. Graepel, Y. Li, J. M. Renders, J. S. Taylor, and A. Vinokourov.",
                "Kernel method for document filtering.",
                "In The Eleventh Text REtrieval Conference (TREC11).",
                "National Institute of Standards and Technology, special publication 500-249, 2003. [5] C. Chelba and A. Acero.",
                "Adaptation of maximum entropy capitalizer: Little data can help a lot.",
                "In D. Lin and D. Wu, editors, Proceedings of EMNLP 2004, pages 285-292, Barcelona, Spain, July 2004.",
                "Association for Computational Linguistics. [6] B. Croft and J. Lafferty, editors.",
                "Language Modeling for Information Retrieval.",
                "Kluwer, 2002. [7] A. Dayanik, D. D. Lewis, D. Madigan, V. Menkov, and A. Genkin.",
                "Constructing informative prior distributions from domain knowledge in text classification.",
                "In SIGIR 06: Proceedings of the 29th annual international ACM SIGIR conference on Research and development in information retrieval, pages 493-500, New York, NY, USA, 2006.",
                "ACM Press. [8] J. Delgado and N. Ishii.",
                "Memory-based weightedmajority prediction for recommender systems.",
                "In ACM SIGIR99 Workshop on Recommender Systems, 1999. [9] GroupLens.",
                "Movielens. http://www.grouplens.org/taxonomy/term/14, 2006. [10] D. Heckerman.",
                "A tutorial on learning with bayesian networks.",
                "In M. Jordan, editor, Learning in Graphical Models.",
                "Kluwer Academic, 1998. [11] J. L. Herlocker, J.",
                "A. Konstan, A. Borchers, and J. Riedl.",
                "An algorithmic framework for performing collaborative filtering.",
                "In SIGIR 99: Proceedings of the 22nd annual international ACM SIGIR conference on Research and development in information retrieval, pages 230-237, New York, NY, USA, 1999.",
                "ACM Press. [12] T. Hofmann and J. Puzicha.",
                "Latent class models for collaborative filtering.",
                "In IJCAI 99: Proceedings of the Sixteenth International Joint Conference on Artificial Intelligence, pages 688-693, San Francisco, CA, USA, 1999.",
                "Morgan Kaufmann Publishers Inc. [13] I. M. D. (IMDB).",
                "Internet movie database. http://www.imdb.com/interfaces/, 2006. [14] R. Jin, J. Y. Chai, and L. Si.",
                "An automatic weighting scheme for collaborative filtering.",
                "In SIGIR 04: Proceedings of the 27th annual international ACM SIGIR conference on Research and development in information retrieval, pages 337-344, New York, NY, USA, 2004.",
                "ACM Press. [15] J.",
                "A. Konstan, B. N. Miller, D. Maltz, J. L. Herlocker, L. R. Gordon, and J. Riedl.",
                "GroupLens: Applying collaborative filtering to Usenet news.",
                "Communications of the ACM, 40(3):77-87, 1997. [16] D. Lewis.",
                "Applying support vector machines to the TREC-2001 batch filtering and routing tasks.",
                "In Proceedings of the Eleventh Text REtrieval Conference (TREC-11), 2002. [17] B. Liu, X. Li, W. S. Lee, , and P. Yu.",
                "Text classification by labeling words.",
                "In Proceedings of The Nineteenth National Conference on Artificial Intelligence (AAAI-2004), July 25-29, 2004. [18] P. Melville, R. J. Mooney, and R. Nagarajan.",
                "Content-boosted collaborative filtering for improved recommendations.",
                "In Proceedings of the Eighteenth National Conference on Artificial Intelligence (AAAI-2002), Edmonton, Canada, 2002. [19] Netflix.",
                "Netflix prize. http://www.netflixprize.com (visited on Nov. 30, 2006), 2006. [20] S. Robertson and K. Sparck-Jones.",
                "Relevance weighting of search terms.",
                "In Journal of the American Society for Information Science, volume 27, pages 129-146, 1976. [21] J. Wang, A. P. de Vries, and M. J. T. Reinders.",
                "Unifying user-based and item-based collaborative filtering approaches by similarity fusion.",
                "In SIGIR 06: Proceedings of the 29th annual international ACM SIGIR conference on Research and development in information retrieval, pages 501-508, New York, NY, USA, 2006.",
                "ACM Press. [22] X. Wu and R. K. Srihari.",
                "Incorporating prior knowledge with weighted margin support vector machines.",
                "In Proc.",
                "ACM Knowledge Discovery Data Mining Conf. (ACM SIGKDD 2004), Aug. 2004. [23] Y. Yang, S. Yoo, J. Zhang, and B. Kisiel.",
                "Robustness of adaptive filtering methods in a cross-benchmark evaluation.",
                "In Proceedings of the 28th Annual International ACM SIGIR Conference on Research and Development in Information Retrieval, 2005. [24] K. Yu, V. Tresp, and A. Schwaighofer.",
                "Learning gaussian processes from multiple tasks.",
                "In ICML 05: Proceedings of the 22nd international conference on Machine learning, pages 1012-1019, New York, NY, USA, 2005.",
                "ACM Press. [25] K. Yu, V. Tresp, and S. Yu.",
                "A nonparametric hierarchical bayesian framework for information filtering.",
                "In SIGIR 04: Proceedings of the 27th annual international ACM SIGIR conference on Research and development in information retrieval, pages 353-360.",
                "ACM Press, 2004. [26] X. Zhu.",
                "Semi-supervised learning literature survey.",
                "Technical report, University of Wisconsin - Madison, December 9, 2006. [27] P. Zigoris and Y. Zhang.",
                "Bayesian adaptive user profiling with explicit & implicit feedback.",
                "In Conference on Information and Knowledge Mangement 2006, 2006."
            ],
            "original_annotated_samples": [],
            "translated_annotated_samples": [
                "Introducción \"Personalización\" es el futuro de la Web, y ha logrado un gran éxito en aplicaciones industriales.",
                "Un tema importante de \"personalización\" estudiado en la comunidad de recuperación de información son los sistemas de recomendación personal basados en el contenido1.",
                "Un desafío importante de construir una recomendación o sistema de \"personalización\" es que el perfil aprendido para un usuario en particular suele ser de baja calidad cuando la cantidad de datos de ese usuario en particular es pequeña."
            ],
            "translated_text": "",
            "candidates": [
                "personalización",
                "Personalización",
                "personalización",
                "personalización",
                "personalización",
                "personalización"
            ],
            "error": []
        },
        "bayesian hierarchical model": {
            "translated_key": "modelo jerárquico bayesiano",
            "is_in_text": true,
            "original_annotated_sentences": [
                "Efficient Bayesian Hierarchical User Modeling for Recommendation Systems Yi Zhang, Jonathan Koren School of Engineering University of California Santa Cruz Santa Cruz, CA, USA {yiz, jonathan}@soe.ucsc.edu ABSTRACT A content-based personalized recommendation system learns user specific profiles from user feedback so that it can deliver information tailored to each individual users interest.",
                "A system serving millions of users can learn a better user profile for a new user, or a user with little feedback, by borrowing information from other users through the use of a <br>bayesian hierarchical model</br>.",
                "Learning the model parameters to optimize the joint data likelihood from millions of users is very computationally expensive.",
                "The commonly used EM algorithm converges very slowly due to the sparseness of the data in IR applications.",
                "This paper proposes a new fast learning technique to learn a large number of individual user profiles.",
                "The efficacy and efficiency of the proposed algorithm are justified by theory and demonstrated on actual user data from Netflix and MovieLens.",
                "Categories and Subject Descriptors: B.3.3 [Information Search and Retrieval]: Information filtering General Terms: Algorithms 1.",
                "INTRODUCTION Personalization is the future of the Web, and it has achieved great success in industrial applications.",
                "For example, online stores, such as Amazon and Netflix, provide customized recommendations for additional products or services based on a users history.",
                "Recent offerings such as My MSN, My Yahoo!, My Google, and Google News have attracted much attention due to their potential ability to infer a users interests from his/her history.",
                "One major personalization topic studied in the information retrieval community is content-based personal recommendation systems1 .",
                "These systems learn user-specific profiles from user feedback so that they can recommend information tailored to each individual users interest without requiring the user to make an explicit query.",
                "Learning the user profiles is the core problem for these systems.",
                "A user profile is usually a classifier that can identify whether a document is relevant to the user or not, or a regression model that tells how relevant a document is to the user.",
                "One major challenge of building a recommendation or personalization system is that the profile learned for a particular user is usually of low quality when the amount of data from that particular user is small.",
                "This is known as the cold start problem.",
                "This means that any new user must endure poor initial performance until sufficient feedback from that user is provided to learn a reliable user profile.",
                "There has been much research on improving classification accuracy when the amount of labeled training data is small.",
                "The semi-supervised learning approach combines unlabeled and labeled data together to achieve this goal [26].",
                "Another approach is using domain knowledge.",
                "Researchers have modified different learning algorithms, such as Na¨ıveBayes [17], logistic regression [7], and SVMs [22], to integrate domain knowledge into a text classifier.",
                "The third approach is borrowing training data from other resources [5][7].",
                "The effectiveness of these different approaches is mixed, due to how well the underlying model assumption fits the data.",
                "One well-received approach to improve recommendation system performance for a particular user is borrowing information from other users through a Bayesian hierarchical modeling approach.",
                "Several researchers have demonstrated that this approach effectively trades off between shared and user-specific information, thus alleviating poor initial performance for each user[27][25].",
                "In order to learn a <br>bayesian hierarchical model</br>, the system usually tries to find the most likely model parameters for the given data.",
                "A mature recommendation system usually works for millions of users.",
                "It is well known that learning the optimal parameters of a <br>bayesian hierarchical model</br> is computationally expensive when there are thousands or millions of users.",
                "The EM algorithm is a commonly used technique for parameter learning due to its simplicity and convergence guarantee.",
                "However, a content based recommendation system often handles documents in a very high dimensional space, in which each document is represented by a very sparse vector.",
                "With careful analysis of the EM algorithm in this scenario (Section 4), we find that the EM tering, or item-based collaborative filtering.",
                "In this paper, the words filtering and recommendation are used interchangeably. algorithm converges very slowly due to the sparseness of the input variables.",
                "We also find that updating the model parameter at each EM iteration is also expensive with computational complexity of O(MK), where M is the number of users and K is the number of dimensions.",
                "This paper modifies the standard EM algorithm to create an improved learning algorithm, which we call the Modified EM algorithm.",
                "The basic idea is that instead of calculating the numerical solution for all the user profile parameters, we derive the analytical solution of the parameters for some feature dimensions, and at the M step use the analytical solution instead of the numerical solution estimated at E step for those parameters.",
                "This greatly reduces the computation at a single EM iteration, and also has the benefit of increasing the convergence speed of the learning algorithm.",
                "The proposed technique is not only well supported by theory, but also by experimental results.",
                "The organization of the remaining parts of this paper is as follows: Section 3 describes the Bayesian hierarchical linear regression modeling framework used for content-based recommendations.",
                "Section 4 describes how to learn the model parameters using the standard EM algorithm, along with using the new technique proposed in this paper.",
                "The experimental setting and results used to validate the proposed learning technique are reported in Sections 5 and 6.",
                "Section 7 summarizes and offers concluding remarks. 2.",
                "RELATED WORK Providing personalized recommendations to users has been identified as a very important problem in the IR community since the 1970s.",
                "The approaches that have been used to solve this problem can be roughly classified into two major categories: content based filtering versus collaborative filtering.",
                "Content-based filtering studies the scenario where a recommendation system monitors a document stream and pushes documents that match a user profile to the corresponding user.",
                "The user may read the delivered documents and provide explicit relevance feedback, which the filtering system then uses to update the users profile using relevance feedback retrieval models (e.g.",
                "Boolean models, vector space models, traditional probabilistic models [20] , inference networks [3] and language models [6]) or machine learning algorithms (e.g.",
                "Support Vector Machines (SVM), K nearest neighbors (K-NN) clustering, neural networks, logistic regression, or Winnow [16] [4] [23]).",
                "Collaborative filtering goes beyond merely using document content to recommend items to a user by leveraging information from other users with similar tastes and preferences in the past.",
                "Memorybased heuristics and model based approaches have been used in collaborative filtering task [15] [8] [2] [14] [12] [11].",
                "This paper contributes to the content-based recommendation research by improving the efficiency and effectiveness of Bayesian hierarchical linear models, which have a strong theoretical basis and good empirical performance on recommendation tasks[27][25].",
                "This paper does not intend to compare content-based filtering with collaborative filtering or claim which one is a better.",
                "We think each complements the other, and that content-based filtering is extremely useful for handling new documents/items with little or no user feedback.",
                "Similar to some other researchers[18][1][21], we found that a recommendation system will be more effective when both techniques are combined.",
                "However, this is beyond the scope of this paper and thus not discussed here. 3.",
                "BAYESIAN HIERARCHICAL LINEAR REGRESSION Assume there are M users in the system.",
                "The task of the system is to recommend documents that are relevant to each user.",
                "For each user, the system learns a user model from the users history.",
                "In the rest of this paper, we will use the following notations to represent the variables in the system. m = 1, 2, ..., M: The index for each individual user.",
                "M is the total number of users. wm: The user model parameter associated with user m. wm is a K dimensional vector. j = 1, 2, ..., Jm: The index for a set of data for user m. Jm is the number of training data for user m. Dm = {(xm,j, ym,j)}: A set of data associated with user m. xm,j is a K dimensional vector that represents the mth users jth training document.2 ym,j is a scalar that represents the label of document xm,j. k = 1, 2, ..., K: The dimensional index of input variable x.",
                "The Bayesian hierarchical modeling approach has been widely used in real-world information retrieval applications.",
                "Generalized Bayesian hierarchical linear models, one of the simplest Bayesian hierarchical models, are commonly used and have achieved good performance on collaborative filtering [25] and content-based adaptive filtering [27] tasks.",
                "Figure 1 shows the graphical representation of a <br>bayesian hierarchical model</br>.",
                "In this graph, each user model is represented by a random vector wm.",
                "We assume a user model is sampled randomly from a prior distribution P(w|Φ).",
                "The system can predict the user label y of a document x given an estimation of wm (or wms distribution) using a function y = f(x, w).",
                "The model is called generalized Bayesian hierarchical linear model when y = f(wT x) is any generalized linear model such as logistic regression, SVM, and linear regression.",
                "To reliably estimate the user model wm, the system can borrow information from other users through the prior Φ = (µ, Σ).",
                "Now we look at one commonly used model where y = wT x + , where ∼ N(0, σ2 ) is a random noise [25][27].",
                "Assume that each user model wm is an independent draw from a population distribution P(w|Φ), which is governed by some unknown hyperparameter Φ.",
                "Let the prior distribution of user model w be a Gaussian distribution with parameter Φ = (µ, Σ), which is the commonly used prior for linear models. µ = (µ1, µ2, ..., µK ) is a K dimensional vector that represents the mean of the Gaussian distribution, and Σ is the covariance matrix of the Gaussian.",
                "Usually, a Normal distribution N(0, aI) and an Inverse Wishart distribution P(Σ) ∝ |Σ|− 1 2 b exp(−1 2 ctr(Σ−1 )) are used as hyperprior to model the prior distribution of µ and Σ respectively.",
                "I is the K dimensional identity matrix, and a, b, and c are real numbers.",
                "With these settings, we have the following model for the system: 1. µ and Σ are sampled from N(0, aI) and IWν (aI), respectively. 2 The first dimension of x is a dummy variable that always equals to 1.",
                "Figure 1: Illustration of dependencies of variables in the hierarchical model.",
                "The rating, y, for a document, x, is conditioned on the document and the user model, wm, associated with the user m. Users share information about their models through the prior, Φ = (µ, Σ). 2.",
                "For each user m, wm is sampled randomly from a Normal distribution: wm ∼ N(µ, Σ2 ) 3.",
                "For each item xm,j, ym,j is sampled randomly from a Normal distribution: ym,j ∼ N(wT mxm,j, σ2 ).",
                "Let θ = (Φ, w1, w2, ..., wM ) represent the parameters of this system that needs to be estimated.",
                "The joint likelihood for all the variables in the probabilistic model, which includes the data and the parameters, is: P(D, θ) = P(Φ) m P(wm|Φ) j P(ym,j|xm,j, wm) (1) For simplicity, we assume a, b, c, and σ are provided to the system. 4.",
                "MODEL PARAMETER LEARNING If the prior Φ is known, finding the optimal wm is straightforward: it is a simple linear regression.",
                "Therefore, we will focus on estimating Φ.",
                "The maximum a priori solution of Φ is given by ΦMAP = arg max Φ P(Φ|D) (2) = arg max Φ P(Φ, D) P(D) (3) = arg max Φ P(D|Φ)P(Φ) (4) = arg max Φ w P(D|w, Φ)P(w|Φ)P(Φ)dw (5) Finding the optimal solution for the above problem is challenging, since we need to integrate over all w = (w1, w2, ..., wM ), which are unobserved hidden variables. 4.1 EM Algorithm for Bayesian Hierarchical Linear Models In Equation 5, Φ is the parameter needs to be estimated, and the result depends on unobserved latent variables w. This kind of optimization problem is usually solved by the EM algorithm.",
                "Applying EM to the above problem, the set of user models w are the unobservable hidden variables and we have: Q = w P(w|µ, Σ2 , Dm) log P(µ, Σ2 , w, D)dw Based on the derivation of the EM formulas presented in [24], we have the following Expectation-Maximization steps for finding the optimal hyperparameters.",
                "For space considerations, we omit the derivation in this paper since it is not the focus of our work.",
                "E step: For each user m, estimate the user model distribution P(wm|Dm, Φ) = N(wm; ¯wm, Σ2 m) based on the current estimation of the prior Φ = (µ, Σ2 ). ¯wm = ((Σ2 )−1 + Sxx,m σ2 )−1 ( Sxy,m σ2 + (Σ2 )−1 µ) (6) Σ2 m = ((Σ2 )−1 + Sxx,m σ2 )−1 (7) where Sxx,m = j xm,jxT m,j Sxy,m = j xm,jym,j M step: Optimize the prior Φ = (µ, Σ2 ) based on the estimation from the last E step. µ = 1 M m ¯wm (8) Σ2 = 1 M m Σ2 m + ( ¯wm − µ)( ¯wm − µ)T (9) Many machine learning driven IR systems use a point estimate of the parameters at different stages in the system.",
                "However, we are estimating the posterior distribution of the variables at the E step.",
                "This avoids overfitting wm to a particular users data, which may be small and noisy.",
                "A detailed discussion about this subject appears in [10]. 4.2 New Algorithm: Modified EM Although the EM algorithm is widely studied and used in machine learning applications, using the above EM process to solve Bayesian hierarchical linear models in large-scale information retrieval systems is still too computationally expensive.",
                "In this section, we describe why the learning rate of the EM algorithm is slow in our application and introduce a new technique to make the learning of the Bayesian hierarchical linear model scalable.",
                "The derivation of the new learning algorithm will be based on the EM algorithm described in the previous section.",
                "First, the covariance matrices Σ2 , Σ2 m are usually too large to be computationally feasible.",
                "For simplicity, and as a common practice in IR, we do not model the correlation between features.",
                "Thus we approximate these matrices with K dimensional diagonal matrices.",
                "In the rest of the paper, we use these symbols to represent their diagonal approximations: Σ2 =     σ2 1 0 .. 0 0 σ2 2 .. 0 .. .. .. .. 0 0 .. σ2 K     Σ2 m =     σ2 m,1 0 .. 0 0 σ2 m,2 .. 0 .. .. .. .. 0 0 .. σ2 m,K     Secondly, and most importantly, the input space is very sparse and there are many dimensions that are not related to a particular user in a real IR application.",
                "For example, let us consider a movie recommendation system, with the input variable x representing a particular movie.",
                "For the jth movie that the user m has seen, let xm,j,k = 1 if the director of the movie is Jean-Pierre Jeunet (indexed by k).",
                "Here we assume that whether or not that this director directed a specific movie is represented by the kth dimension.",
                "If the user m has never seen a movie directed by Jean-Pierre Jeunet, then the corresponding dimension is always zero (xm,j,k = 0 for all j) .",
                "One major drawback of the EM algorithm is that the importance of a feature, µk, may be greatly dominated by users who have never encountered this feature (i.e. j xm,j,k = 0) at the M step (Equation 8).",
                "Assume that 100 out of 1 million users have viewed the movie directed by Jean-Pierre Jeunet, and that the viewers have rated all of his movies as excellent.",
                "Intuitively, he is a good director and the weight for him (µk) should be high.",
                "Before the EM iteration, the initial value of µ is usually set to 0.",
                "Since the other 999,900 users have not seen this movie, their corresponding weights (w1,k, w2,k, ..., wm,k..., w999900,k) for that director would be very small initially.",
                "Thus the corresponding weight of the director in the prior µk at the first M step would be very low , and the variance σm,k will be large (Equations 8 and 7).",
                "It is undesirable that users who have never seen any movie produced by the director influence the importance of the director so much.",
                "This makes the convergence of the standard EM algorithm very slow.",
                "Now lets look at whether we can improve the learning speed of the algorithm.",
                "Without a loss of generality, let us assume that the kth dimension of the input variable x is not related to a particular user m. By which we mean, xm,j,k = 0 for all j = 1, ..., Jm.",
                "It is straightforward to prove that the kth row and kth column of Sxx,m are completely filled with zeros, and that the kth dimension of Sxy,m is zeroed as well.",
                "Thus the corresponding kth dimension of the user models mean, ¯wm, should be equal to that of the prior: ¯wm,k = µk, with the corresponding covariance of σm,k = σk.",
                "At the M step, the standard EM algorithm uses the numerical solution of the distribution P(wm|Dm, Φ) estimated at E step (Equation 8 and Equation 7).",
                "However, the numerical solutions are very unreliable for ¯wm,k and σm,k when the kth dimension is not related to the mth user.",
                "A better approach is using the analytical solutions ¯wm,k = µk, and σm,k = σk for the unrelated (m, k) pairs, along with the numerical solution estimated at E step for the other (m, k) pairs.",
                "Thus we get the following new EM-like algorithm: Modified E step: For each user m, estimate the user model distribution P(wm|Dm, Φ) = N(wm; ¯wm, Σ2 m) based on the current estimation of σ , µ, Σ2 . ¯wm = ((Σ2 )−1 + Sxx,m σ2 )−1 ( Sxy,m σ2 + (Σ2 )−1 µ)(10) σ2 m,k = ((σ2 k)−1 + sxx,m,k σ2 )−1 (11) where sxx,m,k = j x2 m,j,k and sxy,m,k = j xm,j,kym,j Modified M Step Optimize the prior Φ = (µ, Σ2 ) based on the estimation from the last E step for related userfeature pairs.",
                "The M step implicitly uses the analytical solution for unrelated user-feature pairs. µk = 1 Mk m:related ¯wm,k (12) σ2 k = 1 Mk m:related σ2 m,k +( ¯wm,k − µk)( ¯wm,k − µk)T (13) where Mk is the number of users that are related to feature k We only estimate the diagonal of Σ2 m and Σ since we are using the diagonal approximation of the covariance matrices.",
                "To estimate ¯wm, we only need to calculate the numerical solutions for dimensions that are related to user m. To estimate σ2 k and µk, we only sum over users that are related to the kth feature.",
                "There are two major benefits of the new algorithm.",
                "First, because only the related (m, k) pairs are needed at the modified M step, the computational complexity in a single EM iteration is much smaller when the data is sparse, and many of (m, k) pairs are unrelated.",
                "Second, the parameters estimated at the modified M step (Equations 12 - 13) are more accurate than the standard M step described in Section 4.1 because the exact analytical solutions ¯wm,k = µk and σm,k = σk for the unrelated (m, k) pairs were used in the new algorithm instead of an approximate solution as in the standard algorithm. 5.",
                "EXPERIMENTAL METHODOLOGY 5.1 Evaluation Data Set To evaluate the proposed technique, we used the following three major data sets (Table 1): MovieLens Data: This data set was created by combining the relevance judgments from the MovieLens[9] data set with documents from the Internet Movie Database (IMDB).",
                "MovieLens allows users to rank how much he/she enjoyed a specific movie on a scale from 1 to 5.",
                "This likeability rating was used as a measurement of how relevant the document representing the corresponding movie is to the user.",
                "We considered documents with likeability scores of 4 or 5 as relevant, and documents with a score of 1 to 3 as irrelevant to the user.",
                "MovieLens provided relevance judgments on 3,057 documents from 6,040 separate users.",
                "On average, each user rated 151 movies, of these 87 were judged to be relevant.",
                "The average score for a document was 3.58.",
                "Documents representing each movie were constructed from the portion of the IMDB database that is available for public download[13].",
                "Based on this database, we created one document per movie that contained the relevant information about it (e.g. directors, actors, etc.).",
                "Table 1: Data Set Statistics.",
                "On Reuters, the number of rating for a simulated user is the number of documents relevant to the corresponding topic.",
                "Data Users Docs Ratings per User MovieLens 6,040 3,057 151 Netflix-all 480,189 17,770 208 Netflix-1000 1000 17,770 127 Reuters-C 34 100,000 3949 Reuters-E 26 100,000 1632 Reuters-G 33 100,000 2222 Reuters-M 10 100,000 6529 Netflix Data: This data set was constructed by combining documents about movies crawled from the web with a set of actual movie rental customer relevance judgments from Netflix[19].",
                "Netflix publicly provides the relevance judgments of 480,189 anonymous customers.",
                "There are around 100 million rating on a scale of 1 to 5 for 17,770 documents.",
                "Similar to MovieLens, we considered documents with likeability scores of 4 or 5 as relevant.",
                "This number was reduced to 1000 customers through random sampling.",
                "The average customer on the reduced data set provided 127 judgments, with 70 being deemed relevant.",
                "The average score for documents is 3.55.",
                "Reuters Data: This is the Reuters Corpus, Volume 1.",
                "It covers 810,000 Reuters English language news stories from August 20, 1996 to August 19, 1997.",
                "Only the first 100,000 news were used in our experiments.",
                "The Reuters corpus comes with a topic hierarchy.",
                "Each document is assigned to one of several locations on the hierarchical tree.",
                "The first level of the tree contains four topics, denoted as C, E, M, and G. For the experiments in this paper, the tree was cut at level 1 to create four smaller trees, each of which corresponds to one smaller data set: Reuters-E Reuters-C, ReutersM and Reuters-G. For each small data set, we created several profiles, one profile for each node in a sub-tree, to simulate multiple users, each with a related, yet separate definition of relevance.",
                "All the user profiles on a sub-tree are supposed to share the same prior model distribution.",
                "Since this corpus explicitly indicates only the relevant documents for a topic(user), all other documents are considered irrelevant. 5.2 Evaluation We designed the experiments to answer the following three questions: 1.",
                "Do we need to take the effort to use a Bayesian approach and learn a prior from other users? 2.",
                "Does the new algorithm work better than the standard EM algorithm for learning the Bayesian hierarchical linear model? 3.",
                "Can the new algorithm quickly learn many user models?",
                "To answer the first question, we compared the Bayesian hierarchical models with commonly used Norm-2 regularized linear regression models.",
                "In fact, the commonly used approach is equivalent to the model learned at the end of the first EM iteration.",
                "To answer the second question, we compared the proposed new algorithm with the standard EM algorithm to see whether the new learning algorithm is better.",
                "To answer the third question, we tested the efficiency of the new algorithm on the entire Netflix data set where about half a million user models need to be learned together.",
                "For the MovieLens and Netflix data sets, algorithm effectiveness was measured by mean square error, while on the Reuters data set classification error was used because it was more informative.",
                "We first evaluated the performance on each individual user, and then estimated the macro average over all users.",
                "Statistical tests (t-tests) were carried out to see whether the results are significant.",
                "For the experiments on the MovieLens and Netflix data sets, we used a random sample of 90% of each user for training, and the rest for testing.",
                "On Reuters data set, because there are too many relevant documents for each topic in the corpus, we used a random sample of 10% of each topic for training, and 10% of the remaining documents for testing.",
                "For all runs, we set (a, b, c, Σ ) = (0.1, 10, 0.1, 1) manually. 6.",
                "EXPERIMENTAL RESULTS Figure 2, Figure 3, and Figure 4 show that on all data sets, the Bayesian hierarchical modeling approach has a statistical significant improvement over the regularized linear regression model, which is equivalent to the Bayesian hierarchical models learned at the first iteration.",
                "Further analysis shows a negative correlation between the number of training data for a user and the improvement the system gets.",
                "This suggests that the borrowing information from other users has more significant improvements for users with less training data, which is as expected.",
                "However, the strength of the correlation differs over data sets, and the amount of training data is not the only characteristics that will influence the final performance.",
                "Figure 2 and Figure 3 show that the proposed new algorithm works better than the standard EM algorithm on the Netflix and MovieLens data sets.",
                "This is not surprising since the number of related feature-users pairs is much smaller than the number of unrelated feature-user pairs on these two data sets, and thus the proposed new algorithm is expected to work better.",
                "Figure 4 shows that the two algorithms work similarly on the Reuters-E data set.",
                "The accuracy of the new algorithm is similar to that of the standard EM algorithm at each iteration.",
                "The general patterns are very similar on other Reuters subsets.",
                "Further analysis shows that only 58% of the user-feature pairs are unrelated on this data set.",
                "Since the number of unrelated user-feature pairs is not extremely large, the sparseness is not a serious problem on the Reuters data set.",
                "Thus the two learning algorithms perform similarly.",
                "The results suggest that only on a corpus where the number of unrelated user-feature pairs is much larger than the number of related pairs, such as on the Netflix data set, the proposed technique will get a significant improvement over standard EM.",
                "However, the experiments also show that when the assumption does not hold, the new algorithm does not hurt performance.",
                "Although the proposed technique is faster than standard Figure 2: Performance on a Netflix subset with 1,000 users.",
                "The new algorithm is statistical significantly better than EM algorithm at iterations 2 - 10.",
                "Norm-2 regularized linear models are equivalent to the Bayesian hierarchical models learned at the first iteration, and are statistical significantly worse than the Bayesian hierarchical models. 0 2 4 6 8 10 1 1.05 1.1 1.15 1.2 1.25 1.3 1.35 1.4 Iterations MeanSquareError New Algorithm Traditional EM 1 2 3 4 5 6 7 8 9 10 0.33 0.34 0.35 0.36 0.37 0.38 0.39 Iterations ClassificationError New Algorithm Traditional EM Figure 3: Performance on a MovieLens subset with 1,000 users.",
                "The new algorithm is statistical significantly better than EM algorithm at iteration 2 to 17 (evaluated with mean square error). 1 6 11 16 21 0.5 1 1.5 2 2.5 3 3.5 Iterations MeanSquareError New Algorithm Traditional EM 1 6 11 16 21 0.35 0.4 0.45 0.5 0.55 0.6 0.65 Iterations ClassificationError New Algorithm Traditional EM Figure 4: Performance on a Reuters-E subset with 26 profiles.",
                "Performances on Reuters-C, Reuters-M, Reuters-G are similar. 1 2 3 4 5 0.011 0.0115 0.012 0.0125 0.013 0.0135 0.014 Iterations MeanSquareError New Algorithm Traditional EM 1 2 3 4 5 0.0102 0.0104 0.0106 0.0108 0.011 0.0112 0.0114 Iterations ClassificationError New Algorithm Traditional EM EM, can it really learn millions of user models quickly?",
                "Our results show that the modified EM algorithm converges quickly, and 2 - 3 modified EM iterations would result in a reliable estimation.",
                "We evaluated the algorithm on the whole Netflix data set (480,189 users, 159,836 features, and 100 million ratings) running on a single CPU PC (2GB memory, P4 3GHz).",
                "The system finished one modified EM iteration in about 4 hours.",
                "This demonstrates that the proposed technique can efficiently handle large-scale system like Netflix. 7.",
                "CONCLUSION Content-based user profile learning is an important problem and is the key to providing personal recommendations to a user, especially for recommending new items with a small number of ratings.",
                "The Bayesian hierarchical modeling approach is becoming an important user profile learning approach due to its theoretically justified ability to help one user through information transfer from the other users by way of hyperpriors.",
                "This paper examined the weakness of the popular EM based learning approach for Bayesian hierarchical linear models and proposed a better learning technique called Modified EM.",
                "We showed that the new technique is theoretically more computationally efficient than the standard EM algorithm.",
                "Evaluation on the MovieLens and Netflix data sets demonstrated the effectiveness of the new technique when the data is sparse, by which we mean the ratio of related user-feature pairs to unrelated pairs is small.",
                "Evaluation on the Reuters data set showed that the new technique performed similar to the standard EM algorithm when the sparseness condition does not hold.",
                "In general, it is better to use the new algorithm since it is as simple as standard EM, the performance is either better or similar to EM, and the computation complexity is lower at each iteration.",
                "It is worth mentioning that even if the original problem space is not sparse, sparseness can be created artificially when a recommendation system uses user-specific feature selection techniques to reduce the noise and user model complexity.",
                "The proposed technique can also be adapted to improve the learning in such a scenario.",
                "We also demonstrated that the proposed technique can learn half a million user profiles from 100 million ratings in a few hours with a single CPU.",
                "The research is important because scalability is a major concern for researchers when using the Bayesian hierarchical linear modeling approach to build a practical large scale system, even though the literature have demonstrated the effectiveness of the models in many applications.",
                "Our work is one major step on the road to make Bayesian hierarchical linear models more practical.",
                "The proposed new technique can be easily adapted to run on a cluster of machines, and thus further speed up the learning process to handle a larger scale system with hundreds of millions of users.",
                "The research has much potential to benefit people using EM algorithm on many other IR problems as well as machine learning problems.",
                "EM algorithm is a commonly used machine learning technique.",
                "It is used to find model parameters in many IR problems where the training data is very sparse.",
                "Although we are focusing on the Bayesian hierarchical linear models for recommendation and filtering, the new idea of using analytical solution instead of numerical solution for unrelated user-feature pairs at the M step could be adapted to many other problems. 8.",
                "ACKNOWLEDGMENTS We thank Wei Xu, David Lewis and anonymous reviewers for valuable feedback on the work described in this paper.",
                "Part of the work was supported by Yahoo, Google, the Petascale Data Storage Institute and the Institute for Scalable Scientific Data Management.",
                "Any opinions, findings, conclusions, or recommendations expressed in this material are those of the authors, and do not necessarily reflect those of the sponsors. 9.",
                "REFERENCES [1] C. Basu, H. Hirsh, and W. Cohen.",
                "Recommendation as classification: Using social and content-based information in recommendation.",
                "In Proceedings of the Fifteenth National Conference on Artificial Intelligence, 1998. [2] J. S. Breese, D. Heckerman, and C. Kadie.",
                "Empirical analysis of predictive algorithms for collaborative filtering.",
                "Technical report, Microsoft Research, One Microsoft Way, Redmond, WA 98052, 1998. [3] J. Callan.",
                "Document filtering with inference networks.",
                "In Proceedings of the Nineteenth Annual International ACM SIGIR Conference on Research and Development in Information Retrieval, pages 262-269, 1996. [4] N. Cancedda, N. Cesa-Bianchi, A. Conconi, C. Gentile, C. Goutte, T. Graepel, Y. Li, J. M. Renders, J. S. Taylor, and A. Vinokourov.",
                "Kernel method for document filtering.",
                "In The Eleventh Text REtrieval Conference (TREC11).",
                "National Institute of Standards and Technology, special publication 500-249, 2003. [5] C. Chelba and A. Acero.",
                "Adaptation of maximum entropy capitalizer: Little data can help a lot.",
                "In D. Lin and D. Wu, editors, Proceedings of EMNLP 2004, pages 285-292, Barcelona, Spain, July 2004.",
                "Association for Computational Linguistics. [6] B. Croft and J. Lafferty, editors.",
                "Language Modeling for Information Retrieval.",
                "Kluwer, 2002. [7] A. Dayanik, D. D. Lewis, D. Madigan, V. Menkov, and A. Genkin.",
                "Constructing informative prior distributions from domain knowledge in text classification.",
                "In SIGIR 06: Proceedings of the 29th annual international ACM SIGIR conference on Research and development in information retrieval, pages 493-500, New York, NY, USA, 2006.",
                "ACM Press. [8] J. Delgado and N. Ishii.",
                "Memory-based weightedmajority prediction for recommender systems.",
                "In ACM SIGIR99 Workshop on Recommender Systems, 1999. [9] GroupLens.",
                "Movielens. http://www.grouplens.org/taxonomy/term/14, 2006. [10] D. Heckerman.",
                "A tutorial on learning with bayesian networks.",
                "In M. Jordan, editor, Learning in Graphical Models.",
                "Kluwer Academic, 1998. [11] J. L. Herlocker, J.",
                "A. Konstan, A. Borchers, and J. Riedl.",
                "An algorithmic framework for performing collaborative filtering.",
                "In SIGIR 99: Proceedings of the 22nd annual international ACM SIGIR conference on Research and development in information retrieval, pages 230-237, New York, NY, USA, 1999.",
                "ACM Press. [12] T. Hofmann and J. Puzicha.",
                "Latent class models for collaborative filtering.",
                "In IJCAI 99: Proceedings of the Sixteenth International Joint Conference on Artificial Intelligence, pages 688-693, San Francisco, CA, USA, 1999.",
                "Morgan Kaufmann Publishers Inc. [13] I. M. D. (IMDB).",
                "Internet movie database. http://www.imdb.com/interfaces/, 2006. [14] R. Jin, J. Y. Chai, and L. Si.",
                "An automatic weighting scheme for collaborative filtering.",
                "In SIGIR 04: Proceedings of the 27th annual international ACM SIGIR conference on Research and development in information retrieval, pages 337-344, New York, NY, USA, 2004.",
                "ACM Press. [15] J.",
                "A. Konstan, B. N. Miller, D. Maltz, J. L. Herlocker, L. R. Gordon, and J. Riedl.",
                "GroupLens: Applying collaborative filtering to Usenet news.",
                "Communications of the ACM, 40(3):77-87, 1997. [16] D. Lewis.",
                "Applying support vector machines to the TREC-2001 batch filtering and routing tasks.",
                "In Proceedings of the Eleventh Text REtrieval Conference (TREC-11), 2002. [17] B. Liu, X. Li, W. S. Lee, , and P. Yu.",
                "Text classification by labeling words.",
                "In Proceedings of The Nineteenth National Conference on Artificial Intelligence (AAAI-2004), July 25-29, 2004. [18] P. Melville, R. J. Mooney, and R. Nagarajan.",
                "Content-boosted collaborative filtering for improved recommendations.",
                "In Proceedings of the Eighteenth National Conference on Artificial Intelligence (AAAI-2002), Edmonton, Canada, 2002. [19] Netflix.",
                "Netflix prize. http://www.netflixprize.com (visited on Nov. 30, 2006), 2006. [20] S. Robertson and K. Sparck-Jones.",
                "Relevance weighting of search terms.",
                "In Journal of the American Society for Information Science, volume 27, pages 129-146, 1976. [21] J. Wang, A. P. de Vries, and M. J. T. Reinders.",
                "Unifying user-based and item-based collaborative filtering approaches by similarity fusion.",
                "In SIGIR 06: Proceedings of the 29th annual international ACM SIGIR conference on Research and development in information retrieval, pages 501-508, New York, NY, USA, 2006.",
                "ACM Press. [22] X. Wu and R. K. Srihari.",
                "Incorporating prior knowledge with weighted margin support vector machines.",
                "In Proc.",
                "ACM Knowledge Discovery Data Mining Conf. (ACM SIGKDD 2004), Aug. 2004. [23] Y. Yang, S. Yoo, J. Zhang, and B. Kisiel.",
                "Robustness of adaptive filtering methods in a cross-benchmark evaluation.",
                "In Proceedings of the 28th Annual International ACM SIGIR Conference on Research and Development in Information Retrieval, 2005. [24] K. Yu, V. Tresp, and A. Schwaighofer.",
                "Learning gaussian processes from multiple tasks.",
                "In ICML 05: Proceedings of the 22nd international conference on Machine learning, pages 1012-1019, New York, NY, USA, 2005.",
                "ACM Press. [25] K. Yu, V. Tresp, and S. Yu.",
                "A nonparametric hierarchical bayesian framework for information filtering.",
                "In SIGIR 04: Proceedings of the 27th annual international ACM SIGIR conference on Research and development in information retrieval, pages 353-360.",
                "ACM Press, 2004. [26] X. Zhu.",
                "Semi-supervised learning literature survey.",
                "Technical report, University of Wisconsin - Madison, December 9, 2006. [27] P. Zigoris and Y. Zhang.",
                "Bayesian adaptive user profiling with explicit & implicit feedback.",
                "In Conference on Information and Knowledge Mangement 2006, 2006."
            ],
            "original_annotated_samples": [],
            "translated_annotated_samples": [
                "Un sistema que sirve a millones de usuarios puede aprender un mejor perfil de usuario para un nuevo usuario, o un usuario con pocos comentarios, tomando prestada información de otros usuarios mediante el uso de un \"modelo jerárquico bayesiano\".",
                "Para aprender un \"modelo jerárquico bayesiano\", el sistema generalmente trata de encontrar los parámetros del modelo más probables para los datos dados.",
                "Es bien sabido que aprender los parámetros óptimos de un \"modelo jerárquico bayesiano\" es computacionalmente costoso cuando hay miles o millones de usuarios.",
                "La Figura 1 muestra la representación gráfica de un \"modelo jerárquico bayesiano\"."
            ],
            "translated_text": "",
            "candidates": [
                "modelo jerárquico bayesiano",
                "modelo jerárquico bayesiano",
                "modelo jerárquico bayesiano",
                "modelo jerárquico bayesiano",
                "modelo jerárquico bayesiano",
                "modelo jerárquico bayesiano",
                "modelo jerárquico bayesiano",
                "modelo jerárquico bayesiano"
            ],
            "error": []
        }
    }
}