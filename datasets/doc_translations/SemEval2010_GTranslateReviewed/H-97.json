{
    "id": "H-97",
    "original_text": "Feature Representation for Effective Action-Item Detection Paul N. Bennett Computer Science Department Carnegie Mellon University Pittsburgh, PA 15213 pbennett+@cs.cmu.edu Jaime Carbonell Language Technologies Institute. Carnegie Mellon University Pittsburgh, PA 15213 jgc+@cs.cmu.edu ABSTRACT E-mail users face an ever-growing challenge in managing their inboxes due to the growing centrality of email in the workplace for task assignment, action requests, and other roles beyond information dissemination. Whereas Information Retrieval and Machine Learning techniques are gaining initial acceptance in spam filtering and automated folder assignment, this paper reports on a new task: automated action-item detection, in order to flag emails that require responses, and to highlight the specific passage(s) indicating the request(s) for action. Unlike standard topic-driven text classification, action-item detection requires inferring the senders intent, and as such responds less well to pure bag-of-words classification. However, using enriched feature sets, such as n-grams (up to n=4) with chi-squared feature selection, and contextual cues for action-item location improve performance by up to 10% over unigrams, using in both cases state of the art classifiers such as SVMs with automated model selection via embedded cross-validation. Categories and Subject Descriptors H.3.3 [Information Storage and Retrieval]: Information Search and Retrieval; I.2.6 [Artificial Intelligence]: Learning; I.5.4 [Pattern Recognition]: Applications General Terms Experimentation 1. INTRODUCTION E-mail users are facing an increasingly difficult task of managing their inboxes in the face of mounting challenges that result from rising e-mail usage. This includes prioritizing e-mails over a range of sources from business partners to family members, filtering and reducing junk e-mail, and quickly managing requests that demand From: Henry Hutchins <hhutchins@innovative.company.com> To: Sara Smith; Joe Johnson; William Woolings Subject: meeting with prospective customers Sent: Fri 12/10/2005 8:08 AM Hi All, Id like to remind all of you that the group from GRTY will be visiting us next Friday at 4:30 p.m. The current schedule looks like this: + 9:30 a.m. Informal Breakfast and Discussion in Cafeteria + 10:30 a.m. Company Overview + 11:00 a.m. Individual Meetings (Continue Over Lunch) + 2:00 p.m. Tour of Facilities + 3:00 p.m. Sales Pitch In order to have this go off smoothly, I would like to practice the presentation well in advance. As a result, I will need each of your parts by Wednesday. Keep up the good work! -Henry Figure 1: An E-mail with emphasized Action-Item, an explicit request that requires the recipients attention or action. the receivers attention or action. Automated action-item detection targets the third of these problems by attempting to detect which e-mails require an action or response with information, and within those e-mails, attempting to highlight the sentence (or other passage length) that directly indicates the action request. Such a detection system can be used as one part of an e-mail agent which would assist a user in processing important e-mails quicker than would have been possible without the agent. We view action-item detection as one necessary component of a successful e-mail agent which would perform spam detection, action-item detection, topic classification and priority ranking, among other functions. The utility of such a detector can manifest as a method of prioritizing e-mails according to task-oriented criteria other than the standard ones of topic and sender or as a means of ensuring that the email user hasnt dropped the proverbial ball by forgetting to address an action request. Action-item detection differs from standard text classification in two important ways. First, the user is interested both in detecting whether an email contains action items and in locating exactly where these action item requests are contained within the email body. In contrast, standard text categorization merely assigns a topic label to each text, whether that label corresponds to an e-mail folder or a controlled indexing vocabulary [12, 15, 22]. Second, action-item detection attempts to recover the email senders intent - whether she means to elicit response or action on the part of the receiver; note that for this task, classifiers using only unigrams as features do not perform optimally, as evidenced in our results below. Instead we find that we need more information-laden features such as higher-order n-grams. Text categorization by topic, on the other hand, works very well using just individual words as features [2, 9, 13, 17]. In fact, genre-classification, which one would think may require more than a bag-of-words approach, also works quite well using just unigram features [14]. Topic detection and tracking (TDT), also works well with unigram feature sets [1, 20]. We believe that action-item detection is one of the first clear instances of an IR-related task where we must move beyond bag-of-words to achieve high performance, albeit not too far, as bag-of-n-grams seem to suffice. We first review related work for similar text classification problems such as e-mail priority ranking and speech act identification. Then we more formally define the action-item detection problem, discuss the aspects that distinguish it from more common problems like topic classification, and highlight the challenges in constructing systems that can perform well at the sentence and document level. From there, we move to a discussion of feature representation and selection techniques appropriate for this problem and how standard text classification approaches can be adapted to smoothly move from the sentence-level detection problem to the documentlevel classification problem. We then conduct an empirical analysis that helps us determine the effectiveness of our feature extraction procedures as well as establish baselines for a number of classification algorithms on this task. Finally, we summarize this papers contributions and consider interesting directions for future work. 2. RELATED WORK Several other researchers have considered very similar text classification tasks. Cohen et al. [5] describe an ontology of speech acts, such as Propose a Meeting, and attempt to predict when an e-mail contains one of these speech acts. We consider action-items to be an important specific type of speech act that falls within their more general classification. While they provide results for several classification methods, their methods only make use of human judgments at the document-level. In contrast, we consider whether accuracy can be increased by using finer-grained human judgments that mark the specific sentences and phrases of interest. Corston-Oliver et al. [6] consider detecting items in e-mail to Put on a To-Do List. This classification task is very similar to ours except they do not consider simple factual questions to belong to this category. We include questions, but note that not all questions are action-items - some are rhetorical or simply social convention, How are you?. From a learning perspective, while they make use of judgments at the sentence-level, they do not explicitly compare what if any benefits finer-grained judgments offer. Additionally, they do not study alternative choices or approaches to the classification task. Instead, they simply apply a standard SVM at the sentence-level and focus primarily on a linguistic analysis of how the sentence can be logically reformulated before adding it to the task list. In this study, we examine several alternative classification methods, compare document-level and sentence-level approaches and analyze the machine learning issues implicit in these problems. Interest in a variety of learning tasks related to e-mail has been rapidly growing in the recent literature. For example, in a forum dedicated to e-mail learning tasks, Culotta et al. [7] presented methods for learning social networks from e-mail. In this work, we do not focus on peer relationships; however, such methods could complement those here since peer relationships often influence word choice when requesting an action. 3. PROBLEM DEFINITION & APPROACH In contrast to previous work, we explicitly focus on the benefits that finer-grained, more costly, sentence-level human judgments offer over coarse-grained document-level judgments. Additionally, we consider multiple standard text classification approaches and analyze both the quantitative and qualitative differences that arise from taking a document-level vs. a sentence-level approach to classification. Finally, we focus on the representation necessary to achieve the most competitive performance. 3.1 Problem Definition In order to provide the most benefit to the user, a system would not only detect the document, but it would also indicate the specific sentences in the e-mail which contain the action-items. Therefore, there are three basic problems: 1. Document detection: Classify a document as to whether or not it contains an action-item. 2. Document ranking: Rank the documents such that all documents containing action-items occur as high as possible in the ranking. 3. Sentence detection: Classify each sentence in a document as to whether or not it is an action-item. As in most Information Retrieval tasks, the weight the evaluation metric should give to precision and recall depends on the nature of the application. In situations where a user will eventually read all received messages, ranking (e.g., via precision at recall of 1) may be most important since this will help encourage shorter delays in communications between users. In contrast, high-precision detection at low recall will be of increasing importance when the user is under severe time-pressure and therefore will likely not read all mail. This can be the case for crisis managers during disaster management. Finally, sentence detection plays a role in both timepressure situations and simply to alleviate the users required time to gist the message. 3.2 Approach As mentioned above, the labeled data can come in one of two forms: a document-labeling provides a yes/no label for each document as to whether it contains an action-item; a phrase-labeling provides only a yes label for the specific items of interest. We term the human judgments a phrase-labeling since the users view of the action-item may not correspond with actual sentence boundaries or predicted sentence boundaries. Obviously, it is straightforward to generate a document-labeling consistent with a phrase-labeling by labeling a document yes if and only if it contains at least one phrase labeled yes. To train classifiers for this task, we can take several viewpoints related to both the basic problems we have enumerated and the form of the labeled data. The document-level view treats each e-mail as a learning instance with an associated class-label. Then, the document can be converted to a feature-value vector and learning progresses as usual. Applying a document-level classifier to document detection and ranking is straightforward. In order to apply it to sentence detection, one must make additional steps. For example, if the classifier predicts a document contains an action-item, then areas of the document that contain a high-concentration of words which the model weights heavily in favor of action-items can be indicated. The obvious benefit of the document-level approach is that training set collection costs are lower since the user only has to specify whether or not an e-mail contains an action-item and not the specific sentences. In the sentence-level view, each e-mail is automatically segmented into sentences, and each sentence is treated as a learning instance with an associated class-label. Since the phrase-labeling provided by the user may not coincide with the automatic segmentation, we must determine what label to assign a partially overlapping sentence when converting it to a learning instance. Once trained, applying the resulting classifiers to sentence detection is now straightforward, but in order to apply the classifiers to document detection and document ranking, the individual predictions over each sentence must be aggregated in order to make a document-level prediction. This approach has the potential to benefit from morespecific labels that enable the learner to focus attention on the key sentences instead of having to learn based on data that the majority of the words in the e-mail provide no or little information about class membership. 3.2.1 Features Consider some of the phrases that might constitute part of an action item: would like to know, let me know, as soon as possible, have you. Each of these phrases consists of common words that occur in many e-mails. However, when they occur in the same sentence, they are far more indicative of an action-item. Additionally, order can be important: consider have you versus you have. Because of this, we posit that n-grams play a larger role in this problem than is typical of problems like topic classification. Therefore, we consider all n-grams up to size 4. When using n-grams, if we find an n-gram of size 4 in a segment of text, we can represent the text as just one occurrence of the ngram or as one occurrence of the n-gram and an occurrence of each smaller n-gram contained by it. We choose the second of these alternatives since this will allow the algorithm itself to smoothly back-off in terms of recall. Methods such as na¨ıve Bayes may be hurt by such a representation because of double-counting. Since sentence-ending punctuation can provide information, we retain the terminating punctuation token when it is identifiable. Additionally, we add a beginning-of-sentence and end-of-sentence token in order to capture patterns that are often indicators at the beginning or end of a sentence. Assuming proper punctuation, these extra tokens are unnecessary, but often e-mail lacks proper punctuation. In addition, for the sentence-level classifiers that use ngrams, we additionally code for each sentence a binary encoding of the position of the sentence relative to the document. This encoding has eight associated features that represent which octile (the first eighth, second eighth, etc.) contains the sentence. 3.2.2 Implementation Details In order to compare the document-level to the sentence-level approach, we compare predictions at the document-level. We do not address how to use a document-level classifier to make predictions at the sentence-level. In order to automatically segment the text of the e-mail, we use the RASP statistical parser [4]. Since the automatically segmented sentences may not correspond directly with the phrase-level boundaries, we treat any sentence that contains at least 30% of a marked action-item segment as an action-item. When evaluating sentencedetection for the sentence-level system, we use these class labels as ground truth. Since we are not evaluating multiple segmentation approaches, this does not bias any of the methods. If multiple segmentation systems were under evaluation, one would need to use a metric that matched predicted positive sentences to phrases labeled positive. The metric would need to punish overly long true predictions as well as too short predictions. Our criteria for converting to labeled instances implicitly includes both criteria. Since the segmentation is fixed, an overly long prediction would be predicting yes for many no instances since presumably the extra length corresponds to additional segmented sentences all of which do not contain 30% of action-item. Likewise, a too short prediction must correspond to a small sentence included in the action-item but not constituting all of the action-item. Therefore, in order to consider the prediction to be too short, there will be an additional preceding/following sentence that is an action-item where we incorrectly predicted no. Once a sentence-level classifier has made a prediction for each sentence, we must combine these predictions to make both a document-level prediction and a document-level score. We use the simple policy of predicting positive when any of the sentences is predicted positive. In order to produce a document score for ranking, the confidence that the document contains an action-item is: ψ(d) = 1 n(d) s∈d|π(s)=1 ψ(s) if for any s ∈ d, π(s) = 1 1 n(d) maxs∈d ψ(s) o.w. where s is a sentence in document d, π is the classifiers 1/0 prediction, ψ is the score the classifier assigns as its confidence that π(s) = 1, and n(d) is the greater of 1 and the number of (unigram) tokens in the document. In other words, when any sentence is predicted positive, the document score is the length normalized sum of the sentence scores above threshold. When no sentence is predicted positive, the document score is the maximum sentence score normalized by length. As in other text problems, we are more likely to emit false positives for documents with more words or sentences. Thus we include a length normalization factor. 4. EXPERIMENTAL ANALYSIS 4.1 The Data Our corpus consists of e-mails obtained from volunteers at an educational institution and cover subjects such as: organizing a research workshop, arranging for job-candidate interviews, publishing proceedings, and talk announcements. The messages were anonymized by replacing the names of each individual and institution with a pseudonym.1 After attempting to identify and eliminate duplicate e-mails, the corpus contains 744 e-mail messages. After identity anonymization, the corpora has three basic versions. Quoted material refers to the text of a previous e-mail that an author often leaves in an e-mail message when responding to the e-mail. Quoted material can act as noise when learning since it may include action-items from previous messages that are no longer relevant. To isolate the effects of quoted material, we have three versions of the corpora. The raw form contains the basic messages. The auto-stripped version contains the messages after quoted material has been automatically removed. The hand-stripped version contains the messages after quoted material has been removed by a human. Additionally, the hand-stripped version has had any xml content and e-mail signatures removed - leaving only the essential content of the message. The studies reported here are performed with the hand-stripped version. This allows us to balance the cognitive load in terms of number of tokens that must be read in the user-studies we report - including quoted material would complicate the user studies since some users might skip the material while others read it. Additionally, ensuring all quoted material is removed 1 We have an even more highly anonymized version of the corpus that can be made available for some outside experimentation. Please contact the authors for more information on obtaining this data. prevents tainting the cross-validation since otherwise a test item could occur as quoted material in a training document. 4.1.1 Data Labeling Two human annotators labeled each message as to whether or not it contained an action-item. In addition, they identified each segment of the e-mail which contained an action-item. A segment is a contiguous section of text selected by the human annotators and may span several sentences or a complete phrase contained in a sentence. They were instructed that an action item is an explicit request for information that requires the recipients attention or a required action and told to highlight the phrases or sentences that make up the request. Annotator 1 No Yes Annotator 2 No 391 26 Yes 29 298 Table 1: Agreement of Human Annotators at Document Level Annotator One labeled 324 messages as containing action items. Annotator Two labeled 327 messages as containing action items. The agreement of the human annotators is shown in Tables 1 and 2. The annotators are said to agree at the document-level when both marked the same document as containing no action-items or both marked at least one action-item regardless of whether the text segments were the same. At the document-level, the annotators agreed 93% of the time. The kappa statistic [3, 5] is often used to evaluate inter-annotator agreement: κ = A − R 1 − R A is the empirical estimate of the probability of agreement. R is the empirical estimate of the probability of random agreement given the empirical class priors. A value close to −1 implies the annotators agree far less often than would be expected randomly, while a value close to 1 means they agree more often than randomly expected. At the document-level, the kappa statistic for inter-annotator agreement is 0.85. This value is both strong enough to expect the problem to be learnable and is comparable with results for similar tasks [5, 6]. In order to determine the sentence-level agreement, we use each judgment to create a sentence-corpus with labels as described in Section 3.2.2, then consider the agreement over these sentences. This allows us to compare agreement over no judgments. We perform this comparison over the hand-stripped corpus since that eliminates spurious no judgments that would come from including quoted material, etc. Both annotators were free to label the subject as an action-item, but since neither did, we omit the subject line of the message as well. This only reduces the number of no agreements. This leaves 6301 automatically segmented sentences. At the sentence-level, the annotators agreed 98% of the time, and the kappa statistic for inter-annotator agreement is 0.82. In order to produce one single set of judgments, the human annotators went through each annotation where there was disagreement and came to a consensus opinion. The annotators did not collect statistics during this process but anecdotally reported that the majority of disagreements were either cases of clear annotator oversight or different interpretations of conditional statements. For example, If you would like to keep your job, come to tomorrows meeting implies a required action where If you would like to join Annotator 1 No Yes Annotator 2 No 5810 65 Yes 74 352 Table 2: Agreement of Human Annotators at Sentence Level the football betting pool, come to tomorrows meeting does not. The first would be an action-item in most contexts while the second would not. Of course, many conditional statements are not so clearly interpretable. After reconciling the judgments there are 416 e-mails with no action-items and 328 e-mails containing actionitems. Of the 328 e-mails containing action-items, 259 messages have one action-item segment; 55 messages have two action-item segments; 11 messages have three action-item segments. Two messages have four action-item segments, and one message has six action-item segments. Computing the sentence-level agreement using the reconciled gold standard judgments with each of the annotators individual judgments gives a kappa of 0.89 for Annotator One and a kappa of 0.92 for Annotator Two. In terms of message characteristics, there were on average 132 content tokens in the body after stripping. For action-item messages, there were 115. However, by examining Figure 2 we see the length distributions are nearly identical. As would be expected for e-mail, it is a long-tailed distribution with about half the messages having more than 60 tokens in the body (this paragraph has 65 tokens). 4.2 Classifiers For this experiment, we have selected a variety of standard text classification algorithms. In selecting algorithms, we have chosen algorithms that are not only known to work well but which differ along such lines as discriminative vs. generative and lazy vs. eager. We have done this in order to provide both a competitive and thorough sampling of learning methods for the task at hand. This is important since it is easy to improve a strawman classifier by introducing a new representation. By thoroughly sampling alternative classifier choices we demonstrate that representation improvements over bag-of-words are not due to using the information in the bag-of-words poorly. 4.2.1 kNN We employ a standard variant of the k-nearest neighbor algorithm used in text classification, kNN with s-cut score thresholding [19]. We use a tfidf-weighting of the terms with a distanceweighted vote of the neighbors to compute the score before thresholding it. In order to choose the value of s for thresholding, we perform leave-one-out cross-validation over the training set. The value of k is set to be 2( log2 N + 1) where N is the number of training points. This rule for choosing k is theoretically motivated by results which show such a rule converges to the optimal classifier as the number of training points increases [8]. In practice, we have also found it to be a computational convenience that frequently leads to comparable results with numerically optimizing k via a cross-validation procedure. 4.2.2 Na¨ıve Bayes We use a standard multinomial na¨ıve Bayes classifier [16]. In using this classifier, we smoothed word and class probabilities using a Bayesian estimate (with the word prior) and a Laplace m-estimate, respectively. 0 20 40 60 80 100 120 140 160 0 200 400 600 800 1000 1200 1400 NumberofMessages Number of Tokens All Messages Action-Item Messages 0 0.02 0.04 0.06 0.08 0.1 0.12 0.14 0.16 0.18 0.2 0 200 400 600 800 1000 1200 1400 PercentageofMessages Number of Tokens All Messages Action-Item Messages Figure 2: The Histogram (left) and Distribution (right) of Message Length. A bin size of 20 words was used. Only tokens in the body after hand-stripping were counted. After stripping, the majority of words left are usually actual message content. Classifiers Document Unigram Document Ngram Sentence Unigram Sentence Ngram F1 kNN 0.6670 ± 0.0288 0.7108 ± 0.0699 0.7615 ± 0.0504 0.7790 ± 0.0460 na¨ıve Bayes 0.6572 ± 0.0749 0.6484 ± 0.0513 0.7715 ± 0.0597 0.7777 ± 0.0426 SVM 0.6904 ± 0.0347 0.7428 ± 0.0422 0.7282 ± 0.0698 0.7682 ± 0.0451 Voted Perceptron 0.6288 ± 0.0395 0.6774 ± 0.0422 0.6511 ± 0.0506 0.6798 ± 0.0913 Accuracy kNN 0.7029 ± 0.0659 0.7486 ± 0.0505 0.7972 ± 0.0435 0.8092 ± 0.0352 na¨ıve Bayes 0.6074 ± 0.0651 0.5816 ± 0.1075 0.7863 ± 0.0553 0.8145 ± 0.0268 SVM 0.7595 ± 0.0309 0.7904 ± 0.0349 0.7958 ± 0.0551 0.8173 ± 0.0258 Voted Perceptron 0.6531 ± 0.0390 0.7164 ± 0.0376 0.6413 ± 0.0833 0.7082 ± 0.1032 Table 3: Average Document-Detection Performance during Cross-Validation for Each Method and the Sample Standard Deviation (Sn−1) in italics. The best performance for each classifier is shown in bold. 4.2.3 SVM We have used a linear SVM with a tfidf feature representation and L2-norm as implemented in the SVMlight package v6.01 [11]. All default settings were used. 4.2.4 Voted Perceptron Like the SVM, the Voted Perceptron is a kernel-based learning method. We use the same feature representation and kernel as we have for the SVM, a linear kernel with tfidf-weighting and an L2-norm. The voted perceptron is an online-learning method that keeps a history of past perceptrons used, as well as a weight signifying how often that perceptron was correct. With each new training example, a correct classification increases the weight on the current perceptron and an incorrect classification updates the perceptron. The output of the classifier uses the weights on the perceptra to make a final voted classification. When used in an offline-manner, multiple passes can be made through the training data. Both the voted perceptron and the SVM give a solution from the same hypothesis space - in this case, a linear classifier. Furthermore, it is well-known that the Voted Perceptron increases the margin of the solution after each pass through the training data [10]. Since Cohen et al. [5] obtain worse results using an SVM than a Voted Perceptron with one training iteration, they conclude that the best solution for detecting speech acts may not lie in an area with a large margin. Because their tasks are highly similar to ours, we employ both classifiers to ensure we are not overlooking a competitive alternative classifier to the SVM for the basic bag-of-words representation. 4.3 Performance Measures To compare the performance of the classification methods, we look at two standard performance measures, F1 and accuracy. The F1 measure [18, 21] is the harmonic mean of precision and recall where Precision = Correct Positives Predicted Positives and Recall = Correct Positives Actual Positives . 4.4 Experimental Methodology We perform standard 10-fold cross-validation on the set of documents. For the sentence-level approach, all sentences in a document are either entirely in the training set or entirely in the test set for each fold. For significance tests, we use a two-tailed t-test [21] to compare the values obtained during each cross-validation fold with a p-value of 0.05. Feature selection was performed using the chi-squared statistic. Different levels of feature selection were considered for each classifier. Each of the following number of features was tried: 10, 25, 50, 100, 250, 750, 1000, 2000, 4000. There are approximately 4700 unigram tokens without feature selection. In order to choose the number of features to use for each classifier, we perform nested cross-validation and choose the settings that yield the optimal document-level F1 for that classifier. For this study, only the body of each e-mail message was used. Feature selection is always applied to all candidate features. That is, for the n-gram representation, the n-grams and position features are also subject to removal by the feature selection method. 4.5 Results The results for document-level classification are given in Table 3. The primary hypothesis we are concerned with is that n-grams are critical for this task; if this is true, we expect to see a significant gap in performance between the document-level classifiers that use n-grams (denoted Document Ngram) and those using only unigram features (denoted Document Unigram). Examining Table 3, we observe that this is indeed the case for every classifier except na¨ıve Bayes. This difference in performance produced by the n-gram representation is statistically significant for each classifier except for na¨ıve Bayes and the accuracy metric for kNN (see Table 4). Na¨ıve Bayes poor performance with the n-gram representation is not surprising since the bag-of-n-grams causes excessive doublecounting as mentioned in Section 3.2.1; however, na¨ıve Bayes is not hurt at the sentence-level because the sparse examples provide few chances for agglomerative effects of double counting. In either case, when a language-modeling approach is desired, modeling the n-grams directly would be preferable to na¨ıve Bayes. More importantly for the n-gram hypothesis, the n-grams lead to the best document-level classifier performance as well. As would be expected, the difference between the sentence-level n-gram representation and unigram representation is small. This is because the window of text is so small that the unigram representation, when done at the sentence-level, implicitly picks up on the power of the n-grams. Further improvement would signify that the order of the words matter even when only considering a small sentence-size window. Therefore, the finer-grained sentence-level judgments allows a unigram representation to succeed but only when performed in a small window - behaving as an n-gram representation for all practical purposes. Document Winner Sentence Winner kNN Ngram Ngram na¨ıve Bayes Unigram Ngram SVM Ngram† Ngram Voted Perceptron Ngram† Ngram Table 4: Significance results for n-grams versus unigrams for document detection using document-level and sentence-level classifiers. When the F1 result is statistically significant, it is shown in bold. When the accuracy result is significant, it is shown with a † . F1 Winner Accuracy Winner kNN Sentence Sentence na¨ıve Bayes Sentence Sentence SVM Sentence Sentence Voted Perceptron Sentence Document Table 5: Significance results for sentence-level classifiers vs. document-level classifiers for the document detection problem. When the result is statistically significant, it is shown in bold. Further highlighting the improvement from finer-grained judgments and n-grams, Figure 3 graphically depicts the edge the SVM sentence-level classifier has over the standard bag-of-words approach with a precision-recall curve. In the high precision area of the graph, the consistent edge of the sentence-level classifier is rather impressive - continuing at precision 1 out to 0.1 recall. This would mean that a tenth of the users action-items would be placed at the top of their action-item sorted inbox. Additionally, the large separation at the top right of the curves corresponds to the area where the optimal F1 occurs for each classifier, agreeing with the large improvement from 0.6904 to 0.7682 in F1 score. Considering the relative unexplored nature of classification at the sentence-level, this gives great hope for further increases in performance. Accuracy F1 Unigram Ngram Unigram Ngram kNN 0.9519 0.9536 0.6540 0.6686 na¨ıve Bayes 0.9419 0.9550 0.6176 0.6676 SVM 0.9559 0.9579 0.6271 0.6672 Voted Perceptron 0.8895 0.9247 0.3744 0.5164 Table 6: Performance of the Sentence-Level Classifiers at Sentence Detection Although Cohen et al. [5] observed that the Voted Perceptron with a single training iteration outperformed SVM in a set of similar tasks, we see no such behavior here. This further strengthens the evidence that an alternate classifier with the bag-of-words representation could not reach the same level of performance. The Voted Perceptron classifier does improve when the number of training iterations are increased, but it is still lower than the SVM classifier. Sentence detection results are presented in Table 6. With regard to the sentence detection problem, we note that the F1 measure gives a better feel for the remaining room for improvement in this difficult problem. That is, unlike document detection where actionitem documents are fairly common, action-item sentences are very rare. Thus, as in other text problems, the accuracy numbers are deceptively high sheerly because of the default accuracy attainable by always predicting no. Although, the results here are significantly above-random, it is unclear what level of performance is necessary for sentence detection to be useful in and of itself and not simply as a means to document ranking and classification. Figure 4: Users find action-items quicker when assisted by a classification system. Finally, when considering a new type of classification task, one of the most basic questions is whether an accurate classifier built for the task can have an impact on the end-user. In order to demonstrate the impact this task can have on e-mail users, we conducted a user study using an earlier less-accurate version of the sentence classifier - where instead of using just a single sentence, a threesentence windowed-approach was used. There were three distinct sets of e-mail in which users had to find action-items. These sets were either presented in a random order (Unordered), ordered by the classifier (Ordered), or ordered by the classifier and with the 0.4 0.5 0.6 0.7 0.8 0.9 1 0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1 Precision Recall Action-Item Detection SVM Performance (Post Model Selection) Document Unigram Sentence Ngram Figure 3: Both n-grams and a small prediction window lead to consistent improvements over the standard approach. center sentence in the highest confidence window highlighted (Order+help). In order to perform fair comparisons between conditions, the overall number of tokens in each message set should be approximately equal; that is, the cognitive reading load should be approximately the same before the classifiers reordering. Additionally, users typically show practice effects by improving at the overall task and thus performing better at later message sets. This is typically handled by varying the ordering of the sets across users so that the means are comparable. While omitting further detail, we note the sets were balanced for the total number of tokens and a latin square design was used to balance practice effects. Figure 4 shows that at intervals of 5, 10, and 15 minutes, users consistently found significantly more action-items when assisted by the classifier, but were most critically aided in the first five minutes. Although, the classifier consistently aids the users, we did not gain an additional end-user impact by highlighting. As mentioned above, this might be a result of the large room for improvement that still exists for sentence detection, but anecdotal evidence suggests this might also be a result of how the information is presented to the user rather than the accuracy of sentence detection. For example, highlighting the wrong sentence near an actual action-item hurts the users trust, but if a vague indicator (e.g., an arrow) points to the approximate area the user is not aware of the near-miss. Since the user studies used a three sentence window, we believe this played a role as well as sentence detection accuracy. 4.6 Discussion In contrast to problems where n-grams have yielded little difference, we believe their power here stems from the fact that many of the meaningful n-grams for action-items consist of common words, e.g., let me know. Therefore, the document-level unigram approach cannot gain much leverage, even when modeling their joint probability correctly, since these words will often co-occur in the document but not necessarily in a phrase. Additionally, action-item detection is distinct from many text classification tasks in that a single sentence can change the class label of the document. As a result, good classifiers cannot rely on aggregating evidence from a large number of weak indicators across the entire document. Even though we discarded the header information, examining the top-ranked features at the document-level reveals that many of the features are names or parts of e-mail addresses that occurred in the body and are highly associated with e-mails that tend to contain many or no action-items. A few examples are terms such as org, bob, and gov. We note that these features will be sensitive to the particular distribution (senders/receivers) and thus the document-level approach may produce classifiers that transfer less readily to alternate contexts and users at different institutions. This points out that part of the problem of going beyond bag-of-words may be the methodology, and investigating such properties as learning curves and how well a model transfers may highlight differences in models which appear to have similar performance when tested on the distributions they were trained on. We are currently investigating whether the sentence-level classifiers do perform better over different test corpora without retraining. 5. FUTURE WORK While applying text classifiers at the document-level is fairly well-understood, there exists the potential for significantly increasing the performance of the sentence-level classifiers. Such methods include alternate ways of combining the predictions over each sentence, weightings other than tfidf, which may not be appropriate since sentences are small, better sentence segmentation, and other types of phrasal analysis. Additionally, named entity tagging, time expressions, etc., seem likely candidates for features that can further improve this task. We are currently pursuing some of these avenues to see what additional gains these offer. Finally, it would be interesting to investigate the best methods for combining the document-level and sentence-level classifiers. Since the simple bag-of-words representation at the document-level leads to a learned model that behaves somewhat like a context-specific prior dependent on the sender/receiver and general topic, a first choice would be to treat it as such when combining probability estimates with the sentence-level classifier. Such a model might serve as a general example for other problems where bag-of-words can establish a baseline model but richer approaches are needed to achieve performance beyond that baseline. 6. SUMMARY AND CONCLUSIONS The effectiveness of sentence-level detection argues that labeling at the sentence-level provides significant value. Further experiments are needed to see how this interacts with the amount of training data available. Sentence detection that is then agglomerated to document-level detection works surprisingly better given low recall than would be expected with sentence-level items. This, in turn, indicates that improved sentence segmentation methods could yield further improvements in classification. In this work, we examined how action-items can be effectively detected in e-mails. Our empirical analysis has demonstrated that n-grams are of key importance to making the most of documentlevel judgments. When finer-grained judgments are available, then a standard bag-of-words approach using a small (sentence) window size and automatic segmentation techniques can produce results almost as good as the n-gram based approaches. Acknowledgments This material is based upon work supported by the Defense Advanced Research Projects Agency (DARPA) under Contract No. NBCHD030010. Any opinions, findings and conclusions or recommendations expressed in this material are those of the author(s) and do not necessarily reflect the views of the Defense Advanced Research Projects Agency (DARPA), or the Department of InteriorNational Business Center (DOI-NBC). We would like to extend our sincerest thanks to Jill Lehman whose efforts in data collection were essential in constructing the corpus, and both Jill and Aaron Steinfeld for their direction of the HCI experiments. We would also like to thank Django Wexler for constructing and supporting the corpus labeling tools and Curtis Huttenhowers support of the text preprocessing package. Finally, we gratefully acknowledge Scott Fahlman for his encouragement and useful discussions on this topic. 7. REFERENCES [1] J. Allan, J. Carbonell, G. Doddington, J. Yamron, and Y. Yang. Topic detection and tracking pilot study: Final report. In Proceedings of the DARPA Broadcast News Transcription and Understanding Workshop, Washington, D.C., 1998. [2] C. Apte, F. Damerau, and S. M. Weiss. Automated learning of decision rules for text categorization. ACM Transactions on Information Systems, 12(3):233-251, July 1994. [3] J. Carletta. Assessing agreement on classification tasks: The kappa statistic. Computational Linguistics, 22(2):249-254, 1996. [4] J. Carroll. High precision extraction of grammatical relations. In Proceedings of the 19th International Conference on Computational Linguistics (COLING), pages 134-140, 2002. [5] W. W. Cohen, V. R. Carvalho, and T. M. Mitchell. Learning to classify email into speech acts. In EMNLP-2004 (Conference on Empirical Methods in Natural Language Processing), pages 309-316, 2004. [6] S. Corston-Oliver, E. Ringger, M. Gamon, and R. Campbell. Task-focused summarization of email. In Text Summarization Branches Out: Proceedings of the ACL-04 Workshop, pages 43-50, 2004. [7] A. Culotta, R. Bekkerman, and A. McCallum. Extracting social networks and contact information from email and the web. In CEAS-2004 (Conference on Email and Anti-Spam), Mountain View, CA, July 2004. [8] L. Devroye, L. Gy¨orfi, and G. Lugosi. A Probabilistic Theory of Pattern Recognition. Springer-Verlag, New York, NY, 1996. [9] S. T. Dumais, J. Platt, D. Heckerman, and M. Sahami. Inductive learning algorithms and representations for text categorization. In CIKM 98, Proceedings of the 7th ACM Conference on Information and Knowledge Management, pages 148-155, 1998. [10] Y. Freund and R. Schapire. Large margin classification using the perceptron algorithm. Machine Learning, 37(3):277-296, 1999. [11] T. Joachims. Making large-scale svm learning practical. In B. Sch¨olkopf, C. J. Burges, and A. J. Smola, editors, Advances in Kernel Methods - Support Vector Learning, pages 41-56. MIT Press, 1999. [12] L. S. Larkey. A patent search and classification system. In Proceedings of the Fourth ACM Conference on Digital Libraries, pages 179 - 187, 1999. [13] D. D. Lewis. An evaluation of phrasal and clustered representations on a text categorization task. In SIGIR 92, Proceedings of the 15th Annual International ACM Conference on Research and Development in Information Retrieval, pages 37-50, 1992. [14] Y. Liu, J. Carbonell, and R. Jin. A pairwise ensemble approach for accurate genre classification. In Proceedings of the European Conference on Machine Learning (ECML), 2003. [15] Y. Liu, R. Yan, R. Jin, and J. Carbonell. A comparison study of kernels for multi-label text classification using category association. In The Twenty-first International Conference on Machine Learning (ICML), 2004. [16] A. McCallum and K. Nigam. A comparison of event models for naive bayes text classification. In Working Notes of AAAI 98 (The 15th National Conference on Artificial Intelligence), Workshop on Learning for Text Categorization, pages 41-48, 1998. TR WS-98-05. [17] F. Sebastiani. Machine learning in automated text categorization. ACM Computing Surveys, 34(1):1-47, March 2002. [18] C. J. van Rijsbergen. Information Retrieval. Butterworths, London, 1979. [19] Y. Yang. An evaluation of statistical approaches to text categorization. Information Retrieval, 1(1/2):67-88, 1999. [20] Y. Yang, J. Carbonell, R. Brown, T. Pierce, B. T. Archibald, and X. Liu. Learning approaches to topic detection and tracking. IEEE EXPERT, Special Issue on Applications of Intelligent Information Retrieval, 1999. [21] Y. Yang and X. Liu. A re-examination of text categorization methods. In SIGIR 99, Proceedings of the 22nd Annual International ACM Conference on Research and Development in Information Retrieval, pages 42-49, 1999. [22] Y. Yang, J. Zhang, J. Carbonell, and C. Jin. Topic-conditioned novelty detection. In Proceedings of the ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, July 2002.",
    "original_translation": "Representación de características para la detección efectiva de acción-ítem Paul N. Bennett Departamento de Ciencias de la Computación Carnegie Mellon University Pittsburgh, PA 15213 pbennett+@cs.cmu.edu Jaime Carbonell Language Technologies Institu. Universidad de Carnegie Mellon Pittsburgh, PA 15213 jgc+@cs.cmu.edu Los usuarios de correo electrónico abstracto enfrentan un desafío cada vez mayor en la administración de sus bandejas de entrada debido a la creciente centralidad del correo electrónico en el lugar de trabajo para la asignación de tareas, las solicitudes de acción y otras funciones más alládiseminacion de informacion. Mientras que las técnicas de recuperación de información y aprendizaje automático están obteniendo aceptación inicial en el filtrado de spam y la asignación de carpetas automatizadas, este documento informa sobre una nueva tarea: detección automatizada de elementos de acción, para marcar correos electrónicos que requieren respuestas, y para resaltar el pasaje específico (s) indicando la (s) solicitud (s) de acción. A diferencia de la clasificación de texto estándar basada en el tema, la detección de mensajes de acción requiere inferir la intención de los remitentes, y como tal, responde menos bien a la clasificación pura de la bolsa de palabras. Sin embargo, el uso de conjuntos de características enriquecidos, como N-grams (hasta n = 4) con selección de características chi cuadrado, y las señales contextuales para la ubicación de acción de acción mejoran el rendimiento hasta un 10% sobre unigramas, utilizando en ambos casos de estado deLos clasificadores de ART, como SVM con selección automatizada de modelos a través de validación cruzada integrada. Categorías y descriptores de sujetos H.3.3 [Almacenamiento y recuperación de información]: Búsqueda y recuperación de información;I.2.6 [Inteligencia artificial]: aprendizaje;I.5.4 [Reconocimiento de patrones]: Aplicaciones Términos generales Experimentación 1. Introducción Los usuarios de correo electrónico enfrentan una tarea cada vez más difícil de administrar sus bandejas de entrada frente a los crecientes desafíos que resultan del aumento del uso del correo electrónico. Esto incluye priorizar los correos electrónicos sobre una variedad de fuentes, desde socios comerciales hasta miembros de la familia, filtrar y reducir el correo electrónico de basura, y administrar rápidamente las solicitudes que demandan de: Henry Hutchins <hhutchins@innovative.comPany.com> a: Sara Smith;Joe Johnson;William Woolings Asunto: Reunión con posibles clientes enviados: Vie 10/10/2005 8:08 AM Hola a todos, me gustaría recordarles a todos que el grupo de Grty nos visitará el próximo viernes a las 4:30 p.m. El horario actual se ve así: + 9:30 a.m. Desayuno informal y discusión en cafetería + 10:30 a.m. Descripción general de la compañía + 11:00 a.m. Reuniones individuales (continuar durante el almuerzo) + 2:00 p.m.Tour de instalaciones + 3:00 p.m. Argumento de venta Para que esto se apague sin problemas, me gustaría practicar la presentación con mucha anticipación. Como resultado, necesitaré cada una de sus piezas para el miércoles. ¡Sigan con el buen trabajo!-Henry Figura 1: Un correo electrónico con ítem de acción enfatizada, una solicitud explícita que requiere la atención o acción de los destinatarios.la atención o acción de los receptores. La detección automatizada de información de acción se dirige al tercio de estos problemas al intentar detectar qué correos electrónicos requieren una acción o respuesta con información, y dentro de esos correos electrónicos, intentando resaltar la oración (u otra longitud de pasaje) que indica directamente la acciónpedido. Tal sistema de detección se puede usar como una parte de un agente de correo electrónico que ayudaría a un usuario a procesar correos electrónicos importantes más rápido de lo que hubiera sido posible sin el agente. Vemos la detección de elementos de acción como un componente necesario de un agente de correo electrónico exitoso que realizaría detección de spam, detección de elementos de acción, clasificación de temas y clasificación de prioridad, entre otras funciones. La utilidad de dicho detector puede manifestarse como un método para priorizar los correos electrónicos de acuerdo con los criterios orientados a las tareas distintos de los estándares del tema y el remitente o como un medio para garantizar que el usuario del correo electrónico no haya dejado caer la pelota proverbial al olvidar abordar abordaruna solicitud de acción. La detección de ítems de acción difiere de la clasificación de texto estándar de dos maneras importantes. Primero, el usuario está interesado tanto en detectar si un correo electrónico contiene elementos de acción y en localizar exactamente dónde están contenidas estas solicitudes de elementos de acción en el cuerpo de correo electrónico. Por el contrario, la categorización de texto estándar simplemente asigna una etiqueta de tema a cada texto, si esa etiqueta corresponde a una carpeta de correo electrónico o un vocabulario de indexación controlado [12, 15, 22]. En segundo lugar, la detección de acción de acción intenta recuperar la intención de los remitentes de correo electrónico, ya sea que se refiera a obtener respuesta o acción por parte del receptor;Tenga en cuenta que para esta tarea, los clasificadores que usan solo unigramas como características no funcionan de manera óptima, como se evidencia en nuestros resultados a continuación. En su lugar, encontramos que necesitamos más funciones cargadas de información, como N-Grams de orden superior. La categorización de texto por tema, por otro lado, funciona muy bien usando solo palabras individuales como características [2, 9, 13, 17]. De hecho, la clasificación de género, que uno pensaría que puede requerir más que un enfoque de la bolsa de palabras, también funciona bastante bien utilizando solo características unigram [14]. Detección y seguimiento de temas (TDT), también funciona bien con conjuntos de características unigram [1, 20]. Creemos que la detección de acción de acción es una de las primeras instancias claras de una tarea relacionada con IR en la que debemos ir más allá de las palabras de las palabras para lograr un alto rendimiento, aunque no muy lejos, ya que parecesatisfacer. Primero revisamos el trabajo relacionado para problemas de clasificación de texto similares, como la clasificación de prioridad de correo electrónico y la identificación de la Ley del habla. Luego, definimos más formalmente el problema de detección de elementos de acción, discutimos los aspectos que lo distinguen de problemas más comunes como la clasificación de temas, y resaltan los desafíos en la construcción de sistemas que pueden funcionar bien a nivel de oración y documento. A partir de ahí, pasamos a una discusión sobre la representación de características y las técnicas de selección apropiadas para este problema y cómo los enfoques de clasificación de texto estándar pueden adaptarse para pasar sin problemas del problema de detección de nivel de oración al problema de clasificación de nivel de documento. Luego realizamos un análisis empírico que nos ayuda a determinar la efectividad de nuestros procedimientos de extracción de características, así como a establecer líneas de base para una serie de algoritmos de clasificación en esta tarea. Finalmente, resumimos las contribuciones de estos documentos y consideramos direcciones interesantes para el trabajo futuro.2. Trabajo relacionado Varios otros investigadores han considerado tareas de clasificación de texto muy similares. Cohen et al.[5] Describa una ontología de los actos del habla, como proponer una reunión, e intentar predecir cuándo un correo electrónico contiene una de estas actos de habla. Consideramos que los elementos de acción son un tipo de habla específico importante que cae dentro de su clasificación más general. Si bien proporcionan resultados para varios métodos de clasificación, sus métodos solo hacen uso de juicios humanos en el nivel de documento. Por el contrario, consideramos si la precisión se puede aumentar mediante el uso de juicios humanos de grano más fino que marcan las oraciones y frases específicas de interés. Corston-Oliver et al.[6] Considere detectar elementos en el correo electrónico para poner en una lista de tareas pendientes. Esta tarea de clasificación es muy similar a la nuestra, excepto que no consideran que las preguntas objetivas simples pertenezcan a esta categoría. Incluimos preguntas, pero tenga en cuenta que no todas las preguntas son elementos de acción: algunas son una convención retórica o simplemente social, ¿cómo estás? Desde una perspectiva de aprendizaje, mientras utilizan juicios en el nivel de oración, no comparan explícitamente qué si algún beneficio ofrece los juicios de grano más finos. Además, no estudian opciones o enfoques alternativos para la tarea de clasificación. En cambio, simplemente aplican una SVM estándar en el nivel de oración y se centran principalmente en un análisis lingüístico de cómo la oración puede reformularse lógicamente antes de agregarla a la lista de tareas. En este estudio, examinamos varios métodos de clasificación alternativos, comparamos los enfoques a nivel de documento y a nivel de oración y analizamos los problemas de aprendizaje automático implícitos en estos problemas. El interés en una variedad de tareas de aprendizaje relacionadas con el correo electrónico ha estado creciendo rápidamente en la literatura reciente. Por ejemplo, en un foro dedicado a tareas de aprendizaje por correo electrónico, Culotta et al.[7] presentaron métodos para aprender redes sociales del correo electrónico. En este trabajo, no nos centramos en las relaciones entre pares;Sin embargo, tales métodos podrían complementar los aquí, ya que las relaciones entre pares a menudo influyen en la elección de palabras al solicitar una acción.3. Definición y enfoque del problema En contraste con el trabajo previo, nos centramos explícitamente en los beneficios que los juicios humanos a nivel de oración más costosos ofrecen los juicios de nivel de documentos de grano grueso. Además, consideramos múltiples enfoques de clasificación de texto estándar y analizamos las diferencias cuantitativas y cualitativas que surgen al tomar un nivel de documento frente a un enfoque a nivel de oración para la clasificación. Finalmente, nos centramos en la representación necesaria para lograr el rendimiento más competitivo.3.1 Definición del problema Para proporcionar el mayor beneficio al usuario, un sistema no solo detectaría el documento, sino que también indicaría las oraciones específicas en el correo electrónico que contienen los elementos de acción. Por lo tanto, hay tres problemas básicos: 1. Detección de documentos: Clasifique un documento sobre si contiene o no un Item de acción.2. Ranking de documentos: clasifique los documentos de tal manera que todos los documentos que contienen esmeriles de acción ocurren lo más alto posible en la clasificación.3. Detección de oraciones: clasifique cada oración en un documento sobre si es o no un elemento de acción. Como en la mayoría de las tareas de recuperación de información, el peso que la métrica de evaluación debe dar a la precisión y el recuerdo depende de la naturaleza de la aplicación. En situaciones en las que un usuario eventualmente leerá todos los mensajes recibidos, la clasificación (por ejemplo, a través de la precisión en el retiro de 1) puede ser lo más importante ya que esto ayudará a fomentar retrasos más cortos en las comunicaciones entre los usuarios. Por el contrario, la detección de alta precisión en un recuerdo bajo será de creciente importancia cuando el usuario esté bajo presión de tiempo severa y, por lo tanto, probablemente no lea todo el correo. Este puede ser el caso de los gerentes de crisis durante la gestión de desastres. Finalmente, la detección de oraciones juega un papel en las situaciones de tiempo de tiempo y simplemente para aliviar los usuarios requirió el tiempo para inyectar el mensaje.3.2 Enfoque Como se mencionó anteriormente, los datos etiquetados pueden venir en una de dos formas: un marcado de documentos proporciona una etiqueta sí/no para cada documento sobre si contiene un elemento de acción;Un marcado de frases proporciona solo una etiqueta de sí para los elementos específicos de interés. Aficionamos a los juicios humanos un marcado de frases ya que la vista de los usuarios del ítem de acción puede no corresponder con los límites de oraciones reales o los límites de oración predichos. Obviamente, es sencillo generar un marcado de documentos consistente con un marcado de frases etiquetando un documento sí si y solo si contiene al menos una frase etiquetada por sí. Para entrenar clasificadores para esta tarea, podemos tomar varios puntos de vista relacionados con los problemas básicos que hemos enumerado como la forma de los datos etiquetados. La vista a nivel de documento trata cada correo electrónico como una instancia de aprendizaje con una etiqueta de clase asociada. Luego, el documento se puede convertir a un vector de valor de características y el aprendizaje progresa como de costumbre. La aplicación de un clasificador a nivel de documento para la detección y clasificación de documentos es sencillo. Para aplicarlo a la detección de oraciones, uno debe hacer pasos adicionales. Por ejemplo, si el clasificador predice un documento contiene un elemento de acción, entonces las áreas del documento que contienen una alta concentración de palabras que el modelo pesa en favor de los elementos de acción. El beneficio obvio del enfoque a nivel de documento es que los costos de recolección del conjunto de capacitación son más bajos ya que el usuario solo tiene que especificar si un correo electrónico contiene o no un elemento de acción y no las oraciones específicas. En la vista a nivel de oración, cada correo electrónico se segmenta automáticamente en oraciones, y cada oración se trata como una instancia de aprendizaje con una etiqueta de clase asociada. Dado que el marcado de frases proporcionado por el usuario puede no coincidir con la segmentación automática, debemos determinar qué etiqueta asignar una oración parcialmente superpuesta al convertirla a una instancia de aprendizaje. Una vez capacitado, la aplicación de los clasificadores resultantes a la detección de oraciones ahora es sencilla, pero para aplicar los clasificadores a la detección de documentos y la clasificación de documentos, las predicciones individuales sobre cada oración deben agregarse para hacer una predicción a nivel de documento. Este enfoque tiene el potencial de beneficiarse de las etiquetas Morespecíficas que permiten al alumno centrar la atención en las oraciones clave en lugar de tener que aprender en base a los datos que la mayoría de las palabras en el correo electrónico proporcionan no o poca información sobre la membresía de la clase.3.2.1 Características Considere algunas de las frases que podrían constituir parte de un elemento de acción: me gustaría saber, avíseme, lo antes posible, lo tenga. Cada una de estas frases consiste en palabras comunes que ocurren en muchos correos electrónicos. Sin embargo, cuando ocurren en la misma oración, son mucho más indicativos de un elemento de acción. Además, el orden puede ser importante: considere tenerlo frente a usted. Debido a esto, postulamos que los n-gramos juegan un papel más importante en este problema que típico de problemas como la clasificación de temas. Por lo tanto, consideramos todos los N-gramos hasta el tamaño 4. Cuando usamos N-Grams, si encontramos un N-gramo de tamaño 4 en un segmento de texto, podemos representar el texto como solo una aparición del ngram o como una ocurrencia del N-gramo y una ocurrencia de cada N más pequeño N-gram contenido por él. Elegimos la segunda de estas alternativas, ya que esto permitirá que el algoritmo mismo retroceda sin problemas en términos de retiro. Métodos como Na¨ıve Bayes pueden verse afectados por tal representación debido a la doble conteo. Dado que la puntuación que termina las oraciones puede proporcionar información, conservamos el token de puntuación de terminación cuando es identificable. Además, agregamos un token de comienzo y fin de oración para capturar patrones que a menudo son indicadores al principio o al final de una oración. Suponiendo la puntuación adecuada, estos tokens adicionales son innecesarios, pero a menudo el correo electrónico carece de puntuación adecuada. Además, para los clasificadores a nivel de oración que usan NGRAM, además codificamos para cada oración una codificación binaria de la posición de la oración en relación con el documento. Esta codificación tiene ocho características asociadas que representan qué octual (el primer octavo, segundo octavo, etc.) contiene la oración.3.2.2 Detalles de implementación Para comparar el nivel de documento con el enfoque a nivel de oración, comparamos las predicciones en el nivel de documento. No abordamos cómo usar un clasificador a nivel de documento para hacer predicciones a nivel de oración. Para segmentar automáticamente el texto del correo electrónico, utilizamos el analizador estadístico RASP [4]. Dado que las oraciones segmentadas automáticamente pueden no corresponder directamente con los límites de nivel de frase, tratamos cualquier oración que contenga al menos el 30% de un segmento de acción de acción marcado como un elemento de acción. Al evaluar la detección de sentencia para el sistema a nivel de oración, utilizamos estas etiquetas de clase como verdad terrestre. Dado que no estamos evaluando múltiples enfoques de segmentación, esto no sesga a ninguno de los métodos. Si se estuvieran evaluando múltiples sistemas de segmentación, uno necesitaría usar una métrica que coincida con oraciones positivas predichas con frases etiquetadas como positivas. La métrica necesitaría castigar a las predicciones verdaderas demasiado largas, así como predicciones demasiado cortas. Nuestros criterios para convertir en instancias etiquetadas incluyen implícitamente ambos criterios. Dado que la segmentación es fija, una predicción demasiado larga predeciría que sí para muchos casos no, ya que presumiblemente la longitud adicional corresponde a oraciones segmentadas adicionales, todas las cuales no contienen el 30% del elemento de acción. Del mismo modo, una predicción demasiado corta debe corresponder a una pequeña oración incluida en el ítem de acción pero no constituir toda la acción-ítem. Por lo tanto, para considerar que la predicción es demasiado corta, habrá una oración anterior anterior/siguiente que es un elemento de acción donde predijimos incorrectamente no. Una vez que un clasificador a nivel de oración ha hecho una predicción para cada oración, debemos combinar estas predicciones para hacer una predicción a nivel de documento y una puntuación a nivel de documento. Utilizamos la política simple de predecir positivo cuando cualquiera de las oraciones se predice positivas. Para producir una puntuación de documento para la clasificación, la confianza de que el documento contiene un elemento de acción es: ψ (d) = 1 n (d) s∈D | π (s) = 1 ψ (s) si es para cualquier S∈ D, π (s) = 1 1 n (d) maxs∈D ψ (s) O.W.donde s es una oración en el documento d, π es la predicción de clasificadores 1/0, ψ es la puntuación que el clasificador asigna como su confianza en que π (s) = 1 y n (d) es el mayor de 1 y el número de(Unigram) Tokens en el documento. En otras palabras, cuando cualquier oración se predice positiva, la puntuación del documento es la suma de longitud normalizada de los puntajes de la oración por encima del umbral. Cuando no se predice positivo, la puntuación del documento es el puntaje de oración máximo normalizado por longitud. Como en otros problemas de texto, es más probable que emitamos falsos positivos para documentos con más palabras o oraciones. Por lo tanto, incluimos un factor de normalización de longitud.4. Análisis experimental 4.1 Los datos que nuestro corpus consiste en correos electrónicos obtenidos de voluntarios en una institución educativa y cubren temas como: organizar un taller de investigación, organizar entrevistas con candidatos laborales, procedimientos de publicación y anuncios de conversaciones. Los mensajes se anonimizaron reemplazando los nombres de cada individuo e institución con una seudonía. Después del anonimato de identidad, los corpus tienen tres versiones básicas. El material citado se refiere al texto de un correo electrónico anterior que un autor a menudo deja en un mensaje de correo electrónico al responder al correo electrónico. El material citado puede actuar como ruido cuando se aprende, ya que puede incluir elementos de acción de mensajes anteriores que ya no son relevantes. Para aislar los efectos del material citado, tenemos tres versiones de los corpus. La forma sin procesar contiene los mensajes básicos. La versión de salto automático contiene los mensajes después de que el material citado se haya eliminado automáticamente. La versión salpicada a mano contiene los mensajes después de que un humano haya eliminado el material citado. Además, la versión salpicada a mano ha tenido cualquier contenido XML y firmas de correo electrónico, dejando solo el contenido esencial del mensaje. Los estudios informados aquí se realizan con la versión salpicada a mano. Esto nos permite equilibrar la carga cognitiva en términos de número de tokens que deben leerse en los estudios de usuario que informamos; incluido el material citado, complicaría los estudios de usuarios ya que algunos usuarios podrían omitir el material mientras que otros lo leen. Además, asegurando que se elimine todo el material citado 1, tenemos una versión aún más anonimizada del corpus que puede estar disponible para alguna experimentación externa. Comuníquese con los autores para obtener más información sobre cómo obtener estos datos.evita que la validación cruzada, ya que de lo contrario, un elemento de prueba podría ocurrir como material citado en un documento de capacitación.4.1.1 Etiquetado de datos Dos anotadores humanos etiquetaron cada mensaje en cuanto a si contenía o no un elemento de acción. Además, identificaron cada segmento del correo electrónico que contenía un ítem de acción. Un segmento es una sección contigua de texto seleccionada por los anotadores humanos y puede abarcar varias oraciones o una frase completa contenida en una oración. Se les indicó que un elemento de acción es una solicitud explícita de información que requiere la atención de los destinatarios o una acción requerida y se les dijo que resalte las frases o oraciones que componen la solicitud. Anotador 1 No YES Annotator 2 No 391 26 Sí 29 298 Tabla 1: Acuerdo de anotadores humanos a nivel de documento Anotador Uno etiquetado 324 Mensajes como elementos de acción. Anotador dos etiquetados 327 mensajes como elementos de acción. El acuerdo de los anotadores humanos se muestra en las Tablas 1 y 2. Se dice que los anotadores están de acuerdo en el nivel del documento cuando ambos marcaron el mismo documento que no contenían elementos de acción o ambos marcados al menos un ítem de acción, independientemente de si los segmentos de texto eran los mismos. A nivel de documento, los anotadores acordaron el 93% del tiempo. La estadística de Kappa [3, 5] a menudo se usa para evaluar el acuerdo entre anotador: κ = a-r 1-r a es la estimación empírica de la probabilidad de acuerdo. R es la estimación empírica de la probabilidad de acuerdo aleatorio dada la clase empírica Priors. Un valor cercano a −1 implica que los anotadores están de acuerdo con mucha menos frecuencia de lo esperado al azar, mientras que un valor cercano a 1 significa que están de acuerdo con más frecuencia de lo que se esperaba al azar. En el nivel del documento, la estadística de Kappa para el acuerdo entre annotador es 0.85. Este valor es lo suficientemente fuerte como para esperar que el problema sea aprendible y es comparable con los resultados para tareas similares [5, 6]. Para determinar el acuerdo a nivel de oración, utilizamos cada sentencia para crear una oración-Corpus con etiquetas como se describe en la Sección 3.2.2, luego considere el acuerdo sobre estas oraciones. Esto nos permite comparar el acuerdo sobre ningún juicio. Realizamos esta comparación sobre el corpus salpicado a mano, ya que eso elimina los juicios de NO espurios que provienen de incluir material citado, etc. Ambos anotadores eran libres de etiquetar al sujeto como un elemento de acción, pero como tampoco, también omitimos la línea de asunto del mensaje. Esto solo reduce el número de acuerdos no. Esto deja 6301 oraciones segmentadas automáticamente. A nivel de oración, los anotadores acordaron el 98% del tiempo, y la estadística de Kappa para el Acuerdo entre Annotador es 0.82. Para producir un solo conjunto de juicios, los anotadores humanos pasaron por cada anotación donde hubo desacuerdo y llegaron a una opinión de consenso. Los anotadores no recopilaron estadísticas durante este proceso, pero anecdóticamente informaron que la mayoría de los desacuerdos eran casos de supervisión clara del anotador o diferentes interpretaciones de las declaraciones condicionales. Por ejemplo, si desea mantener su trabajo, llegar a la reunión de mañana implica una acción requerida en la que si desea unirse al anotador 1 no sí anotador 2 no 5810 65 sí 74 352 Tabla 2: Acuerdo de anotadores humanos a nivel de oración elLa piscina de apuestas de fútbol, Ven a la reunión de mañana no. El primero sería un ítem de acción en la mayoría de los contextos, mientras que el segundo no. Por supuesto, muchas declaraciones condicionales no son tan claramente interpretables. Después de conciliar los juicios, hay 416 correos electrónicos sin ítems de acción y 328 correos electrónicos que contienen ActionItems. De los 328 correos electrónicos que contienen elementos de acción, 259 mensajes tienen un segmento de acción de acción;55 mensajes tienen dos segmentos de acción de acción;11 mensajes tienen tres segmentos de acción de acción. Dos mensajes tienen cuatro segmentos de acción de acción, y un mensaje tiene seis segmentos de acción de acción. Calcular el acuerdo a nivel de oración utilizando los juicios estándar de oro reconciliados con cada uno de los juicios individuales de los anotadores ofrece un kappa de 0.89 para el anotador uno y un kappa de 0.92 para el anotador dos. En términos de características del mensaje, había en promedio 132 tokens de contenido en el cuerpo después de desnudar. Para los mensajes de acción de acción, hubo 115. Sin embargo, al examinar la Figura 2, vemos que las distribuciones de longitud son casi idénticas. Como se esperaría para el correo electrónico, es una distribución de cola larga con aproximadamente la mitad de los mensajes que tienen más de 60 tokens en el cuerpo (este párrafo tiene 65 tokens).4.2 Clasificadores para este experimento, hemos seleccionado una variedad de algoritmos de clasificación de texto estándar. Al seleccionar algoritmos, hemos elegido algoritmos que no solo se sabe que funcionan bien, sino que difieren en líneas como discriminativos versus generativos y perezosos versus ansiosos. Hemos hecho esto para proporcionar un muestreo competitivo y completo de los métodos de aprendizaje para la tarea en cuestión. Esto es importante ya que es fácil mejorar un clasificador de Strawman mediante la introducción de una nueva representación. Al probar a fondo las opciones de clasificadores alternativos, demostramos que las mejoras de representación sobre la bolsa de palabras no se deben al uso de la información en la bolsa de palabras mal.4.2.1 KNN Empleamos una variante estándar del algoritmo vecino K-Nearest utilizado en la clasificación de texto, KNN con el umbral de puntaje de corte S [19]. Utilizamos un aumento de los términos de los términos con un voto de la distancia conmovidos de los vecinos para calcular el puntaje antes de umbrarlo. Para elegir el valor de S para el umbral, realizamos una validación cruzada de baja uno sobre el conjunto de entrenamiento. El valor de K se establece en 2 (log2 n + 1) donde n es el número de puntos de entrenamiento. Esta regla para elegir K está teóricamente motivada por resultados que muestran que tal regla converge al clasificador óptimo a medida que aumenta el número de puntos de entrenamiento [8]. En la práctica, también hemos encontrado que es una conveniencia computacional que con frecuencia conduce a resultados comparables con optimización numérica K a través de un procedimiento de validación cruzada.4.2.2 Na¨ıve Bayes Utilizamos un clasificador multinomial Na¨ıve Bayes estándar [16]. Al usar este clasificador, suavizamos las probabilidades de palabras y clases utilizando una estimación bayesiana (con la palabra anterior) y una condima m de Laplace, respectivamente.0 20 40 60 80 100 120 140 160 0 200 400 600 800 1000 1200 1400 Número deMessages Número de tokens Todos los mensajes Mensajes de acción de acción 0 0.02 0.04 0.06 0.08 0.1 0.12 0.14 0.16 0.18 0.2 0 200 400 600 800 1000 1200 1400 Porcentaje de mensajes Número de tokens de tokensTodos los mensajes Mensajes de acción de acción Figura 2: El histograma (izquierda) y la distribución (derecha) de la longitud del mensaje. Se usó un tamaño de contenedor de 20 palabras. Solo se contaron las fichas en el cuerpo después de la mano. Después de desnudar, la mayoría de las palabras que quedan suelen ser contenido de mensaje real. Clasificadores Documento Documento unigram Ngram oración Unigram oración ngram f1 knn 0.6670 ± 0.0288 0.7108 ± 0.0699 0.7615 ± 0.0504 0.7790 ± 0.0460 na¨ıve bayes 0.6572 ± 0.0749 0.6484 ± 0.05513 0.7715 ± SVM 0.6904 ± 0.0347 0.7428 ± 0.0422 0.7282 ± 0.0698 0.7682± 0.0451 Perceptron votado 0.6288 ± 0.0395 0.6774 ± 0.0422 0.6511 ± 0.0506 0.6798 ± 0.0913 Precisión KNN 0.7029 ± 0.0659 0.7486 ± 0.0505 0.7972 ± 0.0435 0.80922 ± 0.0752. 0651 0.5816 ± 0.1075 0.7863 ± 0.0553 0.8145 ± 0.0268 SVM 0.7595 ± 0.03090.7904 ± 0.0349 0.7958 ± 0.0551 0.8173 ± 0.0258 Perceptron votado 0.6531 ± 0.0390 0.7164 ± 0.0376 0.6413 ± 0.0833 0.7082 ± 0.1032 Tabla 3: rendimiento de documento promedio de la detección de documentos durante la cross-validación para la cross-validación de la cross de la cross de la cross de la cross.. El mejor rendimiento para cada clasificador se muestra en negrita.4.2.3 SVM Hemos utilizado un SVM lineal con una representación de características TFIDF y la norma L2 como se implementa en el paquete SVMLight V6.01 [11]. Se utilizaron todas las configuraciones predeterminadas.4.2.4 Perceptron votado como el SVM, el Perceptron votado es un método de aprendizaje basado en el núcleo. Utilizamos la misma representación de características y kernel que lo hemos hecho para el SVM, un núcleo lineal con peso TFIDF y una norma L2. El Perceptron votado es un método de aprendizaje en línea que mantiene un historial de perceptrones pasados utilizados, así como un peso que significa con qué frecuencia ese perceptrón era correcto. Con cada nuevo ejemplo de entrenamiento, una clasificación correcta aumenta el peso en el Perceptron actual y una clasificación incorrecta actualiza el Perceptron. La salida del clasificador utiliza los pesos en el Perceptra para hacer una clasificación votada final. Cuando se usa en una maniña fuera de línea, se pueden realizar múltiples pases a través de los datos de entrenamiento. Tanto el perceptrón votado como el SVM dan una solución del mismo espacio de hipótesis, en este caso, un clasificador lineal. Además, es bien sabido que el perceptrón votado aumenta el margen de la solución después de cada pase a través de los datos de entrenamiento [10]. Desde Cohen et al.[5] Obtenga peores resultados utilizando un SVM que un perceptrón votado con una iteración de entrenamiento, concluyen que la mejor solución para detectar actos de habla puede no estar en un área con un gran margen. Debido a que sus tareas son muy similares a las nuestras, empleamos ambos clasificadores para asegurarnos de que no vaya a pasar por alto un clasificador alternativo competitivo para el SVM para la representación básica de la bolsa de palabras.4.3 Medidas de rendimiento Para comparar el rendimiento de los métodos de clasificación, observamos dos medidas de rendimiento estándar, F1 y precisión. La medida F1 [18, 21] es la media armónica de precisión y recuerdo donde la precisión = positivos correctos predijeron positivos y recuerdo = positivos correctos positivos reales.4.4 Metodología experimental Realizamos una validación cruzada de 10 veces estándar en el conjunto de documentos. Para el enfoque a nivel de oración, todas las oraciones en un documento están completamente en el conjunto de capacitación o completamente en el conjunto de pruebas para cada pliegue. Para las pruebas de significación, utilizamos una prueba t de dos colas [21] para comparar los valores obtenidos durante cada pliegue de validación cruzada con un valor P de 0.05. La selección de características se realizó utilizando la estadística de chi-cuadrado. Se consideraron diferentes niveles de selección de características para cada clasificador. Cada uno de los siguientes números de características se probó: 10, 25, 50, 100, 250, 750, 1000, 2000, 4000. Hay aproximadamente 4700 tokens unigram sin selección de características. Para elegir el número de características que se pueden usar para cada clasificador, realizamos una validación cruzada anidada y elegimos la configuración que produce el F1 de nivel de documento óptimo para ese clasificador. Para este estudio, solo se utilizó el cuerpo de cada mensaje de correo electrónico. La selección de características siempre se aplica a todas las características candidatas. Es decir, para la representación de N-Gram, las características de N-grams y la posición también están sujetas a la eliminación del método de selección de características.4.5 Resultados Los resultados para la clasificación a nivel de documento se dan en la Tabla 3. La hipótesis principal que nos preocupa es que los N-Grams son críticos para esta tarea;Si esto es cierto, esperamos ver una brecha significativa en el rendimiento entre los clasificadores a nivel de documento que usan n-gram (documento denotado ngram) y aquellos que usan solo características unigram (documento unigram). Examinando la Tabla 3, observamos que este es el caso de cada clasificador, excepto Na¨ıve Bayes. Esta diferencia en el rendimiento producida por la representación de N-Gram es estadísticamente significativa para cada clasificador, excepto para Na¨ıve Bayes y la métrica de precisión para KNN (ver Tabla 4). Na¨ıve Bayes El bajo rendimiento con la representación de N-Gram no es sorprendente ya que la bolsa de N-Grams causa doble cuenta de duplicación como se menciona en la Sección 3.2.1;Sin embargo, Na¨ıve Bayes no se duele a nivel de oración porque los ejemplos escasos proporcionan pocas posibilidades de efectos aglomerativos del conteo doble. En cualquier caso, cuando se desea un enfoque de modelado del lenguaje, modelar los N-Grams directamente sería preferible a Na¨ıve Bayes. Más importante aún, para la hipótesis de N-Gram, los N-Grams conducen al mejor rendimiento del clasificador a nivel de documento. Como era de esperar, la diferencia entre la representación N-Gram a nivel de oración y la representación unigram es pequeña. Esto se debe a que la ventana de texto es tan pequeña que la representación unigram, cuando se hace en el nivel de oración, se retira implícitamente en el poder de los N-Grams. Una mejora adicional significaría que el orden de las palabras es importante incluso cuando solo se considera una pequeña ventana del tamaño de la oración. Por lo tanto, los juicios a nivel de oración de grano más fino permiten que una representación unigram tenga éxito, pero solo cuando se realiza en una pequeña ventana, que se comporta como una representación de N-gram para todos los fines prácticos. Ganador del documento Ganador de la oración Knn Ngram ngram na¨ıve bayes unigram ngram svm ngram † ngram votado perceptron ngram † ngram Tabla 4: resultados de significancia para n-gram versus unigrams para la detección de documentos utilizando clasificadores de nivel de documento y niveles de nivel de oración. Cuando el resultado F1 es estadísticamente significativo, se muestra en negrita. Cuando el resultado de precisión es significativo, se muestra con A †. F1 Ganador de precisión Ganador de la oración KNN Sentencia Na¨ıve Bayes Sentencia Sentencia SVM Sentencia Sentencia Votado Documento de oración de percepción Tabla 5: Resultados de significación para clasificadores a nivel de oración frente a clasificadores a nivel de documento para el problema de detección de documentos. Cuando el resultado es estadísticamente significativo, se muestra en negrita. Al destacar aún más la mejora de juicios de grano más fino y N-Grams, la Figura 3 representa gráficamente el borde que el clasificador de nivel de oración SVM tiene sobre el enfoque estándar de la bolsa de palabras con una curva de recolección de precisión. En el área de alta precisión del gráfico, el borde consistente del clasificador a nivel de oración es bastante impresionante, continuando en la precisión 1 a 0.1 de recuerdo. Esto significaría que una décima parte de los elementos de acción de los usuarios se colocarían en la parte superior de su bandeja de entrada ordenada de acción. Además, la gran separación en la parte superior derecha de las curvas corresponde al área donde se produce el F1 óptimo para cada clasificador, de acuerdo con la gran mejora de 0.6904 a 0.7682 en la puntuación F1. Teniendo en cuenta la naturaleza relativa inexplorada de la clasificación a nivel de oración, esto da una gran esperanza de mayores aumentos en el rendimiento. Precisión F1 unigram ngram unigram ngram knn 0.9519 0.9536 0.6540 0.6686 na¨ıve bayes 0.9419 0.9550 0.6176 0.6676 SVM 0.95559 0.9579 0.6271 0.6672 CLASEMIENTO DEL SENTENCIA DEL SENTENCIÓN 0.8895 0.9247 0.3744 4. Iers en detección de oraciones aunque Cohen et al.[5] observó que el perceptrón votado con una sola iteración de entrenamiento superó a SVM en un conjunto de tareas similares, no vemos tal comportamiento aquí. Esto fortalece aún más la evidencia de que un clasificador alternativo con la representación de la bolsa de palabras no podría alcanzar el mismo nivel de rendimiento. El clasificador de Perceptron votado mejora cuando aumentan el número de iteraciones de capacitación, pero aún es más bajo que el clasificador SVM. Los resultados de la detección de oraciones se presentan en la Tabla 6. Con respecto al problema de detección de oraciones, observamos que la medida F1 da una mejor idea del espacio restante de mejora en este problema difícil. Es decir, a diferencia de la detección de documentos, donde los documentos de ActionItem son bastante comunes, las oraciones de acción de acción son muy raras. Por lo tanto, como en otros problemas de texto, los números de precisión son engañosamente altos debido a la precisión predeterminada alcanzable al predecir siempre no. Aunque los resultados aquí están significativamente superiores al aleación, no está claro qué nivel de rendimiento es necesario para que la detección de oraciones sea útil en sí misma y no simplemente como un medio para documentar la clasificación y la clasificación. Figura 4: Los usuarios encuentran elementos de acción más rápido cuando están ayudados por un sistema de clasificación. Finalmente, al considerar un nuevo tipo de tarea de clasificación, una de las preguntas más básicas es si un clasificador preciso creado para la tarea puede tener un impacto en el usuario final. Para demostrar el impacto que esta tarea puede tener en los usuarios de correo electrónico, realizamos un estudio de usuarios utilizando una versión anterior menos precisa del clasificador de oraciones, donde en lugar de usar solo una oración, se utilizó un enfoque de ventosas de trío. Había tres conjuntos distintos de correo electrónico en los que los usuarios tuvieron que encontrar elementos de acción. Estos conjuntos se presentaron en un orden aleatorio (desordenado), ordenados por el clasificador (ordenado) o ordenados por el clasificador y con el 0.4 0.5 0.6 0.7 0.8 0.9 1 0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1 Acción de recuperación de precisión-Tuento de detección de ítem SVM (selección posterior al modelo) Documento de oración unigram NGRAM Figura 3: Tanto N-Grams como una pequeña ventana de predicción conducen a mejoras consistentes sobre el enfoque estándar.oración central en la ventana de confianza más alta resaltada (orden+ayuda). Para realizar comparaciones justas entre las condiciones, el número total de tokens en cada conjunto de mensajes debe ser aproximadamente igual;Es decir, la carga de lectura cognitiva debe ser aproximadamente la misma antes de la reordenamiento de los clasificadores. Además, los usuarios generalmente muestran efectos de práctica mejorando en la tarea general y, por lo tanto, se desempeñan mejor en conjuntos de mensajes posteriores. Esto generalmente se maneja variando el orden de los conjuntos entre los usuarios para que las medias sean comparables. Al omitir más detalles, observamos que los conjuntos estaban equilibrados para el número total de tokens y se usó un diseño de cuadrado latino para equilibrar los efectos de práctica. La Figura 4 muestra que a intervalos de 5, 10 y 15 minutos, los usuarios encontraron constantemente significativamente más elementos de acción cuando el clasificador lo asistió, pero se ayudaron de manera más crítica en los primeros cinco minutos. Aunque el clasificador ayuda constantemente a los usuarios, no obtuvimos un impacto adicional en el usuario final al destacar. Como se mencionó anteriormente, esto podría ser el resultado de la gran sala de mejora que todavía existe para la detección de oraciones, pero la evidencia anecdótica sugiere que esto también podría ser el resultado de cómo se presenta la información al usuario en lugar de la precisión de la detección de oraciones. Por ejemplo, destacar la oración incorrecta cerca de un Item de acción real perjudica a los usuarios, pero si un indicador vago (por ejemplo, una flecha) apunta al área aproximada que el usuario no es consciente de la falta de falta. Dado que los estudios de usuarios utilizaron una ventana de tres oraciones, creemos que esto jugó un papel, así como una precisión de detección de oraciones.4.6 Discusión En contraste con los problemas en los que los N-Grams han producido poca diferencia, creemos que su poder aquí proviene del hecho de que muchos de los N-Grams significativos para los elementos de acción consisten en palabras comunes, por ejemplo, hágamelo saber. Por lo tanto, el enfoque unigram a nivel de documento no puede obtener mucha influencia, incluso al modelar su probabilidad conjunta correctamente, ya que estas palabras a menudo coinciden en el documento, pero no necesariamente en una frase. Además, la detección de acción de acción es distinta de muchas tareas de clasificación de texto en que una sola oración puede cambiar la etiqueta de clase del documento. Como resultado, los buenos clasificadores no pueden confiar en agregar evidencia de una gran cantidad de indicadores débiles en todo el documento. A pesar de que descartamos la información del encabezado, examinar las características mejor clasificadas en el nivel de documento revela que muchas de las características son nombres o partes de las direcciones de correo electrónico que ocurrieron en el cuerpo y están altamente asociadas con correos electrónicos que tienden acontienen muchos o ningún elemento de acción. Algunos ejemplos son términos como Org, Bob y Gov. Observamos que estas características serán sensibles a la distribución particular (remitentes/receptores) y, por lo tanto, el enfoque a nivel de documento puede producir clasificadores que se transfieren menos fácilmente a contextos y usuarios alternativos en diferentes instituciones. Esto señala que parte del problema de ir más allá de la bolsa de palabras puede ser la metodología, e investigar propiedades tales como curvas de aprendizaje y qué tan bien las transferencias de modelo pueden resaltar las diferencias en los modelos que parecen tener un rendimiento similar cuando se prueban en las distribuciones.Fueron entrenados. Actualmente estamos investigando si los clasificadores a nivel de oración funcionan mejor en diferentes corpus de prueba sin reentrenamiento.5. El trabajo futuro Al aplicar clasificadores de texto a nivel de documento está bastante bien entendido, existe el potencial para aumentar significativamente el rendimiento de los clasificadores a nivel de oración. Dichos métodos incluyen formas alternativas de combinar las predicciones sobre cada oración, ponderaciones distintas de TFIDF, que pueden no ser apropiadas ya que las oraciones son pequeñas, una mejor segmentación de oraciones y otros tipos de análisis por frase. Además, el etiquetado de entidad con nombre, las expresiones de tiempo, etc., parecen probables candidatos para características que pueden mejorar aún más esta tarea. Actualmente estamos cursando algunas de estas vías para ver qué ganancias adicionales ofrecen. Finalmente, sería interesante investigar los mejores métodos para combinar los clasificadores a nivel de documento y a nivel de oración. Dado que la simple representación de la bolsa de las palabras en el nivel de documento conduce a un modelo aprendido que se comporta como un contexto específico de la previa dependiente del remitente/receptor y el tema general, una primera opción sería tratarlo como tal al combinarEstimaciones de probabilidad con el clasificador de nivel de oración. Tal modelo podría servir como un ejemplo general para otros problemas en los que la bolsa de palabras puede establecer un modelo de referencia, pero se necesitan enfoques más ricos para lograr el rendimiento más allá de esa línea de base.6. Resumen y conclusiones La efectividad de la detección de nivel de oración argumenta que el etiquetado a nivel de oración proporciona un valor significativo. Se necesitan más experimentos para ver cómo interactúa esto con la cantidad de datos de capacitación disponibles. La detección de oraciones que luego se aglomera para la detección a nivel de documento funciona sorprendentemente mejor dado un retiro bajo de lo esperado con los elementos a nivel de oración. Esto, a su vez, indica que los métodos de segmentación de oraciones mejorados podrían producir mejoras adicionales en la clasificación. En este trabajo, examinamos cómo los elementos de acción pueden detectarse de manera efectiva en los correos electrónicos. Nuestro análisis empírico ha demostrado que los N-Grams son de importancia clave para aprovechar al máximo los juicios de nivel de documentos. Cuando los juicios de grano más fino están disponibles, entonces un enfoque estándar de la bolsa de palabras utilizando un pequeño tamaño de la ventana (oración) y las técnicas de segmentación automática pueden producir resultados casi tan buenos como los enfoques basados en N-Gram. Agradecimientos Este material se basa en el trabajo respaldado por la Agencia de Proyectos de Investigación Avanzada de Defensa (DARPA) bajo el No. NBCHD030010. Cualquier opinión, hallazgos y conclusiones o recomendaciones expresadas en este material son las del autor (s) y no reflejan necesariamente las opiniones de la Agencia de Proyectos de Investigación Avanzada de Defensa (DARPA), o el Centro de Negocios Interiornornational (DOI-NBC). Nos gustaría extender nuestro más sincero agradecimiento a Jill Lehman, cuyos esfuerzos en la recopilación de datos fueron esenciales para construir el corpus, y Jill y Aaron Steinfeld por su dirección de los experimentos HCI. También nos gustaría agradecer a Django Wexler por construir y apoyar las herramientas de etiquetado de Corpus y el soporte de Curtis Huttenhowers del paquete de preprocesamiento de texto. Finalmente, agradecemos a Scott Fahlman por su aliento y discusiones útiles sobre este tema.7. Referencias [1] J. Allan, J. Carbonell, G. Doddington, J. Yamron e Y. Yang. Estudio piloto de detección y seguimiento de temas: informe final. En Actas del Taller de transcripción y comprensión de Noticias de Broadcast DARPA, Washington, D.C., 1998. [2] C. Apte, F. Damerau y S. M. Weiss. Aprendizaje automático de reglas de decisión para la categorización de texto. Transacciones ACM en Sistemas de Información, 12 (3): 233-251, julio de 1994. [3] J. Carletta. Evaluación del acuerdo sobre tareas de clasificación: la estadística de Kappa. Computacional Lingüística, 22 (2): 249-254, 1996. [4] J. Carroll. Extracción de alta precisión de relaciones gramaticales. En Actas de la 19ª Conferencia Internacional sobre Lingüística Computacional (Coling), páginas 134-140, 2002. [5] W. W. Cohen, V. R. Carvalho y T. M. Mitchell. Aprender a clasificar el correo electrónico en actos de habla. En EMNLP-2004 (Conferencia sobre métodos empíricos en procesamiento del lenguaje natural), páginas 309-316, 2004. [6] S. Corston-Oliver, E. Ringger, M. Gamon y R. Campbell. Resumen de correo electrónico centrado en la tarea. En el resumen de texto se ramifican: Actas del taller de ACL-04, páginas 43-50, 2004. [7] A. Culotta, R. Bekkerman y A. McCallum. Extraer redes sociales e información de contacto del correo electrónico y la web. En CEAS-2004 (Conferencia sobre correo electrónico y anti-Spam), Mountain View, CA, julio de 2004. [8] L. Devroye, L. Gy¨orfi y G. Lugosi. Una teoría probabilística del reconocimiento de patrones. Springer-Verlag, Nueva York, NY, 1996. [9] S. T. Dumais, J. Platt, D. Heckerman y M. Sahami. Algoritmos de aprendizaje inductivo y representaciones para la categorización de texto. En CIKM 98, Actas de la 7ª Conferencia ACM sobre Gestión de Información y Conocimiento, páginas 148-155, 1998. [10] Y. Freund y R. Schapire. Gran clasificación de margen utilizando el algoritmo Perceptron. Aprendizaje automático, 37 (3): 277-296, 1999. [11] T. Joachims. Hacer práctico el aprendizaje SVM a gran escala. En B. Sch¨olkopf, C. J. Burges y A. J. Smola, editores, Avances en los métodos del núcleo - Aprendizaje de vectores de soporte, páginas 41-56. MIT Press, 1999. [12] L. S. Larkey. Un sistema de búsqueda y clasificación de patentes. En Actas de la Cuarta Conferencia ACM sobre Bibliotecas Digitales, páginas 179 - 187, 1999. [13] D. D. Lewis. Una evaluación de representaciones frasales y agrupadas en una tarea de categorización de texto. En Sigir 92, Actas de la 15ª Conferencia Anual de ACM internacional sobre investigación y desarrollo en recuperación de información, páginas 37-50, 1992. [14] Y. Liu, J. Carbonell y R. Jin. Un enfoque de conjunto por pares para la clasificación de género precisa. En Actas de la Conferencia Europea sobre Aprendizaje Autor (ECML), 2003. [15] Y. Liu, R. Yan, R. Jin y J. Carbonell. Un estudio de comparación de núcleos para la clasificación de texto multiclabel utilizando la asociación de categorías. En la vigésima primera conferencia internacional sobre aprendizaje automático (ICML), 2004. [16] A. McCallum y K. Nigam. Una comparación de modelos de eventos para la clasificación de texto de Naive Bayes. En Notas de trabajo de AAAI 98 (la 15ª Conferencia Nacional sobre Inteligencia Artificial), Taller sobre aprendizaje para la categorización de texto, páginas 41-48, 1998. TR WS-98-05.[17] F. Sebastiani. Aprendizaje automático en categorización de texto automatizado. ACM Computing Surveys, 34 (1): 1-47, marzo de 2002. [18] C. J. Van Rijsbergen. Recuperación de información. Butterworths, Londres, 1979. [19] Y. Yang. Una evaluación de los enfoques estadísticos para la categorización de texto. Recuperación de información, 1 (1/2): 67-88, 1999. [20] Y. Yang, J. Carbonell, R. Brown, T. Pierce, B. T. Archibald y X. Liu. Enfoques de aprendizaje para la detección y el seguimiento de los temas. IEEE Expert, número especial sobre aplicaciones de recuperación de información inteligente, 1999. [21] Y. Yang y X. Liu. Un reexamen de los métodos de categorización de texto. En Sigir 99, Actas de la 22a Conferencia Internacional de ACM anual sobre investigación y desarrollo en recuperación de información, páginas 42-49, 1999. [22] Y. Yang, J. Zhang, J. Carbonell y C. Jin. Detección de novedad condicionada por el tema. En Actas de la Conferencia Internacional de ACM SIGKDD sobre Discovery y Minería de datos de conocimiento, julio de 2002.",
    "original_sentences": [
        "Feature Representation for Effective Action-Item Detection Paul N. Bennett Computer Science Department Carnegie Mellon University Pittsburgh, PA 15213 pbennett+@cs.cmu.edu Jaime Carbonell Language Technologies Institute.",
        "Carnegie Mellon University Pittsburgh, PA 15213 jgc+@cs.cmu.edu ABSTRACT E-mail users face an ever-growing challenge in managing their inboxes due to the growing centrality of email in the workplace for task assignment, action requests, and other roles beyond information dissemination.",
        "Whereas Information Retrieval and Machine Learning techniques are gaining initial acceptance in spam filtering and automated folder assignment, this paper reports on a new task: automated action-item detection, in order to flag emails that require responses, and to highlight the specific passage(s) indicating the request(s) for action.",
        "Unlike standard topic-driven text classification, action-item detection requires inferring the senders intent, and as such responds less well to pure bag-of-words classification.",
        "However, using enriched feature sets, such as n-grams (up to n=4) with chi-squared feature selection, and contextual cues for action-item location improve performance by up to 10% over unigrams, using in both cases state of the art classifiers such as SVMs with automated model selection via embedded cross-validation.",
        "Categories and Subject Descriptors H.3.3 [Information Storage and Retrieval]: Information Search and Retrieval; I.2.6 [Artificial Intelligence]: Learning; I.5.4 [Pattern Recognition]: Applications General Terms Experimentation 1.",
        "INTRODUCTION E-mail users are facing an increasingly difficult task of managing their inboxes in the face of mounting challenges that result from rising e-mail usage.",
        "This includes prioritizing e-mails over a range of sources from business partners to family members, filtering and reducing junk e-mail, and quickly managing requests that demand From: Henry Hutchins <hhutchins@innovative.company.com> To: Sara Smith; Joe Johnson; William Woolings Subject: meeting with prospective customers Sent: Fri 12/10/2005 8:08 AM Hi All, Id like to remind all of you that the group from GRTY will be visiting us next Friday at 4:30 p.m.",
        "The current schedule looks like this: + 9:30 a.m.",
        "Informal Breakfast and Discussion in Cafeteria + 10:30 a.m. Company Overview + 11:00 a.m.",
        "Individual Meetings (Continue Over Lunch) + 2:00 p.m. Tour of Facilities + 3:00 p.m.",
        "Sales Pitch In order to have this go off smoothly, I would like to practice the presentation well in advance.",
        "As a result, I will need each of your parts by Wednesday.",
        "Keep up the good work! -Henry Figure 1: An E-mail with emphasized Action-Item, an explicit request that requires the recipients attention or action. the receivers attention or action.",
        "Automated action-item detection targets the third of these problems by attempting to detect which e-mails require an action or response with information, and within those e-mails, attempting to highlight the sentence (or other passage length) that directly indicates the action request.",
        "Such a detection system can be used as one part of an e-mail agent which would assist a user in processing important e-mails quicker than would have been possible without the agent.",
        "We view action-item detection as one necessary component of a successful e-mail agent which would perform spam detection, action-item detection, topic classification and priority ranking, among other functions.",
        "The utility of such a detector can manifest as a method of prioritizing e-mails according to task-oriented criteria other than the standard ones of topic and sender or as a means of ensuring that the email user hasnt dropped the proverbial ball by forgetting to address an action request.",
        "Action-item detection differs from standard text classification in two important ways.",
        "First, the user is interested both in detecting whether an email contains action items and in locating exactly where these action item requests are contained within the email body.",
        "In contrast, standard text categorization merely assigns a topic label to each text, whether that label corresponds to an e-mail folder or a controlled indexing vocabulary [12, 15, 22].",
        "Second, action-item detection attempts to recover the email senders intent - whether she means to elicit response or action on the part of the receiver; note that for this task, classifiers using only unigrams as features do not perform optimally, as evidenced in our results below.",
        "Instead we find that we need more information-laden features such as higher-order n-grams.",
        "Text categorization by topic, on the other hand, works very well using just individual words as features [2, 9, 13, 17].",
        "In fact, genre-classification, which one would think may require more than a bag-of-words approach, also works quite well using just unigram features [14].",
        "Topic detection and tracking (TDT), also works well with unigram feature sets [1, 20].",
        "We believe that action-item detection is one of the first clear instances of an IR-related task where we must move beyond bag-of-words to achieve high performance, albeit not too far, as bag-of-n-grams seem to suffice.",
        "We first review related work for similar text classification problems such as e-mail priority ranking and speech act identification.",
        "Then we more formally define the action-item detection problem, discuss the aspects that distinguish it from more common problems like topic classification, and highlight the challenges in constructing systems that can perform well at the sentence and document level.",
        "From there, we move to a discussion of feature representation and selection techniques appropriate for this problem and how standard text classification approaches can be adapted to smoothly move from the sentence-level detection problem to the documentlevel classification problem.",
        "We then conduct an empirical analysis that helps us determine the effectiveness of our feature extraction procedures as well as establish baselines for a number of classification algorithms on this task.",
        "Finally, we summarize this papers contributions and consider interesting directions for future work. 2.",
        "RELATED WORK Several other researchers have considered very similar text classification tasks.",
        "Cohen et al. [5] describe an ontology of speech acts, such as Propose a Meeting, and attempt to predict when an e-mail contains one of these speech acts.",
        "We consider action-items to be an important specific type of speech act that falls within their more general classification.",
        "While they provide results for several classification methods, their methods only make use of human judgments at the document-level.",
        "In contrast, we consider whether accuracy can be increased by using finer-grained human judgments that mark the specific sentences and phrases of interest.",
        "Corston-Oliver et al. [6] consider detecting items in e-mail to Put on a To-Do List.",
        "This classification task is very similar to ours except they do not consider simple factual questions to belong to this category.",
        "We include questions, but note that not all questions are action-items - some are rhetorical or simply social convention, How are you?.",
        "From a learning perspective, while they make use of judgments at the sentence-level, they do not explicitly compare what if any benefits finer-grained judgments offer.",
        "Additionally, they do not study alternative choices or approaches to the classification task.",
        "Instead, they simply apply a standard SVM at the sentence-level and focus primarily on a linguistic analysis of how the sentence can be logically reformulated before adding it to the task list.",
        "In this study, we examine several alternative classification methods, compare document-level and sentence-level approaches and analyze the machine learning issues implicit in these problems.",
        "Interest in a variety of learning tasks related to e-mail has been rapidly growing in the recent literature.",
        "For example, in a forum dedicated to e-mail learning tasks, Culotta et al. [7] presented methods for learning social networks from e-mail.",
        "In this work, we do not focus on peer relationships; however, such methods could complement those here since peer relationships often influence word choice when requesting an action. 3.",
        "PROBLEM DEFINITION & APPROACH In contrast to previous work, we explicitly focus on the benefits that finer-grained, more costly, sentence-level human judgments offer over coarse-grained document-level judgments.",
        "Additionally, we consider multiple standard text classification approaches and analyze both the quantitative and qualitative differences that arise from taking a document-level vs. a sentence-level approach to classification.",
        "Finally, we focus on the representation necessary to achieve the most competitive performance. 3.1 Problem Definition In order to provide the most benefit to the user, a system would not only detect the document, but it would also indicate the specific sentences in the e-mail which contain the action-items.",
        "Therefore, there are three basic problems: 1.",
        "Document detection: Classify a document as to whether or not it contains an action-item. 2.",
        "Document ranking: Rank the documents such that all documents containing action-items occur as high as possible in the ranking. 3.",
        "Sentence detection: Classify each sentence in a document as to whether or not it is an action-item.",
        "As in most Information Retrieval tasks, the weight the evaluation metric should give to precision and recall depends on the nature of the application.",
        "In situations where a user will eventually read all received messages, ranking (e.g., via precision at recall of 1) may be most important since this will help encourage shorter delays in communications between users.",
        "In contrast, high-precision detection at low recall will be of increasing importance when the user is under severe time-pressure and therefore will likely not read all mail.",
        "This can be the case for crisis managers during disaster management.",
        "Finally, sentence detection plays a role in both timepressure situations and simply to alleviate the users required time to gist the message. 3.2 Approach As mentioned above, the labeled data can come in one of two forms: a document-labeling provides a yes/no label for each document as to whether it contains an action-item; a phrase-labeling provides only a yes label for the specific items of interest.",
        "We term the human judgments a phrase-labeling since the users view of the action-item may not correspond with actual sentence boundaries or predicted sentence boundaries.",
        "Obviously, it is straightforward to generate a document-labeling consistent with a phrase-labeling by labeling a document yes if and only if it contains at least one phrase labeled yes.",
        "To train classifiers for this task, we can take several viewpoints related to both the basic problems we have enumerated and the form of the labeled data.",
        "The document-level view treats each e-mail as a learning instance with an associated class-label.",
        "Then, the document can be converted to a feature-value vector and learning progresses as usual.",
        "Applying a document-level classifier to document detection and ranking is straightforward.",
        "In order to apply it to sentence detection, one must make additional steps.",
        "For example, if the classifier predicts a document contains an action-item, then areas of the document that contain a high-concentration of words which the model weights heavily in favor of action-items can be indicated.",
        "The obvious benefit of the document-level approach is that training set collection costs are lower since the user only has to specify whether or not an e-mail contains an action-item and not the specific sentences.",
        "In the sentence-level view, each e-mail is automatically segmented into sentences, and each sentence is treated as a learning instance with an associated class-label.",
        "Since the phrase-labeling provided by the user may not coincide with the automatic segmentation, we must determine what label to assign a partially overlapping sentence when converting it to a learning instance.",
        "Once trained, applying the resulting classifiers to sentence detection is now straightforward, but in order to apply the classifiers to document detection and document ranking, the individual predictions over each sentence must be aggregated in order to make a document-level prediction.",
        "This approach has the potential to benefit from morespecific labels that enable the learner to focus attention on the key sentences instead of having to learn based on data that the majority of the words in the e-mail provide no or little information about class membership. 3.2.1 Features Consider some of the phrases that might constitute part of an action item: would like to know, let me know, as soon as possible, have you.",
        "Each of these phrases consists of common words that occur in many e-mails.",
        "However, when they occur in the same sentence, they are far more indicative of an action-item.",
        "Additionally, order can be important: consider have you versus you have.",
        "Because of this, we posit that n-grams play a larger role in this problem than is typical of problems like topic classification.",
        "Therefore, we consider all n-grams up to size 4.",
        "When using n-grams, if we find an n-gram of size 4 in a segment of text, we can represent the text as just one occurrence of the ngram or as one occurrence of the n-gram and an occurrence of each smaller n-gram contained by it.",
        "We choose the second of these alternatives since this will allow the algorithm itself to smoothly back-off in terms of recall.",
        "Methods such as na¨ıve Bayes may be hurt by such a representation because of double-counting.",
        "Since sentence-ending punctuation can provide information, we retain the terminating punctuation token when it is identifiable.",
        "Additionally, we add a beginning-of-sentence and end-of-sentence token in order to capture patterns that are often indicators at the beginning or end of a sentence.",
        "Assuming proper punctuation, these extra tokens are unnecessary, but often e-mail lacks proper punctuation.",
        "In addition, for the sentence-level classifiers that use ngrams, we additionally code for each sentence a binary encoding of the position of the sentence relative to the document.",
        "This encoding has eight associated features that represent which octile (the first eighth, second eighth, etc.) contains the sentence. 3.2.2 Implementation Details In order to compare the document-level to the sentence-level approach, we compare predictions at the document-level.",
        "We do not address how to use a document-level classifier to make predictions at the sentence-level.",
        "In order to automatically segment the text of the e-mail, we use the RASP statistical parser [4].",
        "Since the automatically segmented sentences may not correspond directly with the phrase-level boundaries, we treat any sentence that contains at least 30% of a marked action-item segment as an action-item.",
        "When evaluating sentencedetection for the sentence-level system, we use these class labels as ground truth.",
        "Since we are not evaluating multiple segmentation approaches, this does not bias any of the methods.",
        "If multiple segmentation systems were under evaluation, one would need to use a metric that matched predicted positive sentences to phrases labeled positive.",
        "The metric would need to punish overly long true predictions as well as too short predictions.",
        "Our criteria for converting to labeled instances implicitly includes both criteria.",
        "Since the segmentation is fixed, an overly long prediction would be predicting yes for many no instances since presumably the extra length corresponds to additional segmented sentences all of which do not contain 30% of action-item.",
        "Likewise, a too short prediction must correspond to a small sentence included in the action-item but not constituting all of the action-item.",
        "Therefore, in order to consider the prediction to be too short, there will be an additional preceding/following sentence that is an action-item where we incorrectly predicted no.",
        "Once a sentence-level classifier has made a prediction for each sentence, we must combine these predictions to make both a document-level prediction and a document-level score.",
        "We use the simple policy of predicting positive when any of the sentences is predicted positive.",
        "In order to produce a document score for ranking, the confidence that the document contains an action-item is: ψ(d) = 1 n(d) s∈d|π(s)=1 ψ(s) if for any s ∈ d, π(s) = 1 1 n(d) maxs∈d ψ(s) o.w. where s is a sentence in document d, π is the classifiers 1/0 prediction, ψ is the score the classifier assigns as its confidence that π(s) = 1, and n(d) is the greater of 1 and the number of (unigram) tokens in the document.",
        "In other words, when any sentence is predicted positive, the document score is the length normalized sum of the sentence scores above threshold.",
        "When no sentence is predicted positive, the document score is the maximum sentence score normalized by length.",
        "As in other text problems, we are more likely to emit false positives for documents with more words or sentences.",
        "Thus we include a length normalization factor. 4.",
        "EXPERIMENTAL ANALYSIS 4.1 The Data Our corpus consists of e-mails obtained from volunteers at an educational institution and cover subjects such as: organizing a research workshop, arranging for job-candidate interviews, publishing proceedings, and talk announcements.",
        "The messages were anonymized by replacing the names of each individual and institution with a pseudonym.1 After attempting to identify and eliminate duplicate e-mails, the corpus contains 744 e-mail messages.",
        "After identity anonymization, the corpora has three basic versions.",
        "Quoted material refers to the text of a previous e-mail that an author often leaves in an e-mail message when responding to the e-mail.",
        "Quoted material can act as noise when learning since it may include action-items from previous messages that are no longer relevant.",
        "To isolate the effects of quoted material, we have three versions of the corpora.",
        "The raw form contains the basic messages.",
        "The auto-stripped version contains the messages after quoted material has been automatically removed.",
        "The hand-stripped version contains the messages after quoted material has been removed by a human.",
        "Additionally, the hand-stripped version has had any xml content and e-mail signatures removed - leaving only the essential content of the message.",
        "The studies reported here are performed with the hand-stripped version.",
        "This allows us to balance the cognitive load in terms of number of tokens that must be read in the user-studies we report - including quoted material would complicate the user studies since some users might skip the material while others read it.",
        "Additionally, ensuring all quoted material is removed 1 We have an even more highly anonymized version of the corpus that can be made available for some outside experimentation.",
        "Please contact the authors for more information on obtaining this data. prevents tainting the cross-validation since otherwise a test item could occur as quoted material in a training document. 4.1.1 Data Labeling Two human annotators labeled each message as to whether or not it contained an action-item.",
        "In addition, they identified each segment of the e-mail which contained an action-item.",
        "A segment is a contiguous section of text selected by the human annotators and may span several sentences or a complete phrase contained in a sentence.",
        "They were instructed that an action item is an explicit request for information that requires the recipients attention or a required action and told to highlight the phrases or sentences that make up the request.",
        "Annotator 1 No Yes Annotator 2 No 391 26 Yes 29 298 Table 1: Agreement of Human Annotators at Document Level Annotator One labeled 324 messages as containing action items.",
        "Annotator Two labeled 327 messages as containing action items.",
        "The agreement of the human annotators is shown in Tables 1 and 2.",
        "The annotators are said to agree at the document-level when both marked the same document as containing no action-items or both marked at least one action-item regardless of whether the text segments were the same.",
        "At the document-level, the annotators agreed 93% of the time.",
        "The kappa statistic [3, 5] is often used to evaluate inter-annotator agreement: κ = A − R 1 − R A is the empirical estimate of the probability of agreement.",
        "R is the empirical estimate of the probability of random agreement given the empirical class priors.",
        "A value close to −1 implies the annotators agree far less often than would be expected randomly, while a value close to 1 means they agree more often than randomly expected.",
        "At the document-level, the kappa statistic for inter-annotator agreement is 0.85.",
        "This value is both strong enough to expect the problem to be learnable and is comparable with results for similar tasks [5, 6].",
        "In order to determine the sentence-level agreement, we use each judgment to create a sentence-corpus with labels as described in Section 3.2.2, then consider the agreement over these sentences.",
        "This allows us to compare agreement over no judgments.",
        "We perform this comparison over the hand-stripped corpus since that eliminates spurious no judgments that would come from including quoted material, etc.",
        "Both annotators were free to label the subject as an action-item, but since neither did, we omit the subject line of the message as well.",
        "This only reduces the number of no agreements.",
        "This leaves 6301 automatically segmented sentences.",
        "At the sentence-level, the annotators agreed 98% of the time, and the kappa statistic for inter-annotator agreement is 0.82.",
        "In order to produce one single set of judgments, the human annotators went through each annotation where there was disagreement and came to a consensus opinion.",
        "The annotators did not collect statistics during this process but anecdotally reported that the majority of disagreements were either cases of clear annotator oversight or different interpretations of conditional statements.",
        "For example, If you would like to keep your job, come to tomorrows meeting implies a required action where If you would like to join Annotator 1 No Yes Annotator 2 No 5810 65 Yes 74 352 Table 2: Agreement of Human Annotators at Sentence Level the football betting pool, come to tomorrows meeting does not.",
        "The first would be an action-item in most contexts while the second would not.",
        "Of course, many conditional statements are not so clearly interpretable.",
        "After reconciling the judgments there are 416 e-mails with no action-items and 328 e-mails containing actionitems.",
        "Of the 328 e-mails containing action-items, 259 messages have one action-item segment; 55 messages have two action-item segments; 11 messages have three action-item segments.",
        "Two messages have four action-item segments, and one message has six action-item segments.",
        "Computing the sentence-level agreement using the reconciled gold standard judgments with each of the annotators individual judgments gives a kappa of 0.89 for Annotator One and a kappa of 0.92 for Annotator Two.",
        "In terms of message characteristics, there were on average 132 content tokens in the body after stripping.",
        "For action-item messages, there were 115.",
        "However, by examining Figure 2 we see the length distributions are nearly identical.",
        "As would be expected for e-mail, it is a long-tailed distribution with about half the messages having more than 60 tokens in the body (this paragraph has 65 tokens). 4.2 Classifiers For this experiment, we have selected a variety of standard text classification algorithms.",
        "In selecting algorithms, we have chosen algorithms that are not only known to work well but which differ along such lines as discriminative vs. generative and lazy vs. eager.",
        "We have done this in order to provide both a competitive and thorough sampling of learning methods for the task at hand.",
        "This is important since it is easy to improve a strawman classifier by introducing a new representation.",
        "By thoroughly sampling alternative classifier choices we demonstrate that representation improvements over bag-of-words are not due to using the information in the bag-of-words poorly. 4.2.1 kNN We employ a standard variant of the k-nearest neighbor algorithm used in text classification, kNN with s-cut score thresholding [19].",
        "We use a tfidf-weighting of the terms with a distanceweighted vote of the neighbors to compute the score before thresholding it.",
        "In order to choose the value of s for thresholding, we perform leave-one-out cross-validation over the training set.",
        "The value of k is set to be 2( log2 N + 1) where N is the number of training points.",
        "This rule for choosing k is theoretically motivated by results which show such a rule converges to the optimal classifier as the number of training points increases [8].",
        "In practice, we have also found it to be a computational convenience that frequently leads to comparable results with numerically optimizing k via a cross-validation procedure. 4.2.2 Na¨ıve Bayes We use a standard multinomial na¨ıve Bayes classifier [16].",
        "In using this classifier, we smoothed word and class probabilities using a Bayesian estimate (with the word prior) and a Laplace m-estimate, respectively. 0 20 40 60 80 100 120 140 160 0 200 400 600 800 1000 1200 1400 NumberofMessages Number of Tokens All Messages Action-Item Messages 0 0.02 0.04 0.06 0.08 0.1 0.12 0.14 0.16 0.18 0.2 0 200 400 600 800 1000 1200 1400 PercentageofMessages Number of Tokens All Messages Action-Item Messages Figure 2: The Histogram (left) and Distribution (right) of Message Length.",
        "A bin size of 20 words was used.",
        "Only tokens in the body after hand-stripping were counted.",
        "After stripping, the majority of words left are usually actual message content.",
        "Classifiers Document Unigram Document Ngram Sentence Unigram Sentence Ngram F1 kNN 0.6670 ± 0.0288 0.7108 ± 0.0699 0.7615 ± 0.0504 0.7790 ± 0.0460 na¨ıve Bayes 0.6572 ± 0.0749 0.6484 ± 0.0513 0.7715 ± 0.0597 0.7777 ± 0.0426 SVM 0.6904 ± 0.0347 0.7428 ± 0.0422 0.7282 ± 0.0698 0.7682 ± 0.0451 Voted Perceptron 0.6288 ± 0.0395 0.6774 ± 0.0422 0.6511 ± 0.0506 0.6798 ± 0.0913 Accuracy kNN 0.7029 ± 0.0659 0.7486 ± 0.0505 0.7972 ± 0.0435 0.8092 ± 0.0352 na¨ıve Bayes 0.6074 ± 0.0651 0.5816 ± 0.1075 0.7863 ± 0.0553 0.8145 ± 0.0268 SVM 0.7595 ± 0.0309 0.7904 ± 0.0349 0.7958 ± 0.0551 0.8173 ± 0.0258 Voted Perceptron 0.6531 ± 0.0390 0.7164 ± 0.0376 0.6413 ± 0.0833 0.7082 ± 0.1032 Table 3: Average Document-Detection Performance during Cross-Validation for Each Method and the Sample Standard Deviation (Sn−1) in italics.",
        "The best performance for each classifier is shown in bold. 4.2.3 SVM We have used a linear SVM with a tfidf feature representation and L2-norm as implemented in the SVMlight package v6.01 [11].",
        "All default settings were used. 4.2.4 Voted Perceptron Like the SVM, the Voted Perceptron is a kernel-based learning method.",
        "We use the same feature representation and kernel as we have for the SVM, a linear kernel with tfidf-weighting and an L2-norm.",
        "The voted perceptron is an online-learning method that keeps a history of past perceptrons used, as well as a weight signifying how often that perceptron was correct.",
        "With each new training example, a correct classification increases the weight on the current perceptron and an incorrect classification updates the perceptron.",
        "The output of the classifier uses the weights on the perceptra to make a final voted classification.",
        "When used in an offline-manner, multiple passes can be made through the training data.",
        "Both the voted perceptron and the SVM give a solution from the same hypothesis space - in this case, a linear classifier.",
        "Furthermore, it is well-known that the Voted Perceptron increases the margin of the solution after each pass through the training data [10].",
        "Since Cohen et al. [5] obtain worse results using an SVM than a Voted Perceptron with one training iteration, they conclude that the best solution for detecting speech acts may not lie in an area with a large margin.",
        "Because their tasks are highly similar to ours, we employ both classifiers to ensure we are not overlooking a competitive alternative classifier to the SVM for the basic bag-of-words representation. 4.3 Performance Measures To compare the performance of the classification methods, we look at two standard performance measures, F1 and accuracy.",
        "The F1 measure [18, 21] is the harmonic mean of precision and recall where Precision = Correct Positives Predicted Positives and Recall = Correct Positives Actual Positives . 4.4 Experimental Methodology We perform standard 10-fold cross-validation on the set of documents.",
        "For the sentence-level approach, all sentences in a document are either entirely in the training set or entirely in the test set for each fold.",
        "For significance tests, we use a two-tailed t-test [21] to compare the values obtained during each cross-validation fold with a p-value of 0.05.",
        "Feature selection was performed using the chi-squared statistic.",
        "Different levels of feature selection were considered for each classifier.",
        "Each of the following number of features was tried: 10, 25, 50, 100, 250, 750, 1000, 2000, 4000.",
        "There are approximately 4700 unigram tokens without feature selection.",
        "In order to choose the number of features to use for each classifier, we perform nested cross-validation and choose the settings that yield the optimal document-level F1 for that classifier.",
        "For this study, only the body of each e-mail message was used.",
        "Feature selection is always applied to all candidate features.",
        "That is, for the n-gram representation, the n-grams and position features are also subject to removal by the feature selection method. 4.5 Results The results for document-level classification are given in Table 3.",
        "The primary hypothesis we are concerned with is that n-grams are critical for this task; if this is true, we expect to see a significant gap in performance between the document-level classifiers that use n-grams (denoted Document Ngram) and those using only unigram features (denoted Document Unigram).",
        "Examining Table 3, we observe that this is indeed the case for every classifier except na¨ıve Bayes.",
        "This difference in performance produced by the n-gram representation is statistically significant for each classifier except for na¨ıve Bayes and the accuracy metric for kNN (see Table 4).",
        "Na¨ıve Bayes poor performance with the n-gram representation is not surprising since the bag-of-n-grams causes excessive doublecounting as mentioned in Section 3.2.1; however, na¨ıve Bayes is not hurt at the sentence-level because the sparse examples provide few chances for agglomerative effects of double counting.",
        "In either case, when a language-modeling approach is desired, modeling the n-grams directly would be preferable to na¨ıve Bayes.",
        "More importantly for the n-gram hypothesis, the n-grams lead to the best document-level classifier performance as well.",
        "As would be expected, the difference between the sentence-level n-gram representation and unigram representation is small.",
        "This is because the window of text is so small that the unigram representation, when done at the sentence-level, implicitly picks up on the power of the n-grams.",
        "Further improvement would signify that the order of the words matter even when only considering a small sentence-size window.",
        "Therefore, the finer-grained sentence-level judgments allows a unigram representation to succeed but only when performed in a small window - behaving as an n-gram representation for all practical purposes.",
        "Document Winner Sentence Winner kNN Ngram Ngram na¨ıve Bayes Unigram Ngram SVM Ngram† Ngram Voted Perceptron Ngram† Ngram Table 4: Significance results for n-grams versus unigrams for document detection using document-level and sentence-level classifiers.",
        "When the F1 result is statistically significant, it is shown in bold.",
        "When the accuracy result is significant, it is shown with a † .",
        "F1 Winner Accuracy Winner kNN Sentence Sentence na¨ıve Bayes Sentence Sentence SVM Sentence Sentence Voted Perceptron Sentence Document Table 5: Significance results for sentence-level classifiers vs. document-level classifiers for the document detection problem.",
        "When the result is statistically significant, it is shown in bold.",
        "Further highlighting the improvement from finer-grained judgments and n-grams, Figure 3 graphically depicts the edge the SVM sentence-level classifier has over the standard bag-of-words approach with a precision-recall curve.",
        "In the high precision area of the graph, the consistent edge of the sentence-level classifier is rather impressive - continuing at precision 1 out to 0.1 recall.",
        "This would mean that a tenth of the users action-items would be placed at the top of their action-item sorted inbox.",
        "Additionally, the large separation at the top right of the curves corresponds to the area where the optimal F1 occurs for each classifier, agreeing with the large improvement from 0.6904 to 0.7682 in F1 score.",
        "Considering the relative unexplored nature of classification at the sentence-level, this gives great hope for further increases in performance.",
        "Accuracy F1 Unigram Ngram Unigram Ngram kNN 0.9519 0.9536 0.6540 0.6686 na¨ıve Bayes 0.9419 0.9550 0.6176 0.6676 SVM 0.9559 0.9579 0.6271 0.6672 Voted Perceptron 0.8895 0.9247 0.3744 0.5164 Table 6: Performance of the Sentence-Level Classifiers at Sentence Detection Although Cohen et al. [5] observed that the Voted Perceptron with a single training iteration outperformed SVM in a set of similar tasks, we see no such behavior here.",
        "This further strengthens the evidence that an alternate classifier with the bag-of-words representation could not reach the same level of performance.",
        "The Voted Perceptron classifier does improve when the number of training iterations are increased, but it is still lower than the SVM classifier.",
        "Sentence detection results are presented in Table 6.",
        "With regard to the sentence detection problem, we note that the F1 measure gives a better feel for the remaining room for improvement in this difficult problem.",
        "That is, unlike document detection where actionitem documents are fairly common, action-item sentences are very rare.",
        "Thus, as in other text problems, the accuracy numbers are deceptively high sheerly because of the default accuracy attainable by always predicting no.",
        "Although, the results here are significantly above-random, it is unclear what level of performance is necessary for sentence detection to be useful in and of itself and not simply as a means to document ranking and classification.",
        "Figure 4: Users find action-items quicker when assisted by a classification system.",
        "Finally, when considering a new type of classification task, one of the most basic questions is whether an accurate classifier built for the task can have an impact on the end-user.",
        "In order to demonstrate the impact this task can have on e-mail users, we conducted a user study using an earlier less-accurate version of the sentence classifier - where instead of using just a single sentence, a threesentence windowed-approach was used.",
        "There were three distinct sets of e-mail in which users had to find action-items.",
        "These sets were either presented in a random order (Unordered), ordered by the classifier (Ordered), or ordered by the classifier and with the 0.4 0.5 0.6 0.7 0.8 0.9 1 0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1 Precision Recall Action-Item Detection SVM Performance (Post Model Selection) Document Unigram Sentence Ngram Figure 3: Both n-grams and a small prediction window lead to consistent improvements over the standard approach. center sentence in the highest confidence window highlighted (Order+help).",
        "In order to perform fair comparisons between conditions, the overall number of tokens in each message set should be approximately equal; that is, the cognitive reading load should be approximately the same before the classifiers reordering.",
        "Additionally, users typically show practice effects by improving at the overall task and thus performing better at later message sets.",
        "This is typically handled by varying the ordering of the sets across users so that the means are comparable.",
        "While omitting further detail, we note the sets were balanced for the total number of tokens and a latin square design was used to balance practice effects.",
        "Figure 4 shows that at intervals of 5, 10, and 15 minutes, users consistently found significantly more action-items when assisted by the classifier, but were most critically aided in the first five minutes.",
        "Although, the classifier consistently aids the users, we did not gain an additional end-user impact by highlighting.",
        "As mentioned above, this might be a result of the large room for improvement that still exists for sentence detection, but anecdotal evidence suggests this might also be a result of how the information is presented to the user rather than the accuracy of sentence detection.",
        "For example, highlighting the wrong sentence near an actual action-item hurts the users trust, but if a vague indicator (e.g., an arrow) points to the approximate area the user is not aware of the near-miss.",
        "Since the user studies used a three sentence window, we believe this played a role as well as sentence detection accuracy. 4.6 Discussion In contrast to problems where n-grams have yielded little difference, we believe their power here stems from the fact that many of the meaningful n-grams for action-items consist of common words, e.g., let me know.",
        "Therefore, the document-level unigram approach cannot gain much leverage, even when modeling their joint probability correctly, since these words will often co-occur in the document but not necessarily in a phrase.",
        "Additionally, action-item detection is distinct from many text classification tasks in that a single sentence can change the class label of the document.",
        "As a result, good classifiers cannot rely on aggregating evidence from a large number of weak indicators across the entire document.",
        "Even though we discarded the header information, examining the top-ranked features at the document-level reveals that many of the features are names or parts of e-mail addresses that occurred in the body and are highly associated with e-mails that tend to contain many or no action-items.",
        "A few examples are terms such as org, bob, and gov.",
        "We note that these features will be sensitive to the particular distribution (senders/receivers) and thus the document-level approach may produce classifiers that transfer less readily to alternate contexts and users at different institutions.",
        "This points out that part of the problem of going beyond bag-of-words may be the methodology, and investigating such properties as learning curves and how well a model transfers may highlight differences in models which appear to have similar performance when tested on the distributions they were trained on.",
        "We are currently investigating whether the sentence-level classifiers do perform better over different test corpora without retraining. 5.",
        "FUTURE WORK While applying text classifiers at the document-level is fairly well-understood, there exists the potential for significantly increasing the performance of the sentence-level classifiers.",
        "Such methods include alternate ways of combining the predictions over each sentence, weightings other than tfidf, which may not be appropriate since sentences are small, better sentence segmentation, and other types of phrasal analysis.",
        "Additionally, named entity tagging, time expressions, etc., seem likely candidates for features that can further improve this task.",
        "We are currently pursuing some of these avenues to see what additional gains these offer.",
        "Finally, it would be interesting to investigate the best methods for combining the document-level and sentence-level classifiers.",
        "Since the simple bag-of-words representation at the document-level leads to a learned model that behaves somewhat like a context-specific prior dependent on the sender/receiver and general topic, a first choice would be to treat it as such when combining probability estimates with the sentence-level classifier.",
        "Such a model might serve as a general example for other problems where bag-of-words can establish a baseline model but richer approaches are needed to achieve performance beyond that baseline. 6.",
        "SUMMARY AND CONCLUSIONS The effectiveness of sentence-level detection argues that labeling at the sentence-level provides significant value.",
        "Further experiments are needed to see how this interacts with the amount of training data available.",
        "Sentence detection that is then agglomerated to document-level detection works surprisingly better given low recall than would be expected with sentence-level items.",
        "This, in turn, indicates that improved sentence segmentation methods could yield further improvements in classification.",
        "In this work, we examined how action-items can be effectively detected in e-mails.",
        "Our empirical analysis has demonstrated that n-grams are of key importance to making the most of documentlevel judgments.",
        "When finer-grained judgments are available, then a standard bag-of-words approach using a small (sentence) window size and automatic segmentation techniques can produce results almost as good as the n-gram based approaches.",
        "Acknowledgments This material is based upon work supported by the Defense Advanced Research Projects Agency (DARPA) under Contract No.",
        "NBCHD030010.",
        "Any opinions, findings and conclusions or recommendations expressed in this material are those of the author(s) and do not necessarily reflect the views of the Defense Advanced Research Projects Agency (DARPA), or the Department of InteriorNational Business Center (DOI-NBC).",
        "We would like to extend our sincerest thanks to Jill Lehman whose efforts in data collection were essential in constructing the corpus, and both Jill and Aaron Steinfeld for their direction of the HCI experiments.",
        "We would also like to thank Django Wexler for constructing and supporting the corpus labeling tools and Curtis Huttenhowers support of the text preprocessing package.",
        "Finally, we gratefully acknowledge Scott Fahlman for his encouragement and useful discussions on this topic. 7.",
        "REFERENCES [1] J. Allan, J. Carbonell, G. Doddington, J. Yamron, and Y. Yang.",
        "Topic detection and tracking pilot study: Final report.",
        "In Proceedings of the DARPA Broadcast News Transcription and Understanding Workshop, Washington, D.C., 1998. [2] C. Apte, F. Damerau, and S. M. Weiss.",
        "Automated learning of decision rules for text categorization.",
        "ACM Transactions on Information Systems, 12(3):233-251, July 1994. [3] J. Carletta.",
        "Assessing agreement on classification tasks: The kappa statistic.",
        "Computational Linguistics, 22(2):249-254, 1996. [4] J. Carroll.",
        "High precision extraction of grammatical relations.",
        "In Proceedings of the 19th International Conference on Computational Linguistics (COLING), pages 134-140, 2002. [5] W. W. Cohen, V. R. Carvalho, and T. M. Mitchell.",
        "Learning to classify email into speech acts.",
        "In EMNLP-2004 (Conference on Empirical Methods in Natural Language Processing), pages 309-316, 2004. [6] S. Corston-Oliver, E. Ringger, M. Gamon, and R. Campbell.",
        "Task-focused summarization of email.",
        "In Text Summarization Branches Out: Proceedings of the ACL-04 Workshop, pages 43-50, 2004. [7] A. Culotta, R. Bekkerman, and A. McCallum.",
        "Extracting social networks and contact information from email and the web.",
        "In CEAS-2004 (Conference on Email and Anti-Spam), Mountain View, CA, July 2004. [8] L. Devroye, L. Gy¨orfi, and G. Lugosi.",
        "A Probabilistic Theory of Pattern Recognition.",
        "Springer-Verlag, New York, NY, 1996. [9] S. T. Dumais, J. Platt, D. Heckerman, and M. Sahami.",
        "Inductive learning algorithms and representations for text categorization.",
        "In CIKM 98, Proceedings of the 7th ACM Conference on Information and Knowledge Management, pages 148-155, 1998. [10] Y. Freund and R. Schapire.",
        "Large margin classification using the perceptron algorithm.",
        "Machine Learning, 37(3):277-296, 1999. [11] T. Joachims.",
        "Making large-scale svm learning practical.",
        "In B. Sch¨olkopf, C. J. Burges, and A. J. Smola, editors, Advances in Kernel Methods - Support Vector Learning, pages 41-56.",
        "MIT Press, 1999. [12] L. S. Larkey.",
        "A patent search and classification system.",
        "In Proceedings of the Fourth ACM Conference on Digital Libraries, pages 179 - 187, 1999. [13] D. D. Lewis.",
        "An evaluation of phrasal and clustered representations on a text categorization task.",
        "In SIGIR 92, Proceedings of the 15th Annual International ACM Conference on Research and Development in Information Retrieval, pages 37-50, 1992. [14] Y. Liu, J. Carbonell, and R. Jin.",
        "A pairwise ensemble approach for accurate genre classification.",
        "In Proceedings of the European Conference on Machine Learning (ECML), 2003. [15] Y. Liu, R. Yan, R. Jin, and J. Carbonell.",
        "A comparison study of kernels for multi-label text classification using category association.",
        "In The Twenty-first International Conference on Machine Learning (ICML), 2004. [16] A. McCallum and K. Nigam.",
        "A comparison of event models for naive bayes text classification.",
        "In Working Notes of AAAI 98 (The 15th National Conference on Artificial Intelligence), Workshop on Learning for Text Categorization, pages 41-48, 1998.",
        "TR WS-98-05. [17] F. Sebastiani.",
        "Machine learning in automated text categorization.",
        "ACM Computing Surveys, 34(1):1-47, March 2002. [18] C. J. van Rijsbergen.",
        "Information Retrieval.",
        "Butterworths, London, 1979. [19] Y. Yang.",
        "An evaluation of statistical approaches to text categorization.",
        "Information Retrieval, 1(1/2):67-88, 1999. [20] Y. Yang, J. Carbonell, R. Brown, T. Pierce, B. T. Archibald, and X. Liu.",
        "Learning approaches to topic detection and tracking.",
        "IEEE EXPERT, Special Issue on Applications of Intelligent Information Retrieval, 1999. [21] Y. Yang and X. Liu.",
        "A re-examination of text categorization methods.",
        "In SIGIR 99, Proceedings of the 22nd Annual International ACM Conference on Research and Development in Information Retrieval, pages 42-49, 1999. [22] Y. Yang, J. Zhang, J. Carbonell, and C. Jin.",
        "Topic-conditioned novelty detection.",
        "In Proceedings of the ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, July 2002."
    ],
    "error_count": 0,
    "keys": {
        "information retrieval": {
            "translated_key": "recuperación de información",
            "is_in_text": true,
            "original_annotated_sentences": [
                "Feature Representation for Effective Action-Item Detection Paul N. Bennett Computer Science Department Carnegie Mellon University Pittsburgh, PA 15213 pbennett+@cs.cmu.edu Jaime Carbonell Language Technologies Institute.",
                "Carnegie Mellon University Pittsburgh, PA 15213 jgc+@cs.cmu.edu ABSTRACT E-mail users face an ever-growing challenge in managing their inboxes due to the growing centrality of email in the workplace for task assignment, action requests, and other roles beyond information dissemination.",
                "Whereas <br>information retrieval</br> and Machine Learning techniques are gaining initial acceptance in spam filtering and automated folder assignment, this paper reports on a new task: automated action-item detection, in order to flag emails that require responses, and to highlight the specific passage(s) indicating the request(s) for action.",
                "Unlike standard topic-driven text classification, action-item detection requires inferring the senders intent, and as such responds less well to pure bag-of-words classification.",
                "However, using enriched feature sets, such as n-grams (up to n=4) with chi-squared feature selection, and contextual cues for action-item location improve performance by up to 10% over unigrams, using in both cases state of the art classifiers such as SVMs with automated model selection via embedded cross-validation.",
                "Categories and Subject Descriptors H.3.3 [Information Storage and Retrieval]: Information Search and Retrieval; I.2.6 [Artificial Intelligence]: Learning; I.5.4 [Pattern Recognition]: Applications General Terms Experimentation 1.",
                "INTRODUCTION E-mail users are facing an increasingly difficult task of managing their inboxes in the face of mounting challenges that result from rising e-mail usage.",
                "This includes prioritizing e-mails over a range of sources from business partners to family members, filtering and reducing junk e-mail, and quickly managing requests that demand From: Henry Hutchins <hhutchins@innovative.company.com> To: Sara Smith; Joe Johnson; William Woolings Subject: meeting with prospective customers Sent: Fri 12/10/2005 8:08 AM Hi All, Id like to remind all of you that the group from GRTY will be visiting us next Friday at 4:30 p.m.",
                "The current schedule looks like this: + 9:30 a.m.",
                "Informal Breakfast and Discussion in Cafeteria + 10:30 a.m. Company Overview + 11:00 a.m.",
                "Individual Meetings (Continue Over Lunch) + 2:00 p.m. Tour of Facilities + 3:00 p.m.",
                "Sales Pitch In order to have this go off smoothly, I would like to practice the presentation well in advance.",
                "As a result, I will need each of your parts by Wednesday.",
                "Keep up the good work! -Henry Figure 1: An E-mail with emphasized Action-Item, an explicit request that requires the recipients attention or action. the receivers attention or action.",
                "Automated action-item detection targets the third of these problems by attempting to detect which e-mails require an action or response with information, and within those e-mails, attempting to highlight the sentence (or other passage length) that directly indicates the action request.",
                "Such a detection system can be used as one part of an e-mail agent which would assist a user in processing important e-mails quicker than would have been possible without the agent.",
                "We view action-item detection as one necessary component of a successful e-mail agent which would perform spam detection, action-item detection, topic classification and priority ranking, among other functions.",
                "The utility of such a detector can manifest as a method of prioritizing e-mails according to task-oriented criteria other than the standard ones of topic and sender or as a means of ensuring that the email user hasnt dropped the proverbial ball by forgetting to address an action request.",
                "Action-item detection differs from standard text classification in two important ways.",
                "First, the user is interested both in detecting whether an email contains action items and in locating exactly where these action item requests are contained within the email body.",
                "In contrast, standard text categorization merely assigns a topic label to each text, whether that label corresponds to an e-mail folder or a controlled indexing vocabulary [12, 15, 22].",
                "Second, action-item detection attempts to recover the email senders intent - whether she means to elicit response or action on the part of the receiver; note that for this task, classifiers using only unigrams as features do not perform optimally, as evidenced in our results below.",
                "Instead we find that we need more information-laden features such as higher-order n-grams.",
                "Text categorization by topic, on the other hand, works very well using just individual words as features [2, 9, 13, 17].",
                "In fact, genre-classification, which one would think may require more than a bag-of-words approach, also works quite well using just unigram features [14].",
                "Topic detection and tracking (TDT), also works well with unigram feature sets [1, 20].",
                "We believe that action-item detection is one of the first clear instances of an IR-related task where we must move beyond bag-of-words to achieve high performance, albeit not too far, as bag-of-n-grams seem to suffice.",
                "We first review related work for similar text classification problems such as e-mail priority ranking and speech act identification.",
                "Then we more formally define the action-item detection problem, discuss the aspects that distinguish it from more common problems like topic classification, and highlight the challenges in constructing systems that can perform well at the sentence and document level.",
                "From there, we move to a discussion of feature representation and selection techniques appropriate for this problem and how standard text classification approaches can be adapted to smoothly move from the sentence-level detection problem to the documentlevel classification problem.",
                "We then conduct an empirical analysis that helps us determine the effectiveness of our feature extraction procedures as well as establish baselines for a number of classification algorithms on this task.",
                "Finally, we summarize this papers contributions and consider interesting directions for future work. 2.",
                "RELATED WORK Several other researchers have considered very similar text classification tasks.",
                "Cohen et al. [5] describe an ontology of speech acts, such as Propose a Meeting, and attempt to predict when an e-mail contains one of these speech acts.",
                "We consider action-items to be an important specific type of speech act that falls within their more general classification.",
                "While they provide results for several classification methods, their methods only make use of human judgments at the document-level.",
                "In contrast, we consider whether accuracy can be increased by using finer-grained human judgments that mark the specific sentences and phrases of interest.",
                "Corston-Oliver et al. [6] consider detecting items in e-mail to Put on a To-Do List.",
                "This classification task is very similar to ours except they do not consider simple factual questions to belong to this category.",
                "We include questions, but note that not all questions are action-items - some are rhetorical or simply social convention, How are you?.",
                "From a learning perspective, while they make use of judgments at the sentence-level, they do not explicitly compare what if any benefits finer-grained judgments offer.",
                "Additionally, they do not study alternative choices or approaches to the classification task.",
                "Instead, they simply apply a standard SVM at the sentence-level and focus primarily on a linguistic analysis of how the sentence can be logically reformulated before adding it to the task list.",
                "In this study, we examine several alternative classification methods, compare document-level and sentence-level approaches and analyze the machine learning issues implicit in these problems.",
                "Interest in a variety of learning tasks related to e-mail has been rapidly growing in the recent literature.",
                "For example, in a forum dedicated to e-mail learning tasks, Culotta et al. [7] presented methods for learning social networks from e-mail.",
                "In this work, we do not focus on peer relationships; however, such methods could complement those here since peer relationships often influence word choice when requesting an action. 3.",
                "PROBLEM DEFINITION & APPROACH In contrast to previous work, we explicitly focus on the benefits that finer-grained, more costly, sentence-level human judgments offer over coarse-grained document-level judgments.",
                "Additionally, we consider multiple standard text classification approaches and analyze both the quantitative and qualitative differences that arise from taking a document-level vs. a sentence-level approach to classification.",
                "Finally, we focus on the representation necessary to achieve the most competitive performance. 3.1 Problem Definition In order to provide the most benefit to the user, a system would not only detect the document, but it would also indicate the specific sentences in the e-mail which contain the action-items.",
                "Therefore, there are three basic problems: 1.",
                "Document detection: Classify a document as to whether or not it contains an action-item. 2.",
                "Document ranking: Rank the documents such that all documents containing action-items occur as high as possible in the ranking. 3.",
                "Sentence detection: Classify each sentence in a document as to whether or not it is an action-item.",
                "As in most <br>information retrieval</br> tasks, the weight the evaluation metric should give to precision and recall depends on the nature of the application.",
                "In situations where a user will eventually read all received messages, ranking (e.g., via precision at recall of 1) may be most important since this will help encourage shorter delays in communications between users.",
                "In contrast, high-precision detection at low recall will be of increasing importance when the user is under severe time-pressure and therefore will likely not read all mail.",
                "This can be the case for crisis managers during disaster management.",
                "Finally, sentence detection plays a role in both timepressure situations and simply to alleviate the users required time to gist the message. 3.2 Approach As mentioned above, the labeled data can come in one of two forms: a document-labeling provides a yes/no label for each document as to whether it contains an action-item; a phrase-labeling provides only a yes label for the specific items of interest.",
                "We term the human judgments a phrase-labeling since the users view of the action-item may not correspond with actual sentence boundaries or predicted sentence boundaries.",
                "Obviously, it is straightforward to generate a document-labeling consistent with a phrase-labeling by labeling a document yes if and only if it contains at least one phrase labeled yes.",
                "To train classifiers for this task, we can take several viewpoints related to both the basic problems we have enumerated and the form of the labeled data.",
                "The document-level view treats each e-mail as a learning instance with an associated class-label.",
                "Then, the document can be converted to a feature-value vector and learning progresses as usual.",
                "Applying a document-level classifier to document detection and ranking is straightforward.",
                "In order to apply it to sentence detection, one must make additional steps.",
                "For example, if the classifier predicts a document contains an action-item, then areas of the document that contain a high-concentration of words which the model weights heavily in favor of action-items can be indicated.",
                "The obvious benefit of the document-level approach is that training set collection costs are lower since the user only has to specify whether or not an e-mail contains an action-item and not the specific sentences.",
                "In the sentence-level view, each e-mail is automatically segmented into sentences, and each sentence is treated as a learning instance with an associated class-label.",
                "Since the phrase-labeling provided by the user may not coincide with the automatic segmentation, we must determine what label to assign a partially overlapping sentence when converting it to a learning instance.",
                "Once trained, applying the resulting classifiers to sentence detection is now straightforward, but in order to apply the classifiers to document detection and document ranking, the individual predictions over each sentence must be aggregated in order to make a document-level prediction.",
                "This approach has the potential to benefit from morespecific labels that enable the learner to focus attention on the key sentences instead of having to learn based on data that the majority of the words in the e-mail provide no or little information about class membership. 3.2.1 Features Consider some of the phrases that might constitute part of an action item: would like to know, let me know, as soon as possible, have you.",
                "Each of these phrases consists of common words that occur in many e-mails.",
                "However, when they occur in the same sentence, they are far more indicative of an action-item.",
                "Additionally, order can be important: consider have you versus you have.",
                "Because of this, we posit that n-grams play a larger role in this problem than is typical of problems like topic classification.",
                "Therefore, we consider all n-grams up to size 4.",
                "When using n-grams, if we find an n-gram of size 4 in a segment of text, we can represent the text as just one occurrence of the ngram or as one occurrence of the n-gram and an occurrence of each smaller n-gram contained by it.",
                "We choose the second of these alternatives since this will allow the algorithm itself to smoothly back-off in terms of recall.",
                "Methods such as na¨ıve Bayes may be hurt by such a representation because of double-counting.",
                "Since sentence-ending punctuation can provide information, we retain the terminating punctuation token when it is identifiable.",
                "Additionally, we add a beginning-of-sentence and end-of-sentence token in order to capture patterns that are often indicators at the beginning or end of a sentence.",
                "Assuming proper punctuation, these extra tokens are unnecessary, but often e-mail lacks proper punctuation.",
                "In addition, for the sentence-level classifiers that use ngrams, we additionally code for each sentence a binary encoding of the position of the sentence relative to the document.",
                "This encoding has eight associated features that represent which octile (the first eighth, second eighth, etc.) contains the sentence. 3.2.2 Implementation Details In order to compare the document-level to the sentence-level approach, we compare predictions at the document-level.",
                "We do not address how to use a document-level classifier to make predictions at the sentence-level.",
                "In order to automatically segment the text of the e-mail, we use the RASP statistical parser [4].",
                "Since the automatically segmented sentences may not correspond directly with the phrase-level boundaries, we treat any sentence that contains at least 30% of a marked action-item segment as an action-item.",
                "When evaluating sentencedetection for the sentence-level system, we use these class labels as ground truth.",
                "Since we are not evaluating multiple segmentation approaches, this does not bias any of the methods.",
                "If multiple segmentation systems were under evaluation, one would need to use a metric that matched predicted positive sentences to phrases labeled positive.",
                "The metric would need to punish overly long true predictions as well as too short predictions.",
                "Our criteria for converting to labeled instances implicitly includes both criteria.",
                "Since the segmentation is fixed, an overly long prediction would be predicting yes for many no instances since presumably the extra length corresponds to additional segmented sentences all of which do not contain 30% of action-item.",
                "Likewise, a too short prediction must correspond to a small sentence included in the action-item but not constituting all of the action-item.",
                "Therefore, in order to consider the prediction to be too short, there will be an additional preceding/following sentence that is an action-item where we incorrectly predicted no.",
                "Once a sentence-level classifier has made a prediction for each sentence, we must combine these predictions to make both a document-level prediction and a document-level score.",
                "We use the simple policy of predicting positive when any of the sentences is predicted positive.",
                "In order to produce a document score for ranking, the confidence that the document contains an action-item is: ψ(d) = 1 n(d) s∈d|π(s)=1 ψ(s) if for any s ∈ d, π(s) = 1 1 n(d) maxs∈d ψ(s) o.w. where s is a sentence in document d, π is the classifiers 1/0 prediction, ψ is the score the classifier assigns as its confidence that π(s) = 1, and n(d) is the greater of 1 and the number of (unigram) tokens in the document.",
                "In other words, when any sentence is predicted positive, the document score is the length normalized sum of the sentence scores above threshold.",
                "When no sentence is predicted positive, the document score is the maximum sentence score normalized by length.",
                "As in other text problems, we are more likely to emit false positives for documents with more words or sentences.",
                "Thus we include a length normalization factor. 4.",
                "EXPERIMENTAL ANALYSIS 4.1 The Data Our corpus consists of e-mails obtained from volunteers at an educational institution and cover subjects such as: organizing a research workshop, arranging for job-candidate interviews, publishing proceedings, and talk announcements.",
                "The messages were anonymized by replacing the names of each individual and institution with a pseudonym.1 After attempting to identify and eliminate duplicate e-mails, the corpus contains 744 e-mail messages.",
                "After identity anonymization, the corpora has three basic versions.",
                "Quoted material refers to the text of a previous e-mail that an author often leaves in an e-mail message when responding to the e-mail.",
                "Quoted material can act as noise when learning since it may include action-items from previous messages that are no longer relevant.",
                "To isolate the effects of quoted material, we have three versions of the corpora.",
                "The raw form contains the basic messages.",
                "The auto-stripped version contains the messages after quoted material has been automatically removed.",
                "The hand-stripped version contains the messages after quoted material has been removed by a human.",
                "Additionally, the hand-stripped version has had any xml content and e-mail signatures removed - leaving only the essential content of the message.",
                "The studies reported here are performed with the hand-stripped version.",
                "This allows us to balance the cognitive load in terms of number of tokens that must be read in the user-studies we report - including quoted material would complicate the user studies since some users might skip the material while others read it.",
                "Additionally, ensuring all quoted material is removed 1 We have an even more highly anonymized version of the corpus that can be made available for some outside experimentation.",
                "Please contact the authors for more information on obtaining this data. prevents tainting the cross-validation since otherwise a test item could occur as quoted material in a training document. 4.1.1 Data Labeling Two human annotators labeled each message as to whether or not it contained an action-item.",
                "In addition, they identified each segment of the e-mail which contained an action-item.",
                "A segment is a contiguous section of text selected by the human annotators and may span several sentences or a complete phrase contained in a sentence.",
                "They were instructed that an action item is an explicit request for information that requires the recipients attention or a required action and told to highlight the phrases or sentences that make up the request.",
                "Annotator 1 No Yes Annotator 2 No 391 26 Yes 29 298 Table 1: Agreement of Human Annotators at Document Level Annotator One labeled 324 messages as containing action items.",
                "Annotator Two labeled 327 messages as containing action items.",
                "The agreement of the human annotators is shown in Tables 1 and 2.",
                "The annotators are said to agree at the document-level when both marked the same document as containing no action-items or both marked at least one action-item regardless of whether the text segments were the same.",
                "At the document-level, the annotators agreed 93% of the time.",
                "The kappa statistic [3, 5] is often used to evaluate inter-annotator agreement: κ = A − R 1 − R A is the empirical estimate of the probability of agreement.",
                "R is the empirical estimate of the probability of random agreement given the empirical class priors.",
                "A value close to −1 implies the annotators agree far less often than would be expected randomly, while a value close to 1 means they agree more often than randomly expected.",
                "At the document-level, the kappa statistic for inter-annotator agreement is 0.85.",
                "This value is both strong enough to expect the problem to be learnable and is comparable with results for similar tasks [5, 6].",
                "In order to determine the sentence-level agreement, we use each judgment to create a sentence-corpus with labels as described in Section 3.2.2, then consider the agreement over these sentences.",
                "This allows us to compare agreement over no judgments.",
                "We perform this comparison over the hand-stripped corpus since that eliminates spurious no judgments that would come from including quoted material, etc.",
                "Both annotators were free to label the subject as an action-item, but since neither did, we omit the subject line of the message as well.",
                "This only reduces the number of no agreements.",
                "This leaves 6301 automatically segmented sentences.",
                "At the sentence-level, the annotators agreed 98% of the time, and the kappa statistic for inter-annotator agreement is 0.82.",
                "In order to produce one single set of judgments, the human annotators went through each annotation where there was disagreement and came to a consensus opinion.",
                "The annotators did not collect statistics during this process but anecdotally reported that the majority of disagreements were either cases of clear annotator oversight or different interpretations of conditional statements.",
                "For example, If you would like to keep your job, come to tomorrows meeting implies a required action where If you would like to join Annotator 1 No Yes Annotator 2 No 5810 65 Yes 74 352 Table 2: Agreement of Human Annotators at Sentence Level the football betting pool, come to tomorrows meeting does not.",
                "The first would be an action-item in most contexts while the second would not.",
                "Of course, many conditional statements are not so clearly interpretable.",
                "After reconciling the judgments there are 416 e-mails with no action-items and 328 e-mails containing actionitems.",
                "Of the 328 e-mails containing action-items, 259 messages have one action-item segment; 55 messages have two action-item segments; 11 messages have three action-item segments.",
                "Two messages have four action-item segments, and one message has six action-item segments.",
                "Computing the sentence-level agreement using the reconciled gold standard judgments with each of the annotators individual judgments gives a kappa of 0.89 for Annotator One and a kappa of 0.92 for Annotator Two.",
                "In terms of message characteristics, there were on average 132 content tokens in the body after stripping.",
                "For action-item messages, there were 115.",
                "However, by examining Figure 2 we see the length distributions are nearly identical.",
                "As would be expected for e-mail, it is a long-tailed distribution with about half the messages having more than 60 tokens in the body (this paragraph has 65 tokens). 4.2 Classifiers For this experiment, we have selected a variety of standard text classification algorithms.",
                "In selecting algorithms, we have chosen algorithms that are not only known to work well but which differ along such lines as discriminative vs. generative and lazy vs. eager.",
                "We have done this in order to provide both a competitive and thorough sampling of learning methods for the task at hand.",
                "This is important since it is easy to improve a strawman classifier by introducing a new representation.",
                "By thoroughly sampling alternative classifier choices we demonstrate that representation improvements over bag-of-words are not due to using the information in the bag-of-words poorly. 4.2.1 kNN We employ a standard variant of the k-nearest neighbor algorithm used in text classification, kNN with s-cut score thresholding [19].",
                "We use a tfidf-weighting of the terms with a distanceweighted vote of the neighbors to compute the score before thresholding it.",
                "In order to choose the value of s for thresholding, we perform leave-one-out cross-validation over the training set.",
                "The value of k is set to be 2( log2 N + 1) where N is the number of training points.",
                "This rule for choosing k is theoretically motivated by results which show such a rule converges to the optimal classifier as the number of training points increases [8].",
                "In practice, we have also found it to be a computational convenience that frequently leads to comparable results with numerically optimizing k via a cross-validation procedure. 4.2.2 Na¨ıve Bayes We use a standard multinomial na¨ıve Bayes classifier [16].",
                "In using this classifier, we smoothed word and class probabilities using a Bayesian estimate (with the word prior) and a Laplace m-estimate, respectively. 0 20 40 60 80 100 120 140 160 0 200 400 600 800 1000 1200 1400 NumberofMessages Number of Tokens All Messages Action-Item Messages 0 0.02 0.04 0.06 0.08 0.1 0.12 0.14 0.16 0.18 0.2 0 200 400 600 800 1000 1200 1400 PercentageofMessages Number of Tokens All Messages Action-Item Messages Figure 2: The Histogram (left) and Distribution (right) of Message Length.",
                "A bin size of 20 words was used.",
                "Only tokens in the body after hand-stripping were counted.",
                "After stripping, the majority of words left are usually actual message content.",
                "Classifiers Document Unigram Document Ngram Sentence Unigram Sentence Ngram F1 kNN 0.6670 ± 0.0288 0.7108 ± 0.0699 0.7615 ± 0.0504 0.7790 ± 0.0460 na¨ıve Bayes 0.6572 ± 0.0749 0.6484 ± 0.0513 0.7715 ± 0.0597 0.7777 ± 0.0426 SVM 0.6904 ± 0.0347 0.7428 ± 0.0422 0.7282 ± 0.0698 0.7682 ± 0.0451 Voted Perceptron 0.6288 ± 0.0395 0.6774 ± 0.0422 0.6511 ± 0.0506 0.6798 ± 0.0913 Accuracy kNN 0.7029 ± 0.0659 0.7486 ± 0.0505 0.7972 ± 0.0435 0.8092 ± 0.0352 na¨ıve Bayes 0.6074 ± 0.0651 0.5816 ± 0.1075 0.7863 ± 0.0553 0.8145 ± 0.0268 SVM 0.7595 ± 0.0309 0.7904 ± 0.0349 0.7958 ± 0.0551 0.8173 ± 0.0258 Voted Perceptron 0.6531 ± 0.0390 0.7164 ± 0.0376 0.6413 ± 0.0833 0.7082 ± 0.1032 Table 3: Average Document-Detection Performance during Cross-Validation for Each Method and the Sample Standard Deviation (Sn−1) in italics.",
                "The best performance for each classifier is shown in bold. 4.2.3 SVM We have used a linear SVM with a tfidf feature representation and L2-norm as implemented in the SVMlight package v6.01 [11].",
                "All default settings were used. 4.2.4 Voted Perceptron Like the SVM, the Voted Perceptron is a kernel-based learning method.",
                "We use the same feature representation and kernel as we have for the SVM, a linear kernel with tfidf-weighting and an L2-norm.",
                "The voted perceptron is an online-learning method that keeps a history of past perceptrons used, as well as a weight signifying how often that perceptron was correct.",
                "With each new training example, a correct classification increases the weight on the current perceptron and an incorrect classification updates the perceptron.",
                "The output of the classifier uses the weights on the perceptra to make a final voted classification.",
                "When used in an offline-manner, multiple passes can be made through the training data.",
                "Both the voted perceptron and the SVM give a solution from the same hypothesis space - in this case, a linear classifier.",
                "Furthermore, it is well-known that the Voted Perceptron increases the margin of the solution after each pass through the training data [10].",
                "Since Cohen et al. [5] obtain worse results using an SVM than a Voted Perceptron with one training iteration, they conclude that the best solution for detecting speech acts may not lie in an area with a large margin.",
                "Because their tasks are highly similar to ours, we employ both classifiers to ensure we are not overlooking a competitive alternative classifier to the SVM for the basic bag-of-words representation. 4.3 Performance Measures To compare the performance of the classification methods, we look at two standard performance measures, F1 and accuracy.",
                "The F1 measure [18, 21] is the harmonic mean of precision and recall where Precision = Correct Positives Predicted Positives and Recall = Correct Positives Actual Positives . 4.4 Experimental Methodology We perform standard 10-fold cross-validation on the set of documents.",
                "For the sentence-level approach, all sentences in a document are either entirely in the training set or entirely in the test set for each fold.",
                "For significance tests, we use a two-tailed t-test [21] to compare the values obtained during each cross-validation fold with a p-value of 0.05.",
                "Feature selection was performed using the chi-squared statistic.",
                "Different levels of feature selection were considered for each classifier.",
                "Each of the following number of features was tried: 10, 25, 50, 100, 250, 750, 1000, 2000, 4000.",
                "There are approximately 4700 unigram tokens without feature selection.",
                "In order to choose the number of features to use for each classifier, we perform nested cross-validation and choose the settings that yield the optimal document-level F1 for that classifier.",
                "For this study, only the body of each e-mail message was used.",
                "Feature selection is always applied to all candidate features.",
                "That is, for the n-gram representation, the n-grams and position features are also subject to removal by the feature selection method. 4.5 Results The results for document-level classification are given in Table 3.",
                "The primary hypothesis we are concerned with is that n-grams are critical for this task; if this is true, we expect to see a significant gap in performance between the document-level classifiers that use n-grams (denoted Document Ngram) and those using only unigram features (denoted Document Unigram).",
                "Examining Table 3, we observe that this is indeed the case for every classifier except na¨ıve Bayes.",
                "This difference in performance produced by the n-gram representation is statistically significant for each classifier except for na¨ıve Bayes and the accuracy metric for kNN (see Table 4).",
                "Na¨ıve Bayes poor performance with the n-gram representation is not surprising since the bag-of-n-grams causes excessive doublecounting as mentioned in Section 3.2.1; however, na¨ıve Bayes is not hurt at the sentence-level because the sparse examples provide few chances for agglomerative effects of double counting.",
                "In either case, when a language-modeling approach is desired, modeling the n-grams directly would be preferable to na¨ıve Bayes.",
                "More importantly for the n-gram hypothesis, the n-grams lead to the best document-level classifier performance as well.",
                "As would be expected, the difference between the sentence-level n-gram representation and unigram representation is small.",
                "This is because the window of text is so small that the unigram representation, when done at the sentence-level, implicitly picks up on the power of the n-grams.",
                "Further improvement would signify that the order of the words matter even when only considering a small sentence-size window.",
                "Therefore, the finer-grained sentence-level judgments allows a unigram representation to succeed but only when performed in a small window - behaving as an n-gram representation for all practical purposes.",
                "Document Winner Sentence Winner kNN Ngram Ngram na¨ıve Bayes Unigram Ngram SVM Ngram† Ngram Voted Perceptron Ngram† Ngram Table 4: Significance results for n-grams versus unigrams for document detection using document-level and sentence-level classifiers.",
                "When the F1 result is statistically significant, it is shown in bold.",
                "When the accuracy result is significant, it is shown with a † .",
                "F1 Winner Accuracy Winner kNN Sentence Sentence na¨ıve Bayes Sentence Sentence SVM Sentence Sentence Voted Perceptron Sentence Document Table 5: Significance results for sentence-level classifiers vs. document-level classifiers for the document detection problem.",
                "When the result is statistically significant, it is shown in bold.",
                "Further highlighting the improvement from finer-grained judgments and n-grams, Figure 3 graphically depicts the edge the SVM sentence-level classifier has over the standard bag-of-words approach with a precision-recall curve.",
                "In the high precision area of the graph, the consistent edge of the sentence-level classifier is rather impressive - continuing at precision 1 out to 0.1 recall.",
                "This would mean that a tenth of the users action-items would be placed at the top of their action-item sorted inbox.",
                "Additionally, the large separation at the top right of the curves corresponds to the area where the optimal F1 occurs for each classifier, agreeing with the large improvement from 0.6904 to 0.7682 in F1 score.",
                "Considering the relative unexplored nature of classification at the sentence-level, this gives great hope for further increases in performance.",
                "Accuracy F1 Unigram Ngram Unigram Ngram kNN 0.9519 0.9536 0.6540 0.6686 na¨ıve Bayes 0.9419 0.9550 0.6176 0.6676 SVM 0.9559 0.9579 0.6271 0.6672 Voted Perceptron 0.8895 0.9247 0.3744 0.5164 Table 6: Performance of the Sentence-Level Classifiers at Sentence Detection Although Cohen et al. [5] observed that the Voted Perceptron with a single training iteration outperformed SVM in a set of similar tasks, we see no such behavior here.",
                "This further strengthens the evidence that an alternate classifier with the bag-of-words representation could not reach the same level of performance.",
                "The Voted Perceptron classifier does improve when the number of training iterations are increased, but it is still lower than the SVM classifier.",
                "Sentence detection results are presented in Table 6.",
                "With regard to the sentence detection problem, we note that the F1 measure gives a better feel for the remaining room for improvement in this difficult problem.",
                "That is, unlike document detection where actionitem documents are fairly common, action-item sentences are very rare.",
                "Thus, as in other text problems, the accuracy numbers are deceptively high sheerly because of the default accuracy attainable by always predicting no.",
                "Although, the results here are significantly above-random, it is unclear what level of performance is necessary for sentence detection to be useful in and of itself and not simply as a means to document ranking and classification.",
                "Figure 4: Users find action-items quicker when assisted by a classification system.",
                "Finally, when considering a new type of classification task, one of the most basic questions is whether an accurate classifier built for the task can have an impact on the end-user.",
                "In order to demonstrate the impact this task can have on e-mail users, we conducted a user study using an earlier less-accurate version of the sentence classifier - where instead of using just a single sentence, a threesentence windowed-approach was used.",
                "There were three distinct sets of e-mail in which users had to find action-items.",
                "These sets were either presented in a random order (Unordered), ordered by the classifier (Ordered), or ordered by the classifier and with the 0.4 0.5 0.6 0.7 0.8 0.9 1 0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1 Precision Recall Action-Item Detection SVM Performance (Post Model Selection) Document Unigram Sentence Ngram Figure 3: Both n-grams and a small prediction window lead to consistent improvements over the standard approach. center sentence in the highest confidence window highlighted (Order+help).",
                "In order to perform fair comparisons between conditions, the overall number of tokens in each message set should be approximately equal; that is, the cognitive reading load should be approximately the same before the classifiers reordering.",
                "Additionally, users typically show practice effects by improving at the overall task and thus performing better at later message sets.",
                "This is typically handled by varying the ordering of the sets across users so that the means are comparable.",
                "While omitting further detail, we note the sets were balanced for the total number of tokens and a latin square design was used to balance practice effects.",
                "Figure 4 shows that at intervals of 5, 10, and 15 minutes, users consistently found significantly more action-items when assisted by the classifier, but were most critically aided in the first five minutes.",
                "Although, the classifier consistently aids the users, we did not gain an additional end-user impact by highlighting.",
                "As mentioned above, this might be a result of the large room for improvement that still exists for sentence detection, but anecdotal evidence suggests this might also be a result of how the information is presented to the user rather than the accuracy of sentence detection.",
                "For example, highlighting the wrong sentence near an actual action-item hurts the users trust, but if a vague indicator (e.g., an arrow) points to the approximate area the user is not aware of the near-miss.",
                "Since the user studies used a three sentence window, we believe this played a role as well as sentence detection accuracy. 4.6 Discussion In contrast to problems where n-grams have yielded little difference, we believe their power here stems from the fact that many of the meaningful n-grams for action-items consist of common words, e.g., let me know.",
                "Therefore, the document-level unigram approach cannot gain much leverage, even when modeling their joint probability correctly, since these words will often co-occur in the document but not necessarily in a phrase.",
                "Additionally, action-item detection is distinct from many text classification tasks in that a single sentence can change the class label of the document.",
                "As a result, good classifiers cannot rely on aggregating evidence from a large number of weak indicators across the entire document.",
                "Even though we discarded the header information, examining the top-ranked features at the document-level reveals that many of the features are names or parts of e-mail addresses that occurred in the body and are highly associated with e-mails that tend to contain many or no action-items.",
                "A few examples are terms such as org, bob, and gov.",
                "We note that these features will be sensitive to the particular distribution (senders/receivers) and thus the document-level approach may produce classifiers that transfer less readily to alternate contexts and users at different institutions.",
                "This points out that part of the problem of going beyond bag-of-words may be the methodology, and investigating such properties as learning curves and how well a model transfers may highlight differences in models which appear to have similar performance when tested on the distributions they were trained on.",
                "We are currently investigating whether the sentence-level classifiers do perform better over different test corpora without retraining. 5.",
                "FUTURE WORK While applying text classifiers at the document-level is fairly well-understood, there exists the potential for significantly increasing the performance of the sentence-level classifiers.",
                "Such methods include alternate ways of combining the predictions over each sentence, weightings other than tfidf, which may not be appropriate since sentences are small, better sentence segmentation, and other types of phrasal analysis.",
                "Additionally, named entity tagging, time expressions, etc., seem likely candidates for features that can further improve this task.",
                "We are currently pursuing some of these avenues to see what additional gains these offer.",
                "Finally, it would be interesting to investigate the best methods for combining the document-level and sentence-level classifiers.",
                "Since the simple bag-of-words representation at the document-level leads to a learned model that behaves somewhat like a context-specific prior dependent on the sender/receiver and general topic, a first choice would be to treat it as such when combining probability estimates with the sentence-level classifier.",
                "Such a model might serve as a general example for other problems where bag-of-words can establish a baseline model but richer approaches are needed to achieve performance beyond that baseline. 6.",
                "SUMMARY AND CONCLUSIONS The effectiveness of sentence-level detection argues that labeling at the sentence-level provides significant value.",
                "Further experiments are needed to see how this interacts with the amount of training data available.",
                "Sentence detection that is then agglomerated to document-level detection works surprisingly better given low recall than would be expected with sentence-level items.",
                "This, in turn, indicates that improved sentence segmentation methods could yield further improvements in classification.",
                "In this work, we examined how action-items can be effectively detected in e-mails.",
                "Our empirical analysis has demonstrated that n-grams are of key importance to making the most of documentlevel judgments.",
                "When finer-grained judgments are available, then a standard bag-of-words approach using a small (sentence) window size and automatic segmentation techniques can produce results almost as good as the n-gram based approaches.",
                "Acknowledgments This material is based upon work supported by the Defense Advanced Research Projects Agency (DARPA) under Contract No.",
                "NBCHD030010.",
                "Any opinions, findings and conclusions or recommendations expressed in this material are those of the author(s) and do not necessarily reflect the views of the Defense Advanced Research Projects Agency (DARPA), or the Department of InteriorNational Business Center (DOI-NBC).",
                "We would like to extend our sincerest thanks to Jill Lehman whose efforts in data collection were essential in constructing the corpus, and both Jill and Aaron Steinfeld for their direction of the HCI experiments.",
                "We would also like to thank Django Wexler for constructing and supporting the corpus labeling tools and Curtis Huttenhowers support of the text preprocessing package.",
                "Finally, we gratefully acknowledge Scott Fahlman for his encouragement and useful discussions on this topic. 7.",
                "REFERENCES [1] J. Allan, J. Carbonell, G. Doddington, J. Yamron, and Y. Yang.",
                "Topic detection and tracking pilot study: Final report.",
                "In Proceedings of the DARPA Broadcast News Transcription and Understanding Workshop, Washington, D.C., 1998. [2] C. Apte, F. Damerau, and S. M. Weiss.",
                "Automated learning of decision rules for text categorization.",
                "ACM Transactions on Information Systems, 12(3):233-251, July 1994. [3] J. Carletta.",
                "Assessing agreement on classification tasks: The kappa statistic.",
                "Computational Linguistics, 22(2):249-254, 1996. [4] J. Carroll.",
                "High precision extraction of grammatical relations.",
                "In Proceedings of the 19th International Conference on Computational Linguistics (COLING), pages 134-140, 2002. [5] W. W. Cohen, V. R. Carvalho, and T. M. Mitchell.",
                "Learning to classify email into speech acts.",
                "In EMNLP-2004 (Conference on Empirical Methods in Natural Language Processing), pages 309-316, 2004. [6] S. Corston-Oliver, E. Ringger, M. Gamon, and R. Campbell.",
                "Task-focused summarization of email.",
                "In Text Summarization Branches Out: Proceedings of the ACL-04 Workshop, pages 43-50, 2004. [7] A. Culotta, R. Bekkerman, and A. McCallum.",
                "Extracting social networks and contact information from email and the web.",
                "In CEAS-2004 (Conference on Email and Anti-Spam), Mountain View, CA, July 2004. [8] L. Devroye, L. Gy¨orfi, and G. Lugosi.",
                "A Probabilistic Theory of Pattern Recognition.",
                "Springer-Verlag, New York, NY, 1996. [9] S. T. Dumais, J. Platt, D. Heckerman, and M. Sahami.",
                "Inductive learning algorithms and representations for text categorization.",
                "In CIKM 98, Proceedings of the 7th ACM Conference on Information and Knowledge Management, pages 148-155, 1998. [10] Y. Freund and R. Schapire.",
                "Large margin classification using the perceptron algorithm.",
                "Machine Learning, 37(3):277-296, 1999. [11] T. Joachims.",
                "Making large-scale svm learning practical.",
                "In B. Sch¨olkopf, C. J. Burges, and A. J. Smola, editors, Advances in Kernel Methods - Support Vector Learning, pages 41-56.",
                "MIT Press, 1999. [12] L. S. Larkey.",
                "A patent search and classification system.",
                "In Proceedings of the Fourth ACM Conference on Digital Libraries, pages 179 - 187, 1999. [13] D. D. Lewis.",
                "An evaluation of phrasal and clustered representations on a text categorization task.",
                "In SIGIR 92, Proceedings of the 15th Annual International ACM Conference on Research and Development in <br>information retrieval</br>, pages 37-50, 1992. [14] Y. Liu, J. Carbonell, and R. Jin.",
                "A pairwise ensemble approach for accurate genre classification.",
                "In Proceedings of the European Conference on Machine Learning (ECML), 2003. [15] Y. Liu, R. Yan, R. Jin, and J. Carbonell.",
                "A comparison study of kernels for multi-label text classification using category association.",
                "In The Twenty-first International Conference on Machine Learning (ICML), 2004. [16] A. McCallum and K. Nigam.",
                "A comparison of event models for naive bayes text classification.",
                "In Working Notes of AAAI 98 (The 15th National Conference on Artificial Intelligence), Workshop on Learning for Text Categorization, pages 41-48, 1998.",
                "TR WS-98-05. [17] F. Sebastiani.",
                "Machine learning in automated text categorization.",
                "ACM Computing Surveys, 34(1):1-47, March 2002. [18] C. J. van Rijsbergen.",
                "<br>information retrieval</br>.",
                "Butterworths, London, 1979. [19] Y. Yang.",
                "An evaluation of statistical approaches to text categorization.",
                "<br>information retrieval</br>, 1(1/2):67-88, 1999. [20] Y. Yang, J. Carbonell, R. Brown, T. Pierce, B. T. Archibald, and X. Liu.",
                "Learning approaches to topic detection and tracking.",
                "IEEE EXPERT, Special Issue on Applications of Intelligent <br>information retrieval</br>, 1999. [21] Y. Yang and X. Liu.",
                "A re-examination of text categorization methods.",
                "In SIGIR 99, Proceedings of the 22nd Annual International ACM Conference on Research and Development in <br>information retrieval</br>, pages 42-49, 1999. [22] Y. Yang, J. Zhang, J. Carbonell, and C. Jin.",
                "Topic-conditioned novelty detection.",
                "In Proceedings of the ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, July 2002."
            ],
            "original_annotated_samples": [],
            "translated_annotated_samples": [
                "Mientras que la \"recuperación de información\" y las técnicas de aprendizaje automático están obteniendo aceptación inicial en el filtrado de spam y la asignación de carpetas automatizadas, este documento informa sobre una nueva tarea: detección automatizada de elementos de acción, para marcar correos electrónicos que requieren respuestas y para resaltar el pasaje específico(S) que indica la (s) solicitud (s) de acción.",
                "Como en la mayoría de las tareas de \"recuperación de información\", el peso que la métrica de evaluación debe dar a la precisión y el recuerdo depende de la naturaleza de la aplicación.",
                "En Sigir 92, Actas de la 15ª Conferencia Internacional de ACM anual sobre investigación y desarrollo en \"Recuperación de información\", páginas 37-50, 1992. [14] Y. Liu, J. Carbonell y R. Jin.",
                "\"recuperación de información\".",
                "\"Recuperación de información\", 1 (1/2): 67-88, 1999. [20] Y. Yang, J. Carbonell, R. Brown, T. Pierce, B. T. Archibald y X. Liu.",
                "IEEE Expert, número especial sobre aplicaciones de \"recuperación de información\" inteligente, 1999. [21] Y. Yang y X. Liu.",
                "En Sigir 99, Actas de la 22a Conferencia Anual Internacional de ACM sobre investigación y desarrollo en \"Recuperación de información\", páginas 42-49, 1999. [22] Y. Yang, J. Zhang, J. Carbonell y C. Jin."
            ],
            "translated_text": "",
            "candidates": [
                "recuperación de información",
                "recuperación de información",
                "recuperación de información",
                "recuperación de información",
                "recuperación de información",
                "Recuperación de información",
                "recuperación de información",
                "recuperación de información",
                "recuperación de información",
                "Recuperación de información",
                "recuperación de información",
                "recuperación de información",
                "recuperación de información",
                "Recuperación de información"
            ],
            "error": []
        },
        "topic-driven text classification": {
            "translated_key": "Clasificación de texto impulsada por el tema",
            "is_in_text": true,
            "original_annotated_sentences": [
                "Feature Representation for Effective Action-Item Detection Paul N. Bennett Computer Science Department Carnegie Mellon University Pittsburgh, PA 15213 pbennett+@cs.cmu.edu Jaime Carbonell Language Technologies Institute.",
                "Carnegie Mellon University Pittsburgh, PA 15213 jgc+@cs.cmu.edu ABSTRACT E-mail users face an ever-growing challenge in managing their inboxes due to the growing centrality of email in the workplace for task assignment, action requests, and other roles beyond information dissemination.",
                "Whereas Information Retrieval and Machine Learning techniques are gaining initial acceptance in spam filtering and automated folder assignment, this paper reports on a new task: automated action-item detection, in order to flag emails that require responses, and to highlight the specific passage(s) indicating the request(s) for action.",
                "Unlike standard <br>topic-driven text classification</br>, action-item detection requires inferring the senders intent, and as such responds less well to pure bag-of-words classification.",
                "However, using enriched feature sets, such as n-grams (up to n=4) with chi-squared feature selection, and contextual cues for action-item location improve performance by up to 10% over unigrams, using in both cases state of the art classifiers such as SVMs with automated model selection via embedded cross-validation.",
                "Categories and Subject Descriptors H.3.3 [Information Storage and Retrieval]: Information Search and Retrieval; I.2.6 [Artificial Intelligence]: Learning; I.5.4 [Pattern Recognition]: Applications General Terms Experimentation 1.",
                "INTRODUCTION E-mail users are facing an increasingly difficult task of managing their inboxes in the face of mounting challenges that result from rising e-mail usage.",
                "This includes prioritizing e-mails over a range of sources from business partners to family members, filtering and reducing junk e-mail, and quickly managing requests that demand From: Henry Hutchins <hhutchins@innovative.company.com> To: Sara Smith; Joe Johnson; William Woolings Subject: meeting with prospective customers Sent: Fri 12/10/2005 8:08 AM Hi All, Id like to remind all of you that the group from GRTY will be visiting us next Friday at 4:30 p.m.",
                "The current schedule looks like this: + 9:30 a.m.",
                "Informal Breakfast and Discussion in Cafeteria + 10:30 a.m. Company Overview + 11:00 a.m.",
                "Individual Meetings (Continue Over Lunch) + 2:00 p.m. Tour of Facilities + 3:00 p.m.",
                "Sales Pitch In order to have this go off smoothly, I would like to practice the presentation well in advance.",
                "As a result, I will need each of your parts by Wednesday.",
                "Keep up the good work! -Henry Figure 1: An E-mail with emphasized Action-Item, an explicit request that requires the recipients attention or action. the receivers attention or action.",
                "Automated action-item detection targets the third of these problems by attempting to detect which e-mails require an action or response with information, and within those e-mails, attempting to highlight the sentence (or other passage length) that directly indicates the action request.",
                "Such a detection system can be used as one part of an e-mail agent which would assist a user in processing important e-mails quicker than would have been possible without the agent.",
                "We view action-item detection as one necessary component of a successful e-mail agent which would perform spam detection, action-item detection, topic classification and priority ranking, among other functions.",
                "The utility of such a detector can manifest as a method of prioritizing e-mails according to task-oriented criteria other than the standard ones of topic and sender or as a means of ensuring that the email user hasnt dropped the proverbial ball by forgetting to address an action request.",
                "Action-item detection differs from standard text classification in two important ways.",
                "First, the user is interested both in detecting whether an email contains action items and in locating exactly where these action item requests are contained within the email body.",
                "In contrast, standard text categorization merely assigns a topic label to each text, whether that label corresponds to an e-mail folder or a controlled indexing vocabulary [12, 15, 22].",
                "Second, action-item detection attempts to recover the email senders intent - whether she means to elicit response or action on the part of the receiver; note that for this task, classifiers using only unigrams as features do not perform optimally, as evidenced in our results below.",
                "Instead we find that we need more information-laden features such as higher-order n-grams.",
                "Text categorization by topic, on the other hand, works very well using just individual words as features [2, 9, 13, 17].",
                "In fact, genre-classification, which one would think may require more than a bag-of-words approach, also works quite well using just unigram features [14].",
                "Topic detection and tracking (TDT), also works well with unigram feature sets [1, 20].",
                "We believe that action-item detection is one of the first clear instances of an IR-related task where we must move beyond bag-of-words to achieve high performance, albeit not too far, as bag-of-n-grams seem to suffice.",
                "We first review related work for similar text classification problems such as e-mail priority ranking and speech act identification.",
                "Then we more formally define the action-item detection problem, discuss the aspects that distinguish it from more common problems like topic classification, and highlight the challenges in constructing systems that can perform well at the sentence and document level.",
                "From there, we move to a discussion of feature representation and selection techniques appropriate for this problem and how standard text classification approaches can be adapted to smoothly move from the sentence-level detection problem to the documentlevel classification problem.",
                "We then conduct an empirical analysis that helps us determine the effectiveness of our feature extraction procedures as well as establish baselines for a number of classification algorithms on this task.",
                "Finally, we summarize this papers contributions and consider interesting directions for future work. 2.",
                "RELATED WORK Several other researchers have considered very similar text classification tasks.",
                "Cohen et al. [5] describe an ontology of speech acts, such as Propose a Meeting, and attempt to predict when an e-mail contains one of these speech acts.",
                "We consider action-items to be an important specific type of speech act that falls within their more general classification.",
                "While they provide results for several classification methods, their methods only make use of human judgments at the document-level.",
                "In contrast, we consider whether accuracy can be increased by using finer-grained human judgments that mark the specific sentences and phrases of interest.",
                "Corston-Oliver et al. [6] consider detecting items in e-mail to Put on a To-Do List.",
                "This classification task is very similar to ours except they do not consider simple factual questions to belong to this category.",
                "We include questions, but note that not all questions are action-items - some are rhetorical or simply social convention, How are you?.",
                "From a learning perspective, while they make use of judgments at the sentence-level, they do not explicitly compare what if any benefits finer-grained judgments offer.",
                "Additionally, they do not study alternative choices or approaches to the classification task.",
                "Instead, they simply apply a standard SVM at the sentence-level and focus primarily on a linguistic analysis of how the sentence can be logically reformulated before adding it to the task list.",
                "In this study, we examine several alternative classification methods, compare document-level and sentence-level approaches and analyze the machine learning issues implicit in these problems.",
                "Interest in a variety of learning tasks related to e-mail has been rapidly growing in the recent literature.",
                "For example, in a forum dedicated to e-mail learning tasks, Culotta et al. [7] presented methods for learning social networks from e-mail.",
                "In this work, we do not focus on peer relationships; however, such methods could complement those here since peer relationships often influence word choice when requesting an action. 3.",
                "PROBLEM DEFINITION & APPROACH In contrast to previous work, we explicitly focus on the benefits that finer-grained, more costly, sentence-level human judgments offer over coarse-grained document-level judgments.",
                "Additionally, we consider multiple standard text classification approaches and analyze both the quantitative and qualitative differences that arise from taking a document-level vs. a sentence-level approach to classification.",
                "Finally, we focus on the representation necessary to achieve the most competitive performance. 3.1 Problem Definition In order to provide the most benefit to the user, a system would not only detect the document, but it would also indicate the specific sentences in the e-mail which contain the action-items.",
                "Therefore, there are three basic problems: 1.",
                "Document detection: Classify a document as to whether or not it contains an action-item. 2.",
                "Document ranking: Rank the documents such that all documents containing action-items occur as high as possible in the ranking. 3.",
                "Sentence detection: Classify each sentence in a document as to whether or not it is an action-item.",
                "As in most Information Retrieval tasks, the weight the evaluation metric should give to precision and recall depends on the nature of the application.",
                "In situations where a user will eventually read all received messages, ranking (e.g., via precision at recall of 1) may be most important since this will help encourage shorter delays in communications between users.",
                "In contrast, high-precision detection at low recall will be of increasing importance when the user is under severe time-pressure and therefore will likely not read all mail.",
                "This can be the case for crisis managers during disaster management.",
                "Finally, sentence detection plays a role in both timepressure situations and simply to alleviate the users required time to gist the message. 3.2 Approach As mentioned above, the labeled data can come in one of two forms: a document-labeling provides a yes/no label for each document as to whether it contains an action-item; a phrase-labeling provides only a yes label for the specific items of interest.",
                "We term the human judgments a phrase-labeling since the users view of the action-item may not correspond with actual sentence boundaries or predicted sentence boundaries.",
                "Obviously, it is straightforward to generate a document-labeling consistent with a phrase-labeling by labeling a document yes if and only if it contains at least one phrase labeled yes.",
                "To train classifiers for this task, we can take several viewpoints related to both the basic problems we have enumerated and the form of the labeled data.",
                "The document-level view treats each e-mail as a learning instance with an associated class-label.",
                "Then, the document can be converted to a feature-value vector and learning progresses as usual.",
                "Applying a document-level classifier to document detection and ranking is straightforward.",
                "In order to apply it to sentence detection, one must make additional steps.",
                "For example, if the classifier predicts a document contains an action-item, then areas of the document that contain a high-concentration of words which the model weights heavily in favor of action-items can be indicated.",
                "The obvious benefit of the document-level approach is that training set collection costs are lower since the user only has to specify whether or not an e-mail contains an action-item and not the specific sentences.",
                "In the sentence-level view, each e-mail is automatically segmented into sentences, and each sentence is treated as a learning instance with an associated class-label.",
                "Since the phrase-labeling provided by the user may not coincide with the automatic segmentation, we must determine what label to assign a partially overlapping sentence when converting it to a learning instance.",
                "Once trained, applying the resulting classifiers to sentence detection is now straightforward, but in order to apply the classifiers to document detection and document ranking, the individual predictions over each sentence must be aggregated in order to make a document-level prediction.",
                "This approach has the potential to benefit from morespecific labels that enable the learner to focus attention on the key sentences instead of having to learn based on data that the majority of the words in the e-mail provide no or little information about class membership. 3.2.1 Features Consider some of the phrases that might constitute part of an action item: would like to know, let me know, as soon as possible, have you.",
                "Each of these phrases consists of common words that occur in many e-mails.",
                "However, when they occur in the same sentence, they are far more indicative of an action-item.",
                "Additionally, order can be important: consider have you versus you have.",
                "Because of this, we posit that n-grams play a larger role in this problem than is typical of problems like topic classification.",
                "Therefore, we consider all n-grams up to size 4.",
                "When using n-grams, if we find an n-gram of size 4 in a segment of text, we can represent the text as just one occurrence of the ngram or as one occurrence of the n-gram and an occurrence of each smaller n-gram contained by it.",
                "We choose the second of these alternatives since this will allow the algorithm itself to smoothly back-off in terms of recall.",
                "Methods such as na¨ıve Bayes may be hurt by such a representation because of double-counting.",
                "Since sentence-ending punctuation can provide information, we retain the terminating punctuation token when it is identifiable.",
                "Additionally, we add a beginning-of-sentence and end-of-sentence token in order to capture patterns that are often indicators at the beginning or end of a sentence.",
                "Assuming proper punctuation, these extra tokens are unnecessary, but often e-mail lacks proper punctuation.",
                "In addition, for the sentence-level classifiers that use ngrams, we additionally code for each sentence a binary encoding of the position of the sentence relative to the document.",
                "This encoding has eight associated features that represent which octile (the first eighth, second eighth, etc.) contains the sentence. 3.2.2 Implementation Details In order to compare the document-level to the sentence-level approach, we compare predictions at the document-level.",
                "We do not address how to use a document-level classifier to make predictions at the sentence-level.",
                "In order to automatically segment the text of the e-mail, we use the RASP statistical parser [4].",
                "Since the automatically segmented sentences may not correspond directly with the phrase-level boundaries, we treat any sentence that contains at least 30% of a marked action-item segment as an action-item.",
                "When evaluating sentencedetection for the sentence-level system, we use these class labels as ground truth.",
                "Since we are not evaluating multiple segmentation approaches, this does not bias any of the methods.",
                "If multiple segmentation systems were under evaluation, one would need to use a metric that matched predicted positive sentences to phrases labeled positive.",
                "The metric would need to punish overly long true predictions as well as too short predictions.",
                "Our criteria for converting to labeled instances implicitly includes both criteria.",
                "Since the segmentation is fixed, an overly long prediction would be predicting yes for many no instances since presumably the extra length corresponds to additional segmented sentences all of which do not contain 30% of action-item.",
                "Likewise, a too short prediction must correspond to a small sentence included in the action-item but not constituting all of the action-item.",
                "Therefore, in order to consider the prediction to be too short, there will be an additional preceding/following sentence that is an action-item where we incorrectly predicted no.",
                "Once a sentence-level classifier has made a prediction for each sentence, we must combine these predictions to make both a document-level prediction and a document-level score.",
                "We use the simple policy of predicting positive when any of the sentences is predicted positive.",
                "In order to produce a document score for ranking, the confidence that the document contains an action-item is: ψ(d) = 1 n(d) s∈d|π(s)=1 ψ(s) if for any s ∈ d, π(s) = 1 1 n(d) maxs∈d ψ(s) o.w. where s is a sentence in document d, π is the classifiers 1/0 prediction, ψ is the score the classifier assigns as its confidence that π(s) = 1, and n(d) is the greater of 1 and the number of (unigram) tokens in the document.",
                "In other words, when any sentence is predicted positive, the document score is the length normalized sum of the sentence scores above threshold.",
                "When no sentence is predicted positive, the document score is the maximum sentence score normalized by length.",
                "As in other text problems, we are more likely to emit false positives for documents with more words or sentences.",
                "Thus we include a length normalization factor. 4.",
                "EXPERIMENTAL ANALYSIS 4.1 The Data Our corpus consists of e-mails obtained from volunteers at an educational institution and cover subjects such as: organizing a research workshop, arranging for job-candidate interviews, publishing proceedings, and talk announcements.",
                "The messages were anonymized by replacing the names of each individual and institution with a pseudonym.1 After attempting to identify and eliminate duplicate e-mails, the corpus contains 744 e-mail messages.",
                "After identity anonymization, the corpora has three basic versions.",
                "Quoted material refers to the text of a previous e-mail that an author often leaves in an e-mail message when responding to the e-mail.",
                "Quoted material can act as noise when learning since it may include action-items from previous messages that are no longer relevant.",
                "To isolate the effects of quoted material, we have three versions of the corpora.",
                "The raw form contains the basic messages.",
                "The auto-stripped version contains the messages after quoted material has been automatically removed.",
                "The hand-stripped version contains the messages after quoted material has been removed by a human.",
                "Additionally, the hand-stripped version has had any xml content and e-mail signatures removed - leaving only the essential content of the message.",
                "The studies reported here are performed with the hand-stripped version.",
                "This allows us to balance the cognitive load in terms of number of tokens that must be read in the user-studies we report - including quoted material would complicate the user studies since some users might skip the material while others read it.",
                "Additionally, ensuring all quoted material is removed 1 We have an even more highly anonymized version of the corpus that can be made available for some outside experimentation.",
                "Please contact the authors for more information on obtaining this data. prevents tainting the cross-validation since otherwise a test item could occur as quoted material in a training document. 4.1.1 Data Labeling Two human annotators labeled each message as to whether or not it contained an action-item.",
                "In addition, they identified each segment of the e-mail which contained an action-item.",
                "A segment is a contiguous section of text selected by the human annotators and may span several sentences or a complete phrase contained in a sentence.",
                "They were instructed that an action item is an explicit request for information that requires the recipients attention or a required action and told to highlight the phrases or sentences that make up the request.",
                "Annotator 1 No Yes Annotator 2 No 391 26 Yes 29 298 Table 1: Agreement of Human Annotators at Document Level Annotator One labeled 324 messages as containing action items.",
                "Annotator Two labeled 327 messages as containing action items.",
                "The agreement of the human annotators is shown in Tables 1 and 2.",
                "The annotators are said to agree at the document-level when both marked the same document as containing no action-items or both marked at least one action-item regardless of whether the text segments were the same.",
                "At the document-level, the annotators agreed 93% of the time.",
                "The kappa statistic [3, 5] is often used to evaluate inter-annotator agreement: κ = A − R 1 − R A is the empirical estimate of the probability of agreement.",
                "R is the empirical estimate of the probability of random agreement given the empirical class priors.",
                "A value close to −1 implies the annotators agree far less often than would be expected randomly, while a value close to 1 means they agree more often than randomly expected.",
                "At the document-level, the kappa statistic for inter-annotator agreement is 0.85.",
                "This value is both strong enough to expect the problem to be learnable and is comparable with results for similar tasks [5, 6].",
                "In order to determine the sentence-level agreement, we use each judgment to create a sentence-corpus with labels as described in Section 3.2.2, then consider the agreement over these sentences.",
                "This allows us to compare agreement over no judgments.",
                "We perform this comparison over the hand-stripped corpus since that eliminates spurious no judgments that would come from including quoted material, etc.",
                "Both annotators were free to label the subject as an action-item, but since neither did, we omit the subject line of the message as well.",
                "This only reduces the number of no agreements.",
                "This leaves 6301 automatically segmented sentences.",
                "At the sentence-level, the annotators agreed 98% of the time, and the kappa statistic for inter-annotator agreement is 0.82.",
                "In order to produce one single set of judgments, the human annotators went through each annotation where there was disagreement and came to a consensus opinion.",
                "The annotators did not collect statistics during this process but anecdotally reported that the majority of disagreements were either cases of clear annotator oversight or different interpretations of conditional statements.",
                "For example, If you would like to keep your job, come to tomorrows meeting implies a required action where If you would like to join Annotator 1 No Yes Annotator 2 No 5810 65 Yes 74 352 Table 2: Agreement of Human Annotators at Sentence Level the football betting pool, come to tomorrows meeting does not.",
                "The first would be an action-item in most contexts while the second would not.",
                "Of course, many conditional statements are not so clearly interpretable.",
                "After reconciling the judgments there are 416 e-mails with no action-items and 328 e-mails containing actionitems.",
                "Of the 328 e-mails containing action-items, 259 messages have one action-item segment; 55 messages have two action-item segments; 11 messages have three action-item segments.",
                "Two messages have four action-item segments, and one message has six action-item segments.",
                "Computing the sentence-level agreement using the reconciled gold standard judgments with each of the annotators individual judgments gives a kappa of 0.89 for Annotator One and a kappa of 0.92 for Annotator Two.",
                "In terms of message characteristics, there were on average 132 content tokens in the body after stripping.",
                "For action-item messages, there were 115.",
                "However, by examining Figure 2 we see the length distributions are nearly identical.",
                "As would be expected for e-mail, it is a long-tailed distribution with about half the messages having more than 60 tokens in the body (this paragraph has 65 tokens). 4.2 Classifiers For this experiment, we have selected a variety of standard text classification algorithms.",
                "In selecting algorithms, we have chosen algorithms that are not only known to work well but which differ along such lines as discriminative vs. generative and lazy vs. eager.",
                "We have done this in order to provide both a competitive and thorough sampling of learning methods for the task at hand.",
                "This is important since it is easy to improve a strawman classifier by introducing a new representation.",
                "By thoroughly sampling alternative classifier choices we demonstrate that representation improvements over bag-of-words are not due to using the information in the bag-of-words poorly. 4.2.1 kNN We employ a standard variant of the k-nearest neighbor algorithm used in text classification, kNN with s-cut score thresholding [19].",
                "We use a tfidf-weighting of the terms with a distanceweighted vote of the neighbors to compute the score before thresholding it.",
                "In order to choose the value of s for thresholding, we perform leave-one-out cross-validation over the training set.",
                "The value of k is set to be 2( log2 N + 1) where N is the number of training points.",
                "This rule for choosing k is theoretically motivated by results which show such a rule converges to the optimal classifier as the number of training points increases [8].",
                "In practice, we have also found it to be a computational convenience that frequently leads to comparable results with numerically optimizing k via a cross-validation procedure. 4.2.2 Na¨ıve Bayes We use a standard multinomial na¨ıve Bayes classifier [16].",
                "In using this classifier, we smoothed word and class probabilities using a Bayesian estimate (with the word prior) and a Laplace m-estimate, respectively. 0 20 40 60 80 100 120 140 160 0 200 400 600 800 1000 1200 1400 NumberofMessages Number of Tokens All Messages Action-Item Messages 0 0.02 0.04 0.06 0.08 0.1 0.12 0.14 0.16 0.18 0.2 0 200 400 600 800 1000 1200 1400 PercentageofMessages Number of Tokens All Messages Action-Item Messages Figure 2: The Histogram (left) and Distribution (right) of Message Length.",
                "A bin size of 20 words was used.",
                "Only tokens in the body after hand-stripping were counted.",
                "After stripping, the majority of words left are usually actual message content.",
                "Classifiers Document Unigram Document Ngram Sentence Unigram Sentence Ngram F1 kNN 0.6670 ± 0.0288 0.7108 ± 0.0699 0.7615 ± 0.0504 0.7790 ± 0.0460 na¨ıve Bayes 0.6572 ± 0.0749 0.6484 ± 0.0513 0.7715 ± 0.0597 0.7777 ± 0.0426 SVM 0.6904 ± 0.0347 0.7428 ± 0.0422 0.7282 ± 0.0698 0.7682 ± 0.0451 Voted Perceptron 0.6288 ± 0.0395 0.6774 ± 0.0422 0.6511 ± 0.0506 0.6798 ± 0.0913 Accuracy kNN 0.7029 ± 0.0659 0.7486 ± 0.0505 0.7972 ± 0.0435 0.8092 ± 0.0352 na¨ıve Bayes 0.6074 ± 0.0651 0.5816 ± 0.1075 0.7863 ± 0.0553 0.8145 ± 0.0268 SVM 0.7595 ± 0.0309 0.7904 ± 0.0349 0.7958 ± 0.0551 0.8173 ± 0.0258 Voted Perceptron 0.6531 ± 0.0390 0.7164 ± 0.0376 0.6413 ± 0.0833 0.7082 ± 0.1032 Table 3: Average Document-Detection Performance during Cross-Validation for Each Method and the Sample Standard Deviation (Sn−1) in italics.",
                "The best performance for each classifier is shown in bold. 4.2.3 SVM We have used a linear SVM with a tfidf feature representation and L2-norm as implemented in the SVMlight package v6.01 [11].",
                "All default settings were used. 4.2.4 Voted Perceptron Like the SVM, the Voted Perceptron is a kernel-based learning method.",
                "We use the same feature representation and kernel as we have for the SVM, a linear kernel with tfidf-weighting and an L2-norm.",
                "The voted perceptron is an online-learning method that keeps a history of past perceptrons used, as well as a weight signifying how often that perceptron was correct.",
                "With each new training example, a correct classification increases the weight on the current perceptron and an incorrect classification updates the perceptron.",
                "The output of the classifier uses the weights on the perceptra to make a final voted classification.",
                "When used in an offline-manner, multiple passes can be made through the training data.",
                "Both the voted perceptron and the SVM give a solution from the same hypothesis space - in this case, a linear classifier.",
                "Furthermore, it is well-known that the Voted Perceptron increases the margin of the solution after each pass through the training data [10].",
                "Since Cohen et al. [5] obtain worse results using an SVM than a Voted Perceptron with one training iteration, they conclude that the best solution for detecting speech acts may not lie in an area with a large margin.",
                "Because their tasks are highly similar to ours, we employ both classifiers to ensure we are not overlooking a competitive alternative classifier to the SVM for the basic bag-of-words representation. 4.3 Performance Measures To compare the performance of the classification methods, we look at two standard performance measures, F1 and accuracy.",
                "The F1 measure [18, 21] is the harmonic mean of precision and recall where Precision = Correct Positives Predicted Positives and Recall = Correct Positives Actual Positives . 4.4 Experimental Methodology We perform standard 10-fold cross-validation on the set of documents.",
                "For the sentence-level approach, all sentences in a document are either entirely in the training set or entirely in the test set for each fold.",
                "For significance tests, we use a two-tailed t-test [21] to compare the values obtained during each cross-validation fold with a p-value of 0.05.",
                "Feature selection was performed using the chi-squared statistic.",
                "Different levels of feature selection were considered for each classifier.",
                "Each of the following number of features was tried: 10, 25, 50, 100, 250, 750, 1000, 2000, 4000.",
                "There are approximately 4700 unigram tokens without feature selection.",
                "In order to choose the number of features to use for each classifier, we perform nested cross-validation and choose the settings that yield the optimal document-level F1 for that classifier.",
                "For this study, only the body of each e-mail message was used.",
                "Feature selection is always applied to all candidate features.",
                "That is, for the n-gram representation, the n-grams and position features are also subject to removal by the feature selection method. 4.5 Results The results for document-level classification are given in Table 3.",
                "The primary hypothesis we are concerned with is that n-grams are critical for this task; if this is true, we expect to see a significant gap in performance between the document-level classifiers that use n-grams (denoted Document Ngram) and those using only unigram features (denoted Document Unigram).",
                "Examining Table 3, we observe that this is indeed the case for every classifier except na¨ıve Bayes.",
                "This difference in performance produced by the n-gram representation is statistically significant for each classifier except for na¨ıve Bayes and the accuracy metric for kNN (see Table 4).",
                "Na¨ıve Bayes poor performance with the n-gram representation is not surprising since the bag-of-n-grams causes excessive doublecounting as mentioned in Section 3.2.1; however, na¨ıve Bayes is not hurt at the sentence-level because the sparse examples provide few chances for agglomerative effects of double counting.",
                "In either case, when a language-modeling approach is desired, modeling the n-grams directly would be preferable to na¨ıve Bayes.",
                "More importantly for the n-gram hypothesis, the n-grams lead to the best document-level classifier performance as well.",
                "As would be expected, the difference between the sentence-level n-gram representation and unigram representation is small.",
                "This is because the window of text is so small that the unigram representation, when done at the sentence-level, implicitly picks up on the power of the n-grams.",
                "Further improvement would signify that the order of the words matter even when only considering a small sentence-size window.",
                "Therefore, the finer-grained sentence-level judgments allows a unigram representation to succeed but only when performed in a small window - behaving as an n-gram representation for all practical purposes.",
                "Document Winner Sentence Winner kNN Ngram Ngram na¨ıve Bayes Unigram Ngram SVM Ngram† Ngram Voted Perceptron Ngram† Ngram Table 4: Significance results for n-grams versus unigrams for document detection using document-level and sentence-level classifiers.",
                "When the F1 result is statistically significant, it is shown in bold.",
                "When the accuracy result is significant, it is shown with a † .",
                "F1 Winner Accuracy Winner kNN Sentence Sentence na¨ıve Bayes Sentence Sentence SVM Sentence Sentence Voted Perceptron Sentence Document Table 5: Significance results for sentence-level classifiers vs. document-level classifiers for the document detection problem.",
                "When the result is statistically significant, it is shown in bold.",
                "Further highlighting the improvement from finer-grained judgments and n-grams, Figure 3 graphically depicts the edge the SVM sentence-level classifier has over the standard bag-of-words approach with a precision-recall curve.",
                "In the high precision area of the graph, the consistent edge of the sentence-level classifier is rather impressive - continuing at precision 1 out to 0.1 recall.",
                "This would mean that a tenth of the users action-items would be placed at the top of their action-item sorted inbox.",
                "Additionally, the large separation at the top right of the curves corresponds to the area where the optimal F1 occurs for each classifier, agreeing with the large improvement from 0.6904 to 0.7682 in F1 score.",
                "Considering the relative unexplored nature of classification at the sentence-level, this gives great hope for further increases in performance.",
                "Accuracy F1 Unigram Ngram Unigram Ngram kNN 0.9519 0.9536 0.6540 0.6686 na¨ıve Bayes 0.9419 0.9550 0.6176 0.6676 SVM 0.9559 0.9579 0.6271 0.6672 Voted Perceptron 0.8895 0.9247 0.3744 0.5164 Table 6: Performance of the Sentence-Level Classifiers at Sentence Detection Although Cohen et al. [5] observed that the Voted Perceptron with a single training iteration outperformed SVM in a set of similar tasks, we see no such behavior here.",
                "This further strengthens the evidence that an alternate classifier with the bag-of-words representation could not reach the same level of performance.",
                "The Voted Perceptron classifier does improve when the number of training iterations are increased, but it is still lower than the SVM classifier.",
                "Sentence detection results are presented in Table 6.",
                "With regard to the sentence detection problem, we note that the F1 measure gives a better feel for the remaining room for improvement in this difficult problem.",
                "That is, unlike document detection where actionitem documents are fairly common, action-item sentences are very rare.",
                "Thus, as in other text problems, the accuracy numbers are deceptively high sheerly because of the default accuracy attainable by always predicting no.",
                "Although, the results here are significantly above-random, it is unclear what level of performance is necessary for sentence detection to be useful in and of itself and not simply as a means to document ranking and classification.",
                "Figure 4: Users find action-items quicker when assisted by a classification system.",
                "Finally, when considering a new type of classification task, one of the most basic questions is whether an accurate classifier built for the task can have an impact on the end-user.",
                "In order to demonstrate the impact this task can have on e-mail users, we conducted a user study using an earlier less-accurate version of the sentence classifier - where instead of using just a single sentence, a threesentence windowed-approach was used.",
                "There were three distinct sets of e-mail in which users had to find action-items.",
                "These sets were either presented in a random order (Unordered), ordered by the classifier (Ordered), or ordered by the classifier and with the 0.4 0.5 0.6 0.7 0.8 0.9 1 0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1 Precision Recall Action-Item Detection SVM Performance (Post Model Selection) Document Unigram Sentence Ngram Figure 3: Both n-grams and a small prediction window lead to consistent improvements over the standard approach. center sentence in the highest confidence window highlighted (Order+help).",
                "In order to perform fair comparisons between conditions, the overall number of tokens in each message set should be approximately equal; that is, the cognitive reading load should be approximately the same before the classifiers reordering.",
                "Additionally, users typically show practice effects by improving at the overall task and thus performing better at later message sets.",
                "This is typically handled by varying the ordering of the sets across users so that the means are comparable.",
                "While omitting further detail, we note the sets were balanced for the total number of tokens and a latin square design was used to balance practice effects.",
                "Figure 4 shows that at intervals of 5, 10, and 15 minutes, users consistently found significantly more action-items when assisted by the classifier, but were most critically aided in the first five minutes.",
                "Although, the classifier consistently aids the users, we did not gain an additional end-user impact by highlighting.",
                "As mentioned above, this might be a result of the large room for improvement that still exists for sentence detection, but anecdotal evidence suggests this might also be a result of how the information is presented to the user rather than the accuracy of sentence detection.",
                "For example, highlighting the wrong sentence near an actual action-item hurts the users trust, but if a vague indicator (e.g., an arrow) points to the approximate area the user is not aware of the near-miss.",
                "Since the user studies used a three sentence window, we believe this played a role as well as sentence detection accuracy. 4.6 Discussion In contrast to problems where n-grams have yielded little difference, we believe their power here stems from the fact that many of the meaningful n-grams for action-items consist of common words, e.g., let me know.",
                "Therefore, the document-level unigram approach cannot gain much leverage, even when modeling their joint probability correctly, since these words will often co-occur in the document but not necessarily in a phrase.",
                "Additionally, action-item detection is distinct from many text classification tasks in that a single sentence can change the class label of the document.",
                "As a result, good classifiers cannot rely on aggregating evidence from a large number of weak indicators across the entire document.",
                "Even though we discarded the header information, examining the top-ranked features at the document-level reveals that many of the features are names or parts of e-mail addresses that occurred in the body and are highly associated with e-mails that tend to contain many or no action-items.",
                "A few examples are terms such as org, bob, and gov.",
                "We note that these features will be sensitive to the particular distribution (senders/receivers) and thus the document-level approach may produce classifiers that transfer less readily to alternate contexts and users at different institutions.",
                "This points out that part of the problem of going beyond bag-of-words may be the methodology, and investigating such properties as learning curves and how well a model transfers may highlight differences in models which appear to have similar performance when tested on the distributions they were trained on.",
                "We are currently investigating whether the sentence-level classifiers do perform better over different test corpora without retraining. 5.",
                "FUTURE WORK While applying text classifiers at the document-level is fairly well-understood, there exists the potential for significantly increasing the performance of the sentence-level classifiers.",
                "Such methods include alternate ways of combining the predictions over each sentence, weightings other than tfidf, which may not be appropriate since sentences are small, better sentence segmentation, and other types of phrasal analysis.",
                "Additionally, named entity tagging, time expressions, etc., seem likely candidates for features that can further improve this task.",
                "We are currently pursuing some of these avenues to see what additional gains these offer.",
                "Finally, it would be interesting to investigate the best methods for combining the document-level and sentence-level classifiers.",
                "Since the simple bag-of-words representation at the document-level leads to a learned model that behaves somewhat like a context-specific prior dependent on the sender/receiver and general topic, a first choice would be to treat it as such when combining probability estimates with the sentence-level classifier.",
                "Such a model might serve as a general example for other problems where bag-of-words can establish a baseline model but richer approaches are needed to achieve performance beyond that baseline. 6.",
                "SUMMARY AND CONCLUSIONS The effectiveness of sentence-level detection argues that labeling at the sentence-level provides significant value.",
                "Further experiments are needed to see how this interacts with the amount of training data available.",
                "Sentence detection that is then agglomerated to document-level detection works surprisingly better given low recall than would be expected with sentence-level items.",
                "This, in turn, indicates that improved sentence segmentation methods could yield further improvements in classification.",
                "In this work, we examined how action-items can be effectively detected in e-mails.",
                "Our empirical analysis has demonstrated that n-grams are of key importance to making the most of documentlevel judgments.",
                "When finer-grained judgments are available, then a standard bag-of-words approach using a small (sentence) window size and automatic segmentation techniques can produce results almost as good as the n-gram based approaches.",
                "Acknowledgments This material is based upon work supported by the Defense Advanced Research Projects Agency (DARPA) under Contract No.",
                "NBCHD030010.",
                "Any opinions, findings and conclusions or recommendations expressed in this material are those of the author(s) and do not necessarily reflect the views of the Defense Advanced Research Projects Agency (DARPA), or the Department of InteriorNational Business Center (DOI-NBC).",
                "We would like to extend our sincerest thanks to Jill Lehman whose efforts in data collection were essential in constructing the corpus, and both Jill and Aaron Steinfeld for their direction of the HCI experiments.",
                "We would also like to thank Django Wexler for constructing and supporting the corpus labeling tools and Curtis Huttenhowers support of the text preprocessing package.",
                "Finally, we gratefully acknowledge Scott Fahlman for his encouragement and useful discussions on this topic. 7.",
                "REFERENCES [1] J. Allan, J. Carbonell, G. Doddington, J. Yamron, and Y. Yang.",
                "Topic detection and tracking pilot study: Final report.",
                "In Proceedings of the DARPA Broadcast News Transcription and Understanding Workshop, Washington, D.C., 1998. [2] C. Apte, F. Damerau, and S. M. Weiss.",
                "Automated learning of decision rules for text categorization.",
                "ACM Transactions on Information Systems, 12(3):233-251, July 1994. [3] J. Carletta.",
                "Assessing agreement on classification tasks: The kappa statistic.",
                "Computational Linguistics, 22(2):249-254, 1996. [4] J. Carroll.",
                "High precision extraction of grammatical relations.",
                "In Proceedings of the 19th International Conference on Computational Linguistics (COLING), pages 134-140, 2002. [5] W. W. Cohen, V. R. Carvalho, and T. M. Mitchell.",
                "Learning to classify email into speech acts.",
                "In EMNLP-2004 (Conference on Empirical Methods in Natural Language Processing), pages 309-316, 2004. [6] S. Corston-Oliver, E. Ringger, M. Gamon, and R. Campbell.",
                "Task-focused summarization of email.",
                "In Text Summarization Branches Out: Proceedings of the ACL-04 Workshop, pages 43-50, 2004. [7] A. Culotta, R. Bekkerman, and A. McCallum.",
                "Extracting social networks and contact information from email and the web.",
                "In CEAS-2004 (Conference on Email and Anti-Spam), Mountain View, CA, July 2004. [8] L. Devroye, L. Gy¨orfi, and G. Lugosi.",
                "A Probabilistic Theory of Pattern Recognition.",
                "Springer-Verlag, New York, NY, 1996. [9] S. T. Dumais, J. Platt, D. Heckerman, and M. Sahami.",
                "Inductive learning algorithms and representations for text categorization.",
                "In CIKM 98, Proceedings of the 7th ACM Conference on Information and Knowledge Management, pages 148-155, 1998. [10] Y. Freund and R. Schapire.",
                "Large margin classification using the perceptron algorithm.",
                "Machine Learning, 37(3):277-296, 1999. [11] T. Joachims.",
                "Making large-scale svm learning practical.",
                "In B. Sch¨olkopf, C. J. Burges, and A. J. Smola, editors, Advances in Kernel Methods - Support Vector Learning, pages 41-56.",
                "MIT Press, 1999. [12] L. S. Larkey.",
                "A patent search and classification system.",
                "In Proceedings of the Fourth ACM Conference on Digital Libraries, pages 179 - 187, 1999. [13] D. D. Lewis.",
                "An evaluation of phrasal and clustered representations on a text categorization task.",
                "In SIGIR 92, Proceedings of the 15th Annual International ACM Conference on Research and Development in Information Retrieval, pages 37-50, 1992. [14] Y. Liu, J. Carbonell, and R. Jin.",
                "A pairwise ensemble approach for accurate genre classification.",
                "In Proceedings of the European Conference on Machine Learning (ECML), 2003. [15] Y. Liu, R. Yan, R. Jin, and J. Carbonell.",
                "A comparison study of kernels for multi-label text classification using category association.",
                "In The Twenty-first International Conference on Machine Learning (ICML), 2004. [16] A. McCallum and K. Nigam.",
                "A comparison of event models for naive bayes text classification.",
                "In Working Notes of AAAI 98 (The 15th National Conference on Artificial Intelligence), Workshop on Learning for Text Categorization, pages 41-48, 1998.",
                "TR WS-98-05. [17] F. Sebastiani.",
                "Machine learning in automated text categorization.",
                "ACM Computing Surveys, 34(1):1-47, March 2002. [18] C. J. van Rijsbergen.",
                "Information Retrieval.",
                "Butterworths, London, 1979. [19] Y. Yang.",
                "An evaluation of statistical approaches to text categorization.",
                "Information Retrieval, 1(1/2):67-88, 1999. [20] Y. Yang, J. Carbonell, R. Brown, T. Pierce, B. T. Archibald, and X. Liu.",
                "Learning approaches to topic detection and tracking.",
                "IEEE EXPERT, Special Issue on Applications of Intelligent Information Retrieval, 1999. [21] Y. Yang and X. Liu.",
                "A re-examination of text categorization methods.",
                "In SIGIR 99, Proceedings of the 22nd Annual International ACM Conference on Research and Development in Information Retrieval, pages 42-49, 1999. [22] Y. Yang, J. Zhang, J. Carbonell, and C. Jin.",
                "Topic-conditioned novelty detection.",
                "In Proceedings of the ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, July 2002."
            ],
            "original_annotated_samples": [],
            "translated_annotated_samples": [
                "A diferencia de la \"clasificación de texto impulsada por el tema\" estándar, la detección de mensajes de acción requiere inferir la intención de los remitentes, y como tal responde menos bien a la clasificación pura de las palabras."
            ],
            "translated_text": "",
            "candidates": [
                "Clasificación de texto impulsada por el tema",
                "clasificación de texto impulsada por el tema"
            ],
            "error": []
        },
        "action-item detection": {
            "translated_key": "detección de ítems de acción",
            "is_in_text": true,
            "original_annotated_sentences": [
                "Feature Representation for Effective <br>action-item detection</br> Paul N. Bennett Computer Science Department Carnegie Mellon University Pittsburgh, PA 15213 pbennett+@cs.cmu.edu Jaime Carbonell Language Technologies Institute.",
                "Carnegie Mellon University Pittsburgh, PA 15213 jgc+@cs.cmu.edu ABSTRACT E-mail users face an ever-growing challenge in managing their inboxes due to the growing centrality of email in the workplace for task assignment, action requests, and other roles beyond information dissemination.",
                "Whereas Information Retrieval and Machine Learning techniques are gaining initial acceptance in spam filtering and automated folder assignment, this paper reports on a new task: automated <br>action-item detection</br>, in order to flag emails that require responses, and to highlight the specific passage(s) indicating the request(s) for action.",
                "Unlike standard topic-driven text classification, <br>action-item detection</br> requires inferring the senders intent, and as such responds less well to pure bag-of-words classification.",
                "However, using enriched feature sets, such as n-grams (up to n=4) with chi-squared feature selection, and contextual cues for action-item location improve performance by up to 10% over unigrams, using in both cases state of the art classifiers such as SVMs with automated model selection via embedded cross-validation.",
                "Categories and Subject Descriptors H.3.3 [Information Storage and Retrieval]: Information Search and Retrieval; I.2.6 [Artificial Intelligence]: Learning; I.5.4 [Pattern Recognition]: Applications General Terms Experimentation 1.",
                "INTRODUCTION E-mail users are facing an increasingly difficult task of managing their inboxes in the face of mounting challenges that result from rising e-mail usage.",
                "This includes prioritizing e-mails over a range of sources from business partners to family members, filtering and reducing junk e-mail, and quickly managing requests that demand From: Henry Hutchins <hhutchins@innovative.company.com> To: Sara Smith; Joe Johnson; William Woolings Subject: meeting with prospective customers Sent: Fri 12/10/2005 8:08 AM Hi All, Id like to remind all of you that the group from GRTY will be visiting us next Friday at 4:30 p.m.",
                "The current schedule looks like this: + 9:30 a.m.",
                "Informal Breakfast and Discussion in Cafeteria + 10:30 a.m. Company Overview + 11:00 a.m.",
                "Individual Meetings (Continue Over Lunch) + 2:00 p.m. Tour of Facilities + 3:00 p.m.",
                "Sales Pitch In order to have this go off smoothly, I would like to practice the presentation well in advance.",
                "As a result, I will need each of your parts by Wednesday.",
                "Keep up the good work! -Henry Figure 1: An E-mail with emphasized Action-Item, an explicit request that requires the recipients attention or action. the receivers attention or action.",
                "Automated <br>action-item detection</br> targets the third of these problems by attempting to detect which e-mails require an action or response with information, and within those e-mails, attempting to highlight the sentence (or other passage length) that directly indicates the action request.",
                "Such a detection system can be used as one part of an e-mail agent which would assist a user in processing important e-mails quicker than would have been possible without the agent.",
                "We view <br>action-item detection</br> as one necessary component of a successful e-mail agent which would perform spam detection, <br>action-item detection</br>, topic classification and priority ranking, among other functions.",
                "The utility of such a detector can manifest as a method of prioritizing e-mails according to task-oriented criteria other than the standard ones of topic and sender or as a means of ensuring that the email user hasnt dropped the proverbial ball by forgetting to address an action request.",
                "<br>action-item detection</br> differs from standard text classification in two important ways.",
                "First, the user is interested both in detecting whether an email contains action items and in locating exactly where these action item requests are contained within the email body.",
                "In contrast, standard text categorization merely assigns a topic label to each text, whether that label corresponds to an e-mail folder or a controlled indexing vocabulary [12, 15, 22].",
                "Second, <br>action-item detection</br> attempts to recover the email senders intent - whether she means to elicit response or action on the part of the receiver; note that for this task, classifiers using only unigrams as features do not perform optimally, as evidenced in our results below.",
                "Instead we find that we need more information-laden features such as higher-order n-grams.",
                "Text categorization by topic, on the other hand, works very well using just individual words as features [2, 9, 13, 17].",
                "In fact, genre-classification, which one would think may require more than a bag-of-words approach, also works quite well using just unigram features [14].",
                "Topic detection and tracking (TDT), also works well with unigram feature sets [1, 20].",
                "We believe that <br>action-item detection</br> is one of the first clear instances of an IR-related task where we must move beyond bag-of-words to achieve high performance, albeit not too far, as bag-of-n-grams seem to suffice.",
                "We first review related work for similar text classification problems such as e-mail priority ranking and speech act identification.",
                "Then we more formally define the <br>action-item detection</br> problem, discuss the aspects that distinguish it from more common problems like topic classification, and highlight the challenges in constructing systems that can perform well at the sentence and document level.",
                "From there, we move to a discussion of feature representation and selection techniques appropriate for this problem and how standard text classification approaches can be adapted to smoothly move from the sentence-level detection problem to the documentlevel classification problem.",
                "We then conduct an empirical analysis that helps us determine the effectiveness of our feature extraction procedures as well as establish baselines for a number of classification algorithms on this task.",
                "Finally, we summarize this papers contributions and consider interesting directions for future work. 2.",
                "RELATED WORK Several other researchers have considered very similar text classification tasks.",
                "Cohen et al. [5] describe an ontology of speech acts, such as Propose a Meeting, and attempt to predict when an e-mail contains one of these speech acts.",
                "We consider action-items to be an important specific type of speech act that falls within their more general classification.",
                "While they provide results for several classification methods, their methods only make use of human judgments at the document-level.",
                "In contrast, we consider whether accuracy can be increased by using finer-grained human judgments that mark the specific sentences and phrases of interest.",
                "Corston-Oliver et al. [6] consider detecting items in e-mail to Put on a To-Do List.",
                "This classification task is very similar to ours except they do not consider simple factual questions to belong to this category.",
                "We include questions, but note that not all questions are action-items - some are rhetorical or simply social convention, How are you?.",
                "From a learning perspective, while they make use of judgments at the sentence-level, they do not explicitly compare what if any benefits finer-grained judgments offer.",
                "Additionally, they do not study alternative choices or approaches to the classification task.",
                "Instead, they simply apply a standard SVM at the sentence-level and focus primarily on a linguistic analysis of how the sentence can be logically reformulated before adding it to the task list.",
                "In this study, we examine several alternative classification methods, compare document-level and sentence-level approaches and analyze the machine learning issues implicit in these problems.",
                "Interest in a variety of learning tasks related to e-mail has been rapidly growing in the recent literature.",
                "For example, in a forum dedicated to e-mail learning tasks, Culotta et al. [7] presented methods for learning social networks from e-mail.",
                "In this work, we do not focus on peer relationships; however, such methods could complement those here since peer relationships often influence word choice when requesting an action. 3.",
                "PROBLEM DEFINITION & APPROACH In contrast to previous work, we explicitly focus on the benefits that finer-grained, more costly, sentence-level human judgments offer over coarse-grained document-level judgments.",
                "Additionally, we consider multiple standard text classification approaches and analyze both the quantitative and qualitative differences that arise from taking a document-level vs. a sentence-level approach to classification.",
                "Finally, we focus on the representation necessary to achieve the most competitive performance. 3.1 Problem Definition In order to provide the most benefit to the user, a system would not only detect the document, but it would also indicate the specific sentences in the e-mail which contain the action-items.",
                "Therefore, there are three basic problems: 1.",
                "Document detection: Classify a document as to whether or not it contains an action-item. 2.",
                "Document ranking: Rank the documents such that all documents containing action-items occur as high as possible in the ranking. 3.",
                "Sentence detection: Classify each sentence in a document as to whether or not it is an action-item.",
                "As in most Information Retrieval tasks, the weight the evaluation metric should give to precision and recall depends on the nature of the application.",
                "In situations where a user will eventually read all received messages, ranking (e.g., via precision at recall of 1) may be most important since this will help encourage shorter delays in communications between users.",
                "In contrast, high-precision detection at low recall will be of increasing importance when the user is under severe time-pressure and therefore will likely not read all mail.",
                "This can be the case for crisis managers during disaster management.",
                "Finally, sentence detection plays a role in both timepressure situations and simply to alleviate the users required time to gist the message. 3.2 Approach As mentioned above, the labeled data can come in one of two forms: a document-labeling provides a yes/no label for each document as to whether it contains an action-item; a phrase-labeling provides only a yes label for the specific items of interest.",
                "We term the human judgments a phrase-labeling since the users view of the action-item may not correspond with actual sentence boundaries or predicted sentence boundaries.",
                "Obviously, it is straightforward to generate a document-labeling consistent with a phrase-labeling by labeling a document yes if and only if it contains at least one phrase labeled yes.",
                "To train classifiers for this task, we can take several viewpoints related to both the basic problems we have enumerated and the form of the labeled data.",
                "The document-level view treats each e-mail as a learning instance with an associated class-label.",
                "Then, the document can be converted to a feature-value vector and learning progresses as usual.",
                "Applying a document-level classifier to document detection and ranking is straightforward.",
                "In order to apply it to sentence detection, one must make additional steps.",
                "For example, if the classifier predicts a document contains an action-item, then areas of the document that contain a high-concentration of words which the model weights heavily in favor of action-items can be indicated.",
                "The obvious benefit of the document-level approach is that training set collection costs are lower since the user only has to specify whether or not an e-mail contains an action-item and not the specific sentences.",
                "In the sentence-level view, each e-mail is automatically segmented into sentences, and each sentence is treated as a learning instance with an associated class-label.",
                "Since the phrase-labeling provided by the user may not coincide with the automatic segmentation, we must determine what label to assign a partially overlapping sentence when converting it to a learning instance.",
                "Once trained, applying the resulting classifiers to sentence detection is now straightforward, but in order to apply the classifiers to document detection and document ranking, the individual predictions over each sentence must be aggregated in order to make a document-level prediction.",
                "This approach has the potential to benefit from morespecific labels that enable the learner to focus attention on the key sentences instead of having to learn based on data that the majority of the words in the e-mail provide no or little information about class membership. 3.2.1 Features Consider some of the phrases that might constitute part of an action item: would like to know, let me know, as soon as possible, have you.",
                "Each of these phrases consists of common words that occur in many e-mails.",
                "However, when they occur in the same sentence, they are far more indicative of an action-item.",
                "Additionally, order can be important: consider have you versus you have.",
                "Because of this, we posit that n-grams play a larger role in this problem than is typical of problems like topic classification.",
                "Therefore, we consider all n-grams up to size 4.",
                "When using n-grams, if we find an n-gram of size 4 in a segment of text, we can represent the text as just one occurrence of the ngram or as one occurrence of the n-gram and an occurrence of each smaller n-gram contained by it.",
                "We choose the second of these alternatives since this will allow the algorithm itself to smoothly back-off in terms of recall.",
                "Methods such as na¨ıve Bayes may be hurt by such a representation because of double-counting.",
                "Since sentence-ending punctuation can provide information, we retain the terminating punctuation token when it is identifiable.",
                "Additionally, we add a beginning-of-sentence and end-of-sentence token in order to capture patterns that are often indicators at the beginning or end of a sentence.",
                "Assuming proper punctuation, these extra tokens are unnecessary, but often e-mail lacks proper punctuation.",
                "In addition, for the sentence-level classifiers that use ngrams, we additionally code for each sentence a binary encoding of the position of the sentence relative to the document.",
                "This encoding has eight associated features that represent which octile (the first eighth, second eighth, etc.) contains the sentence. 3.2.2 Implementation Details In order to compare the document-level to the sentence-level approach, we compare predictions at the document-level.",
                "We do not address how to use a document-level classifier to make predictions at the sentence-level.",
                "In order to automatically segment the text of the e-mail, we use the RASP statistical parser [4].",
                "Since the automatically segmented sentences may not correspond directly with the phrase-level boundaries, we treat any sentence that contains at least 30% of a marked action-item segment as an action-item.",
                "When evaluating sentencedetection for the sentence-level system, we use these class labels as ground truth.",
                "Since we are not evaluating multiple segmentation approaches, this does not bias any of the methods.",
                "If multiple segmentation systems were under evaluation, one would need to use a metric that matched predicted positive sentences to phrases labeled positive.",
                "The metric would need to punish overly long true predictions as well as too short predictions.",
                "Our criteria for converting to labeled instances implicitly includes both criteria.",
                "Since the segmentation is fixed, an overly long prediction would be predicting yes for many no instances since presumably the extra length corresponds to additional segmented sentences all of which do not contain 30% of action-item.",
                "Likewise, a too short prediction must correspond to a small sentence included in the action-item but not constituting all of the action-item.",
                "Therefore, in order to consider the prediction to be too short, there will be an additional preceding/following sentence that is an action-item where we incorrectly predicted no.",
                "Once a sentence-level classifier has made a prediction for each sentence, we must combine these predictions to make both a document-level prediction and a document-level score.",
                "We use the simple policy of predicting positive when any of the sentences is predicted positive.",
                "In order to produce a document score for ranking, the confidence that the document contains an action-item is: ψ(d) = 1 n(d) s∈d|π(s)=1 ψ(s) if for any s ∈ d, π(s) = 1 1 n(d) maxs∈d ψ(s) o.w. where s is a sentence in document d, π is the classifiers 1/0 prediction, ψ is the score the classifier assigns as its confidence that π(s) = 1, and n(d) is the greater of 1 and the number of (unigram) tokens in the document.",
                "In other words, when any sentence is predicted positive, the document score is the length normalized sum of the sentence scores above threshold.",
                "When no sentence is predicted positive, the document score is the maximum sentence score normalized by length.",
                "As in other text problems, we are more likely to emit false positives for documents with more words or sentences.",
                "Thus we include a length normalization factor. 4.",
                "EXPERIMENTAL ANALYSIS 4.1 The Data Our corpus consists of e-mails obtained from volunteers at an educational institution and cover subjects such as: organizing a research workshop, arranging for job-candidate interviews, publishing proceedings, and talk announcements.",
                "The messages were anonymized by replacing the names of each individual and institution with a pseudonym.1 After attempting to identify and eliminate duplicate e-mails, the corpus contains 744 e-mail messages.",
                "After identity anonymization, the corpora has three basic versions.",
                "Quoted material refers to the text of a previous e-mail that an author often leaves in an e-mail message when responding to the e-mail.",
                "Quoted material can act as noise when learning since it may include action-items from previous messages that are no longer relevant.",
                "To isolate the effects of quoted material, we have three versions of the corpora.",
                "The raw form contains the basic messages.",
                "The auto-stripped version contains the messages after quoted material has been automatically removed.",
                "The hand-stripped version contains the messages after quoted material has been removed by a human.",
                "Additionally, the hand-stripped version has had any xml content and e-mail signatures removed - leaving only the essential content of the message.",
                "The studies reported here are performed with the hand-stripped version.",
                "This allows us to balance the cognitive load in terms of number of tokens that must be read in the user-studies we report - including quoted material would complicate the user studies since some users might skip the material while others read it.",
                "Additionally, ensuring all quoted material is removed 1 We have an even more highly anonymized version of the corpus that can be made available for some outside experimentation.",
                "Please contact the authors for more information on obtaining this data. prevents tainting the cross-validation since otherwise a test item could occur as quoted material in a training document. 4.1.1 Data Labeling Two human annotators labeled each message as to whether or not it contained an action-item.",
                "In addition, they identified each segment of the e-mail which contained an action-item.",
                "A segment is a contiguous section of text selected by the human annotators and may span several sentences or a complete phrase contained in a sentence.",
                "They were instructed that an action item is an explicit request for information that requires the recipients attention or a required action and told to highlight the phrases or sentences that make up the request.",
                "Annotator 1 No Yes Annotator 2 No 391 26 Yes 29 298 Table 1: Agreement of Human Annotators at Document Level Annotator One labeled 324 messages as containing action items.",
                "Annotator Two labeled 327 messages as containing action items.",
                "The agreement of the human annotators is shown in Tables 1 and 2.",
                "The annotators are said to agree at the document-level when both marked the same document as containing no action-items or both marked at least one action-item regardless of whether the text segments were the same.",
                "At the document-level, the annotators agreed 93% of the time.",
                "The kappa statistic [3, 5] is often used to evaluate inter-annotator agreement: κ = A − R 1 − R A is the empirical estimate of the probability of agreement.",
                "R is the empirical estimate of the probability of random agreement given the empirical class priors.",
                "A value close to −1 implies the annotators agree far less often than would be expected randomly, while a value close to 1 means they agree more often than randomly expected.",
                "At the document-level, the kappa statistic for inter-annotator agreement is 0.85.",
                "This value is both strong enough to expect the problem to be learnable and is comparable with results for similar tasks [5, 6].",
                "In order to determine the sentence-level agreement, we use each judgment to create a sentence-corpus with labels as described in Section 3.2.2, then consider the agreement over these sentences.",
                "This allows us to compare agreement over no judgments.",
                "We perform this comparison over the hand-stripped corpus since that eliminates spurious no judgments that would come from including quoted material, etc.",
                "Both annotators were free to label the subject as an action-item, but since neither did, we omit the subject line of the message as well.",
                "This only reduces the number of no agreements.",
                "This leaves 6301 automatically segmented sentences.",
                "At the sentence-level, the annotators agreed 98% of the time, and the kappa statistic for inter-annotator agreement is 0.82.",
                "In order to produce one single set of judgments, the human annotators went through each annotation where there was disagreement and came to a consensus opinion.",
                "The annotators did not collect statistics during this process but anecdotally reported that the majority of disagreements were either cases of clear annotator oversight or different interpretations of conditional statements.",
                "For example, If you would like to keep your job, come to tomorrows meeting implies a required action where If you would like to join Annotator 1 No Yes Annotator 2 No 5810 65 Yes 74 352 Table 2: Agreement of Human Annotators at Sentence Level the football betting pool, come to tomorrows meeting does not.",
                "The first would be an action-item in most contexts while the second would not.",
                "Of course, many conditional statements are not so clearly interpretable.",
                "After reconciling the judgments there are 416 e-mails with no action-items and 328 e-mails containing actionitems.",
                "Of the 328 e-mails containing action-items, 259 messages have one action-item segment; 55 messages have two action-item segments; 11 messages have three action-item segments.",
                "Two messages have four action-item segments, and one message has six action-item segments.",
                "Computing the sentence-level agreement using the reconciled gold standard judgments with each of the annotators individual judgments gives a kappa of 0.89 for Annotator One and a kappa of 0.92 for Annotator Two.",
                "In terms of message characteristics, there were on average 132 content tokens in the body after stripping.",
                "For action-item messages, there were 115.",
                "However, by examining Figure 2 we see the length distributions are nearly identical.",
                "As would be expected for e-mail, it is a long-tailed distribution with about half the messages having more than 60 tokens in the body (this paragraph has 65 tokens). 4.2 Classifiers For this experiment, we have selected a variety of standard text classification algorithms.",
                "In selecting algorithms, we have chosen algorithms that are not only known to work well but which differ along such lines as discriminative vs. generative and lazy vs. eager.",
                "We have done this in order to provide both a competitive and thorough sampling of learning methods for the task at hand.",
                "This is important since it is easy to improve a strawman classifier by introducing a new representation.",
                "By thoroughly sampling alternative classifier choices we demonstrate that representation improvements over bag-of-words are not due to using the information in the bag-of-words poorly. 4.2.1 kNN We employ a standard variant of the k-nearest neighbor algorithm used in text classification, kNN with s-cut score thresholding [19].",
                "We use a tfidf-weighting of the terms with a distanceweighted vote of the neighbors to compute the score before thresholding it.",
                "In order to choose the value of s for thresholding, we perform leave-one-out cross-validation over the training set.",
                "The value of k is set to be 2( log2 N + 1) where N is the number of training points.",
                "This rule for choosing k is theoretically motivated by results which show such a rule converges to the optimal classifier as the number of training points increases [8].",
                "In practice, we have also found it to be a computational convenience that frequently leads to comparable results with numerically optimizing k via a cross-validation procedure. 4.2.2 Na¨ıve Bayes We use a standard multinomial na¨ıve Bayes classifier [16].",
                "In using this classifier, we smoothed word and class probabilities using a Bayesian estimate (with the word prior) and a Laplace m-estimate, respectively. 0 20 40 60 80 100 120 140 160 0 200 400 600 800 1000 1200 1400 NumberofMessages Number of Tokens All Messages Action-Item Messages 0 0.02 0.04 0.06 0.08 0.1 0.12 0.14 0.16 0.18 0.2 0 200 400 600 800 1000 1200 1400 PercentageofMessages Number of Tokens All Messages Action-Item Messages Figure 2: The Histogram (left) and Distribution (right) of Message Length.",
                "A bin size of 20 words was used.",
                "Only tokens in the body after hand-stripping were counted.",
                "After stripping, the majority of words left are usually actual message content.",
                "Classifiers Document Unigram Document Ngram Sentence Unigram Sentence Ngram F1 kNN 0.6670 ± 0.0288 0.7108 ± 0.0699 0.7615 ± 0.0504 0.7790 ± 0.0460 na¨ıve Bayes 0.6572 ± 0.0749 0.6484 ± 0.0513 0.7715 ± 0.0597 0.7777 ± 0.0426 SVM 0.6904 ± 0.0347 0.7428 ± 0.0422 0.7282 ± 0.0698 0.7682 ± 0.0451 Voted Perceptron 0.6288 ± 0.0395 0.6774 ± 0.0422 0.6511 ± 0.0506 0.6798 ± 0.0913 Accuracy kNN 0.7029 ± 0.0659 0.7486 ± 0.0505 0.7972 ± 0.0435 0.8092 ± 0.0352 na¨ıve Bayes 0.6074 ± 0.0651 0.5816 ± 0.1075 0.7863 ± 0.0553 0.8145 ± 0.0268 SVM 0.7595 ± 0.0309 0.7904 ± 0.0349 0.7958 ± 0.0551 0.8173 ± 0.0258 Voted Perceptron 0.6531 ± 0.0390 0.7164 ± 0.0376 0.6413 ± 0.0833 0.7082 ± 0.1032 Table 3: Average Document-Detection Performance during Cross-Validation for Each Method and the Sample Standard Deviation (Sn−1) in italics.",
                "The best performance for each classifier is shown in bold. 4.2.3 SVM We have used a linear SVM with a tfidf feature representation and L2-norm as implemented in the SVMlight package v6.01 [11].",
                "All default settings were used. 4.2.4 Voted Perceptron Like the SVM, the Voted Perceptron is a kernel-based learning method.",
                "We use the same feature representation and kernel as we have for the SVM, a linear kernel with tfidf-weighting and an L2-norm.",
                "The voted perceptron is an online-learning method that keeps a history of past perceptrons used, as well as a weight signifying how often that perceptron was correct.",
                "With each new training example, a correct classification increases the weight on the current perceptron and an incorrect classification updates the perceptron.",
                "The output of the classifier uses the weights on the perceptra to make a final voted classification.",
                "When used in an offline-manner, multiple passes can be made through the training data.",
                "Both the voted perceptron and the SVM give a solution from the same hypothesis space - in this case, a linear classifier.",
                "Furthermore, it is well-known that the Voted Perceptron increases the margin of the solution after each pass through the training data [10].",
                "Since Cohen et al. [5] obtain worse results using an SVM than a Voted Perceptron with one training iteration, they conclude that the best solution for detecting speech acts may not lie in an area with a large margin.",
                "Because their tasks are highly similar to ours, we employ both classifiers to ensure we are not overlooking a competitive alternative classifier to the SVM for the basic bag-of-words representation. 4.3 Performance Measures To compare the performance of the classification methods, we look at two standard performance measures, F1 and accuracy.",
                "The F1 measure [18, 21] is the harmonic mean of precision and recall where Precision = Correct Positives Predicted Positives and Recall = Correct Positives Actual Positives . 4.4 Experimental Methodology We perform standard 10-fold cross-validation on the set of documents.",
                "For the sentence-level approach, all sentences in a document are either entirely in the training set or entirely in the test set for each fold.",
                "For significance tests, we use a two-tailed t-test [21] to compare the values obtained during each cross-validation fold with a p-value of 0.05.",
                "Feature selection was performed using the chi-squared statistic.",
                "Different levels of feature selection were considered for each classifier.",
                "Each of the following number of features was tried: 10, 25, 50, 100, 250, 750, 1000, 2000, 4000.",
                "There are approximately 4700 unigram tokens without feature selection.",
                "In order to choose the number of features to use for each classifier, we perform nested cross-validation and choose the settings that yield the optimal document-level F1 for that classifier.",
                "For this study, only the body of each e-mail message was used.",
                "Feature selection is always applied to all candidate features.",
                "That is, for the n-gram representation, the n-grams and position features are also subject to removal by the feature selection method. 4.5 Results The results for document-level classification are given in Table 3.",
                "The primary hypothesis we are concerned with is that n-grams are critical for this task; if this is true, we expect to see a significant gap in performance between the document-level classifiers that use n-grams (denoted Document Ngram) and those using only unigram features (denoted Document Unigram).",
                "Examining Table 3, we observe that this is indeed the case for every classifier except na¨ıve Bayes.",
                "This difference in performance produced by the n-gram representation is statistically significant for each classifier except for na¨ıve Bayes and the accuracy metric for kNN (see Table 4).",
                "Na¨ıve Bayes poor performance with the n-gram representation is not surprising since the bag-of-n-grams causes excessive doublecounting as mentioned in Section 3.2.1; however, na¨ıve Bayes is not hurt at the sentence-level because the sparse examples provide few chances for agglomerative effects of double counting.",
                "In either case, when a language-modeling approach is desired, modeling the n-grams directly would be preferable to na¨ıve Bayes.",
                "More importantly for the n-gram hypothesis, the n-grams lead to the best document-level classifier performance as well.",
                "As would be expected, the difference between the sentence-level n-gram representation and unigram representation is small.",
                "This is because the window of text is so small that the unigram representation, when done at the sentence-level, implicitly picks up on the power of the n-grams.",
                "Further improvement would signify that the order of the words matter even when only considering a small sentence-size window.",
                "Therefore, the finer-grained sentence-level judgments allows a unigram representation to succeed but only when performed in a small window - behaving as an n-gram representation for all practical purposes.",
                "Document Winner Sentence Winner kNN Ngram Ngram na¨ıve Bayes Unigram Ngram SVM Ngram† Ngram Voted Perceptron Ngram† Ngram Table 4: Significance results for n-grams versus unigrams for document detection using document-level and sentence-level classifiers.",
                "When the F1 result is statistically significant, it is shown in bold.",
                "When the accuracy result is significant, it is shown with a † .",
                "F1 Winner Accuracy Winner kNN Sentence Sentence na¨ıve Bayes Sentence Sentence SVM Sentence Sentence Voted Perceptron Sentence Document Table 5: Significance results for sentence-level classifiers vs. document-level classifiers for the document detection problem.",
                "When the result is statistically significant, it is shown in bold.",
                "Further highlighting the improvement from finer-grained judgments and n-grams, Figure 3 graphically depicts the edge the SVM sentence-level classifier has over the standard bag-of-words approach with a precision-recall curve.",
                "In the high precision area of the graph, the consistent edge of the sentence-level classifier is rather impressive - continuing at precision 1 out to 0.1 recall.",
                "This would mean that a tenth of the users action-items would be placed at the top of their action-item sorted inbox.",
                "Additionally, the large separation at the top right of the curves corresponds to the area where the optimal F1 occurs for each classifier, agreeing with the large improvement from 0.6904 to 0.7682 in F1 score.",
                "Considering the relative unexplored nature of classification at the sentence-level, this gives great hope for further increases in performance.",
                "Accuracy F1 Unigram Ngram Unigram Ngram kNN 0.9519 0.9536 0.6540 0.6686 na¨ıve Bayes 0.9419 0.9550 0.6176 0.6676 SVM 0.9559 0.9579 0.6271 0.6672 Voted Perceptron 0.8895 0.9247 0.3744 0.5164 Table 6: Performance of the Sentence-Level Classifiers at Sentence Detection Although Cohen et al. [5] observed that the Voted Perceptron with a single training iteration outperformed SVM in a set of similar tasks, we see no such behavior here.",
                "This further strengthens the evidence that an alternate classifier with the bag-of-words representation could not reach the same level of performance.",
                "The Voted Perceptron classifier does improve when the number of training iterations are increased, but it is still lower than the SVM classifier.",
                "Sentence detection results are presented in Table 6.",
                "With regard to the sentence detection problem, we note that the F1 measure gives a better feel for the remaining room for improvement in this difficult problem.",
                "That is, unlike document detection where actionitem documents are fairly common, action-item sentences are very rare.",
                "Thus, as in other text problems, the accuracy numbers are deceptively high sheerly because of the default accuracy attainable by always predicting no.",
                "Although, the results here are significantly above-random, it is unclear what level of performance is necessary for sentence detection to be useful in and of itself and not simply as a means to document ranking and classification.",
                "Figure 4: Users find action-items quicker when assisted by a classification system.",
                "Finally, when considering a new type of classification task, one of the most basic questions is whether an accurate classifier built for the task can have an impact on the end-user.",
                "In order to demonstrate the impact this task can have on e-mail users, we conducted a user study using an earlier less-accurate version of the sentence classifier - where instead of using just a single sentence, a threesentence windowed-approach was used.",
                "There were three distinct sets of e-mail in which users had to find action-items.",
                "These sets were either presented in a random order (Unordered), ordered by the classifier (Ordered), or ordered by the classifier and with the 0.4 0.5 0.6 0.7 0.8 0.9 1 0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1 Precision Recall <br>action-item detection</br> SVM Performance (Post Model Selection) Document Unigram Sentence Ngram Figure 3: Both n-grams and a small prediction window lead to consistent improvements over the standard approach. center sentence in the highest confidence window highlighted (Order+help).",
                "In order to perform fair comparisons between conditions, the overall number of tokens in each message set should be approximately equal; that is, the cognitive reading load should be approximately the same before the classifiers reordering.",
                "Additionally, users typically show practice effects by improving at the overall task and thus performing better at later message sets.",
                "This is typically handled by varying the ordering of the sets across users so that the means are comparable.",
                "While omitting further detail, we note the sets were balanced for the total number of tokens and a latin square design was used to balance practice effects.",
                "Figure 4 shows that at intervals of 5, 10, and 15 minutes, users consistently found significantly more action-items when assisted by the classifier, but were most critically aided in the first five minutes.",
                "Although, the classifier consistently aids the users, we did not gain an additional end-user impact by highlighting.",
                "As mentioned above, this might be a result of the large room for improvement that still exists for sentence detection, but anecdotal evidence suggests this might also be a result of how the information is presented to the user rather than the accuracy of sentence detection.",
                "For example, highlighting the wrong sentence near an actual action-item hurts the users trust, but if a vague indicator (e.g., an arrow) points to the approximate area the user is not aware of the near-miss.",
                "Since the user studies used a three sentence window, we believe this played a role as well as sentence detection accuracy. 4.6 Discussion In contrast to problems where n-grams have yielded little difference, we believe their power here stems from the fact that many of the meaningful n-grams for action-items consist of common words, e.g., let me know.",
                "Therefore, the document-level unigram approach cannot gain much leverage, even when modeling their joint probability correctly, since these words will often co-occur in the document but not necessarily in a phrase.",
                "Additionally, <br>action-item detection</br> is distinct from many text classification tasks in that a single sentence can change the class label of the document.",
                "As a result, good classifiers cannot rely on aggregating evidence from a large number of weak indicators across the entire document.",
                "Even though we discarded the header information, examining the top-ranked features at the document-level reveals that many of the features are names or parts of e-mail addresses that occurred in the body and are highly associated with e-mails that tend to contain many or no action-items.",
                "A few examples are terms such as org, bob, and gov.",
                "We note that these features will be sensitive to the particular distribution (senders/receivers) and thus the document-level approach may produce classifiers that transfer less readily to alternate contexts and users at different institutions.",
                "This points out that part of the problem of going beyond bag-of-words may be the methodology, and investigating such properties as learning curves and how well a model transfers may highlight differences in models which appear to have similar performance when tested on the distributions they were trained on.",
                "We are currently investigating whether the sentence-level classifiers do perform better over different test corpora without retraining. 5.",
                "FUTURE WORK While applying text classifiers at the document-level is fairly well-understood, there exists the potential for significantly increasing the performance of the sentence-level classifiers.",
                "Such methods include alternate ways of combining the predictions over each sentence, weightings other than tfidf, which may not be appropriate since sentences are small, better sentence segmentation, and other types of phrasal analysis.",
                "Additionally, named entity tagging, time expressions, etc., seem likely candidates for features that can further improve this task.",
                "We are currently pursuing some of these avenues to see what additional gains these offer.",
                "Finally, it would be interesting to investigate the best methods for combining the document-level and sentence-level classifiers.",
                "Since the simple bag-of-words representation at the document-level leads to a learned model that behaves somewhat like a context-specific prior dependent on the sender/receiver and general topic, a first choice would be to treat it as such when combining probability estimates with the sentence-level classifier.",
                "Such a model might serve as a general example for other problems where bag-of-words can establish a baseline model but richer approaches are needed to achieve performance beyond that baseline. 6.",
                "SUMMARY AND CONCLUSIONS The effectiveness of sentence-level detection argues that labeling at the sentence-level provides significant value.",
                "Further experiments are needed to see how this interacts with the amount of training data available.",
                "Sentence detection that is then agglomerated to document-level detection works surprisingly better given low recall than would be expected with sentence-level items.",
                "This, in turn, indicates that improved sentence segmentation methods could yield further improvements in classification.",
                "In this work, we examined how action-items can be effectively detected in e-mails.",
                "Our empirical analysis has demonstrated that n-grams are of key importance to making the most of documentlevel judgments.",
                "When finer-grained judgments are available, then a standard bag-of-words approach using a small (sentence) window size and automatic segmentation techniques can produce results almost as good as the n-gram based approaches.",
                "Acknowledgments This material is based upon work supported by the Defense Advanced Research Projects Agency (DARPA) under Contract No.",
                "NBCHD030010.",
                "Any opinions, findings and conclusions or recommendations expressed in this material are those of the author(s) and do not necessarily reflect the views of the Defense Advanced Research Projects Agency (DARPA), or the Department of InteriorNational Business Center (DOI-NBC).",
                "We would like to extend our sincerest thanks to Jill Lehman whose efforts in data collection were essential in constructing the corpus, and both Jill and Aaron Steinfeld for their direction of the HCI experiments.",
                "We would also like to thank Django Wexler for constructing and supporting the corpus labeling tools and Curtis Huttenhowers support of the text preprocessing package.",
                "Finally, we gratefully acknowledge Scott Fahlman for his encouragement and useful discussions on this topic. 7.",
                "REFERENCES [1] J. Allan, J. Carbonell, G. Doddington, J. Yamron, and Y. Yang.",
                "Topic detection and tracking pilot study: Final report.",
                "In Proceedings of the DARPA Broadcast News Transcription and Understanding Workshop, Washington, D.C., 1998. [2] C. Apte, F. Damerau, and S. M. Weiss.",
                "Automated learning of decision rules for text categorization.",
                "ACM Transactions on Information Systems, 12(3):233-251, July 1994. [3] J. Carletta.",
                "Assessing agreement on classification tasks: The kappa statistic.",
                "Computational Linguistics, 22(2):249-254, 1996. [4] J. Carroll.",
                "High precision extraction of grammatical relations.",
                "In Proceedings of the 19th International Conference on Computational Linguistics (COLING), pages 134-140, 2002. [5] W. W. Cohen, V. R. Carvalho, and T. M. Mitchell.",
                "Learning to classify email into speech acts.",
                "In EMNLP-2004 (Conference on Empirical Methods in Natural Language Processing), pages 309-316, 2004. [6] S. Corston-Oliver, E. Ringger, M. Gamon, and R. Campbell.",
                "Task-focused summarization of email.",
                "In Text Summarization Branches Out: Proceedings of the ACL-04 Workshop, pages 43-50, 2004. [7] A. Culotta, R. Bekkerman, and A. McCallum.",
                "Extracting social networks and contact information from email and the web.",
                "In CEAS-2004 (Conference on Email and Anti-Spam), Mountain View, CA, July 2004. [8] L. Devroye, L. Gy¨orfi, and G. Lugosi.",
                "A Probabilistic Theory of Pattern Recognition.",
                "Springer-Verlag, New York, NY, 1996. [9] S. T. Dumais, J. Platt, D. Heckerman, and M. Sahami.",
                "Inductive learning algorithms and representations for text categorization.",
                "In CIKM 98, Proceedings of the 7th ACM Conference on Information and Knowledge Management, pages 148-155, 1998. [10] Y. Freund and R. Schapire.",
                "Large margin classification using the perceptron algorithm.",
                "Machine Learning, 37(3):277-296, 1999. [11] T. Joachims.",
                "Making large-scale svm learning practical.",
                "In B. Sch¨olkopf, C. J. Burges, and A. J. Smola, editors, Advances in Kernel Methods - Support Vector Learning, pages 41-56.",
                "MIT Press, 1999. [12] L. S. Larkey.",
                "A patent search and classification system.",
                "In Proceedings of the Fourth ACM Conference on Digital Libraries, pages 179 - 187, 1999. [13] D. D. Lewis.",
                "An evaluation of phrasal and clustered representations on a text categorization task.",
                "In SIGIR 92, Proceedings of the 15th Annual International ACM Conference on Research and Development in Information Retrieval, pages 37-50, 1992. [14] Y. Liu, J. Carbonell, and R. Jin.",
                "A pairwise ensemble approach for accurate genre classification.",
                "In Proceedings of the European Conference on Machine Learning (ECML), 2003. [15] Y. Liu, R. Yan, R. Jin, and J. Carbonell.",
                "A comparison study of kernels for multi-label text classification using category association.",
                "In The Twenty-first International Conference on Machine Learning (ICML), 2004. [16] A. McCallum and K. Nigam.",
                "A comparison of event models for naive bayes text classification.",
                "In Working Notes of AAAI 98 (The 15th National Conference on Artificial Intelligence), Workshop on Learning for Text Categorization, pages 41-48, 1998.",
                "TR WS-98-05. [17] F. Sebastiani.",
                "Machine learning in automated text categorization.",
                "ACM Computing Surveys, 34(1):1-47, March 2002. [18] C. J. van Rijsbergen.",
                "Information Retrieval.",
                "Butterworths, London, 1979. [19] Y. Yang.",
                "An evaluation of statistical approaches to text categorization.",
                "Information Retrieval, 1(1/2):67-88, 1999. [20] Y. Yang, J. Carbonell, R. Brown, T. Pierce, B. T. Archibald, and X. Liu.",
                "Learning approaches to topic detection and tracking.",
                "IEEE EXPERT, Special Issue on Applications of Intelligent Information Retrieval, 1999. [21] Y. Yang and X. Liu.",
                "A re-examination of text categorization methods.",
                "In SIGIR 99, Proceedings of the 22nd Annual International ACM Conference on Research and Development in Information Retrieval, pages 42-49, 1999. [22] Y. Yang, J. Zhang, J. Carbonell, and C. Jin.",
                "Topic-conditioned novelty detection.",
                "In Proceedings of the ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, July 2002."
            ],
            "original_annotated_samples": [],
            "translated_annotated_samples": [
                "Representación de características para la \"detección de ítems de acción\" efectiva de Paul N. Bennett Departamento de Ciencias de la Computación Carnegie Mellon University Pittsburgh, PA 15213 pbennett+@cs.cmu.edu Jaime Carbonell Language Technologies Institute.",
                "Mientras que las técnicas de recuperación de información y aprendizaje automático están obteniendo aceptación inicial en el filtrado de spam y la asignación automatizada de carpetas, este documento informa sobre una nueva tarea: \"detección de elementos de acción\" automatizados, para marcar correos electrónicos que requieren respuestas, y para resaltar el pasaje específico(S) que indica la (s) solicitud (s) de acción.",
                "A diferencia de la clasificación de texto estándar basada en el tema, la \"detección de ítems de acción\" requiere inferir la intención de los remitentes y, como tal, responde menos bien a la clasificación pura de las palabras.",
                "La \"detección de elementos de acción\" automatizada se dirige al tercio de estos problemas al intentar detectar qué correos electrónicos requieren una acción o respuesta con información, y dentro de esos correos electrónicos, intentando resaltar la oración (u otra longitud de pasaje) que indica directamentela solicitud de acción.",
                "Vemos la \"detección de elementos de acción\" como un componente necesario de un agente de correo electrónico exitoso que realizaría detección de spam, \"detección de elementos de acción\", clasificación de temas y clasificación de prioridad, entre otras funciones.",
                "La \"detección de elementos de acción\" difiere de la clasificación de texto estándar de dos maneras importantes.",
                "Segundo, la \"detección de acción de acción\" intenta recuperar la intención de los remitentes de correo electrónico, ya sea que se refiera a obtener respuesta o acción por parte del receptor;Tenga en cuenta que para esta tarea, los clasificadores que usan solo unigramas como características no funcionan de manera óptima, como se evidencia en nuestros resultados a continuación.",
                "Creemos que la \"detección de elementos de acción\" es una de las primeras instancias claras de una tarea relacionada con IR donde debemos ir más allá de la bolsa de palabras para lograr un alto rendimiento, aunque no demasiado lejos, como la bolsa de n-gramos.parece suficiente.",
                "Luego, definimos más formalmente el problema de \"detección de elementos de acción\", discutimos los aspectos que lo distinguen de problemas más comunes como la clasificación de temas y resaltan los desafíos en la construcción de sistemas que pueden funcionar bien a nivel de oración y documento.",
                "Estos conjuntos se presentaron en un orden aleatorio (desordenado), ordenados por el clasificador (ordenado) u ordenado por el clasificador y con el 0.4 0.5 0.6 0.7 0.8 0.9 1 0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1 recuerdo de precisión \"Detección de ítems de acción \"Rendimiento SVM (Selección posterior al modelo) Documento de la oración unigram NGRAM Figura 3: Tanto N-Grams como una pequeña ventana de predicción conducen a mejoras consistentes sobre el enfoque estándar.oración central en la ventana de confianza más alta resaltada (orden+ayuda).",
                "Además, la \"detección de acción de acción\" es distinta de muchas tareas de clasificación de texto en que una sola oración puede cambiar la etiqueta de clase del documento."
            ],
            "translated_text": "",
            "candidates": [
                "detección de ítems de acción",
                "detección de ítems de acción",
                "detección de ítems de acción",
                "detección de elementos de acción",
                "detección de ítems de acción",
                "detección de ítems de acción",
                "detección de ítems de acción",
                "detección de elementos de acción",
                "detección de ítems de acción",
                "detección de elementos de acción",
                "detección de elementos de acción",
                "detección de ítems de acción",
                "detección de elementos de acción",
                "detección de ítems de acción",
                "detección de acción de acción",
                "detección de ítems de acción",
                "detección de elementos de acción",
                "detección de ítems de acción",
                "detección de elementos de acción",
                "detección de ítems de acción",
                "Detección de ítems de acción ",
                "detección de ítems de acción",
                "detección de acción de acción"
            ],
            "error": []
        },
        "chi-squared feature selection": {
            "translated_key": "selección de características chi cuadrado",
            "is_in_text": true,
            "original_annotated_sentences": [
                "Feature Representation for Effective Action-Item Detection Paul N. Bennett Computer Science Department Carnegie Mellon University Pittsburgh, PA 15213 pbennett+@cs.cmu.edu Jaime Carbonell Language Technologies Institute.",
                "Carnegie Mellon University Pittsburgh, PA 15213 jgc+@cs.cmu.edu ABSTRACT E-mail users face an ever-growing challenge in managing their inboxes due to the growing centrality of email in the workplace for task assignment, action requests, and other roles beyond information dissemination.",
                "Whereas Information Retrieval and Machine Learning techniques are gaining initial acceptance in spam filtering and automated folder assignment, this paper reports on a new task: automated action-item detection, in order to flag emails that require responses, and to highlight the specific passage(s) indicating the request(s) for action.",
                "Unlike standard topic-driven text classification, action-item detection requires inferring the senders intent, and as such responds less well to pure bag-of-words classification.",
                "However, using enriched feature sets, such as n-grams (up to n=4) with <br>chi-squared feature selection</br>, and contextual cues for action-item location improve performance by up to 10% over unigrams, using in both cases state of the art classifiers such as SVMs with automated model selection via embedded cross-validation.",
                "Categories and Subject Descriptors H.3.3 [Information Storage and Retrieval]: Information Search and Retrieval; I.2.6 [Artificial Intelligence]: Learning; I.5.4 [Pattern Recognition]: Applications General Terms Experimentation 1.",
                "INTRODUCTION E-mail users are facing an increasingly difficult task of managing their inboxes in the face of mounting challenges that result from rising e-mail usage.",
                "This includes prioritizing e-mails over a range of sources from business partners to family members, filtering and reducing junk e-mail, and quickly managing requests that demand From: Henry Hutchins <hhutchins@innovative.company.com> To: Sara Smith; Joe Johnson; William Woolings Subject: meeting with prospective customers Sent: Fri 12/10/2005 8:08 AM Hi All, Id like to remind all of you that the group from GRTY will be visiting us next Friday at 4:30 p.m.",
                "The current schedule looks like this: + 9:30 a.m.",
                "Informal Breakfast and Discussion in Cafeteria + 10:30 a.m. Company Overview + 11:00 a.m.",
                "Individual Meetings (Continue Over Lunch) + 2:00 p.m. Tour of Facilities + 3:00 p.m.",
                "Sales Pitch In order to have this go off smoothly, I would like to practice the presentation well in advance.",
                "As a result, I will need each of your parts by Wednesday.",
                "Keep up the good work! -Henry Figure 1: An E-mail with emphasized Action-Item, an explicit request that requires the recipients attention or action. the receivers attention or action.",
                "Automated action-item detection targets the third of these problems by attempting to detect which e-mails require an action or response with information, and within those e-mails, attempting to highlight the sentence (or other passage length) that directly indicates the action request.",
                "Such a detection system can be used as one part of an e-mail agent which would assist a user in processing important e-mails quicker than would have been possible without the agent.",
                "We view action-item detection as one necessary component of a successful e-mail agent which would perform spam detection, action-item detection, topic classification and priority ranking, among other functions.",
                "The utility of such a detector can manifest as a method of prioritizing e-mails according to task-oriented criteria other than the standard ones of topic and sender or as a means of ensuring that the email user hasnt dropped the proverbial ball by forgetting to address an action request.",
                "Action-item detection differs from standard text classification in two important ways.",
                "First, the user is interested both in detecting whether an email contains action items and in locating exactly where these action item requests are contained within the email body.",
                "In contrast, standard text categorization merely assigns a topic label to each text, whether that label corresponds to an e-mail folder or a controlled indexing vocabulary [12, 15, 22].",
                "Second, action-item detection attempts to recover the email senders intent - whether she means to elicit response or action on the part of the receiver; note that for this task, classifiers using only unigrams as features do not perform optimally, as evidenced in our results below.",
                "Instead we find that we need more information-laden features such as higher-order n-grams.",
                "Text categorization by topic, on the other hand, works very well using just individual words as features [2, 9, 13, 17].",
                "In fact, genre-classification, which one would think may require more than a bag-of-words approach, also works quite well using just unigram features [14].",
                "Topic detection and tracking (TDT), also works well with unigram feature sets [1, 20].",
                "We believe that action-item detection is one of the first clear instances of an IR-related task where we must move beyond bag-of-words to achieve high performance, albeit not too far, as bag-of-n-grams seem to suffice.",
                "We first review related work for similar text classification problems such as e-mail priority ranking and speech act identification.",
                "Then we more formally define the action-item detection problem, discuss the aspects that distinguish it from more common problems like topic classification, and highlight the challenges in constructing systems that can perform well at the sentence and document level.",
                "From there, we move to a discussion of feature representation and selection techniques appropriate for this problem and how standard text classification approaches can be adapted to smoothly move from the sentence-level detection problem to the documentlevel classification problem.",
                "We then conduct an empirical analysis that helps us determine the effectiveness of our feature extraction procedures as well as establish baselines for a number of classification algorithms on this task.",
                "Finally, we summarize this papers contributions and consider interesting directions for future work. 2.",
                "RELATED WORK Several other researchers have considered very similar text classification tasks.",
                "Cohen et al. [5] describe an ontology of speech acts, such as Propose a Meeting, and attempt to predict when an e-mail contains one of these speech acts.",
                "We consider action-items to be an important specific type of speech act that falls within their more general classification.",
                "While they provide results for several classification methods, their methods only make use of human judgments at the document-level.",
                "In contrast, we consider whether accuracy can be increased by using finer-grained human judgments that mark the specific sentences and phrases of interest.",
                "Corston-Oliver et al. [6] consider detecting items in e-mail to Put on a To-Do List.",
                "This classification task is very similar to ours except they do not consider simple factual questions to belong to this category.",
                "We include questions, but note that not all questions are action-items - some are rhetorical or simply social convention, How are you?.",
                "From a learning perspective, while they make use of judgments at the sentence-level, they do not explicitly compare what if any benefits finer-grained judgments offer.",
                "Additionally, they do not study alternative choices or approaches to the classification task.",
                "Instead, they simply apply a standard SVM at the sentence-level and focus primarily on a linguistic analysis of how the sentence can be logically reformulated before adding it to the task list.",
                "In this study, we examine several alternative classification methods, compare document-level and sentence-level approaches and analyze the machine learning issues implicit in these problems.",
                "Interest in a variety of learning tasks related to e-mail has been rapidly growing in the recent literature.",
                "For example, in a forum dedicated to e-mail learning tasks, Culotta et al. [7] presented methods for learning social networks from e-mail.",
                "In this work, we do not focus on peer relationships; however, such methods could complement those here since peer relationships often influence word choice when requesting an action. 3.",
                "PROBLEM DEFINITION & APPROACH In contrast to previous work, we explicitly focus on the benefits that finer-grained, more costly, sentence-level human judgments offer over coarse-grained document-level judgments.",
                "Additionally, we consider multiple standard text classification approaches and analyze both the quantitative and qualitative differences that arise from taking a document-level vs. a sentence-level approach to classification.",
                "Finally, we focus on the representation necessary to achieve the most competitive performance. 3.1 Problem Definition In order to provide the most benefit to the user, a system would not only detect the document, but it would also indicate the specific sentences in the e-mail which contain the action-items.",
                "Therefore, there are three basic problems: 1.",
                "Document detection: Classify a document as to whether or not it contains an action-item. 2.",
                "Document ranking: Rank the documents such that all documents containing action-items occur as high as possible in the ranking. 3.",
                "Sentence detection: Classify each sentence in a document as to whether or not it is an action-item.",
                "As in most Information Retrieval tasks, the weight the evaluation metric should give to precision and recall depends on the nature of the application.",
                "In situations where a user will eventually read all received messages, ranking (e.g., via precision at recall of 1) may be most important since this will help encourage shorter delays in communications between users.",
                "In contrast, high-precision detection at low recall will be of increasing importance when the user is under severe time-pressure and therefore will likely not read all mail.",
                "This can be the case for crisis managers during disaster management.",
                "Finally, sentence detection plays a role in both timepressure situations and simply to alleviate the users required time to gist the message. 3.2 Approach As mentioned above, the labeled data can come in one of two forms: a document-labeling provides a yes/no label for each document as to whether it contains an action-item; a phrase-labeling provides only a yes label for the specific items of interest.",
                "We term the human judgments a phrase-labeling since the users view of the action-item may not correspond with actual sentence boundaries or predicted sentence boundaries.",
                "Obviously, it is straightforward to generate a document-labeling consistent with a phrase-labeling by labeling a document yes if and only if it contains at least one phrase labeled yes.",
                "To train classifiers for this task, we can take several viewpoints related to both the basic problems we have enumerated and the form of the labeled data.",
                "The document-level view treats each e-mail as a learning instance with an associated class-label.",
                "Then, the document can be converted to a feature-value vector and learning progresses as usual.",
                "Applying a document-level classifier to document detection and ranking is straightforward.",
                "In order to apply it to sentence detection, one must make additional steps.",
                "For example, if the classifier predicts a document contains an action-item, then areas of the document that contain a high-concentration of words which the model weights heavily in favor of action-items can be indicated.",
                "The obvious benefit of the document-level approach is that training set collection costs are lower since the user only has to specify whether or not an e-mail contains an action-item and not the specific sentences.",
                "In the sentence-level view, each e-mail is automatically segmented into sentences, and each sentence is treated as a learning instance with an associated class-label.",
                "Since the phrase-labeling provided by the user may not coincide with the automatic segmentation, we must determine what label to assign a partially overlapping sentence when converting it to a learning instance.",
                "Once trained, applying the resulting classifiers to sentence detection is now straightforward, but in order to apply the classifiers to document detection and document ranking, the individual predictions over each sentence must be aggregated in order to make a document-level prediction.",
                "This approach has the potential to benefit from morespecific labels that enable the learner to focus attention on the key sentences instead of having to learn based on data that the majority of the words in the e-mail provide no or little information about class membership. 3.2.1 Features Consider some of the phrases that might constitute part of an action item: would like to know, let me know, as soon as possible, have you.",
                "Each of these phrases consists of common words that occur in many e-mails.",
                "However, when they occur in the same sentence, they are far more indicative of an action-item.",
                "Additionally, order can be important: consider have you versus you have.",
                "Because of this, we posit that n-grams play a larger role in this problem than is typical of problems like topic classification.",
                "Therefore, we consider all n-grams up to size 4.",
                "When using n-grams, if we find an n-gram of size 4 in a segment of text, we can represent the text as just one occurrence of the ngram or as one occurrence of the n-gram and an occurrence of each smaller n-gram contained by it.",
                "We choose the second of these alternatives since this will allow the algorithm itself to smoothly back-off in terms of recall.",
                "Methods such as na¨ıve Bayes may be hurt by such a representation because of double-counting.",
                "Since sentence-ending punctuation can provide information, we retain the terminating punctuation token when it is identifiable.",
                "Additionally, we add a beginning-of-sentence and end-of-sentence token in order to capture patterns that are often indicators at the beginning or end of a sentence.",
                "Assuming proper punctuation, these extra tokens are unnecessary, but often e-mail lacks proper punctuation.",
                "In addition, for the sentence-level classifiers that use ngrams, we additionally code for each sentence a binary encoding of the position of the sentence relative to the document.",
                "This encoding has eight associated features that represent which octile (the first eighth, second eighth, etc.) contains the sentence. 3.2.2 Implementation Details In order to compare the document-level to the sentence-level approach, we compare predictions at the document-level.",
                "We do not address how to use a document-level classifier to make predictions at the sentence-level.",
                "In order to automatically segment the text of the e-mail, we use the RASP statistical parser [4].",
                "Since the automatically segmented sentences may not correspond directly with the phrase-level boundaries, we treat any sentence that contains at least 30% of a marked action-item segment as an action-item.",
                "When evaluating sentencedetection for the sentence-level system, we use these class labels as ground truth.",
                "Since we are not evaluating multiple segmentation approaches, this does not bias any of the methods.",
                "If multiple segmentation systems were under evaluation, one would need to use a metric that matched predicted positive sentences to phrases labeled positive.",
                "The metric would need to punish overly long true predictions as well as too short predictions.",
                "Our criteria for converting to labeled instances implicitly includes both criteria.",
                "Since the segmentation is fixed, an overly long prediction would be predicting yes for many no instances since presumably the extra length corresponds to additional segmented sentences all of which do not contain 30% of action-item.",
                "Likewise, a too short prediction must correspond to a small sentence included in the action-item but not constituting all of the action-item.",
                "Therefore, in order to consider the prediction to be too short, there will be an additional preceding/following sentence that is an action-item where we incorrectly predicted no.",
                "Once a sentence-level classifier has made a prediction for each sentence, we must combine these predictions to make both a document-level prediction and a document-level score.",
                "We use the simple policy of predicting positive when any of the sentences is predicted positive.",
                "In order to produce a document score for ranking, the confidence that the document contains an action-item is: ψ(d) = 1 n(d) s∈d|π(s)=1 ψ(s) if for any s ∈ d, π(s) = 1 1 n(d) maxs∈d ψ(s) o.w. where s is a sentence in document d, π is the classifiers 1/0 prediction, ψ is the score the classifier assigns as its confidence that π(s) = 1, and n(d) is the greater of 1 and the number of (unigram) tokens in the document.",
                "In other words, when any sentence is predicted positive, the document score is the length normalized sum of the sentence scores above threshold.",
                "When no sentence is predicted positive, the document score is the maximum sentence score normalized by length.",
                "As in other text problems, we are more likely to emit false positives for documents with more words or sentences.",
                "Thus we include a length normalization factor. 4.",
                "EXPERIMENTAL ANALYSIS 4.1 The Data Our corpus consists of e-mails obtained from volunteers at an educational institution and cover subjects such as: organizing a research workshop, arranging for job-candidate interviews, publishing proceedings, and talk announcements.",
                "The messages were anonymized by replacing the names of each individual and institution with a pseudonym.1 After attempting to identify and eliminate duplicate e-mails, the corpus contains 744 e-mail messages.",
                "After identity anonymization, the corpora has three basic versions.",
                "Quoted material refers to the text of a previous e-mail that an author often leaves in an e-mail message when responding to the e-mail.",
                "Quoted material can act as noise when learning since it may include action-items from previous messages that are no longer relevant.",
                "To isolate the effects of quoted material, we have three versions of the corpora.",
                "The raw form contains the basic messages.",
                "The auto-stripped version contains the messages after quoted material has been automatically removed.",
                "The hand-stripped version contains the messages after quoted material has been removed by a human.",
                "Additionally, the hand-stripped version has had any xml content and e-mail signatures removed - leaving only the essential content of the message.",
                "The studies reported here are performed with the hand-stripped version.",
                "This allows us to balance the cognitive load in terms of number of tokens that must be read in the user-studies we report - including quoted material would complicate the user studies since some users might skip the material while others read it.",
                "Additionally, ensuring all quoted material is removed 1 We have an even more highly anonymized version of the corpus that can be made available for some outside experimentation.",
                "Please contact the authors for more information on obtaining this data. prevents tainting the cross-validation since otherwise a test item could occur as quoted material in a training document. 4.1.1 Data Labeling Two human annotators labeled each message as to whether or not it contained an action-item.",
                "In addition, they identified each segment of the e-mail which contained an action-item.",
                "A segment is a contiguous section of text selected by the human annotators and may span several sentences or a complete phrase contained in a sentence.",
                "They were instructed that an action item is an explicit request for information that requires the recipients attention or a required action and told to highlight the phrases or sentences that make up the request.",
                "Annotator 1 No Yes Annotator 2 No 391 26 Yes 29 298 Table 1: Agreement of Human Annotators at Document Level Annotator One labeled 324 messages as containing action items.",
                "Annotator Two labeled 327 messages as containing action items.",
                "The agreement of the human annotators is shown in Tables 1 and 2.",
                "The annotators are said to agree at the document-level when both marked the same document as containing no action-items or both marked at least one action-item regardless of whether the text segments were the same.",
                "At the document-level, the annotators agreed 93% of the time.",
                "The kappa statistic [3, 5] is often used to evaluate inter-annotator agreement: κ = A − R 1 − R A is the empirical estimate of the probability of agreement.",
                "R is the empirical estimate of the probability of random agreement given the empirical class priors.",
                "A value close to −1 implies the annotators agree far less often than would be expected randomly, while a value close to 1 means they agree more often than randomly expected.",
                "At the document-level, the kappa statistic for inter-annotator agreement is 0.85.",
                "This value is both strong enough to expect the problem to be learnable and is comparable with results for similar tasks [5, 6].",
                "In order to determine the sentence-level agreement, we use each judgment to create a sentence-corpus with labels as described in Section 3.2.2, then consider the agreement over these sentences.",
                "This allows us to compare agreement over no judgments.",
                "We perform this comparison over the hand-stripped corpus since that eliminates spurious no judgments that would come from including quoted material, etc.",
                "Both annotators were free to label the subject as an action-item, but since neither did, we omit the subject line of the message as well.",
                "This only reduces the number of no agreements.",
                "This leaves 6301 automatically segmented sentences.",
                "At the sentence-level, the annotators agreed 98% of the time, and the kappa statistic for inter-annotator agreement is 0.82.",
                "In order to produce one single set of judgments, the human annotators went through each annotation where there was disagreement and came to a consensus opinion.",
                "The annotators did not collect statistics during this process but anecdotally reported that the majority of disagreements were either cases of clear annotator oversight or different interpretations of conditional statements.",
                "For example, If you would like to keep your job, come to tomorrows meeting implies a required action where If you would like to join Annotator 1 No Yes Annotator 2 No 5810 65 Yes 74 352 Table 2: Agreement of Human Annotators at Sentence Level the football betting pool, come to tomorrows meeting does not.",
                "The first would be an action-item in most contexts while the second would not.",
                "Of course, many conditional statements are not so clearly interpretable.",
                "After reconciling the judgments there are 416 e-mails with no action-items and 328 e-mails containing actionitems.",
                "Of the 328 e-mails containing action-items, 259 messages have one action-item segment; 55 messages have two action-item segments; 11 messages have three action-item segments.",
                "Two messages have four action-item segments, and one message has six action-item segments.",
                "Computing the sentence-level agreement using the reconciled gold standard judgments with each of the annotators individual judgments gives a kappa of 0.89 for Annotator One and a kappa of 0.92 for Annotator Two.",
                "In terms of message characteristics, there were on average 132 content tokens in the body after stripping.",
                "For action-item messages, there were 115.",
                "However, by examining Figure 2 we see the length distributions are nearly identical.",
                "As would be expected for e-mail, it is a long-tailed distribution with about half the messages having more than 60 tokens in the body (this paragraph has 65 tokens). 4.2 Classifiers For this experiment, we have selected a variety of standard text classification algorithms.",
                "In selecting algorithms, we have chosen algorithms that are not only known to work well but which differ along such lines as discriminative vs. generative and lazy vs. eager.",
                "We have done this in order to provide both a competitive and thorough sampling of learning methods for the task at hand.",
                "This is important since it is easy to improve a strawman classifier by introducing a new representation.",
                "By thoroughly sampling alternative classifier choices we demonstrate that representation improvements over bag-of-words are not due to using the information in the bag-of-words poorly. 4.2.1 kNN We employ a standard variant of the k-nearest neighbor algorithm used in text classification, kNN with s-cut score thresholding [19].",
                "We use a tfidf-weighting of the terms with a distanceweighted vote of the neighbors to compute the score before thresholding it.",
                "In order to choose the value of s for thresholding, we perform leave-one-out cross-validation over the training set.",
                "The value of k is set to be 2( log2 N + 1) where N is the number of training points.",
                "This rule for choosing k is theoretically motivated by results which show such a rule converges to the optimal classifier as the number of training points increases [8].",
                "In practice, we have also found it to be a computational convenience that frequently leads to comparable results with numerically optimizing k via a cross-validation procedure. 4.2.2 Na¨ıve Bayes We use a standard multinomial na¨ıve Bayes classifier [16].",
                "In using this classifier, we smoothed word and class probabilities using a Bayesian estimate (with the word prior) and a Laplace m-estimate, respectively. 0 20 40 60 80 100 120 140 160 0 200 400 600 800 1000 1200 1400 NumberofMessages Number of Tokens All Messages Action-Item Messages 0 0.02 0.04 0.06 0.08 0.1 0.12 0.14 0.16 0.18 0.2 0 200 400 600 800 1000 1200 1400 PercentageofMessages Number of Tokens All Messages Action-Item Messages Figure 2: The Histogram (left) and Distribution (right) of Message Length.",
                "A bin size of 20 words was used.",
                "Only tokens in the body after hand-stripping were counted.",
                "After stripping, the majority of words left are usually actual message content.",
                "Classifiers Document Unigram Document Ngram Sentence Unigram Sentence Ngram F1 kNN 0.6670 ± 0.0288 0.7108 ± 0.0699 0.7615 ± 0.0504 0.7790 ± 0.0460 na¨ıve Bayes 0.6572 ± 0.0749 0.6484 ± 0.0513 0.7715 ± 0.0597 0.7777 ± 0.0426 SVM 0.6904 ± 0.0347 0.7428 ± 0.0422 0.7282 ± 0.0698 0.7682 ± 0.0451 Voted Perceptron 0.6288 ± 0.0395 0.6774 ± 0.0422 0.6511 ± 0.0506 0.6798 ± 0.0913 Accuracy kNN 0.7029 ± 0.0659 0.7486 ± 0.0505 0.7972 ± 0.0435 0.8092 ± 0.0352 na¨ıve Bayes 0.6074 ± 0.0651 0.5816 ± 0.1075 0.7863 ± 0.0553 0.8145 ± 0.0268 SVM 0.7595 ± 0.0309 0.7904 ± 0.0349 0.7958 ± 0.0551 0.8173 ± 0.0258 Voted Perceptron 0.6531 ± 0.0390 0.7164 ± 0.0376 0.6413 ± 0.0833 0.7082 ± 0.1032 Table 3: Average Document-Detection Performance during Cross-Validation for Each Method and the Sample Standard Deviation (Sn−1) in italics.",
                "The best performance for each classifier is shown in bold. 4.2.3 SVM We have used a linear SVM with a tfidf feature representation and L2-norm as implemented in the SVMlight package v6.01 [11].",
                "All default settings were used. 4.2.4 Voted Perceptron Like the SVM, the Voted Perceptron is a kernel-based learning method.",
                "We use the same feature representation and kernel as we have for the SVM, a linear kernel with tfidf-weighting and an L2-norm.",
                "The voted perceptron is an online-learning method that keeps a history of past perceptrons used, as well as a weight signifying how often that perceptron was correct.",
                "With each new training example, a correct classification increases the weight on the current perceptron and an incorrect classification updates the perceptron.",
                "The output of the classifier uses the weights on the perceptra to make a final voted classification.",
                "When used in an offline-manner, multiple passes can be made through the training data.",
                "Both the voted perceptron and the SVM give a solution from the same hypothesis space - in this case, a linear classifier.",
                "Furthermore, it is well-known that the Voted Perceptron increases the margin of the solution after each pass through the training data [10].",
                "Since Cohen et al. [5] obtain worse results using an SVM than a Voted Perceptron with one training iteration, they conclude that the best solution for detecting speech acts may not lie in an area with a large margin.",
                "Because their tasks are highly similar to ours, we employ both classifiers to ensure we are not overlooking a competitive alternative classifier to the SVM for the basic bag-of-words representation. 4.3 Performance Measures To compare the performance of the classification methods, we look at two standard performance measures, F1 and accuracy.",
                "The F1 measure [18, 21] is the harmonic mean of precision and recall where Precision = Correct Positives Predicted Positives and Recall = Correct Positives Actual Positives . 4.4 Experimental Methodology We perform standard 10-fold cross-validation on the set of documents.",
                "For the sentence-level approach, all sentences in a document are either entirely in the training set or entirely in the test set for each fold.",
                "For significance tests, we use a two-tailed t-test [21] to compare the values obtained during each cross-validation fold with a p-value of 0.05.",
                "Feature selection was performed using the chi-squared statistic.",
                "Different levels of feature selection were considered for each classifier.",
                "Each of the following number of features was tried: 10, 25, 50, 100, 250, 750, 1000, 2000, 4000.",
                "There are approximately 4700 unigram tokens without feature selection.",
                "In order to choose the number of features to use for each classifier, we perform nested cross-validation and choose the settings that yield the optimal document-level F1 for that classifier.",
                "For this study, only the body of each e-mail message was used.",
                "Feature selection is always applied to all candidate features.",
                "That is, for the n-gram representation, the n-grams and position features are also subject to removal by the feature selection method. 4.5 Results The results for document-level classification are given in Table 3.",
                "The primary hypothesis we are concerned with is that n-grams are critical for this task; if this is true, we expect to see a significant gap in performance between the document-level classifiers that use n-grams (denoted Document Ngram) and those using only unigram features (denoted Document Unigram).",
                "Examining Table 3, we observe that this is indeed the case for every classifier except na¨ıve Bayes.",
                "This difference in performance produced by the n-gram representation is statistically significant for each classifier except for na¨ıve Bayes and the accuracy metric for kNN (see Table 4).",
                "Na¨ıve Bayes poor performance with the n-gram representation is not surprising since the bag-of-n-grams causes excessive doublecounting as mentioned in Section 3.2.1; however, na¨ıve Bayes is not hurt at the sentence-level because the sparse examples provide few chances for agglomerative effects of double counting.",
                "In either case, when a language-modeling approach is desired, modeling the n-grams directly would be preferable to na¨ıve Bayes.",
                "More importantly for the n-gram hypothesis, the n-grams lead to the best document-level classifier performance as well.",
                "As would be expected, the difference between the sentence-level n-gram representation and unigram representation is small.",
                "This is because the window of text is so small that the unigram representation, when done at the sentence-level, implicitly picks up on the power of the n-grams.",
                "Further improvement would signify that the order of the words matter even when only considering a small sentence-size window.",
                "Therefore, the finer-grained sentence-level judgments allows a unigram representation to succeed but only when performed in a small window - behaving as an n-gram representation for all practical purposes.",
                "Document Winner Sentence Winner kNN Ngram Ngram na¨ıve Bayes Unigram Ngram SVM Ngram† Ngram Voted Perceptron Ngram† Ngram Table 4: Significance results for n-grams versus unigrams for document detection using document-level and sentence-level classifiers.",
                "When the F1 result is statistically significant, it is shown in bold.",
                "When the accuracy result is significant, it is shown with a † .",
                "F1 Winner Accuracy Winner kNN Sentence Sentence na¨ıve Bayes Sentence Sentence SVM Sentence Sentence Voted Perceptron Sentence Document Table 5: Significance results for sentence-level classifiers vs. document-level classifiers for the document detection problem.",
                "When the result is statistically significant, it is shown in bold.",
                "Further highlighting the improvement from finer-grained judgments and n-grams, Figure 3 graphically depicts the edge the SVM sentence-level classifier has over the standard bag-of-words approach with a precision-recall curve.",
                "In the high precision area of the graph, the consistent edge of the sentence-level classifier is rather impressive - continuing at precision 1 out to 0.1 recall.",
                "This would mean that a tenth of the users action-items would be placed at the top of their action-item sorted inbox.",
                "Additionally, the large separation at the top right of the curves corresponds to the area where the optimal F1 occurs for each classifier, agreeing with the large improvement from 0.6904 to 0.7682 in F1 score.",
                "Considering the relative unexplored nature of classification at the sentence-level, this gives great hope for further increases in performance.",
                "Accuracy F1 Unigram Ngram Unigram Ngram kNN 0.9519 0.9536 0.6540 0.6686 na¨ıve Bayes 0.9419 0.9550 0.6176 0.6676 SVM 0.9559 0.9579 0.6271 0.6672 Voted Perceptron 0.8895 0.9247 0.3744 0.5164 Table 6: Performance of the Sentence-Level Classifiers at Sentence Detection Although Cohen et al. [5] observed that the Voted Perceptron with a single training iteration outperformed SVM in a set of similar tasks, we see no such behavior here.",
                "This further strengthens the evidence that an alternate classifier with the bag-of-words representation could not reach the same level of performance.",
                "The Voted Perceptron classifier does improve when the number of training iterations are increased, but it is still lower than the SVM classifier.",
                "Sentence detection results are presented in Table 6.",
                "With regard to the sentence detection problem, we note that the F1 measure gives a better feel for the remaining room for improvement in this difficult problem.",
                "That is, unlike document detection where actionitem documents are fairly common, action-item sentences are very rare.",
                "Thus, as in other text problems, the accuracy numbers are deceptively high sheerly because of the default accuracy attainable by always predicting no.",
                "Although, the results here are significantly above-random, it is unclear what level of performance is necessary for sentence detection to be useful in and of itself and not simply as a means to document ranking and classification.",
                "Figure 4: Users find action-items quicker when assisted by a classification system.",
                "Finally, when considering a new type of classification task, one of the most basic questions is whether an accurate classifier built for the task can have an impact on the end-user.",
                "In order to demonstrate the impact this task can have on e-mail users, we conducted a user study using an earlier less-accurate version of the sentence classifier - where instead of using just a single sentence, a threesentence windowed-approach was used.",
                "There were three distinct sets of e-mail in which users had to find action-items.",
                "These sets were either presented in a random order (Unordered), ordered by the classifier (Ordered), or ordered by the classifier and with the 0.4 0.5 0.6 0.7 0.8 0.9 1 0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1 Precision Recall Action-Item Detection SVM Performance (Post Model Selection) Document Unigram Sentence Ngram Figure 3: Both n-grams and a small prediction window lead to consistent improvements over the standard approach. center sentence in the highest confidence window highlighted (Order+help).",
                "In order to perform fair comparisons between conditions, the overall number of tokens in each message set should be approximately equal; that is, the cognitive reading load should be approximately the same before the classifiers reordering.",
                "Additionally, users typically show practice effects by improving at the overall task and thus performing better at later message sets.",
                "This is typically handled by varying the ordering of the sets across users so that the means are comparable.",
                "While omitting further detail, we note the sets were balanced for the total number of tokens and a latin square design was used to balance practice effects.",
                "Figure 4 shows that at intervals of 5, 10, and 15 minutes, users consistently found significantly more action-items when assisted by the classifier, but were most critically aided in the first five minutes.",
                "Although, the classifier consistently aids the users, we did not gain an additional end-user impact by highlighting.",
                "As mentioned above, this might be a result of the large room for improvement that still exists for sentence detection, but anecdotal evidence suggests this might also be a result of how the information is presented to the user rather than the accuracy of sentence detection.",
                "For example, highlighting the wrong sentence near an actual action-item hurts the users trust, but if a vague indicator (e.g., an arrow) points to the approximate area the user is not aware of the near-miss.",
                "Since the user studies used a three sentence window, we believe this played a role as well as sentence detection accuracy. 4.6 Discussion In contrast to problems where n-grams have yielded little difference, we believe their power here stems from the fact that many of the meaningful n-grams for action-items consist of common words, e.g., let me know.",
                "Therefore, the document-level unigram approach cannot gain much leverage, even when modeling their joint probability correctly, since these words will often co-occur in the document but not necessarily in a phrase.",
                "Additionally, action-item detection is distinct from many text classification tasks in that a single sentence can change the class label of the document.",
                "As a result, good classifiers cannot rely on aggregating evidence from a large number of weak indicators across the entire document.",
                "Even though we discarded the header information, examining the top-ranked features at the document-level reveals that many of the features are names or parts of e-mail addresses that occurred in the body and are highly associated with e-mails that tend to contain many or no action-items.",
                "A few examples are terms such as org, bob, and gov.",
                "We note that these features will be sensitive to the particular distribution (senders/receivers) and thus the document-level approach may produce classifiers that transfer less readily to alternate contexts and users at different institutions.",
                "This points out that part of the problem of going beyond bag-of-words may be the methodology, and investigating such properties as learning curves and how well a model transfers may highlight differences in models which appear to have similar performance when tested on the distributions they were trained on.",
                "We are currently investigating whether the sentence-level classifiers do perform better over different test corpora without retraining. 5.",
                "FUTURE WORK While applying text classifiers at the document-level is fairly well-understood, there exists the potential for significantly increasing the performance of the sentence-level classifiers.",
                "Such methods include alternate ways of combining the predictions over each sentence, weightings other than tfidf, which may not be appropriate since sentences are small, better sentence segmentation, and other types of phrasal analysis.",
                "Additionally, named entity tagging, time expressions, etc., seem likely candidates for features that can further improve this task.",
                "We are currently pursuing some of these avenues to see what additional gains these offer.",
                "Finally, it would be interesting to investigate the best methods for combining the document-level and sentence-level classifiers.",
                "Since the simple bag-of-words representation at the document-level leads to a learned model that behaves somewhat like a context-specific prior dependent on the sender/receiver and general topic, a first choice would be to treat it as such when combining probability estimates with the sentence-level classifier.",
                "Such a model might serve as a general example for other problems where bag-of-words can establish a baseline model but richer approaches are needed to achieve performance beyond that baseline. 6.",
                "SUMMARY AND CONCLUSIONS The effectiveness of sentence-level detection argues that labeling at the sentence-level provides significant value.",
                "Further experiments are needed to see how this interacts with the amount of training data available.",
                "Sentence detection that is then agglomerated to document-level detection works surprisingly better given low recall than would be expected with sentence-level items.",
                "This, in turn, indicates that improved sentence segmentation methods could yield further improvements in classification.",
                "In this work, we examined how action-items can be effectively detected in e-mails.",
                "Our empirical analysis has demonstrated that n-grams are of key importance to making the most of documentlevel judgments.",
                "When finer-grained judgments are available, then a standard bag-of-words approach using a small (sentence) window size and automatic segmentation techniques can produce results almost as good as the n-gram based approaches.",
                "Acknowledgments This material is based upon work supported by the Defense Advanced Research Projects Agency (DARPA) under Contract No.",
                "NBCHD030010.",
                "Any opinions, findings and conclusions or recommendations expressed in this material are those of the author(s) and do not necessarily reflect the views of the Defense Advanced Research Projects Agency (DARPA), or the Department of InteriorNational Business Center (DOI-NBC).",
                "We would like to extend our sincerest thanks to Jill Lehman whose efforts in data collection were essential in constructing the corpus, and both Jill and Aaron Steinfeld for their direction of the HCI experiments.",
                "We would also like to thank Django Wexler for constructing and supporting the corpus labeling tools and Curtis Huttenhowers support of the text preprocessing package.",
                "Finally, we gratefully acknowledge Scott Fahlman for his encouragement and useful discussions on this topic. 7.",
                "REFERENCES [1] J. Allan, J. Carbonell, G. Doddington, J. Yamron, and Y. Yang.",
                "Topic detection and tracking pilot study: Final report.",
                "In Proceedings of the DARPA Broadcast News Transcription and Understanding Workshop, Washington, D.C., 1998. [2] C. Apte, F. Damerau, and S. M. Weiss.",
                "Automated learning of decision rules for text categorization.",
                "ACM Transactions on Information Systems, 12(3):233-251, July 1994. [3] J. Carletta.",
                "Assessing agreement on classification tasks: The kappa statistic.",
                "Computational Linguistics, 22(2):249-254, 1996. [4] J. Carroll.",
                "High precision extraction of grammatical relations.",
                "In Proceedings of the 19th International Conference on Computational Linguistics (COLING), pages 134-140, 2002. [5] W. W. Cohen, V. R. Carvalho, and T. M. Mitchell.",
                "Learning to classify email into speech acts.",
                "In EMNLP-2004 (Conference on Empirical Methods in Natural Language Processing), pages 309-316, 2004. [6] S. Corston-Oliver, E. Ringger, M. Gamon, and R. Campbell.",
                "Task-focused summarization of email.",
                "In Text Summarization Branches Out: Proceedings of the ACL-04 Workshop, pages 43-50, 2004. [7] A. Culotta, R. Bekkerman, and A. McCallum.",
                "Extracting social networks and contact information from email and the web.",
                "In CEAS-2004 (Conference on Email and Anti-Spam), Mountain View, CA, July 2004. [8] L. Devroye, L. Gy¨orfi, and G. Lugosi.",
                "A Probabilistic Theory of Pattern Recognition.",
                "Springer-Verlag, New York, NY, 1996. [9] S. T. Dumais, J. Platt, D. Heckerman, and M. Sahami.",
                "Inductive learning algorithms and representations for text categorization.",
                "In CIKM 98, Proceedings of the 7th ACM Conference on Information and Knowledge Management, pages 148-155, 1998. [10] Y. Freund and R. Schapire.",
                "Large margin classification using the perceptron algorithm.",
                "Machine Learning, 37(3):277-296, 1999. [11] T. Joachims.",
                "Making large-scale svm learning practical.",
                "In B. Sch¨olkopf, C. J. Burges, and A. J. Smola, editors, Advances in Kernel Methods - Support Vector Learning, pages 41-56.",
                "MIT Press, 1999. [12] L. S. Larkey.",
                "A patent search and classification system.",
                "In Proceedings of the Fourth ACM Conference on Digital Libraries, pages 179 - 187, 1999. [13] D. D. Lewis.",
                "An evaluation of phrasal and clustered representations on a text categorization task.",
                "In SIGIR 92, Proceedings of the 15th Annual International ACM Conference on Research and Development in Information Retrieval, pages 37-50, 1992. [14] Y. Liu, J. Carbonell, and R. Jin.",
                "A pairwise ensemble approach for accurate genre classification.",
                "In Proceedings of the European Conference on Machine Learning (ECML), 2003. [15] Y. Liu, R. Yan, R. Jin, and J. Carbonell.",
                "A comparison study of kernels for multi-label text classification using category association.",
                "In The Twenty-first International Conference on Machine Learning (ICML), 2004. [16] A. McCallum and K. Nigam.",
                "A comparison of event models for naive bayes text classification.",
                "In Working Notes of AAAI 98 (The 15th National Conference on Artificial Intelligence), Workshop on Learning for Text Categorization, pages 41-48, 1998.",
                "TR WS-98-05. [17] F. Sebastiani.",
                "Machine learning in automated text categorization.",
                "ACM Computing Surveys, 34(1):1-47, March 2002. [18] C. J. van Rijsbergen.",
                "Information Retrieval.",
                "Butterworths, London, 1979. [19] Y. Yang.",
                "An evaluation of statistical approaches to text categorization.",
                "Information Retrieval, 1(1/2):67-88, 1999. [20] Y. Yang, J. Carbonell, R. Brown, T. Pierce, B. T. Archibald, and X. Liu.",
                "Learning approaches to topic detection and tracking.",
                "IEEE EXPERT, Special Issue on Applications of Intelligent Information Retrieval, 1999. [21] Y. Yang and X. Liu.",
                "A re-examination of text categorization methods.",
                "In SIGIR 99, Proceedings of the 22nd Annual International ACM Conference on Research and Development in Information Retrieval, pages 42-49, 1999. [22] Y. Yang, J. Zhang, J. Carbonell, and C. Jin.",
                "Topic-conditioned novelty detection.",
                "In Proceedings of the ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, July 2002."
            ],
            "original_annotated_samples": [],
            "translated_annotated_samples": [
                "Sin embargo, utilizando conjuntos de características enriquecidos, como N-grams (hasta N = 4) con \"selección de características de chi-cuadrado\", y las señales contextuales para la ubicación de acción de acción mejoran el rendimiento hasta un 10% sobre unigramas, utilizando en ambos casosClasificadores de última generación, como SVM con selección automatizada de modelos a través de validación cruzada integrada."
            ],
            "translated_text": "",
            "candidates": [
                "selección de características chi cuadrado",
                "selección de características de chi-cuadrado"
            ],
            "error": []
        },
        "automated model selection": {
            "translated_key": "Selección de modelo automatizado",
            "is_in_text": true,
            "original_annotated_sentences": [
                "Feature Representation for Effective Action-Item Detection Paul N. Bennett Computer Science Department Carnegie Mellon University Pittsburgh, PA 15213 pbennett+@cs.cmu.edu Jaime Carbonell Language Technologies Institute.",
                "Carnegie Mellon University Pittsburgh, PA 15213 jgc+@cs.cmu.edu ABSTRACT E-mail users face an ever-growing challenge in managing their inboxes due to the growing centrality of email in the workplace for task assignment, action requests, and other roles beyond information dissemination.",
                "Whereas Information Retrieval and Machine Learning techniques are gaining initial acceptance in spam filtering and automated folder assignment, this paper reports on a new task: automated action-item detection, in order to flag emails that require responses, and to highlight the specific passage(s) indicating the request(s) for action.",
                "Unlike standard topic-driven text classification, action-item detection requires inferring the senders intent, and as such responds less well to pure bag-of-words classification.",
                "However, using enriched feature sets, such as n-grams (up to n=4) with chi-squared feature selection, and contextual cues for action-item location improve performance by up to 10% over unigrams, using in both cases state of the art classifiers such as SVMs with <br>automated model selection</br> via embedded cross-validation.",
                "Categories and Subject Descriptors H.3.3 [Information Storage and Retrieval]: Information Search and Retrieval; I.2.6 [Artificial Intelligence]: Learning; I.5.4 [Pattern Recognition]: Applications General Terms Experimentation 1.",
                "INTRODUCTION E-mail users are facing an increasingly difficult task of managing their inboxes in the face of mounting challenges that result from rising e-mail usage.",
                "This includes prioritizing e-mails over a range of sources from business partners to family members, filtering and reducing junk e-mail, and quickly managing requests that demand From: Henry Hutchins <hhutchins@innovative.company.com> To: Sara Smith; Joe Johnson; William Woolings Subject: meeting with prospective customers Sent: Fri 12/10/2005 8:08 AM Hi All, Id like to remind all of you that the group from GRTY will be visiting us next Friday at 4:30 p.m.",
                "The current schedule looks like this: + 9:30 a.m.",
                "Informal Breakfast and Discussion in Cafeteria + 10:30 a.m. Company Overview + 11:00 a.m.",
                "Individual Meetings (Continue Over Lunch) + 2:00 p.m. Tour of Facilities + 3:00 p.m.",
                "Sales Pitch In order to have this go off smoothly, I would like to practice the presentation well in advance.",
                "As a result, I will need each of your parts by Wednesday.",
                "Keep up the good work! -Henry Figure 1: An E-mail with emphasized Action-Item, an explicit request that requires the recipients attention or action. the receivers attention or action.",
                "Automated action-item detection targets the third of these problems by attempting to detect which e-mails require an action or response with information, and within those e-mails, attempting to highlight the sentence (or other passage length) that directly indicates the action request.",
                "Such a detection system can be used as one part of an e-mail agent which would assist a user in processing important e-mails quicker than would have been possible without the agent.",
                "We view action-item detection as one necessary component of a successful e-mail agent which would perform spam detection, action-item detection, topic classification and priority ranking, among other functions.",
                "The utility of such a detector can manifest as a method of prioritizing e-mails according to task-oriented criteria other than the standard ones of topic and sender or as a means of ensuring that the email user hasnt dropped the proverbial ball by forgetting to address an action request.",
                "Action-item detection differs from standard text classification in two important ways.",
                "First, the user is interested both in detecting whether an email contains action items and in locating exactly where these action item requests are contained within the email body.",
                "In contrast, standard text categorization merely assigns a topic label to each text, whether that label corresponds to an e-mail folder or a controlled indexing vocabulary [12, 15, 22].",
                "Second, action-item detection attempts to recover the email senders intent - whether she means to elicit response or action on the part of the receiver; note that for this task, classifiers using only unigrams as features do not perform optimally, as evidenced in our results below.",
                "Instead we find that we need more information-laden features such as higher-order n-grams.",
                "Text categorization by topic, on the other hand, works very well using just individual words as features [2, 9, 13, 17].",
                "In fact, genre-classification, which one would think may require more than a bag-of-words approach, also works quite well using just unigram features [14].",
                "Topic detection and tracking (TDT), also works well with unigram feature sets [1, 20].",
                "We believe that action-item detection is one of the first clear instances of an IR-related task where we must move beyond bag-of-words to achieve high performance, albeit not too far, as bag-of-n-grams seem to suffice.",
                "We first review related work for similar text classification problems such as e-mail priority ranking and speech act identification.",
                "Then we more formally define the action-item detection problem, discuss the aspects that distinguish it from more common problems like topic classification, and highlight the challenges in constructing systems that can perform well at the sentence and document level.",
                "From there, we move to a discussion of feature representation and selection techniques appropriate for this problem and how standard text classification approaches can be adapted to smoothly move from the sentence-level detection problem to the documentlevel classification problem.",
                "We then conduct an empirical analysis that helps us determine the effectiveness of our feature extraction procedures as well as establish baselines for a number of classification algorithms on this task.",
                "Finally, we summarize this papers contributions and consider interesting directions for future work. 2.",
                "RELATED WORK Several other researchers have considered very similar text classification tasks.",
                "Cohen et al. [5] describe an ontology of speech acts, such as Propose a Meeting, and attempt to predict when an e-mail contains one of these speech acts.",
                "We consider action-items to be an important specific type of speech act that falls within their more general classification.",
                "While they provide results for several classification methods, their methods only make use of human judgments at the document-level.",
                "In contrast, we consider whether accuracy can be increased by using finer-grained human judgments that mark the specific sentences and phrases of interest.",
                "Corston-Oliver et al. [6] consider detecting items in e-mail to Put on a To-Do List.",
                "This classification task is very similar to ours except they do not consider simple factual questions to belong to this category.",
                "We include questions, but note that not all questions are action-items - some are rhetorical or simply social convention, How are you?.",
                "From a learning perspective, while they make use of judgments at the sentence-level, they do not explicitly compare what if any benefits finer-grained judgments offer.",
                "Additionally, they do not study alternative choices or approaches to the classification task.",
                "Instead, they simply apply a standard SVM at the sentence-level and focus primarily on a linguistic analysis of how the sentence can be logically reformulated before adding it to the task list.",
                "In this study, we examine several alternative classification methods, compare document-level and sentence-level approaches and analyze the machine learning issues implicit in these problems.",
                "Interest in a variety of learning tasks related to e-mail has been rapidly growing in the recent literature.",
                "For example, in a forum dedicated to e-mail learning tasks, Culotta et al. [7] presented methods for learning social networks from e-mail.",
                "In this work, we do not focus on peer relationships; however, such methods could complement those here since peer relationships often influence word choice when requesting an action. 3.",
                "PROBLEM DEFINITION & APPROACH In contrast to previous work, we explicitly focus on the benefits that finer-grained, more costly, sentence-level human judgments offer over coarse-grained document-level judgments.",
                "Additionally, we consider multiple standard text classification approaches and analyze both the quantitative and qualitative differences that arise from taking a document-level vs. a sentence-level approach to classification.",
                "Finally, we focus on the representation necessary to achieve the most competitive performance. 3.1 Problem Definition In order to provide the most benefit to the user, a system would not only detect the document, but it would also indicate the specific sentences in the e-mail which contain the action-items.",
                "Therefore, there are three basic problems: 1.",
                "Document detection: Classify a document as to whether or not it contains an action-item. 2.",
                "Document ranking: Rank the documents such that all documents containing action-items occur as high as possible in the ranking. 3.",
                "Sentence detection: Classify each sentence in a document as to whether or not it is an action-item.",
                "As in most Information Retrieval tasks, the weight the evaluation metric should give to precision and recall depends on the nature of the application.",
                "In situations where a user will eventually read all received messages, ranking (e.g., via precision at recall of 1) may be most important since this will help encourage shorter delays in communications between users.",
                "In contrast, high-precision detection at low recall will be of increasing importance when the user is under severe time-pressure and therefore will likely not read all mail.",
                "This can be the case for crisis managers during disaster management.",
                "Finally, sentence detection plays a role in both timepressure situations and simply to alleviate the users required time to gist the message. 3.2 Approach As mentioned above, the labeled data can come in one of two forms: a document-labeling provides a yes/no label for each document as to whether it contains an action-item; a phrase-labeling provides only a yes label for the specific items of interest.",
                "We term the human judgments a phrase-labeling since the users view of the action-item may not correspond with actual sentence boundaries or predicted sentence boundaries.",
                "Obviously, it is straightforward to generate a document-labeling consistent with a phrase-labeling by labeling a document yes if and only if it contains at least one phrase labeled yes.",
                "To train classifiers for this task, we can take several viewpoints related to both the basic problems we have enumerated and the form of the labeled data.",
                "The document-level view treats each e-mail as a learning instance with an associated class-label.",
                "Then, the document can be converted to a feature-value vector and learning progresses as usual.",
                "Applying a document-level classifier to document detection and ranking is straightforward.",
                "In order to apply it to sentence detection, one must make additional steps.",
                "For example, if the classifier predicts a document contains an action-item, then areas of the document that contain a high-concentration of words which the model weights heavily in favor of action-items can be indicated.",
                "The obvious benefit of the document-level approach is that training set collection costs are lower since the user only has to specify whether or not an e-mail contains an action-item and not the specific sentences.",
                "In the sentence-level view, each e-mail is automatically segmented into sentences, and each sentence is treated as a learning instance with an associated class-label.",
                "Since the phrase-labeling provided by the user may not coincide with the automatic segmentation, we must determine what label to assign a partially overlapping sentence when converting it to a learning instance.",
                "Once trained, applying the resulting classifiers to sentence detection is now straightforward, but in order to apply the classifiers to document detection and document ranking, the individual predictions over each sentence must be aggregated in order to make a document-level prediction.",
                "This approach has the potential to benefit from morespecific labels that enable the learner to focus attention on the key sentences instead of having to learn based on data that the majority of the words in the e-mail provide no or little information about class membership. 3.2.1 Features Consider some of the phrases that might constitute part of an action item: would like to know, let me know, as soon as possible, have you.",
                "Each of these phrases consists of common words that occur in many e-mails.",
                "However, when they occur in the same sentence, they are far more indicative of an action-item.",
                "Additionally, order can be important: consider have you versus you have.",
                "Because of this, we posit that n-grams play a larger role in this problem than is typical of problems like topic classification.",
                "Therefore, we consider all n-grams up to size 4.",
                "When using n-grams, if we find an n-gram of size 4 in a segment of text, we can represent the text as just one occurrence of the ngram or as one occurrence of the n-gram and an occurrence of each smaller n-gram contained by it.",
                "We choose the second of these alternatives since this will allow the algorithm itself to smoothly back-off in terms of recall.",
                "Methods such as na¨ıve Bayes may be hurt by such a representation because of double-counting.",
                "Since sentence-ending punctuation can provide information, we retain the terminating punctuation token when it is identifiable.",
                "Additionally, we add a beginning-of-sentence and end-of-sentence token in order to capture patterns that are often indicators at the beginning or end of a sentence.",
                "Assuming proper punctuation, these extra tokens are unnecessary, but often e-mail lacks proper punctuation.",
                "In addition, for the sentence-level classifiers that use ngrams, we additionally code for each sentence a binary encoding of the position of the sentence relative to the document.",
                "This encoding has eight associated features that represent which octile (the first eighth, second eighth, etc.) contains the sentence. 3.2.2 Implementation Details In order to compare the document-level to the sentence-level approach, we compare predictions at the document-level.",
                "We do not address how to use a document-level classifier to make predictions at the sentence-level.",
                "In order to automatically segment the text of the e-mail, we use the RASP statistical parser [4].",
                "Since the automatically segmented sentences may not correspond directly with the phrase-level boundaries, we treat any sentence that contains at least 30% of a marked action-item segment as an action-item.",
                "When evaluating sentencedetection for the sentence-level system, we use these class labels as ground truth.",
                "Since we are not evaluating multiple segmentation approaches, this does not bias any of the methods.",
                "If multiple segmentation systems were under evaluation, one would need to use a metric that matched predicted positive sentences to phrases labeled positive.",
                "The metric would need to punish overly long true predictions as well as too short predictions.",
                "Our criteria for converting to labeled instances implicitly includes both criteria.",
                "Since the segmentation is fixed, an overly long prediction would be predicting yes for many no instances since presumably the extra length corresponds to additional segmented sentences all of which do not contain 30% of action-item.",
                "Likewise, a too short prediction must correspond to a small sentence included in the action-item but not constituting all of the action-item.",
                "Therefore, in order to consider the prediction to be too short, there will be an additional preceding/following sentence that is an action-item where we incorrectly predicted no.",
                "Once a sentence-level classifier has made a prediction for each sentence, we must combine these predictions to make both a document-level prediction and a document-level score.",
                "We use the simple policy of predicting positive when any of the sentences is predicted positive.",
                "In order to produce a document score for ranking, the confidence that the document contains an action-item is: ψ(d) = 1 n(d) s∈d|π(s)=1 ψ(s) if for any s ∈ d, π(s) = 1 1 n(d) maxs∈d ψ(s) o.w. where s is a sentence in document d, π is the classifiers 1/0 prediction, ψ is the score the classifier assigns as its confidence that π(s) = 1, and n(d) is the greater of 1 and the number of (unigram) tokens in the document.",
                "In other words, when any sentence is predicted positive, the document score is the length normalized sum of the sentence scores above threshold.",
                "When no sentence is predicted positive, the document score is the maximum sentence score normalized by length.",
                "As in other text problems, we are more likely to emit false positives for documents with more words or sentences.",
                "Thus we include a length normalization factor. 4.",
                "EXPERIMENTAL ANALYSIS 4.1 The Data Our corpus consists of e-mails obtained from volunteers at an educational institution and cover subjects such as: organizing a research workshop, arranging for job-candidate interviews, publishing proceedings, and talk announcements.",
                "The messages were anonymized by replacing the names of each individual and institution with a pseudonym.1 After attempting to identify and eliminate duplicate e-mails, the corpus contains 744 e-mail messages.",
                "After identity anonymization, the corpora has three basic versions.",
                "Quoted material refers to the text of a previous e-mail that an author often leaves in an e-mail message when responding to the e-mail.",
                "Quoted material can act as noise when learning since it may include action-items from previous messages that are no longer relevant.",
                "To isolate the effects of quoted material, we have three versions of the corpora.",
                "The raw form contains the basic messages.",
                "The auto-stripped version contains the messages after quoted material has been automatically removed.",
                "The hand-stripped version contains the messages after quoted material has been removed by a human.",
                "Additionally, the hand-stripped version has had any xml content and e-mail signatures removed - leaving only the essential content of the message.",
                "The studies reported here are performed with the hand-stripped version.",
                "This allows us to balance the cognitive load in terms of number of tokens that must be read in the user-studies we report - including quoted material would complicate the user studies since some users might skip the material while others read it.",
                "Additionally, ensuring all quoted material is removed 1 We have an even more highly anonymized version of the corpus that can be made available for some outside experimentation.",
                "Please contact the authors for more information on obtaining this data. prevents tainting the cross-validation since otherwise a test item could occur as quoted material in a training document. 4.1.1 Data Labeling Two human annotators labeled each message as to whether or not it contained an action-item.",
                "In addition, they identified each segment of the e-mail which contained an action-item.",
                "A segment is a contiguous section of text selected by the human annotators and may span several sentences or a complete phrase contained in a sentence.",
                "They were instructed that an action item is an explicit request for information that requires the recipients attention or a required action and told to highlight the phrases or sentences that make up the request.",
                "Annotator 1 No Yes Annotator 2 No 391 26 Yes 29 298 Table 1: Agreement of Human Annotators at Document Level Annotator One labeled 324 messages as containing action items.",
                "Annotator Two labeled 327 messages as containing action items.",
                "The agreement of the human annotators is shown in Tables 1 and 2.",
                "The annotators are said to agree at the document-level when both marked the same document as containing no action-items or both marked at least one action-item regardless of whether the text segments were the same.",
                "At the document-level, the annotators agreed 93% of the time.",
                "The kappa statistic [3, 5] is often used to evaluate inter-annotator agreement: κ = A − R 1 − R A is the empirical estimate of the probability of agreement.",
                "R is the empirical estimate of the probability of random agreement given the empirical class priors.",
                "A value close to −1 implies the annotators agree far less often than would be expected randomly, while a value close to 1 means they agree more often than randomly expected.",
                "At the document-level, the kappa statistic for inter-annotator agreement is 0.85.",
                "This value is both strong enough to expect the problem to be learnable and is comparable with results for similar tasks [5, 6].",
                "In order to determine the sentence-level agreement, we use each judgment to create a sentence-corpus with labels as described in Section 3.2.2, then consider the agreement over these sentences.",
                "This allows us to compare agreement over no judgments.",
                "We perform this comparison over the hand-stripped corpus since that eliminates spurious no judgments that would come from including quoted material, etc.",
                "Both annotators were free to label the subject as an action-item, but since neither did, we omit the subject line of the message as well.",
                "This only reduces the number of no agreements.",
                "This leaves 6301 automatically segmented sentences.",
                "At the sentence-level, the annotators agreed 98% of the time, and the kappa statistic for inter-annotator agreement is 0.82.",
                "In order to produce one single set of judgments, the human annotators went through each annotation where there was disagreement and came to a consensus opinion.",
                "The annotators did not collect statistics during this process but anecdotally reported that the majority of disagreements were either cases of clear annotator oversight or different interpretations of conditional statements.",
                "For example, If you would like to keep your job, come to tomorrows meeting implies a required action where If you would like to join Annotator 1 No Yes Annotator 2 No 5810 65 Yes 74 352 Table 2: Agreement of Human Annotators at Sentence Level the football betting pool, come to tomorrows meeting does not.",
                "The first would be an action-item in most contexts while the second would not.",
                "Of course, many conditional statements are not so clearly interpretable.",
                "After reconciling the judgments there are 416 e-mails with no action-items and 328 e-mails containing actionitems.",
                "Of the 328 e-mails containing action-items, 259 messages have one action-item segment; 55 messages have two action-item segments; 11 messages have three action-item segments.",
                "Two messages have four action-item segments, and one message has six action-item segments.",
                "Computing the sentence-level agreement using the reconciled gold standard judgments with each of the annotators individual judgments gives a kappa of 0.89 for Annotator One and a kappa of 0.92 for Annotator Two.",
                "In terms of message characteristics, there were on average 132 content tokens in the body after stripping.",
                "For action-item messages, there were 115.",
                "However, by examining Figure 2 we see the length distributions are nearly identical.",
                "As would be expected for e-mail, it is a long-tailed distribution with about half the messages having more than 60 tokens in the body (this paragraph has 65 tokens). 4.2 Classifiers For this experiment, we have selected a variety of standard text classification algorithms.",
                "In selecting algorithms, we have chosen algorithms that are not only known to work well but which differ along such lines as discriminative vs. generative and lazy vs. eager.",
                "We have done this in order to provide both a competitive and thorough sampling of learning methods for the task at hand.",
                "This is important since it is easy to improve a strawman classifier by introducing a new representation.",
                "By thoroughly sampling alternative classifier choices we demonstrate that representation improvements over bag-of-words are not due to using the information in the bag-of-words poorly. 4.2.1 kNN We employ a standard variant of the k-nearest neighbor algorithm used in text classification, kNN with s-cut score thresholding [19].",
                "We use a tfidf-weighting of the terms with a distanceweighted vote of the neighbors to compute the score before thresholding it.",
                "In order to choose the value of s for thresholding, we perform leave-one-out cross-validation over the training set.",
                "The value of k is set to be 2( log2 N + 1) where N is the number of training points.",
                "This rule for choosing k is theoretically motivated by results which show such a rule converges to the optimal classifier as the number of training points increases [8].",
                "In practice, we have also found it to be a computational convenience that frequently leads to comparable results with numerically optimizing k via a cross-validation procedure. 4.2.2 Na¨ıve Bayes We use a standard multinomial na¨ıve Bayes classifier [16].",
                "In using this classifier, we smoothed word and class probabilities using a Bayesian estimate (with the word prior) and a Laplace m-estimate, respectively. 0 20 40 60 80 100 120 140 160 0 200 400 600 800 1000 1200 1400 NumberofMessages Number of Tokens All Messages Action-Item Messages 0 0.02 0.04 0.06 0.08 0.1 0.12 0.14 0.16 0.18 0.2 0 200 400 600 800 1000 1200 1400 PercentageofMessages Number of Tokens All Messages Action-Item Messages Figure 2: The Histogram (left) and Distribution (right) of Message Length.",
                "A bin size of 20 words was used.",
                "Only tokens in the body after hand-stripping were counted.",
                "After stripping, the majority of words left are usually actual message content.",
                "Classifiers Document Unigram Document Ngram Sentence Unigram Sentence Ngram F1 kNN 0.6670 ± 0.0288 0.7108 ± 0.0699 0.7615 ± 0.0504 0.7790 ± 0.0460 na¨ıve Bayes 0.6572 ± 0.0749 0.6484 ± 0.0513 0.7715 ± 0.0597 0.7777 ± 0.0426 SVM 0.6904 ± 0.0347 0.7428 ± 0.0422 0.7282 ± 0.0698 0.7682 ± 0.0451 Voted Perceptron 0.6288 ± 0.0395 0.6774 ± 0.0422 0.6511 ± 0.0506 0.6798 ± 0.0913 Accuracy kNN 0.7029 ± 0.0659 0.7486 ± 0.0505 0.7972 ± 0.0435 0.8092 ± 0.0352 na¨ıve Bayes 0.6074 ± 0.0651 0.5816 ± 0.1075 0.7863 ± 0.0553 0.8145 ± 0.0268 SVM 0.7595 ± 0.0309 0.7904 ± 0.0349 0.7958 ± 0.0551 0.8173 ± 0.0258 Voted Perceptron 0.6531 ± 0.0390 0.7164 ± 0.0376 0.6413 ± 0.0833 0.7082 ± 0.1032 Table 3: Average Document-Detection Performance during Cross-Validation for Each Method and the Sample Standard Deviation (Sn−1) in italics.",
                "The best performance for each classifier is shown in bold. 4.2.3 SVM We have used a linear SVM with a tfidf feature representation and L2-norm as implemented in the SVMlight package v6.01 [11].",
                "All default settings were used. 4.2.4 Voted Perceptron Like the SVM, the Voted Perceptron is a kernel-based learning method.",
                "We use the same feature representation and kernel as we have for the SVM, a linear kernel with tfidf-weighting and an L2-norm.",
                "The voted perceptron is an online-learning method that keeps a history of past perceptrons used, as well as a weight signifying how often that perceptron was correct.",
                "With each new training example, a correct classification increases the weight on the current perceptron and an incorrect classification updates the perceptron.",
                "The output of the classifier uses the weights on the perceptra to make a final voted classification.",
                "When used in an offline-manner, multiple passes can be made through the training data.",
                "Both the voted perceptron and the SVM give a solution from the same hypothesis space - in this case, a linear classifier.",
                "Furthermore, it is well-known that the Voted Perceptron increases the margin of the solution after each pass through the training data [10].",
                "Since Cohen et al. [5] obtain worse results using an SVM than a Voted Perceptron with one training iteration, they conclude that the best solution for detecting speech acts may not lie in an area with a large margin.",
                "Because their tasks are highly similar to ours, we employ both classifiers to ensure we are not overlooking a competitive alternative classifier to the SVM for the basic bag-of-words representation. 4.3 Performance Measures To compare the performance of the classification methods, we look at two standard performance measures, F1 and accuracy.",
                "The F1 measure [18, 21] is the harmonic mean of precision and recall where Precision = Correct Positives Predicted Positives and Recall = Correct Positives Actual Positives . 4.4 Experimental Methodology We perform standard 10-fold cross-validation on the set of documents.",
                "For the sentence-level approach, all sentences in a document are either entirely in the training set or entirely in the test set for each fold.",
                "For significance tests, we use a two-tailed t-test [21] to compare the values obtained during each cross-validation fold with a p-value of 0.05.",
                "Feature selection was performed using the chi-squared statistic.",
                "Different levels of feature selection were considered for each classifier.",
                "Each of the following number of features was tried: 10, 25, 50, 100, 250, 750, 1000, 2000, 4000.",
                "There are approximately 4700 unigram tokens without feature selection.",
                "In order to choose the number of features to use for each classifier, we perform nested cross-validation and choose the settings that yield the optimal document-level F1 for that classifier.",
                "For this study, only the body of each e-mail message was used.",
                "Feature selection is always applied to all candidate features.",
                "That is, for the n-gram representation, the n-grams and position features are also subject to removal by the feature selection method. 4.5 Results The results for document-level classification are given in Table 3.",
                "The primary hypothesis we are concerned with is that n-grams are critical for this task; if this is true, we expect to see a significant gap in performance between the document-level classifiers that use n-grams (denoted Document Ngram) and those using only unigram features (denoted Document Unigram).",
                "Examining Table 3, we observe that this is indeed the case for every classifier except na¨ıve Bayes.",
                "This difference in performance produced by the n-gram representation is statistically significant for each classifier except for na¨ıve Bayes and the accuracy metric for kNN (see Table 4).",
                "Na¨ıve Bayes poor performance with the n-gram representation is not surprising since the bag-of-n-grams causes excessive doublecounting as mentioned in Section 3.2.1; however, na¨ıve Bayes is not hurt at the sentence-level because the sparse examples provide few chances for agglomerative effects of double counting.",
                "In either case, when a language-modeling approach is desired, modeling the n-grams directly would be preferable to na¨ıve Bayes.",
                "More importantly for the n-gram hypothesis, the n-grams lead to the best document-level classifier performance as well.",
                "As would be expected, the difference between the sentence-level n-gram representation and unigram representation is small.",
                "This is because the window of text is so small that the unigram representation, when done at the sentence-level, implicitly picks up on the power of the n-grams.",
                "Further improvement would signify that the order of the words matter even when only considering a small sentence-size window.",
                "Therefore, the finer-grained sentence-level judgments allows a unigram representation to succeed but only when performed in a small window - behaving as an n-gram representation for all practical purposes.",
                "Document Winner Sentence Winner kNN Ngram Ngram na¨ıve Bayes Unigram Ngram SVM Ngram† Ngram Voted Perceptron Ngram† Ngram Table 4: Significance results for n-grams versus unigrams for document detection using document-level and sentence-level classifiers.",
                "When the F1 result is statistically significant, it is shown in bold.",
                "When the accuracy result is significant, it is shown with a † .",
                "F1 Winner Accuracy Winner kNN Sentence Sentence na¨ıve Bayes Sentence Sentence SVM Sentence Sentence Voted Perceptron Sentence Document Table 5: Significance results for sentence-level classifiers vs. document-level classifiers for the document detection problem.",
                "When the result is statistically significant, it is shown in bold.",
                "Further highlighting the improvement from finer-grained judgments and n-grams, Figure 3 graphically depicts the edge the SVM sentence-level classifier has over the standard bag-of-words approach with a precision-recall curve.",
                "In the high precision area of the graph, the consistent edge of the sentence-level classifier is rather impressive - continuing at precision 1 out to 0.1 recall.",
                "This would mean that a tenth of the users action-items would be placed at the top of their action-item sorted inbox.",
                "Additionally, the large separation at the top right of the curves corresponds to the area where the optimal F1 occurs for each classifier, agreeing with the large improvement from 0.6904 to 0.7682 in F1 score.",
                "Considering the relative unexplored nature of classification at the sentence-level, this gives great hope for further increases in performance.",
                "Accuracy F1 Unigram Ngram Unigram Ngram kNN 0.9519 0.9536 0.6540 0.6686 na¨ıve Bayes 0.9419 0.9550 0.6176 0.6676 SVM 0.9559 0.9579 0.6271 0.6672 Voted Perceptron 0.8895 0.9247 0.3744 0.5164 Table 6: Performance of the Sentence-Level Classifiers at Sentence Detection Although Cohen et al. [5] observed that the Voted Perceptron with a single training iteration outperformed SVM in a set of similar tasks, we see no such behavior here.",
                "This further strengthens the evidence that an alternate classifier with the bag-of-words representation could not reach the same level of performance.",
                "The Voted Perceptron classifier does improve when the number of training iterations are increased, but it is still lower than the SVM classifier.",
                "Sentence detection results are presented in Table 6.",
                "With regard to the sentence detection problem, we note that the F1 measure gives a better feel for the remaining room for improvement in this difficult problem.",
                "That is, unlike document detection where actionitem documents are fairly common, action-item sentences are very rare.",
                "Thus, as in other text problems, the accuracy numbers are deceptively high sheerly because of the default accuracy attainable by always predicting no.",
                "Although, the results here are significantly above-random, it is unclear what level of performance is necessary for sentence detection to be useful in and of itself and not simply as a means to document ranking and classification.",
                "Figure 4: Users find action-items quicker when assisted by a classification system.",
                "Finally, when considering a new type of classification task, one of the most basic questions is whether an accurate classifier built for the task can have an impact on the end-user.",
                "In order to demonstrate the impact this task can have on e-mail users, we conducted a user study using an earlier less-accurate version of the sentence classifier - where instead of using just a single sentence, a threesentence windowed-approach was used.",
                "There were three distinct sets of e-mail in which users had to find action-items.",
                "These sets were either presented in a random order (Unordered), ordered by the classifier (Ordered), or ordered by the classifier and with the 0.4 0.5 0.6 0.7 0.8 0.9 1 0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1 Precision Recall Action-Item Detection SVM Performance (Post Model Selection) Document Unigram Sentence Ngram Figure 3: Both n-grams and a small prediction window lead to consistent improvements over the standard approach. center sentence in the highest confidence window highlighted (Order+help).",
                "In order to perform fair comparisons between conditions, the overall number of tokens in each message set should be approximately equal; that is, the cognitive reading load should be approximately the same before the classifiers reordering.",
                "Additionally, users typically show practice effects by improving at the overall task and thus performing better at later message sets.",
                "This is typically handled by varying the ordering of the sets across users so that the means are comparable.",
                "While omitting further detail, we note the sets were balanced for the total number of tokens and a latin square design was used to balance practice effects.",
                "Figure 4 shows that at intervals of 5, 10, and 15 minutes, users consistently found significantly more action-items when assisted by the classifier, but were most critically aided in the first five minutes.",
                "Although, the classifier consistently aids the users, we did not gain an additional end-user impact by highlighting.",
                "As mentioned above, this might be a result of the large room for improvement that still exists for sentence detection, but anecdotal evidence suggests this might also be a result of how the information is presented to the user rather than the accuracy of sentence detection.",
                "For example, highlighting the wrong sentence near an actual action-item hurts the users trust, but if a vague indicator (e.g., an arrow) points to the approximate area the user is not aware of the near-miss.",
                "Since the user studies used a three sentence window, we believe this played a role as well as sentence detection accuracy. 4.6 Discussion In contrast to problems where n-grams have yielded little difference, we believe their power here stems from the fact that many of the meaningful n-grams for action-items consist of common words, e.g., let me know.",
                "Therefore, the document-level unigram approach cannot gain much leverage, even when modeling their joint probability correctly, since these words will often co-occur in the document but not necessarily in a phrase.",
                "Additionally, action-item detection is distinct from many text classification tasks in that a single sentence can change the class label of the document.",
                "As a result, good classifiers cannot rely on aggregating evidence from a large number of weak indicators across the entire document.",
                "Even though we discarded the header information, examining the top-ranked features at the document-level reveals that many of the features are names or parts of e-mail addresses that occurred in the body and are highly associated with e-mails that tend to contain many or no action-items.",
                "A few examples are terms such as org, bob, and gov.",
                "We note that these features will be sensitive to the particular distribution (senders/receivers) and thus the document-level approach may produce classifiers that transfer less readily to alternate contexts and users at different institutions.",
                "This points out that part of the problem of going beyond bag-of-words may be the methodology, and investigating such properties as learning curves and how well a model transfers may highlight differences in models which appear to have similar performance when tested on the distributions they were trained on.",
                "We are currently investigating whether the sentence-level classifiers do perform better over different test corpora without retraining. 5.",
                "FUTURE WORK While applying text classifiers at the document-level is fairly well-understood, there exists the potential for significantly increasing the performance of the sentence-level classifiers.",
                "Such methods include alternate ways of combining the predictions over each sentence, weightings other than tfidf, which may not be appropriate since sentences are small, better sentence segmentation, and other types of phrasal analysis.",
                "Additionally, named entity tagging, time expressions, etc., seem likely candidates for features that can further improve this task.",
                "We are currently pursuing some of these avenues to see what additional gains these offer.",
                "Finally, it would be interesting to investigate the best methods for combining the document-level and sentence-level classifiers.",
                "Since the simple bag-of-words representation at the document-level leads to a learned model that behaves somewhat like a context-specific prior dependent on the sender/receiver and general topic, a first choice would be to treat it as such when combining probability estimates with the sentence-level classifier.",
                "Such a model might serve as a general example for other problems where bag-of-words can establish a baseline model but richer approaches are needed to achieve performance beyond that baseline. 6.",
                "SUMMARY AND CONCLUSIONS The effectiveness of sentence-level detection argues that labeling at the sentence-level provides significant value.",
                "Further experiments are needed to see how this interacts with the amount of training data available.",
                "Sentence detection that is then agglomerated to document-level detection works surprisingly better given low recall than would be expected with sentence-level items.",
                "This, in turn, indicates that improved sentence segmentation methods could yield further improvements in classification.",
                "In this work, we examined how action-items can be effectively detected in e-mails.",
                "Our empirical analysis has demonstrated that n-grams are of key importance to making the most of documentlevel judgments.",
                "When finer-grained judgments are available, then a standard bag-of-words approach using a small (sentence) window size and automatic segmentation techniques can produce results almost as good as the n-gram based approaches.",
                "Acknowledgments This material is based upon work supported by the Defense Advanced Research Projects Agency (DARPA) under Contract No.",
                "NBCHD030010.",
                "Any opinions, findings and conclusions or recommendations expressed in this material are those of the author(s) and do not necessarily reflect the views of the Defense Advanced Research Projects Agency (DARPA), or the Department of InteriorNational Business Center (DOI-NBC).",
                "We would like to extend our sincerest thanks to Jill Lehman whose efforts in data collection were essential in constructing the corpus, and both Jill and Aaron Steinfeld for their direction of the HCI experiments.",
                "We would also like to thank Django Wexler for constructing and supporting the corpus labeling tools and Curtis Huttenhowers support of the text preprocessing package.",
                "Finally, we gratefully acknowledge Scott Fahlman for his encouragement and useful discussions on this topic. 7.",
                "REFERENCES [1] J. Allan, J. Carbonell, G. Doddington, J. Yamron, and Y. Yang.",
                "Topic detection and tracking pilot study: Final report.",
                "In Proceedings of the DARPA Broadcast News Transcription and Understanding Workshop, Washington, D.C., 1998. [2] C. Apte, F. Damerau, and S. M. Weiss.",
                "Automated learning of decision rules for text categorization.",
                "ACM Transactions on Information Systems, 12(3):233-251, July 1994. [3] J. Carletta.",
                "Assessing agreement on classification tasks: The kappa statistic.",
                "Computational Linguistics, 22(2):249-254, 1996. [4] J. Carroll.",
                "High precision extraction of grammatical relations.",
                "In Proceedings of the 19th International Conference on Computational Linguistics (COLING), pages 134-140, 2002. [5] W. W. Cohen, V. R. Carvalho, and T. M. Mitchell.",
                "Learning to classify email into speech acts.",
                "In EMNLP-2004 (Conference on Empirical Methods in Natural Language Processing), pages 309-316, 2004. [6] S. Corston-Oliver, E. Ringger, M. Gamon, and R. Campbell.",
                "Task-focused summarization of email.",
                "In Text Summarization Branches Out: Proceedings of the ACL-04 Workshop, pages 43-50, 2004. [7] A. Culotta, R. Bekkerman, and A. McCallum.",
                "Extracting social networks and contact information from email and the web.",
                "In CEAS-2004 (Conference on Email and Anti-Spam), Mountain View, CA, July 2004. [8] L. Devroye, L. Gy¨orfi, and G. Lugosi.",
                "A Probabilistic Theory of Pattern Recognition.",
                "Springer-Verlag, New York, NY, 1996. [9] S. T. Dumais, J. Platt, D. Heckerman, and M. Sahami.",
                "Inductive learning algorithms and representations for text categorization.",
                "In CIKM 98, Proceedings of the 7th ACM Conference on Information and Knowledge Management, pages 148-155, 1998. [10] Y. Freund and R. Schapire.",
                "Large margin classification using the perceptron algorithm.",
                "Machine Learning, 37(3):277-296, 1999. [11] T. Joachims.",
                "Making large-scale svm learning practical.",
                "In B. Sch¨olkopf, C. J. Burges, and A. J. Smola, editors, Advances in Kernel Methods - Support Vector Learning, pages 41-56.",
                "MIT Press, 1999. [12] L. S. Larkey.",
                "A patent search and classification system.",
                "In Proceedings of the Fourth ACM Conference on Digital Libraries, pages 179 - 187, 1999. [13] D. D. Lewis.",
                "An evaluation of phrasal and clustered representations on a text categorization task.",
                "In SIGIR 92, Proceedings of the 15th Annual International ACM Conference on Research and Development in Information Retrieval, pages 37-50, 1992. [14] Y. Liu, J. Carbonell, and R. Jin.",
                "A pairwise ensemble approach for accurate genre classification.",
                "In Proceedings of the European Conference on Machine Learning (ECML), 2003. [15] Y. Liu, R. Yan, R. Jin, and J. Carbonell.",
                "A comparison study of kernels for multi-label text classification using category association.",
                "In The Twenty-first International Conference on Machine Learning (ICML), 2004. [16] A. McCallum and K. Nigam.",
                "A comparison of event models for naive bayes text classification.",
                "In Working Notes of AAAI 98 (The 15th National Conference on Artificial Intelligence), Workshop on Learning for Text Categorization, pages 41-48, 1998.",
                "TR WS-98-05. [17] F. Sebastiani.",
                "Machine learning in automated text categorization.",
                "ACM Computing Surveys, 34(1):1-47, March 2002. [18] C. J. van Rijsbergen.",
                "Information Retrieval.",
                "Butterworths, London, 1979. [19] Y. Yang.",
                "An evaluation of statistical approaches to text categorization.",
                "Information Retrieval, 1(1/2):67-88, 1999. [20] Y. Yang, J. Carbonell, R. Brown, T. Pierce, B. T. Archibald, and X. Liu.",
                "Learning approaches to topic detection and tracking.",
                "IEEE EXPERT, Special Issue on Applications of Intelligent Information Retrieval, 1999. [21] Y. Yang and X. Liu.",
                "A re-examination of text categorization methods.",
                "In SIGIR 99, Proceedings of the 22nd Annual International ACM Conference on Research and Development in Information Retrieval, pages 42-49, 1999. [22] Y. Yang, J. Zhang, J. Carbonell, and C. Jin.",
                "Topic-conditioned novelty detection.",
                "In Proceedings of the ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, July 2002."
            ],
            "original_annotated_samples": [],
            "translated_annotated_samples": [
                "Sin embargo, el uso de conjuntos de características enriquecidos, como N-grams (hasta n = 4) con selección de características chi cuadrado, y las señales contextuales para la ubicación de acción de acción mejoran el rendimiento hasta un 10% sobre unigramas, utilizando en ambos casos de estado deLos clasificadores de ART, como SVM con \"selección automatizada de modelos\" a través de validación cruzada integrada."
            ],
            "translated_text": "",
            "candidates": [
                "Selección de modelo automatizado",
                "selección automatizada de modelos"
            ],
            "error": []
        },
        "embedded cross-validation": {
            "translated_key": "validación cruzada",
            "is_in_text": true,
            "original_annotated_sentences": [
                "Feature Representation for Effective Action-Item Detection Paul N. Bennett Computer Science Department Carnegie Mellon University Pittsburgh, PA 15213 pbennett+@cs.cmu.edu Jaime Carbonell Language Technologies Institute.",
                "Carnegie Mellon University Pittsburgh, PA 15213 jgc+@cs.cmu.edu ABSTRACT E-mail users face an ever-growing challenge in managing their inboxes due to the growing centrality of email in the workplace for task assignment, action requests, and other roles beyond information dissemination.",
                "Whereas Information Retrieval and Machine Learning techniques are gaining initial acceptance in spam filtering and automated folder assignment, this paper reports on a new task: automated action-item detection, in order to flag emails that require responses, and to highlight the specific passage(s) indicating the request(s) for action.",
                "Unlike standard topic-driven text classification, action-item detection requires inferring the senders intent, and as such responds less well to pure bag-of-words classification.",
                "However, using enriched feature sets, such as n-grams (up to n=4) with chi-squared feature selection, and contextual cues for action-item location improve performance by up to 10% over unigrams, using in both cases state of the art classifiers such as SVMs with automated model selection via <br>embedded cross-validation</br>.",
                "Categories and Subject Descriptors H.3.3 [Information Storage and Retrieval]: Information Search and Retrieval; I.2.6 [Artificial Intelligence]: Learning; I.5.4 [Pattern Recognition]: Applications General Terms Experimentation 1.",
                "INTRODUCTION E-mail users are facing an increasingly difficult task of managing their inboxes in the face of mounting challenges that result from rising e-mail usage.",
                "This includes prioritizing e-mails over a range of sources from business partners to family members, filtering and reducing junk e-mail, and quickly managing requests that demand From: Henry Hutchins <hhutchins@innovative.company.com> To: Sara Smith; Joe Johnson; William Woolings Subject: meeting with prospective customers Sent: Fri 12/10/2005 8:08 AM Hi All, Id like to remind all of you that the group from GRTY will be visiting us next Friday at 4:30 p.m.",
                "The current schedule looks like this: + 9:30 a.m.",
                "Informal Breakfast and Discussion in Cafeteria + 10:30 a.m. Company Overview + 11:00 a.m.",
                "Individual Meetings (Continue Over Lunch) + 2:00 p.m. Tour of Facilities + 3:00 p.m.",
                "Sales Pitch In order to have this go off smoothly, I would like to practice the presentation well in advance.",
                "As a result, I will need each of your parts by Wednesday.",
                "Keep up the good work! -Henry Figure 1: An E-mail with emphasized Action-Item, an explicit request that requires the recipients attention or action. the receivers attention or action.",
                "Automated action-item detection targets the third of these problems by attempting to detect which e-mails require an action or response with information, and within those e-mails, attempting to highlight the sentence (or other passage length) that directly indicates the action request.",
                "Such a detection system can be used as one part of an e-mail agent which would assist a user in processing important e-mails quicker than would have been possible without the agent.",
                "We view action-item detection as one necessary component of a successful e-mail agent which would perform spam detection, action-item detection, topic classification and priority ranking, among other functions.",
                "The utility of such a detector can manifest as a method of prioritizing e-mails according to task-oriented criteria other than the standard ones of topic and sender or as a means of ensuring that the email user hasnt dropped the proverbial ball by forgetting to address an action request.",
                "Action-item detection differs from standard text classification in two important ways.",
                "First, the user is interested both in detecting whether an email contains action items and in locating exactly where these action item requests are contained within the email body.",
                "In contrast, standard text categorization merely assigns a topic label to each text, whether that label corresponds to an e-mail folder or a controlled indexing vocabulary [12, 15, 22].",
                "Second, action-item detection attempts to recover the email senders intent - whether she means to elicit response or action on the part of the receiver; note that for this task, classifiers using only unigrams as features do not perform optimally, as evidenced in our results below.",
                "Instead we find that we need more information-laden features such as higher-order n-grams.",
                "Text categorization by topic, on the other hand, works very well using just individual words as features [2, 9, 13, 17].",
                "In fact, genre-classification, which one would think may require more than a bag-of-words approach, also works quite well using just unigram features [14].",
                "Topic detection and tracking (TDT), also works well with unigram feature sets [1, 20].",
                "We believe that action-item detection is one of the first clear instances of an IR-related task where we must move beyond bag-of-words to achieve high performance, albeit not too far, as bag-of-n-grams seem to suffice.",
                "We first review related work for similar text classification problems such as e-mail priority ranking and speech act identification.",
                "Then we more formally define the action-item detection problem, discuss the aspects that distinguish it from more common problems like topic classification, and highlight the challenges in constructing systems that can perform well at the sentence and document level.",
                "From there, we move to a discussion of feature representation and selection techniques appropriate for this problem and how standard text classification approaches can be adapted to smoothly move from the sentence-level detection problem to the documentlevel classification problem.",
                "We then conduct an empirical analysis that helps us determine the effectiveness of our feature extraction procedures as well as establish baselines for a number of classification algorithms on this task.",
                "Finally, we summarize this papers contributions and consider interesting directions for future work. 2.",
                "RELATED WORK Several other researchers have considered very similar text classification tasks.",
                "Cohen et al. [5] describe an ontology of speech acts, such as Propose a Meeting, and attempt to predict when an e-mail contains one of these speech acts.",
                "We consider action-items to be an important specific type of speech act that falls within their more general classification.",
                "While they provide results for several classification methods, their methods only make use of human judgments at the document-level.",
                "In contrast, we consider whether accuracy can be increased by using finer-grained human judgments that mark the specific sentences and phrases of interest.",
                "Corston-Oliver et al. [6] consider detecting items in e-mail to Put on a To-Do List.",
                "This classification task is very similar to ours except they do not consider simple factual questions to belong to this category.",
                "We include questions, but note that not all questions are action-items - some are rhetorical or simply social convention, How are you?.",
                "From a learning perspective, while they make use of judgments at the sentence-level, they do not explicitly compare what if any benefits finer-grained judgments offer.",
                "Additionally, they do not study alternative choices or approaches to the classification task.",
                "Instead, they simply apply a standard SVM at the sentence-level and focus primarily on a linguistic analysis of how the sentence can be logically reformulated before adding it to the task list.",
                "In this study, we examine several alternative classification methods, compare document-level and sentence-level approaches and analyze the machine learning issues implicit in these problems.",
                "Interest in a variety of learning tasks related to e-mail has been rapidly growing in the recent literature.",
                "For example, in a forum dedicated to e-mail learning tasks, Culotta et al. [7] presented methods for learning social networks from e-mail.",
                "In this work, we do not focus on peer relationships; however, such methods could complement those here since peer relationships often influence word choice when requesting an action. 3.",
                "PROBLEM DEFINITION & APPROACH In contrast to previous work, we explicitly focus on the benefits that finer-grained, more costly, sentence-level human judgments offer over coarse-grained document-level judgments.",
                "Additionally, we consider multiple standard text classification approaches and analyze both the quantitative and qualitative differences that arise from taking a document-level vs. a sentence-level approach to classification.",
                "Finally, we focus on the representation necessary to achieve the most competitive performance. 3.1 Problem Definition In order to provide the most benefit to the user, a system would not only detect the document, but it would also indicate the specific sentences in the e-mail which contain the action-items.",
                "Therefore, there are three basic problems: 1.",
                "Document detection: Classify a document as to whether or not it contains an action-item. 2.",
                "Document ranking: Rank the documents such that all documents containing action-items occur as high as possible in the ranking. 3.",
                "Sentence detection: Classify each sentence in a document as to whether or not it is an action-item.",
                "As in most Information Retrieval tasks, the weight the evaluation metric should give to precision and recall depends on the nature of the application.",
                "In situations where a user will eventually read all received messages, ranking (e.g., via precision at recall of 1) may be most important since this will help encourage shorter delays in communications between users.",
                "In contrast, high-precision detection at low recall will be of increasing importance when the user is under severe time-pressure and therefore will likely not read all mail.",
                "This can be the case for crisis managers during disaster management.",
                "Finally, sentence detection plays a role in both timepressure situations and simply to alleviate the users required time to gist the message. 3.2 Approach As mentioned above, the labeled data can come in one of two forms: a document-labeling provides a yes/no label for each document as to whether it contains an action-item; a phrase-labeling provides only a yes label for the specific items of interest.",
                "We term the human judgments a phrase-labeling since the users view of the action-item may not correspond with actual sentence boundaries or predicted sentence boundaries.",
                "Obviously, it is straightforward to generate a document-labeling consistent with a phrase-labeling by labeling a document yes if and only if it contains at least one phrase labeled yes.",
                "To train classifiers for this task, we can take several viewpoints related to both the basic problems we have enumerated and the form of the labeled data.",
                "The document-level view treats each e-mail as a learning instance with an associated class-label.",
                "Then, the document can be converted to a feature-value vector and learning progresses as usual.",
                "Applying a document-level classifier to document detection and ranking is straightforward.",
                "In order to apply it to sentence detection, one must make additional steps.",
                "For example, if the classifier predicts a document contains an action-item, then areas of the document that contain a high-concentration of words which the model weights heavily in favor of action-items can be indicated.",
                "The obvious benefit of the document-level approach is that training set collection costs are lower since the user only has to specify whether or not an e-mail contains an action-item and not the specific sentences.",
                "In the sentence-level view, each e-mail is automatically segmented into sentences, and each sentence is treated as a learning instance with an associated class-label.",
                "Since the phrase-labeling provided by the user may not coincide with the automatic segmentation, we must determine what label to assign a partially overlapping sentence when converting it to a learning instance.",
                "Once trained, applying the resulting classifiers to sentence detection is now straightforward, but in order to apply the classifiers to document detection and document ranking, the individual predictions over each sentence must be aggregated in order to make a document-level prediction.",
                "This approach has the potential to benefit from morespecific labels that enable the learner to focus attention on the key sentences instead of having to learn based on data that the majority of the words in the e-mail provide no or little information about class membership. 3.2.1 Features Consider some of the phrases that might constitute part of an action item: would like to know, let me know, as soon as possible, have you.",
                "Each of these phrases consists of common words that occur in many e-mails.",
                "However, when they occur in the same sentence, they are far more indicative of an action-item.",
                "Additionally, order can be important: consider have you versus you have.",
                "Because of this, we posit that n-grams play a larger role in this problem than is typical of problems like topic classification.",
                "Therefore, we consider all n-grams up to size 4.",
                "When using n-grams, if we find an n-gram of size 4 in a segment of text, we can represent the text as just one occurrence of the ngram or as one occurrence of the n-gram and an occurrence of each smaller n-gram contained by it.",
                "We choose the second of these alternatives since this will allow the algorithm itself to smoothly back-off in terms of recall.",
                "Methods such as na¨ıve Bayes may be hurt by such a representation because of double-counting.",
                "Since sentence-ending punctuation can provide information, we retain the terminating punctuation token when it is identifiable.",
                "Additionally, we add a beginning-of-sentence and end-of-sentence token in order to capture patterns that are often indicators at the beginning or end of a sentence.",
                "Assuming proper punctuation, these extra tokens are unnecessary, but often e-mail lacks proper punctuation.",
                "In addition, for the sentence-level classifiers that use ngrams, we additionally code for each sentence a binary encoding of the position of the sentence relative to the document.",
                "This encoding has eight associated features that represent which octile (the first eighth, second eighth, etc.) contains the sentence. 3.2.2 Implementation Details In order to compare the document-level to the sentence-level approach, we compare predictions at the document-level.",
                "We do not address how to use a document-level classifier to make predictions at the sentence-level.",
                "In order to automatically segment the text of the e-mail, we use the RASP statistical parser [4].",
                "Since the automatically segmented sentences may not correspond directly with the phrase-level boundaries, we treat any sentence that contains at least 30% of a marked action-item segment as an action-item.",
                "When evaluating sentencedetection for the sentence-level system, we use these class labels as ground truth.",
                "Since we are not evaluating multiple segmentation approaches, this does not bias any of the methods.",
                "If multiple segmentation systems were under evaluation, one would need to use a metric that matched predicted positive sentences to phrases labeled positive.",
                "The metric would need to punish overly long true predictions as well as too short predictions.",
                "Our criteria for converting to labeled instances implicitly includes both criteria.",
                "Since the segmentation is fixed, an overly long prediction would be predicting yes for many no instances since presumably the extra length corresponds to additional segmented sentences all of which do not contain 30% of action-item.",
                "Likewise, a too short prediction must correspond to a small sentence included in the action-item but not constituting all of the action-item.",
                "Therefore, in order to consider the prediction to be too short, there will be an additional preceding/following sentence that is an action-item where we incorrectly predicted no.",
                "Once a sentence-level classifier has made a prediction for each sentence, we must combine these predictions to make both a document-level prediction and a document-level score.",
                "We use the simple policy of predicting positive when any of the sentences is predicted positive.",
                "In order to produce a document score for ranking, the confidence that the document contains an action-item is: ψ(d) = 1 n(d) s∈d|π(s)=1 ψ(s) if for any s ∈ d, π(s) = 1 1 n(d) maxs∈d ψ(s) o.w. where s is a sentence in document d, π is the classifiers 1/0 prediction, ψ is the score the classifier assigns as its confidence that π(s) = 1, and n(d) is the greater of 1 and the number of (unigram) tokens in the document.",
                "In other words, when any sentence is predicted positive, the document score is the length normalized sum of the sentence scores above threshold.",
                "When no sentence is predicted positive, the document score is the maximum sentence score normalized by length.",
                "As in other text problems, we are more likely to emit false positives for documents with more words or sentences.",
                "Thus we include a length normalization factor. 4.",
                "EXPERIMENTAL ANALYSIS 4.1 The Data Our corpus consists of e-mails obtained from volunteers at an educational institution and cover subjects such as: organizing a research workshop, arranging for job-candidate interviews, publishing proceedings, and talk announcements.",
                "The messages were anonymized by replacing the names of each individual and institution with a pseudonym.1 After attempting to identify and eliminate duplicate e-mails, the corpus contains 744 e-mail messages.",
                "After identity anonymization, the corpora has three basic versions.",
                "Quoted material refers to the text of a previous e-mail that an author often leaves in an e-mail message when responding to the e-mail.",
                "Quoted material can act as noise when learning since it may include action-items from previous messages that are no longer relevant.",
                "To isolate the effects of quoted material, we have three versions of the corpora.",
                "The raw form contains the basic messages.",
                "The auto-stripped version contains the messages after quoted material has been automatically removed.",
                "The hand-stripped version contains the messages after quoted material has been removed by a human.",
                "Additionally, the hand-stripped version has had any xml content and e-mail signatures removed - leaving only the essential content of the message.",
                "The studies reported here are performed with the hand-stripped version.",
                "This allows us to balance the cognitive load in terms of number of tokens that must be read in the user-studies we report - including quoted material would complicate the user studies since some users might skip the material while others read it.",
                "Additionally, ensuring all quoted material is removed 1 We have an even more highly anonymized version of the corpus that can be made available for some outside experimentation.",
                "Please contact the authors for more information on obtaining this data. prevents tainting the cross-validation since otherwise a test item could occur as quoted material in a training document. 4.1.1 Data Labeling Two human annotators labeled each message as to whether or not it contained an action-item.",
                "In addition, they identified each segment of the e-mail which contained an action-item.",
                "A segment is a contiguous section of text selected by the human annotators and may span several sentences or a complete phrase contained in a sentence.",
                "They were instructed that an action item is an explicit request for information that requires the recipients attention or a required action and told to highlight the phrases or sentences that make up the request.",
                "Annotator 1 No Yes Annotator 2 No 391 26 Yes 29 298 Table 1: Agreement of Human Annotators at Document Level Annotator One labeled 324 messages as containing action items.",
                "Annotator Two labeled 327 messages as containing action items.",
                "The agreement of the human annotators is shown in Tables 1 and 2.",
                "The annotators are said to agree at the document-level when both marked the same document as containing no action-items or both marked at least one action-item regardless of whether the text segments were the same.",
                "At the document-level, the annotators agreed 93% of the time.",
                "The kappa statistic [3, 5] is often used to evaluate inter-annotator agreement: κ = A − R 1 − R A is the empirical estimate of the probability of agreement.",
                "R is the empirical estimate of the probability of random agreement given the empirical class priors.",
                "A value close to −1 implies the annotators agree far less often than would be expected randomly, while a value close to 1 means they agree more often than randomly expected.",
                "At the document-level, the kappa statistic for inter-annotator agreement is 0.85.",
                "This value is both strong enough to expect the problem to be learnable and is comparable with results for similar tasks [5, 6].",
                "In order to determine the sentence-level agreement, we use each judgment to create a sentence-corpus with labels as described in Section 3.2.2, then consider the agreement over these sentences.",
                "This allows us to compare agreement over no judgments.",
                "We perform this comparison over the hand-stripped corpus since that eliminates spurious no judgments that would come from including quoted material, etc.",
                "Both annotators were free to label the subject as an action-item, but since neither did, we omit the subject line of the message as well.",
                "This only reduces the number of no agreements.",
                "This leaves 6301 automatically segmented sentences.",
                "At the sentence-level, the annotators agreed 98% of the time, and the kappa statistic for inter-annotator agreement is 0.82.",
                "In order to produce one single set of judgments, the human annotators went through each annotation where there was disagreement and came to a consensus opinion.",
                "The annotators did not collect statistics during this process but anecdotally reported that the majority of disagreements were either cases of clear annotator oversight or different interpretations of conditional statements.",
                "For example, If you would like to keep your job, come to tomorrows meeting implies a required action where If you would like to join Annotator 1 No Yes Annotator 2 No 5810 65 Yes 74 352 Table 2: Agreement of Human Annotators at Sentence Level the football betting pool, come to tomorrows meeting does not.",
                "The first would be an action-item in most contexts while the second would not.",
                "Of course, many conditional statements are not so clearly interpretable.",
                "After reconciling the judgments there are 416 e-mails with no action-items and 328 e-mails containing actionitems.",
                "Of the 328 e-mails containing action-items, 259 messages have one action-item segment; 55 messages have two action-item segments; 11 messages have three action-item segments.",
                "Two messages have four action-item segments, and one message has six action-item segments.",
                "Computing the sentence-level agreement using the reconciled gold standard judgments with each of the annotators individual judgments gives a kappa of 0.89 for Annotator One and a kappa of 0.92 for Annotator Two.",
                "In terms of message characteristics, there were on average 132 content tokens in the body after stripping.",
                "For action-item messages, there were 115.",
                "However, by examining Figure 2 we see the length distributions are nearly identical.",
                "As would be expected for e-mail, it is a long-tailed distribution with about half the messages having more than 60 tokens in the body (this paragraph has 65 tokens). 4.2 Classifiers For this experiment, we have selected a variety of standard text classification algorithms.",
                "In selecting algorithms, we have chosen algorithms that are not only known to work well but which differ along such lines as discriminative vs. generative and lazy vs. eager.",
                "We have done this in order to provide both a competitive and thorough sampling of learning methods for the task at hand.",
                "This is important since it is easy to improve a strawman classifier by introducing a new representation.",
                "By thoroughly sampling alternative classifier choices we demonstrate that representation improvements over bag-of-words are not due to using the information in the bag-of-words poorly. 4.2.1 kNN We employ a standard variant of the k-nearest neighbor algorithm used in text classification, kNN with s-cut score thresholding [19].",
                "We use a tfidf-weighting of the terms with a distanceweighted vote of the neighbors to compute the score before thresholding it.",
                "In order to choose the value of s for thresholding, we perform leave-one-out cross-validation over the training set.",
                "The value of k is set to be 2( log2 N + 1) where N is the number of training points.",
                "This rule for choosing k is theoretically motivated by results which show such a rule converges to the optimal classifier as the number of training points increases [8].",
                "In practice, we have also found it to be a computational convenience that frequently leads to comparable results with numerically optimizing k via a cross-validation procedure. 4.2.2 Na¨ıve Bayes We use a standard multinomial na¨ıve Bayes classifier [16].",
                "In using this classifier, we smoothed word and class probabilities using a Bayesian estimate (with the word prior) and a Laplace m-estimate, respectively. 0 20 40 60 80 100 120 140 160 0 200 400 600 800 1000 1200 1400 NumberofMessages Number of Tokens All Messages Action-Item Messages 0 0.02 0.04 0.06 0.08 0.1 0.12 0.14 0.16 0.18 0.2 0 200 400 600 800 1000 1200 1400 PercentageofMessages Number of Tokens All Messages Action-Item Messages Figure 2: The Histogram (left) and Distribution (right) of Message Length.",
                "A bin size of 20 words was used.",
                "Only tokens in the body after hand-stripping were counted.",
                "After stripping, the majority of words left are usually actual message content.",
                "Classifiers Document Unigram Document Ngram Sentence Unigram Sentence Ngram F1 kNN 0.6670 ± 0.0288 0.7108 ± 0.0699 0.7615 ± 0.0504 0.7790 ± 0.0460 na¨ıve Bayes 0.6572 ± 0.0749 0.6484 ± 0.0513 0.7715 ± 0.0597 0.7777 ± 0.0426 SVM 0.6904 ± 0.0347 0.7428 ± 0.0422 0.7282 ± 0.0698 0.7682 ± 0.0451 Voted Perceptron 0.6288 ± 0.0395 0.6774 ± 0.0422 0.6511 ± 0.0506 0.6798 ± 0.0913 Accuracy kNN 0.7029 ± 0.0659 0.7486 ± 0.0505 0.7972 ± 0.0435 0.8092 ± 0.0352 na¨ıve Bayes 0.6074 ± 0.0651 0.5816 ± 0.1075 0.7863 ± 0.0553 0.8145 ± 0.0268 SVM 0.7595 ± 0.0309 0.7904 ± 0.0349 0.7958 ± 0.0551 0.8173 ± 0.0258 Voted Perceptron 0.6531 ± 0.0390 0.7164 ± 0.0376 0.6413 ± 0.0833 0.7082 ± 0.1032 Table 3: Average Document-Detection Performance during Cross-Validation for Each Method and the Sample Standard Deviation (Sn−1) in italics.",
                "The best performance for each classifier is shown in bold. 4.2.3 SVM We have used a linear SVM with a tfidf feature representation and L2-norm as implemented in the SVMlight package v6.01 [11].",
                "All default settings were used. 4.2.4 Voted Perceptron Like the SVM, the Voted Perceptron is a kernel-based learning method.",
                "We use the same feature representation and kernel as we have for the SVM, a linear kernel with tfidf-weighting and an L2-norm.",
                "The voted perceptron is an online-learning method that keeps a history of past perceptrons used, as well as a weight signifying how often that perceptron was correct.",
                "With each new training example, a correct classification increases the weight on the current perceptron and an incorrect classification updates the perceptron.",
                "The output of the classifier uses the weights on the perceptra to make a final voted classification.",
                "When used in an offline-manner, multiple passes can be made through the training data.",
                "Both the voted perceptron and the SVM give a solution from the same hypothesis space - in this case, a linear classifier.",
                "Furthermore, it is well-known that the Voted Perceptron increases the margin of the solution after each pass through the training data [10].",
                "Since Cohen et al. [5] obtain worse results using an SVM than a Voted Perceptron with one training iteration, they conclude that the best solution for detecting speech acts may not lie in an area with a large margin.",
                "Because their tasks are highly similar to ours, we employ both classifiers to ensure we are not overlooking a competitive alternative classifier to the SVM for the basic bag-of-words representation. 4.3 Performance Measures To compare the performance of the classification methods, we look at two standard performance measures, F1 and accuracy.",
                "The F1 measure [18, 21] is the harmonic mean of precision and recall where Precision = Correct Positives Predicted Positives and Recall = Correct Positives Actual Positives . 4.4 Experimental Methodology We perform standard 10-fold cross-validation on the set of documents.",
                "For the sentence-level approach, all sentences in a document are either entirely in the training set or entirely in the test set for each fold.",
                "For significance tests, we use a two-tailed t-test [21] to compare the values obtained during each cross-validation fold with a p-value of 0.05.",
                "Feature selection was performed using the chi-squared statistic.",
                "Different levels of feature selection were considered for each classifier.",
                "Each of the following number of features was tried: 10, 25, 50, 100, 250, 750, 1000, 2000, 4000.",
                "There are approximately 4700 unigram tokens without feature selection.",
                "In order to choose the number of features to use for each classifier, we perform nested cross-validation and choose the settings that yield the optimal document-level F1 for that classifier.",
                "For this study, only the body of each e-mail message was used.",
                "Feature selection is always applied to all candidate features.",
                "That is, for the n-gram representation, the n-grams and position features are also subject to removal by the feature selection method. 4.5 Results The results for document-level classification are given in Table 3.",
                "The primary hypothesis we are concerned with is that n-grams are critical for this task; if this is true, we expect to see a significant gap in performance between the document-level classifiers that use n-grams (denoted Document Ngram) and those using only unigram features (denoted Document Unigram).",
                "Examining Table 3, we observe that this is indeed the case for every classifier except na¨ıve Bayes.",
                "This difference in performance produced by the n-gram representation is statistically significant for each classifier except for na¨ıve Bayes and the accuracy metric for kNN (see Table 4).",
                "Na¨ıve Bayes poor performance with the n-gram representation is not surprising since the bag-of-n-grams causes excessive doublecounting as mentioned in Section 3.2.1; however, na¨ıve Bayes is not hurt at the sentence-level because the sparse examples provide few chances for agglomerative effects of double counting.",
                "In either case, when a language-modeling approach is desired, modeling the n-grams directly would be preferable to na¨ıve Bayes.",
                "More importantly for the n-gram hypothesis, the n-grams lead to the best document-level classifier performance as well.",
                "As would be expected, the difference between the sentence-level n-gram representation and unigram representation is small.",
                "This is because the window of text is so small that the unigram representation, when done at the sentence-level, implicitly picks up on the power of the n-grams.",
                "Further improvement would signify that the order of the words matter even when only considering a small sentence-size window.",
                "Therefore, the finer-grained sentence-level judgments allows a unigram representation to succeed but only when performed in a small window - behaving as an n-gram representation for all practical purposes.",
                "Document Winner Sentence Winner kNN Ngram Ngram na¨ıve Bayes Unigram Ngram SVM Ngram† Ngram Voted Perceptron Ngram† Ngram Table 4: Significance results for n-grams versus unigrams for document detection using document-level and sentence-level classifiers.",
                "When the F1 result is statistically significant, it is shown in bold.",
                "When the accuracy result is significant, it is shown with a † .",
                "F1 Winner Accuracy Winner kNN Sentence Sentence na¨ıve Bayes Sentence Sentence SVM Sentence Sentence Voted Perceptron Sentence Document Table 5: Significance results for sentence-level classifiers vs. document-level classifiers for the document detection problem.",
                "When the result is statistically significant, it is shown in bold.",
                "Further highlighting the improvement from finer-grained judgments and n-grams, Figure 3 graphically depicts the edge the SVM sentence-level classifier has over the standard bag-of-words approach with a precision-recall curve.",
                "In the high precision area of the graph, the consistent edge of the sentence-level classifier is rather impressive - continuing at precision 1 out to 0.1 recall.",
                "This would mean that a tenth of the users action-items would be placed at the top of their action-item sorted inbox.",
                "Additionally, the large separation at the top right of the curves corresponds to the area where the optimal F1 occurs for each classifier, agreeing with the large improvement from 0.6904 to 0.7682 in F1 score.",
                "Considering the relative unexplored nature of classification at the sentence-level, this gives great hope for further increases in performance.",
                "Accuracy F1 Unigram Ngram Unigram Ngram kNN 0.9519 0.9536 0.6540 0.6686 na¨ıve Bayes 0.9419 0.9550 0.6176 0.6676 SVM 0.9559 0.9579 0.6271 0.6672 Voted Perceptron 0.8895 0.9247 0.3744 0.5164 Table 6: Performance of the Sentence-Level Classifiers at Sentence Detection Although Cohen et al. [5] observed that the Voted Perceptron with a single training iteration outperformed SVM in a set of similar tasks, we see no such behavior here.",
                "This further strengthens the evidence that an alternate classifier with the bag-of-words representation could not reach the same level of performance.",
                "The Voted Perceptron classifier does improve when the number of training iterations are increased, but it is still lower than the SVM classifier.",
                "Sentence detection results are presented in Table 6.",
                "With regard to the sentence detection problem, we note that the F1 measure gives a better feel for the remaining room for improvement in this difficult problem.",
                "That is, unlike document detection where actionitem documents are fairly common, action-item sentences are very rare.",
                "Thus, as in other text problems, the accuracy numbers are deceptively high sheerly because of the default accuracy attainable by always predicting no.",
                "Although, the results here are significantly above-random, it is unclear what level of performance is necessary for sentence detection to be useful in and of itself and not simply as a means to document ranking and classification.",
                "Figure 4: Users find action-items quicker when assisted by a classification system.",
                "Finally, when considering a new type of classification task, one of the most basic questions is whether an accurate classifier built for the task can have an impact on the end-user.",
                "In order to demonstrate the impact this task can have on e-mail users, we conducted a user study using an earlier less-accurate version of the sentence classifier - where instead of using just a single sentence, a threesentence windowed-approach was used.",
                "There were three distinct sets of e-mail in which users had to find action-items.",
                "These sets were either presented in a random order (Unordered), ordered by the classifier (Ordered), or ordered by the classifier and with the 0.4 0.5 0.6 0.7 0.8 0.9 1 0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1 Precision Recall Action-Item Detection SVM Performance (Post Model Selection) Document Unigram Sentence Ngram Figure 3: Both n-grams and a small prediction window lead to consistent improvements over the standard approach. center sentence in the highest confidence window highlighted (Order+help).",
                "In order to perform fair comparisons between conditions, the overall number of tokens in each message set should be approximately equal; that is, the cognitive reading load should be approximately the same before the classifiers reordering.",
                "Additionally, users typically show practice effects by improving at the overall task and thus performing better at later message sets.",
                "This is typically handled by varying the ordering of the sets across users so that the means are comparable.",
                "While omitting further detail, we note the sets were balanced for the total number of tokens and a latin square design was used to balance practice effects.",
                "Figure 4 shows that at intervals of 5, 10, and 15 minutes, users consistently found significantly more action-items when assisted by the classifier, but were most critically aided in the first five minutes.",
                "Although, the classifier consistently aids the users, we did not gain an additional end-user impact by highlighting.",
                "As mentioned above, this might be a result of the large room for improvement that still exists for sentence detection, but anecdotal evidence suggests this might also be a result of how the information is presented to the user rather than the accuracy of sentence detection.",
                "For example, highlighting the wrong sentence near an actual action-item hurts the users trust, but if a vague indicator (e.g., an arrow) points to the approximate area the user is not aware of the near-miss.",
                "Since the user studies used a three sentence window, we believe this played a role as well as sentence detection accuracy. 4.6 Discussion In contrast to problems where n-grams have yielded little difference, we believe their power here stems from the fact that many of the meaningful n-grams for action-items consist of common words, e.g., let me know.",
                "Therefore, the document-level unigram approach cannot gain much leverage, even when modeling their joint probability correctly, since these words will often co-occur in the document but not necessarily in a phrase.",
                "Additionally, action-item detection is distinct from many text classification tasks in that a single sentence can change the class label of the document.",
                "As a result, good classifiers cannot rely on aggregating evidence from a large number of weak indicators across the entire document.",
                "Even though we discarded the header information, examining the top-ranked features at the document-level reveals that many of the features are names or parts of e-mail addresses that occurred in the body and are highly associated with e-mails that tend to contain many or no action-items.",
                "A few examples are terms such as org, bob, and gov.",
                "We note that these features will be sensitive to the particular distribution (senders/receivers) and thus the document-level approach may produce classifiers that transfer less readily to alternate contexts and users at different institutions.",
                "This points out that part of the problem of going beyond bag-of-words may be the methodology, and investigating such properties as learning curves and how well a model transfers may highlight differences in models which appear to have similar performance when tested on the distributions they were trained on.",
                "We are currently investigating whether the sentence-level classifiers do perform better over different test corpora without retraining. 5.",
                "FUTURE WORK While applying text classifiers at the document-level is fairly well-understood, there exists the potential for significantly increasing the performance of the sentence-level classifiers.",
                "Such methods include alternate ways of combining the predictions over each sentence, weightings other than tfidf, which may not be appropriate since sentences are small, better sentence segmentation, and other types of phrasal analysis.",
                "Additionally, named entity tagging, time expressions, etc., seem likely candidates for features that can further improve this task.",
                "We are currently pursuing some of these avenues to see what additional gains these offer.",
                "Finally, it would be interesting to investigate the best methods for combining the document-level and sentence-level classifiers.",
                "Since the simple bag-of-words representation at the document-level leads to a learned model that behaves somewhat like a context-specific prior dependent on the sender/receiver and general topic, a first choice would be to treat it as such when combining probability estimates with the sentence-level classifier.",
                "Such a model might serve as a general example for other problems where bag-of-words can establish a baseline model but richer approaches are needed to achieve performance beyond that baseline. 6.",
                "SUMMARY AND CONCLUSIONS The effectiveness of sentence-level detection argues that labeling at the sentence-level provides significant value.",
                "Further experiments are needed to see how this interacts with the amount of training data available.",
                "Sentence detection that is then agglomerated to document-level detection works surprisingly better given low recall than would be expected with sentence-level items.",
                "This, in turn, indicates that improved sentence segmentation methods could yield further improvements in classification.",
                "In this work, we examined how action-items can be effectively detected in e-mails.",
                "Our empirical analysis has demonstrated that n-grams are of key importance to making the most of documentlevel judgments.",
                "When finer-grained judgments are available, then a standard bag-of-words approach using a small (sentence) window size and automatic segmentation techniques can produce results almost as good as the n-gram based approaches.",
                "Acknowledgments This material is based upon work supported by the Defense Advanced Research Projects Agency (DARPA) under Contract No.",
                "NBCHD030010.",
                "Any opinions, findings and conclusions or recommendations expressed in this material are those of the author(s) and do not necessarily reflect the views of the Defense Advanced Research Projects Agency (DARPA), or the Department of InteriorNational Business Center (DOI-NBC).",
                "We would like to extend our sincerest thanks to Jill Lehman whose efforts in data collection were essential in constructing the corpus, and both Jill and Aaron Steinfeld for their direction of the HCI experiments.",
                "We would also like to thank Django Wexler for constructing and supporting the corpus labeling tools and Curtis Huttenhowers support of the text preprocessing package.",
                "Finally, we gratefully acknowledge Scott Fahlman for his encouragement and useful discussions on this topic. 7.",
                "REFERENCES [1] J. Allan, J. Carbonell, G. Doddington, J. Yamron, and Y. Yang.",
                "Topic detection and tracking pilot study: Final report.",
                "In Proceedings of the DARPA Broadcast News Transcription and Understanding Workshop, Washington, D.C., 1998. [2] C. Apte, F. Damerau, and S. M. Weiss.",
                "Automated learning of decision rules for text categorization.",
                "ACM Transactions on Information Systems, 12(3):233-251, July 1994. [3] J. Carletta.",
                "Assessing agreement on classification tasks: The kappa statistic.",
                "Computational Linguistics, 22(2):249-254, 1996. [4] J. Carroll.",
                "High precision extraction of grammatical relations.",
                "In Proceedings of the 19th International Conference on Computational Linguistics (COLING), pages 134-140, 2002. [5] W. W. Cohen, V. R. Carvalho, and T. M. Mitchell.",
                "Learning to classify email into speech acts.",
                "In EMNLP-2004 (Conference on Empirical Methods in Natural Language Processing), pages 309-316, 2004. [6] S. Corston-Oliver, E. Ringger, M. Gamon, and R. Campbell.",
                "Task-focused summarization of email.",
                "In Text Summarization Branches Out: Proceedings of the ACL-04 Workshop, pages 43-50, 2004. [7] A. Culotta, R. Bekkerman, and A. McCallum.",
                "Extracting social networks and contact information from email and the web.",
                "In CEAS-2004 (Conference on Email and Anti-Spam), Mountain View, CA, July 2004. [8] L. Devroye, L. Gy¨orfi, and G. Lugosi.",
                "A Probabilistic Theory of Pattern Recognition.",
                "Springer-Verlag, New York, NY, 1996. [9] S. T. Dumais, J. Platt, D. Heckerman, and M. Sahami.",
                "Inductive learning algorithms and representations for text categorization.",
                "In CIKM 98, Proceedings of the 7th ACM Conference on Information and Knowledge Management, pages 148-155, 1998. [10] Y. Freund and R. Schapire.",
                "Large margin classification using the perceptron algorithm.",
                "Machine Learning, 37(3):277-296, 1999. [11] T. Joachims.",
                "Making large-scale svm learning practical.",
                "In B. Sch¨olkopf, C. J. Burges, and A. J. Smola, editors, Advances in Kernel Methods - Support Vector Learning, pages 41-56.",
                "MIT Press, 1999. [12] L. S. Larkey.",
                "A patent search and classification system.",
                "In Proceedings of the Fourth ACM Conference on Digital Libraries, pages 179 - 187, 1999. [13] D. D. Lewis.",
                "An evaluation of phrasal and clustered representations on a text categorization task.",
                "In SIGIR 92, Proceedings of the 15th Annual International ACM Conference on Research and Development in Information Retrieval, pages 37-50, 1992. [14] Y. Liu, J. Carbonell, and R. Jin.",
                "A pairwise ensemble approach for accurate genre classification.",
                "In Proceedings of the European Conference on Machine Learning (ECML), 2003. [15] Y. Liu, R. Yan, R. Jin, and J. Carbonell.",
                "A comparison study of kernels for multi-label text classification using category association.",
                "In The Twenty-first International Conference on Machine Learning (ICML), 2004. [16] A. McCallum and K. Nigam.",
                "A comparison of event models for naive bayes text classification.",
                "In Working Notes of AAAI 98 (The 15th National Conference on Artificial Intelligence), Workshop on Learning for Text Categorization, pages 41-48, 1998.",
                "TR WS-98-05. [17] F. Sebastiani.",
                "Machine learning in automated text categorization.",
                "ACM Computing Surveys, 34(1):1-47, March 2002. [18] C. J. van Rijsbergen.",
                "Information Retrieval.",
                "Butterworths, London, 1979. [19] Y. Yang.",
                "An evaluation of statistical approaches to text categorization.",
                "Information Retrieval, 1(1/2):67-88, 1999. [20] Y. Yang, J. Carbonell, R. Brown, T. Pierce, B. T. Archibald, and X. Liu.",
                "Learning approaches to topic detection and tracking.",
                "IEEE EXPERT, Special Issue on Applications of Intelligent Information Retrieval, 1999. [21] Y. Yang and X. Liu.",
                "A re-examination of text categorization methods.",
                "In SIGIR 99, Proceedings of the 22nd Annual International ACM Conference on Research and Development in Information Retrieval, pages 42-49, 1999. [22] Y. Yang, J. Zhang, J. Carbonell, and C. Jin.",
                "Topic-conditioned novelty detection.",
                "In Proceedings of the ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, July 2002."
            ],
            "original_annotated_samples": [],
            "translated_annotated_samples": [
                "Sin embargo, el uso de conjuntos de características enriquecidos, como N-grams (hasta n = 4) con selección de características chi cuadrado, y las señales contextuales para la ubicación de acción de acción mejoran el rendimiento hasta un 10% sobre unigramas, utilizando en ambos casos de estado deLos clasificadores de ART, como SVM con selección automatizada del modelo a través de \"validación cruzada integrada\"."
            ],
            "translated_text": "",
            "candidates": [
                "validación cruzada",
                "validación cruzada integrada"
            ],
            "error": []
        },
        "text categorization": {
            "translated_key": "categorización de texto",
            "is_in_text": true,
            "original_annotated_sentences": [
                "Feature Representation for Effective Action-Item Detection Paul N. Bennett Computer Science Department Carnegie Mellon University Pittsburgh, PA 15213 pbennett+@cs.cmu.edu Jaime Carbonell Language Technologies Institute.",
                "Carnegie Mellon University Pittsburgh, PA 15213 jgc+@cs.cmu.edu ABSTRACT E-mail users face an ever-growing challenge in managing their inboxes due to the growing centrality of email in the workplace for task assignment, action requests, and other roles beyond information dissemination.",
                "Whereas Information Retrieval and Machine Learning techniques are gaining initial acceptance in spam filtering and automated folder assignment, this paper reports on a new task: automated action-item detection, in order to flag emails that require responses, and to highlight the specific passage(s) indicating the request(s) for action.",
                "Unlike standard topic-driven text classification, action-item detection requires inferring the senders intent, and as such responds less well to pure bag-of-words classification.",
                "However, using enriched feature sets, such as n-grams (up to n=4) with chi-squared feature selection, and contextual cues for action-item location improve performance by up to 10% over unigrams, using in both cases state of the art classifiers such as SVMs with automated model selection via embedded cross-validation.",
                "Categories and Subject Descriptors H.3.3 [Information Storage and Retrieval]: Information Search and Retrieval; I.2.6 [Artificial Intelligence]: Learning; I.5.4 [Pattern Recognition]: Applications General Terms Experimentation 1.",
                "INTRODUCTION E-mail users are facing an increasingly difficult task of managing their inboxes in the face of mounting challenges that result from rising e-mail usage.",
                "This includes prioritizing e-mails over a range of sources from business partners to family members, filtering and reducing junk e-mail, and quickly managing requests that demand From: Henry Hutchins <hhutchins@innovative.company.com> To: Sara Smith; Joe Johnson; William Woolings Subject: meeting with prospective customers Sent: Fri 12/10/2005 8:08 AM Hi All, Id like to remind all of you that the group from GRTY will be visiting us next Friday at 4:30 p.m.",
                "The current schedule looks like this: + 9:30 a.m.",
                "Informal Breakfast and Discussion in Cafeteria + 10:30 a.m. Company Overview + 11:00 a.m.",
                "Individual Meetings (Continue Over Lunch) + 2:00 p.m. Tour of Facilities + 3:00 p.m.",
                "Sales Pitch In order to have this go off smoothly, I would like to practice the presentation well in advance.",
                "As a result, I will need each of your parts by Wednesday.",
                "Keep up the good work! -Henry Figure 1: An E-mail with emphasized Action-Item, an explicit request that requires the recipients attention or action. the receivers attention or action.",
                "Automated action-item detection targets the third of these problems by attempting to detect which e-mails require an action or response with information, and within those e-mails, attempting to highlight the sentence (or other passage length) that directly indicates the action request.",
                "Such a detection system can be used as one part of an e-mail agent which would assist a user in processing important e-mails quicker than would have been possible without the agent.",
                "We view action-item detection as one necessary component of a successful e-mail agent which would perform spam detection, action-item detection, topic classification and priority ranking, among other functions.",
                "The utility of such a detector can manifest as a method of prioritizing e-mails according to task-oriented criteria other than the standard ones of topic and sender or as a means of ensuring that the email user hasnt dropped the proverbial ball by forgetting to address an action request.",
                "Action-item detection differs from standard text classification in two important ways.",
                "First, the user is interested both in detecting whether an email contains action items and in locating exactly where these action item requests are contained within the email body.",
                "In contrast, standard <br>text categorization</br> merely assigns a topic label to each text, whether that label corresponds to an e-mail folder or a controlled indexing vocabulary [12, 15, 22].",
                "Second, action-item detection attempts to recover the email senders intent - whether she means to elicit response or action on the part of the receiver; note that for this task, classifiers using only unigrams as features do not perform optimally, as evidenced in our results below.",
                "Instead we find that we need more information-laden features such as higher-order n-grams.",
                "<br>text categorization</br> by topic, on the other hand, works very well using just individual words as features [2, 9, 13, 17].",
                "In fact, genre-classification, which one would think may require more than a bag-of-words approach, also works quite well using just unigram features [14].",
                "Topic detection and tracking (TDT), also works well with unigram feature sets [1, 20].",
                "We believe that action-item detection is one of the first clear instances of an IR-related task where we must move beyond bag-of-words to achieve high performance, albeit not too far, as bag-of-n-grams seem to suffice.",
                "We first review related work for similar text classification problems such as e-mail priority ranking and speech act identification.",
                "Then we more formally define the action-item detection problem, discuss the aspects that distinguish it from more common problems like topic classification, and highlight the challenges in constructing systems that can perform well at the sentence and document level.",
                "From there, we move to a discussion of feature representation and selection techniques appropriate for this problem and how standard text classification approaches can be adapted to smoothly move from the sentence-level detection problem to the documentlevel classification problem.",
                "We then conduct an empirical analysis that helps us determine the effectiveness of our feature extraction procedures as well as establish baselines for a number of classification algorithms on this task.",
                "Finally, we summarize this papers contributions and consider interesting directions for future work. 2.",
                "RELATED WORK Several other researchers have considered very similar text classification tasks.",
                "Cohen et al. [5] describe an ontology of speech acts, such as Propose a Meeting, and attempt to predict when an e-mail contains one of these speech acts.",
                "We consider action-items to be an important specific type of speech act that falls within their more general classification.",
                "While they provide results for several classification methods, their methods only make use of human judgments at the document-level.",
                "In contrast, we consider whether accuracy can be increased by using finer-grained human judgments that mark the specific sentences and phrases of interest.",
                "Corston-Oliver et al. [6] consider detecting items in e-mail to Put on a To-Do List.",
                "This classification task is very similar to ours except they do not consider simple factual questions to belong to this category.",
                "We include questions, but note that not all questions are action-items - some are rhetorical or simply social convention, How are you?.",
                "From a learning perspective, while they make use of judgments at the sentence-level, they do not explicitly compare what if any benefits finer-grained judgments offer.",
                "Additionally, they do not study alternative choices or approaches to the classification task.",
                "Instead, they simply apply a standard SVM at the sentence-level and focus primarily on a linguistic analysis of how the sentence can be logically reformulated before adding it to the task list.",
                "In this study, we examine several alternative classification methods, compare document-level and sentence-level approaches and analyze the machine learning issues implicit in these problems.",
                "Interest in a variety of learning tasks related to e-mail has been rapidly growing in the recent literature.",
                "For example, in a forum dedicated to e-mail learning tasks, Culotta et al. [7] presented methods for learning social networks from e-mail.",
                "In this work, we do not focus on peer relationships; however, such methods could complement those here since peer relationships often influence word choice when requesting an action. 3.",
                "PROBLEM DEFINITION & APPROACH In contrast to previous work, we explicitly focus on the benefits that finer-grained, more costly, sentence-level human judgments offer over coarse-grained document-level judgments.",
                "Additionally, we consider multiple standard text classification approaches and analyze both the quantitative and qualitative differences that arise from taking a document-level vs. a sentence-level approach to classification.",
                "Finally, we focus on the representation necessary to achieve the most competitive performance. 3.1 Problem Definition In order to provide the most benefit to the user, a system would not only detect the document, but it would also indicate the specific sentences in the e-mail which contain the action-items.",
                "Therefore, there are three basic problems: 1.",
                "Document detection: Classify a document as to whether or not it contains an action-item. 2.",
                "Document ranking: Rank the documents such that all documents containing action-items occur as high as possible in the ranking. 3.",
                "Sentence detection: Classify each sentence in a document as to whether or not it is an action-item.",
                "As in most Information Retrieval tasks, the weight the evaluation metric should give to precision and recall depends on the nature of the application.",
                "In situations where a user will eventually read all received messages, ranking (e.g., via precision at recall of 1) may be most important since this will help encourage shorter delays in communications between users.",
                "In contrast, high-precision detection at low recall will be of increasing importance when the user is under severe time-pressure and therefore will likely not read all mail.",
                "This can be the case for crisis managers during disaster management.",
                "Finally, sentence detection plays a role in both timepressure situations and simply to alleviate the users required time to gist the message. 3.2 Approach As mentioned above, the labeled data can come in one of two forms: a document-labeling provides a yes/no label for each document as to whether it contains an action-item; a phrase-labeling provides only a yes label for the specific items of interest.",
                "We term the human judgments a phrase-labeling since the users view of the action-item may not correspond with actual sentence boundaries or predicted sentence boundaries.",
                "Obviously, it is straightforward to generate a document-labeling consistent with a phrase-labeling by labeling a document yes if and only if it contains at least one phrase labeled yes.",
                "To train classifiers for this task, we can take several viewpoints related to both the basic problems we have enumerated and the form of the labeled data.",
                "The document-level view treats each e-mail as a learning instance with an associated class-label.",
                "Then, the document can be converted to a feature-value vector and learning progresses as usual.",
                "Applying a document-level classifier to document detection and ranking is straightforward.",
                "In order to apply it to sentence detection, one must make additional steps.",
                "For example, if the classifier predicts a document contains an action-item, then areas of the document that contain a high-concentration of words which the model weights heavily in favor of action-items can be indicated.",
                "The obvious benefit of the document-level approach is that training set collection costs are lower since the user only has to specify whether or not an e-mail contains an action-item and not the specific sentences.",
                "In the sentence-level view, each e-mail is automatically segmented into sentences, and each sentence is treated as a learning instance with an associated class-label.",
                "Since the phrase-labeling provided by the user may not coincide with the automatic segmentation, we must determine what label to assign a partially overlapping sentence when converting it to a learning instance.",
                "Once trained, applying the resulting classifiers to sentence detection is now straightforward, but in order to apply the classifiers to document detection and document ranking, the individual predictions over each sentence must be aggregated in order to make a document-level prediction.",
                "This approach has the potential to benefit from morespecific labels that enable the learner to focus attention on the key sentences instead of having to learn based on data that the majority of the words in the e-mail provide no or little information about class membership. 3.2.1 Features Consider some of the phrases that might constitute part of an action item: would like to know, let me know, as soon as possible, have you.",
                "Each of these phrases consists of common words that occur in many e-mails.",
                "However, when they occur in the same sentence, they are far more indicative of an action-item.",
                "Additionally, order can be important: consider have you versus you have.",
                "Because of this, we posit that n-grams play a larger role in this problem than is typical of problems like topic classification.",
                "Therefore, we consider all n-grams up to size 4.",
                "When using n-grams, if we find an n-gram of size 4 in a segment of text, we can represent the text as just one occurrence of the ngram or as one occurrence of the n-gram and an occurrence of each smaller n-gram contained by it.",
                "We choose the second of these alternatives since this will allow the algorithm itself to smoothly back-off in terms of recall.",
                "Methods such as na¨ıve Bayes may be hurt by such a representation because of double-counting.",
                "Since sentence-ending punctuation can provide information, we retain the terminating punctuation token when it is identifiable.",
                "Additionally, we add a beginning-of-sentence and end-of-sentence token in order to capture patterns that are often indicators at the beginning or end of a sentence.",
                "Assuming proper punctuation, these extra tokens are unnecessary, but often e-mail lacks proper punctuation.",
                "In addition, for the sentence-level classifiers that use ngrams, we additionally code for each sentence a binary encoding of the position of the sentence relative to the document.",
                "This encoding has eight associated features that represent which octile (the first eighth, second eighth, etc.) contains the sentence. 3.2.2 Implementation Details In order to compare the document-level to the sentence-level approach, we compare predictions at the document-level.",
                "We do not address how to use a document-level classifier to make predictions at the sentence-level.",
                "In order to automatically segment the text of the e-mail, we use the RASP statistical parser [4].",
                "Since the automatically segmented sentences may not correspond directly with the phrase-level boundaries, we treat any sentence that contains at least 30% of a marked action-item segment as an action-item.",
                "When evaluating sentencedetection for the sentence-level system, we use these class labels as ground truth.",
                "Since we are not evaluating multiple segmentation approaches, this does not bias any of the methods.",
                "If multiple segmentation systems were under evaluation, one would need to use a metric that matched predicted positive sentences to phrases labeled positive.",
                "The metric would need to punish overly long true predictions as well as too short predictions.",
                "Our criteria for converting to labeled instances implicitly includes both criteria.",
                "Since the segmentation is fixed, an overly long prediction would be predicting yes for many no instances since presumably the extra length corresponds to additional segmented sentences all of which do not contain 30% of action-item.",
                "Likewise, a too short prediction must correspond to a small sentence included in the action-item but not constituting all of the action-item.",
                "Therefore, in order to consider the prediction to be too short, there will be an additional preceding/following sentence that is an action-item where we incorrectly predicted no.",
                "Once a sentence-level classifier has made a prediction for each sentence, we must combine these predictions to make both a document-level prediction and a document-level score.",
                "We use the simple policy of predicting positive when any of the sentences is predicted positive.",
                "In order to produce a document score for ranking, the confidence that the document contains an action-item is: ψ(d) = 1 n(d) s∈d|π(s)=1 ψ(s) if for any s ∈ d, π(s) = 1 1 n(d) maxs∈d ψ(s) o.w. where s is a sentence in document d, π is the classifiers 1/0 prediction, ψ is the score the classifier assigns as its confidence that π(s) = 1, and n(d) is the greater of 1 and the number of (unigram) tokens in the document.",
                "In other words, when any sentence is predicted positive, the document score is the length normalized sum of the sentence scores above threshold.",
                "When no sentence is predicted positive, the document score is the maximum sentence score normalized by length.",
                "As in other text problems, we are more likely to emit false positives for documents with more words or sentences.",
                "Thus we include a length normalization factor. 4.",
                "EXPERIMENTAL ANALYSIS 4.1 The Data Our corpus consists of e-mails obtained from volunteers at an educational institution and cover subjects such as: organizing a research workshop, arranging for job-candidate interviews, publishing proceedings, and talk announcements.",
                "The messages were anonymized by replacing the names of each individual and institution with a pseudonym.1 After attempting to identify and eliminate duplicate e-mails, the corpus contains 744 e-mail messages.",
                "After identity anonymization, the corpora has three basic versions.",
                "Quoted material refers to the text of a previous e-mail that an author often leaves in an e-mail message when responding to the e-mail.",
                "Quoted material can act as noise when learning since it may include action-items from previous messages that are no longer relevant.",
                "To isolate the effects of quoted material, we have three versions of the corpora.",
                "The raw form contains the basic messages.",
                "The auto-stripped version contains the messages after quoted material has been automatically removed.",
                "The hand-stripped version contains the messages after quoted material has been removed by a human.",
                "Additionally, the hand-stripped version has had any xml content and e-mail signatures removed - leaving only the essential content of the message.",
                "The studies reported here are performed with the hand-stripped version.",
                "This allows us to balance the cognitive load in terms of number of tokens that must be read in the user-studies we report - including quoted material would complicate the user studies since some users might skip the material while others read it.",
                "Additionally, ensuring all quoted material is removed 1 We have an even more highly anonymized version of the corpus that can be made available for some outside experimentation.",
                "Please contact the authors for more information on obtaining this data. prevents tainting the cross-validation since otherwise a test item could occur as quoted material in a training document. 4.1.1 Data Labeling Two human annotators labeled each message as to whether or not it contained an action-item.",
                "In addition, they identified each segment of the e-mail which contained an action-item.",
                "A segment is a contiguous section of text selected by the human annotators and may span several sentences or a complete phrase contained in a sentence.",
                "They were instructed that an action item is an explicit request for information that requires the recipients attention or a required action and told to highlight the phrases or sentences that make up the request.",
                "Annotator 1 No Yes Annotator 2 No 391 26 Yes 29 298 Table 1: Agreement of Human Annotators at Document Level Annotator One labeled 324 messages as containing action items.",
                "Annotator Two labeled 327 messages as containing action items.",
                "The agreement of the human annotators is shown in Tables 1 and 2.",
                "The annotators are said to agree at the document-level when both marked the same document as containing no action-items or both marked at least one action-item regardless of whether the text segments were the same.",
                "At the document-level, the annotators agreed 93% of the time.",
                "The kappa statistic [3, 5] is often used to evaluate inter-annotator agreement: κ = A − R 1 − R A is the empirical estimate of the probability of agreement.",
                "R is the empirical estimate of the probability of random agreement given the empirical class priors.",
                "A value close to −1 implies the annotators agree far less often than would be expected randomly, while a value close to 1 means they agree more often than randomly expected.",
                "At the document-level, the kappa statistic for inter-annotator agreement is 0.85.",
                "This value is both strong enough to expect the problem to be learnable and is comparable with results for similar tasks [5, 6].",
                "In order to determine the sentence-level agreement, we use each judgment to create a sentence-corpus with labels as described in Section 3.2.2, then consider the agreement over these sentences.",
                "This allows us to compare agreement over no judgments.",
                "We perform this comparison over the hand-stripped corpus since that eliminates spurious no judgments that would come from including quoted material, etc.",
                "Both annotators were free to label the subject as an action-item, but since neither did, we omit the subject line of the message as well.",
                "This only reduces the number of no agreements.",
                "This leaves 6301 automatically segmented sentences.",
                "At the sentence-level, the annotators agreed 98% of the time, and the kappa statistic for inter-annotator agreement is 0.82.",
                "In order to produce one single set of judgments, the human annotators went through each annotation where there was disagreement and came to a consensus opinion.",
                "The annotators did not collect statistics during this process but anecdotally reported that the majority of disagreements were either cases of clear annotator oversight or different interpretations of conditional statements.",
                "For example, If you would like to keep your job, come to tomorrows meeting implies a required action where If you would like to join Annotator 1 No Yes Annotator 2 No 5810 65 Yes 74 352 Table 2: Agreement of Human Annotators at Sentence Level the football betting pool, come to tomorrows meeting does not.",
                "The first would be an action-item in most contexts while the second would not.",
                "Of course, many conditional statements are not so clearly interpretable.",
                "After reconciling the judgments there are 416 e-mails with no action-items and 328 e-mails containing actionitems.",
                "Of the 328 e-mails containing action-items, 259 messages have one action-item segment; 55 messages have two action-item segments; 11 messages have three action-item segments.",
                "Two messages have four action-item segments, and one message has six action-item segments.",
                "Computing the sentence-level agreement using the reconciled gold standard judgments with each of the annotators individual judgments gives a kappa of 0.89 for Annotator One and a kappa of 0.92 for Annotator Two.",
                "In terms of message characteristics, there were on average 132 content tokens in the body after stripping.",
                "For action-item messages, there were 115.",
                "However, by examining Figure 2 we see the length distributions are nearly identical.",
                "As would be expected for e-mail, it is a long-tailed distribution with about half the messages having more than 60 tokens in the body (this paragraph has 65 tokens). 4.2 Classifiers For this experiment, we have selected a variety of standard text classification algorithms.",
                "In selecting algorithms, we have chosen algorithms that are not only known to work well but which differ along such lines as discriminative vs. generative and lazy vs. eager.",
                "We have done this in order to provide both a competitive and thorough sampling of learning methods for the task at hand.",
                "This is important since it is easy to improve a strawman classifier by introducing a new representation.",
                "By thoroughly sampling alternative classifier choices we demonstrate that representation improvements over bag-of-words are not due to using the information in the bag-of-words poorly. 4.2.1 kNN We employ a standard variant of the k-nearest neighbor algorithm used in text classification, kNN with s-cut score thresholding [19].",
                "We use a tfidf-weighting of the terms with a distanceweighted vote of the neighbors to compute the score before thresholding it.",
                "In order to choose the value of s for thresholding, we perform leave-one-out cross-validation over the training set.",
                "The value of k is set to be 2( log2 N + 1) where N is the number of training points.",
                "This rule for choosing k is theoretically motivated by results which show such a rule converges to the optimal classifier as the number of training points increases [8].",
                "In practice, we have also found it to be a computational convenience that frequently leads to comparable results with numerically optimizing k via a cross-validation procedure. 4.2.2 Na¨ıve Bayes We use a standard multinomial na¨ıve Bayes classifier [16].",
                "In using this classifier, we smoothed word and class probabilities using a Bayesian estimate (with the word prior) and a Laplace m-estimate, respectively. 0 20 40 60 80 100 120 140 160 0 200 400 600 800 1000 1200 1400 NumberofMessages Number of Tokens All Messages Action-Item Messages 0 0.02 0.04 0.06 0.08 0.1 0.12 0.14 0.16 0.18 0.2 0 200 400 600 800 1000 1200 1400 PercentageofMessages Number of Tokens All Messages Action-Item Messages Figure 2: The Histogram (left) and Distribution (right) of Message Length.",
                "A bin size of 20 words was used.",
                "Only tokens in the body after hand-stripping were counted.",
                "After stripping, the majority of words left are usually actual message content.",
                "Classifiers Document Unigram Document Ngram Sentence Unigram Sentence Ngram F1 kNN 0.6670 ± 0.0288 0.7108 ± 0.0699 0.7615 ± 0.0504 0.7790 ± 0.0460 na¨ıve Bayes 0.6572 ± 0.0749 0.6484 ± 0.0513 0.7715 ± 0.0597 0.7777 ± 0.0426 SVM 0.6904 ± 0.0347 0.7428 ± 0.0422 0.7282 ± 0.0698 0.7682 ± 0.0451 Voted Perceptron 0.6288 ± 0.0395 0.6774 ± 0.0422 0.6511 ± 0.0506 0.6798 ± 0.0913 Accuracy kNN 0.7029 ± 0.0659 0.7486 ± 0.0505 0.7972 ± 0.0435 0.8092 ± 0.0352 na¨ıve Bayes 0.6074 ± 0.0651 0.5816 ± 0.1075 0.7863 ± 0.0553 0.8145 ± 0.0268 SVM 0.7595 ± 0.0309 0.7904 ± 0.0349 0.7958 ± 0.0551 0.8173 ± 0.0258 Voted Perceptron 0.6531 ± 0.0390 0.7164 ± 0.0376 0.6413 ± 0.0833 0.7082 ± 0.1032 Table 3: Average Document-Detection Performance during Cross-Validation for Each Method and the Sample Standard Deviation (Sn−1) in italics.",
                "The best performance for each classifier is shown in bold. 4.2.3 SVM We have used a linear SVM with a tfidf feature representation and L2-norm as implemented in the SVMlight package v6.01 [11].",
                "All default settings were used. 4.2.4 Voted Perceptron Like the SVM, the Voted Perceptron is a kernel-based learning method.",
                "We use the same feature representation and kernel as we have for the SVM, a linear kernel with tfidf-weighting and an L2-norm.",
                "The voted perceptron is an online-learning method that keeps a history of past perceptrons used, as well as a weight signifying how often that perceptron was correct.",
                "With each new training example, a correct classification increases the weight on the current perceptron and an incorrect classification updates the perceptron.",
                "The output of the classifier uses the weights on the perceptra to make a final voted classification.",
                "When used in an offline-manner, multiple passes can be made through the training data.",
                "Both the voted perceptron and the SVM give a solution from the same hypothesis space - in this case, a linear classifier.",
                "Furthermore, it is well-known that the Voted Perceptron increases the margin of the solution after each pass through the training data [10].",
                "Since Cohen et al. [5] obtain worse results using an SVM than a Voted Perceptron with one training iteration, they conclude that the best solution for detecting speech acts may not lie in an area with a large margin.",
                "Because their tasks are highly similar to ours, we employ both classifiers to ensure we are not overlooking a competitive alternative classifier to the SVM for the basic bag-of-words representation. 4.3 Performance Measures To compare the performance of the classification methods, we look at two standard performance measures, F1 and accuracy.",
                "The F1 measure [18, 21] is the harmonic mean of precision and recall where Precision = Correct Positives Predicted Positives and Recall = Correct Positives Actual Positives . 4.4 Experimental Methodology We perform standard 10-fold cross-validation on the set of documents.",
                "For the sentence-level approach, all sentences in a document are either entirely in the training set or entirely in the test set for each fold.",
                "For significance tests, we use a two-tailed t-test [21] to compare the values obtained during each cross-validation fold with a p-value of 0.05.",
                "Feature selection was performed using the chi-squared statistic.",
                "Different levels of feature selection were considered for each classifier.",
                "Each of the following number of features was tried: 10, 25, 50, 100, 250, 750, 1000, 2000, 4000.",
                "There are approximately 4700 unigram tokens without feature selection.",
                "In order to choose the number of features to use for each classifier, we perform nested cross-validation and choose the settings that yield the optimal document-level F1 for that classifier.",
                "For this study, only the body of each e-mail message was used.",
                "Feature selection is always applied to all candidate features.",
                "That is, for the n-gram representation, the n-grams and position features are also subject to removal by the feature selection method. 4.5 Results The results for document-level classification are given in Table 3.",
                "The primary hypothesis we are concerned with is that n-grams are critical for this task; if this is true, we expect to see a significant gap in performance between the document-level classifiers that use n-grams (denoted Document Ngram) and those using only unigram features (denoted Document Unigram).",
                "Examining Table 3, we observe that this is indeed the case for every classifier except na¨ıve Bayes.",
                "This difference in performance produced by the n-gram representation is statistically significant for each classifier except for na¨ıve Bayes and the accuracy metric for kNN (see Table 4).",
                "Na¨ıve Bayes poor performance with the n-gram representation is not surprising since the bag-of-n-grams causes excessive doublecounting as mentioned in Section 3.2.1; however, na¨ıve Bayes is not hurt at the sentence-level because the sparse examples provide few chances for agglomerative effects of double counting.",
                "In either case, when a language-modeling approach is desired, modeling the n-grams directly would be preferable to na¨ıve Bayes.",
                "More importantly for the n-gram hypothesis, the n-grams lead to the best document-level classifier performance as well.",
                "As would be expected, the difference between the sentence-level n-gram representation and unigram representation is small.",
                "This is because the window of text is so small that the unigram representation, when done at the sentence-level, implicitly picks up on the power of the n-grams.",
                "Further improvement would signify that the order of the words matter even when only considering a small sentence-size window.",
                "Therefore, the finer-grained sentence-level judgments allows a unigram representation to succeed but only when performed in a small window - behaving as an n-gram representation for all practical purposes.",
                "Document Winner Sentence Winner kNN Ngram Ngram na¨ıve Bayes Unigram Ngram SVM Ngram† Ngram Voted Perceptron Ngram† Ngram Table 4: Significance results for n-grams versus unigrams for document detection using document-level and sentence-level classifiers.",
                "When the F1 result is statistically significant, it is shown in bold.",
                "When the accuracy result is significant, it is shown with a † .",
                "F1 Winner Accuracy Winner kNN Sentence Sentence na¨ıve Bayes Sentence Sentence SVM Sentence Sentence Voted Perceptron Sentence Document Table 5: Significance results for sentence-level classifiers vs. document-level classifiers for the document detection problem.",
                "When the result is statistically significant, it is shown in bold.",
                "Further highlighting the improvement from finer-grained judgments and n-grams, Figure 3 graphically depicts the edge the SVM sentence-level classifier has over the standard bag-of-words approach with a precision-recall curve.",
                "In the high precision area of the graph, the consistent edge of the sentence-level classifier is rather impressive - continuing at precision 1 out to 0.1 recall.",
                "This would mean that a tenth of the users action-items would be placed at the top of their action-item sorted inbox.",
                "Additionally, the large separation at the top right of the curves corresponds to the area where the optimal F1 occurs for each classifier, agreeing with the large improvement from 0.6904 to 0.7682 in F1 score.",
                "Considering the relative unexplored nature of classification at the sentence-level, this gives great hope for further increases in performance.",
                "Accuracy F1 Unigram Ngram Unigram Ngram kNN 0.9519 0.9536 0.6540 0.6686 na¨ıve Bayes 0.9419 0.9550 0.6176 0.6676 SVM 0.9559 0.9579 0.6271 0.6672 Voted Perceptron 0.8895 0.9247 0.3744 0.5164 Table 6: Performance of the Sentence-Level Classifiers at Sentence Detection Although Cohen et al. [5] observed that the Voted Perceptron with a single training iteration outperformed SVM in a set of similar tasks, we see no such behavior here.",
                "This further strengthens the evidence that an alternate classifier with the bag-of-words representation could not reach the same level of performance.",
                "The Voted Perceptron classifier does improve when the number of training iterations are increased, but it is still lower than the SVM classifier.",
                "Sentence detection results are presented in Table 6.",
                "With regard to the sentence detection problem, we note that the F1 measure gives a better feel for the remaining room for improvement in this difficult problem.",
                "That is, unlike document detection where actionitem documents are fairly common, action-item sentences are very rare.",
                "Thus, as in other text problems, the accuracy numbers are deceptively high sheerly because of the default accuracy attainable by always predicting no.",
                "Although, the results here are significantly above-random, it is unclear what level of performance is necessary for sentence detection to be useful in and of itself and not simply as a means to document ranking and classification.",
                "Figure 4: Users find action-items quicker when assisted by a classification system.",
                "Finally, when considering a new type of classification task, one of the most basic questions is whether an accurate classifier built for the task can have an impact on the end-user.",
                "In order to demonstrate the impact this task can have on e-mail users, we conducted a user study using an earlier less-accurate version of the sentence classifier - where instead of using just a single sentence, a threesentence windowed-approach was used.",
                "There were three distinct sets of e-mail in which users had to find action-items.",
                "These sets were either presented in a random order (Unordered), ordered by the classifier (Ordered), or ordered by the classifier and with the 0.4 0.5 0.6 0.7 0.8 0.9 1 0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1 Precision Recall Action-Item Detection SVM Performance (Post Model Selection) Document Unigram Sentence Ngram Figure 3: Both n-grams and a small prediction window lead to consistent improvements over the standard approach. center sentence in the highest confidence window highlighted (Order+help).",
                "In order to perform fair comparisons between conditions, the overall number of tokens in each message set should be approximately equal; that is, the cognitive reading load should be approximately the same before the classifiers reordering.",
                "Additionally, users typically show practice effects by improving at the overall task and thus performing better at later message sets.",
                "This is typically handled by varying the ordering of the sets across users so that the means are comparable.",
                "While omitting further detail, we note the sets were balanced for the total number of tokens and a latin square design was used to balance practice effects.",
                "Figure 4 shows that at intervals of 5, 10, and 15 minutes, users consistently found significantly more action-items when assisted by the classifier, but were most critically aided in the first five minutes.",
                "Although, the classifier consistently aids the users, we did not gain an additional end-user impact by highlighting.",
                "As mentioned above, this might be a result of the large room for improvement that still exists for sentence detection, but anecdotal evidence suggests this might also be a result of how the information is presented to the user rather than the accuracy of sentence detection.",
                "For example, highlighting the wrong sentence near an actual action-item hurts the users trust, but if a vague indicator (e.g., an arrow) points to the approximate area the user is not aware of the near-miss.",
                "Since the user studies used a three sentence window, we believe this played a role as well as sentence detection accuracy. 4.6 Discussion In contrast to problems where n-grams have yielded little difference, we believe their power here stems from the fact that many of the meaningful n-grams for action-items consist of common words, e.g., let me know.",
                "Therefore, the document-level unigram approach cannot gain much leverage, even when modeling their joint probability correctly, since these words will often co-occur in the document but not necessarily in a phrase.",
                "Additionally, action-item detection is distinct from many text classification tasks in that a single sentence can change the class label of the document.",
                "As a result, good classifiers cannot rely on aggregating evidence from a large number of weak indicators across the entire document.",
                "Even though we discarded the header information, examining the top-ranked features at the document-level reveals that many of the features are names or parts of e-mail addresses that occurred in the body and are highly associated with e-mails that tend to contain many or no action-items.",
                "A few examples are terms such as org, bob, and gov.",
                "We note that these features will be sensitive to the particular distribution (senders/receivers) and thus the document-level approach may produce classifiers that transfer less readily to alternate contexts and users at different institutions.",
                "This points out that part of the problem of going beyond bag-of-words may be the methodology, and investigating such properties as learning curves and how well a model transfers may highlight differences in models which appear to have similar performance when tested on the distributions they were trained on.",
                "We are currently investigating whether the sentence-level classifiers do perform better over different test corpora without retraining. 5.",
                "FUTURE WORK While applying text classifiers at the document-level is fairly well-understood, there exists the potential for significantly increasing the performance of the sentence-level classifiers.",
                "Such methods include alternate ways of combining the predictions over each sentence, weightings other than tfidf, which may not be appropriate since sentences are small, better sentence segmentation, and other types of phrasal analysis.",
                "Additionally, named entity tagging, time expressions, etc., seem likely candidates for features that can further improve this task.",
                "We are currently pursuing some of these avenues to see what additional gains these offer.",
                "Finally, it would be interesting to investigate the best methods for combining the document-level and sentence-level classifiers.",
                "Since the simple bag-of-words representation at the document-level leads to a learned model that behaves somewhat like a context-specific prior dependent on the sender/receiver and general topic, a first choice would be to treat it as such when combining probability estimates with the sentence-level classifier.",
                "Such a model might serve as a general example for other problems where bag-of-words can establish a baseline model but richer approaches are needed to achieve performance beyond that baseline. 6.",
                "SUMMARY AND CONCLUSIONS The effectiveness of sentence-level detection argues that labeling at the sentence-level provides significant value.",
                "Further experiments are needed to see how this interacts with the amount of training data available.",
                "Sentence detection that is then agglomerated to document-level detection works surprisingly better given low recall than would be expected with sentence-level items.",
                "This, in turn, indicates that improved sentence segmentation methods could yield further improvements in classification.",
                "In this work, we examined how action-items can be effectively detected in e-mails.",
                "Our empirical analysis has demonstrated that n-grams are of key importance to making the most of documentlevel judgments.",
                "When finer-grained judgments are available, then a standard bag-of-words approach using a small (sentence) window size and automatic segmentation techniques can produce results almost as good as the n-gram based approaches.",
                "Acknowledgments This material is based upon work supported by the Defense Advanced Research Projects Agency (DARPA) under Contract No.",
                "NBCHD030010.",
                "Any opinions, findings and conclusions or recommendations expressed in this material are those of the author(s) and do not necessarily reflect the views of the Defense Advanced Research Projects Agency (DARPA), or the Department of InteriorNational Business Center (DOI-NBC).",
                "We would like to extend our sincerest thanks to Jill Lehman whose efforts in data collection were essential in constructing the corpus, and both Jill and Aaron Steinfeld for their direction of the HCI experiments.",
                "We would also like to thank Django Wexler for constructing and supporting the corpus labeling tools and Curtis Huttenhowers support of the text preprocessing package.",
                "Finally, we gratefully acknowledge Scott Fahlman for his encouragement and useful discussions on this topic. 7.",
                "REFERENCES [1] J. Allan, J. Carbonell, G. Doddington, J. Yamron, and Y. Yang.",
                "Topic detection and tracking pilot study: Final report.",
                "In Proceedings of the DARPA Broadcast News Transcription and Understanding Workshop, Washington, D.C., 1998. [2] C. Apte, F. Damerau, and S. M. Weiss.",
                "Automated learning of decision rules for <br>text categorization</br>.",
                "ACM Transactions on Information Systems, 12(3):233-251, July 1994. [3] J. Carletta.",
                "Assessing agreement on classification tasks: The kappa statistic.",
                "Computational Linguistics, 22(2):249-254, 1996. [4] J. Carroll.",
                "High precision extraction of grammatical relations.",
                "In Proceedings of the 19th International Conference on Computational Linguistics (COLING), pages 134-140, 2002. [5] W. W. Cohen, V. R. Carvalho, and T. M. Mitchell.",
                "Learning to classify email into speech acts.",
                "In EMNLP-2004 (Conference on Empirical Methods in Natural Language Processing), pages 309-316, 2004. [6] S. Corston-Oliver, E. Ringger, M. Gamon, and R. Campbell.",
                "Task-focused summarization of email.",
                "In Text Summarization Branches Out: Proceedings of the ACL-04 Workshop, pages 43-50, 2004. [7] A. Culotta, R. Bekkerman, and A. McCallum.",
                "Extracting social networks and contact information from email and the web.",
                "In CEAS-2004 (Conference on Email and Anti-Spam), Mountain View, CA, July 2004. [8] L. Devroye, L. Gy¨orfi, and G. Lugosi.",
                "A Probabilistic Theory of Pattern Recognition.",
                "Springer-Verlag, New York, NY, 1996. [9] S. T. Dumais, J. Platt, D. Heckerman, and M. Sahami.",
                "Inductive learning algorithms and representations for <br>text categorization</br>.",
                "In CIKM 98, Proceedings of the 7th ACM Conference on Information and Knowledge Management, pages 148-155, 1998. [10] Y. Freund and R. Schapire.",
                "Large margin classification using the perceptron algorithm.",
                "Machine Learning, 37(3):277-296, 1999. [11] T. Joachims.",
                "Making large-scale svm learning practical.",
                "In B. Sch¨olkopf, C. J. Burges, and A. J. Smola, editors, Advances in Kernel Methods - Support Vector Learning, pages 41-56.",
                "MIT Press, 1999. [12] L. S. Larkey.",
                "A patent search and classification system.",
                "In Proceedings of the Fourth ACM Conference on Digital Libraries, pages 179 - 187, 1999. [13] D. D. Lewis.",
                "An evaluation of phrasal and clustered representations on a <br>text categorization</br> task.",
                "In SIGIR 92, Proceedings of the 15th Annual International ACM Conference on Research and Development in Information Retrieval, pages 37-50, 1992. [14] Y. Liu, J. Carbonell, and R. Jin.",
                "A pairwise ensemble approach for accurate genre classification.",
                "In Proceedings of the European Conference on Machine Learning (ECML), 2003. [15] Y. Liu, R. Yan, R. Jin, and J. Carbonell.",
                "A comparison study of kernels for multi-label text classification using category association.",
                "In The Twenty-first International Conference on Machine Learning (ICML), 2004. [16] A. McCallum and K. Nigam.",
                "A comparison of event models for naive bayes text classification.",
                "In Working Notes of AAAI 98 (The 15th National Conference on Artificial Intelligence), Workshop on Learning for <br>text categorization</br>, pages 41-48, 1998.",
                "TR WS-98-05. [17] F. Sebastiani.",
                "Machine learning in automated <br>text categorization</br>.",
                "ACM Computing Surveys, 34(1):1-47, March 2002. [18] C. J. van Rijsbergen.",
                "Information Retrieval.",
                "Butterworths, London, 1979. [19] Y. Yang.",
                "An evaluation of statistical approaches to <br>text categorization</br>.",
                "Information Retrieval, 1(1/2):67-88, 1999. [20] Y. Yang, J. Carbonell, R. Brown, T. Pierce, B. T. Archibald, and X. Liu.",
                "Learning approaches to topic detection and tracking.",
                "IEEE EXPERT, Special Issue on Applications of Intelligent Information Retrieval, 1999. [21] Y. Yang and X. Liu.",
                "A re-examination of <br>text categorization</br> methods.",
                "In SIGIR 99, Proceedings of the 22nd Annual International ACM Conference on Research and Development in Information Retrieval, pages 42-49, 1999. [22] Y. Yang, J. Zhang, J. Carbonell, and C. Jin.",
                "Topic-conditioned novelty detection.",
                "In Proceedings of the ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, July 2002."
            ],
            "original_annotated_samples": [],
            "translated_annotated_samples": [
                "Por el contrario, la \"categorización de texto\" estándar simplemente asigna una etiqueta de tema a cada texto, ya sea que esa etiqueta corresponde a una carpeta de correo electrónico o un vocabulario de indexación controlado [12, 15, 22].",
                "La \"categorización de texto\" por tema, por otro lado, funciona muy bien usando solo palabras individuales como características [2, 9, 13, 17].",
                "Aprendizaje automático de reglas de decisión para \"categorización de texto\".",
                "Algoritmos de aprendizaje inductivo y representaciones para \"categorización de texto\".",
                "Una evaluación de representaciones componentes y agrupadas en una tarea de \"categorización de texto\".",
                "En Notas de trabajo de AAAI 98 (la 15ª Conferencia Nacional sobre Inteligencia Artificial), Taller sobre aprendizaje para \"categorización de texto\", páginas 41-48, 1998.",
                "Aprendizaje automático en \"categorización de texto\" automatizada.",
                "Una evaluación de los enfoques estadísticos para la \"categorización de texto\".",
                "Un reexamen de los métodos de \"categorización de texto\"."
            ],
            "translated_text": "",
            "candidates": [
                "categorización de texto",
                "categorización de texto",
                "categorización de texto",
                "categorización de texto",
                "categorización de texto",
                "categorización de texto",
                "categorización de texto",
                "categorización de texto",
                "categorización de texto",
                "categorización de texto",
                "Categorización de texto",
                "categorización de texto",
                "categorización de texto",
                "categorización de texto",
                "categorización de texto",
                "categorización de texto",
                "categorización de texto",
                "categorización de texto"
            ],
            "error": []
        },
        "genre-classification": {
            "translated_key": "clasificación de género",
            "is_in_text": true,
            "original_annotated_sentences": [
                "Feature Representation for Effective Action-Item Detection Paul N. Bennett Computer Science Department Carnegie Mellon University Pittsburgh, PA 15213 pbennett+@cs.cmu.edu Jaime Carbonell Language Technologies Institute.",
                "Carnegie Mellon University Pittsburgh, PA 15213 jgc+@cs.cmu.edu ABSTRACT E-mail users face an ever-growing challenge in managing their inboxes due to the growing centrality of email in the workplace for task assignment, action requests, and other roles beyond information dissemination.",
                "Whereas Information Retrieval and Machine Learning techniques are gaining initial acceptance in spam filtering and automated folder assignment, this paper reports on a new task: automated action-item detection, in order to flag emails that require responses, and to highlight the specific passage(s) indicating the request(s) for action.",
                "Unlike standard topic-driven text classification, action-item detection requires inferring the senders intent, and as such responds less well to pure bag-of-words classification.",
                "However, using enriched feature sets, such as n-grams (up to n=4) with chi-squared feature selection, and contextual cues for action-item location improve performance by up to 10% over unigrams, using in both cases state of the art classifiers such as SVMs with automated model selection via embedded cross-validation.",
                "Categories and Subject Descriptors H.3.3 [Information Storage and Retrieval]: Information Search and Retrieval; I.2.6 [Artificial Intelligence]: Learning; I.5.4 [Pattern Recognition]: Applications General Terms Experimentation 1.",
                "INTRODUCTION E-mail users are facing an increasingly difficult task of managing their inboxes in the face of mounting challenges that result from rising e-mail usage.",
                "This includes prioritizing e-mails over a range of sources from business partners to family members, filtering and reducing junk e-mail, and quickly managing requests that demand From: Henry Hutchins <hhutchins@innovative.company.com> To: Sara Smith; Joe Johnson; William Woolings Subject: meeting with prospective customers Sent: Fri 12/10/2005 8:08 AM Hi All, Id like to remind all of you that the group from GRTY will be visiting us next Friday at 4:30 p.m.",
                "The current schedule looks like this: + 9:30 a.m.",
                "Informal Breakfast and Discussion in Cafeteria + 10:30 a.m. Company Overview + 11:00 a.m.",
                "Individual Meetings (Continue Over Lunch) + 2:00 p.m. Tour of Facilities + 3:00 p.m.",
                "Sales Pitch In order to have this go off smoothly, I would like to practice the presentation well in advance.",
                "As a result, I will need each of your parts by Wednesday.",
                "Keep up the good work! -Henry Figure 1: An E-mail with emphasized Action-Item, an explicit request that requires the recipients attention or action. the receivers attention or action.",
                "Automated action-item detection targets the third of these problems by attempting to detect which e-mails require an action or response with information, and within those e-mails, attempting to highlight the sentence (or other passage length) that directly indicates the action request.",
                "Such a detection system can be used as one part of an e-mail agent which would assist a user in processing important e-mails quicker than would have been possible without the agent.",
                "We view action-item detection as one necessary component of a successful e-mail agent which would perform spam detection, action-item detection, topic classification and priority ranking, among other functions.",
                "The utility of such a detector can manifest as a method of prioritizing e-mails according to task-oriented criteria other than the standard ones of topic and sender or as a means of ensuring that the email user hasnt dropped the proverbial ball by forgetting to address an action request.",
                "Action-item detection differs from standard text classification in two important ways.",
                "First, the user is interested both in detecting whether an email contains action items and in locating exactly where these action item requests are contained within the email body.",
                "In contrast, standard text categorization merely assigns a topic label to each text, whether that label corresponds to an e-mail folder or a controlled indexing vocabulary [12, 15, 22].",
                "Second, action-item detection attempts to recover the email senders intent - whether she means to elicit response or action on the part of the receiver; note that for this task, classifiers using only unigrams as features do not perform optimally, as evidenced in our results below.",
                "Instead we find that we need more information-laden features such as higher-order n-grams.",
                "Text categorization by topic, on the other hand, works very well using just individual words as features [2, 9, 13, 17].",
                "In fact, <br>genre-classification</br>, which one would think may require more than a bag-of-words approach, also works quite well using just unigram features [14].",
                "Topic detection and tracking (TDT), also works well with unigram feature sets [1, 20].",
                "We believe that action-item detection is one of the first clear instances of an IR-related task where we must move beyond bag-of-words to achieve high performance, albeit not too far, as bag-of-n-grams seem to suffice.",
                "We first review related work for similar text classification problems such as e-mail priority ranking and speech act identification.",
                "Then we more formally define the action-item detection problem, discuss the aspects that distinguish it from more common problems like topic classification, and highlight the challenges in constructing systems that can perform well at the sentence and document level.",
                "From there, we move to a discussion of feature representation and selection techniques appropriate for this problem and how standard text classification approaches can be adapted to smoothly move from the sentence-level detection problem to the documentlevel classification problem.",
                "We then conduct an empirical analysis that helps us determine the effectiveness of our feature extraction procedures as well as establish baselines for a number of classification algorithms on this task.",
                "Finally, we summarize this papers contributions and consider interesting directions for future work. 2.",
                "RELATED WORK Several other researchers have considered very similar text classification tasks.",
                "Cohen et al. [5] describe an ontology of speech acts, such as Propose a Meeting, and attempt to predict when an e-mail contains one of these speech acts.",
                "We consider action-items to be an important specific type of speech act that falls within their more general classification.",
                "While they provide results for several classification methods, their methods only make use of human judgments at the document-level.",
                "In contrast, we consider whether accuracy can be increased by using finer-grained human judgments that mark the specific sentences and phrases of interest.",
                "Corston-Oliver et al. [6] consider detecting items in e-mail to Put on a To-Do List.",
                "This classification task is very similar to ours except they do not consider simple factual questions to belong to this category.",
                "We include questions, but note that not all questions are action-items - some are rhetorical or simply social convention, How are you?.",
                "From a learning perspective, while they make use of judgments at the sentence-level, they do not explicitly compare what if any benefits finer-grained judgments offer.",
                "Additionally, they do not study alternative choices or approaches to the classification task.",
                "Instead, they simply apply a standard SVM at the sentence-level and focus primarily on a linguistic analysis of how the sentence can be logically reformulated before adding it to the task list.",
                "In this study, we examine several alternative classification methods, compare document-level and sentence-level approaches and analyze the machine learning issues implicit in these problems.",
                "Interest in a variety of learning tasks related to e-mail has been rapidly growing in the recent literature.",
                "For example, in a forum dedicated to e-mail learning tasks, Culotta et al. [7] presented methods for learning social networks from e-mail.",
                "In this work, we do not focus on peer relationships; however, such methods could complement those here since peer relationships often influence word choice when requesting an action. 3.",
                "PROBLEM DEFINITION & APPROACH In contrast to previous work, we explicitly focus on the benefits that finer-grained, more costly, sentence-level human judgments offer over coarse-grained document-level judgments.",
                "Additionally, we consider multiple standard text classification approaches and analyze both the quantitative and qualitative differences that arise from taking a document-level vs. a sentence-level approach to classification.",
                "Finally, we focus on the representation necessary to achieve the most competitive performance. 3.1 Problem Definition In order to provide the most benefit to the user, a system would not only detect the document, but it would also indicate the specific sentences in the e-mail which contain the action-items.",
                "Therefore, there are three basic problems: 1.",
                "Document detection: Classify a document as to whether or not it contains an action-item. 2.",
                "Document ranking: Rank the documents such that all documents containing action-items occur as high as possible in the ranking. 3.",
                "Sentence detection: Classify each sentence in a document as to whether or not it is an action-item.",
                "As in most Information Retrieval tasks, the weight the evaluation metric should give to precision and recall depends on the nature of the application.",
                "In situations where a user will eventually read all received messages, ranking (e.g., via precision at recall of 1) may be most important since this will help encourage shorter delays in communications between users.",
                "In contrast, high-precision detection at low recall will be of increasing importance when the user is under severe time-pressure and therefore will likely not read all mail.",
                "This can be the case for crisis managers during disaster management.",
                "Finally, sentence detection plays a role in both timepressure situations and simply to alleviate the users required time to gist the message. 3.2 Approach As mentioned above, the labeled data can come in one of two forms: a document-labeling provides a yes/no label for each document as to whether it contains an action-item; a phrase-labeling provides only a yes label for the specific items of interest.",
                "We term the human judgments a phrase-labeling since the users view of the action-item may not correspond with actual sentence boundaries or predicted sentence boundaries.",
                "Obviously, it is straightforward to generate a document-labeling consistent with a phrase-labeling by labeling a document yes if and only if it contains at least one phrase labeled yes.",
                "To train classifiers for this task, we can take several viewpoints related to both the basic problems we have enumerated and the form of the labeled data.",
                "The document-level view treats each e-mail as a learning instance with an associated class-label.",
                "Then, the document can be converted to a feature-value vector and learning progresses as usual.",
                "Applying a document-level classifier to document detection and ranking is straightforward.",
                "In order to apply it to sentence detection, one must make additional steps.",
                "For example, if the classifier predicts a document contains an action-item, then areas of the document that contain a high-concentration of words which the model weights heavily in favor of action-items can be indicated.",
                "The obvious benefit of the document-level approach is that training set collection costs are lower since the user only has to specify whether or not an e-mail contains an action-item and not the specific sentences.",
                "In the sentence-level view, each e-mail is automatically segmented into sentences, and each sentence is treated as a learning instance with an associated class-label.",
                "Since the phrase-labeling provided by the user may not coincide with the automatic segmentation, we must determine what label to assign a partially overlapping sentence when converting it to a learning instance.",
                "Once trained, applying the resulting classifiers to sentence detection is now straightforward, but in order to apply the classifiers to document detection and document ranking, the individual predictions over each sentence must be aggregated in order to make a document-level prediction.",
                "This approach has the potential to benefit from morespecific labels that enable the learner to focus attention on the key sentences instead of having to learn based on data that the majority of the words in the e-mail provide no or little information about class membership. 3.2.1 Features Consider some of the phrases that might constitute part of an action item: would like to know, let me know, as soon as possible, have you.",
                "Each of these phrases consists of common words that occur in many e-mails.",
                "However, when they occur in the same sentence, they are far more indicative of an action-item.",
                "Additionally, order can be important: consider have you versus you have.",
                "Because of this, we posit that n-grams play a larger role in this problem than is typical of problems like topic classification.",
                "Therefore, we consider all n-grams up to size 4.",
                "When using n-grams, if we find an n-gram of size 4 in a segment of text, we can represent the text as just one occurrence of the ngram or as one occurrence of the n-gram and an occurrence of each smaller n-gram contained by it.",
                "We choose the second of these alternatives since this will allow the algorithm itself to smoothly back-off in terms of recall.",
                "Methods such as na¨ıve Bayes may be hurt by such a representation because of double-counting.",
                "Since sentence-ending punctuation can provide information, we retain the terminating punctuation token when it is identifiable.",
                "Additionally, we add a beginning-of-sentence and end-of-sentence token in order to capture patterns that are often indicators at the beginning or end of a sentence.",
                "Assuming proper punctuation, these extra tokens are unnecessary, but often e-mail lacks proper punctuation.",
                "In addition, for the sentence-level classifiers that use ngrams, we additionally code for each sentence a binary encoding of the position of the sentence relative to the document.",
                "This encoding has eight associated features that represent which octile (the first eighth, second eighth, etc.) contains the sentence. 3.2.2 Implementation Details In order to compare the document-level to the sentence-level approach, we compare predictions at the document-level.",
                "We do not address how to use a document-level classifier to make predictions at the sentence-level.",
                "In order to automatically segment the text of the e-mail, we use the RASP statistical parser [4].",
                "Since the automatically segmented sentences may not correspond directly with the phrase-level boundaries, we treat any sentence that contains at least 30% of a marked action-item segment as an action-item.",
                "When evaluating sentencedetection for the sentence-level system, we use these class labels as ground truth.",
                "Since we are not evaluating multiple segmentation approaches, this does not bias any of the methods.",
                "If multiple segmentation systems were under evaluation, one would need to use a metric that matched predicted positive sentences to phrases labeled positive.",
                "The metric would need to punish overly long true predictions as well as too short predictions.",
                "Our criteria for converting to labeled instances implicitly includes both criteria.",
                "Since the segmentation is fixed, an overly long prediction would be predicting yes for many no instances since presumably the extra length corresponds to additional segmented sentences all of which do not contain 30% of action-item.",
                "Likewise, a too short prediction must correspond to a small sentence included in the action-item but not constituting all of the action-item.",
                "Therefore, in order to consider the prediction to be too short, there will be an additional preceding/following sentence that is an action-item where we incorrectly predicted no.",
                "Once a sentence-level classifier has made a prediction for each sentence, we must combine these predictions to make both a document-level prediction and a document-level score.",
                "We use the simple policy of predicting positive when any of the sentences is predicted positive.",
                "In order to produce a document score for ranking, the confidence that the document contains an action-item is: ψ(d) = 1 n(d) s∈d|π(s)=1 ψ(s) if for any s ∈ d, π(s) = 1 1 n(d) maxs∈d ψ(s) o.w. where s is a sentence in document d, π is the classifiers 1/0 prediction, ψ is the score the classifier assigns as its confidence that π(s) = 1, and n(d) is the greater of 1 and the number of (unigram) tokens in the document.",
                "In other words, when any sentence is predicted positive, the document score is the length normalized sum of the sentence scores above threshold.",
                "When no sentence is predicted positive, the document score is the maximum sentence score normalized by length.",
                "As in other text problems, we are more likely to emit false positives for documents with more words or sentences.",
                "Thus we include a length normalization factor. 4.",
                "EXPERIMENTAL ANALYSIS 4.1 The Data Our corpus consists of e-mails obtained from volunteers at an educational institution and cover subjects such as: organizing a research workshop, arranging for job-candidate interviews, publishing proceedings, and talk announcements.",
                "The messages were anonymized by replacing the names of each individual and institution with a pseudonym.1 After attempting to identify and eliminate duplicate e-mails, the corpus contains 744 e-mail messages.",
                "After identity anonymization, the corpora has three basic versions.",
                "Quoted material refers to the text of a previous e-mail that an author often leaves in an e-mail message when responding to the e-mail.",
                "Quoted material can act as noise when learning since it may include action-items from previous messages that are no longer relevant.",
                "To isolate the effects of quoted material, we have three versions of the corpora.",
                "The raw form contains the basic messages.",
                "The auto-stripped version contains the messages after quoted material has been automatically removed.",
                "The hand-stripped version contains the messages after quoted material has been removed by a human.",
                "Additionally, the hand-stripped version has had any xml content and e-mail signatures removed - leaving only the essential content of the message.",
                "The studies reported here are performed with the hand-stripped version.",
                "This allows us to balance the cognitive load in terms of number of tokens that must be read in the user-studies we report - including quoted material would complicate the user studies since some users might skip the material while others read it.",
                "Additionally, ensuring all quoted material is removed 1 We have an even more highly anonymized version of the corpus that can be made available for some outside experimentation.",
                "Please contact the authors for more information on obtaining this data. prevents tainting the cross-validation since otherwise a test item could occur as quoted material in a training document. 4.1.1 Data Labeling Two human annotators labeled each message as to whether or not it contained an action-item.",
                "In addition, they identified each segment of the e-mail which contained an action-item.",
                "A segment is a contiguous section of text selected by the human annotators and may span several sentences or a complete phrase contained in a sentence.",
                "They were instructed that an action item is an explicit request for information that requires the recipients attention or a required action and told to highlight the phrases or sentences that make up the request.",
                "Annotator 1 No Yes Annotator 2 No 391 26 Yes 29 298 Table 1: Agreement of Human Annotators at Document Level Annotator One labeled 324 messages as containing action items.",
                "Annotator Two labeled 327 messages as containing action items.",
                "The agreement of the human annotators is shown in Tables 1 and 2.",
                "The annotators are said to agree at the document-level when both marked the same document as containing no action-items or both marked at least one action-item regardless of whether the text segments were the same.",
                "At the document-level, the annotators agreed 93% of the time.",
                "The kappa statistic [3, 5] is often used to evaluate inter-annotator agreement: κ = A − R 1 − R A is the empirical estimate of the probability of agreement.",
                "R is the empirical estimate of the probability of random agreement given the empirical class priors.",
                "A value close to −1 implies the annotators agree far less often than would be expected randomly, while a value close to 1 means they agree more often than randomly expected.",
                "At the document-level, the kappa statistic for inter-annotator agreement is 0.85.",
                "This value is both strong enough to expect the problem to be learnable and is comparable with results for similar tasks [5, 6].",
                "In order to determine the sentence-level agreement, we use each judgment to create a sentence-corpus with labels as described in Section 3.2.2, then consider the agreement over these sentences.",
                "This allows us to compare agreement over no judgments.",
                "We perform this comparison over the hand-stripped corpus since that eliminates spurious no judgments that would come from including quoted material, etc.",
                "Both annotators were free to label the subject as an action-item, but since neither did, we omit the subject line of the message as well.",
                "This only reduces the number of no agreements.",
                "This leaves 6301 automatically segmented sentences.",
                "At the sentence-level, the annotators agreed 98% of the time, and the kappa statistic for inter-annotator agreement is 0.82.",
                "In order to produce one single set of judgments, the human annotators went through each annotation where there was disagreement and came to a consensus opinion.",
                "The annotators did not collect statistics during this process but anecdotally reported that the majority of disagreements were either cases of clear annotator oversight or different interpretations of conditional statements.",
                "For example, If you would like to keep your job, come to tomorrows meeting implies a required action where If you would like to join Annotator 1 No Yes Annotator 2 No 5810 65 Yes 74 352 Table 2: Agreement of Human Annotators at Sentence Level the football betting pool, come to tomorrows meeting does not.",
                "The first would be an action-item in most contexts while the second would not.",
                "Of course, many conditional statements are not so clearly interpretable.",
                "After reconciling the judgments there are 416 e-mails with no action-items and 328 e-mails containing actionitems.",
                "Of the 328 e-mails containing action-items, 259 messages have one action-item segment; 55 messages have two action-item segments; 11 messages have three action-item segments.",
                "Two messages have four action-item segments, and one message has six action-item segments.",
                "Computing the sentence-level agreement using the reconciled gold standard judgments with each of the annotators individual judgments gives a kappa of 0.89 for Annotator One and a kappa of 0.92 for Annotator Two.",
                "In terms of message characteristics, there were on average 132 content tokens in the body after stripping.",
                "For action-item messages, there were 115.",
                "However, by examining Figure 2 we see the length distributions are nearly identical.",
                "As would be expected for e-mail, it is a long-tailed distribution with about half the messages having more than 60 tokens in the body (this paragraph has 65 tokens). 4.2 Classifiers For this experiment, we have selected a variety of standard text classification algorithms.",
                "In selecting algorithms, we have chosen algorithms that are not only known to work well but which differ along such lines as discriminative vs. generative and lazy vs. eager.",
                "We have done this in order to provide both a competitive and thorough sampling of learning methods for the task at hand.",
                "This is important since it is easy to improve a strawman classifier by introducing a new representation.",
                "By thoroughly sampling alternative classifier choices we demonstrate that representation improvements over bag-of-words are not due to using the information in the bag-of-words poorly. 4.2.1 kNN We employ a standard variant of the k-nearest neighbor algorithm used in text classification, kNN with s-cut score thresholding [19].",
                "We use a tfidf-weighting of the terms with a distanceweighted vote of the neighbors to compute the score before thresholding it.",
                "In order to choose the value of s for thresholding, we perform leave-one-out cross-validation over the training set.",
                "The value of k is set to be 2( log2 N + 1) where N is the number of training points.",
                "This rule for choosing k is theoretically motivated by results which show such a rule converges to the optimal classifier as the number of training points increases [8].",
                "In practice, we have also found it to be a computational convenience that frequently leads to comparable results with numerically optimizing k via a cross-validation procedure. 4.2.2 Na¨ıve Bayes We use a standard multinomial na¨ıve Bayes classifier [16].",
                "In using this classifier, we smoothed word and class probabilities using a Bayesian estimate (with the word prior) and a Laplace m-estimate, respectively. 0 20 40 60 80 100 120 140 160 0 200 400 600 800 1000 1200 1400 NumberofMessages Number of Tokens All Messages Action-Item Messages 0 0.02 0.04 0.06 0.08 0.1 0.12 0.14 0.16 0.18 0.2 0 200 400 600 800 1000 1200 1400 PercentageofMessages Number of Tokens All Messages Action-Item Messages Figure 2: The Histogram (left) and Distribution (right) of Message Length.",
                "A bin size of 20 words was used.",
                "Only tokens in the body after hand-stripping were counted.",
                "After stripping, the majority of words left are usually actual message content.",
                "Classifiers Document Unigram Document Ngram Sentence Unigram Sentence Ngram F1 kNN 0.6670 ± 0.0288 0.7108 ± 0.0699 0.7615 ± 0.0504 0.7790 ± 0.0460 na¨ıve Bayes 0.6572 ± 0.0749 0.6484 ± 0.0513 0.7715 ± 0.0597 0.7777 ± 0.0426 SVM 0.6904 ± 0.0347 0.7428 ± 0.0422 0.7282 ± 0.0698 0.7682 ± 0.0451 Voted Perceptron 0.6288 ± 0.0395 0.6774 ± 0.0422 0.6511 ± 0.0506 0.6798 ± 0.0913 Accuracy kNN 0.7029 ± 0.0659 0.7486 ± 0.0505 0.7972 ± 0.0435 0.8092 ± 0.0352 na¨ıve Bayes 0.6074 ± 0.0651 0.5816 ± 0.1075 0.7863 ± 0.0553 0.8145 ± 0.0268 SVM 0.7595 ± 0.0309 0.7904 ± 0.0349 0.7958 ± 0.0551 0.8173 ± 0.0258 Voted Perceptron 0.6531 ± 0.0390 0.7164 ± 0.0376 0.6413 ± 0.0833 0.7082 ± 0.1032 Table 3: Average Document-Detection Performance during Cross-Validation for Each Method and the Sample Standard Deviation (Sn−1) in italics.",
                "The best performance for each classifier is shown in bold. 4.2.3 SVM We have used a linear SVM with a tfidf feature representation and L2-norm as implemented in the SVMlight package v6.01 [11].",
                "All default settings were used. 4.2.4 Voted Perceptron Like the SVM, the Voted Perceptron is a kernel-based learning method.",
                "We use the same feature representation and kernel as we have for the SVM, a linear kernel with tfidf-weighting and an L2-norm.",
                "The voted perceptron is an online-learning method that keeps a history of past perceptrons used, as well as a weight signifying how often that perceptron was correct.",
                "With each new training example, a correct classification increases the weight on the current perceptron and an incorrect classification updates the perceptron.",
                "The output of the classifier uses the weights on the perceptra to make a final voted classification.",
                "When used in an offline-manner, multiple passes can be made through the training data.",
                "Both the voted perceptron and the SVM give a solution from the same hypothesis space - in this case, a linear classifier.",
                "Furthermore, it is well-known that the Voted Perceptron increases the margin of the solution after each pass through the training data [10].",
                "Since Cohen et al. [5] obtain worse results using an SVM than a Voted Perceptron with one training iteration, they conclude that the best solution for detecting speech acts may not lie in an area with a large margin.",
                "Because their tasks are highly similar to ours, we employ both classifiers to ensure we are not overlooking a competitive alternative classifier to the SVM for the basic bag-of-words representation. 4.3 Performance Measures To compare the performance of the classification methods, we look at two standard performance measures, F1 and accuracy.",
                "The F1 measure [18, 21] is the harmonic mean of precision and recall where Precision = Correct Positives Predicted Positives and Recall = Correct Positives Actual Positives . 4.4 Experimental Methodology We perform standard 10-fold cross-validation on the set of documents.",
                "For the sentence-level approach, all sentences in a document are either entirely in the training set or entirely in the test set for each fold.",
                "For significance tests, we use a two-tailed t-test [21] to compare the values obtained during each cross-validation fold with a p-value of 0.05.",
                "Feature selection was performed using the chi-squared statistic.",
                "Different levels of feature selection were considered for each classifier.",
                "Each of the following number of features was tried: 10, 25, 50, 100, 250, 750, 1000, 2000, 4000.",
                "There are approximately 4700 unigram tokens without feature selection.",
                "In order to choose the number of features to use for each classifier, we perform nested cross-validation and choose the settings that yield the optimal document-level F1 for that classifier.",
                "For this study, only the body of each e-mail message was used.",
                "Feature selection is always applied to all candidate features.",
                "That is, for the n-gram representation, the n-grams and position features are also subject to removal by the feature selection method. 4.5 Results The results for document-level classification are given in Table 3.",
                "The primary hypothesis we are concerned with is that n-grams are critical for this task; if this is true, we expect to see a significant gap in performance between the document-level classifiers that use n-grams (denoted Document Ngram) and those using only unigram features (denoted Document Unigram).",
                "Examining Table 3, we observe that this is indeed the case for every classifier except na¨ıve Bayes.",
                "This difference in performance produced by the n-gram representation is statistically significant for each classifier except for na¨ıve Bayes and the accuracy metric for kNN (see Table 4).",
                "Na¨ıve Bayes poor performance with the n-gram representation is not surprising since the bag-of-n-grams causes excessive doublecounting as mentioned in Section 3.2.1; however, na¨ıve Bayes is not hurt at the sentence-level because the sparse examples provide few chances for agglomerative effects of double counting.",
                "In either case, when a language-modeling approach is desired, modeling the n-grams directly would be preferable to na¨ıve Bayes.",
                "More importantly for the n-gram hypothesis, the n-grams lead to the best document-level classifier performance as well.",
                "As would be expected, the difference between the sentence-level n-gram representation and unigram representation is small.",
                "This is because the window of text is so small that the unigram representation, when done at the sentence-level, implicitly picks up on the power of the n-grams.",
                "Further improvement would signify that the order of the words matter even when only considering a small sentence-size window.",
                "Therefore, the finer-grained sentence-level judgments allows a unigram representation to succeed but only when performed in a small window - behaving as an n-gram representation for all practical purposes.",
                "Document Winner Sentence Winner kNN Ngram Ngram na¨ıve Bayes Unigram Ngram SVM Ngram† Ngram Voted Perceptron Ngram† Ngram Table 4: Significance results for n-grams versus unigrams for document detection using document-level and sentence-level classifiers.",
                "When the F1 result is statistically significant, it is shown in bold.",
                "When the accuracy result is significant, it is shown with a † .",
                "F1 Winner Accuracy Winner kNN Sentence Sentence na¨ıve Bayes Sentence Sentence SVM Sentence Sentence Voted Perceptron Sentence Document Table 5: Significance results for sentence-level classifiers vs. document-level classifiers for the document detection problem.",
                "When the result is statistically significant, it is shown in bold.",
                "Further highlighting the improvement from finer-grained judgments and n-grams, Figure 3 graphically depicts the edge the SVM sentence-level classifier has over the standard bag-of-words approach with a precision-recall curve.",
                "In the high precision area of the graph, the consistent edge of the sentence-level classifier is rather impressive - continuing at precision 1 out to 0.1 recall.",
                "This would mean that a tenth of the users action-items would be placed at the top of their action-item sorted inbox.",
                "Additionally, the large separation at the top right of the curves corresponds to the area where the optimal F1 occurs for each classifier, agreeing with the large improvement from 0.6904 to 0.7682 in F1 score.",
                "Considering the relative unexplored nature of classification at the sentence-level, this gives great hope for further increases in performance.",
                "Accuracy F1 Unigram Ngram Unigram Ngram kNN 0.9519 0.9536 0.6540 0.6686 na¨ıve Bayes 0.9419 0.9550 0.6176 0.6676 SVM 0.9559 0.9579 0.6271 0.6672 Voted Perceptron 0.8895 0.9247 0.3744 0.5164 Table 6: Performance of the Sentence-Level Classifiers at Sentence Detection Although Cohen et al. [5] observed that the Voted Perceptron with a single training iteration outperformed SVM in a set of similar tasks, we see no such behavior here.",
                "This further strengthens the evidence that an alternate classifier with the bag-of-words representation could not reach the same level of performance.",
                "The Voted Perceptron classifier does improve when the number of training iterations are increased, but it is still lower than the SVM classifier.",
                "Sentence detection results are presented in Table 6.",
                "With regard to the sentence detection problem, we note that the F1 measure gives a better feel for the remaining room for improvement in this difficult problem.",
                "That is, unlike document detection where actionitem documents are fairly common, action-item sentences are very rare.",
                "Thus, as in other text problems, the accuracy numbers are deceptively high sheerly because of the default accuracy attainable by always predicting no.",
                "Although, the results here are significantly above-random, it is unclear what level of performance is necessary for sentence detection to be useful in and of itself and not simply as a means to document ranking and classification.",
                "Figure 4: Users find action-items quicker when assisted by a classification system.",
                "Finally, when considering a new type of classification task, one of the most basic questions is whether an accurate classifier built for the task can have an impact on the end-user.",
                "In order to demonstrate the impact this task can have on e-mail users, we conducted a user study using an earlier less-accurate version of the sentence classifier - where instead of using just a single sentence, a threesentence windowed-approach was used.",
                "There were three distinct sets of e-mail in which users had to find action-items.",
                "These sets were either presented in a random order (Unordered), ordered by the classifier (Ordered), or ordered by the classifier and with the 0.4 0.5 0.6 0.7 0.8 0.9 1 0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1 Precision Recall Action-Item Detection SVM Performance (Post Model Selection) Document Unigram Sentence Ngram Figure 3: Both n-grams and a small prediction window lead to consistent improvements over the standard approach. center sentence in the highest confidence window highlighted (Order+help).",
                "In order to perform fair comparisons between conditions, the overall number of tokens in each message set should be approximately equal; that is, the cognitive reading load should be approximately the same before the classifiers reordering.",
                "Additionally, users typically show practice effects by improving at the overall task and thus performing better at later message sets.",
                "This is typically handled by varying the ordering of the sets across users so that the means are comparable.",
                "While omitting further detail, we note the sets were balanced for the total number of tokens and a latin square design was used to balance practice effects.",
                "Figure 4 shows that at intervals of 5, 10, and 15 minutes, users consistently found significantly more action-items when assisted by the classifier, but were most critically aided in the first five minutes.",
                "Although, the classifier consistently aids the users, we did not gain an additional end-user impact by highlighting.",
                "As mentioned above, this might be a result of the large room for improvement that still exists for sentence detection, but anecdotal evidence suggests this might also be a result of how the information is presented to the user rather than the accuracy of sentence detection.",
                "For example, highlighting the wrong sentence near an actual action-item hurts the users trust, but if a vague indicator (e.g., an arrow) points to the approximate area the user is not aware of the near-miss.",
                "Since the user studies used a three sentence window, we believe this played a role as well as sentence detection accuracy. 4.6 Discussion In contrast to problems where n-grams have yielded little difference, we believe their power here stems from the fact that many of the meaningful n-grams for action-items consist of common words, e.g., let me know.",
                "Therefore, the document-level unigram approach cannot gain much leverage, even when modeling their joint probability correctly, since these words will often co-occur in the document but not necessarily in a phrase.",
                "Additionally, action-item detection is distinct from many text classification tasks in that a single sentence can change the class label of the document.",
                "As a result, good classifiers cannot rely on aggregating evidence from a large number of weak indicators across the entire document.",
                "Even though we discarded the header information, examining the top-ranked features at the document-level reveals that many of the features are names or parts of e-mail addresses that occurred in the body and are highly associated with e-mails that tend to contain many or no action-items.",
                "A few examples are terms such as org, bob, and gov.",
                "We note that these features will be sensitive to the particular distribution (senders/receivers) and thus the document-level approach may produce classifiers that transfer less readily to alternate contexts and users at different institutions.",
                "This points out that part of the problem of going beyond bag-of-words may be the methodology, and investigating such properties as learning curves and how well a model transfers may highlight differences in models which appear to have similar performance when tested on the distributions they were trained on.",
                "We are currently investigating whether the sentence-level classifiers do perform better over different test corpora without retraining. 5.",
                "FUTURE WORK While applying text classifiers at the document-level is fairly well-understood, there exists the potential for significantly increasing the performance of the sentence-level classifiers.",
                "Such methods include alternate ways of combining the predictions over each sentence, weightings other than tfidf, which may not be appropriate since sentences are small, better sentence segmentation, and other types of phrasal analysis.",
                "Additionally, named entity tagging, time expressions, etc., seem likely candidates for features that can further improve this task.",
                "We are currently pursuing some of these avenues to see what additional gains these offer.",
                "Finally, it would be interesting to investigate the best methods for combining the document-level and sentence-level classifiers.",
                "Since the simple bag-of-words representation at the document-level leads to a learned model that behaves somewhat like a context-specific prior dependent on the sender/receiver and general topic, a first choice would be to treat it as such when combining probability estimates with the sentence-level classifier.",
                "Such a model might serve as a general example for other problems where bag-of-words can establish a baseline model but richer approaches are needed to achieve performance beyond that baseline. 6.",
                "SUMMARY AND CONCLUSIONS The effectiveness of sentence-level detection argues that labeling at the sentence-level provides significant value.",
                "Further experiments are needed to see how this interacts with the amount of training data available.",
                "Sentence detection that is then agglomerated to document-level detection works surprisingly better given low recall than would be expected with sentence-level items.",
                "This, in turn, indicates that improved sentence segmentation methods could yield further improvements in classification.",
                "In this work, we examined how action-items can be effectively detected in e-mails.",
                "Our empirical analysis has demonstrated that n-grams are of key importance to making the most of documentlevel judgments.",
                "When finer-grained judgments are available, then a standard bag-of-words approach using a small (sentence) window size and automatic segmentation techniques can produce results almost as good as the n-gram based approaches.",
                "Acknowledgments This material is based upon work supported by the Defense Advanced Research Projects Agency (DARPA) under Contract No.",
                "NBCHD030010.",
                "Any opinions, findings and conclusions or recommendations expressed in this material are those of the author(s) and do not necessarily reflect the views of the Defense Advanced Research Projects Agency (DARPA), or the Department of InteriorNational Business Center (DOI-NBC).",
                "We would like to extend our sincerest thanks to Jill Lehman whose efforts in data collection were essential in constructing the corpus, and both Jill and Aaron Steinfeld for their direction of the HCI experiments.",
                "We would also like to thank Django Wexler for constructing and supporting the corpus labeling tools and Curtis Huttenhowers support of the text preprocessing package.",
                "Finally, we gratefully acknowledge Scott Fahlman for his encouragement and useful discussions on this topic. 7.",
                "REFERENCES [1] J. Allan, J. Carbonell, G. Doddington, J. Yamron, and Y. Yang.",
                "Topic detection and tracking pilot study: Final report.",
                "In Proceedings of the DARPA Broadcast News Transcription and Understanding Workshop, Washington, D.C., 1998. [2] C. Apte, F. Damerau, and S. M. Weiss.",
                "Automated learning of decision rules for text categorization.",
                "ACM Transactions on Information Systems, 12(3):233-251, July 1994. [3] J. Carletta.",
                "Assessing agreement on classification tasks: The kappa statistic.",
                "Computational Linguistics, 22(2):249-254, 1996. [4] J. Carroll.",
                "High precision extraction of grammatical relations.",
                "In Proceedings of the 19th International Conference on Computational Linguistics (COLING), pages 134-140, 2002. [5] W. W. Cohen, V. R. Carvalho, and T. M. Mitchell.",
                "Learning to classify email into speech acts.",
                "In EMNLP-2004 (Conference on Empirical Methods in Natural Language Processing), pages 309-316, 2004. [6] S. Corston-Oliver, E. Ringger, M. Gamon, and R. Campbell.",
                "Task-focused summarization of email.",
                "In Text Summarization Branches Out: Proceedings of the ACL-04 Workshop, pages 43-50, 2004. [7] A. Culotta, R. Bekkerman, and A. McCallum.",
                "Extracting social networks and contact information from email and the web.",
                "In CEAS-2004 (Conference on Email and Anti-Spam), Mountain View, CA, July 2004. [8] L. Devroye, L. Gy¨orfi, and G. Lugosi.",
                "A Probabilistic Theory of Pattern Recognition.",
                "Springer-Verlag, New York, NY, 1996. [9] S. T. Dumais, J. Platt, D. Heckerman, and M. Sahami.",
                "Inductive learning algorithms and representations for text categorization.",
                "In CIKM 98, Proceedings of the 7th ACM Conference on Information and Knowledge Management, pages 148-155, 1998. [10] Y. Freund and R. Schapire.",
                "Large margin classification using the perceptron algorithm.",
                "Machine Learning, 37(3):277-296, 1999. [11] T. Joachims.",
                "Making large-scale svm learning practical.",
                "In B. Sch¨olkopf, C. J. Burges, and A. J. Smola, editors, Advances in Kernel Methods - Support Vector Learning, pages 41-56.",
                "MIT Press, 1999. [12] L. S. Larkey.",
                "A patent search and classification system.",
                "In Proceedings of the Fourth ACM Conference on Digital Libraries, pages 179 - 187, 1999. [13] D. D. Lewis.",
                "An evaluation of phrasal and clustered representations on a text categorization task.",
                "In SIGIR 92, Proceedings of the 15th Annual International ACM Conference on Research and Development in Information Retrieval, pages 37-50, 1992. [14] Y. Liu, J. Carbonell, and R. Jin.",
                "A pairwise ensemble approach for accurate genre classification.",
                "In Proceedings of the European Conference on Machine Learning (ECML), 2003. [15] Y. Liu, R. Yan, R. Jin, and J. Carbonell.",
                "A comparison study of kernels for multi-label text classification using category association.",
                "In The Twenty-first International Conference on Machine Learning (ICML), 2004. [16] A. McCallum and K. Nigam.",
                "A comparison of event models for naive bayes text classification.",
                "In Working Notes of AAAI 98 (The 15th National Conference on Artificial Intelligence), Workshop on Learning for Text Categorization, pages 41-48, 1998.",
                "TR WS-98-05. [17] F. Sebastiani.",
                "Machine learning in automated text categorization.",
                "ACM Computing Surveys, 34(1):1-47, March 2002. [18] C. J. van Rijsbergen.",
                "Information Retrieval.",
                "Butterworths, London, 1979. [19] Y. Yang.",
                "An evaluation of statistical approaches to text categorization.",
                "Information Retrieval, 1(1/2):67-88, 1999. [20] Y. Yang, J. Carbonell, R. Brown, T. Pierce, B. T. Archibald, and X. Liu.",
                "Learning approaches to topic detection and tracking.",
                "IEEE EXPERT, Special Issue on Applications of Intelligent Information Retrieval, 1999. [21] Y. Yang and X. Liu.",
                "A re-examination of text categorization methods.",
                "In SIGIR 99, Proceedings of the 22nd Annual International ACM Conference on Research and Development in Information Retrieval, pages 42-49, 1999. [22] Y. Yang, J. Zhang, J. Carbonell, and C. Jin.",
                "Topic-conditioned novelty detection.",
                "In Proceedings of the ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, July 2002."
            ],
            "original_annotated_samples": [],
            "translated_annotated_samples": [
                "De hecho, la \"clasificación de género\", que uno pensaría que puede requerir más que un enfoque de la bolsa de palabras, también funciona bastante bien utilizando solo características unigram [14]."
            ],
            "translated_text": "",
            "candidates": [
                "clasificación de género",
                "clasificación de género"
            ],
            "error": []
        },
        "e-mail priority ranking": {
            "translated_key": "clasificación de prioridad de correo electrónico",
            "is_in_text": true,
            "original_annotated_sentences": [
                "Feature Representation for Effective Action-Item Detection Paul N. Bennett Computer Science Department Carnegie Mellon University Pittsburgh, PA 15213 pbennett+@cs.cmu.edu Jaime Carbonell Language Technologies Institute.",
                "Carnegie Mellon University Pittsburgh, PA 15213 jgc+@cs.cmu.edu ABSTRACT E-mail users face an ever-growing challenge in managing their inboxes due to the growing centrality of email in the workplace for task assignment, action requests, and other roles beyond information dissemination.",
                "Whereas Information Retrieval and Machine Learning techniques are gaining initial acceptance in spam filtering and automated folder assignment, this paper reports on a new task: automated action-item detection, in order to flag emails that require responses, and to highlight the specific passage(s) indicating the request(s) for action.",
                "Unlike standard topic-driven text classification, action-item detection requires inferring the senders intent, and as such responds less well to pure bag-of-words classification.",
                "However, using enriched feature sets, such as n-grams (up to n=4) with chi-squared feature selection, and contextual cues for action-item location improve performance by up to 10% over unigrams, using in both cases state of the art classifiers such as SVMs with automated model selection via embedded cross-validation.",
                "Categories and Subject Descriptors H.3.3 [Information Storage and Retrieval]: Information Search and Retrieval; I.2.6 [Artificial Intelligence]: Learning; I.5.4 [Pattern Recognition]: Applications General Terms Experimentation 1.",
                "INTRODUCTION E-mail users are facing an increasingly difficult task of managing their inboxes in the face of mounting challenges that result from rising e-mail usage.",
                "This includes prioritizing e-mails over a range of sources from business partners to family members, filtering and reducing junk e-mail, and quickly managing requests that demand From: Henry Hutchins <hhutchins@innovative.company.com> To: Sara Smith; Joe Johnson; William Woolings Subject: meeting with prospective customers Sent: Fri 12/10/2005 8:08 AM Hi All, Id like to remind all of you that the group from GRTY will be visiting us next Friday at 4:30 p.m.",
                "The current schedule looks like this: + 9:30 a.m.",
                "Informal Breakfast and Discussion in Cafeteria + 10:30 a.m. Company Overview + 11:00 a.m.",
                "Individual Meetings (Continue Over Lunch) + 2:00 p.m. Tour of Facilities + 3:00 p.m.",
                "Sales Pitch In order to have this go off smoothly, I would like to practice the presentation well in advance.",
                "As a result, I will need each of your parts by Wednesday.",
                "Keep up the good work! -Henry Figure 1: An E-mail with emphasized Action-Item, an explicit request that requires the recipients attention or action. the receivers attention or action.",
                "Automated action-item detection targets the third of these problems by attempting to detect which e-mails require an action or response with information, and within those e-mails, attempting to highlight the sentence (or other passage length) that directly indicates the action request.",
                "Such a detection system can be used as one part of an e-mail agent which would assist a user in processing important e-mails quicker than would have been possible without the agent.",
                "We view action-item detection as one necessary component of a successful e-mail agent which would perform spam detection, action-item detection, topic classification and priority ranking, among other functions.",
                "The utility of such a detector can manifest as a method of prioritizing e-mails according to task-oriented criteria other than the standard ones of topic and sender or as a means of ensuring that the email user hasnt dropped the proverbial ball by forgetting to address an action request.",
                "Action-item detection differs from standard text classification in two important ways.",
                "First, the user is interested both in detecting whether an email contains action items and in locating exactly where these action item requests are contained within the email body.",
                "In contrast, standard text categorization merely assigns a topic label to each text, whether that label corresponds to an e-mail folder or a controlled indexing vocabulary [12, 15, 22].",
                "Second, action-item detection attempts to recover the email senders intent - whether she means to elicit response or action on the part of the receiver; note that for this task, classifiers using only unigrams as features do not perform optimally, as evidenced in our results below.",
                "Instead we find that we need more information-laden features such as higher-order n-grams.",
                "Text categorization by topic, on the other hand, works very well using just individual words as features [2, 9, 13, 17].",
                "In fact, genre-classification, which one would think may require more than a bag-of-words approach, also works quite well using just unigram features [14].",
                "Topic detection and tracking (TDT), also works well with unigram feature sets [1, 20].",
                "We believe that action-item detection is one of the first clear instances of an IR-related task where we must move beyond bag-of-words to achieve high performance, albeit not too far, as bag-of-n-grams seem to suffice.",
                "We first review related work for similar text classification problems such as <br>e-mail priority ranking</br> and speech act identification.",
                "Then we more formally define the action-item detection problem, discuss the aspects that distinguish it from more common problems like topic classification, and highlight the challenges in constructing systems that can perform well at the sentence and document level.",
                "From there, we move to a discussion of feature representation and selection techniques appropriate for this problem and how standard text classification approaches can be adapted to smoothly move from the sentence-level detection problem to the documentlevel classification problem.",
                "We then conduct an empirical analysis that helps us determine the effectiveness of our feature extraction procedures as well as establish baselines for a number of classification algorithms on this task.",
                "Finally, we summarize this papers contributions and consider interesting directions for future work. 2.",
                "RELATED WORK Several other researchers have considered very similar text classification tasks.",
                "Cohen et al. [5] describe an ontology of speech acts, such as Propose a Meeting, and attempt to predict when an e-mail contains one of these speech acts.",
                "We consider action-items to be an important specific type of speech act that falls within their more general classification.",
                "While they provide results for several classification methods, their methods only make use of human judgments at the document-level.",
                "In contrast, we consider whether accuracy can be increased by using finer-grained human judgments that mark the specific sentences and phrases of interest.",
                "Corston-Oliver et al. [6] consider detecting items in e-mail to Put on a To-Do List.",
                "This classification task is very similar to ours except they do not consider simple factual questions to belong to this category.",
                "We include questions, but note that not all questions are action-items - some are rhetorical or simply social convention, How are you?.",
                "From a learning perspective, while they make use of judgments at the sentence-level, they do not explicitly compare what if any benefits finer-grained judgments offer.",
                "Additionally, they do not study alternative choices or approaches to the classification task.",
                "Instead, they simply apply a standard SVM at the sentence-level and focus primarily on a linguistic analysis of how the sentence can be logically reformulated before adding it to the task list.",
                "In this study, we examine several alternative classification methods, compare document-level and sentence-level approaches and analyze the machine learning issues implicit in these problems.",
                "Interest in a variety of learning tasks related to e-mail has been rapidly growing in the recent literature.",
                "For example, in a forum dedicated to e-mail learning tasks, Culotta et al. [7] presented methods for learning social networks from e-mail.",
                "In this work, we do not focus on peer relationships; however, such methods could complement those here since peer relationships often influence word choice when requesting an action. 3.",
                "PROBLEM DEFINITION & APPROACH In contrast to previous work, we explicitly focus on the benefits that finer-grained, more costly, sentence-level human judgments offer over coarse-grained document-level judgments.",
                "Additionally, we consider multiple standard text classification approaches and analyze both the quantitative and qualitative differences that arise from taking a document-level vs. a sentence-level approach to classification.",
                "Finally, we focus on the representation necessary to achieve the most competitive performance. 3.1 Problem Definition In order to provide the most benefit to the user, a system would not only detect the document, but it would also indicate the specific sentences in the e-mail which contain the action-items.",
                "Therefore, there are three basic problems: 1.",
                "Document detection: Classify a document as to whether or not it contains an action-item. 2.",
                "Document ranking: Rank the documents such that all documents containing action-items occur as high as possible in the ranking. 3.",
                "Sentence detection: Classify each sentence in a document as to whether or not it is an action-item.",
                "As in most Information Retrieval tasks, the weight the evaluation metric should give to precision and recall depends on the nature of the application.",
                "In situations where a user will eventually read all received messages, ranking (e.g., via precision at recall of 1) may be most important since this will help encourage shorter delays in communications between users.",
                "In contrast, high-precision detection at low recall will be of increasing importance when the user is under severe time-pressure and therefore will likely not read all mail.",
                "This can be the case for crisis managers during disaster management.",
                "Finally, sentence detection plays a role in both timepressure situations and simply to alleviate the users required time to gist the message. 3.2 Approach As mentioned above, the labeled data can come in one of two forms: a document-labeling provides a yes/no label for each document as to whether it contains an action-item; a phrase-labeling provides only a yes label for the specific items of interest.",
                "We term the human judgments a phrase-labeling since the users view of the action-item may not correspond with actual sentence boundaries or predicted sentence boundaries.",
                "Obviously, it is straightforward to generate a document-labeling consistent with a phrase-labeling by labeling a document yes if and only if it contains at least one phrase labeled yes.",
                "To train classifiers for this task, we can take several viewpoints related to both the basic problems we have enumerated and the form of the labeled data.",
                "The document-level view treats each e-mail as a learning instance with an associated class-label.",
                "Then, the document can be converted to a feature-value vector and learning progresses as usual.",
                "Applying a document-level classifier to document detection and ranking is straightforward.",
                "In order to apply it to sentence detection, one must make additional steps.",
                "For example, if the classifier predicts a document contains an action-item, then areas of the document that contain a high-concentration of words which the model weights heavily in favor of action-items can be indicated.",
                "The obvious benefit of the document-level approach is that training set collection costs are lower since the user only has to specify whether or not an e-mail contains an action-item and not the specific sentences.",
                "In the sentence-level view, each e-mail is automatically segmented into sentences, and each sentence is treated as a learning instance with an associated class-label.",
                "Since the phrase-labeling provided by the user may not coincide with the automatic segmentation, we must determine what label to assign a partially overlapping sentence when converting it to a learning instance.",
                "Once trained, applying the resulting classifiers to sentence detection is now straightforward, but in order to apply the classifiers to document detection and document ranking, the individual predictions over each sentence must be aggregated in order to make a document-level prediction.",
                "This approach has the potential to benefit from morespecific labels that enable the learner to focus attention on the key sentences instead of having to learn based on data that the majority of the words in the e-mail provide no or little information about class membership. 3.2.1 Features Consider some of the phrases that might constitute part of an action item: would like to know, let me know, as soon as possible, have you.",
                "Each of these phrases consists of common words that occur in many e-mails.",
                "However, when they occur in the same sentence, they are far more indicative of an action-item.",
                "Additionally, order can be important: consider have you versus you have.",
                "Because of this, we posit that n-grams play a larger role in this problem than is typical of problems like topic classification.",
                "Therefore, we consider all n-grams up to size 4.",
                "When using n-grams, if we find an n-gram of size 4 in a segment of text, we can represent the text as just one occurrence of the ngram or as one occurrence of the n-gram and an occurrence of each smaller n-gram contained by it.",
                "We choose the second of these alternatives since this will allow the algorithm itself to smoothly back-off in terms of recall.",
                "Methods such as na¨ıve Bayes may be hurt by such a representation because of double-counting.",
                "Since sentence-ending punctuation can provide information, we retain the terminating punctuation token when it is identifiable.",
                "Additionally, we add a beginning-of-sentence and end-of-sentence token in order to capture patterns that are often indicators at the beginning or end of a sentence.",
                "Assuming proper punctuation, these extra tokens are unnecessary, but often e-mail lacks proper punctuation.",
                "In addition, for the sentence-level classifiers that use ngrams, we additionally code for each sentence a binary encoding of the position of the sentence relative to the document.",
                "This encoding has eight associated features that represent which octile (the first eighth, second eighth, etc.) contains the sentence. 3.2.2 Implementation Details In order to compare the document-level to the sentence-level approach, we compare predictions at the document-level.",
                "We do not address how to use a document-level classifier to make predictions at the sentence-level.",
                "In order to automatically segment the text of the e-mail, we use the RASP statistical parser [4].",
                "Since the automatically segmented sentences may not correspond directly with the phrase-level boundaries, we treat any sentence that contains at least 30% of a marked action-item segment as an action-item.",
                "When evaluating sentencedetection for the sentence-level system, we use these class labels as ground truth.",
                "Since we are not evaluating multiple segmentation approaches, this does not bias any of the methods.",
                "If multiple segmentation systems were under evaluation, one would need to use a metric that matched predicted positive sentences to phrases labeled positive.",
                "The metric would need to punish overly long true predictions as well as too short predictions.",
                "Our criteria for converting to labeled instances implicitly includes both criteria.",
                "Since the segmentation is fixed, an overly long prediction would be predicting yes for many no instances since presumably the extra length corresponds to additional segmented sentences all of which do not contain 30% of action-item.",
                "Likewise, a too short prediction must correspond to a small sentence included in the action-item but not constituting all of the action-item.",
                "Therefore, in order to consider the prediction to be too short, there will be an additional preceding/following sentence that is an action-item where we incorrectly predicted no.",
                "Once a sentence-level classifier has made a prediction for each sentence, we must combine these predictions to make both a document-level prediction and a document-level score.",
                "We use the simple policy of predicting positive when any of the sentences is predicted positive.",
                "In order to produce a document score for ranking, the confidence that the document contains an action-item is: ψ(d) = 1 n(d) s∈d|π(s)=1 ψ(s) if for any s ∈ d, π(s) = 1 1 n(d) maxs∈d ψ(s) o.w. where s is a sentence in document d, π is the classifiers 1/0 prediction, ψ is the score the classifier assigns as its confidence that π(s) = 1, and n(d) is the greater of 1 and the number of (unigram) tokens in the document.",
                "In other words, when any sentence is predicted positive, the document score is the length normalized sum of the sentence scores above threshold.",
                "When no sentence is predicted positive, the document score is the maximum sentence score normalized by length.",
                "As in other text problems, we are more likely to emit false positives for documents with more words or sentences.",
                "Thus we include a length normalization factor. 4.",
                "EXPERIMENTAL ANALYSIS 4.1 The Data Our corpus consists of e-mails obtained from volunteers at an educational institution and cover subjects such as: organizing a research workshop, arranging for job-candidate interviews, publishing proceedings, and talk announcements.",
                "The messages were anonymized by replacing the names of each individual and institution with a pseudonym.1 After attempting to identify and eliminate duplicate e-mails, the corpus contains 744 e-mail messages.",
                "After identity anonymization, the corpora has three basic versions.",
                "Quoted material refers to the text of a previous e-mail that an author often leaves in an e-mail message when responding to the e-mail.",
                "Quoted material can act as noise when learning since it may include action-items from previous messages that are no longer relevant.",
                "To isolate the effects of quoted material, we have three versions of the corpora.",
                "The raw form contains the basic messages.",
                "The auto-stripped version contains the messages after quoted material has been automatically removed.",
                "The hand-stripped version contains the messages after quoted material has been removed by a human.",
                "Additionally, the hand-stripped version has had any xml content and e-mail signatures removed - leaving only the essential content of the message.",
                "The studies reported here are performed with the hand-stripped version.",
                "This allows us to balance the cognitive load in terms of number of tokens that must be read in the user-studies we report - including quoted material would complicate the user studies since some users might skip the material while others read it.",
                "Additionally, ensuring all quoted material is removed 1 We have an even more highly anonymized version of the corpus that can be made available for some outside experimentation.",
                "Please contact the authors for more information on obtaining this data. prevents tainting the cross-validation since otherwise a test item could occur as quoted material in a training document. 4.1.1 Data Labeling Two human annotators labeled each message as to whether or not it contained an action-item.",
                "In addition, they identified each segment of the e-mail which contained an action-item.",
                "A segment is a contiguous section of text selected by the human annotators and may span several sentences or a complete phrase contained in a sentence.",
                "They were instructed that an action item is an explicit request for information that requires the recipients attention or a required action and told to highlight the phrases or sentences that make up the request.",
                "Annotator 1 No Yes Annotator 2 No 391 26 Yes 29 298 Table 1: Agreement of Human Annotators at Document Level Annotator One labeled 324 messages as containing action items.",
                "Annotator Two labeled 327 messages as containing action items.",
                "The agreement of the human annotators is shown in Tables 1 and 2.",
                "The annotators are said to agree at the document-level when both marked the same document as containing no action-items or both marked at least one action-item regardless of whether the text segments were the same.",
                "At the document-level, the annotators agreed 93% of the time.",
                "The kappa statistic [3, 5] is often used to evaluate inter-annotator agreement: κ = A − R 1 − R A is the empirical estimate of the probability of agreement.",
                "R is the empirical estimate of the probability of random agreement given the empirical class priors.",
                "A value close to −1 implies the annotators agree far less often than would be expected randomly, while a value close to 1 means they agree more often than randomly expected.",
                "At the document-level, the kappa statistic for inter-annotator agreement is 0.85.",
                "This value is both strong enough to expect the problem to be learnable and is comparable with results for similar tasks [5, 6].",
                "In order to determine the sentence-level agreement, we use each judgment to create a sentence-corpus with labels as described in Section 3.2.2, then consider the agreement over these sentences.",
                "This allows us to compare agreement over no judgments.",
                "We perform this comparison over the hand-stripped corpus since that eliminates spurious no judgments that would come from including quoted material, etc.",
                "Both annotators were free to label the subject as an action-item, but since neither did, we omit the subject line of the message as well.",
                "This only reduces the number of no agreements.",
                "This leaves 6301 automatically segmented sentences.",
                "At the sentence-level, the annotators agreed 98% of the time, and the kappa statistic for inter-annotator agreement is 0.82.",
                "In order to produce one single set of judgments, the human annotators went through each annotation where there was disagreement and came to a consensus opinion.",
                "The annotators did not collect statistics during this process but anecdotally reported that the majority of disagreements were either cases of clear annotator oversight or different interpretations of conditional statements.",
                "For example, If you would like to keep your job, come to tomorrows meeting implies a required action where If you would like to join Annotator 1 No Yes Annotator 2 No 5810 65 Yes 74 352 Table 2: Agreement of Human Annotators at Sentence Level the football betting pool, come to tomorrows meeting does not.",
                "The first would be an action-item in most contexts while the second would not.",
                "Of course, many conditional statements are not so clearly interpretable.",
                "After reconciling the judgments there are 416 e-mails with no action-items and 328 e-mails containing actionitems.",
                "Of the 328 e-mails containing action-items, 259 messages have one action-item segment; 55 messages have two action-item segments; 11 messages have three action-item segments.",
                "Two messages have four action-item segments, and one message has six action-item segments.",
                "Computing the sentence-level agreement using the reconciled gold standard judgments with each of the annotators individual judgments gives a kappa of 0.89 for Annotator One and a kappa of 0.92 for Annotator Two.",
                "In terms of message characteristics, there were on average 132 content tokens in the body after stripping.",
                "For action-item messages, there were 115.",
                "However, by examining Figure 2 we see the length distributions are nearly identical.",
                "As would be expected for e-mail, it is a long-tailed distribution with about half the messages having more than 60 tokens in the body (this paragraph has 65 tokens). 4.2 Classifiers For this experiment, we have selected a variety of standard text classification algorithms.",
                "In selecting algorithms, we have chosen algorithms that are not only known to work well but which differ along such lines as discriminative vs. generative and lazy vs. eager.",
                "We have done this in order to provide both a competitive and thorough sampling of learning methods for the task at hand.",
                "This is important since it is easy to improve a strawman classifier by introducing a new representation.",
                "By thoroughly sampling alternative classifier choices we demonstrate that representation improvements over bag-of-words are not due to using the information in the bag-of-words poorly. 4.2.1 kNN We employ a standard variant of the k-nearest neighbor algorithm used in text classification, kNN with s-cut score thresholding [19].",
                "We use a tfidf-weighting of the terms with a distanceweighted vote of the neighbors to compute the score before thresholding it.",
                "In order to choose the value of s for thresholding, we perform leave-one-out cross-validation over the training set.",
                "The value of k is set to be 2( log2 N + 1) where N is the number of training points.",
                "This rule for choosing k is theoretically motivated by results which show such a rule converges to the optimal classifier as the number of training points increases [8].",
                "In practice, we have also found it to be a computational convenience that frequently leads to comparable results with numerically optimizing k via a cross-validation procedure. 4.2.2 Na¨ıve Bayes We use a standard multinomial na¨ıve Bayes classifier [16].",
                "In using this classifier, we smoothed word and class probabilities using a Bayesian estimate (with the word prior) and a Laplace m-estimate, respectively. 0 20 40 60 80 100 120 140 160 0 200 400 600 800 1000 1200 1400 NumberofMessages Number of Tokens All Messages Action-Item Messages 0 0.02 0.04 0.06 0.08 0.1 0.12 0.14 0.16 0.18 0.2 0 200 400 600 800 1000 1200 1400 PercentageofMessages Number of Tokens All Messages Action-Item Messages Figure 2: The Histogram (left) and Distribution (right) of Message Length.",
                "A bin size of 20 words was used.",
                "Only tokens in the body after hand-stripping were counted.",
                "After stripping, the majority of words left are usually actual message content.",
                "Classifiers Document Unigram Document Ngram Sentence Unigram Sentence Ngram F1 kNN 0.6670 ± 0.0288 0.7108 ± 0.0699 0.7615 ± 0.0504 0.7790 ± 0.0460 na¨ıve Bayes 0.6572 ± 0.0749 0.6484 ± 0.0513 0.7715 ± 0.0597 0.7777 ± 0.0426 SVM 0.6904 ± 0.0347 0.7428 ± 0.0422 0.7282 ± 0.0698 0.7682 ± 0.0451 Voted Perceptron 0.6288 ± 0.0395 0.6774 ± 0.0422 0.6511 ± 0.0506 0.6798 ± 0.0913 Accuracy kNN 0.7029 ± 0.0659 0.7486 ± 0.0505 0.7972 ± 0.0435 0.8092 ± 0.0352 na¨ıve Bayes 0.6074 ± 0.0651 0.5816 ± 0.1075 0.7863 ± 0.0553 0.8145 ± 0.0268 SVM 0.7595 ± 0.0309 0.7904 ± 0.0349 0.7958 ± 0.0551 0.8173 ± 0.0258 Voted Perceptron 0.6531 ± 0.0390 0.7164 ± 0.0376 0.6413 ± 0.0833 0.7082 ± 0.1032 Table 3: Average Document-Detection Performance during Cross-Validation for Each Method and the Sample Standard Deviation (Sn−1) in italics.",
                "The best performance for each classifier is shown in bold. 4.2.3 SVM We have used a linear SVM with a tfidf feature representation and L2-norm as implemented in the SVMlight package v6.01 [11].",
                "All default settings were used. 4.2.4 Voted Perceptron Like the SVM, the Voted Perceptron is a kernel-based learning method.",
                "We use the same feature representation and kernel as we have for the SVM, a linear kernel with tfidf-weighting and an L2-norm.",
                "The voted perceptron is an online-learning method that keeps a history of past perceptrons used, as well as a weight signifying how often that perceptron was correct.",
                "With each new training example, a correct classification increases the weight on the current perceptron and an incorrect classification updates the perceptron.",
                "The output of the classifier uses the weights on the perceptra to make a final voted classification.",
                "When used in an offline-manner, multiple passes can be made through the training data.",
                "Both the voted perceptron and the SVM give a solution from the same hypothesis space - in this case, a linear classifier.",
                "Furthermore, it is well-known that the Voted Perceptron increases the margin of the solution after each pass through the training data [10].",
                "Since Cohen et al. [5] obtain worse results using an SVM than a Voted Perceptron with one training iteration, they conclude that the best solution for detecting speech acts may not lie in an area with a large margin.",
                "Because their tasks are highly similar to ours, we employ both classifiers to ensure we are not overlooking a competitive alternative classifier to the SVM for the basic bag-of-words representation. 4.3 Performance Measures To compare the performance of the classification methods, we look at two standard performance measures, F1 and accuracy.",
                "The F1 measure [18, 21] is the harmonic mean of precision and recall where Precision = Correct Positives Predicted Positives and Recall = Correct Positives Actual Positives . 4.4 Experimental Methodology We perform standard 10-fold cross-validation on the set of documents.",
                "For the sentence-level approach, all sentences in a document are either entirely in the training set or entirely in the test set for each fold.",
                "For significance tests, we use a two-tailed t-test [21] to compare the values obtained during each cross-validation fold with a p-value of 0.05.",
                "Feature selection was performed using the chi-squared statistic.",
                "Different levels of feature selection were considered for each classifier.",
                "Each of the following number of features was tried: 10, 25, 50, 100, 250, 750, 1000, 2000, 4000.",
                "There are approximately 4700 unigram tokens without feature selection.",
                "In order to choose the number of features to use for each classifier, we perform nested cross-validation and choose the settings that yield the optimal document-level F1 for that classifier.",
                "For this study, only the body of each e-mail message was used.",
                "Feature selection is always applied to all candidate features.",
                "That is, for the n-gram representation, the n-grams and position features are also subject to removal by the feature selection method. 4.5 Results The results for document-level classification are given in Table 3.",
                "The primary hypothesis we are concerned with is that n-grams are critical for this task; if this is true, we expect to see a significant gap in performance between the document-level classifiers that use n-grams (denoted Document Ngram) and those using only unigram features (denoted Document Unigram).",
                "Examining Table 3, we observe that this is indeed the case for every classifier except na¨ıve Bayes.",
                "This difference in performance produced by the n-gram representation is statistically significant for each classifier except for na¨ıve Bayes and the accuracy metric for kNN (see Table 4).",
                "Na¨ıve Bayes poor performance with the n-gram representation is not surprising since the bag-of-n-grams causes excessive doublecounting as mentioned in Section 3.2.1; however, na¨ıve Bayes is not hurt at the sentence-level because the sparse examples provide few chances for agglomerative effects of double counting.",
                "In either case, when a language-modeling approach is desired, modeling the n-grams directly would be preferable to na¨ıve Bayes.",
                "More importantly for the n-gram hypothesis, the n-grams lead to the best document-level classifier performance as well.",
                "As would be expected, the difference between the sentence-level n-gram representation and unigram representation is small.",
                "This is because the window of text is so small that the unigram representation, when done at the sentence-level, implicitly picks up on the power of the n-grams.",
                "Further improvement would signify that the order of the words matter even when only considering a small sentence-size window.",
                "Therefore, the finer-grained sentence-level judgments allows a unigram representation to succeed but only when performed in a small window - behaving as an n-gram representation for all practical purposes.",
                "Document Winner Sentence Winner kNN Ngram Ngram na¨ıve Bayes Unigram Ngram SVM Ngram† Ngram Voted Perceptron Ngram† Ngram Table 4: Significance results for n-grams versus unigrams for document detection using document-level and sentence-level classifiers.",
                "When the F1 result is statistically significant, it is shown in bold.",
                "When the accuracy result is significant, it is shown with a † .",
                "F1 Winner Accuracy Winner kNN Sentence Sentence na¨ıve Bayes Sentence Sentence SVM Sentence Sentence Voted Perceptron Sentence Document Table 5: Significance results for sentence-level classifiers vs. document-level classifiers for the document detection problem.",
                "When the result is statistically significant, it is shown in bold.",
                "Further highlighting the improvement from finer-grained judgments and n-grams, Figure 3 graphically depicts the edge the SVM sentence-level classifier has over the standard bag-of-words approach with a precision-recall curve.",
                "In the high precision area of the graph, the consistent edge of the sentence-level classifier is rather impressive - continuing at precision 1 out to 0.1 recall.",
                "This would mean that a tenth of the users action-items would be placed at the top of their action-item sorted inbox.",
                "Additionally, the large separation at the top right of the curves corresponds to the area where the optimal F1 occurs for each classifier, agreeing with the large improvement from 0.6904 to 0.7682 in F1 score.",
                "Considering the relative unexplored nature of classification at the sentence-level, this gives great hope for further increases in performance.",
                "Accuracy F1 Unigram Ngram Unigram Ngram kNN 0.9519 0.9536 0.6540 0.6686 na¨ıve Bayes 0.9419 0.9550 0.6176 0.6676 SVM 0.9559 0.9579 0.6271 0.6672 Voted Perceptron 0.8895 0.9247 0.3744 0.5164 Table 6: Performance of the Sentence-Level Classifiers at Sentence Detection Although Cohen et al. [5] observed that the Voted Perceptron with a single training iteration outperformed SVM in a set of similar tasks, we see no such behavior here.",
                "This further strengthens the evidence that an alternate classifier with the bag-of-words representation could not reach the same level of performance.",
                "The Voted Perceptron classifier does improve when the number of training iterations are increased, but it is still lower than the SVM classifier.",
                "Sentence detection results are presented in Table 6.",
                "With regard to the sentence detection problem, we note that the F1 measure gives a better feel for the remaining room for improvement in this difficult problem.",
                "That is, unlike document detection where actionitem documents are fairly common, action-item sentences are very rare.",
                "Thus, as in other text problems, the accuracy numbers are deceptively high sheerly because of the default accuracy attainable by always predicting no.",
                "Although, the results here are significantly above-random, it is unclear what level of performance is necessary for sentence detection to be useful in and of itself and not simply as a means to document ranking and classification.",
                "Figure 4: Users find action-items quicker when assisted by a classification system.",
                "Finally, when considering a new type of classification task, one of the most basic questions is whether an accurate classifier built for the task can have an impact on the end-user.",
                "In order to demonstrate the impact this task can have on e-mail users, we conducted a user study using an earlier less-accurate version of the sentence classifier - where instead of using just a single sentence, a threesentence windowed-approach was used.",
                "There were three distinct sets of e-mail in which users had to find action-items.",
                "These sets were either presented in a random order (Unordered), ordered by the classifier (Ordered), or ordered by the classifier and with the 0.4 0.5 0.6 0.7 0.8 0.9 1 0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1 Precision Recall Action-Item Detection SVM Performance (Post Model Selection) Document Unigram Sentence Ngram Figure 3: Both n-grams and a small prediction window lead to consistent improvements over the standard approach. center sentence in the highest confidence window highlighted (Order+help).",
                "In order to perform fair comparisons between conditions, the overall number of tokens in each message set should be approximately equal; that is, the cognitive reading load should be approximately the same before the classifiers reordering.",
                "Additionally, users typically show practice effects by improving at the overall task and thus performing better at later message sets.",
                "This is typically handled by varying the ordering of the sets across users so that the means are comparable.",
                "While omitting further detail, we note the sets were balanced for the total number of tokens and a latin square design was used to balance practice effects.",
                "Figure 4 shows that at intervals of 5, 10, and 15 minutes, users consistently found significantly more action-items when assisted by the classifier, but were most critically aided in the first five minutes.",
                "Although, the classifier consistently aids the users, we did not gain an additional end-user impact by highlighting.",
                "As mentioned above, this might be a result of the large room for improvement that still exists for sentence detection, but anecdotal evidence suggests this might also be a result of how the information is presented to the user rather than the accuracy of sentence detection.",
                "For example, highlighting the wrong sentence near an actual action-item hurts the users trust, but if a vague indicator (e.g., an arrow) points to the approximate area the user is not aware of the near-miss.",
                "Since the user studies used a three sentence window, we believe this played a role as well as sentence detection accuracy. 4.6 Discussion In contrast to problems where n-grams have yielded little difference, we believe their power here stems from the fact that many of the meaningful n-grams for action-items consist of common words, e.g., let me know.",
                "Therefore, the document-level unigram approach cannot gain much leverage, even when modeling their joint probability correctly, since these words will often co-occur in the document but not necessarily in a phrase.",
                "Additionally, action-item detection is distinct from many text classification tasks in that a single sentence can change the class label of the document.",
                "As a result, good classifiers cannot rely on aggregating evidence from a large number of weak indicators across the entire document.",
                "Even though we discarded the header information, examining the top-ranked features at the document-level reveals that many of the features are names or parts of e-mail addresses that occurred in the body and are highly associated with e-mails that tend to contain many or no action-items.",
                "A few examples are terms such as org, bob, and gov.",
                "We note that these features will be sensitive to the particular distribution (senders/receivers) and thus the document-level approach may produce classifiers that transfer less readily to alternate contexts and users at different institutions.",
                "This points out that part of the problem of going beyond bag-of-words may be the methodology, and investigating such properties as learning curves and how well a model transfers may highlight differences in models which appear to have similar performance when tested on the distributions they were trained on.",
                "We are currently investigating whether the sentence-level classifiers do perform better over different test corpora without retraining. 5.",
                "FUTURE WORK While applying text classifiers at the document-level is fairly well-understood, there exists the potential for significantly increasing the performance of the sentence-level classifiers.",
                "Such methods include alternate ways of combining the predictions over each sentence, weightings other than tfidf, which may not be appropriate since sentences are small, better sentence segmentation, and other types of phrasal analysis.",
                "Additionally, named entity tagging, time expressions, etc., seem likely candidates for features that can further improve this task.",
                "We are currently pursuing some of these avenues to see what additional gains these offer.",
                "Finally, it would be interesting to investigate the best methods for combining the document-level and sentence-level classifiers.",
                "Since the simple bag-of-words representation at the document-level leads to a learned model that behaves somewhat like a context-specific prior dependent on the sender/receiver and general topic, a first choice would be to treat it as such when combining probability estimates with the sentence-level classifier.",
                "Such a model might serve as a general example for other problems where bag-of-words can establish a baseline model but richer approaches are needed to achieve performance beyond that baseline. 6.",
                "SUMMARY AND CONCLUSIONS The effectiveness of sentence-level detection argues that labeling at the sentence-level provides significant value.",
                "Further experiments are needed to see how this interacts with the amount of training data available.",
                "Sentence detection that is then agglomerated to document-level detection works surprisingly better given low recall than would be expected with sentence-level items.",
                "This, in turn, indicates that improved sentence segmentation methods could yield further improvements in classification.",
                "In this work, we examined how action-items can be effectively detected in e-mails.",
                "Our empirical analysis has demonstrated that n-grams are of key importance to making the most of documentlevel judgments.",
                "When finer-grained judgments are available, then a standard bag-of-words approach using a small (sentence) window size and automatic segmentation techniques can produce results almost as good as the n-gram based approaches.",
                "Acknowledgments This material is based upon work supported by the Defense Advanced Research Projects Agency (DARPA) under Contract No.",
                "NBCHD030010.",
                "Any opinions, findings and conclusions or recommendations expressed in this material are those of the author(s) and do not necessarily reflect the views of the Defense Advanced Research Projects Agency (DARPA), or the Department of InteriorNational Business Center (DOI-NBC).",
                "We would like to extend our sincerest thanks to Jill Lehman whose efforts in data collection were essential in constructing the corpus, and both Jill and Aaron Steinfeld for their direction of the HCI experiments.",
                "We would also like to thank Django Wexler for constructing and supporting the corpus labeling tools and Curtis Huttenhowers support of the text preprocessing package.",
                "Finally, we gratefully acknowledge Scott Fahlman for his encouragement and useful discussions on this topic. 7.",
                "REFERENCES [1] J. Allan, J. Carbonell, G. Doddington, J. Yamron, and Y. Yang.",
                "Topic detection and tracking pilot study: Final report.",
                "In Proceedings of the DARPA Broadcast News Transcription and Understanding Workshop, Washington, D.C., 1998. [2] C. Apte, F. Damerau, and S. M. Weiss.",
                "Automated learning of decision rules for text categorization.",
                "ACM Transactions on Information Systems, 12(3):233-251, July 1994. [3] J. Carletta.",
                "Assessing agreement on classification tasks: The kappa statistic.",
                "Computational Linguistics, 22(2):249-254, 1996. [4] J. Carroll.",
                "High precision extraction of grammatical relations.",
                "In Proceedings of the 19th International Conference on Computational Linguistics (COLING), pages 134-140, 2002. [5] W. W. Cohen, V. R. Carvalho, and T. M. Mitchell.",
                "Learning to classify email into speech acts.",
                "In EMNLP-2004 (Conference on Empirical Methods in Natural Language Processing), pages 309-316, 2004. [6] S. Corston-Oliver, E. Ringger, M. Gamon, and R. Campbell.",
                "Task-focused summarization of email.",
                "In Text Summarization Branches Out: Proceedings of the ACL-04 Workshop, pages 43-50, 2004. [7] A. Culotta, R. Bekkerman, and A. McCallum.",
                "Extracting social networks and contact information from email and the web.",
                "In CEAS-2004 (Conference on Email and Anti-Spam), Mountain View, CA, July 2004. [8] L. Devroye, L. Gy¨orfi, and G. Lugosi.",
                "A Probabilistic Theory of Pattern Recognition.",
                "Springer-Verlag, New York, NY, 1996. [9] S. T. Dumais, J. Platt, D. Heckerman, and M. Sahami.",
                "Inductive learning algorithms and representations for text categorization.",
                "In CIKM 98, Proceedings of the 7th ACM Conference on Information and Knowledge Management, pages 148-155, 1998. [10] Y. Freund and R. Schapire.",
                "Large margin classification using the perceptron algorithm.",
                "Machine Learning, 37(3):277-296, 1999. [11] T. Joachims.",
                "Making large-scale svm learning practical.",
                "In B. Sch¨olkopf, C. J. Burges, and A. J. Smola, editors, Advances in Kernel Methods - Support Vector Learning, pages 41-56.",
                "MIT Press, 1999. [12] L. S. Larkey.",
                "A patent search and classification system.",
                "In Proceedings of the Fourth ACM Conference on Digital Libraries, pages 179 - 187, 1999. [13] D. D. Lewis.",
                "An evaluation of phrasal and clustered representations on a text categorization task.",
                "In SIGIR 92, Proceedings of the 15th Annual International ACM Conference on Research and Development in Information Retrieval, pages 37-50, 1992. [14] Y. Liu, J. Carbonell, and R. Jin.",
                "A pairwise ensemble approach for accurate genre classification.",
                "In Proceedings of the European Conference on Machine Learning (ECML), 2003. [15] Y. Liu, R. Yan, R. Jin, and J. Carbonell.",
                "A comparison study of kernels for multi-label text classification using category association.",
                "In The Twenty-first International Conference on Machine Learning (ICML), 2004. [16] A. McCallum and K. Nigam.",
                "A comparison of event models for naive bayes text classification.",
                "In Working Notes of AAAI 98 (The 15th National Conference on Artificial Intelligence), Workshop on Learning for Text Categorization, pages 41-48, 1998.",
                "TR WS-98-05. [17] F. Sebastiani.",
                "Machine learning in automated text categorization.",
                "ACM Computing Surveys, 34(1):1-47, March 2002. [18] C. J. van Rijsbergen.",
                "Information Retrieval.",
                "Butterworths, London, 1979. [19] Y. Yang.",
                "An evaluation of statistical approaches to text categorization.",
                "Information Retrieval, 1(1/2):67-88, 1999. [20] Y. Yang, J. Carbonell, R. Brown, T. Pierce, B. T. Archibald, and X. Liu.",
                "Learning approaches to topic detection and tracking.",
                "IEEE EXPERT, Special Issue on Applications of Intelligent Information Retrieval, 1999. [21] Y. Yang and X. Liu.",
                "A re-examination of text categorization methods.",
                "In SIGIR 99, Proceedings of the 22nd Annual International ACM Conference on Research and Development in Information Retrieval, pages 42-49, 1999. [22] Y. Yang, J. Zhang, J. Carbonell, and C. Jin.",
                "Topic-conditioned novelty detection.",
                "In Proceedings of the ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, July 2002."
            ],
            "original_annotated_samples": [],
            "translated_annotated_samples": [
                "Primero revisamos el trabajo relacionado para problemas de clasificación de texto similares, como la \"clasificación de prioridad de correo electrónico\" y la identificación de la Ley del habla."
            ],
            "translated_text": "",
            "candidates": [
                "clasificación de prioridad de correo electrónico",
                "clasificación de prioridad de correo electrónico"
            ],
            "error": []
        },
        "speech act identification": {
            "translated_key": "Identificación del acto del habla",
            "is_in_text": true,
            "original_annotated_sentences": [
                "Feature Representation for Effective Action-Item Detection Paul N. Bennett Computer Science Department Carnegie Mellon University Pittsburgh, PA 15213 pbennett+@cs.cmu.edu Jaime Carbonell Language Technologies Institute.",
                "Carnegie Mellon University Pittsburgh, PA 15213 jgc+@cs.cmu.edu ABSTRACT E-mail users face an ever-growing challenge in managing their inboxes due to the growing centrality of email in the workplace for task assignment, action requests, and other roles beyond information dissemination.",
                "Whereas Information Retrieval and Machine Learning techniques are gaining initial acceptance in spam filtering and automated folder assignment, this paper reports on a new task: automated action-item detection, in order to flag emails that require responses, and to highlight the specific passage(s) indicating the request(s) for action.",
                "Unlike standard topic-driven text classification, action-item detection requires inferring the senders intent, and as such responds less well to pure bag-of-words classification.",
                "However, using enriched feature sets, such as n-grams (up to n=4) with chi-squared feature selection, and contextual cues for action-item location improve performance by up to 10% over unigrams, using in both cases state of the art classifiers such as SVMs with automated model selection via embedded cross-validation.",
                "Categories and Subject Descriptors H.3.3 [Information Storage and Retrieval]: Information Search and Retrieval; I.2.6 [Artificial Intelligence]: Learning; I.5.4 [Pattern Recognition]: Applications General Terms Experimentation 1.",
                "INTRODUCTION E-mail users are facing an increasingly difficult task of managing their inboxes in the face of mounting challenges that result from rising e-mail usage.",
                "This includes prioritizing e-mails over a range of sources from business partners to family members, filtering and reducing junk e-mail, and quickly managing requests that demand From: Henry Hutchins <hhutchins@innovative.company.com> To: Sara Smith; Joe Johnson; William Woolings Subject: meeting with prospective customers Sent: Fri 12/10/2005 8:08 AM Hi All, Id like to remind all of you that the group from GRTY will be visiting us next Friday at 4:30 p.m.",
                "The current schedule looks like this: + 9:30 a.m.",
                "Informal Breakfast and Discussion in Cafeteria + 10:30 a.m. Company Overview + 11:00 a.m.",
                "Individual Meetings (Continue Over Lunch) + 2:00 p.m. Tour of Facilities + 3:00 p.m.",
                "Sales Pitch In order to have this go off smoothly, I would like to practice the presentation well in advance.",
                "As a result, I will need each of your parts by Wednesday.",
                "Keep up the good work! -Henry Figure 1: An E-mail with emphasized Action-Item, an explicit request that requires the recipients attention or action. the receivers attention or action.",
                "Automated action-item detection targets the third of these problems by attempting to detect which e-mails require an action or response with information, and within those e-mails, attempting to highlight the sentence (or other passage length) that directly indicates the action request.",
                "Such a detection system can be used as one part of an e-mail agent which would assist a user in processing important e-mails quicker than would have been possible without the agent.",
                "We view action-item detection as one necessary component of a successful e-mail agent which would perform spam detection, action-item detection, topic classification and priority ranking, among other functions.",
                "The utility of such a detector can manifest as a method of prioritizing e-mails according to task-oriented criteria other than the standard ones of topic and sender or as a means of ensuring that the email user hasnt dropped the proverbial ball by forgetting to address an action request.",
                "Action-item detection differs from standard text classification in two important ways.",
                "First, the user is interested both in detecting whether an email contains action items and in locating exactly where these action item requests are contained within the email body.",
                "In contrast, standard text categorization merely assigns a topic label to each text, whether that label corresponds to an e-mail folder or a controlled indexing vocabulary [12, 15, 22].",
                "Second, action-item detection attempts to recover the email senders intent - whether she means to elicit response or action on the part of the receiver; note that for this task, classifiers using only unigrams as features do not perform optimally, as evidenced in our results below.",
                "Instead we find that we need more information-laden features such as higher-order n-grams.",
                "Text categorization by topic, on the other hand, works very well using just individual words as features [2, 9, 13, 17].",
                "In fact, genre-classification, which one would think may require more than a bag-of-words approach, also works quite well using just unigram features [14].",
                "Topic detection and tracking (TDT), also works well with unigram feature sets [1, 20].",
                "We believe that action-item detection is one of the first clear instances of an IR-related task where we must move beyond bag-of-words to achieve high performance, albeit not too far, as bag-of-n-grams seem to suffice.",
                "We first review related work for similar text classification problems such as e-mail priority ranking and <br>speech act identification</br>.",
                "Then we more formally define the action-item detection problem, discuss the aspects that distinguish it from more common problems like topic classification, and highlight the challenges in constructing systems that can perform well at the sentence and document level.",
                "From there, we move to a discussion of feature representation and selection techniques appropriate for this problem and how standard text classification approaches can be adapted to smoothly move from the sentence-level detection problem to the documentlevel classification problem.",
                "We then conduct an empirical analysis that helps us determine the effectiveness of our feature extraction procedures as well as establish baselines for a number of classification algorithms on this task.",
                "Finally, we summarize this papers contributions and consider interesting directions for future work. 2.",
                "RELATED WORK Several other researchers have considered very similar text classification tasks.",
                "Cohen et al. [5] describe an ontology of speech acts, such as Propose a Meeting, and attempt to predict when an e-mail contains one of these speech acts.",
                "We consider action-items to be an important specific type of speech act that falls within their more general classification.",
                "While they provide results for several classification methods, their methods only make use of human judgments at the document-level.",
                "In contrast, we consider whether accuracy can be increased by using finer-grained human judgments that mark the specific sentences and phrases of interest.",
                "Corston-Oliver et al. [6] consider detecting items in e-mail to Put on a To-Do List.",
                "This classification task is very similar to ours except they do not consider simple factual questions to belong to this category.",
                "We include questions, but note that not all questions are action-items - some are rhetorical or simply social convention, How are you?.",
                "From a learning perspective, while they make use of judgments at the sentence-level, they do not explicitly compare what if any benefits finer-grained judgments offer.",
                "Additionally, they do not study alternative choices or approaches to the classification task.",
                "Instead, they simply apply a standard SVM at the sentence-level and focus primarily on a linguistic analysis of how the sentence can be logically reformulated before adding it to the task list.",
                "In this study, we examine several alternative classification methods, compare document-level and sentence-level approaches and analyze the machine learning issues implicit in these problems.",
                "Interest in a variety of learning tasks related to e-mail has been rapidly growing in the recent literature.",
                "For example, in a forum dedicated to e-mail learning tasks, Culotta et al. [7] presented methods for learning social networks from e-mail.",
                "In this work, we do not focus on peer relationships; however, such methods could complement those here since peer relationships often influence word choice when requesting an action. 3.",
                "PROBLEM DEFINITION & APPROACH In contrast to previous work, we explicitly focus on the benefits that finer-grained, more costly, sentence-level human judgments offer over coarse-grained document-level judgments.",
                "Additionally, we consider multiple standard text classification approaches and analyze both the quantitative and qualitative differences that arise from taking a document-level vs. a sentence-level approach to classification.",
                "Finally, we focus on the representation necessary to achieve the most competitive performance. 3.1 Problem Definition In order to provide the most benefit to the user, a system would not only detect the document, but it would also indicate the specific sentences in the e-mail which contain the action-items.",
                "Therefore, there are three basic problems: 1.",
                "Document detection: Classify a document as to whether or not it contains an action-item. 2.",
                "Document ranking: Rank the documents such that all documents containing action-items occur as high as possible in the ranking. 3.",
                "Sentence detection: Classify each sentence in a document as to whether or not it is an action-item.",
                "As in most Information Retrieval tasks, the weight the evaluation metric should give to precision and recall depends on the nature of the application.",
                "In situations where a user will eventually read all received messages, ranking (e.g., via precision at recall of 1) may be most important since this will help encourage shorter delays in communications between users.",
                "In contrast, high-precision detection at low recall will be of increasing importance when the user is under severe time-pressure and therefore will likely not read all mail.",
                "This can be the case for crisis managers during disaster management.",
                "Finally, sentence detection plays a role in both timepressure situations and simply to alleviate the users required time to gist the message. 3.2 Approach As mentioned above, the labeled data can come in one of two forms: a document-labeling provides a yes/no label for each document as to whether it contains an action-item; a phrase-labeling provides only a yes label for the specific items of interest.",
                "We term the human judgments a phrase-labeling since the users view of the action-item may not correspond with actual sentence boundaries or predicted sentence boundaries.",
                "Obviously, it is straightforward to generate a document-labeling consistent with a phrase-labeling by labeling a document yes if and only if it contains at least one phrase labeled yes.",
                "To train classifiers for this task, we can take several viewpoints related to both the basic problems we have enumerated and the form of the labeled data.",
                "The document-level view treats each e-mail as a learning instance with an associated class-label.",
                "Then, the document can be converted to a feature-value vector and learning progresses as usual.",
                "Applying a document-level classifier to document detection and ranking is straightforward.",
                "In order to apply it to sentence detection, one must make additional steps.",
                "For example, if the classifier predicts a document contains an action-item, then areas of the document that contain a high-concentration of words which the model weights heavily in favor of action-items can be indicated.",
                "The obvious benefit of the document-level approach is that training set collection costs are lower since the user only has to specify whether or not an e-mail contains an action-item and not the specific sentences.",
                "In the sentence-level view, each e-mail is automatically segmented into sentences, and each sentence is treated as a learning instance with an associated class-label.",
                "Since the phrase-labeling provided by the user may not coincide with the automatic segmentation, we must determine what label to assign a partially overlapping sentence when converting it to a learning instance.",
                "Once trained, applying the resulting classifiers to sentence detection is now straightforward, but in order to apply the classifiers to document detection and document ranking, the individual predictions over each sentence must be aggregated in order to make a document-level prediction.",
                "This approach has the potential to benefit from morespecific labels that enable the learner to focus attention on the key sentences instead of having to learn based on data that the majority of the words in the e-mail provide no or little information about class membership. 3.2.1 Features Consider some of the phrases that might constitute part of an action item: would like to know, let me know, as soon as possible, have you.",
                "Each of these phrases consists of common words that occur in many e-mails.",
                "However, when they occur in the same sentence, they are far more indicative of an action-item.",
                "Additionally, order can be important: consider have you versus you have.",
                "Because of this, we posit that n-grams play a larger role in this problem than is typical of problems like topic classification.",
                "Therefore, we consider all n-grams up to size 4.",
                "When using n-grams, if we find an n-gram of size 4 in a segment of text, we can represent the text as just one occurrence of the ngram or as one occurrence of the n-gram and an occurrence of each smaller n-gram contained by it.",
                "We choose the second of these alternatives since this will allow the algorithm itself to smoothly back-off in terms of recall.",
                "Methods such as na¨ıve Bayes may be hurt by such a representation because of double-counting.",
                "Since sentence-ending punctuation can provide information, we retain the terminating punctuation token when it is identifiable.",
                "Additionally, we add a beginning-of-sentence and end-of-sentence token in order to capture patterns that are often indicators at the beginning or end of a sentence.",
                "Assuming proper punctuation, these extra tokens are unnecessary, but often e-mail lacks proper punctuation.",
                "In addition, for the sentence-level classifiers that use ngrams, we additionally code for each sentence a binary encoding of the position of the sentence relative to the document.",
                "This encoding has eight associated features that represent which octile (the first eighth, second eighth, etc.) contains the sentence. 3.2.2 Implementation Details In order to compare the document-level to the sentence-level approach, we compare predictions at the document-level.",
                "We do not address how to use a document-level classifier to make predictions at the sentence-level.",
                "In order to automatically segment the text of the e-mail, we use the RASP statistical parser [4].",
                "Since the automatically segmented sentences may not correspond directly with the phrase-level boundaries, we treat any sentence that contains at least 30% of a marked action-item segment as an action-item.",
                "When evaluating sentencedetection for the sentence-level system, we use these class labels as ground truth.",
                "Since we are not evaluating multiple segmentation approaches, this does not bias any of the methods.",
                "If multiple segmentation systems were under evaluation, one would need to use a metric that matched predicted positive sentences to phrases labeled positive.",
                "The metric would need to punish overly long true predictions as well as too short predictions.",
                "Our criteria for converting to labeled instances implicitly includes both criteria.",
                "Since the segmentation is fixed, an overly long prediction would be predicting yes for many no instances since presumably the extra length corresponds to additional segmented sentences all of which do not contain 30% of action-item.",
                "Likewise, a too short prediction must correspond to a small sentence included in the action-item but not constituting all of the action-item.",
                "Therefore, in order to consider the prediction to be too short, there will be an additional preceding/following sentence that is an action-item where we incorrectly predicted no.",
                "Once a sentence-level classifier has made a prediction for each sentence, we must combine these predictions to make both a document-level prediction and a document-level score.",
                "We use the simple policy of predicting positive when any of the sentences is predicted positive.",
                "In order to produce a document score for ranking, the confidence that the document contains an action-item is: ψ(d) = 1 n(d) s∈d|π(s)=1 ψ(s) if for any s ∈ d, π(s) = 1 1 n(d) maxs∈d ψ(s) o.w. where s is a sentence in document d, π is the classifiers 1/0 prediction, ψ is the score the classifier assigns as its confidence that π(s) = 1, and n(d) is the greater of 1 and the number of (unigram) tokens in the document.",
                "In other words, when any sentence is predicted positive, the document score is the length normalized sum of the sentence scores above threshold.",
                "When no sentence is predicted positive, the document score is the maximum sentence score normalized by length.",
                "As in other text problems, we are more likely to emit false positives for documents with more words or sentences.",
                "Thus we include a length normalization factor. 4.",
                "EXPERIMENTAL ANALYSIS 4.1 The Data Our corpus consists of e-mails obtained from volunteers at an educational institution and cover subjects such as: organizing a research workshop, arranging for job-candidate interviews, publishing proceedings, and talk announcements.",
                "The messages were anonymized by replacing the names of each individual and institution with a pseudonym.1 After attempting to identify and eliminate duplicate e-mails, the corpus contains 744 e-mail messages.",
                "After identity anonymization, the corpora has three basic versions.",
                "Quoted material refers to the text of a previous e-mail that an author often leaves in an e-mail message when responding to the e-mail.",
                "Quoted material can act as noise when learning since it may include action-items from previous messages that are no longer relevant.",
                "To isolate the effects of quoted material, we have three versions of the corpora.",
                "The raw form contains the basic messages.",
                "The auto-stripped version contains the messages after quoted material has been automatically removed.",
                "The hand-stripped version contains the messages after quoted material has been removed by a human.",
                "Additionally, the hand-stripped version has had any xml content and e-mail signatures removed - leaving only the essential content of the message.",
                "The studies reported here are performed with the hand-stripped version.",
                "This allows us to balance the cognitive load in terms of number of tokens that must be read in the user-studies we report - including quoted material would complicate the user studies since some users might skip the material while others read it.",
                "Additionally, ensuring all quoted material is removed 1 We have an even more highly anonymized version of the corpus that can be made available for some outside experimentation.",
                "Please contact the authors for more information on obtaining this data. prevents tainting the cross-validation since otherwise a test item could occur as quoted material in a training document. 4.1.1 Data Labeling Two human annotators labeled each message as to whether or not it contained an action-item.",
                "In addition, they identified each segment of the e-mail which contained an action-item.",
                "A segment is a contiguous section of text selected by the human annotators and may span several sentences or a complete phrase contained in a sentence.",
                "They were instructed that an action item is an explicit request for information that requires the recipients attention or a required action and told to highlight the phrases or sentences that make up the request.",
                "Annotator 1 No Yes Annotator 2 No 391 26 Yes 29 298 Table 1: Agreement of Human Annotators at Document Level Annotator One labeled 324 messages as containing action items.",
                "Annotator Two labeled 327 messages as containing action items.",
                "The agreement of the human annotators is shown in Tables 1 and 2.",
                "The annotators are said to agree at the document-level when both marked the same document as containing no action-items or both marked at least one action-item regardless of whether the text segments were the same.",
                "At the document-level, the annotators agreed 93% of the time.",
                "The kappa statistic [3, 5] is often used to evaluate inter-annotator agreement: κ = A − R 1 − R A is the empirical estimate of the probability of agreement.",
                "R is the empirical estimate of the probability of random agreement given the empirical class priors.",
                "A value close to −1 implies the annotators agree far less often than would be expected randomly, while a value close to 1 means they agree more often than randomly expected.",
                "At the document-level, the kappa statistic for inter-annotator agreement is 0.85.",
                "This value is both strong enough to expect the problem to be learnable and is comparable with results for similar tasks [5, 6].",
                "In order to determine the sentence-level agreement, we use each judgment to create a sentence-corpus with labels as described in Section 3.2.2, then consider the agreement over these sentences.",
                "This allows us to compare agreement over no judgments.",
                "We perform this comparison over the hand-stripped corpus since that eliminates spurious no judgments that would come from including quoted material, etc.",
                "Both annotators were free to label the subject as an action-item, but since neither did, we omit the subject line of the message as well.",
                "This only reduces the number of no agreements.",
                "This leaves 6301 automatically segmented sentences.",
                "At the sentence-level, the annotators agreed 98% of the time, and the kappa statistic for inter-annotator agreement is 0.82.",
                "In order to produce one single set of judgments, the human annotators went through each annotation where there was disagreement and came to a consensus opinion.",
                "The annotators did not collect statistics during this process but anecdotally reported that the majority of disagreements were either cases of clear annotator oversight or different interpretations of conditional statements.",
                "For example, If you would like to keep your job, come to tomorrows meeting implies a required action where If you would like to join Annotator 1 No Yes Annotator 2 No 5810 65 Yes 74 352 Table 2: Agreement of Human Annotators at Sentence Level the football betting pool, come to tomorrows meeting does not.",
                "The first would be an action-item in most contexts while the second would not.",
                "Of course, many conditional statements are not so clearly interpretable.",
                "After reconciling the judgments there are 416 e-mails with no action-items and 328 e-mails containing actionitems.",
                "Of the 328 e-mails containing action-items, 259 messages have one action-item segment; 55 messages have two action-item segments; 11 messages have three action-item segments.",
                "Two messages have four action-item segments, and one message has six action-item segments.",
                "Computing the sentence-level agreement using the reconciled gold standard judgments with each of the annotators individual judgments gives a kappa of 0.89 for Annotator One and a kappa of 0.92 for Annotator Two.",
                "In terms of message characteristics, there were on average 132 content tokens in the body after stripping.",
                "For action-item messages, there were 115.",
                "However, by examining Figure 2 we see the length distributions are nearly identical.",
                "As would be expected for e-mail, it is a long-tailed distribution with about half the messages having more than 60 tokens in the body (this paragraph has 65 tokens). 4.2 Classifiers For this experiment, we have selected a variety of standard text classification algorithms.",
                "In selecting algorithms, we have chosen algorithms that are not only known to work well but which differ along such lines as discriminative vs. generative and lazy vs. eager.",
                "We have done this in order to provide both a competitive and thorough sampling of learning methods for the task at hand.",
                "This is important since it is easy to improve a strawman classifier by introducing a new representation.",
                "By thoroughly sampling alternative classifier choices we demonstrate that representation improvements over bag-of-words are not due to using the information in the bag-of-words poorly. 4.2.1 kNN We employ a standard variant of the k-nearest neighbor algorithm used in text classification, kNN with s-cut score thresholding [19].",
                "We use a tfidf-weighting of the terms with a distanceweighted vote of the neighbors to compute the score before thresholding it.",
                "In order to choose the value of s for thresholding, we perform leave-one-out cross-validation over the training set.",
                "The value of k is set to be 2( log2 N + 1) where N is the number of training points.",
                "This rule for choosing k is theoretically motivated by results which show such a rule converges to the optimal classifier as the number of training points increases [8].",
                "In practice, we have also found it to be a computational convenience that frequently leads to comparable results with numerically optimizing k via a cross-validation procedure. 4.2.2 Na¨ıve Bayes We use a standard multinomial na¨ıve Bayes classifier [16].",
                "In using this classifier, we smoothed word and class probabilities using a Bayesian estimate (with the word prior) and a Laplace m-estimate, respectively. 0 20 40 60 80 100 120 140 160 0 200 400 600 800 1000 1200 1400 NumberofMessages Number of Tokens All Messages Action-Item Messages 0 0.02 0.04 0.06 0.08 0.1 0.12 0.14 0.16 0.18 0.2 0 200 400 600 800 1000 1200 1400 PercentageofMessages Number of Tokens All Messages Action-Item Messages Figure 2: The Histogram (left) and Distribution (right) of Message Length.",
                "A bin size of 20 words was used.",
                "Only tokens in the body after hand-stripping were counted.",
                "After stripping, the majority of words left are usually actual message content.",
                "Classifiers Document Unigram Document Ngram Sentence Unigram Sentence Ngram F1 kNN 0.6670 ± 0.0288 0.7108 ± 0.0699 0.7615 ± 0.0504 0.7790 ± 0.0460 na¨ıve Bayes 0.6572 ± 0.0749 0.6484 ± 0.0513 0.7715 ± 0.0597 0.7777 ± 0.0426 SVM 0.6904 ± 0.0347 0.7428 ± 0.0422 0.7282 ± 0.0698 0.7682 ± 0.0451 Voted Perceptron 0.6288 ± 0.0395 0.6774 ± 0.0422 0.6511 ± 0.0506 0.6798 ± 0.0913 Accuracy kNN 0.7029 ± 0.0659 0.7486 ± 0.0505 0.7972 ± 0.0435 0.8092 ± 0.0352 na¨ıve Bayes 0.6074 ± 0.0651 0.5816 ± 0.1075 0.7863 ± 0.0553 0.8145 ± 0.0268 SVM 0.7595 ± 0.0309 0.7904 ± 0.0349 0.7958 ± 0.0551 0.8173 ± 0.0258 Voted Perceptron 0.6531 ± 0.0390 0.7164 ± 0.0376 0.6413 ± 0.0833 0.7082 ± 0.1032 Table 3: Average Document-Detection Performance during Cross-Validation for Each Method and the Sample Standard Deviation (Sn−1) in italics.",
                "The best performance for each classifier is shown in bold. 4.2.3 SVM We have used a linear SVM with a tfidf feature representation and L2-norm as implemented in the SVMlight package v6.01 [11].",
                "All default settings were used. 4.2.4 Voted Perceptron Like the SVM, the Voted Perceptron is a kernel-based learning method.",
                "We use the same feature representation and kernel as we have for the SVM, a linear kernel with tfidf-weighting and an L2-norm.",
                "The voted perceptron is an online-learning method that keeps a history of past perceptrons used, as well as a weight signifying how often that perceptron was correct.",
                "With each new training example, a correct classification increases the weight on the current perceptron and an incorrect classification updates the perceptron.",
                "The output of the classifier uses the weights on the perceptra to make a final voted classification.",
                "When used in an offline-manner, multiple passes can be made through the training data.",
                "Both the voted perceptron and the SVM give a solution from the same hypothesis space - in this case, a linear classifier.",
                "Furthermore, it is well-known that the Voted Perceptron increases the margin of the solution after each pass through the training data [10].",
                "Since Cohen et al. [5] obtain worse results using an SVM than a Voted Perceptron with one training iteration, they conclude that the best solution for detecting speech acts may not lie in an area with a large margin.",
                "Because their tasks are highly similar to ours, we employ both classifiers to ensure we are not overlooking a competitive alternative classifier to the SVM for the basic bag-of-words representation. 4.3 Performance Measures To compare the performance of the classification methods, we look at two standard performance measures, F1 and accuracy.",
                "The F1 measure [18, 21] is the harmonic mean of precision and recall where Precision = Correct Positives Predicted Positives and Recall = Correct Positives Actual Positives . 4.4 Experimental Methodology We perform standard 10-fold cross-validation on the set of documents.",
                "For the sentence-level approach, all sentences in a document are either entirely in the training set or entirely in the test set for each fold.",
                "For significance tests, we use a two-tailed t-test [21] to compare the values obtained during each cross-validation fold with a p-value of 0.05.",
                "Feature selection was performed using the chi-squared statistic.",
                "Different levels of feature selection were considered for each classifier.",
                "Each of the following number of features was tried: 10, 25, 50, 100, 250, 750, 1000, 2000, 4000.",
                "There are approximately 4700 unigram tokens without feature selection.",
                "In order to choose the number of features to use for each classifier, we perform nested cross-validation and choose the settings that yield the optimal document-level F1 for that classifier.",
                "For this study, only the body of each e-mail message was used.",
                "Feature selection is always applied to all candidate features.",
                "That is, for the n-gram representation, the n-grams and position features are also subject to removal by the feature selection method. 4.5 Results The results for document-level classification are given in Table 3.",
                "The primary hypothesis we are concerned with is that n-grams are critical for this task; if this is true, we expect to see a significant gap in performance between the document-level classifiers that use n-grams (denoted Document Ngram) and those using only unigram features (denoted Document Unigram).",
                "Examining Table 3, we observe that this is indeed the case for every classifier except na¨ıve Bayes.",
                "This difference in performance produced by the n-gram representation is statistically significant for each classifier except for na¨ıve Bayes and the accuracy metric for kNN (see Table 4).",
                "Na¨ıve Bayes poor performance with the n-gram representation is not surprising since the bag-of-n-grams causes excessive doublecounting as mentioned in Section 3.2.1; however, na¨ıve Bayes is not hurt at the sentence-level because the sparse examples provide few chances for agglomerative effects of double counting.",
                "In either case, when a language-modeling approach is desired, modeling the n-grams directly would be preferable to na¨ıve Bayes.",
                "More importantly for the n-gram hypothesis, the n-grams lead to the best document-level classifier performance as well.",
                "As would be expected, the difference between the sentence-level n-gram representation and unigram representation is small.",
                "This is because the window of text is so small that the unigram representation, when done at the sentence-level, implicitly picks up on the power of the n-grams.",
                "Further improvement would signify that the order of the words matter even when only considering a small sentence-size window.",
                "Therefore, the finer-grained sentence-level judgments allows a unigram representation to succeed but only when performed in a small window - behaving as an n-gram representation for all practical purposes.",
                "Document Winner Sentence Winner kNN Ngram Ngram na¨ıve Bayes Unigram Ngram SVM Ngram† Ngram Voted Perceptron Ngram† Ngram Table 4: Significance results for n-grams versus unigrams for document detection using document-level and sentence-level classifiers.",
                "When the F1 result is statistically significant, it is shown in bold.",
                "When the accuracy result is significant, it is shown with a † .",
                "F1 Winner Accuracy Winner kNN Sentence Sentence na¨ıve Bayes Sentence Sentence SVM Sentence Sentence Voted Perceptron Sentence Document Table 5: Significance results for sentence-level classifiers vs. document-level classifiers for the document detection problem.",
                "When the result is statistically significant, it is shown in bold.",
                "Further highlighting the improvement from finer-grained judgments and n-grams, Figure 3 graphically depicts the edge the SVM sentence-level classifier has over the standard bag-of-words approach with a precision-recall curve.",
                "In the high precision area of the graph, the consistent edge of the sentence-level classifier is rather impressive - continuing at precision 1 out to 0.1 recall.",
                "This would mean that a tenth of the users action-items would be placed at the top of their action-item sorted inbox.",
                "Additionally, the large separation at the top right of the curves corresponds to the area where the optimal F1 occurs for each classifier, agreeing with the large improvement from 0.6904 to 0.7682 in F1 score.",
                "Considering the relative unexplored nature of classification at the sentence-level, this gives great hope for further increases in performance.",
                "Accuracy F1 Unigram Ngram Unigram Ngram kNN 0.9519 0.9536 0.6540 0.6686 na¨ıve Bayes 0.9419 0.9550 0.6176 0.6676 SVM 0.9559 0.9579 0.6271 0.6672 Voted Perceptron 0.8895 0.9247 0.3744 0.5164 Table 6: Performance of the Sentence-Level Classifiers at Sentence Detection Although Cohen et al. [5] observed that the Voted Perceptron with a single training iteration outperformed SVM in a set of similar tasks, we see no such behavior here.",
                "This further strengthens the evidence that an alternate classifier with the bag-of-words representation could not reach the same level of performance.",
                "The Voted Perceptron classifier does improve when the number of training iterations are increased, but it is still lower than the SVM classifier.",
                "Sentence detection results are presented in Table 6.",
                "With regard to the sentence detection problem, we note that the F1 measure gives a better feel for the remaining room for improvement in this difficult problem.",
                "That is, unlike document detection where actionitem documents are fairly common, action-item sentences are very rare.",
                "Thus, as in other text problems, the accuracy numbers are deceptively high sheerly because of the default accuracy attainable by always predicting no.",
                "Although, the results here are significantly above-random, it is unclear what level of performance is necessary for sentence detection to be useful in and of itself and not simply as a means to document ranking and classification.",
                "Figure 4: Users find action-items quicker when assisted by a classification system.",
                "Finally, when considering a new type of classification task, one of the most basic questions is whether an accurate classifier built for the task can have an impact on the end-user.",
                "In order to demonstrate the impact this task can have on e-mail users, we conducted a user study using an earlier less-accurate version of the sentence classifier - where instead of using just a single sentence, a threesentence windowed-approach was used.",
                "There were three distinct sets of e-mail in which users had to find action-items.",
                "These sets were either presented in a random order (Unordered), ordered by the classifier (Ordered), or ordered by the classifier and with the 0.4 0.5 0.6 0.7 0.8 0.9 1 0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1 Precision Recall Action-Item Detection SVM Performance (Post Model Selection) Document Unigram Sentence Ngram Figure 3: Both n-grams and a small prediction window lead to consistent improvements over the standard approach. center sentence in the highest confidence window highlighted (Order+help).",
                "In order to perform fair comparisons between conditions, the overall number of tokens in each message set should be approximately equal; that is, the cognitive reading load should be approximately the same before the classifiers reordering.",
                "Additionally, users typically show practice effects by improving at the overall task and thus performing better at later message sets.",
                "This is typically handled by varying the ordering of the sets across users so that the means are comparable.",
                "While omitting further detail, we note the sets were balanced for the total number of tokens and a latin square design was used to balance practice effects.",
                "Figure 4 shows that at intervals of 5, 10, and 15 minutes, users consistently found significantly more action-items when assisted by the classifier, but were most critically aided in the first five minutes.",
                "Although, the classifier consistently aids the users, we did not gain an additional end-user impact by highlighting.",
                "As mentioned above, this might be a result of the large room for improvement that still exists for sentence detection, but anecdotal evidence suggests this might also be a result of how the information is presented to the user rather than the accuracy of sentence detection.",
                "For example, highlighting the wrong sentence near an actual action-item hurts the users trust, but if a vague indicator (e.g., an arrow) points to the approximate area the user is not aware of the near-miss.",
                "Since the user studies used a three sentence window, we believe this played a role as well as sentence detection accuracy. 4.6 Discussion In contrast to problems where n-grams have yielded little difference, we believe their power here stems from the fact that many of the meaningful n-grams for action-items consist of common words, e.g., let me know.",
                "Therefore, the document-level unigram approach cannot gain much leverage, even when modeling their joint probability correctly, since these words will often co-occur in the document but not necessarily in a phrase.",
                "Additionally, action-item detection is distinct from many text classification tasks in that a single sentence can change the class label of the document.",
                "As a result, good classifiers cannot rely on aggregating evidence from a large number of weak indicators across the entire document.",
                "Even though we discarded the header information, examining the top-ranked features at the document-level reveals that many of the features are names or parts of e-mail addresses that occurred in the body and are highly associated with e-mails that tend to contain many or no action-items.",
                "A few examples are terms such as org, bob, and gov.",
                "We note that these features will be sensitive to the particular distribution (senders/receivers) and thus the document-level approach may produce classifiers that transfer less readily to alternate contexts and users at different institutions.",
                "This points out that part of the problem of going beyond bag-of-words may be the methodology, and investigating such properties as learning curves and how well a model transfers may highlight differences in models which appear to have similar performance when tested on the distributions they were trained on.",
                "We are currently investigating whether the sentence-level classifiers do perform better over different test corpora without retraining. 5.",
                "FUTURE WORK While applying text classifiers at the document-level is fairly well-understood, there exists the potential for significantly increasing the performance of the sentence-level classifiers.",
                "Such methods include alternate ways of combining the predictions over each sentence, weightings other than tfidf, which may not be appropriate since sentences are small, better sentence segmentation, and other types of phrasal analysis.",
                "Additionally, named entity tagging, time expressions, etc., seem likely candidates for features that can further improve this task.",
                "We are currently pursuing some of these avenues to see what additional gains these offer.",
                "Finally, it would be interesting to investigate the best methods for combining the document-level and sentence-level classifiers.",
                "Since the simple bag-of-words representation at the document-level leads to a learned model that behaves somewhat like a context-specific prior dependent on the sender/receiver and general topic, a first choice would be to treat it as such when combining probability estimates with the sentence-level classifier.",
                "Such a model might serve as a general example for other problems where bag-of-words can establish a baseline model but richer approaches are needed to achieve performance beyond that baseline. 6.",
                "SUMMARY AND CONCLUSIONS The effectiveness of sentence-level detection argues that labeling at the sentence-level provides significant value.",
                "Further experiments are needed to see how this interacts with the amount of training data available.",
                "Sentence detection that is then agglomerated to document-level detection works surprisingly better given low recall than would be expected with sentence-level items.",
                "This, in turn, indicates that improved sentence segmentation methods could yield further improvements in classification.",
                "In this work, we examined how action-items can be effectively detected in e-mails.",
                "Our empirical analysis has demonstrated that n-grams are of key importance to making the most of documentlevel judgments.",
                "When finer-grained judgments are available, then a standard bag-of-words approach using a small (sentence) window size and automatic segmentation techniques can produce results almost as good as the n-gram based approaches.",
                "Acknowledgments This material is based upon work supported by the Defense Advanced Research Projects Agency (DARPA) under Contract No.",
                "NBCHD030010.",
                "Any opinions, findings and conclusions or recommendations expressed in this material are those of the author(s) and do not necessarily reflect the views of the Defense Advanced Research Projects Agency (DARPA), or the Department of InteriorNational Business Center (DOI-NBC).",
                "We would like to extend our sincerest thanks to Jill Lehman whose efforts in data collection were essential in constructing the corpus, and both Jill and Aaron Steinfeld for their direction of the HCI experiments.",
                "We would also like to thank Django Wexler for constructing and supporting the corpus labeling tools and Curtis Huttenhowers support of the text preprocessing package.",
                "Finally, we gratefully acknowledge Scott Fahlman for his encouragement and useful discussions on this topic. 7.",
                "REFERENCES [1] J. Allan, J. Carbonell, G. Doddington, J. Yamron, and Y. Yang.",
                "Topic detection and tracking pilot study: Final report.",
                "In Proceedings of the DARPA Broadcast News Transcription and Understanding Workshop, Washington, D.C., 1998. [2] C. Apte, F. Damerau, and S. M. Weiss.",
                "Automated learning of decision rules for text categorization.",
                "ACM Transactions on Information Systems, 12(3):233-251, July 1994. [3] J. Carletta.",
                "Assessing agreement on classification tasks: The kappa statistic.",
                "Computational Linguistics, 22(2):249-254, 1996. [4] J. Carroll.",
                "High precision extraction of grammatical relations.",
                "In Proceedings of the 19th International Conference on Computational Linguistics (COLING), pages 134-140, 2002. [5] W. W. Cohen, V. R. Carvalho, and T. M. Mitchell.",
                "Learning to classify email into speech acts.",
                "In EMNLP-2004 (Conference on Empirical Methods in Natural Language Processing), pages 309-316, 2004. [6] S. Corston-Oliver, E. Ringger, M. Gamon, and R. Campbell.",
                "Task-focused summarization of email.",
                "In Text Summarization Branches Out: Proceedings of the ACL-04 Workshop, pages 43-50, 2004. [7] A. Culotta, R. Bekkerman, and A. McCallum.",
                "Extracting social networks and contact information from email and the web.",
                "In CEAS-2004 (Conference on Email and Anti-Spam), Mountain View, CA, July 2004. [8] L. Devroye, L. Gy¨orfi, and G. Lugosi.",
                "A Probabilistic Theory of Pattern Recognition.",
                "Springer-Verlag, New York, NY, 1996. [9] S. T. Dumais, J. Platt, D. Heckerman, and M. Sahami.",
                "Inductive learning algorithms and representations for text categorization.",
                "In CIKM 98, Proceedings of the 7th ACM Conference on Information and Knowledge Management, pages 148-155, 1998. [10] Y. Freund and R. Schapire.",
                "Large margin classification using the perceptron algorithm.",
                "Machine Learning, 37(3):277-296, 1999. [11] T. Joachims.",
                "Making large-scale svm learning practical.",
                "In B. Sch¨olkopf, C. J. Burges, and A. J. Smola, editors, Advances in Kernel Methods - Support Vector Learning, pages 41-56.",
                "MIT Press, 1999. [12] L. S. Larkey.",
                "A patent search and classification system.",
                "In Proceedings of the Fourth ACM Conference on Digital Libraries, pages 179 - 187, 1999. [13] D. D. Lewis.",
                "An evaluation of phrasal and clustered representations on a text categorization task.",
                "In SIGIR 92, Proceedings of the 15th Annual International ACM Conference on Research and Development in Information Retrieval, pages 37-50, 1992. [14] Y. Liu, J. Carbonell, and R. Jin.",
                "A pairwise ensemble approach for accurate genre classification.",
                "In Proceedings of the European Conference on Machine Learning (ECML), 2003. [15] Y. Liu, R. Yan, R. Jin, and J. Carbonell.",
                "A comparison study of kernels for multi-label text classification using category association.",
                "In The Twenty-first International Conference on Machine Learning (ICML), 2004. [16] A. McCallum and K. Nigam.",
                "A comparison of event models for naive bayes text classification.",
                "In Working Notes of AAAI 98 (The 15th National Conference on Artificial Intelligence), Workshop on Learning for Text Categorization, pages 41-48, 1998.",
                "TR WS-98-05. [17] F. Sebastiani.",
                "Machine learning in automated text categorization.",
                "ACM Computing Surveys, 34(1):1-47, March 2002. [18] C. J. van Rijsbergen.",
                "Information Retrieval.",
                "Butterworths, London, 1979. [19] Y. Yang.",
                "An evaluation of statistical approaches to text categorization.",
                "Information Retrieval, 1(1/2):67-88, 1999. [20] Y. Yang, J. Carbonell, R. Brown, T. Pierce, B. T. Archibald, and X. Liu.",
                "Learning approaches to topic detection and tracking.",
                "IEEE EXPERT, Special Issue on Applications of Intelligent Information Retrieval, 1999. [21] Y. Yang and X. Liu.",
                "A re-examination of text categorization methods.",
                "In SIGIR 99, Proceedings of the 22nd Annual International ACM Conference on Research and Development in Information Retrieval, pages 42-49, 1999. [22] Y. Yang, J. Zhang, J. Carbonell, and C. Jin.",
                "Topic-conditioned novelty detection.",
                "In Proceedings of the ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, July 2002."
            ],
            "original_annotated_samples": [],
            "translated_annotated_samples": [
                "Primero revisamos el trabajo relacionado para problemas de clasificación de texto similares, como la clasificación de prioridad de correo electrónico y la \"identificación de la Ley del Discurso\"."
            ],
            "translated_text": "",
            "candidates": [
                "Identificación del acto del habla",
                "identificación de la Ley del Discurso"
            ],
            "error": []
        },
        "simple factual question": {
            "translated_key": "Pregunta fáctica simple",
            "is_in_text": true,
            "original_annotated_sentences": [
                "Feature Representation for Effective Action-Item Detection Paul N. Bennett Computer Science Department Carnegie Mellon University Pittsburgh, PA 15213 pbennett+@cs.cmu.edu Jaime Carbonell Language Technologies Institute.",
                "Carnegie Mellon University Pittsburgh, PA 15213 jgc+@cs.cmu.edu ABSTRACT E-mail users face an ever-growing challenge in managing their inboxes due to the growing centrality of email in the workplace for task assignment, action requests, and other roles beyond information dissemination.",
                "Whereas Information Retrieval and Machine Learning techniques are gaining initial acceptance in spam filtering and automated folder assignment, this paper reports on a new task: automated action-item detection, in order to flag emails that require responses, and to highlight the specific passage(s) indicating the request(s) for action.",
                "Unlike standard topic-driven text classification, action-item detection requires inferring the senders intent, and as such responds less well to pure bag-of-words classification.",
                "However, using enriched feature sets, such as n-grams (up to n=4) with chi-squared feature selection, and contextual cues for action-item location improve performance by up to 10% over unigrams, using in both cases state of the art classifiers such as SVMs with automated model selection via embedded cross-validation.",
                "Categories and Subject Descriptors H.3.3 [Information Storage and Retrieval]: Information Search and Retrieval; I.2.6 [Artificial Intelligence]: Learning; I.5.4 [Pattern Recognition]: Applications General Terms Experimentation 1.",
                "INTRODUCTION E-mail users are facing an increasingly difficult task of managing their inboxes in the face of mounting challenges that result from rising e-mail usage.",
                "This includes prioritizing e-mails over a range of sources from business partners to family members, filtering and reducing junk e-mail, and quickly managing requests that demand From: Henry Hutchins <hhutchins@innovative.company.com> To: Sara Smith; Joe Johnson; William Woolings Subject: meeting with prospective customers Sent: Fri 12/10/2005 8:08 AM Hi All, Id like to remind all of you that the group from GRTY will be visiting us next Friday at 4:30 p.m.",
                "The current schedule looks like this: + 9:30 a.m.",
                "Informal Breakfast and Discussion in Cafeteria + 10:30 a.m. Company Overview + 11:00 a.m.",
                "Individual Meetings (Continue Over Lunch) + 2:00 p.m. Tour of Facilities + 3:00 p.m.",
                "Sales Pitch In order to have this go off smoothly, I would like to practice the presentation well in advance.",
                "As a result, I will need each of your parts by Wednesday.",
                "Keep up the good work! -Henry Figure 1: An E-mail with emphasized Action-Item, an explicit request that requires the recipients attention or action. the receivers attention or action.",
                "Automated action-item detection targets the third of these problems by attempting to detect which e-mails require an action or response with information, and within those e-mails, attempting to highlight the sentence (or other passage length) that directly indicates the action request.",
                "Such a detection system can be used as one part of an e-mail agent which would assist a user in processing important e-mails quicker than would have been possible without the agent.",
                "We view action-item detection as one necessary component of a successful e-mail agent which would perform spam detection, action-item detection, topic classification and priority ranking, among other functions.",
                "The utility of such a detector can manifest as a method of prioritizing e-mails according to task-oriented criteria other than the standard ones of topic and sender or as a means of ensuring that the email user hasnt dropped the proverbial ball by forgetting to address an action request.",
                "Action-item detection differs from standard text classification in two important ways.",
                "First, the user is interested both in detecting whether an email contains action items and in locating exactly where these action item requests are contained within the email body.",
                "In contrast, standard text categorization merely assigns a topic label to each text, whether that label corresponds to an e-mail folder or a controlled indexing vocabulary [12, 15, 22].",
                "Second, action-item detection attempts to recover the email senders intent - whether she means to elicit response or action on the part of the receiver; note that for this task, classifiers using only unigrams as features do not perform optimally, as evidenced in our results below.",
                "Instead we find that we need more information-laden features such as higher-order n-grams.",
                "Text categorization by topic, on the other hand, works very well using just individual words as features [2, 9, 13, 17].",
                "In fact, genre-classification, which one would think may require more than a bag-of-words approach, also works quite well using just unigram features [14].",
                "Topic detection and tracking (TDT), also works well with unigram feature sets [1, 20].",
                "We believe that action-item detection is one of the first clear instances of an IR-related task where we must move beyond bag-of-words to achieve high performance, albeit not too far, as bag-of-n-grams seem to suffice.",
                "We first review related work for similar text classification problems such as e-mail priority ranking and speech act identification.",
                "Then we more formally define the action-item detection problem, discuss the aspects that distinguish it from more common problems like topic classification, and highlight the challenges in constructing systems that can perform well at the sentence and document level.",
                "From there, we move to a discussion of feature representation and selection techniques appropriate for this problem and how standard text classification approaches can be adapted to smoothly move from the sentence-level detection problem to the documentlevel classification problem.",
                "We then conduct an empirical analysis that helps us determine the effectiveness of our feature extraction procedures as well as establish baselines for a number of classification algorithms on this task.",
                "Finally, we summarize this papers contributions and consider interesting directions for future work. 2.",
                "RELATED WORK Several other researchers have considered very similar text classification tasks.",
                "Cohen et al. [5] describe an ontology of speech acts, such as Propose a Meeting, and attempt to predict when an e-mail contains one of these speech acts.",
                "We consider action-items to be an important specific type of speech act that falls within their more general classification.",
                "While they provide results for several classification methods, their methods only make use of human judgments at the document-level.",
                "In contrast, we consider whether accuracy can be increased by using finer-grained human judgments that mark the specific sentences and phrases of interest.",
                "Corston-Oliver et al. [6] consider detecting items in e-mail to Put on a To-Do List.",
                "This classification task is very similar to ours except they do not consider <br>simple factual question</br>s to belong to this category.",
                "We include questions, but note that not all questions are action-items - some are rhetorical or simply social convention, How are you?.",
                "From a learning perspective, while they make use of judgments at the sentence-level, they do not explicitly compare what if any benefits finer-grained judgments offer.",
                "Additionally, they do not study alternative choices or approaches to the classification task.",
                "Instead, they simply apply a standard SVM at the sentence-level and focus primarily on a linguistic analysis of how the sentence can be logically reformulated before adding it to the task list.",
                "In this study, we examine several alternative classification methods, compare document-level and sentence-level approaches and analyze the machine learning issues implicit in these problems.",
                "Interest in a variety of learning tasks related to e-mail has been rapidly growing in the recent literature.",
                "For example, in a forum dedicated to e-mail learning tasks, Culotta et al. [7] presented methods for learning social networks from e-mail.",
                "In this work, we do not focus on peer relationships; however, such methods could complement those here since peer relationships often influence word choice when requesting an action. 3.",
                "PROBLEM DEFINITION & APPROACH In contrast to previous work, we explicitly focus on the benefits that finer-grained, more costly, sentence-level human judgments offer over coarse-grained document-level judgments.",
                "Additionally, we consider multiple standard text classification approaches and analyze both the quantitative and qualitative differences that arise from taking a document-level vs. a sentence-level approach to classification.",
                "Finally, we focus on the representation necessary to achieve the most competitive performance. 3.1 Problem Definition In order to provide the most benefit to the user, a system would not only detect the document, but it would also indicate the specific sentences in the e-mail which contain the action-items.",
                "Therefore, there are three basic problems: 1.",
                "Document detection: Classify a document as to whether or not it contains an action-item. 2.",
                "Document ranking: Rank the documents such that all documents containing action-items occur as high as possible in the ranking. 3.",
                "Sentence detection: Classify each sentence in a document as to whether or not it is an action-item.",
                "As in most Information Retrieval tasks, the weight the evaluation metric should give to precision and recall depends on the nature of the application.",
                "In situations where a user will eventually read all received messages, ranking (e.g., via precision at recall of 1) may be most important since this will help encourage shorter delays in communications between users.",
                "In contrast, high-precision detection at low recall will be of increasing importance when the user is under severe time-pressure and therefore will likely not read all mail.",
                "This can be the case for crisis managers during disaster management.",
                "Finally, sentence detection plays a role in both timepressure situations and simply to alleviate the users required time to gist the message. 3.2 Approach As mentioned above, the labeled data can come in one of two forms: a document-labeling provides a yes/no label for each document as to whether it contains an action-item; a phrase-labeling provides only a yes label for the specific items of interest.",
                "We term the human judgments a phrase-labeling since the users view of the action-item may not correspond with actual sentence boundaries or predicted sentence boundaries.",
                "Obviously, it is straightforward to generate a document-labeling consistent with a phrase-labeling by labeling a document yes if and only if it contains at least one phrase labeled yes.",
                "To train classifiers for this task, we can take several viewpoints related to both the basic problems we have enumerated and the form of the labeled data.",
                "The document-level view treats each e-mail as a learning instance with an associated class-label.",
                "Then, the document can be converted to a feature-value vector and learning progresses as usual.",
                "Applying a document-level classifier to document detection and ranking is straightforward.",
                "In order to apply it to sentence detection, one must make additional steps.",
                "For example, if the classifier predicts a document contains an action-item, then areas of the document that contain a high-concentration of words which the model weights heavily in favor of action-items can be indicated.",
                "The obvious benefit of the document-level approach is that training set collection costs are lower since the user only has to specify whether or not an e-mail contains an action-item and not the specific sentences.",
                "In the sentence-level view, each e-mail is automatically segmented into sentences, and each sentence is treated as a learning instance with an associated class-label.",
                "Since the phrase-labeling provided by the user may not coincide with the automatic segmentation, we must determine what label to assign a partially overlapping sentence when converting it to a learning instance.",
                "Once trained, applying the resulting classifiers to sentence detection is now straightforward, but in order to apply the classifiers to document detection and document ranking, the individual predictions over each sentence must be aggregated in order to make a document-level prediction.",
                "This approach has the potential to benefit from morespecific labels that enable the learner to focus attention on the key sentences instead of having to learn based on data that the majority of the words in the e-mail provide no or little information about class membership. 3.2.1 Features Consider some of the phrases that might constitute part of an action item: would like to know, let me know, as soon as possible, have you.",
                "Each of these phrases consists of common words that occur in many e-mails.",
                "However, when they occur in the same sentence, they are far more indicative of an action-item.",
                "Additionally, order can be important: consider have you versus you have.",
                "Because of this, we posit that n-grams play a larger role in this problem than is typical of problems like topic classification.",
                "Therefore, we consider all n-grams up to size 4.",
                "When using n-grams, if we find an n-gram of size 4 in a segment of text, we can represent the text as just one occurrence of the ngram or as one occurrence of the n-gram and an occurrence of each smaller n-gram contained by it.",
                "We choose the second of these alternatives since this will allow the algorithm itself to smoothly back-off in terms of recall.",
                "Methods such as na¨ıve Bayes may be hurt by such a representation because of double-counting.",
                "Since sentence-ending punctuation can provide information, we retain the terminating punctuation token when it is identifiable.",
                "Additionally, we add a beginning-of-sentence and end-of-sentence token in order to capture patterns that are often indicators at the beginning or end of a sentence.",
                "Assuming proper punctuation, these extra tokens are unnecessary, but often e-mail lacks proper punctuation.",
                "In addition, for the sentence-level classifiers that use ngrams, we additionally code for each sentence a binary encoding of the position of the sentence relative to the document.",
                "This encoding has eight associated features that represent which octile (the first eighth, second eighth, etc.) contains the sentence. 3.2.2 Implementation Details In order to compare the document-level to the sentence-level approach, we compare predictions at the document-level.",
                "We do not address how to use a document-level classifier to make predictions at the sentence-level.",
                "In order to automatically segment the text of the e-mail, we use the RASP statistical parser [4].",
                "Since the automatically segmented sentences may not correspond directly with the phrase-level boundaries, we treat any sentence that contains at least 30% of a marked action-item segment as an action-item.",
                "When evaluating sentencedetection for the sentence-level system, we use these class labels as ground truth.",
                "Since we are not evaluating multiple segmentation approaches, this does not bias any of the methods.",
                "If multiple segmentation systems were under evaluation, one would need to use a metric that matched predicted positive sentences to phrases labeled positive.",
                "The metric would need to punish overly long true predictions as well as too short predictions.",
                "Our criteria for converting to labeled instances implicitly includes both criteria.",
                "Since the segmentation is fixed, an overly long prediction would be predicting yes for many no instances since presumably the extra length corresponds to additional segmented sentences all of which do not contain 30% of action-item.",
                "Likewise, a too short prediction must correspond to a small sentence included in the action-item but not constituting all of the action-item.",
                "Therefore, in order to consider the prediction to be too short, there will be an additional preceding/following sentence that is an action-item where we incorrectly predicted no.",
                "Once a sentence-level classifier has made a prediction for each sentence, we must combine these predictions to make both a document-level prediction and a document-level score.",
                "We use the simple policy of predicting positive when any of the sentences is predicted positive.",
                "In order to produce a document score for ranking, the confidence that the document contains an action-item is: ψ(d) = 1 n(d) s∈d|π(s)=1 ψ(s) if for any s ∈ d, π(s) = 1 1 n(d) maxs∈d ψ(s) o.w. where s is a sentence in document d, π is the classifiers 1/0 prediction, ψ is the score the classifier assigns as its confidence that π(s) = 1, and n(d) is the greater of 1 and the number of (unigram) tokens in the document.",
                "In other words, when any sentence is predicted positive, the document score is the length normalized sum of the sentence scores above threshold.",
                "When no sentence is predicted positive, the document score is the maximum sentence score normalized by length.",
                "As in other text problems, we are more likely to emit false positives for documents with more words or sentences.",
                "Thus we include a length normalization factor. 4.",
                "EXPERIMENTAL ANALYSIS 4.1 The Data Our corpus consists of e-mails obtained from volunteers at an educational institution and cover subjects such as: organizing a research workshop, arranging for job-candidate interviews, publishing proceedings, and talk announcements.",
                "The messages were anonymized by replacing the names of each individual and institution with a pseudonym.1 After attempting to identify and eliminate duplicate e-mails, the corpus contains 744 e-mail messages.",
                "After identity anonymization, the corpora has three basic versions.",
                "Quoted material refers to the text of a previous e-mail that an author often leaves in an e-mail message when responding to the e-mail.",
                "Quoted material can act as noise when learning since it may include action-items from previous messages that are no longer relevant.",
                "To isolate the effects of quoted material, we have three versions of the corpora.",
                "The raw form contains the basic messages.",
                "The auto-stripped version contains the messages after quoted material has been automatically removed.",
                "The hand-stripped version contains the messages after quoted material has been removed by a human.",
                "Additionally, the hand-stripped version has had any xml content and e-mail signatures removed - leaving only the essential content of the message.",
                "The studies reported here are performed with the hand-stripped version.",
                "This allows us to balance the cognitive load in terms of number of tokens that must be read in the user-studies we report - including quoted material would complicate the user studies since some users might skip the material while others read it.",
                "Additionally, ensuring all quoted material is removed 1 We have an even more highly anonymized version of the corpus that can be made available for some outside experimentation.",
                "Please contact the authors for more information on obtaining this data. prevents tainting the cross-validation since otherwise a test item could occur as quoted material in a training document. 4.1.1 Data Labeling Two human annotators labeled each message as to whether or not it contained an action-item.",
                "In addition, they identified each segment of the e-mail which contained an action-item.",
                "A segment is a contiguous section of text selected by the human annotators and may span several sentences or a complete phrase contained in a sentence.",
                "They were instructed that an action item is an explicit request for information that requires the recipients attention or a required action and told to highlight the phrases or sentences that make up the request.",
                "Annotator 1 No Yes Annotator 2 No 391 26 Yes 29 298 Table 1: Agreement of Human Annotators at Document Level Annotator One labeled 324 messages as containing action items.",
                "Annotator Two labeled 327 messages as containing action items.",
                "The agreement of the human annotators is shown in Tables 1 and 2.",
                "The annotators are said to agree at the document-level when both marked the same document as containing no action-items or both marked at least one action-item regardless of whether the text segments were the same.",
                "At the document-level, the annotators agreed 93% of the time.",
                "The kappa statistic [3, 5] is often used to evaluate inter-annotator agreement: κ = A − R 1 − R A is the empirical estimate of the probability of agreement.",
                "R is the empirical estimate of the probability of random agreement given the empirical class priors.",
                "A value close to −1 implies the annotators agree far less often than would be expected randomly, while a value close to 1 means they agree more often than randomly expected.",
                "At the document-level, the kappa statistic for inter-annotator agreement is 0.85.",
                "This value is both strong enough to expect the problem to be learnable and is comparable with results for similar tasks [5, 6].",
                "In order to determine the sentence-level agreement, we use each judgment to create a sentence-corpus with labels as described in Section 3.2.2, then consider the agreement over these sentences.",
                "This allows us to compare agreement over no judgments.",
                "We perform this comparison over the hand-stripped corpus since that eliminates spurious no judgments that would come from including quoted material, etc.",
                "Both annotators were free to label the subject as an action-item, but since neither did, we omit the subject line of the message as well.",
                "This only reduces the number of no agreements.",
                "This leaves 6301 automatically segmented sentences.",
                "At the sentence-level, the annotators agreed 98% of the time, and the kappa statistic for inter-annotator agreement is 0.82.",
                "In order to produce one single set of judgments, the human annotators went through each annotation where there was disagreement and came to a consensus opinion.",
                "The annotators did not collect statistics during this process but anecdotally reported that the majority of disagreements were either cases of clear annotator oversight or different interpretations of conditional statements.",
                "For example, If you would like to keep your job, come to tomorrows meeting implies a required action where If you would like to join Annotator 1 No Yes Annotator 2 No 5810 65 Yes 74 352 Table 2: Agreement of Human Annotators at Sentence Level the football betting pool, come to tomorrows meeting does not.",
                "The first would be an action-item in most contexts while the second would not.",
                "Of course, many conditional statements are not so clearly interpretable.",
                "After reconciling the judgments there are 416 e-mails with no action-items and 328 e-mails containing actionitems.",
                "Of the 328 e-mails containing action-items, 259 messages have one action-item segment; 55 messages have two action-item segments; 11 messages have three action-item segments.",
                "Two messages have four action-item segments, and one message has six action-item segments.",
                "Computing the sentence-level agreement using the reconciled gold standard judgments with each of the annotators individual judgments gives a kappa of 0.89 for Annotator One and a kappa of 0.92 for Annotator Two.",
                "In terms of message characteristics, there were on average 132 content tokens in the body after stripping.",
                "For action-item messages, there were 115.",
                "However, by examining Figure 2 we see the length distributions are nearly identical.",
                "As would be expected for e-mail, it is a long-tailed distribution with about half the messages having more than 60 tokens in the body (this paragraph has 65 tokens). 4.2 Classifiers For this experiment, we have selected a variety of standard text classification algorithms.",
                "In selecting algorithms, we have chosen algorithms that are not only known to work well but which differ along such lines as discriminative vs. generative and lazy vs. eager.",
                "We have done this in order to provide both a competitive and thorough sampling of learning methods for the task at hand.",
                "This is important since it is easy to improve a strawman classifier by introducing a new representation.",
                "By thoroughly sampling alternative classifier choices we demonstrate that representation improvements over bag-of-words are not due to using the information in the bag-of-words poorly. 4.2.1 kNN We employ a standard variant of the k-nearest neighbor algorithm used in text classification, kNN with s-cut score thresholding [19].",
                "We use a tfidf-weighting of the terms with a distanceweighted vote of the neighbors to compute the score before thresholding it.",
                "In order to choose the value of s for thresholding, we perform leave-one-out cross-validation over the training set.",
                "The value of k is set to be 2( log2 N + 1) where N is the number of training points.",
                "This rule for choosing k is theoretically motivated by results which show such a rule converges to the optimal classifier as the number of training points increases [8].",
                "In practice, we have also found it to be a computational convenience that frequently leads to comparable results with numerically optimizing k via a cross-validation procedure. 4.2.2 Na¨ıve Bayes We use a standard multinomial na¨ıve Bayes classifier [16].",
                "In using this classifier, we smoothed word and class probabilities using a Bayesian estimate (with the word prior) and a Laplace m-estimate, respectively. 0 20 40 60 80 100 120 140 160 0 200 400 600 800 1000 1200 1400 NumberofMessages Number of Tokens All Messages Action-Item Messages 0 0.02 0.04 0.06 0.08 0.1 0.12 0.14 0.16 0.18 0.2 0 200 400 600 800 1000 1200 1400 PercentageofMessages Number of Tokens All Messages Action-Item Messages Figure 2: The Histogram (left) and Distribution (right) of Message Length.",
                "A bin size of 20 words was used.",
                "Only tokens in the body after hand-stripping were counted.",
                "After stripping, the majority of words left are usually actual message content.",
                "Classifiers Document Unigram Document Ngram Sentence Unigram Sentence Ngram F1 kNN 0.6670 ± 0.0288 0.7108 ± 0.0699 0.7615 ± 0.0504 0.7790 ± 0.0460 na¨ıve Bayes 0.6572 ± 0.0749 0.6484 ± 0.0513 0.7715 ± 0.0597 0.7777 ± 0.0426 SVM 0.6904 ± 0.0347 0.7428 ± 0.0422 0.7282 ± 0.0698 0.7682 ± 0.0451 Voted Perceptron 0.6288 ± 0.0395 0.6774 ± 0.0422 0.6511 ± 0.0506 0.6798 ± 0.0913 Accuracy kNN 0.7029 ± 0.0659 0.7486 ± 0.0505 0.7972 ± 0.0435 0.8092 ± 0.0352 na¨ıve Bayes 0.6074 ± 0.0651 0.5816 ± 0.1075 0.7863 ± 0.0553 0.8145 ± 0.0268 SVM 0.7595 ± 0.0309 0.7904 ± 0.0349 0.7958 ± 0.0551 0.8173 ± 0.0258 Voted Perceptron 0.6531 ± 0.0390 0.7164 ± 0.0376 0.6413 ± 0.0833 0.7082 ± 0.1032 Table 3: Average Document-Detection Performance during Cross-Validation for Each Method and the Sample Standard Deviation (Sn−1) in italics.",
                "The best performance for each classifier is shown in bold. 4.2.3 SVM We have used a linear SVM with a tfidf feature representation and L2-norm as implemented in the SVMlight package v6.01 [11].",
                "All default settings were used. 4.2.4 Voted Perceptron Like the SVM, the Voted Perceptron is a kernel-based learning method.",
                "We use the same feature representation and kernel as we have for the SVM, a linear kernel with tfidf-weighting and an L2-norm.",
                "The voted perceptron is an online-learning method that keeps a history of past perceptrons used, as well as a weight signifying how often that perceptron was correct.",
                "With each new training example, a correct classification increases the weight on the current perceptron and an incorrect classification updates the perceptron.",
                "The output of the classifier uses the weights on the perceptra to make a final voted classification.",
                "When used in an offline-manner, multiple passes can be made through the training data.",
                "Both the voted perceptron and the SVM give a solution from the same hypothesis space - in this case, a linear classifier.",
                "Furthermore, it is well-known that the Voted Perceptron increases the margin of the solution after each pass through the training data [10].",
                "Since Cohen et al. [5] obtain worse results using an SVM than a Voted Perceptron with one training iteration, they conclude that the best solution for detecting speech acts may not lie in an area with a large margin.",
                "Because their tasks are highly similar to ours, we employ both classifiers to ensure we are not overlooking a competitive alternative classifier to the SVM for the basic bag-of-words representation. 4.3 Performance Measures To compare the performance of the classification methods, we look at two standard performance measures, F1 and accuracy.",
                "The F1 measure [18, 21] is the harmonic mean of precision and recall where Precision = Correct Positives Predicted Positives and Recall = Correct Positives Actual Positives . 4.4 Experimental Methodology We perform standard 10-fold cross-validation on the set of documents.",
                "For the sentence-level approach, all sentences in a document are either entirely in the training set or entirely in the test set for each fold.",
                "For significance tests, we use a two-tailed t-test [21] to compare the values obtained during each cross-validation fold with a p-value of 0.05.",
                "Feature selection was performed using the chi-squared statistic.",
                "Different levels of feature selection were considered for each classifier.",
                "Each of the following number of features was tried: 10, 25, 50, 100, 250, 750, 1000, 2000, 4000.",
                "There are approximately 4700 unigram tokens without feature selection.",
                "In order to choose the number of features to use for each classifier, we perform nested cross-validation and choose the settings that yield the optimal document-level F1 for that classifier.",
                "For this study, only the body of each e-mail message was used.",
                "Feature selection is always applied to all candidate features.",
                "That is, for the n-gram representation, the n-grams and position features are also subject to removal by the feature selection method. 4.5 Results The results for document-level classification are given in Table 3.",
                "The primary hypothesis we are concerned with is that n-grams are critical for this task; if this is true, we expect to see a significant gap in performance between the document-level classifiers that use n-grams (denoted Document Ngram) and those using only unigram features (denoted Document Unigram).",
                "Examining Table 3, we observe that this is indeed the case for every classifier except na¨ıve Bayes.",
                "This difference in performance produced by the n-gram representation is statistically significant for each classifier except for na¨ıve Bayes and the accuracy metric for kNN (see Table 4).",
                "Na¨ıve Bayes poor performance with the n-gram representation is not surprising since the bag-of-n-grams causes excessive doublecounting as mentioned in Section 3.2.1; however, na¨ıve Bayes is not hurt at the sentence-level because the sparse examples provide few chances for agglomerative effects of double counting.",
                "In either case, when a language-modeling approach is desired, modeling the n-grams directly would be preferable to na¨ıve Bayes.",
                "More importantly for the n-gram hypothesis, the n-grams lead to the best document-level classifier performance as well.",
                "As would be expected, the difference between the sentence-level n-gram representation and unigram representation is small.",
                "This is because the window of text is so small that the unigram representation, when done at the sentence-level, implicitly picks up on the power of the n-grams.",
                "Further improvement would signify that the order of the words matter even when only considering a small sentence-size window.",
                "Therefore, the finer-grained sentence-level judgments allows a unigram representation to succeed but only when performed in a small window - behaving as an n-gram representation for all practical purposes.",
                "Document Winner Sentence Winner kNN Ngram Ngram na¨ıve Bayes Unigram Ngram SVM Ngram† Ngram Voted Perceptron Ngram† Ngram Table 4: Significance results for n-grams versus unigrams for document detection using document-level and sentence-level classifiers.",
                "When the F1 result is statistically significant, it is shown in bold.",
                "When the accuracy result is significant, it is shown with a † .",
                "F1 Winner Accuracy Winner kNN Sentence Sentence na¨ıve Bayes Sentence Sentence SVM Sentence Sentence Voted Perceptron Sentence Document Table 5: Significance results for sentence-level classifiers vs. document-level classifiers for the document detection problem.",
                "When the result is statistically significant, it is shown in bold.",
                "Further highlighting the improvement from finer-grained judgments and n-grams, Figure 3 graphically depicts the edge the SVM sentence-level classifier has over the standard bag-of-words approach with a precision-recall curve.",
                "In the high precision area of the graph, the consistent edge of the sentence-level classifier is rather impressive - continuing at precision 1 out to 0.1 recall.",
                "This would mean that a tenth of the users action-items would be placed at the top of their action-item sorted inbox.",
                "Additionally, the large separation at the top right of the curves corresponds to the area where the optimal F1 occurs for each classifier, agreeing with the large improvement from 0.6904 to 0.7682 in F1 score.",
                "Considering the relative unexplored nature of classification at the sentence-level, this gives great hope for further increases in performance.",
                "Accuracy F1 Unigram Ngram Unigram Ngram kNN 0.9519 0.9536 0.6540 0.6686 na¨ıve Bayes 0.9419 0.9550 0.6176 0.6676 SVM 0.9559 0.9579 0.6271 0.6672 Voted Perceptron 0.8895 0.9247 0.3744 0.5164 Table 6: Performance of the Sentence-Level Classifiers at Sentence Detection Although Cohen et al. [5] observed that the Voted Perceptron with a single training iteration outperformed SVM in a set of similar tasks, we see no such behavior here.",
                "This further strengthens the evidence that an alternate classifier with the bag-of-words representation could not reach the same level of performance.",
                "The Voted Perceptron classifier does improve when the number of training iterations are increased, but it is still lower than the SVM classifier.",
                "Sentence detection results are presented in Table 6.",
                "With regard to the sentence detection problem, we note that the F1 measure gives a better feel for the remaining room for improvement in this difficult problem.",
                "That is, unlike document detection where actionitem documents are fairly common, action-item sentences are very rare.",
                "Thus, as in other text problems, the accuracy numbers are deceptively high sheerly because of the default accuracy attainable by always predicting no.",
                "Although, the results here are significantly above-random, it is unclear what level of performance is necessary for sentence detection to be useful in and of itself and not simply as a means to document ranking and classification.",
                "Figure 4: Users find action-items quicker when assisted by a classification system.",
                "Finally, when considering a new type of classification task, one of the most basic questions is whether an accurate classifier built for the task can have an impact on the end-user.",
                "In order to demonstrate the impact this task can have on e-mail users, we conducted a user study using an earlier less-accurate version of the sentence classifier - where instead of using just a single sentence, a threesentence windowed-approach was used.",
                "There were three distinct sets of e-mail in which users had to find action-items.",
                "These sets were either presented in a random order (Unordered), ordered by the classifier (Ordered), or ordered by the classifier and with the 0.4 0.5 0.6 0.7 0.8 0.9 1 0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1 Precision Recall Action-Item Detection SVM Performance (Post Model Selection) Document Unigram Sentence Ngram Figure 3: Both n-grams and a small prediction window lead to consistent improvements over the standard approach. center sentence in the highest confidence window highlighted (Order+help).",
                "In order to perform fair comparisons between conditions, the overall number of tokens in each message set should be approximately equal; that is, the cognitive reading load should be approximately the same before the classifiers reordering.",
                "Additionally, users typically show practice effects by improving at the overall task and thus performing better at later message sets.",
                "This is typically handled by varying the ordering of the sets across users so that the means are comparable.",
                "While omitting further detail, we note the sets were balanced for the total number of tokens and a latin square design was used to balance practice effects.",
                "Figure 4 shows that at intervals of 5, 10, and 15 minutes, users consistently found significantly more action-items when assisted by the classifier, but were most critically aided in the first five minutes.",
                "Although, the classifier consistently aids the users, we did not gain an additional end-user impact by highlighting.",
                "As mentioned above, this might be a result of the large room for improvement that still exists for sentence detection, but anecdotal evidence suggests this might also be a result of how the information is presented to the user rather than the accuracy of sentence detection.",
                "For example, highlighting the wrong sentence near an actual action-item hurts the users trust, but if a vague indicator (e.g., an arrow) points to the approximate area the user is not aware of the near-miss.",
                "Since the user studies used a three sentence window, we believe this played a role as well as sentence detection accuracy. 4.6 Discussion In contrast to problems where n-grams have yielded little difference, we believe their power here stems from the fact that many of the meaningful n-grams for action-items consist of common words, e.g., let me know.",
                "Therefore, the document-level unigram approach cannot gain much leverage, even when modeling their joint probability correctly, since these words will often co-occur in the document but not necessarily in a phrase.",
                "Additionally, action-item detection is distinct from many text classification tasks in that a single sentence can change the class label of the document.",
                "As a result, good classifiers cannot rely on aggregating evidence from a large number of weak indicators across the entire document.",
                "Even though we discarded the header information, examining the top-ranked features at the document-level reveals that many of the features are names or parts of e-mail addresses that occurred in the body and are highly associated with e-mails that tend to contain many or no action-items.",
                "A few examples are terms such as org, bob, and gov.",
                "We note that these features will be sensitive to the particular distribution (senders/receivers) and thus the document-level approach may produce classifiers that transfer less readily to alternate contexts and users at different institutions.",
                "This points out that part of the problem of going beyond bag-of-words may be the methodology, and investigating such properties as learning curves and how well a model transfers may highlight differences in models which appear to have similar performance when tested on the distributions they were trained on.",
                "We are currently investigating whether the sentence-level classifiers do perform better over different test corpora without retraining. 5.",
                "FUTURE WORK While applying text classifiers at the document-level is fairly well-understood, there exists the potential for significantly increasing the performance of the sentence-level classifiers.",
                "Such methods include alternate ways of combining the predictions over each sentence, weightings other than tfidf, which may not be appropriate since sentences are small, better sentence segmentation, and other types of phrasal analysis.",
                "Additionally, named entity tagging, time expressions, etc., seem likely candidates for features that can further improve this task.",
                "We are currently pursuing some of these avenues to see what additional gains these offer.",
                "Finally, it would be interesting to investigate the best methods for combining the document-level and sentence-level classifiers.",
                "Since the simple bag-of-words representation at the document-level leads to a learned model that behaves somewhat like a context-specific prior dependent on the sender/receiver and general topic, a first choice would be to treat it as such when combining probability estimates with the sentence-level classifier.",
                "Such a model might serve as a general example for other problems where bag-of-words can establish a baseline model but richer approaches are needed to achieve performance beyond that baseline. 6.",
                "SUMMARY AND CONCLUSIONS The effectiveness of sentence-level detection argues that labeling at the sentence-level provides significant value.",
                "Further experiments are needed to see how this interacts with the amount of training data available.",
                "Sentence detection that is then agglomerated to document-level detection works surprisingly better given low recall than would be expected with sentence-level items.",
                "This, in turn, indicates that improved sentence segmentation methods could yield further improvements in classification.",
                "In this work, we examined how action-items can be effectively detected in e-mails.",
                "Our empirical analysis has demonstrated that n-grams are of key importance to making the most of documentlevel judgments.",
                "When finer-grained judgments are available, then a standard bag-of-words approach using a small (sentence) window size and automatic segmentation techniques can produce results almost as good as the n-gram based approaches.",
                "Acknowledgments This material is based upon work supported by the Defense Advanced Research Projects Agency (DARPA) under Contract No.",
                "NBCHD030010.",
                "Any opinions, findings and conclusions or recommendations expressed in this material are those of the author(s) and do not necessarily reflect the views of the Defense Advanced Research Projects Agency (DARPA), or the Department of InteriorNational Business Center (DOI-NBC).",
                "We would like to extend our sincerest thanks to Jill Lehman whose efforts in data collection were essential in constructing the corpus, and both Jill and Aaron Steinfeld for their direction of the HCI experiments.",
                "We would also like to thank Django Wexler for constructing and supporting the corpus labeling tools and Curtis Huttenhowers support of the text preprocessing package.",
                "Finally, we gratefully acknowledge Scott Fahlman for his encouragement and useful discussions on this topic. 7.",
                "REFERENCES [1] J. Allan, J. Carbonell, G. Doddington, J. Yamron, and Y. Yang.",
                "Topic detection and tracking pilot study: Final report.",
                "In Proceedings of the DARPA Broadcast News Transcription and Understanding Workshop, Washington, D.C., 1998. [2] C. Apte, F. Damerau, and S. M. Weiss.",
                "Automated learning of decision rules for text categorization.",
                "ACM Transactions on Information Systems, 12(3):233-251, July 1994. [3] J. Carletta.",
                "Assessing agreement on classification tasks: The kappa statistic.",
                "Computational Linguistics, 22(2):249-254, 1996. [4] J. Carroll.",
                "High precision extraction of grammatical relations.",
                "In Proceedings of the 19th International Conference on Computational Linguistics (COLING), pages 134-140, 2002. [5] W. W. Cohen, V. R. Carvalho, and T. M. Mitchell.",
                "Learning to classify email into speech acts.",
                "In EMNLP-2004 (Conference on Empirical Methods in Natural Language Processing), pages 309-316, 2004. [6] S. Corston-Oliver, E. Ringger, M. Gamon, and R. Campbell.",
                "Task-focused summarization of email.",
                "In Text Summarization Branches Out: Proceedings of the ACL-04 Workshop, pages 43-50, 2004. [7] A. Culotta, R. Bekkerman, and A. McCallum.",
                "Extracting social networks and contact information from email and the web.",
                "In CEAS-2004 (Conference on Email and Anti-Spam), Mountain View, CA, July 2004. [8] L. Devroye, L. Gy¨orfi, and G. Lugosi.",
                "A Probabilistic Theory of Pattern Recognition.",
                "Springer-Verlag, New York, NY, 1996. [9] S. T. Dumais, J. Platt, D. Heckerman, and M. Sahami.",
                "Inductive learning algorithms and representations for text categorization.",
                "In CIKM 98, Proceedings of the 7th ACM Conference on Information and Knowledge Management, pages 148-155, 1998. [10] Y. Freund and R. Schapire.",
                "Large margin classification using the perceptron algorithm.",
                "Machine Learning, 37(3):277-296, 1999. [11] T. Joachims.",
                "Making large-scale svm learning practical.",
                "In B. Sch¨olkopf, C. J. Burges, and A. J. Smola, editors, Advances in Kernel Methods - Support Vector Learning, pages 41-56.",
                "MIT Press, 1999. [12] L. S. Larkey.",
                "A patent search and classification system.",
                "In Proceedings of the Fourth ACM Conference on Digital Libraries, pages 179 - 187, 1999. [13] D. D. Lewis.",
                "An evaluation of phrasal and clustered representations on a text categorization task.",
                "In SIGIR 92, Proceedings of the 15th Annual International ACM Conference on Research and Development in Information Retrieval, pages 37-50, 1992. [14] Y. Liu, J. Carbonell, and R. Jin.",
                "A pairwise ensemble approach for accurate genre classification.",
                "In Proceedings of the European Conference on Machine Learning (ECML), 2003. [15] Y. Liu, R. Yan, R. Jin, and J. Carbonell.",
                "A comparison study of kernels for multi-label text classification using category association.",
                "In The Twenty-first International Conference on Machine Learning (ICML), 2004. [16] A. McCallum and K. Nigam.",
                "A comparison of event models for naive bayes text classification.",
                "In Working Notes of AAAI 98 (The 15th National Conference on Artificial Intelligence), Workshop on Learning for Text Categorization, pages 41-48, 1998.",
                "TR WS-98-05. [17] F. Sebastiani.",
                "Machine learning in automated text categorization.",
                "ACM Computing Surveys, 34(1):1-47, March 2002. [18] C. J. van Rijsbergen.",
                "Information Retrieval.",
                "Butterworths, London, 1979. [19] Y. Yang.",
                "An evaluation of statistical approaches to text categorization.",
                "Information Retrieval, 1(1/2):67-88, 1999. [20] Y. Yang, J. Carbonell, R. Brown, T. Pierce, B. T. Archibald, and X. Liu.",
                "Learning approaches to topic detection and tracking.",
                "IEEE EXPERT, Special Issue on Applications of Intelligent Information Retrieval, 1999. [21] Y. Yang and X. Liu.",
                "A re-examination of text categorization methods.",
                "In SIGIR 99, Proceedings of the 22nd Annual International ACM Conference on Research and Development in Information Retrieval, pages 42-49, 1999. [22] Y. Yang, J. Zhang, J. Carbonell, and C. Jin.",
                "Topic-conditioned novelty detection.",
                "In Proceedings of the ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, July 2002."
            ],
            "original_annotated_samples": [],
            "translated_annotated_samples": [
                "Esta tarea de clasificación es muy similar a la nuestra, excepto que no consideran que las \"preguntas de hecho simples\" pertenezcan a esta categoría."
            ],
            "translated_text": "",
            "candidates": [
                "Pregunta fáctica simple",
                "preguntas de hecho simples"
            ],
            "error": []
        },
        "document detection": {
            "translated_key": "Detección de documentos",
            "is_in_text": true,
            "original_annotated_sentences": [
                "Feature Representation for Effective Action-Item Detection Paul N. Bennett Computer Science Department Carnegie Mellon University Pittsburgh, PA 15213 pbennett+@cs.cmu.edu Jaime Carbonell Language Technologies Institute.",
                "Carnegie Mellon University Pittsburgh, PA 15213 jgc+@cs.cmu.edu ABSTRACT E-mail users face an ever-growing challenge in managing their inboxes due to the growing centrality of email in the workplace for task assignment, action requests, and other roles beyond information dissemination.",
                "Whereas Information Retrieval and Machine Learning techniques are gaining initial acceptance in spam filtering and automated folder assignment, this paper reports on a new task: automated action-item detection, in order to flag emails that require responses, and to highlight the specific passage(s) indicating the request(s) for action.",
                "Unlike standard topic-driven text classification, action-item detection requires inferring the senders intent, and as such responds less well to pure bag-of-words classification.",
                "However, using enriched feature sets, such as n-grams (up to n=4) with chi-squared feature selection, and contextual cues for action-item location improve performance by up to 10% over unigrams, using in both cases state of the art classifiers such as SVMs with automated model selection via embedded cross-validation.",
                "Categories and Subject Descriptors H.3.3 [Information Storage and Retrieval]: Information Search and Retrieval; I.2.6 [Artificial Intelligence]: Learning; I.5.4 [Pattern Recognition]: Applications General Terms Experimentation 1.",
                "INTRODUCTION E-mail users are facing an increasingly difficult task of managing their inboxes in the face of mounting challenges that result from rising e-mail usage.",
                "This includes prioritizing e-mails over a range of sources from business partners to family members, filtering and reducing junk e-mail, and quickly managing requests that demand From: Henry Hutchins <hhutchins@innovative.company.com> To: Sara Smith; Joe Johnson; William Woolings Subject: meeting with prospective customers Sent: Fri 12/10/2005 8:08 AM Hi All, Id like to remind all of you that the group from GRTY will be visiting us next Friday at 4:30 p.m.",
                "The current schedule looks like this: + 9:30 a.m.",
                "Informal Breakfast and Discussion in Cafeteria + 10:30 a.m. Company Overview + 11:00 a.m.",
                "Individual Meetings (Continue Over Lunch) + 2:00 p.m. Tour of Facilities + 3:00 p.m.",
                "Sales Pitch In order to have this go off smoothly, I would like to practice the presentation well in advance.",
                "As a result, I will need each of your parts by Wednesday.",
                "Keep up the good work! -Henry Figure 1: An E-mail with emphasized Action-Item, an explicit request that requires the recipients attention or action. the receivers attention or action.",
                "Automated action-item detection targets the third of these problems by attempting to detect which e-mails require an action or response with information, and within those e-mails, attempting to highlight the sentence (or other passage length) that directly indicates the action request.",
                "Such a detection system can be used as one part of an e-mail agent which would assist a user in processing important e-mails quicker than would have been possible without the agent.",
                "We view action-item detection as one necessary component of a successful e-mail agent which would perform spam detection, action-item detection, topic classification and priority ranking, among other functions.",
                "The utility of such a detector can manifest as a method of prioritizing e-mails according to task-oriented criteria other than the standard ones of topic and sender or as a means of ensuring that the email user hasnt dropped the proverbial ball by forgetting to address an action request.",
                "Action-item detection differs from standard text classification in two important ways.",
                "First, the user is interested both in detecting whether an email contains action items and in locating exactly where these action item requests are contained within the email body.",
                "In contrast, standard text categorization merely assigns a topic label to each text, whether that label corresponds to an e-mail folder or a controlled indexing vocabulary [12, 15, 22].",
                "Second, action-item detection attempts to recover the email senders intent - whether she means to elicit response or action on the part of the receiver; note that for this task, classifiers using only unigrams as features do not perform optimally, as evidenced in our results below.",
                "Instead we find that we need more information-laden features such as higher-order n-grams.",
                "Text categorization by topic, on the other hand, works very well using just individual words as features [2, 9, 13, 17].",
                "In fact, genre-classification, which one would think may require more than a bag-of-words approach, also works quite well using just unigram features [14].",
                "Topic detection and tracking (TDT), also works well with unigram feature sets [1, 20].",
                "We believe that action-item detection is one of the first clear instances of an IR-related task where we must move beyond bag-of-words to achieve high performance, albeit not too far, as bag-of-n-grams seem to suffice.",
                "We first review related work for similar text classification problems such as e-mail priority ranking and speech act identification.",
                "Then we more formally define the action-item detection problem, discuss the aspects that distinguish it from more common problems like topic classification, and highlight the challenges in constructing systems that can perform well at the sentence and document level.",
                "From there, we move to a discussion of feature representation and selection techniques appropriate for this problem and how standard text classification approaches can be adapted to smoothly move from the sentence-level detection problem to the documentlevel classification problem.",
                "We then conduct an empirical analysis that helps us determine the effectiveness of our feature extraction procedures as well as establish baselines for a number of classification algorithms on this task.",
                "Finally, we summarize this papers contributions and consider interesting directions for future work. 2.",
                "RELATED WORK Several other researchers have considered very similar text classification tasks.",
                "Cohen et al. [5] describe an ontology of speech acts, such as Propose a Meeting, and attempt to predict when an e-mail contains one of these speech acts.",
                "We consider action-items to be an important specific type of speech act that falls within their more general classification.",
                "While they provide results for several classification methods, their methods only make use of human judgments at the document-level.",
                "In contrast, we consider whether accuracy can be increased by using finer-grained human judgments that mark the specific sentences and phrases of interest.",
                "Corston-Oliver et al. [6] consider detecting items in e-mail to Put on a To-Do List.",
                "This classification task is very similar to ours except they do not consider simple factual questions to belong to this category.",
                "We include questions, but note that not all questions are action-items - some are rhetorical or simply social convention, How are you?.",
                "From a learning perspective, while they make use of judgments at the sentence-level, they do not explicitly compare what if any benefits finer-grained judgments offer.",
                "Additionally, they do not study alternative choices or approaches to the classification task.",
                "Instead, they simply apply a standard SVM at the sentence-level and focus primarily on a linguistic analysis of how the sentence can be logically reformulated before adding it to the task list.",
                "In this study, we examine several alternative classification methods, compare document-level and sentence-level approaches and analyze the machine learning issues implicit in these problems.",
                "Interest in a variety of learning tasks related to e-mail has been rapidly growing in the recent literature.",
                "For example, in a forum dedicated to e-mail learning tasks, Culotta et al. [7] presented methods for learning social networks from e-mail.",
                "In this work, we do not focus on peer relationships; however, such methods could complement those here since peer relationships often influence word choice when requesting an action. 3.",
                "PROBLEM DEFINITION & APPROACH In contrast to previous work, we explicitly focus on the benefits that finer-grained, more costly, sentence-level human judgments offer over coarse-grained document-level judgments.",
                "Additionally, we consider multiple standard text classification approaches and analyze both the quantitative and qualitative differences that arise from taking a document-level vs. a sentence-level approach to classification.",
                "Finally, we focus on the representation necessary to achieve the most competitive performance. 3.1 Problem Definition In order to provide the most benefit to the user, a system would not only detect the document, but it would also indicate the specific sentences in the e-mail which contain the action-items.",
                "Therefore, there are three basic problems: 1.",
                "<br>document detection</br>: Classify a document as to whether or not it contains an action-item. 2.",
                "Document ranking: Rank the documents such that all documents containing action-items occur as high as possible in the ranking. 3.",
                "Sentence detection: Classify each sentence in a document as to whether or not it is an action-item.",
                "As in most Information Retrieval tasks, the weight the evaluation metric should give to precision and recall depends on the nature of the application.",
                "In situations where a user will eventually read all received messages, ranking (e.g., via precision at recall of 1) may be most important since this will help encourage shorter delays in communications between users.",
                "In contrast, high-precision detection at low recall will be of increasing importance when the user is under severe time-pressure and therefore will likely not read all mail.",
                "This can be the case for crisis managers during disaster management.",
                "Finally, sentence detection plays a role in both timepressure situations and simply to alleviate the users required time to gist the message. 3.2 Approach As mentioned above, the labeled data can come in one of two forms: a document-labeling provides a yes/no label for each document as to whether it contains an action-item; a phrase-labeling provides only a yes label for the specific items of interest.",
                "We term the human judgments a phrase-labeling since the users view of the action-item may not correspond with actual sentence boundaries or predicted sentence boundaries.",
                "Obviously, it is straightforward to generate a document-labeling consistent with a phrase-labeling by labeling a document yes if and only if it contains at least one phrase labeled yes.",
                "To train classifiers for this task, we can take several viewpoints related to both the basic problems we have enumerated and the form of the labeled data.",
                "The document-level view treats each e-mail as a learning instance with an associated class-label.",
                "Then, the document can be converted to a feature-value vector and learning progresses as usual.",
                "Applying a document-level classifier to <br>document detection</br> and ranking is straightforward.",
                "In order to apply it to sentence detection, one must make additional steps.",
                "For example, if the classifier predicts a document contains an action-item, then areas of the document that contain a high-concentration of words which the model weights heavily in favor of action-items can be indicated.",
                "The obvious benefit of the document-level approach is that training set collection costs are lower since the user only has to specify whether or not an e-mail contains an action-item and not the specific sentences.",
                "In the sentence-level view, each e-mail is automatically segmented into sentences, and each sentence is treated as a learning instance with an associated class-label.",
                "Since the phrase-labeling provided by the user may not coincide with the automatic segmentation, we must determine what label to assign a partially overlapping sentence when converting it to a learning instance.",
                "Once trained, applying the resulting classifiers to sentence detection is now straightforward, but in order to apply the classifiers to <br>document detection</br> and document ranking, the individual predictions over each sentence must be aggregated in order to make a document-level prediction.",
                "This approach has the potential to benefit from morespecific labels that enable the learner to focus attention on the key sentences instead of having to learn based on data that the majority of the words in the e-mail provide no or little information about class membership. 3.2.1 Features Consider some of the phrases that might constitute part of an action item: would like to know, let me know, as soon as possible, have you.",
                "Each of these phrases consists of common words that occur in many e-mails.",
                "However, when they occur in the same sentence, they are far more indicative of an action-item.",
                "Additionally, order can be important: consider have you versus you have.",
                "Because of this, we posit that n-grams play a larger role in this problem than is typical of problems like topic classification.",
                "Therefore, we consider all n-grams up to size 4.",
                "When using n-grams, if we find an n-gram of size 4 in a segment of text, we can represent the text as just one occurrence of the ngram or as one occurrence of the n-gram and an occurrence of each smaller n-gram contained by it.",
                "We choose the second of these alternatives since this will allow the algorithm itself to smoothly back-off in terms of recall.",
                "Methods such as na¨ıve Bayes may be hurt by such a representation because of double-counting.",
                "Since sentence-ending punctuation can provide information, we retain the terminating punctuation token when it is identifiable.",
                "Additionally, we add a beginning-of-sentence and end-of-sentence token in order to capture patterns that are often indicators at the beginning or end of a sentence.",
                "Assuming proper punctuation, these extra tokens are unnecessary, but often e-mail lacks proper punctuation.",
                "In addition, for the sentence-level classifiers that use ngrams, we additionally code for each sentence a binary encoding of the position of the sentence relative to the document.",
                "This encoding has eight associated features that represent which octile (the first eighth, second eighth, etc.) contains the sentence. 3.2.2 Implementation Details In order to compare the document-level to the sentence-level approach, we compare predictions at the document-level.",
                "We do not address how to use a document-level classifier to make predictions at the sentence-level.",
                "In order to automatically segment the text of the e-mail, we use the RASP statistical parser [4].",
                "Since the automatically segmented sentences may not correspond directly with the phrase-level boundaries, we treat any sentence that contains at least 30% of a marked action-item segment as an action-item.",
                "When evaluating sentencedetection for the sentence-level system, we use these class labels as ground truth.",
                "Since we are not evaluating multiple segmentation approaches, this does not bias any of the methods.",
                "If multiple segmentation systems were under evaluation, one would need to use a metric that matched predicted positive sentences to phrases labeled positive.",
                "The metric would need to punish overly long true predictions as well as too short predictions.",
                "Our criteria for converting to labeled instances implicitly includes both criteria.",
                "Since the segmentation is fixed, an overly long prediction would be predicting yes for many no instances since presumably the extra length corresponds to additional segmented sentences all of which do not contain 30% of action-item.",
                "Likewise, a too short prediction must correspond to a small sentence included in the action-item but not constituting all of the action-item.",
                "Therefore, in order to consider the prediction to be too short, there will be an additional preceding/following sentence that is an action-item where we incorrectly predicted no.",
                "Once a sentence-level classifier has made a prediction for each sentence, we must combine these predictions to make both a document-level prediction and a document-level score.",
                "We use the simple policy of predicting positive when any of the sentences is predicted positive.",
                "In order to produce a document score for ranking, the confidence that the document contains an action-item is: ψ(d) = 1 n(d) s∈d|π(s)=1 ψ(s) if for any s ∈ d, π(s) = 1 1 n(d) maxs∈d ψ(s) o.w. where s is a sentence in document d, π is the classifiers 1/0 prediction, ψ is the score the classifier assigns as its confidence that π(s) = 1, and n(d) is the greater of 1 and the number of (unigram) tokens in the document.",
                "In other words, when any sentence is predicted positive, the document score is the length normalized sum of the sentence scores above threshold.",
                "When no sentence is predicted positive, the document score is the maximum sentence score normalized by length.",
                "As in other text problems, we are more likely to emit false positives for documents with more words or sentences.",
                "Thus we include a length normalization factor. 4.",
                "EXPERIMENTAL ANALYSIS 4.1 The Data Our corpus consists of e-mails obtained from volunteers at an educational institution and cover subjects such as: organizing a research workshop, arranging for job-candidate interviews, publishing proceedings, and talk announcements.",
                "The messages were anonymized by replacing the names of each individual and institution with a pseudonym.1 After attempting to identify and eliminate duplicate e-mails, the corpus contains 744 e-mail messages.",
                "After identity anonymization, the corpora has three basic versions.",
                "Quoted material refers to the text of a previous e-mail that an author often leaves in an e-mail message when responding to the e-mail.",
                "Quoted material can act as noise when learning since it may include action-items from previous messages that are no longer relevant.",
                "To isolate the effects of quoted material, we have three versions of the corpora.",
                "The raw form contains the basic messages.",
                "The auto-stripped version contains the messages after quoted material has been automatically removed.",
                "The hand-stripped version contains the messages after quoted material has been removed by a human.",
                "Additionally, the hand-stripped version has had any xml content and e-mail signatures removed - leaving only the essential content of the message.",
                "The studies reported here are performed with the hand-stripped version.",
                "This allows us to balance the cognitive load in terms of number of tokens that must be read in the user-studies we report - including quoted material would complicate the user studies since some users might skip the material while others read it.",
                "Additionally, ensuring all quoted material is removed 1 We have an even more highly anonymized version of the corpus that can be made available for some outside experimentation.",
                "Please contact the authors for more information on obtaining this data. prevents tainting the cross-validation since otherwise a test item could occur as quoted material in a training document. 4.1.1 Data Labeling Two human annotators labeled each message as to whether or not it contained an action-item.",
                "In addition, they identified each segment of the e-mail which contained an action-item.",
                "A segment is a contiguous section of text selected by the human annotators and may span several sentences or a complete phrase contained in a sentence.",
                "They were instructed that an action item is an explicit request for information that requires the recipients attention or a required action and told to highlight the phrases or sentences that make up the request.",
                "Annotator 1 No Yes Annotator 2 No 391 26 Yes 29 298 Table 1: Agreement of Human Annotators at Document Level Annotator One labeled 324 messages as containing action items.",
                "Annotator Two labeled 327 messages as containing action items.",
                "The agreement of the human annotators is shown in Tables 1 and 2.",
                "The annotators are said to agree at the document-level when both marked the same document as containing no action-items or both marked at least one action-item regardless of whether the text segments were the same.",
                "At the document-level, the annotators agreed 93% of the time.",
                "The kappa statistic [3, 5] is often used to evaluate inter-annotator agreement: κ = A − R 1 − R A is the empirical estimate of the probability of agreement.",
                "R is the empirical estimate of the probability of random agreement given the empirical class priors.",
                "A value close to −1 implies the annotators agree far less often than would be expected randomly, while a value close to 1 means they agree more often than randomly expected.",
                "At the document-level, the kappa statistic for inter-annotator agreement is 0.85.",
                "This value is both strong enough to expect the problem to be learnable and is comparable with results for similar tasks [5, 6].",
                "In order to determine the sentence-level agreement, we use each judgment to create a sentence-corpus with labels as described in Section 3.2.2, then consider the agreement over these sentences.",
                "This allows us to compare agreement over no judgments.",
                "We perform this comparison over the hand-stripped corpus since that eliminates spurious no judgments that would come from including quoted material, etc.",
                "Both annotators were free to label the subject as an action-item, but since neither did, we omit the subject line of the message as well.",
                "This only reduces the number of no agreements.",
                "This leaves 6301 automatically segmented sentences.",
                "At the sentence-level, the annotators agreed 98% of the time, and the kappa statistic for inter-annotator agreement is 0.82.",
                "In order to produce one single set of judgments, the human annotators went through each annotation where there was disagreement and came to a consensus opinion.",
                "The annotators did not collect statistics during this process but anecdotally reported that the majority of disagreements were either cases of clear annotator oversight or different interpretations of conditional statements.",
                "For example, If you would like to keep your job, come to tomorrows meeting implies a required action where If you would like to join Annotator 1 No Yes Annotator 2 No 5810 65 Yes 74 352 Table 2: Agreement of Human Annotators at Sentence Level the football betting pool, come to tomorrows meeting does not.",
                "The first would be an action-item in most contexts while the second would not.",
                "Of course, many conditional statements are not so clearly interpretable.",
                "After reconciling the judgments there are 416 e-mails with no action-items and 328 e-mails containing actionitems.",
                "Of the 328 e-mails containing action-items, 259 messages have one action-item segment; 55 messages have two action-item segments; 11 messages have three action-item segments.",
                "Two messages have four action-item segments, and one message has six action-item segments.",
                "Computing the sentence-level agreement using the reconciled gold standard judgments with each of the annotators individual judgments gives a kappa of 0.89 for Annotator One and a kappa of 0.92 for Annotator Two.",
                "In terms of message characteristics, there were on average 132 content tokens in the body after stripping.",
                "For action-item messages, there were 115.",
                "However, by examining Figure 2 we see the length distributions are nearly identical.",
                "As would be expected for e-mail, it is a long-tailed distribution with about half the messages having more than 60 tokens in the body (this paragraph has 65 tokens). 4.2 Classifiers For this experiment, we have selected a variety of standard text classification algorithms.",
                "In selecting algorithms, we have chosen algorithms that are not only known to work well but which differ along such lines as discriminative vs. generative and lazy vs. eager.",
                "We have done this in order to provide both a competitive and thorough sampling of learning methods for the task at hand.",
                "This is important since it is easy to improve a strawman classifier by introducing a new representation.",
                "By thoroughly sampling alternative classifier choices we demonstrate that representation improvements over bag-of-words are not due to using the information in the bag-of-words poorly. 4.2.1 kNN We employ a standard variant of the k-nearest neighbor algorithm used in text classification, kNN with s-cut score thresholding [19].",
                "We use a tfidf-weighting of the terms with a distanceweighted vote of the neighbors to compute the score before thresholding it.",
                "In order to choose the value of s for thresholding, we perform leave-one-out cross-validation over the training set.",
                "The value of k is set to be 2( log2 N + 1) where N is the number of training points.",
                "This rule for choosing k is theoretically motivated by results which show such a rule converges to the optimal classifier as the number of training points increases [8].",
                "In practice, we have also found it to be a computational convenience that frequently leads to comparable results with numerically optimizing k via a cross-validation procedure. 4.2.2 Na¨ıve Bayes We use a standard multinomial na¨ıve Bayes classifier [16].",
                "In using this classifier, we smoothed word and class probabilities using a Bayesian estimate (with the word prior) and a Laplace m-estimate, respectively. 0 20 40 60 80 100 120 140 160 0 200 400 600 800 1000 1200 1400 NumberofMessages Number of Tokens All Messages Action-Item Messages 0 0.02 0.04 0.06 0.08 0.1 0.12 0.14 0.16 0.18 0.2 0 200 400 600 800 1000 1200 1400 PercentageofMessages Number of Tokens All Messages Action-Item Messages Figure 2: The Histogram (left) and Distribution (right) of Message Length.",
                "A bin size of 20 words was used.",
                "Only tokens in the body after hand-stripping were counted.",
                "After stripping, the majority of words left are usually actual message content.",
                "Classifiers Document Unigram Document Ngram Sentence Unigram Sentence Ngram F1 kNN 0.6670 ± 0.0288 0.7108 ± 0.0699 0.7615 ± 0.0504 0.7790 ± 0.0460 na¨ıve Bayes 0.6572 ± 0.0749 0.6484 ± 0.0513 0.7715 ± 0.0597 0.7777 ± 0.0426 SVM 0.6904 ± 0.0347 0.7428 ± 0.0422 0.7282 ± 0.0698 0.7682 ± 0.0451 Voted Perceptron 0.6288 ± 0.0395 0.6774 ± 0.0422 0.6511 ± 0.0506 0.6798 ± 0.0913 Accuracy kNN 0.7029 ± 0.0659 0.7486 ± 0.0505 0.7972 ± 0.0435 0.8092 ± 0.0352 na¨ıve Bayes 0.6074 ± 0.0651 0.5816 ± 0.1075 0.7863 ± 0.0553 0.8145 ± 0.0268 SVM 0.7595 ± 0.0309 0.7904 ± 0.0349 0.7958 ± 0.0551 0.8173 ± 0.0258 Voted Perceptron 0.6531 ± 0.0390 0.7164 ± 0.0376 0.6413 ± 0.0833 0.7082 ± 0.1032 Table 3: Average Document-Detection Performance during Cross-Validation for Each Method and the Sample Standard Deviation (Sn−1) in italics.",
                "The best performance for each classifier is shown in bold. 4.2.3 SVM We have used a linear SVM with a tfidf feature representation and L2-norm as implemented in the SVMlight package v6.01 [11].",
                "All default settings were used. 4.2.4 Voted Perceptron Like the SVM, the Voted Perceptron is a kernel-based learning method.",
                "We use the same feature representation and kernel as we have for the SVM, a linear kernel with tfidf-weighting and an L2-norm.",
                "The voted perceptron is an online-learning method that keeps a history of past perceptrons used, as well as a weight signifying how often that perceptron was correct.",
                "With each new training example, a correct classification increases the weight on the current perceptron and an incorrect classification updates the perceptron.",
                "The output of the classifier uses the weights on the perceptra to make a final voted classification.",
                "When used in an offline-manner, multiple passes can be made through the training data.",
                "Both the voted perceptron and the SVM give a solution from the same hypothesis space - in this case, a linear classifier.",
                "Furthermore, it is well-known that the Voted Perceptron increases the margin of the solution after each pass through the training data [10].",
                "Since Cohen et al. [5] obtain worse results using an SVM than a Voted Perceptron with one training iteration, they conclude that the best solution for detecting speech acts may not lie in an area with a large margin.",
                "Because their tasks are highly similar to ours, we employ both classifiers to ensure we are not overlooking a competitive alternative classifier to the SVM for the basic bag-of-words representation. 4.3 Performance Measures To compare the performance of the classification methods, we look at two standard performance measures, F1 and accuracy.",
                "The F1 measure [18, 21] is the harmonic mean of precision and recall where Precision = Correct Positives Predicted Positives and Recall = Correct Positives Actual Positives . 4.4 Experimental Methodology We perform standard 10-fold cross-validation on the set of documents.",
                "For the sentence-level approach, all sentences in a document are either entirely in the training set or entirely in the test set for each fold.",
                "For significance tests, we use a two-tailed t-test [21] to compare the values obtained during each cross-validation fold with a p-value of 0.05.",
                "Feature selection was performed using the chi-squared statistic.",
                "Different levels of feature selection were considered for each classifier.",
                "Each of the following number of features was tried: 10, 25, 50, 100, 250, 750, 1000, 2000, 4000.",
                "There are approximately 4700 unigram tokens without feature selection.",
                "In order to choose the number of features to use for each classifier, we perform nested cross-validation and choose the settings that yield the optimal document-level F1 for that classifier.",
                "For this study, only the body of each e-mail message was used.",
                "Feature selection is always applied to all candidate features.",
                "That is, for the n-gram representation, the n-grams and position features are also subject to removal by the feature selection method. 4.5 Results The results for document-level classification are given in Table 3.",
                "The primary hypothesis we are concerned with is that n-grams are critical for this task; if this is true, we expect to see a significant gap in performance between the document-level classifiers that use n-grams (denoted Document Ngram) and those using only unigram features (denoted Document Unigram).",
                "Examining Table 3, we observe that this is indeed the case for every classifier except na¨ıve Bayes.",
                "This difference in performance produced by the n-gram representation is statistically significant for each classifier except for na¨ıve Bayes and the accuracy metric for kNN (see Table 4).",
                "Na¨ıve Bayes poor performance with the n-gram representation is not surprising since the bag-of-n-grams causes excessive doublecounting as mentioned in Section 3.2.1; however, na¨ıve Bayes is not hurt at the sentence-level because the sparse examples provide few chances for agglomerative effects of double counting.",
                "In either case, when a language-modeling approach is desired, modeling the n-grams directly would be preferable to na¨ıve Bayes.",
                "More importantly for the n-gram hypothesis, the n-grams lead to the best document-level classifier performance as well.",
                "As would be expected, the difference between the sentence-level n-gram representation and unigram representation is small.",
                "This is because the window of text is so small that the unigram representation, when done at the sentence-level, implicitly picks up on the power of the n-grams.",
                "Further improvement would signify that the order of the words matter even when only considering a small sentence-size window.",
                "Therefore, the finer-grained sentence-level judgments allows a unigram representation to succeed but only when performed in a small window - behaving as an n-gram representation for all practical purposes.",
                "Document Winner Sentence Winner kNN Ngram Ngram na¨ıve Bayes Unigram Ngram SVM Ngram† Ngram Voted Perceptron Ngram† Ngram Table 4: Significance results for n-grams versus unigrams for <br>document detection</br> using document-level and sentence-level classifiers.",
                "When the F1 result is statistically significant, it is shown in bold.",
                "When the accuracy result is significant, it is shown with a † .",
                "F1 Winner Accuracy Winner kNN Sentence Sentence na¨ıve Bayes Sentence Sentence SVM Sentence Sentence Voted Perceptron Sentence Document Table 5: Significance results for sentence-level classifiers vs. document-level classifiers for the <br>document detection</br> problem.",
                "When the result is statistically significant, it is shown in bold.",
                "Further highlighting the improvement from finer-grained judgments and n-grams, Figure 3 graphically depicts the edge the SVM sentence-level classifier has over the standard bag-of-words approach with a precision-recall curve.",
                "In the high precision area of the graph, the consistent edge of the sentence-level classifier is rather impressive - continuing at precision 1 out to 0.1 recall.",
                "This would mean that a tenth of the users action-items would be placed at the top of their action-item sorted inbox.",
                "Additionally, the large separation at the top right of the curves corresponds to the area where the optimal F1 occurs for each classifier, agreeing with the large improvement from 0.6904 to 0.7682 in F1 score.",
                "Considering the relative unexplored nature of classification at the sentence-level, this gives great hope for further increases in performance.",
                "Accuracy F1 Unigram Ngram Unigram Ngram kNN 0.9519 0.9536 0.6540 0.6686 na¨ıve Bayes 0.9419 0.9550 0.6176 0.6676 SVM 0.9559 0.9579 0.6271 0.6672 Voted Perceptron 0.8895 0.9247 0.3744 0.5164 Table 6: Performance of the Sentence-Level Classifiers at Sentence Detection Although Cohen et al. [5] observed that the Voted Perceptron with a single training iteration outperformed SVM in a set of similar tasks, we see no such behavior here.",
                "This further strengthens the evidence that an alternate classifier with the bag-of-words representation could not reach the same level of performance.",
                "The Voted Perceptron classifier does improve when the number of training iterations are increased, but it is still lower than the SVM classifier.",
                "Sentence detection results are presented in Table 6.",
                "With regard to the sentence detection problem, we note that the F1 measure gives a better feel for the remaining room for improvement in this difficult problem.",
                "That is, unlike <br>document detection</br> where actionitem documents are fairly common, action-item sentences are very rare.",
                "Thus, as in other text problems, the accuracy numbers are deceptively high sheerly because of the default accuracy attainable by always predicting no.",
                "Although, the results here are significantly above-random, it is unclear what level of performance is necessary for sentence detection to be useful in and of itself and not simply as a means to document ranking and classification.",
                "Figure 4: Users find action-items quicker when assisted by a classification system.",
                "Finally, when considering a new type of classification task, one of the most basic questions is whether an accurate classifier built for the task can have an impact on the end-user.",
                "In order to demonstrate the impact this task can have on e-mail users, we conducted a user study using an earlier less-accurate version of the sentence classifier - where instead of using just a single sentence, a threesentence windowed-approach was used.",
                "There were three distinct sets of e-mail in which users had to find action-items.",
                "These sets were either presented in a random order (Unordered), ordered by the classifier (Ordered), or ordered by the classifier and with the 0.4 0.5 0.6 0.7 0.8 0.9 1 0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1 Precision Recall Action-Item Detection SVM Performance (Post Model Selection) Document Unigram Sentence Ngram Figure 3: Both n-grams and a small prediction window lead to consistent improvements over the standard approach. center sentence in the highest confidence window highlighted (Order+help).",
                "In order to perform fair comparisons between conditions, the overall number of tokens in each message set should be approximately equal; that is, the cognitive reading load should be approximately the same before the classifiers reordering.",
                "Additionally, users typically show practice effects by improving at the overall task and thus performing better at later message sets.",
                "This is typically handled by varying the ordering of the sets across users so that the means are comparable.",
                "While omitting further detail, we note the sets were balanced for the total number of tokens and a latin square design was used to balance practice effects.",
                "Figure 4 shows that at intervals of 5, 10, and 15 minutes, users consistently found significantly more action-items when assisted by the classifier, but were most critically aided in the first five minutes.",
                "Although, the classifier consistently aids the users, we did not gain an additional end-user impact by highlighting.",
                "As mentioned above, this might be a result of the large room for improvement that still exists for sentence detection, but anecdotal evidence suggests this might also be a result of how the information is presented to the user rather than the accuracy of sentence detection.",
                "For example, highlighting the wrong sentence near an actual action-item hurts the users trust, but if a vague indicator (e.g., an arrow) points to the approximate area the user is not aware of the near-miss.",
                "Since the user studies used a three sentence window, we believe this played a role as well as sentence detection accuracy. 4.6 Discussion In contrast to problems where n-grams have yielded little difference, we believe their power here stems from the fact that many of the meaningful n-grams for action-items consist of common words, e.g., let me know.",
                "Therefore, the document-level unigram approach cannot gain much leverage, even when modeling their joint probability correctly, since these words will often co-occur in the document but not necessarily in a phrase.",
                "Additionally, action-item detection is distinct from many text classification tasks in that a single sentence can change the class label of the document.",
                "As a result, good classifiers cannot rely on aggregating evidence from a large number of weak indicators across the entire document.",
                "Even though we discarded the header information, examining the top-ranked features at the document-level reveals that many of the features are names or parts of e-mail addresses that occurred in the body and are highly associated with e-mails that tend to contain many or no action-items.",
                "A few examples are terms such as org, bob, and gov.",
                "We note that these features will be sensitive to the particular distribution (senders/receivers) and thus the document-level approach may produce classifiers that transfer less readily to alternate contexts and users at different institutions.",
                "This points out that part of the problem of going beyond bag-of-words may be the methodology, and investigating such properties as learning curves and how well a model transfers may highlight differences in models which appear to have similar performance when tested on the distributions they were trained on.",
                "We are currently investigating whether the sentence-level classifiers do perform better over different test corpora without retraining. 5.",
                "FUTURE WORK While applying text classifiers at the document-level is fairly well-understood, there exists the potential for significantly increasing the performance of the sentence-level classifiers.",
                "Such methods include alternate ways of combining the predictions over each sentence, weightings other than tfidf, which may not be appropriate since sentences are small, better sentence segmentation, and other types of phrasal analysis.",
                "Additionally, named entity tagging, time expressions, etc., seem likely candidates for features that can further improve this task.",
                "We are currently pursuing some of these avenues to see what additional gains these offer.",
                "Finally, it would be interesting to investigate the best methods for combining the document-level and sentence-level classifiers.",
                "Since the simple bag-of-words representation at the document-level leads to a learned model that behaves somewhat like a context-specific prior dependent on the sender/receiver and general topic, a first choice would be to treat it as such when combining probability estimates with the sentence-level classifier.",
                "Such a model might serve as a general example for other problems where bag-of-words can establish a baseline model but richer approaches are needed to achieve performance beyond that baseline. 6.",
                "SUMMARY AND CONCLUSIONS The effectiveness of sentence-level detection argues that labeling at the sentence-level provides significant value.",
                "Further experiments are needed to see how this interacts with the amount of training data available.",
                "Sentence detection that is then agglomerated to document-level detection works surprisingly better given low recall than would be expected with sentence-level items.",
                "This, in turn, indicates that improved sentence segmentation methods could yield further improvements in classification.",
                "In this work, we examined how action-items can be effectively detected in e-mails.",
                "Our empirical analysis has demonstrated that n-grams are of key importance to making the most of documentlevel judgments.",
                "When finer-grained judgments are available, then a standard bag-of-words approach using a small (sentence) window size and automatic segmentation techniques can produce results almost as good as the n-gram based approaches.",
                "Acknowledgments This material is based upon work supported by the Defense Advanced Research Projects Agency (DARPA) under Contract No.",
                "NBCHD030010.",
                "Any opinions, findings and conclusions or recommendations expressed in this material are those of the author(s) and do not necessarily reflect the views of the Defense Advanced Research Projects Agency (DARPA), or the Department of InteriorNational Business Center (DOI-NBC).",
                "We would like to extend our sincerest thanks to Jill Lehman whose efforts in data collection were essential in constructing the corpus, and both Jill and Aaron Steinfeld for their direction of the HCI experiments.",
                "We would also like to thank Django Wexler for constructing and supporting the corpus labeling tools and Curtis Huttenhowers support of the text preprocessing package.",
                "Finally, we gratefully acknowledge Scott Fahlman for his encouragement and useful discussions on this topic. 7.",
                "REFERENCES [1] J. Allan, J. Carbonell, G. Doddington, J. Yamron, and Y. Yang.",
                "Topic detection and tracking pilot study: Final report.",
                "In Proceedings of the DARPA Broadcast News Transcription and Understanding Workshop, Washington, D.C., 1998. [2] C. Apte, F. Damerau, and S. M. Weiss.",
                "Automated learning of decision rules for text categorization.",
                "ACM Transactions on Information Systems, 12(3):233-251, July 1994. [3] J. Carletta.",
                "Assessing agreement on classification tasks: The kappa statistic.",
                "Computational Linguistics, 22(2):249-254, 1996. [4] J. Carroll.",
                "High precision extraction of grammatical relations.",
                "In Proceedings of the 19th International Conference on Computational Linguistics (COLING), pages 134-140, 2002. [5] W. W. Cohen, V. R. Carvalho, and T. M. Mitchell.",
                "Learning to classify email into speech acts.",
                "In EMNLP-2004 (Conference on Empirical Methods in Natural Language Processing), pages 309-316, 2004. [6] S. Corston-Oliver, E. Ringger, M. Gamon, and R. Campbell.",
                "Task-focused summarization of email.",
                "In Text Summarization Branches Out: Proceedings of the ACL-04 Workshop, pages 43-50, 2004. [7] A. Culotta, R. Bekkerman, and A. McCallum.",
                "Extracting social networks and contact information from email and the web.",
                "In CEAS-2004 (Conference on Email and Anti-Spam), Mountain View, CA, July 2004. [8] L. Devroye, L. Gy¨orfi, and G. Lugosi.",
                "A Probabilistic Theory of Pattern Recognition.",
                "Springer-Verlag, New York, NY, 1996. [9] S. T. Dumais, J. Platt, D. Heckerman, and M. Sahami.",
                "Inductive learning algorithms and representations for text categorization.",
                "In CIKM 98, Proceedings of the 7th ACM Conference on Information and Knowledge Management, pages 148-155, 1998. [10] Y. Freund and R. Schapire.",
                "Large margin classification using the perceptron algorithm.",
                "Machine Learning, 37(3):277-296, 1999. [11] T. Joachims.",
                "Making large-scale svm learning practical.",
                "In B. Sch¨olkopf, C. J. Burges, and A. J. Smola, editors, Advances in Kernel Methods - Support Vector Learning, pages 41-56.",
                "MIT Press, 1999. [12] L. S. Larkey.",
                "A patent search and classification system.",
                "In Proceedings of the Fourth ACM Conference on Digital Libraries, pages 179 - 187, 1999. [13] D. D. Lewis.",
                "An evaluation of phrasal and clustered representations on a text categorization task.",
                "In SIGIR 92, Proceedings of the 15th Annual International ACM Conference on Research and Development in Information Retrieval, pages 37-50, 1992. [14] Y. Liu, J. Carbonell, and R. Jin.",
                "A pairwise ensemble approach for accurate genre classification.",
                "In Proceedings of the European Conference on Machine Learning (ECML), 2003. [15] Y. Liu, R. Yan, R. Jin, and J. Carbonell.",
                "A comparison study of kernels for multi-label text classification using category association.",
                "In The Twenty-first International Conference on Machine Learning (ICML), 2004. [16] A. McCallum and K. Nigam.",
                "A comparison of event models for naive bayes text classification.",
                "In Working Notes of AAAI 98 (The 15th National Conference on Artificial Intelligence), Workshop on Learning for Text Categorization, pages 41-48, 1998.",
                "TR WS-98-05. [17] F. Sebastiani.",
                "Machine learning in automated text categorization.",
                "ACM Computing Surveys, 34(1):1-47, March 2002. [18] C. J. van Rijsbergen.",
                "Information Retrieval.",
                "Butterworths, London, 1979. [19] Y. Yang.",
                "An evaluation of statistical approaches to text categorization.",
                "Information Retrieval, 1(1/2):67-88, 1999. [20] Y. Yang, J. Carbonell, R. Brown, T. Pierce, B. T. Archibald, and X. Liu.",
                "Learning approaches to topic detection and tracking.",
                "IEEE EXPERT, Special Issue on Applications of Intelligent Information Retrieval, 1999. [21] Y. Yang and X. Liu.",
                "A re-examination of text categorization methods.",
                "In SIGIR 99, Proceedings of the 22nd Annual International ACM Conference on Research and Development in Information Retrieval, pages 42-49, 1999. [22] Y. Yang, J. Zhang, J. Carbonell, and C. Jin.",
                "Topic-conditioned novelty detection.",
                "In Proceedings of the ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, July 2002."
            ],
            "original_annotated_samples": [],
            "translated_annotated_samples": [
                "\"Detección de documentos\": Clasifique un documento sobre si contiene o no un Item de acción.2.",
                "Aplicar un clasificador a nivel de documento a \"Detección de documentos\" y la clasificación es sencilla.",
                "Una vez capacitado, la aplicación de los clasificadores resultantes a la detección de oraciones ahora es sencillo, pero para aplicar los clasificadores a la \"detección de documentos\" y la clasificación de documentos, las predicciones individuales sobre cada oración deben agregarse para hacer una predicción a nivel de documento.",
                "Ganador del documento Ganador de la oración Knn Ngram ngram na¨ıve bayes unigram ngram svm ngram † ngram votado perceptron ngram † ngram Tabla 4: Resultados de significancia para n-gram versus unigrams para \"detección de documentos\" utilizando clasificadores de nivel de documento y a nivel de a nivel de oración.",
                "F1 Ganador de precisión Ganador KNN SENCIÓN SENCIÓN NA ¨ıve Bayes Sentencia Sentencia SVM Sentencia Sentencia Votado Documento de oración de percepción Tabla 5: Resultados de significancia para clasificadores a nivel de oración frente a clasificadores a nivel de documento para el problema de \"detección de documentos\".",
                "Es decir, a diferencia de la \"detección de documentos\" donde los documentos de ActionItem son bastante comunes, las oraciones de acción de acción son muy raras."
            ],
            "translated_text": "",
            "candidates": [
                "Detección de documentos",
                "Detección de documentos",
                "detección de documentos",
                "Detección de documentos",
                "detección de documentos",
                "detección de documentos",
                "detección de documentos",
                "detección de documentos",
                "detección de documentos",
                "detección de documentos",
                "detección de documentos",
                "detección de documentos"
            ],
            "error": []
        },
        "document ranking": {
            "translated_key": "Ranking de documentos",
            "is_in_text": true,
            "original_annotated_sentences": [
                "Feature Representation for Effective Action-Item Detection Paul N. Bennett Computer Science Department Carnegie Mellon University Pittsburgh, PA 15213 pbennett+@cs.cmu.edu Jaime Carbonell Language Technologies Institute.",
                "Carnegie Mellon University Pittsburgh, PA 15213 jgc+@cs.cmu.edu ABSTRACT E-mail users face an ever-growing challenge in managing their inboxes due to the growing centrality of email in the workplace for task assignment, action requests, and other roles beyond information dissemination.",
                "Whereas Information Retrieval and Machine Learning techniques are gaining initial acceptance in spam filtering and automated folder assignment, this paper reports on a new task: automated action-item detection, in order to flag emails that require responses, and to highlight the specific passage(s) indicating the request(s) for action.",
                "Unlike standard topic-driven text classification, action-item detection requires inferring the senders intent, and as such responds less well to pure bag-of-words classification.",
                "However, using enriched feature sets, such as n-grams (up to n=4) with chi-squared feature selection, and contextual cues for action-item location improve performance by up to 10% over unigrams, using in both cases state of the art classifiers such as SVMs with automated model selection via embedded cross-validation.",
                "Categories and Subject Descriptors H.3.3 [Information Storage and Retrieval]: Information Search and Retrieval; I.2.6 [Artificial Intelligence]: Learning; I.5.4 [Pattern Recognition]: Applications General Terms Experimentation 1.",
                "INTRODUCTION E-mail users are facing an increasingly difficult task of managing their inboxes in the face of mounting challenges that result from rising e-mail usage.",
                "This includes prioritizing e-mails over a range of sources from business partners to family members, filtering and reducing junk e-mail, and quickly managing requests that demand From: Henry Hutchins <hhutchins@innovative.company.com> To: Sara Smith; Joe Johnson; William Woolings Subject: meeting with prospective customers Sent: Fri 12/10/2005 8:08 AM Hi All, Id like to remind all of you that the group from GRTY will be visiting us next Friday at 4:30 p.m.",
                "The current schedule looks like this: + 9:30 a.m.",
                "Informal Breakfast and Discussion in Cafeteria + 10:30 a.m. Company Overview + 11:00 a.m.",
                "Individual Meetings (Continue Over Lunch) + 2:00 p.m. Tour of Facilities + 3:00 p.m.",
                "Sales Pitch In order to have this go off smoothly, I would like to practice the presentation well in advance.",
                "As a result, I will need each of your parts by Wednesday.",
                "Keep up the good work! -Henry Figure 1: An E-mail with emphasized Action-Item, an explicit request that requires the recipients attention or action. the receivers attention or action.",
                "Automated action-item detection targets the third of these problems by attempting to detect which e-mails require an action or response with information, and within those e-mails, attempting to highlight the sentence (or other passage length) that directly indicates the action request.",
                "Such a detection system can be used as one part of an e-mail agent which would assist a user in processing important e-mails quicker than would have been possible without the agent.",
                "We view action-item detection as one necessary component of a successful e-mail agent which would perform spam detection, action-item detection, topic classification and priority ranking, among other functions.",
                "The utility of such a detector can manifest as a method of prioritizing e-mails according to task-oriented criteria other than the standard ones of topic and sender or as a means of ensuring that the email user hasnt dropped the proverbial ball by forgetting to address an action request.",
                "Action-item detection differs from standard text classification in two important ways.",
                "First, the user is interested both in detecting whether an email contains action items and in locating exactly where these action item requests are contained within the email body.",
                "In contrast, standard text categorization merely assigns a topic label to each text, whether that label corresponds to an e-mail folder or a controlled indexing vocabulary [12, 15, 22].",
                "Second, action-item detection attempts to recover the email senders intent - whether she means to elicit response or action on the part of the receiver; note that for this task, classifiers using only unigrams as features do not perform optimally, as evidenced in our results below.",
                "Instead we find that we need more information-laden features such as higher-order n-grams.",
                "Text categorization by topic, on the other hand, works very well using just individual words as features [2, 9, 13, 17].",
                "In fact, genre-classification, which one would think may require more than a bag-of-words approach, also works quite well using just unigram features [14].",
                "Topic detection and tracking (TDT), also works well with unigram feature sets [1, 20].",
                "We believe that action-item detection is one of the first clear instances of an IR-related task where we must move beyond bag-of-words to achieve high performance, albeit not too far, as bag-of-n-grams seem to suffice.",
                "We first review related work for similar text classification problems such as e-mail priority ranking and speech act identification.",
                "Then we more formally define the action-item detection problem, discuss the aspects that distinguish it from more common problems like topic classification, and highlight the challenges in constructing systems that can perform well at the sentence and document level.",
                "From there, we move to a discussion of feature representation and selection techniques appropriate for this problem and how standard text classification approaches can be adapted to smoothly move from the sentence-level detection problem to the documentlevel classification problem.",
                "We then conduct an empirical analysis that helps us determine the effectiveness of our feature extraction procedures as well as establish baselines for a number of classification algorithms on this task.",
                "Finally, we summarize this papers contributions and consider interesting directions for future work. 2.",
                "RELATED WORK Several other researchers have considered very similar text classification tasks.",
                "Cohen et al. [5] describe an ontology of speech acts, such as Propose a Meeting, and attempt to predict when an e-mail contains one of these speech acts.",
                "We consider action-items to be an important specific type of speech act that falls within their more general classification.",
                "While they provide results for several classification methods, their methods only make use of human judgments at the document-level.",
                "In contrast, we consider whether accuracy can be increased by using finer-grained human judgments that mark the specific sentences and phrases of interest.",
                "Corston-Oliver et al. [6] consider detecting items in e-mail to Put on a To-Do List.",
                "This classification task is very similar to ours except they do not consider simple factual questions to belong to this category.",
                "We include questions, but note that not all questions are action-items - some are rhetorical or simply social convention, How are you?.",
                "From a learning perspective, while they make use of judgments at the sentence-level, they do not explicitly compare what if any benefits finer-grained judgments offer.",
                "Additionally, they do not study alternative choices or approaches to the classification task.",
                "Instead, they simply apply a standard SVM at the sentence-level and focus primarily on a linguistic analysis of how the sentence can be logically reformulated before adding it to the task list.",
                "In this study, we examine several alternative classification methods, compare document-level and sentence-level approaches and analyze the machine learning issues implicit in these problems.",
                "Interest in a variety of learning tasks related to e-mail has been rapidly growing in the recent literature.",
                "For example, in a forum dedicated to e-mail learning tasks, Culotta et al. [7] presented methods for learning social networks from e-mail.",
                "In this work, we do not focus on peer relationships; however, such methods could complement those here since peer relationships often influence word choice when requesting an action. 3.",
                "PROBLEM DEFINITION & APPROACH In contrast to previous work, we explicitly focus on the benefits that finer-grained, more costly, sentence-level human judgments offer over coarse-grained document-level judgments.",
                "Additionally, we consider multiple standard text classification approaches and analyze both the quantitative and qualitative differences that arise from taking a document-level vs. a sentence-level approach to classification.",
                "Finally, we focus on the representation necessary to achieve the most competitive performance. 3.1 Problem Definition In order to provide the most benefit to the user, a system would not only detect the document, but it would also indicate the specific sentences in the e-mail which contain the action-items.",
                "Therefore, there are three basic problems: 1.",
                "Document detection: Classify a document as to whether or not it contains an action-item. 2.",
                "<br>document ranking</br>: Rank the documents such that all documents containing action-items occur as high as possible in the ranking. 3.",
                "Sentence detection: Classify each sentence in a document as to whether or not it is an action-item.",
                "As in most Information Retrieval tasks, the weight the evaluation metric should give to precision and recall depends on the nature of the application.",
                "In situations where a user will eventually read all received messages, ranking (e.g., via precision at recall of 1) may be most important since this will help encourage shorter delays in communications between users.",
                "In contrast, high-precision detection at low recall will be of increasing importance when the user is under severe time-pressure and therefore will likely not read all mail.",
                "This can be the case for crisis managers during disaster management.",
                "Finally, sentence detection plays a role in both timepressure situations and simply to alleviate the users required time to gist the message. 3.2 Approach As mentioned above, the labeled data can come in one of two forms: a document-labeling provides a yes/no label for each document as to whether it contains an action-item; a phrase-labeling provides only a yes label for the specific items of interest.",
                "We term the human judgments a phrase-labeling since the users view of the action-item may not correspond with actual sentence boundaries or predicted sentence boundaries.",
                "Obviously, it is straightforward to generate a document-labeling consistent with a phrase-labeling by labeling a document yes if and only if it contains at least one phrase labeled yes.",
                "To train classifiers for this task, we can take several viewpoints related to both the basic problems we have enumerated and the form of the labeled data.",
                "The document-level view treats each e-mail as a learning instance with an associated class-label.",
                "Then, the document can be converted to a feature-value vector and learning progresses as usual.",
                "Applying a document-level classifier to document detection and ranking is straightforward.",
                "In order to apply it to sentence detection, one must make additional steps.",
                "For example, if the classifier predicts a document contains an action-item, then areas of the document that contain a high-concentration of words which the model weights heavily in favor of action-items can be indicated.",
                "The obvious benefit of the document-level approach is that training set collection costs are lower since the user only has to specify whether or not an e-mail contains an action-item and not the specific sentences.",
                "In the sentence-level view, each e-mail is automatically segmented into sentences, and each sentence is treated as a learning instance with an associated class-label.",
                "Since the phrase-labeling provided by the user may not coincide with the automatic segmentation, we must determine what label to assign a partially overlapping sentence when converting it to a learning instance.",
                "Once trained, applying the resulting classifiers to sentence detection is now straightforward, but in order to apply the classifiers to document detection and <br>document ranking</br>, the individual predictions over each sentence must be aggregated in order to make a document-level prediction.",
                "This approach has the potential to benefit from morespecific labels that enable the learner to focus attention on the key sentences instead of having to learn based on data that the majority of the words in the e-mail provide no or little information about class membership. 3.2.1 Features Consider some of the phrases that might constitute part of an action item: would like to know, let me know, as soon as possible, have you.",
                "Each of these phrases consists of common words that occur in many e-mails.",
                "However, when they occur in the same sentence, they are far more indicative of an action-item.",
                "Additionally, order can be important: consider have you versus you have.",
                "Because of this, we posit that n-grams play a larger role in this problem than is typical of problems like topic classification.",
                "Therefore, we consider all n-grams up to size 4.",
                "When using n-grams, if we find an n-gram of size 4 in a segment of text, we can represent the text as just one occurrence of the ngram or as one occurrence of the n-gram and an occurrence of each smaller n-gram contained by it.",
                "We choose the second of these alternatives since this will allow the algorithm itself to smoothly back-off in terms of recall.",
                "Methods such as na¨ıve Bayes may be hurt by such a representation because of double-counting.",
                "Since sentence-ending punctuation can provide information, we retain the terminating punctuation token when it is identifiable.",
                "Additionally, we add a beginning-of-sentence and end-of-sentence token in order to capture patterns that are often indicators at the beginning or end of a sentence.",
                "Assuming proper punctuation, these extra tokens are unnecessary, but often e-mail lacks proper punctuation.",
                "In addition, for the sentence-level classifiers that use ngrams, we additionally code for each sentence a binary encoding of the position of the sentence relative to the document.",
                "This encoding has eight associated features that represent which octile (the first eighth, second eighth, etc.) contains the sentence. 3.2.2 Implementation Details In order to compare the document-level to the sentence-level approach, we compare predictions at the document-level.",
                "We do not address how to use a document-level classifier to make predictions at the sentence-level.",
                "In order to automatically segment the text of the e-mail, we use the RASP statistical parser [4].",
                "Since the automatically segmented sentences may not correspond directly with the phrase-level boundaries, we treat any sentence that contains at least 30% of a marked action-item segment as an action-item.",
                "When evaluating sentencedetection for the sentence-level system, we use these class labels as ground truth.",
                "Since we are not evaluating multiple segmentation approaches, this does not bias any of the methods.",
                "If multiple segmentation systems were under evaluation, one would need to use a metric that matched predicted positive sentences to phrases labeled positive.",
                "The metric would need to punish overly long true predictions as well as too short predictions.",
                "Our criteria for converting to labeled instances implicitly includes both criteria.",
                "Since the segmentation is fixed, an overly long prediction would be predicting yes for many no instances since presumably the extra length corresponds to additional segmented sentences all of which do not contain 30% of action-item.",
                "Likewise, a too short prediction must correspond to a small sentence included in the action-item but not constituting all of the action-item.",
                "Therefore, in order to consider the prediction to be too short, there will be an additional preceding/following sentence that is an action-item where we incorrectly predicted no.",
                "Once a sentence-level classifier has made a prediction for each sentence, we must combine these predictions to make both a document-level prediction and a document-level score.",
                "We use the simple policy of predicting positive when any of the sentences is predicted positive.",
                "In order to produce a document score for ranking, the confidence that the document contains an action-item is: ψ(d) = 1 n(d) s∈d|π(s)=1 ψ(s) if for any s ∈ d, π(s) = 1 1 n(d) maxs∈d ψ(s) o.w. where s is a sentence in document d, π is the classifiers 1/0 prediction, ψ is the score the classifier assigns as its confidence that π(s) = 1, and n(d) is the greater of 1 and the number of (unigram) tokens in the document.",
                "In other words, when any sentence is predicted positive, the document score is the length normalized sum of the sentence scores above threshold.",
                "When no sentence is predicted positive, the document score is the maximum sentence score normalized by length.",
                "As in other text problems, we are more likely to emit false positives for documents with more words or sentences.",
                "Thus we include a length normalization factor. 4.",
                "EXPERIMENTAL ANALYSIS 4.1 The Data Our corpus consists of e-mails obtained from volunteers at an educational institution and cover subjects such as: organizing a research workshop, arranging for job-candidate interviews, publishing proceedings, and talk announcements.",
                "The messages were anonymized by replacing the names of each individual and institution with a pseudonym.1 After attempting to identify and eliminate duplicate e-mails, the corpus contains 744 e-mail messages.",
                "After identity anonymization, the corpora has three basic versions.",
                "Quoted material refers to the text of a previous e-mail that an author often leaves in an e-mail message when responding to the e-mail.",
                "Quoted material can act as noise when learning since it may include action-items from previous messages that are no longer relevant.",
                "To isolate the effects of quoted material, we have three versions of the corpora.",
                "The raw form contains the basic messages.",
                "The auto-stripped version contains the messages after quoted material has been automatically removed.",
                "The hand-stripped version contains the messages after quoted material has been removed by a human.",
                "Additionally, the hand-stripped version has had any xml content and e-mail signatures removed - leaving only the essential content of the message.",
                "The studies reported here are performed with the hand-stripped version.",
                "This allows us to balance the cognitive load in terms of number of tokens that must be read in the user-studies we report - including quoted material would complicate the user studies since some users might skip the material while others read it.",
                "Additionally, ensuring all quoted material is removed 1 We have an even more highly anonymized version of the corpus that can be made available for some outside experimentation.",
                "Please contact the authors for more information on obtaining this data. prevents tainting the cross-validation since otherwise a test item could occur as quoted material in a training document. 4.1.1 Data Labeling Two human annotators labeled each message as to whether or not it contained an action-item.",
                "In addition, they identified each segment of the e-mail which contained an action-item.",
                "A segment is a contiguous section of text selected by the human annotators and may span several sentences or a complete phrase contained in a sentence.",
                "They were instructed that an action item is an explicit request for information that requires the recipients attention or a required action and told to highlight the phrases or sentences that make up the request.",
                "Annotator 1 No Yes Annotator 2 No 391 26 Yes 29 298 Table 1: Agreement of Human Annotators at Document Level Annotator One labeled 324 messages as containing action items.",
                "Annotator Two labeled 327 messages as containing action items.",
                "The agreement of the human annotators is shown in Tables 1 and 2.",
                "The annotators are said to agree at the document-level when both marked the same document as containing no action-items or both marked at least one action-item regardless of whether the text segments were the same.",
                "At the document-level, the annotators agreed 93% of the time.",
                "The kappa statistic [3, 5] is often used to evaluate inter-annotator agreement: κ = A − R 1 − R A is the empirical estimate of the probability of agreement.",
                "R is the empirical estimate of the probability of random agreement given the empirical class priors.",
                "A value close to −1 implies the annotators agree far less often than would be expected randomly, while a value close to 1 means they agree more often than randomly expected.",
                "At the document-level, the kappa statistic for inter-annotator agreement is 0.85.",
                "This value is both strong enough to expect the problem to be learnable and is comparable with results for similar tasks [5, 6].",
                "In order to determine the sentence-level agreement, we use each judgment to create a sentence-corpus with labels as described in Section 3.2.2, then consider the agreement over these sentences.",
                "This allows us to compare agreement over no judgments.",
                "We perform this comparison over the hand-stripped corpus since that eliminates spurious no judgments that would come from including quoted material, etc.",
                "Both annotators were free to label the subject as an action-item, but since neither did, we omit the subject line of the message as well.",
                "This only reduces the number of no agreements.",
                "This leaves 6301 automatically segmented sentences.",
                "At the sentence-level, the annotators agreed 98% of the time, and the kappa statistic for inter-annotator agreement is 0.82.",
                "In order to produce one single set of judgments, the human annotators went through each annotation where there was disagreement and came to a consensus opinion.",
                "The annotators did not collect statistics during this process but anecdotally reported that the majority of disagreements were either cases of clear annotator oversight or different interpretations of conditional statements.",
                "For example, If you would like to keep your job, come to tomorrows meeting implies a required action where If you would like to join Annotator 1 No Yes Annotator 2 No 5810 65 Yes 74 352 Table 2: Agreement of Human Annotators at Sentence Level the football betting pool, come to tomorrows meeting does not.",
                "The first would be an action-item in most contexts while the second would not.",
                "Of course, many conditional statements are not so clearly interpretable.",
                "After reconciling the judgments there are 416 e-mails with no action-items and 328 e-mails containing actionitems.",
                "Of the 328 e-mails containing action-items, 259 messages have one action-item segment; 55 messages have two action-item segments; 11 messages have three action-item segments.",
                "Two messages have four action-item segments, and one message has six action-item segments.",
                "Computing the sentence-level agreement using the reconciled gold standard judgments with each of the annotators individual judgments gives a kappa of 0.89 for Annotator One and a kappa of 0.92 for Annotator Two.",
                "In terms of message characteristics, there were on average 132 content tokens in the body after stripping.",
                "For action-item messages, there were 115.",
                "However, by examining Figure 2 we see the length distributions are nearly identical.",
                "As would be expected for e-mail, it is a long-tailed distribution with about half the messages having more than 60 tokens in the body (this paragraph has 65 tokens). 4.2 Classifiers For this experiment, we have selected a variety of standard text classification algorithms.",
                "In selecting algorithms, we have chosen algorithms that are not only known to work well but which differ along such lines as discriminative vs. generative and lazy vs. eager.",
                "We have done this in order to provide both a competitive and thorough sampling of learning methods for the task at hand.",
                "This is important since it is easy to improve a strawman classifier by introducing a new representation.",
                "By thoroughly sampling alternative classifier choices we demonstrate that representation improvements over bag-of-words are not due to using the information in the bag-of-words poorly. 4.2.1 kNN We employ a standard variant of the k-nearest neighbor algorithm used in text classification, kNN with s-cut score thresholding [19].",
                "We use a tfidf-weighting of the terms with a distanceweighted vote of the neighbors to compute the score before thresholding it.",
                "In order to choose the value of s for thresholding, we perform leave-one-out cross-validation over the training set.",
                "The value of k is set to be 2( log2 N + 1) where N is the number of training points.",
                "This rule for choosing k is theoretically motivated by results which show such a rule converges to the optimal classifier as the number of training points increases [8].",
                "In practice, we have also found it to be a computational convenience that frequently leads to comparable results with numerically optimizing k via a cross-validation procedure. 4.2.2 Na¨ıve Bayes We use a standard multinomial na¨ıve Bayes classifier [16].",
                "In using this classifier, we smoothed word and class probabilities using a Bayesian estimate (with the word prior) and a Laplace m-estimate, respectively. 0 20 40 60 80 100 120 140 160 0 200 400 600 800 1000 1200 1400 NumberofMessages Number of Tokens All Messages Action-Item Messages 0 0.02 0.04 0.06 0.08 0.1 0.12 0.14 0.16 0.18 0.2 0 200 400 600 800 1000 1200 1400 PercentageofMessages Number of Tokens All Messages Action-Item Messages Figure 2: The Histogram (left) and Distribution (right) of Message Length.",
                "A bin size of 20 words was used.",
                "Only tokens in the body after hand-stripping were counted.",
                "After stripping, the majority of words left are usually actual message content.",
                "Classifiers Document Unigram Document Ngram Sentence Unigram Sentence Ngram F1 kNN 0.6670 ± 0.0288 0.7108 ± 0.0699 0.7615 ± 0.0504 0.7790 ± 0.0460 na¨ıve Bayes 0.6572 ± 0.0749 0.6484 ± 0.0513 0.7715 ± 0.0597 0.7777 ± 0.0426 SVM 0.6904 ± 0.0347 0.7428 ± 0.0422 0.7282 ± 0.0698 0.7682 ± 0.0451 Voted Perceptron 0.6288 ± 0.0395 0.6774 ± 0.0422 0.6511 ± 0.0506 0.6798 ± 0.0913 Accuracy kNN 0.7029 ± 0.0659 0.7486 ± 0.0505 0.7972 ± 0.0435 0.8092 ± 0.0352 na¨ıve Bayes 0.6074 ± 0.0651 0.5816 ± 0.1075 0.7863 ± 0.0553 0.8145 ± 0.0268 SVM 0.7595 ± 0.0309 0.7904 ± 0.0349 0.7958 ± 0.0551 0.8173 ± 0.0258 Voted Perceptron 0.6531 ± 0.0390 0.7164 ± 0.0376 0.6413 ± 0.0833 0.7082 ± 0.1032 Table 3: Average Document-Detection Performance during Cross-Validation for Each Method and the Sample Standard Deviation (Sn−1) in italics.",
                "The best performance for each classifier is shown in bold. 4.2.3 SVM We have used a linear SVM with a tfidf feature representation and L2-norm as implemented in the SVMlight package v6.01 [11].",
                "All default settings were used. 4.2.4 Voted Perceptron Like the SVM, the Voted Perceptron is a kernel-based learning method.",
                "We use the same feature representation and kernel as we have for the SVM, a linear kernel with tfidf-weighting and an L2-norm.",
                "The voted perceptron is an online-learning method that keeps a history of past perceptrons used, as well as a weight signifying how often that perceptron was correct.",
                "With each new training example, a correct classification increases the weight on the current perceptron and an incorrect classification updates the perceptron.",
                "The output of the classifier uses the weights on the perceptra to make a final voted classification.",
                "When used in an offline-manner, multiple passes can be made through the training data.",
                "Both the voted perceptron and the SVM give a solution from the same hypothesis space - in this case, a linear classifier.",
                "Furthermore, it is well-known that the Voted Perceptron increases the margin of the solution after each pass through the training data [10].",
                "Since Cohen et al. [5] obtain worse results using an SVM than a Voted Perceptron with one training iteration, they conclude that the best solution for detecting speech acts may not lie in an area with a large margin.",
                "Because their tasks are highly similar to ours, we employ both classifiers to ensure we are not overlooking a competitive alternative classifier to the SVM for the basic bag-of-words representation. 4.3 Performance Measures To compare the performance of the classification methods, we look at two standard performance measures, F1 and accuracy.",
                "The F1 measure [18, 21] is the harmonic mean of precision and recall where Precision = Correct Positives Predicted Positives and Recall = Correct Positives Actual Positives . 4.4 Experimental Methodology We perform standard 10-fold cross-validation on the set of documents.",
                "For the sentence-level approach, all sentences in a document are either entirely in the training set or entirely in the test set for each fold.",
                "For significance tests, we use a two-tailed t-test [21] to compare the values obtained during each cross-validation fold with a p-value of 0.05.",
                "Feature selection was performed using the chi-squared statistic.",
                "Different levels of feature selection were considered for each classifier.",
                "Each of the following number of features was tried: 10, 25, 50, 100, 250, 750, 1000, 2000, 4000.",
                "There are approximately 4700 unigram tokens without feature selection.",
                "In order to choose the number of features to use for each classifier, we perform nested cross-validation and choose the settings that yield the optimal document-level F1 for that classifier.",
                "For this study, only the body of each e-mail message was used.",
                "Feature selection is always applied to all candidate features.",
                "That is, for the n-gram representation, the n-grams and position features are also subject to removal by the feature selection method. 4.5 Results The results for document-level classification are given in Table 3.",
                "The primary hypothesis we are concerned with is that n-grams are critical for this task; if this is true, we expect to see a significant gap in performance between the document-level classifiers that use n-grams (denoted Document Ngram) and those using only unigram features (denoted Document Unigram).",
                "Examining Table 3, we observe that this is indeed the case for every classifier except na¨ıve Bayes.",
                "This difference in performance produced by the n-gram representation is statistically significant for each classifier except for na¨ıve Bayes and the accuracy metric for kNN (see Table 4).",
                "Na¨ıve Bayes poor performance with the n-gram representation is not surprising since the bag-of-n-grams causes excessive doublecounting as mentioned in Section 3.2.1; however, na¨ıve Bayes is not hurt at the sentence-level because the sparse examples provide few chances for agglomerative effects of double counting.",
                "In either case, when a language-modeling approach is desired, modeling the n-grams directly would be preferable to na¨ıve Bayes.",
                "More importantly for the n-gram hypothesis, the n-grams lead to the best document-level classifier performance as well.",
                "As would be expected, the difference between the sentence-level n-gram representation and unigram representation is small.",
                "This is because the window of text is so small that the unigram representation, when done at the sentence-level, implicitly picks up on the power of the n-grams.",
                "Further improvement would signify that the order of the words matter even when only considering a small sentence-size window.",
                "Therefore, the finer-grained sentence-level judgments allows a unigram representation to succeed but only when performed in a small window - behaving as an n-gram representation for all practical purposes.",
                "Document Winner Sentence Winner kNN Ngram Ngram na¨ıve Bayes Unigram Ngram SVM Ngram† Ngram Voted Perceptron Ngram† Ngram Table 4: Significance results for n-grams versus unigrams for document detection using document-level and sentence-level classifiers.",
                "When the F1 result is statistically significant, it is shown in bold.",
                "When the accuracy result is significant, it is shown with a † .",
                "F1 Winner Accuracy Winner kNN Sentence Sentence na¨ıve Bayes Sentence Sentence SVM Sentence Sentence Voted Perceptron Sentence Document Table 5: Significance results for sentence-level classifiers vs. document-level classifiers for the document detection problem.",
                "When the result is statistically significant, it is shown in bold.",
                "Further highlighting the improvement from finer-grained judgments and n-grams, Figure 3 graphically depicts the edge the SVM sentence-level classifier has over the standard bag-of-words approach with a precision-recall curve.",
                "In the high precision area of the graph, the consistent edge of the sentence-level classifier is rather impressive - continuing at precision 1 out to 0.1 recall.",
                "This would mean that a tenth of the users action-items would be placed at the top of their action-item sorted inbox.",
                "Additionally, the large separation at the top right of the curves corresponds to the area where the optimal F1 occurs for each classifier, agreeing with the large improvement from 0.6904 to 0.7682 in F1 score.",
                "Considering the relative unexplored nature of classification at the sentence-level, this gives great hope for further increases in performance.",
                "Accuracy F1 Unigram Ngram Unigram Ngram kNN 0.9519 0.9536 0.6540 0.6686 na¨ıve Bayes 0.9419 0.9550 0.6176 0.6676 SVM 0.9559 0.9579 0.6271 0.6672 Voted Perceptron 0.8895 0.9247 0.3744 0.5164 Table 6: Performance of the Sentence-Level Classifiers at Sentence Detection Although Cohen et al. [5] observed that the Voted Perceptron with a single training iteration outperformed SVM in a set of similar tasks, we see no such behavior here.",
                "This further strengthens the evidence that an alternate classifier with the bag-of-words representation could not reach the same level of performance.",
                "The Voted Perceptron classifier does improve when the number of training iterations are increased, but it is still lower than the SVM classifier.",
                "Sentence detection results are presented in Table 6.",
                "With regard to the sentence detection problem, we note that the F1 measure gives a better feel for the remaining room for improvement in this difficult problem.",
                "That is, unlike document detection where actionitem documents are fairly common, action-item sentences are very rare.",
                "Thus, as in other text problems, the accuracy numbers are deceptively high sheerly because of the default accuracy attainable by always predicting no.",
                "Although, the results here are significantly above-random, it is unclear what level of performance is necessary for sentence detection to be useful in and of itself and not simply as a means to <br>document ranking</br> and classification.",
                "Figure 4: Users find action-items quicker when assisted by a classification system.",
                "Finally, when considering a new type of classification task, one of the most basic questions is whether an accurate classifier built for the task can have an impact on the end-user.",
                "In order to demonstrate the impact this task can have on e-mail users, we conducted a user study using an earlier less-accurate version of the sentence classifier - where instead of using just a single sentence, a threesentence windowed-approach was used.",
                "There were three distinct sets of e-mail in which users had to find action-items.",
                "These sets were either presented in a random order (Unordered), ordered by the classifier (Ordered), or ordered by the classifier and with the 0.4 0.5 0.6 0.7 0.8 0.9 1 0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1 Precision Recall Action-Item Detection SVM Performance (Post Model Selection) Document Unigram Sentence Ngram Figure 3: Both n-grams and a small prediction window lead to consistent improvements over the standard approach. center sentence in the highest confidence window highlighted (Order+help).",
                "In order to perform fair comparisons between conditions, the overall number of tokens in each message set should be approximately equal; that is, the cognitive reading load should be approximately the same before the classifiers reordering.",
                "Additionally, users typically show practice effects by improving at the overall task and thus performing better at later message sets.",
                "This is typically handled by varying the ordering of the sets across users so that the means are comparable.",
                "While omitting further detail, we note the sets were balanced for the total number of tokens and a latin square design was used to balance practice effects.",
                "Figure 4 shows that at intervals of 5, 10, and 15 minutes, users consistently found significantly more action-items when assisted by the classifier, but were most critically aided in the first five minutes.",
                "Although, the classifier consistently aids the users, we did not gain an additional end-user impact by highlighting.",
                "As mentioned above, this might be a result of the large room for improvement that still exists for sentence detection, but anecdotal evidence suggests this might also be a result of how the information is presented to the user rather than the accuracy of sentence detection.",
                "For example, highlighting the wrong sentence near an actual action-item hurts the users trust, but if a vague indicator (e.g., an arrow) points to the approximate area the user is not aware of the near-miss.",
                "Since the user studies used a three sentence window, we believe this played a role as well as sentence detection accuracy. 4.6 Discussion In contrast to problems where n-grams have yielded little difference, we believe their power here stems from the fact that many of the meaningful n-grams for action-items consist of common words, e.g., let me know.",
                "Therefore, the document-level unigram approach cannot gain much leverage, even when modeling their joint probability correctly, since these words will often co-occur in the document but not necessarily in a phrase.",
                "Additionally, action-item detection is distinct from many text classification tasks in that a single sentence can change the class label of the document.",
                "As a result, good classifiers cannot rely on aggregating evidence from a large number of weak indicators across the entire document.",
                "Even though we discarded the header information, examining the top-ranked features at the document-level reveals that many of the features are names or parts of e-mail addresses that occurred in the body and are highly associated with e-mails that tend to contain many or no action-items.",
                "A few examples are terms such as org, bob, and gov.",
                "We note that these features will be sensitive to the particular distribution (senders/receivers) and thus the document-level approach may produce classifiers that transfer less readily to alternate contexts and users at different institutions.",
                "This points out that part of the problem of going beyond bag-of-words may be the methodology, and investigating such properties as learning curves and how well a model transfers may highlight differences in models which appear to have similar performance when tested on the distributions they were trained on.",
                "We are currently investigating whether the sentence-level classifiers do perform better over different test corpora without retraining. 5.",
                "FUTURE WORK While applying text classifiers at the document-level is fairly well-understood, there exists the potential for significantly increasing the performance of the sentence-level classifiers.",
                "Such methods include alternate ways of combining the predictions over each sentence, weightings other than tfidf, which may not be appropriate since sentences are small, better sentence segmentation, and other types of phrasal analysis.",
                "Additionally, named entity tagging, time expressions, etc., seem likely candidates for features that can further improve this task.",
                "We are currently pursuing some of these avenues to see what additional gains these offer.",
                "Finally, it would be interesting to investigate the best methods for combining the document-level and sentence-level classifiers.",
                "Since the simple bag-of-words representation at the document-level leads to a learned model that behaves somewhat like a context-specific prior dependent on the sender/receiver and general topic, a first choice would be to treat it as such when combining probability estimates with the sentence-level classifier.",
                "Such a model might serve as a general example for other problems where bag-of-words can establish a baseline model but richer approaches are needed to achieve performance beyond that baseline. 6.",
                "SUMMARY AND CONCLUSIONS The effectiveness of sentence-level detection argues that labeling at the sentence-level provides significant value.",
                "Further experiments are needed to see how this interacts with the amount of training data available.",
                "Sentence detection that is then agglomerated to document-level detection works surprisingly better given low recall than would be expected with sentence-level items.",
                "This, in turn, indicates that improved sentence segmentation methods could yield further improvements in classification.",
                "In this work, we examined how action-items can be effectively detected in e-mails.",
                "Our empirical analysis has demonstrated that n-grams are of key importance to making the most of documentlevel judgments.",
                "When finer-grained judgments are available, then a standard bag-of-words approach using a small (sentence) window size and automatic segmentation techniques can produce results almost as good as the n-gram based approaches.",
                "Acknowledgments This material is based upon work supported by the Defense Advanced Research Projects Agency (DARPA) under Contract No.",
                "NBCHD030010.",
                "Any opinions, findings and conclusions or recommendations expressed in this material are those of the author(s) and do not necessarily reflect the views of the Defense Advanced Research Projects Agency (DARPA), or the Department of InteriorNational Business Center (DOI-NBC).",
                "We would like to extend our sincerest thanks to Jill Lehman whose efforts in data collection were essential in constructing the corpus, and both Jill and Aaron Steinfeld for their direction of the HCI experiments.",
                "We would also like to thank Django Wexler for constructing and supporting the corpus labeling tools and Curtis Huttenhowers support of the text preprocessing package.",
                "Finally, we gratefully acknowledge Scott Fahlman for his encouragement and useful discussions on this topic. 7.",
                "REFERENCES [1] J. Allan, J. Carbonell, G. Doddington, J. Yamron, and Y. Yang.",
                "Topic detection and tracking pilot study: Final report.",
                "In Proceedings of the DARPA Broadcast News Transcription and Understanding Workshop, Washington, D.C., 1998. [2] C. Apte, F. Damerau, and S. M. Weiss.",
                "Automated learning of decision rules for text categorization.",
                "ACM Transactions on Information Systems, 12(3):233-251, July 1994. [3] J. Carletta.",
                "Assessing agreement on classification tasks: The kappa statistic.",
                "Computational Linguistics, 22(2):249-254, 1996. [4] J. Carroll.",
                "High precision extraction of grammatical relations.",
                "In Proceedings of the 19th International Conference on Computational Linguistics (COLING), pages 134-140, 2002. [5] W. W. Cohen, V. R. Carvalho, and T. M. Mitchell.",
                "Learning to classify email into speech acts.",
                "In EMNLP-2004 (Conference on Empirical Methods in Natural Language Processing), pages 309-316, 2004. [6] S. Corston-Oliver, E. Ringger, M. Gamon, and R. Campbell.",
                "Task-focused summarization of email.",
                "In Text Summarization Branches Out: Proceedings of the ACL-04 Workshop, pages 43-50, 2004. [7] A. Culotta, R. Bekkerman, and A. McCallum.",
                "Extracting social networks and contact information from email and the web.",
                "In CEAS-2004 (Conference on Email and Anti-Spam), Mountain View, CA, July 2004. [8] L. Devroye, L. Gy¨orfi, and G. Lugosi.",
                "A Probabilistic Theory of Pattern Recognition.",
                "Springer-Verlag, New York, NY, 1996. [9] S. T. Dumais, J. Platt, D. Heckerman, and M. Sahami.",
                "Inductive learning algorithms and representations for text categorization.",
                "In CIKM 98, Proceedings of the 7th ACM Conference on Information and Knowledge Management, pages 148-155, 1998. [10] Y. Freund and R. Schapire.",
                "Large margin classification using the perceptron algorithm.",
                "Machine Learning, 37(3):277-296, 1999. [11] T. Joachims.",
                "Making large-scale svm learning practical.",
                "In B. Sch¨olkopf, C. J. Burges, and A. J. Smola, editors, Advances in Kernel Methods - Support Vector Learning, pages 41-56.",
                "MIT Press, 1999. [12] L. S. Larkey.",
                "A patent search and classification system.",
                "In Proceedings of the Fourth ACM Conference on Digital Libraries, pages 179 - 187, 1999. [13] D. D. Lewis.",
                "An evaluation of phrasal and clustered representations on a text categorization task.",
                "In SIGIR 92, Proceedings of the 15th Annual International ACM Conference on Research and Development in Information Retrieval, pages 37-50, 1992. [14] Y. Liu, J. Carbonell, and R. Jin.",
                "A pairwise ensemble approach for accurate genre classification.",
                "In Proceedings of the European Conference on Machine Learning (ECML), 2003. [15] Y. Liu, R. Yan, R. Jin, and J. Carbonell.",
                "A comparison study of kernels for multi-label text classification using category association.",
                "In The Twenty-first International Conference on Machine Learning (ICML), 2004. [16] A. McCallum and K. Nigam.",
                "A comparison of event models for naive bayes text classification.",
                "In Working Notes of AAAI 98 (The 15th National Conference on Artificial Intelligence), Workshop on Learning for Text Categorization, pages 41-48, 1998.",
                "TR WS-98-05. [17] F. Sebastiani.",
                "Machine learning in automated text categorization.",
                "ACM Computing Surveys, 34(1):1-47, March 2002. [18] C. J. van Rijsbergen.",
                "Information Retrieval.",
                "Butterworths, London, 1979. [19] Y. Yang.",
                "An evaluation of statistical approaches to text categorization.",
                "Information Retrieval, 1(1/2):67-88, 1999. [20] Y. Yang, J. Carbonell, R. Brown, T. Pierce, B. T. Archibald, and X. Liu.",
                "Learning approaches to topic detection and tracking.",
                "IEEE EXPERT, Special Issue on Applications of Intelligent Information Retrieval, 1999. [21] Y. Yang and X. Liu.",
                "A re-examination of text categorization methods.",
                "In SIGIR 99, Proceedings of the 22nd Annual International ACM Conference on Research and Development in Information Retrieval, pages 42-49, 1999. [22] Y. Yang, J. Zhang, J. Carbonell, and C. Jin.",
                "Topic-conditioned novelty detection.",
                "In Proceedings of the ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, July 2002."
            ],
            "original_annotated_samples": [],
            "translated_annotated_samples": [
                "\"Ranking de documentos\": clasifique los documentos de tal manera que todos los documentos que contengan elementos de acción ocurran lo más alto posible en la clasificación.3.",
                "Una vez capacitado, la aplicación de los clasificadores resultantes a la detección de oraciones ahora es sencillo, pero para aplicar los clasificadores a la detección de documentos y \"clasificación de documentos\", las predicciones individuales sobre cada oración deben agregarse para hacer una predicción a nivel de documento.",
                "Aunque los resultados aquí están significativamente superiores al aleación, no está claro qué nivel de rendimiento es necesario para que la detección de oraciones sea útil en sí misma y no simplemente como un medio para \"documentar la clasificación\" y la clasificación."
            ],
            "translated_text": "",
            "candidates": [
                "Ranking de documentos",
                "Ranking de documentos",
                "clasificación de documentos",
                "clasificación de documentos",
                "clasificación de documentos",
                "documentar la clasificación"
            ],
            "error": []
        },
        "sentence detection": {
            "translated_key": "detección de oraciones",
            "is_in_text": true,
            "original_annotated_sentences": [
                "Feature Representation for Effective Action-Item Detection Paul N. Bennett Computer Science Department Carnegie Mellon University Pittsburgh, PA 15213 pbennett+@cs.cmu.edu Jaime Carbonell Language Technologies Institute.",
                "Carnegie Mellon University Pittsburgh, PA 15213 jgc+@cs.cmu.edu ABSTRACT E-mail users face an ever-growing challenge in managing their inboxes due to the growing centrality of email in the workplace for task assignment, action requests, and other roles beyond information dissemination.",
                "Whereas Information Retrieval and Machine Learning techniques are gaining initial acceptance in spam filtering and automated folder assignment, this paper reports on a new task: automated action-item detection, in order to flag emails that require responses, and to highlight the specific passage(s) indicating the request(s) for action.",
                "Unlike standard topic-driven text classification, action-item detection requires inferring the senders intent, and as such responds less well to pure bag-of-words classification.",
                "However, using enriched feature sets, such as n-grams (up to n=4) with chi-squared feature selection, and contextual cues for action-item location improve performance by up to 10% over unigrams, using in both cases state of the art classifiers such as SVMs with automated model selection via embedded cross-validation.",
                "Categories and Subject Descriptors H.3.3 [Information Storage and Retrieval]: Information Search and Retrieval; I.2.6 [Artificial Intelligence]: Learning; I.5.4 [Pattern Recognition]: Applications General Terms Experimentation 1.",
                "INTRODUCTION E-mail users are facing an increasingly difficult task of managing their inboxes in the face of mounting challenges that result from rising e-mail usage.",
                "This includes prioritizing e-mails over a range of sources from business partners to family members, filtering and reducing junk e-mail, and quickly managing requests that demand From: Henry Hutchins <hhutchins@innovative.company.com> To: Sara Smith; Joe Johnson; William Woolings Subject: meeting with prospective customers Sent: Fri 12/10/2005 8:08 AM Hi All, Id like to remind all of you that the group from GRTY will be visiting us next Friday at 4:30 p.m.",
                "The current schedule looks like this: + 9:30 a.m.",
                "Informal Breakfast and Discussion in Cafeteria + 10:30 a.m. Company Overview + 11:00 a.m.",
                "Individual Meetings (Continue Over Lunch) + 2:00 p.m. Tour of Facilities + 3:00 p.m.",
                "Sales Pitch In order to have this go off smoothly, I would like to practice the presentation well in advance.",
                "As a result, I will need each of your parts by Wednesday.",
                "Keep up the good work! -Henry Figure 1: An E-mail with emphasized Action-Item, an explicit request that requires the recipients attention or action. the receivers attention or action.",
                "Automated action-item detection targets the third of these problems by attempting to detect which e-mails require an action or response with information, and within those e-mails, attempting to highlight the sentence (or other passage length) that directly indicates the action request.",
                "Such a detection system can be used as one part of an e-mail agent which would assist a user in processing important e-mails quicker than would have been possible without the agent.",
                "We view action-item detection as one necessary component of a successful e-mail agent which would perform spam detection, action-item detection, topic classification and priority ranking, among other functions.",
                "The utility of such a detector can manifest as a method of prioritizing e-mails according to task-oriented criteria other than the standard ones of topic and sender or as a means of ensuring that the email user hasnt dropped the proverbial ball by forgetting to address an action request.",
                "Action-item detection differs from standard text classification in two important ways.",
                "First, the user is interested both in detecting whether an email contains action items and in locating exactly where these action item requests are contained within the email body.",
                "In contrast, standard text categorization merely assigns a topic label to each text, whether that label corresponds to an e-mail folder or a controlled indexing vocabulary [12, 15, 22].",
                "Second, action-item detection attempts to recover the email senders intent - whether she means to elicit response or action on the part of the receiver; note that for this task, classifiers using only unigrams as features do not perform optimally, as evidenced in our results below.",
                "Instead we find that we need more information-laden features such as higher-order n-grams.",
                "Text categorization by topic, on the other hand, works very well using just individual words as features [2, 9, 13, 17].",
                "In fact, genre-classification, which one would think may require more than a bag-of-words approach, also works quite well using just unigram features [14].",
                "Topic detection and tracking (TDT), also works well with unigram feature sets [1, 20].",
                "We believe that action-item detection is one of the first clear instances of an IR-related task where we must move beyond bag-of-words to achieve high performance, albeit not too far, as bag-of-n-grams seem to suffice.",
                "We first review related work for similar text classification problems such as e-mail priority ranking and speech act identification.",
                "Then we more formally define the action-item detection problem, discuss the aspects that distinguish it from more common problems like topic classification, and highlight the challenges in constructing systems that can perform well at the sentence and document level.",
                "From there, we move to a discussion of feature representation and selection techniques appropriate for this problem and how standard text classification approaches can be adapted to smoothly move from the sentence-level detection problem to the documentlevel classification problem.",
                "We then conduct an empirical analysis that helps us determine the effectiveness of our feature extraction procedures as well as establish baselines for a number of classification algorithms on this task.",
                "Finally, we summarize this papers contributions and consider interesting directions for future work. 2.",
                "RELATED WORK Several other researchers have considered very similar text classification tasks.",
                "Cohen et al. [5] describe an ontology of speech acts, such as Propose a Meeting, and attempt to predict when an e-mail contains one of these speech acts.",
                "We consider action-items to be an important specific type of speech act that falls within their more general classification.",
                "While they provide results for several classification methods, their methods only make use of human judgments at the document-level.",
                "In contrast, we consider whether accuracy can be increased by using finer-grained human judgments that mark the specific sentences and phrases of interest.",
                "Corston-Oliver et al. [6] consider detecting items in e-mail to Put on a To-Do List.",
                "This classification task is very similar to ours except they do not consider simple factual questions to belong to this category.",
                "We include questions, but note that not all questions are action-items - some are rhetorical or simply social convention, How are you?.",
                "From a learning perspective, while they make use of judgments at the sentence-level, they do not explicitly compare what if any benefits finer-grained judgments offer.",
                "Additionally, they do not study alternative choices or approaches to the classification task.",
                "Instead, they simply apply a standard SVM at the sentence-level and focus primarily on a linguistic analysis of how the sentence can be logically reformulated before adding it to the task list.",
                "In this study, we examine several alternative classification methods, compare document-level and sentence-level approaches and analyze the machine learning issues implicit in these problems.",
                "Interest in a variety of learning tasks related to e-mail has been rapidly growing in the recent literature.",
                "For example, in a forum dedicated to e-mail learning tasks, Culotta et al. [7] presented methods for learning social networks from e-mail.",
                "In this work, we do not focus on peer relationships; however, such methods could complement those here since peer relationships often influence word choice when requesting an action. 3.",
                "PROBLEM DEFINITION & APPROACH In contrast to previous work, we explicitly focus on the benefits that finer-grained, more costly, sentence-level human judgments offer over coarse-grained document-level judgments.",
                "Additionally, we consider multiple standard text classification approaches and analyze both the quantitative and qualitative differences that arise from taking a document-level vs. a sentence-level approach to classification.",
                "Finally, we focus on the representation necessary to achieve the most competitive performance. 3.1 Problem Definition In order to provide the most benefit to the user, a system would not only detect the document, but it would also indicate the specific sentences in the e-mail which contain the action-items.",
                "Therefore, there are three basic problems: 1.",
                "Document detection: Classify a document as to whether or not it contains an action-item. 2.",
                "Document ranking: Rank the documents such that all documents containing action-items occur as high as possible in the ranking. 3.",
                "<br>sentence detection</br>: Classify each sentence in a document as to whether or not it is an action-item.",
                "As in most Information Retrieval tasks, the weight the evaluation metric should give to precision and recall depends on the nature of the application.",
                "In situations where a user will eventually read all received messages, ranking (e.g., via precision at recall of 1) may be most important since this will help encourage shorter delays in communications between users.",
                "In contrast, high-precision detection at low recall will be of increasing importance when the user is under severe time-pressure and therefore will likely not read all mail.",
                "This can be the case for crisis managers during disaster management.",
                "Finally, <br>sentence detection</br> plays a role in both timepressure situations and simply to alleviate the users required time to gist the message. 3.2 Approach As mentioned above, the labeled data can come in one of two forms: a document-labeling provides a yes/no label for each document as to whether it contains an action-item; a phrase-labeling provides only a yes label for the specific items of interest.",
                "We term the human judgments a phrase-labeling since the users view of the action-item may not correspond with actual sentence boundaries or predicted sentence boundaries.",
                "Obviously, it is straightforward to generate a document-labeling consistent with a phrase-labeling by labeling a document yes if and only if it contains at least one phrase labeled yes.",
                "To train classifiers for this task, we can take several viewpoints related to both the basic problems we have enumerated and the form of the labeled data.",
                "The document-level view treats each e-mail as a learning instance with an associated class-label.",
                "Then, the document can be converted to a feature-value vector and learning progresses as usual.",
                "Applying a document-level classifier to document detection and ranking is straightforward.",
                "In order to apply it to <br>sentence detection</br>, one must make additional steps.",
                "For example, if the classifier predicts a document contains an action-item, then areas of the document that contain a high-concentration of words which the model weights heavily in favor of action-items can be indicated.",
                "The obvious benefit of the document-level approach is that training set collection costs are lower since the user only has to specify whether or not an e-mail contains an action-item and not the specific sentences.",
                "In the sentence-level view, each e-mail is automatically segmented into sentences, and each sentence is treated as a learning instance with an associated class-label.",
                "Since the phrase-labeling provided by the user may not coincide with the automatic segmentation, we must determine what label to assign a partially overlapping sentence when converting it to a learning instance.",
                "Once trained, applying the resulting classifiers to <br>sentence detection</br> is now straightforward, but in order to apply the classifiers to document detection and document ranking, the individual predictions over each sentence must be aggregated in order to make a document-level prediction.",
                "This approach has the potential to benefit from morespecific labels that enable the learner to focus attention on the key sentences instead of having to learn based on data that the majority of the words in the e-mail provide no or little information about class membership. 3.2.1 Features Consider some of the phrases that might constitute part of an action item: would like to know, let me know, as soon as possible, have you.",
                "Each of these phrases consists of common words that occur in many e-mails.",
                "However, when they occur in the same sentence, they are far more indicative of an action-item.",
                "Additionally, order can be important: consider have you versus you have.",
                "Because of this, we posit that n-grams play a larger role in this problem than is typical of problems like topic classification.",
                "Therefore, we consider all n-grams up to size 4.",
                "When using n-grams, if we find an n-gram of size 4 in a segment of text, we can represent the text as just one occurrence of the ngram or as one occurrence of the n-gram and an occurrence of each smaller n-gram contained by it.",
                "We choose the second of these alternatives since this will allow the algorithm itself to smoothly back-off in terms of recall.",
                "Methods such as na¨ıve Bayes may be hurt by such a representation because of double-counting.",
                "Since sentence-ending punctuation can provide information, we retain the terminating punctuation token when it is identifiable.",
                "Additionally, we add a beginning-of-sentence and end-of-sentence token in order to capture patterns that are often indicators at the beginning or end of a sentence.",
                "Assuming proper punctuation, these extra tokens are unnecessary, but often e-mail lacks proper punctuation.",
                "In addition, for the sentence-level classifiers that use ngrams, we additionally code for each sentence a binary encoding of the position of the sentence relative to the document.",
                "This encoding has eight associated features that represent which octile (the first eighth, second eighth, etc.) contains the sentence. 3.2.2 Implementation Details In order to compare the document-level to the sentence-level approach, we compare predictions at the document-level.",
                "We do not address how to use a document-level classifier to make predictions at the sentence-level.",
                "In order to automatically segment the text of the e-mail, we use the RASP statistical parser [4].",
                "Since the automatically segmented sentences may not correspond directly with the phrase-level boundaries, we treat any sentence that contains at least 30% of a marked action-item segment as an action-item.",
                "When evaluating sentencedetection for the sentence-level system, we use these class labels as ground truth.",
                "Since we are not evaluating multiple segmentation approaches, this does not bias any of the methods.",
                "If multiple segmentation systems were under evaluation, one would need to use a metric that matched predicted positive sentences to phrases labeled positive.",
                "The metric would need to punish overly long true predictions as well as too short predictions.",
                "Our criteria for converting to labeled instances implicitly includes both criteria.",
                "Since the segmentation is fixed, an overly long prediction would be predicting yes for many no instances since presumably the extra length corresponds to additional segmented sentences all of which do not contain 30% of action-item.",
                "Likewise, a too short prediction must correspond to a small sentence included in the action-item but not constituting all of the action-item.",
                "Therefore, in order to consider the prediction to be too short, there will be an additional preceding/following sentence that is an action-item where we incorrectly predicted no.",
                "Once a sentence-level classifier has made a prediction for each sentence, we must combine these predictions to make both a document-level prediction and a document-level score.",
                "We use the simple policy of predicting positive when any of the sentences is predicted positive.",
                "In order to produce a document score for ranking, the confidence that the document contains an action-item is: ψ(d) = 1 n(d) s∈d|π(s)=1 ψ(s) if for any s ∈ d, π(s) = 1 1 n(d) maxs∈d ψ(s) o.w. where s is a sentence in document d, π is the classifiers 1/0 prediction, ψ is the score the classifier assigns as its confidence that π(s) = 1, and n(d) is the greater of 1 and the number of (unigram) tokens in the document.",
                "In other words, when any sentence is predicted positive, the document score is the length normalized sum of the sentence scores above threshold.",
                "When no sentence is predicted positive, the document score is the maximum sentence score normalized by length.",
                "As in other text problems, we are more likely to emit false positives for documents with more words or sentences.",
                "Thus we include a length normalization factor. 4.",
                "EXPERIMENTAL ANALYSIS 4.1 The Data Our corpus consists of e-mails obtained from volunteers at an educational institution and cover subjects such as: organizing a research workshop, arranging for job-candidate interviews, publishing proceedings, and talk announcements.",
                "The messages were anonymized by replacing the names of each individual and institution with a pseudonym.1 After attempting to identify and eliminate duplicate e-mails, the corpus contains 744 e-mail messages.",
                "After identity anonymization, the corpora has three basic versions.",
                "Quoted material refers to the text of a previous e-mail that an author often leaves in an e-mail message when responding to the e-mail.",
                "Quoted material can act as noise when learning since it may include action-items from previous messages that are no longer relevant.",
                "To isolate the effects of quoted material, we have three versions of the corpora.",
                "The raw form contains the basic messages.",
                "The auto-stripped version contains the messages after quoted material has been automatically removed.",
                "The hand-stripped version contains the messages after quoted material has been removed by a human.",
                "Additionally, the hand-stripped version has had any xml content and e-mail signatures removed - leaving only the essential content of the message.",
                "The studies reported here are performed with the hand-stripped version.",
                "This allows us to balance the cognitive load in terms of number of tokens that must be read in the user-studies we report - including quoted material would complicate the user studies since some users might skip the material while others read it.",
                "Additionally, ensuring all quoted material is removed 1 We have an even more highly anonymized version of the corpus that can be made available for some outside experimentation.",
                "Please contact the authors for more information on obtaining this data. prevents tainting the cross-validation since otherwise a test item could occur as quoted material in a training document. 4.1.1 Data Labeling Two human annotators labeled each message as to whether or not it contained an action-item.",
                "In addition, they identified each segment of the e-mail which contained an action-item.",
                "A segment is a contiguous section of text selected by the human annotators and may span several sentences or a complete phrase contained in a sentence.",
                "They were instructed that an action item is an explicit request for information that requires the recipients attention or a required action and told to highlight the phrases or sentences that make up the request.",
                "Annotator 1 No Yes Annotator 2 No 391 26 Yes 29 298 Table 1: Agreement of Human Annotators at Document Level Annotator One labeled 324 messages as containing action items.",
                "Annotator Two labeled 327 messages as containing action items.",
                "The agreement of the human annotators is shown in Tables 1 and 2.",
                "The annotators are said to agree at the document-level when both marked the same document as containing no action-items or both marked at least one action-item regardless of whether the text segments were the same.",
                "At the document-level, the annotators agreed 93% of the time.",
                "The kappa statistic [3, 5] is often used to evaluate inter-annotator agreement: κ = A − R 1 − R A is the empirical estimate of the probability of agreement.",
                "R is the empirical estimate of the probability of random agreement given the empirical class priors.",
                "A value close to −1 implies the annotators agree far less often than would be expected randomly, while a value close to 1 means they agree more often than randomly expected.",
                "At the document-level, the kappa statistic for inter-annotator agreement is 0.85.",
                "This value is both strong enough to expect the problem to be learnable and is comparable with results for similar tasks [5, 6].",
                "In order to determine the sentence-level agreement, we use each judgment to create a sentence-corpus with labels as described in Section 3.2.2, then consider the agreement over these sentences.",
                "This allows us to compare agreement over no judgments.",
                "We perform this comparison over the hand-stripped corpus since that eliminates spurious no judgments that would come from including quoted material, etc.",
                "Both annotators were free to label the subject as an action-item, but since neither did, we omit the subject line of the message as well.",
                "This only reduces the number of no agreements.",
                "This leaves 6301 automatically segmented sentences.",
                "At the sentence-level, the annotators agreed 98% of the time, and the kappa statistic for inter-annotator agreement is 0.82.",
                "In order to produce one single set of judgments, the human annotators went through each annotation where there was disagreement and came to a consensus opinion.",
                "The annotators did not collect statistics during this process but anecdotally reported that the majority of disagreements were either cases of clear annotator oversight or different interpretations of conditional statements.",
                "For example, If you would like to keep your job, come to tomorrows meeting implies a required action where If you would like to join Annotator 1 No Yes Annotator 2 No 5810 65 Yes 74 352 Table 2: Agreement of Human Annotators at Sentence Level the football betting pool, come to tomorrows meeting does not.",
                "The first would be an action-item in most contexts while the second would not.",
                "Of course, many conditional statements are not so clearly interpretable.",
                "After reconciling the judgments there are 416 e-mails with no action-items and 328 e-mails containing actionitems.",
                "Of the 328 e-mails containing action-items, 259 messages have one action-item segment; 55 messages have two action-item segments; 11 messages have three action-item segments.",
                "Two messages have four action-item segments, and one message has six action-item segments.",
                "Computing the sentence-level agreement using the reconciled gold standard judgments with each of the annotators individual judgments gives a kappa of 0.89 for Annotator One and a kappa of 0.92 for Annotator Two.",
                "In terms of message characteristics, there were on average 132 content tokens in the body after stripping.",
                "For action-item messages, there were 115.",
                "However, by examining Figure 2 we see the length distributions are nearly identical.",
                "As would be expected for e-mail, it is a long-tailed distribution with about half the messages having more than 60 tokens in the body (this paragraph has 65 tokens). 4.2 Classifiers For this experiment, we have selected a variety of standard text classification algorithms.",
                "In selecting algorithms, we have chosen algorithms that are not only known to work well but which differ along such lines as discriminative vs. generative and lazy vs. eager.",
                "We have done this in order to provide both a competitive and thorough sampling of learning methods for the task at hand.",
                "This is important since it is easy to improve a strawman classifier by introducing a new representation.",
                "By thoroughly sampling alternative classifier choices we demonstrate that representation improvements over bag-of-words are not due to using the information in the bag-of-words poorly. 4.2.1 kNN We employ a standard variant of the k-nearest neighbor algorithm used in text classification, kNN with s-cut score thresholding [19].",
                "We use a tfidf-weighting of the terms with a distanceweighted vote of the neighbors to compute the score before thresholding it.",
                "In order to choose the value of s for thresholding, we perform leave-one-out cross-validation over the training set.",
                "The value of k is set to be 2( log2 N + 1) where N is the number of training points.",
                "This rule for choosing k is theoretically motivated by results which show such a rule converges to the optimal classifier as the number of training points increases [8].",
                "In practice, we have also found it to be a computational convenience that frequently leads to comparable results with numerically optimizing k via a cross-validation procedure. 4.2.2 Na¨ıve Bayes We use a standard multinomial na¨ıve Bayes classifier [16].",
                "In using this classifier, we smoothed word and class probabilities using a Bayesian estimate (with the word prior) and a Laplace m-estimate, respectively. 0 20 40 60 80 100 120 140 160 0 200 400 600 800 1000 1200 1400 NumberofMessages Number of Tokens All Messages Action-Item Messages 0 0.02 0.04 0.06 0.08 0.1 0.12 0.14 0.16 0.18 0.2 0 200 400 600 800 1000 1200 1400 PercentageofMessages Number of Tokens All Messages Action-Item Messages Figure 2: The Histogram (left) and Distribution (right) of Message Length.",
                "A bin size of 20 words was used.",
                "Only tokens in the body after hand-stripping were counted.",
                "After stripping, the majority of words left are usually actual message content.",
                "Classifiers Document Unigram Document Ngram Sentence Unigram Sentence Ngram F1 kNN 0.6670 ± 0.0288 0.7108 ± 0.0699 0.7615 ± 0.0504 0.7790 ± 0.0460 na¨ıve Bayes 0.6572 ± 0.0749 0.6484 ± 0.0513 0.7715 ± 0.0597 0.7777 ± 0.0426 SVM 0.6904 ± 0.0347 0.7428 ± 0.0422 0.7282 ± 0.0698 0.7682 ± 0.0451 Voted Perceptron 0.6288 ± 0.0395 0.6774 ± 0.0422 0.6511 ± 0.0506 0.6798 ± 0.0913 Accuracy kNN 0.7029 ± 0.0659 0.7486 ± 0.0505 0.7972 ± 0.0435 0.8092 ± 0.0352 na¨ıve Bayes 0.6074 ± 0.0651 0.5816 ± 0.1075 0.7863 ± 0.0553 0.8145 ± 0.0268 SVM 0.7595 ± 0.0309 0.7904 ± 0.0349 0.7958 ± 0.0551 0.8173 ± 0.0258 Voted Perceptron 0.6531 ± 0.0390 0.7164 ± 0.0376 0.6413 ± 0.0833 0.7082 ± 0.1032 Table 3: Average Document-Detection Performance during Cross-Validation for Each Method and the Sample Standard Deviation (Sn−1) in italics.",
                "The best performance for each classifier is shown in bold. 4.2.3 SVM We have used a linear SVM with a tfidf feature representation and L2-norm as implemented in the SVMlight package v6.01 [11].",
                "All default settings were used. 4.2.4 Voted Perceptron Like the SVM, the Voted Perceptron is a kernel-based learning method.",
                "We use the same feature representation and kernel as we have for the SVM, a linear kernel with tfidf-weighting and an L2-norm.",
                "The voted perceptron is an online-learning method that keeps a history of past perceptrons used, as well as a weight signifying how often that perceptron was correct.",
                "With each new training example, a correct classification increases the weight on the current perceptron and an incorrect classification updates the perceptron.",
                "The output of the classifier uses the weights on the perceptra to make a final voted classification.",
                "When used in an offline-manner, multiple passes can be made through the training data.",
                "Both the voted perceptron and the SVM give a solution from the same hypothesis space - in this case, a linear classifier.",
                "Furthermore, it is well-known that the Voted Perceptron increases the margin of the solution after each pass through the training data [10].",
                "Since Cohen et al. [5] obtain worse results using an SVM than a Voted Perceptron with one training iteration, they conclude that the best solution for detecting speech acts may not lie in an area with a large margin.",
                "Because their tasks are highly similar to ours, we employ both classifiers to ensure we are not overlooking a competitive alternative classifier to the SVM for the basic bag-of-words representation. 4.3 Performance Measures To compare the performance of the classification methods, we look at two standard performance measures, F1 and accuracy.",
                "The F1 measure [18, 21] is the harmonic mean of precision and recall where Precision = Correct Positives Predicted Positives and Recall = Correct Positives Actual Positives . 4.4 Experimental Methodology We perform standard 10-fold cross-validation on the set of documents.",
                "For the sentence-level approach, all sentences in a document are either entirely in the training set or entirely in the test set for each fold.",
                "For significance tests, we use a two-tailed t-test [21] to compare the values obtained during each cross-validation fold with a p-value of 0.05.",
                "Feature selection was performed using the chi-squared statistic.",
                "Different levels of feature selection were considered for each classifier.",
                "Each of the following number of features was tried: 10, 25, 50, 100, 250, 750, 1000, 2000, 4000.",
                "There are approximately 4700 unigram tokens without feature selection.",
                "In order to choose the number of features to use for each classifier, we perform nested cross-validation and choose the settings that yield the optimal document-level F1 for that classifier.",
                "For this study, only the body of each e-mail message was used.",
                "Feature selection is always applied to all candidate features.",
                "That is, for the n-gram representation, the n-grams and position features are also subject to removal by the feature selection method. 4.5 Results The results for document-level classification are given in Table 3.",
                "The primary hypothesis we are concerned with is that n-grams are critical for this task; if this is true, we expect to see a significant gap in performance between the document-level classifiers that use n-grams (denoted Document Ngram) and those using only unigram features (denoted Document Unigram).",
                "Examining Table 3, we observe that this is indeed the case for every classifier except na¨ıve Bayes.",
                "This difference in performance produced by the n-gram representation is statistically significant for each classifier except for na¨ıve Bayes and the accuracy metric for kNN (see Table 4).",
                "Na¨ıve Bayes poor performance with the n-gram representation is not surprising since the bag-of-n-grams causes excessive doublecounting as mentioned in Section 3.2.1; however, na¨ıve Bayes is not hurt at the sentence-level because the sparse examples provide few chances for agglomerative effects of double counting.",
                "In either case, when a language-modeling approach is desired, modeling the n-grams directly would be preferable to na¨ıve Bayes.",
                "More importantly for the n-gram hypothesis, the n-grams lead to the best document-level classifier performance as well.",
                "As would be expected, the difference between the sentence-level n-gram representation and unigram representation is small.",
                "This is because the window of text is so small that the unigram representation, when done at the sentence-level, implicitly picks up on the power of the n-grams.",
                "Further improvement would signify that the order of the words matter even when only considering a small sentence-size window.",
                "Therefore, the finer-grained sentence-level judgments allows a unigram representation to succeed but only when performed in a small window - behaving as an n-gram representation for all practical purposes.",
                "Document Winner Sentence Winner kNN Ngram Ngram na¨ıve Bayes Unigram Ngram SVM Ngram† Ngram Voted Perceptron Ngram† Ngram Table 4: Significance results for n-grams versus unigrams for document detection using document-level and sentence-level classifiers.",
                "When the F1 result is statistically significant, it is shown in bold.",
                "When the accuracy result is significant, it is shown with a † .",
                "F1 Winner Accuracy Winner kNN Sentence Sentence na¨ıve Bayes Sentence Sentence SVM Sentence Sentence Voted Perceptron Sentence Document Table 5: Significance results for sentence-level classifiers vs. document-level classifiers for the document detection problem.",
                "When the result is statistically significant, it is shown in bold.",
                "Further highlighting the improvement from finer-grained judgments and n-grams, Figure 3 graphically depicts the edge the SVM sentence-level classifier has over the standard bag-of-words approach with a precision-recall curve.",
                "In the high precision area of the graph, the consistent edge of the sentence-level classifier is rather impressive - continuing at precision 1 out to 0.1 recall.",
                "This would mean that a tenth of the users action-items would be placed at the top of their action-item sorted inbox.",
                "Additionally, the large separation at the top right of the curves corresponds to the area where the optimal F1 occurs for each classifier, agreeing with the large improvement from 0.6904 to 0.7682 in F1 score.",
                "Considering the relative unexplored nature of classification at the sentence-level, this gives great hope for further increases in performance.",
                "Accuracy F1 Unigram Ngram Unigram Ngram kNN 0.9519 0.9536 0.6540 0.6686 na¨ıve Bayes 0.9419 0.9550 0.6176 0.6676 SVM 0.9559 0.9579 0.6271 0.6672 Voted Perceptron 0.8895 0.9247 0.3744 0.5164 Table 6: Performance of the Sentence-Level Classifiers at <br>sentence detection</br> Although Cohen et al. [5] observed that the Voted Perceptron with a single training iteration outperformed SVM in a set of similar tasks, we see no such behavior here.",
                "This further strengthens the evidence that an alternate classifier with the bag-of-words representation could not reach the same level of performance.",
                "The Voted Perceptron classifier does improve when the number of training iterations are increased, but it is still lower than the SVM classifier.",
                "<br>sentence detection</br> results are presented in Table 6.",
                "With regard to the <br>sentence detection</br> problem, we note that the F1 measure gives a better feel for the remaining room for improvement in this difficult problem.",
                "That is, unlike document detection where actionitem documents are fairly common, action-item sentences are very rare.",
                "Thus, as in other text problems, the accuracy numbers are deceptively high sheerly because of the default accuracy attainable by always predicting no.",
                "Although, the results here are significantly above-random, it is unclear what level of performance is necessary for <br>sentence detection</br> to be useful in and of itself and not simply as a means to document ranking and classification.",
                "Figure 4: Users find action-items quicker when assisted by a classification system.",
                "Finally, when considering a new type of classification task, one of the most basic questions is whether an accurate classifier built for the task can have an impact on the end-user.",
                "In order to demonstrate the impact this task can have on e-mail users, we conducted a user study using an earlier less-accurate version of the sentence classifier - where instead of using just a single sentence, a threesentence windowed-approach was used.",
                "There were three distinct sets of e-mail in which users had to find action-items.",
                "These sets were either presented in a random order (Unordered), ordered by the classifier (Ordered), or ordered by the classifier and with the 0.4 0.5 0.6 0.7 0.8 0.9 1 0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1 Precision Recall Action-Item Detection SVM Performance (Post Model Selection) Document Unigram Sentence Ngram Figure 3: Both n-grams and a small prediction window lead to consistent improvements over the standard approach. center sentence in the highest confidence window highlighted (Order+help).",
                "In order to perform fair comparisons between conditions, the overall number of tokens in each message set should be approximately equal; that is, the cognitive reading load should be approximately the same before the classifiers reordering.",
                "Additionally, users typically show practice effects by improving at the overall task and thus performing better at later message sets.",
                "This is typically handled by varying the ordering of the sets across users so that the means are comparable.",
                "While omitting further detail, we note the sets were balanced for the total number of tokens and a latin square design was used to balance practice effects.",
                "Figure 4 shows that at intervals of 5, 10, and 15 minutes, users consistently found significantly more action-items when assisted by the classifier, but were most critically aided in the first five minutes.",
                "Although, the classifier consistently aids the users, we did not gain an additional end-user impact by highlighting.",
                "As mentioned above, this might be a result of the large room for improvement that still exists for <br>sentence detection</br>, but anecdotal evidence suggests this might also be a result of how the information is presented to the user rather than the accuracy of <br>sentence detection</br>.",
                "For example, highlighting the wrong sentence near an actual action-item hurts the users trust, but if a vague indicator (e.g., an arrow) points to the approximate area the user is not aware of the near-miss.",
                "Since the user studies used a three sentence window, we believe this played a role as well as <br>sentence detection</br> accuracy. 4.6 Discussion In contrast to problems where n-grams have yielded little difference, we believe their power here stems from the fact that many of the meaningful n-grams for action-items consist of common words, e.g., let me know.",
                "Therefore, the document-level unigram approach cannot gain much leverage, even when modeling their joint probability correctly, since these words will often co-occur in the document but not necessarily in a phrase.",
                "Additionally, action-item detection is distinct from many text classification tasks in that a single sentence can change the class label of the document.",
                "As a result, good classifiers cannot rely on aggregating evidence from a large number of weak indicators across the entire document.",
                "Even though we discarded the header information, examining the top-ranked features at the document-level reveals that many of the features are names or parts of e-mail addresses that occurred in the body and are highly associated with e-mails that tend to contain many or no action-items.",
                "A few examples are terms such as org, bob, and gov.",
                "We note that these features will be sensitive to the particular distribution (senders/receivers) and thus the document-level approach may produce classifiers that transfer less readily to alternate contexts and users at different institutions.",
                "This points out that part of the problem of going beyond bag-of-words may be the methodology, and investigating such properties as learning curves and how well a model transfers may highlight differences in models which appear to have similar performance when tested on the distributions they were trained on.",
                "We are currently investigating whether the sentence-level classifiers do perform better over different test corpora without retraining. 5.",
                "FUTURE WORK While applying text classifiers at the document-level is fairly well-understood, there exists the potential for significantly increasing the performance of the sentence-level classifiers.",
                "Such methods include alternate ways of combining the predictions over each sentence, weightings other than tfidf, which may not be appropriate since sentences are small, better sentence segmentation, and other types of phrasal analysis.",
                "Additionally, named entity tagging, time expressions, etc., seem likely candidates for features that can further improve this task.",
                "We are currently pursuing some of these avenues to see what additional gains these offer.",
                "Finally, it would be interesting to investigate the best methods for combining the document-level and sentence-level classifiers.",
                "Since the simple bag-of-words representation at the document-level leads to a learned model that behaves somewhat like a context-specific prior dependent on the sender/receiver and general topic, a first choice would be to treat it as such when combining probability estimates with the sentence-level classifier.",
                "Such a model might serve as a general example for other problems where bag-of-words can establish a baseline model but richer approaches are needed to achieve performance beyond that baseline. 6.",
                "SUMMARY AND CONCLUSIONS The effectiveness of sentence-level detection argues that labeling at the sentence-level provides significant value.",
                "Further experiments are needed to see how this interacts with the amount of training data available.",
                "<br>sentence detection</br> that is then agglomerated to document-level detection works surprisingly better given low recall than would be expected with sentence-level items.",
                "This, in turn, indicates that improved sentence segmentation methods could yield further improvements in classification.",
                "In this work, we examined how action-items can be effectively detected in e-mails.",
                "Our empirical analysis has demonstrated that n-grams are of key importance to making the most of documentlevel judgments.",
                "When finer-grained judgments are available, then a standard bag-of-words approach using a small (sentence) window size and automatic segmentation techniques can produce results almost as good as the n-gram based approaches.",
                "Acknowledgments This material is based upon work supported by the Defense Advanced Research Projects Agency (DARPA) under Contract No.",
                "NBCHD030010.",
                "Any opinions, findings and conclusions or recommendations expressed in this material are those of the author(s) and do not necessarily reflect the views of the Defense Advanced Research Projects Agency (DARPA), or the Department of InteriorNational Business Center (DOI-NBC).",
                "We would like to extend our sincerest thanks to Jill Lehman whose efforts in data collection were essential in constructing the corpus, and both Jill and Aaron Steinfeld for their direction of the HCI experiments.",
                "We would also like to thank Django Wexler for constructing and supporting the corpus labeling tools and Curtis Huttenhowers support of the text preprocessing package.",
                "Finally, we gratefully acknowledge Scott Fahlman for his encouragement and useful discussions on this topic. 7.",
                "REFERENCES [1] J. Allan, J. Carbonell, G. Doddington, J. Yamron, and Y. Yang.",
                "Topic detection and tracking pilot study: Final report.",
                "In Proceedings of the DARPA Broadcast News Transcription and Understanding Workshop, Washington, D.C., 1998. [2] C. Apte, F. Damerau, and S. M. Weiss.",
                "Automated learning of decision rules for text categorization.",
                "ACM Transactions on Information Systems, 12(3):233-251, July 1994. [3] J. Carletta.",
                "Assessing agreement on classification tasks: The kappa statistic.",
                "Computational Linguistics, 22(2):249-254, 1996. [4] J. Carroll.",
                "High precision extraction of grammatical relations.",
                "In Proceedings of the 19th International Conference on Computational Linguistics (COLING), pages 134-140, 2002. [5] W. W. Cohen, V. R. Carvalho, and T. M. Mitchell.",
                "Learning to classify email into speech acts.",
                "In EMNLP-2004 (Conference on Empirical Methods in Natural Language Processing), pages 309-316, 2004. [6] S. Corston-Oliver, E. Ringger, M. Gamon, and R. Campbell.",
                "Task-focused summarization of email.",
                "In Text Summarization Branches Out: Proceedings of the ACL-04 Workshop, pages 43-50, 2004. [7] A. Culotta, R. Bekkerman, and A. McCallum.",
                "Extracting social networks and contact information from email and the web.",
                "In CEAS-2004 (Conference on Email and Anti-Spam), Mountain View, CA, July 2004. [8] L. Devroye, L. Gy¨orfi, and G. Lugosi.",
                "A Probabilistic Theory of Pattern Recognition.",
                "Springer-Verlag, New York, NY, 1996. [9] S. T. Dumais, J. Platt, D. Heckerman, and M. Sahami.",
                "Inductive learning algorithms and representations for text categorization.",
                "In CIKM 98, Proceedings of the 7th ACM Conference on Information and Knowledge Management, pages 148-155, 1998. [10] Y. Freund and R. Schapire.",
                "Large margin classification using the perceptron algorithm.",
                "Machine Learning, 37(3):277-296, 1999. [11] T. Joachims.",
                "Making large-scale svm learning practical.",
                "In B. Sch¨olkopf, C. J. Burges, and A. J. Smola, editors, Advances in Kernel Methods - Support Vector Learning, pages 41-56.",
                "MIT Press, 1999. [12] L. S. Larkey.",
                "A patent search and classification system.",
                "In Proceedings of the Fourth ACM Conference on Digital Libraries, pages 179 - 187, 1999. [13] D. D. Lewis.",
                "An evaluation of phrasal and clustered representations on a text categorization task.",
                "In SIGIR 92, Proceedings of the 15th Annual International ACM Conference on Research and Development in Information Retrieval, pages 37-50, 1992. [14] Y. Liu, J. Carbonell, and R. Jin.",
                "A pairwise ensemble approach for accurate genre classification.",
                "In Proceedings of the European Conference on Machine Learning (ECML), 2003. [15] Y. Liu, R. Yan, R. Jin, and J. Carbonell.",
                "A comparison study of kernels for multi-label text classification using category association.",
                "In The Twenty-first International Conference on Machine Learning (ICML), 2004. [16] A. McCallum and K. Nigam.",
                "A comparison of event models for naive bayes text classification.",
                "In Working Notes of AAAI 98 (The 15th National Conference on Artificial Intelligence), Workshop on Learning for Text Categorization, pages 41-48, 1998.",
                "TR WS-98-05. [17] F. Sebastiani.",
                "Machine learning in automated text categorization.",
                "ACM Computing Surveys, 34(1):1-47, March 2002. [18] C. J. van Rijsbergen.",
                "Information Retrieval.",
                "Butterworths, London, 1979. [19] Y. Yang.",
                "An evaluation of statistical approaches to text categorization.",
                "Information Retrieval, 1(1/2):67-88, 1999. [20] Y. Yang, J. Carbonell, R. Brown, T. Pierce, B. T. Archibald, and X. Liu.",
                "Learning approaches to topic detection and tracking.",
                "IEEE EXPERT, Special Issue on Applications of Intelligent Information Retrieval, 1999. [21] Y. Yang and X. Liu.",
                "A re-examination of text categorization methods.",
                "In SIGIR 99, Proceedings of the 22nd Annual International ACM Conference on Research and Development in Information Retrieval, pages 42-49, 1999. [22] Y. Yang, J. Zhang, J. Carbonell, and C. Jin.",
                "Topic-conditioned novelty detection.",
                "In Proceedings of the ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, July 2002."
            ],
            "original_annotated_samples": [],
            "translated_annotated_samples": [
                "\"Detección de oraciones\": Clasifique cada oración en un documento sobre si es o no un ítem de acción.",
                "Finalmente, la \"detección de oraciones\" juega un papel en las situaciones de tiempo de tiempo y simplemente para aliviar los usuarios requirió tiempo para inyecciones al mensaje.3.2 Enfoque Como se mencionó anteriormente, los datos etiquetados pueden venir en una de dos formas: un marcado de documentos proporciona una etiqueta sí/no para cada documento sobre si contiene un elemento de acción;Un marcado de frases proporciona solo una etiqueta de sí para los elementos específicos de interés.",
                "Para aplicarlo a la \"detección de oraciones\", uno debe hacer pasos adicionales.",
                "Una vez capacitado, la aplicación de los clasificadores resultantes a la \"detección de oraciones\" ahora es sencilla, pero para aplicar los clasificadores a la detección de documentos y la clasificación de documentos, las predicciones individuales sobre cada oración deben agregarse para hacer una predicción a nivel de documento.",
                "Precisión F1 unigram ngram unigram ngram knn 0.9519 0.9536 0.6540 0.6686 na¨ıve bayes 0.9419 0.9550 0.6176 0.6676 SVM 0.95559 0.9579 0.6271 0.6672 CLASEMIENTO DEL SENTENCIA DEL SENTENCIÓN 0.8895 0.9247 0.3744 4. Iers en \"Detección de oraciones\" aunque Cohen et al.[5] observó que el perceptrón votado con una sola iteración de entrenamiento superó a SVM en un conjunto de tareas similares, no vemos tal comportamiento aquí.",
                "Los resultados de la \"detección de oraciones\" se presentan en la Tabla 6.",
                "Con respecto al problema de la \"detección de oraciones\", observamos que la medida F1 da una mejor idea del espacio restante de mejora en este problema difícil.",
                "Aunque los resultados aquí están significativamente superiores al aleación, no está claro qué nivel de rendimiento es necesario para que la \"detección de oraciones\" sea útil en sí misma y no simplemente como un medio para documentar la clasificación y la clasificación.",
                "Como se mencionó anteriormente, esto podría ser el resultado del gran margen de mejora que todavía existe para la \"detección de oraciones\", pero la evidencia anecdótica sugiere que esto también podría ser el resultado de cómo se presenta la información al usuario en lugar de la precisión de la \"oracióndetección\".",
                "Dado que los estudios de usuarios utilizaron una ventana de tres oraciones, creemos que esto jugó un papel, así como una precisión de \"detección de oraciones\".4.6 Discusión En contraste con los problemas en los que los N-Grams han producido poca diferencia, creemos que su poder aquí proviene del hecho de que muchos de los N-Grams significativos para los elementos de acción consisten en palabras comunes, por ejemplo, hágamelo saber.",
                "La \"detección de oraciones\" que luego se aglomera para que la detección de nivel de documento funciona sorprendentemente mejor dado un bajo retiro de lo esperado con los elementos a nivel de oración."
            ],
            "translated_text": "",
            "candidates": [
                "detección de oraciones",
                "Detección de oraciones",
                "detección de oraciones",
                "detección de oraciones",
                "detección de oraciones",
                "detección de oraciones",
                "detección de oraciones",
                "detección de oraciones",
                "detección de oraciones",
                "Detección de oraciones",
                "Detección de oraciones",
                "detección de oraciones",
                "detección de oraciones",
                "detección de oraciones",
                "detección de oraciones",
                "detección de oraciones",
                "detección de oraciones",
                "detección de oraciones",
                "oracióndetección",
                "detección de oraciones",
                "detección de oraciones",
                "detección de oraciones",
                "detección de oraciones"
            ],
            "error": []
        },
        "sentence-level classifier": {
            "translated_key": "clasificador a nivel de oración",
            "is_in_text": true,
            "original_annotated_sentences": [
                "Feature Representation for Effective Action-Item Detection Paul N. Bennett Computer Science Department Carnegie Mellon University Pittsburgh, PA 15213 pbennett+@cs.cmu.edu Jaime Carbonell Language Technologies Institute.",
                "Carnegie Mellon University Pittsburgh, PA 15213 jgc+@cs.cmu.edu ABSTRACT E-mail users face an ever-growing challenge in managing their inboxes due to the growing centrality of email in the workplace for task assignment, action requests, and other roles beyond information dissemination.",
                "Whereas Information Retrieval and Machine Learning techniques are gaining initial acceptance in spam filtering and automated folder assignment, this paper reports on a new task: automated action-item detection, in order to flag emails that require responses, and to highlight the specific passage(s) indicating the request(s) for action.",
                "Unlike standard topic-driven text classification, action-item detection requires inferring the senders intent, and as such responds less well to pure bag-of-words classification.",
                "However, using enriched feature sets, such as n-grams (up to n=4) with chi-squared feature selection, and contextual cues for action-item location improve performance by up to 10% over unigrams, using in both cases state of the art classifiers such as SVMs with automated model selection via embedded cross-validation.",
                "Categories and Subject Descriptors H.3.3 [Information Storage and Retrieval]: Information Search and Retrieval; I.2.6 [Artificial Intelligence]: Learning; I.5.4 [Pattern Recognition]: Applications General Terms Experimentation 1.",
                "INTRODUCTION E-mail users are facing an increasingly difficult task of managing their inboxes in the face of mounting challenges that result from rising e-mail usage.",
                "This includes prioritizing e-mails over a range of sources from business partners to family members, filtering and reducing junk e-mail, and quickly managing requests that demand From: Henry Hutchins <hhutchins@innovative.company.com> To: Sara Smith; Joe Johnson; William Woolings Subject: meeting with prospective customers Sent: Fri 12/10/2005 8:08 AM Hi All, Id like to remind all of you that the group from GRTY will be visiting us next Friday at 4:30 p.m.",
                "The current schedule looks like this: + 9:30 a.m.",
                "Informal Breakfast and Discussion in Cafeteria + 10:30 a.m. Company Overview + 11:00 a.m.",
                "Individual Meetings (Continue Over Lunch) + 2:00 p.m. Tour of Facilities + 3:00 p.m.",
                "Sales Pitch In order to have this go off smoothly, I would like to practice the presentation well in advance.",
                "As a result, I will need each of your parts by Wednesday.",
                "Keep up the good work! -Henry Figure 1: An E-mail with emphasized Action-Item, an explicit request that requires the recipients attention or action. the receivers attention or action.",
                "Automated action-item detection targets the third of these problems by attempting to detect which e-mails require an action or response with information, and within those e-mails, attempting to highlight the sentence (or other passage length) that directly indicates the action request.",
                "Such a detection system can be used as one part of an e-mail agent which would assist a user in processing important e-mails quicker than would have been possible without the agent.",
                "We view action-item detection as one necessary component of a successful e-mail agent which would perform spam detection, action-item detection, topic classification and priority ranking, among other functions.",
                "The utility of such a detector can manifest as a method of prioritizing e-mails according to task-oriented criteria other than the standard ones of topic and sender or as a means of ensuring that the email user hasnt dropped the proverbial ball by forgetting to address an action request.",
                "Action-item detection differs from standard text classification in two important ways.",
                "First, the user is interested both in detecting whether an email contains action items and in locating exactly where these action item requests are contained within the email body.",
                "In contrast, standard text categorization merely assigns a topic label to each text, whether that label corresponds to an e-mail folder or a controlled indexing vocabulary [12, 15, 22].",
                "Second, action-item detection attempts to recover the email senders intent - whether she means to elicit response or action on the part of the receiver; note that for this task, classifiers using only unigrams as features do not perform optimally, as evidenced in our results below.",
                "Instead we find that we need more information-laden features such as higher-order n-grams.",
                "Text categorization by topic, on the other hand, works very well using just individual words as features [2, 9, 13, 17].",
                "In fact, genre-classification, which one would think may require more than a bag-of-words approach, also works quite well using just unigram features [14].",
                "Topic detection and tracking (TDT), also works well with unigram feature sets [1, 20].",
                "We believe that action-item detection is one of the first clear instances of an IR-related task where we must move beyond bag-of-words to achieve high performance, albeit not too far, as bag-of-n-grams seem to suffice.",
                "We first review related work for similar text classification problems such as e-mail priority ranking and speech act identification.",
                "Then we more formally define the action-item detection problem, discuss the aspects that distinguish it from more common problems like topic classification, and highlight the challenges in constructing systems that can perform well at the sentence and document level.",
                "From there, we move to a discussion of feature representation and selection techniques appropriate for this problem and how standard text classification approaches can be adapted to smoothly move from the sentence-level detection problem to the documentlevel classification problem.",
                "We then conduct an empirical analysis that helps us determine the effectiveness of our feature extraction procedures as well as establish baselines for a number of classification algorithms on this task.",
                "Finally, we summarize this papers contributions and consider interesting directions for future work. 2.",
                "RELATED WORK Several other researchers have considered very similar text classification tasks.",
                "Cohen et al. [5] describe an ontology of speech acts, such as Propose a Meeting, and attempt to predict when an e-mail contains one of these speech acts.",
                "We consider action-items to be an important specific type of speech act that falls within their more general classification.",
                "While they provide results for several classification methods, their methods only make use of human judgments at the document-level.",
                "In contrast, we consider whether accuracy can be increased by using finer-grained human judgments that mark the specific sentences and phrases of interest.",
                "Corston-Oliver et al. [6] consider detecting items in e-mail to Put on a To-Do List.",
                "This classification task is very similar to ours except they do not consider simple factual questions to belong to this category.",
                "We include questions, but note that not all questions are action-items - some are rhetorical or simply social convention, How are you?.",
                "From a learning perspective, while they make use of judgments at the sentence-level, they do not explicitly compare what if any benefits finer-grained judgments offer.",
                "Additionally, they do not study alternative choices or approaches to the classification task.",
                "Instead, they simply apply a standard SVM at the sentence-level and focus primarily on a linguistic analysis of how the sentence can be logically reformulated before adding it to the task list.",
                "In this study, we examine several alternative classification methods, compare document-level and sentence-level approaches and analyze the machine learning issues implicit in these problems.",
                "Interest in a variety of learning tasks related to e-mail has been rapidly growing in the recent literature.",
                "For example, in a forum dedicated to e-mail learning tasks, Culotta et al. [7] presented methods for learning social networks from e-mail.",
                "In this work, we do not focus on peer relationships; however, such methods could complement those here since peer relationships often influence word choice when requesting an action. 3.",
                "PROBLEM DEFINITION & APPROACH In contrast to previous work, we explicitly focus on the benefits that finer-grained, more costly, sentence-level human judgments offer over coarse-grained document-level judgments.",
                "Additionally, we consider multiple standard text classification approaches and analyze both the quantitative and qualitative differences that arise from taking a document-level vs. a sentence-level approach to classification.",
                "Finally, we focus on the representation necessary to achieve the most competitive performance. 3.1 Problem Definition In order to provide the most benefit to the user, a system would not only detect the document, but it would also indicate the specific sentences in the e-mail which contain the action-items.",
                "Therefore, there are three basic problems: 1.",
                "Document detection: Classify a document as to whether or not it contains an action-item. 2.",
                "Document ranking: Rank the documents such that all documents containing action-items occur as high as possible in the ranking. 3.",
                "Sentence detection: Classify each sentence in a document as to whether or not it is an action-item.",
                "As in most Information Retrieval tasks, the weight the evaluation metric should give to precision and recall depends on the nature of the application.",
                "In situations where a user will eventually read all received messages, ranking (e.g., via precision at recall of 1) may be most important since this will help encourage shorter delays in communications between users.",
                "In contrast, high-precision detection at low recall will be of increasing importance when the user is under severe time-pressure and therefore will likely not read all mail.",
                "This can be the case for crisis managers during disaster management.",
                "Finally, sentence detection plays a role in both timepressure situations and simply to alleviate the users required time to gist the message. 3.2 Approach As mentioned above, the labeled data can come in one of two forms: a document-labeling provides a yes/no label for each document as to whether it contains an action-item; a phrase-labeling provides only a yes label for the specific items of interest.",
                "We term the human judgments a phrase-labeling since the users view of the action-item may not correspond with actual sentence boundaries or predicted sentence boundaries.",
                "Obviously, it is straightforward to generate a document-labeling consistent with a phrase-labeling by labeling a document yes if and only if it contains at least one phrase labeled yes.",
                "To train classifiers for this task, we can take several viewpoints related to both the basic problems we have enumerated and the form of the labeled data.",
                "The document-level view treats each e-mail as a learning instance with an associated class-label.",
                "Then, the document can be converted to a feature-value vector and learning progresses as usual.",
                "Applying a document-level classifier to document detection and ranking is straightforward.",
                "In order to apply it to sentence detection, one must make additional steps.",
                "For example, if the classifier predicts a document contains an action-item, then areas of the document that contain a high-concentration of words which the model weights heavily in favor of action-items can be indicated.",
                "The obvious benefit of the document-level approach is that training set collection costs are lower since the user only has to specify whether or not an e-mail contains an action-item and not the specific sentences.",
                "In the sentence-level view, each e-mail is automatically segmented into sentences, and each sentence is treated as a learning instance with an associated class-label.",
                "Since the phrase-labeling provided by the user may not coincide with the automatic segmentation, we must determine what label to assign a partially overlapping sentence when converting it to a learning instance.",
                "Once trained, applying the resulting classifiers to sentence detection is now straightforward, but in order to apply the classifiers to document detection and document ranking, the individual predictions over each sentence must be aggregated in order to make a document-level prediction.",
                "This approach has the potential to benefit from morespecific labels that enable the learner to focus attention on the key sentences instead of having to learn based on data that the majority of the words in the e-mail provide no or little information about class membership. 3.2.1 Features Consider some of the phrases that might constitute part of an action item: would like to know, let me know, as soon as possible, have you.",
                "Each of these phrases consists of common words that occur in many e-mails.",
                "However, when they occur in the same sentence, they are far more indicative of an action-item.",
                "Additionally, order can be important: consider have you versus you have.",
                "Because of this, we posit that n-grams play a larger role in this problem than is typical of problems like topic classification.",
                "Therefore, we consider all n-grams up to size 4.",
                "When using n-grams, if we find an n-gram of size 4 in a segment of text, we can represent the text as just one occurrence of the ngram or as one occurrence of the n-gram and an occurrence of each smaller n-gram contained by it.",
                "We choose the second of these alternatives since this will allow the algorithm itself to smoothly back-off in terms of recall.",
                "Methods such as na¨ıve Bayes may be hurt by such a representation because of double-counting.",
                "Since sentence-ending punctuation can provide information, we retain the terminating punctuation token when it is identifiable.",
                "Additionally, we add a beginning-of-sentence and end-of-sentence token in order to capture patterns that are often indicators at the beginning or end of a sentence.",
                "Assuming proper punctuation, these extra tokens are unnecessary, but often e-mail lacks proper punctuation.",
                "In addition, for the sentence-level classifiers that use ngrams, we additionally code for each sentence a binary encoding of the position of the sentence relative to the document.",
                "This encoding has eight associated features that represent which octile (the first eighth, second eighth, etc.) contains the sentence. 3.2.2 Implementation Details In order to compare the document-level to the sentence-level approach, we compare predictions at the document-level.",
                "We do not address how to use a document-level classifier to make predictions at the sentence-level.",
                "In order to automatically segment the text of the e-mail, we use the RASP statistical parser [4].",
                "Since the automatically segmented sentences may not correspond directly with the phrase-level boundaries, we treat any sentence that contains at least 30% of a marked action-item segment as an action-item.",
                "When evaluating sentencedetection for the sentence-level system, we use these class labels as ground truth.",
                "Since we are not evaluating multiple segmentation approaches, this does not bias any of the methods.",
                "If multiple segmentation systems were under evaluation, one would need to use a metric that matched predicted positive sentences to phrases labeled positive.",
                "The metric would need to punish overly long true predictions as well as too short predictions.",
                "Our criteria for converting to labeled instances implicitly includes both criteria.",
                "Since the segmentation is fixed, an overly long prediction would be predicting yes for many no instances since presumably the extra length corresponds to additional segmented sentences all of which do not contain 30% of action-item.",
                "Likewise, a too short prediction must correspond to a small sentence included in the action-item but not constituting all of the action-item.",
                "Therefore, in order to consider the prediction to be too short, there will be an additional preceding/following sentence that is an action-item where we incorrectly predicted no.",
                "Once a <br>sentence-level classifier</br> has made a prediction for each sentence, we must combine these predictions to make both a document-level prediction and a document-level score.",
                "We use the simple policy of predicting positive when any of the sentences is predicted positive.",
                "In order to produce a document score for ranking, the confidence that the document contains an action-item is: ψ(d) = 1 n(d) s∈d|π(s)=1 ψ(s) if for any s ∈ d, π(s) = 1 1 n(d) maxs∈d ψ(s) o.w. where s is a sentence in document d, π is the classifiers 1/0 prediction, ψ is the score the classifier assigns as its confidence that π(s) = 1, and n(d) is the greater of 1 and the number of (unigram) tokens in the document.",
                "In other words, when any sentence is predicted positive, the document score is the length normalized sum of the sentence scores above threshold.",
                "When no sentence is predicted positive, the document score is the maximum sentence score normalized by length.",
                "As in other text problems, we are more likely to emit false positives for documents with more words or sentences.",
                "Thus we include a length normalization factor. 4.",
                "EXPERIMENTAL ANALYSIS 4.1 The Data Our corpus consists of e-mails obtained from volunteers at an educational institution and cover subjects such as: organizing a research workshop, arranging for job-candidate interviews, publishing proceedings, and talk announcements.",
                "The messages were anonymized by replacing the names of each individual and institution with a pseudonym.1 After attempting to identify and eliminate duplicate e-mails, the corpus contains 744 e-mail messages.",
                "After identity anonymization, the corpora has three basic versions.",
                "Quoted material refers to the text of a previous e-mail that an author often leaves in an e-mail message when responding to the e-mail.",
                "Quoted material can act as noise when learning since it may include action-items from previous messages that are no longer relevant.",
                "To isolate the effects of quoted material, we have three versions of the corpora.",
                "The raw form contains the basic messages.",
                "The auto-stripped version contains the messages after quoted material has been automatically removed.",
                "The hand-stripped version contains the messages after quoted material has been removed by a human.",
                "Additionally, the hand-stripped version has had any xml content and e-mail signatures removed - leaving only the essential content of the message.",
                "The studies reported here are performed with the hand-stripped version.",
                "This allows us to balance the cognitive load in terms of number of tokens that must be read in the user-studies we report - including quoted material would complicate the user studies since some users might skip the material while others read it.",
                "Additionally, ensuring all quoted material is removed 1 We have an even more highly anonymized version of the corpus that can be made available for some outside experimentation.",
                "Please contact the authors for more information on obtaining this data. prevents tainting the cross-validation since otherwise a test item could occur as quoted material in a training document. 4.1.1 Data Labeling Two human annotators labeled each message as to whether or not it contained an action-item.",
                "In addition, they identified each segment of the e-mail which contained an action-item.",
                "A segment is a contiguous section of text selected by the human annotators and may span several sentences or a complete phrase contained in a sentence.",
                "They were instructed that an action item is an explicit request for information that requires the recipients attention or a required action and told to highlight the phrases or sentences that make up the request.",
                "Annotator 1 No Yes Annotator 2 No 391 26 Yes 29 298 Table 1: Agreement of Human Annotators at Document Level Annotator One labeled 324 messages as containing action items.",
                "Annotator Two labeled 327 messages as containing action items.",
                "The agreement of the human annotators is shown in Tables 1 and 2.",
                "The annotators are said to agree at the document-level when both marked the same document as containing no action-items or both marked at least one action-item regardless of whether the text segments were the same.",
                "At the document-level, the annotators agreed 93% of the time.",
                "The kappa statistic [3, 5] is often used to evaluate inter-annotator agreement: κ = A − R 1 − R A is the empirical estimate of the probability of agreement.",
                "R is the empirical estimate of the probability of random agreement given the empirical class priors.",
                "A value close to −1 implies the annotators agree far less often than would be expected randomly, while a value close to 1 means they agree more often than randomly expected.",
                "At the document-level, the kappa statistic for inter-annotator agreement is 0.85.",
                "This value is both strong enough to expect the problem to be learnable and is comparable with results for similar tasks [5, 6].",
                "In order to determine the sentence-level agreement, we use each judgment to create a sentence-corpus with labels as described in Section 3.2.2, then consider the agreement over these sentences.",
                "This allows us to compare agreement over no judgments.",
                "We perform this comparison over the hand-stripped corpus since that eliminates spurious no judgments that would come from including quoted material, etc.",
                "Both annotators were free to label the subject as an action-item, but since neither did, we omit the subject line of the message as well.",
                "This only reduces the number of no agreements.",
                "This leaves 6301 automatically segmented sentences.",
                "At the sentence-level, the annotators agreed 98% of the time, and the kappa statistic for inter-annotator agreement is 0.82.",
                "In order to produce one single set of judgments, the human annotators went through each annotation where there was disagreement and came to a consensus opinion.",
                "The annotators did not collect statistics during this process but anecdotally reported that the majority of disagreements were either cases of clear annotator oversight or different interpretations of conditional statements.",
                "For example, If you would like to keep your job, come to tomorrows meeting implies a required action where If you would like to join Annotator 1 No Yes Annotator 2 No 5810 65 Yes 74 352 Table 2: Agreement of Human Annotators at Sentence Level the football betting pool, come to tomorrows meeting does not.",
                "The first would be an action-item in most contexts while the second would not.",
                "Of course, many conditional statements are not so clearly interpretable.",
                "After reconciling the judgments there are 416 e-mails with no action-items and 328 e-mails containing actionitems.",
                "Of the 328 e-mails containing action-items, 259 messages have one action-item segment; 55 messages have two action-item segments; 11 messages have three action-item segments.",
                "Two messages have four action-item segments, and one message has six action-item segments.",
                "Computing the sentence-level agreement using the reconciled gold standard judgments with each of the annotators individual judgments gives a kappa of 0.89 for Annotator One and a kappa of 0.92 for Annotator Two.",
                "In terms of message characteristics, there were on average 132 content tokens in the body after stripping.",
                "For action-item messages, there were 115.",
                "However, by examining Figure 2 we see the length distributions are nearly identical.",
                "As would be expected for e-mail, it is a long-tailed distribution with about half the messages having more than 60 tokens in the body (this paragraph has 65 tokens). 4.2 Classifiers For this experiment, we have selected a variety of standard text classification algorithms.",
                "In selecting algorithms, we have chosen algorithms that are not only known to work well but which differ along such lines as discriminative vs. generative and lazy vs. eager.",
                "We have done this in order to provide both a competitive and thorough sampling of learning methods for the task at hand.",
                "This is important since it is easy to improve a strawman classifier by introducing a new representation.",
                "By thoroughly sampling alternative classifier choices we demonstrate that representation improvements over bag-of-words are not due to using the information in the bag-of-words poorly. 4.2.1 kNN We employ a standard variant of the k-nearest neighbor algorithm used in text classification, kNN with s-cut score thresholding [19].",
                "We use a tfidf-weighting of the terms with a distanceweighted vote of the neighbors to compute the score before thresholding it.",
                "In order to choose the value of s for thresholding, we perform leave-one-out cross-validation over the training set.",
                "The value of k is set to be 2( log2 N + 1) where N is the number of training points.",
                "This rule for choosing k is theoretically motivated by results which show such a rule converges to the optimal classifier as the number of training points increases [8].",
                "In practice, we have also found it to be a computational convenience that frequently leads to comparable results with numerically optimizing k via a cross-validation procedure. 4.2.2 Na¨ıve Bayes We use a standard multinomial na¨ıve Bayes classifier [16].",
                "In using this classifier, we smoothed word and class probabilities using a Bayesian estimate (with the word prior) and a Laplace m-estimate, respectively. 0 20 40 60 80 100 120 140 160 0 200 400 600 800 1000 1200 1400 NumberofMessages Number of Tokens All Messages Action-Item Messages 0 0.02 0.04 0.06 0.08 0.1 0.12 0.14 0.16 0.18 0.2 0 200 400 600 800 1000 1200 1400 PercentageofMessages Number of Tokens All Messages Action-Item Messages Figure 2: The Histogram (left) and Distribution (right) of Message Length.",
                "A bin size of 20 words was used.",
                "Only tokens in the body after hand-stripping were counted.",
                "After stripping, the majority of words left are usually actual message content.",
                "Classifiers Document Unigram Document Ngram Sentence Unigram Sentence Ngram F1 kNN 0.6670 ± 0.0288 0.7108 ± 0.0699 0.7615 ± 0.0504 0.7790 ± 0.0460 na¨ıve Bayes 0.6572 ± 0.0749 0.6484 ± 0.0513 0.7715 ± 0.0597 0.7777 ± 0.0426 SVM 0.6904 ± 0.0347 0.7428 ± 0.0422 0.7282 ± 0.0698 0.7682 ± 0.0451 Voted Perceptron 0.6288 ± 0.0395 0.6774 ± 0.0422 0.6511 ± 0.0506 0.6798 ± 0.0913 Accuracy kNN 0.7029 ± 0.0659 0.7486 ± 0.0505 0.7972 ± 0.0435 0.8092 ± 0.0352 na¨ıve Bayes 0.6074 ± 0.0651 0.5816 ± 0.1075 0.7863 ± 0.0553 0.8145 ± 0.0268 SVM 0.7595 ± 0.0309 0.7904 ± 0.0349 0.7958 ± 0.0551 0.8173 ± 0.0258 Voted Perceptron 0.6531 ± 0.0390 0.7164 ± 0.0376 0.6413 ± 0.0833 0.7082 ± 0.1032 Table 3: Average Document-Detection Performance during Cross-Validation for Each Method and the Sample Standard Deviation (Sn−1) in italics.",
                "The best performance for each classifier is shown in bold. 4.2.3 SVM We have used a linear SVM with a tfidf feature representation and L2-norm as implemented in the SVMlight package v6.01 [11].",
                "All default settings were used. 4.2.4 Voted Perceptron Like the SVM, the Voted Perceptron is a kernel-based learning method.",
                "We use the same feature representation and kernel as we have for the SVM, a linear kernel with tfidf-weighting and an L2-norm.",
                "The voted perceptron is an online-learning method that keeps a history of past perceptrons used, as well as a weight signifying how often that perceptron was correct.",
                "With each new training example, a correct classification increases the weight on the current perceptron and an incorrect classification updates the perceptron.",
                "The output of the classifier uses the weights on the perceptra to make a final voted classification.",
                "When used in an offline-manner, multiple passes can be made through the training data.",
                "Both the voted perceptron and the SVM give a solution from the same hypothesis space - in this case, a linear classifier.",
                "Furthermore, it is well-known that the Voted Perceptron increases the margin of the solution after each pass through the training data [10].",
                "Since Cohen et al. [5] obtain worse results using an SVM than a Voted Perceptron with one training iteration, they conclude that the best solution for detecting speech acts may not lie in an area with a large margin.",
                "Because their tasks are highly similar to ours, we employ both classifiers to ensure we are not overlooking a competitive alternative classifier to the SVM for the basic bag-of-words representation. 4.3 Performance Measures To compare the performance of the classification methods, we look at two standard performance measures, F1 and accuracy.",
                "The F1 measure [18, 21] is the harmonic mean of precision and recall where Precision = Correct Positives Predicted Positives and Recall = Correct Positives Actual Positives . 4.4 Experimental Methodology We perform standard 10-fold cross-validation on the set of documents.",
                "For the sentence-level approach, all sentences in a document are either entirely in the training set or entirely in the test set for each fold.",
                "For significance tests, we use a two-tailed t-test [21] to compare the values obtained during each cross-validation fold with a p-value of 0.05.",
                "Feature selection was performed using the chi-squared statistic.",
                "Different levels of feature selection were considered for each classifier.",
                "Each of the following number of features was tried: 10, 25, 50, 100, 250, 750, 1000, 2000, 4000.",
                "There are approximately 4700 unigram tokens without feature selection.",
                "In order to choose the number of features to use for each classifier, we perform nested cross-validation and choose the settings that yield the optimal document-level F1 for that classifier.",
                "For this study, only the body of each e-mail message was used.",
                "Feature selection is always applied to all candidate features.",
                "That is, for the n-gram representation, the n-grams and position features are also subject to removal by the feature selection method. 4.5 Results The results for document-level classification are given in Table 3.",
                "The primary hypothesis we are concerned with is that n-grams are critical for this task; if this is true, we expect to see a significant gap in performance between the document-level classifiers that use n-grams (denoted Document Ngram) and those using only unigram features (denoted Document Unigram).",
                "Examining Table 3, we observe that this is indeed the case for every classifier except na¨ıve Bayes.",
                "This difference in performance produced by the n-gram representation is statistically significant for each classifier except for na¨ıve Bayes and the accuracy metric for kNN (see Table 4).",
                "Na¨ıve Bayes poor performance with the n-gram representation is not surprising since the bag-of-n-grams causes excessive doublecounting as mentioned in Section 3.2.1; however, na¨ıve Bayes is not hurt at the sentence-level because the sparse examples provide few chances for agglomerative effects of double counting.",
                "In either case, when a language-modeling approach is desired, modeling the n-grams directly would be preferable to na¨ıve Bayes.",
                "More importantly for the n-gram hypothesis, the n-grams lead to the best document-level classifier performance as well.",
                "As would be expected, the difference between the sentence-level n-gram representation and unigram representation is small.",
                "This is because the window of text is so small that the unigram representation, when done at the sentence-level, implicitly picks up on the power of the n-grams.",
                "Further improvement would signify that the order of the words matter even when only considering a small sentence-size window.",
                "Therefore, the finer-grained sentence-level judgments allows a unigram representation to succeed but only when performed in a small window - behaving as an n-gram representation for all practical purposes.",
                "Document Winner Sentence Winner kNN Ngram Ngram na¨ıve Bayes Unigram Ngram SVM Ngram† Ngram Voted Perceptron Ngram† Ngram Table 4: Significance results for n-grams versus unigrams for document detection using document-level and sentence-level classifiers.",
                "When the F1 result is statistically significant, it is shown in bold.",
                "When the accuracy result is significant, it is shown with a † .",
                "F1 Winner Accuracy Winner kNN Sentence Sentence na¨ıve Bayes Sentence Sentence SVM Sentence Sentence Voted Perceptron Sentence Document Table 5: Significance results for sentence-level classifiers vs. document-level classifiers for the document detection problem.",
                "When the result is statistically significant, it is shown in bold.",
                "Further highlighting the improvement from finer-grained judgments and n-grams, Figure 3 graphically depicts the edge the SVM <br>sentence-level classifier</br> has over the standard bag-of-words approach with a precision-recall curve.",
                "In the high precision area of the graph, the consistent edge of the <br>sentence-level classifier</br> is rather impressive - continuing at precision 1 out to 0.1 recall.",
                "This would mean that a tenth of the users action-items would be placed at the top of their action-item sorted inbox.",
                "Additionally, the large separation at the top right of the curves corresponds to the area where the optimal F1 occurs for each classifier, agreeing with the large improvement from 0.6904 to 0.7682 in F1 score.",
                "Considering the relative unexplored nature of classification at the sentence-level, this gives great hope for further increases in performance.",
                "Accuracy F1 Unigram Ngram Unigram Ngram kNN 0.9519 0.9536 0.6540 0.6686 na¨ıve Bayes 0.9419 0.9550 0.6176 0.6676 SVM 0.9559 0.9579 0.6271 0.6672 Voted Perceptron 0.8895 0.9247 0.3744 0.5164 Table 6: Performance of the Sentence-Level Classifiers at Sentence Detection Although Cohen et al. [5] observed that the Voted Perceptron with a single training iteration outperformed SVM in a set of similar tasks, we see no such behavior here.",
                "This further strengthens the evidence that an alternate classifier with the bag-of-words representation could not reach the same level of performance.",
                "The Voted Perceptron classifier does improve when the number of training iterations are increased, but it is still lower than the SVM classifier.",
                "Sentence detection results are presented in Table 6.",
                "With regard to the sentence detection problem, we note that the F1 measure gives a better feel for the remaining room for improvement in this difficult problem.",
                "That is, unlike document detection where actionitem documents are fairly common, action-item sentences are very rare.",
                "Thus, as in other text problems, the accuracy numbers are deceptively high sheerly because of the default accuracy attainable by always predicting no.",
                "Although, the results here are significantly above-random, it is unclear what level of performance is necessary for sentence detection to be useful in and of itself and not simply as a means to document ranking and classification.",
                "Figure 4: Users find action-items quicker when assisted by a classification system.",
                "Finally, when considering a new type of classification task, one of the most basic questions is whether an accurate classifier built for the task can have an impact on the end-user.",
                "In order to demonstrate the impact this task can have on e-mail users, we conducted a user study using an earlier less-accurate version of the sentence classifier - where instead of using just a single sentence, a threesentence windowed-approach was used.",
                "There were three distinct sets of e-mail in which users had to find action-items.",
                "These sets were either presented in a random order (Unordered), ordered by the classifier (Ordered), or ordered by the classifier and with the 0.4 0.5 0.6 0.7 0.8 0.9 1 0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1 Precision Recall Action-Item Detection SVM Performance (Post Model Selection) Document Unigram Sentence Ngram Figure 3: Both n-grams and a small prediction window lead to consistent improvements over the standard approach. center sentence in the highest confidence window highlighted (Order+help).",
                "In order to perform fair comparisons between conditions, the overall number of tokens in each message set should be approximately equal; that is, the cognitive reading load should be approximately the same before the classifiers reordering.",
                "Additionally, users typically show practice effects by improving at the overall task and thus performing better at later message sets.",
                "This is typically handled by varying the ordering of the sets across users so that the means are comparable.",
                "While omitting further detail, we note the sets were balanced for the total number of tokens and a latin square design was used to balance practice effects.",
                "Figure 4 shows that at intervals of 5, 10, and 15 minutes, users consistently found significantly more action-items when assisted by the classifier, but were most critically aided in the first five minutes.",
                "Although, the classifier consistently aids the users, we did not gain an additional end-user impact by highlighting.",
                "As mentioned above, this might be a result of the large room for improvement that still exists for sentence detection, but anecdotal evidence suggests this might also be a result of how the information is presented to the user rather than the accuracy of sentence detection.",
                "For example, highlighting the wrong sentence near an actual action-item hurts the users trust, but if a vague indicator (e.g., an arrow) points to the approximate area the user is not aware of the near-miss.",
                "Since the user studies used a three sentence window, we believe this played a role as well as sentence detection accuracy. 4.6 Discussion In contrast to problems where n-grams have yielded little difference, we believe their power here stems from the fact that many of the meaningful n-grams for action-items consist of common words, e.g., let me know.",
                "Therefore, the document-level unigram approach cannot gain much leverage, even when modeling their joint probability correctly, since these words will often co-occur in the document but not necessarily in a phrase.",
                "Additionally, action-item detection is distinct from many text classification tasks in that a single sentence can change the class label of the document.",
                "As a result, good classifiers cannot rely on aggregating evidence from a large number of weak indicators across the entire document.",
                "Even though we discarded the header information, examining the top-ranked features at the document-level reveals that many of the features are names or parts of e-mail addresses that occurred in the body and are highly associated with e-mails that tend to contain many or no action-items.",
                "A few examples are terms such as org, bob, and gov.",
                "We note that these features will be sensitive to the particular distribution (senders/receivers) and thus the document-level approach may produce classifiers that transfer less readily to alternate contexts and users at different institutions.",
                "This points out that part of the problem of going beyond bag-of-words may be the methodology, and investigating such properties as learning curves and how well a model transfers may highlight differences in models which appear to have similar performance when tested on the distributions they were trained on.",
                "We are currently investigating whether the sentence-level classifiers do perform better over different test corpora without retraining. 5.",
                "FUTURE WORK While applying text classifiers at the document-level is fairly well-understood, there exists the potential for significantly increasing the performance of the sentence-level classifiers.",
                "Such methods include alternate ways of combining the predictions over each sentence, weightings other than tfidf, which may not be appropriate since sentences are small, better sentence segmentation, and other types of phrasal analysis.",
                "Additionally, named entity tagging, time expressions, etc., seem likely candidates for features that can further improve this task.",
                "We are currently pursuing some of these avenues to see what additional gains these offer.",
                "Finally, it would be interesting to investigate the best methods for combining the document-level and sentence-level classifiers.",
                "Since the simple bag-of-words representation at the document-level leads to a learned model that behaves somewhat like a context-specific prior dependent on the sender/receiver and general topic, a first choice would be to treat it as such when combining probability estimates with the <br>sentence-level classifier</br>.",
                "Such a model might serve as a general example for other problems where bag-of-words can establish a baseline model but richer approaches are needed to achieve performance beyond that baseline. 6.",
                "SUMMARY AND CONCLUSIONS The effectiveness of sentence-level detection argues that labeling at the sentence-level provides significant value.",
                "Further experiments are needed to see how this interacts with the amount of training data available.",
                "Sentence detection that is then agglomerated to document-level detection works surprisingly better given low recall than would be expected with sentence-level items.",
                "This, in turn, indicates that improved sentence segmentation methods could yield further improvements in classification.",
                "In this work, we examined how action-items can be effectively detected in e-mails.",
                "Our empirical analysis has demonstrated that n-grams are of key importance to making the most of documentlevel judgments.",
                "When finer-grained judgments are available, then a standard bag-of-words approach using a small (sentence) window size and automatic segmentation techniques can produce results almost as good as the n-gram based approaches.",
                "Acknowledgments This material is based upon work supported by the Defense Advanced Research Projects Agency (DARPA) under Contract No.",
                "NBCHD030010.",
                "Any opinions, findings and conclusions or recommendations expressed in this material are those of the author(s) and do not necessarily reflect the views of the Defense Advanced Research Projects Agency (DARPA), or the Department of InteriorNational Business Center (DOI-NBC).",
                "We would like to extend our sincerest thanks to Jill Lehman whose efforts in data collection were essential in constructing the corpus, and both Jill and Aaron Steinfeld for their direction of the HCI experiments.",
                "We would also like to thank Django Wexler for constructing and supporting the corpus labeling tools and Curtis Huttenhowers support of the text preprocessing package.",
                "Finally, we gratefully acknowledge Scott Fahlman for his encouragement and useful discussions on this topic. 7.",
                "REFERENCES [1] J. Allan, J. Carbonell, G. Doddington, J. Yamron, and Y. Yang.",
                "Topic detection and tracking pilot study: Final report.",
                "In Proceedings of the DARPA Broadcast News Transcription and Understanding Workshop, Washington, D.C., 1998. [2] C. Apte, F. Damerau, and S. M. Weiss.",
                "Automated learning of decision rules for text categorization.",
                "ACM Transactions on Information Systems, 12(3):233-251, July 1994. [3] J. Carletta.",
                "Assessing agreement on classification tasks: The kappa statistic.",
                "Computational Linguistics, 22(2):249-254, 1996. [4] J. Carroll.",
                "High precision extraction of grammatical relations.",
                "In Proceedings of the 19th International Conference on Computational Linguistics (COLING), pages 134-140, 2002. [5] W. W. Cohen, V. R. Carvalho, and T. M. Mitchell.",
                "Learning to classify email into speech acts.",
                "In EMNLP-2004 (Conference on Empirical Methods in Natural Language Processing), pages 309-316, 2004. [6] S. Corston-Oliver, E. Ringger, M. Gamon, and R. Campbell.",
                "Task-focused summarization of email.",
                "In Text Summarization Branches Out: Proceedings of the ACL-04 Workshop, pages 43-50, 2004. [7] A. Culotta, R. Bekkerman, and A. McCallum.",
                "Extracting social networks and contact information from email and the web.",
                "In CEAS-2004 (Conference on Email and Anti-Spam), Mountain View, CA, July 2004. [8] L. Devroye, L. Gy¨orfi, and G. Lugosi.",
                "A Probabilistic Theory of Pattern Recognition.",
                "Springer-Verlag, New York, NY, 1996. [9] S. T. Dumais, J. Platt, D. Heckerman, and M. Sahami.",
                "Inductive learning algorithms and representations for text categorization.",
                "In CIKM 98, Proceedings of the 7th ACM Conference on Information and Knowledge Management, pages 148-155, 1998. [10] Y. Freund and R. Schapire.",
                "Large margin classification using the perceptron algorithm.",
                "Machine Learning, 37(3):277-296, 1999. [11] T. Joachims.",
                "Making large-scale svm learning practical.",
                "In B. Sch¨olkopf, C. J. Burges, and A. J. Smola, editors, Advances in Kernel Methods - Support Vector Learning, pages 41-56.",
                "MIT Press, 1999. [12] L. S. Larkey.",
                "A patent search and classification system.",
                "In Proceedings of the Fourth ACM Conference on Digital Libraries, pages 179 - 187, 1999. [13] D. D. Lewis.",
                "An evaluation of phrasal and clustered representations on a text categorization task.",
                "In SIGIR 92, Proceedings of the 15th Annual International ACM Conference on Research and Development in Information Retrieval, pages 37-50, 1992. [14] Y. Liu, J. Carbonell, and R. Jin.",
                "A pairwise ensemble approach for accurate genre classification.",
                "In Proceedings of the European Conference on Machine Learning (ECML), 2003. [15] Y. Liu, R. Yan, R. Jin, and J. Carbonell.",
                "A comparison study of kernels for multi-label text classification using category association.",
                "In The Twenty-first International Conference on Machine Learning (ICML), 2004. [16] A. McCallum and K. Nigam.",
                "A comparison of event models for naive bayes text classification.",
                "In Working Notes of AAAI 98 (The 15th National Conference on Artificial Intelligence), Workshop on Learning for Text Categorization, pages 41-48, 1998.",
                "TR WS-98-05. [17] F. Sebastiani.",
                "Machine learning in automated text categorization.",
                "ACM Computing Surveys, 34(1):1-47, March 2002. [18] C. J. van Rijsbergen.",
                "Information Retrieval.",
                "Butterworths, London, 1979. [19] Y. Yang.",
                "An evaluation of statistical approaches to text categorization.",
                "Information Retrieval, 1(1/2):67-88, 1999. [20] Y. Yang, J. Carbonell, R. Brown, T. Pierce, B. T. Archibald, and X. Liu.",
                "Learning approaches to topic detection and tracking.",
                "IEEE EXPERT, Special Issue on Applications of Intelligent Information Retrieval, 1999. [21] Y. Yang and X. Liu.",
                "A re-examination of text categorization methods.",
                "In SIGIR 99, Proceedings of the 22nd Annual International ACM Conference on Research and Development in Information Retrieval, pages 42-49, 1999. [22] Y. Yang, J. Zhang, J. Carbonell, and C. Jin.",
                "Topic-conditioned novelty detection.",
                "In Proceedings of the ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, July 2002."
            ],
            "original_annotated_samples": [],
            "translated_annotated_samples": [
                "Una vez que un \"clasificador de nivel de oración\" ha hecho una predicción para cada oración, debemos combinar estas predicciones para hacer una predicción a nivel de documento y una puntuación a nivel de documento.",
                "Al resaltar aún más la mejora de los juicios de grano más fino y los N-Grams, la Figura 3 representa gráficamente el borde que el \"clasificador de nivel de oración\" SVM tiene sobre el enfoque estándar de la bolsa de palabras con una curva de recolección de precisión.",
                "En el área de alta precisión del gráfico, el borde consistente del \"clasificador a nivel de oración\" es bastante impresionante, que continúa en la precisión 1 a 0.1 de recuperación.",
                "Dado que la simple representación de la bolsa de las palabras en el nivel de documento conduce a un modelo aprendido que se comporta como un contexto específico de la previa dependiente del remitente/receptor y el tema general, una primera opción sería tratarlo como tal al combinarEstimaciones de probabilidad con el \"clasificador a nivel de oración\"."
            ],
            "translated_text": "",
            "candidates": [
                "clasificador a nivel de oración",
                "clasificador de nivel de oración",
                "clasificador a nivel de oración",
                "clasificador de nivel de oración",
                "clasificador a nivel de oración",
                "clasificador a nivel de oración",
                "clasificador a nivel de oración",
                "clasificador a nivel de oración"
            ],
            "error": []
        },
        "text classification": {
            "translated_key": "clasificación de texto",
            "is_in_text": true,
            "original_annotated_sentences": [
                "Feature Representation for Effective Action-Item Detection Paul N. Bennett Computer Science Department Carnegie Mellon University Pittsburgh, PA 15213 pbennett+@cs.cmu.edu Jaime Carbonell Language Technologies Institute.",
                "Carnegie Mellon University Pittsburgh, PA 15213 jgc+@cs.cmu.edu ABSTRACT E-mail users face an ever-growing challenge in managing their inboxes due to the growing centrality of email in the workplace for task assignment, action requests, and other roles beyond information dissemination.",
                "Whereas Information Retrieval and Machine Learning techniques are gaining initial acceptance in spam filtering and automated folder assignment, this paper reports on a new task: automated action-item detection, in order to flag emails that require responses, and to highlight the specific passage(s) indicating the request(s) for action.",
                "Unlike standard topic-driven <br>text classification</br>, action-item detection requires inferring the senders intent, and as such responds less well to pure bag-of-words classification.",
                "However, using enriched feature sets, such as n-grams (up to n=4) with chi-squared feature selection, and contextual cues for action-item location improve performance by up to 10% over unigrams, using in both cases state of the art classifiers such as SVMs with automated model selection via embedded cross-validation.",
                "Categories and Subject Descriptors H.3.3 [Information Storage and Retrieval]: Information Search and Retrieval; I.2.6 [Artificial Intelligence]: Learning; I.5.4 [Pattern Recognition]: Applications General Terms Experimentation 1.",
                "INTRODUCTION E-mail users are facing an increasingly difficult task of managing their inboxes in the face of mounting challenges that result from rising e-mail usage.",
                "This includes prioritizing e-mails over a range of sources from business partners to family members, filtering and reducing junk e-mail, and quickly managing requests that demand From: Henry Hutchins <hhutchins@innovative.company.com> To: Sara Smith; Joe Johnson; William Woolings Subject: meeting with prospective customers Sent: Fri 12/10/2005 8:08 AM Hi All, Id like to remind all of you that the group from GRTY will be visiting us next Friday at 4:30 p.m.",
                "The current schedule looks like this: + 9:30 a.m.",
                "Informal Breakfast and Discussion in Cafeteria + 10:30 a.m. Company Overview + 11:00 a.m.",
                "Individual Meetings (Continue Over Lunch) + 2:00 p.m. Tour of Facilities + 3:00 p.m.",
                "Sales Pitch In order to have this go off smoothly, I would like to practice the presentation well in advance.",
                "As a result, I will need each of your parts by Wednesday.",
                "Keep up the good work! -Henry Figure 1: An E-mail with emphasized Action-Item, an explicit request that requires the recipients attention or action. the receivers attention or action.",
                "Automated action-item detection targets the third of these problems by attempting to detect which e-mails require an action or response with information, and within those e-mails, attempting to highlight the sentence (or other passage length) that directly indicates the action request.",
                "Such a detection system can be used as one part of an e-mail agent which would assist a user in processing important e-mails quicker than would have been possible without the agent.",
                "We view action-item detection as one necessary component of a successful e-mail agent which would perform spam detection, action-item detection, topic classification and priority ranking, among other functions.",
                "The utility of such a detector can manifest as a method of prioritizing e-mails according to task-oriented criteria other than the standard ones of topic and sender or as a means of ensuring that the email user hasnt dropped the proverbial ball by forgetting to address an action request.",
                "Action-item detection differs from standard <br>text classification</br> in two important ways.",
                "First, the user is interested both in detecting whether an email contains action items and in locating exactly where these action item requests are contained within the email body.",
                "In contrast, standard text categorization merely assigns a topic label to each text, whether that label corresponds to an e-mail folder or a controlled indexing vocabulary [12, 15, 22].",
                "Second, action-item detection attempts to recover the email senders intent - whether she means to elicit response or action on the part of the receiver; note that for this task, classifiers using only unigrams as features do not perform optimally, as evidenced in our results below.",
                "Instead we find that we need more information-laden features such as higher-order n-grams.",
                "Text categorization by topic, on the other hand, works very well using just individual words as features [2, 9, 13, 17].",
                "In fact, genre-classification, which one would think may require more than a bag-of-words approach, also works quite well using just unigram features [14].",
                "Topic detection and tracking (TDT), also works well with unigram feature sets [1, 20].",
                "We believe that action-item detection is one of the first clear instances of an IR-related task where we must move beyond bag-of-words to achieve high performance, albeit not too far, as bag-of-n-grams seem to suffice.",
                "We first review related work for similar <br>text classification</br> problems such as e-mail priority ranking and speech act identification.",
                "Then we more formally define the action-item detection problem, discuss the aspects that distinguish it from more common problems like topic classification, and highlight the challenges in constructing systems that can perform well at the sentence and document level.",
                "From there, we move to a discussion of feature representation and selection techniques appropriate for this problem and how standard <br>text classification</br> approaches can be adapted to smoothly move from the sentence-level detection problem to the documentlevel classification problem.",
                "We then conduct an empirical analysis that helps us determine the effectiveness of our feature extraction procedures as well as establish baselines for a number of classification algorithms on this task.",
                "Finally, we summarize this papers contributions and consider interesting directions for future work. 2.",
                "RELATED WORK Several other researchers have considered very similar <br>text classification</br> tasks.",
                "Cohen et al. [5] describe an ontology of speech acts, such as Propose a Meeting, and attempt to predict when an e-mail contains one of these speech acts.",
                "We consider action-items to be an important specific type of speech act that falls within their more general classification.",
                "While they provide results for several classification methods, their methods only make use of human judgments at the document-level.",
                "In contrast, we consider whether accuracy can be increased by using finer-grained human judgments that mark the specific sentences and phrases of interest.",
                "Corston-Oliver et al. [6] consider detecting items in e-mail to Put on a To-Do List.",
                "This classification task is very similar to ours except they do not consider simple factual questions to belong to this category.",
                "We include questions, but note that not all questions are action-items - some are rhetorical or simply social convention, How are you?.",
                "From a learning perspective, while they make use of judgments at the sentence-level, they do not explicitly compare what if any benefits finer-grained judgments offer.",
                "Additionally, they do not study alternative choices or approaches to the classification task.",
                "Instead, they simply apply a standard SVM at the sentence-level and focus primarily on a linguistic analysis of how the sentence can be logically reformulated before adding it to the task list.",
                "In this study, we examine several alternative classification methods, compare document-level and sentence-level approaches and analyze the machine learning issues implicit in these problems.",
                "Interest in a variety of learning tasks related to e-mail has been rapidly growing in the recent literature.",
                "For example, in a forum dedicated to e-mail learning tasks, Culotta et al. [7] presented methods for learning social networks from e-mail.",
                "In this work, we do not focus on peer relationships; however, such methods could complement those here since peer relationships often influence word choice when requesting an action. 3.",
                "PROBLEM DEFINITION & APPROACH In contrast to previous work, we explicitly focus on the benefits that finer-grained, more costly, sentence-level human judgments offer over coarse-grained document-level judgments.",
                "Additionally, we consider multiple standard <br>text classification</br> approaches and analyze both the quantitative and qualitative differences that arise from taking a document-level vs. a sentence-level approach to classification.",
                "Finally, we focus on the representation necessary to achieve the most competitive performance. 3.1 Problem Definition In order to provide the most benefit to the user, a system would not only detect the document, but it would also indicate the specific sentences in the e-mail which contain the action-items.",
                "Therefore, there are three basic problems: 1.",
                "Document detection: Classify a document as to whether or not it contains an action-item. 2.",
                "Document ranking: Rank the documents such that all documents containing action-items occur as high as possible in the ranking. 3.",
                "Sentence detection: Classify each sentence in a document as to whether or not it is an action-item.",
                "As in most Information Retrieval tasks, the weight the evaluation metric should give to precision and recall depends on the nature of the application.",
                "In situations where a user will eventually read all received messages, ranking (e.g., via precision at recall of 1) may be most important since this will help encourage shorter delays in communications between users.",
                "In contrast, high-precision detection at low recall will be of increasing importance when the user is under severe time-pressure and therefore will likely not read all mail.",
                "This can be the case for crisis managers during disaster management.",
                "Finally, sentence detection plays a role in both timepressure situations and simply to alleviate the users required time to gist the message. 3.2 Approach As mentioned above, the labeled data can come in one of two forms: a document-labeling provides a yes/no label for each document as to whether it contains an action-item; a phrase-labeling provides only a yes label for the specific items of interest.",
                "We term the human judgments a phrase-labeling since the users view of the action-item may not correspond with actual sentence boundaries or predicted sentence boundaries.",
                "Obviously, it is straightforward to generate a document-labeling consistent with a phrase-labeling by labeling a document yes if and only if it contains at least one phrase labeled yes.",
                "To train classifiers for this task, we can take several viewpoints related to both the basic problems we have enumerated and the form of the labeled data.",
                "The document-level view treats each e-mail as a learning instance with an associated class-label.",
                "Then, the document can be converted to a feature-value vector and learning progresses as usual.",
                "Applying a document-level classifier to document detection and ranking is straightforward.",
                "In order to apply it to sentence detection, one must make additional steps.",
                "For example, if the classifier predicts a document contains an action-item, then areas of the document that contain a high-concentration of words which the model weights heavily in favor of action-items can be indicated.",
                "The obvious benefit of the document-level approach is that training set collection costs are lower since the user only has to specify whether or not an e-mail contains an action-item and not the specific sentences.",
                "In the sentence-level view, each e-mail is automatically segmented into sentences, and each sentence is treated as a learning instance with an associated class-label.",
                "Since the phrase-labeling provided by the user may not coincide with the automatic segmentation, we must determine what label to assign a partially overlapping sentence when converting it to a learning instance.",
                "Once trained, applying the resulting classifiers to sentence detection is now straightforward, but in order to apply the classifiers to document detection and document ranking, the individual predictions over each sentence must be aggregated in order to make a document-level prediction.",
                "This approach has the potential to benefit from morespecific labels that enable the learner to focus attention on the key sentences instead of having to learn based on data that the majority of the words in the e-mail provide no or little information about class membership. 3.2.1 Features Consider some of the phrases that might constitute part of an action item: would like to know, let me know, as soon as possible, have you.",
                "Each of these phrases consists of common words that occur in many e-mails.",
                "However, when they occur in the same sentence, they are far more indicative of an action-item.",
                "Additionally, order can be important: consider have you versus you have.",
                "Because of this, we posit that n-grams play a larger role in this problem than is typical of problems like topic classification.",
                "Therefore, we consider all n-grams up to size 4.",
                "When using n-grams, if we find an n-gram of size 4 in a segment of text, we can represent the text as just one occurrence of the ngram or as one occurrence of the n-gram and an occurrence of each smaller n-gram contained by it.",
                "We choose the second of these alternatives since this will allow the algorithm itself to smoothly back-off in terms of recall.",
                "Methods such as na¨ıve Bayes may be hurt by such a representation because of double-counting.",
                "Since sentence-ending punctuation can provide information, we retain the terminating punctuation token when it is identifiable.",
                "Additionally, we add a beginning-of-sentence and end-of-sentence token in order to capture patterns that are often indicators at the beginning or end of a sentence.",
                "Assuming proper punctuation, these extra tokens are unnecessary, but often e-mail lacks proper punctuation.",
                "In addition, for the sentence-level classifiers that use ngrams, we additionally code for each sentence a binary encoding of the position of the sentence relative to the document.",
                "This encoding has eight associated features that represent which octile (the first eighth, second eighth, etc.) contains the sentence. 3.2.2 Implementation Details In order to compare the document-level to the sentence-level approach, we compare predictions at the document-level.",
                "We do not address how to use a document-level classifier to make predictions at the sentence-level.",
                "In order to automatically segment the text of the e-mail, we use the RASP statistical parser [4].",
                "Since the automatically segmented sentences may not correspond directly with the phrase-level boundaries, we treat any sentence that contains at least 30% of a marked action-item segment as an action-item.",
                "When evaluating sentencedetection for the sentence-level system, we use these class labels as ground truth.",
                "Since we are not evaluating multiple segmentation approaches, this does not bias any of the methods.",
                "If multiple segmentation systems were under evaluation, one would need to use a metric that matched predicted positive sentences to phrases labeled positive.",
                "The metric would need to punish overly long true predictions as well as too short predictions.",
                "Our criteria for converting to labeled instances implicitly includes both criteria.",
                "Since the segmentation is fixed, an overly long prediction would be predicting yes for many no instances since presumably the extra length corresponds to additional segmented sentences all of which do not contain 30% of action-item.",
                "Likewise, a too short prediction must correspond to a small sentence included in the action-item but not constituting all of the action-item.",
                "Therefore, in order to consider the prediction to be too short, there will be an additional preceding/following sentence that is an action-item where we incorrectly predicted no.",
                "Once a sentence-level classifier has made a prediction for each sentence, we must combine these predictions to make both a document-level prediction and a document-level score.",
                "We use the simple policy of predicting positive when any of the sentences is predicted positive.",
                "In order to produce a document score for ranking, the confidence that the document contains an action-item is: ψ(d) = 1 n(d) s∈d|π(s)=1 ψ(s) if for any s ∈ d, π(s) = 1 1 n(d) maxs∈d ψ(s) o.w. where s is a sentence in document d, π is the classifiers 1/0 prediction, ψ is the score the classifier assigns as its confidence that π(s) = 1, and n(d) is the greater of 1 and the number of (unigram) tokens in the document.",
                "In other words, when any sentence is predicted positive, the document score is the length normalized sum of the sentence scores above threshold.",
                "When no sentence is predicted positive, the document score is the maximum sentence score normalized by length.",
                "As in other text problems, we are more likely to emit false positives for documents with more words or sentences.",
                "Thus we include a length normalization factor. 4.",
                "EXPERIMENTAL ANALYSIS 4.1 The Data Our corpus consists of e-mails obtained from volunteers at an educational institution and cover subjects such as: organizing a research workshop, arranging for job-candidate interviews, publishing proceedings, and talk announcements.",
                "The messages were anonymized by replacing the names of each individual and institution with a pseudonym.1 After attempting to identify and eliminate duplicate e-mails, the corpus contains 744 e-mail messages.",
                "After identity anonymization, the corpora has three basic versions.",
                "Quoted material refers to the text of a previous e-mail that an author often leaves in an e-mail message when responding to the e-mail.",
                "Quoted material can act as noise when learning since it may include action-items from previous messages that are no longer relevant.",
                "To isolate the effects of quoted material, we have three versions of the corpora.",
                "The raw form contains the basic messages.",
                "The auto-stripped version contains the messages after quoted material has been automatically removed.",
                "The hand-stripped version contains the messages after quoted material has been removed by a human.",
                "Additionally, the hand-stripped version has had any xml content and e-mail signatures removed - leaving only the essential content of the message.",
                "The studies reported here are performed with the hand-stripped version.",
                "This allows us to balance the cognitive load in terms of number of tokens that must be read in the user-studies we report - including quoted material would complicate the user studies since some users might skip the material while others read it.",
                "Additionally, ensuring all quoted material is removed 1 We have an even more highly anonymized version of the corpus that can be made available for some outside experimentation.",
                "Please contact the authors for more information on obtaining this data. prevents tainting the cross-validation since otherwise a test item could occur as quoted material in a training document. 4.1.1 Data Labeling Two human annotators labeled each message as to whether or not it contained an action-item.",
                "In addition, they identified each segment of the e-mail which contained an action-item.",
                "A segment is a contiguous section of text selected by the human annotators and may span several sentences or a complete phrase contained in a sentence.",
                "They were instructed that an action item is an explicit request for information that requires the recipients attention or a required action and told to highlight the phrases or sentences that make up the request.",
                "Annotator 1 No Yes Annotator 2 No 391 26 Yes 29 298 Table 1: Agreement of Human Annotators at Document Level Annotator One labeled 324 messages as containing action items.",
                "Annotator Two labeled 327 messages as containing action items.",
                "The agreement of the human annotators is shown in Tables 1 and 2.",
                "The annotators are said to agree at the document-level when both marked the same document as containing no action-items or both marked at least one action-item regardless of whether the text segments were the same.",
                "At the document-level, the annotators agreed 93% of the time.",
                "The kappa statistic [3, 5] is often used to evaluate inter-annotator agreement: κ = A − R 1 − R A is the empirical estimate of the probability of agreement.",
                "R is the empirical estimate of the probability of random agreement given the empirical class priors.",
                "A value close to −1 implies the annotators agree far less often than would be expected randomly, while a value close to 1 means they agree more often than randomly expected.",
                "At the document-level, the kappa statistic for inter-annotator agreement is 0.85.",
                "This value is both strong enough to expect the problem to be learnable and is comparable with results for similar tasks [5, 6].",
                "In order to determine the sentence-level agreement, we use each judgment to create a sentence-corpus with labels as described in Section 3.2.2, then consider the agreement over these sentences.",
                "This allows us to compare agreement over no judgments.",
                "We perform this comparison over the hand-stripped corpus since that eliminates spurious no judgments that would come from including quoted material, etc.",
                "Both annotators were free to label the subject as an action-item, but since neither did, we omit the subject line of the message as well.",
                "This only reduces the number of no agreements.",
                "This leaves 6301 automatically segmented sentences.",
                "At the sentence-level, the annotators agreed 98% of the time, and the kappa statistic for inter-annotator agreement is 0.82.",
                "In order to produce one single set of judgments, the human annotators went through each annotation where there was disagreement and came to a consensus opinion.",
                "The annotators did not collect statistics during this process but anecdotally reported that the majority of disagreements were either cases of clear annotator oversight or different interpretations of conditional statements.",
                "For example, If you would like to keep your job, come to tomorrows meeting implies a required action where If you would like to join Annotator 1 No Yes Annotator 2 No 5810 65 Yes 74 352 Table 2: Agreement of Human Annotators at Sentence Level the football betting pool, come to tomorrows meeting does not.",
                "The first would be an action-item in most contexts while the second would not.",
                "Of course, many conditional statements are not so clearly interpretable.",
                "After reconciling the judgments there are 416 e-mails with no action-items and 328 e-mails containing actionitems.",
                "Of the 328 e-mails containing action-items, 259 messages have one action-item segment; 55 messages have two action-item segments; 11 messages have three action-item segments.",
                "Two messages have four action-item segments, and one message has six action-item segments.",
                "Computing the sentence-level agreement using the reconciled gold standard judgments with each of the annotators individual judgments gives a kappa of 0.89 for Annotator One and a kappa of 0.92 for Annotator Two.",
                "In terms of message characteristics, there were on average 132 content tokens in the body after stripping.",
                "For action-item messages, there were 115.",
                "However, by examining Figure 2 we see the length distributions are nearly identical.",
                "As would be expected for e-mail, it is a long-tailed distribution with about half the messages having more than 60 tokens in the body (this paragraph has 65 tokens). 4.2 Classifiers For this experiment, we have selected a variety of standard <br>text classification</br> algorithms.",
                "In selecting algorithms, we have chosen algorithms that are not only known to work well but which differ along such lines as discriminative vs. generative and lazy vs. eager.",
                "We have done this in order to provide both a competitive and thorough sampling of learning methods for the task at hand.",
                "This is important since it is easy to improve a strawman classifier by introducing a new representation.",
                "By thoroughly sampling alternative classifier choices we demonstrate that representation improvements over bag-of-words are not due to using the information in the bag-of-words poorly. 4.2.1 kNN We employ a standard variant of the k-nearest neighbor algorithm used in <br>text classification</br>, kNN with s-cut score thresholding [19].",
                "We use a tfidf-weighting of the terms with a distanceweighted vote of the neighbors to compute the score before thresholding it.",
                "In order to choose the value of s for thresholding, we perform leave-one-out cross-validation over the training set.",
                "The value of k is set to be 2( log2 N + 1) where N is the number of training points.",
                "This rule for choosing k is theoretically motivated by results which show such a rule converges to the optimal classifier as the number of training points increases [8].",
                "In practice, we have also found it to be a computational convenience that frequently leads to comparable results with numerically optimizing k via a cross-validation procedure. 4.2.2 Na¨ıve Bayes We use a standard multinomial na¨ıve Bayes classifier [16].",
                "In using this classifier, we smoothed word and class probabilities using a Bayesian estimate (with the word prior) and a Laplace m-estimate, respectively. 0 20 40 60 80 100 120 140 160 0 200 400 600 800 1000 1200 1400 NumberofMessages Number of Tokens All Messages Action-Item Messages 0 0.02 0.04 0.06 0.08 0.1 0.12 0.14 0.16 0.18 0.2 0 200 400 600 800 1000 1200 1400 PercentageofMessages Number of Tokens All Messages Action-Item Messages Figure 2: The Histogram (left) and Distribution (right) of Message Length.",
                "A bin size of 20 words was used.",
                "Only tokens in the body after hand-stripping were counted.",
                "After stripping, the majority of words left are usually actual message content.",
                "Classifiers Document Unigram Document Ngram Sentence Unigram Sentence Ngram F1 kNN 0.6670 ± 0.0288 0.7108 ± 0.0699 0.7615 ± 0.0504 0.7790 ± 0.0460 na¨ıve Bayes 0.6572 ± 0.0749 0.6484 ± 0.0513 0.7715 ± 0.0597 0.7777 ± 0.0426 SVM 0.6904 ± 0.0347 0.7428 ± 0.0422 0.7282 ± 0.0698 0.7682 ± 0.0451 Voted Perceptron 0.6288 ± 0.0395 0.6774 ± 0.0422 0.6511 ± 0.0506 0.6798 ± 0.0913 Accuracy kNN 0.7029 ± 0.0659 0.7486 ± 0.0505 0.7972 ± 0.0435 0.8092 ± 0.0352 na¨ıve Bayes 0.6074 ± 0.0651 0.5816 ± 0.1075 0.7863 ± 0.0553 0.8145 ± 0.0268 SVM 0.7595 ± 0.0309 0.7904 ± 0.0349 0.7958 ± 0.0551 0.8173 ± 0.0258 Voted Perceptron 0.6531 ± 0.0390 0.7164 ± 0.0376 0.6413 ± 0.0833 0.7082 ± 0.1032 Table 3: Average Document-Detection Performance during Cross-Validation for Each Method and the Sample Standard Deviation (Sn−1) in italics.",
                "The best performance for each classifier is shown in bold. 4.2.3 SVM We have used a linear SVM with a tfidf feature representation and L2-norm as implemented in the SVMlight package v6.01 [11].",
                "All default settings were used. 4.2.4 Voted Perceptron Like the SVM, the Voted Perceptron is a kernel-based learning method.",
                "We use the same feature representation and kernel as we have for the SVM, a linear kernel with tfidf-weighting and an L2-norm.",
                "The voted perceptron is an online-learning method that keeps a history of past perceptrons used, as well as a weight signifying how often that perceptron was correct.",
                "With each new training example, a correct classification increases the weight on the current perceptron and an incorrect classification updates the perceptron.",
                "The output of the classifier uses the weights on the perceptra to make a final voted classification.",
                "When used in an offline-manner, multiple passes can be made through the training data.",
                "Both the voted perceptron and the SVM give a solution from the same hypothesis space - in this case, a linear classifier.",
                "Furthermore, it is well-known that the Voted Perceptron increases the margin of the solution after each pass through the training data [10].",
                "Since Cohen et al. [5] obtain worse results using an SVM than a Voted Perceptron with one training iteration, they conclude that the best solution for detecting speech acts may not lie in an area with a large margin.",
                "Because their tasks are highly similar to ours, we employ both classifiers to ensure we are not overlooking a competitive alternative classifier to the SVM for the basic bag-of-words representation. 4.3 Performance Measures To compare the performance of the classification methods, we look at two standard performance measures, F1 and accuracy.",
                "The F1 measure [18, 21] is the harmonic mean of precision and recall where Precision = Correct Positives Predicted Positives and Recall = Correct Positives Actual Positives . 4.4 Experimental Methodology We perform standard 10-fold cross-validation on the set of documents.",
                "For the sentence-level approach, all sentences in a document are either entirely in the training set or entirely in the test set for each fold.",
                "For significance tests, we use a two-tailed t-test [21] to compare the values obtained during each cross-validation fold with a p-value of 0.05.",
                "Feature selection was performed using the chi-squared statistic.",
                "Different levels of feature selection were considered for each classifier.",
                "Each of the following number of features was tried: 10, 25, 50, 100, 250, 750, 1000, 2000, 4000.",
                "There are approximately 4700 unigram tokens without feature selection.",
                "In order to choose the number of features to use for each classifier, we perform nested cross-validation and choose the settings that yield the optimal document-level F1 for that classifier.",
                "For this study, only the body of each e-mail message was used.",
                "Feature selection is always applied to all candidate features.",
                "That is, for the n-gram representation, the n-grams and position features are also subject to removal by the feature selection method. 4.5 Results The results for document-level classification are given in Table 3.",
                "The primary hypothesis we are concerned with is that n-grams are critical for this task; if this is true, we expect to see a significant gap in performance between the document-level classifiers that use n-grams (denoted Document Ngram) and those using only unigram features (denoted Document Unigram).",
                "Examining Table 3, we observe that this is indeed the case for every classifier except na¨ıve Bayes.",
                "This difference in performance produced by the n-gram representation is statistically significant for each classifier except for na¨ıve Bayes and the accuracy metric for kNN (see Table 4).",
                "Na¨ıve Bayes poor performance with the n-gram representation is not surprising since the bag-of-n-grams causes excessive doublecounting as mentioned in Section 3.2.1; however, na¨ıve Bayes is not hurt at the sentence-level because the sparse examples provide few chances for agglomerative effects of double counting.",
                "In either case, when a language-modeling approach is desired, modeling the n-grams directly would be preferable to na¨ıve Bayes.",
                "More importantly for the n-gram hypothesis, the n-grams lead to the best document-level classifier performance as well.",
                "As would be expected, the difference between the sentence-level n-gram representation and unigram representation is small.",
                "This is because the window of text is so small that the unigram representation, when done at the sentence-level, implicitly picks up on the power of the n-grams.",
                "Further improvement would signify that the order of the words matter even when only considering a small sentence-size window.",
                "Therefore, the finer-grained sentence-level judgments allows a unigram representation to succeed but only when performed in a small window - behaving as an n-gram representation for all practical purposes.",
                "Document Winner Sentence Winner kNN Ngram Ngram na¨ıve Bayes Unigram Ngram SVM Ngram† Ngram Voted Perceptron Ngram† Ngram Table 4: Significance results for n-grams versus unigrams for document detection using document-level and sentence-level classifiers.",
                "When the F1 result is statistically significant, it is shown in bold.",
                "When the accuracy result is significant, it is shown with a † .",
                "F1 Winner Accuracy Winner kNN Sentence Sentence na¨ıve Bayes Sentence Sentence SVM Sentence Sentence Voted Perceptron Sentence Document Table 5: Significance results for sentence-level classifiers vs. document-level classifiers for the document detection problem.",
                "When the result is statistically significant, it is shown in bold.",
                "Further highlighting the improvement from finer-grained judgments and n-grams, Figure 3 graphically depicts the edge the SVM sentence-level classifier has over the standard bag-of-words approach with a precision-recall curve.",
                "In the high precision area of the graph, the consistent edge of the sentence-level classifier is rather impressive - continuing at precision 1 out to 0.1 recall.",
                "This would mean that a tenth of the users action-items would be placed at the top of their action-item sorted inbox.",
                "Additionally, the large separation at the top right of the curves corresponds to the area where the optimal F1 occurs for each classifier, agreeing with the large improvement from 0.6904 to 0.7682 in F1 score.",
                "Considering the relative unexplored nature of classification at the sentence-level, this gives great hope for further increases in performance.",
                "Accuracy F1 Unigram Ngram Unigram Ngram kNN 0.9519 0.9536 0.6540 0.6686 na¨ıve Bayes 0.9419 0.9550 0.6176 0.6676 SVM 0.9559 0.9579 0.6271 0.6672 Voted Perceptron 0.8895 0.9247 0.3744 0.5164 Table 6: Performance of the Sentence-Level Classifiers at Sentence Detection Although Cohen et al. [5] observed that the Voted Perceptron with a single training iteration outperformed SVM in a set of similar tasks, we see no such behavior here.",
                "This further strengthens the evidence that an alternate classifier with the bag-of-words representation could not reach the same level of performance.",
                "The Voted Perceptron classifier does improve when the number of training iterations are increased, but it is still lower than the SVM classifier.",
                "Sentence detection results are presented in Table 6.",
                "With regard to the sentence detection problem, we note that the F1 measure gives a better feel for the remaining room for improvement in this difficult problem.",
                "That is, unlike document detection where actionitem documents are fairly common, action-item sentences are very rare.",
                "Thus, as in other text problems, the accuracy numbers are deceptively high sheerly because of the default accuracy attainable by always predicting no.",
                "Although, the results here are significantly above-random, it is unclear what level of performance is necessary for sentence detection to be useful in and of itself and not simply as a means to document ranking and classification.",
                "Figure 4: Users find action-items quicker when assisted by a classification system.",
                "Finally, when considering a new type of classification task, one of the most basic questions is whether an accurate classifier built for the task can have an impact on the end-user.",
                "In order to demonstrate the impact this task can have on e-mail users, we conducted a user study using an earlier less-accurate version of the sentence classifier - where instead of using just a single sentence, a threesentence windowed-approach was used.",
                "There were three distinct sets of e-mail in which users had to find action-items.",
                "These sets were either presented in a random order (Unordered), ordered by the classifier (Ordered), or ordered by the classifier and with the 0.4 0.5 0.6 0.7 0.8 0.9 1 0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1 Precision Recall Action-Item Detection SVM Performance (Post Model Selection) Document Unigram Sentence Ngram Figure 3: Both n-grams and a small prediction window lead to consistent improvements over the standard approach. center sentence in the highest confidence window highlighted (Order+help).",
                "In order to perform fair comparisons between conditions, the overall number of tokens in each message set should be approximately equal; that is, the cognitive reading load should be approximately the same before the classifiers reordering.",
                "Additionally, users typically show practice effects by improving at the overall task and thus performing better at later message sets.",
                "This is typically handled by varying the ordering of the sets across users so that the means are comparable.",
                "While omitting further detail, we note the sets were balanced for the total number of tokens and a latin square design was used to balance practice effects.",
                "Figure 4 shows that at intervals of 5, 10, and 15 minutes, users consistently found significantly more action-items when assisted by the classifier, but were most critically aided in the first five minutes.",
                "Although, the classifier consistently aids the users, we did not gain an additional end-user impact by highlighting.",
                "As mentioned above, this might be a result of the large room for improvement that still exists for sentence detection, but anecdotal evidence suggests this might also be a result of how the information is presented to the user rather than the accuracy of sentence detection.",
                "For example, highlighting the wrong sentence near an actual action-item hurts the users trust, but if a vague indicator (e.g., an arrow) points to the approximate area the user is not aware of the near-miss.",
                "Since the user studies used a three sentence window, we believe this played a role as well as sentence detection accuracy. 4.6 Discussion In contrast to problems where n-grams have yielded little difference, we believe their power here stems from the fact that many of the meaningful n-grams for action-items consist of common words, e.g., let me know.",
                "Therefore, the document-level unigram approach cannot gain much leverage, even when modeling their joint probability correctly, since these words will often co-occur in the document but not necessarily in a phrase.",
                "Additionally, action-item detection is distinct from many <br>text classification</br> tasks in that a single sentence can change the class label of the document.",
                "As a result, good classifiers cannot rely on aggregating evidence from a large number of weak indicators across the entire document.",
                "Even though we discarded the header information, examining the top-ranked features at the document-level reveals that many of the features are names or parts of e-mail addresses that occurred in the body and are highly associated with e-mails that tend to contain many or no action-items.",
                "A few examples are terms such as org, bob, and gov.",
                "We note that these features will be sensitive to the particular distribution (senders/receivers) and thus the document-level approach may produce classifiers that transfer less readily to alternate contexts and users at different institutions.",
                "This points out that part of the problem of going beyond bag-of-words may be the methodology, and investigating such properties as learning curves and how well a model transfers may highlight differences in models which appear to have similar performance when tested on the distributions they were trained on.",
                "We are currently investigating whether the sentence-level classifiers do perform better over different test corpora without retraining. 5.",
                "FUTURE WORK While applying text classifiers at the document-level is fairly well-understood, there exists the potential for significantly increasing the performance of the sentence-level classifiers.",
                "Such methods include alternate ways of combining the predictions over each sentence, weightings other than tfidf, which may not be appropriate since sentences are small, better sentence segmentation, and other types of phrasal analysis.",
                "Additionally, named entity tagging, time expressions, etc., seem likely candidates for features that can further improve this task.",
                "We are currently pursuing some of these avenues to see what additional gains these offer.",
                "Finally, it would be interesting to investigate the best methods for combining the document-level and sentence-level classifiers.",
                "Since the simple bag-of-words representation at the document-level leads to a learned model that behaves somewhat like a context-specific prior dependent on the sender/receiver and general topic, a first choice would be to treat it as such when combining probability estimates with the sentence-level classifier.",
                "Such a model might serve as a general example for other problems where bag-of-words can establish a baseline model but richer approaches are needed to achieve performance beyond that baseline. 6.",
                "SUMMARY AND CONCLUSIONS The effectiveness of sentence-level detection argues that labeling at the sentence-level provides significant value.",
                "Further experiments are needed to see how this interacts with the amount of training data available.",
                "Sentence detection that is then agglomerated to document-level detection works surprisingly better given low recall than would be expected with sentence-level items.",
                "This, in turn, indicates that improved sentence segmentation methods could yield further improvements in classification.",
                "In this work, we examined how action-items can be effectively detected in e-mails.",
                "Our empirical analysis has demonstrated that n-grams are of key importance to making the most of documentlevel judgments.",
                "When finer-grained judgments are available, then a standard bag-of-words approach using a small (sentence) window size and automatic segmentation techniques can produce results almost as good as the n-gram based approaches.",
                "Acknowledgments This material is based upon work supported by the Defense Advanced Research Projects Agency (DARPA) under Contract No.",
                "NBCHD030010.",
                "Any opinions, findings and conclusions or recommendations expressed in this material are those of the author(s) and do not necessarily reflect the views of the Defense Advanced Research Projects Agency (DARPA), or the Department of InteriorNational Business Center (DOI-NBC).",
                "We would like to extend our sincerest thanks to Jill Lehman whose efforts in data collection were essential in constructing the corpus, and both Jill and Aaron Steinfeld for their direction of the HCI experiments.",
                "We would also like to thank Django Wexler for constructing and supporting the corpus labeling tools and Curtis Huttenhowers support of the text preprocessing package.",
                "Finally, we gratefully acknowledge Scott Fahlman for his encouragement and useful discussions on this topic. 7.",
                "REFERENCES [1] J. Allan, J. Carbonell, G. Doddington, J. Yamron, and Y. Yang.",
                "Topic detection and tracking pilot study: Final report.",
                "In Proceedings of the DARPA Broadcast News Transcription and Understanding Workshop, Washington, D.C., 1998. [2] C. Apte, F. Damerau, and S. M. Weiss.",
                "Automated learning of decision rules for text categorization.",
                "ACM Transactions on Information Systems, 12(3):233-251, July 1994. [3] J. Carletta.",
                "Assessing agreement on classification tasks: The kappa statistic.",
                "Computational Linguistics, 22(2):249-254, 1996. [4] J. Carroll.",
                "High precision extraction of grammatical relations.",
                "In Proceedings of the 19th International Conference on Computational Linguistics (COLING), pages 134-140, 2002. [5] W. W. Cohen, V. R. Carvalho, and T. M. Mitchell.",
                "Learning to classify email into speech acts.",
                "In EMNLP-2004 (Conference on Empirical Methods in Natural Language Processing), pages 309-316, 2004. [6] S. Corston-Oliver, E. Ringger, M. Gamon, and R. Campbell.",
                "Task-focused summarization of email.",
                "In Text Summarization Branches Out: Proceedings of the ACL-04 Workshop, pages 43-50, 2004. [7] A. Culotta, R. Bekkerman, and A. McCallum.",
                "Extracting social networks and contact information from email and the web.",
                "In CEAS-2004 (Conference on Email and Anti-Spam), Mountain View, CA, July 2004. [8] L. Devroye, L. Gy¨orfi, and G. Lugosi.",
                "A Probabilistic Theory of Pattern Recognition.",
                "Springer-Verlag, New York, NY, 1996. [9] S. T. Dumais, J. Platt, D. Heckerman, and M. Sahami.",
                "Inductive learning algorithms and representations for text categorization.",
                "In CIKM 98, Proceedings of the 7th ACM Conference on Information and Knowledge Management, pages 148-155, 1998. [10] Y. Freund and R. Schapire.",
                "Large margin classification using the perceptron algorithm.",
                "Machine Learning, 37(3):277-296, 1999. [11] T. Joachims.",
                "Making large-scale svm learning practical.",
                "In B. Sch¨olkopf, C. J. Burges, and A. J. Smola, editors, Advances in Kernel Methods - Support Vector Learning, pages 41-56.",
                "MIT Press, 1999. [12] L. S. Larkey.",
                "A patent search and classification system.",
                "In Proceedings of the Fourth ACM Conference on Digital Libraries, pages 179 - 187, 1999. [13] D. D. Lewis.",
                "An evaluation of phrasal and clustered representations on a text categorization task.",
                "In SIGIR 92, Proceedings of the 15th Annual International ACM Conference on Research and Development in Information Retrieval, pages 37-50, 1992. [14] Y. Liu, J. Carbonell, and R. Jin.",
                "A pairwise ensemble approach for accurate genre classification.",
                "In Proceedings of the European Conference on Machine Learning (ECML), 2003. [15] Y. Liu, R. Yan, R. Jin, and J. Carbonell.",
                "A comparison study of kernels for multi-label <br>text classification</br> using category association.",
                "In The Twenty-first International Conference on Machine Learning (ICML), 2004. [16] A. McCallum and K. Nigam.",
                "A comparison of event models for naive bayes <br>text classification</br>.",
                "In Working Notes of AAAI 98 (The 15th National Conference on Artificial Intelligence), Workshop on Learning for Text Categorization, pages 41-48, 1998.",
                "TR WS-98-05. [17] F. Sebastiani.",
                "Machine learning in automated text categorization.",
                "ACM Computing Surveys, 34(1):1-47, March 2002. [18] C. J. van Rijsbergen.",
                "Information Retrieval.",
                "Butterworths, London, 1979. [19] Y. Yang.",
                "An evaluation of statistical approaches to text categorization.",
                "Information Retrieval, 1(1/2):67-88, 1999. [20] Y. Yang, J. Carbonell, R. Brown, T. Pierce, B. T. Archibald, and X. Liu.",
                "Learning approaches to topic detection and tracking.",
                "IEEE EXPERT, Special Issue on Applications of Intelligent Information Retrieval, 1999. [21] Y. Yang and X. Liu.",
                "A re-examination of text categorization methods.",
                "In SIGIR 99, Proceedings of the 22nd Annual International ACM Conference on Research and Development in Information Retrieval, pages 42-49, 1999. [22] Y. Yang, J. Zhang, J. Carbonell, and C. Jin.",
                "Topic-conditioned novelty detection.",
                "In Proceedings of the ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, July 2002."
            ],
            "original_annotated_samples": [],
            "translated_annotated_samples": [
                "A diferencia de la \"clasificación de texto\" estándar, impulsada por el tema, la detección de mensajes de acción requiere inferir la intención de los remitentes, y como tal responde menos bien a la clasificación pura de las palabras.",
                "La detección de ítems de acción difiere de la \"clasificación de texto\" estándar de dos maneras importantes.",
                "Primero revisamos el trabajo relacionado para problemas similares de \"clasificación de texto\", como la clasificación de prioridad de correo electrónico y la identificación de la Ley del habla.",
                "A partir de ahí, pasamos a una discusión sobre la representación de características y las técnicas de selección apropiadas para este problema y cómo los enfoques estándar de \"clasificación de texto\" pueden adaptarse para pasar sin problemas del problema de detección de nivel de oración al problema de clasificación de nivel de documento.",
                "Trabajo relacionado Varios otros investigadores han considerado tareas muy similares de \"clasificación de texto\".",
                "Además, consideramos múltiples enfoques estándar de \"clasificación de texto\" y analizamos las diferencias cuantitativas y cualitativas que surgen de tomar un nivel de documento frente a un enfoque a nivel de oración para la clasificación.",
                "Como se esperaría para el correo electrónico, es una distribución de cola larga con aproximadamente la mitad de los mensajes que tienen más de 60 tokens en el cuerpo (este párrafo tiene 65 tokens).4.2 Clasificadores para este experimento, hemos seleccionado una variedad de algoritmos estándar de \"clasificación de texto\".",
                "Al probar a fondo las opciones de clasificadores alternativos, demostramos que las mejoras de representación sobre la bolsa de palabras no se deben al uso de la información en la bolsa de palabras mal.4.2.1 KNN Empleamos una variante estándar del algoritmo vecino K-Nearest utilizado en \"Clasificación de texto\", KNN con umbral de puntaje de corte S [19].",
                "Además, la detección de Action-Item es distinta de muchas tareas de \"clasificación de texto\" en que una sola oración puede cambiar la etiqueta de clase del documento.",
                "Un estudio de comparación de núcleos para la \"clasificación de texto\" de múltiples etiquetas utilizando la asociación de categorías.",
                "Una comparación de los modelos de eventos para la \"clasificación de texto\" de Bayes ingenuos."
            ],
            "translated_text": "",
            "candidates": [
                "clasificación de texto",
                "clasificación de texto",
                "clasificación de texto",
                "clasificación de texto",
                "clasificación de texto",
                "clasificación de texto",
                "clasificación de texto",
                "clasificación de texto",
                "clasificación de texto",
                "clasificación de texto",
                "clasificación de texto",
                "clasificación de texto",
                "clasificación de texto",
                "clasificación de texto",
                "clasificación de texto",
                "Clasificación de texto",
                "clasificación de texto",
                "clasificación de texto",
                "clasificación de texto",
                "clasificación de texto",
                "clasificación de texto",
                "clasificación de texto"
            ],
            "error": []
        },
        "speech act": {
            "translated_key": "acto de habla",
            "is_in_text": true,
            "original_annotated_sentences": [
                "Feature Representation for Effective Action-Item Detection Paul N. Bennett Computer Science Department Carnegie Mellon University Pittsburgh, PA 15213 pbennett+@cs.cmu.edu Jaime Carbonell Language Technologies Institute.",
                "Carnegie Mellon University Pittsburgh, PA 15213 jgc+@cs.cmu.edu ABSTRACT E-mail users face an ever-growing challenge in managing their inboxes due to the growing centrality of email in the workplace for task assignment, action requests, and other roles beyond information dissemination.",
                "Whereas Information Retrieval and Machine Learning techniques are gaining initial acceptance in spam filtering and automated folder assignment, this paper reports on a new task: automated action-item detection, in order to flag emails that require responses, and to highlight the specific passage(s) indicating the request(s) for action.",
                "Unlike standard topic-driven text classification, action-item detection requires inferring the senders intent, and as such responds less well to pure bag-of-words classification.",
                "However, using enriched feature sets, such as n-grams (up to n=4) with chi-squared feature selection, and contextual cues for action-item location improve performance by up to 10% over unigrams, using in both cases state of the art classifiers such as SVMs with automated model selection via embedded cross-validation.",
                "Categories and Subject Descriptors H.3.3 [Information Storage and Retrieval]: Information Search and Retrieval; I.2.6 [Artificial Intelligence]: Learning; I.5.4 [Pattern Recognition]: Applications General Terms Experimentation 1.",
                "INTRODUCTION E-mail users are facing an increasingly difficult task of managing their inboxes in the face of mounting challenges that result from rising e-mail usage.",
                "This includes prioritizing e-mails over a range of sources from business partners to family members, filtering and reducing junk e-mail, and quickly managing requests that demand From: Henry Hutchins <hhutchins@innovative.company.com> To: Sara Smith; Joe Johnson; William Woolings Subject: meeting with prospective customers Sent: Fri 12/10/2005 8:08 AM Hi All, Id like to remind all of you that the group from GRTY will be visiting us next Friday at 4:30 p.m.",
                "The current schedule looks like this: + 9:30 a.m.",
                "Informal Breakfast and Discussion in Cafeteria + 10:30 a.m. Company Overview + 11:00 a.m.",
                "Individual Meetings (Continue Over Lunch) + 2:00 p.m. Tour of Facilities + 3:00 p.m.",
                "Sales Pitch In order to have this go off smoothly, I would like to practice the presentation well in advance.",
                "As a result, I will need each of your parts by Wednesday.",
                "Keep up the good work! -Henry Figure 1: An E-mail with emphasized Action-Item, an explicit request that requires the recipients attention or action. the receivers attention or action.",
                "Automated action-item detection targets the third of these problems by attempting to detect which e-mails require an action or response with information, and within those e-mails, attempting to highlight the sentence (or other passage length) that directly indicates the action request.",
                "Such a detection system can be used as one part of an e-mail agent which would assist a user in processing important e-mails quicker than would have been possible without the agent.",
                "We view action-item detection as one necessary component of a successful e-mail agent which would perform spam detection, action-item detection, topic classification and priority ranking, among other functions.",
                "The utility of such a detector can manifest as a method of prioritizing e-mails according to task-oriented criteria other than the standard ones of topic and sender or as a means of ensuring that the email user hasnt dropped the proverbial ball by forgetting to address an action request.",
                "Action-item detection differs from standard text classification in two important ways.",
                "First, the user is interested both in detecting whether an email contains action items and in locating exactly where these action item requests are contained within the email body.",
                "In contrast, standard text categorization merely assigns a topic label to each text, whether that label corresponds to an e-mail folder or a controlled indexing vocabulary [12, 15, 22].",
                "Second, action-item detection attempts to recover the email senders intent - whether she means to elicit response or action on the part of the receiver; note that for this task, classifiers using only unigrams as features do not perform optimally, as evidenced in our results below.",
                "Instead we find that we need more information-laden features such as higher-order n-grams.",
                "Text categorization by topic, on the other hand, works very well using just individual words as features [2, 9, 13, 17].",
                "In fact, genre-classification, which one would think may require more than a bag-of-words approach, also works quite well using just unigram features [14].",
                "Topic detection and tracking (TDT), also works well with unigram feature sets [1, 20].",
                "We believe that action-item detection is one of the first clear instances of an IR-related task where we must move beyond bag-of-words to achieve high performance, albeit not too far, as bag-of-n-grams seem to suffice.",
                "We first review related work for similar text classification problems such as e-mail priority ranking and <br>speech act</br> identification.",
                "Then we more formally define the action-item detection problem, discuss the aspects that distinguish it from more common problems like topic classification, and highlight the challenges in constructing systems that can perform well at the sentence and document level.",
                "From there, we move to a discussion of feature representation and selection techniques appropriate for this problem and how standard text classification approaches can be adapted to smoothly move from the sentence-level detection problem to the documentlevel classification problem.",
                "We then conduct an empirical analysis that helps us determine the effectiveness of our feature extraction procedures as well as establish baselines for a number of classification algorithms on this task.",
                "Finally, we summarize this papers contributions and consider interesting directions for future work. 2.",
                "RELATED WORK Several other researchers have considered very similar text classification tasks.",
                "Cohen et al. [5] describe an ontology of speech acts, such as Propose a Meeting, and attempt to predict when an e-mail contains one of these speech acts.",
                "We consider action-items to be an important specific type of <br>speech act</br> that falls within their more general classification.",
                "While they provide results for several classification methods, their methods only make use of human judgments at the document-level.",
                "In contrast, we consider whether accuracy can be increased by using finer-grained human judgments that mark the specific sentences and phrases of interest.",
                "Corston-Oliver et al. [6] consider detecting items in e-mail to Put on a To-Do List.",
                "This classification task is very similar to ours except they do not consider simple factual questions to belong to this category.",
                "We include questions, but note that not all questions are action-items - some are rhetorical or simply social convention, How are you?.",
                "From a learning perspective, while they make use of judgments at the sentence-level, they do not explicitly compare what if any benefits finer-grained judgments offer.",
                "Additionally, they do not study alternative choices or approaches to the classification task.",
                "Instead, they simply apply a standard SVM at the sentence-level and focus primarily on a linguistic analysis of how the sentence can be logically reformulated before adding it to the task list.",
                "In this study, we examine several alternative classification methods, compare document-level and sentence-level approaches and analyze the machine learning issues implicit in these problems.",
                "Interest in a variety of learning tasks related to e-mail has been rapidly growing in the recent literature.",
                "For example, in a forum dedicated to e-mail learning tasks, Culotta et al. [7] presented methods for learning social networks from e-mail.",
                "In this work, we do not focus on peer relationships; however, such methods could complement those here since peer relationships often influence word choice when requesting an action. 3.",
                "PROBLEM DEFINITION & APPROACH In contrast to previous work, we explicitly focus on the benefits that finer-grained, more costly, sentence-level human judgments offer over coarse-grained document-level judgments.",
                "Additionally, we consider multiple standard text classification approaches and analyze both the quantitative and qualitative differences that arise from taking a document-level vs. a sentence-level approach to classification.",
                "Finally, we focus on the representation necessary to achieve the most competitive performance. 3.1 Problem Definition In order to provide the most benefit to the user, a system would not only detect the document, but it would also indicate the specific sentences in the e-mail which contain the action-items.",
                "Therefore, there are three basic problems: 1.",
                "Document detection: Classify a document as to whether or not it contains an action-item. 2.",
                "Document ranking: Rank the documents such that all documents containing action-items occur as high as possible in the ranking. 3.",
                "Sentence detection: Classify each sentence in a document as to whether or not it is an action-item.",
                "As in most Information Retrieval tasks, the weight the evaluation metric should give to precision and recall depends on the nature of the application.",
                "In situations where a user will eventually read all received messages, ranking (e.g., via precision at recall of 1) may be most important since this will help encourage shorter delays in communications between users.",
                "In contrast, high-precision detection at low recall will be of increasing importance when the user is under severe time-pressure and therefore will likely not read all mail.",
                "This can be the case for crisis managers during disaster management.",
                "Finally, sentence detection plays a role in both timepressure situations and simply to alleviate the users required time to gist the message. 3.2 Approach As mentioned above, the labeled data can come in one of two forms: a document-labeling provides a yes/no label for each document as to whether it contains an action-item; a phrase-labeling provides only a yes label for the specific items of interest.",
                "We term the human judgments a phrase-labeling since the users view of the action-item may not correspond with actual sentence boundaries or predicted sentence boundaries.",
                "Obviously, it is straightforward to generate a document-labeling consistent with a phrase-labeling by labeling a document yes if and only if it contains at least one phrase labeled yes.",
                "To train classifiers for this task, we can take several viewpoints related to both the basic problems we have enumerated and the form of the labeled data.",
                "The document-level view treats each e-mail as a learning instance with an associated class-label.",
                "Then, the document can be converted to a feature-value vector and learning progresses as usual.",
                "Applying a document-level classifier to document detection and ranking is straightforward.",
                "In order to apply it to sentence detection, one must make additional steps.",
                "For example, if the classifier predicts a document contains an action-item, then areas of the document that contain a high-concentration of words which the model weights heavily in favor of action-items can be indicated.",
                "The obvious benefit of the document-level approach is that training set collection costs are lower since the user only has to specify whether or not an e-mail contains an action-item and not the specific sentences.",
                "In the sentence-level view, each e-mail is automatically segmented into sentences, and each sentence is treated as a learning instance with an associated class-label.",
                "Since the phrase-labeling provided by the user may not coincide with the automatic segmentation, we must determine what label to assign a partially overlapping sentence when converting it to a learning instance.",
                "Once trained, applying the resulting classifiers to sentence detection is now straightforward, but in order to apply the classifiers to document detection and document ranking, the individual predictions over each sentence must be aggregated in order to make a document-level prediction.",
                "This approach has the potential to benefit from morespecific labels that enable the learner to focus attention on the key sentences instead of having to learn based on data that the majority of the words in the e-mail provide no or little information about class membership. 3.2.1 Features Consider some of the phrases that might constitute part of an action item: would like to know, let me know, as soon as possible, have you.",
                "Each of these phrases consists of common words that occur in many e-mails.",
                "However, when they occur in the same sentence, they are far more indicative of an action-item.",
                "Additionally, order can be important: consider have you versus you have.",
                "Because of this, we posit that n-grams play a larger role in this problem than is typical of problems like topic classification.",
                "Therefore, we consider all n-grams up to size 4.",
                "When using n-grams, if we find an n-gram of size 4 in a segment of text, we can represent the text as just one occurrence of the ngram or as one occurrence of the n-gram and an occurrence of each smaller n-gram contained by it.",
                "We choose the second of these alternatives since this will allow the algorithm itself to smoothly back-off in terms of recall.",
                "Methods such as na¨ıve Bayes may be hurt by such a representation because of double-counting.",
                "Since sentence-ending punctuation can provide information, we retain the terminating punctuation token when it is identifiable.",
                "Additionally, we add a beginning-of-sentence and end-of-sentence token in order to capture patterns that are often indicators at the beginning or end of a sentence.",
                "Assuming proper punctuation, these extra tokens are unnecessary, but often e-mail lacks proper punctuation.",
                "In addition, for the sentence-level classifiers that use ngrams, we additionally code for each sentence a binary encoding of the position of the sentence relative to the document.",
                "This encoding has eight associated features that represent which octile (the first eighth, second eighth, etc.) contains the sentence. 3.2.2 Implementation Details In order to compare the document-level to the sentence-level approach, we compare predictions at the document-level.",
                "We do not address how to use a document-level classifier to make predictions at the sentence-level.",
                "In order to automatically segment the text of the e-mail, we use the RASP statistical parser [4].",
                "Since the automatically segmented sentences may not correspond directly with the phrase-level boundaries, we treat any sentence that contains at least 30% of a marked action-item segment as an action-item.",
                "When evaluating sentencedetection for the sentence-level system, we use these class labels as ground truth.",
                "Since we are not evaluating multiple segmentation approaches, this does not bias any of the methods.",
                "If multiple segmentation systems were under evaluation, one would need to use a metric that matched predicted positive sentences to phrases labeled positive.",
                "The metric would need to punish overly long true predictions as well as too short predictions.",
                "Our criteria for converting to labeled instances implicitly includes both criteria.",
                "Since the segmentation is fixed, an overly long prediction would be predicting yes for many no instances since presumably the extra length corresponds to additional segmented sentences all of which do not contain 30% of action-item.",
                "Likewise, a too short prediction must correspond to a small sentence included in the action-item but not constituting all of the action-item.",
                "Therefore, in order to consider the prediction to be too short, there will be an additional preceding/following sentence that is an action-item where we incorrectly predicted no.",
                "Once a sentence-level classifier has made a prediction for each sentence, we must combine these predictions to make both a document-level prediction and a document-level score.",
                "We use the simple policy of predicting positive when any of the sentences is predicted positive.",
                "In order to produce a document score for ranking, the confidence that the document contains an action-item is: ψ(d) = 1 n(d) s∈d|π(s)=1 ψ(s) if for any s ∈ d, π(s) = 1 1 n(d) maxs∈d ψ(s) o.w. where s is a sentence in document d, π is the classifiers 1/0 prediction, ψ is the score the classifier assigns as its confidence that π(s) = 1, and n(d) is the greater of 1 and the number of (unigram) tokens in the document.",
                "In other words, when any sentence is predicted positive, the document score is the length normalized sum of the sentence scores above threshold.",
                "When no sentence is predicted positive, the document score is the maximum sentence score normalized by length.",
                "As in other text problems, we are more likely to emit false positives for documents with more words or sentences.",
                "Thus we include a length normalization factor. 4.",
                "EXPERIMENTAL ANALYSIS 4.1 The Data Our corpus consists of e-mails obtained from volunteers at an educational institution and cover subjects such as: organizing a research workshop, arranging for job-candidate interviews, publishing proceedings, and talk announcements.",
                "The messages were anonymized by replacing the names of each individual and institution with a pseudonym.1 After attempting to identify and eliminate duplicate e-mails, the corpus contains 744 e-mail messages.",
                "After identity anonymization, the corpora has three basic versions.",
                "Quoted material refers to the text of a previous e-mail that an author often leaves in an e-mail message when responding to the e-mail.",
                "Quoted material can act as noise when learning since it may include action-items from previous messages that are no longer relevant.",
                "To isolate the effects of quoted material, we have three versions of the corpora.",
                "The raw form contains the basic messages.",
                "The auto-stripped version contains the messages after quoted material has been automatically removed.",
                "The hand-stripped version contains the messages after quoted material has been removed by a human.",
                "Additionally, the hand-stripped version has had any xml content and e-mail signatures removed - leaving only the essential content of the message.",
                "The studies reported here are performed with the hand-stripped version.",
                "This allows us to balance the cognitive load in terms of number of tokens that must be read in the user-studies we report - including quoted material would complicate the user studies since some users might skip the material while others read it.",
                "Additionally, ensuring all quoted material is removed 1 We have an even more highly anonymized version of the corpus that can be made available for some outside experimentation.",
                "Please contact the authors for more information on obtaining this data. prevents tainting the cross-validation since otherwise a test item could occur as quoted material in a training document. 4.1.1 Data Labeling Two human annotators labeled each message as to whether or not it contained an action-item.",
                "In addition, they identified each segment of the e-mail which contained an action-item.",
                "A segment is a contiguous section of text selected by the human annotators and may span several sentences or a complete phrase contained in a sentence.",
                "They were instructed that an action item is an explicit request for information that requires the recipients attention or a required action and told to highlight the phrases or sentences that make up the request.",
                "Annotator 1 No Yes Annotator 2 No 391 26 Yes 29 298 Table 1: Agreement of Human Annotators at Document Level Annotator One labeled 324 messages as containing action items.",
                "Annotator Two labeled 327 messages as containing action items.",
                "The agreement of the human annotators is shown in Tables 1 and 2.",
                "The annotators are said to agree at the document-level when both marked the same document as containing no action-items or both marked at least one action-item regardless of whether the text segments were the same.",
                "At the document-level, the annotators agreed 93% of the time.",
                "The kappa statistic [3, 5] is often used to evaluate inter-annotator agreement: κ = A − R 1 − R A is the empirical estimate of the probability of agreement.",
                "R is the empirical estimate of the probability of random agreement given the empirical class priors.",
                "A value close to −1 implies the annotators agree far less often than would be expected randomly, while a value close to 1 means they agree more often than randomly expected.",
                "At the document-level, the kappa statistic for inter-annotator agreement is 0.85.",
                "This value is both strong enough to expect the problem to be learnable and is comparable with results for similar tasks [5, 6].",
                "In order to determine the sentence-level agreement, we use each judgment to create a sentence-corpus with labels as described in Section 3.2.2, then consider the agreement over these sentences.",
                "This allows us to compare agreement over no judgments.",
                "We perform this comparison over the hand-stripped corpus since that eliminates spurious no judgments that would come from including quoted material, etc.",
                "Both annotators were free to label the subject as an action-item, but since neither did, we omit the subject line of the message as well.",
                "This only reduces the number of no agreements.",
                "This leaves 6301 automatically segmented sentences.",
                "At the sentence-level, the annotators agreed 98% of the time, and the kappa statistic for inter-annotator agreement is 0.82.",
                "In order to produce one single set of judgments, the human annotators went through each annotation where there was disagreement and came to a consensus opinion.",
                "The annotators did not collect statistics during this process but anecdotally reported that the majority of disagreements were either cases of clear annotator oversight or different interpretations of conditional statements.",
                "For example, If you would like to keep your job, come to tomorrows meeting implies a required action where If you would like to join Annotator 1 No Yes Annotator 2 No 5810 65 Yes 74 352 Table 2: Agreement of Human Annotators at Sentence Level the football betting pool, come to tomorrows meeting does not.",
                "The first would be an action-item in most contexts while the second would not.",
                "Of course, many conditional statements are not so clearly interpretable.",
                "After reconciling the judgments there are 416 e-mails with no action-items and 328 e-mails containing actionitems.",
                "Of the 328 e-mails containing action-items, 259 messages have one action-item segment; 55 messages have two action-item segments; 11 messages have three action-item segments.",
                "Two messages have four action-item segments, and one message has six action-item segments.",
                "Computing the sentence-level agreement using the reconciled gold standard judgments with each of the annotators individual judgments gives a kappa of 0.89 for Annotator One and a kappa of 0.92 for Annotator Two.",
                "In terms of message characteristics, there were on average 132 content tokens in the body after stripping.",
                "For action-item messages, there were 115.",
                "However, by examining Figure 2 we see the length distributions are nearly identical.",
                "As would be expected for e-mail, it is a long-tailed distribution with about half the messages having more than 60 tokens in the body (this paragraph has 65 tokens). 4.2 Classifiers For this experiment, we have selected a variety of standard text classification algorithms.",
                "In selecting algorithms, we have chosen algorithms that are not only known to work well but which differ along such lines as discriminative vs. generative and lazy vs. eager.",
                "We have done this in order to provide both a competitive and thorough sampling of learning methods for the task at hand.",
                "This is important since it is easy to improve a strawman classifier by introducing a new representation.",
                "By thoroughly sampling alternative classifier choices we demonstrate that representation improvements over bag-of-words are not due to using the information in the bag-of-words poorly. 4.2.1 kNN We employ a standard variant of the k-nearest neighbor algorithm used in text classification, kNN with s-cut score thresholding [19].",
                "We use a tfidf-weighting of the terms with a distanceweighted vote of the neighbors to compute the score before thresholding it.",
                "In order to choose the value of s for thresholding, we perform leave-one-out cross-validation over the training set.",
                "The value of k is set to be 2( log2 N + 1) where N is the number of training points.",
                "This rule for choosing k is theoretically motivated by results which show such a rule converges to the optimal classifier as the number of training points increases [8].",
                "In practice, we have also found it to be a computational convenience that frequently leads to comparable results with numerically optimizing k via a cross-validation procedure. 4.2.2 Na¨ıve Bayes We use a standard multinomial na¨ıve Bayes classifier [16].",
                "In using this classifier, we smoothed word and class probabilities using a Bayesian estimate (with the word prior) and a Laplace m-estimate, respectively. 0 20 40 60 80 100 120 140 160 0 200 400 600 800 1000 1200 1400 NumberofMessages Number of Tokens All Messages Action-Item Messages 0 0.02 0.04 0.06 0.08 0.1 0.12 0.14 0.16 0.18 0.2 0 200 400 600 800 1000 1200 1400 PercentageofMessages Number of Tokens All Messages Action-Item Messages Figure 2: The Histogram (left) and Distribution (right) of Message Length.",
                "A bin size of 20 words was used.",
                "Only tokens in the body after hand-stripping were counted.",
                "After stripping, the majority of words left are usually actual message content.",
                "Classifiers Document Unigram Document Ngram Sentence Unigram Sentence Ngram F1 kNN 0.6670 ± 0.0288 0.7108 ± 0.0699 0.7615 ± 0.0504 0.7790 ± 0.0460 na¨ıve Bayes 0.6572 ± 0.0749 0.6484 ± 0.0513 0.7715 ± 0.0597 0.7777 ± 0.0426 SVM 0.6904 ± 0.0347 0.7428 ± 0.0422 0.7282 ± 0.0698 0.7682 ± 0.0451 Voted Perceptron 0.6288 ± 0.0395 0.6774 ± 0.0422 0.6511 ± 0.0506 0.6798 ± 0.0913 Accuracy kNN 0.7029 ± 0.0659 0.7486 ± 0.0505 0.7972 ± 0.0435 0.8092 ± 0.0352 na¨ıve Bayes 0.6074 ± 0.0651 0.5816 ± 0.1075 0.7863 ± 0.0553 0.8145 ± 0.0268 SVM 0.7595 ± 0.0309 0.7904 ± 0.0349 0.7958 ± 0.0551 0.8173 ± 0.0258 Voted Perceptron 0.6531 ± 0.0390 0.7164 ± 0.0376 0.6413 ± 0.0833 0.7082 ± 0.1032 Table 3: Average Document-Detection Performance during Cross-Validation for Each Method and the Sample Standard Deviation (Sn−1) in italics.",
                "The best performance for each classifier is shown in bold. 4.2.3 SVM We have used a linear SVM with a tfidf feature representation and L2-norm as implemented in the SVMlight package v6.01 [11].",
                "All default settings were used. 4.2.4 Voted Perceptron Like the SVM, the Voted Perceptron is a kernel-based learning method.",
                "We use the same feature representation and kernel as we have for the SVM, a linear kernel with tfidf-weighting and an L2-norm.",
                "The voted perceptron is an online-learning method that keeps a history of past perceptrons used, as well as a weight signifying how often that perceptron was correct.",
                "With each new training example, a correct classification increases the weight on the current perceptron and an incorrect classification updates the perceptron.",
                "The output of the classifier uses the weights on the perceptra to make a final voted classification.",
                "When used in an offline-manner, multiple passes can be made through the training data.",
                "Both the voted perceptron and the SVM give a solution from the same hypothesis space - in this case, a linear classifier.",
                "Furthermore, it is well-known that the Voted Perceptron increases the margin of the solution after each pass through the training data [10].",
                "Since Cohen et al. [5] obtain worse results using an SVM than a Voted Perceptron with one training iteration, they conclude that the best solution for detecting speech acts may not lie in an area with a large margin.",
                "Because their tasks are highly similar to ours, we employ both classifiers to ensure we are not overlooking a competitive alternative classifier to the SVM for the basic bag-of-words representation. 4.3 Performance Measures To compare the performance of the classification methods, we look at two standard performance measures, F1 and accuracy.",
                "The F1 measure [18, 21] is the harmonic mean of precision and recall where Precision = Correct Positives Predicted Positives and Recall = Correct Positives Actual Positives . 4.4 Experimental Methodology We perform standard 10-fold cross-validation on the set of documents.",
                "For the sentence-level approach, all sentences in a document are either entirely in the training set or entirely in the test set for each fold.",
                "For significance tests, we use a two-tailed t-test [21] to compare the values obtained during each cross-validation fold with a p-value of 0.05.",
                "Feature selection was performed using the chi-squared statistic.",
                "Different levels of feature selection were considered for each classifier.",
                "Each of the following number of features was tried: 10, 25, 50, 100, 250, 750, 1000, 2000, 4000.",
                "There are approximately 4700 unigram tokens without feature selection.",
                "In order to choose the number of features to use for each classifier, we perform nested cross-validation and choose the settings that yield the optimal document-level F1 for that classifier.",
                "For this study, only the body of each e-mail message was used.",
                "Feature selection is always applied to all candidate features.",
                "That is, for the n-gram representation, the n-grams and position features are also subject to removal by the feature selection method. 4.5 Results The results for document-level classification are given in Table 3.",
                "The primary hypothesis we are concerned with is that n-grams are critical for this task; if this is true, we expect to see a significant gap in performance between the document-level classifiers that use n-grams (denoted Document Ngram) and those using only unigram features (denoted Document Unigram).",
                "Examining Table 3, we observe that this is indeed the case for every classifier except na¨ıve Bayes.",
                "This difference in performance produced by the n-gram representation is statistically significant for each classifier except for na¨ıve Bayes and the accuracy metric for kNN (see Table 4).",
                "Na¨ıve Bayes poor performance with the n-gram representation is not surprising since the bag-of-n-grams causes excessive doublecounting as mentioned in Section 3.2.1; however, na¨ıve Bayes is not hurt at the sentence-level because the sparse examples provide few chances for agglomerative effects of double counting.",
                "In either case, when a language-modeling approach is desired, modeling the n-grams directly would be preferable to na¨ıve Bayes.",
                "More importantly for the n-gram hypothesis, the n-grams lead to the best document-level classifier performance as well.",
                "As would be expected, the difference between the sentence-level n-gram representation and unigram representation is small.",
                "This is because the window of text is so small that the unigram representation, when done at the sentence-level, implicitly picks up on the power of the n-grams.",
                "Further improvement would signify that the order of the words matter even when only considering a small sentence-size window.",
                "Therefore, the finer-grained sentence-level judgments allows a unigram representation to succeed but only when performed in a small window - behaving as an n-gram representation for all practical purposes.",
                "Document Winner Sentence Winner kNN Ngram Ngram na¨ıve Bayes Unigram Ngram SVM Ngram† Ngram Voted Perceptron Ngram† Ngram Table 4: Significance results for n-grams versus unigrams for document detection using document-level and sentence-level classifiers.",
                "When the F1 result is statistically significant, it is shown in bold.",
                "When the accuracy result is significant, it is shown with a † .",
                "F1 Winner Accuracy Winner kNN Sentence Sentence na¨ıve Bayes Sentence Sentence SVM Sentence Sentence Voted Perceptron Sentence Document Table 5: Significance results for sentence-level classifiers vs. document-level classifiers for the document detection problem.",
                "When the result is statistically significant, it is shown in bold.",
                "Further highlighting the improvement from finer-grained judgments and n-grams, Figure 3 graphically depicts the edge the SVM sentence-level classifier has over the standard bag-of-words approach with a precision-recall curve.",
                "In the high precision area of the graph, the consistent edge of the sentence-level classifier is rather impressive - continuing at precision 1 out to 0.1 recall.",
                "This would mean that a tenth of the users action-items would be placed at the top of their action-item sorted inbox.",
                "Additionally, the large separation at the top right of the curves corresponds to the area where the optimal F1 occurs for each classifier, agreeing with the large improvement from 0.6904 to 0.7682 in F1 score.",
                "Considering the relative unexplored nature of classification at the sentence-level, this gives great hope for further increases in performance.",
                "Accuracy F1 Unigram Ngram Unigram Ngram kNN 0.9519 0.9536 0.6540 0.6686 na¨ıve Bayes 0.9419 0.9550 0.6176 0.6676 SVM 0.9559 0.9579 0.6271 0.6672 Voted Perceptron 0.8895 0.9247 0.3744 0.5164 Table 6: Performance of the Sentence-Level Classifiers at Sentence Detection Although Cohen et al. [5] observed that the Voted Perceptron with a single training iteration outperformed SVM in a set of similar tasks, we see no such behavior here.",
                "This further strengthens the evidence that an alternate classifier with the bag-of-words representation could not reach the same level of performance.",
                "The Voted Perceptron classifier does improve when the number of training iterations are increased, but it is still lower than the SVM classifier.",
                "Sentence detection results are presented in Table 6.",
                "With regard to the sentence detection problem, we note that the F1 measure gives a better feel for the remaining room for improvement in this difficult problem.",
                "That is, unlike document detection where actionitem documents are fairly common, action-item sentences are very rare.",
                "Thus, as in other text problems, the accuracy numbers are deceptively high sheerly because of the default accuracy attainable by always predicting no.",
                "Although, the results here are significantly above-random, it is unclear what level of performance is necessary for sentence detection to be useful in and of itself and not simply as a means to document ranking and classification.",
                "Figure 4: Users find action-items quicker when assisted by a classification system.",
                "Finally, when considering a new type of classification task, one of the most basic questions is whether an accurate classifier built for the task can have an impact on the end-user.",
                "In order to demonstrate the impact this task can have on e-mail users, we conducted a user study using an earlier less-accurate version of the sentence classifier - where instead of using just a single sentence, a threesentence windowed-approach was used.",
                "There were three distinct sets of e-mail in which users had to find action-items.",
                "These sets were either presented in a random order (Unordered), ordered by the classifier (Ordered), or ordered by the classifier and with the 0.4 0.5 0.6 0.7 0.8 0.9 1 0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1 Precision Recall Action-Item Detection SVM Performance (Post Model Selection) Document Unigram Sentence Ngram Figure 3: Both n-grams and a small prediction window lead to consistent improvements over the standard approach. center sentence in the highest confidence window highlighted (Order+help).",
                "In order to perform fair comparisons between conditions, the overall number of tokens in each message set should be approximately equal; that is, the cognitive reading load should be approximately the same before the classifiers reordering.",
                "Additionally, users typically show practice effects by improving at the overall task and thus performing better at later message sets.",
                "This is typically handled by varying the ordering of the sets across users so that the means are comparable.",
                "While omitting further detail, we note the sets were balanced for the total number of tokens and a latin square design was used to balance practice effects.",
                "Figure 4 shows that at intervals of 5, 10, and 15 minutes, users consistently found significantly more action-items when assisted by the classifier, but were most critically aided in the first five minutes.",
                "Although, the classifier consistently aids the users, we did not gain an additional end-user impact by highlighting.",
                "As mentioned above, this might be a result of the large room for improvement that still exists for sentence detection, but anecdotal evidence suggests this might also be a result of how the information is presented to the user rather than the accuracy of sentence detection.",
                "For example, highlighting the wrong sentence near an actual action-item hurts the users trust, but if a vague indicator (e.g., an arrow) points to the approximate area the user is not aware of the near-miss.",
                "Since the user studies used a three sentence window, we believe this played a role as well as sentence detection accuracy. 4.6 Discussion In contrast to problems where n-grams have yielded little difference, we believe their power here stems from the fact that many of the meaningful n-grams for action-items consist of common words, e.g., let me know.",
                "Therefore, the document-level unigram approach cannot gain much leverage, even when modeling their joint probability correctly, since these words will often co-occur in the document but not necessarily in a phrase.",
                "Additionally, action-item detection is distinct from many text classification tasks in that a single sentence can change the class label of the document.",
                "As a result, good classifiers cannot rely on aggregating evidence from a large number of weak indicators across the entire document.",
                "Even though we discarded the header information, examining the top-ranked features at the document-level reveals that many of the features are names or parts of e-mail addresses that occurred in the body and are highly associated with e-mails that tend to contain many or no action-items.",
                "A few examples are terms such as org, bob, and gov.",
                "We note that these features will be sensitive to the particular distribution (senders/receivers) and thus the document-level approach may produce classifiers that transfer less readily to alternate contexts and users at different institutions.",
                "This points out that part of the problem of going beyond bag-of-words may be the methodology, and investigating such properties as learning curves and how well a model transfers may highlight differences in models which appear to have similar performance when tested on the distributions they were trained on.",
                "We are currently investigating whether the sentence-level classifiers do perform better over different test corpora without retraining. 5.",
                "FUTURE WORK While applying text classifiers at the document-level is fairly well-understood, there exists the potential for significantly increasing the performance of the sentence-level classifiers.",
                "Such methods include alternate ways of combining the predictions over each sentence, weightings other than tfidf, which may not be appropriate since sentences are small, better sentence segmentation, and other types of phrasal analysis.",
                "Additionally, named entity tagging, time expressions, etc., seem likely candidates for features that can further improve this task.",
                "We are currently pursuing some of these avenues to see what additional gains these offer.",
                "Finally, it would be interesting to investigate the best methods for combining the document-level and sentence-level classifiers.",
                "Since the simple bag-of-words representation at the document-level leads to a learned model that behaves somewhat like a context-specific prior dependent on the sender/receiver and general topic, a first choice would be to treat it as such when combining probability estimates with the sentence-level classifier.",
                "Such a model might serve as a general example for other problems where bag-of-words can establish a baseline model but richer approaches are needed to achieve performance beyond that baseline. 6.",
                "SUMMARY AND CONCLUSIONS The effectiveness of sentence-level detection argues that labeling at the sentence-level provides significant value.",
                "Further experiments are needed to see how this interacts with the amount of training data available.",
                "Sentence detection that is then agglomerated to document-level detection works surprisingly better given low recall than would be expected with sentence-level items.",
                "This, in turn, indicates that improved sentence segmentation methods could yield further improvements in classification.",
                "In this work, we examined how action-items can be effectively detected in e-mails.",
                "Our empirical analysis has demonstrated that n-grams are of key importance to making the most of documentlevel judgments.",
                "When finer-grained judgments are available, then a standard bag-of-words approach using a small (sentence) window size and automatic segmentation techniques can produce results almost as good as the n-gram based approaches.",
                "Acknowledgments This material is based upon work supported by the Defense Advanced Research Projects Agency (DARPA) under Contract No.",
                "NBCHD030010.",
                "Any opinions, findings and conclusions or recommendations expressed in this material are those of the author(s) and do not necessarily reflect the views of the Defense Advanced Research Projects Agency (DARPA), or the Department of InteriorNational Business Center (DOI-NBC).",
                "We would like to extend our sincerest thanks to Jill Lehman whose efforts in data collection were essential in constructing the corpus, and both Jill and Aaron Steinfeld for their direction of the HCI experiments.",
                "We would also like to thank Django Wexler for constructing and supporting the corpus labeling tools and Curtis Huttenhowers support of the text preprocessing package.",
                "Finally, we gratefully acknowledge Scott Fahlman for his encouragement and useful discussions on this topic. 7.",
                "REFERENCES [1] J. Allan, J. Carbonell, G. Doddington, J. Yamron, and Y. Yang.",
                "Topic detection and tracking pilot study: Final report.",
                "In Proceedings of the DARPA Broadcast News Transcription and Understanding Workshop, Washington, D.C., 1998. [2] C. Apte, F. Damerau, and S. M. Weiss.",
                "Automated learning of decision rules for text categorization.",
                "ACM Transactions on Information Systems, 12(3):233-251, July 1994. [3] J. Carletta.",
                "Assessing agreement on classification tasks: The kappa statistic.",
                "Computational Linguistics, 22(2):249-254, 1996. [4] J. Carroll.",
                "High precision extraction of grammatical relations.",
                "In Proceedings of the 19th International Conference on Computational Linguistics (COLING), pages 134-140, 2002. [5] W. W. Cohen, V. R. Carvalho, and T. M. Mitchell.",
                "Learning to classify email into speech acts.",
                "In EMNLP-2004 (Conference on Empirical Methods in Natural Language Processing), pages 309-316, 2004. [6] S. Corston-Oliver, E. Ringger, M. Gamon, and R. Campbell.",
                "Task-focused summarization of email.",
                "In Text Summarization Branches Out: Proceedings of the ACL-04 Workshop, pages 43-50, 2004. [7] A. Culotta, R. Bekkerman, and A. McCallum.",
                "Extracting social networks and contact information from email and the web.",
                "In CEAS-2004 (Conference on Email and Anti-Spam), Mountain View, CA, July 2004. [8] L. Devroye, L. Gy¨orfi, and G. Lugosi.",
                "A Probabilistic Theory of Pattern Recognition.",
                "Springer-Verlag, New York, NY, 1996. [9] S. T. Dumais, J. Platt, D. Heckerman, and M. Sahami.",
                "Inductive learning algorithms and representations for text categorization.",
                "In CIKM 98, Proceedings of the 7th ACM Conference on Information and Knowledge Management, pages 148-155, 1998. [10] Y. Freund and R. Schapire.",
                "Large margin classification using the perceptron algorithm.",
                "Machine Learning, 37(3):277-296, 1999. [11] T. Joachims.",
                "Making large-scale svm learning practical.",
                "In B. Sch¨olkopf, C. J. Burges, and A. J. Smola, editors, Advances in Kernel Methods - Support Vector Learning, pages 41-56.",
                "MIT Press, 1999. [12] L. S. Larkey.",
                "A patent search and classification system.",
                "In Proceedings of the Fourth ACM Conference on Digital Libraries, pages 179 - 187, 1999. [13] D. D. Lewis.",
                "An evaluation of phrasal and clustered representations on a text categorization task.",
                "In SIGIR 92, Proceedings of the 15th Annual International ACM Conference on Research and Development in Information Retrieval, pages 37-50, 1992. [14] Y. Liu, J. Carbonell, and R. Jin.",
                "A pairwise ensemble approach for accurate genre classification.",
                "In Proceedings of the European Conference on Machine Learning (ECML), 2003. [15] Y. Liu, R. Yan, R. Jin, and J. Carbonell.",
                "A comparison study of kernels for multi-label text classification using category association.",
                "In The Twenty-first International Conference on Machine Learning (ICML), 2004. [16] A. McCallum and K. Nigam.",
                "A comparison of event models for naive bayes text classification.",
                "In Working Notes of AAAI 98 (The 15th National Conference on Artificial Intelligence), Workshop on Learning for Text Categorization, pages 41-48, 1998.",
                "TR WS-98-05. [17] F. Sebastiani.",
                "Machine learning in automated text categorization.",
                "ACM Computing Surveys, 34(1):1-47, March 2002. [18] C. J. van Rijsbergen.",
                "Information Retrieval.",
                "Butterworths, London, 1979. [19] Y. Yang.",
                "An evaluation of statistical approaches to text categorization.",
                "Information Retrieval, 1(1/2):67-88, 1999. [20] Y. Yang, J. Carbonell, R. Brown, T. Pierce, B. T. Archibald, and X. Liu.",
                "Learning approaches to topic detection and tracking.",
                "IEEE EXPERT, Special Issue on Applications of Intelligent Information Retrieval, 1999. [21] Y. Yang and X. Liu.",
                "A re-examination of text categorization methods.",
                "In SIGIR 99, Proceedings of the 22nd Annual International ACM Conference on Research and Development in Information Retrieval, pages 42-49, 1999. [22] Y. Yang, J. Zhang, J. Carbonell, and C. Jin.",
                "Topic-conditioned novelty detection.",
                "In Proceedings of the ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, July 2002."
            ],
            "original_annotated_samples": [],
            "translated_annotated_samples": [
                "Primero revisamos el trabajo relacionado para problemas de clasificación de texto similares, como la clasificación de prioridad de correo electrónico e identificación de \"Ley del Discurso\".",
                "Consideramos que los elementos de acción son un tipo específico importante de \"acto de habla\" que se encuentra dentro de su clasificación más general."
            ],
            "translated_text": "",
            "candidates": [
                "acto de habla",
                "Ley del Discurso",
                "acto de habla",
                "acto de habla"
            ],
            "error": []
        },
        "feature selection": {
            "translated_key": "selección de características",
            "is_in_text": true,
            "original_annotated_sentences": [
                "Feature Representation for Effective Action-Item Detection Paul N. Bennett Computer Science Department Carnegie Mellon University Pittsburgh, PA 15213 pbennett+@cs.cmu.edu Jaime Carbonell Language Technologies Institute.",
                "Carnegie Mellon University Pittsburgh, PA 15213 jgc+@cs.cmu.edu ABSTRACT E-mail users face an ever-growing challenge in managing their inboxes due to the growing centrality of email in the workplace for task assignment, action requests, and other roles beyond information dissemination.",
                "Whereas Information Retrieval and Machine Learning techniques are gaining initial acceptance in spam filtering and automated folder assignment, this paper reports on a new task: automated action-item detection, in order to flag emails that require responses, and to highlight the specific passage(s) indicating the request(s) for action.",
                "Unlike standard topic-driven text classification, action-item detection requires inferring the senders intent, and as such responds less well to pure bag-of-words classification.",
                "However, using enriched feature sets, such as n-grams (up to n=4) with chi-squared <br>feature selection</br>, and contextual cues for action-item location improve performance by up to 10% over unigrams, using in both cases state of the art classifiers such as SVMs with automated model selection via embedded cross-validation.",
                "Categories and Subject Descriptors H.3.3 [Information Storage and Retrieval]: Information Search and Retrieval; I.2.6 [Artificial Intelligence]: Learning; I.5.4 [Pattern Recognition]: Applications General Terms Experimentation 1.",
                "INTRODUCTION E-mail users are facing an increasingly difficult task of managing their inboxes in the face of mounting challenges that result from rising e-mail usage.",
                "This includes prioritizing e-mails over a range of sources from business partners to family members, filtering and reducing junk e-mail, and quickly managing requests that demand From: Henry Hutchins <hhutchins@innovative.company.com> To: Sara Smith; Joe Johnson; William Woolings Subject: meeting with prospective customers Sent: Fri 12/10/2005 8:08 AM Hi All, Id like to remind all of you that the group from GRTY will be visiting us next Friday at 4:30 p.m.",
                "The current schedule looks like this: + 9:30 a.m.",
                "Informal Breakfast and Discussion in Cafeteria + 10:30 a.m. Company Overview + 11:00 a.m.",
                "Individual Meetings (Continue Over Lunch) + 2:00 p.m. Tour of Facilities + 3:00 p.m.",
                "Sales Pitch In order to have this go off smoothly, I would like to practice the presentation well in advance.",
                "As a result, I will need each of your parts by Wednesday.",
                "Keep up the good work! -Henry Figure 1: An E-mail with emphasized Action-Item, an explicit request that requires the recipients attention or action. the receivers attention or action.",
                "Automated action-item detection targets the third of these problems by attempting to detect which e-mails require an action or response with information, and within those e-mails, attempting to highlight the sentence (or other passage length) that directly indicates the action request.",
                "Such a detection system can be used as one part of an e-mail agent which would assist a user in processing important e-mails quicker than would have been possible without the agent.",
                "We view action-item detection as one necessary component of a successful e-mail agent which would perform spam detection, action-item detection, topic classification and priority ranking, among other functions.",
                "The utility of such a detector can manifest as a method of prioritizing e-mails according to task-oriented criteria other than the standard ones of topic and sender or as a means of ensuring that the email user hasnt dropped the proverbial ball by forgetting to address an action request.",
                "Action-item detection differs from standard text classification in two important ways.",
                "First, the user is interested both in detecting whether an email contains action items and in locating exactly where these action item requests are contained within the email body.",
                "In contrast, standard text categorization merely assigns a topic label to each text, whether that label corresponds to an e-mail folder or a controlled indexing vocabulary [12, 15, 22].",
                "Second, action-item detection attempts to recover the email senders intent - whether she means to elicit response or action on the part of the receiver; note that for this task, classifiers using only unigrams as features do not perform optimally, as evidenced in our results below.",
                "Instead we find that we need more information-laden features such as higher-order n-grams.",
                "Text categorization by topic, on the other hand, works very well using just individual words as features [2, 9, 13, 17].",
                "In fact, genre-classification, which one would think may require more than a bag-of-words approach, also works quite well using just unigram features [14].",
                "Topic detection and tracking (TDT), also works well with unigram feature sets [1, 20].",
                "We believe that action-item detection is one of the first clear instances of an IR-related task where we must move beyond bag-of-words to achieve high performance, albeit not too far, as bag-of-n-grams seem to suffice.",
                "We first review related work for similar text classification problems such as e-mail priority ranking and speech act identification.",
                "Then we more formally define the action-item detection problem, discuss the aspects that distinguish it from more common problems like topic classification, and highlight the challenges in constructing systems that can perform well at the sentence and document level.",
                "From there, we move to a discussion of feature representation and selection techniques appropriate for this problem and how standard text classification approaches can be adapted to smoothly move from the sentence-level detection problem to the documentlevel classification problem.",
                "We then conduct an empirical analysis that helps us determine the effectiveness of our feature extraction procedures as well as establish baselines for a number of classification algorithms on this task.",
                "Finally, we summarize this papers contributions and consider interesting directions for future work. 2.",
                "RELATED WORK Several other researchers have considered very similar text classification tasks.",
                "Cohen et al. [5] describe an ontology of speech acts, such as Propose a Meeting, and attempt to predict when an e-mail contains one of these speech acts.",
                "We consider action-items to be an important specific type of speech act that falls within their more general classification.",
                "While they provide results for several classification methods, their methods only make use of human judgments at the document-level.",
                "In contrast, we consider whether accuracy can be increased by using finer-grained human judgments that mark the specific sentences and phrases of interest.",
                "Corston-Oliver et al. [6] consider detecting items in e-mail to Put on a To-Do List.",
                "This classification task is very similar to ours except they do not consider simple factual questions to belong to this category.",
                "We include questions, but note that not all questions are action-items - some are rhetorical or simply social convention, How are you?.",
                "From a learning perspective, while they make use of judgments at the sentence-level, they do not explicitly compare what if any benefits finer-grained judgments offer.",
                "Additionally, they do not study alternative choices or approaches to the classification task.",
                "Instead, they simply apply a standard SVM at the sentence-level and focus primarily on a linguistic analysis of how the sentence can be logically reformulated before adding it to the task list.",
                "In this study, we examine several alternative classification methods, compare document-level and sentence-level approaches and analyze the machine learning issues implicit in these problems.",
                "Interest in a variety of learning tasks related to e-mail has been rapidly growing in the recent literature.",
                "For example, in a forum dedicated to e-mail learning tasks, Culotta et al. [7] presented methods for learning social networks from e-mail.",
                "In this work, we do not focus on peer relationships; however, such methods could complement those here since peer relationships often influence word choice when requesting an action. 3.",
                "PROBLEM DEFINITION & APPROACH In contrast to previous work, we explicitly focus on the benefits that finer-grained, more costly, sentence-level human judgments offer over coarse-grained document-level judgments.",
                "Additionally, we consider multiple standard text classification approaches and analyze both the quantitative and qualitative differences that arise from taking a document-level vs. a sentence-level approach to classification.",
                "Finally, we focus on the representation necessary to achieve the most competitive performance. 3.1 Problem Definition In order to provide the most benefit to the user, a system would not only detect the document, but it would also indicate the specific sentences in the e-mail which contain the action-items.",
                "Therefore, there are three basic problems: 1.",
                "Document detection: Classify a document as to whether or not it contains an action-item. 2.",
                "Document ranking: Rank the documents such that all documents containing action-items occur as high as possible in the ranking. 3.",
                "Sentence detection: Classify each sentence in a document as to whether or not it is an action-item.",
                "As in most Information Retrieval tasks, the weight the evaluation metric should give to precision and recall depends on the nature of the application.",
                "In situations where a user will eventually read all received messages, ranking (e.g., via precision at recall of 1) may be most important since this will help encourage shorter delays in communications between users.",
                "In contrast, high-precision detection at low recall will be of increasing importance when the user is under severe time-pressure and therefore will likely not read all mail.",
                "This can be the case for crisis managers during disaster management.",
                "Finally, sentence detection plays a role in both timepressure situations and simply to alleviate the users required time to gist the message. 3.2 Approach As mentioned above, the labeled data can come in one of two forms: a document-labeling provides a yes/no label for each document as to whether it contains an action-item; a phrase-labeling provides only a yes label for the specific items of interest.",
                "We term the human judgments a phrase-labeling since the users view of the action-item may not correspond with actual sentence boundaries or predicted sentence boundaries.",
                "Obviously, it is straightforward to generate a document-labeling consistent with a phrase-labeling by labeling a document yes if and only if it contains at least one phrase labeled yes.",
                "To train classifiers for this task, we can take several viewpoints related to both the basic problems we have enumerated and the form of the labeled data.",
                "The document-level view treats each e-mail as a learning instance with an associated class-label.",
                "Then, the document can be converted to a feature-value vector and learning progresses as usual.",
                "Applying a document-level classifier to document detection and ranking is straightforward.",
                "In order to apply it to sentence detection, one must make additional steps.",
                "For example, if the classifier predicts a document contains an action-item, then areas of the document that contain a high-concentration of words which the model weights heavily in favor of action-items can be indicated.",
                "The obvious benefit of the document-level approach is that training set collection costs are lower since the user only has to specify whether or not an e-mail contains an action-item and not the specific sentences.",
                "In the sentence-level view, each e-mail is automatically segmented into sentences, and each sentence is treated as a learning instance with an associated class-label.",
                "Since the phrase-labeling provided by the user may not coincide with the automatic segmentation, we must determine what label to assign a partially overlapping sentence when converting it to a learning instance.",
                "Once trained, applying the resulting classifiers to sentence detection is now straightforward, but in order to apply the classifiers to document detection and document ranking, the individual predictions over each sentence must be aggregated in order to make a document-level prediction.",
                "This approach has the potential to benefit from morespecific labels that enable the learner to focus attention on the key sentences instead of having to learn based on data that the majority of the words in the e-mail provide no or little information about class membership. 3.2.1 Features Consider some of the phrases that might constitute part of an action item: would like to know, let me know, as soon as possible, have you.",
                "Each of these phrases consists of common words that occur in many e-mails.",
                "However, when they occur in the same sentence, they are far more indicative of an action-item.",
                "Additionally, order can be important: consider have you versus you have.",
                "Because of this, we posit that n-grams play a larger role in this problem than is typical of problems like topic classification.",
                "Therefore, we consider all n-grams up to size 4.",
                "When using n-grams, if we find an n-gram of size 4 in a segment of text, we can represent the text as just one occurrence of the ngram or as one occurrence of the n-gram and an occurrence of each smaller n-gram contained by it.",
                "We choose the second of these alternatives since this will allow the algorithm itself to smoothly back-off in terms of recall.",
                "Methods such as na¨ıve Bayes may be hurt by such a representation because of double-counting.",
                "Since sentence-ending punctuation can provide information, we retain the terminating punctuation token when it is identifiable.",
                "Additionally, we add a beginning-of-sentence and end-of-sentence token in order to capture patterns that are often indicators at the beginning or end of a sentence.",
                "Assuming proper punctuation, these extra tokens are unnecessary, but often e-mail lacks proper punctuation.",
                "In addition, for the sentence-level classifiers that use ngrams, we additionally code for each sentence a binary encoding of the position of the sentence relative to the document.",
                "This encoding has eight associated features that represent which octile (the first eighth, second eighth, etc.) contains the sentence. 3.2.2 Implementation Details In order to compare the document-level to the sentence-level approach, we compare predictions at the document-level.",
                "We do not address how to use a document-level classifier to make predictions at the sentence-level.",
                "In order to automatically segment the text of the e-mail, we use the RASP statistical parser [4].",
                "Since the automatically segmented sentences may not correspond directly with the phrase-level boundaries, we treat any sentence that contains at least 30% of a marked action-item segment as an action-item.",
                "When evaluating sentencedetection for the sentence-level system, we use these class labels as ground truth.",
                "Since we are not evaluating multiple segmentation approaches, this does not bias any of the methods.",
                "If multiple segmentation systems were under evaluation, one would need to use a metric that matched predicted positive sentences to phrases labeled positive.",
                "The metric would need to punish overly long true predictions as well as too short predictions.",
                "Our criteria for converting to labeled instances implicitly includes both criteria.",
                "Since the segmentation is fixed, an overly long prediction would be predicting yes for many no instances since presumably the extra length corresponds to additional segmented sentences all of which do not contain 30% of action-item.",
                "Likewise, a too short prediction must correspond to a small sentence included in the action-item but not constituting all of the action-item.",
                "Therefore, in order to consider the prediction to be too short, there will be an additional preceding/following sentence that is an action-item where we incorrectly predicted no.",
                "Once a sentence-level classifier has made a prediction for each sentence, we must combine these predictions to make both a document-level prediction and a document-level score.",
                "We use the simple policy of predicting positive when any of the sentences is predicted positive.",
                "In order to produce a document score for ranking, the confidence that the document contains an action-item is: ψ(d) = 1 n(d) s∈d|π(s)=1 ψ(s) if for any s ∈ d, π(s) = 1 1 n(d) maxs∈d ψ(s) o.w. where s is a sentence in document d, π is the classifiers 1/0 prediction, ψ is the score the classifier assigns as its confidence that π(s) = 1, and n(d) is the greater of 1 and the number of (unigram) tokens in the document.",
                "In other words, when any sentence is predicted positive, the document score is the length normalized sum of the sentence scores above threshold.",
                "When no sentence is predicted positive, the document score is the maximum sentence score normalized by length.",
                "As in other text problems, we are more likely to emit false positives for documents with more words or sentences.",
                "Thus we include a length normalization factor. 4.",
                "EXPERIMENTAL ANALYSIS 4.1 The Data Our corpus consists of e-mails obtained from volunteers at an educational institution and cover subjects such as: organizing a research workshop, arranging for job-candidate interviews, publishing proceedings, and talk announcements.",
                "The messages were anonymized by replacing the names of each individual and institution with a pseudonym.1 After attempting to identify and eliminate duplicate e-mails, the corpus contains 744 e-mail messages.",
                "After identity anonymization, the corpora has three basic versions.",
                "Quoted material refers to the text of a previous e-mail that an author often leaves in an e-mail message when responding to the e-mail.",
                "Quoted material can act as noise when learning since it may include action-items from previous messages that are no longer relevant.",
                "To isolate the effects of quoted material, we have three versions of the corpora.",
                "The raw form contains the basic messages.",
                "The auto-stripped version contains the messages after quoted material has been automatically removed.",
                "The hand-stripped version contains the messages after quoted material has been removed by a human.",
                "Additionally, the hand-stripped version has had any xml content and e-mail signatures removed - leaving only the essential content of the message.",
                "The studies reported here are performed with the hand-stripped version.",
                "This allows us to balance the cognitive load in terms of number of tokens that must be read in the user-studies we report - including quoted material would complicate the user studies since some users might skip the material while others read it.",
                "Additionally, ensuring all quoted material is removed 1 We have an even more highly anonymized version of the corpus that can be made available for some outside experimentation.",
                "Please contact the authors for more information on obtaining this data. prevents tainting the cross-validation since otherwise a test item could occur as quoted material in a training document. 4.1.1 Data Labeling Two human annotators labeled each message as to whether or not it contained an action-item.",
                "In addition, they identified each segment of the e-mail which contained an action-item.",
                "A segment is a contiguous section of text selected by the human annotators and may span several sentences or a complete phrase contained in a sentence.",
                "They were instructed that an action item is an explicit request for information that requires the recipients attention or a required action and told to highlight the phrases or sentences that make up the request.",
                "Annotator 1 No Yes Annotator 2 No 391 26 Yes 29 298 Table 1: Agreement of Human Annotators at Document Level Annotator One labeled 324 messages as containing action items.",
                "Annotator Two labeled 327 messages as containing action items.",
                "The agreement of the human annotators is shown in Tables 1 and 2.",
                "The annotators are said to agree at the document-level when both marked the same document as containing no action-items or both marked at least one action-item regardless of whether the text segments were the same.",
                "At the document-level, the annotators agreed 93% of the time.",
                "The kappa statistic [3, 5] is often used to evaluate inter-annotator agreement: κ = A − R 1 − R A is the empirical estimate of the probability of agreement.",
                "R is the empirical estimate of the probability of random agreement given the empirical class priors.",
                "A value close to −1 implies the annotators agree far less often than would be expected randomly, while a value close to 1 means they agree more often than randomly expected.",
                "At the document-level, the kappa statistic for inter-annotator agreement is 0.85.",
                "This value is both strong enough to expect the problem to be learnable and is comparable with results for similar tasks [5, 6].",
                "In order to determine the sentence-level agreement, we use each judgment to create a sentence-corpus with labels as described in Section 3.2.2, then consider the agreement over these sentences.",
                "This allows us to compare agreement over no judgments.",
                "We perform this comparison over the hand-stripped corpus since that eliminates spurious no judgments that would come from including quoted material, etc.",
                "Both annotators were free to label the subject as an action-item, but since neither did, we omit the subject line of the message as well.",
                "This only reduces the number of no agreements.",
                "This leaves 6301 automatically segmented sentences.",
                "At the sentence-level, the annotators agreed 98% of the time, and the kappa statistic for inter-annotator agreement is 0.82.",
                "In order to produce one single set of judgments, the human annotators went through each annotation where there was disagreement and came to a consensus opinion.",
                "The annotators did not collect statistics during this process but anecdotally reported that the majority of disagreements were either cases of clear annotator oversight or different interpretations of conditional statements.",
                "For example, If you would like to keep your job, come to tomorrows meeting implies a required action where If you would like to join Annotator 1 No Yes Annotator 2 No 5810 65 Yes 74 352 Table 2: Agreement of Human Annotators at Sentence Level the football betting pool, come to tomorrows meeting does not.",
                "The first would be an action-item in most contexts while the second would not.",
                "Of course, many conditional statements are not so clearly interpretable.",
                "After reconciling the judgments there are 416 e-mails with no action-items and 328 e-mails containing actionitems.",
                "Of the 328 e-mails containing action-items, 259 messages have one action-item segment; 55 messages have two action-item segments; 11 messages have three action-item segments.",
                "Two messages have four action-item segments, and one message has six action-item segments.",
                "Computing the sentence-level agreement using the reconciled gold standard judgments with each of the annotators individual judgments gives a kappa of 0.89 for Annotator One and a kappa of 0.92 for Annotator Two.",
                "In terms of message characteristics, there were on average 132 content tokens in the body after stripping.",
                "For action-item messages, there were 115.",
                "However, by examining Figure 2 we see the length distributions are nearly identical.",
                "As would be expected for e-mail, it is a long-tailed distribution with about half the messages having more than 60 tokens in the body (this paragraph has 65 tokens). 4.2 Classifiers For this experiment, we have selected a variety of standard text classification algorithms.",
                "In selecting algorithms, we have chosen algorithms that are not only known to work well but which differ along such lines as discriminative vs. generative and lazy vs. eager.",
                "We have done this in order to provide both a competitive and thorough sampling of learning methods for the task at hand.",
                "This is important since it is easy to improve a strawman classifier by introducing a new representation.",
                "By thoroughly sampling alternative classifier choices we demonstrate that representation improvements over bag-of-words are not due to using the information in the bag-of-words poorly. 4.2.1 kNN We employ a standard variant of the k-nearest neighbor algorithm used in text classification, kNN with s-cut score thresholding [19].",
                "We use a tfidf-weighting of the terms with a distanceweighted vote of the neighbors to compute the score before thresholding it.",
                "In order to choose the value of s for thresholding, we perform leave-one-out cross-validation over the training set.",
                "The value of k is set to be 2( log2 N + 1) where N is the number of training points.",
                "This rule for choosing k is theoretically motivated by results which show such a rule converges to the optimal classifier as the number of training points increases [8].",
                "In practice, we have also found it to be a computational convenience that frequently leads to comparable results with numerically optimizing k via a cross-validation procedure. 4.2.2 Na¨ıve Bayes We use a standard multinomial na¨ıve Bayes classifier [16].",
                "In using this classifier, we smoothed word and class probabilities using a Bayesian estimate (with the word prior) and a Laplace m-estimate, respectively. 0 20 40 60 80 100 120 140 160 0 200 400 600 800 1000 1200 1400 NumberofMessages Number of Tokens All Messages Action-Item Messages 0 0.02 0.04 0.06 0.08 0.1 0.12 0.14 0.16 0.18 0.2 0 200 400 600 800 1000 1200 1400 PercentageofMessages Number of Tokens All Messages Action-Item Messages Figure 2: The Histogram (left) and Distribution (right) of Message Length.",
                "A bin size of 20 words was used.",
                "Only tokens in the body after hand-stripping were counted.",
                "After stripping, the majority of words left are usually actual message content.",
                "Classifiers Document Unigram Document Ngram Sentence Unigram Sentence Ngram F1 kNN 0.6670 ± 0.0288 0.7108 ± 0.0699 0.7615 ± 0.0504 0.7790 ± 0.0460 na¨ıve Bayes 0.6572 ± 0.0749 0.6484 ± 0.0513 0.7715 ± 0.0597 0.7777 ± 0.0426 SVM 0.6904 ± 0.0347 0.7428 ± 0.0422 0.7282 ± 0.0698 0.7682 ± 0.0451 Voted Perceptron 0.6288 ± 0.0395 0.6774 ± 0.0422 0.6511 ± 0.0506 0.6798 ± 0.0913 Accuracy kNN 0.7029 ± 0.0659 0.7486 ± 0.0505 0.7972 ± 0.0435 0.8092 ± 0.0352 na¨ıve Bayes 0.6074 ± 0.0651 0.5816 ± 0.1075 0.7863 ± 0.0553 0.8145 ± 0.0268 SVM 0.7595 ± 0.0309 0.7904 ± 0.0349 0.7958 ± 0.0551 0.8173 ± 0.0258 Voted Perceptron 0.6531 ± 0.0390 0.7164 ± 0.0376 0.6413 ± 0.0833 0.7082 ± 0.1032 Table 3: Average Document-Detection Performance during Cross-Validation for Each Method and the Sample Standard Deviation (Sn−1) in italics.",
                "The best performance for each classifier is shown in bold. 4.2.3 SVM We have used a linear SVM with a tfidf feature representation and L2-norm as implemented in the SVMlight package v6.01 [11].",
                "All default settings were used. 4.2.4 Voted Perceptron Like the SVM, the Voted Perceptron is a kernel-based learning method.",
                "We use the same feature representation and kernel as we have for the SVM, a linear kernel with tfidf-weighting and an L2-norm.",
                "The voted perceptron is an online-learning method that keeps a history of past perceptrons used, as well as a weight signifying how often that perceptron was correct.",
                "With each new training example, a correct classification increases the weight on the current perceptron and an incorrect classification updates the perceptron.",
                "The output of the classifier uses the weights on the perceptra to make a final voted classification.",
                "When used in an offline-manner, multiple passes can be made through the training data.",
                "Both the voted perceptron and the SVM give a solution from the same hypothesis space - in this case, a linear classifier.",
                "Furthermore, it is well-known that the Voted Perceptron increases the margin of the solution after each pass through the training data [10].",
                "Since Cohen et al. [5] obtain worse results using an SVM than a Voted Perceptron with one training iteration, they conclude that the best solution for detecting speech acts may not lie in an area with a large margin.",
                "Because their tasks are highly similar to ours, we employ both classifiers to ensure we are not overlooking a competitive alternative classifier to the SVM for the basic bag-of-words representation. 4.3 Performance Measures To compare the performance of the classification methods, we look at two standard performance measures, F1 and accuracy.",
                "The F1 measure [18, 21] is the harmonic mean of precision and recall where Precision = Correct Positives Predicted Positives and Recall = Correct Positives Actual Positives . 4.4 Experimental Methodology We perform standard 10-fold cross-validation on the set of documents.",
                "For the sentence-level approach, all sentences in a document are either entirely in the training set or entirely in the test set for each fold.",
                "For significance tests, we use a two-tailed t-test [21] to compare the values obtained during each cross-validation fold with a p-value of 0.05.",
                "<br>feature selection</br> was performed using the chi-squared statistic.",
                "Different levels of <br>feature selection</br> were considered for each classifier.",
                "Each of the following number of features was tried: 10, 25, 50, 100, 250, 750, 1000, 2000, 4000.",
                "There are approximately 4700 unigram tokens without <br>feature selection</br>.",
                "In order to choose the number of features to use for each classifier, we perform nested cross-validation and choose the settings that yield the optimal document-level F1 for that classifier.",
                "For this study, only the body of each e-mail message was used.",
                "<br>feature selection</br> is always applied to all candidate features.",
                "That is, for the n-gram representation, the n-grams and position features are also subject to removal by the <br>feature selection</br> method. 4.5 Results The results for document-level classification are given in Table 3.",
                "The primary hypothesis we are concerned with is that n-grams are critical for this task; if this is true, we expect to see a significant gap in performance between the document-level classifiers that use n-grams (denoted Document Ngram) and those using only unigram features (denoted Document Unigram).",
                "Examining Table 3, we observe that this is indeed the case for every classifier except na¨ıve Bayes.",
                "This difference in performance produced by the n-gram representation is statistically significant for each classifier except for na¨ıve Bayes and the accuracy metric for kNN (see Table 4).",
                "Na¨ıve Bayes poor performance with the n-gram representation is not surprising since the bag-of-n-grams causes excessive doublecounting as mentioned in Section 3.2.1; however, na¨ıve Bayes is not hurt at the sentence-level because the sparse examples provide few chances for agglomerative effects of double counting.",
                "In either case, when a language-modeling approach is desired, modeling the n-grams directly would be preferable to na¨ıve Bayes.",
                "More importantly for the n-gram hypothesis, the n-grams lead to the best document-level classifier performance as well.",
                "As would be expected, the difference between the sentence-level n-gram representation and unigram representation is small.",
                "This is because the window of text is so small that the unigram representation, when done at the sentence-level, implicitly picks up on the power of the n-grams.",
                "Further improvement would signify that the order of the words matter even when only considering a small sentence-size window.",
                "Therefore, the finer-grained sentence-level judgments allows a unigram representation to succeed but only when performed in a small window - behaving as an n-gram representation for all practical purposes.",
                "Document Winner Sentence Winner kNN Ngram Ngram na¨ıve Bayes Unigram Ngram SVM Ngram† Ngram Voted Perceptron Ngram† Ngram Table 4: Significance results for n-grams versus unigrams for document detection using document-level and sentence-level classifiers.",
                "When the F1 result is statistically significant, it is shown in bold.",
                "When the accuracy result is significant, it is shown with a † .",
                "F1 Winner Accuracy Winner kNN Sentence Sentence na¨ıve Bayes Sentence Sentence SVM Sentence Sentence Voted Perceptron Sentence Document Table 5: Significance results for sentence-level classifiers vs. document-level classifiers for the document detection problem.",
                "When the result is statistically significant, it is shown in bold.",
                "Further highlighting the improvement from finer-grained judgments and n-grams, Figure 3 graphically depicts the edge the SVM sentence-level classifier has over the standard bag-of-words approach with a precision-recall curve.",
                "In the high precision area of the graph, the consistent edge of the sentence-level classifier is rather impressive - continuing at precision 1 out to 0.1 recall.",
                "This would mean that a tenth of the users action-items would be placed at the top of their action-item sorted inbox.",
                "Additionally, the large separation at the top right of the curves corresponds to the area where the optimal F1 occurs for each classifier, agreeing with the large improvement from 0.6904 to 0.7682 in F1 score.",
                "Considering the relative unexplored nature of classification at the sentence-level, this gives great hope for further increases in performance.",
                "Accuracy F1 Unigram Ngram Unigram Ngram kNN 0.9519 0.9536 0.6540 0.6686 na¨ıve Bayes 0.9419 0.9550 0.6176 0.6676 SVM 0.9559 0.9579 0.6271 0.6672 Voted Perceptron 0.8895 0.9247 0.3744 0.5164 Table 6: Performance of the Sentence-Level Classifiers at Sentence Detection Although Cohen et al. [5] observed that the Voted Perceptron with a single training iteration outperformed SVM in a set of similar tasks, we see no such behavior here.",
                "This further strengthens the evidence that an alternate classifier with the bag-of-words representation could not reach the same level of performance.",
                "The Voted Perceptron classifier does improve when the number of training iterations are increased, but it is still lower than the SVM classifier.",
                "Sentence detection results are presented in Table 6.",
                "With regard to the sentence detection problem, we note that the F1 measure gives a better feel for the remaining room for improvement in this difficult problem.",
                "That is, unlike document detection where actionitem documents are fairly common, action-item sentences are very rare.",
                "Thus, as in other text problems, the accuracy numbers are deceptively high sheerly because of the default accuracy attainable by always predicting no.",
                "Although, the results here are significantly above-random, it is unclear what level of performance is necessary for sentence detection to be useful in and of itself and not simply as a means to document ranking and classification.",
                "Figure 4: Users find action-items quicker when assisted by a classification system.",
                "Finally, when considering a new type of classification task, one of the most basic questions is whether an accurate classifier built for the task can have an impact on the end-user.",
                "In order to demonstrate the impact this task can have on e-mail users, we conducted a user study using an earlier less-accurate version of the sentence classifier - where instead of using just a single sentence, a threesentence windowed-approach was used.",
                "There were three distinct sets of e-mail in which users had to find action-items.",
                "These sets were either presented in a random order (Unordered), ordered by the classifier (Ordered), or ordered by the classifier and with the 0.4 0.5 0.6 0.7 0.8 0.9 1 0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1 Precision Recall Action-Item Detection SVM Performance (Post Model Selection) Document Unigram Sentence Ngram Figure 3: Both n-grams and a small prediction window lead to consistent improvements over the standard approach. center sentence in the highest confidence window highlighted (Order+help).",
                "In order to perform fair comparisons between conditions, the overall number of tokens in each message set should be approximately equal; that is, the cognitive reading load should be approximately the same before the classifiers reordering.",
                "Additionally, users typically show practice effects by improving at the overall task and thus performing better at later message sets.",
                "This is typically handled by varying the ordering of the sets across users so that the means are comparable.",
                "While omitting further detail, we note the sets were balanced for the total number of tokens and a latin square design was used to balance practice effects.",
                "Figure 4 shows that at intervals of 5, 10, and 15 minutes, users consistently found significantly more action-items when assisted by the classifier, but were most critically aided in the first five minutes.",
                "Although, the classifier consistently aids the users, we did not gain an additional end-user impact by highlighting.",
                "As mentioned above, this might be a result of the large room for improvement that still exists for sentence detection, but anecdotal evidence suggests this might also be a result of how the information is presented to the user rather than the accuracy of sentence detection.",
                "For example, highlighting the wrong sentence near an actual action-item hurts the users trust, but if a vague indicator (e.g., an arrow) points to the approximate area the user is not aware of the near-miss.",
                "Since the user studies used a three sentence window, we believe this played a role as well as sentence detection accuracy. 4.6 Discussion In contrast to problems where n-grams have yielded little difference, we believe their power here stems from the fact that many of the meaningful n-grams for action-items consist of common words, e.g., let me know.",
                "Therefore, the document-level unigram approach cannot gain much leverage, even when modeling their joint probability correctly, since these words will often co-occur in the document but not necessarily in a phrase.",
                "Additionally, action-item detection is distinct from many text classification tasks in that a single sentence can change the class label of the document.",
                "As a result, good classifiers cannot rely on aggregating evidence from a large number of weak indicators across the entire document.",
                "Even though we discarded the header information, examining the top-ranked features at the document-level reveals that many of the features are names or parts of e-mail addresses that occurred in the body and are highly associated with e-mails that tend to contain many or no action-items.",
                "A few examples are terms such as org, bob, and gov.",
                "We note that these features will be sensitive to the particular distribution (senders/receivers) and thus the document-level approach may produce classifiers that transfer less readily to alternate contexts and users at different institutions.",
                "This points out that part of the problem of going beyond bag-of-words may be the methodology, and investigating such properties as learning curves and how well a model transfers may highlight differences in models which appear to have similar performance when tested on the distributions they were trained on.",
                "We are currently investigating whether the sentence-level classifiers do perform better over different test corpora without retraining. 5.",
                "FUTURE WORK While applying text classifiers at the document-level is fairly well-understood, there exists the potential for significantly increasing the performance of the sentence-level classifiers.",
                "Such methods include alternate ways of combining the predictions over each sentence, weightings other than tfidf, which may not be appropriate since sentences are small, better sentence segmentation, and other types of phrasal analysis.",
                "Additionally, named entity tagging, time expressions, etc., seem likely candidates for features that can further improve this task.",
                "We are currently pursuing some of these avenues to see what additional gains these offer.",
                "Finally, it would be interesting to investigate the best methods for combining the document-level and sentence-level classifiers.",
                "Since the simple bag-of-words representation at the document-level leads to a learned model that behaves somewhat like a context-specific prior dependent on the sender/receiver and general topic, a first choice would be to treat it as such when combining probability estimates with the sentence-level classifier.",
                "Such a model might serve as a general example for other problems where bag-of-words can establish a baseline model but richer approaches are needed to achieve performance beyond that baseline. 6.",
                "SUMMARY AND CONCLUSIONS The effectiveness of sentence-level detection argues that labeling at the sentence-level provides significant value.",
                "Further experiments are needed to see how this interacts with the amount of training data available.",
                "Sentence detection that is then agglomerated to document-level detection works surprisingly better given low recall than would be expected with sentence-level items.",
                "This, in turn, indicates that improved sentence segmentation methods could yield further improvements in classification.",
                "In this work, we examined how action-items can be effectively detected in e-mails.",
                "Our empirical analysis has demonstrated that n-grams are of key importance to making the most of documentlevel judgments.",
                "When finer-grained judgments are available, then a standard bag-of-words approach using a small (sentence) window size and automatic segmentation techniques can produce results almost as good as the n-gram based approaches.",
                "Acknowledgments This material is based upon work supported by the Defense Advanced Research Projects Agency (DARPA) under Contract No.",
                "NBCHD030010.",
                "Any opinions, findings and conclusions or recommendations expressed in this material are those of the author(s) and do not necessarily reflect the views of the Defense Advanced Research Projects Agency (DARPA), or the Department of InteriorNational Business Center (DOI-NBC).",
                "We would like to extend our sincerest thanks to Jill Lehman whose efforts in data collection were essential in constructing the corpus, and both Jill and Aaron Steinfeld for their direction of the HCI experiments.",
                "We would also like to thank Django Wexler for constructing and supporting the corpus labeling tools and Curtis Huttenhowers support of the text preprocessing package.",
                "Finally, we gratefully acknowledge Scott Fahlman for his encouragement and useful discussions on this topic. 7.",
                "REFERENCES [1] J. Allan, J. Carbonell, G. Doddington, J. Yamron, and Y. Yang.",
                "Topic detection and tracking pilot study: Final report.",
                "In Proceedings of the DARPA Broadcast News Transcription and Understanding Workshop, Washington, D.C., 1998. [2] C. Apte, F. Damerau, and S. M. Weiss.",
                "Automated learning of decision rules for text categorization.",
                "ACM Transactions on Information Systems, 12(3):233-251, July 1994. [3] J. Carletta.",
                "Assessing agreement on classification tasks: The kappa statistic.",
                "Computational Linguistics, 22(2):249-254, 1996. [4] J. Carroll.",
                "High precision extraction of grammatical relations.",
                "In Proceedings of the 19th International Conference on Computational Linguistics (COLING), pages 134-140, 2002. [5] W. W. Cohen, V. R. Carvalho, and T. M. Mitchell.",
                "Learning to classify email into speech acts.",
                "In EMNLP-2004 (Conference on Empirical Methods in Natural Language Processing), pages 309-316, 2004. [6] S. Corston-Oliver, E. Ringger, M. Gamon, and R. Campbell.",
                "Task-focused summarization of email.",
                "In Text Summarization Branches Out: Proceedings of the ACL-04 Workshop, pages 43-50, 2004. [7] A. Culotta, R. Bekkerman, and A. McCallum.",
                "Extracting social networks and contact information from email and the web.",
                "In CEAS-2004 (Conference on Email and Anti-Spam), Mountain View, CA, July 2004. [8] L. Devroye, L. Gy¨orfi, and G. Lugosi.",
                "A Probabilistic Theory of Pattern Recognition.",
                "Springer-Verlag, New York, NY, 1996. [9] S. T. Dumais, J. Platt, D. Heckerman, and M. Sahami.",
                "Inductive learning algorithms and representations for text categorization.",
                "In CIKM 98, Proceedings of the 7th ACM Conference on Information and Knowledge Management, pages 148-155, 1998. [10] Y. Freund and R. Schapire.",
                "Large margin classification using the perceptron algorithm.",
                "Machine Learning, 37(3):277-296, 1999. [11] T. Joachims.",
                "Making large-scale svm learning practical.",
                "In B. Sch¨olkopf, C. J. Burges, and A. J. Smola, editors, Advances in Kernel Methods - Support Vector Learning, pages 41-56.",
                "MIT Press, 1999. [12] L. S. Larkey.",
                "A patent search and classification system.",
                "In Proceedings of the Fourth ACM Conference on Digital Libraries, pages 179 - 187, 1999. [13] D. D. Lewis.",
                "An evaluation of phrasal and clustered representations on a text categorization task.",
                "In SIGIR 92, Proceedings of the 15th Annual International ACM Conference on Research and Development in Information Retrieval, pages 37-50, 1992. [14] Y. Liu, J. Carbonell, and R. Jin.",
                "A pairwise ensemble approach for accurate genre classification.",
                "In Proceedings of the European Conference on Machine Learning (ECML), 2003. [15] Y. Liu, R. Yan, R. Jin, and J. Carbonell.",
                "A comparison study of kernels for multi-label text classification using category association.",
                "In The Twenty-first International Conference on Machine Learning (ICML), 2004. [16] A. McCallum and K. Nigam.",
                "A comparison of event models for naive bayes text classification.",
                "In Working Notes of AAAI 98 (The 15th National Conference on Artificial Intelligence), Workshop on Learning for Text Categorization, pages 41-48, 1998.",
                "TR WS-98-05. [17] F. Sebastiani.",
                "Machine learning in automated text categorization.",
                "ACM Computing Surveys, 34(1):1-47, March 2002. [18] C. J. van Rijsbergen.",
                "Information Retrieval.",
                "Butterworths, London, 1979. [19] Y. Yang.",
                "An evaluation of statistical approaches to text categorization.",
                "Information Retrieval, 1(1/2):67-88, 1999. [20] Y. Yang, J. Carbonell, R. Brown, T. Pierce, B. T. Archibald, and X. Liu.",
                "Learning approaches to topic detection and tracking.",
                "IEEE EXPERT, Special Issue on Applications of Intelligent Information Retrieval, 1999. [21] Y. Yang and X. Liu.",
                "A re-examination of text categorization methods.",
                "In SIGIR 99, Proceedings of the 22nd Annual International ACM Conference on Research and Development in Information Retrieval, pages 42-49, 1999. [22] Y. Yang, J. Zhang, J. Carbonell, and C. Jin.",
                "Topic-conditioned novelty detection.",
                "In Proceedings of the ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, July 2002."
            ],
            "original_annotated_samples": [],
            "translated_annotated_samples": [
                "Sin embargo, el uso de conjuntos de características enriquecidos, como N-grams (hasta N = 4) con \"selección de características\" de Chi cuadrado, y las señales contextuales para la ubicación de acción de acción mejoran el rendimiento hasta un 10% sobre unigramas, utilizando en ambos casosClasificadores de última generación, como SVM con selección automatizada de modelos a través de validación cruzada integrada.",
                "La \"selección de características\" se realizó utilizando la estadística de chi cuadrado.",
                "Se consideraron diferentes niveles de \"selección de características\" para cada clasificador.",
                "Hay aproximadamente 4700 tokens unigram sin \"selección de características\".",
                "La \"selección de características\" siempre se aplica a todas las características candidatas.",
                "Es decir, para la representación de N-Gram, las características de N-grams y la posición también están sujetas a la eliminación del método \"Selección de características\".4.5 Resultados Los resultados para la clasificación a nivel de documento se dan en la Tabla 3."
            ],
            "translated_text": "",
            "candidates": [
                "selección de características",
                "selección de características",
                "selección de características",
                "selección de características",
                "selección de características",
                "selección de características",
                "selección de características",
                "selección de características",
                "selección de características",
                "selección de características",
                "Selección de características",
                "Selección de características"
            ],
            "error": []
        },
        "e-mail": {
            "translated_key": "correo electrónico",
            "is_in_text": true,
            "original_annotated_sentences": [
                "Feature Representation for Effective Action-Item Detection Paul N. Bennett Computer Science Department Carnegie Mellon University Pittsburgh, PA 15213 pbennett+@cs.cmu.edu Jaime Carbonell Language Technologies Institute.",
                "Carnegie Mellon University Pittsburgh, PA 15213 jgc+@cs.cmu.edu ABSTRACT <br>e-mail</br> users face an ever-growing challenge in managing their inboxes due to the growing centrality of email in the workplace for task assignment, action requests, and other roles beyond information dissemination.",
                "Whereas Information Retrieval and Machine Learning techniques are gaining initial acceptance in spam filtering and automated folder assignment, this paper reports on a new task: automated action-item detection, in order to flag emails that require responses, and to highlight the specific passage(s) indicating the request(s) for action.",
                "Unlike standard topic-driven text classification, action-item detection requires inferring the senders intent, and as such responds less well to pure bag-of-words classification.",
                "However, using enriched feature sets, such as n-grams (up to n=4) with chi-squared feature selection, and contextual cues for action-item location improve performance by up to 10% over unigrams, using in both cases state of the art classifiers such as SVMs with automated model selection via embedded cross-validation.",
                "Categories and Subject Descriptors H.3.3 [Information Storage and Retrieval]: Information Search and Retrieval; I.2.6 [Artificial Intelligence]: Learning; I.5.4 [Pattern Recognition]: Applications General Terms Experimentation 1.",
                "INTRODUCTION <br>e-mail</br> users are facing an increasingly difficult task of managing their inboxes in the face of mounting challenges that result from rising <br>e-mail</br> usage.",
                "This includes prioritizing e-mails over a range of sources from business partners to family members, filtering and reducing junk <br>e-mail</br>, and quickly managing requests that demand From: Henry Hutchins <hhutchins@innovative.company.com> To: Sara Smith; Joe Johnson; William Woolings Subject: meeting with prospective customers Sent: Fri 12/10/2005 8:08 AM Hi All, Id like to remind all of you that the group from GRTY will be visiting us next Friday at 4:30 p.m.",
                "The current schedule looks like this: + 9:30 a.m.",
                "Informal Breakfast and Discussion in Cafeteria + 10:30 a.m. Company Overview + 11:00 a.m.",
                "Individual Meetings (Continue Over Lunch) + 2:00 p.m. Tour of Facilities + 3:00 p.m.",
                "Sales Pitch In order to have this go off smoothly, I would like to practice the presentation well in advance.",
                "As a result, I will need each of your parts by Wednesday.",
                "Keep up the good work! -Henry Figure 1: An <br>e-mail</br> with emphasized Action-Item, an explicit request that requires the recipients attention or action. the receivers attention or action.",
                "Automated action-item detection targets the third of these problems by attempting to detect which e-mails require an action or response with information, and within those e-mails, attempting to highlight the sentence (or other passage length) that directly indicates the action request.",
                "Such a detection system can be used as one part of an <br>e-mail</br> agent which would assist a user in processing important e-mails quicker than would have been possible without the agent.",
                "We view action-item detection as one necessary component of a successful <br>e-mail</br> agent which would perform spam detection, action-item detection, topic classification and priority ranking, among other functions.",
                "The utility of such a detector can manifest as a method of prioritizing e-mails according to task-oriented criteria other than the standard ones of topic and sender or as a means of ensuring that the email user hasnt dropped the proverbial ball by forgetting to address an action request.",
                "Action-item detection differs from standard text classification in two important ways.",
                "First, the user is interested both in detecting whether an email contains action items and in locating exactly where these action item requests are contained within the email body.",
                "In contrast, standard text categorization merely assigns a topic label to each text, whether that label corresponds to an <br>e-mail</br> folder or a controlled indexing vocabulary [12, 15, 22].",
                "Second, action-item detection attempts to recover the email senders intent - whether she means to elicit response or action on the part of the receiver; note that for this task, classifiers using only unigrams as features do not perform optimally, as evidenced in our results below.",
                "Instead we find that we need more information-laden features such as higher-order n-grams.",
                "Text categorization by topic, on the other hand, works very well using just individual words as features [2, 9, 13, 17].",
                "In fact, genre-classification, which one would think may require more than a bag-of-words approach, also works quite well using just unigram features [14].",
                "Topic detection and tracking (TDT), also works well with unigram feature sets [1, 20].",
                "We believe that action-item detection is one of the first clear instances of an IR-related task where we must move beyond bag-of-words to achieve high performance, albeit not too far, as bag-of-n-grams seem to suffice.",
                "We first review related work for similar text classification problems such as <br>e-mail</br> priority ranking and speech act identification.",
                "Then we more formally define the action-item detection problem, discuss the aspects that distinguish it from more common problems like topic classification, and highlight the challenges in constructing systems that can perform well at the sentence and document level.",
                "From there, we move to a discussion of feature representation and selection techniques appropriate for this problem and how standard text classification approaches can be adapted to smoothly move from the sentence-level detection problem to the documentlevel classification problem.",
                "We then conduct an empirical analysis that helps us determine the effectiveness of our feature extraction procedures as well as establish baselines for a number of classification algorithms on this task.",
                "Finally, we summarize this papers contributions and consider interesting directions for future work. 2.",
                "RELATED WORK Several other researchers have considered very similar text classification tasks.",
                "Cohen et al. [5] describe an ontology of speech acts, such as Propose a Meeting, and attempt to predict when an <br>e-mail</br> contains one of these speech acts.",
                "We consider action-items to be an important specific type of speech act that falls within their more general classification.",
                "While they provide results for several classification methods, their methods only make use of human judgments at the document-level.",
                "In contrast, we consider whether accuracy can be increased by using finer-grained human judgments that mark the specific sentences and phrases of interest.",
                "Corston-Oliver et al. [6] consider detecting items in <br>e-mail</br> to Put on a To-Do List.",
                "This classification task is very similar to ours except they do not consider simple factual questions to belong to this category.",
                "We include questions, but note that not all questions are action-items - some are rhetorical or simply social convention, How are you?.",
                "From a learning perspective, while they make use of judgments at the sentence-level, they do not explicitly compare what if any benefits finer-grained judgments offer.",
                "Additionally, they do not study alternative choices or approaches to the classification task.",
                "Instead, they simply apply a standard SVM at the sentence-level and focus primarily on a linguistic analysis of how the sentence can be logically reformulated before adding it to the task list.",
                "In this study, we examine several alternative classification methods, compare document-level and sentence-level approaches and analyze the machine learning issues implicit in these problems.",
                "Interest in a variety of learning tasks related to <br>e-mail</br> has been rapidly growing in the recent literature.",
                "For example, in a forum dedicated to <br>e-mail</br> learning tasks, Culotta et al. [7] presented methods for learning social networks from <br>e-mail</br>.",
                "In this work, we do not focus on peer relationships; however, such methods could complement those here since peer relationships often influence word choice when requesting an action. 3.",
                "PROBLEM DEFINITION & APPROACH In contrast to previous work, we explicitly focus on the benefits that finer-grained, more costly, sentence-level human judgments offer over coarse-grained document-level judgments.",
                "Additionally, we consider multiple standard text classification approaches and analyze both the quantitative and qualitative differences that arise from taking a document-level vs. a sentence-level approach to classification.",
                "Finally, we focus on the representation necessary to achieve the most competitive performance. 3.1 Problem Definition In order to provide the most benefit to the user, a system would not only detect the document, but it would also indicate the specific sentences in the <br>e-mail</br> which contain the action-items.",
                "Therefore, there are three basic problems: 1.",
                "Document detection: Classify a document as to whether or not it contains an action-item. 2.",
                "Document ranking: Rank the documents such that all documents containing action-items occur as high as possible in the ranking. 3.",
                "Sentence detection: Classify each sentence in a document as to whether or not it is an action-item.",
                "As in most Information Retrieval tasks, the weight the evaluation metric should give to precision and recall depends on the nature of the application.",
                "In situations where a user will eventually read all received messages, ranking (e.g., via precision at recall of 1) may be most important since this will help encourage shorter delays in communications between users.",
                "In contrast, high-precision detection at low recall will be of increasing importance when the user is under severe time-pressure and therefore will likely not read all mail.",
                "This can be the case for crisis managers during disaster management.",
                "Finally, sentence detection plays a role in both timepressure situations and simply to alleviate the users required time to gist the message. 3.2 Approach As mentioned above, the labeled data can come in one of two forms: a document-labeling provides a yes/no label for each document as to whether it contains an action-item; a phrase-labeling provides only a yes label for the specific items of interest.",
                "We term the human judgments a phrase-labeling since the users view of the action-item may not correspond with actual sentence boundaries or predicted sentence boundaries.",
                "Obviously, it is straightforward to generate a document-labeling consistent with a phrase-labeling by labeling a document yes if and only if it contains at least one phrase labeled yes.",
                "To train classifiers for this task, we can take several viewpoints related to both the basic problems we have enumerated and the form of the labeled data.",
                "The document-level view treats each <br>e-mail</br> as a learning instance with an associated class-label.",
                "Then, the document can be converted to a feature-value vector and learning progresses as usual.",
                "Applying a document-level classifier to document detection and ranking is straightforward.",
                "In order to apply it to sentence detection, one must make additional steps.",
                "For example, if the classifier predicts a document contains an action-item, then areas of the document that contain a high-concentration of words which the model weights heavily in favor of action-items can be indicated.",
                "The obvious benefit of the document-level approach is that training set collection costs are lower since the user only has to specify whether or not an <br>e-mail</br> contains an action-item and not the specific sentences.",
                "In the sentence-level view, each <br>e-mail</br> is automatically segmented into sentences, and each sentence is treated as a learning instance with an associated class-label.",
                "Since the phrase-labeling provided by the user may not coincide with the automatic segmentation, we must determine what label to assign a partially overlapping sentence when converting it to a learning instance.",
                "Once trained, applying the resulting classifiers to sentence detection is now straightforward, but in order to apply the classifiers to document detection and document ranking, the individual predictions over each sentence must be aggregated in order to make a document-level prediction.",
                "This approach has the potential to benefit from morespecific labels that enable the learner to focus attention on the key sentences instead of having to learn based on data that the majority of the words in the <br>e-mail</br> provide no or little information about class membership. 3.2.1 Features Consider some of the phrases that might constitute part of an action item: would like to know, let me know, as soon as possible, have you.",
                "Each of these phrases consists of common words that occur in many e-mails.",
                "However, when they occur in the same sentence, they are far more indicative of an action-item.",
                "Additionally, order can be important: consider have you versus you have.",
                "Because of this, we posit that n-grams play a larger role in this problem than is typical of problems like topic classification.",
                "Therefore, we consider all n-grams up to size 4.",
                "When using n-grams, if we find an n-gram of size 4 in a segment of text, we can represent the text as just one occurrence of the ngram or as one occurrence of the n-gram and an occurrence of each smaller n-gram contained by it.",
                "We choose the second of these alternatives since this will allow the algorithm itself to smoothly back-off in terms of recall.",
                "Methods such as na¨ıve Bayes may be hurt by such a representation because of double-counting.",
                "Since sentence-ending punctuation can provide information, we retain the terminating punctuation token when it is identifiable.",
                "Additionally, we add a beginning-of-sentence and end-of-sentence token in order to capture patterns that are often indicators at the beginning or end of a sentence.",
                "Assuming proper punctuation, these extra tokens are unnecessary, but often <br>e-mail</br> lacks proper punctuation.",
                "In addition, for the sentence-level classifiers that use ngrams, we additionally code for each sentence a binary encoding of the position of the sentence relative to the document.",
                "This encoding has eight associated features that represent which octile (the first eighth, second eighth, etc.) contains the sentence. 3.2.2 Implementation Details In order to compare the document-level to the sentence-level approach, we compare predictions at the document-level.",
                "We do not address how to use a document-level classifier to make predictions at the sentence-level.",
                "In order to automatically segment the text of the <br>e-mail</br>, we use the RASP statistical parser [4].",
                "Since the automatically segmented sentences may not correspond directly with the phrase-level boundaries, we treat any sentence that contains at least 30% of a marked action-item segment as an action-item.",
                "When evaluating sentencedetection for the sentence-level system, we use these class labels as ground truth.",
                "Since we are not evaluating multiple segmentation approaches, this does not bias any of the methods.",
                "If multiple segmentation systems were under evaluation, one would need to use a metric that matched predicted positive sentences to phrases labeled positive.",
                "The metric would need to punish overly long true predictions as well as too short predictions.",
                "Our criteria for converting to labeled instances implicitly includes both criteria.",
                "Since the segmentation is fixed, an overly long prediction would be predicting yes for many no instances since presumably the extra length corresponds to additional segmented sentences all of which do not contain 30% of action-item.",
                "Likewise, a too short prediction must correspond to a small sentence included in the action-item but not constituting all of the action-item.",
                "Therefore, in order to consider the prediction to be too short, there will be an additional preceding/following sentence that is an action-item where we incorrectly predicted no.",
                "Once a sentence-level classifier has made a prediction for each sentence, we must combine these predictions to make both a document-level prediction and a document-level score.",
                "We use the simple policy of predicting positive when any of the sentences is predicted positive.",
                "In order to produce a document score for ranking, the confidence that the document contains an action-item is: ψ(d) = 1 n(d) s∈d|π(s)=1 ψ(s) if for any s ∈ d, π(s) = 1 1 n(d) maxs∈d ψ(s) o.w. where s is a sentence in document d, π is the classifiers 1/0 prediction, ψ is the score the classifier assigns as its confidence that π(s) = 1, and n(d) is the greater of 1 and the number of (unigram) tokens in the document.",
                "In other words, when any sentence is predicted positive, the document score is the length normalized sum of the sentence scores above threshold.",
                "When no sentence is predicted positive, the document score is the maximum sentence score normalized by length.",
                "As in other text problems, we are more likely to emit false positives for documents with more words or sentences.",
                "Thus we include a length normalization factor. 4.",
                "EXPERIMENTAL ANALYSIS 4.1 The Data Our corpus consists of e-mails obtained from volunteers at an educational institution and cover subjects such as: organizing a research workshop, arranging for job-candidate interviews, publishing proceedings, and talk announcements.",
                "The messages were anonymized by replacing the names of each individual and institution with a pseudonym.1 After attempting to identify and eliminate duplicate e-mails, the corpus contains 744 <br>e-mail</br> messages.",
                "After identity anonymization, the corpora has three basic versions.",
                "Quoted material refers to the text of a previous <br>e-mail</br> that an author often leaves in an <br>e-mail</br> message when responding to the e-mail.",
                "Quoted material can act as noise when learning since it may include action-items from previous messages that are no longer relevant.",
                "To isolate the effects of quoted material, we have three versions of the corpora.",
                "The raw form contains the basic messages.",
                "The auto-stripped version contains the messages after quoted material has been automatically removed.",
                "The hand-stripped version contains the messages after quoted material has been removed by a human.",
                "Additionally, the hand-stripped version has had any xml content and <br>e-mail</br> signatures removed - leaving only the essential content of the message.",
                "The studies reported here are performed with the hand-stripped version.",
                "This allows us to balance the cognitive load in terms of number of tokens that must be read in the user-studies we report - including quoted material would complicate the user studies since some users might skip the material while others read it.",
                "Additionally, ensuring all quoted material is removed 1 We have an even more highly anonymized version of the corpus that can be made available for some outside experimentation.",
                "Please contact the authors for more information on obtaining this data. prevents tainting the cross-validation since otherwise a test item could occur as quoted material in a training document. 4.1.1 Data Labeling Two human annotators labeled each message as to whether or not it contained an action-item.",
                "In addition, they identified each segment of the <br>e-mail</br> which contained an action-item.",
                "A segment is a contiguous section of text selected by the human annotators and may span several sentences or a complete phrase contained in a sentence.",
                "They were instructed that an action item is an explicit request for information that requires the recipients attention or a required action and told to highlight the phrases or sentences that make up the request.",
                "Annotator 1 No Yes Annotator 2 No 391 26 Yes 29 298 Table 1: Agreement of Human Annotators at Document Level Annotator One labeled 324 messages as containing action items.",
                "Annotator Two labeled 327 messages as containing action items.",
                "The agreement of the human annotators is shown in Tables 1 and 2.",
                "The annotators are said to agree at the document-level when both marked the same document as containing no action-items or both marked at least one action-item regardless of whether the text segments were the same.",
                "At the document-level, the annotators agreed 93% of the time.",
                "The kappa statistic [3, 5] is often used to evaluate inter-annotator agreement: κ = A − R 1 − R A is the empirical estimate of the probability of agreement.",
                "R is the empirical estimate of the probability of random agreement given the empirical class priors.",
                "A value close to −1 implies the annotators agree far less often than would be expected randomly, while a value close to 1 means they agree more often than randomly expected.",
                "At the document-level, the kappa statistic for inter-annotator agreement is 0.85.",
                "This value is both strong enough to expect the problem to be learnable and is comparable with results for similar tasks [5, 6].",
                "In order to determine the sentence-level agreement, we use each judgment to create a sentence-corpus with labels as described in Section 3.2.2, then consider the agreement over these sentences.",
                "This allows us to compare agreement over no judgments.",
                "We perform this comparison over the hand-stripped corpus since that eliminates spurious no judgments that would come from including quoted material, etc.",
                "Both annotators were free to label the subject as an action-item, but since neither did, we omit the subject line of the message as well.",
                "This only reduces the number of no agreements.",
                "This leaves 6301 automatically segmented sentences.",
                "At the sentence-level, the annotators agreed 98% of the time, and the kappa statistic for inter-annotator agreement is 0.82.",
                "In order to produce one single set of judgments, the human annotators went through each annotation where there was disagreement and came to a consensus opinion.",
                "The annotators did not collect statistics during this process but anecdotally reported that the majority of disagreements were either cases of clear annotator oversight or different interpretations of conditional statements.",
                "For example, If you would like to keep your job, come to tomorrows meeting implies a required action where If you would like to join Annotator 1 No Yes Annotator 2 No 5810 65 Yes 74 352 Table 2: Agreement of Human Annotators at Sentence Level the football betting pool, come to tomorrows meeting does not.",
                "The first would be an action-item in most contexts while the second would not.",
                "Of course, many conditional statements are not so clearly interpretable.",
                "After reconciling the judgments there are 416 e-mails with no action-items and 328 e-mails containing actionitems.",
                "Of the 328 e-mails containing action-items, 259 messages have one action-item segment; 55 messages have two action-item segments; 11 messages have three action-item segments.",
                "Two messages have four action-item segments, and one message has six action-item segments.",
                "Computing the sentence-level agreement using the reconciled gold standard judgments with each of the annotators individual judgments gives a kappa of 0.89 for Annotator One and a kappa of 0.92 for Annotator Two.",
                "In terms of message characteristics, there were on average 132 content tokens in the body after stripping.",
                "For action-item messages, there were 115.",
                "However, by examining Figure 2 we see the length distributions are nearly identical.",
                "As would be expected for <br>e-mail</br>, it is a long-tailed distribution with about half the messages having more than 60 tokens in the body (this paragraph has 65 tokens). 4.2 Classifiers For this experiment, we have selected a variety of standard text classification algorithms.",
                "In selecting algorithms, we have chosen algorithms that are not only known to work well but which differ along such lines as discriminative vs. generative and lazy vs. eager.",
                "We have done this in order to provide both a competitive and thorough sampling of learning methods for the task at hand.",
                "This is important since it is easy to improve a strawman classifier by introducing a new representation.",
                "By thoroughly sampling alternative classifier choices we demonstrate that representation improvements over bag-of-words are not due to using the information in the bag-of-words poorly. 4.2.1 kNN We employ a standard variant of the k-nearest neighbor algorithm used in text classification, kNN with s-cut score thresholding [19].",
                "We use a tfidf-weighting of the terms with a distanceweighted vote of the neighbors to compute the score before thresholding it.",
                "In order to choose the value of s for thresholding, we perform leave-one-out cross-validation over the training set.",
                "The value of k is set to be 2( log2 N + 1) where N is the number of training points.",
                "This rule for choosing k is theoretically motivated by results which show such a rule converges to the optimal classifier as the number of training points increases [8].",
                "In practice, we have also found it to be a computational convenience that frequently leads to comparable results with numerically optimizing k via a cross-validation procedure. 4.2.2 Na¨ıve Bayes We use a standard multinomial na¨ıve Bayes classifier [16].",
                "In using this classifier, we smoothed word and class probabilities using a Bayesian estimate (with the word prior) and a Laplace m-estimate, respectively. 0 20 40 60 80 100 120 140 160 0 200 400 600 800 1000 1200 1400 NumberofMessages Number of Tokens All Messages Action-Item Messages 0 0.02 0.04 0.06 0.08 0.1 0.12 0.14 0.16 0.18 0.2 0 200 400 600 800 1000 1200 1400 PercentageofMessages Number of Tokens All Messages Action-Item Messages Figure 2: The Histogram (left) and Distribution (right) of Message Length.",
                "A bin size of 20 words was used.",
                "Only tokens in the body after hand-stripping were counted.",
                "After stripping, the majority of words left are usually actual message content.",
                "Classifiers Document Unigram Document Ngram Sentence Unigram Sentence Ngram F1 kNN 0.6670 ± 0.0288 0.7108 ± 0.0699 0.7615 ± 0.0504 0.7790 ± 0.0460 na¨ıve Bayes 0.6572 ± 0.0749 0.6484 ± 0.0513 0.7715 ± 0.0597 0.7777 ± 0.0426 SVM 0.6904 ± 0.0347 0.7428 ± 0.0422 0.7282 ± 0.0698 0.7682 ± 0.0451 Voted Perceptron 0.6288 ± 0.0395 0.6774 ± 0.0422 0.6511 ± 0.0506 0.6798 ± 0.0913 Accuracy kNN 0.7029 ± 0.0659 0.7486 ± 0.0505 0.7972 ± 0.0435 0.8092 ± 0.0352 na¨ıve Bayes 0.6074 ± 0.0651 0.5816 ± 0.1075 0.7863 ± 0.0553 0.8145 ± 0.0268 SVM 0.7595 ± 0.0309 0.7904 ± 0.0349 0.7958 ± 0.0551 0.8173 ± 0.0258 Voted Perceptron 0.6531 ± 0.0390 0.7164 ± 0.0376 0.6413 ± 0.0833 0.7082 ± 0.1032 Table 3: Average Document-Detection Performance during Cross-Validation for Each Method and the Sample Standard Deviation (Sn−1) in italics.",
                "The best performance for each classifier is shown in bold. 4.2.3 SVM We have used a linear SVM with a tfidf feature representation and L2-norm as implemented in the SVMlight package v6.01 [11].",
                "All default settings were used. 4.2.4 Voted Perceptron Like the SVM, the Voted Perceptron is a kernel-based learning method.",
                "We use the same feature representation and kernel as we have for the SVM, a linear kernel with tfidf-weighting and an L2-norm.",
                "The voted perceptron is an online-learning method that keeps a history of past perceptrons used, as well as a weight signifying how often that perceptron was correct.",
                "With each new training example, a correct classification increases the weight on the current perceptron and an incorrect classification updates the perceptron.",
                "The output of the classifier uses the weights on the perceptra to make a final voted classification.",
                "When used in an offline-manner, multiple passes can be made through the training data.",
                "Both the voted perceptron and the SVM give a solution from the same hypothesis space - in this case, a linear classifier.",
                "Furthermore, it is well-known that the Voted Perceptron increases the margin of the solution after each pass through the training data [10].",
                "Since Cohen et al. [5] obtain worse results using an SVM than a Voted Perceptron with one training iteration, they conclude that the best solution for detecting speech acts may not lie in an area with a large margin.",
                "Because their tasks are highly similar to ours, we employ both classifiers to ensure we are not overlooking a competitive alternative classifier to the SVM for the basic bag-of-words representation. 4.3 Performance Measures To compare the performance of the classification methods, we look at two standard performance measures, F1 and accuracy.",
                "The F1 measure [18, 21] is the harmonic mean of precision and recall where Precision = Correct Positives Predicted Positives and Recall = Correct Positives Actual Positives . 4.4 Experimental Methodology We perform standard 10-fold cross-validation on the set of documents.",
                "For the sentence-level approach, all sentences in a document are either entirely in the training set or entirely in the test set for each fold.",
                "For significance tests, we use a two-tailed t-test [21] to compare the values obtained during each cross-validation fold with a p-value of 0.05.",
                "Feature selection was performed using the chi-squared statistic.",
                "Different levels of feature selection were considered for each classifier.",
                "Each of the following number of features was tried: 10, 25, 50, 100, 250, 750, 1000, 2000, 4000.",
                "There are approximately 4700 unigram tokens without feature selection.",
                "In order to choose the number of features to use for each classifier, we perform nested cross-validation and choose the settings that yield the optimal document-level F1 for that classifier.",
                "For this study, only the body of each <br>e-mail</br> message was used.",
                "Feature selection is always applied to all candidate features.",
                "That is, for the n-gram representation, the n-grams and position features are also subject to removal by the feature selection method. 4.5 Results The results for document-level classification are given in Table 3.",
                "The primary hypothesis we are concerned with is that n-grams are critical for this task; if this is true, we expect to see a significant gap in performance between the document-level classifiers that use n-grams (denoted Document Ngram) and those using only unigram features (denoted Document Unigram).",
                "Examining Table 3, we observe that this is indeed the case for every classifier except na¨ıve Bayes.",
                "This difference in performance produced by the n-gram representation is statistically significant for each classifier except for na¨ıve Bayes and the accuracy metric for kNN (see Table 4).",
                "Na¨ıve Bayes poor performance with the n-gram representation is not surprising since the bag-of-n-grams causes excessive doublecounting as mentioned in Section 3.2.1; however, na¨ıve Bayes is not hurt at the sentence-level because the sparse examples provide few chances for agglomerative effects of double counting.",
                "In either case, when a language-modeling approach is desired, modeling the n-grams directly would be preferable to na¨ıve Bayes.",
                "More importantly for the n-gram hypothesis, the n-grams lead to the best document-level classifier performance as well.",
                "As would be expected, the difference between the sentence-level n-gram representation and unigram representation is small.",
                "This is because the window of text is so small that the unigram representation, when done at the sentence-level, implicitly picks up on the power of the n-grams.",
                "Further improvement would signify that the order of the words matter even when only considering a small sentence-size window.",
                "Therefore, the finer-grained sentence-level judgments allows a unigram representation to succeed but only when performed in a small window - behaving as an n-gram representation for all practical purposes.",
                "Document Winner Sentence Winner kNN Ngram Ngram na¨ıve Bayes Unigram Ngram SVM Ngram† Ngram Voted Perceptron Ngram† Ngram Table 4: Significance results for n-grams versus unigrams for document detection using document-level and sentence-level classifiers.",
                "When the F1 result is statistically significant, it is shown in bold.",
                "When the accuracy result is significant, it is shown with a † .",
                "F1 Winner Accuracy Winner kNN Sentence Sentence na¨ıve Bayes Sentence Sentence SVM Sentence Sentence Voted Perceptron Sentence Document Table 5: Significance results for sentence-level classifiers vs. document-level classifiers for the document detection problem.",
                "When the result is statistically significant, it is shown in bold.",
                "Further highlighting the improvement from finer-grained judgments and n-grams, Figure 3 graphically depicts the edge the SVM sentence-level classifier has over the standard bag-of-words approach with a precision-recall curve.",
                "In the high precision area of the graph, the consistent edge of the sentence-level classifier is rather impressive - continuing at precision 1 out to 0.1 recall.",
                "This would mean that a tenth of the users action-items would be placed at the top of their action-item sorted inbox.",
                "Additionally, the large separation at the top right of the curves corresponds to the area where the optimal F1 occurs for each classifier, agreeing with the large improvement from 0.6904 to 0.7682 in F1 score.",
                "Considering the relative unexplored nature of classification at the sentence-level, this gives great hope for further increases in performance.",
                "Accuracy F1 Unigram Ngram Unigram Ngram kNN 0.9519 0.9536 0.6540 0.6686 na¨ıve Bayes 0.9419 0.9550 0.6176 0.6676 SVM 0.9559 0.9579 0.6271 0.6672 Voted Perceptron 0.8895 0.9247 0.3744 0.5164 Table 6: Performance of the Sentence-Level Classifiers at Sentence Detection Although Cohen et al. [5] observed that the Voted Perceptron with a single training iteration outperformed SVM in a set of similar tasks, we see no such behavior here.",
                "This further strengthens the evidence that an alternate classifier with the bag-of-words representation could not reach the same level of performance.",
                "The Voted Perceptron classifier does improve when the number of training iterations are increased, but it is still lower than the SVM classifier.",
                "Sentence detection results are presented in Table 6.",
                "With regard to the sentence detection problem, we note that the F1 measure gives a better feel for the remaining room for improvement in this difficult problem.",
                "That is, unlike document detection where actionitem documents are fairly common, action-item sentences are very rare.",
                "Thus, as in other text problems, the accuracy numbers are deceptively high sheerly because of the default accuracy attainable by always predicting no.",
                "Although, the results here are significantly above-random, it is unclear what level of performance is necessary for sentence detection to be useful in and of itself and not simply as a means to document ranking and classification.",
                "Figure 4: Users find action-items quicker when assisted by a classification system.",
                "Finally, when considering a new type of classification task, one of the most basic questions is whether an accurate classifier built for the task can have an impact on the end-user.",
                "In order to demonstrate the impact this task can have on <br>e-mail</br> users, we conducted a user study using an earlier less-accurate version of the sentence classifier - where instead of using just a single sentence, a threesentence windowed-approach was used.",
                "There were three distinct sets of <br>e-mail</br> in which users had to find action-items.",
                "These sets were either presented in a random order (Unordered), ordered by the classifier (Ordered), or ordered by the classifier and with the 0.4 0.5 0.6 0.7 0.8 0.9 1 0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1 Precision Recall Action-Item Detection SVM Performance (Post Model Selection) Document Unigram Sentence Ngram Figure 3: Both n-grams and a small prediction window lead to consistent improvements over the standard approach. center sentence in the highest confidence window highlighted (Order+help).",
                "In order to perform fair comparisons between conditions, the overall number of tokens in each message set should be approximately equal; that is, the cognitive reading load should be approximately the same before the classifiers reordering.",
                "Additionally, users typically show practice effects by improving at the overall task and thus performing better at later message sets.",
                "This is typically handled by varying the ordering of the sets across users so that the means are comparable.",
                "While omitting further detail, we note the sets were balanced for the total number of tokens and a latin square design was used to balance practice effects.",
                "Figure 4 shows that at intervals of 5, 10, and 15 minutes, users consistently found significantly more action-items when assisted by the classifier, but were most critically aided in the first five minutes.",
                "Although, the classifier consistently aids the users, we did not gain an additional end-user impact by highlighting.",
                "As mentioned above, this might be a result of the large room for improvement that still exists for sentence detection, but anecdotal evidence suggests this might also be a result of how the information is presented to the user rather than the accuracy of sentence detection.",
                "For example, highlighting the wrong sentence near an actual action-item hurts the users trust, but if a vague indicator (e.g., an arrow) points to the approximate area the user is not aware of the near-miss.",
                "Since the user studies used a three sentence window, we believe this played a role as well as sentence detection accuracy. 4.6 Discussion In contrast to problems where n-grams have yielded little difference, we believe their power here stems from the fact that many of the meaningful n-grams for action-items consist of common words, e.g., let me know.",
                "Therefore, the document-level unigram approach cannot gain much leverage, even when modeling their joint probability correctly, since these words will often co-occur in the document but not necessarily in a phrase.",
                "Additionally, action-item detection is distinct from many text classification tasks in that a single sentence can change the class label of the document.",
                "As a result, good classifiers cannot rely on aggregating evidence from a large number of weak indicators across the entire document.",
                "Even though we discarded the header information, examining the top-ranked features at the document-level reveals that many of the features are names or parts of <br>e-mail</br> addresses that occurred in the body and are highly associated with e-mails that tend to contain many or no action-items.",
                "A few examples are terms such as org, bob, and gov.",
                "We note that these features will be sensitive to the particular distribution (senders/receivers) and thus the document-level approach may produce classifiers that transfer less readily to alternate contexts and users at different institutions.",
                "This points out that part of the problem of going beyond bag-of-words may be the methodology, and investigating such properties as learning curves and how well a model transfers may highlight differences in models which appear to have similar performance when tested on the distributions they were trained on.",
                "We are currently investigating whether the sentence-level classifiers do perform better over different test corpora without retraining. 5.",
                "FUTURE WORK While applying text classifiers at the document-level is fairly well-understood, there exists the potential for significantly increasing the performance of the sentence-level classifiers.",
                "Such methods include alternate ways of combining the predictions over each sentence, weightings other than tfidf, which may not be appropriate since sentences are small, better sentence segmentation, and other types of phrasal analysis.",
                "Additionally, named entity tagging, time expressions, etc., seem likely candidates for features that can further improve this task.",
                "We are currently pursuing some of these avenues to see what additional gains these offer.",
                "Finally, it would be interesting to investigate the best methods for combining the document-level and sentence-level classifiers.",
                "Since the simple bag-of-words representation at the document-level leads to a learned model that behaves somewhat like a context-specific prior dependent on the sender/receiver and general topic, a first choice would be to treat it as such when combining probability estimates with the sentence-level classifier.",
                "Such a model might serve as a general example for other problems where bag-of-words can establish a baseline model but richer approaches are needed to achieve performance beyond that baseline. 6.",
                "SUMMARY AND CONCLUSIONS The effectiveness of sentence-level detection argues that labeling at the sentence-level provides significant value.",
                "Further experiments are needed to see how this interacts with the amount of training data available.",
                "Sentence detection that is then agglomerated to document-level detection works surprisingly better given low recall than would be expected with sentence-level items.",
                "This, in turn, indicates that improved sentence segmentation methods could yield further improvements in classification.",
                "In this work, we examined how action-items can be effectively detected in e-mails.",
                "Our empirical analysis has demonstrated that n-grams are of key importance to making the most of documentlevel judgments.",
                "When finer-grained judgments are available, then a standard bag-of-words approach using a small (sentence) window size and automatic segmentation techniques can produce results almost as good as the n-gram based approaches.",
                "Acknowledgments This material is based upon work supported by the Defense Advanced Research Projects Agency (DARPA) under Contract No.",
                "NBCHD030010.",
                "Any opinions, findings and conclusions or recommendations expressed in this material are those of the author(s) and do not necessarily reflect the views of the Defense Advanced Research Projects Agency (DARPA), or the Department of InteriorNational Business Center (DOI-NBC).",
                "We would like to extend our sincerest thanks to Jill Lehman whose efforts in data collection were essential in constructing the corpus, and both Jill and Aaron Steinfeld for their direction of the HCI experiments.",
                "We would also like to thank Django Wexler for constructing and supporting the corpus labeling tools and Curtis Huttenhowers support of the text preprocessing package.",
                "Finally, we gratefully acknowledge Scott Fahlman for his encouragement and useful discussions on this topic. 7.",
                "REFERENCES [1] J. Allan, J. Carbonell, G. Doddington, J. Yamron, and Y. Yang.",
                "Topic detection and tracking pilot study: Final report.",
                "In Proceedings of the DARPA Broadcast News Transcription and Understanding Workshop, Washington, D.C., 1998. [2] C. Apte, F. Damerau, and S. M. Weiss.",
                "Automated learning of decision rules for text categorization.",
                "ACM Transactions on Information Systems, 12(3):233-251, July 1994. [3] J. Carletta.",
                "Assessing agreement on classification tasks: The kappa statistic.",
                "Computational Linguistics, 22(2):249-254, 1996. [4] J. Carroll.",
                "High precision extraction of grammatical relations.",
                "In Proceedings of the 19th International Conference on Computational Linguistics (COLING), pages 134-140, 2002. [5] W. W. Cohen, V. R. Carvalho, and T. M. Mitchell.",
                "Learning to classify email into speech acts.",
                "In EMNLP-2004 (Conference on Empirical Methods in Natural Language Processing), pages 309-316, 2004. [6] S. Corston-Oliver, E. Ringger, M. Gamon, and R. Campbell.",
                "Task-focused summarization of email.",
                "In Text Summarization Branches Out: Proceedings of the ACL-04 Workshop, pages 43-50, 2004. [7] A. Culotta, R. Bekkerman, and A. McCallum.",
                "Extracting social networks and contact information from email and the web.",
                "In CEAS-2004 (Conference on Email and Anti-Spam), Mountain View, CA, July 2004. [8] L. Devroye, L. Gy¨orfi, and G. Lugosi.",
                "A Probabilistic Theory of Pattern Recognition.",
                "Springer-Verlag, New York, NY, 1996. [9] S. T. Dumais, J. Platt, D. Heckerman, and M. Sahami.",
                "Inductive learning algorithms and representations for text categorization.",
                "In CIKM 98, Proceedings of the 7th ACM Conference on Information and Knowledge Management, pages 148-155, 1998. [10] Y. Freund and R. Schapire.",
                "Large margin classification using the perceptron algorithm.",
                "Machine Learning, 37(3):277-296, 1999. [11] T. Joachims.",
                "Making large-scale svm learning practical.",
                "In B. Sch¨olkopf, C. J. Burges, and A. J. Smola, editors, Advances in Kernel Methods - Support Vector Learning, pages 41-56.",
                "MIT Press, 1999. [12] L. S. Larkey.",
                "A patent search and classification system.",
                "In Proceedings of the Fourth ACM Conference on Digital Libraries, pages 179 - 187, 1999. [13] D. D. Lewis.",
                "An evaluation of phrasal and clustered representations on a text categorization task.",
                "In SIGIR 92, Proceedings of the 15th Annual International ACM Conference on Research and Development in Information Retrieval, pages 37-50, 1992. [14] Y. Liu, J. Carbonell, and R. Jin.",
                "A pairwise ensemble approach for accurate genre classification.",
                "In Proceedings of the European Conference on Machine Learning (ECML), 2003. [15] Y. Liu, R. Yan, R. Jin, and J. Carbonell.",
                "A comparison study of kernels for multi-label text classification using category association.",
                "In The Twenty-first International Conference on Machine Learning (ICML), 2004. [16] A. McCallum and K. Nigam.",
                "A comparison of event models for naive bayes text classification.",
                "In Working Notes of AAAI 98 (The 15th National Conference on Artificial Intelligence), Workshop on Learning for Text Categorization, pages 41-48, 1998.",
                "TR WS-98-05. [17] F. Sebastiani.",
                "Machine learning in automated text categorization.",
                "ACM Computing Surveys, 34(1):1-47, March 2002. [18] C. J. van Rijsbergen.",
                "Information Retrieval.",
                "Butterworths, London, 1979. [19] Y. Yang.",
                "An evaluation of statistical approaches to text categorization.",
                "Information Retrieval, 1(1/2):67-88, 1999. [20] Y. Yang, J. Carbonell, R. Brown, T. Pierce, B. T. Archibald, and X. Liu.",
                "Learning approaches to topic detection and tracking.",
                "IEEE EXPERT, Special Issue on Applications of Intelligent Information Retrieval, 1999. [21] Y. Yang and X. Liu.",
                "A re-examination of text categorization methods.",
                "In SIGIR 99, Proceedings of the 22nd Annual International ACM Conference on Research and Development in Information Retrieval, pages 42-49, 1999. [22] Y. Yang, J. Zhang, J. Carbonell, and C. Jin.",
                "Topic-conditioned novelty detection.",
                "In Proceedings of the ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, July 2002."
            ],
            "original_annotated_samples": [],
            "translated_annotated_samples": [
                "Universidad de Carnegie Mellon Pittsburgh, PA 15213 jgc+@cs.cmu.edu Resumen Los usuarios de \"correo electrónico\" enfrentan un desafío cada vez mayor en la administración de sus bandejas de entrada debido a la creciente centralidad del correo electrónico en el lugar de trabajo para la tarea, las solicitudes de acción y otrosroles más allá de la difusión de información.",
                "Introducción Los usuarios de \"correo electrónico\" enfrentan una tarea cada vez más difícil de administrar sus bandejas de entrada frente a los crecientes desafíos que resultan del aumento del uso de \"correo electrónico\".",
                "Esto incluye priorizar los correos electrónicos sobre una variedad de fuentes, desde socios comerciales hasta miembros de la familia, filtrar y reducir el \"correo electrónico\" de basura, y administrar rápidamente las solicitudes que demandan de: Henry Hutchins <hhutchins@innovative.comPany.com> a: SaraHerrero;Joe Johnson;William Woolings Asunto: Reunión con posibles clientes enviados: Vie 10/10/2005 8:08 AM Hola a todos, me gustaría recordarles a todos que el grupo de Grty nos visitará el próximo viernes a las 4:30 p.m.",
                "¡Sigan con el buen trabajo!-Henry Figura 1: Un \"correo electrónico\" con ítem de acción enfatizada, una solicitud explícita que requiere la atención o la acción de los destinatarios.la atención o acción de los receptores.",
                "Tal sistema de detección se puede usar como una parte de un agente de \"correo electrónico\" que ayudaría a un usuario a procesar correos electrónicos importantes más rápido de lo que hubiera sido posible sin el agente.",
                "Vemos la detección de elementos de acción como un componente necesario de un agente de \"correo electrónico\" exitoso que realizaría detección de spam, detección de elementos de acción, clasificación de temas y clasificación de prioridad, entre otras funciones.",
                "Por el contrario, la categorización de texto estándar simplemente asigna una etiqueta de tema a cada texto, ya sea que esa etiqueta corresponde a una carpeta de \"correo electrónico\" o un vocabulario de indexación controlado [12, 15, 22].",
                "Primero revisamos el trabajo relacionado para problemas similares de clasificación de texto, como la clasificación de prioridad de \"correo electrónico\" y la identificación de la Ley del habla.",
                "Cohen et al.[5] Describa una ontología de los actos del habla, como proponer una reunión, e intentar predecir cuándo un \"correo electrónico\" contiene una de estas actos de habla.",
                "Corston-Oliver et al.[6] Considere detectar elementos en \"correo electrónico\" para poner en una lista de tareas pendientes.",
                "El interés en una variedad de tareas de aprendizaje relacionadas con el \"correo electrónico\" ha estado creciendo rápidamente en la literatura reciente.",
                "Por ejemplo, en un foro dedicado a tareas de aprendizaje \"por correo electrónico\", Culotta et al.[7] presentaron métodos para aprender redes sociales de \"correo electrónico\".",
                "Finalmente, nos centramos en la representación necesaria para lograr el rendimiento más competitivo.3.1 Definición del problema Para proporcionar el mayor beneficio al usuario, un sistema no solo detectaría el documento, sino que también indicaría las oraciones específicas en el \"correo electrónico\" que contiene los elementos de acción.",
                "La vista a nivel de documento trata cada \"correo electrónico\" como una instancia de aprendizaje con una etiqueta de clase asociada.",
                "El beneficio obvio del enfoque a nivel de documento es que los costos de recolección del conjunto de capacitación son más bajos ya que el usuario solo tiene que especificar si un \"correo electrónico\" contiene o no un elemento de acción y no las oraciones específicas.",
                "En la vista a nivel de oración, cada \"correo electrónico\" se segmenta automáticamente en oraciones, y cada oración se trata como una instancia de aprendizaje con una etiqueta de clase asociada.",
                "Este enfoque tiene el potencial de beneficiarse de las etiquetas Morespecíficas que permiten al alumno centrar la atención en las oraciones clave en lugar de tener que aprender en base a los datos que la mayoría de las palabras en el \"correo electrónico\" proporcionan no o poca información sobre la membresía de la clase..3.2.1 Características Considere algunas de las frases que podrían constituir parte de un elemento de acción: me gustaría saber, avíseme, lo antes posible, lo tenga.",
                "Suponiendo la puntuación adecuada, estos tokens adicionales son innecesarios, pero a menudo \"correo electrónico\" carece de puntuación adecuada.",
                "Para segmentar automáticamente el texto del \"correo electrónico\", utilizamos el analizador estadístico RASP [4].",
                "Los mensajes se anonimizaron reemplazando los nombres de cada individuo e institución con una seudonía.",
                "El material citado se refiere al texto de un \"correo electrónico\" anterior que un autor a menudo deja en un mensaje de \"correo electrónico\" al responder al correo electrónico.",
                "Además, la versión salpicada a mano ha tenido cualquier contenido XML y firmas de \"correo electrónico\" eliminadas, dejando solo el contenido esencial del mensaje.",
                "Además, identificaron cada segmento del \"correo electrónico\" que contenía un ítem de acción.",
                "Como se esperaría para el \"correo electrónico\", es una distribución de cola larga con aproximadamente la mitad de los mensajes que tienen más de 60 tokens en el cuerpo (este párrafo tiene 65 tokens).4.2 Clasificadores para este experimento, hemos seleccionado una variedad de algoritmos de clasificación de texto estándar.",
                "Para este estudio, solo se utilizó el cuerpo de cada mensaje de \"correo electrónico\".",
                "Para demostrar el impacto que esta tarea puede tener en los usuarios de \"correo electrónico\", realizamos un estudio de usuarios utilizando una versión anterior menos precisa del clasificador de oraciones, donde en lugar de usar una sola oración, un enfoque de ventosas triunas en el trío erausado.",
                "Había tres conjuntos distintos de \"correo electrónico\" en los que los usuarios tuvieron que encontrar elementos de acción.",
                "A pesar de que descartamos la información del encabezado, examinar las características mejor clasificadas en el nivel de documento revela que muchas de las características son nombres o partes de direcciones de \"correo electrónico\" que ocurrieron en el cuerpo y están altamente asociadas con correos electrónicos queTienden a contener muchos o ningún elemento de acción."
            ],
            "translated_text": "",
            "candidates": [
                "correo electrónico",
                "correo electrónico",
                "correo electrónico",
                "correo electrónico",
                "correo electrónico",
                "correo electrónico",
                "correo electrónico",
                "correo electrónico",
                "correo electrónico",
                "correo electrónico",
                "correo electrónico",
                "correo electrónico",
                "correo electrónico",
                "correo electrónico",
                "correo electrónico",
                "correo electrónico",
                "correo electrónico",
                "correo electrónico",
                "correo electrónico",
                "correo electrónico",
                "correo electrónico",
                "correo electrónico",
                "correo electrónico",
                "correo electrónico",
                "por correo electrónico",
                "correo electrónico",
                "correo electrónico",
                "correo electrónico",
                "correo electrónico",
                "correo electrónico",
                "correo electrónico",
                "correo electrónico",
                "correo electrónico",
                "correo electrónico",
                "correo electrónico",
                "correo electrónico",
                "correo electrónico",
                "correo electrónico",
                "correo electrónico",
                "correo electrónico",
                "correo electrónico",
                "correo electrónico",
                "correo electrónico",
                "correo electrónico",
                "correo electrónico",
                "correo electrónico",
                "correo electrónico",
                "correo electrónico",
                "correo electrónico",
                "correo electrónico",
                "correo electrónico",
                "correo electrónico",
                "correo electrónico",
                "correo electrónico",
                "correo electrónico",
                "correo electrónico",
                "correo electrónico",
                "correo electrónico"
            ],
            "error": []
        },
        "n-gram": {
            "translated_key": "n-gram",
            "is_in_text": true,
            "original_annotated_sentences": [
                "Feature Representation for Effective Action-Item Detection Paul N. Bennett Computer Science Department Carnegie Mellon University Pittsburgh, PA 15213 pbennett+@cs.cmu.edu Jaime Carbonell Language Technologies Institute.",
                "Carnegie Mellon University Pittsburgh, PA 15213 jgc+@cs.cmu.edu ABSTRACT E-mail users face an ever-growing challenge in managing their inboxes due to the growing centrality of email in the workplace for task assignment, action requests, and other roles beyond information dissemination.",
                "Whereas Information Retrieval and Machine Learning techniques are gaining initial acceptance in spam filtering and automated folder assignment, this paper reports on a new task: automated action-item detection, in order to flag emails that require responses, and to highlight the specific passage(s) indicating the request(s) for action.",
                "Unlike standard topic-driven text classification, action-item detection requires inferring the senders intent, and as such responds less well to pure bag-of-words classification.",
                "However, using enriched feature sets, such as n-grams (up to n=4) with chi-squared feature selection, and contextual cues for action-item location improve performance by up to 10% over unigrams, using in both cases state of the art classifiers such as SVMs with automated model selection via embedded cross-validation.",
                "Categories and Subject Descriptors H.3.3 [Information Storage and Retrieval]: Information Search and Retrieval; I.2.6 [Artificial Intelligence]: Learning; I.5.4 [Pattern Recognition]: Applications General Terms Experimentation 1.",
                "INTRODUCTION E-mail users are facing an increasingly difficult task of managing their inboxes in the face of mounting challenges that result from rising e-mail usage.",
                "This includes prioritizing e-mails over a range of sources from business partners to family members, filtering and reducing junk e-mail, and quickly managing requests that demand From: Henry Hutchins <hhutchins@innovative.company.com> To: Sara Smith; Joe Johnson; William Woolings Subject: meeting with prospective customers Sent: Fri 12/10/2005 8:08 AM Hi All, Id like to remind all of you that the group from GRTY will be visiting us next Friday at 4:30 p.m.",
                "The current schedule looks like this: + 9:30 a.m.",
                "Informal Breakfast and Discussion in Cafeteria + 10:30 a.m. Company Overview + 11:00 a.m.",
                "Individual Meetings (Continue Over Lunch) + 2:00 p.m. Tour of Facilities + 3:00 p.m.",
                "Sales Pitch In order to have this go off smoothly, I would like to practice the presentation well in advance.",
                "As a result, I will need each of your parts by Wednesday.",
                "Keep up the good work! -Henry Figure 1: An E-mail with emphasized Action-Item, an explicit request that requires the recipients attention or action. the receivers attention or action.",
                "Automated action-item detection targets the third of these problems by attempting to detect which e-mails require an action or response with information, and within those e-mails, attempting to highlight the sentence (or other passage length) that directly indicates the action request.",
                "Such a detection system can be used as one part of an e-mail agent which would assist a user in processing important e-mails quicker than would have been possible without the agent.",
                "We view action-item detection as one necessary component of a successful e-mail agent which would perform spam detection, action-item detection, topic classification and priority ranking, among other functions.",
                "The utility of such a detector can manifest as a method of prioritizing e-mails according to task-oriented criteria other than the standard ones of topic and sender or as a means of ensuring that the email user hasnt dropped the proverbial ball by forgetting to address an action request.",
                "Action-item detection differs from standard text classification in two important ways.",
                "First, the user is interested both in detecting whether an email contains action items and in locating exactly where these action item requests are contained within the email body.",
                "In contrast, standard text categorization merely assigns a topic label to each text, whether that label corresponds to an e-mail folder or a controlled indexing vocabulary [12, 15, 22].",
                "Second, action-item detection attempts to recover the email senders intent - whether she means to elicit response or action on the part of the receiver; note that for this task, classifiers using only unigrams as features do not perform optimally, as evidenced in our results below.",
                "Instead we find that we need more information-laden features such as higher-order n-grams.",
                "Text categorization by topic, on the other hand, works very well using just individual words as features [2, 9, 13, 17].",
                "In fact, genre-classification, which one would think may require more than a bag-of-words approach, also works quite well using just unigram features [14].",
                "Topic detection and tracking (TDT), also works well with unigram feature sets [1, 20].",
                "We believe that action-item detection is one of the first clear instances of an IR-related task where we must move beyond bag-of-words to achieve high performance, albeit not too far, as bag-of-n-grams seem to suffice.",
                "We first review related work for similar text classification problems such as e-mail priority ranking and speech act identification.",
                "Then we more formally define the action-item detection problem, discuss the aspects that distinguish it from more common problems like topic classification, and highlight the challenges in constructing systems that can perform well at the sentence and document level.",
                "From there, we move to a discussion of feature representation and selection techniques appropriate for this problem and how standard text classification approaches can be adapted to smoothly move from the sentence-level detection problem to the documentlevel classification problem.",
                "We then conduct an empirical analysis that helps us determine the effectiveness of our feature extraction procedures as well as establish baselines for a number of classification algorithms on this task.",
                "Finally, we summarize this papers contributions and consider interesting directions for future work. 2.",
                "RELATED WORK Several other researchers have considered very similar text classification tasks.",
                "Cohen et al. [5] describe an ontology of speech acts, such as Propose a Meeting, and attempt to predict when an e-mail contains one of these speech acts.",
                "We consider action-items to be an important specific type of speech act that falls within their more general classification.",
                "While they provide results for several classification methods, their methods only make use of human judgments at the document-level.",
                "In contrast, we consider whether accuracy can be increased by using finer-grained human judgments that mark the specific sentences and phrases of interest.",
                "Corston-Oliver et al. [6] consider detecting items in e-mail to Put on a To-Do List.",
                "This classification task is very similar to ours except they do not consider simple factual questions to belong to this category.",
                "We include questions, but note that not all questions are action-items - some are rhetorical or simply social convention, How are you?.",
                "From a learning perspective, while they make use of judgments at the sentence-level, they do not explicitly compare what if any benefits finer-grained judgments offer.",
                "Additionally, they do not study alternative choices or approaches to the classification task.",
                "Instead, they simply apply a standard SVM at the sentence-level and focus primarily on a linguistic analysis of how the sentence can be logically reformulated before adding it to the task list.",
                "In this study, we examine several alternative classification methods, compare document-level and sentence-level approaches and analyze the machine learning issues implicit in these problems.",
                "Interest in a variety of learning tasks related to e-mail has been rapidly growing in the recent literature.",
                "For example, in a forum dedicated to e-mail learning tasks, Culotta et al. [7] presented methods for learning social networks from e-mail.",
                "In this work, we do not focus on peer relationships; however, such methods could complement those here since peer relationships often influence word choice when requesting an action. 3.",
                "PROBLEM DEFINITION & APPROACH In contrast to previous work, we explicitly focus on the benefits that finer-grained, more costly, sentence-level human judgments offer over coarse-grained document-level judgments.",
                "Additionally, we consider multiple standard text classification approaches and analyze both the quantitative and qualitative differences that arise from taking a document-level vs. a sentence-level approach to classification.",
                "Finally, we focus on the representation necessary to achieve the most competitive performance. 3.1 Problem Definition In order to provide the most benefit to the user, a system would not only detect the document, but it would also indicate the specific sentences in the e-mail which contain the action-items.",
                "Therefore, there are three basic problems: 1.",
                "Document detection: Classify a document as to whether or not it contains an action-item. 2.",
                "Document ranking: Rank the documents such that all documents containing action-items occur as high as possible in the ranking. 3.",
                "Sentence detection: Classify each sentence in a document as to whether or not it is an action-item.",
                "As in most Information Retrieval tasks, the weight the evaluation metric should give to precision and recall depends on the nature of the application.",
                "In situations where a user will eventually read all received messages, ranking (e.g., via precision at recall of 1) may be most important since this will help encourage shorter delays in communications between users.",
                "In contrast, high-precision detection at low recall will be of increasing importance when the user is under severe time-pressure and therefore will likely not read all mail.",
                "This can be the case for crisis managers during disaster management.",
                "Finally, sentence detection plays a role in both timepressure situations and simply to alleviate the users required time to gist the message. 3.2 Approach As mentioned above, the labeled data can come in one of two forms: a document-labeling provides a yes/no label for each document as to whether it contains an action-item; a phrase-labeling provides only a yes label for the specific items of interest.",
                "We term the human judgments a phrase-labeling since the users view of the action-item may not correspond with actual sentence boundaries or predicted sentence boundaries.",
                "Obviously, it is straightforward to generate a document-labeling consistent with a phrase-labeling by labeling a document yes if and only if it contains at least one phrase labeled yes.",
                "To train classifiers for this task, we can take several viewpoints related to both the basic problems we have enumerated and the form of the labeled data.",
                "The document-level view treats each e-mail as a learning instance with an associated class-label.",
                "Then, the document can be converted to a feature-value vector and learning progresses as usual.",
                "Applying a document-level classifier to document detection and ranking is straightforward.",
                "In order to apply it to sentence detection, one must make additional steps.",
                "For example, if the classifier predicts a document contains an action-item, then areas of the document that contain a high-concentration of words which the model weights heavily in favor of action-items can be indicated.",
                "The obvious benefit of the document-level approach is that training set collection costs are lower since the user only has to specify whether or not an e-mail contains an action-item and not the specific sentences.",
                "In the sentence-level view, each e-mail is automatically segmented into sentences, and each sentence is treated as a learning instance with an associated class-label.",
                "Since the phrase-labeling provided by the user may not coincide with the automatic segmentation, we must determine what label to assign a partially overlapping sentence when converting it to a learning instance.",
                "Once trained, applying the resulting classifiers to sentence detection is now straightforward, but in order to apply the classifiers to document detection and document ranking, the individual predictions over each sentence must be aggregated in order to make a document-level prediction.",
                "This approach has the potential to benefit from morespecific labels that enable the learner to focus attention on the key sentences instead of having to learn based on data that the majority of the words in the e-mail provide no or little information about class membership. 3.2.1 Features Consider some of the phrases that might constitute part of an action item: would like to know, let me know, as soon as possible, have you.",
                "Each of these phrases consists of common words that occur in many e-mails.",
                "However, when they occur in the same sentence, they are far more indicative of an action-item.",
                "Additionally, order can be important: consider have you versus you have.",
                "Because of this, we posit that n-grams play a larger role in this problem than is typical of problems like topic classification.",
                "Therefore, we consider all n-grams up to size 4.",
                "When using n-grams, if we find an <br>n-gram</br> of size 4 in a segment of text, we can represent the text as just one occurrence of the ngram or as one occurrence of the <br>n-gram</br> and an occurrence of each smaller n-gram contained by it.",
                "We choose the second of these alternatives since this will allow the algorithm itself to smoothly back-off in terms of recall.",
                "Methods such as na¨ıve Bayes may be hurt by such a representation because of double-counting.",
                "Since sentence-ending punctuation can provide information, we retain the terminating punctuation token when it is identifiable.",
                "Additionally, we add a beginning-of-sentence and end-of-sentence token in order to capture patterns that are often indicators at the beginning or end of a sentence.",
                "Assuming proper punctuation, these extra tokens are unnecessary, but often e-mail lacks proper punctuation.",
                "In addition, for the sentence-level classifiers that use ngrams, we additionally code for each sentence a binary encoding of the position of the sentence relative to the document.",
                "This encoding has eight associated features that represent which octile (the first eighth, second eighth, etc.) contains the sentence. 3.2.2 Implementation Details In order to compare the document-level to the sentence-level approach, we compare predictions at the document-level.",
                "We do not address how to use a document-level classifier to make predictions at the sentence-level.",
                "In order to automatically segment the text of the e-mail, we use the RASP statistical parser [4].",
                "Since the automatically segmented sentences may not correspond directly with the phrase-level boundaries, we treat any sentence that contains at least 30% of a marked action-item segment as an action-item.",
                "When evaluating sentencedetection for the sentence-level system, we use these class labels as ground truth.",
                "Since we are not evaluating multiple segmentation approaches, this does not bias any of the methods.",
                "If multiple segmentation systems were under evaluation, one would need to use a metric that matched predicted positive sentences to phrases labeled positive.",
                "The metric would need to punish overly long true predictions as well as too short predictions.",
                "Our criteria for converting to labeled instances implicitly includes both criteria.",
                "Since the segmentation is fixed, an overly long prediction would be predicting yes for many no instances since presumably the extra length corresponds to additional segmented sentences all of which do not contain 30% of action-item.",
                "Likewise, a too short prediction must correspond to a small sentence included in the action-item but not constituting all of the action-item.",
                "Therefore, in order to consider the prediction to be too short, there will be an additional preceding/following sentence that is an action-item where we incorrectly predicted no.",
                "Once a sentence-level classifier has made a prediction for each sentence, we must combine these predictions to make both a document-level prediction and a document-level score.",
                "We use the simple policy of predicting positive when any of the sentences is predicted positive.",
                "In order to produce a document score for ranking, the confidence that the document contains an action-item is: ψ(d) = 1 n(d) s∈d|π(s)=1 ψ(s) if for any s ∈ d, π(s) = 1 1 n(d) maxs∈d ψ(s) o.w. where s is a sentence in document d, π is the classifiers 1/0 prediction, ψ is the score the classifier assigns as its confidence that π(s) = 1, and n(d) is the greater of 1 and the number of (unigram) tokens in the document.",
                "In other words, when any sentence is predicted positive, the document score is the length normalized sum of the sentence scores above threshold.",
                "When no sentence is predicted positive, the document score is the maximum sentence score normalized by length.",
                "As in other text problems, we are more likely to emit false positives for documents with more words or sentences.",
                "Thus we include a length normalization factor. 4.",
                "EXPERIMENTAL ANALYSIS 4.1 The Data Our corpus consists of e-mails obtained from volunteers at an educational institution and cover subjects such as: organizing a research workshop, arranging for job-candidate interviews, publishing proceedings, and talk announcements.",
                "The messages were anonymized by replacing the names of each individual and institution with a pseudonym.1 After attempting to identify and eliminate duplicate e-mails, the corpus contains 744 e-mail messages.",
                "After identity anonymization, the corpora has three basic versions.",
                "Quoted material refers to the text of a previous e-mail that an author often leaves in an e-mail message when responding to the e-mail.",
                "Quoted material can act as noise when learning since it may include action-items from previous messages that are no longer relevant.",
                "To isolate the effects of quoted material, we have three versions of the corpora.",
                "The raw form contains the basic messages.",
                "The auto-stripped version contains the messages after quoted material has been automatically removed.",
                "The hand-stripped version contains the messages after quoted material has been removed by a human.",
                "Additionally, the hand-stripped version has had any xml content and e-mail signatures removed - leaving only the essential content of the message.",
                "The studies reported here are performed with the hand-stripped version.",
                "This allows us to balance the cognitive load in terms of number of tokens that must be read in the user-studies we report - including quoted material would complicate the user studies since some users might skip the material while others read it.",
                "Additionally, ensuring all quoted material is removed 1 We have an even more highly anonymized version of the corpus that can be made available for some outside experimentation.",
                "Please contact the authors for more information on obtaining this data. prevents tainting the cross-validation since otherwise a test item could occur as quoted material in a training document. 4.1.1 Data Labeling Two human annotators labeled each message as to whether or not it contained an action-item.",
                "In addition, they identified each segment of the e-mail which contained an action-item.",
                "A segment is a contiguous section of text selected by the human annotators and may span several sentences or a complete phrase contained in a sentence.",
                "They were instructed that an action item is an explicit request for information that requires the recipients attention or a required action and told to highlight the phrases or sentences that make up the request.",
                "Annotator 1 No Yes Annotator 2 No 391 26 Yes 29 298 Table 1: Agreement of Human Annotators at Document Level Annotator One labeled 324 messages as containing action items.",
                "Annotator Two labeled 327 messages as containing action items.",
                "The agreement of the human annotators is shown in Tables 1 and 2.",
                "The annotators are said to agree at the document-level when both marked the same document as containing no action-items or both marked at least one action-item regardless of whether the text segments were the same.",
                "At the document-level, the annotators agreed 93% of the time.",
                "The kappa statistic [3, 5] is often used to evaluate inter-annotator agreement: κ = A − R 1 − R A is the empirical estimate of the probability of agreement.",
                "R is the empirical estimate of the probability of random agreement given the empirical class priors.",
                "A value close to −1 implies the annotators agree far less often than would be expected randomly, while a value close to 1 means they agree more often than randomly expected.",
                "At the document-level, the kappa statistic for inter-annotator agreement is 0.85.",
                "This value is both strong enough to expect the problem to be learnable and is comparable with results for similar tasks [5, 6].",
                "In order to determine the sentence-level agreement, we use each judgment to create a sentence-corpus with labels as described in Section 3.2.2, then consider the agreement over these sentences.",
                "This allows us to compare agreement over no judgments.",
                "We perform this comparison over the hand-stripped corpus since that eliminates spurious no judgments that would come from including quoted material, etc.",
                "Both annotators were free to label the subject as an action-item, but since neither did, we omit the subject line of the message as well.",
                "This only reduces the number of no agreements.",
                "This leaves 6301 automatically segmented sentences.",
                "At the sentence-level, the annotators agreed 98% of the time, and the kappa statistic for inter-annotator agreement is 0.82.",
                "In order to produce one single set of judgments, the human annotators went through each annotation where there was disagreement and came to a consensus opinion.",
                "The annotators did not collect statistics during this process but anecdotally reported that the majority of disagreements were either cases of clear annotator oversight or different interpretations of conditional statements.",
                "For example, If you would like to keep your job, come to tomorrows meeting implies a required action where If you would like to join Annotator 1 No Yes Annotator 2 No 5810 65 Yes 74 352 Table 2: Agreement of Human Annotators at Sentence Level the football betting pool, come to tomorrows meeting does not.",
                "The first would be an action-item in most contexts while the second would not.",
                "Of course, many conditional statements are not so clearly interpretable.",
                "After reconciling the judgments there are 416 e-mails with no action-items and 328 e-mails containing actionitems.",
                "Of the 328 e-mails containing action-items, 259 messages have one action-item segment; 55 messages have two action-item segments; 11 messages have three action-item segments.",
                "Two messages have four action-item segments, and one message has six action-item segments.",
                "Computing the sentence-level agreement using the reconciled gold standard judgments with each of the annotators individual judgments gives a kappa of 0.89 for Annotator One and a kappa of 0.92 for Annotator Two.",
                "In terms of message characteristics, there were on average 132 content tokens in the body after stripping.",
                "For action-item messages, there were 115.",
                "However, by examining Figure 2 we see the length distributions are nearly identical.",
                "As would be expected for e-mail, it is a long-tailed distribution with about half the messages having more than 60 tokens in the body (this paragraph has 65 tokens). 4.2 Classifiers For this experiment, we have selected a variety of standard text classification algorithms.",
                "In selecting algorithms, we have chosen algorithms that are not only known to work well but which differ along such lines as discriminative vs. generative and lazy vs. eager.",
                "We have done this in order to provide both a competitive and thorough sampling of learning methods for the task at hand.",
                "This is important since it is easy to improve a strawman classifier by introducing a new representation.",
                "By thoroughly sampling alternative classifier choices we demonstrate that representation improvements over bag-of-words are not due to using the information in the bag-of-words poorly. 4.2.1 kNN We employ a standard variant of the k-nearest neighbor algorithm used in text classification, kNN with s-cut score thresholding [19].",
                "We use a tfidf-weighting of the terms with a distanceweighted vote of the neighbors to compute the score before thresholding it.",
                "In order to choose the value of s for thresholding, we perform leave-one-out cross-validation over the training set.",
                "The value of k is set to be 2( log2 N + 1) where N is the number of training points.",
                "This rule for choosing k is theoretically motivated by results which show such a rule converges to the optimal classifier as the number of training points increases [8].",
                "In practice, we have also found it to be a computational convenience that frequently leads to comparable results with numerically optimizing k via a cross-validation procedure. 4.2.2 Na¨ıve Bayes We use a standard multinomial na¨ıve Bayes classifier [16].",
                "In using this classifier, we smoothed word and class probabilities using a Bayesian estimate (with the word prior) and a Laplace m-estimate, respectively. 0 20 40 60 80 100 120 140 160 0 200 400 600 800 1000 1200 1400 NumberofMessages Number of Tokens All Messages Action-Item Messages 0 0.02 0.04 0.06 0.08 0.1 0.12 0.14 0.16 0.18 0.2 0 200 400 600 800 1000 1200 1400 PercentageofMessages Number of Tokens All Messages Action-Item Messages Figure 2: The Histogram (left) and Distribution (right) of Message Length.",
                "A bin size of 20 words was used.",
                "Only tokens in the body after hand-stripping were counted.",
                "After stripping, the majority of words left are usually actual message content.",
                "Classifiers Document Unigram Document Ngram Sentence Unigram Sentence Ngram F1 kNN 0.6670 ± 0.0288 0.7108 ± 0.0699 0.7615 ± 0.0504 0.7790 ± 0.0460 na¨ıve Bayes 0.6572 ± 0.0749 0.6484 ± 0.0513 0.7715 ± 0.0597 0.7777 ± 0.0426 SVM 0.6904 ± 0.0347 0.7428 ± 0.0422 0.7282 ± 0.0698 0.7682 ± 0.0451 Voted Perceptron 0.6288 ± 0.0395 0.6774 ± 0.0422 0.6511 ± 0.0506 0.6798 ± 0.0913 Accuracy kNN 0.7029 ± 0.0659 0.7486 ± 0.0505 0.7972 ± 0.0435 0.8092 ± 0.0352 na¨ıve Bayes 0.6074 ± 0.0651 0.5816 ± 0.1075 0.7863 ± 0.0553 0.8145 ± 0.0268 SVM 0.7595 ± 0.0309 0.7904 ± 0.0349 0.7958 ± 0.0551 0.8173 ± 0.0258 Voted Perceptron 0.6531 ± 0.0390 0.7164 ± 0.0376 0.6413 ± 0.0833 0.7082 ± 0.1032 Table 3: Average Document-Detection Performance during Cross-Validation for Each Method and the Sample Standard Deviation (Sn−1) in italics.",
                "The best performance for each classifier is shown in bold. 4.2.3 SVM We have used a linear SVM with a tfidf feature representation and L2-norm as implemented in the SVMlight package v6.01 [11].",
                "All default settings were used. 4.2.4 Voted Perceptron Like the SVM, the Voted Perceptron is a kernel-based learning method.",
                "We use the same feature representation and kernel as we have for the SVM, a linear kernel with tfidf-weighting and an L2-norm.",
                "The voted perceptron is an online-learning method that keeps a history of past perceptrons used, as well as a weight signifying how often that perceptron was correct.",
                "With each new training example, a correct classification increases the weight on the current perceptron and an incorrect classification updates the perceptron.",
                "The output of the classifier uses the weights on the perceptra to make a final voted classification.",
                "When used in an offline-manner, multiple passes can be made through the training data.",
                "Both the voted perceptron and the SVM give a solution from the same hypothesis space - in this case, a linear classifier.",
                "Furthermore, it is well-known that the Voted Perceptron increases the margin of the solution after each pass through the training data [10].",
                "Since Cohen et al. [5] obtain worse results using an SVM than a Voted Perceptron with one training iteration, they conclude that the best solution for detecting speech acts may not lie in an area with a large margin.",
                "Because their tasks are highly similar to ours, we employ both classifiers to ensure we are not overlooking a competitive alternative classifier to the SVM for the basic bag-of-words representation. 4.3 Performance Measures To compare the performance of the classification methods, we look at two standard performance measures, F1 and accuracy.",
                "The F1 measure [18, 21] is the harmonic mean of precision and recall where Precision = Correct Positives Predicted Positives and Recall = Correct Positives Actual Positives . 4.4 Experimental Methodology We perform standard 10-fold cross-validation on the set of documents.",
                "For the sentence-level approach, all sentences in a document are either entirely in the training set or entirely in the test set for each fold.",
                "For significance tests, we use a two-tailed t-test [21] to compare the values obtained during each cross-validation fold with a p-value of 0.05.",
                "Feature selection was performed using the chi-squared statistic.",
                "Different levels of feature selection were considered for each classifier.",
                "Each of the following number of features was tried: 10, 25, 50, 100, 250, 750, 1000, 2000, 4000.",
                "There are approximately 4700 unigram tokens without feature selection.",
                "In order to choose the number of features to use for each classifier, we perform nested cross-validation and choose the settings that yield the optimal document-level F1 for that classifier.",
                "For this study, only the body of each e-mail message was used.",
                "Feature selection is always applied to all candidate features.",
                "That is, for the <br>n-gram</br> representation, the n-grams and position features are also subject to removal by the feature selection method. 4.5 Results The results for document-level classification are given in Table 3.",
                "The primary hypothesis we are concerned with is that n-grams are critical for this task; if this is true, we expect to see a significant gap in performance between the document-level classifiers that use n-grams (denoted Document Ngram) and those using only unigram features (denoted Document Unigram).",
                "Examining Table 3, we observe that this is indeed the case for every classifier except na¨ıve Bayes.",
                "This difference in performance produced by the <br>n-gram</br> representation is statistically significant for each classifier except for na¨ıve Bayes and the accuracy metric for kNN (see Table 4).",
                "Na¨ıve Bayes poor performance with the <br>n-gram</br> representation is not surprising since the bag-of-n-grams causes excessive doublecounting as mentioned in Section 3.2.1; however, na¨ıve Bayes is not hurt at the sentence-level because the sparse examples provide few chances for agglomerative effects of double counting.",
                "In either case, when a language-modeling approach is desired, modeling the n-grams directly would be preferable to na¨ıve Bayes.",
                "More importantly for the <br>n-gram</br> hypothesis, the n-grams lead to the best document-level classifier performance as well.",
                "As would be expected, the difference between the sentence-level <br>n-gram</br> representation and unigram representation is small.",
                "This is because the window of text is so small that the unigram representation, when done at the sentence-level, implicitly picks up on the power of the n-grams.",
                "Further improvement would signify that the order of the words matter even when only considering a small sentence-size window.",
                "Therefore, the finer-grained sentence-level judgments allows a unigram representation to succeed but only when performed in a small window - behaving as an <br>n-gram</br> representation for all practical purposes.",
                "Document Winner Sentence Winner kNN Ngram Ngram na¨ıve Bayes Unigram Ngram SVM Ngram† Ngram Voted Perceptron Ngram† Ngram Table 4: Significance results for n-grams versus unigrams for document detection using document-level and sentence-level classifiers.",
                "When the F1 result is statistically significant, it is shown in bold.",
                "When the accuracy result is significant, it is shown with a † .",
                "F1 Winner Accuracy Winner kNN Sentence Sentence na¨ıve Bayes Sentence Sentence SVM Sentence Sentence Voted Perceptron Sentence Document Table 5: Significance results for sentence-level classifiers vs. document-level classifiers for the document detection problem.",
                "When the result is statistically significant, it is shown in bold.",
                "Further highlighting the improvement from finer-grained judgments and n-grams, Figure 3 graphically depicts the edge the SVM sentence-level classifier has over the standard bag-of-words approach with a precision-recall curve.",
                "In the high precision area of the graph, the consistent edge of the sentence-level classifier is rather impressive - continuing at precision 1 out to 0.1 recall.",
                "This would mean that a tenth of the users action-items would be placed at the top of their action-item sorted inbox.",
                "Additionally, the large separation at the top right of the curves corresponds to the area where the optimal F1 occurs for each classifier, agreeing with the large improvement from 0.6904 to 0.7682 in F1 score.",
                "Considering the relative unexplored nature of classification at the sentence-level, this gives great hope for further increases in performance.",
                "Accuracy F1 Unigram Ngram Unigram Ngram kNN 0.9519 0.9536 0.6540 0.6686 na¨ıve Bayes 0.9419 0.9550 0.6176 0.6676 SVM 0.9559 0.9579 0.6271 0.6672 Voted Perceptron 0.8895 0.9247 0.3744 0.5164 Table 6: Performance of the Sentence-Level Classifiers at Sentence Detection Although Cohen et al. [5] observed that the Voted Perceptron with a single training iteration outperformed SVM in a set of similar tasks, we see no such behavior here.",
                "This further strengthens the evidence that an alternate classifier with the bag-of-words representation could not reach the same level of performance.",
                "The Voted Perceptron classifier does improve when the number of training iterations are increased, but it is still lower than the SVM classifier.",
                "Sentence detection results are presented in Table 6.",
                "With regard to the sentence detection problem, we note that the F1 measure gives a better feel for the remaining room for improvement in this difficult problem.",
                "That is, unlike document detection where actionitem documents are fairly common, action-item sentences are very rare.",
                "Thus, as in other text problems, the accuracy numbers are deceptively high sheerly because of the default accuracy attainable by always predicting no.",
                "Although, the results here are significantly above-random, it is unclear what level of performance is necessary for sentence detection to be useful in and of itself and not simply as a means to document ranking and classification.",
                "Figure 4: Users find action-items quicker when assisted by a classification system.",
                "Finally, when considering a new type of classification task, one of the most basic questions is whether an accurate classifier built for the task can have an impact on the end-user.",
                "In order to demonstrate the impact this task can have on e-mail users, we conducted a user study using an earlier less-accurate version of the sentence classifier - where instead of using just a single sentence, a threesentence windowed-approach was used.",
                "There were three distinct sets of e-mail in which users had to find action-items.",
                "These sets were either presented in a random order (Unordered), ordered by the classifier (Ordered), or ordered by the classifier and with the 0.4 0.5 0.6 0.7 0.8 0.9 1 0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1 Precision Recall Action-Item Detection SVM Performance (Post Model Selection) Document Unigram Sentence Ngram Figure 3: Both n-grams and a small prediction window lead to consistent improvements over the standard approach. center sentence in the highest confidence window highlighted (Order+help).",
                "In order to perform fair comparisons between conditions, the overall number of tokens in each message set should be approximately equal; that is, the cognitive reading load should be approximately the same before the classifiers reordering.",
                "Additionally, users typically show practice effects by improving at the overall task and thus performing better at later message sets.",
                "This is typically handled by varying the ordering of the sets across users so that the means are comparable.",
                "While omitting further detail, we note the sets were balanced for the total number of tokens and a latin square design was used to balance practice effects.",
                "Figure 4 shows that at intervals of 5, 10, and 15 minutes, users consistently found significantly more action-items when assisted by the classifier, but were most critically aided in the first five minutes.",
                "Although, the classifier consistently aids the users, we did not gain an additional end-user impact by highlighting.",
                "As mentioned above, this might be a result of the large room for improvement that still exists for sentence detection, but anecdotal evidence suggests this might also be a result of how the information is presented to the user rather than the accuracy of sentence detection.",
                "For example, highlighting the wrong sentence near an actual action-item hurts the users trust, but if a vague indicator (e.g., an arrow) points to the approximate area the user is not aware of the near-miss.",
                "Since the user studies used a three sentence window, we believe this played a role as well as sentence detection accuracy. 4.6 Discussion In contrast to problems where n-grams have yielded little difference, we believe their power here stems from the fact that many of the meaningful n-grams for action-items consist of common words, e.g., let me know.",
                "Therefore, the document-level unigram approach cannot gain much leverage, even when modeling their joint probability correctly, since these words will often co-occur in the document but not necessarily in a phrase.",
                "Additionally, action-item detection is distinct from many text classification tasks in that a single sentence can change the class label of the document.",
                "As a result, good classifiers cannot rely on aggregating evidence from a large number of weak indicators across the entire document.",
                "Even though we discarded the header information, examining the top-ranked features at the document-level reveals that many of the features are names or parts of e-mail addresses that occurred in the body and are highly associated with e-mails that tend to contain many or no action-items.",
                "A few examples are terms such as org, bob, and gov.",
                "We note that these features will be sensitive to the particular distribution (senders/receivers) and thus the document-level approach may produce classifiers that transfer less readily to alternate contexts and users at different institutions.",
                "This points out that part of the problem of going beyond bag-of-words may be the methodology, and investigating such properties as learning curves and how well a model transfers may highlight differences in models which appear to have similar performance when tested on the distributions they were trained on.",
                "We are currently investigating whether the sentence-level classifiers do perform better over different test corpora without retraining. 5.",
                "FUTURE WORK While applying text classifiers at the document-level is fairly well-understood, there exists the potential for significantly increasing the performance of the sentence-level classifiers.",
                "Such methods include alternate ways of combining the predictions over each sentence, weightings other than tfidf, which may not be appropriate since sentences are small, better sentence segmentation, and other types of phrasal analysis.",
                "Additionally, named entity tagging, time expressions, etc., seem likely candidates for features that can further improve this task.",
                "We are currently pursuing some of these avenues to see what additional gains these offer.",
                "Finally, it would be interesting to investigate the best methods for combining the document-level and sentence-level classifiers.",
                "Since the simple bag-of-words representation at the document-level leads to a learned model that behaves somewhat like a context-specific prior dependent on the sender/receiver and general topic, a first choice would be to treat it as such when combining probability estimates with the sentence-level classifier.",
                "Such a model might serve as a general example for other problems where bag-of-words can establish a baseline model but richer approaches are needed to achieve performance beyond that baseline. 6.",
                "SUMMARY AND CONCLUSIONS The effectiveness of sentence-level detection argues that labeling at the sentence-level provides significant value.",
                "Further experiments are needed to see how this interacts with the amount of training data available.",
                "Sentence detection that is then agglomerated to document-level detection works surprisingly better given low recall than would be expected with sentence-level items.",
                "This, in turn, indicates that improved sentence segmentation methods could yield further improvements in classification.",
                "In this work, we examined how action-items can be effectively detected in e-mails.",
                "Our empirical analysis has demonstrated that n-grams are of key importance to making the most of documentlevel judgments.",
                "When finer-grained judgments are available, then a standard bag-of-words approach using a small (sentence) window size and automatic segmentation techniques can produce results almost as good as the <br>n-gram</br> based approaches.",
                "Acknowledgments This material is based upon work supported by the Defense Advanced Research Projects Agency (DARPA) under Contract No.",
                "NBCHD030010.",
                "Any opinions, findings and conclusions or recommendations expressed in this material are those of the author(s) and do not necessarily reflect the views of the Defense Advanced Research Projects Agency (DARPA), or the Department of InteriorNational Business Center (DOI-NBC).",
                "We would like to extend our sincerest thanks to Jill Lehman whose efforts in data collection were essential in constructing the corpus, and both Jill and Aaron Steinfeld for their direction of the HCI experiments.",
                "We would also like to thank Django Wexler for constructing and supporting the corpus labeling tools and Curtis Huttenhowers support of the text preprocessing package.",
                "Finally, we gratefully acknowledge Scott Fahlman for his encouragement and useful discussions on this topic. 7.",
                "REFERENCES [1] J. Allan, J. Carbonell, G. Doddington, J. Yamron, and Y. Yang.",
                "Topic detection and tracking pilot study: Final report.",
                "In Proceedings of the DARPA Broadcast News Transcription and Understanding Workshop, Washington, D.C., 1998. [2] C. Apte, F. Damerau, and S. M. Weiss.",
                "Automated learning of decision rules for text categorization.",
                "ACM Transactions on Information Systems, 12(3):233-251, July 1994. [3] J. Carletta.",
                "Assessing agreement on classification tasks: The kappa statistic.",
                "Computational Linguistics, 22(2):249-254, 1996. [4] J. Carroll.",
                "High precision extraction of grammatical relations.",
                "In Proceedings of the 19th International Conference on Computational Linguistics (COLING), pages 134-140, 2002. [5] W. W. Cohen, V. R. Carvalho, and T. M. Mitchell.",
                "Learning to classify email into speech acts.",
                "In EMNLP-2004 (Conference on Empirical Methods in Natural Language Processing), pages 309-316, 2004. [6] S. Corston-Oliver, E. Ringger, M. Gamon, and R. Campbell.",
                "Task-focused summarization of email.",
                "In Text Summarization Branches Out: Proceedings of the ACL-04 Workshop, pages 43-50, 2004. [7] A. Culotta, R. Bekkerman, and A. McCallum.",
                "Extracting social networks and contact information from email and the web.",
                "In CEAS-2004 (Conference on Email and Anti-Spam), Mountain View, CA, July 2004. [8] L. Devroye, L. Gy¨orfi, and G. Lugosi.",
                "A Probabilistic Theory of Pattern Recognition.",
                "Springer-Verlag, New York, NY, 1996. [9] S. T. Dumais, J. Platt, D. Heckerman, and M. Sahami.",
                "Inductive learning algorithms and representations for text categorization.",
                "In CIKM 98, Proceedings of the 7th ACM Conference on Information and Knowledge Management, pages 148-155, 1998. [10] Y. Freund and R. Schapire.",
                "Large margin classification using the perceptron algorithm.",
                "Machine Learning, 37(3):277-296, 1999. [11] T. Joachims.",
                "Making large-scale svm learning practical.",
                "In B. Sch¨olkopf, C. J. Burges, and A. J. Smola, editors, Advances in Kernel Methods - Support Vector Learning, pages 41-56.",
                "MIT Press, 1999. [12] L. S. Larkey.",
                "A patent search and classification system.",
                "In Proceedings of the Fourth ACM Conference on Digital Libraries, pages 179 - 187, 1999. [13] D. D. Lewis.",
                "An evaluation of phrasal and clustered representations on a text categorization task.",
                "In SIGIR 92, Proceedings of the 15th Annual International ACM Conference on Research and Development in Information Retrieval, pages 37-50, 1992. [14] Y. Liu, J. Carbonell, and R. Jin.",
                "A pairwise ensemble approach for accurate genre classification.",
                "In Proceedings of the European Conference on Machine Learning (ECML), 2003. [15] Y. Liu, R. Yan, R. Jin, and J. Carbonell.",
                "A comparison study of kernels for multi-label text classification using category association.",
                "In The Twenty-first International Conference on Machine Learning (ICML), 2004. [16] A. McCallum and K. Nigam.",
                "A comparison of event models for naive bayes text classification.",
                "In Working Notes of AAAI 98 (The 15th National Conference on Artificial Intelligence), Workshop on Learning for Text Categorization, pages 41-48, 1998.",
                "TR WS-98-05. [17] F. Sebastiani.",
                "Machine learning in automated text categorization.",
                "ACM Computing Surveys, 34(1):1-47, March 2002. [18] C. J. van Rijsbergen.",
                "Information Retrieval.",
                "Butterworths, London, 1979. [19] Y. Yang.",
                "An evaluation of statistical approaches to text categorization.",
                "Information Retrieval, 1(1/2):67-88, 1999. [20] Y. Yang, J. Carbonell, R. Brown, T. Pierce, B. T. Archibald, and X. Liu.",
                "Learning approaches to topic detection and tracking.",
                "IEEE EXPERT, Special Issue on Applications of Intelligent Information Retrieval, 1999. [21] Y. Yang and X. Liu.",
                "A re-examination of text categorization methods.",
                "In SIGIR 99, Proceedings of the 22nd Annual International ACM Conference on Research and Development in Information Retrieval, pages 42-49, 1999. [22] Y. Yang, J. Zhang, J. Carbonell, and C. Jin.",
                "Topic-conditioned novelty detection.",
                "In Proceedings of the ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, July 2002."
            ],
            "original_annotated_samples": [],
            "translated_annotated_samples": [
                "Cuando usamos N-Grams, si encontramos un \"N-Gram\" de tamaño 4 en un segmento de texto, podemos representar el texto como solo una aparición del NGRAM o como una ocurrencia del \"N-Gram\" y una ocurrenciade cada n-gram más pequeño contenido por él.",
                "Es decir, para la representación \"N-Gram\", los N-Grams y las características de posición también están sujetas a la eliminación del método de selección de características.4.5 Resultados Los resultados para la clasificación a nivel de documento se dan en la Tabla 3.",
                "Esta diferencia en el rendimiento producida por la representación \"N-Gram\" es estadísticamente significativa para cada clasificador, excepto para Na¨ıve Bayes y la métrica de precisión para KNN (ver Tabla 4).",
                "Na¨ıve Bayes El bajo rendimiento con la representación \"n-gram\" no es sorprendente ya que la bolsa de n-grams causa doble cuenta de duplicación como se menciona en la sección 3.2.1;Sin embargo, Na¨ıve Bayes no se duele a nivel de oración porque los ejemplos escasos proporcionan pocas posibilidades de efectos aglomerativos del conteo doble.",
                "Más importante aún, para la hipótesis \"N-Gram\", los N-Grams conducen al mejor rendimiento del clasificador a nivel de documento.",
                "Como era de esperar, la diferencia entre la representación \"n-gram\" a nivel de oración y la representación unigram es pequeña.",
                "Por lo tanto, los juicios a nivel de oración de grano más fino permiten que una representación unigram tenga éxito, pero solo cuando se realiza en una pequeña ventana, que se comporta como una representación de \"n-gram\" para todos los efectos prácticos.",
                "Cuando los juicios de grano más fino están disponibles, entonces un enfoque estándar de la bolsa de palabras utilizando un pequeño tamaño de la ventana (oración) y las técnicas de segmentación automática pueden producir resultados casi tan buenos como los enfoques basados en \"N-Gram\"."
            ],
            "translated_text": "",
            "candidates": [
                "n-gramo",
                "N-Gram",
                "N-Gram",
                "N-Gram",
                "N-Gram",
                "n-gramo",
                "N-Gram",
                "n-gramo",
                "n-gram",
                "n-gramo",
                "N-Gram",
                "n-gramo",
                "n-gram",
                "n-gramo",
                "n-gram",
                "n-gramo",
                "N-Gram"
            ],
            "error": []
        },
        "svm": {
            "translated_key": "SVM",
            "is_in_text": true,
            "original_annotated_sentences": [
                "Feature Representation for Effective Action-Item Detection Paul N. Bennett Computer Science Department Carnegie Mellon University Pittsburgh, PA 15213 pbennett+@cs.cmu.edu Jaime Carbonell Language Technologies Institute.",
                "Carnegie Mellon University Pittsburgh, PA 15213 jgc+@cs.cmu.edu ABSTRACT E-mail users face an ever-growing challenge in managing their inboxes due to the growing centrality of email in the workplace for task assignment, action requests, and other roles beyond information dissemination.",
                "Whereas Information Retrieval and Machine Learning techniques are gaining initial acceptance in spam filtering and automated folder assignment, this paper reports on a new task: automated action-item detection, in order to flag emails that require responses, and to highlight the specific passage(s) indicating the request(s) for action.",
                "Unlike standard topic-driven text classification, action-item detection requires inferring the senders intent, and as such responds less well to pure bag-of-words classification.",
                "However, using enriched feature sets, such as n-grams (up to n=4) with chi-squared feature selection, and contextual cues for action-item location improve performance by up to 10% over unigrams, using in both cases state of the art classifiers such as SVMs with automated model selection via embedded cross-validation.",
                "Categories and Subject Descriptors H.3.3 [Information Storage and Retrieval]: Information Search and Retrieval; I.2.6 [Artificial Intelligence]: Learning; I.5.4 [Pattern Recognition]: Applications General Terms Experimentation 1.",
                "INTRODUCTION E-mail users are facing an increasingly difficult task of managing their inboxes in the face of mounting challenges that result from rising e-mail usage.",
                "This includes prioritizing e-mails over a range of sources from business partners to family members, filtering and reducing junk e-mail, and quickly managing requests that demand From: Henry Hutchins <hhutchins@innovative.company.com> To: Sara Smith; Joe Johnson; William Woolings Subject: meeting with prospective customers Sent: Fri 12/10/2005 8:08 AM Hi All, Id like to remind all of you that the group from GRTY will be visiting us next Friday at 4:30 p.m.",
                "The current schedule looks like this: + 9:30 a.m.",
                "Informal Breakfast and Discussion in Cafeteria + 10:30 a.m. Company Overview + 11:00 a.m.",
                "Individual Meetings (Continue Over Lunch) + 2:00 p.m. Tour of Facilities + 3:00 p.m.",
                "Sales Pitch In order to have this go off smoothly, I would like to practice the presentation well in advance.",
                "As a result, I will need each of your parts by Wednesday.",
                "Keep up the good work! -Henry Figure 1: An E-mail with emphasized Action-Item, an explicit request that requires the recipients attention or action. the receivers attention or action.",
                "Automated action-item detection targets the third of these problems by attempting to detect which e-mails require an action or response with information, and within those e-mails, attempting to highlight the sentence (or other passage length) that directly indicates the action request.",
                "Such a detection system can be used as one part of an e-mail agent which would assist a user in processing important e-mails quicker than would have been possible without the agent.",
                "We view action-item detection as one necessary component of a successful e-mail agent which would perform spam detection, action-item detection, topic classification and priority ranking, among other functions.",
                "The utility of such a detector can manifest as a method of prioritizing e-mails according to task-oriented criteria other than the standard ones of topic and sender or as a means of ensuring that the email user hasnt dropped the proverbial ball by forgetting to address an action request.",
                "Action-item detection differs from standard text classification in two important ways.",
                "First, the user is interested both in detecting whether an email contains action items and in locating exactly where these action item requests are contained within the email body.",
                "In contrast, standard text categorization merely assigns a topic label to each text, whether that label corresponds to an e-mail folder or a controlled indexing vocabulary [12, 15, 22].",
                "Second, action-item detection attempts to recover the email senders intent - whether she means to elicit response or action on the part of the receiver; note that for this task, classifiers using only unigrams as features do not perform optimally, as evidenced in our results below.",
                "Instead we find that we need more information-laden features such as higher-order n-grams.",
                "Text categorization by topic, on the other hand, works very well using just individual words as features [2, 9, 13, 17].",
                "In fact, genre-classification, which one would think may require more than a bag-of-words approach, also works quite well using just unigram features [14].",
                "Topic detection and tracking (TDT), also works well with unigram feature sets [1, 20].",
                "We believe that action-item detection is one of the first clear instances of an IR-related task where we must move beyond bag-of-words to achieve high performance, albeit not too far, as bag-of-n-grams seem to suffice.",
                "We first review related work for similar text classification problems such as e-mail priority ranking and speech act identification.",
                "Then we more formally define the action-item detection problem, discuss the aspects that distinguish it from more common problems like topic classification, and highlight the challenges in constructing systems that can perform well at the sentence and document level.",
                "From there, we move to a discussion of feature representation and selection techniques appropriate for this problem and how standard text classification approaches can be adapted to smoothly move from the sentence-level detection problem to the documentlevel classification problem.",
                "We then conduct an empirical analysis that helps us determine the effectiveness of our feature extraction procedures as well as establish baselines for a number of classification algorithms on this task.",
                "Finally, we summarize this papers contributions and consider interesting directions for future work. 2.",
                "RELATED WORK Several other researchers have considered very similar text classification tasks.",
                "Cohen et al. [5] describe an ontology of speech acts, such as Propose a Meeting, and attempt to predict when an e-mail contains one of these speech acts.",
                "We consider action-items to be an important specific type of speech act that falls within their more general classification.",
                "While they provide results for several classification methods, their methods only make use of human judgments at the document-level.",
                "In contrast, we consider whether accuracy can be increased by using finer-grained human judgments that mark the specific sentences and phrases of interest.",
                "Corston-Oliver et al. [6] consider detecting items in e-mail to Put on a To-Do List.",
                "This classification task is very similar to ours except they do not consider simple factual questions to belong to this category.",
                "We include questions, but note that not all questions are action-items - some are rhetorical or simply social convention, How are you?.",
                "From a learning perspective, while they make use of judgments at the sentence-level, they do not explicitly compare what if any benefits finer-grained judgments offer.",
                "Additionally, they do not study alternative choices or approaches to the classification task.",
                "Instead, they simply apply a standard <br>svm</br> at the sentence-level and focus primarily on a linguistic analysis of how the sentence can be logically reformulated before adding it to the task list.",
                "In this study, we examine several alternative classification methods, compare document-level and sentence-level approaches and analyze the machine learning issues implicit in these problems.",
                "Interest in a variety of learning tasks related to e-mail has been rapidly growing in the recent literature.",
                "For example, in a forum dedicated to e-mail learning tasks, Culotta et al. [7] presented methods for learning social networks from e-mail.",
                "In this work, we do not focus on peer relationships; however, such methods could complement those here since peer relationships often influence word choice when requesting an action. 3.",
                "PROBLEM DEFINITION & APPROACH In contrast to previous work, we explicitly focus on the benefits that finer-grained, more costly, sentence-level human judgments offer over coarse-grained document-level judgments.",
                "Additionally, we consider multiple standard text classification approaches and analyze both the quantitative and qualitative differences that arise from taking a document-level vs. a sentence-level approach to classification.",
                "Finally, we focus on the representation necessary to achieve the most competitive performance. 3.1 Problem Definition In order to provide the most benefit to the user, a system would not only detect the document, but it would also indicate the specific sentences in the e-mail which contain the action-items.",
                "Therefore, there are three basic problems: 1.",
                "Document detection: Classify a document as to whether or not it contains an action-item. 2.",
                "Document ranking: Rank the documents such that all documents containing action-items occur as high as possible in the ranking. 3.",
                "Sentence detection: Classify each sentence in a document as to whether or not it is an action-item.",
                "As in most Information Retrieval tasks, the weight the evaluation metric should give to precision and recall depends on the nature of the application.",
                "In situations where a user will eventually read all received messages, ranking (e.g., via precision at recall of 1) may be most important since this will help encourage shorter delays in communications between users.",
                "In contrast, high-precision detection at low recall will be of increasing importance when the user is under severe time-pressure and therefore will likely not read all mail.",
                "This can be the case for crisis managers during disaster management.",
                "Finally, sentence detection plays a role in both timepressure situations and simply to alleviate the users required time to gist the message. 3.2 Approach As mentioned above, the labeled data can come in one of two forms: a document-labeling provides a yes/no label for each document as to whether it contains an action-item; a phrase-labeling provides only a yes label for the specific items of interest.",
                "We term the human judgments a phrase-labeling since the users view of the action-item may not correspond with actual sentence boundaries or predicted sentence boundaries.",
                "Obviously, it is straightforward to generate a document-labeling consistent with a phrase-labeling by labeling a document yes if and only if it contains at least one phrase labeled yes.",
                "To train classifiers for this task, we can take several viewpoints related to both the basic problems we have enumerated and the form of the labeled data.",
                "The document-level view treats each e-mail as a learning instance with an associated class-label.",
                "Then, the document can be converted to a feature-value vector and learning progresses as usual.",
                "Applying a document-level classifier to document detection and ranking is straightforward.",
                "In order to apply it to sentence detection, one must make additional steps.",
                "For example, if the classifier predicts a document contains an action-item, then areas of the document that contain a high-concentration of words which the model weights heavily in favor of action-items can be indicated.",
                "The obvious benefit of the document-level approach is that training set collection costs are lower since the user only has to specify whether or not an e-mail contains an action-item and not the specific sentences.",
                "In the sentence-level view, each e-mail is automatically segmented into sentences, and each sentence is treated as a learning instance with an associated class-label.",
                "Since the phrase-labeling provided by the user may not coincide with the automatic segmentation, we must determine what label to assign a partially overlapping sentence when converting it to a learning instance.",
                "Once trained, applying the resulting classifiers to sentence detection is now straightforward, but in order to apply the classifiers to document detection and document ranking, the individual predictions over each sentence must be aggregated in order to make a document-level prediction.",
                "This approach has the potential to benefit from morespecific labels that enable the learner to focus attention on the key sentences instead of having to learn based on data that the majority of the words in the e-mail provide no or little information about class membership. 3.2.1 Features Consider some of the phrases that might constitute part of an action item: would like to know, let me know, as soon as possible, have you.",
                "Each of these phrases consists of common words that occur in many e-mails.",
                "However, when they occur in the same sentence, they are far more indicative of an action-item.",
                "Additionally, order can be important: consider have you versus you have.",
                "Because of this, we posit that n-grams play a larger role in this problem than is typical of problems like topic classification.",
                "Therefore, we consider all n-grams up to size 4.",
                "When using n-grams, if we find an n-gram of size 4 in a segment of text, we can represent the text as just one occurrence of the ngram or as one occurrence of the n-gram and an occurrence of each smaller n-gram contained by it.",
                "We choose the second of these alternatives since this will allow the algorithm itself to smoothly back-off in terms of recall.",
                "Methods such as na¨ıve Bayes may be hurt by such a representation because of double-counting.",
                "Since sentence-ending punctuation can provide information, we retain the terminating punctuation token when it is identifiable.",
                "Additionally, we add a beginning-of-sentence and end-of-sentence token in order to capture patterns that are often indicators at the beginning or end of a sentence.",
                "Assuming proper punctuation, these extra tokens are unnecessary, but often e-mail lacks proper punctuation.",
                "In addition, for the sentence-level classifiers that use ngrams, we additionally code for each sentence a binary encoding of the position of the sentence relative to the document.",
                "This encoding has eight associated features that represent which octile (the first eighth, second eighth, etc.) contains the sentence. 3.2.2 Implementation Details In order to compare the document-level to the sentence-level approach, we compare predictions at the document-level.",
                "We do not address how to use a document-level classifier to make predictions at the sentence-level.",
                "In order to automatically segment the text of the e-mail, we use the RASP statistical parser [4].",
                "Since the automatically segmented sentences may not correspond directly with the phrase-level boundaries, we treat any sentence that contains at least 30% of a marked action-item segment as an action-item.",
                "When evaluating sentencedetection for the sentence-level system, we use these class labels as ground truth.",
                "Since we are not evaluating multiple segmentation approaches, this does not bias any of the methods.",
                "If multiple segmentation systems were under evaluation, one would need to use a metric that matched predicted positive sentences to phrases labeled positive.",
                "The metric would need to punish overly long true predictions as well as too short predictions.",
                "Our criteria for converting to labeled instances implicitly includes both criteria.",
                "Since the segmentation is fixed, an overly long prediction would be predicting yes for many no instances since presumably the extra length corresponds to additional segmented sentences all of which do not contain 30% of action-item.",
                "Likewise, a too short prediction must correspond to a small sentence included in the action-item but not constituting all of the action-item.",
                "Therefore, in order to consider the prediction to be too short, there will be an additional preceding/following sentence that is an action-item where we incorrectly predicted no.",
                "Once a sentence-level classifier has made a prediction for each sentence, we must combine these predictions to make both a document-level prediction and a document-level score.",
                "We use the simple policy of predicting positive when any of the sentences is predicted positive.",
                "In order to produce a document score for ranking, the confidence that the document contains an action-item is: ψ(d) = 1 n(d) s∈d|π(s)=1 ψ(s) if for any s ∈ d, π(s) = 1 1 n(d) maxs∈d ψ(s) o.w. where s is a sentence in document d, π is the classifiers 1/0 prediction, ψ is the score the classifier assigns as its confidence that π(s) = 1, and n(d) is the greater of 1 and the number of (unigram) tokens in the document.",
                "In other words, when any sentence is predicted positive, the document score is the length normalized sum of the sentence scores above threshold.",
                "When no sentence is predicted positive, the document score is the maximum sentence score normalized by length.",
                "As in other text problems, we are more likely to emit false positives for documents with more words or sentences.",
                "Thus we include a length normalization factor. 4.",
                "EXPERIMENTAL ANALYSIS 4.1 The Data Our corpus consists of e-mails obtained from volunteers at an educational institution and cover subjects such as: organizing a research workshop, arranging for job-candidate interviews, publishing proceedings, and talk announcements.",
                "The messages were anonymized by replacing the names of each individual and institution with a pseudonym.1 After attempting to identify and eliminate duplicate e-mails, the corpus contains 744 e-mail messages.",
                "After identity anonymization, the corpora has three basic versions.",
                "Quoted material refers to the text of a previous e-mail that an author often leaves in an e-mail message when responding to the e-mail.",
                "Quoted material can act as noise when learning since it may include action-items from previous messages that are no longer relevant.",
                "To isolate the effects of quoted material, we have three versions of the corpora.",
                "The raw form contains the basic messages.",
                "The auto-stripped version contains the messages after quoted material has been automatically removed.",
                "The hand-stripped version contains the messages after quoted material has been removed by a human.",
                "Additionally, the hand-stripped version has had any xml content and e-mail signatures removed - leaving only the essential content of the message.",
                "The studies reported here are performed with the hand-stripped version.",
                "This allows us to balance the cognitive load in terms of number of tokens that must be read in the user-studies we report - including quoted material would complicate the user studies since some users might skip the material while others read it.",
                "Additionally, ensuring all quoted material is removed 1 We have an even more highly anonymized version of the corpus that can be made available for some outside experimentation.",
                "Please contact the authors for more information on obtaining this data. prevents tainting the cross-validation since otherwise a test item could occur as quoted material in a training document. 4.1.1 Data Labeling Two human annotators labeled each message as to whether or not it contained an action-item.",
                "In addition, they identified each segment of the e-mail which contained an action-item.",
                "A segment is a contiguous section of text selected by the human annotators and may span several sentences or a complete phrase contained in a sentence.",
                "They were instructed that an action item is an explicit request for information that requires the recipients attention or a required action and told to highlight the phrases or sentences that make up the request.",
                "Annotator 1 No Yes Annotator 2 No 391 26 Yes 29 298 Table 1: Agreement of Human Annotators at Document Level Annotator One labeled 324 messages as containing action items.",
                "Annotator Two labeled 327 messages as containing action items.",
                "The agreement of the human annotators is shown in Tables 1 and 2.",
                "The annotators are said to agree at the document-level when both marked the same document as containing no action-items or both marked at least one action-item regardless of whether the text segments were the same.",
                "At the document-level, the annotators agreed 93% of the time.",
                "The kappa statistic [3, 5] is often used to evaluate inter-annotator agreement: κ = A − R 1 − R A is the empirical estimate of the probability of agreement.",
                "R is the empirical estimate of the probability of random agreement given the empirical class priors.",
                "A value close to −1 implies the annotators agree far less often than would be expected randomly, while a value close to 1 means they agree more often than randomly expected.",
                "At the document-level, the kappa statistic for inter-annotator agreement is 0.85.",
                "This value is both strong enough to expect the problem to be learnable and is comparable with results for similar tasks [5, 6].",
                "In order to determine the sentence-level agreement, we use each judgment to create a sentence-corpus with labels as described in Section 3.2.2, then consider the agreement over these sentences.",
                "This allows us to compare agreement over no judgments.",
                "We perform this comparison over the hand-stripped corpus since that eliminates spurious no judgments that would come from including quoted material, etc.",
                "Both annotators were free to label the subject as an action-item, but since neither did, we omit the subject line of the message as well.",
                "This only reduces the number of no agreements.",
                "This leaves 6301 automatically segmented sentences.",
                "At the sentence-level, the annotators agreed 98% of the time, and the kappa statistic for inter-annotator agreement is 0.82.",
                "In order to produce one single set of judgments, the human annotators went through each annotation where there was disagreement and came to a consensus opinion.",
                "The annotators did not collect statistics during this process but anecdotally reported that the majority of disagreements were either cases of clear annotator oversight or different interpretations of conditional statements.",
                "For example, If you would like to keep your job, come to tomorrows meeting implies a required action where If you would like to join Annotator 1 No Yes Annotator 2 No 5810 65 Yes 74 352 Table 2: Agreement of Human Annotators at Sentence Level the football betting pool, come to tomorrows meeting does not.",
                "The first would be an action-item in most contexts while the second would not.",
                "Of course, many conditional statements are not so clearly interpretable.",
                "After reconciling the judgments there are 416 e-mails with no action-items and 328 e-mails containing actionitems.",
                "Of the 328 e-mails containing action-items, 259 messages have one action-item segment; 55 messages have two action-item segments; 11 messages have three action-item segments.",
                "Two messages have four action-item segments, and one message has six action-item segments.",
                "Computing the sentence-level agreement using the reconciled gold standard judgments with each of the annotators individual judgments gives a kappa of 0.89 for Annotator One and a kappa of 0.92 for Annotator Two.",
                "In terms of message characteristics, there were on average 132 content tokens in the body after stripping.",
                "For action-item messages, there were 115.",
                "However, by examining Figure 2 we see the length distributions are nearly identical.",
                "As would be expected for e-mail, it is a long-tailed distribution with about half the messages having more than 60 tokens in the body (this paragraph has 65 tokens). 4.2 Classifiers For this experiment, we have selected a variety of standard text classification algorithms.",
                "In selecting algorithms, we have chosen algorithms that are not only known to work well but which differ along such lines as discriminative vs. generative and lazy vs. eager.",
                "We have done this in order to provide both a competitive and thorough sampling of learning methods for the task at hand.",
                "This is important since it is easy to improve a strawman classifier by introducing a new representation.",
                "By thoroughly sampling alternative classifier choices we demonstrate that representation improvements over bag-of-words are not due to using the information in the bag-of-words poorly. 4.2.1 kNN We employ a standard variant of the k-nearest neighbor algorithm used in text classification, kNN with s-cut score thresholding [19].",
                "We use a tfidf-weighting of the terms with a distanceweighted vote of the neighbors to compute the score before thresholding it.",
                "In order to choose the value of s for thresholding, we perform leave-one-out cross-validation over the training set.",
                "The value of k is set to be 2( log2 N + 1) where N is the number of training points.",
                "This rule for choosing k is theoretically motivated by results which show such a rule converges to the optimal classifier as the number of training points increases [8].",
                "In practice, we have also found it to be a computational convenience that frequently leads to comparable results with numerically optimizing k via a cross-validation procedure. 4.2.2 Na¨ıve Bayes We use a standard multinomial na¨ıve Bayes classifier [16].",
                "In using this classifier, we smoothed word and class probabilities using a Bayesian estimate (with the word prior) and a Laplace m-estimate, respectively. 0 20 40 60 80 100 120 140 160 0 200 400 600 800 1000 1200 1400 NumberofMessages Number of Tokens All Messages Action-Item Messages 0 0.02 0.04 0.06 0.08 0.1 0.12 0.14 0.16 0.18 0.2 0 200 400 600 800 1000 1200 1400 PercentageofMessages Number of Tokens All Messages Action-Item Messages Figure 2: The Histogram (left) and Distribution (right) of Message Length.",
                "A bin size of 20 words was used.",
                "Only tokens in the body after hand-stripping were counted.",
                "After stripping, the majority of words left are usually actual message content.",
                "Classifiers Document Unigram Document Ngram Sentence Unigram Sentence Ngram F1 kNN 0.6670 ± 0.0288 0.7108 ± 0.0699 0.7615 ± 0.0504 0.7790 ± 0.0460 na¨ıve Bayes 0.6572 ± 0.0749 0.6484 ± 0.0513 0.7715 ± 0.0597 0.7777 ± 0.0426 <br>svm</br> 0.6904 ± 0.0347 0.7428 ± 0.0422 0.7282 ± 0.0698 0.7682 ± 0.0451 Voted Perceptron 0.6288 ± 0.0395 0.6774 ± 0.0422 0.6511 ± 0.0506 0.6798 ± 0.0913 Accuracy kNN 0.7029 ± 0.0659 0.7486 ± 0.0505 0.7972 ± 0.0435 0.8092 ± 0.0352 na¨ıve Bayes 0.6074 ± 0.0651 0.5816 ± 0.1075 0.7863 ± 0.0553 0.8145 ± 0.0268 <br>svm</br> 0.7595 ± 0.0309 0.7904 ± 0.0349 0.7958 ± 0.0551 0.8173 ± 0.0258 Voted Perceptron 0.6531 ± 0.0390 0.7164 ± 0.0376 0.6413 ± 0.0833 0.7082 ± 0.1032 Table 3: Average Document-Detection Performance during Cross-Validation for Each Method and the Sample Standard Deviation (Sn−1) in italics.",
                "The best performance for each classifier is shown in bold. 4.2.3 <br>svm</br> We have used a linear <br>svm</br> with a tfidf feature representation and L2-norm as implemented in the SVMlight package v6.01 [11].",
                "All default settings were used. 4.2.4 Voted Perceptron Like the <br>svm</br>, the Voted Perceptron is a kernel-based learning method.",
                "We use the same feature representation and kernel as we have for the <br>svm</br>, a linear kernel with tfidf-weighting and an L2-norm.",
                "The voted perceptron is an online-learning method that keeps a history of past perceptrons used, as well as a weight signifying how often that perceptron was correct.",
                "With each new training example, a correct classification increases the weight on the current perceptron and an incorrect classification updates the perceptron.",
                "The output of the classifier uses the weights on the perceptra to make a final voted classification.",
                "When used in an offline-manner, multiple passes can be made through the training data.",
                "Both the voted perceptron and the <br>svm</br> give a solution from the same hypothesis space - in this case, a linear classifier.",
                "Furthermore, it is well-known that the Voted Perceptron increases the margin of the solution after each pass through the training data [10].",
                "Since Cohen et al. [5] obtain worse results using an <br>svm</br> than a Voted Perceptron with one training iteration, they conclude that the best solution for detecting speech acts may not lie in an area with a large margin.",
                "Because their tasks are highly similar to ours, we employ both classifiers to ensure we are not overlooking a competitive alternative classifier to the <br>svm</br> for the basic bag-of-words representation. 4.3 Performance Measures To compare the performance of the classification methods, we look at two standard performance measures, F1 and accuracy.",
                "The F1 measure [18, 21] is the harmonic mean of precision and recall where Precision = Correct Positives Predicted Positives and Recall = Correct Positives Actual Positives . 4.4 Experimental Methodology We perform standard 10-fold cross-validation on the set of documents.",
                "For the sentence-level approach, all sentences in a document are either entirely in the training set or entirely in the test set for each fold.",
                "For significance tests, we use a two-tailed t-test [21] to compare the values obtained during each cross-validation fold with a p-value of 0.05.",
                "Feature selection was performed using the chi-squared statistic.",
                "Different levels of feature selection were considered for each classifier.",
                "Each of the following number of features was tried: 10, 25, 50, 100, 250, 750, 1000, 2000, 4000.",
                "There are approximately 4700 unigram tokens without feature selection.",
                "In order to choose the number of features to use for each classifier, we perform nested cross-validation and choose the settings that yield the optimal document-level F1 for that classifier.",
                "For this study, only the body of each e-mail message was used.",
                "Feature selection is always applied to all candidate features.",
                "That is, for the n-gram representation, the n-grams and position features are also subject to removal by the feature selection method. 4.5 Results The results for document-level classification are given in Table 3.",
                "The primary hypothesis we are concerned with is that n-grams are critical for this task; if this is true, we expect to see a significant gap in performance between the document-level classifiers that use n-grams (denoted Document Ngram) and those using only unigram features (denoted Document Unigram).",
                "Examining Table 3, we observe that this is indeed the case for every classifier except na¨ıve Bayes.",
                "This difference in performance produced by the n-gram representation is statistically significant for each classifier except for na¨ıve Bayes and the accuracy metric for kNN (see Table 4).",
                "Na¨ıve Bayes poor performance with the n-gram representation is not surprising since the bag-of-n-grams causes excessive doublecounting as mentioned in Section 3.2.1; however, na¨ıve Bayes is not hurt at the sentence-level because the sparse examples provide few chances for agglomerative effects of double counting.",
                "In either case, when a language-modeling approach is desired, modeling the n-grams directly would be preferable to na¨ıve Bayes.",
                "More importantly for the n-gram hypothesis, the n-grams lead to the best document-level classifier performance as well.",
                "As would be expected, the difference between the sentence-level n-gram representation and unigram representation is small.",
                "This is because the window of text is so small that the unigram representation, when done at the sentence-level, implicitly picks up on the power of the n-grams.",
                "Further improvement would signify that the order of the words matter even when only considering a small sentence-size window.",
                "Therefore, the finer-grained sentence-level judgments allows a unigram representation to succeed but only when performed in a small window - behaving as an n-gram representation for all practical purposes.",
                "Document Winner Sentence Winner kNN Ngram Ngram na¨ıve Bayes Unigram Ngram <br>svm</br> Ngram† Ngram Voted Perceptron Ngram† Ngram Table 4: Significance results for n-grams versus unigrams for document detection using document-level and sentence-level classifiers.",
                "When the F1 result is statistically significant, it is shown in bold.",
                "When the accuracy result is significant, it is shown with a † .",
                "F1 Winner Accuracy Winner kNN Sentence Sentence na¨ıve Bayes Sentence Sentence <br>svm</br> Sentence Sentence Voted Perceptron Sentence Document Table 5: Significance results for sentence-level classifiers vs. document-level classifiers for the document detection problem.",
                "When the result is statistically significant, it is shown in bold.",
                "Further highlighting the improvement from finer-grained judgments and n-grams, Figure 3 graphically depicts the edge the <br>svm</br> sentence-level classifier has over the standard bag-of-words approach with a precision-recall curve.",
                "In the high precision area of the graph, the consistent edge of the sentence-level classifier is rather impressive - continuing at precision 1 out to 0.1 recall.",
                "This would mean that a tenth of the users action-items would be placed at the top of their action-item sorted inbox.",
                "Additionally, the large separation at the top right of the curves corresponds to the area where the optimal F1 occurs for each classifier, agreeing with the large improvement from 0.6904 to 0.7682 in F1 score.",
                "Considering the relative unexplored nature of classification at the sentence-level, this gives great hope for further increases in performance.",
                "Accuracy F1 Unigram Ngram Unigram Ngram kNN 0.9519 0.9536 0.6540 0.6686 na¨ıve Bayes 0.9419 0.9550 0.6176 0.6676 <br>svm</br> 0.9559 0.9579 0.6271 0.6672 Voted Perceptron 0.8895 0.9247 0.3744 0.5164 Table 6: Performance of the Sentence-Level Classifiers at Sentence Detection Although Cohen et al. [5] observed that the Voted Perceptron with a single training iteration outperformed <br>svm</br> in a set of similar tasks, we see no such behavior here.",
                "This further strengthens the evidence that an alternate classifier with the bag-of-words representation could not reach the same level of performance.",
                "The Voted Perceptron classifier does improve when the number of training iterations are increased, but it is still lower than the <br>svm</br> classifier.",
                "Sentence detection results are presented in Table 6.",
                "With regard to the sentence detection problem, we note that the F1 measure gives a better feel for the remaining room for improvement in this difficult problem.",
                "That is, unlike document detection where actionitem documents are fairly common, action-item sentences are very rare.",
                "Thus, as in other text problems, the accuracy numbers are deceptively high sheerly because of the default accuracy attainable by always predicting no.",
                "Although, the results here are significantly above-random, it is unclear what level of performance is necessary for sentence detection to be useful in and of itself and not simply as a means to document ranking and classification.",
                "Figure 4: Users find action-items quicker when assisted by a classification system.",
                "Finally, when considering a new type of classification task, one of the most basic questions is whether an accurate classifier built for the task can have an impact on the end-user.",
                "In order to demonstrate the impact this task can have on e-mail users, we conducted a user study using an earlier less-accurate version of the sentence classifier - where instead of using just a single sentence, a threesentence windowed-approach was used.",
                "There were three distinct sets of e-mail in which users had to find action-items.",
                "These sets were either presented in a random order (Unordered), ordered by the classifier (Ordered), or ordered by the classifier and with the 0.4 0.5 0.6 0.7 0.8 0.9 1 0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1 Precision Recall Action-Item Detection <br>svm</br> Performance (Post Model Selection) Document Unigram Sentence Ngram Figure 3: Both n-grams and a small prediction window lead to consistent improvements over the standard approach. center sentence in the highest confidence window highlighted (Order+help).",
                "In order to perform fair comparisons between conditions, the overall number of tokens in each message set should be approximately equal; that is, the cognitive reading load should be approximately the same before the classifiers reordering.",
                "Additionally, users typically show practice effects by improving at the overall task and thus performing better at later message sets.",
                "This is typically handled by varying the ordering of the sets across users so that the means are comparable.",
                "While omitting further detail, we note the sets were balanced for the total number of tokens and a latin square design was used to balance practice effects.",
                "Figure 4 shows that at intervals of 5, 10, and 15 minutes, users consistently found significantly more action-items when assisted by the classifier, but were most critically aided in the first five minutes.",
                "Although, the classifier consistently aids the users, we did not gain an additional end-user impact by highlighting.",
                "As mentioned above, this might be a result of the large room for improvement that still exists for sentence detection, but anecdotal evidence suggests this might also be a result of how the information is presented to the user rather than the accuracy of sentence detection.",
                "For example, highlighting the wrong sentence near an actual action-item hurts the users trust, but if a vague indicator (e.g., an arrow) points to the approximate area the user is not aware of the near-miss.",
                "Since the user studies used a three sentence window, we believe this played a role as well as sentence detection accuracy. 4.6 Discussion In contrast to problems where n-grams have yielded little difference, we believe their power here stems from the fact that many of the meaningful n-grams for action-items consist of common words, e.g., let me know.",
                "Therefore, the document-level unigram approach cannot gain much leverage, even when modeling their joint probability correctly, since these words will often co-occur in the document but not necessarily in a phrase.",
                "Additionally, action-item detection is distinct from many text classification tasks in that a single sentence can change the class label of the document.",
                "As a result, good classifiers cannot rely on aggregating evidence from a large number of weak indicators across the entire document.",
                "Even though we discarded the header information, examining the top-ranked features at the document-level reveals that many of the features are names or parts of e-mail addresses that occurred in the body and are highly associated with e-mails that tend to contain many or no action-items.",
                "A few examples are terms such as org, bob, and gov.",
                "We note that these features will be sensitive to the particular distribution (senders/receivers) and thus the document-level approach may produce classifiers that transfer less readily to alternate contexts and users at different institutions.",
                "This points out that part of the problem of going beyond bag-of-words may be the methodology, and investigating such properties as learning curves and how well a model transfers may highlight differences in models which appear to have similar performance when tested on the distributions they were trained on.",
                "We are currently investigating whether the sentence-level classifiers do perform better over different test corpora without retraining. 5.",
                "FUTURE WORK While applying text classifiers at the document-level is fairly well-understood, there exists the potential for significantly increasing the performance of the sentence-level classifiers.",
                "Such methods include alternate ways of combining the predictions over each sentence, weightings other than tfidf, which may not be appropriate since sentences are small, better sentence segmentation, and other types of phrasal analysis.",
                "Additionally, named entity tagging, time expressions, etc., seem likely candidates for features that can further improve this task.",
                "We are currently pursuing some of these avenues to see what additional gains these offer.",
                "Finally, it would be interesting to investigate the best methods for combining the document-level and sentence-level classifiers.",
                "Since the simple bag-of-words representation at the document-level leads to a learned model that behaves somewhat like a context-specific prior dependent on the sender/receiver and general topic, a first choice would be to treat it as such when combining probability estimates with the sentence-level classifier.",
                "Such a model might serve as a general example for other problems where bag-of-words can establish a baseline model but richer approaches are needed to achieve performance beyond that baseline. 6.",
                "SUMMARY AND CONCLUSIONS The effectiveness of sentence-level detection argues that labeling at the sentence-level provides significant value.",
                "Further experiments are needed to see how this interacts with the amount of training data available.",
                "Sentence detection that is then agglomerated to document-level detection works surprisingly better given low recall than would be expected with sentence-level items.",
                "This, in turn, indicates that improved sentence segmentation methods could yield further improvements in classification.",
                "In this work, we examined how action-items can be effectively detected in e-mails.",
                "Our empirical analysis has demonstrated that n-grams are of key importance to making the most of documentlevel judgments.",
                "When finer-grained judgments are available, then a standard bag-of-words approach using a small (sentence) window size and automatic segmentation techniques can produce results almost as good as the n-gram based approaches.",
                "Acknowledgments This material is based upon work supported by the Defense Advanced Research Projects Agency (DARPA) under Contract No.",
                "NBCHD030010.",
                "Any opinions, findings and conclusions or recommendations expressed in this material are those of the author(s) and do not necessarily reflect the views of the Defense Advanced Research Projects Agency (DARPA), or the Department of InteriorNational Business Center (DOI-NBC).",
                "We would like to extend our sincerest thanks to Jill Lehman whose efforts in data collection were essential in constructing the corpus, and both Jill and Aaron Steinfeld for their direction of the HCI experiments.",
                "We would also like to thank Django Wexler for constructing and supporting the corpus labeling tools and Curtis Huttenhowers support of the text preprocessing package.",
                "Finally, we gratefully acknowledge Scott Fahlman for his encouragement and useful discussions on this topic. 7.",
                "REFERENCES [1] J. Allan, J. Carbonell, G. Doddington, J. Yamron, and Y. Yang.",
                "Topic detection and tracking pilot study: Final report.",
                "In Proceedings of the DARPA Broadcast News Transcription and Understanding Workshop, Washington, D.C., 1998. [2] C. Apte, F. Damerau, and S. M. Weiss.",
                "Automated learning of decision rules for text categorization.",
                "ACM Transactions on Information Systems, 12(3):233-251, July 1994. [3] J. Carletta.",
                "Assessing agreement on classification tasks: The kappa statistic.",
                "Computational Linguistics, 22(2):249-254, 1996. [4] J. Carroll.",
                "High precision extraction of grammatical relations.",
                "In Proceedings of the 19th International Conference on Computational Linguistics (COLING), pages 134-140, 2002. [5] W. W. Cohen, V. R. Carvalho, and T. M. Mitchell.",
                "Learning to classify email into speech acts.",
                "In EMNLP-2004 (Conference on Empirical Methods in Natural Language Processing), pages 309-316, 2004. [6] S. Corston-Oliver, E. Ringger, M. Gamon, and R. Campbell.",
                "Task-focused summarization of email.",
                "In Text Summarization Branches Out: Proceedings of the ACL-04 Workshop, pages 43-50, 2004. [7] A. Culotta, R. Bekkerman, and A. McCallum.",
                "Extracting social networks and contact information from email and the web.",
                "In CEAS-2004 (Conference on Email and Anti-Spam), Mountain View, CA, July 2004. [8] L. Devroye, L. Gy¨orfi, and G. Lugosi.",
                "A Probabilistic Theory of Pattern Recognition.",
                "Springer-Verlag, New York, NY, 1996. [9] S. T. Dumais, J. Platt, D. Heckerman, and M. Sahami.",
                "Inductive learning algorithms and representations for text categorization.",
                "In CIKM 98, Proceedings of the 7th ACM Conference on Information and Knowledge Management, pages 148-155, 1998. [10] Y. Freund and R. Schapire.",
                "Large margin classification using the perceptron algorithm.",
                "Machine Learning, 37(3):277-296, 1999. [11] T. Joachims.",
                "Making large-scale <br>svm</br> learning practical.",
                "In B. Sch¨olkopf, C. J. Burges, and A. J. Smola, editors, Advances in Kernel Methods - Support Vector Learning, pages 41-56.",
                "MIT Press, 1999. [12] L. S. Larkey.",
                "A patent search and classification system.",
                "In Proceedings of the Fourth ACM Conference on Digital Libraries, pages 179 - 187, 1999. [13] D. D. Lewis.",
                "An evaluation of phrasal and clustered representations on a text categorization task.",
                "In SIGIR 92, Proceedings of the 15th Annual International ACM Conference on Research and Development in Information Retrieval, pages 37-50, 1992. [14] Y. Liu, J. Carbonell, and R. Jin.",
                "A pairwise ensemble approach for accurate genre classification.",
                "In Proceedings of the European Conference on Machine Learning (ECML), 2003. [15] Y. Liu, R. Yan, R. Jin, and J. Carbonell.",
                "A comparison study of kernels for multi-label text classification using category association.",
                "In The Twenty-first International Conference on Machine Learning (ICML), 2004. [16] A. McCallum and K. Nigam.",
                "A comparison of event models for naive bayes text classification.",
                "In Working Notes of AAAI 98 (The 15th National Conference on Artificial Intelligence), Workshop on Learning for Text Categorization, pages 41-48, 1998.",
                "TR WS-98-05. [17] F. Sebastiani.",
                "Machine learning in automated text categorization.",
                "ACM Computing Surveys, 34(1):1-47, March 2002. [18] C. J. van Rijsbergen.",
                "Information Retrieval.",
                "Butterworths, London, 1979. [19] Y. Yang.",
                "An evaluation of statistical approaches to text categorization.",
                "Information Retrieval, 1(1/2):67-88, 1999. [20] Y. Yang, J. Carbonell, R. Brown, T. Pierce, B. T. Archibald, and X. Liu.",
                "Learning approaches to topic detection and tracking.",
                "IEEE EXPERT, Special Issue on Applications of Intelligent Information Retrieval, 1999. [21] Y. Yang and X. Liu.",
                "A re-examination of text categorization methods.",
                "In SIGIR 99, Proceedings of the 22nd Annual International ACM Conference on Research and Development in Information Retrieval, pages 42-49, 1999. [22] Y. Yang, J. Zhang, J. Carbonell, and C. Jin.",
                "Topic-conditioned novelty detection.",
                "In Proceedings of the ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, July 2002."
            ],
            "original_annotated_samples": [],
            "translated_annotated_samples": [
                "En cambio, simplemente aplican un \"SVM\" estándar en el nivel de oración y se centran principalmente en un análisis lingüístico de cómo la oración puede reformularse lógicamente antes de agregarla a la lista de tareas.",
                "Clasificadores Documento Documento unigram Ngram oración Unigram oración ngram f1 knn 0.6670 ± 0.0288 0.7108 ± 0.0699 0.7615 ± 0.0504 0.7790 ± 0.0460 na¨ıve bayes 0.6572 ± 0.0749 0.6484 ± 0.05513 0.7715 ± \"SVM\" 0.6904 ± 0.0347 0.7428 ± 0.0422 0.7282 ±0.0698 0.7682 ± 0.0451 Votado Perceptron 0.6288 ± 0.0395 0.6774 ± 0.0422 0.6511 ± 0.0506 0.6798 ± 0.0913 Accuracy KNN 0.7029 ± 0.0659 0.7486 ± 0.0505 0.7972 ± 0.0435 ± bot. Bayes 0.6074 ± 0.0651 0.5816 ± 0.1075 0.7863 ± 0.0553 0.8145 ± 0.0268 \"SVM\"\"0.7595 ± 0.0309 0.7904 ± 0.0349 0.7958 ± 0.0551 0.8173 ± 0.0258 Votado Perceptron 0.6531 ± 0.0390 0.7164 ± 0.0376 0.641313 ± 0.0833 0.7082 ± 0.1032 Tabla 3: promedio de la Tabla 3: promedio Documento de la altura del documento CONTRO-VALIDA CONTERACIÓN CONTROTAL DEL ALTA. iation (sn-1) en cursiva.",
                "El mejor rendimiento para cada clasificador se muestra en negrita.4.2.3 \"SVM\" Hemos usado un \"SVM\" lineal con una representación de características TFIDF y una norma L2 implementada en el paquete SVMLight V6.01 [11].",
                "Se utilizaron todas las configuraciones predeterminadas.4.2.4 Perceptron votado como el \"SVM\", el Perceptron votado es un método de aprendizaje basado en el núcleo.",
                "Utilizamos la misma representación de características y kernel que lo hemos hecho para el \"SVM\", un núcleo lineal con peso TFIDF y una norma L2.",
                "Tanto el perceptrón votado como el \"SVM\" dan una solución del mismo espacio de hipótesis, en este caso, un clasificador lineal.",
                "Desde Cohen et al.[5] Obtenga peores resultados usando un \"SVM\" que un perceptrón votado con una iteración de entrenamiento, concluyen que la mejor solución para detectar actos de habla puede no estar en un área con un gran margen.",
                "Debido a que sus tareas son muy similares a las nuestras, empleamos ambos clasificadores para asegurarnos de que no vaya a pasar por alto un clasificador alternativo competitivo para el \"SVM\" para la representación básica de la bolsa de palabras.4.3 Medidas de rendimiento Para comparar el rendimiento de los métodos de clasificación, observamos dos medidas de rendimiento estándar, F1 y precisión.",
                "Ganador del documento Ganador de la oración Knn Ngram ngram na¨ıve bayes unigram ngram \"svm\" ngram † ngram votado perceptron ngram † ngram Tabla 4: resultados de significación para n-gram versus unigrams para la detección de documentos utilizando clasificadores de nivel de documento y a nivel de oración.",
                "F1 Ganador de precisión Ganador de la oración KNN SENCIÓN Na¨ıve Bayes Sentencia Sentencia \"SVM\" Sentencia de oración votada Documento de oración de percepción Tabla 5: Resultados de significación para clasificadores a nivel de oración frente a clasificadores a nivel de documento para el problema de detección de documentos.",
                "Al resaltar aún más la mejora de los juicios de grano más fino y los N-Grams, la Figura 3 representa gráficamente el borde que el clasificador de nivel de oración \"SVM\" tiene sobre el enfoque estándar de la bolsa de palabras con una curva de recuperación de precisión.",
                "Accuracy F1 Unigram Ngram Unigram Ngram kNN 0.9519 0.9536 0.6540 0.6686 na¨ıve Bayes 0.9419 0.9550 0.6176 0.6676 \"svm\" 0.9559 0.9579 0.6271 0.6672 Voted Perceptron 0.8895 0.9247 0.3744 0.5164 Table 6: Performance of the Sentence-Level Classifiers at Sentence Detection Although Cohen et al.[5] observó que el perceptrón votado con una sola iteración de entrenamiento superó a \"SVM\" en un conjunto de tareas similares, no vemos tal comportamiento aquí.",
                "El clasificador de Perceptron votado mejora cuando aumentan el número de iteraciones de capacitación, pero aún es más bajo que el clasificador \"SVM\".",
                "Estos conjuntos se presentaron en un orden aleatorio (desordenado), ordenados por el clasificador (ordenado) o ordenados por el clasificador y con el 0.4 0.5 0.6 0.7 0.8 0.9 1 0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1 Acción de recuperación de precisión-Tecección de detección de ítem \"SVM\" (Selección posterior al modelo) Documento de oración unigram NGRAM Figura 3: Tanto N-Grams como una pequeña ventana de predicción conducen a mejoras consistentes sobre el enfoque estándar.oración central en la ventana de confianza más alta resaltada (orden+ayuda).",
                "Hacer práctico el aprendizaje de \"SVM\" a gran escala."
            ],
            "translated_text": "",
            "candidates": [
                "SVM",
                "SVM",
                "SVM",
                "SVM",
                "SVM",
                "SVM",
                "SVM",
                "SVM",
                "SVM",
                "SVM",
                "SVM",
                "SVM",
                "SVM",
                "SVM",
                "SVM",
                "SVM",
                "SVM",
                "SVM",
                "SVM",
                "svm",
                "SVM",
                "SVM",
                "SVM",
                "SVM",
                "SVM",
                "svm",
                "SVM",
                "SVM",
                "SVM",
                "SVM",
                "SVM",
                "SVM",
                "SVM"
            ],
            "error": []
        }
    }
}