{
    "id": "H-77",
    "original_text": "Automatic Extraction of Titles from General Documents using Machine Learning Yunhua Hu1 Computer Science Department Xian Jiaotong University No 28, Xianning West Road Xian, China, 710049 yunhuahu@mail.xjtu.edu.cn Hang Li, Yunbo Cao Microsoft Research Asia 5F Sigma Center, No. 49 Zhichun Road, Haidian, Beijing, China, 100080 {hangli,yucao}@microsoft.com Qinghua Zheng Computer Science Department Xian Jiaotong University No 28, Xianning West Road Xian, China, 710049 qhzheng@mail.xjtu.edu.cn Dmitriy Meyerzon Microsoft Corporation One Microsoft Way Redmond, WA, USA, 98052 dmitriym@microsoft.com ABSTRACT In this paper, we propose a machine learning approach to title extraction from general documents. By general documents, we mean documents that can belong to any one of a number of specific genres, including presentations, book chapters, technical papers, brochures, reports, and letters. Previously, methods have been proposed mainly for title extraction from research papers. It has not been clear whether it could be possible to conduct automatic title extraction from general documents. As a case study, we consider extraction from Office including Word and PowerPoint. In our approach, we annotate titles in sample documents (for Word and PowerPoint respectively) and take them as training data, train machine learning models, and perform title extraction using the trained models. Our method is unique in that we mainly utilize formatting information such as font size as features in the models. It turns out that the use of formatting information can lead to quite accurate extraction from general documents. Precision and recall for title extraction from Word is 0.810 and 0.837 respectively, and precision and recall for title extraction from PowerPoint is 0.875 and 0.895 respectively in an experiment on intranet data. Other important new findings in this work include that we can train models in one domain and apply them to another domain, and more surprisingly we can even train models in one language and apply them to another language. Moreover, we can significantly improve search ranking results in document retrieval by using the extracted titles. Categories and Subject Descriptors H.3.3 [Information Storage and Retrieval]: Information Search and Retrieval - Search Process; H.4.1 [Information Systems Applications]: Office Automation - Word processing; D.2.8 [Software Engineering]: Metrics - complexity measures, performance measures General Terms Algorithms, Experimentation, Performance. 1. INTRODUCTION Metadata of documents is useful for many kinds of document processing such as search, browsing, and filtering. Ideally, metadata is defined by the authors of documents and is then used by various systems. However, people seldom define document metadata by themselves, even when they have convenient metadata definition tools [26]. Thus, how to automatically extract metadata from the bodies of documents turns out to be an important research issue. Methods for performing the task have been proposed. However, the focus was mainly on extraction from research papers. For instance, Han et al. [10] proposed a machine learning based method to conduct extraction from research papers. They formalized the problem as that of classification and employed Support Vector Machines as the classifier. They mainly used linguistic features in the model.1 In this paper, we consider metadata extraction from general documents. By general documents, we mean documents that may belong to any one of a number of specific genres. General documents are more widely available in digital libraries, intranets and the internet, and thus investigation on extraction from them is sorely needed. Research papers usually have well-formed styles and noticeable characteristics. In contrast, the styles of general documents can vary greatly. It has not been clarified whether a machine learning based approach can work well for this task. There are many types of metadata: title, author, date of creation, etc. As a case study, we consider title extraction in this paper. General documents can be in many different file formats: Microsoft Office, PDF (PS), etc. As a case study, we consider extraction from Office including Word and PowerPoint. We take a machine learning approach. We annotate titles in sample documents (for Word and PowerPoint respectively) and take them as training data to train several types of models, and perform title extraction using any one type of the trained models. In the models, we mainly utilize formatting information such as font size as features. We employ the following models: Maximum Entropy Model, Perceptron with Uneven Margins, Maximum Entropy Markov Model, and Voted Perceptron. In this paper, we also investigate the following three problems, which did not seem to have been examined previously. (1) Comparison between models: among the models above, which model performs best for title extraction; (2) Generality of model: whether it is possible to train a model on one domain and apply it to another domain, and whether it is possible to train a model in one language and apply it to another language; (3) Usefulness of extracted titles: whether extracted titles can improve document processing such as search. Experimental results indicate that our approach works well for title extraction from general documents. Our method can significantly outperform the baselines: one that always uses the first lines as titles and the other that always uses the lines in the largest font sizes as titles. Precision and recall for title extraction from Word are 0.810 and 0.837 respectively, and precision and recall for title extraction from PowerPoint are 0.875 and 0.895 respectively. It turns out that the use of format features is the key to successful title extraction. (1) We have observed that Perceptron based models perform better in terms of extraction accuracies. (2) We have empirically verified that the models trained with our approach are generic in the sense that they can be trained on one domain and applied to another, and they can be trained in one language and applied to another. (3) We have found that using the extracted titles we can significantly improve precision of document retrieval (by 10%). We conclude that we can indeed conduct reliable title extraction from general documents and use the extracted results to improve real applications. The rest of the paper is organized as follows. In section 2, we introduce related work, and in section 3, we explain the motivation and problem setting of our work. In section 4, we describe our method of title extraction, and in section 5, we describe our method of document retrieval using extracted titles. Section 6 gives our experimental results. We make concluding remarks in section 7. 2. RELATED WORK 2.1 Document Metadata Extraction Methods have been proposed for performing automatic metadata extraction from documents; however, the main focus was on extraction from research papers. The proposed methods fall into two categories: the rule based approach and the machine learning based approach. Giuffrida et al. [9], for instance, developed a rule-based system for automatically extracting metadata from research papers in Postscript. They used rules like titles are usually located on the upper portions of the first pages and they are usually in the largest font sizes. Liddy et al. [14] and Yilmazel el al. [23] performed metadata extraction from educational materials using rule-based natural language processing technologies. Mao et al. [16] also conducted automatic metadata extraction from research papers using rules on formatting information. The rule-based approach can achieve high performance. However, it also has disadvantages. It is less adaptive and robust when compared with the machine learning approach. Han et al. [10], for instance, conducted metadata extraction with the machine learning approach. They viewed the problem as that of classifying the lines in a document into the categories of metadata and proposed using Support Vector Machines as the classifier. They mainly used linguistic information as features. They reported high extraction accuracy from research papers in terms of precision and recall. 2.2 Information Extraction Metadata extraction can be viewed as an application of information extraction, in which given a sequence of instances, we identify a subsequence that represents information in which we are interested. Hidden Markov Model [6], Maximum Entropy Model [1, 4], Maximum Entropy Markov Model [17], Support Vector Machines [3], Conditional Random Field [12], and Voted Perceptron [2] are widely used information extraction models. Information extraction has been applied, for instance, to part-ofspeech tagging [20], named entity recognition [25] and table extraction [19]. 2.3 Search Using Title Information Title information is useful for document retrieval. In the system Citeseer, for instance, Giles et al. managed to extract titles from research papers and make use of the extracted titles in metadata search of papers [8]. In web search, the title fields (i.e., file properties) and anchor texts of web pages (HTML documents) can be viewed as titles of the pages [5]. Many search engines seem to utilize them for web page retrieval [7, 11, 18, 22]. Zhang et al., found that web pages with well-defined metadata are more easily retrieved than those without well-defined metadata [24]. To the best of our knowledge, no research has been conducted on using extracted titles from general documents (e.g., Office documents) for search of the documents. 146 3. MOTIVATION AND PROBLEM SETTING We consider the issue of automatically extracting titles from general documents. By general documents, we mean documents that belong to one of any number of specific genres. The documents can be presentations, books, book chapters, technical papers, brochures, reports, memos, specifications, letters, announcements, or resumes. General documents are more widely available in digital libraries, intranets, and internet, and thus investigation on title extraction from them is sorely needed. Figure 1 shows an estimate on distributions of file formats on intranet and internet [15]. Office and PDF are the main file formats on the intranet. Even on the internet, the documents in the formats are still not negligible, given its extremely large size. In this paper, without loss of generality, we take Office documents as an example. Figure 1. Distributions of file formats in internet and intranet. For Office documents, users can define titles as file properties using a feature provided by Office. We found in an experiment, however, that users seldom use the feature and thus titles in file properties are usually very inaccurate. That is to say, titles in file properties are usually inconsistent with the true titles in the file bodies that are created by the authors and are visible to readers. We collected 6,000 Word and 6,000 PowerPoint documents from an intranet and the internet and examined how many titles in the file properties are correct. We found that surprisingly the accuracy was only 0.265 (cf., Section 6.3 for details). A number of reasons can be considered. For example, if one creates a new file by copying an old file, then the file property of the new file will also be copied from the old file. In another experiment, we found that Google uses the titles in file properties of Office documents in search and browsing, but the titles are not very accurate. We created 50 queries to search Word and PowerPoint documents and examined the top 15 results of each query returned by Google. We found that nearly all the titles presented in the search results were from the file properties of the documents. However, only 0.272 of them were correct. Actually, true titles usually exist at the beginnings of the bodies of documents. If we can accurately extract the titles from the bodies of documents, then we can exploit reliable title information in document processing. This is exactly the problem we address in this paper. More specifically, given a Word document, we are to extract the title from the top region of the first page. Given a PowerPoint document, we are to extract the title from the first slide. A title sometimes consists of a main title and one or two subtitles. We only consider extraction of the main title. As baselines for title extraction, we use that of always using the first lines as titles and that of always using the lines with largest font sizes as titles. Figure 2. Title extraction from Word document. Figure 3. Title extraction from PowerPoint document. Next, we define a specification for human judgments in title data annotation. The annotated data will be used in training and testing of the title extraction methods. Summary of the specification: The title of a document should be identified on the basis of common sense, if there is no difficulty in the identification. However, there are many cases in which the identification is not easy. There are some rules defined in the specification that guide identification for such cases. The rules include a title is usually in consecutive lines in the same format, a document can have no title, titles in images are not considered, a title should not contain words like draft, 147 whitepaper, etc, if it is difficult to determine which is the title, select the one in the largest font size, and if it is still difficult to determine which is the title, select the first candidate. (The specification covers all the cases we have encountered in data annotation.) Figures 2 and 3 show examples of Office documents from which we conduct title extraction. In Figure 2, Differences in Win32 API Implementations among Windows Operating Systems is the title of the Word document. Microsoft Windows on the top of this page is a picture and thus is ignored. In Figure 3, Building Competitive Advantages through an Agile Infrastructure is the title of the PowerPoint document. We have developed a tool for annotation of titles by human annotators. Figure 4 shows a snapshot of the tool. Figure 4. Title annotation tool. 4. TITLE EXTRACTION METHOD 4.1 Outline Title extraction based on machine learning consists of training and extraction. The same pre-processing step occurs before training and extraction. During pre-processing, from the top region of the first page of a Word document or the first slide of a PowerPoint document a number of units for processing are extracted. If a line (lines are separated by return symbols) only has a single format, then the line will become a unit. If a line has several parts and each of them has its own format, then each part will become a unit. Each unit will be treated as an instance in learning. A unit contains not only content information (linguistic information) but also formatting information. The input to pre-processing is a document and the output of pre-processing is a sequence of units (instances). Figure 5 shows the units obtained from the document in Figure 2. Figure 5. Example of units. In learning, the input is sequences of units where each sequence corresponds to a document. We take labeled units (labeled as title_begin, title_end, or other) in the sequences as training data and construct models for identifying whether a unit is title_begin title_end, or other. We employ four types of models: Perceptron, Maximum Entropy (ME), Perceptron Markov Model (PMM), and Maximum Entropy Markov Model (MEMM). In extraction, the input is a sequence of units from one document. We employ one type of model to identify whether a unit is title_begin, title_end, or other. We then extract units from the unit labeled with title_begin to the unit labeled with title_end. The result is the extracted title of the document. The unique characteristic of our approach is that we mainly utilize formatting information for title extraction. Our assumption is that although general documents vary in styles, their formats have certain patterns and we can learn and utilize the patterns for title extraction. This is in contrast to the work by Han et al., in which only linguistic features are used for extraction from research papers. 4.2 Models The four models actually can be considered in the same metadata extraction framework. That is why we apply them together to our current problem. Each input is a sequence of instances kxxx L21 together with a sequence of labels kyyy L21 . ix and iy represents an instance and its label, respectively ( ki ,,2,1 L= ). Recall that an instance here represents a unit. A label represents title_begin, title_end, or other. Here, k is the number of units in a document. In learning, we train a model which can be generally denoted as a conditional probability distribution )|( 11 kk XXYYP LL where iX and iY denote random variables taking instance ix and label iy as values, respectively ( ki ,,2,1 L= ). Learning Tool Extraction Tool 21121 2222122221 1121111211 nknnknn kk kk yyyxxx yyyxxx yyyxxx LL LL LL LL → → → )|(maxarg 11 mkmmkm xxyyP LL )|( 11 kk XXYYP LL Conditional Distribution mkmm xxx L21 Figure 6. Metadata extraction model. We can make assumptions about the general model in order to make it simple enough for training. 148 For example, we can assume that kYY ,,1 L are independent of each other given kXX ,,1 L . Thus, we have )|()|( )|( 11 11 kk kk XYPXYP XXYYP L LL = In this way, we decompose the model into a number of classifiers. We train the classifiers locally using the labeled data. As the classifier, we employ the Perceptron or Maximum Entropy model. We can also assume that the first order Markov property holds for kYY ,,1 L given kXX ,,1 L . Thus, we have )|()|( )|( 111 11 kkk kk XYYPXYP XXYYP −= L LL Again, we obtain a number of classifiers. However, the classifiers are conditioned on the previous label. When we employ the Percepton or Maximum Entropy model as a classifier, the models become a Percepton Markov Model or Maximum Entropy Markov Model, respectively. That is to say, the two models are more precise. In extraction, given a new sequence of instances, we resort to one of the constructed models to assign a sequence of labels to the sequence of instances, i.e., perform extraction. For Perceptron and ME, we assign labels locally and combine the results globally later using heuristics. Specifically, we first identify the most likely title_begin. Then we find the most likely title_end within three units after the title_begin. Finally, we extract as a title the units between the title_begin and the title_end. For PMM and MEMM, we employ the Viterbi algorithm to find the globally optimal label sequence. In this paper, for Perceptron, we actually employ an improved variant of it, called Perceptron with Uneven Margin [13]. This version of Perceptron can work well especially when the number of positive instances and the number of negative instances differ greatly, which is exactly the case in our problem. We also employ an improved version of Perceptron Markov Model in which the Perceptron model is the so-called Voted Perceptron [2]. In addition, in training, the parameters of the model are updated globally rather than locally. 4.3 Features There are two types of features: format features and linguistic features. We mainly use the former. The features are used for both the title-begin and the title-end classifiers. 4.3.1 Format Features Font Size: There are four binary features that represent the normalized font size of the unit (recall that a unit has only one type of font). If the font size of the unit is the largest in the document, then the first feature will be 1, otherwise 0. If the font size is the smallest in the document, then the fourth feature will be 1, otherwise 0. If the font size is above the average font size and not the largest in the document, then the second feature will be 1, otherwise 0. If the font size is below the average font size and not the smallest, the third feature will be 1, otherwise 0. It is necessary to conduct normalization on font sizes. For example, in one document the largest font size might be 12pt, while in another the smallest one might be 18pt. Boldface: This binary feature represents whether or not the current unit is in boldface. Alignment: There are four binary features that respectively represent the location of the current unit: left, center, right, and unknown alignment. The following format features with respect to context play an important role in title extraction. Empty Neighboring Unit: There are two binary features that represent, respectively, whether or not the previous unit and the current unit are blank lines. Font Size Change: There are two binary features that represent, respectively, whether or not the font size of the previous unit and the font size of the next unit differ from that of the current unit. Alignment Change: There are two binary features that represent, respectively, whether or not the alignment of the previous unit and the alignment of the next unit differ from that of the current one. Same Paragraph: There are two binary features that represent, respectively, whether or not the previous unit and the next unit are in the same paragraph as the current unit. 4.3.2 Linguistic Features The linguistic features are based on key words. Positive Word: This binary feature represents whether or not the current unit begins with one of the positive words. The positive words include title:, subject:, subject line: For example, in some documents the lines of titles and authors have the same formats. However, if lines begin with one of the positive words, then it is likely that they are title lines. Negative Word: This binary feature represents whether or not the current unit begins with one of the negative words. The negative words include To, By, created by, updated by, etc. There are more negative words than positive words. The above linguistic features are language dependent. Word Count: A title should not be too long. We heuristically create four intervals: [1, 2], [3, 6], [7, 9] and [9, ∞) and define one feature for each interval. If the number of words in a title falls into an interval, then the corresponding feature will be 1; otherwise 0. Ending Character: This feature represents whether the unit ends with :, -, or other special characters. A title usually does not end with such a character. 5. DOCUMENT RETRIEVAL METHOD We describe our method of document retrieval using extracted titles. Typically, in information retrieval a document is split into a number of fields including body, title, and anchor text. A ranking function in search can use different weights for different fields of 149 the document. Also, titles are typically assigned high weights, indicating that they are important for document retrieval. As explained previously, our experiment has shown that a significant number of documents actually have incorrect titles in the file properties, and thus in addition of using them we use the extracted titles as one more field of the document. By doing this, we attempt to improve the overall precision. In this paper, we employ a modification of BM25 that allows field weighting [21]. As fields, we make use of body, title, extracted title and anchor. First, for each term in the query we count the term frequency in each field of the document; each field frequency is then weighted according to the corresponding weight parameter: ∑= f tfft tfwwtf Similarly, we compute the document length as a weighted sum of lengths of each field. Average document length in the corpus becomes the average of all weighted document lengths. ∑= f ff dlwwdl In our experiments we used 75.0,8.11 == bk . Weight for content was 1.0, title was 10.0, anchor was 10.0, and extracted title was 5.0. 6. EXPERIMENTAL RESULTS 6.1 Data Sets and Evaluation Measures We used two data sets in our experiments. First, we downloaded and randomly selected 5,000 Word documents and 5,000 PowerPoint documents from an intranet of Microsoft. We call it MS hereafter. Second, we downloaded and randomly selected 500 Word and 500 PowerPoint documents from the DotGov and DotCom domains on the internet, respectively. Figure 7 shows the distributions of the genres of the documents. We see that the documents are indeed general documents as we define them. Figure 7. Distributions of document genres. Third, a data set in Chinese was also downloaded from the internet. It includes 500 Word documents and 500 PowerPoint documents in Chinese. We manually labeled the titles of all the documents, on the basis of our specification. Not all the documents in the two data sets have titles. Table 1 shows the percentages of the documents having titles. We see that DotCom and DotGov have more PowerPoint documents with titles than MS. This might be because PowerPoint documents published on the internet are more formal than those on the intranet. Table 1. The portion of documents with titles Domain Type MS DotCom DotGov Word 75.7% 77.8% 75.6% PowerPoint 82.1% 93.4% 96.4% In our experiments, we conducted evaluations on title extraction in terms of precision, recall, and F-measure. The evaluation measures are defined as follows: Precision: P = A / ( A + B ) Recall: R = A / ( A + C ) F-measure: F1 = 2PR / ( P + R ) Here, A, B, C, and D are numbers of documents as those defined in Table 2. Table 2. Contingence table with regard to title extraction Is title Is not title Extracted A B Not extracted C D 6.2 Baselines We test the accuracies of the two baselines described in section 4.2. They are denoted as largest font size and first line respectively. 6.3 Accuracy of Titles in File Properties We investigate how many titles in the file properties of the documents are reliable. We view the titles annotated by humans as true titles and test how many titles in the file properties can approximately match with the true titles. We use Edit Distance to conduct the approximate match. (Approximate match is only used in this evaluation). This is because sometimes human annotated titles can be slightly different from the titles in file properties on the surface, e.g., contain extra spaces). Given string A and string B: if ( (D == 0) or ( D / ( La + Lb ) < θ ) ) then string A = string B D: Edit Distance between string A and string B La: length of string A Lb: length of string B θ: 0.1 ∑ × ++− + = t t n N wtf avwdl wdl bbk kwtf FBM )log( ))1(( )1( 25 1 1 150 Table 3. Accuracies of titles in file properties File Type Domain Precision Recall F1 MS 0.299 0.311 0.305 DotCom 0.210 0.214 0.212Word DotGov 0.182 0.177 0.180 MS 0.229 0.245 0.237 DotCom 0.185 0.186 0.186PowerPoint DotGov 0.180 0.182 0.181 6.4 Comparison with Baselines We conducted title extraction from the first data set (Word and PowerPoint in MS). As the model, we used Perceptron. We conduct 4-fold cross validation. Thus, all the results reported here are those averaged over 4 trials. Tables 4 and 5 show the results. We see that Perceptron significantly outperforms the baselines. In the evaluation, we use exact matching between the true titles annotated by humans and the extracted titles. Table 4. Accuracies of title extraction with Word Precision Recall F1 Model Perceptron 0.810 0.837 0.823 Largest font size 0.700 0.758 0.727 Baselines First line 0.707 0.767 0.736 Table 5. Accuracies of title extraction with PowerPoint Precision Recall F1 Model Perceptron 0.875 0. 895 0.885 Largest font size 0.844 0.887 0.865 Baselines First line 0.639 0.671 0.655 We see that the machine learning approach can achieve good performance in title extraction. For Word documents both precision and recall of the approach are 8 percent higher than those of the baselines. For PowerPoint both precision and recall of the approach are 2 percent higher than those of the baselines. We conduct significance tests. The results are shown in Table 6. Here, Largest denotes the baseline of using the largest font size, First denotes the baseline of using the first line. The results indicate that the improvements of machine learning over baselines are statistically significant (in the sense p-value < 0.05) Table 6. Sign test results Documents Type Sign test between p-value Perceptron vs. Largest 3.59e-26 Word Perceptron vs. First 7.12e-10 Perceptron vs. Largest 0.010 PowerPoint Perceptron vs. First 5.13e-40 We see, from the results, that the two baselines can work well for title extraction, suggesting that font size and position information are most useful features for title extraction. However, it is also obvious that using only these two features is not enough. There are cases in which all the lines have the same font size (i.e., the largest font size), or cases in which the lines with the largest font size only contain general descriptions like Confidential, White paper, etc. For those cases, the largest font size method cannot work well. For similar reasons, the first line method alone cannot work well, either. With the combination of different features (evidence in title judgment), Perceptron can outperform Largest and First. We investigate the performance of solely using linguistic features. We found that it does not work well. It seems that the format features play important roles and the linguistic features are supplements.. Figure 8. An example Word document. Figure 9. An example PowerPoint document. We conducted an error analysis on the results of Perceptron. We found that the errors fell into three categories. (1) About one third of the errors were related to hard cases. In these documents, the layouts of the first pages were difficult to understand, even for humans. Figure 8 and 9 shows examples. (2) Nearly one fourth of the errors were from the documents which do not have true titles but only contain bullets. Since we conduct extraction from the top regions, it is difficult to get rid of these errors with the current approach. (3). Confusions between main titles and subtitles were another type of error. Since we only labeled the main titles as titles, the extractions of both titles were considered incorrect. This type of error does little harm to document processing like search, however. 6.5 Comparison between Models To compare the performance of different machine learning models, we conducted another experiment. Again, we perform 4-fold cross 151 validation on the first data set (MS). Table 7, 8 shows the results of all the four models. It turns out that Perceptron and PMM perform the best, followed by MEMM, and ME performs the worst. In general, the Markovian models perform better than or as well as their classifier counterparts. This seems to be because the Markovian models are trained globally, while the classifiers are trained locally. The Perceptron based models perform better than the ME based counterparts. This seems to be because the Perceptron based models are created to make better classifications, while ME models are constructed for better prediction. Table 7. Comparison between different learning models for title extraction with Word Model Precision Recall F1 Perceptron 0.810 0.837 0.823 MEMM 0.797 0.824 0.810 PMM 0.827 0.823 0.825 ME 0.801 0.621 0.699 Table 8. Comparison between different learning models for title extraction with PowerPoint Model Precision Recall F1 Perceptron 0.875 0. 895 0. 885 MEMM 0.841 0.861 0.851 PMM 0.873 0.896 0.885 ME 0.753 0.766 0.759 6.6 Domain Adaptation We apply the model trained with the first data set (MS) to the second data set (DotCom and DotGov). Tables 9-12 show the results. Table 9. Accuracies of title extraction with Word in DotGov Precision Recall F1 Model Perceptron 0.716 0.759 0.737 Largest font size 0.549 0.619 0.582Baselines First line 0.462 0.521 0.490 Table 10. Accuracies of title extraction with PowerPoint in DotGov Precision Recall F1 Model Perceptron 0.900 0.906 0.903 Largest font size 0.871 0.888 0.879Baselines First line 0.554 0.564 0.559 Table 11. Accuracies of title extraction with Word in DotCom Precisio n Recall F1 Model Perceptron 0.832 0.880 0.855 Largest font size 0.676 0.753 0.712Baselines First line 0.577 0.643 0.608 Table 12. Performance of PowerPoint document title extraction in DotCom Precisio n Recall F1 Model Perceptron 0.910 0.903 0.907 Largest font size 0.864 0.886 0.875Baselines First line 0.570 0.585 0.577 From the results, we see that the models can be adapted to different domains well. There is almost no drop in accuracy. The results indicate that the patterns of title formats exist across different domains, and it is possible to construct a domain independent model by mainly using formatting information. 6.7 Language Adaptation We apply the model trained with the data in English (MS) to the data set in Chinese. Tables 13-14 show the results. Table 13. Accuracies of title extraction with Word in Chinese Precision Recall F1 Model Perceptron 0.817 0.805 0.811 Largest font size 0.722 0.755 0.738Baselines First line 0.743 0.777 0.760 Table 14. Accuracies of title extraction with PowerPoint in Chinese Precision Recall F1 Model Perceptron 0.766 0.812 0.789 Largest font size 0.753 0.813 0.782Baselines First line 0.627 0.676 0.650 We see that the models can be adapted to a different language. There are only small drops in accuracy. Obviously, the linguistic features do not work for Chinese, but the effect of not using them is negligible. The results indicate that the patterns of title formats exist across different languages. From the domain adaptation and language adaptation results, we conclude that the use of formatting information is the key to a successful extraction from general documents. 6.8 Search with Extracted Titles We performed experiments on using title extraction for document retrieval. As a baseline, we employed BM25 without using extracted titles. The ranking mechanism was as described in Section 5. The weights were heuristically set. We did not conduct optimization on the weights. The evaluation was conducted on a corpus of 1.3 M documents crawled from the intranet of Microsoft using 100 evaluation queries obtained from this intranets search engine query logs. 50 queries were from the most popular set, while 50 queries other were chosen randomly. Users were asked to provide judgments of the degree of document relevance from a scale of 1to 5 (1 meaning detrimental, 2 - bad, 3 - fair, 4 - good and 5 - excellent). 152 Figure 10 shows the results. In the chart two sets of precision results were obtained by either considering good or excellent documents as relevant (left 3 bars with relevance threshold 0.5), or by considering only excellent documents as relevant (right 3 bars with relevance threshold 1.0) 0 0.05 0.1 0.15 0.2 0.25 0.3 0.35 0.4 0.45 P@10 P@5 Reciprocal P@10 P@5 Reciprocal 0.5 1 BM25 Anchor, Title, Body BM25 Anchor, Title, Body, ExtractedTitle Name All RelevanceThreshold Data Description Figure 10. Search ranking results. Figure 10 shows different document retrieval results with different ranking functions in terms of precision @10, precision @5 and reciprocal rank: • Blue bar - BM25 including the fields body, title (file property), and anchor text. • Purple bar - BM25 including the fields body, title (file property), anchor text, and extracted title. With the additional field of extracted title included in BM25 the precision @10 increased from 0.132 to 0.145, or by ~10%. Thus, it is safe to say that the use of extracted title can indeed improve the precision of document retrieval. 7. CONCLUSION In this paper, we have investigated the problem of automatically extracting titles from general documents. We have tried using a machine learning approach to address the problem. Previous work showed that the machine learning approach can work well for metadata extraction from research papers. In this paper, we showed that the approach can work for extraction from general documents as well. Our experimental results indicated that the machine learning approach can work significantly better than the baselines in title extraction from Office documents. Previous work on metadata extraction mainly used linguistic features in documents, while we mainly used formatting information. It appeared that using formatting information is a key for successfully conducting title extraction from general documents. We tried different machine learning models including Perceptron, Maximum Entropy, Maximum Entropy Markov Model, and Voted Perceptron. We found that the performance of the Perceptorn models was the best. We applied models constructed in one domain to another domain and applied models trained in one language to another language. We found that the accuracies did not drop substantially across different domains and across different languages, indicating that the models were generic. We also attempted to use the extracted titles in document retrieval. We observed a significant improvement in document ranking performance for search when using extracted title information. All the above investigations were not conducted in previous work, and through our investigations we verified the generality and the significance of the title extraction approach. 8. ACKNOWLEDGEMENTS We thank Chunyu Wei and Bojuan Zhao for their work on data annotation. We acknowledge Jinzhu Li for his assistance in conducting the experiments. We thank Ming Zhou, John Chen, Jun Xu, and the anonymous reviewers of JCDL05 for their valuable comments on this paper. 9. REFERENCES [1] Berger, A. L., Della Pietra, S. A., and Della Pietra, V. J. A maximum entropy approach to natural language processing. Computational Linguistics, 22:39-71, 1996. [2] Collins, M. Discriminative training methods for hidden markov models: theory and experiments with perceptron algorithms. In Proceedings of Conference on Empirical Methods in Natural Language Processing, 1-8, 2002. [3] Cortes, C. and Vapnik, V. Support-vector networks. Machine Learning, 20:273-297, 1995. [4] Chieu, H. L. and Ng, H. T. A maximum entropy approach to information extraction from semi-structured and free text. In Proceedings of the Eighteenth National Conference on Artificial Intelligence, 768-791, 2002. [5] Evans, D. K., Klavans, J. L., and McKeown, K. R. Columbia newsblaster: multilingual news summarization on the Web. In Proceedings of Human Language Technology conference / North American chapter of the Association for Computational Linguistics annual meeting, 1-4, 2004. [6] Ghahramani, Z. and Jordan, M. I. Factorial hidden markov models. Machine Learning, 29:245-273, 1997. [7] Gheel, J. and Anderson, T. Data and metadata for finding and reminding, In Proceedings of the 1999 International Conference on Information Visualization, 446-451,1999. [8] Giles, C. L., Petinot, Y., Teregowda P. B., Han, H., Lawrence, S., Rangaswamy, A., and Pal, N. eBizSearch: a niche search engine for e-Business. In Proceedings of the 26th Annual International ACM SIGIR Conference on Research and Development in Information Retrieval, 413414, 2003. [9] Giuffrida, G., Shek, E. C., and Yang, J. Knowledge-based metadata extraction from PostScript files. In Proceedings of the Fifth ACM Conference on Digital Libraries, 77-84, 2000. [10] Han, H., Giles, C. L., Manavoglu, E., Zha, H., Zhang, Z., and Fox, E. A. Automatic document metadata extraction using support vector machines. In Proceedings of the Third ACM/IEEE-CS Joint Conference on Digital Libraries, 37-48, 2003. [11] Kobayashi, M., and Takeda, K. Information retrieval on the Web. ACM Computing Surveys, 32:144-173, 2000. [12] Lafferty, J., McCallum, A., and Pereira, F. Conditional random fields: probabilistic models for segmenting and 153 labeling sequence data. In Proceedings of the Eighteenth International Conference on Machine Learning, 282-289, 2001. [13] Li, Y., Zaragoza, H., Herbrich, R., Shawe-Taylor J., and Kandola, J. S. The perceptron algorithm with uneven margins. In Proceedings of the Nineteenth International Conference on Machine Learning, 379-386, 2002. [14] Liddy, E. D., Sutton, S., Allen, E., Harwell, S., Corieri, S., Yilmazel, O., Ozgencil, N. E., Diekema, A., McCracken, N., and Silverstein, J. Automatic Metadata generation & evaluation. In Proceedings of the 25th Annual International ACM SIGIR Conference on Research and Development in Information Retrieval, 401-402, 2002. [15] Littlefield, A. Effective enterprise information retrieval across new content formats. In Proceedings of the Seventh Search Engine Conference, http://www.infonortics.com/searchengines/sh02/02prog.html, 2002. [16] Mao, S., Kim, J. W., and Thoma, G. R. A dynamic feature generation system for automated metadata extraction in preservation of digital materials. In Proceedings of the First International Workshop on Document Image Analysis for Libraries, 225-232, 2004. [17] McCallum, A., Freitag, D., and Pereira, F. Maximum entropy markov models for information extraction and segmentation. In Proceedings of the Seventeenth International Conference on Machine Learning, 591-598, 2000. [18] Murphy, L. D. Digital document metadata in organizations: roles, analytical approaches, and future research directions. In Proceedings of the Thirty-First Annual Hawaii International Conference on System Sciences, 267-276, 1998. [19] Pinto, D., McCallum, A., Wei, X., and Croft, W. B. Table extraction using conditional random fields. In Proceedings of the 26th Annual International ACM SIGIR Conference on Research and Development in Information Retrieval, 235242, 2003. [20] Ratnaparkhi, A. Unsupervised statistical models for prepositional phrase attachment. In Proceedings of the Seventeenth International Conference on Computational Linguistics. 1079-1085, 1998. [21] Robertson, S., Zaragoza, H., and Taylor, M. Simple BM25 extension to multiple weighted fields, In Proceedings of ACM Thirteenth Conference on Information and Knowledge Management, 42-49, 2004. [22] Yi, J. and Sundaresan, N. Metadata based Web mining for relevance, In Proceedings of the 2000 International Symposium on Database Engineering & Applications, 113121, 2000. [23] Yilmazel, O., Finneran, C. M., and Liddy, E. D. MetaExtract: An NLP system to automatically assign metadata. In Proceedings of the 2004 Joint ACM/IEEE Conference on Digital Libraries, 241-242, 2004. [24] Zhang, J. and Dimitroff, A. Internet search engines response to metadata Dublin Core implementation. Journal of Information Science, 30:310-320, 2004. [25] Zhang, L., Pan, Y., and Zhang, T. Recognising and using named entities: focused named entity recognition using machine learning. In Proceedings of the 27th Annual International ACM SIGIR Conference on Research and Development in Information Retrieval, 281-288, 2004. [26] http://dublincore.org/groups/corporate/Seattle/ 154",
    "original_translation": "Extracción automática de títulos de documentos generales utilizando el aprendizaje automático Yunhua Hu1 Departamento de Ciencias de la Computación Xian Jiaotong Universidad No 28, Xianning West Road Xian, China, 710049 Yunhuahu@mail.xjtu.edu.cn Hang Li, Yunbo Cao Microsoft Research Asia 5f Sigma Center,,No. 49 Zhichun Road, Haidian, Beijing, China, 100080 {Hangli, Yucao}@Microsoft.com Qinghua de Departamento de Ciencias de la Computación Xian Jiaotong No.Microsoft Corporation One Microsoft Way Redmond, WA, EE. UU., 98052 dmitriym@microsoft.com Resumen En este documento, proponemos un enfoque de aprendizaje automático para la extracción de títulos de documentos generales. Por documentos generales, nos referimos a documentos que pueden pertenecer a cualquiera de varios géneros específicos, incluidas presentaciones, capítulos de libros, documentos técnicos, folletos, informes y cartas. Anteriormente, los métodos se han propuesto principalmente para la extracción del título de los trabajos de investigación. No ha estado claro si podría ser posible realizar una extracción automática de títulos de documentos generales. Como estudio de caso, consideramos la extracción de la oficina, incluidas Word y PowerPoint. En nuestro enfoque, anotamos títulos en documentos de muestra (para Word y PowerPoint, respectivamente) y los tomamos como datos de entrenamiento, entrenan modelos de aprendizaje automático y realizamos la extracción de títulos utilizando los modelos capacitados. Nuestro método es único en el sentido de que utilizamos principalmente información de formato como el tamaño de la fuente como características en los modelos. Resulta que el uso de información de formato puede conducir a una extracción bastante precisa de los documentos generales. La precisión y el recuerdo para la extracción del título de Word es 0.810 y 0.837 respectivamente, y la precisión y el recuerdo para la extracción del título de PowerPoint es 0.875 y 0.895 respectivamente en un experimento sobre datos de intranet. Otros nuevos hallazgos importantes en este trabajo incluyen que podemos entrenar modelos en un dominio y aplicarlos a otro dominio, y más sorprendentemente podemos incluso capacitar a los modelos en un idioma y aplicarlos a otro idioma. Además, podemos mejorar significativamente los resultados de clasificación de búsqueda en la recuperación de documentos mediante el uso de los títulos extraídos. Categorías y descriptores de sujetos H.3.3 [Almacenamiento y recuperación de información]: Búsqueda y recuperación de información - Proceso de búsqueda;H.4.1 [Aplicaciones de sistemas de información]: Automatización de Office - Procesamiento de textos;D.2.8 [Ingeniería de software]: Métricas: medidas de complejidad, algoritmos de términos generales de medidas de rendimiento, experimentación, rendimiento.1. Los metadatos de introducción de documentos son útiles para muchos tipos de procesamiento de documentos, como búsqueda, navegación y filtrado. Idealmente, los metadatos son definidos por los autores de los documentos y luego los utilizan varios sistemas. Sin embargo, las personas rara vez definen metadatos de documentos por sí mismas, incluso cuando tienen herramientas de definición de metadatos convenientes [26]. Por lo tanto, cómo extraer automáticamente los metadatos de los cuerpos de los documentos resulta ser un problema de investigación importante. Se han propuesto métodos para realizar la tarea. Sin embargo, el enfoque se centró principalmente en la extracción de trabajos de investigación. Por ejemplo, Han et al.[10] propuso un método basado en el aprendizaje automático para realizar la extracción de los trabajos de investigación. Formalizaron el problema como el de la clasificación y el empleado de las máquinas de vectores de soporte como clasificador. Utilizaron principalmente características lingüísticas en el modelo.1 En este documento, consideramos la extracción de metadatos de los documentos generales. Por documentos generales, nos referimos a documentos que pueden pertenecer a cualquiera de varios géneros específicos. Los documentos generales están más ampliamente disponibles en bibliotecas digitales, intranets e Internet, y por lo tanto, se necesita una investigación sobre la extracción de ellos. Los trabajos de investigación generalmente tienen estilos bien formados y características notables. En contraste, los estilos de documentos generales pueden variar mucho. No se ha aclarado si un enfoque basado en el aprendizaje automático puede funcionar bien para esta tarea. Hay muchos tipos de metadatos: título, autor, fecha de creación, etc. Como estudio de caso, consideramos la extracción del título en este documento. Los documentos generales pueden estar en muchos formatos de archivo diferentes: Microsoft Office, PDF (PS), etc. Como estudio de caso, consideramos la extracción de la oficina, incluidas Word y PowerPoint. Tomamos un enfoque de aprendizaje automático. Anotamos títulos en documentos de muestra (para Word y PowerPoint respectivamente) y los tomamos como datos de entrenamiento para capacitar a varios tipos de modelos, y realizamos la extracción de títulos utilizando cualquier tipo de modelos capacitados. En los modelos, utilizamos principalmente información de formato como el tamaño de la fuente como características. Empleamos los siguientes modelos: modelo de entropía máxima, perceptrones con márgenes desiguales, modelo de entropía máxima Markov y perceptrón votado. En este documento, también investigamos los siguientes tres problemas, que no parecían haber sido examinados anteriormente.(1) Comparación entre modelos: entre los modelos anteriores, qué modelo funciona mejor para la extracción de título;(2) Generalidad del modelo: si es posible entrenar un modelo en un dominio y aplicarlo a otro dominio, y si es posible entrenar un modelo en un idioma y aplicarlo a otro idioma;(3) Utilidad de los títulos extraídos: si los títulos extraídos pueden mejorar el procesamiento de documentos, como la búsqueda. Los resultados experimentales indican que nuestro enfoque funciona bien para la extracción del título de documentos generales. Nuestro método puede superar significativamente las líneas de base: una que siempre usa las primeras líneas como títulos y el otro que siempre usa las líneas en los tamaños de fuentes más grandes como títulos. La precisión y el recuerdo para la extracción del título de Word son 0.810 y 0.837 respectivamente, y la precisión y el recuerdo para la extracción del título de PowerPoint son 0.875 y 0.895 respectivamente. Resulta que el uso de características de formato es la clave para una extracción de título exitosa.(1) Hemos observado que los modelos basados en Perceptron funcionan mejor en términos de precisiones de extracción.(2) Hemos verificado empíricamente que los modelos entrenados con nuestro enfoque son genéricos en el sentido de que pueden ser entrenados en un dominio y aplicados a otro, y pueden ser entrenados en un idioma y aplicarse a otro.(3) Hemos encontrado que utilizando los títulos extraídos podemos mejorar significativamente la precisión de la recuperación de documentos (en un 10%). Concluimos que de hecho podemos realizar una extracción de título confiable de documentos generales y usar los resultados extraídos para mejorar las aplicaciones reales. El resto del documento está organizado de la siguiente manera. En la Sección 2, presentamos un trabajo relacionado, y en la Sección 3, explicamos la motivación y el establecimiento de problemas de nuestro trabajo. En la Sección 4, describimos nuestro método de extracción de título, y en la Sección 5, describimos nuestro método de recuperación de documentos utilizando títulos extraídos. La Sección 6 da nuestros resultados experimentales. Hacemos comentarios finales en la Sección 7. 2. Trabajo relacionado 2.1 Se han propuesto métodos de extracción de metadatos del documento para realizar la extracción automática de metadatos de los documentos;Sin embargo, el enfoque principal fue la extracción de los trabajos de investigación. Los métodos propuestos se dividen en dos categorías: el enfoque basado en reglas y el enfoque basado en el aprendizaje automático. Giuffrida et al.[9], por ejemplo, desarrolló un sistema basado en reglas para extraer automáticamente metadatos de trabajos de investigación en PostScript. Usaron reglas como títulos generalmente ubicados en las partes superiores de las primeras páginas y generalmente se encuentran en los tamaños de fuentes más grandes. Liddy et al.[14] y Yilmazel El Al.[23] realizó la extracción de metadatos de materiales educativos utilizando tecnologías de procesamiento del lenguaje natural basadas en reglas. Mao et al.[16] también realizó extracción de metadatos automáticos de trabajos de investigación utilizando reglas sobre el formato de información. El enfoque basado en reglas puede lograr un alto rendimiento. Sin embargo, también tiene desventajas. Es menos adaptativo y robusto en comparación con el enfoque de aprendizaje automático. Han et al.[10], por ejemplo, realizó extracción de metadatos con el enfoque de aprendizaje automático. Vieron el problema como el de clasificar las líneas en un documento en las categorías de metadatos y propusieron usar máquinas de vectores de soporte como clasificador. Principalmente utilizaron información lingüística como características. Informaron una alta precisión de extracción de trabajos de investigación en términos de precisión y retiro.2.2 La extracción de metadatos de extracción de información se puede ver como una aplicación de extracción de información, en la que dada una secuencia de instancias, identificamos una subsecuencia que representa información en la que estamos interesados. Modelo oculto de Markov [6], modelo de entropía máxima [1, 4], modelo de entropía máxima [17], máquinas de vectores de soporte [3], campo aleatorio condicional [12] y perceptrón votado [2] son modelos de extracción de información ampliamente utilizados. La extracción de información se ha aplicado, por ejemplo, al etiquetado de parto de expresión [20], nombrado reconocimiento de entidad [25] y extracción de tabla [19].2.3 Búsqueda utilizando la información del título La información del título es útil para la recuperación de documentos. En el sistema CitaSeer, por ejemplo, Giles et al.Se las arregló para extraer títulos de los trabajos de investigación y hacer uso de los títulos extraídos en la búsqueda de documentos de metadatos [8]. En la búsqueda web, los campos de título (es decir, las propiedades del archivo) y los textos de anclaje de las páginas web (documentos HTML) pueden verse como títulos de las páginas [5]. Muchos motores de búsqueda parecen utilizarlos para la recuperación de la página web [7, 11, 18, 22]. Zhang et al., Descubrieron que las páginas web con metadatos bien definidos se recuperan más fácilmente que aquellas sin metadatos bien definidos [24]. Hasta donde sabemos, no se ha realizado investigaciones sobre el uso de títulos extraídos de documentos generales (por ejemplo, documentos de oficina) para la búsqueda de los documentos.146 3. Motivación y configuración de problemas Consideramos el problema de extraer automáticamente títulos de documentos generales. Por documentos generales, nos referimos a documentos que pertenecen a uno de cualquier número de géneros específicos. Los documentos pueden ser presentaciones, libros, capítulos de libros, documentos técnicos, folletos, informes, notas, especificaciones, cartas, anuncios o currículums. Los documentos generales están más ampliamente disponibles en bibliotecas digitales, intranets e Internet, y por lo tanto, es muy necesaria la investigación sobre la extracción del título de ellos. La Figura 1 muestra una estimación de las distribuciones de formatos de archivo en Intranet e Internet [15]. Office y PDF son los formatos de archivo principales en la intranet. Incluso en Internet, los documentos en los formatos aún no son insignificantes, dado su tamaño extremadamente grande. En este documento, sin pérdida de generalidad, tomamos documentos de la oficina como ejemplo. Figura 1. Distribuciones de formatos de archivo en Internet e Intranet. Para los documentos de oficina, los usuarios pueden definir títulos como propiedades del archivo utilizando una característica proporcionada por la oficina. Sin embargo, en un experimento, encontramos que los usuarios rara vez usan la función y, por lo tanto, los títulos en las propiedades del archivo suelen ser muy inexactos. Es decir, los títulos en las propiedades del archivo generalmente son inconsistentes con los títulos verdaderos en los cuerpos de archivo creados por los autores y son visibles para los lectores. Recopilamos 6,000 palabras y 6,000 documentos de PowerPoint de una intranet e Internet y examinamos cuántos títulos en las propiedades del archivo son correctos. Descubrimos que sorprendentemente la precisión era solo 0.265 (cf., sección 6.3 para más detalles). Se pueden considerar varias razones. Por ejemplo, si uno crea un nuevo archivo copiando un archivo anterior, entonces la propiedad del archivo del nuevo archivo también se copiará del archivo anterior. En otro experimento, encontramos que Google usa los títulos en las propiedades del archivo de los documentos de oficina en la búsqueda y la navegación, pero los títulos no son muy precisos. Creamos 50 consultas para buscar documentos de Word y PowerPoint y examinamos los 15 resultados principales de cada consulta devuelta por Google. Descubrimos que casi todos los títulos presentados en los resultados de búsqueda eran de las propiedades del archivo de los documentos. Sin embargo, solo 0.272 de ellos eran correctos. En realidad, los verdaderos títulos generalmente existen al comienzo de los cuerpos de los documentos. Si podemos extraer con precisión los títulos de los cuerpos de los documentos, entonces podemos explotar la información de título confiable en el procesamiento de documentos. Este es exactamente el problema que abordamos en este documento. Más específicamente, dado un documento de Word, debemos extraer el título de la región superior de la primera página. Dado un documento de PowerPoint, debemos extraer el título de la primera diapositiva. Un título a veces consiste en un título principal y uno o dos subtítulos. Solo consideramos la extracción del título principal. Como líneas de base para la extracción del título, lo usamos de siempre usar las primeras líneas como títulos y el de usar siempre las líneas con tamaños de fuentes más grandes como títulos. Figura 2. Extracción de título del documento de Word. Figura 3. Extracción de título del documento PowerPoint. A continuación, definimos una especificación para los juicios humanos en la anotación de datos de título. Los datos anotados se utilizarán en la capacitación y prueba de los métodos de extracción de título. Resumen de la especificación: el título de un documento debe identificarse sobre la base del sentido común, si no hay dificultad en la identificación. Sin embargo, hay muchos casos en los que la identificación no es fácil. Hay algunas reglas definidas en la especificación que guían la identificación de tales casos. Las reglas incluyen un título generalmente en líneas consecutivas en el mismo formato, un documento no puede tener título, no se consideran los títulos en las imágenes, un título no debe contener palabras como borrador, 147 documentos técnicos, etc., si es difícil determinar cuáles el título, seleccione el del tamaño de fuente más grande, y si aún es difícil determinar cuál es el título, seleccione el primer candidato.(La especificación cubre todos los casos que hemos encontrado en la anotación de datos). Las Figuras 2 y 3 muestran ejemplos de documentos de oficina de los cuales realizamos extracción de título. En la Figura 2, las diferencias en las implementaciones de la API WIN32 entre los sistemas operativos de Windows es el título del documento Word. Microsoft Windows en la parte superior de esta página es una imagen y, por lo tanto, se ignora. En la Figura 3, construir ventajas competitivas a través de una infraestructura ágil es el título del documento de PowerPoint. Hemos desarrollado una herramienta para la anotación de títulos de anotadores humanos. La Figura 4 muestra una instantánea de la herramienta. Figura 4. Herramienta de anotación de título.4. Método de extracción de título 4.1 Esquema la extracción de títulos basada en el aprendizaje automático consiste en capacitación y extracción. El mismo paso de preprocesamiento ocurre antes del entrenamiento y la extracción. Durante el preprocesamiento, desde la región superior de la primera página de un documento de Word o la primera diapositiva de un documento de PowerPoint se extraen varias unidades para el procesamiento. Si una línea (las líneas están separadas por símbolos de retorno) solo tiene un solo formato, entonces la línea se convertirá en una unidad. Si una línea tiene varias partes y cada una de ellas tiene su propio formato, entonces cada parte se convertirá en una unidad. Cada unidad será tratada como una instancia en el aprendizaje. Una unidad contiene no solo información de contenido (información lingüística) sino también formateo de información. La entrada al preprocesamiento es un documento y la salida del preprocesamiento es una secuencia de unidades (instancias). La Figura 5 muestra las unidades obtenidas del documento en la Figura 2. Figura 5. Ejemplo de unidades. En el aprendizaje, la entrada son secuencias de unidades donde cada secuencia corresponde a un documento. Tomamos las unidades etiquetadas (etiquetadas como Title_Begin, Title_end u otra) en las secuencias como datos de entrenamiento y construimos modelos para identificar si una unidad es Title_Begin Title_end u otro. Empleamos cuatro tipos de modelos: perceptrón, entropía máxima (ME), modelo Perceptron Markov (PMM) y modelo de entropía máxima Markov (MEMM). En extracción, la entrada es una secuencia de unidades de un documento. Empleamos un tipo de modelo para identificar si una unidad es Title_Begin, Title_end u otro. Luego extraemos unidades de la unidad etiquetada con Title_Begin a la unidad etiquetada con Title_end. El resultado es el título extraído del documento. La característica única de nuestro enfoque es que utilizamos principalmente información de formato para la extracción de título. Suponemos que aunque los documentos generales varían en los estilos, sus formatos tienen ciertos patrones y podemos aprender y utilizar los patrones para la extracción de título. Esto contrasta con el trabajo de Han et al., En el que solo se utilizan características lingüísticas para la extracción de los trabajos de investigación.4.2 Modelos Los cuatro modelos en realidad pueden considerarse en el mismo marco de extracción de metadatos. Es por eso que los aplicamos a nuestro problema actual. Cada entrada es una secuencia de instancias KXXX L21 junto con una secuencia de etiquetas Kyyy L21.IX e IY representan una instancia y su etiqueta, respectivamente (ki ,, 2,1 l =). Recuerde que una instancia aquí representa una unidad. Una etiqueta representa Title_Begin, Title_end u otra. Aquí, K es el número de unidades en un documento. En el aprendizaje, capacitamos un modelo que generalmente se puede denotar como una distribución de probabilidad condicional) | (11 kk xxyyp ll donde ix e iy denotan variables aleatorias que toman instancia ix y etiqueta como valores, respectivamente (ki ,, 2,1 l =). Herramienta de extracción de herramientas de aprendizaje 21121 22222122221 1121111211 nknnnn kk kk yyyxxx yyyxxxx yyyxxx ll ll ll ll → → →) | (maxarg 11 mkmmkm xxyyp ll) | (11 kk xxyip ll distribución condicional mkmm xx xx figura 6. Modelo de extracción de metadatos. Podemos hacer suposiciones sobre el modelo general para hacerlo lo suficientemente simple para el entrenamiento.148 Por ejemplo, podemos suponer que Kyy, 1 L son independientes entre sí, dado Kxx ,, 1 l. Por lo tanto, tenemos) | () | () | (11 11 kk kk xypxyp xxyyp l ll = De esta manera, descomponemos el modelo en varios clasificadores. Entrenamos a los clasificadores localmente utilizando los datos etiquetados. Como clasificador, empleamos el modelo Perceptron o de entropía máxima. También podemos suponer que la propiedad de primer pedido de Markov es válida para Kyy, 1 L dada Kxx ,, 1 l. Por lo tanto, tenemos) | () | () | (111 11 KKK KK XYYPXYP XXYYP - = L LL nuevamente, obtenemos una serie de clasificadores. Sin embargo, los clasificadores están condicionados en la etiqueta anterior. Cuando empleamos la percepción del modelo de entropía máxima como clasificador, los modelos se convierten en un modelo de percepción de Markov o modelo de entropía máxima Markov, respectivamente. Es decir, los dos modelos son más precisos. En extracción, dada una nueva secuencia de instancias, recurrimos a uno de los modelos construidos para asignar una secuencia de etiquetas a la secuencia de instancias, es decir, realizar extracción. Para Perceptron y para mí, asignamos etiquetas localmente y combinamos los resultados a nivel mundial mediante la heurística. Específicamente, primero identificamos el Title_Begin más probable. Luego encontramos el Title_end más probable dentro de tres unidades después del Title_Begin. Finalmente, extraemos como título las unidades entre el Title_Begin y el Title_end. Para PMM y MEMM, empleamos el algoritmo Viterbi para encontrar la secuencia de etiqueta globalmente óptima. En este artículo, para Perceptron, en realidad empleamos una variante mejorada, llamada Perceptron con margen desigual [13]. Esta versión de Perceptron puede funcionar bien, especialmente cuando el número de instancias positivas y el número de instancias negativas difieren enormemente, lo cual es exactamente el caso en nuestro problema. También empleamos una versión mejorada del modelo Perceptron Markov en el que el modelo Perceptron es el llamado Perceptron votado [2]. Además, en el entrenamiento, los parámetros del modelo se actualizan a nivel mundial en lugar de localmente.4.3 Características Hay dos tipos de características: características de formato y características lingüísticas. Principalmente usamos el primero. Las características se utilizan tanto para el título de Begin como para los clasificadores de fin de título.4.3.1 Características del formato Tamaño de fuente: Hay cuatro características binarias que representan el tamaño de fuente normalizado de la unidad (recuerde que una unidad tiene solo un tipo de fuente). Si el tamaño de fuente de la unidad es el más grande en el documento, la primera característica será 1, de lo contrario 0. Si el tamaño de fuente es el más pequeño en el documento, entonces la cuarta característica será 1, de lo contrario 0. Si el tamaño de fuente está por encima del tamaño de fuente promedio y no es el más grande del documento, entonces la segunda característica será 1, de lo contrario 0. Si el tamaño de la fuente está por debajo del tamaño de fuente promedio y no el más pequeño, la tercera característica será 1, de lo contrario 0. Es necesario realizar la normalización en tamaños de fuente. Por ejemplo, en un documento, el tamaño de fuente más grande podría ser 12pt, mientras que en otro el más pequeño podría ser 18pt. Boldface: esta característica binaria representa si la unidad actual está en negrita o no. Alineación: hay cuatro características binarias que representan respectivamente la ubicación de la unidad actual: alineación izquierda, central, derecha y desconocida. Las siguientes características del formato con respecto al contexto juegan un papel importante en la extracción de título. Unidad vecina vacía: hay dos características binarias que representan, respectivamente, si la unidad anterior y la unidad actual son líneas en blanco. Cambio de tamaño de fuente: hay dos características binarias que representan, respectivamente, ya sea que el tamaño de fuente de la unidad anterior y el tamaño de fuente de la siguiente unidad difieran del de la unidad actual. Cambio de alineación: hay dos características binarias que representan, respectivamente, ya sea que la alineación de la unidad anterior y la alineación de la siguiente unidad difieran de la de la actual. El mismo párrafo: hay dos características binarias que representan, respectivamente, ya sea que la unidad anterior y la siguiente unidad estén en el mismo párrafo que la unidad actual.4.3.2 Características lingüísticas Las características lingüísticas se basan en palabras clave. Palabra positiva: esta característica binaria representa si la unidad actual comienza o no con una de las palabras positivas. Las palabras positivas incluyen Título:, Asunto:, por ejemplo, por ejemplo, en algunos documentos, las líneas de títulos y autores tienen los mismos formatos. Sin embargo, si las líneas comienzan con una de las palabras positivas, entonces es probable que sean líneas de título. Palabra negativa: esta característica binaria representa si la unidad actual comienza o no con una de las palabras negativas. Las palabras negativas incluyen, por, creadas por, actualizadas por, etc. Hay más palabras negativas que palabras positivas. Las características lingüísticas anteriores dependen del lenguaje. Recuento de palabras: un título no debe ser demasiado largo. Creamos heurísticamente cuatro intervalos: [1, 2], [3, 6], [7, 9] y [9, ∞) y definimos una característica para cada intervalo. Si el número de palabras en un título cae en un intervalo, la característica correspondiente será 1;de lo contrario 0. Carácter final: esta característica representa si la unidad termina con :, -u otros caracteres especiales. Un título generalmente no termina con tal personaje.5. Método de recuperación de documentos Describimos nuestro método de recuperación de documentos utilizando títulos extraídos. Por lo general, en la recuperación de la información, un documento se divide en varios campos, incluidos el cuerpo, el título y el texto de anclaje. Una función de clasificación en la búsqueda puede usar diferentes pesos para diferentes campos de 149 el documento. Además, los títulos generalmente se les asigna altos pesos, lo que indica que son importantes para la recuperación de documentos. Como se explicó anteriormente, nuestro experimento ha demostrado que un número significativo de documentos en realidad tiene títulos incorrectos en las propiedades del archivo y, por lo tanto, además de usarlos usamos los títulos extraídos como un campo más del documento. Al hacer esto, intentamos mejorar la precisión general. En este documento, empleamos una modificación de BM25 que permite la ponderación de campo [21]. Como campos, utilizamos el cuerpo, el título, el título extraído y el ancla. Primero, para cada término en la consulta contamos el término frecuencia en cada campo del documento;Cada frecuencia de campo se pondera de acuerdo con el parámetro de peso correspondiente: ∑ = f tfft tfwwtf De manera similar, calculamos la longitud del documento como una suma ponderada de longitudes de cada campo. La longitud promedio del documento en el corpus se convierte en el promedio de todas las longitudes de documentos ponderadas.∑ = f ff dlwwdl En nuestros experimentos usamos 75.0,8.11 == bk. El peso para el contenido fue de 1.0, el título fue de 10.0, el ancla fue de 10.0 y el título extraído fue de 5.0.6. Resultados experimentales 6.1 Conjuntos de datos y medidas de evaluación Utilizamos dos conjuntos de datos en nuestros experimentos. Primero, descargamos y seleccionamos al azar 5,000 documentos de Word y 5,000 documentos de PowerPoint de una intranet de Microsoft. Lo llamamos Sra. En adelante. En segundo lugar, descargamos y seleccionamos al azar 500 Word y 500 Documentos de PowerPoint de los dominios DOTGOV y DOTCOM en Internet, respectivamente. La Figura 7 muestra las distribuciones de los géneros de los documentos. Vemos que los documentos son de hecho documentos generales tal como los definimos. Figura 7. Distribuciones de géneros de documentos. Tercero, un conjunto de datos en chino también se descargó de Internet. Incluye 500 documentos de Word y 500 documentos de PowerPoint en chino. Relacionamos manualmente los títulos de todos los documentos, sobre la base de nuestra especificación. No todos los documentos en los dos conjuntos de datos tienen títulos. La Tabla 1 muestra los porcentajes de los documentos que tienen títulos. Vemos que Dotcom y Dotgov tienen más documentos de PowerPoint con títulos que MS. Esto podría deberse a que los documentos de PowerPoint publicados en Internet son más formales que los de la intranet. Tabla 1. La parte de los documentos con títulos Tipo de dominio MS Dotcom Dotgov Word 75.7% 77.8% 75.6% PowerPoint 82.1% 93.4% 96.4% En nuestros experimentos, realizamos evaluaciones sobre la extracción del título en términos de precisión, recuperación y medición F. Las medidas de evaluación se definen de la siguiente manera: Precisión: P = A / (A + B) Recuerde: R = A / (A + C) F-Medición: F1 = 2Pr / (P + R) aquí, A, B, Cy D son números de documentos como los definidos en la Tabla 2. Tabla 2. La tabla de contingencia con respecto a la extracción del título es el título no se extrae el título A B No se extrae C D 6.2 Basas Base Probamos las precisiones de las dos líneas de base descritas en la Sección 4.2. Se denotan como un tamaño de fuente más grande y primera línea respectivamente.6.3 Precisión de los títulos en las propiedades del archivo Investigamos cuántos títulos en las propiedades del archivo de los documentos son confiables. Vemos los títulos anotados por humanos como títulos verdaderos y probamos cuántos títulos en las propiedades del archivo pueden coincidir aproximadamente con los títulos verdaderos. Utilizamos la distancia de edición para realizar la coincidencia aproximada.(La coincidencia aproximada solo se usa en esta evaluación). Esto se debe a que a veces los títulos anotados humanos pueden ser ligeramente diferentes de los títulos en las propiedades del archivo en la superficie, por ejemplo, contienen espacios adicionales). Dada la cadena A y String B: if ((d == 0) o (d / (la + lb) <θ)) luego cadena a = cadena B D: Editar distancia entre la cadena A y la cadena B La: Longitud de la cadena A LB: Longitud de la cadena b θ: 0.1 ∑ × ++ - += t t n n wtf avwdl wdl bbk kwtf fbm) log ()) 1 (() 1 (25 1 1 150 Tabla 3. Preciosas de los títulos en propiedades de archivo Tipo de archivo Dominio Precisión Recarga F1 MS 0.299 0.311 0.305 DOTCOM 0.210 0.214 0.212 Word Dotgov 0.182 0.177 0.180 ms 0.229 0.245 0.237 Dotcom 0.185 0.186 0.186Power Dotgov 0.180 0.182 6. los primeros datosEstablecer (Word y PowerPoint en MS). Como modelo, utilizamos Perceptron. Realizamos una validación cruzada de 4 veces. Por lo tanto, todos los resultados informados aquí son los promediados en más de 4 ensayos. Las tablas 4 y 5 muestran los resultados. Vemos que Perceptron supera significativamente las líneas de base. En la evaluación, utilizamos una coincidencia exacta entre los verdaderos títulos anotados por humanos y los títulos extraídos. Tabla 4. Precisiones de extracción del título con el modelo de precisión de palabras F1 Modelo Perceptron 0.810 0.837 0.823 Tamaño de fuente más grande 0.700 0.758 0.727 Líneas de base Primera línea 0.707 0.767 0.736 Tabla 5. Precisiones de extracción del título con PowerPoint Precision Retiro F1 Modelo Perceptron 0.875 0. 895 0.885 Tamaño de fuente más grande 0.844 0.887 0.865 líneas de base Primera línea 0.639 0.671 0.655 Vemos que el enfoque de aprendizaje automático puede lograr un buen rendimiento en la extracción del título. Para los documentos de Word, tanto la precisión como el recuerdo del enfoque son un 8 por ciento más altos que los de las líneas de base. Para PowerPoint, tanto la precisión como el recuerdo del enfoque son un 2 por ciento más altos que los de las líneas de base. Realizamos pruebas de significancia. Los resultados se muestran en la Tabla 6. Aquí, la más grande denota la línea de base de usar el tamaño de fuente más grande, primero denota la línea de base de usar la primera línea. Los resultados indican que las mejoras del aprendizaje automático sobre las líneas de base son estadísticamente significativas (en el sentido del valor p <0.05) Tabla 6. Sign Resultado de la prueba Documentos Tipo de prueba de signo entre el valor p de perceptrón frente a 3.59e-26 Word Perceptron vs. Primer 7.12e-10 perceptrón frente a 0.010 PowerPoint Perceptron vs. Primero 5.13e-40 que vemos, de los resultados, que, queLas dos líneas de base pueden funcionar bien para la extracción del título, lo que sugiere que el tamaño de la fuente y la información de posición son las características más útiles para la extracción de título. Sin embargo, también es obvio que usar solo estas dos características no es suficiente. Hay casos en los que todas las líneas tienen el mismo tamaño de fuente (es decir, el tamaño de fuente más grande), o casos en los que las líneas con el tamaño de fuente más grande solo contienen descripciones generales como confidencial, papel blanco, etc. Para esos casos, el método de tamaño de fuente más grande no puede funcionar bien. Por razones similares, el método de primera línea por sí solo no puede funcionar bien. Con la combinación de diferentes características (evidencia en el juicio del título), Perceptron puede superar más grande y primero. Investigamos el rendimiento de utilizar únicamente características lingüísticas. Descubrimos que no funciona bien. Parece que las características del formato juegan papeles importantes y las características lingüísticas son suplementos. Figura 8. Un documento de palabras de ejemplo. Figura 9. Un ejemplo de documento de PowerPoint. Realizamos un análisis de error sobre los resultados de Perceptron. Descubrimos que los errores cayeron en tres categorías.(1) Aproximadamente un tercio de los errores estaban relacionados con casos duros. En estos documentos, los diseños de las primeras páginas fueron difíciles de entender, incluso para los humanos. La Figura 8 y 9 muestra ejemplos.(2) Casi una cuarta parte de los errores fueron de los documentos que no tienen títulos verdaderos pero solo contienen balas. Dado que realizamos la extracción de las principales regiones, es difícil deshacerse de estos errores con el enfoque actual.(3). Las confusiones entre los títulos principales y los subtítulos fueron otro tipo de error. Dado que solo etiquetamos los títulos principales como títulos, las extracciones de ambos títulos se consideraron incorrectas. Sin embargo, este tipo de error hace poco daño al procesamiento de documentos como la búsqueda.6.5 Comparación entre modelos Para comparar el rendimiento de diferentes modelos de aprendizaje automático, realizamos otro experimento. Nuevamente, realizamos una validación de 4 veces 151 en el primer conjunto de datos (MS). La Tabla 7, 8 muestra los resultados de los cuatro modelos. Resulta que Perceptron y PMM realizan lo mejor, seguido de MEMM, y yo realiza lo peor. En general, los modelos de Markovian funcionan mejor que o sus homólogos clasificadores. Esto parece deberse a que los modelos de Markovian están entrenados a nivel mundial, mientras que los clasificadores están entrenados localmente. Los modelos basados en Perceptron funcionan mejor que las contrapartes basadas en ME. Esto parece deberse a que los modelos basados en Perceptron se crean para hacer mejores clasificaciones, mientras que los modelos ME se construyen para una mejor predicción. Tabla 7. Comparación entre diferentes modelos de aprendizaje para la extracción del título con el modelo de Word Precision Record F1 Perceptron 0.810 0.837 0.823 MEMM 0.797 0.824 0.810 PMM 0.827 0.823 0.825 ME 0.801 0.621 0.699 Tabla 8. Comparación entre diferentes modelos de aprendizaje para la extracción de título con el modelo PowerPoint Precision Recording F1 Perceptron 0.875 0. 895 0. 885 MEMM 0.841 0.861 0.851 PMM 0.873 0.896 0.885 ME 0.753 0.766 0.759 6.6 Adaptación de dominio Aplicamos el modelo entrenado con el primer conjunto de datos (MS)al segundo conjunto de datos (DOTCOM y DOTGOV). Las tablas 9-12 muestran los resultados. Tabla 9. Precisiones de extracción del título con Word en el modelo de recuerdo de precisión de Dotgov F1 Perceptron 0.716 0.759 0.737 Tamaño de fuente más grande 0.549 0.619 0.582BASELINES Primera línea 0.462 0.521 0.490 Tabla 10. Precisiones de extracción del título con PowerPoint en el modelo de recuerdo de precisión de Dotgov F1 Perceptron 0.900 0.906 0.903 Tamaño de fuente más grande 0.871 0.888 0.879BASELINES Primera línea 0.554 0.564 0.559 Tabla 11. Precisiones de extracción del título con Word en Dotcom Precision Recording F1 Modelo Perceptron 0.832 0.880 0.855 Tamaño de fuente más grande 0.676 0.753 0.712BASELINES Primera línea 0.577 0.643 0.608 Tabla 12. Rendimiento de la extracción del título del documento de PowerPoint en el modelo de precisión de DOTCOM Perceptron 0.910 0.903 0.907 Tamaño de fuente más grande 0.864 0.886 0.875BASELINES Primera línea 0.570 0.585 0.577 De los resultados, vemos que los modelos se pueden adaptar bien a diferentes dominios. Casi no hay caída en la precisión. Los resultados indican que los patrones de formatos de título existen en diferentes dominios, y es posible construir un modelo independiente del dominio mediante el uso principalmente de información de formato.6.7 Adaptación del idioma Aplicamos el modelo capacitado con los datos en inglés (MS) al conjunto de datos en chino. Las tablas 13-14 muestran los resultados. Tabla 13. Precisiones de extracción del título con Word en el modelo de recuerdo de precisión chino F1 Perceptron 0.817 0.805 0.811 Tamaño de fuente más grande 0.722 0.755 0.738BASELINES Primera línea 0.743 0.777 0.760 Tabla 14. Precisiones de extracción del título con PowerPoint en el modelo de precisión chino F1 Modelo Perceptron 0.766 0.812 0.789 Tamaño de fuente más grande 0.753 0.813 0.782BASELINES Primera línea 0.627 0.676 0.650 Nosotros vemos que los modelos pueden adaptarse a un idioma diferente. Solo hay pequeñas gotas en precisión. Obviamente, las características lingüísticas no funcionan para los chinos, pero el efecto de no usarlos es insignificante. Los resultados indican que los patrones de formatos de título existen en diferentes idiomas. De la adaptación del dominio y los resultados de la adaptación del lenguaje, concluimos que el uso de información de formato es la clave para una extracción exitosa de documentos generales.6.8 Búsqueda con títulos extraídos realizamos experimentos sobre el uso de la extracción de título para la recuperación de documentos. Como línea de base, empleamos BM25 sin usar títulos extraídos. El mecanismo de clasificación fue como se describe en la Sección 5. Los pesos se establecieron heurísticamente. No realizamos optimización en los pesos. La evaluación se realizó en un corpus de documentos 1.3 m rastreados de la intranet de Microsoft utilizando 100 consultas de evaluación obtenidas de estos registros de consultas de motores de búsqueda de intranets.50 consultas eran del set más popular, mientras que 50 consultas otras fueron elegidas al azar. Se pidió a los usuarios que proporcionaran juicios sobre el grado de relevancia del documento de una escala de 1 a 5 (1 que significa perjudicial, 2 - malo, 3 - justo, 4 - bueno y 5 - excelente).152 La Figura 10 muestra los resultados. En la tabla, se obtuvieron dos conjuntos de resultados de precisión considerando documentos buenos o excelentes como relevantes (izquierda 3 barras con umbral de relevancia 0.5), o considerando solo documentos excelentes como relevantes (derecha 3 barras con umbral de relevancia 1.0) 0 0.05 0.1 0.150.2 0.25 0.3 0.35 0.4 0.45 P@10 P@5 Recíproco P@10 P@5 Recíproco 0.5 1 BM25 Anchor, Título, Ancla BM25 BM25, Título, Cuerpo, Nombre ExtractedTitle All RelevancTethresshold Data Descripción Figura 10. Resultados de clasificación de búsqueda. La Figura 10 muestra diferentes resultados de recuperación de documentos con diferentes funciones de clasificación en términos de precisión @10, precisión @5 y rango recíproco: • Barra azul - BM25 que incluye el cuerpo de campos, el título (propiedad del archivo) y el texto de anclaje.• Purple Bar: BM25, incluido el cuerpo de campos, el título (propiedad del archivo), el texto de anclaje y el título extraído. Con el campo adicional del título extraído incluido en BM25, la precisión @10 aumentó de 0.132 a 0.145, o en ~ 10%. Por lo tanto, es seguro decir que el uso del título extraído puede mejorar la precisión de la recuperación de documentos.7. Conclusión En este documento, hemos investigado el problema de extraer automáticamente títulos de documentos generales. Hemos intentado usar un enfoque de aprendizaje automático para abordar el problema. El trabajo anterior mostró que el enfoque de aprendizaje automático puede funcionar bien para la extracción de metadatos de los trabajos de investigación. En este documento, demostramos que el enfoque también puede funcionar para la extracción de documentos generales. Nuestros resultados experimentales indicaron que el enfoque de aprendizaje automático puede funcionar significativamente mejor que las líneas de base en la extracción del título de los documentos de la oficina. Trabajo previo sobre la extracción de metadatos utilizó principalmente características lingüísticas en documentos, mientras que utilizamos principalmente información de formato. Parecía que usar información de formato es una clave para realizar con éxito la extracción de título de los documentos generales. Probamos diferentes modelos de aprendizaje automático que incluyen perceptrones, entropía máxima, modelo de entropía máxima Markov y perceptrón votado. Descubrimos que el rendimiento de los modelos de perceptornio era el mejor. Aplicamos modelos construidos en un dominio a otro dominio y aplicamos modelos capacitados en un idioma a otro idioma. Descubrimos que las precisiones no cayeron sustancialmente en diferentes dominios y en diferentes idiomas, lo que indica que los modelos eran genéricos. También intentamos usar los títulos extraídos en la recuperación de documentos. Observamos una mejora significativa en el rendimiento de clasificación de documentos para la búsqueda cuando se utilizan información de título extraída. Todas las investigaciones anteriores no se realizaron en trabajos anteriores, y a través de nuestras investigaciones verificamos la generalidad y la importancia del enfoque de extracción de título.8. Agradecimientos Agradecemos a Chunyu Wei y Bojuan Zhao por su trabajo en la anotación de datos. Reconocemos a Jinzhu Li por su ayuda para realizar los experimentos. Agradecemos a Ming Zhou, John Chen, Jun Xu y los revisores anónimos de JCDL05 por sus valiosos comentarios en este documento.9. Referencias [1] Berger, A. L., Della Pietra, S. A. y Della Pietra, V. J. Un enfoque de entropía máxima para el procesamiento del lenguaje natural. Computational Linguistics, 22: 39-71, 1996. [2] Collins, M. Métodos de entrenamiento discriminativo para modelos ocultos de Markov: teoría y experimentos con algoritmos de percepción. En Actas de la Conferencia sobre Métodos Empíricos en Procesamiento del Lenguaje Natural, 1-8, 2002. [3] Cortes, C. y Vapnik, V. Redes de soporte-vector. Machine Learning, 20: 273-297, 1995. [4] Chieu, H. L. y Ng, H. T. Un enfoque de entropía máxima para la extracción de información de texto semiestructurado y libre. En Actas de la Decimura Conferencia Nacional sobre Inteligencia Artificial, 768-791, 2002. [5] Evans, D. K., Klavans, J. L. y McKeown, K. R. Columbia Newsblaster: resumen de noticias multilingües en la Web. En Actas de la Conferencia de Tecnología del Lenguaje Humano / Capítulo de América del Norte de la Reunión Anual de la Asociación para la Lingüística Computacional, 1-4, 2004. [6] Ghahramani, Z. y Jordan, M. I. Modelos Factoriales Hidden Markov. Machine Learning, 29: 245-273, 1997. [7] Gheel, J. y Anderson, T. Datos y metadatos para encontrar y recordar, en Actas de la Conferencia Internacional de Visualización de la Información de 1999, 446-451,1999.[8] Giles, C. L., Petinot, Y., Teregowda P. B., Han, H., Lawrence, S., Rangaswamy, A. y Pal, N. Ebizsearch: un motor de búsqueda de nicho para e-Business. En Actas de la 26ª Conferencia Anual de ACM Sigir sobre investigación y desarrollo en recuperación de información, 413414, 2003. [9] Giuffrida, G., Shek, E. C. y Yang, J. Extracción de metadatos basados en el conocimiento de archivos PostScript. En Actas de la Quinta Conferencia ACM sobre Bibliotecas Digitales, 77-84, 2000. [10] Han, H., Giles, C. L., Manavoglu, E., Zha, H., Zhang, Z. y Fox, E. A. Extracción de metadatos automáticos del documento utilizando máquinas de vectores de soporte. En Actas de la tercera conferencia conjunta de ACM/IEEE-CS sobre bibliotecas digitales, 37-48, 2003. [11] Kobayashi, M. y Takeda, K. Recuperación de información en la web. ACM Computing Surveys, 32: 144-173, 2000. [12] Lafferty, J., McCallum, A. y Pereira, F. Campos aleatorios condicionales: modelos probabilísticos para segmentación y 153 datos de secuencia de etiquetado. En Actas de la Decimoctava Conferencia Internacional sobre Aprendizaje Autor, 282-289, 2001. [13] Li, Y., Zaragoza, H., Herbrich, R., Shawe-Taylor J. y Kandola, J. S. El algoritmo Perceptron con desigualmárgenes. En Actas de la Decimonovena Conferencia Internacional sobre Aprendizaje Autor, 379-386, 2002. [14] Liddy, E. D., Sutton, S., Allen, E., Harwell, S., Corieri, S., Yilmazel, O., Ozgencil, N. E., Diekema, A., McCracken, N. y Silverstein, J. Generación y evaluación automática de metadatos. En Actas de la 25ª Conferencia Anual de ACM Sigir sobre investigación y desarrollo en recuperación de información, 401-402, 2002. [15] Littlefield, A. Recuperación de información empresarial efectiva en nuevos formatos de contenido. En Actas de la Séptima Conferencia de motores de búsqueda, http://www.infonortics.com/searchengines/sh02/02prog.html, 2002. [16] Mao, S., Kim, J. W. y Thoma, G. R. Un sistema de generación dinámica de característicaspara la extracción de metadatos automatizados en la conservación de materiales digitales. En Actas del primer taller internacional sobre el análisis de imágenes de documentos para bibliotecas, 225-232, 2004. [17] McCallum, A., Freitag, D. y Pereira, F. Modelos máximos de entropía Markov para extracción y segmentación de información. En Actas de la Decimoséptima Conferencia Internacional sobre Aprendizaje Machine, 591-598, 2000. [18] Murphy, L. D. Metadatos de documentos digitales en organizaciones: roles, enfoques analíticos y futuras direcciones de investigación. En Actas de la trigésima primera conferencia internacional anual de Hawaii sobre Ciencias del Sistema, 267-276, 1998. [19] Pinto, D., McCallum, A., Wei, X. y Croft, W. B. Extracción de tabla utilizando campos aleatorios condicionales. En Actas de la 26ª Conferencia Anual de ACM Sigir sobre investigación y desarrollo en recuperación de información, 235242, 2003. [20] Ratnaparkhi, A. Modelos estadísticos no supervisados para el apego de frases preposicionales. En Actas de la Decimoséptima Conferencia Internacional sobre Lingüística Computacional.1079-1085, 1998. [21] Robertson, S., Zaragoza, H. y Taylor, M. Extensión simple de BM25 a múltiples campos ponderados, en Actas de la Decimotercera Conferencia sobre Gestión de Información y Conocimiento, 42-49, 2004.[22] Yi, J. y Sundaresan, N. Minería web basada en metadatos por relevancia, en Actas del Simposio Internacional 2000 sobre Ingeniería y Aplicaciones de Base de Datos, 113121, 2000. [23] Yilmazel, O., Finneran, C. M. y Liddy, E. D. MetaExtract: un sistema NLP para asignar automáticamente metadatos. En Actas de la Conferencia Conjunta ACM/IEEE 2004 sobre Bibliotecas Digitales, 241-242, 2004. [24] Zhang, J. y Dimitroff, A. Respuesta de motores de búsqueda de Internet a la implementación central de Dublín de metadatos. Journal of Information Science, 30: 310-320, 2004. [25] Zhang, L., Pan, Y. y Zhang, T. Reconocimiento y uso de entidades con nombre: reconocimiento de entidades con nombre enfocado utilizando aprendizaje automático. En Actas de la 27ª Conferencia Anual de ACM Sigir sobre investigación y desarrollo en recuperación de información, 281-288, 2004. [26] http://dublincore.org/groups/corporate/seattle/ 154",
    "original_sentences": [
        "Automatic Extraction of Titles from General Documents using Machine Learning Yunhua Hu1 Computer Science Department Xian Jiaotong University No 28, Xianning West Road Xian, China, 710049 yunhuahu@mail.xjtu.edu.cn Hang Li, Yunbo Cao Microsoft Research Asia 5F Sigma Center, No.",
        "49 Zhichun Road, Haidian, Beijing, China, 100080 {hangli,yucao}@microsoft.com Qinghua Zheng Computer Science Department Xian Jiaotong University No 28, Xianning West Road Xian, China, 710049 qhzheng@mail.xjtu.edu.cn Dmitriy Meyerzon Microsoft Corporation One Microsoft Way Redmond, WA, USA, 98052 dmitriym@microsoft.com ABSTRACT In this paper, we propose a machine learning approach to title extraction from general documents.",
        "By general documents, we mean documents that can belong to any one of a number of specific genres, including presentations, book chapters, technical papers, brochures, reports, and letters.",
        "Previously, methods have been proposed mainly for title extraction from research papers.",
        "It has not been clear whether it could be possible to conduct automatic title extraction from general documents.",
        "As a case study, we consider extraction from Office including Word and PowerPoint.",
        "In our approach, we annotate titles in sample documents (for Word and PowerPoint respectively) and take them as training data, train machine learning models, and perform title extraction using the trained models.",
        "Our method is unique in that we mainly utilize formatting information such as font size as features in the models.",
        "It turns out that the use of formatting information can lead to quite accurate extraction from general documents.",
        "Precision and recall for title extraction from Word is 0.810 and 0.837 respectively, and precision and recall for title extraction from PowerPoint is 0.875 and 0.895 respectively in an experiment on intranet data.",
        "Other important new findings in this work include that we can train models in one domain and apply them to another domain, and more surprisingly we can even train models in one language and apply them to another language.",
        "Moreover, we can significantly improve search ranking results in document retrieval by using the extracted titles.",
        "Categories and Subject Descriptors H.3.3 [Information Storage and Retrieval]: Information Search and Retrieval - Search Process; H.4.1 [Information Systems Applications]: Office Automation - Word processing; D.2.8 [Software Engineering]: Metrics - complexity measures, performance measures General Terms Algorithms, Experimentation, Performance. 1.",
        "INTRODUCTION Metadata of documents is useful for many kinds of document processing such as search, browsing, and filtering.",
        "Ideally, metadata is defined by the authors of documents and is then used by various systems.",
        "However, people seldom define document metadata by themselves, even when they have convenient metadata definition tools [26].",
        "Thus, how to automatically extract metadata from the bodies of documents turns out to be an important research issue.",
        "Methods for performing the task have been proposed.",
        "However, the focus was mainly on extraction from research papers.",
        "For instance, Han et al. [10] proposed a machine learning based method to conduct extraction from research papers.",
        "They formalized the problem as that of classification and employed Support Vector Machines as the classifier.",
        "They mainly used linguistic features in the model.1 In this paper, we consider metadata extraction from general documents.",
        "By general documents, we mean documents that may belong to any one of a number of specific genres.",
        "General documents are more widely available in digital libraries, intranets and the internet, and thus investigation on extraction from them is sorely needed.",
        "Research papers usually have well-formed styles and noticeable characteristics.",
        "In contrast, the styles of general documents can vary greatly.",
        "It has not been clarified whether a machine learning based approach can work well for this task.",
        "There are many types of metadata: title, author, date of creation, etc.",
        "As a case study, we consider title extraction in this paper.",
        "General documents can be in many different file formats: Microsoft Office, PDF (PS), etc.",
        "As a case study, we consider extraction from Office including Word and PowerPoint.",
        "We take a machine learning approach.",
        "We annotate titles in sample documents (for Word and PowerPoint respectively) and take them as training data to train several types of models, and perform title extraction using any one type of the trained models.",
        "In the models, we mainly utilize formatting information such as font size as features.",
        "We employ the following models: Maximum Entropy Model, Perceptron with Uneven Margins, Maximum Entropy Markov Model, and Voted Perceptron.",
        "In this paper, we also investigate the following three problems, which did not seem to have been examined previously. (1) Comparison between models: among the models above, which model performs best for title extraction; (2) Generality of model: whether it is possible to train a model on one domain and apply it to another domain, and whether it is possible to train a model in one language and apply it to another language; (3) Usefulness of extracted titles: whether extracted titles can improve document processing such as search.",
        "Experimental results indicate that our approach works well for title extraction from general documents.",
        "Our method can significantly outperform the baselines: one that always uses the first lines as titles and the other that always uses the lines in the largest font sizes as titles.",
        "Precision and recall for title extraction from Word are 0.810 and 0.837 respectively, and precision and recall for title extraction from PowerPoint are 0.875 and 0.895 respectively.",
        "It turns out that the use of format features is the key to successful title extraction. (1) We have observed that Perceptron based models perform better in terms of extraction accuracies. (2) We have empirically verified that the models trained with our approach are generic in the sense that they can be trained on one domain and applied to another, and they can be trained in one language and applied to another. (3) We have found that using the extracted titles we can significantly improve precision of document retrieval (by 10%).",
        "We conclude that we can indeed conduct reliable title extraction from general documents and use the extracted results to improve real applications.",
        "The rest of the paper is organized as follows.",
        "In section 2, we introduce related work, and in section 3, we explain the motivation and problem setting of our work.",
        "In section 4, we describe our method of title extraction, and in section 5, we describe our method of document retrieval using extracted titles.",
        "Section 6 gives our experimental results.",
        "We make concluding remarks in section 7. 2.",
        "RELATED WORK 2.1 Document Metadata Extraction Methods have been proposed for performing automatic metadata extraction from documents; however, the main focus was on extraction from research papers.",
        "The proposed methods fall into two categories: the rule based approach and the machine learning based approach.",
        "Giuffrida et al. [9], for instance, developed a rule-based system for automatically extracting metadata from research papers in Postscript.",
        "They used rules like titles are usually located on the upper portions of the first pages and they are usually in the largest font sizes.",
        "Liddy et al. [14] and Yilmazel el al. [23] performed metadata extraction from educational materials using rule-based natural language processing technologies.",
        "Mao et al. [16] also conducted automatic metadata extraction from research papers using rules on formatting information.",
        "The rule-based approach can achieve high performance.",
        "However, it also has disadvantages.",
        "It is less adaptive and robust when compared with the machine learning approach.",
        "Han et al. [10], for instance, conducted metadata extraction with the machine learning approach.",
        "They viewed the problem as that of classifying the lines in a document into the categories of metadata and proposed using Support Vector Machines as the classifier.",
        "They mainly used linguistic information as features.",
        "They reported high extraction accuracy from research papers in terms of precision and recall. 2.2 Information Extraction Metadata extraction can be viewed as an application of information extraction, in which given a sequence of instances, we identify a subsequence that represents information in which we are interested.",
        "Hidden Markov Model [6], Maximum Entropy Model [1, 4], Maximum Entropy Markov Model [17], Support Vector Machines [3], Conditional Random Field [12], and Voted Perceptron [2] are widely used information extraction models.",
        "Information extraction has been applied, for instance, to part-ofspeech tagging [20], named entity recognition [25] and table extraction [19]. 2.3 Search Using Title Information Title information is useful for document retrieval.",
        "In the system Citeseer, for instance, Giles et al. managed to extract titles from research papers and make use of the extracted titles in metadata search of papers [8].",
        "In web search, the title fields (i.e., file properties) and anchor texts of web pages (HTML documents) can be viewed as titles of the pages [5].",
        "Many search engines seem to utilize them for web page retrieval [7, 11, 18, 22].",
        "Zhang et al., found that web pages with well-defined metadata are more easily retrieved than those without well-defined metadata [24].",
        "To the best of our knowledge, no research has been conducted on using extracted titles from general documents (e.g., Office documents) for search of the documents. 146 3.",
        "MOTIVATION AND PROBLEM SETTING We consider the issue of automatically extracting titles from general documents.",
        "By general documents, we mean documents that belong to one of any number of specific genres.",
        "The documents can be presentations, books, book chapters, technical papers, brochures, reports, memos, specifications, letters, announcements, or resumes.",
        "General documents are more widely available in digital libraries, intranets, and internet, and thus investigation on title extraction from them is sorely needed.",
        "Figure 1 shows an estimate on distributions of file formats on intranet and internet [15].",
        "Office and PDF are the main file formats on the intranet.",
        "Even on the internet, the documents in the formats are still not negligible, given its extremely large size.",
        "In this paper, without loss of generality, we take Office documents as an example.",
        "Figure 1.",
        "Distributions of file formats in internet and intranet.",
        "For Office documents, users can define titles as file properties using a feature provided by Office.",
        "We found in an experiment, however, that users seldom use the feature and thus titles in file properties are usually very inaccurate.",
        "That is to say, titles in file properties are usually inconsistent with the true titles in the file bodies that are created by the authors and are visible to readers.",
        "We collected 6,000 Word and 6,000 PowerPoint documents from an intranet and the internet and examined how many titles in the file properties are correct.",
        "We found that surprisingly the accuracy was only 0.265 (cf., Section 6.3 for details).",
        "A number of reasons can be considered.",
        "For example, if one creates a new file by copying an old file, then the file property of the new file will also be copied from the old file.",
        "In another experiment, we found that Google uses the titles in file properties of Office documents in search and browsing, but the titles are not very accurate.",
        "We created 50 queries to search Word and PowerPoint documents and examined the top 15 results of each query returned by Google.",
        "We found that nearly all the titles presented in the search results were from the file properties of the documents.",
        "However, only 0.272 of them were correct.",
        "Actually, true titles usually exist at the beginnings of the bodies of documents.",
        "If we can accurately extract the titles from the bodies of documents, then we can exploit reliable title information in document processing.",
        "This is exactly the problem we address in this paper.",
        "More specifically, given a Word document, we are to extract the title from the top region of the first page.",
        "Given a PowerPoint document, we are to extract the title from the first slide.",
        "A title sometimes consists of a main title and one or two subtitles.",
        "We only consider extraction of the main title.",
        "As baselines for title extraction, we use that of always using the first lines as titles and that of always using the lines with largest font sizes as titles.",
        "Figure 2.",
        "Title extraction from Word document.",
        "Figure 3.",
        "Title extraction from PowerPoint document.",
        "Next, we define a specification for human judgments in title data annotation.",
        "The annotated data will be used in training and testing of the title extraction methods.",
        "Summary of the specification: The title of a document should be identified on the basis of common sense, if there is no difficulty in the identification.",
        "However, there are many cases in which the identification is not easy.",
        "There are some rules defined in the specification that guide identification for such cases.",
        "The rules include a title is usually in consecutive lines in the same format, a document can have no title, titles in images are not considered, a title should not contain words like draft, 147 whitepaper, etc, if it is difficult to determine which is the title, select the one in the largest font size, and if it is still difficult to determine which is the title, select the first candidate. (The specification covers all the cases we have encountered in data annotation.)",
        "Figures 2 and 3 show examples of Office documents from which we conduct title extraction.",
        "In Figure 2, Differences in Win32 API Implementations among Windows Operating Systems is the title of the Word document.",
        "Microsoft Windows on the top of this page is a picture and thus is ignored.",
        "In Figure 3, Building Competitive Advantages through an Agile Infrastructure is the title of the PowerPoint document.",
        "We have developed a tool for annotation of titles by human annotators.",
        "Figure 4 shows a snapshot of the tool.",
        "Figure 4.",
        "Title annotation tool. 4.",
        "TITLE EXTRACTION METHOD 4.1 Outline Title extraction based on machine learning consists of training and extraction.",
        "The same pre-processing step occurs before training and extraction.",
        "During pre-processing, from the top region of the first page of a Word document or the first slide of a PowerPoint document a number of units for processing are extracted.",
        "If a line (lines are separated by return symbols) only has a single format, then the line will become a unit.",
        "If a line has several parts and each of them has its own format, then each part will become a unit.",
        "Each unit will be treated as an instance in learning.",
        "A unit contains not only content information (linguistic information) but also formatting information.",
        "The input to pre-processing is a document and the output of pre-processing is a sequence of units (instances).",
        "Figure 5 shows the units obtained from the document in Figure 2.",
        "Figure 5.",
        "Example of units.",
        "In learning, the input is sequences of units where each sequence corresponds to a document.",
        "We take labeled units (labeled as title_begin, title_end, or other) in the sequences as training data and construct models for identifying whether a unit is title_begin title_end, or other.",
        "We employ four types of models: Perceptron, Maximum Entropy (ME), Perceptron Markov Model (PMM), and Maximum Entropy Markov Model (MEMM).",
        "In extraction, the input is a sequence of units from one document.",
        "We employ one type of model to identify whether a unit is title_begin, title_end, or other.",
        "We then extract units from the unit labeled with title_begin to the unit labeled with title_end.",
        "The result is the extracted title of the document.",
        "The unique characteristic of our approach is that we mainly utilize formatting information for title extraction.",
        "Our assumption is that although general documents vary in styles, their formats have certain patterns and we can learn and utilize the patterns for title extraction.",
        "This is in contrast to the work by Han et al., in which only linguistic features are used for extraction from research papers. 4.2 Models The four models actually can be considered in the same metadata extraction framework.",
        "That is why we apply them together to our current problem.",
        "Each input is a sequence of instances kxxx L21 together with a sequence of labels kyyy L21 . ix and iy represents an instance and its label, respectively ( ki ,,2,1 L= ).",
        "Recall that an instance here represents a unit.",
        "A label represents title_begin, title_end, or other.",
        "Here, k is the number of units in a document.",
        "In learning, we train a model which can be generally denoted as a conditional probability distribution )|( 11 kk XXYYP LL where iX and iY denote random variables taking instance ix and label iy as values, respectively ( ki ,,2,1 L= ).",
        "Learning Tool Extraction Tool 21121 2222122221 1121111211 nknnknn kk kk yyyxxx yyyxxx yyyxxx LL LL LL LL → → → )|(maxarg 11 mkmmkm xxyyP LL )|( 11 kk XXYYP LL Conditional Distribution mkmm xxx L21 Figure 6.",
        "Metadata extraction model.",
        "We can make assumptions about the general model in order to make it simple enough for training. 148 For example, we can assume that kYY ,,1 L are independent of each other given kXX ,,1 L .",
        "Thus, we have )|()|( )|( 11 11 kk kk XYPXYP XXYYP L LL = In this way, we decompose the model into a number of classifiers.",
        "We train the classifiers locally using the labeled data.",
        "As the classifier, we employ the Perceptron or Maximum Entropy model.",
        "We can also assume that the first order Markov property holds for kYY ,,1 L given kXX ,,1 L .",
        "Thus, we have )|()|( )|( 111 11 kkk kk XYYPXYP XXYYP −= L LL Again, we obtain a number of classifiers.",
        "However, the classifiers are conditioned on the previous label.",
        "When we employ the Percepton or Maximum Entropy model as a classifier, the models become a Percepton Markov Model or Maximum Entropy Markov Model, respectively.",
        "That is to say, the two models are more precise.",
        "In extraction, given a new sequence of instances, we resort to one of the constructed models to assign a sequence of labels to the sequence of instances, i.e., perform extraction.",
        "For Perceptron and ME, we assign labels locally and combine the results globally later using heuristics.",
        "Specifically, we first identify the most likely title_begin.",
        "Then we find the most likely title_end within three units after the title_begin.",
        "Finally, we extract as a title the units between the title_begin and the title_end.",
        "For PMM and MEMM, we employ the Viterbi algorithm to find the globally optimal label sequence.",
        "In this paper, for Perceptron, we actually employ an improved variant of it, called Perceptron with Uneven Margin [13].",
        "This version of Perceptron can work well especially when the number of positive instances and the number of negative instances differ greatly, which is exactly the case in our problem.",
        "We also employ an improved version of Perceptron Markov Model in which the Perceptron model is the so-called Voted Perceptron [2].",
        "In addition, in training, the parameters of the model are updated globally rather than locally. 4.3 Features There are two types of features: format features and linguistic features.",
        "We mainly use the former.",
        "The features are used for both the title-begin and the title-end classifiers. 4.3.1 Format Features Font Size: There are four binary features that represent the normalized font size of the unit (recall that a unit has only one type of font).",
        "If the font size of the unit is the largest in the document, then the first feature will be 1, otherwise 0.",
        "If the font size is the smallest in the document, then the fourth feature will be 1, otherwise 0.",
        "If the font size is above the average font size and not the largest in the document, then the second feature will be 1, otherwise 0.",
        "If the font size is below the average font size and not the smallest, the third feature will be 1, otherwise 0.",
        "It is necessary to conduct normalization on font sizes.",
        "For example, in one document the largest font size might be 12pt, while in another the smallest one might be 18pt.",
        "Boldface: This binary feature represents whether or not the current unit is in boldface.",
        "Alignment: There are four binary features that respectively represent the location of the current unit: left, center, right, and unknown alignment.",
        "The following format features with respect to context play an important role in title extraction.",
        "Empty Neighboring Unit: There are two binary features that represent, respectively, whether or not the previous unit and the current unit are blank lines.",
        "Font Size Change: There are two binary features that represent, respectively, whether or not the font size of the previous unit and the font size of the next unit differ from that of the current unit.",
        "Alignment Change: There are two binary features that represent, respectively, whether or not the alignment of the previous unit and the alignment of the next unit differ from that of the current one.",
        "Same Paragraph: There are two binary features that represent, respectively, whether or not the previous unit and the next unit are in the same paragraph as the current unit. 4.3.2 Linguistic Features The linguistic features are based on key words.",
        "Positive Word: This binary feature represents whether or not the current unit begins with one of the positive words.",
        "The positive words include title:, subject:, subject line: For example, in some documents the lines of titles and authors have the same formats.",
        "However, if lines begin with one of the positive words, then it is likely that they are title lines.",
        "Negative Word: This binary feature represents whether or not the current unit begins with one of the negative words.",
        "The negative words include To, By, created by, updated by, etc.",
        "There are more negative words than positive words.",
        "The above linguistic features are language dependent.",
        "Word Count: A title should not be too long.",
        "We heuristically create four intervals: [1, 2], [3, 6], [7, 9] and [9, ∞) and define one feature for each interval.",
        "If the number of words in a title falls into an interval, then the corresponding feature will be 1; otherwise 0.",
        "Ending Character: This feature represents whether the unit ends with :, -, or other special characters.",
        "A title usually does not end with such a character. 5.",
        "DOCUMENT RETRIEVAL METHOD We describe our method of document retrieval using extracted titles.",
        "Typically, in information retrieval a document is split into a number of fields including body, title, and anchor text.",
        "A ranking function in search can use different weights for different fields of 149 the document.",
        "Also, titles are typically assigned high weights, indicating that they are important for document retrieval.",
        "As explained previously, our experiment has shown that a significant number of documents actually have incorrect titles in the file properties, and thus in addition of using them we use the extracted titles as one more field of the document.",
        "By doing this, we attempt to improve the overall precision.",
        "In this paper, we employ a modification of BM25 that allows field weighting [21].",
        "As fields, we make use of body, title, extracted title and anchor.",
        "First, for each term in the query we count the term frequency in each field of the document; each field frequency is then weighted according to the corresponding weight parameter: ∑= f tfft tfwwtf Similarly, we compute the document length as a weighted sum of lengths of each field.",
        "Average document length in the corpus becomes the average of all weighted document lengths. ∑= f ff dlwwdl In our experiments we used 75.0,8.11 == bk .",
        "Weight for content was 1.0, title was 10.0, anchor was 10.0, and extracted title was 5.0. 6.",
        "EXPERIMENTAL RESULTS 6.1 Data Sets and Evaluation Measures We used two data sets in our experiments.",
        "First, we downloaded and randomly selected 5,000 Word documents and 5,000 PowerPoint documents from an intranet of Microsoft.",
        "We call it MS hereafter.",
        "Second, we downloaded and randomly selected 500 Word and 500 PowerPoint documents from the DotGov and DotCom domains on the internet, respectively.",
        "Figure 7 shows the distributions of the genres of the documents.",
        "We see that the documents are indeed general documents as we define them.",
        "Figure 7.",
        "Distributions of document genres.",
        "Third, a data set in Chinese was also downloaded from the internet.",
        "It includes 500 Word documents and 500 PowerPoint documents in Chinese.",
        "We manually labeled the titles of all the documents, on the basis of our specification.",
        "Not all the documents in the two data sets have titles.",
        "Table 1 shows the percentages of the documents having titles.",
        "We see that DotCom and DotGov have more PowerPoint documents with titles than MS.",
        "This might be because PowerPoint documents published on the internet are more formal than those on the intranet.",
        "Table 1.",
        "The portion of documents with titles Domain Type MS DotCom DotGov Word 75.7% 77.8% 75.6% PowerPoint 82.1% 93.4% 96.4% In our experiments, we conducted evaluations on title extraction in terms of precision, recall, and F-measure.",
        "The evaluation measures are defined as follows: Precision: P = A / ( A + B ) Recall: R = A / ( A + C ) F-measure: F1 = 2PR / ( P + R ) Here, A, B, C, and D are numbers of documents as those defined in Table 2.",
        "Table 2.",
        "Contingence table with regard to title extraction Is title Is not title Extracted A B Not extracted C D 6.2 Baselines We test the accuracies of the two baselines described in section 4.2.",
        "They are denoted as largest font size and first line respectively. 6.3 Accuracy of Titles in File Properties We investigate how many titles in the file properties of the documents are reliable.",
        "We view the titles annotated by humans as true titles and test how many titles in the file properties can approximately match with the true titles.",
        "We use Edit Distance to conduct the approximate match. (Approximate match is only used in this evaluation).",
        "This is because sometimes human annotated titles can be slightly different from the titles in file properties on the surface, e.g., contain extra spaces).",
        "Given string A and string B: if ( (D == 0) or ( D / ( La + Lb ) < θ ) ) then string A = string B D: Edit Distance between string A and string B La: length of string A Lb: length of string B θ: 0.1 ∑ × ++− + = t t n N wtf avwdl wdl bbk kwtf FBM )log( ))1(( )1( 25 1 1 150 Table 3.",
        "Accuracies of titles in file properties File Type Domain Precision Recall F1 MS 0.299 0.311 0.305 DotCom 0.210 0.214 0.212Word DotGov 0.182 0.177 0.180 MS 0.229 0.245 0.237 DotCom 0.185 0.186 0.186PowerPoint DotGov 0.180 0.182 0.181 6.4 Comparison with Baselines We conducted title extraction from the first data set (Word and PowerPoint in MS).",
        "As the model, we used Perceptron.",
        "We conduct 4-fold cross validation.",
        "Thus, all the results reported here are those averaged over 4 trials.",
        "Tables 4 and 5 show the results.",
        "We see that Perceptron significantly outperforms the baselines.",
        "In the evaluation, we use exact matching between the true titles annotated by humans and the extracted titles.",
        "Table 4.",
        "Accuracies of title extraction with Word Precision Recall F1 Model Perceptron 0.810 0.837 0.823 Largest font size 0.700 0.758 0.727 Baselines First line 0.707 0.767 0.736 Table 5.",
        "Accuracies of title extraction with PowerPoint Precision Recall F1 Model Perceptron 0.875 0. 895 0.885 Largest font size 0.844 0.887 0.865 Baselines First line 0.639 0.671 0.655 We see that the machine learning approach can achieve good performance in title extraction.",
        "For Word documents both precision and recall of the approach are 8 percent higher than those of the baselines.",
        "For PowerPoint both precision and recall of the approach are 2 percent higher than those of the baselines.",
        "We conduct significance tests.",
        "The results are shown in Table 6.",
        "Here, Largest denotes the baseline of using the largest font size, First denotes the baseline of using the first line.",
        "The results indicate that the improvements of machine learning over baselines are statistically significant (in the sense p-value < 0.05) Table 6.",
        "Sign test results Documents Type Sign test between p-value Perceptron vs. Largest 3.59e-26 Word Perceptron vs. First 7.12e-10 Perceptron vs. Largest 0.010 PowerPoint Perceptron vs. First 5.13e-40 We see, from the results, that the two baselines can work well for title extraction, suggesting that font size and position information are most useful features for title extraction.",
        "However, it is also obvious that using only these two features is not enough.",
        "There are cases in which all the lines have the same font size (i.e., the largest font size), or cases in which the lines with the largest font size only contain general descriptions like Confidential, White paper, etc.",
        "For those cases, the largest font size method cannot work well.",
        "For similar reasons, the first line method alone cannot work well, either.",
        "With the combination of different features (evidence in title judgment), Perceptron can outperform Largest and First.",
        "We investigate the performance of solely using linguistic features.",
        "We found that it does not work well.",
        "It seems that the format features play important roles and the linguistic features are supplements..",
        "Figure 8.",
        "An example Word document.",
        "Figure 9.",
        "An example PowerPoint document.",
        "We conducted an error analysis on the results of Perceptron.",
        "We found that the errors fell into three categories. (1) About one third of the errors were related to hard cases.",
        "In these documents, the layouts of the first pages were difficult to understand, even for humans.",
        "Figure 8 and 9 shows examples. (2) Nearly one fourth of the errors were from the documents which do not have true titles but only contain bullets.",
        "Since we conduct extraction from the top regions, it is difficult to get rid of these errors with the current approach. (3).",
        "Confusions between main titles and subtitles were another type of error.",
        "Since we only labeled the main titles as titles, the extractions of both titles were considered incorrect.",
        "This type of error does little harm to document processing like search, however. 6.5 Comparison between Models To compare the performance of different machine learning models, we conducted another experiment.",
        "Again, we perform 4-fold cross 151 validation on the first data set (MS).",
        "Table 7, 8 shows the results of all the four models.",
        "It turns out that Perceptron and PMM perform the best, followed by MEMM, and ME performs the worst.",
        "In general, the Markovian models perform better than or as well as their classifier counterparts.",
        "This seems to be because the Markovian models are trained globally, while the classifiers are trained locally.",
        "The Perceptron based models perform better than the ME based counterparts.",
        "This seems to be because the Perceptron based models are created to make better classifications, while ME models are constructed for better prediction.",
        "Table 7.",
        "Comparison between different learning models for title extraction with Word Model Precision Recall F1 Perceptron 0.810 0.837 0.823 MEMM 0.797 0.824 0.810 PMM 0.827 0.823 0.825 ME 0.801 0.621 0.699 Table 8.",
        "Comparison between different learning models for title extraction with PowerPoint Model Precision Recall F1 Perceptron 0.875 0. 895 0. 885 MEMM 0.841 0.861 0.851 PMM 0.873 0.896 0.885 ME 0.753 0.766 0.759 6.6 Domain Adaptation We apply the model trained with the first data set (MS) to the second data set (DotCom and DotGov).",
        "Tables 9-12 show the results.",
        "Table 9.",
        "Accuracies of title extraction with Word in DotGov Precision Recall F1 Model Perceptron 0.716 0.759 0.737 Largest font size 0.549 0.619 0.582Baselines First line 0.462 0.521 0.490 Table 10.",
        "Accuracies of title extraction with PowerPoint in DotGov Precision Recall F1 Model Perceptron 0.900 0.906 0.903 Largest font size 0.871 0.888 0.879Baselines First line 0.554 0.564 0.559 Table 11.",
        "Accuracies of title extraction with Word in DotCom Precisio n Recall F1 Model Perceptron 0.832 0.880 0.855 Largest font size 0.676 0.753 0.712Baselines First line 0.577 0.643 0.608 Table 12.",
        "Performance of PowerPoint document title extraction in DotCom Precisio n Recall F1 Model Perceptron 0.910 0.903 0.907 Largest font size 0.864 0.886 0.875Baselines First line 0.570 0.585 0.577 From the results, we see that the models can be adapted to different domains well.",
        "There is almost no drop in accuracy.",
        "The results indicate that the patterns of title formats exist across different domains, and it is possible to construct a domain independent model by mainly using formatting information. 6.7 Language Adaptation We apply the model trained with the data in English (MS) to the data set in Chinese.",
        "Tables 13-14 show the results.",
        "Table 13.",
        "Accuracies of title extraction with Word in Chinese Precision Recall F1 Model Perceptron 0.817 0.805 0.811 Largest font size 0.722 0.755 0.738Baselines First line 0.743 0.777 0.760 Table 14.",
        "Accuracies of title extraction with PowerPoint in Chinese Precision Recall F1 Model Perceptron 0.766 0.812 0.789 Largest font size 0.753 0.813 0.782Baselines First line 0.627 0.676 0.650 We see that the models can be adapted to a different language.",
        "There are only small drops in accuracy.",
        "Obviously, the linguistic features do not work for Chinese, but the effect of not using them is negligible.",
        "The results indicate that the patterns of title formats exist across different languages.",
        "From the domain adaptation and language adaptation results, we conclude that the use of formatting information is the key to a successful extraction from general documents. 6.8 Search with Extracted Titles We performed experiments on using title extraction for document retrieval.",
        "As a baseline, we employed BM25 without using extracted titles.",
        "The ranking mechanism was as described in Section 5.",
        "The weights were heuristically set.",
        "We did not conduct optimization on the weights.",
        "The evaluation was conducted on a corpus of 1.3 M documents crawled from the intranet of Microsoft using 100 evaluation queries obtained from this intranets search engine query logs. 50 queries were from the most popular set, while 50 queries other were chosen randomly.",
        "Users were asked to provide judgments of the degree of document relevance from a scale of 1to 5 (1 meaning detrimental, 2 - bad, 3 - fair, 4 - good and 5 - excellent). 152 Figure 10 shows the results.",
        "In the chart two sets of precision results were obtained by either considering good or excellent documents as relevant (left 3 bars with relevance threshold 0.5), or by considering only excellent documents as relevant (right 3 bars with relevance threshold 1.0) 0 0.05 0.1 0.15 0.2 0.25 0.3 0.35 0.4 0.45 P@10 P@5 Reciprocal P@10 P@5 Reciprocal 0.5 1 BM25 Anchor, Title, Body BM25 Anchor, Title, Body, ExtractedTitle Name All RelevanceThreshold Data Description Figure 10.",
        "Search ranking results.",
        "Figure 10 shows different document retrieval results with different ranking functions in terms of precision @10, precision @5 and reciprocal rank: • Blue bar - BM25 including the fields body, title (file property), and anchor text. • Purple bar - BM25 including the fields body, title (file property), anchor text, and extracted title.",
        "With the additional field of extracted title included in BM25 the precision @10 increased from 0.132 to 0.145, or by ~10%.",
        "Thus, it is safe to say that the use of extracted title can indeed improve the precision of document retrieval. 7.",
        "CONCLUSION In this paper, we have investigated the problem of automatically extracting titles from general documents.",
        "We have tried using a machine learning approach to address the problem.",
        "Previous work showed that the machine learning approach can work well for metadata extraction from research papers.",
        "In this paper, we showed that the approach can work for extraction from general documents as well.",
        "Our experimental results indicated that the machine learning approach can work significantly better than the baselines in title extraction from Office documents.",
        "Previous work on metadata extraction mainly used linguistic features in documents, while we mainly used formatting information.",
        "It appeared that using formatting information is a key for successfully conducting title extraction from general documents.",
        "We tried different machine learning models including Perceptron, Maximum Entropy, Maximum Entropy Markov Model, and Voted Perceptron.",
        "We found that the performance of the Perceptorn models was the best.",
        "We applied models constructed in one domain to another domain and applied models trained in one language to another language.",
        "We found that the accuracies did not drop substantially across different domains and across different languages, indicating that the models were generic.",
        "We also attempted to use the extracted titles in document retrieval.",
        "We observed a significant improvement in document ranking performance for search when using extracted title information.",
        "All the above investigations were not conducted in previous work, and through our investigations we verified the generality and the significance of the title extraction approach. 8.",
        "ACKNOWLEDGEMENTS We thank Chunyu Wei and Bojuan Zhao for their work on data annotation.",
        "We acknowledge Jinzhu Li for his assistance in conducting the experiments.",
        "We thank Ming Zhou, John Chen, Jun Xu, and the anonymous reviewers of JCDL05 for their valuable comments on this paper. 9.",
        "REFERENCES [1] Berger, A. L., Della Pietra, S. A., and Della Pietra, V. J.",
        "A maximum entropy approach to natural language processing.",
        "Computational Linguistics, 22:39-71, 1996. [2] Collins, M. Discriminative training methods for hidden markov models: theory and experiments with perceptron algorithms.",
        "In Proceedings of Conference on Empirical Methods in Natural Language Processing, 1-8, 2002. [3] Cortes, C. and Vapnik, V. Support-vector networks.",
        "Machine Learning, 20:273-297, 1995. [4] Chieu, H. L. and Ng, H. T. A maximum entropy approach to information extraction from semi-structured and free text.",
        "In Proceedings of the Eighteenth National Conference on Artificial Intelligence, 768-791, 2002. [5] Evans, D. K., Klavans, J. L., and McKeown, K. R. Columbia newsblaster: multilingual news summarization on the Web.",
        "In Proceedings of Human Language Technology conference / North American chapter of the Association for Computational Linguistics annual meeting, 1-4, 2004. [6] Ghahramani, Z. and Jordan, M. I. Factorial hidden markov models.",
        "Machine Learning, 29:245-273, 1997. [7] Gheel, J. and Anderson, T. Data and metadata for finding and reminding, In Proceedings of the 1999 International Conference on Information Visualization, 446-451,1999. [8] Giles, C. L., Petinot, Y., Teregowda P. B., Han, H., Lawrence, S., Rangaswamy, A., and Pal, N. eBizSearch: a niche search engine for e-Business.",
        "In Proceedings of the 26th Annual International ACM SIGIR Conference on Research and Development in Information Retrieval, 413414, 2003. [9] Giuffrida, G., Shek, E. C., and Yang, J. Knowledge-based metadata extraction from PostScript files.",
        "In Proceedings of the Fifth ACM Conference on Digital Libraries, 77-84, 2000. [10] Han, H., Giles, C. L., Manavoglu, E., Zha, H., Zhang, Z., and Fox, E. A.",
        "Automatic document metadata extraction using support vector machines.",
        "In Proceedings of the Third ACM/IEEE-CS Joint Conference on Digital Libraries, 37-48, 2003. [11] Kobayashi, M., and Takeda, K. Information retrieval on the Web.",
        "ACM Computing Surveys, 32:144-173, 2000. [12] Lafferty, J., McCallum, A., and Pereira, F. Conditional random fields: probabilistic models for segmenting and 153 labeling sequence data.",
        "In Proceedings of the Eighteenth International Conference on Machine Learning, 282-289, 2001. [13] Li, Y., Zaragoza, H., Herbrich, R., Shawe-Taylor J., and Kandola, J. S. The perceptron algorithm with uneven margins.",
        "In Proceedings of the Nineteenth International Conference on Machine Learning, 379-386, 2002. [14] Liddy, E. D., Sutton, S., Allen, E., Harwell, S., Corieri, S., Yilmazel, O., Ozgencil, N. E., Diekema, A., McCracken, N., and Silverstein, J.",
        "Automatic Metadata generation & evaluation.",
        "In Proceedings of the 25th Annual International ACM SIGIR Conference on Research and Development in Information Retrieval, 401-402, 2002. [15] Littlefield, A.",
        "Effective enterprise information retrieval across new content formats.",
        "In Proceedings of the Seventh Search Engine Conference, http://www.infonortics.com/searchengines/sh02/02prog.html, 2002. [16] Mao, S., Kim, J. W., and Thoma, G. R. A dynamic feature generation system for automated metadata extraction in preservation of digital materials.",
        "In Proceedings of the First International Workshop on Document Image Analysis for Libraries, 225-232, 2004. [17] McCallum, A., Freitag, D., and Pereira, F. Maximum entropy markov models for information extraction and segmentation.",
        "In Proceedings of the Seventeenth International Conference on Machine Learning, 591-598, 2000. [18] Murphy, L. D. Digital document metadata in organizations: roles, analytical approaches, and future research directions.",
        "In Proceedings of the Thirty-First Annual Hawaii International Conference on System Sciences, 267-276, 1998. [19] Pinto, D., McCallum, A., Wei, X., and Croft, W. B.",
        "Table extraction using conditional random fields.",
        "In Proceedings of the 26th Annual International ACM SIGIR Conference on Research and Development in Information Retrieval, 235242, 2003. [20] Ratnaparkhi, A. Unsupervised statistical models for prepositional phrase attachment.",
        "In Proceedings of the Seventeenth International Conference on Computational Linguistics. 1079-1085, 1998. [21] Robertson, S., Zaragoza, H., and Taylor, M. Simple BM25 extension to multiple weighted fields, In Proceedings of ACM Thirteenth Conference on Information and Knowledge Management, 42-49, 2004. [22] Yi, J. and Sundaresan, N. Metadata based Web mining for relevance, In Proceedings of the 2000 International Symposium on Database Engineering & Applications, 113121, 2000. [23] Yilmazel, O., Finneran, C. M., and Liddy, E. D. MetaExtract: An NLP system to automatically assign metadata.",
        "In Proceedings of the 2004 Joint ACM/IEEE Conference on Digital Libraries, 241-242, 2004. [24] Zhang, J. and Dimitroff, A. Internet search engines response to metadata Dublin Core implementation.",
        "Journal of Information Science, 30:310-320, 2004. [25] Zhang, L., Pan, Y., and Zhang, T. Recognising and using named entities: focused named entity recognition using machine learning.",
        "In Proceedings of the 27th Annual International ACM SIGIR Conference on Research and Development in Information Retrieval, 281-288, 2004. [26] http://dublincore.org/groups/corporate/Seattle/ 154"
    ],
    "error_count": 0,
    "keys": {
        "title extraction": {
            "translated_key": "extracción de título",
            "is_in_text": true,
            "original_annotated_sentences": [
                "Automatic Extraction of Titles from General Documents using Machine Learning Yunhua Hu1 Computer Science Department Xian Jiaotong University No 28, Xianning West Road Xian, China, 710049 yunhuahu@mail.xjtu.edu.cn Hang Li, Yunbo Cao Microsoft Research Asia 5F Sigma Center, No.",
                "49 Zhichun Road, Haidian, Beijing, China, 100080 {hangli,yucao}@microsoft.com Qinghua Zheng Computer Science Department Xian Jiaotong University No 28, Xianning West Road Xian, China, 710049 qhzheng@mail.xjtu.edu.cn Dmitriy Meyerzon Microsoft Corporation One Microsoft Way Redmond, WA, USA, 98052 dmitriym@microsoft.com ABSTRACT In this paper, we propose a machine learning approach to <br>title extraction</br> from general documents.",
                "By general documents, we mean documents that can belong to any one of a number of specific genres, including presentations, book chapters, technical papers, brochures, reports, and letters.",
                "Previously, methods have been proposed mainly for <br>title extraction</br> from research papers.",
                "It has not been clear whether it could be possible to conduct automatic <br>title extraction</br> from general documents.",
                "As a case study, we consider extraction from Office including Word and PowerPoint.",
                "In our approach, we annotate titles in sample documents (for Word and PowerPoint respectively) and take them as training data, train machine learning models, and perform <br>title extraction</br> using the trained models.",
                "Our method is unique in that we mainly utilize formatting information such as font size as features in the models.",
                "It turns out that the use of formatting information can lead to quite accurate extraction from general documents.",
                "Precision and recall for <br>title extraction</br> from Word is 0.810 and 0.837 respectively, and precision and recall for <br>title extraction</br> from PowerPoint is 0.875 and 0.895 respectively in an experiment on intranet data.",
                "Other important new findings in this work include that we can train models in one domain and apply them to another domain, and more surprisingly we can even train models in one language and apply them to another language.",
                "Moreover, we can significantly improve search ranking results in document retrieval by using the extracted titles.",
                "Categories and Subject Descriptors H.3.3 [Information Storage and Retrieval]: Information Search and Retrieval - Search Process; H.4.1 [Information Systems Applications]: Office Automation - Word processing; D.2.8 [Software Engineering]: Metrics - complexity measures, performance measures General Terms Algorithms, Experimentation, Performance. 1.",
                "INTRODUCTION Metadata of documents is useful for many kinds of document processing such as search, browsing, and filtering.",
                "Ideally, metadata is defined by the authors of documents and is then used by various systems.",
                "However, people seldom define document metadata by themselves, even when they have convenient metadata definition tools [26].",
                "Thus, how to automatically extract metadata from the bodies of documents turns out to be an important research issue.",
                "Methods for performing the task have been proposed.",
                "However, the focus was mainly on extraction from research papers.",
                "For instance, Han et al. [10] proposed a machine learning based method to conduct extraction from research papers.",
                "They formalized the problem as that of classification and employed Support Vector Machines as the classifier.",
                "They mainly used linguistic features in the model.1 In this paper, we consider metadata extraction from general documents.",
                "By general documents, we mean documents that may belong to any one of a number of specific genres.",
                "General documents are more widely available in digital libraries, intranets and the internet, and thus investigation on extraction from them is sorely needed.",
                "Research papers usually have well-formed styles and noticeable characteristics.",
                "In contrast, the styles of general documents can vary greatly.",
                "It has not been clarified whether a machine learning based approach can work well for this task.",
                "There are many types of metadata: title, author, date of creation, etc.",
                "As a case study, we consider <br>title extraction</br> in this paper.",
                "General documents can be in many different file formats: Microsoft Office, PDF (PS), etc.",
                "As a case study, we consider extraction from Office including Word and PowerPoint.",
                "We take a machine learning approach.",
                "We annotate titles in sample documents (for Word and PowerPoint respectively) and take them as training data to train several types of models, and perform <br>title extraction</br> using any one type of the trained models.",
                "In the models, we mainly utilize formatting information such as font size as features.",
                "We employ the following models: Maximum Entropy Model, Perceptron with Uneven Margins, Maximum Entropy Markov Model, and Voted Perceptron.",
                "In this paper, we also investigate the following three problems, which did not seem to have been examined previously. (1) Comparison between models: among the models above, which model performs best for <br>title extraction</br>; (2) Generality of model: whether it is possible to train a model on one domain and apply it to another domain, and whether it is possible to train a model in one language and apply it to another language; (3) Usefulness of extracted titles: whether extracted titles can improve document processing such as search.",
                "Experimental results indicate that our approach works well for <br>title extraction</br> from general documents.",
                "Our method can significantly outperform the baselines: one that always uses the first lines as titles and the other that always uses the lines in the largest font sizes as titles.",
                "Precision and recall for <br>title extraction</br> from Word are 0.810 and 0.837 respectively, and precision and recall for <br>title extraction</br> from PowerPoint are 0.875 and 0.895 respectively.",
                "It turns out that the use of format features is the key to successful <br>title extraction</br>. (1) We have observed that Perceptron based models perform better in terms of extraction accuracies. (2) We have empirically verified that the models trained with our approach are generic in the sense that they can be trained on one domain and applied to another, and they can be trained in one language and applied to another. (3) We have found that using the extracted titles we can significantly improve precision of document retrieval (by 10%).",
                "We conclude that we can indeed conduct reliable <br>title extraction</br> from general documents and use the extracted results to improve real applications.",
                "The rest of the paper is organized as follows.",
                "In section 2, we introduce related work, and in section 3, we explain the motivation and problem setting of our work.",
                "In section 4, we describe our method of <br>title extraction</br>, and in section 5, we describe our method of document retrieval using extracted titles.",
                "Section 6 gives our experimental results.",
                "We make concluding remarks in section 7. 2.",
                "RELATED WORK 2.1 Document Metadata Extraction Methods have been proposed for performing automatic metadata extraction from documents; however, the main focus was on extraction from research papers.",
                "The proposed methods fall into two categories: the rule based approach and the machine learning based approach.",
                "Giuffrida et al. [9], for instance, developed a rule-based system for automatically extracting metadata from research papers in Postscript.",
                "They used rules like titles are usually located on the upper portions of the first pages and they are usually in the largest font sizes.",
                "Liddy et al. [14] and Yilmazel el al. [23] performed metadata extraction from educational materials using rule-based natural language processing technologies.",
                "Mao et al. [16] also conducted automatic metadata extraction from research papers using rules on formatting information.",
                "The rule-based approach can achieve high performance.",
                "However, it also has disadvantages.",
                "It is less adaptive and robust when compared with the machine learning approach.",
                "Han et al. [10], for instance, conducted metadata extraction with the machine learning approach.",
                "They viewed the problem as that of classifying the lines in a document into the categories of metadata and proposed using Support Vector Machines as the classifier.",
                "They mainly used linguistic information as features.",
                "They reported high extraction accuracy from research papers in terms of precision and recall. 2.2 Information Extraction Metadata extraction can be viewed as an application of information extraction, in which given a sequence of instances, we identify a subsequence that represents information in which we are interested.",
                "Hidden Markov Model [6], Maximum Entropy Model [1, 4], Maximum Entropy Markov Model [17], Support Vector Machines [3], Conditional Random Field [12], and Voted Perceptron [2] are widely used information extraction models.",
                "Information extraction has been applied, for instance, to part-ofspeech tagging [20], named entity recognition [25] and table extraction [19]. 2.3 Search Using Title Information Title information is useful for document retrieval.",
                "In the system Citeseer, for instance, Giles et al. managed to extract titles from research papers and make use of the extracted titles in metadata search of papers [8].",
                "In web search, the title fields (i.e., file properties) and anchor texts of web pages (HTML documents) can be viewed as titles of the pages [5].",
                "Many search engines seem to utilize them for web page retrieval [7, 11, 18, 22].",
                "Zhang et al., found that web pages with well-defined metadata are more easily retrieved than those without well-defined metadata [24].",
                "To the best of our knowledge, no research has been conducted on using extracted titles from general documents (e.g., Office documents) for search of the documents. 146 3.",
                "MOTIVATION AND PROBLEM SETTING We consider the issue of automatically extracting titles from general documents.",
                "By general documents, we mean documents that belong to one of any number of specific genres.",
                "The documents can be presentations, books, book chapters, technical papers, brochures, reports, memos, specifications, letters, announcements, or resumes.",
                "General documents are more widely available in digital libraries, intranets, and internet, and thus investigation on <br>title extraction</br> from them is sorely needed.",
                "Figure 1 shows an estimate on distributions of file formats on intranet and internet [15].",
                "Office and PDF are the main file formats on the intranet.",
                "Even on the internet, the documents in the formats are still not negligible, given its extremely large size.",
                "In this paper, without loss of generality, we take Office documents as an example.",
                "Figure 1.",
                "Distributions of file formats in internet and intranet.",
                "For Office documents, users can define titles as file properties using a feature provided by Office.",
                "We found in an experiment, however, that users seldom use the feature and thus titles in file properties are usually very inaccurate.",
                "That is to say, titles in file properties are usually inconsistent with the true titles in the file bodies that are created by the authors and are visible to readers.",
                "We collected 6,000 Word and 6,000 PowerPoint documents from an intranet and the internet and examined how many titles in the file properties are correct.",
                "We found that surprisingly the accuracy was only 0.265 (cf., Section 6.3 for details).",
                "A number of reasons can be considered.",
                "For example, if one creates a new file by copying an old file, then the file property of the new file will also be copied from the old file.",
                "In another experiment, we found that Google uses the titles in file properties of Office documents in search and browsing, but the titles are not very accurate.",
                "We created 50 queries to search Word and PowerPoint documents and examined the top 15 results of each query returned by Google.",
                "We found that nearly all the titles presented in the search results were from the file properties of the documents.",
                "However, only 0.272 of them were correct.",
                "Actually, true titles usually exist at the beginnings of the bodies of documents.",
                "If we can accurately extract the titles from the bodies of documents, then we can exploit reliable title information in document processing.",
                "This is exactly the problem we address in this paper.",
                "More specifically, given a Word document, we are to extract the title from the top region of the first page.",
                "Given a PowerPoint document, we are to extract the title from the first slide.",
                "A title sometimes consists of a main title and one or two subtitles.",
                "We only consider extraction of the main title.",
                "As baselines for <br>title extraction</br>, we use that of always using the first lines as titles and that of always using the lines with largest font sizes as titles.",
                "Figure 2.",
                "<br>title extraction</br> from Word document.",
                "Figure 3.",
                "<br>title extraction</br> from PowerPoint document.",
                "Next, we define a specification for human judgments in title data annotation.",
                "The annotated data will be used in training and testing of the <br>title extraction</br> methods.",
                "Summary of the specification: The title of a document should be identified on the basis of common sense, if there is no difficulty in the identification.",
                "However, there are many cases in which the identification is not easy.",
                "There are some rules defined in the specification that guide identification for such cases.",
                "The rules include a title is usually in consecutive lines in the same format, a document can have no title, titles in images are not considered, a title should not contain words like draft, 147 whitepaper, etc, if it is difficult to determine which is the title, select the one in the largest font size, and if it is still difficult to determine which is the title, select the first candidate. (The specification covers all the cases we have encountered in data annotation.)",
                "Figures 2 and 3 show examples of Office documents from which we conduct <br>title extraction</br>.",
                "In Figure 2, Differences in Win32 API Implementations among Windows Operating Systems is the title of the Word document.",
                "Microsoft Windows on the top of this page is a picture and thus is ignored.",
                "In Figure 3, Building Competitive Advantages through an Agile Infrastructure is the title of the PowerPoint document.",
                "We have developed a tool for annotation of titles by human annotators.",
                "Figure 4 shows a snapshot of the tool.",
                "Figure 4.",
                "Title annotation tool. 4.",
                "<br>title extraction</br> METHOD 4.1 Outline <br>title extraction</br> based on machine learning consists of training and extraction.",
                "The same pre-processing step occurs before training and extraction.",
                "During pre-processing, from the top region of the first page of a Word document or the first slide of a PowerPoint document a number of units for processing are extracted.",
                "If a line (lines are separated by return symbols) only has a single format, then the line will become a unit.",
                "If a line has several parts and each of them has its own format, then each part will become a unit.",
                "Each unit will be treated as an instance in learning.",
                "A unit contains not only content information (linguistic information) but also formatting information.",
                "The input to pre-processing is a document and the output of pre-processing is a sequence of units (instances).",
                "Figure 5 shows the units obtained from the document in Figure 2.",
                "Figure 5.",
                "Example of units.",
                "In learning, the input is sequences of units where each sequence corresponds to a document.",
                "We take labeled units (labeled as title_begin, title_end, or other) in the sequences as training data and construct models for identifying whether a unit is title_begin title_end, or other.",
                "We employ four types of models: Perceptron, Maximum Entropy (ME), Perceptron Markov Model (PMM), and Maximum Entropy Markov Model (MEMM).",
                "In extraction, the input is a sequence of units from one document.",
                "We employ one type of model to identify whether a unit is title_begin, title_end, or other.",
                "We then extract units from the unit labeled with title_begin to the unit labeled with title_end.",
                "The result is the extracted title of the document.",
                "The unique characteristic of our approach is that we mainly utilize formatting information for <br>title extraction</br>.",
                "Our assumption is that although general documents vary in styles, their formats have certain patterns and we can learn and utilize the patterns for <br>title extraction</br>.",
                "This is in contrast to the work by Han et al., in which only linguistic features are used for extraction from research papers. 4.2 Models The four models actually can be considered in the same metadata extraction framework.",
                "That is why we apply them together to our current problem.",
                "Each input is a sequence of instances kxxx L21 together with a sequence of labels kyyy L21 . ix and iy represents an instance and its label, respectively ( ki ,,2,1 L= ).",
                "Recall that an instance here represents a unit.",
                "A label represents title_begin, title_end, or other.",
                "Here, k is the number of units in a document.",
                "In learning, we train a model which can be generally denoted as a conditional probability distribution )|( 11 kk XXYYP LL where iX and iY denote random variables taking instance ix and label iy as values, respectively ( ki ,,2,1 L= ).",
                "Learning Tool Extraction Tool 21121 2222122221 1121111211 nknnknn kk kk yyyxxx yyyxxx yyyxxx LL LL LL LL → → → )|(maxarg 11 mkmmkm xxyyP LL )|( 11 kk XXYYP LL Conditional Distribution mkmm xxx L21 Figure 6.",
                "Metadata extraction model.",
                "We can make assumptions about the general model in order to make it simple enough for training. 148 For example, we can assume that kYY ,,1 L are independent of each other given kXX ,,1 L .",
                "Thus, we have )|()|( )|( 11 11 kk kk XYPXYP XXYYP L LL = In this way, we decompose the model into a number of classifiers.",
                "We train the classifiers locally using the labeled data.",
                "As the classifier, we employ the Perceptron or Maximum Entropy model.",
                "We can also assume that the first order Markov property holds for kYY ,,1 L given kXX ,,1 L .",
                "Thus, we have )|()|( )|( 111 11 kkk kk XYYPXYP XXYYP −= L LL Again, we obtain a number of classifiers.",
                "However, the classifiers are conditioned on the previous label.",
                "When we employ the Percepton or Maximum Entropy model as a classifier, the models become a Percepton Markov Model or Maximum Entropy Markov Model, respectively.",
                "That is to say, the two models are more precise.",
                "In extraction, given a new sequence of instances, we resort to one of the constructed models to assign a sequence of labels to the sequence of instances, i.e., perform extraction.",
                "For Perceptron and ME, we assign labels locally and combine the results globally later using heuristics.",
                "Specifically, we first identify the most likely title_begin.",
                "Then we find the most likely title_end within three units after the title_begin.",
                "Finally, we extract as a title the units between the title_begin and the title_end.",
                "For PMM and MEMM, we employ the Viterbi algorithm to find the globally optimal label sequence.",
                "In this paper, for Perceptron, we actually employ an improved variant of it, called Perceptron with Uneven Margin [13].",
                "This version of Perceptron can work well especially when the number of positive instances and the number of negative instances differ greatly, which is exactly the case in our problem.",
                "We also employ an improved version of Perceptron Markov Model in which the Perceptron model is the so-called Voted Perceptron [2].",
                "In addition, in training, the parameters of the model are updated globally rather than locally. 4.3 Features There are two types of features: format features and linguistic features.",
                "We mainly use the former.",
                "The features are used for both the title-begin and the title-end classifiers. 4.3.1 Format Features Font Size: There are four binary features that represent the normalized font size of the unit (recall that a unit has only one type of font).",
                "If the font size of the unit is the largest in the document, then the first feature will be 1, otherwise 0.",
                "If the font size is the smallest in the document, then the fourth feature will be 1, otherwise 0.",
                "If the font size is above the average font size and not the largest in the document, then the second feature will be 1, otherwise 0.",
                "If the font size is below the average font size and not the smallest, the third feature will be 1, otherwise 0.",
                "It is necessary to conduct normalization on font sizes.",
                "For example, in one document the largest font size might be 12pt, while in another the smallest one might be 18pt.",
                "Boldface: This binary feature represents whether or not the current unit is in boldface.",
                "Alignment: There are four binary features that respectively represent the location of the current unit: left, center, right, and unknown alignment.",
                "The following format features with respect to context play an important role in <br>title extraction</br>.",
                "Empty Neighboring Unit: There are two binary features that represent, respectively, whether or not the previous unit and the current unit are blank lines.",
                "Font Size Change: There are two binary features that represent, respectively, whether or not the font size of the previous unit and the font size of the next unit differ from that of the current unit.",
                "Alignment Change: There are two binary features that represent, respectively, whether or not the alignment of the previous unit and the alignment of the next unit differ from that of the current one.",
                "Same Paragraph: There are two binary features that represent, respectively, whether or not the previous unit and the next unit are in the same paragraph as the current unit. 4.3.2 Linguistic Features The linguistic features are based on key words.",
                "Positive Word: This binary feature represents whether or not the current unit begins with one of the positive words.",
                "The positive words include title:, subject:, subject line: For example, in some documents the lines of titles and authors have the same formats.",
                "However, if lines begin with one of the positive words, then it is likely that they are title lines.",
                "Negative Word: This binary feature represents whether or not the current unit begins with one of the negative words.",
                "The negative words include To, By, created by, updated by, etc.",
                "There are more negative words than positive words.",
                "The above linguistic features are language dependent.",
                "Word Count: A title should not be too long.",
                "We heuristically create four intervals: [1, 2], [3, 6], [7, 9] and [9, ∞) and define one feature for each interval.",
                "If the number of words in a title falls into an interval, then the corresponding feature will be 1; otherwise 0.",
                "Ending Character: This feature represents whether the unit ends with :, -, or other special characters.",
                "A title usually does not end with such a character. 5.",
                "DOCUMENT RETRIEVAL METHOD We describe our method of document retrieval using extracted titles.",
                "Typically, in information retrieval a document is split into a number of fields including body, title, and anchor text.",
                "A ranking function in search can use different weights for different fields of 149 the document.",
                "Also, titles are typically assigned high weights, indicating that they are important for document retrieval.",
                "As explained previously, our experiment has shown that a significant number of documents actually have incorrect titles in the file properties, and thus in addition of using them we use the extracted titles as one more field of the document.",
                "By doing this, we attempt to improve the overall precision.",
                "In this paper, we employ a modification of BM25 that allows field weighting [21].",
                "As fields, we make use of body, title, extracted title and anchor.",
                "First, for each term in the query we count the term frequency in each field of the document; each field frequency is then weighted according to the corresponding weight parameter: ∑= f tfft tfwwtf Similarly, we compute the document length as a weighted sum of lengths of each field.",
                "Average document length in the corpus becomes the average of all weighted document lengths. ∑= f ff dlwwdl In our experiments we used 75.0,8.11 == bk .",
                "Weight for content was 1.0, title was 10.0, anchor was 10.0, and extracted title was 5.0. 6.",
                "EXPERIMENTAL RESULTS 6.1 Data Sets and Evaluation Measures We used two data sets in our experiments.",
                "First, we downloaded and randomly selected 5,000 Word documents and 5,000 PowerPoint documents from an intranet of Microsoft.",
                "We call it MS hereafter.",
                "Second, we downloaded and randomly selected 500 Word and 500 PowerPoint documents from the DotGov and DotCom domains on the internet, respectively.",
                "Figure 7 shows the distributions of the genres of the documents.",
                "We see that the documents are indeed general documents as we define them.",
                "Figure 7.",
                "Distributions of document genres.",
                "Third, a data set in Chinese was also downloaded from the internet.",
                "It includes 500 Word documents and 500 PowerPoint documents in Chinese.",
                "We manually labeled the titles of all the documents, on the basis of our specification.",
                "Not all the documents in the two data sets have titles.",
                "Table 1 shows the percentages of the documents having titles.",
                "We see that DotCom and DotGov have more PowerPoint documents with titles than MS.",
                "This might be because PowerPoint documents published on the internet are more formal than those on the intranet.",
                "Table 1.",
                "The portion of documents with titles Domain Type MS DotCom DotGov Word 75.7% 77.8% 75.6% PowerPoint 82.1% 93.4% 96.4% In our experiments, we conducted evaluations on <br>title extraction</br> in terms of precision, recall, and F-measure.",
                "The evaluation measures are defined as follows: Precision: P = A / ( A + B ) Recall: R = A / ( A + C ) F-measure: F1 = 2PR / ( P + R ) Here, A, B, C, and D are numbers of documents as those defined in Table 2.",
                "Table 2.",
                "Contingence table with regard to <br>title extraction</br> Is title Is not title Extracted A B Not extracted C D 6.2 Baselines We test the accuracies of the two baselines described in section 4.2.",
                "They are denoted as largest font size and first line respectively. 6.3 Accuracy of Titles in File Properties We investigate how many titles in the file properties of the documents are reliable.",
                "We view the titles annotated by humans as true titles and test how many titles in the file properties can approximately match with the true titles.",
                "We use Edit Distance to conduct the approximate match. (Approximate match is only used in this evaluation).",
                "This is because sometimes human annotated titles can be slightly different from the titles in file properties on the surface, e.g., contain extra spaces).",
                "Given string A and string B: if ( (D == 0) or ( D / ( La + Lb ) < θ ) ) then string A = string B D: Edit Distance between string A and string B La: length of string A Lb: length of string B θ: 0.1 ∑ × ++− + = t t n N wtf avwdl wdl bbk kwtf FBM )log( ))1(( )1( 25 1 1 150 Table 3.",
                "Accuracies of titles in file properties File Type Domain Precision Recall F1 MS 0.299 0.311 0.305 DotCom 0.210 0.214 0.212Word DotGov 0.182 0.177 0.180 MS 0.229 0.245 0.237 DotCom 0.185 0.186 0.186PowerPoint DotGov 0.180 0.182 0.181 6.4 Comparison with Baselines We conducted <br>title extraction</br> from the first data set (Word and PowerPoint in MS).",
                "As the model, we used Perceptron.",
                "We conduct 4-fold cross validation.",
                "Thus, all the results reported here are those averaged over 4 trials.",
                "Tables 4 and 5 show the results.",
                "We see that Perceptron significantly outperforms the baselines.",
                "In the evaluation, we use exact matching between the true titles annotated by humans and the extracted titles.",
                "Table 4.",
                "Accuracies of <br>title extraction</br> with Word Precision Recall F1 Model Perceptron 0.810 0.837 0.823 Largest font size 0.700 0.758 0.727 Baselines First line 0.707 0.767 0.736 Table 5.",
                "Accuracies of <br>title extraction</br> with PowerPoint Precision Recall F1 Model Perceptron 0.875 0. 895 0.885 Largest font size 0.844 0.887 0.865 Baselines First line 0.639 0.671 0.655 We see that the machine learning approach can achieve good performance in <br>title extraction</br>.",
                "For Word documents both precision and recall of the approach are 8 percent higher than those of the baselines.",
                "For PowerPoint both precision and recall of the approach are 2 percent higher than those of the baselines.",
                "We conduct significance tests.",
                "The results are shown in Table 6.",
                "Here, Largest denotes the baseline of using the largest font size, First denotes the baseline of using the first line.",
                "The results indicate that the improvements of machine learning over baselines are statistically significant (in the sense p-value < 0.05) Table 6.",
                "Sign test results Documents Type Sign test between p-value Perceptron vs. Largest 3.59e-26 Word Perceptron vs. First 7.12e-10 Perceptron vs. Largest 0.010 PowerPoint Perceptron vs. First 5.13e-40 We see, from the results, that the two baselines can work well for <br>title extraction</br>, suggesting that font size and position information are most useful features for <br>title extraction</br>.",
                "However, it is also obvious that using only these two features is not enough.",
                "There are cases in which all the lines have the same font size (i.e., the largest font size), or cases in which the lines with the largest font size only contain general descriptions like Confidential, White paper, etc.",
                "For those cases, the largest font size method cannot work well.",
                "For similar reasons, the first line method alone cannot work well, either.",
                "With the combination of different features (evidence in title judgment), Perceptron can outperform Largest and First.",
                "We investigate the performance of solely using linguistic features.",
                "We found that it does not work well.",
                "It seems that the format features play important roles and the linguistic features are supplements..",
                "Figure 8.",
                "An example Word document.",
                "Figure 9.",
                "An example PowerPoint document.",
                "We conducted an error analysis on the results of Perceptron.",
                "We found that the errors fell into three categories. (1) About one third of the errors were related to hard cases.",
                "In these documents, the layouts of the first pages were difficult to understand, even for humans.",
                "Figure 8 and 9 shows examples. (2) Nearly one fourth of the errors were from the documents which do not have true titles but only contain bullets.",
                "Since we conduct extraction from the top regions, it is difficult to get rid of these errors with the current approach. (3).",
                "Confusions between main titles and subtitles were another type of error.",
                "Since we only labeled the main titles as titles, the extractions of both titles were considered incorrect.",
                "This type of error does little harm to document processing like search, however. 6.5 Comparison between Models To compare the performance of different machine learning models, we conducted another experiment.",
                "Again, we perform 4-fold cross 151 validation on the first data set (MS).",
                "Table 7, 8 shows the results of all the four models.",
                "It turns out that Perceptron and PMM perform the best, followed by MEMM, and ME performs the worst.",
                "In general, the Markovian models perform better than or as well as their classifier counterparts.",
                "This seems to be because the Markovian models are trained globally, while the classifiers are trained locally.",
                "The Perceptron based models perform better than the ME based counterparts.",
                "This seems to be because the Perceptron based models are created to make better classifications, while ME models are constructed for better prediction.",
                "Table 7.",
                "Comparison between different learning models for <br>title extraction</br> with Word Model Precision Recall F1 Perceptron 0.810 0.837 0.823 MEMM 0.797 0.824 0.810 PMM 0.827 0.823 0.825 ME 0.801 0.621 0.699 Table 8.",
                "Comparison between different learning models for <br>title extraction</br> with PowerPoint Model Precision Recall F1 Perceptron 0.875 0. 895 0. 885 MEMM 0.841 0.861 0.851 PMM 0.873 0.896 0.885 ME 0.753 0.766 0.759 6.6 Domain Adaptation We apply the model trained with the first data set (MS) to the second data set (DotCom and DotGov).",
                "Tables 9-12 show the results.",
                "Table 9.",
                "Accuracies of <br>title extraction</br> with Word in DotGov Precision Recall F1 Model Perceptron 0.716 0.759 0.737 Largest font size 0.549 0.619 0.582Baselines First line 0.462 0.521 0.490 Table 10.",
                "Accuracies of <br>title extraction</br> with PowerPoint in DotGov Precision Recall F1 Model Perceptron 0.900 0.906 0.903 Largest font size 0.871 0.888 0.879Baselines First line 0.554 0.564 0.559 Table 11.",
                "Accuracies of <br>title extraction</br> with Word in DotCom Precisio n Recall F1 Model Perceptron 0.832 0.880 0.855 Largest font size 0.676 0.753 0.712Baselines First line 0.577 0.643 0.608 Table 12.",
                "Performance of PowerPoint document <br>title extraction</br> in DotCom Precisio n Recall F1 Model Perceptron 0.910 0.903 0.907 Largest font size 0.864 0.886 0.875Baselines First line 0.570 0.585 0.577 From the results, we see that the models can be adapted to different domains well.",
                "There is almost no drop in accuracy.",
                "The results indicate that the patterns of title formats exist across different domains, and it is possible to construct a domain independent model by mainly using formatting information. 6.7 Language Adaptation We apply the model trained with the data in English (MS) to the data set in Chinese.",
                "Tables 13-14 show the results.",
                "Table 13.",
                "Accuracies of <br>title extraction</br> with Word in Chinese Precision Recall F1 Model Perceptron 0.817 0.805 0.811 Largest font size 0.722 0.755 0.738Baselines First line 0.743 0.777 0.760 Table 14.",
                "Accuracies of <br>title extraction</br> with PowerPoint in Chinese Precision Recall F1 Model Perceptron 0.766 0.812 0.789 Largest font size 0.753 0.813 0.782Baselines First line 0.627 0.676 0.650 We see that the models can be adapted to a different language.",
                "There are only small drops in accuracy.",
                "Obviously, the linguistic features do not work for Chinese, but the effect of not using them is negligible.",
                "The results indicate that the patterns of title formats exist across different languages.",
                "From the domain adaptation and language adaptation results, we conclude that the use of formatting information is the key to a successful extraction from general documents. 6.8 Search with Extracted Titles We performed experiments on using <br>title extraction</br> for document retrieval.",
                "As a baseline, we employed BM25 without using extracted titles.",
                "The ranking mechanism was as described in Section 5.",
                "The weights were heuristically set.",
                "We did not conduct optimization on the weights.",
                "The evaluation was conducted on a corpus of 1.3 M documents crawled from the intranet of Microsoft using 100 evaluation queries obtained from this intranets search engine query logs. 50 queries were from the most popular set, while 50 queries other were chosen randomly.",
                "Users were asked to provide judgments of the degree of document relevance from a scale of 1to 5 (1 meaning detrimental, 2 - bad, 3 - fair, 4 - good and 5 - excellent). 152 Figure 10 shows the results.",
                "In the chart two sets of precision results were obtained by either considering good or excellent documents as relevant (left 3 bars with relevance threshold 0.5), or by considering only excellent documents as relevant (right 3 bars with relevance threshold 1.0) 0 0.05 0.1 0.15 0.2 0.25 0.3 0.35 0.4 0.45 P@10 P@5 Reciprocal P@10 P@5 Reciprocal 0.5 1 BM25 Anchor, Title, Body BM25 Anchor, Title, Body, ExtractedTitle Name All RelevanceThreshold Data Description Figure 10.",
                "Search ranking results.",
                "Figure 10 shows different document retrieval results with different ranking functions in terms of precision @10, precision @5 and reciprocal rank: • Blue bar - BM25 including the fields body, title (file property), and anchor text. • Purple bar - BM25 including the fields body, title (file property), anchor text, and extracted title.",
                "With the additional field of extracted title included in BM25 the precision @10 increased from 0.132 to 0.145, or by ~10%.",
                "Thus, it is safe to say that the use of extracted title can indeed improve the precision of document retrieval. 7.",
                "CONCLUSION In this paper, we have investigated the problem of automatically extracting titles from general documents.",
                "We have tried using a machine learning approach to address the problem.",
                "Previous work showed that the machine learning approach can work well for metadata extraction from research papers.",
                "In this paper, we showed that the approach can work for extraction from general documents as well.",
                "Our experimental results indicated that the machine learning approach can work significantly better than the baselines in <br>title extraction</br> from Office documents.",
                "Previous work on metadata extraction mainly used linguistic features in documents, while we mainly used formatting information.",
                "It appeared that using formatting information is a key for successfully conducting <br>title extraction</br> from general documents.",
                "We tried different machine learning models including Perceptron, Maximum Entropy, Maximum Entropy Markov Model, and Voted Perceptron.",
                "We found that the performance of the Perceptorn models was the best.",
                "We applied models constructed in one domain to another domain and applied models trained in one language to another language.",
                "We found that the accuracies did not drop substantially across different domains and across different languages, indicating that the models were generic.",
                "We also attempted to use the extracted titles in document retrieval.",
                "We observed a significant improvement in document ranking performance for search when using extracted title information.",
                "All the above investigations were not conducted in previous work, and through our investigations we verified the generality and the significance of the <br>title extraction</br> approach. 8.",
                "ACKNOWLEDGEMENTS We thank Chunyu Wei and Bojuan Zhao for their work on data annotation.",
                "We acknowledge Jinzhu Li for his assistance in conducting the experiments.",
                "We thank Ming Zhou, John Chen, Jun Xu, and the anonymous reviewers of JCDL05 for their valuable comments on this paper. 9.",
                "REFERENCES [1] Berger, A. L., Della Pietra, S. A., and Della Pietra, V. J.",
                "A maximum entropy approach to natural language processing.",
                "Computational Linguistics, 22:39-71, 1996. [2] Collins, M. Discriminative training methods for hidden markov models: theory and experiments with perceptron algorithms.",
                "In Proceedings of Conference on Empirical Methods in Natural Language Processing, 1-8, 2002. [3] Cortes, C. and Vapnik, V. Support-vector networks.",
                "Machine Learning, 20:273-297, 1995. [4] Chieu, H. L. and Ng, H. T. A maximum entropy approach to information extraction from semi-structured and free text.",
                "In Proceedings of the Eighteenth National Conference on Artificial Intelligence, 768-791, 2002. [5] Evans, D. K., Klavans, J. L., and McKeown, K. R. Columbia newsblaster: multilingual news summarization on the Web.",
                "In Proceedings of Human Language Technology conference / North American chapter of the Association for Computational Linguistics annual meeting, 1-4, 2004. [6] Ghahramani, Z. and Jordan, M. I. Factorial hidden markov models.",
                "Machine Learning, 29:245-273, 1997. [7] Gheel, J. and Anderson, T. Data and metadata for finding and reminding, In Proceedings of the 1999 International Conference on Information Visualization, 446-451,1999. [8] Giles, C. L., Petinot, Y., Teregowda P. B., Han, H., Lawrence, S., Rangaswamy, A., and Pal, N. eBizSearch: a niche search engine for e-Business.",
                "In Proceedings of the 26th Annual International ACM SIGIR Conference on Research and Development in Information Retrieval, 413414, 2003. [9] Giuffrida, G., Shek, E. C., and Yang, J. Knowledge-based metadata extraction from PostScript files.",
                "In Proceedings of the Fifth ACM Conference on Digital Libraries, 77-84, 2000. [10] Han, H., Giles, C. L., Manavoglu, E., Zha, H., Zhang, Z., and Fox, E. A.",
                "Automatic document metadata extraction using support vector machines.",
                "In Proceedings of the Third ACM/IEEE-CS Joint Conference on Digital Libraries, 37-48, 2003. [11] Kobayashi, M., and Takeda, K. Information retrieval on the Web.",
                "ACM Computing Surveys, 32:144-173, 2000. [12] Lafferty, J., McCallum, A., and Pereira, F. Conditional random fields: probabilistic models for segmenting and 153 labeling sequence data.",
                "In Proceedings of the Eighteenth International Conference on Machine Learning, 282-289, 2001. [13] Li, Y., Zaragoza, H., Herbrich, R., Shawe-Taylor J., and Kandola, J. S. The perceptron algorithm with uneven margins.",
                "In Proceedings of the Nineteenth International Conference on Machine Learning, 379-386, 2002. [14] Liddy, E. D., Sutton, S., Allen, E., Harwell, S., Corieri, S., Yilmazel, O., Ozgencil, N. E., Diekema, A., McCracken, N., and Silverstein, J.",
                "Automatic Metadata generation & evaluation.",
                "In Proceedings of the 25th Annual International ACM SIGIR Conference on Research and Development in Information Retrieval, 401-402, 2002. [15] Littlefield, A.",
                "Effective enterprise information retrieval across new content formats.",
                "In Proceedings of the Seventh Search Engine Conference, http://www.infonortics.com/searchengines/sh02/02prog.html, 2002. [16] Mao, S., Kim, J. W., and Thoma, G. R. A dynamic feature generation system for automated metadata extraction in preservation of digital materials.",
                "In Proceedings of the First International Workshop on Document Image Analysis for Libraries, 225-232, 2004. [17] McCallum, A., Freitag, D., and Pereira, F. Maximum entropy markov models for information extraction and segmentation.",
                "In Proceedings of the Seventeenth International Conference on Machine Learning, 591-598, 2000. [18] Murphy, L. D. Digital document metadata in organizations: roles, analytical approaches, and future research directions.",
                "In Proceedings of the Thirty-First Annual Hawaii International Conference on System Sciences, 267-276, 1998. [19] Pinto, D., McCallum, A., Wei, X., and Croft, W. B.",
                "Table extraction using conditional random fields.",
                "In Proceedings of the 26th Annual International ACM SIGIR Conference on Research and Development in Information Retrieval, 235242, 2003. [20] Ratnaparkhi, A. Unsupervised statistical models for prepositional phrase attachment.",
                "In Proceedings of the Seventeenth International Conference on Computational Linguistics. 1079-1085, 1998. [21] Robertson, S., Zaragoza, H., and Taylor, M. Simple BM25 extension to multiple weighted fields, In Proceedings of ACM Thirteenth Conference on Information and Knowledge Management, 42-49, 2004. [22] Yi, J. and Sundaresan, N. Metadata based Web mining for relevance, In Proceedings of the 2000 International Symposium on Database Engineering & Applications, 113121, 2000. [23] Yilmazel, O., Finneran, C. M., and Liddy, E. D. MetaExtract: An NLP system to automatically assign metadata.",
                "In Proceedings of the 2004 Joint ACM/IEEE Conference on Digital Libraries, 241-242, 2004. [24] Zhang, J. and Dimitroff, A. Internet search engines response to metadata Dublin Core implementation.",
                "Journal of Information Science, 30:310-320, 2004. [25] Zhang, L., Pan, Y., and Zhang, T. Recognising and using named entities: focused named entity recognition using machine learning.",
                "In Proceedings of the 27th Annual International ACM SIGIR Conference on Research and Development in Information Retrieval, 281-288, 2004. [26] http://dublincore.org/groups/corporate/Seattle/ 154"
            ],
            "original_annotated_samples": [],
            "translated_annotated_samples": [
                "49 Zhichun Road, Haidian, Beijing, China, 100080 {Hangli, Yucao}@Microsoft.com Qinghua de Departamento de Ciencias de la Computación Xian Jiaotong No.Microsoft Corporation One Microsoft Way Redmond, WA, EE. UU., 98052 dmitriym@microsoft.com Resumen En este documento, proponemos un enfoque de aprendizaje automático para la \"extracción de títulos\" de documentos generales.",
                "Anteriormente, se han propuesto métodos principalmente para la \"extracción de título\" de los trabajos de investigación.",
                "No ha estado claro si podría ser posible realizar una \"extracción de título\" automática de documentos generales.",
                "En nuestro enfoque, anotamos títulos en documentos de muestra (para Word y PowerPoint, respectivamente) y los tomamos como datos de entrenamiento, entrenan modelos de aprendizaje automático y realizamos una \"extracción de títulos\" utilizando los modelos entrenados.",
                "La precisión y el recuerdo para la \"extracción del título\" de Word es 0.810 y 0.837 respectivamente, y la precisión y el recuerdo para la \"extracción del título\" de PowerPoint es 0.875 y 0.895 respectivamente en un experimento sobre datos de intranet.",
                "Como estudio de caso, consideramos \"extracción de título\" en este documento.",
                "Anotamos títulos en documentos de muestra (para Word y PowerPoint respectivamente) y los tomamos como datos de entrenamiento para capacitar a varios tipos de modelos, y realizamos \"extracción de título\" utilizando cualquier tipo de modelos entrenados.",
                "En este documento, también investigamos los siguientes tres problemas, que no parecían haber sido examinados anteriormente.(1) Comparación entre modelos: entre los modelos anteriores, cuyo modelo funciona mejor para la \"extracción de título\";(2) Generalidad del modelo: si es posible entrenar un modelo en un dominio y aplicarlo a otro dominio, y si es posible entrenar un modelo en un idioma y aplicarlo a otro idioma;(3) Utilidad de los títulos extraídos: si los títulos extraídos pueden mejorar el procesamiento de documentos, como la búsqueda.",
                "Los resultados experimentales indican que nuestro enfoque funciona bien para la \"extracción del título\" de los documentos generales.",
                "La precisión y el recuerdo para la \"extracción del título\" de Word son 0.810 y 0.837 respectivamente, y la precisión y el recuerdo para la \"extracción del título\" de PowerPoint son 0.875 y 0.895 respectivamente."
            ],
            "translated_text": "",
            "candidates": [
                "extracción de título",
                "extracción de títulos",
                "extracción de título",
                "extracción de título",
                "extracción de título",
                "extracción de título",
                "extracción de título",
                "extracción de títulos",
                "extracción de título",
                "extracción del título",
                "extracción del título",
                "extracción de título",
                "extracción de título",
                "extracción de título",
                "extracción de título",
                "extracción de título",
                "extracción de título",
                "extracción de título",
                "extracción del título",
                "extracción de título",
                "extracción del título",
                "extracción del título"
            ],
            "error": []
        },
        "formatting information": {
            "translated_key": "información de formato",
            "is_in_text": true,
            "original_annotated_sentences": [
                "Automatic Extraction of Titles from General Documents using Machine Learning Yunhua Hu1 Computer Science Department Xian Jiaotong University No 28, Xianning West Road Xian, China, 710049 yunhuahu@mail.xjtu.edu.cn Hang Li, Yunbo Cao Microsoft Research Asia 5F Sigma Center, No.",
                "49 Zhichun Road, Haidian, Beijing, China, 100080 {hangli,yucao}@microsoft.com Qinghua Zheng Computer Science Department Xian Jiaotong University No 28, Xianning West Road Xian, China, 710049 qhzheng@mail.xjtu.edu.cn Dmitriy Meyerzon Microsoft Corporation One Microsoft Way Redmond, WA, USA, 98052 dmitriym@microsoft.com ABSTRACT In this paper, we propose a machine learning approach to title extraction from general documents.",
                "By general documents, we mean documents that can belong to any one of a number of specific genres, including presentations, book chapters, technical papers, brochures, reports, and letters.",
                "Previously, methods have been proposed mainly for title extraction from research papers.",
                "It has not been clear whether it could be possible to conduct automatic title extraction from general documents.",
                "As a case study, we consider extraction from Office including Word and PowerPoint.",
                "In our approach, we annotate titles in sample documents (for Word and PowerPoint respectively) and take them as training data, train machine learning models, and perform title extraction using the trained models.",
                "Our method is unique in that we mainly utilize <br>formatting information</br> such as font size as features in the models.",
                "It turns out that the use of <br>formatting information</br> can lead to quite accurate extraction from general documents.",
                "Precision and recall for title extraction from Word is 0.810 and 0.837 respectively, and precision and recall for title extraction from PowerPoint is 0.875 and 0.895 respectively in an experiment on intranet data.",
                "Other important new findings in this work include that we can train models in one domain and apply them to another domain, and more surprisingly we can even train models in one language and apply them to another language.",
                "Moreover, we can significantly improve search ranking results in document retrieval by using the extracted titles.",
                "Categories and Subject Descriptors H.3.3 [Information Storage and Retrieval]: Information Search and Retrieval - Search Process; H.4.1 [Information Systems Applications]: Office Automation - Word processing; D.2.8 [Software Engineering]: Metrics - complexity measures, performance measures General Terms Algorithms, Experimentation, Performance. 1.",
                "INTRODUCTION Metadata of documents is useful for many kinds of document processing such as search, browsing, and filtering.",
                "Ideally, metadata is defined by the authors of documents and is then used by various systems.",
                "However, people seldom define document metadata by themselves, even when they have convenient metadata definition tools [26].",
                "Thus, how to automatically extract metadata from the bodies of documents turns out to be an important research issue.",
                "Methods for performing the task have been proposed.",
                "However, the focus was mainly on extraction from research papers.",
                "For instance, Han et al. [10] proposed a machine learning based method to conduct extraction from research papers.",
                "They formalized the problem as that of classification and employed Support Vector Machines as the classifier.",
                "They mainly used linguistic features in the model.1 In this paper, we consider metadata extraction from general documents.",
                "By general documents, we mean documents that may belong to any one of a number of specific genres.",
                "General documents are more widely available in digital libraries, intranets and the internet, and thus investigation on extraction from them is sorely needed.",
                "Research papers usually have well-formed styles and noticeable characteristics.",
                "In contrast, the styles of general documents can vary greatly.",
                "It has not been clarified whether a machine learning based approach can work well for this task.",
                "There are many types of metadata: title, author, date of creation, etc.",
                "As a case study, we consider title extraction in this paper.",
                "General documents can be in many different file formats: Microsoft Office, PDF (PS), etc.",
                "As a case study, we consider extraction from Office including Word and PowerPoint.",
                "We take a machine learning approach.",
                "We annotate titles in sample documents (for Word and PowerPoint respectively) and take them as training data to train several types of models, and perform title extraction using any one type of the trained models.",
                "In the models, we mainly utilize <br>formatting information</br> such as font size as features.",
                "We employ the following models: Maximum Entropy Model, Perceptron with Uneven Margins, Maximum Entropy Markov Model, and Voted Perceptron.",
                "In this paper, we also investigate the following three problems, which did not seem to have been examined previously. (1) Comparison between models: among the models above, which model performs best for title extraction; (2) Generality of model: whether it is possible to train a model on one domain and apply it to another domain, and whether it is possible to train a model in one language and apply it to another language; (3) Usefulness of extracted titles: whether extracted titles can improve document processing such as search.",
                "Experimental results indicate that our approach works well for title extraction from general documents.",
                "Our method can significantly outperform the baselines: one that always uses the first lines as titles and the other that always uses the lines in the largest font sizes as titles.",
                "Precision and recall for title extraction from Word are 0.810 and 0.837 respectively, and precision and recall for title extraction from PowerPoint are 0.875 and 0.895 respectively.",
                "It turns out that the use of format features is the key to successful title extraction. (1) We have observed that Perceptron based models perform better in terms of extraction accuracies. (2) We have empirically verified that the models trained with our approach are generic in the sense that they can be trained on one domain and applied to another, and they can be trained in one language and applied to another. (3) We have found that using the extracted titles we can significantly improve precision of document retrieval (by 10%).",
                "We conclude that we can indeed conduct reliable title extraction from general documents and use the extracted results to improve real applications.",
                "The rest of the paper is organized as follows.",
                "In section 2, we introduce related work, and in section 3, we explain the motivation and problem setting of our work.",
                "In section 4, we describe our method of title extraction, and in section 5, we describe our method of document retrieval using extracted titles.",
                "Section 6 gives our experimental results.",
                "We make concluding remarks in section 7. 2.",
                "RELATED WORK 2.1 Document Metadata Extraction Methods have been proposed for performing automatic metadata extraction from documents; however, the main focus was on extraction from research papers.",
                "The proposed methods fall into two categories: the rule based approach and the machine learning based approach.",
                "Giuffrida et al. [9], for instance, developed a rule-based system for automatically extracting metadata from research papers in Postscript.",
                "They used rules like titles are usually located on the upper portions of the first pages and they are usually in the largest font sizes.",
                "Liddy et al. [14] and Yilmazel el al. [23] performed metadata extraction from educational materials using rule-based natural language processing technologies.",
                "Mao et al. [16] also conducted automatic metadata extraction from research papers using rules on <br>formatting information</br>.",
                "The rule-based approach can achieve high performance.",
                "However, it also has disadvantages.",
                "It is less adaptive and robust when compared with the machine learning approach.",
                "Han et al. [10], for instance, conducted metadata extraction with the machine learning approach.",
                "They viewed the problem as that of classifying the lines in a document into the categories of metadata and proposed using Support Vector Machines as the classifier.",
                "They mainly used linguistic information as features.",
                "They reported high extraction accuracy from research papers in terms of precision and recall. 2.2 Information Extraction Metadata extraction can be viewed as an application of information extraction, in which given a sequence of instances, we identify a subsequence that represents information in which we are interested.",
                "Hidden Markov Model [6], Maximum Entropy Model [1, 4], Maximum Entropy Markov Model [17], Support Vector Machines [3], Conditional Random Field [12], and Voted Perceptron [2] are widely used information extraction models.",
                "Information extraction has been applied, for instance, to part-ofspeech tagging [20], named entity recognition [25] and table extraction [19]. 2.3 Search Using Title Information Title information is useful for document retrieval.",
                "In the system Citeseer, for instance, Giles et al. managed to extract titles from research papers and make use of the extracted titles in metadata search of papers [8].",
                "In web search, the title fields (i.e., file properties) and anchor texts of web pages (HTML documents) can be viewed as titles of the pages [5].",
                "Many search engines seem to utilize them for web page retrieval [7, 11, 18, 22].",
                "Zhang et al., found that web pages with well-defined metadata are more easily retrieved than those without well-defined metadata [24].",
                "To the best of our knowledge, no research has been conducted on using extracted titles from general documents (e.g., Office documents) for search of the documents. 146 3.",
                "MOTIVATION AND PROBLEM SETTING We consider the issue of automatically extracting titles from general documents.",
                "By general documents, we mean documents that belong to one of any number of specific genres.",
                "The documents can be presentations, books, book chapters, technical papers, brochures, reports, memos, specifications, letters, announcements, or resumes.",
                "General documents are more widely available in digital libraries, intranets, and internet, and thus investigation on title extraction from them is sorely needed.",
                "Figure 1 shows an estimate on distributions of file formats on intranet and internet [15].",
                "Office and PDF are the main file formats on the intranet.",
                "Even on the internet, the documents in the formats are still not negligible, given its extremely large size.",
                "In this paper, without loss of generality, we take Office documents as an example.",
                "Figure 1.",
                "Distributions of file formats in internet and intranet.",
                "For Office documents, users can define titles as file properties using a feature provided by Office.",
                "We found in an experiment, however, that users seldom use the feature and thus titles in file properties are usually very inaccurate.",
                "That is to say, titles in file properties are usually inconsistent with the true titles in the file bodies that are created by the authors and are visible to readers.",
                "We collected 6,000 Word and 6,000 PowerPoint documents from an intranet and the internet and examined how many titles in the file properties are correct.",
                "We found that surprisingly the accuracy was only 0.265 (cf., Section 6.3 for details).",
                "A number of reasons can be considered.",
                "For example, if one creates a new file by copying an old file, then the file property of the new file will also be copied from the old file.",
                "In another experiment, we found that Google uses the titles in file properties of Office documents in search and browsing, but the titles are not very accurate.",
                "We created 50 queries to search Word and PowerPoint documents and examined the top 15 results of each query returned by Google.",
                "We found that nearly all the titles presented in the search results were from the file properties of the documents.",
                "However, only 0.272 of them were correct.",
                "Actually, true titles usually exist at the beginnings of the bodies of documents.",
                "If we can accurately extract the titles from the bodies of documents, then we can exploit reliable title information in document processing.",
                "This is exactly the problem we address in this paper.",
                "More specifically, given a Word document, we are to extract the title from the top region of the first page.",
                "Given a PowerPoint document, we are to extract the title from the first slide.",
                "A title sometimes consists of a main title and one or two subtitles.",
                "We only consider extraction of the main title.",
                "As baselines for title extraction, we use that of always using the first lines as titles and that of always using the lines with largest font sizes as titles.",
                "Figure 2.",
                "Title extraction from Word document.",
                "Figure 3.",
                "Title extraction from PowerPoint document.",
                "Next, we define a specification for human judgments in title data annotation.",
                "The annotated data will be used in training and testing of the title extraction methods.",
                "Summary of the specification: The title of a document should be identified on the basis of common sense, if there is no difficulty in the identification.",
                "However, there are many cases in which the identification is not easy.",
                "There are some rules defined in the specification that guide identification for such cases.",
                "The rules include a title is usually in consecutive lines in the same format, a document can have no title, titles in images are not considered, a title should not contain words like draft, 147 whitepaper, etc, if it is difficult to determine which is the title, select the one in the largest font size, and if it is still difficult to determine which is the title, select the first candidate. (The specification covers all the cases we have encountered in data annotation.)",
                "Figures 2 and 3 show examples of Office documents from which we conduct title extraction.",
                "In Figure 2, Differences in Win32 API Implementations among Windows Operating Systems is the title of the Word document.",
                "Microsoft Windows on the top of this page is a picture and thus is ignored.",
                "In Figure 3, Building Competitive Advantages through an Agile Infrastructure is the title of the PowerPoint document.",
                "We have developed a tool for annotation of titles by human annotators.",
                "Figure 4 shows a snapshot of the tool.",
                "Figure 4.",
                "Title annotation tool. 4.",
                "TITLE EXTRACTION METHOD 4.1 Outline Title extraction based on machine learning consists of training and extraction.",
                "The same pre-processing step occurs before training and extraction.",
                "During pre-processing, from the top region of the first page of a Word document or the first slide of a PowerPoint document a number of units for processing are extracted.",
                "If a line (lines are separated by return symbols) only has a single format, then the line will become a unit.",
                "If a line has several parts and each of them has its own format, then each part will become a unit.",
                "Each unit will be treated as an instance in learning.",
                "A unit contains not only content information (linguistic information) but also <br>formatting information</br>.",
                "The input to pre-processing is a document and the output of pre-processing is a sequence of units (instances).",
                "Figure 5 shows the units obtained from the document in Figure 2.",
                "Figure 5.",
                "Example of units.",
                "In learning, the input is sequences of units where each sequence corresponds to a document.",
                "We take labeled units (labeled as title_begin, title_end, or other) in the sequences as training data and construct models for identifying whether a unit is title_begin title_end, or other.",
                "We employ four types of models: Perceptron, Maximum Entropy (ME), Perceptron Markov Model (PMM), and Maximum Entropy Markov Model (MEMM).",
                "In extraction, the input is a sequence of units from one document.",
                "We employ one type of model to identify whether a unit is title_begin, title_end, or other.",
                "We then extract units from the unit labeled with title_begin to the unit labeled with title_end.",
                "The result is the extracted title of the document.",
                "The unique characteristic of our approach is that we mainly utilize <br>formatting information</br> for title extraction.",
                "Our assumption is that although general documents vary in styles, their formats have certain patterns and we can learn and utilize the patterns for title extraction.",
                "This is in contrast to the work by Han et al., in which only linguistic features are used for extraction from research papers. 4.2 Models The four models actually can be considered in the same metadata extraction framework.",
                "That is why we apply them together to our current problem.",
                "Each input is a sequence of instances kxxx L21 together with a sequence of labels kyyy L21 . ix and iy represents an instance and its label, respectively ( ki ,,2,1 L= ).",
                "Recall that an instance here represents a unit.",
                "A label represents title_begin, title_end, or other.",
                "Here, k is the number of units in a document.",
                "In learning, we train a model which can be generally denoted as a conditional probability distribution )|( 11 kk XXYYP LL where iX and iY denote random variables taking instance ix and label iy as values, respectively ( ki ,,2,1 L= ).",
                "Learning Tool Extraction Tool 21121 2222122221 1121111211 nknnknn kk kk yyyxxx yyyxxx yyyxxx LL LL LL LL → → → )|(maxarg 11 mkmmkm xxyyP LL )|( 11 kk XXYYP LL Conditional Distribution mkmm xxx L21 Figure 6.",
                "Metadata extraction model.",
                "We can make assumptions about the general model in order to make it simple enough for training. 148 For example, we can assume that kYY ,,1 L are independent of each other given kXX ,,1 L .",
                "Thus, we have )|()|( )|( 11 11 kk kk XYPXYP XXYYP L LL = In this way, we decompose the model into a number of classifiers.",
                "We train the classifiers locally using the labeled data.",
                "As the classifier, we employ the Perceptron or Maximum Entropy model.",
                "We can also assume that the first order Markov property holds for kYY ,,1 L given kXX ,,1 L .",
                "Thus, we have )|()|( )|( 111 11 kkk kk XYYPXYP XXYYP −= L LL Again, we obtain a number of classifiers.",
                "However, the classifiers are conditioned on the previous label.",
                "When we employ the Percepton or Maximum Entropy model as a classifier, the models become a Percepton Markov Model or Maximum Entropy Markov Model, respectively.",
                "That is to say, the two models are more precise.",
                "In extraction, given a new sequence of instances, we resort to one of the constructed models to assign a sequence of labels to the sequence of instances, i.e., perform extraction.",
                "For Perceptron and ME, we assign labels locally and combine the results globally later using heuristics.",
                "Specifically, we first identify the most likely title_begin.",
                "Then we find the most likely title_end within three units after the title_begin.",
                "Finally, we extract as a title the units between the title_begin and the title_end.",
                "For PMM and MEMM, we employ the Viterbi algorithm to find the globally optimal label sequence.",
                "In this paper, for Perceptron, we actually employ an improved variant of it, called Perceptron with Uneven Margin [13].",
                "This version of Perceptron can work well especially when the number of positive instances and the number of negative instances differ greatly, which is exactly the case in our problem.",
                "We also employ an improved version of Perceptron Markov Model in which the Perceptron model is the so-called Voted Perceptron [2].",
                "In addition, in training, the parameters of the model are updated globally rather than locally. 4.3 Features There are two types of features: format features and linguistic features.",
                "We mainly use the former.",
                "The features are used for both the title-begin and the title-end classifiers. 4.3.1 Format Features Font Size: There are four binary features that represent the normalized font size of the unit (recall that a unit has only one type of font).",
                "If the font size of the unit is the largest in the document, then the first feature will be 1, otherwise 0.",
                "If the font size is the smallest in the document, then the fourth feature will be 1, otherwise 0.",
                "If the font size is above the average font size and not the largest in the document, then the second feature will be 1, otherwise 0.",
                "If the font size is below the average font size and not the smallest, the third feature will be 1, otherwise 0.",
                "It is necessary to conduct normalization on font sizes.",
                "For example, in one document the largest font size might be 12pt, while in another the smallest one might be 18pt.",
                "Boldface: This binary feature represents whether or not the current unit is in boldface.",
                "Alignment: There are four binary features that respectively represent the location of the current unit: left, center, right, and unknown alignment.",
                "The following format features with respect to context play an important role in title extraction.",
                "Empty Neighboring Unit: There are two binary features that represent, respectively, whether or not the previous unit and the current unit are blank lines.",
                "Font Size Change: There are two binary features that represent, respectively, whether or not the font size of the previous unit and the font size of the next unit differ from that of the current unit.",
                "Alignment Change: There are two binary features that represent, respectively, whether or not the alignment of the previous unit and the alignment of the next unit differ from that of the current one.",
                "Same Paragraph: There are two binary features that represent, respectively, whether or not the previous unit and the next unit are in the same paragraph as the current unit. 4.3.2 Linguistic Features The linguistic features are based on key words.",
                "Positive Word: This binary feature represents whether or not the current unit begins with one of the positive words.",
                "The positive words include title:, subject:, subject line: For example, in some documents the lines of titles and authors have the same formats.",
                "However, if lines begin with one of the positive words, then it is likely that they are title lines.",
                "Negative Word: This binary feature represents whether or not the current unit begins with one of the negative words.",
                "The negative words include To, By, created by, updated by, etc.",
                "There are more negative words than positive words.",
                "The above linguistic features are language dependent.",
                "Word Count: A title should not be too long.",
                "We heuristically create four intervals: [1, 2], [3, 6], [7, 9] and [9, ∞) and define one feature for each interval.",
                "If the number of words in a title falls into an interval, then the corresponding feature will be 1; otherwise 0.",
                "Ending Character: This feature represents whether the unit ends with :, -, or other special characters.",
                "A title usually does not end with such a character. 5.",
                "DOCUMENT RETRIEVAL METHOD We describe our method of document retrieval using extracted titles.",
                "Typically, in information retrieval a document is split into a number of fields including body, title, and anchor text.",
                "A ranking function in search can use different weights for different fields of 149 the document.",
                "Also, titles are typically assigned high weights, indicating that they are important for document retrieval.",
                "As explained previously, our experiment has shown that a significant number of documents actually have incorrect titles in the file properties, and thus in addition of using them we use the extracted titles as one more field of the document.",
                "By doing this, we attempt to improve the overall precision.",
                "In this paper, we employ a modification of BM25 that allows field weighting [21].",
                "As fields, we make use of body, title, extracted title and anchor.",
                "First, for each term in the query we count the term frequency in each field of the document; each field frequency is then weighted according to the corresponding weight parameter: ∑= f tfft tfwwtf Similarly, we compute the document length as a weighted sum of lengths of each field.",
                "Average document length in the corpus becomes the average of all weighted document lengths. ∑= f ff dlwwdl In our experiments we used 75.0,8.11 == bk .",
                "Weight for content was 1.0, title was 10.0, anchor was 10.0, and extracted title was 5.0. 6.",
                "EXPERIMENTAL RESULTS 6.1 Data Sets and Evaluation Measures We used two data sets in our experiments.",
                "First, we downloaded and randomly selected 5,000 Word documents and 5,000 PowerPoint documents from an intranet of Microsoft.",
                "We call it MS hereafter.",
                "Second, we downloaded and randomly selected 500 Word and 500 PowerPoint documents from the DotGov and DotCom domains on the internet, respectively.",
                "Figure 7 shows the distributions of the genres of the documents.",
                "We see that the documents are indeed general documents as we define them.",
                "Figure 7.",
                "Distributions of document genres.",
                "Third, a data set in Chinese was also downloaded from the internet.",
                "It includes 500 Word documents and 500 PowerPoint documents in Chinese.",
                "We manually labeled the titles of all the documents, on the basis of our specification.",
                "Not all the documents in the two data sets have titles.",
                "Table 1 shows the percentages of the documents having titles.",
                "We see that DotCom and DotGov have more PowerPoint documents with titles than MS.",
                "This might be because PowerPoint documents published on the internet are more formal than those on the intranet.",
                "Table 1.",
                "The portion of documents with titles Domain Type MS DotCom DotGov Word 75.7% 77.8% 75.6% PowerPoint 82.1% 93.4% 96.4% In our experiments, we conducted evaluations on title extraction in terms of precision, recall, and F-measure.",
                "The evaluation measures are defined as follows: Precision: P = A / ( A + B ) Recall: R = A / ( A + C ) F-measure: F1 = 2PR / ( P + R ) Here, A, B, C, and D are numbers of documents as those defined in Table 2.",
                "Table 2.",
                "Contingence table with regard to title extraction Is title Is not title Extracted A B Not extracted C D 6.2 Baselines We test the accuracies of the two baselines described in section 4.2.",
                "They are denoted as largest font size and first line respectively. 6.3 Accuracy of Titles in File Properties We investigate how many titles in the file properties of the documents are reliable.",
                "We view the titles annotated by humans as true titles and test how many titles in the file properties can approximately match with the true titles.",
                "We use Edit Distance to conduct the approximate match. (Approximate match is only used in this evaluation).",
                "This is because sometimes human annotated titles can be slightly different from the titles in file properties on the surface, e.g., contain extra spaces).",
                "Given string A and string B: if ( (D == 0) or ( D / ( La + Lb ) < θ ) ) then string A = string B D: Edit Distance between string A and string B La: length of string A Lb: length of string B θ: 0.1 ∑ × ++− + = t t n N wtf avwdl wdl bbk kwtf FBM )log( ))1(( )1( 25 1 1 150 Table 3.",
                "Accuracies of titles in file properties File Type Domain Precision Recall F1 MS 0.299 0.311 0.305 DotCom 0.210 0.214 0.212Word DotGov 0.182 0.177 0.180 MS 0.229 0.245 0.237 DotCom 0.185 0.186 0.186PowerPoint DotGov 0.180 0.182 0.181 6.4 Comparison with Baselines We conducted title extraction from the first data set (Word and PowerPoint in MS).",
                "As the model, we used Perceptron.",
                "We conduct 4-fold cross validation.",
                "Thus, all the results reported here are those averaged over 4 trials.",
                "Tables 4 and 5 show the results.",
                "We see that Perceptron significantly outperforms the baselines.",
                "In the evaluation, we use exact matching between the true titles annotated by humans and the extracted titles.",
                "Table 4.",
                "Accuracies of title extraction with Word Precision Recall F1 Model Perceptron 0.810 0.837 0.823 Largest font size 0.700 0.758 0.727 Baselines First line 0.707 0.767 0.736 Table 5.",
                "Accuracies of title extraction with PowerPoint Precision Recall F1 Model Perceptron 0.875 0. 895 0.885 Largest font size 0.844 0.887 0.865 Baselines First line 0.639 0.671 0.655 We see that the machine learning approach can achieve good performance in title extraction.",
                "For Word documents both precision and recall of the approach are 8 percent higher than those of the baselines.",
                "For PowerPoint both precision and recall of the approach are 2 percent higher than those of the baselines.",
                "We conduct significance tests.",
                "The results are shown in Table 6.",
                "Here, Largest denotes the baseline of using the largest font size, First denotes the baseline of using the first line.",
                "The results indicate that the improvements of machine learning over baselines are statistically significant (in the sense p-value < 0.05) Table 6.",
                "Sign test results Documents Type Sign test between p-value Perceptron vs. Largest 3.59e-26 Word Perceptron vs. First 7.12e-10 Perceptron vs. Largest 0.010 PowerPoint Perceptron vs. First 5.13e-40 We see, from the results, that the two baselines can work well for title extraction, suggesting that font size and position information are most useful features for title extraction.",
                "However, it is also obvious that using only these two features is not enough.",
                "There are cases in which all the lines have the same font size (i.e., the largest font size), or cases in which the lines with the largest font size only contain general descriptions like Confidential, White paper, etc.",
                "For those cases, the largest font size method cannot work well.",
                "For similar reasons, the first line method alone cannot work well, either.",
                "With the combination of different features (evidence in title judgment), Perceptron can outperform Largest and First.",
                "We investigate the performance of solely using linguistic features.",
                "We found that it does not work well.",
                "It seems that the format features play important roles and the linguistic features are supplements..",
                "Figure 8.",
                "An example Word document.",
                "Figure 9.",
                "An example PowerPoint document.",
                "We conducted an error analysis on the results of Perceptron.",
                "We found that the errors fell into three categories. (1) About one third of the errors were related to hard cases.",
                "In these documents, the layouts of the first pages were difficult to understand, even for humans.",
                "Figure 8 and 9 shows examples. (2) Nearly one fourth of the errors were from the documents which do not have true titles but only contain bullets.",
                "Since we conduct extraction from the top regions, it is difficult to get rid of these errors with the current approach. (3).",
                "Confusions between main titles and subtitles were another type of error.",
                "Since we only labeled the main titles as titles, the extractions of both titles were considered incorrect.",
                "This type of error does little harm to document processing like search, however. 6.5 Comparison between Models To compare the performance of different machine learning models, we conducted another experiment.",
                "Again, we perform 4-fold cross 151 validation on the first data set (MS).",
                "Table 7, 8 shows the results of all the four models.",
                "It turns out that Perceptron and PMM perform the best, followed by MEMM, and ME performs the worst.",
                "In general, the Markovian models perform better than or as well as their classifier counterparts.",
                "This seems to be because the Markovian models are trained globally, while the classifiers are trained locally.",
                "The Perceptron based models perform better than the ME based counterparts.",
                "This seems to be because the Perceptron based models are created to make better classifications, while ME models are constructed for better prediction.",
                "Table 7.",
                "Comparison between different learning models for title extraction with Word Model Precision Recall F1 Perceptron 0.810 0.837 0.823 MEMM 0.797 0.824 0.810 PMM 0.827 0.823 0.825 ME 0.801 0.621 0.699 Table 8.",
                "Comparison between different learning models for title extraction with PowerPoint Model Precision Recall F1 Perceptron 0.875 0. 895 0. 885 MEMM 0.841 0.861 0.851 PMM 0.873 0.896 0.885 ME 0.753 0.766 0.759 6.6 Domain Adaptation We apply the model trained with the first data set (MS) to the second data set (DotCom and DotGov).",
                "Tables 9-12 show the results.",
                "Table 9.",
                "Accuracies of title extraction with Word in DotGov Precision Recall F1 Model Perceptron 0.716 0.759 0.737 Largest font size 0.549 0.619 0.582Baselines First line 0.462 0.521 0.490 Table 10.",
                "Accuracies of title extraction with PowerPoint in DotGov Precision Recall F1 Model Perceptron 0.900 0.906 0.903 Largest font size 0.871 0.888 0.879Baselines First line 0.554 0.564 0.559 Table 11.",
                "Accuracies of title extraction with Word in DotCom Precisio n Recall F1 Model Perceptron 0.832 0.880 0.855 Largest font size 0.676 0.753 0.712Baselines First line 0.577 0.643 0.608 Table 12.",
                "Performance of PowerPoint document title extraction in DotCom Precisio n Recall F1 Model Perceptron 0.910 0.903 0.907 Largest font size 0.864 0.886 0.875Baselines First line 0.570 0.585 0.577 From the results, we see that the models can be adapted to different domains well.",
                "There is almost no drop in accuracy.",
                "The results indicate that the patterns of title formats exist across different domains, and it is possible to construct a domain independent model by mainly using <br>formatting information</br>. 6.7 Language Adaptation We apply the model trained with the data in English (MS) to the data set in Chinese.",
                "Tables 13-14 show the results.",
                "Table 13.",
                "Accuracies of title extraction with Word in Chinese Precision Recall F1 Model Perceptron 0.817 0.805 0.811 Largest font size 0.722 0.755 0.738Baselines First line 0.743 0.777 0.760 Table 14.",
                "Accuracies of title extraction with PowerPoint in Chinese Precision Recall F1 Model Perceptron 0.766 0.812 0.789 Largest font size 0.753 0.813 0.782Baselines First line 0.627 0.676 0.650 We see that the models can be adapted to a different language.",
                "There are only small drops in accuracy.",
                "Obviously, the linguistic features do not work for Chinese, but the effect of not using them is negligible.",
                "The results indicate that the patterns of title formats exist across different languages.",
                "From the domain adaptation and language adaptation results, we conclude that the use of <br>formatting information</br> is the key to a successful extraction from general documents. 6.8 Search with Extracted Titles We performed experiments on using title extraction for document retrieval.",
                "As a baseline, we employed BM25 without using extracted titles.",
                "The ranking mechanism was as described in Section 5.",
                "The weights were heuristically set.",
                "We did not conduct optimization on the weights.",
                "The evaluation was conducted on a corpus of 1.3 M documents crawled from the intranet of Microsoft using 100 evaluation queries obtained from this intranets search engine query logs. 50 queries were from the most popular set, while 50 queries other were chosen randomly.",
                "Users were asked to provide judgments of the degree of document relevance from a scale of 1to 5 (1 meaning detrimental, 2 - bad, 3 - fair, 4 - good and 5 - excellent). 152 Figure 10 shows the results.",
                "In the chart two sets of precision results were obtained by either considering good or excellent documents as relevant (left 3 bars with relevance threshold 0.5), or by considering only excellent documents as relevant (right 3 bars with relevance threshold 1.0) 0 0.05 0.1 0.15 0.2 0.25 0.3 0.35 0.4 0.45 P@10 P@5 Reciprocal P@10 P@5 Reciprocal 0.5 1 BM25 Anchor, Title, Body BM25 Anchor, Title, Body, ExtractedTitle Name All RelevanceThreshold Data Description Figure 10.",
                "Search ranking results.",
                "Figure 10 shows different document retrieval results with different ranking functions in terms of precision @10, precision @5 and reciprocal rank: • Blue bar - BM25 including the fields body, title (file property), and anchor text. • Purple bar - BM25 including the fields body, title (file property), anchor text, and extracted title.",
                "With the additional field of extracted title included in BM25 the precision @10 increased from 0.132 to 0.145, or by ~10%.",
                "Thus, it is safe to say that the use of extracted title can indeed improve the precision of document retrieval. 7.",
                "CONCLUSION In this paper, we have investigated the problem of automatically extracting titles from general documents.",
                "We have tried using a machine learning approach to address the problem.",
                "Previous work showed that the machine learning approach can work well for metadata extraction from research papers.",
                "In this paper, we showed that the approach can work for extraction from general documents as well.",
                "Our experimental results indicated that the machine learning approach can work significantly better than the baselines in title extraction from Office documents.",
                "Previous work on metadata extraction mainly used linguistic features in documents, while we mainly used <br>formatting information</br>.",
                "It appeared that using <br>formatting information</br> is a key for successfully conducting title extraction from general documents.",
                "We tried different machine learning models including Perceptron, Maximum Entropy, Maximum Entropy Markov Model, and Voted Perceptron.",
                "We found that the performance of the Perceptorn models was the best.",
                "We applied models constructed in one domain to another domain and applied models trained in one language to another language.",
                "We found that the accuracies did not drop substantially across different domains and across different languages, indicating that the models were generic.",
                "We also attempted to use the extracted titles in document retrieval.",
                "We observed a significant improvement in document ranking performance for search when using extracted title information.",
                "All the above investigations were not conducted in previous work, and through our investigations we verified the generality and the significance of the title extraction approach. 8.",
                "ACKNOWLEDGEMENTS We thank Chunyu Wei and Bojuan Zhao for their work on data annotation.",
                "We acknowledge Jinzhu Li for his assistance in conducting the experiments.",
                "We thank Ming Zhou, John Chen, Jun Xu, and the anonymous reviewers of JCDL05 for their valuable comments on this paper. 9.",
                "REFERENCES [1] Berger, A. L., Della Pietra, S. A., and Della Pietra, V. J.",
                "A maximum entropy approach to natural language processing.",
                "Computational Linguistics, 22:39-71, 1996. [2] Collins, M. Discriminative training methods for hidden markov models: theory and experiments with perceptron algorithms.",
                "In Proceedings of Conference on Empirical Methods in Natural Language Processing, 1-8, 2002. [3] Cortes, C. and Vapnik, V. Support-vector networks.",
                "Machine Learning, 20:273-297, 1995. [4] Chieu, H. L. and Ng, H. T. A maximum entropy approach to information extraction from semi-structured and free text.",
                "In Proceedings of the Eighteenth National Conference on Artificial Intelligence, 768-791, 2002. [5] Evans, D. K., Klavans, J. L., and McKeown, K. R. Columbia newsblaster: multilingual news summarization on the Web.",
                "In Proceedings of Human Language Technology conference / North American chapter of the Association for Computational Linguistics annual meeting, 1-4, 2004. [6] Ghahramani, Z. and Jordan, M. I. Factorial hidden markov models.",
                "Machine Learning, 29:245-273, 1997. [7] Gheel, J. and Anderson, T. Data and metadata for finding and reminding, In Proceedings of the 1999 International Conference on Information Visualization, 446-451,1999. [8] Giles, C. L., Petinot, Y., Teregowda P. B., Han, H., Lawrence, S., Rangaswamy, A., and Pal, N. eBizSearch: a niche search engine for e-Business.",
                "In Proceedings of the 26th Annual International ACM SIGIR Conference on Research and Development in Information Retrieval, 413414, 2003. [9] Giuffrida, G., Shek, E. C., and Yang, J. Knowledge-based metadata extraction from PostScript files.",
                "In Proceedings of the Fifth ACM Conference on Digital Libraries, 77-84, 2000. [10] Han, H., Giles, C. L., Manavoglu, E., Zha, H., Zhang, Z., and Fox, E. A.",
                "Automatic document metadata extraction using support vector machines.",
                "In Proceedings of the Third ACM/IEEE-CS Joint Conference on Digital Libraries, 37-48, 2003. [11] Kobayashi, M., and Takeda, K. Information retrieval on the Web.",
                "ACM Computing Surveys, 32:144-173, 2000. [12] Lafferty, J., McCallum, A., and Pereira, F. Conditional random fields: probabilistic models for segmenting and 153 labeling sequence data.",
                "In Proceedings of the Eighteenth International Conference on Machine Learning, 282-289, 2001. [13] Li, Y., Zaragoza, H., Herbrich, R., Shawe-Taylor J., and Kandola, J. S. The perceptron algorithm with uneven margins.",
                "In Proceedings of the Nineteenth International Conference on Machine Learning, 379-386, 2002. [14] Liddy, E. D., Sutton, S., Allen, E., Harwell, S., Corieri, S., Yilmazel, O., Ozgencil, N. E., Diekema, A., McCracken, N., and Silverstein, J.",
                "Automatic Metadata generation & evaluation.",
                "In Proceedings of the 25th Annual International ACM SIGIR Conference on Research and Development in Information Retrieval, 401-402, 2002. [15] Littlefield, A.",
                "Effective enterprise information retrieval across new content formats.",
                "In Proceedings of the Seventh Search Engine Conference, http://www.infonortics.com/searchengines/sh02/02prog.html, 2002. [16] Mao, S., Kim, J. W., and Thoma, G. R. A dynamic feature generation system for automated metadata extraction in preservation of digital materials.",
                "In Proceedings of the First International Workshop on Document Image Analysis for Libraries, 225-232, 2004. [17] McCallum, A., Freitag, D., and Pereira, F. Maximum entropy markov models for information extraction and segmentation.",
                "In Proceedings of the Seventeenth International Conference on Machine Learning, 591-598, 2000. [18] Murphy, L. D. Digital document metadata in organizations: roles, analytical approaches, and future research directions.",
                "In Proceedings of the Thirty-First Annual Hawaii International Conference on System Sciences, 267-276, 1998. [19] Pinto, D., McCallum, A., Wei, X., and Croft, W. B.",
                "Table extraction using conditional random fields.",
                "In Proceedings of the 26th Annual International ACM SIGIR Conference on Research and Development in Information Retrieval, 235242, 2003. [20] Ratnaparkhi, A. Unsupervised statistical models for prepositional phrase attachment.",
                "In Proceedings of the Seventeenth International Conference on Computational Linguistics. 1079-1085, 1998. [21] Robertson, S., Zaragoza, H., and Taylor, M. Simple BM25 extension to multiple weighted fields, In Proceedings of ACM Thirteenth Conference on Information and Knowledge Management, 42-49, 2004. [22] Yi, J. and Sundaresan, N. Metadata based Web mining for relevance, In Proceedings of the 2000 International Symposium on Database Engineering & Applications, 113121, 2000. [23] Yilmazel, O., Finneran, C. M., and Liddy, E. D. MetaExtract: An NLP system to automatically assign metadata.",
                "In Proceedings of the 2004 Joint ACM/IEEE Conference on Digital Libraries, 241-242, 2004. [24] Zhang, J. and Dimitroff, A. Internet search engines response to metadata Dublin Core implementation.",
                "Journal of Information Science, 30:310-320, 2004. [25] Zhang, L., Pan, Y., and Zhang, T. Recognising and using named entities: focused named entity recognition using machine learning.",
                "In Proceedings of the 27th Annual International ACM SIGIR Conference on Research and Development in Information Retrieval, 281-288, 2004. [26] http://dublincore.org/groups/corporate/Seattle/ 154"
            ],
            "original_annotated_samples": [],
            "translated_annotated_samples": [
                "Nuestro método es único en el sentido de que utilizamos principalmente \"información de formato\" como el tamaño de la fuente como características en los modelos.",
                "Resulta que el uso de \"información de formato\" puede conducir a una extracción bastante precisa de los documentos generales.",
                "En los modelos, utilizamos principalmente \"información de formato\" como el tamaño de la fuente como características.",
                "Mao et al.[16] también realizó extracción de metadatos automáticos de trabajos de investigación utilizando reglas sobre \"información de formato\".",
                "Una unidad contiene no solo información de contenido (información lingüística) sino también \"información de formato\".",
                "La característica única de nuestro enfoque es que utilizamos principalmente \"información de formato\" para la extracción de título.",
                "Los resultados indican que los patrones de formatos de título existen en diferentes dominios, y es posible construir un modelo independiente de dominio mediante el uso principalmente de \"información de formato\".6.7 Adaptación del idioma Aplicamos el modelo capacitado con los datos en inglés (MS) al conjunto de datos en chino.",
                "De la adaptación del dominio y los resultados de la adaptación del lenguaje, concluimos que el uso de \"información de formato\" es la clave para una extracción exitosa de los documentos generales.6.8 Búsqueda con títulos extraídos realizamos experimentos sobre el uso de la extracción de título para la recuperación de documentos.",
                "Trabajo previo sobre la extracción de metadatos utilizó principalmente características lingüísticas en documentos, mientras que utilizamos principalmente \"información de formato\".",
                "Parecía que usar \"información de formato\" es una clave para realizar con éxito la extracción de título de los documentos generales."
            ],
            "translated_text": "",
            "candidates": [
                "formato de información",
                "información de formato",
                "formato de información",
                "información de formato",
                "formato de información",
                "información de formato",
                "formato de información",
                "información de formato",
                "formato de información",
                "información de formato",
                "formato de información",
                "información de formato",
                "formato de información",
                "información de formato",
                "formato de información",
                "información de formato",
                "formato de información",
                "información de formato",
                "formato de información",
                "información de formato"
            ],
            "error": []
        },
        "language independence": {
            "translated_key": "Independencia del idioma",
            "is_in_text": false,
            "original_annotated_sentences": [
                "Automatic Extraction of Titles from General Documents using Machine Learning Yunhua Hu1 Computer Science Department Xian Jiaotong University No 28, Xianning West Road Xian, China, 710049 yunhuahu@mail.xjtu.edu.cn Hang Li, Yunbo Cao Microsoft Research Asia 5F Sigma Center, No.",
                "49 Zhichun Road, Haidian, Beijing, China, 100080 {hangli,yucao}@microsoft.com Qinghua Zheng Computer Science Department Xian Jiaotong University No 28, Xianning West Road Xian, China, 710049 qhzheng@mail.xjtu.edu.cn Dmitriy Meyerzon Microsoft Corporation One Microsoft Way Redmond, WA, USA, 98052 dmitriym@microsoft.com ABSTRACT In this paper, we propose a machine learning approach to title extraction from general documents.",
                "By general documents, we mean documents that can belong to any one of a number of specific genres, including presentations, book chapters, technical papers, brochures, reports, and letters.",
                "Previously, methods have been proposed mainly for title extraction from research papers.",
                "It has not been clear whether it could be possible to conduct automatic title extraction from general documents.",
                "As a case study, we consider extraction from Office including Word and PowerPoint.",
                "In our approach, we annotate titles in sample documents (for Word and PowerPoint respectively) and take them as training data, train machine learning models, and perform title extraction using the trained models.",
                "Our method is unique in that we mainly utilize formatting information such as font size as features in the models.",
                "It turns out that the use of formatting information can lead to quite accurate extraction from general documents.",
                "Precision and recall for title extraction from Word is 0.810 and 0.837 respectively, and precision and recall for title extraction from PowerPoint is 0.875 and 0.895 respectively in an experiment on intranet data.",
                "Other important new findings in this work include that we can train models in one domain and apply them to another domain, and more surprisingly we can even train models in one language and apply them to another language.",
                "Moreover, we can significantly improve search ranking results in document retrieval by using the extracted titles.",
                "Categories and Subject Descriptors H.3.3 [Information Storage and Retrieval]: Information Search and Retrieval - Search Process; H.4.1 [Information Systems Applications]: Office Automation - Word processing; D.2.8 [Software Engineering]: Metrics - complexity measures, performance measures General Terms Algorithms, Experimentation, Performance. 1.",
                "INTRODUCTION Metadata of documents is useful for many kinds of document processing such as search, browsing, and filtering.",
                "Ideally, metadata is defined by the authors of documents and is then used by various systems.",
                "However, people seldom define document metadata by themselves, even when they have convenient metadata definition tools [26].",
                "Thus, how to automatically extract metadata from the bodies of documents turns out to be an important research issue.",
                "Methods for performing the task have been proposed.",
                "However, the focus was mainly on extraction from research papers.",
                "For instance, Han et al. [10] proposed a machine learning based method to conduct extraction from research papers.",
                "They formalized the problem as that of classification and employed Support Vector Machines as the classifier.",
                "They mainly used linguistic features in the model.1 In this paper, we consider metadata extraction from general documents.",
                "By general documents, we mean documents that may belong to any one of a number of specific genres.",
                "General documents are more widely available in digital libraries, intranets and the internet, and thus investigation on extraction from them is sorely needed.",
                "Research papers usually have well-formed styles and noticeable characteristics.",
                "In contrast, the styles of general documents can vary greatly.",
                "It has not been clarified whether a machine learning based approach can work well for this task.",
                "There are many types of metadata: title, author, date of creation, etc.",
                "As a case study, we consider title extraction in this paper.",
                "General documents can be in many different file formats: Microsoft Office, PDF (PS), etc.",
                "As a case study, we consider extraction from Office including Word and PowerPoint.",
                "We take a machine learning approach.",
                "We annotate titles in sample documents (for Word and PowerPoint respectively) and take them as training data to train several types of models, and perform title extraction using any one type of the trained models.",
                "In the models, we mainly utilize formatting information such as font size as features.",
                "We employ the following models: Maximum Entropy Model, Perceptron with Uneven Margins, Maximum Entropy Markov Model, and Voted Perceptron.",
                "In this paper, we also investigate the following three problems, which did not seem to have been examined previously. (1) Comparison between models: among the models above, which model performs best for title extraction; (2) Generality of model: whether it is possible to train a model on one domain and apply it to another domain, and whether it is possible to train a model in one language and apply it to another language; (3) Usefulness of extracted titles: whether extracted titles can improve document processing such as search.",
                "Experimental results indicate that our approach works well for title extraction from general documents.",
                "Our method can significantly outperform the baselines: one that always uses the first lines as titles and the other that always uses the lines in the largest font sizes as titles.",
                "Precision and recall for title extraction from Word are 0.810 and 0.837 respectively, and precision and recall for title extraction from PowerPoint are 0.875 and 0.895 respectively.",
                "It turns out that the use of format features is the key to successful title extraction. (1) We have observed that Perceptron based models perform better in terms of extraction accuracies. (2) We have empirically verified that the models trained with our approach are generic in the sense that they can be trained on one domain and applied to another, and they can be trained in one language and applied to another. (3) We have found that using the extracted titles we can significantly improve precision of document retrieval (by 10%).",
                "We conclude that we can indeed conduct reliable title extraction from general documents and use the extracted results to improve real applications.",
                "The rest of the paper is organized as follows.",
                "In section 2, we introduce related work, and in section 3, we explain the motivation and problem setting of our work.",
                "In section 4, we describe our method of title extraction, and in section 5, we describe our method of document retrieval using extracted titles.",
                "Section 6 gives our experimental results.",
                "We make concluding remarks in section 7. 2.",
                "RELATED WORK 2.1 Document Metadata Extraction Methods have been proposed for performing automatic metadata extraction from documents; however, the main focus was on extraction from research papers.",
                "The proposed methods fall into two categories: the rule based approach and the machine learning based approach.",
                "Giuffrida et al. [9], for instance, developed a rule-based system for automatically extracting metadata from research papers in Postscript.",
                "They used rules like titles are usually located on the upper portions of the first pages and they are usually in the largest font sizes.",
                "Liddy et al. [14] and Yilmazel el al. [23] performed metadata extraction from educational materials using rule-based natural language processing technologies.",
                "Mao et al. [16] also conducted automatic metadata extraction from research papers using rules on formatting information.",
                "The rule-based approach can achieve high performance.",
                "However, it also has disadvantages.",
                "It is less adaptive and robust when compared with the machine learning approach.",
                "Han et al. [10], for instance, conducted metadata extraction with the machine learning approach.",
                "They viewed the problem as that of classifying the lines in a document into the categories of metadata and proposed using Support Vector Machines as the classifier.",
                "They mainly used linguistic information as features.",
                "They reported high extraction accuracy from research papers in terms of precision and recall. 2.2 Information Extraction Metadata extraction can be viewed as an application of information extraction, in which given a sequence of instances, we identify a subsequence that represents information in which we are interested.",
                "Hidden Markov Model [6], Maximum Entropy Model [1, 4], Maximum Entropy Markov Model [17], Support Vector Machines [3], Conditional Random Field [12], and Voted Perceptron [2] are widely used information extraction models.",
                "Information extraction has been applied, for instance, to part-ofspeech tagging [20], named entity recognition [25] and table extraction [19]. 2.3 Search Using Title Information Title information is useful for document retrieval.",
                "In the system Citeseer, for instance, Giles et al. managed to extract titles from research papers and make use of the extracted titles in metadata search of papers [8].",
                "In web search, the title fields (i.e., file properties) and anchor texts of web pages (HTML documents) can be viewed as titles of the pages [5].",
                "Many search engines seem to utilize them for web page retrieval [7, 11, 18, 22].",
                "Zhang et al., found that web pages with well-defined metadata are more easily retrieved than those without well-defined metadata [24].",
                "To the best of our knowledge, no research has been conducted on using extracted titles from general documents (e.g., Office documents) for search of the documents. 146 3.",
                "MOTIVATION AND PROBLEM SETTING We consider the issue of automatically extracting titles from general documents.",
                "By general documents, we mean documents that belong to one of any number of specific genres.",
                "The documents can be presentations, books, book chapters, technical papers, brochures, reports, memos, specifications, letters, announcements, or resumes.",
                "General documents are more widely available in digital libraries, intranets, and internet, and thus investigation on title extraction from them is sorely needed.",
                "Figure 1 shows an estimate on distributions of file formats on intranet and internet [15].",
                "Office and PDF are the main file formats on the intranet.",
                "Even on the internet, the documents in the formats are still not negligible, given its extremely large size.",
                "In this paper, without loss of generality, we take Office documents as an example.",
                "Figure 1.",
                "Distributions of file formats in internet and intranet.",
                "For Office documents, users can define titles as file properties using a feature provided by Office.",
                "We found in an experiment, however, that users seldom use the feature and thus titles in file properties are usually very inaccurate.",
                "That is to say, titles in file properties are usually inconsistent with the true titles in the file bodies that are created by the authors and are visible to readers.",
                "We collected 6,000 Word and 6,000 PowerPoint documents from an intranet and the internet and examined how many titles in the file properties are correct.",
                "We found that surprisingly the accuracy was only 0.265 (cf., Section 6.3 for details).",
                "A number of reasons can be considered.",
                "For example, if one creates a new file by copying an old file, then the file property of the new file will also be copied from the old file.",
                "In another experiment, we found that Google uses the titles in file properties of Office documents in search and browsing, but the titles are not very accurate.",
                "We created 50 queries to search Word and PowerPoint documents and examined the top 15 results of each query returned by Google.",
                "We found that nearly all the titles presented in the search results were from the file properties of the documents.",
                "However, only 0.272 of them were correct.",
                "Actually, true titles usually exist at the beginnings of the bodies of documents.",
                "If we can accurately extract the titles from the bodies of documents, then we can exploit reliable title information in document processing.",
                "This is exactly the problem we address in this paper.",
                "More specifically, given a Word document, we are to extract the title from the top region of the first page.",
                "Given a PowerPoint document, we are to extract the title from the first slide.",
                "A title sometimes consists of a main title and one or two subtitles.",
                "We only consider extraction of the main title.",
                "As baselines for title extraction, we use that of always using the first lines as titles and that of always using the lines with largest font sizes as titles.",
                "Figure 2.",
                "Title extraction from Word document.",
                "Figure 3.",
                "Title extraction from PowerPoint document.",
                "Next, we define a specification for human judgments in title data annotation.",
                "The annotated data will be used in training and testing of the title extraction methods.",
                "Summary of the specification: The title of a document should be identified on the basis of common sense, if there is no difficulty in the identification.",
                "However, there are many cases in which the identification is not easy.",
                "There are some rules defined in the specification that guide identification for such cases.",
                "The rules include a title is usually in consecutive lines in the same format, a document can have no title, titles in images are not considered, a title should not contain words like draft, 147 whitepaper, etc, if it is difficult to determine which is the title, select the one in the largest font size, and if it is still difficult to determine which is the title, select the first candidate. (The specification covers all the cases we have encountered in data annotation.)",
                "Figures 2 and 3 show examples of Office documents from which we conduct title extraction.",
                "In Figure 2, Differences in Win32 API Implementations among Windows Operating Systems is the title of the Word document.",
                "Microsoft Windows on the top of this page is a picture and thus is ignored.",
                "In Figure 3, Building Competitive Advantages through an Agile Infrastructure is the title of the PowerPoint document.",
                "We have developed a tool for annotation of titles by human annotators.",
                "Figure 4 shows a snapshot of the tool.",
                "Figure 4.",
                "Title annotation tool. 4.",
                "TITLE EXTRACTION METHOD 4.1 Outline Title extraction based on machine learning consists of training and extraction.",
                "The same pre-processing step occurs before training and extraction.",
                "During pre-processing, from the top region of the first page of a Word document or the first slide of a PowerPoint document a number of units for processing are extracted.",
                "If a line (lines are separated by return symbols) only has a single format, then the line will become a unit.",
                "If a line has several parts and each of them has its own format, then each part will become a unit.",
                "Each unit will be treated as an instance in learning.",
                "A unit contains not only content information (linguistic information) but also formatting information.",
                "The input to pre-processing is a document and the output of pre-processing is a sequence of units (instances).",
                "Figure 5 shows the units obtained from the document in Figure 2.",
                "Figure 5.",
                "Example of units.",
                "In learning, the input is sequences of units where each sequence corresponds to a document.",
                "We take labeled units (labeled as title_begin, title_end, or other) in the sequences as training data and construct models for identifying whether a unit is title_begin title_end, or other.",
                "We employ four types of models: Perceptron, Maximum Entropy (ME), Perceptron Markov Model (PMM), and Maximum Entropy Markov Model (MEMM).",
                "In extraction, the input is a sequence of units from one document.",
                "We employ one type of model to identify whether a unit is title_begin, title_end, or other.",
                "We then extract units from the unit labeled with title_begin to the unit labeled with title_end.",
                "The result is the extracted title of the document.",
                "The unique characteristic of our approach is that we mainly utilize formatting information for title extraction.",
                "Our assumption is that although general documents vary in styles, their formats have certain patterns and we can learn and utilize the patterns for title extraction.",
                "This is in contrast to the work by Han et al., in which only linguistic features are used for extraction from research papers. 4.2 Models The four models actually can be considered in the same metadata extraction framework.",
                "That is why we apply them together to our current problem.",
                "Each input is a sequence of instances kxxx L21 together with a sequence of labels kyyy L21 . ix and iy represents an instance and its label, respectively ( ki ,,2,1 L= ).",
                "Recall that an instance here represents a unit.",
                "A label represents title_begin, title_end, or other.",
                "Here, k is the number of units in a document.",
                "In learning, we train a model which can be generally denoted as a conditional probability distribution )|( 11 kk XXYYP LL where iX and iY denote random variables taking instance ix and label iy as values, respectively ( ki ,,2,1 L= ).",
                "Learning Tool Extraction Tool 21121 2222122221 1121111211 nknnknn kk kk yyyxxx yyyxxx yyyxxx LL LL LL LL → → → )|(maxarg 11 mkmmkm xxyyP LL )|( 11 kk XXYYP LL Conditional Distribution mkmm xxx L21 Figure 6.",
                "Metadata extraction model.",
                "We can make assumptions about the general model in order to make it simple enough for training. 148 For example, we can assume that kYY ,,1 L are independent of each other given kXX ,,1 L .",
                "Thus, we have )|()|( )|( 11 11 kk kk XYPXYP XXYYP L LL = In this way, we decompose the model into a number of classifiers.",
                "We train the classifiers locally using the labeled data.",
                "As the classifier, we employ the Perceptron or Maximum Entropy model.",
                "We can also assume that the first order Markov property holds for kYY ,,1 L given kXX ,,1 L .",
                "Thus, we have )|()|( )|( 111 11 kkk kk XYYPXYP XXYYP −= L LL Again, we obtain a number of classifiers.",
                "However, the classifiers are conditioned on the previous label.",
                "When we employ the Percepton or Maximum Entropy model as a classifier, the models become a Percepton Markov Model or Maximum Entropy Markov Model, respectively.",
                "That is to say, the two models are more precise.",
                "In extraction, given a new sequence of instances, we resort to one of the constructed models to assign a sequence of labels to the sequence of instances, i.e., perform extraction.",
                "For Perceptron and ME, we assign labels locally and combine the results globally later using heuristics.",
                "Specifically, we first identify the most likely title_begin.",
                "Then we find the most likely title_end within three units after the title_begin.",
                "Finally, we extract as a title the units between the title_begin and the title_end.",
                "For PMM and MEMM, we employ the Viterbi algorithm to find the globally optimal label sequence.",
                "In this paper, for Perceptron, we actually employ an improved variant of it, called Perceptron with Uneven Margin [13].",
                "This version of Perceptron can work well especially when the number of positive instances and the number of negative instances differ greatly, which is exactly the case in our problem.",
                "We also employ an improved version of Perceptron Markov Model in which the Perceptron model is the so-called Voted Perceptron [2].",
                "In addition, in training, the parameters of the model are updated globally rather than locally. 4.3 Features There are two types of features: format features and linguistic features.",
                "We mainly use the former.",
                "The features are used for both the title-begin and the title-end classifiers. 4.3.1 Format Features Font Size: There are four binary features that represent the normalized font size of the unit (recall that a unit has only one type of font).",
                "If the font size of the unit is the largest in the document, then the first feature will be 1, otherwise 0.",
                "If the font size is the smallest in the document, then the fourth feature will be 1, otherwise 0.",
                "If the font size is above the average font size and not the largest in the document, then the second feature will be 1, otherwise 0.",
                "If the font size is below the average font size and not the smallest, the third feature will be 1, otherwise 0.",
                "It is necessary to conduct normalization on font sizes.",
                "For example, in one document the largest font size might be 12pt, while in another the smallest one might be 18pt.",
                "Boldface: This binary feature represents whether or not the current unit is in boldface.",
                "Alignment: There are four binary features that respectively represent the location of the current unit: left, center, right, and unknown alignment.",
                "The following format features with respect to context play an important role in title extraction.",
                "Empty Neighboring Unit: There are two binary features that represent, respectively, whether or not the previous unit and the current unit are blank lines.",
                "Font Size Change: There are two binary features that represent, respectively, whether or not the font size of the previous unit and the font size of the next unit differ from that of the current unit.",
                "Alignment Change: There are two binary features that represent, respectively, whether or not the alignment of the previous unit and the alignment of the next unit differ from that of the current one.",
                "Same Paragraph: There are two binary features that represent, respectively, whether or not the previous unit and the next unit are in the same paragraph as the current unit. 4.3.2 Linguistic Features The linguistic features are based on key words.",
                "Positive Word: This binary feature represents whether or not the current unit begins with one of the positive words.",
                "The positive words include title:, subject:, subject line: For example, in some documents the lines of titles and authors have the same formats.",
                "However, if lines begin with one of the positive words, then it is likely that they are title lines.",
                "Negative Word: This binary feature represents whether or not the current unit begins with one of the negative words.",
                "The negative words include To, By, created by, updated by, etc.",
                "There are more negative words than positive words.",
                "The above linguistic features are language dependent.",
                "Word Count: A title should not be too long.",
                "We heuristically create four intervals: [1, 2], [3, 6], [7, 9] and [9, ∞) and define one feature for each interval.",
                "If the number of words in a title falls into an interval, then the corresponding feature will be 1; otherwise 0.",
                "Ending Character: This feature represents whether the unit ends with :, -, or other special characters.",
                "A title usually does not end with such a character. 5.",
                "DOCUMENT RETRIEVAL METHOD We describe our method of document retrieval using extracted titles.",
                "Typically, in information retrieval a document is split into a number of fields including body, title, and anchor text.",
                "A ranking function in search can use different weights for different fields of 149 the document.",
                "Also, titles are typically assigned high weights, indicating that they are important for document retrieval.",
                "As explained previously, our experiment has shown that a significant number of documents actually have incorrect titles in the file properties, and thus in addition of using them we use the extracted titles as one more field of the document.",
                "By doing this, we attempt to improve the overall precision.",
                "In this paper, we employ a modification of BM25 that allows field weighting [21].",
                "As fields, we make use of body, title, extracted title and anchor.",
                "First, for each term in the query we count the term frequency in each field of the document; each field frequency is then weighted according to the corresponding weight parameter: ∑= f tfft tfwwtf Similarly, we compute the document length as a weighted sum of lengths of each field.",
                "Average document length in the corpus becomes the average of all weighted document lengths. ∑= f ff dlwwdl In our experiments we used 75.0,8.11 == bk .",
                "Weight for content was 1.0, title was 10.0, anchor was 10.0, and extracted title was 5.0. 6.",
                "EXPERIMENTAL RESULTS 6.1 Data Sets and Evaluation Measures We used two data sets in our experiments.",
                "First, we downloaded and randomly selected 5,000 Word documents and 5,000 PowerPoint documents from an intranet of Microsoft.",
                "We call it MS hereafter.",
                "Second, we downloaded and randomly selected 500 Word and 500 PowerPoint documents from the DotGov and DotCom domains on the internet, respectively.",
                "Figure 7 shows the distributions of the genres of the documents.",
                "We see that the documents are indeed general documents as we define them.",
                "Figure 7.",
                "Distributions of document genres.",
                "Third, a data set in Chinese was also downloaded from the internet.",
                "It includes 500 Word documents and 500 PowerPoint documents in Chinese.",
                "We manually labeled the titles of all the documents, on the basis of our specification.",
                "Not all the documents in the two data sets have titles.",
                "Table 1 shows the percentages of the documents having titles.",
                "We see that DotCom and DotGov have more PowerPoint documents with titles than MS.",
                "This might be because PowerPoint documents published on the internet are more formal than those on the intranet.",
                "Table 1.",
                "The portion of documents with titles Domain Type MS DotCom DotGov Word 75.7% 77.8% 75.6% PowerPoint 82.1% 93.4% 96.4% In our experiments, we conducted evaluations on title extraction in terms of precision, recall, and F-measure.",
                "The evaluation measures are defined as follows: Precision: P = A / ( A + B ) Recall: R = A / ( A + C ) F-measure: F1 = 2PR / ( P + R ) Here, A, B, C, and D are numbers of documents as those defined in Table 2.",
                "Table 2.",
                "Contingence table with regard to title extraction Is title Is not title Extracted A B Not extracted C D 6.2 Baselines We test the accuracies of the two baselines described in section 4.2.",
                "They are denoted as largest font size and first line respectively. 6.3 Accuracy of Titles in File Properties We investigate how many titles in the file properties of the documents are reliable.",
                "We view the titles annotated by humans as true titles and test how many titles in the file properties can approximately match with the true titles.",
                "We use Edit Distance to conduct the approximate match. (Approximate match is only used in this evaluation).",
                "This is because sometimes human annotated titles can be slightly different from the titles in file properties on the surface, e.g., contain extra spaces).",
                "Given string A and string B: if ( (D == 0) or ( D / ( La + Lb ) < θ ) ) then string A = string B D: Edit Distance between string A and string B La: length of string A Lb: length of string B θ: 0.1 ∑ × ++− + = t t n N wtf avwdl wdl bbk kwtf FBM )log( ))1(( )1( 25 1 1 150 Table 3.",
                "Accuracies of titles in file properties File Type Domain Precision Recall F1 MS 0.299 0.311 0.305 DotCom 0.210 0.214 0.212Word DotGov 0.182 0.177 0.180 MS 0.229 0.245 0.237 DotCom 0.185 0.186 0.186PowerPoint DotGov 0.180 0.182 0.181 6.4 Comparison with Baselines We conducted title extraction from the first data set (Word and PowerPoint in MS).",
                "As the model, we used Perceptron.",
                "We conduct 4-fold cross validation.",
                "Thus, all the results reported here are those averaged over 4 trials.",
                "Tables 4 and 5 show the results.",
                "We see that Perceptron significantly outperforms the baselines.",
                "In the evaluation, we use exact matching between the true titles annotated by humans and the extracted titles.",
                "Table 4.",
                "Accuracies of title extraction with Word Precision Recall F1 Model Perceptron 0.810 0.837 0.823 Largest font size 0.700 0.758 0.727 Baselines First line 0.707 0.767 0.736 Table 5.",
                "Accuracies of title extraction with PowerPoint Precision Recall F1 Model Perceptron 0.875 0. 895 0.885 Largest font size 0.844 0.887 0.865 Baselines First line 0.639 0.671 0.655 We see that the machine learning approach can achieve good performance in title extraction.",
                "For Word documents both precision and recall of the approach are 8 percent higher than those of the baselines.",
                "For PowerPoint both precision and recall of the approach are 2 percent higher than those of the baselines.",
                "We conduct significance tests.",
                "The results are shown in Table 6.",
                "Here, Largest denotes the baseline of using the largest font size, First denotes the baseline of using the first line.",
                "The results indicate that the improvements of machine learning over baselines are statistically significant (in the sense p-value < 0.05) Table 6.",
                "Sign test results Documents Type Sign test between p-value Perceptron vs. Largest 3.59e-26 Word Perceptron vs. First 7.12e-10 Perceptron vs. Largest 0.010 PowerPoint Perceptron vs. First 5.13e-40 We see, from the results, that the two baselines can work well for title extraction, suggesting that font size and position information are most useful features for title extraction.",
                "However, it is also obvious that using only these two features is not enough.",
                "There are cases in which all the lines have the same font size (i.e., the largest font size), or cases in which the lines with the largest font size only contain general descriptions like Confidential, White paper, etc.",
                "For those cases, the largest font size method cannot work well.",
                "For similar reasons, the first line method alone cannot work well, either.",
                "With the combination of different features (evidence in title judgment), Perceptron can outperform Largest and First.",
                "We investigate the performance of solely using linguistic features.",
                "We found that it does not work well.",
                "It seems that the format features play important roles and the linguistic features are supplements..",
                "Figure 8.",
                "An example Word document.",
                "Figure 9.",
                "An example PowerPoint document.",
                "We conducted an error analysis on the results of Perceptron.",
                "We found that the errors fell into three categories. (1) About one third of the errors were related to hard cases.",
                "In these documents, the layouts of the first pages were difficult to understand, even for humans.",
                "Figure 8 and 9 shows examples. (2) Nearly one fourth of the errors were from the documents which do not have true titles but only contain bullets.",
                "Since we conduct extraction from the top regions, it is difficult to get rid of these errors with the current approach. (3).",
                "Confusions between main titles and subtitles were another type of error.",
                "Since we only labeled the main titles as titles, the extractions of both titles were considered incorrect.",
                "This type of error does little harm to document processing like search, however. 6.5 Comparison between Models To compare the performance of different machine learning models, we conducted another experiment.",
                "Again, we perform 4-fold cross 151 validation on the first data set (MS).",
                "Table 7, 8 shows the results of all the four models.",
                "It turns out that Perceptron and PMM perform the best, followed by MEMM, and ME performs the worst.",
                "In general, the Markovian models perform better than or as well as their classifier counterparts.",
                "This seems to be because the Markovian models are trained globally, while the classifiers are trained locally.",
                "The Perceptron based models perform better than the ME based counterparts.",
                "This seems to be because the Perceptron based models are created to make better classifications, while ME models are constructed for better prediction.",
                "Table 7.",
                "Comparison between different learning models for title extraction with Word Model Precision Recall F1 Perceptron 0.810 0.837 0.823 MEMM 0.797 0.824 0.810 PMM 0.827 0.823 0.825 ME 0.801 0.621 0.699 Table 8.",
                "Comparison between different learning models for title extraction with PowerPoint Model Precision Recall F1 Perceptron 0.875 0. 895 0. 885 MEMM 0.841 0.861 0.851 PMM 0.873 0.896 0.885 ME 0.753 0.766 0.759 6.6 Domain Adaptation We apply the model trained with the first data set (MS) to the second data set (DotCom and DotGov).",
                "Tables 9-12 show the results.",
                "Table 9.",
                "Accuracies of title extraction with Word in DotGov Precision Recall F1 Model Perceptron 0.716 0.759 0.737 Largest font size 0.549 0.619 0.582Baselines First line 0.462 0.521 0.490 Table 10.",
                "Accuracies of title extraction with PowerPoint in DotGov Precision Recall F1 Model Perceptron 0.900 0.906 0.903 Largest font size 0.871 0.888 0.879Baselines First line 0.554 0.564 0.559 Table 11.",
                "Accuracies of title extraction with Word in DotCom Precisio n Recall F1 Model Perceptron 0.832 0.880 0.855 Largest font size 0.676 0.753 0.712Baselines First line 0.577 0.643 0.608 Table 12.",
                "Performance of PowerPoint document title extraction in DotCom Precisio n Recall F1 Model Perceptron 0.910 0.903 0.907 Largest font size 0.864 0.886 0.875Baselines First line 0.570 0.585 0.577 From the results, we see that the models can be adapted to different domains well.",
                "There is almost no drop in accuracy.",
                "The results indicate that the patterns of title formats exist across different domains, and it is possible to construct a domain independent model by mainly using formatting information. 6.7 Language Adaptation We apply the model trained with the data in English (MS) to the data set in Chinese.",
                "Tables 13-14 show the results.",
                "Table 13.",
                "Accuracies of title extraction with Word in Chinese Precision Recall F1 Model Perceptron 0.817 0.805 0.811 Largest font size 0.722 0.755 0.738Baselines First line 0.743 0.777 0.760 Table 14.",
                "Accuracies of title extraction with PowerPoint in Chinese Precision Recall F1 Model Perceptron 0.766 0.812 0.789 Largest font size 0.753 0.813 0.782Baselines First line 0.627 0.676 0.650 We see that the models can be adapted to a different language.",
                "There are only small drops in accuracy.",
                "Obviously, the linguistic features do not work for Chinese, but the effect of not using them is negligible.",
                "The results indicate that the patterns of title formats exist across different languages.",
                "From the domain adaptation and language adaptation results, we conclude that the use of formatting information is the key to a successful extraction from general documents. 6.8 Search with Extracted Titles We performed experiments on using title extraction for document retrieval.",
                "As a baseline, we employed BM25 without using extracted titles.",
                "The ranking mechanism was as described in Section 5.",
                "The weights were heuristically set.",
                "We did not conduct optimization on the weights.",
                "The evaluation was conducted on a corpus of 1.3 M documents crawled from the intranet of Microsoft using 100 evaluation queries obtained from this intranets search engine query logs. 50 queries were from the most popular set, while 50 queries other were chosen randomly.",
                "Users were asked to provide judgments of the degree of document relevance from a scale of 1to 5 (1 meaning detrimental, 2 - bad, 3 - fair, 4 - good and 5 - excellent). 152 Figure 10 shows the results.",
                "In the chart two sets of precision results were obtained by either considering good or excellent documents as relevant (left 3 bars with relevance threshold 0.5), or by considering only excellent documents as relevant (right 3 bars with relevance threshold 1.0) 0 0.05 0.1 0.15 0.2 0.25 0.3 0.35 0.4 0.45 P@10 P@5 Reciprocal P@10 P@5 Reciprocal 0.5 1 BM25 Anchor, Title, Body BM25 Anchor, Title, Body, ExtractedTitle Name All RelevanceThreshold Data Description Figure 10.",
                "Search ranking results.",
                "Figure 10 shows different document retrieval results with different ranking functions in terms of precision @10, precision @5 and reciprocal rank: • Blue bar - BM25 including the fields body, title (file property), and anchor text. • Purple bar - BM25 including the fields body, title (file property), anchor text, and extracted title.",
                "With the additional field of extracted title included in BM25 the precision @10 increased from 0.132 to 0.145, or by ~10%.",
                "Thus, it is safe to say that the use of extracted title can indeed improve the precision of document retrieval. 7.",
                "CONCLUSION In this paper, we have investigated the problem of automatically extracting titles from general documents.",
                "We have tried using a machine learning approach to address the problem.",
                "Previous work showed that the machine learning approach can work well for metadata extraction from research papers.",
                "In this paper, we showed that the approach can work for extraction from general documents as well.",
                "Our experimental results indicated that the machine learning approach can work significantly better than the baselines in title extraction from Office documents.",
                "Previous work on metadata extraction mainly used linguistic features in documents, while we mainly used formatting information.",
                "It appeared that using formatting information is a key for successfully conducting title extraction from general documents.",
                "We tried different machine learning models including Perceptron, Maximum Entropy, Maximum Entropy Markov Model, and Voted Perceptron.",
                "We found that the performance of the Perceptorn models was the best.",
                "We applied models constructed in one domain to another domain and applied models trained in one language to another language.",
                "We found that the accuracies did not drop substantially across different domains and across different languages, indicating that the models were generic.",
                "We also attempted to use the extracted titles in document retrieval.",
                "We observed a significant improvement in document ranking performance for search when using extracted title information.",
                "All the above investigations were not conducted in previous work, and through our investigations we verified the generality and the significance of the title extraction approach. 8.",
                "ACKNOWLEDGEMENTS We thank Chunyu Wei and Bojuan Zhao for their work on data annotation.",
                "We acknowledge Jinzhu Li for his assistance in conducting the experiments.",
                "We thank Ming Zhou, John Chen, Jun Xu, and the anonymous reviewers of JCDL05 for their valuable comments on this paper. 9.",
                "REFERENCES [1] Berger, A. L., Della Pietra, S. A., and Della Pietra, V. J.",
                "A maximum entropy approach to natural language processing.",
                "Computational Linguistics, 22:39-71, 1996. [2] Collins, M. Discriminative training methods for hidden markov models: theory and experiments with perceptron algorithms.",
                "In Proceedings of Conference on Empirical Methods in Natural Language Processing, 1-8, 2002. [3] Cortes, C. and Vapnik, V. Support-vector networks.",
                "Machine Learning, 20:273-297, 1995. [4] Chieu, H. L. and Ng, H. T. A maximum entropy approach to information extraction from semi-structured and free text.",
                "In Proceedings of the Eighteenth National Conference on Artificial Intelligence, 768-791, 2002. [5] Evans, D. K., Klavans, J. L., and McKeown, K. R. Columbia newsblaster: multilingual news summarization on the Web.",
                "In Proceedings of Human Language Technology conference / North American chapter of the Association for Computational Linguistics annual meeting, 1-4, 2004. [6] Ghahramani, Z. and Jordan, M. I. Factorial hidden markov models.",
                "Machine Learning, 29:245-273, 1997. [7] Gheel, J. and Anderson, T. Data and metadata for finding and reminding, In Proceedings of the 1999 International Conference on Information Visualization, 446-451,1999. [8] Giles, C. L., Petinot, Y., Teregowda P. B., Han, H., Lawrence, S., Rangaswamy, A., and Pal, N. eBizSearch: a niche search engine for e-Business.",
                "In Proceedings of the 26th Annual International ACM SIGIR Conference on Research and Development in Information Retrieval, 413414, 2003. [9] Giuffrida, G., Shek, E. C., and Yang, J. Knowledge-based metadata extraction from PostScript files.",
                "In Proceedings of the Fifth ACM Conference on Digital Libraries, 77-84, 2000. [10] Han, H., Giles, C. L., Manavoglu, E., Zha, H., Zhang, Z., and Fox, E. A.",
                "Automatic document metadata extraction using support vector machines.",
                "In Proceedings of the Third ACM/IEEE-CS Joint Conference on Digital Libraries, 37-48, 2003. [11] Kobayashi, M., and Takeda, K. Information retrieval on the Web.",
                "ACM Computing Surveys, 32:144-173, 2000. [12] Lafferty, J., McCallum, A., and Pereira, F. Conditional random fields: probabilistic models for segmenting and 153 labeling sequence data.",
                "In Proceedings of the Eighteenth International Conference on Machine Learning, 282-289, 2001. [13] Li, Y., Zaragoza, H., Herbrich, R., Shawe-Taylor J., and Kandola, J. S. The perceptron algorithm with uneven margins.",
                "In Proceedings of the Nineteenth International Conference on Machine Learning, 379-386, 2002. [14] Liddy, E. D., Sutton, S., Allen, E., Harwell, S., Corieri, S., Yilmazel, O., Ozgencil, N. E., Diekema, A., McCracken, N., and Silverstein, J.",
                "Automatic Metadata generation & evaluation.",
                "In Proceedings of the 25th Annual International ACM SIGIR Conference on Research and Development in Information Retrieval, 401-402, 2002. [15] Littlefield, A.",
                "Effective enterprise information retrieval across new content formats.",
                "In Proceedings of the Seventh Search Engine Conference, http://www.infonortics.com/searchengines/sh02/02prog.html, 2002. [16] Mao, S., Kim, J. W., and Thoma, G. R. A dynamic feature generation system for automated metadata extraction in preservation of digital materials.",
                "In Proceedings of the First International Workshop on Document Image Analysis for Libraries, 225-232, 2004. [17] McCallum, A., Freitag, D., and Pereira, F. Maximum entropy markov models for information extraction and segmentation.",
                "In Proceedings of the Seventeenth International Conference on Machine Learning, 591-598, 2000. [18] Murphy, L. D. Digital document metadata in organizations: roles, analytical approaches, and future research directions.",
                "In Proceedings of the Thirty-First Annual Hawaii International Conference on System Sciences, 267-276, 1998. [19] Pinto, D., McCallum, A., Wei, X., and Croft, W. B.",
                "Table extraction using conditional random fields.",
                "In Proceedings of the 26th Annual International ACM SIGIR Conference on Research and Development in Information Retrieval, 235242, 2003. [20] Ratnaparkhi, A. Unsupervised statistical models for prepositional phrase attachment.",
                "In Proceedings of the Seventeenth International Conference on Computational Linguistics. 1079-1085, 1998. [21] Robertson, S., Zaragoza, H., and Taylor, M. Simple BM25 extension to multiple weighted fields, In Proceedings of ACM Thirteenth Conference on Information and Knowledge Management, 42-49, 2004. [22] Yi, J. and Sundaresan, N. Metadata based Web mining for relevance, In Proceedings of the 2000 International Symposium on Database Engineering & Applications, 113121, 2000. [23] Yilmazel, O., Finneran, C. M., and Liddy, E. D. MetaExtract: An NLP system to automatically assign metadata.",
                "In Proceedings of the 2004 Joint ACM/IEEE Conference on Digital Libraries, 241-242, 2004. [24] Zhang, J. and Dimitroff, A. Internet search engines response to metadata Dublin Core implementation.",
                "Journal of Information Science, 30:310-320, 2004. [25] Zhang, L., Pan, Y., and Zhang, T. Recognising and using named entities: focused named entity recognition using machine learning.",
                "In Proceedings of the 27th Annual International ACM SIGIR Conference on Research and Development in Information Retrieval, 281-288, 2004. [26] http://dublincore.org/groups/corporate/Seattle/ 154"
            ],
            "original_annotated_samples": [],
            "translated_annotated_samples": [],
            "translated_text": "",
            "candidates": [],
            "error": []
        },
        "metada of document": {
            "translated_key": "metadatos de documento",
            "is_in_text": false,
            "original_annotated_sentences": [
                "Automatic Extraction of Titles from General Documents using Machine Learning Yunhua Hu1 Computer Science Department Xian Jiaotong University No 28, Xianning West Road Xian, China, 710049 yunhuahu@mail.xjtu.edu.cn Hang Li, Yunbo Cao Microsoft Research Asia 5F Sigma Center, No.",
                "49 Zhichun Road, Haidian, Beijing, China, 100080 {hangli,yucao}@microsoft.com Qinghua Zheng Computer Science Department Xian Jiaotong University No 28, Xianning West Road Xian, China, 710049 qhzheng@mail.xjtu.edu.cn Dmitriy Meyerzon Microsoft Corporation One Microsoft Way Redmond, WA, USA, 98052 dmitriym@microsoft.com ABSTRACT In this paper, we propose a machine learning approach to title extraction from general documents.",
                "By general documents, we mean documents that can belong to any one of a number of specific genres, including presentations, book chapters, technical papers, brochures, reports, and letters.",
                "Previously, methods have been proposed mainly for title extraction from research papers.",
                "It has not been clear whether it could be possible to conduct automatic title extraction from general documents.",
                "As a case study, we consider extraction from Office including Word and PowerPoint.",
                "In our approach, we annotate titles in sample documents (for Word and PowerPoint respectively) and take them as training data, train machine learning models, and perform title extraction using the trained models.",
                "Our method is unique in that we mainly utilize formatting information such as font size as features in the models.",
                "It turns out that the use of formatting information can lead to quite accurate extraction from general documents.",
                "Precision and recall for title extraction from Word is 0.810 and 0.837 respectively, and precision and recall for title extraction from PowerPoint is 0.875 and 0.895 respectively in an experiment on intranet data.",
                "Other important new findings in this work include that we can train models in one domain and apply them to another domain, and more surprisingly we can even train models in one language and apply them to another language.",
                "Moreover, we can significantly improve search ranking results in document retrieval by using the extracted titles.",
                "Categories and Subject Descriptors H.3.3 [Information Storage and Retrieval]: Information Search and Retrieval - Search Process; H.4.1 [Information Systems Applications]: Office Automation - Word processing; D.2.8 [Software Engineering]: Metrics - complexity measures, performance measures General Terms Algorithms, Experimentation, Performance. 1.",
                "INTRODUCTION Metadata of documents is useful for many kinds of document processing such as search, browsing, and filtering.",
                "Ideally, metadata is defined by the authors of documents and is then used by various systems.",
                "However, people seldom define document metadata by themselves, even when they have convenient metadata definition tools [26].",
                "Thus, how to automatically extract metadata from the bodies of documents turns out to be an important research issue.",
                "Methods for performing the task have been proposed.",
                "However, the focus was mainly on extraction from research papers.",
                "For instance, Han et al. [10] proposed a machine learning based method to conduct extraction from research papers.",
                "They formalized the problem as that of classification and employed Support Vector Machines as the classifier.",
                "They mainly used linguistic features in the model.1 In this paper, we consider metadata extraction from general documents.",
                "By general documents, we mean documents that may belong to any one of a number of specific genres.",
                "General documents are more widely available in digital libraries, intranets and the internet, and thus investigation on extraction from them is sorely needed.",
                "Research papers usually have well-formed styles and noticeable characteristics.",
                "In contrast, the styles of general documents can vary greatly.",
                "It has not been clarified whether a machine learning based approach can work well for this task.",
                "There are many types of metadata: title, author, date of creation, etc.",
                "As a case study, we consider title extraction in this paper.",
                "General documents can be in many different file formats: Microsoft Office, PDF (PS), etc.",
                "As a case study, we consider extraction from Office including Word and PowerPoint.",
                "We take a machine learning approach.",
                "We annotate titles in sample documents (for Word and PowerPoint respectively) and take them as training data to train several types of models, and perform title extraction using any one type of the trained models.",
                "In the models, we mainly utilize formatting information such as font size as features.",
                "We employ the following models: Maximum Entropy Model, Perceptron with Uneven Margins, Maximum Entropy Markov Model, and Voted Perceptron.",
                "In this paper, we also investigate the following three problems, which did not seem to have been examined previously. (1) Comparison between models: among the models above, which model performs best for title extraction; (2) Generality of model: whether it is possible to train a model on one domain and apply it to another domain, and whether it is possible to train a model in one language and apply it to another language; (3) Usefulness of extracted titles: whether extracted titles can improve document processing such as search.",
                "Experimental results indicate that our approach works well for title extraction from general documents.",
                "Our method can significantly outperform the baselines: one that always uses the first lines as titles and the other that always uses the lines in the largest font sizes as titles.",
                "Precision and recall for title extraction from Word are 0.810 and 0.837 respectively, and precision and recall for title extraction from PowerPoint are 0.875 and 0.895 respectively.",
                "It turns out that the use of format features is the key to successful title extraction. (1) We have observed that Perceptron based models perform better in terms of extraction accuracies. (2) We have empirically verified that the models trained with our approach are generic in the sense that they can be trained on one domain and applied to another, and they can be trained in one language and applied to another. (3) We have found that using the extracted titles we can significantly improve precision of document retrieval (by 10%).",
                "We conclude that we can indeed conduct reliable title extraction from general documents and use the extracted results to improve real applications.",
                "The rest of the paper is organized as follows.",
                "In section 2, we introduce related work, and in section 3, we explain the motivation and problem setting of our work.",
                "In section 4, we describe our method of title extraction, and in section 5, we describe our method of document retrieval using extracted titles.",
                "Section 6 gives our experimental results.",
                "We make concluding remarks in section 7. 2.",
                "RELATED WORK 2.1 Document Metadata Extraction Methods have been proposed for performing automatic metadata extraction from documents; however, the main focus was on extraction from research papers.",
                "The proposed methods fall into two categories: the rule based approach and the machine learning based approach.",
                "Giuffrida et al. [9], for instance, developed a rule-based system for automatically extracting metadata from research papers in Postscript.",
                "They used rules like titles are usually located on the upper portions of the first pages and they are usually in the largest font sizes.",
                "Liddy et al. [14] and Yilmazel el al. [23] performed metadata extraction from educational materials using rule-based natural language processing technologies.",
                "Mao et al. [16] also conducted automatic metadata extraction from research papers using rules on formatting information.",
                "The rule-based approach can achieve high performance.",
                "However, it also has disadvantages.",
                "It is less adaptive and robust when compared with the machine learning approach.",
                "Han et al. [10], for instance, conducted metadata extraction with the machine learning approach.",
                "They viewed the problem as that of classifying the lines in a document into the categories of metadata and proposed using Support Vector Machines as the classifier.",
                "They mainly used linguistic information as features.",
                "They reported high extraction accuracy from research papers in terms of precision and recall. 2.2 Information Extraction Metadata extraction can be viewed as an application of information extraction, in which given a sequence of instances, we identify a subsequence that represents information in which we are interested.",
                "Hidden Markov Model [6], Maximum Entropy Model [1, 4], Maximum Entropy Markov Model [17], Support Vector Machines [3], Conditional Random Field [12], and Voted Perceptron [2] are widely used information extraction models.",
                "Information extraction has been applied, for instance, to part-ofspeech tagging [20], named entity recognition [25] and table extraction [19]. 2.3 Search Using Title Information Title information is useful for document retrieval.",
                "In the system Citeseer, for instance, Giles et al. managed to extract titles from research papers and make use of the extracted titles in metadata search of papers [8].",
                "In web search, the title fields (i.e., file properties) and anchor texts of web pages (HTML documents) can be viewed as titles of the pages [5].",
                "Many search engines seem to utilize them for web page retrieval [7, 11, 18, 22].",
                "Zhang et al., found that web pages with well-defined metadata are more easily retrieved than those without well-defined metadata [24].",
                "To the best of our knowledge, no research has been conducted on using extracted titles from general documents (e.g., Office documents) for search of the documents. 146 3.",
                "MOTIVATION AND PROBLEM SETTING We consider the issue of automatically extracting titles from general documents.",
                "By general documents, we mean documents that belong to one of any number of specific genres.",
                "The documents can be presentations, books, book chapters, technical papers, brochures, reports, memos, specifications, letters, announcements, or resumes.",
                "General documents are more widely available in digital libraries, intranets, and internet, and thus investigation on title extraction from them is sorely needed.",
                "Figure 1 shows an estimate on distributions of file formats on intranet and internet [15].",
                "Office and PDF are the main file formats on the intranet.",
                "Even on the internet, the documents in the formats are still not negligible, given its extremely large size.",
                "In this paper, without loss of generality, we take Office documents as an example.",
                "Figure 1.",
                "Distributions of file formats in internet and intranet.",
                "For Office documents, users can define titles as file properties using a feature provided by Office.",
                "We found in an experiment, however, that users seldom use the feature and thus titles in file properties are usually very inaccurate.",
                "That is to say, titles in file properties are usually inconsistent with the true titles in the file bodies that are created by the authors and are visible to readers.",
                "We collected 6,000 Word and 6,000 PowerPoint documents from an intranet and the internet and examined how many titles in the file properties are correct.",
                "We found that surprisingly the accuracy was only 0.265 (cf., Section 6.3 for details).",
                "A number of reasons can be considered.",
                "For example, if one creates a new file by copying an old file, then the file property of the new file will also be copied from the old file.",
                "In another experiment, we found that Google uses the titles in file properties of Office documents in search and browsing, but the titles are not very accurate.",
                "We created 50 queries to search Word and PowerPoint documents and examined the top 15 results of each query returned by Google.",
                "We found that nearly all the titles presented in the search results were from the file properties of the documents.",
                "However, only 0.272 of them were correct.",
                "Actually, true titles usually exist at the beginnings of the bodies of documents.",
                "If we can accurately extract the titles from the bodies of documents, then we can exploit reliable title information in document processing.",
                "This is exactly the problem we address in this paper.",
                "More specifically, given a Word document, we are to extract the title from the top region of the first page.",
                "Given a PowerPoint document, we are to extract the title from the first slide.",
                "A title sometimes consists of a main title and one or two subtitles.",
                "We only consider extraction of the main title.",
                "As baselines for title extraction, we use that of always using the first lines as titles and that of always using the lines with largest font sizes as titles.",
                "Figure 2.",
                "Title extraction from Word document.",
                "Figure 3.",
                "Title extraction from PowerPoint document.",
                "Next, we define a specification for human judgments in title data annotation.",
                "The annotated data will be used in training and testing of the title extraction methods.",
                "Summary of the specification: The title of a document should be identified on the basis of common sense, if there is no difficulty in the identification.",
                "However, there are many cases in which the identification is not easy.",
                "There are some rules defined in the specification that guide identification for such cases.",
                "The rules include a title is usually in consecutive lines in the same format, a document can have no title, titles in images are not considered, a title should not contain words like draft, 147 whitepaper, etc, if it is difficult to determine which is the title, select the one in the largest font size, and if it is still difficult to determine which is the title, select the first candidate. (The specification covers all the cases we have encountered in data annotation.)",
                "Figures 2 and 3 show examples of Office documents from which we conduct title extraction.",
                "In Figure 2, Differences in Win32 API Implementations among Windows Operating Systems is the title of the Word document.",
                "Microsoft Windows on the top of this page is a picture and thus is ignored.",
                "In Figure 3, Building Competitive Advantages through an Agile Infrastructure is the title of the PowerPoint document.",
                "We have developed a tool for annotation of titles by human annotators.",
                "Figure 4 shows a snapshot of the tool.",
                "Figure 4.",
                "Title annotation tool. 4.",
                "TITLE EXTRACTION METHOD 4.1 Outline Title extraction based on machine learning consists of training and extraction.",
                "The same pre-processing step occurs before training and extraction.",
                "During pre-processing, from the top region of the first page of a Word document or the first slide of a PowerPoint document a number of units for processing are extracted.",
                "If a line (lines are separated by return symbols) only has a single format, then the line will become a unit.",
                "If a line has several parts and each of them has its own format, then each part will become a unit.",
                "Each unit will be treated as an instance in learning.",
                "A unit contains not only content information (linguistic information) but also formatting information.",
                "The input to pre-processing is a document and the output of pre-processing is a sequence of units (instances).",
                "Figure 5 shows the units obtained from the document in Figure 2.",
                "Figure 5.",
                "Example of units.",
                "In learning, the input is sequences of units where each sequence corresponds to a document.",
                "We take labeled units (labeled as title_begin, title_end, or other) in the sequences as training data and construct models for identifying whether a unit is title_begin title_end, or other.",
                "We employ four types of models: Perceptron, Maximum Entropy (ME), Perceptron Markov Model (PMM), and Maximum Entropy Markov Model (MEMM).",
                "In extraction, the input is a sequence of units from one document.",
                "We employ one type of model to identify whether a unit is title_begin, title_end, or other.",
                "We then extract units from the unit labeled with title_begin to the unit labeled with title_end.",
                "The result is the extracted title of the document.",
                "The unique characteristic of our approach is that we mainly utilize formatting information for title extraction.",
                "Our assumption is that although general documents vary in styles, their formats have certain patterns and we can learn and utilize the patterns for title extraction.",
                "This is in contrast to the work by Han et al., in which only linguistic features are used for extraction from research papers. 4.2 Models The four models actually can be considered in the same metadata extraction framework.",
                "That is why we apply them together to our current problem.",
                "Each input is a sequence of instances kxxx L21 together with a sequence of labels kyyy L21 . ix and iy represents an instance and its label, respectively ( ki ,,2,1 L= ).",
                "Recall that an instance here represents a unit.",
                "A label represents title_begin, title_end, or other.",
                "Here, k is the number of units in a document.",
                "In learning, we train a model which can be generally denoted as a conditional probability distribution )|( 11 kk XXYYP LL where iX and iY denote random variables taking instance ix and label iy as values, respectively ( ki ,,2,1 L= ).",
                "Learning Tool Extraction Tool 21121 2222122221 1121111211 nknnknn kk kk yyyxxx yyyxxx yyyxxx LL LL LL LL → → → )|(maxarg 11 mkmmkm xxyyP LL )|( 11 kk XXYYP LL Conditional Distribution mkmm xxx L21 Figure 6.",
                "Metadata extraction model.",
                "We can make assumptions about the general model in order to make it simple enough for training. 148 For example, we can assume that kYY ,,1 L are independent of each other given kXX ,,1 L .",
                "Thus, we have )|()|( )|( 11 11 kk kk XYPXYP XXYYP L LL = In this way, we decompose the model into a number of classifiers.",
                "We train the classifiers locally using the labeled data.",
                "As the classifier, we employ the Perceptron or Maximum Entropy model.",
                "We can also assume that the first order Markov property holds for kYY ,,1 L given kXX ,,1 L .",
                "Thus, we have )|()|( )|( 111 11 kkk kk XYYPXYP XXYYP −= L LL Again, we obtain a number of classifiers.",
                "However, the classifiers are conditioned on the previous label.",
                "When we employ the Percepton or Maximum Entropy model as a classifier, the models become a Percepton Markov Model or Maximum Entropy Markov Model, respectively.",
                "That is to say, the two models are more precise.",
                "In extraction, given a new sequence of instances, we resort to one of the constructed models to assign a sequence of labels to the sequence of instances, i.e., perform extraction.",
                "For Perceptron and ME, we assign labels locally and combine the results globally later using heuristics.",
                "Specifically, we first identify the most likely title_begin.",
                "Then we find the most likely title_end within three units after the title_begin.",
                "Finally, we extract as a title the units between the title_begin and the title_end.",
                "For PMM and MEMM, we employ the Viterbi algorithm to find the globally optimal label sequence.",
                "In this paper, for Perceptron, we actually employ an improved variant of it, called Perceptron with Uneven Margin [13].",
                "This version of Perceptron can work well especially when the number of positive instances and the number of negative instances differ greatly, which is exactly the case in our problem.",
                "We also employ an improved version of Perceptron Markov Model in which the Perceptron model is the so-called Voted Perceptron [2].",
                "In addition, in training, the parameters of the model are updated globally rather than locally. 4.3 Features There are two types of features: format features and linguistic features.",
                "We mainly use the former.",
                "The features are used for both the title-begin and the title-end classifiers. 4.3.1 Format Features Font Size: There are four binary features that represent the normalized font size of the unit (recall that a unit has only one type of font).",
                "If the font size of the unit is the largest in the document, then the first feature will be 1, otherwise 0.",
                "If the font size is the smallest in the document, then the fourth feature will be 1, otherwise 0.",
                "If the font size is above the average font size and not the largest in the document, then the second feature will be 1, otherwise 0.",
                "If the font size is below the average font size and not the smallest, the third feature will be 1, otherwise 0.",
                "It is necessary to conduct normalization on font sizes.",
                "For example, in one document the largest font size might be 12pt, while in another the smallest one might be 18pt.",
                "Boldface: This binary feature represents whether or not the current unit is in boldface.",
                "Alignment: There are four binary features that respectively represent the location of the current unit: left, center, right, and unknown alignment.",
                "The following format features with respect to context play an important role in title extraction.",
                "Empty Neighboring Unit: There are two binary features that represent, respectively, whether or not the previous unit and the current unit are blank lines.",
                "Font Size Change: There are two binary features that represent, respectively, whether or not the font size of the previous unit and the font size of the next unit differ from that of the current unit.",
                "Alignment Change: There are two binary features that represent, respectively, whether or not the alignment of the previous unit and the alignment of the next unit differ from that of the current one.",
                "Same Paragraph: There are two binary features that represent, respectively, whether or not the previous unit and the next unit are in the same paragraph as the current unit. 4.3.2 Linguistic Features The linguistic features are based on key words.",
                "Positive Word: This binary feature represents whether or not the current unit begins with one of the positive words.",
                "The positive words include title:, subject:, subject line: For example, in some documents the lines of titles and authors have the same formats.",
                "However, if lines begin with one of the positive words, then it is likely that they are title lines.",
                "Negative Word: This binary feature represents whether or not the current unit begins with one of the negative words.",
                "The negative words include To, By, created by, updated by, etc.",
                "There are more negative words than positive words.",
                "The above linguistic features are language dependent.",
                "Word Count: A title should not be too long.",
                "We heuristically create four intervals: [1, 2], [3, 6], [7, 9] and [9, ∞) and define one feature for each interval.",
                "If the number of words in a title falls into an interval, then the corresponding feature will be 1; otherwise 0.",
                "Ending Character: This feature represents whether the unit ends with :, -, or other special characters.",
                "A title usually does not end with such a character. 5.",
                "DOCUMENT RETRIEVAL METHOD We describe our method of document retrieval using extracted titles.",
                "Typically, in information retrieval a document is split into a number of fields including body, title, and anchor text.",
                "A ranking function in search can use different weights for different fields of 149 the document.",
                "Also, titles are typically assigned high weights, indicating that they are important for document retrieval.",
                "As explained previously, our experiment has shown that a significant number of documents actually have incorrect titles in the file properties, and thus in addition of using them we use the extracted titles as one more field of the document.",
                "By doing this, we attempt to improve the overall precision.",
                "In this paper, we employ a modification of BM25 that allows field weighting [21].",
                "As fields, we make use of body, title, extracted title and anchor.",
                "First, for each term in the query we count the term frequency in each field of the document; each field frequency is then weighted according to the corresponding weight parameter: ∑= f tfft tfwwtf Similarly, we compute the document length as a weighted sum of lengths of each field.",
                "Average document length in the corpus becomes the average of all weighted document lengths. ∑= f ff dlwwdl In our experiments we used 75.0,8.11 == bk .",
                "Weight for content was 1.0, title was 10.0, anchor was 10.0, and extracted title was 5.0. 6.",
                "EXPERIMENTAL RESULTS 6.1 Data Sets and Evaluation Measures We used two data sets in our experiments.",
                "First, we downloaded and randomly selected 5,000 Word documents and 5,000 PowerPoint documents from an intranet of Microsoft.",
                "We call it MS hereafter.",
                "Second, we downloaded and randomly selected 500 Word and 500 PowerPoint documents from the DotGov and DotCom domains on the internet, respectively.",
                "Figure 7 shows the distributions of the genres of the documents.",
                "We see that the documents are indeed general documents as we define them.",
                "Figure 7.",
                "Distributions of document genres.",
                "Third, a data set in Chinese was also downloaded from the internet.",
                "It includes 500 Word documents and 500 PowerPoint documents in Chinese.",
                "We manually labeled the titles of all the documents, on the basis of our specification.",
                "Not all the documents in the two data sets have titles.",
                "Table 1 shows the percentages of the documents having titles.",
                "We see that DotCom and DotGov have more PowerPoint documents with titles than MS.",
                "This might be because PowerPoint documents published on the internet are more formal than those on the intranet.",
                "Table 1.",
                "The portion of documents with titles Domain Type MS DotCom DotGov Word 75.7% 77.8% 75.6% PowerPoint 82.1% 93.4% 96.4% In our experiments, we conducted evaluations on title extraction in terms of precision, recall, and F-measure.",
                "The evaluation measures are defined as follows: Precision: P = A / ( A + B ) Recall: R = A / ( A + C ) F-measure: F1 = 2PR / ( P + R ) Here, A, B, C, and D are numbers of documents as those defined in Table 2.",
                "Table 2.",
                "Contingence table with regard to title extraction Is title Is not title Extracted A B Not extracted C D 6.2 Baselines We test the accuracies of the two baselines described in section 4.2.",
                "They are denoted as largest font size and first line respectively. 6.3 Accuracy of Titles in File Properties We investigate how many titles in the file properties of the documents are reliable.",
                "We view the titles annotated by humans as true titles and test how many titles in the file properties can approximately match with the true titles.",
                "We use Edit Distance to conduct the approximate match. (Approximate match is only used in this evaluation).",
                "This is because sometimes human annotated titles can be slightly different from the titles in file properties on the surface, e.g., contain extra spaces).",
                "Given string A and string B: if ( (D == 0) or ( D / ( La + Lb ) < θ ) ) then string A = string B D: Edit Distance between string A and string B La: length of string A Lb: length of string B θ: 0.1 ∑ × ++− + = t t n N wtf avwdl wdl bbk kwtf FBM )log( ))1(( )1( 25 1 1 150 Table 3.",
                "Accuracies of titles in file properties File Type Domain Precision Recall F1 MS 0.299 0.311 0.305 DotCom 0.210 0.214 0.212Word DotGov 0.182 0.177 0.180 MS 0.229 0.245 0.237 DotCom 0.185 0.186 0.186PowerPoint DotGov 0.180 0.182 0.181 6.4 Comparison with Baselines We conducted title extraction from the first data set (Word and PowerPoint in MS).",
                "As the model, we used Perceptron.",
                "We conduct 4-fold cross validation.",
                "Thus, all the results reported here are those averaged over 4 trials.",
                "Tables 4 and 5 show the results.",
                "We see that Perceptron significantly outperforms the baselines.",
                "In the evaluation, we use exact matching between the true titles annotated by humans and the extracted titles.",
                "Table 4.",
                "Accuracies of title extraction with Word Precision Recall F1 Model Perceptron 0.810 0.837 0.823 Largest font size 0.700 0.758 0.727 Baselines First line 0.707 0.767 0.736 Table 5.",
                "Accuracies of title extraction with PowerPoint Precision Recall F1 Model Perceptron 0.875 0. 895 0.885 Largest font size 0.844 0.887 0.865 Baselines First line 0.639 0.671 0.655 We see that the machine learning approach can achieve good performance in title extraction.",
                "For Word documents both precision and recall of the approach are 8 percent higher than those of the baselines.",
                "For PowerPoint both precision and recall of the approach are 2 percent higher than those of the baselines.",
                "We conduct significance tests.",
                "The results are shown in Table 6.",
                "Here, Largest denotes the baseline of using the largest font size, First denotes the baseline of using the first line.",
                "The results indicate that the improvements of machine learning over baselines are statistically significant (in the sense p-value < 0.05) Table 6.",
                "Sign test results Documents Type Sign test between p-value Perceptron vs. Largest 3.59e-26 Word Perceptron vs. First 7.12e-10 Perceptron vs. Largest 0.010 PowerPoint Perceptron vs. First 5.13e-40 We see, from the results, that the two baselines can work well for title extraction, suggesting that font size and position information are most useful features for title extraction.",
                "However, it is also obvious that using only these two features is not enough.",
                "There are cases in which all the lines have the same font size (i.e., the largest font size), or cases in which the lines with the largest font size only contain general descriptions like Confidential, White paper, etc.",
                "For those cases, the largest font size method cannot work well.",
                "For similar reasons, the first line method alone cannot work well, either.",
                "With the combination of different features (evidence in title judgment), Perceptron can outperform Largest and First.",
                "We investigate the performance of solely using linguistic features.",
                "We found that it does not work well.",
                "It seems that the format features play important roles and the linguistic features are supplements..",
                "Figure 8.",
                "An example Word document.",
                "Figure 9.",
                "An example PowerPoint document.",
                "We conducted an error analysis on the results of Perceptron.",
                "We found that the errors fell into three categories. (1) About one third of the errors were related to hard cases.",
                "In these documents, the layouts of the first pages were difficult to understand, even for humans.",
                "Figure 8 and 9 shows examples. (2) Nearly one fourth of the errors were from the documents which do not have true titles but only contain bullets.",
                "Since we conduct extraction from the top regions, it is difficult to get rid of these errors with the current approach. (3).",
                "Confusions between main titles and subtitles were another type of error.",
                "Since we only labeled the main titles as titles, the extractions of both titles were considered incorrect.",
                "This type of error does little harm to document processing like search, however. 6.5 Comparison between Models To compare the performance of different machine learning models, we conducted another experiment.",
                "Again, we perform 4-fold cross 151 validation on the first data set (MS).",
                "Table 7, 8 shows the results of all the four models.",
                "It turns out that Perceptron and PMM perform the best, followed by MEMM, and ME performs the worst.",
                "In general, the Markovian models perform better than or as well as their classifier counterparts.",
                "This seems to be because the Markovian models are trained globally, while the classifiers are trained locally.",
                "The Perceptron based models perform better than the ME based counterparts.",
                "This seems to be because the Perceptron based models are created to make better classifications, while ME models are constructed for better prediction.",
                "Table 7.",
                "Comparison between different learning models for title extraction with Word Model Precision Recall F1 Perceptron 0.810 0.837 0.823 MEMM 0.797 0.824 0.810 PMM 0.827 0.823 0.825 ME 0.801 0.621 0.699 Table 8.",
                "Comparison between different learning models for title extraction with PowerPoint Model Precision Recall F1 Perceptron 0.875 0. 895 0. 885 MEMM 0.841 0.861 0.851 PMM 0.873 0.896 0.885 ME 0.753 0.766 0.759 6.6 Domain Adaptation We apply the model trained with the first data set (MS) to the second data set (DotCom and DotGov).",
                "Tables 9-12 show the results.",
                "Table 9.",
                "Accuracies of title extraction with Word in DotGov Precision Recall F1 Model Perceptron 0.716 0.759 0.737 Largest font size 0.549 0.619 0.582Baselines First line 0.462 0.521 0.490 Table 10.",
                "Accuracies of title extraction with PowerPoint in DotGov Precision Recall F1 Model Perceptron 0.900 0.906 0.903 Largest font size 0.871 0.888 0.879Baselines First line 0.554 0.564 0.559 Table 11.",
                "Accuracies of title extraction with Word in DotCom Precisio n Recall F1 Model Perceptron 0.832 0.880 0.855 Largest font size 0.676 0.753 0.712Baselines First line 0.577 0.643 0.608 Table 12.",
                "Performance of PowerPoint document title extraction in DotCom Precisio n Recall F1 Model Perceptron 0.910 0.903 0.907 Largest font size 0.864 0.886 0.875Baselines First line 0.570 0.585 0.577 From the results, we see that the models can be adapted to different domains well.",
                "There is almost no drop in accuracy.",
                "The results indicate that the patterns of title formats exist across different domains, and it is possible to construct a domain independent model by mainly using formatting information. 6.7 Language Adaptation We apply the model trained with the data in English (MS) to the data set in Chinese.",
                "Tables 13-14 show the results.",
                "Table 13.",
                "Accuracies of title extraction with Word in Chinese Precision Recall F1 Model Perceptron 0.817 0.805 0.811 Largest font size 0.722 0.755 0.738Baselines First line 0.743 0.777 0.760 Table 14.",
                "Accuracies of title extraction with PowerPoint in Chinese Precision Recall F1 Model Perceptron 0.766 0.812 0.789 Largest font size 0.753 0.813 0.782Baselines First line 0.627 0.676 0.650 We see that the models can be adapted to a different language.",
                "There are only small drops in accuracy.",
                "Obviously, the linguistic features do not work for Chinese, but the effect of not using them is negligible.",
                "The results indicate that the patterns of title formats exist across different languages.",
                "From the domain adaptation and language adaptation results, we conclude that the use of formatting information is the key to a successful extraction from general documents. 6.8 Search with Extracted Titles We performed experiments on using title extraction for document retrieval.",
                "As a baseline, we employed BM25 without using extracted titles.",
                "The ranking mechanism was as described in Section 5.",
                "The weights were heuristically set.",
                "We did not conduct optimization on the weights.",
                "The evaluation was conducted on a corpus of 1.3 M documents crawled from the intranet of Microsoft using 100 evaluation queries obtained from this intranets search engine query logs. 50 queries were from the most popular set, while 50 queries other were chosen randomly.",
                "Users were asked to provide judgments of the degree of document relevance from a scale of 1to 5 (1 meaning detrimental, 2 - bad, 3 - fair, 4 - good and 5 - excellent). 152 Figure 10 shows the results.",
                "In the chart two sets of precision results were obtained by either considering good or excellent documents as relevant (left 3 bars with relevance threshold 0.5), or by considering only excellent documents as relevant (right 3 bars with relevance threshold 1.0) 0 0.05 0.1 0.15 0.2 0.25 0.3 0.35 0.4 0.45 P@10 P@5 Reciprocal P@10 P@5 Reciprocal 0.5 1 BM25 Anchor, Title, Body BM25 Anchor, Title, Body, ExtractedTitle Name All RelevanceThreshold Data Description Figure 10.",
                "Search ranking results.",
                "Figure 10 shows different document retrieval results with different ranking functions in terms of precision @10, precision @5 and reciprocal rank: • Blue bar - BM25 including the fields body, title (file property), and anchor text. • Purple bar - BM25 including the fields body, title (file property), anchor text, and extracted title.",
                "With the additional field of extracted title included in BM25 the precision @10 increased from 0.132 to 0.145, or by ~10%.",
                "Thus, it is safe to say that the use of extracted title can indeed improve the precision of document retrieval. 7.",
                "CONCLUSION In this paper, we have investigated the problem of automatically extracting titles from general documents.",
                "We have tried using a machine learning approach to address the problem.",
                "Previous work showed that the machine learning approach can work well for metadata extraction from research papers.",
                "In this paper, we showed that the approach can work for extraction from general documents as well.",
                "Our experimental results indicated that the machine learning approach can work significantly better than the baselines in title extraction from Office documents.",
                "Previous work on metadata extraction mainly used linguistic features in documents, while we mainly used formatting information.",
                "It appeared that using formatting information is a key for successfully conducting title extraction from general documents.",
                "We tried different machine learning models including Perceptron, Maximum Entropy, Maximum Entropy Markov Model, and Voted Perceptron.",
                "We found that the performance of the Perceptorn models was the best.",
                "We applied models constructed in one domain to another domain and applied models trained in one language to another language.",
                "We found that the accuracies did not drop substantially across different domains and across different languages, indicating that the models were generic.",
                "We also attempted to use the extracted titles in document retrieval.",
                "We observed a significant improvement in document ranking performance for search when using extracted title information.",
                "All the above investigations were not conducted in previous work, and through our investigations we verified the generality and the significance of the title extraction approach. 8.",
                "ACKNOWLEDGEMENTS We thank Chunyu Wei and Bojuan Zhao for their work on data annotation.",
                "We acknowledge Jinzhu Li for his assistance in conducting the experiments.",
                "We thank Ming Zhou, John Chen, Jun Xu, and the anonymous reviewers of JCDL05 for their valuable comments on this paper. 9.",
                "REFERENCES [1] Berger, A. L., Della Pietra, S. A., and Della Pietra, V. J.",
                "A maximum entropy approach to natural language processing.",
                "Computational Linguistics, 22:39-71, 1996. [2] Collins, M. Discriminative training methods for hidden markov models: theory and experiments with perceptron algorithms.",
                "In Proceedings of Conference on Empirical Methods in Natural Language Processing, 1-8, 2002. [3] Cortes, C. and Vapnik, V. Support-vector networks.",
                "Machine Learning, 20:273-297, 1995. [4] Chieu, H. L. and Ng, H. T. A maximum entropy approach to information extraction from semi-structured and free text.",
                "In Proceedings of the Eighteenth National Conference on Artificial Intelligence, 768-791, 2002. [5] Evans, D. K., Klavans, J. L., and McKeown, K. R. Columbia newsblaster: multilingual news summarization on the Web.",
                "In Proceedings of Human Language Technology conference / North American chapter of the Association for Computational Linguistics annual meeting, 1-4, 2004. [6] Ghahramani, Z. and Jordan, M. I. Factorial hidden markov models.",
                "Machine Learning, 29:245-273, 1997. [7] Gheel, J. and Anderson, T. Data and metadata for finding and reminding, In Proceedings of the 1999 International Conference on Information Visualization, 446-451,1999. [8] Giles, C. L., Petinot, Y., Teregowda P. B., Han, H., Lawrence, S., Rangaswamy, A., and Pal, N. eBizSearch: a niche search engine for e-Business.",
                "In Proceedings of the 26th Annual International ACM SIGIR Conference on Research and Development in Information Retrieval, 413414, 2003. [9] Giuffrida, G., Shek, E. C., and Yang, J. Knowledge-based metadata extraction from PostScript files.",
                "In Proceedings of the Fifth ACM Conference on Digital Libraries, 77-84, 2000. [10] Han, H., Giles, C. L., Manavoglu, E., Zha, H., Zhang, Z., and Fox, E. A.",
                "Automatic document metadata extraction using support vector machines.",
                "In Proceedings of the Third ACM/IEEE-CS Joint Conference on Digital Libraries, 37-48, 2003. [11] Kobayashi, M., and Takeda, K. Information retrieval on the Web.",
                "ACM Computing Surveys, 32:144-173, 2000. [12] Lafferty, J., McCallum, A., and Pereira, F. Conditional random fields: probabilistic models for segmenting and 153 labeling sequence data.",
                "In Proceedings of the Eighteenth International Conference on Machine Learning, 282-289, 2001. [13] Li, Y., Zaragoza, H., Herbrich, R., Shawe-Taylor J., and Kandola, J. S. The perceptron algorithm with uneven margins.",
                "In Proceedings of the Nineteenth International Conference on Machine Learning, 379-386, 2002. [14] Liddy, E. D., Sutton, S., Allen, E., Harwell, S., Corieri, S., Yilmazel, O., Ozgencil, N. E., Diekema, A., McCracken, N., and Silverstein, J.",
                "Automatic Metadata generation & evaluation.",
                "In Proceedings of the 25th Annual International ACM SIGIR Conference on Research and Development in Information Retrieval, 401-402, 2002. [15] Littlefield, A.",
                "Effective enterprise information retrieval across new content formats.",
                "In Proceedings of the Seventh Search Engine Conference, http://www.infonortics.com/searchengines/sh02/02prog.html, 2002. [16] Mao, S., Kim, J. W., and Thoma, G. R. A dynamic feature generation system for automated metadata extraction in preservation of digital materials.",
                "In Proceedings of the First International Workshop on Document Image Analysis for Libraries, 225-232, 2004. [17] McCallum, A., Freitag, D., and Pereira, F. Maximum entropy markov models for information extraction and segmentation.",
                "In Proceedings of the Seventeenth International Conference on Machine Learning, 591-598, 2000. [18] Murphy, L. D. Digital document metadata in organizations: roles, analytical approaches, and future research directions.",
                "In Proceedings of the Thirty-First Annual Hawaii International Conference on System Sciences, 267-276, 1998. [19] Pinto, D., McCallum, A., Wei, X., and Croft, W. B.",
                "Table extraction using conditional random fields.",
                "In Proceedings of the 26th Annual International ACM SIGIR Conference on Research and Development in Information Retrieval, 235242, 2003. [20] Ratnaparkhi, A. Unsupervised statistical models for prepositional phrase attachment.",
                "In Proceedings of the Seventeenth International Conference on Computational Linguistics. 1079-1085, 1998. [21] Robertson, S., Zaragoza, H., and Taylor, M. Simple BM25 extension to multiple weighted fields, In Proceedings of ACM Thirteenth Conference on Information and Knowledge Management, 42-49, 2004. [22] Yi, J. and Sundaresan, N. Metadata based Web mining for relevance, In Proceedings of the 2000 International Symposium on Database Engineering & Applications, 113121, 2000. [23] Yilmazel, O., Finneran, C. M., and Liddy, E. D. MetaExtract: An NLP system to automatically assign metadata.",
                "In Proceedings of the 2004 Joint ACM/IEEE Conference on Digital Libraries, 241-242, 2004. [24] Zhang, J. and Dimitroff, A. Internet search engines response to metadata Dublin Core implementation.",
                "Journal of Information Science, 30:310-320, 2004. [25] Zhang, L., Pan, Y., and Zhang, T. Recognising and using named entities: focused named entity recognition using machine learning.",
                "In Proceedings of the 27th Annual International ACM SIGIR Conference on Research and Development in Information Retrieval, 281-288, 2004. [26] http://dublincore.org/groups/corporate/Seattle/ 154"
            ],
            "original_annotated_samples": [],
            "translated_annotated_samples": [],
            "translated_text": "",
            "candidates": [],
            "error": []
        },
        "linguistic feature": {
            "translated_key": "rasgo lingüístico",
            "is_in_text": true,
            "original_annotated_sentences": [
                "Automatic Extraction of Titles from General Documents using Machine Learning Yunhua Hu1 Computer Science Department Xian Jiaotong University No 28, Xianning West Road Xian, China, 710049 yunhuahu@mail.xjtu.edu.cn Hang Li, Yunbo Cao Microsoft Research Asia 5F Sigma Center, No.",
                "49 Zhichun Road, Haidian, Beijing, China, 100080 {hangli,yucao}@microsoft.com Qinghua Zheng Computer Science Department Xian Jiaotong University No 28, Xianning West Road Xian, China, 710049 qhzheng@mail.xjtu.edu.cn Dmitriy Meyerzon Microsoft Corporation One Microsoft Way Redmond, WA, USA, 98052 dmitriym@microsoft.com ABSTRACT In this paper, we propose a machine learning approach to title extraction from general documents.",
                "By general documents, we mean documents that can belong to any one of a number of specific genres, including presentations, book chapters, technical papers, brochures, reports, and letters.",
                "Previously, methods have been proposed mainly for title extraction from research papers.",
                "It has not been clear whether it could be possible to conduct automatic title extraction from general documents.",
                "As a case study, we consider extraction from Office including Word and PowerPoint.",
                "In our approach, we annotate titles in sample documents (for Word and PowerPoint respectively) and take them as training data, train machine learning models, and perform title extraction using the trained models.",
                "Our method is unique in that we mainly utilize formatting information such as font size as features in the models.",
                "It turns out that the use of formatting information can lead to quite accurate extraction from general documents.",
                "Precision and recall for title extraction from Word is 0.810 and 0.837 respectively, and precision and recall for title extraction from PowerPoint is 0.875 and 0.895 respectively in an experiment on intranet data.",
                "Other important new findings in this work include that we can train models in one domain and apply them to another domain, and more surprisingly we can even train models in one language and apply them to another language.",
                "Moreover, we can significantly improve search ranking results in document retrieval by using the extracted titles.",
                "Categories and Subject Descriptors H.3.3 [Information Storage and Retrieval]: Information Search and Retrieval - Search Process; H.4.1 [Information Systems Applications]: Office Automation - Word processing; D.2.8 [Software Engineering]: Metrics - complexity measures, performance measures General Terms Algorithms, Experimentation, Performance. 1.",
                "INTRODUCTION Metadata of documents is useful for many kinds of document processing such as search, browsing, and filtering.",
                "Ideally, metadata is defined by the authors of documents and is then used by various systems.",
                "However, people seldom define document metadata by themselves, even when they have convenient metadata definition tools [26].",
                "Thus, how to automatically extract metadata from the bodies of documents turns out to be an important research issue.",
                "Methods for performing the task have been proposed.",
                "However, the focus was mainly on extraction from research papers.",
                "For instance, Han et al. [10] proposed a machine learning based method to conduct extraction from research papers.",
                "They formalized the problem as that of classification and employed Support Vector Machines as the classifier.",
                "They mainly used <br>linguistic feature</br>s in the model.1 In this paper, we consider metadata extraction from general documents.",
                "By general documents, we mean documents that may belong to any one of a number of specific genres.",
                "General documents are more widely available in digital libraries, intranets and the internet, and thus investigation on extraction from them is sorely needed.",
                "Research papers usually have well-formed styles and noticeable characteristics.",
                "In contrast, the styles of general documents can vary greatly.",
                "It has not been clarified whether a machine learning based approach can work well for this task.",
                "There are many types of metadata: title, author, date of creation, etc.",
                "As a case study, we consider title extraction in this paper.",
                "General documents can be in many different file formats: Microsoft Office, PDF (PS), etc.",
                "As a case study, we consider extraction from Office including Word and PowerPoint.",
                "We take a machine learning approach.",
                "We annotate titles in sample documents (for Word and PowerPoint respectively) and take them as training data to train several types of models, and perform title extraction using any one type of the trained models.",
                "In the models, we mainly utilize formatting information such as font size as features.",
                "We employ the following models: Maximum Entropy Model, Perceptron with Uneven Margins, Maximum Entropy Markov Model, and Voted Perceptron.",
                "In this paper, we also investigate the following three problems, which did not seem to have been examined previously. (1) Comparison between models: among the models above, which model performs best for title extraction; (2) Generality of model: whether it is possible to train a model on one domain and apply it to another domain, and whether it is possible to train a model in one language and apply it to another language; (3) Usefulness of extracted titles: whether extracted titles can improve document processing such as search.",
                "Experimental results indicate that our approach works well for title extraction from general documents.",
                "Our method can significantly outperform the baselines: one that always uses the first lines as titles and the other that always uses the lines in the largest font sizes as titles.",
                "Precision and recall for title extraction from Word are 0.810 and 0.837 respectively, and precision and recall for title extraction from PowerPoint are 0.875 and 0.895 respectively.",
                "It turns out that the use of format features is the key to successful title extraction. (1) We have observed that Perceptron based models perform better in terms of extraction accuracies. (2) We have empirically verified that the models trained with our approach are generic in the sense that they can be trained on one domain and applied to another, and they can be trained in one language and applied to another. (3) We have found that using the extracted titles we can significantly improve precision of document retrieval (by 10%).",
                "We conclude that we can indeed conduct reliable title extraction from general documents and use the extracted results to improve real applications.",
                "The rest of the paper is organized as follows.",
                "In section 2, we introduce related work, and in section 3, we explain the motivation and problem setting of our work.",
                "In section 4, we describe our method of title extraction, and in section 5, we describe our method of document retrieval using extracted titles.",
                "Section 6 gives our experimental results.",
                "We make concluding remarks in section 7. 2.",
                "RELATED WORK 2.1 Document Metadata Extraction Methods have been proposed for performing automatic metadata extraction from documents; however, the main focus was on extraction from research papers.",
                "The proposed methods fall into two categories: the rule based approach and the machine learning based approach.",
                "Giuffrida et al. [9], for instance, developed a rule-based system for automatically extracting metadata from research papers in Postscript.",
                "They used rules like titles are usually located on the upper portions of the first pages and they are usually in the largest font sizes.",
                "Liddy et al. [14] and Yilmazel el al. [23] performed metadata extraction from educational materials using rule-based natural language processing technologies.",
                "Mao et al. [16] also conducted automatic metadata extraction from research papers using rules on formatting information.",
                "The rule-based approach can achieve high performance.",
                "However, it also has disadvantages.",
                "It is less adaptive and robust when compared with the machine learning approach.",
                "Han et al. [10], for instance, conducted metadata extraction with the machine learning approach.",
                "They viewed the problem as that of classifying the lines in a document into the categories of metadata and proposed using Support Vector Machines as the classifier.",
                "They mainly used linguistic information as features.",
                "They reported high extraction accuracy from research papers in terms of precision and recall. 2.2 Information Extraction Metadata extraction can be viewed as an application of information extraction, in which given a sequence of instances, we identify a subsequence that represents information in which we are interested.",
                "Hidden Markov Model [6], Maximum Entropy Model [1, 4], Maximum Entropy Markov Model [17], Support Vector Machines [3], Conditional Random Field [12], and Voted Perceptron [2] are widely used information extraction models.",
                "Information extraction has been applied, for instance, to part-ofspeech tagging [20], named entity recognition [25] and table extraction [19]. 2.3 Search Using Title Information Title information is useful for document retrieval.",
                "In the system Citeseer, for instance, Giles et al. managed to extract titles from research papers and make use of the extracted titles in metadata search of papers [8].",
                "In web search, the title fields (i.e., file properties) and anchor texts of web pages (HTML documents) can be viewed as titles of the pages [5].",
                "Many search engines seem to utilize them for web page retrieval [7, 11, 18, 22].",
                "Zhang et al., found that web pages with well-defined metadata are more easily retrieved than those without well-defined metadata [24].",
                "To the best of our knowledge, no research has been conducted on using extracted titles from general documents (e.g., Office documents) for search of the documents. 146 3.",
                "MOTIVATION AND PROBLEM SETTING We consider the issue of automatically extracting titles from general documents.",
                "By general documents, we mean documents that belong to one of any number of specific genres.",
                "The documents can be presentations, books, book chapters, technical papers, brochures, reports, memos, specifications, letters, announcements, or resumes.",
                "General documents are more widely available in digital libraries, intranets, and internet, and thus investigation on title extraction from them is sorely needed.",
                "Figure 1 shows an estimate on distributions of file formats on intranet and internet [15].",
                "Office and PDF are the main file formats on the intranet.",
                "Even on the internet, the documents in the formats are still not negligible, given its extremely large size.",
                "In this paper, without loss of generality, we take Office documents as an example.",
                "Figure 1.",
                "Distributions of file formats in internet and intranet.",
                "For Office documents, users can define titles as file properties using a feature provided by Office.",
                "We found in an experiment, however, that users seldom use the feature and thus titles in file properties are usually very inaccurate.",
                "That is to say, titles in file properties are usually inconsistent with the true titles in the file bodies that are created by the authors and are visible to readers.",
                "We collected 6,000 Word and 6,000 PowerPoint documents from an intranet and the internet and examined how many titles in the file properties are correct.",
                "We found that surprisingly the accuracy was only 0.265 (cf., Section 6.3 for details).",
                "A number of reasons can be considered.",
                "For example, if one creates a new file by copying an old file, then the file property of the new file will also be copied from the old file.",
                "In another experiment, we found that Google uses the titles in file properties of Office documents in search and browsing, but the titles are not very accurate.",
                "We created 50 queries to search Word and PowerPoint documents and examined the top 15 results of each query returned by Google.",
                "We found that nearly all the titles presented in the search results were from the file properties of the documents.",
                "However, only 0.272 of them were correct.",
                "Actually, true titles usually exist at the beginnings of the bodies of documents.",
                "If we can accurately extract the titles from the bodies of documents, then we can exploit reliable title information in document processing.",
                "This is exactly the problem we address in this paper.",
                "More specifically, given a Word document, we are to extract the title from the top region of the first page.",
                "Given a PowerPoint document, we are to extract the title from the first slide.",
                "A title sometimes consists of a main title and one or two subtitles.",
                "We only consider extraction of the main title.",
                "As baselines for title extraction, we use that of always using the first lines as titles and that of always using the lines with largest font sizes as titles.",
                "Figure 2.",
                "Title extraction from Word document.",
                "Figure 3.",
                "Title extraction from PowerPoint document.",
                "Next, we define a specification for human judgments in title data annotation.",
                "The annotated data will be used in training and testing of the title extraction methods.",
                "Summary of the specification: The title of a document should be identified on the basis of common sense, if there is no difficulty in the identification.",
                "However, there are many cases in which the identification is not easy.",
                "There are some rules defined in the specification that guide identification for such cases.",
                "The rules include a title is usually in consecutive lines in the same format, a document can have no title, titles in images are not considered, a title should not contain words like draft, 147 whitepaper, etc, if it is difficult to determine which is the title, select the one in the largest font size, and if it is still difficult to determine which is the title, select the first candidate. (The specification covers all the cases we have encountered in data annotation.)",
                "Figures 2 and 3 show examples of Office documents from which we conduct title extraction.",
                "In Figure 2, Differences in Win32 API Implementations among Windows Operating Systems is the title of the Word document.",
                "Microsoft Windows on the top of this page is a picture and thus is ignored.",
                "In Figure 3, Building Competitive Advantages through an Agile Infrastructure is the title of the PowerPoint document.",
                "We have developed a tool for annotation of titles by human annotators.",
                "Figure 4 shows a snapshot of the tool.",
                "Figure 4.",
                "Title annotation tool. 4.",
                "TITLE EXTRACTION METHOD 4.1 Outline Title extraction based on machine learning consists of training and extraction.",
                "The same pre-processing step occurs before training and extraction.",
                "During pre-processing, from the top region of the first page of a Word document or the first slide of a PowerPoint document a number of units for processing are extracted.",
                "If a line (lines are separated by return symbols) only has a single format, then the line will become a unit.",
                "If a line has several parts and each of them has its own format, then each part will become a unit.",
                "Each unit will be treated as an instance in learning.",
                "A unit contains not only content information (linguistic information) but also formatting information.",
                "The input to pre-processing is a document and the output of pre-processing is a sequence of units (instances).",
                "Figure 5 shows the units obtained from the document in Figure 2.",
                "Figure 5.",
                "Example of units.",
                "In learning, the input is sequences of units where each sequence corresponds to a document.",
                "We take labeled units (labeled as title_begin, title_end, or other) in the sequences as training data and construct models for identifying whether a unit is title_begin title_end, or other.",
                "We employ four types of models: Perceptron, Maximum Entropy (ME), Perceptron Markov Model (PMM), and Maximum Entropy Markov Model (MEMM).",
                "In extraction, the input is a sequence of units from one document.",
                "We employ one type of model to identify whether a unit is title_begin, title_end, or other.",
                "We then extract units from the unit labeled with title_begin to the unit labeled with title_end.",
                "The result is the extracted title of the document.",
                "The unique characteristic of our approach is that we mainly utilize formatting information for title extraction.",
                "Our assumption is that although general documents vary in styles, their formats have certain patterns and we can learn and utilize the patterns for title extraction.",
                "This is in contrast to the work by Han et al., in which only <br>linguistic feature</br>s are used for extraction from research papers. 4.2 Models The four models actually can be considered in the same metadata extraction framework.",
                "That is why we apply them together to our current problem.",
                "Each input is a sequence of instances kxxx L21 together with a sequence of labels kyyy L21 . ix and iy represents an instance and its label, respectively ( ki ,,2,1 L= ).",
                "Recall that an instance here represents a unit.",
                "A label represents title_begin, title_end, or other.",
                "Here, k is the number of units in a document.",
                "In learning, we train a model which can be generally denoted as a conditional probability distribution )|( 11 kk XXYYP LL where iX and iY denote random variables taking instance ix and label iy as values, respectively ( ki ,,2,1 L= ).",
                "Learning Tool Extraction Tool 21121 2222122221 1121111211 nknnknn kk kk yyyxxx yyyxxx yyyxxx LL LL LL LL → → → )|(maxarg 11 mkmmkm xxyyP LL )|( 11 kk XXYYP LL Conditional Distribution mkmm xxx L21 Figure 6.",
                "Metadata extraction model.",
                "We can make assumptions about the general model in order to make it simple enough for training. 148 For example, we can assume that kYY ,,1 L are independent of each other given kXX ,,1 L .",
                "Thus, we have )|()|( )|( 11 11 kk kk XYPXYP XXYYP L LL = In this way, we decompose the model into a number of classifiers.",
                "We train the classifiers locally using the labeled data.",
                "As the classifier, we employ the Perceptron or Maximum Entropy model.",
                "We can also assume that the first order Markov property holds for kYY ,,1 L given kXX ,,1 L .",
                "Thus, we have )|()|( )|( 111 11 kkk kk XYYPXYP XXYYP −= L LL Again, we obtain a number of classifiers.",
                "However, the classifiers are conditioned on the previous label.",
                "When we employ the Percepton or Maximum Entropy model as a classifier, the models become a Percepton Markov Model or Maximum Entropy Markov Model, respectively.",
                "That is to say, the two models are more precise.",
                "In extraction, given a new sequence of instances, we resort to one of the constructed models to assign a sequence of labels to the sequence of instances, i.e., perform extraction.",
                "For Perceptron and ME, we assign labels locally and combine the results globally later using heuristics.",
                "Specifically, we first identify the most likely title_begin.",
                "Then we find the most likely title_end within three units after the title_begin.",
                "Finally, we extract as a title the units between the title_begin and the title_end.",
                "For PMM and MEMM, we employ the Viterbi algorithm to find the globally optimal label sequence.",
                "In this paper, for Perceptron, we actually employ an improved variant of it, called Perceptron with Uneven Margin [13].",
                "This version of Perceptron can work well especially when the number of positive instances and the number of negative instances differ greatly, which is exactly the case in our problem.",
                "We also employ an improved version of Perceptron Markov Model in which the Perceptron model is the so-called Voted Perceptron [2].",
                "In addition, in training, the parameters of the model are updated globally rather than locally. 4.3 Features There are two types of features: format features and <br>linguistic feature</br>s.",
                "We mainly use the former.",
                "The features are used for both the title-begin and the title-end classifiers. 4.3.1 Format Features Font Size: There are four binary features that represent the normalized font size of the unit (recall that a unit has only one type of font).",
                "If the font size of the unit is the largest in the document, then the first feature will be 1, otherwise 0.",
                "If the font size is the smallest in the document, then the fourth feature will be 1, otherwise 0.",
                "If the font size is above the average font size and not the largest in the document, then the second feature will be 1, otherwise 0.",
                "If the font size is below the average font size and not the smallest, the third feature will be 1, otherwise 0.",
                "It is necessary to conduct normalization on font sizes.",
                "For example, in one document the largest font size might be 12pt, while in another the smallest one might be 18pt.",
                "Boldface: This binary feature represents whether or not the current unit is in boldface.",
                "Alignment: There are four binary features that respectively represent the location of the current unit: left, center, right, and unknown alignment.",
                "The following format features with respect to context play an important role in title extraction.",
                "Empty Neighboring Unit: There are two binary features that represent, respectively, whether or not the previous unit and the current unit are blank lines.",
                "Font Size Change: There are two binary features that represent, respectively, whether or not the font size of the previous unit and the font size of the next unit differ from that of the current unit.",
                "Alignment Change: There are two binary features that represent, respectively, whether or not the alignment of the previous unit and the alignment of the next unit differ from that of the current one.",
                "Same Paragraph: There are two binary features that represent, respectively, whether or not the previous unit and the next unit are in the same paragraph as the current unit. 4.3.2 Linguistic Features The <br>linguistic feature</br>s are based on key words.",
                "Positive Word: This binary feature represents whether or not the current unit begins with one of the positive words.",
                "The positive words include title:, subject:, subject line: For example, in some documents the lines of titles and authors have the same formats.",
                "However, if lines begin with one of the positive words, then it is likely that they are title lines.",
                "Negative Word: This binary feature represents whether or not the current unit begins with one of the negative words.",
                "The negative words include To, By, created by, updated by, etc.",
                "There are more negative words than positive words.",
                "The above <br>linguistic feature</br>s are language dependent.",
                "Word Count: A title should not be too long.",
                "We heuristically create four intervals: [1, 2], [3, 6], [7, 9] and [9, ∞) and define one feature for each interval.",
                "If the number of words in a title falls into an interval, then the corresponding feature will be 1; otherwise 0.",
                "Ending Character: This feature represents whether the unit ends with :, -, or other special characters.",
                "A title usually does not end with such a character. 5.",
                "DOCUMENT RETRIEVAL METHOD We describe our method of document retrieval using extracted titles.",
                "Typically, in information retrieval a document is split into a number of fields including body, title, and anchor text.",
                "A ranking function in search can use different weights for different fields of 149 the document.",
                "Also, titles are typically assigned high weights, indicating that they are important for document retrieval.",
                "As explained previously, our experiment has shown that a significant number of documents actually have incorrect titles in the file properties, and thus in addition of using them we use the extracted titles as one more field of the document.",
                "By doing this, we attempt to improve the overall precision.",
                "In this paper, we employ a modification of BM25 that allows field weighting [21].",
                "As fields, we make use of body, title, extracted title and anchor.",
                "First, for each term in the query we count the term frequency in each field of the document; each field frequency is then weighted according to the corresponding weight parameter: ∑= f tfft tfwwtf Similarly, we compute the document length as a weighted sum of lengths of each field.",
                "Average document length in the corpus becomes the average of all weighted document lengths. ∑= f ff dlwwdl In our experiments we used 75.0,8.11 == bk .",
                "Weight for content was 1.0, title was 10.0, anchor was 10.0, and extracted title was 5.0. 6.",
                "EXPERIMENTAL RESULTS 6.1 Data Sets and Evaluation Measures We used two data sets in our experiments.",
                "First, we downloaded and randomly selected 5,000 Word documents and 5,000 PowerPoint documents from an intranet of Microsoft.",
                "We call it MS hereafter.",
                "Second, we downloaded and randomly selected 500 Word and 500 PowerPoint documents from the DotGov and DotCom domains on the internet, respectively.",
                "Figure 7 shows the distributions of the genres of the documents.",
                "We see that the documents are indeed general documents as we define them.",
                "Figure 7.",
                "Distributions of document genres.",
                "Third, a data set in Chinese was also downloaded from the internet.",
                "It includes 500 Word documents and 500 PowerPoint documents in Chinese.",
                "We manually labeled the titles of all the documents, on the basis of our specification.",
                "Not all the documents in the two data sets have titles.",
                "Table 1 shows the percentages of the documents having titles.",
                "We see that DotCom and DotGov have more PowerPoint documents with titles than MS.",
                "This might be because PowerPoint documents published on the internet are more formal than those on the intranet.",
                "Table 1.",
                "The portion of documents with titles Domain Type MS DotCom DotGov Word 75.7% 77.8% 75.6% PowerPoint 82.1% 93.4% 96.4% In our experiments, we conducted evaluations on title extraction in terms of precision, recall, and F-measure.",
                "The evaluation measures are defined as follows: Precision: P = A / ( A + B ) Recall: R = A / ( A + C ) F-measure: F1 = 2PR / ( P + R ) Here, A, B, C, and D are numbers of documents as those defined in Table 2.",
                "Table 2.",
                "Contingence table with regard to title extraction Is title Is not title Extracted A B Not extracted C D 6.2 Baselines We test the accuracies of the two baselines described in section 4.2.",
                "They are denoted as largest font size and first line respectively. 6.3 Accuracy of Titles in File Properties We investigate how many titles in the file properties of the documents are reliable.",
                "We view the titles annotated by humans as true titles and test how many titles in the file properties can approximately match with the true titles.",
                "We use Edit Distance to conduct the approximate match. (Approximate match is only used in this evaluation).",
                "This is because sometimes human annotated titles can be slightly different from the titles in file properties on the surface, e.g., contain extra spaces).",
                "Given string A and string B: if ( (D == 0) or ( D / ( La + Lb ) < θ ) ) then string A = string B D: Edit Distance between string A and string B La: length of string A Lb: length of string B θ: 0.1 ∑ × ++− + = t t n N wtf avwdl wdl bbk kwtf FBM )log( ))1(( )1( 25 1 1 150 Table 3.",
                "Accuracies of titles in file properties File Type Domain Precision Recall F1 MS 0.299 0.311 0.305 DotCom 0.210 0.214 0.212Word DotGov 0.182 0.177 0.180 MS 0.229 0.245 0.237 DotCom 0.185 0.186 0.186PowerPoint DotGov 0.180 0.182 0.181 6.4 Comparison with Baselines We conducted title extraction from the first data set (Word and PowerPoint in MS).",
                "As the model, we used Perceptron.",
                "We conduct 4-fold cross validation.",
                "Thus, all the results reported here are those averaged over 4 trials.",
                "Tables 4 and 5 show the results.",
                "We see that Perceptron significantly outperforms the baselines.",
                "In the evaluation, we use exact matching between the true titles annotated by humans and the extracted titles.",
                "Table 4.",
                "Accuracies of title extraction with Word Precision Recall F1 Model Perceptron 0.810 0.837 0.823 Largest font size 0.700 0.758 0.727 Baselines First line 0.707 0.767 0.736 Table 5.",
                "Accuracies of title extraction with PowerPoint Precision Recall F1 Model Perceptron 0.875 0. 895 0.885 Largest font size 0.844 0.887 0.865 Baselines First line 0.639 0.671 0.655 We see that the machine learning approach can achieve good performance in title extraction.",
                "For Word documents both precision and recall of the approach are 8 percent higher than those of the baselines.",
                "For PowerPoint both precision and recall of the approach are 2 percent higher than those of the baselines.",
                "We conduct significance tests.",
                "The results are shown in Table 6.",
                "Here, Largest denotes the baseline of using the largest font size, First denotes the baseline of using the first line.",
                "The results indicate that the improvements of machine learning over baselines are statistically significant (in the sense p-value < 0.05) Table 6.",
                "Sign test results Documents Type Sign test between p-value Perceptron vs. Largest 3.59e-26 Word Perceptron vs. First 7.12e-10 Perceptron vs. Largest 0.010 PowerPoint Perceptron vs. First 5.13e-40 We see, from the results, that the two baselines can work well for title extraction, suggesting that font size and position information are most useful features for title extraction.",
                "However, it is also obvious that using only these two features is not enough.",
                "There are cases in which all the lines have the same font size (i.e., the largest font size), or cases in which the lines with the largest font size only contain general descriptions like Confidential, White paper, etc.",
                "For those cases, the largest font size method cannot work well.",
                "For similar reasons, the first line method alone cannot work well, either.",
                "With the combination of different features (evidence in title judgment), Perceptron can outperform Largest and First.",
                "We investigate the performance of solely using <br>linguistic feature</br>s.",
                "We found that it does not work well.",
                "It seems that the format features play important roles and the <br>linguistic feature</br>s are supplements..",
                "Figure 8.",
                "An example Word document.",
                "Figure 9.",
                "An example PowerPoint document.",
                "We conducted an error analysis on the results of Perceptron.",
                "We found that the errors fell into three categories. (1) About one third of the errors were related to hard cases.",
                "In these documents, the layouts of the first pages were difficult to understand, even for humans.",
                "Figure 8 and 9 shows examples. (2) Nearly one fourth of the errors were from the documents which do not have true titles but only contain bullets.",
                "Since we conduct extraction from the top regions, it is difficult to get rid of these errors with the current approach. (3).",
                "Confusions between main titles and subtitles were another type of error.",
                "Since we only labeled the main titles as titles, the extractions of both titles were considered incorrect.",
                "This type of error does little harm to document processing like search, however. 6.5 Comparison between Models To compare the performance of different machine learning models, we conducted another experiment.",
                "Again, we perform 4-fold cross 151 validation on the first data set (MS).",
                "Table 7, 8 shows the results of all the four models.",
                "It turns out that Perceptron and PMM perform the best, followed by MEMM, and ME performs the worst.",
                "In general, the Markovian models perform better than or as well as their classifier counterparts.",
                "This seems to be because the Markovian models are trained globally, while the classifiers are trained locally.",
                "The Perceptron based models perform better than the ME based counterparts.",
                "This seems to be because the Perceptron based models are created to make better classifications, while ME models are constructed for better prediction.",
                "Table 7.",
                "Comparison between different learning models for title extraction with Word Model Precision Recall F1 Perceptron 0.810 0.837 0.823 MEMM 0.797 0.824 0.810 PMM 0.827 0.823 0.825 ME 0.801 0.621 0.699 Table 8.",
                "Comparison between different learning models for title extraction with PowerPoint Model Precision Recall F1 Perceptron 0.875 0. 895 0. 885 MEMM 0.841 0.861 0.851 PMM 0.873 0.896 0.885 ME 0.753 0.766 0.759 6.6 Domain Adaptation We apply the model trained with the first data set (MS) to the second data set (DotCom and DotGov).",
                "Tables 9-12 show the results.",
                "Table 9.",
                "Accuracies of title extraction with Word in DotGov Precision Recall F1 Model Perceptron 0.716 0.759 0.737 Largest font size 0.549 0.619 0.582Baselines First line 0.462 0.521 0.490 Table 10.",
                "Accuracies of title extraction with PowerPoint in DotGov Precision Recall F1 Model Perceptron 0.900 0.906 0.903 Largest font size 0.871 0.888 0.879Baselines First line 0.554 0.564 0.559 Table 11.",
                "Accuracies of title extraction with Word in DotCom Precisio n Recall F1 Model Perceptron 0.832 0.880 0.855 Largest font size 0.676 0.753 0.712Baselines First line 0.577 0.643 0.608 Table 12.",
                "Performance of PowerPoint document title extraction in DotCom Precisio n Recall F1 Model Perceptron 0.910 0.903 0.907 Largest font size 0.864 0.886 0.875Baselines First line 0.570 0.585 0.577 From the results, we see that the models can be adapted to different domains well.",
                "There is almost no drop in accuracy.",
                "The results indicate that the patterns of title formats exist across different domains, and it is possible to construct a domain independent model by mainly using formatting information. 6.7 Language Adaptation We apply the model trained with the data in English (MS) to the data set in Chinese.",
                "Tables 13-14 show the results.",
                "Table 13.",
                "Accuracies of title extraction with Word in Chinese Precision Recall F1 Model Perceptron 0.817 0.805 0.811 Largest font size 0.722 0.755 0.738Baselines First line 0.743 0.777 0.760 Table 14.",
                "Accuracies of title extraction with PowerPoint in Chinese Precision Recall F1 Model Perceptron 0.766 0.812 0.789 Largest font size 0.753 0.813 0.782Baselines First line 0.627 0.676 0.650 We see that the models can be adapted to a different language.",
                "There are only small drops in accuracy.",
                "Obviously, the <br>linguistic feature</br>s do not work for Chinese, but the effect of not using them is negligible.",
                "The results indicate that the patterns of title formats exist across different languages.",
                "From the domain adaptation and language adaptation results, we conclude that the use of formatting information is the key to a successful extraction from general documents. 6.8 Search with Extracted Titles We performed experiments on using title extraction for document retrieval.",
                "As a baseline, we employed BM25 without using extracted titles.",
                "The ranking mechanism was as described in Section 5.",
                "The weights were heuristically set.",
                "We did not conduct optimization on the weights.",
                "The evaluation was conducted on a corpus of 1.3 M documents crawled from the intranet of Microsoft using 100 evaluation queries obtained from this intranets search engine query logs. 50 queries were from the most popular set, while 50 queries other were chosen randomly.",
                "Users were asked to provide judgments of the degree of document relevance from a scale of 1to 5 (1 meaning detrimental, 2 - bad, 3 - fair, 4 - good and 5 - excellent). 152 Figure 10 shows the results.",
                "In the chart two sets of precision results were obtained by either considering good or excellent documents as relevant (left 3 bars with relevance threshold 0.5), or by considering only excellent documents as relevant (right 3 bars with relevance threshold 1.0) 0 0.05 0.1 0.15 0.2 0.25 0.3 0.35 0.4 0.45 P@10 P@5 Reciprocal P@10 P@5 Reciprocal 0.5 1 BM25 Anchor, Title, Body BM25 Anchor, Title, Body, ExtractedTitle Name All RelevanceThreshold Data Description Figure 10.",
                "Search ranking results.",
                "Figure 10 shows different document retrieval results with different ranking functions in terms of precision @10, precision @5 and reciprocal rank: • Blue bar - BM25 including the fields body, title (file property), and anchor text. • Purple bar - BM25 including the fields body, title (file property), anchor text, and extracted title.",
                "With the additional field of extracted title included in BM25 the precision @10 increased from 0.132 to 0.145, or by ~10%.",
                "Thus, it is safe to say that the use of extracted title can indeed improve the precision of document retrieval. 7.",
                "CONCLUSION In this paper, we have investigated the problem of automatically extracting titles from general documents.",
                "We have tried using a machine learning approach to address the problem.",
                "Previous work showed that the machine learning approach can work well for metadata extraction from research papers.",
                "In this paper, we showed that the approach can work for extraction from general documents as well.",
                "Our experimental results indicated that the machine learning approach can work significantly better than the baselines in title extraction from Office documents.",
                "Previous work on metadata extraction mainly used <br>linguistic feature</br>s in documents, while we mainly used formatting information.",
                "It appeared that using formatting information is a key for successfully conducting title extraction from general documents.",
                "We tried different machine learning models including Perceptron, Maximum Entropy, Maximum Entropy Markov Model, and Voted Perceptron.",
                "We found that the performance of the Perceptorn models was the best.",
                "We applied models constructed in one domain to another domain and applied models trained in one language to another language.",
                "We found that the accuracies did not drop substantially across different domains and across different languages, indicating that the models were generic.",
                "We also attempted to use the extracted titles in document retrieval.",
                "We observed a significant improvement in document ranking performance for search when using extracted title information.",
                "All the above investigations were not conducted in previous work, and through our investigations we verified the generality and the significance of the title extraction approach. 8.",
                "ACKNOWLEDGEMENTS We thank Chunyu Wei and Bojuan Zhao for their work on data annotation.",
                "We acknowledge Jinzhu Li for his assistance in conducting the experiments.",
                "We thank Ming Zhou, John Chen, Jun Xu, and the anonymous reviewers of JCDL05 for their valuable comments on this paper. 9.",
                "REFERENCES [1] Berger, A. L., Della Pietra, S. A., and Della Pietra, V. J.",
                "A maximum entropy approach to natural language processing.",
                "Computational Linguistics, 22:39-71, 1996. [2] Collins, M. Discriminative training methods for hidden markov models: theory and experiments with perceptron algorithms.",
                "In Proceedings of Conference on Empirical Methods in Natural Language Processing, 1-8, 2002. [3] Cortes, C. and Vapnik, V. Support-vector networks.",
                "Machine Learning, 20:273-297, 1995. [4] Chieu, H. L. and Ng, H. T. A maximum entropy approach to information extraction from semi-structured and free text.",
                "In Proceedings of the Eighteenth National Conference on Artificial Intelligence, 768-791, 2002. [5] Evans, D. K., Klavans, J. L., and McKeown, K. R. Columbia newsblaster: multilingual news summarization on the Web.",
                "In Proceedings of Human Language Technology conference / North American chapter of the Association for Computational Linguistics annual meeting, 1-4, 2004. [6] Ghahramani, Z. and Jordan, M. I. Factorial hidden markov models.",
                "Machine Learning, 29:245-273, 1997. [7] Gheel, J. and Anderson, T. Data and metadata for finding and reminding, In Proceedings of the 1999 International Conference on Information Visualization, 446-451,1999. [8] Giles, C. L., Petinot, Y., Teregowda P. B., Han, H., Lawrence, S., Rangaswamy, A., and Pal, N. eBizSearch: a niche search engine for e-Business.",
                "In Proceedings of the 26th Annual International ACM SIGIR Conference on Research and Development in Information Retrieval, 413414, 2003. [9] Giuffrida, G., Shek, E. C., and Yang, J. Knowledge-based metadata extraction from PostScript files.",
                "In Proceedings of the Fifth ACM Conference on Digital Libraries, 77-84, 2000. [10] Han, H., Giles, C. L., Manavoglu, E., Zha, H., Zhang, Z., and Fox, E. A.",
                "Automatic document metadata extraction using support vector machines.",
                "In Proceedings of the Third ACM/IEEE-CS Joint Conference on Digital Libraries, 37-48, 2003. [11] Kobayashi, M., and Takeda, K. Information retrieval on the Web.",
                "ACM Computing Surveys, 32:144-173, 2000. [12] Lafferty, J., McCallum, A., and Pereira, F. Conditional random fields: probabilistic models for segmenting and 153 labeling sequence data.",
                "In Proceedings of the Eighteenth International Conference on Machine Learning, 282-289, 2001. [13] Li, Y., Zaragoza, H., Herbrich, R., Shawe-Taylor J., and Kandola, J. S. The perceptron algorithm with uneven margins.",
                "In Proceedings of the Nineteenth International Conference on Machine Learning, 379-386, 2002. [14] Liddy, E. D., Sutton, S., Allen, E., Harwell, S., Corieri, S., Yilmazel, O., Ozgencil, N. E., Diekema, A., McCracken, N., and Silverstein, J.",
                "Automatic Metadata generation & evaluation.",
                "In Proceedings of the 25th Annual International ACM SIGIR Conference on Research and Development in Information Retrieval, 401-402, 2002. [15] Littlefield, A.",
                "Effective enterprise information retrieval across new content formats.",
                "In Proceedings of the Seventh Search Engine Conference, http://www.infonortics.com/searchengines/sh02/02prog.html, 2002. [16] Mao, S., Kim, J. W., and Thoma, G. R. A dynamic feature generation system for automated metadata extraction in preservation of digital materials.",
                "In Proceedings of the First International Workshop on Document Image Analysis for Libraries, 225-232, 2004. [17] McCallum, A., Freitag, D., and Pereira, F. Maximum entropy markov models for information extraction and segmentation.",
                "In Proceedings of the Seventeenth International Conference on Machine Learning, 591-598, 2000. [18] Murphy, L. D. Digital document metadata in organizations: roles, analytical approaches, and future research directions.",
                "In Proceedings of the Thirty-First Annual Hawaii International Conference on System Sciences, 267-276, 1998. [19] Pinto, D., McCallum, A., Wei, X., and Croft, W. B.",
                "Table extraction using conditional random fields.",
                "In Proceedings of the 26th Annual International ACM SIGIR Conference on Research and Development in Information Retrieval, 235242, 2003. [20] Ratnaparkhi, A. Unsupervised statistical models for prepositional phrase attachment.",
                "In Proceedings of the Seventeenth International Conference on Computational Linguistics. 1079-1085, 1998. [21] Robertson, S., Zaragoza, H., and Taylor, M. Simple BM25 extension to multiple weighted fields, In Proceedings of ACM Thirteenth Conference on Information and Knowledge Management, 42-49, 2004. [22] Yi, J. and Sundaresan, N. Metadata based Web mining for relevance, In Proceedings of the 2000 International Symposium on Database Engineering & Applications, 113121, 2000. [23] Yilmazel, O., Finneran, C. M., and Liddy, E. D. MetaExtract: An NLP system to automatically assign metadata.",
                "In Proceedings of the 2004 Joint ACM/IEEE Conference on Digital Libraries, 241-242, 2004. [24] Zhang, J. and Dimitroff, A. Internet search engines response to metadata Dublin Core implementation.",
                "Journal of Information Science, 30:310-320, 2004. [25] Zhang, L., Pan, Y., and Zhang, T. Recognising and using named entities: focused named entity recognition using machine learning.",
                "In Proceedings of the 27th Annual International ACM SIGIR Conference on Research and Development in Information Retrieval, 281-288, 2004. [26] http://dublincore.org/groups/corporate/Seattle/ 154"
            ],
            "original_annotated_samples": [],
            "translated_annotated_samples": [
                "Utilizaron principalmente \"características lingüísticas\" en el modelo.1 En este documento, consideramos la extracción de metadatos de los documentos generales.",
                "Esto contrasta con el trabajo de Han et al., En el que solo se utilizan \"características lingüísticas\" para la extracción de los trabajos de investigación.4.2 Modelos Los cuatro modelos en realidad pueden considerarse en el mismo marco de extracción de metadatos.",
                "Además, en el entrenamiento, los parámetros del modelo se actualizan a nivel mundial en lugar de localmente.4.3 Características Hay dos tipos de características: características de formato y \"características lingüísticas\" s.",
                "El mismo párrafo: hay dos características binarias que representan, respectivamente, ya sea que la unidad anterior y la siguiente unidad estén en el mismo párrafo que la unidad actual.4.3.2 Características lingüísticas Las \"características lingüísticas\" se basan en palabras clave.",
                "Los \"característicos lingüísticos\" anteriores son dependientes del lenguaje.",
                "Investigamos el rendimiento de utilizar únicamente la \"característica lingüística\" s.",
                "",
                "Obviamente, la \"característica lingüística\" no funcionan para los chinos, pero el efecto de no usarlos es insignificante.",
                "Trabajo previo sobre la extracción de metadatos utilizó principalmente \"características lingüísticas\" en documentos, mientras que utilizamos principalmente información de formato."
            ],
            "translated_text": "",
            "candidates": [
                "rasgo lingüístico",
                "características lingüísticas",
                "rasgo lingüístico",
                "características lingüísticas",
                "rasgo lingüístico",
                "características lingüísticas",
                "rasgo lingüístico",
                "características lingüísticas",
                "rasgo lingüístico",
                "característicos lingüísticos",
                "rasgo lingüístico",
                "característica lingüística",
                "",
                "características lingüísticas",
                "rasgo lingüístico",
                "característica lingüística",
                "rasgo lingüístico",
                "características lingüísticas"
            ],
            "error": []
        },
        "comparison between model": {
            "translated_key": "Comparación entre modelo",
            "is_in_text": false,
            "original_annotated_sentences": [
                "Automatic Extraction of Titles from General Documents using Machine Learning Yunhua Hu1 Computer Science Department Xian Jiaotong University No 28, Xianning West Road Xian, China, 710049 yunhuahu@mail.xjtu.edu.cn Hang Li, Yunbo Cao Microsoft Research Asia 5F Sigma Center, No.",
                "49 Zhichun Road, Haidian, Beijing, China, 100080 {hangli,yucao}@microsoft.com Qinghua Zheng Computer Science Department Xian Jiaotong University No 28, Xianning West Road Xian, China, 710049 qhzheng@mail.xjtu.edu.cn Dmitriy Meyerzon Microsoft Corporation One Microsoft Way Redmond, WA, USA, 98052 dmitriym@microsoft.com ABSTRACT In this paper, we propose a machine learning approach to title extraction from general documents.",
                "By general documents, we mean documents that can belong to any one of a number of specific genres, including presentations, book chapters, technical papers, brochures, reports, and letters.",
                "Previously, methods have been proposed mainly for title extraction from research papers.",
                "It has not been clear whether it could be possible to conduct automatic title extraction from general documents.",
                "As a case study, we consider extraction from Office including Word and PowerPoint.",
                "In our approach, we annotate titles in sample documents (for Word and PowerPoint respectively) and take them as training data, train machine learning models, and perform title extraction using the trained models.",
                "Our method is unique in that we mainly utilize formatting information such as font size as features in the models.",
                "It turns out that the use of formatting information can lead to quite accurate extraction from general documents.",
                "Precision and recall for title extraction from Word is 0.810 and 0.837 respectively, and precision and recall for title extraction from PowerPoint is 0.875 and 0.895 respectively in an experiment on intranet data.",
                "Other important new findings in this work include that we can train models in one domain and apply them to another domain, and more surprisingly we can even train models in one language and apply them to another language.",
                "Moreover, we can significantly improve search ranking results in document retrieval by using the extracted titles.",
                "Categories and Subject Descriptors H.3.3 [Information Storage and Retrieval]: Information Search and Retrieval - Search Process; H.4.1 [Information Systems Applications]: Office Automation - Word processing; D.2.8 [Software Engineering]: Metrics - complexity measures, performance measures General Terms Algorithms, Experimentation, Performance. 1.",
                "INTRODUCTION Metadata of documents is useful for many kinds of document processing such as search, browsing, and filtering.",
                "Ideally, metadata is defined by the authors of documents and is then used by various systems.",
                "However, people seldom define document metadata by themselves, even when they have convenient metadata definition tools [26].",
                "Thus, how to automatically extract metadata from the bodies of documents turns out to be an important research issue.",
                "Methods for performing the task have been proposed.",
                "However, the focus was mainly on extraction from research papers.",
                "For instance, Han et al. [10] proposed a machine learning based method to conduct extraction from research papers.",
                "They formalized the problem as that of classification and employed Support Vector Machines as the classifier.",
                "They mainly used linguistic features in the model.1 In this paper, we consider metadata extraction from general documents.",
                "By general documents, we mean documents that may belong to any one of a number of specific genres.",
                "General documents are more widely available in digital libraries, intranets and the internet, and thus investigation on extraction from them is sorely needed.",
                "Research papers usually have well-formed styles and noticeable characteristics.",
                "In contrast, the styles of general documents can vary greatly.",
                "It has not been clarified whether a machine learning based approach can work well for this task.",
                "There are many types of metadata: title, author, date of creation, etc.",
                "As a case study, we consider title extraction in this paper.",
                "General documents can be in many different file formats: Microsoft Office, PDF (PS), etc.",
                "As a case study, we consider extraction from Office including Word and PowerPoint.",
                "We take a machine learning approach.",
                "We annotate titles in sample documents (for Word and PowerPoint respectively) and take them as training data to train several types of models, and perform title extraction using any one type of the trained models.",
                "In the models, we mainly utilize formatting information such as font size as features.",
                "We employ the following models: Maximum Entropy Model, Perceptron with Uneven Margins, Maximum Entropy Markov Model, and Voted Perceptron.",
                "In this paper, we also investigate the following three problems, which did not seem to have been examined previously. (1) Comparison between models: among the models above, which model performs best for title extraction; (2) Generality of model: whether it is possible to train a model on one domain and apply it to another domain, and whether it is possible to train a model in one language and apply it to another language; (3) Usefulness of extracted titles: whether extracted titles can improve document processing such as search.",
                "Experimental results indicate that our approach works well for title extraction from general documents.",
                "Our method can significantly outperform the baselines: one that always uses the first lines as titles and the other that always uses the lines in the largest font sizes as titles.",
                "Precision and recall for title extraction from Word are 0.810 and 0.837 respectively, and precision and recall for title extraction from PowerPoint are 0.875 and 0.895 respectively.",
                "It turns out that the use of format features is the key to successful title extraction. (1) We have observed that Perceptron based models perform better in terms of extraction accuracies. (2) We have empirically verified that the models trained with our approach are generic in the sense that they can be trained on one domain and applied to another, and they can be trained in one language and applied to another. (3) We have found that using the extracted titles we can significantly improve precision of document retrieval (by 10%).",
                "We conclude that we can indeed conduct reliable title extraction from general documents and use the extracted results to improve real applications.",
                "The rest of the paper is organized as follows.",
                "In section 2, we introduce related work, and in section 3, we explain the motivation and problem setting of our work.",
                "In section 4, we describe our method of title extraction, and in section 5, we describe our method of document retrieval using extracted titles.",
                "Section 6 gives our experimental results.",
                "We make concluding remarks in section 7. 2.",
                "RELATED WORK 2.1 Document Metadata Extraction Methods have been proposed for performing automatic metadata extraction from documents; however, the main focus was on extraction from research papers.",
                "The proposed methods fall into two categories: the rule based approach and the machine learning based approach.",
                "Giuffrida et al. [9], for instance, developed a rule-based system for automatically extracting metadata from research papers in Postscript.",
                "They used rules like titles are usually located on the upper portions of the first pages and they are usually in the largest font sizes.",
                "Liddy et al. [14] and Yilmazel el al. [23] performed metadata extraction from educational materials using rule-based natural language processing technologies.",
                "Mao et al. [16] also conducted automatic metadata extraction from research papers using rules on formatting information.",
                "The rule-based approach can achieve high performance.",
                "However, it also has disadvantages.",
                "It is less adaptive and robust when compared with the machine learning approach.",
                "Han et al. [10], for instance, conducted metadata extraction with the machine learning approach.",
                "They viewed the problem as that of classifying the lines in a document into the categories of metadata and proposed using Support Vector Machines as the classifier.",
                "They mainly used linguistic information as features.",
                "They reported high extraction accuracy from research papers in terms of precision and recall. 2.2 Information Extraction Metadata extraction can be viewed as an application of information extraction, in which given a sequence of instances, we identify a subsequence that represents information in which we are interested.",
                "Hidden Markov Model [6], Maximum Entropy Model [1, 4], Maximum Entropy Markov Model [17], Support Vector Machines [3], Conditional Random Field [12], and Voted Perceptron [2] are widely used information extraction models.",
                "Information extraction has been applied, for instance, to part-ofspeech tagging [20], named entity recognition [25] and table extraction [19]. 2.3 Search Using Title Information Title information is useful for document retrieval.",
                "In the system Citeseer, for instance, Giles et al. managed to extract titles from research papers and make use of the extracted titles in metadata search of papers [8].",
                "In web search, the title fields (i.e., file properties) and anchor texts of web pages (HTML documents) can be viewed as titles of the pages [5].",
                "Many search engines seem to utilize them for web page retrieval [7, 11, 18, 22].",
                "Zhang et al., found that web pages with well-defined metadata are more easily retrieved than those without well-defined metadata [24].",
                "To the best of our knowledge, no research has been conducted on using extracted titles from general documents (e.g., Office documents) for search of the documents. 146 3.",
                "MOTIVATION AND PROBLEM SETTING We consider the issue of automatically extracting titles from general documents.",
                "By general documents, we mean documents that belong to one of any number of specific genres.",
                "The documents can be presentations, books, book chapters, technical papers, brochures, reports, memos, specifications, letters, announcements, or resumes.",
                "General documents are more widely available in digital libraries, intranets, and internet, and thus investigation on title extraction from them is sorely needed.",
                "Figure 1 shows an estimate on distributions of file formats on intranet and internet [15].",
                "Office and PDF are the main file formats on the intranet.",
                "Even on the internet, the documents in the formats are still not negligible, given its extremely large size.",
                "In this paper, without loss of generality, we take Office documents as an example.",
                "Figure 1.",
                "Distributions of file formats in internet and intranet.",
                "For Office documents, users can define titles as file properties using a feature provided by Office.",
                "We found in an experiment, however, that users seldom use the feature and thus titles in file properties are usually very inaccurate.",
                "That is to say, titles in file properties are usually inconsistent with the true titles in the file bodies that are created by the authors and are visible to readers.",
                "We collected 6,000 Word and 6,000 PowerPoint documents from an intranet and the internet and examined how many titles in the file properties are correct.",
                "We found that surprisingly the accuracy was only 0.265 (cf., Section 6.3 for details).",
                "A number of reasons can be considered.",
                "For example, if one creates a new file by copying an old file, then the file property of the new file will also be copied from the old file.",
                "In another experiment, we found that Google uses the titles in file properties of Office documents in search and browsing, but the titles are not very accurate.",
                "We created 50 queries to search Word and PowerPoint documents and examined the top 15 results of each query returned by Google.",
                "We found that nearly all the titles presented in the search results were from the file properties of the documents.",
                "However, only 0.272 of them were correct.",
                "Actually, true titles usually exist at the beginnings of the bodies of documents.",
                "If we can accurately extract the titles from the bodies of documents, then we can exploit reliable title information in document processing.",
                "This is exactly the problem we address in this paper.",
                "More specifically, given a Word document, we are to extract the title from the top region of the first page.",
                "Given a PowerPoint document, we are to extract the title from the first slide.",
                "A title sometimes consists of a main title and one or two subtitles.",
                "We only consider extraction of the main title.",
                "As baselines for title extraction, we use that of always using the first lines as titles and that of always using the lines with largest font sizes as titles.",
                "Figure 2.",
                "Title extraction from Word document.",
                "Figure 3.",
                "Title extraction from PowerPoint document.",
                "Next, we define a specification for human judgments in title data annotation.",
                "The annotated data will be used in training and testing of the title extraction methods.",
                "Summary of the specification: The title of a document should be identified on the basis of common sense, if there is no difficulty in the identification.",
                "However, there are many cases in which the identification is not easy.",
                "There are some rules defined in the specification that guide identification for such cases.",
                "The rules include a title is usually in consecutive lines in the same format, a document can have no title, titles in images are not considered, a title should not contain words like draft, 147 whitepaper, etc, if it is difficult to determine which is the title, select the one in the largest font size, and if it is still difficult to determine which is the title, select the first candidate. (The specification covers all the cases we have encountered in data annotation.)",
                "Figures 2 and 3 show examples of Office documents from which we conduct title extraction.",
                "In Figure 2, Differences in Win32 API Implementations among Windows Operating Systems is the title of the Word document.",
                "Microsoft Windows on the top of this page is a picture and thus is ignored.",
                "In Figure 3, Building Competitive Advantages through an Agile Infrastructure is the title of the PowerPoint document.",
                "We have developed a tool for annotation of titles by human annotators.",
                "Figure 4 shows a snapshot of the tool.",
                "Figure 4.",
                "Title annotation tool. 4.",
                "TITLE EXTRACTION METHOD 4.1 Outline Title extraction based on machine learning consists of training and extraction.",
                "The same pre-processing step occurs before training and extraction.",
                "During pre-processing, from the top region of the first page of a Word document or the first slide of a PowerPoint document a number of units for processing are extracted.",
                "If a line (lines are separated by return symbols) only has a single format, then the line will become a unit.",
                "If a line has several parts and each of them has its own format, then each part will become a unit.",
                "Each unit will be treated as an instance in learning.",
                "A unit contains not only content information (linguistic information) but also formatting information.",
                "The input to pre-processing is a document and the output of pre-processing is a sequence of units (instances).",
                "Figure 5 shows the units obtained from the document in Figure 2.",
                "Figure 5.",
                "Example of units.",
                "In learning, the input is sequences of units where each sequence corresponds to a document.",
                "We take labeled units (labeled as title_begin, title_end, or other) in the sequences as training data and construct models for identifying whether a unit is title_begin title_end, or other.",
                "We employ four types of models: Perceptron, Maximum Entropy (ME), Perceptron Markov Model (PMM), and Maximum Entropy Markov Model (MEMM).",
                "In extraction, the input is a sequence of units from one document.",
                "We employ one type of model to identify whether a unit is title_begin, title_end, or other.",
                "We then extract units from the unit labeled with title_begin to the unit labeled with title_end.",
                "The result is the extracted title of the document.",
                "The unique characteristic of our approach is that we mainly utilize formatting information for title extraction.",
                "Our assumption is that although general documents vary in styles, their formats have certain patterns and we can learn and utilize the patterns for title extraction.",
                "This is in contrast to the work by Han et al., in which only linguistic features are used for extraction from research papers. 4.2 Models The four models actually can be considered in the same metadata extraction framework.",
                "That is why we apply them together to our current problem.",
                "Each input is a sequence of instances kxxx L21 together with a sequence of labels kyyy L21 . ix and iy represents an instance and its label, respectively ( ki ,,2,1 L= ).",
                "Recall that an instance here represents a unit.",
                "A label represents title_begin, title_end, or other.",
                "Here, k is the number of units in a document.",
                "In learning, we train a model which can be generally denoted as a conditional probability distribution )|( 11 kk XXYYP LL where iX and iY denote random variables taking instance ix and label iy as values, respectively ( ki ,,2,1 L= ).",
                "Learning Tool Extraction Tool 21121 2222122221 1121111211 nknnknn kk kk yyyxxx yyyxxx yyyxxx LL LL LL LL → → → )|(maxarg 11 mkmmkm xxyyP LL )|( 11 kk XXYYP LL Conditional Distribution mkmm xxx L21 Figure 6.",
                "Metadata extraction model.",
                "We can make assumptions about the general model in order to make it simple enough for training. 148 For example, we can assume that kYY ,,1 L are independent of each other given kXX ,,1 L .",
                "Thus, we have )|()|( )|( 11 11 kk kk XYPXYP XXYYP L LL = In this way, we decompose the model into a number of classifiers.",
                "We train the classifiers locally using the labeled data.",
                "As the classifier, we employ the Perceptron or Maximum Entropy model.",
                "We can also assume that the first order Markov property holds for kYY ,,1 L given kXX ,,1 L .",
                "Thus, we have )|()|( )|( 111 11 kkk kk XYYPXYP XXYYP −= L LL Again, we obtain a number of classifiers.",
                "However, the classifiers are conditioned on the previous label.",
                "When we employ the Percepton or Maximum Entropy model as a classifier, the models become a Percepton Markov Model or Maximum Entropy Markov Model, respectively.",
                "That is to say, the two models are more precise.",
                "In extraction, given a new sequence of instances, we resort to one of the constructed models to assign a sequence of labels to the sequence of instances, i.e., perform extraction.",
                "For Perceptron and ME, we assign labels locally and combine the results globally later using heuristics.",
                "Specifically, we first identify the most likely title_begin.",
                "Then we find the most likely title_end within three units after the title_begin.",
                "Finally, we extract as a title the units between the title_begin and the title_end.",
                "For PMM and MEMM, we employ the Viterbi algorithm to find the globally optimal label sequence.",
                "In this paper, for Perceptron, we actually employ an improved variant of it, called Perceptron with Uneven Margin [13].",
                "This version of Perceptron can work well especially when the number of positive instances and the number of negative instances differ greatly, which is exactly the case in our problem.",
                "We also employ an improved version of Perceptron Markov Model in which the Perceptron model is the so-called Voted Perceptron [2].",
                "In addition, in training, the parameters of the model are updated globally rather than locally. 4.3 Features There are two types of features: format features and linguistic features.",
                "We mainly use the former.",
                "The features are used for both the title-begin and the title-end classifiers. 4.3.1 Format Features Font Size: There are four binary features that represent the normalized font size of the unit (recall that a unit has only one type of font).",
                "If the font size of the unit is the largest in the document, then the first feature will be 1, otherwise 0.",
                "If the font size is the smallest in the document, then the fourth feature will be 1, otherwise 0.",
                "If the font size is above the average font size and not the largest in the document, then the second feature will be 1, otherwise 0.",
                "If the font size is below the average font size and not the smallest, the third feature will be 1, otherwise 0.",
                "It is necessary to conduct normalization on font sizes.",
                "For example, in one document the largest font size might be 12pt, while in another the smallest one might be 18pt.",
                "Boldface: This binary feature represents whether or not the current unit is in boldface.",
                "Alignment: There are four binary features that respectively represent the location of the current unit: left, center, right, and unknown alignment.",
                "The following format features with respect to context play an important role in title extraction.",
                "Empty Neighboring Unit: There are two binary features that represent, respectively, whether or not the previous unit and the current unit are blank lines.",
                "Font Size Change: There are two binary features that represent, respectively, whether or not the font size of the previous unit and the font size of the next unit differ from that of the current unit.",
                "Alignment Change: There are two binary features that represent, respectively, whether or not the alignment of the previous unit and the alignment of the next unit differ from that of the current one.",
                "Same Paragraph: There are two binary features that represent, respectively, whether or not the previous unit and the next unit are in the same paragraph as the current unit. 4.3.2 Linguistic Features The linguistic features are based on key words.",
                "Positive Word: This binary feature represents whether or not the current unit begins with one of the positive words.",
                "The positive words include title:, subject:, subject line: For example, in some documents the lines of titles and authors have the same formats.",
                "However, if lines begin with one of the positive words, then it is likely that they are title lines.",
                "Negative Word: This binary feature represents whether or not the current unit begins with one of the negative words.",
                "The negative words include To, By, created by, updated by, etc.",
                "There are more negative words than positive words.",
                "The above linguistic features are language dependent.",
                "Word Count: A title should not be too long.",
                "We heuristically create four intervals: [1, 2], [3, 6], [7, 9] and [9, ∞) and define one feature for each interval.",
                "If the number of words in a title falls into an interval, then the corresponding feature will be 1; otherwise 0.",
                "Ending Character: This feature represents whether the unit ends with :, -, or other special characters.",
                "A title usually does not end with such a character. 5.",
                "DOCUMENT RETRIEVAL METHOD We describe our method of document retrieval using extracted titles.",
                "Typically, in information retrieval a document is split into a number of fields including body, title, and anchor text.",
                "A ranking function in search can use different weights for different fields of 149 the document.",
                "Also, titles are typically assigned high weights, indicating that they are important for document retrieval.",
                "As explained previously, our experiment has shown that a significant number of documents actually have incorrect titles in the file properties, and thus in addition of using them we use the extracted titles as one more field of the document.",
                "By doing this, we attempt to improve the overall precision.",
                "In this paper, we employ a modification of BM25 that allows field weighting [21].",
                "As fields, we make use of body, title, extracted title and anchor.",
                "First, for each term in the query we count the term frequency in each field of the document; each field frequency is then weighted according to the corresponding weight parameter: ∑= f tfft tfwwtf Similarly, we compute the document length as a weighted sum of lengths of each field.",
                "Average document length in the corpus becomes the average of all weighted document lengths. ∑= f ff dlwwdl In our experiments we used 75.0,8.11 == bk .",
                "Weight for content was 1.0, title was 10.0, anchor was 10.0, and extracted title was 5.0. 6.",
                "EXPERIMENTAL RESULTS 6.1 Data Sets and Evaluation Measures We used two data sets in our experiments.",
                "First, we downloaded and randomly selected 5,000 Word documents and 5,000 PowerPoint documents from an intranet of Microsoft.",
                "We call it MS hereafter.",
                "Second, we downloaded and randomly selected 500 Word and 500 PowerPoint documents from the DotGov and DotCom domains on the internet, respectively.",
                "Figure 7 shows the distributions of the genres of the documents.",
                "We see that the documents are indeed general documents as we define them.",
                "Figure 7.",
                "Distributions of document genres.",
                "Third, a data set in Chinese was also downloaded from the internet.",
                "It includes 500 Word documents and 500 PowerPoint documents in Chinese.",
                "We manually labeled the titles of all the documents, on the basis of our specification.",
                "Not all the documents in the two data sets have titles.",
                "Table 1 shows the percentages of the documents having titles.",
                "We see that DotCom and DotGov have more PowerPoint documents with titles than MS.",
                "This might be because PowerPoint documents published on the internet are more formal than those on the intranet.",
                "Table 1.",
                "The portion of documents with titles Domain Type MS DotCom DotGov Word 75.7% 77.8% 75.6% PowerPoint 82.1% 93.4% 96.4% In our experiments, we conducted evaluations on title extraction in terms of precision, recall, and F-measure.",
                "The evaluation measures are defined as follows: Precision: P = A / ( A + B ) Recall: R = A / ( A + C ) F-measure: F1 = 2PR / ( P + R ) Here, A, B, C, and D are numbers of documents as those defined in Table 2.",
                "Table 2.",
                "Contingence table with regard to title extraction Is title Is not title Extracted A B Not extracted C D 6.2 Baselines We test the accuracies of the two baselines described in section 4.2.",
                "They are denoted as largest font size and first line respectively. 6.3 Accuracy of Titles in File Properties We investigate how many titles in the file properties of the documents are reliable.",
                "We view the titles annotated by humans as true titles and test how many titles in the file properties can approximately match with the true titles.",
                "We use Edit Distance to conduct the approximate match. (Approximate match is only used in this evaluation).",
                "This is because sometimes human annotated titles can be slightly different from the titles in file properties on the surface, e.g., contain extra spaces).",
                "Given string A and string B: if ( (D == 0) or ( D / ( La + Lb ) < θ ) ) then string A = string B D: Edit Distance between string A and string B La: length of string A Lb: length of string B θ: 0.1 ∑ × ++− + = t t n N wtf avwdl wdl bbk kwtf FBM )log( ))1(( )1( 25 1 1 150 Table 3.",
                "Accuracies of titles in file properties File Type Domain Precision Recall F1 MS 0.299 0.311 0.305 DotCom 0.210 0.214 0.212Word DotGov 0.182 0.177 0.180 MS 0.229 0.245 0.237 DotCom 0.185 0.186 0.186PowerPoint DotGov 0.180 0.182 0.181 6.4 Comparison with Baselines We conducted title extraction from the first data set (Word and PowerPoint in MS).",
                "As the model, we used Perceptron.",
                "We conduct 4-fold cross validation.",
                "Thus, all the results reported here are those averaged over 4 trials.",
                "Tables 4 and 5 show the results.",
                "We see that Perceptron significantly outperforms the baselines.",
                "In the evaluation, we use exact matching between the true titles annotated by humans and the extracted titles.",
                "Table 4.",
                "Accuracies of title extraction with Word Precision Recall F1 Model Perceptron 0.810 0.837 0.823 Largest font size 0.700 0.758 0.727 Baselines First line 0.707 0.767 0.736 Table 5.",
                "Accuracies of title extraction with PowerPoint Precision Recall F1 Model Perceptron 0.875 0. 895 0.885 Largest font size 0.844 0.887 0.865 Baselines First line 0.639 0.671 0.655 We see that the machine learning approach can achieve good performance in title extraction.",
                "For Word documents both precision and recall of the approach are 8 percent higher than those of the baselines.",
                "For PowerPoint both precision and recall of the approach are 2 percent higher than those of the baselines.",
                "We conduct significance tests.",
                "The results are shown in Table 6.",
                "Here, Largest denotes the baseline of using the largest font size, First denotes the baseline of using the first line.",
                "The results indicate that the improvements of machine learning over baselines are statistically significant (in the sense p-value < 0.05) Table 6.",
                "Sign test results Documents Type Sign test between p-value Perceptron vs. Largest 3.59e-26 Word Perceptron vs. First 7.12e-10 Perceptron vs. Largest 0.010 PowerPoint Perceptron vs. First 5.13e-40 We see, from the results, that the two baselines can work well for title extraction, suggesting that font size and position information are most useful features for title extraction.",
                "However, it is also obvious that using only these two features is not enough.",
                "There are cases in which all the lines have the same font size (i.e., the largest font size), or cases in which the lines with the largest font size only contain general descriptions like Confidential, White paper, etc.",
                "For those cases, the largest font size method cannot work well.",
                "For similar reasons, the first line method alone cannot work well, either.",
                "With the combination of different features (evidence in title judgment), Perceptron can outperform Largest and First.",
                "We investigate the performance of solely using linguistic features.",
                "We found that it does not work well.",
                "It seems that the format features play important roles and the linguistic features are supplements..",
                "Figure 8.",
                "An example Word document.",
                "Figure 9.",
                "An example PowerPoint document.",
                "We conducted an error analysis on the results of Perceptron.",
                "We found that the errors fell into three categories. (1) About one third of the errors were related to hard cases.",
                "In these documents, the layouts of the first pages were difficult to understand, even for humans.",
                "Figure 8 and 9 shows examples. (2) Nearly one fourth of the errors were from the documents which do not have true titles but only contain bullets.",
                "Since we conduct extraction from the top regions, it is difficult to get rid of these errors with the current approach. (3).",
                "Confusions between main titles and subtitles were another type of error.",
                "Since we only labeled the main titles as titles, the extractions of both titles were considered incorrect.",
                "This type of error does little harm to document processing like search, however. 6.5 Comparison between Models To compare the performance of different machine learning models, we conducted another experiment.",
                "Again, we perform 4-fold cross 151 validation on the first data set (MS).",
                "Table 7, 8 shows the results of all the four models.",
                "It turns out that Perceptron and PMM perform the best, followed by MEMM, and ME performs the worst.",
                "In general, the Markovian models perform better than or as well as their classifier counterparts.",
                "This seems to be because the Markovian models are trained globally, while the classifiers are trained locally.",
                "The Perceptron based models perform better than the ME based counterparts.",
                "This seems to be because the Perceptron based models are created to make better classifications, while ME models are constructed for better prediction.",
                "Table 7.",
                "Comparison between different learning models for title extraction with Word Model Precision Recall F1 Perceptron 0.810 0.837 0.823 MEMM 0.797 0.824 0.810 PMM 0.827 0.823 0.825 ME 0.801 0.621 0.699 Table 8.",
                "Comparison between different learning models for title extraction with PowerPoint Model Precision Recall F1 Perceptron 0.875 0. 895 0. 885 MEMM 0.841 0.861 0.851 PMM 0.873 0.896 0.885 ME 0.753 0.766 0.759 6.6 Domain Adaptation We apply the model trained with the first data set (MS) to the second data set (DotCom and DotGov).",
                "Tables 9-12 show the results.",
                "Table 9.",
                "Accuracies of title extraction with Word in DotGov Precision Recall F1 Model Perceptron 0.716 0.759 0.737 Largest font size 0.549 0.619 0.582Baselines First line 0.462 0.521 0.490 Table 10.",
                "Accuracies of title extraction with PowerPoint in DotGov Precision Recall F1 Model Perceptron 0.900 0.906 0.903 Largest font size 0.871 0.888 0.879Baselines First line 0.554 0.564 0.559 Table 11.",
                "Accuracies of title extraction with Word in DotCom Precisio n Recall F1 Model Perceptron 0.832 0.880 0.855 Largest font size 0.676 0.753 0.712Baselines First line 0.577 0.643 0.608 Table 12.",
                "Performance of PowerPoint document title extraction in DotCom Precisio n Recall F1 Model Perceptron 0.910 0.903 0.907 Largest font size 0.864 0.886 0.875Baselines First line 0.570 0.585 0.577 From the results, we see that the models can be adapted to different domains well.",
                "There is almost no drop in accuracy.",
                "The results indicate that the patterns of title formats exist across different domains, and it is possible to construct a domain independent model by mainly using formatting information. 6.7 Language Adaptation We apply the model trained with the data in English (MS) to the data set in Chinese.",
                "Tables 13-14 show the results.",
                "Table 13.",
                "Accuracies of title extraction with Word in Chinese Precision Recall F1 Model Perceptron 0.817 0.805 0.811 Largest font size 0.722 0.755 0.738Baselines First line 0.743 0.777 0.760 Table 14.",
                "Accuracies of title extraction with PowerPoint in Chinese Precision Recall F1 Model Perceptron 0.766 0.812 0.789 Largest font size 0.753 0.813 0.782Baselines First line 0.627 0.676 0.650 We see that the models can be adapted to a different language.",
                "There are only small drops in accuracy.",
                "Obviously, the linguistic features do not work for Chinese, but the effect of not using them is negligible.",
                "The results indicate that the patterns of title formats exist across different languages.",
                "From the domain adaptation and language adaptation results, we conclude that the use of formatting information is the key to a successful extraction from general documents. 6.8 Search with Extracted Titles We performed experiments on using title extraction for document retrieval.",
                "As a baseline, we employed BM25 without using extracted titles.",
                "The ranking mechanism was as described in Section 5.",
                "The weights were heuristically set.",
                "We did not conduct optimization on the weights.",
                "The evaluation was conducted on a corpus of 1.3 M documents crawled from the intranet of Microsoft using 100 evaluation queries obtained from this intranets search engine query logs. 50 queries were from the most popular set, while 50 queries other were chosen randomly.",
                "Users were asked to provide judgments of the degree of document relevance from a scale of 1to 5 (1 meaning detrimental, 2 - bad, 3 - fair, 4 - good and 5 - excellent). 152 Figure 10 shows the results.",
                "In the chart two sets of precision results were obtained by either considering good or excellent documents as relevant (left 3 bars with relevance threshold 0.5), or by considering only excellent documents as relevant (right 3 bars with relevance threshold 1.0) 0 0.05 0.1 0.15 0.2 0.25 0.3 0.35 0.4 0.45 P@10 P@5 Reciprocal P@10 P@5 Reciprocal 0.5 1 BM25 Anchor, Title, Body BM25 Anchor, Title, Body, ExtractedTitle Name All RelevanceThreshold Data Description Figure 10.",
                "Search ranking results.",
                "Figure 10 shows different document retrieval results with different ranking functions in terms of precision @10, precision @5 and reciprocal rank: • Blue bar - BM25 including the fields body, title (file property), and anchor text. • Purple bar - BM25 including the fields body, title (file property), anchor text, and extracted title.",
                "With the additional field of extracted title included in BM25 the precision @10 increased from 0.132 to 0.145, or by ~10%.",
                "Thus, it is safe to say that the use of extracted title can indeed improve the precision of document retrieval. 7.",
                "CONCLUSION In this paper, we have investigated the problem of automatically extracting titles from general documents.",
                "We have tried using a machine learning approach to address the problem.",
                "Previous work showed that the machine learning approach can work well for metadata extraction from research papers.",
                "In this paper, we showed that the approach can work for extraction from general documents as well.",
                "Our experimental results indicated that the machine learning approach can work significantly better than the baselines in title extraction from Office documents.",
                "Previous work on metadata extraction mainly used linguistic features in documents, while we mainly used formatting information.",
                "It appeared that using formatting information is a key for successfully conducting title extraction from general documents.",
                "We tried different machine learning models including Perceptron, Maximum Entropy, Maximum Entropy Markov Model, and Voted Perceptron.",
                "We found that the performance of the Perceptorn models was the best.",
                "We applied models constructed in one domain to another domain and applied models trained in one language to another language.",
                "We found that the accuracies did not drop substantially across different domains and across different languages, indicating that the models were generic.",
                "We also attempted to use the extracted titles in document retrieval.",
                "We observed a significant improvement in document ranking performance for search when using extracted title information.",
                "All the above investigations were not conducted in previous work, and through our investigations we verified the generality and the significance of the title extraction approach. 8.",
                "ACKNOWLEDGEMENTS We thank Chunyu Wei and Bojuan Zhao for their work on data annotation.",
                "We acknowledge Jinzhu Li for his assistance in conducting the experiments.",
                "We thank Ming Zhou, John Chen, Jun Xu, and the anonymous reviewers of JCDL05 for their valuable comments on this paper. 9.",
                "REFERENCES [1] Berger, A. L., Della Pietra, S. A., and Della Pietra, V. J.",
                "A maximum entropy approach to natural language processing.",
                "Computational Linguistics, 22:39-71, 1996. [2] Collins, M. Discriminative training methods for hidden markov models: theory and experiments with perceptron algorithms.",
                "In Proceedings of Conference on Empirical Methods in Natural Language Processing, 1-8, 2002. [3] Cortes, C. and Vapnik, V. Support-vector networks.",
                "Machine Learning, 20:273-297, 1995. [4] Chieu, H. L. and Ng, H. T. A maximum entropy approach to information extraction from semi-structured and free text.",
                "In Proceedings of the Eighteenth National Conference on Artificial Intelligence, 768-791, 2002. [5] Evans, D. K., Klavans, J. L., and McKeown, K. R. Columbia newsblaster: multilingual news summarization on the Web.",
                "In Proceedings of Human Language Technology conference / North American chapter of the Association for Computational Linguistics annual meeting, 1-4, 2004. [6] Ghahramani, Z. and Jordan, M. I. Factorial hidden markov models.",
                "Machine Learning, 29:245-273, 1997. [7] Gheel, J. and Anderson, T. Data and metadata for finding and reminding, In Proceedings of the 1999 International Conference on Information Visualization, 446-451,1999. [8] Giles, C. L., Petinot, Y., Teregowda P. B., Han, H., Lawrence, S., Rangaswamy, A., and Pal, N. eBizSearch: a niche search engine for e-Business.",
                "In Proceedings of the 26th Annual International ACM SIGIR Conference on Research and Development in Information Retrieval, 413414, 2003. [9] Giuffrida, G., Shek, E. C., and Yang, J. Knowledge-based metadata extraction from PostScript files.",
                "In Proceedings of the Fifth ACM Conference on Digital Libraries, 77-84, 2000. [10] Han, H., Giles, C. L., Manavoglu, E., Zha, H., Zhang, Z., and Fox, E. A.",
                "Automatic document metadata extraction using support vector machines.",
                "In Proceedings of the Third ACM/IEEE-CS Joint Conference on Digital Libraries, 37-48, 2003. [11] Kobayashi, M., and Takeda, K. Information retrieval on the Web.",
                "ACM Computing Surveys, 32:144-173, 2000. [12] Lafferty, J., McCallum, A., and Pereira, F. Conditional random fields: probabilistic models for segmenting and 153 labeling sequence data.",
                "In Proceedings of the Eighteenth International Conference on Machine Learning, 282-289, 2001. [13] Li, Y., Zaragoza, H., Herbrich, R., Shawe-Taylor J., and Kandola, J. S. The perceptron algorithm with uneven margins.",
                "In Proceedings of the Nineteenth International Conference on Machine Learning, 379-386, 2002. [14] Liddy, E. D., Sutton, S., Allen, E., Harwell, S., Corieri, S., Yilmazel, O., Ozgencil, N. E., Diekema, A., McCracken, N., and Silverstein, J.",
                "Automatic Metadata generation & evaluation.",
                "In Proceedings of the 25th Annual International ACM SIGIR Conference on Research and Development in Information Retrieval, 401-402, 2002. [15] Littlefield, A.",
                "Effective enterprise information retrieval across new content formats.",
                "In Proceedings of the Seventh Search Engine Conference, http://www.infonortics.com/searchengines/sh02/02prog.html, 2002. [16] Mao, S., Kim, J. W., and Thoma, G. R. A dynamic feature generation system for automated metadata extraction in preservation of digital materials.",
                "In Proceedings of the First International Workshop on Document Image Analysis for Libraries, 225-232, 2004. [17] McCallum, A., Freitag, D., and Pereira, F. Maximum entropy markov models for information extraction and segmentation.",
                "In Proceedings of the Seventeenth International Conference on Machine Learning, 591-598, 2000. [18] Murphy, L. D. Digital document metadata in organizations: roles, analytical approaches, and future research directions.",
                "In Proceedings of the Thirty-First Annual Hawaii International Conference on System Sciences, 267-276, 1998. [19] Pinto, D., McCallum, A., Wei, X., and Croft, W. B.",
                "Table extraction using conditional random fields.",
                "In Proceedings of the 26th Annual International ACM SIGIR Conference on Research and Development in Information Retrieval, 235242, 2003. [20] Ratnaparkhi, A. Unsupervised statistical models for prepositional phrase attachment.",
                "In Proceedings of the Seventeenth International Conference on Computational Linguistics. 1079-1085, 1998. [21] Robertson, S., Zaragoza, H., and Taylor, M. Simple BM25 extension to multiple weighted fields, In Proceedings of ACM Thirteenth Conference on Information and Knowledge Management, 42-49, 2004. [22] Yi, J. and Sundaresan, N. Metadata based Web mining for relevance, In Proceedings of the 2000 International Symposium on Database Engineering & Applications, 113121, 2000. [23] Yilmazel, O., Finneran, C. M., and Liddy, E. D. MetaExtract: An NLP system to automatically assign metadata.",
                "In Proceedings of the 2004 Joint ACM/IEEE Conference on Digital Libraries, 241-242, 2004. [24] Zhang, J. and Dimitroff, A. Internet search engines response to metadata Dublin Core implementation.",
                "Journal of Information Science, 30:310-320, 2004. [25] Zhang, L., Pan, Y., and Zhang, T. Recognising and using named entities: focused named entity recognition using machine learning.",
                "In Proceedings of the 27th Annual International ACM SIGIR Conference on Research and Development in Information Retrieval, 281-288, 2004. [26] http://dublincore.org/groups/corporate/Seattle/ 154"
            ],
            "original_annotated_samples": [],
            "translated_annotated_samples": [],
            "translated_text": "",
            "candidates": [],
            "error": []
        },
        "generality of model": {
            "translated_key": "generalidad de modelo",
            "is_in_text": true,
            "original_annotated_sentences": [
                "Automatic Extraction of Titles from General Documents using Machine Learning Yunhua Hu1 Computer Science Department Xian Jiaotong University No 28, Xianning West Road Xian, China, 710049 yunhuahu@mail.xjtu.edu.cn Hang Li, Yunbo Cao Microsoft Research Asia 5F Sigma Center, No.",
                "49 Zhichun Road, Haidian, Beijing, China, 100080 {hangli,yucao}@microsoft.com Qinghua Zheng Computer Science Department Xian Jiaotong University No 28, Xianning West Road Xian, China, 710049 qhzheng@mail.xjtu.edu.cn Dmitriy Meyerzon Microsoft Corporation One Microsoft Way Redmond, WA, USA, 98052 dmitriym@microsoft.com ABSTRACT In this paper, we propose a machine learning approach to title extraction from general documents.",
                "By general documents, we mean documents that can belong to any one of a number of specific genres, including presentations, book chapters, technical papers, brochures, reports, and letters.",
                "Previously, methods have been proposed mainly for title extraction from research papers.",
                "It has not been clear whether it could be possible to conduct automatic title extraction from general documents.",
                "As a case study, we consider extraction from Office including Word and PowerPoint.",
                "In our approach, we annotate titles in sample documents (for Word and PowerPoint respectively) and take them as training data, train machine learning models, and perform title extraction using the trained models.",
                "Our method is unique in that we mainly utilize formatting information such as font size as features in the models.",
                "It turns out that the use of formatting information can lead to quite accurate extraction from general documents.",
                "Precision and recall for title extraction from Word is 0.810 and 0.837 respectively, and precision and recall for title extraction from PowerPoint is 0.875 and 0.895 respectively in an experiment on intranet data.",
                "Other important new findings in this work include that we can train models in one domain and apply them to another domain, and more surprisingly we can even train models in one language and apply them to another language.",
                "Moreover, we can significantly improve search ranking results in document retrieval by using the extracted titles.",
                "Categories and Subject Descriptors H.3.3 [Information Storage and Retrieval]: Information Search and Retrieval - Search Process; H.4.1 [Information Systems Applications]: Office Automation - Word processing; D.2.8 [Software Engineering]: Metrics - complexity measures, performance measures General Terms Algorithms, Experimentation, Performance. 1.",
                "INTRODUCTION Metadata of documents is useful for many kinds of document processing such as search, browsing, and filtering.",
                "Ideally, metadata is defined by the authors of documents and is then used by various systems.",
                "However, people seldom define document metadata by themselves, even when they have convenient metadata definition tools [26].",
                "Thus, how to automatically extract metadata from the bodies of documents turns out to be an important research issue.",
                "Methods for performing the task have been proposed.",
                "However, the focus was mainly on extraction from research papers.",
                "For instance, Han et al. [10] proposed a machine learning based method to conduct extraction from research papers.",
                "They formalized the problem as that of classification and employed Support Vector Machines as the classifier.",
                "They mainly used linguistic features in the model.1 In this paper, we consider metadata extraction from general documents.",
                "By general documents, we mean documents that may belong to any one of a number of specific genres.",
                "General documents are more widely available in digital libraries, intranets and the internet, and thus investigation on extraction from them is sorely needed.",
                "Research papers usually have well-formed styles and noticeable characteristics.",
                "In contrast, the styles of general documents can vary greatly.",
                "It has not been clarified whether a machine learning based approach can work well for this task.",
                "There are many types of metadata: title, author, date of creation, etc.",
                "As a case study, we consider title extraction in this paper.",
                "General documents can be in many different file formats: Microsoft Office, PDF (PS), etc.",
                "As a case study, we consider extraction from Office including Word and PowerPoint.",
                "We take a machine learning approach.",
                "We annotate titles in sample documents (for Word and PowerPoint respectively) and take them as training data to train several types of models, and perform title extraction using any one type of the trained models.",
                "In the models, we mainly utilize formatting information such as font size as features.",
                "We employ the following models: Maximum Entropy Model, Perceptron with Uneven Margins, Maximum Entropy Markov Model, and Voted Perceptron.",
                "In this paper, we also investigate the following three problems, which did not seem to have been examined previously. (1) Comparison between models: among the models above, which model performs best for title extraction; (2) <br>generality of model</br>: whether it is possible to train a model on one domain and apply it to another domain, and whether it is possible to train a model in one language and apply it to another language; (3) Usefulness of extracted titles: whether extracted titles can improve document processing such as search.",
                "Experimental results indicate that our approach works well for title extraction from general documents.",
                "Our method can significantly outperform the baselines: one that always uses the first lines as titles and the other that always uses the lines in the largest font sizes as titles.",
                "Precision and recall for title extraction from Word are 0.810 and 0.837 respectively, and precision and recall for title extraction from PowerPoint are 0.875 and 0.895 respectively.",
                "It turns out that the use of format features is the key to successful title extraction. (1) We have observed that Perceptron based models perform better in terms of extraction accuracies. (2) We have empirically verified that the models trained with our approach are generic in the sense that they can be trained on one domain and applied to another, and they can be trained in one language and applied to another. (3) We have found that using the extracted titles we can significantly improve precision of document retrieval (by 10%).",
                "We conclude that we can indeed conduct reliable title extraction from general documents and use the extracted results to improve real applications.",
                "The rest of the paper is organized as follows.",
                "In section 2, we introduce related work, and in section 3, we explain the motivation and problem setting of our work.",
                "In section 4, we describe our method of title extraction, and in section 5, we describe our method of document retrieval using extracted titles.",
                "Section 6 gives our experimental results.",
                "We make concluding remarks in section 7. 2.",
                "RELATED WORK 2.1 Document Metadata Extraction Methods have been proposed for performing automatic metadata extraction from documents; however, the main focus was on extraction from research papers.",
                "The proposed methods fall into two categories: the rule based approach and the machine learning based approach.",
                "Giuffrida et al. [9], for instance, developed a rule-based system for automatically extracting metadata from research papers in Postscript.",
                "They used rules like titles are usually located on the upper portions of the first pages and they are usually in the largest font sizes.",
                "Liddy et al. [14] and Yilmazel el al. [23] performed metadata extraction from educational materials using rule-based natural language processing technologies.",
                "Mao et al. [16] also conducted automatic metadata extraction from research papers using rules on formatting information.",
                "The rule-based approach can achieve high performance.",
                "However, it also has disadvantages.",
                "It is less adaptive and robust when compared with the machine learning approach.",
                "Han et al. [10], for instance, conducted metadata extraction with the machine learning approach.",
                "They viewed the problem as that of classifying the lines in a document into the categories of metadata and proposed using Support Vector Machines as the classifier.",
                "They mainly used linguistic information as features.",
                "They reported high extraction accuracy from research papers in terms of precision and recall. 2.2 Information Extraction Metadata extraction can be viewed as an application of information extraction, in which given a sequence of instances, we identify a subsequence that represents information in which we are interested.",
                "Hidden Markov Model [6], Maximum Entropy Model [1, 4], Maximum Entropy Markov Model [17], Support Vector Machines [3], Conditional Random Field [12], and Voted Perceptron [2] are widely used information extraction models.",
                "Information extraction has been applied, for instance, to part-ofspeech tagging [20], named entity recognition [25] and table extraction [19]. 2.3 Search Using Title Information Title information is useful for document retrieval.",
                "In the system Citeseer, for instance, Giles et al. managed to extract titles from research papers and make use of the extracted titles in metadata search of papers [8].",
                "In web search, the title fields (i.e., file properties) and anchor texts of web pages (HTML documents) can be viewed as titles of the pages [5].",
                "Many search engines seem to utilize them for web page retrieval [7, 11, 18, 22].",
                "Zhang et al., found that web pages with well-defined metadata are more easily retrieved than those without well-defined metadata [24].",
                "To the best of our knowledge, no research has been conducted on using extracted titles from general documents (e.g., Office documents) for search of the documents. 146 3.",
                "MOTIVATION AND PROBLEM SETTING We consider the issue of automatically extracting titles from general documents.",
                "By general documents, we mean documents that belong to one of any number of specific genres.",
                "The documents can be presentations, books, book chapters, technical papers, brochures, reports, memos, specifications, letters, announcements, or resumes.",
                "General documents are more widely available in digital libraries, intranets, and internet, and thus investigation on title extraction from them is sorely needed.",
                "Figure 1 shows an estimate on distributions of file formats on intranet and internet [15].",
                "Office and PDF are the main file formats on the intranet.",
                "Even on the internet, the documents in the formats are still not negligible, given its extremely large size.",
                "In this paper, without loss of generality, we take Office documents as an example.",
                "Figure 1.",
                "Distributions of file formats in internet and intranet.",
                "For Office documents, users can define titles as file properties using a feature provided by Office.",
                "We found in an experiment, however, that users seldom use the feature and thus titles in file properties are usually very inaccurate.",
                "That is to say, titles in file properties are usually inconsistent with the true titles in the file bodies that are created by the authors and are visible to readers.",
                "We collected 6,000 Word and 6,000 PowerPoint documents from an intranet and the internet and examined how many titles in the file properties are correct.",
                "We found that surprisingly the accuracy was only 0.265 (cf., Section 6.3 for details).",
                "A number of reasons can be considered.",
                "For example, if one creates a new file by copying an old file, then the file property of the new file will also be copied from the old file.",
                "In another experiment, we found that Google uses the titles in file properties of Office documents in search and browsing, but the titles are not very accurate.",
                "We created 50 queries to search Word and PowerPoint documents and examined the top 15 results of each query returned by Google.",
                "We found that nearly all the titles presented in the search results were from the file properties of the documents.",
                "However, only 0.272 of them were correct.",
                "Actually, true titles usually exist at the beginnings of the bodies of documents.",
                "If we can accurately extract the titles from the bodies of documents, then we can exploit reliable title information in document processing.",
                "This is exactly the problem we address in this paper.",
                "More specifically, given a Word document, we are to extract the title from the top region of the first page.",
                "Given a PowerPoint document, we are to extract the title from the first slide.",
                "A title sometimes consists of a main title and one or two subtitles.",
                "We only consider extraction of the main title.",
                "As baselines for title extraction, we use that of always using the first lines as titles and that of always using the lines with largest font sizes as titles.",
                "Figure 2.",
                "Title extraction from Word document.",
                "Figure 3.",
                "Title extraction from PowerPoint document.",
                "Next, we define a specification for human judgments in title data annotation.",
                "The annotated data will be used in training and testing of the title extraction methods.",
                "Summary of the specification: The title of a document should be identified on the basis of common sense, if there is no difficulty in the identification.",
                "However, there are many cases in which the identification is not easy.",
                "There are some rules defined in the specification that guide identification for such cases.",
                "The rules include a title is usually in consecutive lines in the same format, a document can have no title, titles in images are not considered, a title should not contain words like draft, 147 whitepaper, etc, if it is difficult to determine which is the title, select the one in the largest font size, and if it is still difficult to determine which is the title, select the first candidate. (The specification covers all the cases we have encountered in data annotation.)",
                "Figures 2 and 3 show examples of Office documents from which we conduct title extraction.",
                "In Figure 2, Differences in Win32 API Implementations among Windows Operating Systems is the title of the Word document.",
                "Microsoft Windows on the top of this page is a picture and thus is ignored.",
                "In Figure 3, Building Competitive Advantages through an Agile Infrastructure is the title of the PowerPoint document.",
                "We have developed a tool for annotation of titles by human annotators.",
                "Figure 4 shows a snapshot of the tool.",
                "Figure 4.",
                "Title annotation tool. 4.",
                "TITLE EXTRACTION METHOD 4.1 Outline Title extraction based on machine learning consists of training and extraction.",
                "The same pre-processing step occurs before training and extraction.",
                "During pre-processing, from the top region of the first page of a Word document or the first slide of a PowerPoint document a number of units for processing are extracted.",
                "If a line (lines are separated by return symbols) only has a single format, then the line will become a unit.",
                "If a line has several parts and each of them has its own format, then each part will become a unit.",
                "Each unit will be treated as an instance in learning.",
                "A unit contains not only content information (linguistic information) but also formatting information.",
                "The input to pre-processing is a document and the output of pre-processing is a sequence of units (instances).",
                "Figure 5 shows the units obtained from the document in Figure 2.",
                "Figure 5.",
                "Example of units.",
                "In learning, the input is sequences of units where each sequence corresponds to a document.",
                "We take labeled units (labeled as title_begin, title_end, or other) in the sequences as training data and construct models for identifying whether a unit is title_begin title_end, or other.",
                "We employ four types of models: Perceptron, Maximum Entropy (ME), Perceptron Markov Model (PMM), and Maximum Entropy Markov Model (MEMM).",
                "In extraction, the input is a sequence of units from one document.",
                "We employ one type of model to identify whether a unit is title_begin, title_end, or other.",
                "We then extract units from the unit labeled with title_begin to the unit labeled with title_end.",
                "The result is the extracted title of the document.",
                "The unique characteristic of our approach is that we mainly utilize formatting information for title extraction.",
                "Our assumption is that although general documents vary in styles, their formats have certain patterns and we can learn and utilize the patterns for title extraction.",
                "This is in contrast to the work by Han et al., in which only linguistic features are used for extraction from research papers. 4.2 Models The four models actually can be considered in the same metadata extraction framework.",
                "That is why we apply them together to our current problem.",
                "Each input is a sequence of instances kxxx L21 together with a sequence of labels kyyy L21 . ix and iy represents an instance and its label, respectively ( ki ,,2,1 L= ).",
                "Recall that an instance here represents a unit.",
                "A label represents title_begin, title_end, or other.",
                "Here, k is the number of units in a document.",
                "In learning, we train a model which can be generally denoted as a conditional probability distribution )|( 11 kk XXYYP LL where iX and iY denote random variables taking instance ix and label iy as values, respectively ( ki ,,2,1 L= ).",
                "Learning Tool Extraction Tool 21121 2222122221 1121111211 nknnknn kk kk yyyxxx yyyxxx yyyxxx LL LL LL LL → → → )|(maxarg 11 mkmmkm xxyyP LL )|( 11 kk XXYYP LL Conditional Distribution mkmm xxx L21 Figure 6.",
                "Metadata extraction model.",
                "We can make assumptions about the general model in order to make it simple enough for training. 148 For example, we can assume that kYY ,,1 L are independent of each other given kXX ,,1 L .",
                "Thus, we have )|()|( )|( 11 11 kk kk XYPXYP XXYYP L LL = In this way, we decompose the model into a number of classifiers.",
                "We train the classifiers locally using the labeled data.",
                "As the classifier, we employ the Perceptron or Maximum Entropy model.",
                "We can also assume that the first order Markov property holds for kYY ,,1 L given kXX ,,1 L .",
                "Thus, we have )|()|( )|( 111 11 kkk kk XYYPXYP XXYYP −= L LL Again, we obtain a number of classifiers.",
                "However, the classifiers are conditioned on the previous label.",
                "When we employ the Percepton or Maximum Entropy model as a classifier, the models become a Percepton Markov Model or Maximum Entropy Markov Model, respectively.",
                "That is to say, the two models are more precise.",
                "In extraction, given a new sequence of instances, we resort to one of the constructed models to assign a sequence of labels to the sequence of instances, i.e., perform extraction.",
                "For Perceptron and ME, we assign labels locally and combine the results globally later using heuristics.",
                "Specifically, we first identify the most likely title_begin.",
                "Then we find the most likely title_end within three units after the title_begin.",
                "Finally, we extract as a title the units between the title_begin and the title_end.",
                "For PMM and MEMM, we employ the Viterbi algorithm to find the globally optimal label sequence.",
                "In this paper, for Perceptron, we actually employ an improved variant of it, called Perceptron with Uneven Margin [13].",
                "This version of Perceptron can work well especially when the number of positive instances and the number of negative instances differ greatly, which is exactly the case in our problem.",
                "We also employ an improved version of Perceptron Markov Model in which the Perceptron model is the so-called Voted Perceptron [2].",
                "In addition, in training, the parameters of the model are updated globally rather than locally. 4.3 Features There are two types of features: format features and linguistic features.",
                "We mainly use the former.",
                "The features are used for both the title-begin and the title-end classifiers. 4.3.1 Format Features Font Size: There are four binary features that represent the normalized font size of the unit (recall that a unit has only one type of font).",
                "If the font size of the unit is the largest in the document, then the first feature will be 1, otherwise 0.",
                "If the font size is the smallest in the document, then the fourth feature will be 1, otherwise 0.",
                "If the font size is above the average font size and not the largest in the document, then the second feature will be 1, otherwise 0.",
                "If the font size is below the average font size and not the smallest, the third feature will be 1, otherwise 0.",
                "It is necessary to conduct normalization on font sizes.",
                "For example, in one document the largest font size might be 12pt, while in another the smallest one might be 18pt.",
                "Boldface: This binary feature represents whether or not the current unit is in boldface.",
                "Alignment: There are four binary features that respectively represent the location of the current unit: left, center, right, and unknown alignment.",
                "The following format features with respect to context play an important role in title extraction.",
                "Empty Neighboring Unit: There are two binary features that represent, respectively, whether or not the previous unit and the current unit are blank lines.",
                "Font Size Change: There are two binary features that represent, respectively, whether or not the font size of the previous unit and the font size of the next unit differ from that of the current unit.",
                "Alignment Change: There are two binary features that represent, respectively, whether or not the alignment of the previous unit and the alignment of the next unit differ from that of the current one.",
                "Same Paragraph: There are two binary features that represent, respectively, whether or not the previous unit and the next unit are in the same paragraph as the current unit. 4.3.2 Linguistic Features The linguistic features are based on key words.",
                "Positive Word: This binary feature represents whether or not the current unit begins with one of the positive words.",
                "The positive words include title:, subject:, subject line: For example, in some documents the lines of titles and authors have the same formats.",
                "However, if lines begin with one of the positive words, then it is likely that they are title lines.",
                "Negative Word: This binary feature represents whether or not the current unit begins with one of the negative words.",
                "The negative words include To, By, created by, updated by, etc.",
                "There are more negative words than positive words.",
                "The above linguistic features are language dependent.",
                "Word Count: A title should not be too long.",
                "We heuristically create four intervals: [1, 2], [3, 6], [7, 9] and [9, ∞) and define one feature for each interval.",
                "If the number of words in a title falls into an interval, then the corresponding feature will be 1; otherwise 0.",
                "Ending Character: This feature represents whether the unit ends with :, -, or other special characters.",
                "A title usually does not end with such a character. 5.",
                "DOCUMENT RETRIEVAL METHOD We describe our method of document retrieval using extracted titles.",
                "Typically, in information retrieval a document is split into a number of fields including body, title, and anchor text.",
                "A ranking function in search can use different weights for different fields of 149 the document.",
                "Also, titles are typically assigned high weights, indicating that they are important for document retrieval.",
                "As explained previously, our experiment has shown that a significant number of documents actually have incorrect titles in the file properties, and thus in addition of using them we use the extracted titles as one more field of the document.",
                "By doing this, we attempt to improve the overall precision.",
                "In this paper, we employ a modification of BM25 that allows field weighting [21].",
                "As fields, we make use of body, title, extracted title and anchor.",
                "First, for each term in the query we count the term frequency in each field of the document; each field frequency is then weighted according to the corresponding weight parameter: ∑= f tfft tfwwtf Similarly, we compute the document length as a weighted sum of lengths of each field.",
                "Average document length in the corpus becomes the average of all weighted document lengths. ∑= f ff dlwwdl In our experiments we used 75.0,8.11 == bk .",
                "Weight for content was 1.0, title was 10.0, anchor was 10.0, and extracted title was 5.0. 6.",
                "EXPERIMENTAL RESULTS 6.1 Data Sets and Evaluation Measures We used two data sets in our experiments.",
                "First, we downloaded and randomly selected 5,000 Word documents and 5,000 PowerPoint documents from an intranet of Microsoft.",
                "We call it MS hereafter.",
                "Second, we downloaded and randomly selected 500 Word and 500 PowerPoint documents from the DotGov and DotCom domains on the internet, respectively.",
                "Figure 7 shows the distributions of the genres of the documents.",
                "We see that the documents are indeed general documents as we define them.",
                "Figure 7.",
                "Distributions of document genres.",
                "Third, a data set in Chinese was also downloaded from the internet.",
                "It includes 500 Word documents and 500 PowerPoint documents in Chinese.",
                "We manually labeled the titles of all the documents, on the basis of our specification.",
                "Not all the documents in the two data sets have titles.",
                "Table 1 shows the percentages of the documents having titles.",
                "We see that DotCom and DotGov have more PowerPoint documents with titles than MS.",
                "This might be because PowerPoint documents published on the internet are more formal than those on the intranet.",
                "Table 1.",
                "The portion of documents with titles Domain Type MS DotCom DotGov Word 75.7% 77.8% 75.6% PowerPoint 82.1% 93.4% 96.4% In our experiments, we conducted evaluations on title extraction in terms of precision, recall, and F-measure.",
                "The evaluation measures are defined as follows: Precision: P = A / ( A + B ) Recall: R = A / ( A + C ) F-measure: F1 = 2PR / ( P + R ) Here, A, B, C, and D are numbers of documents as those defined in Table 2.",
                "Table 2.",
                "Contingence table with regard to title extraction Is title Is not title Extracted A B Not extracted C D 6.2 Baselines We test the accuracies of the two baselines described in section 4.2.",
                "They are denoted as largest font size and first line respectively. 6.3 Accuracy of Titles in File Properties We investigate how many titles in the file properties of the documents are reliable.",
                "We view the titles annotated by humans as true titles and test how many titles in the file properties can approximately match with the true titles.",
                "We use Edit Distance to conduct the approximate match. (Approximate match is only used in this evaluation).",
                "This is because sometimes human annotated titles can be slightly different from the titles in file properties on the surface, e.g., contain extra spaces).",
                "Given string A and string B: if ( (D == 0) or ( D / ( La + Lb ) < θ ) ) then string A = string B D: Edit Distance between string A and string B La: length of string A Lb: length of string B θ: 0.1 ∑ × ++− + = t t n N wtf avwdl wdl bbk kwtf FBM )log( ))1(( )1( 25 1 1 150 Table 3.",
                "Accuracies of titles in file properties File Type Domain Precision Recall F1 MS 0.299 0.311 0.305 DotCom 0.210 0.214 0.212Word DotGov 0.182 0.177 0.180 MS 0.229 0.245 0.237 DotCom 0.185 0.186 0.186PowerPoint DotGov 0.180 0.182 0.181 6.4 Comparison with Baselines We conducted title extraction from the first data set (Word and PowerPoint in MS).",
                "As the model, we used Perceptron.",
                "We conduct 4-fold cross validation.",
                "Thus, all the results reported here are those averaged over 4 trials.",
                "Tables 4 and 5 show the results.",
                "We see that Perceptron significantly outperforms the baselines.",
                "In the evaluation, we use exact matching between the true titles annotated by humans and the extracted titles.",
                "Table 4.",
                "Accuracies of title extraction with Word Precision Recall F1 Model Perceptron 0.810 0.837 0.823 Largest font size 0.700 0.758 0.727 Baselines First line 0.707 0.767 0.736 Table 5.",
                "Accuracies of title extraction with PowerPoint Precision Recall F1 Model Perceptron 0.875 0. 895 0.885 Largest font size 0.844 0.887 0.865 Baselines First line 0.639 0.671 0.655 We see that the machine learning approach can achieve good performance in title extraction.",
                "For Word documents both precision and recall of the approach are 8 percent higher than those of the baselines.",
                "For PowerPoint both precision and recall of the approach are 2 percent higher than those of the baselines.",
                "We conduct significance tests.",
                "The results are shown in Table 6.",
                "Here, Largest denotes the baseline of using the largest font size, First denotes the baseline of using the first line.",
                "The results indicate that the improvements of machine learning over baselines are statistically significant (in the sense p-value < 0.05) Table 6.",
                "Sign test results Documents Type Sign test between p-value Perceptron vs. Largest 3.59e-26 Word Perceptron vs. First 7.12e-10 Perceptron vs. Largest 0.010 PowerPoint Perceptron vs. First 5.13e-40 We see, from the results, that the two baselines can work well for title extraction, suggesting that font size and position information are most useful features for title extraction.",
                "However, it is also obvious that using only these two features is not enough.",
                "There are cases in which all the lines have the same font size (i.e., the largest font size), or cases in which the lines with the largest font size only contain general descriptions like Confidential, White paper, etc.",
                "For those cases, the largest font size method cannot work well.",
                "For similar reasons, the first line method alone cannot work well, either.",
                "With the combination of different features (evidence in title judgment), Perceptron can outperform Largest and First.",
                "We investigate the performance of solely using linguistic features.",
                "We found that it does not work well.",
                "It seems that the format features play important roles and the linguistic features are supplements..",
                "Figure 8.",
                "An example Word document.",
                "Figure 9.",
                "An example PowerPoint document.",
                "We conducted an error analysis on the results of Perceptron.",
                "We found that the errors fell into three categories. (1) About one third of the errors were related to hard cases.",
                "In these documents, the layouts of the first pages were difficult to understand, even for humans.",
                "Figure 8 and 9 shows examples. (2) Nearly one fourth of the errors were from the documents which do not have true titles but only contain bullets.",
                "Since we conduct extraction from the top regions, it is difficult to get rid of these errors with the current approach. (3).",
                "Confusions between main titles and subtitles were another type of error.",
                "Since we only labeled the main titles as titles, the extractions of both titles were considered incorrect.",
                "This type of error does little harm to document processing like search, however. 6.5 Comparison between Models To compare the performance of different machine learning models, we conducted another experiment.",
                "Again, we perform 4-fold cross 151 validation on the first data set (MS).",
                "Table 7, 8 shows the results of all the four models.",
                "It turns out that Perceptron and PMM perform the best, followed by MEMM, and ME performs the worst.",
                "In general, the Markovian models perform better than or as well as their classifier counterparts.",
                "This seems to be because the Markovian models are trained globally, while the classifiers are trained locally.",
                "The Perceptron based models perform better than the ME based counterparts.",
                "This seems to be because the Perceptron based models are created to make better classifications, while ME models are constructed for better prediction.",
                "Table 7.",
                "Comparison between different learning models for title extraction with Word Model Precision Recall F1 Perceptron 0.810 0.837 0.823 MEMM 0.797 0.824 0.810 PMM 0.827 0.823 0.825 ME 0.801 0.621 0.699 Table 8.",
                "Comparison between different learning models for title extraction with PowerPoint Model Precision Recall F1 Perceptron 0.875 0. 895 0. 885 MEMM 0.841 0.861 0.851 PMM 0.873 0.896 0.885 ME 0.753 0.766 0.759 6.6 Domain Adaptation We apply the model trained with the first data set (MS) to the second data set (DotCom and DotGov).",
                "Tables 9-12 show the results.",
                "Table 9.",
                "Accuracies of title extraction with Word in DotGov Precision Recall F1 Model Perceptron 0.716 0.759 0.737 Largest font size 0.549 0.619 0.582Baselines First line 0.462 0.521 0.490 Table 10.",
                "Accuracies of title extraction with PowerPoint in DotGov Precision Recall F1 Model Perceptron 0.900 0.906 0.903 Largest font size 0.871 0.888 0.879Baselines First line 0.554 0.564 0.559 Table 11.",
                "Accuracies of title extraction with Word in DotCom Precisio n Recall F1 Model Perceptron 0.832 0.880 0.855 Largest font size 0.676 0.753 0.712Baselines First line 0.577 0.643 0.608 Table 12.",
                "Performance of PowerPoint document title extraction in DotCom Precisio n Recall F1 Model Perceptron 0.910 0.903 0.907 Largest font size 0.864 0.886 0.875Baselines First line 0.570 0.585 0.577 From the results, we see that the models can be adapted to different domains well.",
                "There is almost no drop in accuracy.",
                "The results indicate that the patterns of title formats exist across different domains, and it is possible to construct a domain independent model by mainly using formatting information. 6.7 Language Adaptation We apply the model trained with the data in English (MS) to the data set in Chinese.",
                "Tables 13-14 show the results.",
                "Table 13.",
                "Accuracies of title extraction with Word in Chinese Precision Recall F1 Model Perceptron 0.817 0.805 0.811 Largest font size 0.722 0.755 0.738Baselines First line 0.743 0.777 0.760 Table 14.",
                "Accuracies of title extraction with PowerPoint in Chinese Precision Recall F1 Model Perceptron 0.766 0.812 0.789 Largest font size 0.753 0.813 0.782Baselines First line 0.627 0.676 0.650 We see that the models can be adapted to a different language.",
                "There are only small drops in accuracy.",
                "Obviously, the linguistic features do not work for Chinese, but the effect of not using them is negligible.",
                "The results indicate that the patterns of title formats exist across different languages.",
                "From the domain adaptation and language adaptation results, we conclude that the use of formatting information is the key to a successful extraction from general documents. 6.8 Search with Extracted Titles We performed experiments on using title extraction for document retrieval.",
                "As a baseline, we employed BM25 without using extracted titles.",
                "The ranking mechanism was as described in Section 5.",
                "The weights were heuristically set.",
                "We did not conduct optimization on the weights.",
                "The evaluation was conducted on a corpus of 1.3 M documents crawled from the intranet of Microsoft using 100 evaluation queries obtained from this intranets search engine query logs. 50 queries were from the most popular set, while 50 queries other were chosen randomly.",
                "Users were asked to provide judgments of the degree of document relevance from a scale of 1to 5 (1 meaning detrimental, 2 - bad, 3 - fair, 4 - good and 5 - excellent). 152 Figure 10 shows the results.",
                "In the chart two sets of precision results were obtained by either considering good or excellent documents as relevant (left 3 bars with relevance threshold 0.5), or by considering only excellent documents as relevant (right 3 bars with relevance threshold 1.0) 0 0.05 0.1 0.15 0.2 0.25 0.3 0.35 0.4 0.45 P@10 P@5 Reciprocal P@10 P@5 Reciprocal 0.5 1 BM25 Anchor, Title, Body BM25 Anchor, Title, Body, ExtractedTitle Name All RelevanceThreshold Data Description Figure 10.",
                "Search ranking results.",
                "Figure 10 shows different document retrieval results with different ranking functions in terms of precision @10, precision @5 and reciprocal rank: • Blue bar - BM25 including the fields body, title (file property), and anchor text. • Purple bar - BM25 including the fields body, title (file property), anchor text, and extracted title.",
                "With the additional field of extracted title included in BM25 the precision @10 increased from 0.132 to 0.145, or by ~10%.",
                "Thus, it is safe to say that the use of extracted title can indeed improve the precision of document retrieval. 7.",
                "CONCLUSION In this paper, we have investigated the problem of automatically extracting titles from general documents.",
                "We have tried using a machine learning approach to address the problem.",
                "Previous work showed that the machine learning approach can work well for metadata extraction from research papers.",
                "In this paper, we showed that the approach can work for extraction from general documents as well.",
                "Our experimental results indicated that the machine learning approach can work significantly better than the baselines in title extraction from Office documents.",
                "Previous work on metadata extraction mainly used linguistic features in documents, while we mainly used formatting information.",
                "It appeared that using formatting information is a key for successfully conducting title extraction from general documents.",
                "We tried different machine learning models including Perceptron, Maximum Entropy, Maximum Entropy Markov Model, and Voted Perceptron.",
                "We found that the performance of the Perceptorn models was the best.",
                "We applied models constructed in one domain to another domain and applied models trained in one language to another language.",
                "We found that the accuracies did not drop substantially across different domains and across different languages, indicating that the models were generic.",
                "We also attempted to use the extracted titles in document retrieval.",
                "We observed a significant improvement in document ranking performance for search when using extracted title information.",
                "All the above investigations were not conducted in previous work, and through our investigations we verified the generality and the significance of the title extraction approach. 8.",
                "ACKNOWLEDGEMENTS We thank Chunyu Wei and Bojuan Zhao for their work on data annotation.",
                "We acknowledge Jinzhu Li for his assistance in conducting the experiments.",
                "We thank Ming Zhou, John Chen, Jun Xu, and the anonymous reviewers of JCDL05 for their valuable comments on this paper. 9.",
                "REFERENCES [1] Berger, A. L., Della Pietra, S. A., and Della Pietra, V. J.",
                "A maximum entropy approach to natural language processing.",
                "Computational Linguistics, 22:39-71, 1996. [2] Collins, M. Discriminative training methods for hidden markov models: theory and experiments with perceptron algorithms.",
                "In Proceedings of Conference on Empirical Methods in Natural Language Processing, 1-8, 2002. [3] Cortes, C. and Vapnik, V. Support-vector networks.",
                "Machine Learning, 20:273-297, 1995. [4] Chieu, H. L. and Ng, H. T. A maximum entropy approach to information extraction from semi-structured and free text.",
                "In Proceedings of the Eighteenth National Conference on Artificial Intelligence, 768-791, 2002. [5] Evans, D. K., Klavans, J. L., and McKeown, K. R. Columbia newsblaster: multilingual news summarization on the Web.",
                "In Proceedings of Human Language Technology conference / North American chapter of the Association for Computational Linguistics annual meeting, 1-4, 2004. [6] Ghahramani, Z. and Jordan, M. I. Factorial hidden markov models.",
                "Machine Learning, 29:245-273, 1997. [7] Gheel, J. and Anderson, T. Data and metadata for finding and reminding, In Proceedings of the 1999 International Conference on Information Visualization, 446-451,1999. [8] Giles, C. L., Petinot, Y., Teregowda P. B., Han, H., Lawrence, S., Rangaswamy, A., and Pal, N. eBizSearch: a niche search engine for e-Business.",
                "In Proceedings of the 26th Annual International ACM SIGIR Conference on Research and Development in Information Retrieval, 413414, 2003. [9] Giuffrida, G., Shek, E. C., and Yang, J. Knowledge-based metadata extraction from PostScript files.",
                "In Proceedings of the Fifth ACM Conference on Digital Libraries, 77-84, 2000. [10] Han, H., Giles, C. L., Manavoglu, E., Zha, H., Zhang, Z., and Fox, E. A.",
                "Automatic document metadata extraction using support vector machines.",
                "In Proceedings of the Third ACM/IEEE-CS Joint Conference on Digital Libraries, 37-48, 2003. [11] Kobayashi, M., and Takeda, K. Information retrieval on the Web.",
                "ACM Computing Surveys, 32:144-173, 2000. [12] Lafferty, J., McCallum, A., and Pereira, F. Conditional random fields: probabilistic models for segmenting and 153 labeling sequence data.",
                "In Proceedings of the Eighteenth International Conference on Machine Learning, 282-289, 2001. [13] Li, Y., Zaragoza, H., Herbrich, R., Shawe-Taylor J., and Kandola, J. S. The perceptron algorithm with uneven margins.",
                "In Proceedings of the Nineteenth International Conference on Machine Learning, 379-386, 2002. [14] Liddy, E. D., Sutton, S., Allen, E., Harwell, S., Corieri, S., Yilmazel, O., Ozgencil, N. E., Diekema, A., McCracken, N., and Silverstein, J.",
                "Automatic Metadata generation & evaluation.",
                "In Proceedings of the 25th Annual International ACM SIGIR Conference on Research and Development in Information Retrieval, 401-402, 2002. [15] Littlefield, A.",
                "Effective enterprise information retrieval across new content formats.",
                "In Proceedings of the Seventh Search Engine Conference, http://www.infonortics.com/searchengines/sh02/02prog.html, 2002. [16] Mao, S., Kim, J. W., and Thoma, G. R. A dynamic feature generation system for automated metadata extraction in preservation of digital materials.",
                "In Proceedings of the First International Workshop on Document Image Analysis for Libraries, 225-232, 2004. [17] McCallum, A., Freitag, D., and Pereira, F. Maximum entropy markov models for information extraction and segmentation.",
                "In Proceedings of the Seventeenth International Conference on Machine Learning, 591-598, 2000. [18] Murphy, L. D. Digital document metadata in organizations: roles, analytical approaches, and future research directions.",
                "In Proceedings of the Thirty-First Annual Hawaii International Conference on System Sciences, 267-276, 1998. [19] Pinto, D., McCallum, A., Wei, X., and Croft, W. B.",
                "Table extraction using conditional random fields.",
                "In Proceedings of the 26th Annual International ACM SIGIR Conference on Research and Development in Information Retrieval, 235242, 2003. [20] Ratnaparkhi, A. Unsupervised statistical models for prepositional phrase attachment.",
                "In Proceedings of the Seventeenth International Conference on Computational Linguistics. 1079-1085, 1998. [21] Robertson, S., Zaragoza, H., and Taylor, M. Simple BM25 extension to multiple weighted fields, In Proceedings of ACM Thirteenth Conference on Information and Knowledge Management, 42-49, 2004. [22] Yi, J. and Sundaresan, N. Metadata based Web mining for relevance, In Proceedings of the 2000 International Symposium on Database Engineering & Applications, 113121, 2000. [23] Yilmazel, O., Finneran, C. M., and Liddy, E. D. MetaExtract: An NLP system to automatically assign metadata.",
                "In Proceedings of the 2004 Joint ACM/IEEE Conference on Digital Libraries, 241-242, 2004. [24] Zhang, J. and Dimitroff, A. Internet search engines response to metadata Dublin Core implementation.",
                "Journal of Information Science, 30:310-320, 2004. [25] Zhang, L., Pan, Y., and Zhang, T. Recognising and using named entities: focused named entity recognition using machine learning.",
                "In Proceedings of the 27th Annual International ACM SIGIR Conference on Research and Development in Information Retrieval, 281-288, 2004. [26] http://dublincore.org/groups/corporate/Seattle/ 154"
            ],
            "original_annotated_samples": [],
            "translated_annotated_samples": [
                "En este documento, también investigamos los siguientes tres problemas, que no parecían haber sido examinados anteriormente.(1) Comparación entre modelos: entre los modelos anteriores, qué modelo funciona mejor para la extracción de título;(2) \"Generalidad del modelo\": si es posible entrenar un modelo en un dominio y aplicarlo a otro dominio, y si es posible entrenar un modelo en un idioma y aplicarlo a otro idioma;(3) Utilidad de los títulos extraídos: si los títulos extraídos pueden mejorar el procesamiento de documentos, como la búsqueda."
            ],
            "translated_text": "",
            "candidates": [
                "generalidad de modelo",
                "Generalidad del modelo"
            ],
            "error": []
        },
        "model generality": {
            "translated_key": "generalidad de modelo",
            "is_in_text": false,
            "original_annotated_sentences": [
                "Automatic Extraction of Titles from General Documents using Machine Learning Yunhua Hu1 Computer Science Department Xian Jiaotong University No 28, Xianning West Road Xian, China, 710049 yunhuahu@mail.xjtu.edu.cn Hang Li, Yunbo Cao Microsoft Research Asia 5F Sigma Center, No.",
                "49 Zhichun Road, Haidian, Beijing, China, 100080 {hangli,yucao}@microsoft.com Qinghua Zheng Computer Science Department Xian Jiaotong University No 28, Xianning West Road Xian, China, 710049 qhzheng@mail.xjtu.edu.cn Dmitriy Meyerzon Microsoft Corporation One Microsoft Way Redmond, WA, USA, 98052 dmitriym@microsoft.com ABSTRACT In this paper, we propose a machine learning approach to title extraction from general documents.",
                "By general documents, we mean documents that can belong to any one of a number of specific genres, including presentations, book chapters, technical papers, brochures, reports, and letters.",
                "Previously, methods have been proposed mainly for title extraction from research papers.",
                "It has not been clear whether it could be possible to conduct automatic title extraction from general documents.",
                "As a case study, we consider extraction from Office including Word and PowerPoint.",
                "In our approach, we annotate titles in sample documents (for Word and PowerPoint respectively) and take them as training data, train machine learning models, and perform title extraction using the trained models.",
                "Our method is unique in that we mainly utilize formatting information such as font size as features in the models.",
                "It turns out that the use of formatting information can lead to quite accurate extraction from general documents.",
                "Precision and recall for title extraction from Word is 0.810 and 0.837 respectively, and precision and recall for title extraction from PowerPoint is 0.875 and 0.895 respectively in an experiment on intranet data.",
                "Other important new findings in this work include that we can train models in one domain and apply them to another domain, and more surprisingly we can even train models in one language and apply them to another language.",
                "Moreover, we can significantly improve search ranking results in document retrieval by using the extracted titles.",
                "Categories and Subject Descriptors H.3.3 [Information Storage and Retrieval]: Information Search and Retrieval - Search Process; H.4.1 [Information Systems Applications]: Office Automation - Word processing; D.2.8 [Software Engineering]: Metrics - complexity measures, performance measures General Terms Algorithms, Experimentation, Performance. 1.",
                "INTRODUCTION Metadata of documents is useful for many kinds of document processing such as search, browsing, and filtering.",
                "Ideally, metadata is defined by the authors of documents and is then used by various systems.",
                "However, people seldom define document metadata by themselves, even when they have convenient metadata definition tools [26].",
                "Thus, how to automatically extract metadata from the bodies of documents turns out to be an important research issue.",
                "Methods for performing the task have been proposed.",
                "However, the focus was mainly on extraction from research papers.",
                "For instance, Han et al. [10] proposed a machine learning based method to conduct extraction from research papers.",
                "They formalized the problem as that of classification and employed Support Vector Machines as the classifier.",
                "They mainly used linguistic features in the model.1 In this paper, we consider metadata extraction from general documents.",
                "By general documents, we mean documents that may belong to any one of a number of specific genres.",
                "General documents are more widely available in digital libraries, intranets and the internet, and thus investigation on extraction from them is sorely needed.",
                "Research papers usually have well-formed styles and noticeable characteristics.",
                "In contrast, the styles of general documents can vary greatly.",
                "It has not been clarified whether a machine learning based approach can work well for this task.",
                "There are many types of metadata: title, author, date of creation, etc.",
                "As a case study, we consider title extraction in this paper.",
                "General documents can be in many different file formats: Microsoft Office, PDF (PS), etc.",
                "As a case study, we consider extraction from Office including Word and PowerPoint.",
                "We take a machine learning approach.",
                "We annotate titles in sample documents (for Word and PowerPoint respectively) and take them as training data to train several types of models, and perform title extraction using any one type of the trained models.",
                "In the models, we mainly utilize formatting information such as font size as features.",
                "We employ the following models: Maximum Entropy Model, Perceptron with Uneven Margins, Maximum Entropy Markov Model, and Voted Perceptron.",
                "In this paper, we also investigate the following three problems, which did not seem to have been examined previously. (1) Comparison between models: among the models above, which model performs best for title extraction; (2) Generality of model: whether it is possible to train a model on one domain and apply it to another domain, and whether it is possible to train a model in one language and apply it to another language; (3) Usefulness of extracted titles: whether extracted titles can improve document processing such as search.",
                "Experimental results indicate that our approach works well for title extraction from general documents.",
                "Our method can significantly outperform the baselines: one that always uses the first lines as titles and the other that always uses the lines in the largest font sizes as titles.",
                "Precision and recall for title extraction from Word are 0.810 and 0.837 respectively, and precision and recall for title extraction from PowerPoint are 0.875 and 0.895 respectively.",
                "It turns out that the use of format features is the key to successful title extraction. (1) We have observed that Perceptron based models perform better in terms of extraction accuracies. (2) We have empirically verified that the models trained with our approach are generic in the sense that they can be trained on one domain and applied to another, and they can be trained in one language and applied to another. (3) We have found that using the extracted titles we can significantly improve precision of document retrieval (by 10%).",
                "We conclude that we can indeed conduct reliable title extraction from general documents and use the extracted results to improve real applications.",
                "The rest of the paper is organized as follows.",
                "In section 2, we introduce related work, and in section 3, we explain the motivation and problem setting of our work.",
                "In section 4, we describe our method of title extraction, and in section 5, we describe our method of document retrieval using extracted titles.",
                "Section 6 gives our experimental results.",
                "We make concluding remarks in section 7. 2.",
                "RELATED WORK 2.1 Document Metadata Extraction Methods have been proposed for performing automatic metadata extraction from documents; however, the main focus was on extraction from research papers.",
                "The proposed methods fall into two categories: the rule based approach and the machine learning based approach.",
                "Giuffrida et al. [9], for instance, developed a rule-based system for automatically extracting metadata from research papers in Postscript.",
                "They used rules like titles are usually located on the upper portions of the first pages and they are usually in the largest font sizes.",
                "Liddy et al. [14] and Yilmazel el al. [23] performed metadata extraction from educational materials using rule-based natural language processing technologies.",
                "Mao et al. [16] also conducted automatic metadata extraction from research papers using rules on formatting information.",
                "The rule-based approach can achieve high performance.",
                "However, it also has disadvantages.",
                "It is less adaptive and robust when compared with the machine learning approach.",
                "Han et al. [10], for instance, conducted metadata extraction with the machine learning approach.",
                "They viewed the problem as that of classifying the lines in a document into the categories of metadata and proposed using Support Vector Machines as the classifier.",
                "They mainly used linguistic information as features.",
                "They reported high extraction accuracy from research papers in terms of precision and recall. 2.2 Information Extraction Metadata extraction can be viewed as an application of information extraction, in which given a sequence of instances, we identify a subsequence that represents information in which we are interested.",
                "Hidden Markov Model [6], Maximum Entropy Model [1, 4], Maximum Entropy Markov Model [17], Support Vector Machines [3], Conditional Random Field [12], and Voted Perceptron [2] are widely used information extraction models.",
                "Information extraction has been applied, for instance, to part-ofspeech tagging [20], named entity recognition [25] and table extraction [19]. 2.3 Search Using Title Information Title information is useful for document retrieval.",
                "In the system Citeseer, for instance, Giles et al. managed to extract titles from research papers and make use of the extracted titles in metadata search of papers [8].",
                "In web search, the title fields (i.e., file properties) and anchor texts of web pages (HTML documents) can be viewed as titles of the pages [5].",
                "Many search engines seem to utilize them for web page retrieval [7, 11, 18, 22].",
                "Zhang et al., found that web pages with well-defined metadata are more easily retrieved than those without well-defined metadata [24].",
                "To the best of our knowledge, no research has been conducted on using extracted titles from general documents (e.g., Office documents) for search of the documents. 146 3.",
                "MOTIVATION AND PROBLEM SETTING We consider the issue of automatically extracting titles from general documents.",
                "By general documents, we mean documents that belong to one of any number of specific genres.",
                "The documents can be presentations, books, book chapters, technical papers, brochures, reports, memos, specifications, letters, announcements, or resumes.",
                "General documents are more widely available in digital libraries, intranets, and internet, and thus investigation on title extraction from them is sorely needed.",
                "Figure 1 shows an estimate on distributions of file formats on intranet and internet [15].",
                "Office and PDF are the main file formats on the intranet.",
                "Even on the internet, the documents in the formats are still not negligible, given its extremely large size.",
                "In this paper, without loss of generality, we take Office documents as an example.",
                "Figure 1.",
                "Distributions of file formats in internet and intranet.",
                "For Office documents, users can define titles as file properties using a feature provided by Office.",
                "We found in an experiment, however, that users seldom use the feature and thus titles in file properties are usually very inaccurate.",
                "That is to say, titles in file properties are usually inconsistent with the true titles in the file bodies that are created by the authors and are visible to readers.",
                "We collected 6,000 Word and 6,000 PowerPoint documents from an intranet and the internet and examined how many titles in the file properties are correct.",
                "We found that surprisingly the accuracy was only 0.265 (cf., Section 6.3 for details).",
                "A number of reasons can be considered.",
                "For example, if one creates a new file by copying an old file, then the file property of the new file will also be copied from the old file.",
                "In another experiment, we found that Google uses the titles in file properties of Office documents in search and browsing, but the titles are not very accurate.",
                "We created 50 queries to search Word and PowerPoint documents and examined the top 15 results of each query returned by Google.",
                "We found that nearly all the titles presented in the search results were from the file properties of the documents.",
                "However, only 0.272 of them were correct.",
                "Actually, true titles usually exist at the beginnings of the bodies of documents.",
                "If we can accurately extract the titles from the bodies of documents, then we can exploit reliable title information in document processing.",
                "This is exactly the problem we address in this paper.",
                "More specifically, given a Word document, we are to extract the title from the top region of the first page.",
                "Given a PowerPoint document, we are to extract the title from the first slide.",
                "A title sometimes consists of a main title and one or two subtitles.",
                "We only consider extraction of the main title.",
                "As baselines for title extraction, we use that of always using the first lines as titles and that of always using the lines with largest font sizes as titles.",
                "Figure 2.",
                "Title extraction from Word document.",
                "Figure 3.",
                "Title extraction from PowerPoint document.",
                "Next, we define a specification for human judgments in title data annotation.",
                "The annotated data will be used in training and testing of the title extraction methods.",
                "Summary of the specification: The title of a document should be identified on the basis of common sense, if there is no difficulty in the identification.",
                "However, there are many cases in which the identification is not easy.",
                "There are some rules defined in the specification that guide identification for such cases.",
                "The rules include a title is usually in consecutive lines in the same format, a document can have no title, titles in images are not considered, a title should not contain words like draft, 147 whitepaper, etc, if it is difficult to determine which is the title, select the one in the largest font size, and if it is still difficult to determine which is the title, select the first candidate. (The specification covers all the cases we have encountered in data annotation.)",
                "Figures 2 and 3 show examples of Office documents from which we conduct title extraction.",
                "In Figure 2, Differences in Win32 API Implementations among Windows Operating Systems is the title of the Word document.",
                "Microsoft Windows on the top of this page is a picture and thus is ignored.",
                "In Figure 3, Building Competitive Advantages through an Agile Infrastructure is the title of the PowerPoint document.",
                "We have developed a tool for annotation of titles by human annotators.",
                "Figure 4 shows a snapshot of the tool.",
                "Figure 4.",
                "Title annotation tool. 4.",
                "TITLE EXTRACTION METHOD 4.1 Outline Title extraction based on machine learning consists of training and extraction.",
                "The same pre-processing step occurs before training and extraction.",
                "During pre-processing, from the top region of the first page of a Word document or the first slide of a PowerPoint document a number of units for processing are extracted.",
                "If a line (lines are separated by return symbols) only has a single format, then the line will become a unit.",
                "If a line has several parts and each of them has its own format, then each part will become a unit.",
                "Each unit will be treated as an instance in learning.",
                "A unit contains not only content information (linguistic information) but also formatting information.",
                "The input to pre-processing is a document and the output of pre-processing is a sequence of units (instances).",
                "Figure 5 shows the units obtained from the document in Figure 2.",
                "Figure 5.",
                "Example of units.",
                "In learning, the input is sequences of units where each sequence corresponds to a document.",
                "We take labeled units (labeled as title_begin, title_end, or other) in the sequences as training data and construct models for identifying whether a unit is title_begin title_end, or other.",
                "We employ four types of models: Perceptron, Maximum Entropy (ME), Perceptron Markov Model (PMM), and Maximum Entropy Markov Model (MEMM).",
                "In extraction, the input is a sequence of units from one document.",
                "We employ one type of model to identify whether a unit is title_begin, title_end, or other.",
                "We then extract units from the unit labeled with title_begin to the unit labeled with title_end.",
                "The result is the extracted title of the document.",
                "The unique characteristic of our approach is that we mainly utilize formatting information for title extraction.",
                "Our assumption is that although general documents vary in styles, their formats have certain patterns and we can learn and utilize the patterns for title extraction.",
                "This is in contrast to the work by Han et al., in which only linguistic features are used for extraction from research papers. 4.2 Models The four models actually can be considered in the same metadata extraction framework.",
                "That is why we apply them together to our current problem.",
                "Each input is a sequence of instances kxxx L21 together with a sequence of labels kyyy L21 . ix and iy represents an instance and its label, respectively ( ki ,,2,1 L= ).",
                "Recall that an instance here represents a unit.",
                "A label represents title_begin, title_end, or other.",
                "Here, k is the number of units in a document.",
                "In learning, we train a model which can be generally denoted as a conditional probability distribution )|( 11 kk XXYYP LL where iX and iY denote random variables taking instance ix and label iy as values, respectively ( ki ,,2,1 L= ).",
                "Learning Tool Extraction Tool 21121 2222122221 1121111211 nknnknn kk kk yyyxxx yyyxxx yyyxxx LL LL LL LL → → → )|(maxarg 11 mkmmkm xxyyP LL )|( 11 kk XXYYP LL Conditional Distribution mkmm xxx L21 Figure 6.",
                "Metadata extraction model.",
                "We can make assumptions about the general model in order to make it simple enough for training. 148 For example, we can assume that kYY ,,1 L are independent of each other given kXX ,,1 L .",
                "Thus, we have )|()|( )|( 11 11 kk kk XYPXYP XXYYP L LL = In this way, we decompose the model into a number of classifiers.",
                "We train the classifiers locally using the labeled data.",
                "As the classifier, we employ the Perceptron or Maximum Entropy model.",
                "We can also assume that the first order Markov property holds for kYY ,,1 L given kXX ,,1 L .",
                "Thus, we have )|()|( )|( 111 11 kkk kk XYYPXYP XXYYP −= L LL Again, we obtain a number of classifiers.",
                "However, the classifiers are conditioned on the previous label.",
                "When we employ the Percepton or Maximum Entropy model as a classifier, the models become a Percepton Markov Model or Maximum Entropy Markov Model, respectively.",
                "That is to say, the two models are more precise.",
                "In extraction, given a new sequence of instances, we resort to one of the constructed models to assign a sequence of labels to the sequence of instances, i.e., perform extraction.",
                "For Perceptron and ME, we assign labels locally and combine the results globally later using heuristics.",
                "Specifically, we first identify the most likely title_begin.",
                "Then we find the most likely title_end within three units after the title_begin.",
                "Finally, we extract as a title the units between the title_begin and the title_end.",
                "For PMM and MEMM, we employ the Viterbi algorithm to find the globally optimal label sequence.",
                "In this paper, for Perceptron, we actually employ an improved variant of it, called Perceptron with Uneven Margin [13].",
                "This version of Perceptron can work well especially when the number of positive instances and the number of negative instances differ greatly, which is exactly the case in our problem.",
                "We also employ an improved version of Perceptron Markov Model in which the Perceptron model is the so-called Voted Perceptron [2].",
                "In addition, in training, the parameters of the model are updated globally rather than locally. 4.3 Features There are two types of features: format features and linguistic features.",
                "We mainly use the former.",
                "The features are used for both the title-begin and the title-end classifiers. 4.3.1 Format Features Font Size: There are four binary features that represent the normalized font size of the unit (recall that a unit has only one type of font).",
                "If the font size of the unit is the largest in the document, then the first feature will be 1, otherwise 0.",
                "If the font size is the smallest in the document, then the fourth feature will be 1, otherwise 0.",
                "If the font size is above the average font size and not the largest in the document, then the second feature will be 1, otherwise 0.",
                "If the font size is below the average font size and not the smallest, the third feature will be 1, otherwise 0.",
                "It is necessary to conduct normalization on font sizes.",
                "For example, in one document the largest font size might be 12pt, while in another the smallest one might be 18pt.",
                "Boldface: This binary feature represents whether or not the current unit is in boldface.",
                "Alignment: There are four binary features that respectively represent the location of the current unit: left, center, right, and unknown alignment.",
                "The following format features with respect to context play an important role in title extraction.",
                "Empty Neighboring Unit: There are two binary features that represent, respectively, whether or not the previous unit and the current unit are blank lines.",
                "Font Size Change: There are two binary features that represent, respectively, whether or not the font size of the previous unit and the font size of the next unit differ from that of the current unit.",
                "Alignment Change: There are two binary features that represent, respectively, whether or not the alignment of the previous unit and the alignment of the next unit differ from that of the current one.",
                "Same Paragraph: There are two binary features that represent, respectively, whether or not the previous unit and the next unit are in the same paragraph as the current unit. 4.3.2 Linguistic Features The linguistic features are based on key words.",
                "Positive Word: This binary feature represents whether or not the current unit begins with one of the positive words.",
                "The positive words include title:, subject:, subject line: For example, in some documents the lines of titles and authors have the same formats.",
                "However, if lines begin with one of the positive words, then it is likely that they are title lines.",
                "Negative Word: This binary feature represents whether or not the current unit begins with one of the negative words.",
                "The negative words include To, By, created by, updated by, etc.",
                "There are more negative words than positive words.",
                "The above linguistic features are language dependent.",
                "Word Count: A title should not be too long.",
                "We heuristically create four intervals: [1, 2], [3, 6], [7, 9] and [9, ∞) and define one feature for each interval.",
                "If the number of words in a title falls into an interval, then the corresponding feature will be 1; otherwise 0.",
                "Ending Character: This feature represents whether the unit ends with :, -, or other special characters.",
                "A title usually does not end with such a character. 5.",
                "DOCUMENT RETRIEVAL METHOD We describe our method of document retrieval using extracted titles.",
                "Typically, in information retrieval a document is split into a number of fields including body, title, and anchor text.",
                "A ranking function in search can use different weights for different fields of 149 the document.",
                "Also, titles are typically assigned high weights, indicating that they are important for document retrieval.",
                "As explained previously, our experiment has shown that a significant number of documents actually have incorrect titles in the file properties, and thus in addition of using them we use the extracted titles as one more field of the document.",
                "By doing this, we attempt to improve the overall precision.",
                "In this paper, we employ a modification of BM25 that allows field weighting [21].",
                "As fields, we make use of body, title, extracted title and anchor.",
                "First, for each term in the query we count the term frequency in each field of the document; each field frequency is then weighted according to the corresponding weight parameter: ∑= f tfft tfwwtf Similarly, we compute the document length as a weighted sum of lengths of each field.",
                "Average document length in the corpus becomes the average of all weighted document lengths. ∑= f ff dlwwdl In our experiments we used 75.0,8.11 == bk .",
                "Weight for content was 1.0, title was 10.0, anchor was 10.0, and extracted title was 5.0. 6.",
                "EXPERIMENTAL RESULTS 6.1 Data Sets and Evaluation Measures We used two data sets in our experiments.",
                "First, we downloaded and randomly selected 5,000 Word documents and 5,000 PowerPoint documents from an intranet of Microsoft.",
                "We call it MS hereafter.",
                "Second, we downloaded and randomly selected 500 Word and 500 PowerPoint documents from the DotGov and DotCom domains on the internet, respectively.",
                "Figure 7 shows the distributions of the genres of the documents.",
                "We see that the documents are indeed general documents as we define them.",
                "Figure 7.",
                "Distributions of document genres.",
                "Third, a data set in Chinese was also downloaded from the internet.",
                "It includes 500 Word documents and 500 PowerPoint documents in Chinese.",
                "We manually labeled the titles of all the documents, on the basis of our specification.",
                "Not all the documents in the two data sets have titles.",
                "Table 1 shows the percentages of the documents having titles.",
                "We see that DotCom and DotGov have more PowerPoint documents with titles than MS.",
                "This might be because PowerPoint documents published on the internet are more formal than those on the intranet.",
                "Table 1.",
                "The portion of documents with titles Domain Type MS DotCom DotGov Word 75.7% 77.8% 75.6% PowerPoint 82.1% 93.4% 96.4% In our experiments, we conducted evaluations on title extraction in terms of precision, recall, and F-measure.",
                "The evaluation measures are defined as follows: Precision: P = A / ( A + B ) Recall: R = A / ( A + C ) F-measure: F1 = 2PR / ( P + R ) Here, A, B, C, and D are numbers of documents as those defined in Table 2.",
                "Table 2.",
                "Contingence table with regard to title extraction Is title Is not title Extracted A B Not extracted C D 6.2 Baselines We test the accuracies of the two baselines described in section 4.2.",
                "They are denoted as largest font size and first line respectively. 6.3 Accuracy of Titles in File Properties We investigate how many titles in the file properties of the documents are reliable.",
                "We view the titles annotated by humans as true titles and test how many titles in the file properties can approximately match with the true titles.",
                "We use Edit Distance to conduct the approximate match. (Approximate match is only used in this evaluation).",
                "This is because sometimes human annotated titles can be slightly different from the titles in file properties on the surface, e.g., contain extra spaces).",
                "Given string A and string B: if ( (D == 0) or ( D / ( La + Lb ) < θ ) ) then string A = string B D: Edit Distance between string A and string B La: length of string A Lb: length of string B θ: 0.1 ∑ × ++− + = t t n N wtf avwdl wdl bbk kwtf FBM )log( ))1(( )1( 25 1 1 150 Table 3.",
                "Accuracies of titles in file properties File Type Domain Precision Recall F1 MS 0.299 0.311 0.305 DotCom 0.210 0.214 0.212Word DotGov 0.182 0.177 0.180 MS 0.229 0.245 0.237 DotCom 0.185 0.186 0.186PowerPoint DotGov 0.180 0.182 0.181 6.4 Comparison with Baselines We conducted title extraction from the first data set (Word and PowerPoint in MS).",
                "As the model, we used Perceptron.",
                "We conduct 4-fold cross validation.",
                "Thus, all the results reported here are those averaged over 4 trials.",
                "Tables 4 and 5 show the results.",
                "We see that Perceptron significantly outperforms the baselines.",
                "In the evaluation, we use exact matching between the true titles annotated by humans and the extracted titles.",
                "Table 4.",
                "Accuracies of title extraction with Word Precision Recall F1 Model Perceptron 0.810 0.837 0.823 Largest font size 0.700 0.758 0.727 Baselines First line 0.707 0.767 0.736 Table 5.",
                "Accuracies of title extraction with PowerPoint Precision Recall F1 Model Perceptron 0.875 0. 895 0.885 Largest font size 0.844 0.887 0.865 Baselines First line 0.639 0.671 0.655 We see that the machine learning approach can achieve good performance in title extraction.",
                "For Word documents both precision and recall of the approach are 8 percent higher than those of the baselines.",
                "For PowerPoint both precision and recall of the approach are 2 percent higher than those of the baselines.",
                "We conduct significance tests.",
                "The results are shown in Table 6.",
                "Here, Largest denotes the baseline of using the largest font size, First denotes the baseline of using the first line.",
                "The results indicate that the improvements of machine learning over baselines are statistically significant (in the sense p-value < 0.05) Table 6.",
                "Sign test results Documents Type Sign test between p-value Perceptron vs. Largest 3.59e-26 Word Perceptron vs. First 7.12e-10 Perceptron vs. Largest 0.010 PowerPoint Perceptron vs. First 5.13e-40 We see, from the results, that the two baselines can work well for title extraction, suggesting that font size and position information are most useful features for title extraction.",
                "However, it is also obvious that using only these two features is not enough.",
                "There are cases in which all the lines have the same font size (i.e., the largest font size), or cases in which the lines with the largest font size only contain general descriptions like Confidential, White paper, etc.",
                "For those cases, the largest font size method cannot work well.",
                "For similar reasons, the first line method alone cannot work well, either.",
                "With the combination of different features (evidence in title judgment), Perceptron can outperform Largest and First.",
                "We investigate the performance of solely using linguistic features.",
                "We found that it does not work well.",
                "It seems that the format features play important roles and the linguistic features are supplements..",
                "Figure 8.",
                "An example Word document.",
                "Figure 9.",
                "An example PowerPoint document.",
                "We conducted an error analysis on the results of Perceptron.",
                "We found that the errors fell into three categories. (1) About one third of the errors were related to hard cases.",
                "In these documents, the layouts of the first pages were difficult to understand, even for humans.",
                "Figure 8 and 9 shows examples. (2) Nearly one fourth of the errors were from the documents which do not have true titles but only contain bullets.",
                "Since we conduct extraction from the top regions, it is difficult to get rid of these errors with the current approach. (3).",
                "Confusions between main titles and subtitles were another type of error.",
                "Since we only labeled the main titles as titles, the extractions of both titles were considered incorrect.",
                "This type of error does little harm to document processing like search, however. 6.5 Comparison between Models To compare the performance of different machine learning models, we conducted another experiment.",
                "Again, we perform 4-fold cross 151 validation on the first data set (MS).",
                "Table 7, 8 shows the results of all the four models.",
                "It turns out that Perceptron and PMM perform the best, followed by MEMM, and ME performs the worst.",
                "In general, the Markovian models perform better than or as well as their classifier counterparts.",
                "This seems to be because the Markovian models are trained globally, while the classifiers are trained locally.",
                "The Perceptron based models perform better than the ME based counterparts.",
                "This seems to be because the Perceptron based models are created to make better classifications, while ME models are constructed for better prediction.",
                "Table 7.",
                "Comparison between different learning models for title extraction with Word Model Precision Recall F1 Perceptron 0.810 0.837 0.823 MEMM 0.797 0.824 0.810 PMM 0.827 0.823 0.825 ME 0.801 0.621 0.699 Table 8.",
                "Comparison between different learning models for title extraction with PowerPoint Model Precision Recall F1 Perceptron 0.875 0. 895 0. 885 MEMM 0.841 0.861 0.851 PMM 0.873 0.896 0.885 ME 0.753 0.766 0.759 6.6 Domain Adaptation We apply the model trained with the first data set (MS) to the second data set (DotCom and DotGov).",
                "Tables 9-12 show the results.",
                "Table 9.",
                "Accuracies of title extraction with Word in DotGov Precision Recall F1 Model Perceptron 0.716 0.759 0.737 Largest font size 0.549 0.619 0.582Baselines First line 0.462 0.521 0.490 Table 10.",
                "Accuracies of title extraction with PowerPoint in DotGov Precision Recall F1 Model Perceptron 0.900 0.906 0.903 Largest font size 0.871 0.888 0.879Baselines First line 0.554 0.564 0.559 Table 11.",
                "Accuracies of title extraction with Word in DotCom Precisio n Recall F1 Model Perceptron 0.832 0.880 0.855 Largest font size 0.676 0.753 0.712Baselines First line 0.577 0.643 0.608 Table 12.",
                "Performance of PowerPoint document title extraction in DotCom Precisio n Recall F1 Model Perceptron 0.910 0.903 0.907 Largest font size 0.864 0.886 0.875Baselines First line 0.570 0.585 0.577 From the results, we see that the models can be adapted to different domains well.",
                "There is almost no drop in accuracy.",
                "The results indicate that the patterns of title formats exist across different domains, and it is possible to construct a domain independent model by mainly using formatting information. 6.7 Language Adaptation We apply the model trained with the data in English (MS) to the data set in Chinese.",
                "Tables 13-14 show the results.",
                "Table 13.",
                "Accuracies of title extraction with Word in Chinese Precision Recall F1 Model Perceptron 0.817 0.805 0.811 Largest font size 0.722 0.755 0.738Baselines First line 0.743 0.777 0.760 Table 14.",
                "Accuracies of title extraction with PowerPoint in Chinese Precision Recall F1 Model Perceptron 0.766 0.812 0.789 Largest font size 0.753 0.813 0.782Baselines First line 0.627 0.676 0.650 We see that the models can be adapted to a different language.",
                "There are only small drops in accuracy.",
                "Obviously, the linguistic features do not work for Chinese, but the effect of not using them is negligible.",
                "The results indicate that the patterns of title formats exist across different languages.",
                "From the domain adaptation and language adaptation results, we conclude that the use of formatting information is the key to a successful extraction from general documents. 6.8 Search with Extracted Titles We performed experiments on using title extraction for document retrieval.",
                "As a baseline, we employed BM25 without using extracted titles.",
                "The ranking mechanism was as described in Section 5.",
                "The weights were heuristically set.",
                "We did not conduct optimization on the weights.",
                "The evaluation was conducted on a corpus of 1.3 M documents crawled from the intranet of Microsoft using 100 evaluation queries obtained from this intranets search engine query logs. 50 queries were from the most popular set, while 50 queries other were chosen randomly.",
                "Users were asked to provide judgments of the degree of document relevance from a scale of 1to 5 (1 meaning detrimental, 2 - bad, 3 - fair, 4 - good and 5 - excellent). 152 Figure 10 shows the results.",
                "In the chart two sets of precision results were obtained by either considering good or excellent documents as relevant (left 3 bars with relevance threshold 0.5), or by considering only excellent documents as relevant (right 3 bars with relevance threshold 1.0) 0 0.05 0.1 0.15 0.2 0.25 0.3 0.35 0.4 0.45 P@10 P@5 Reciprocal P@10 P@5 Reciprocal 0.5 1 BM25 Anchor, Title, Body BM25 Anchor, Title, Body, ExtractedTitle Name All RelevanceThreshold Data Description Figure 10.",
                "Search ranking results.",
                "Figure 10 shows different document retrieval results with different ranking functions in terms of precision @10, precision @5 and reciprocal rank: • Blue bar - BM25 including the fields body, title (file property), and anchor text. • Purple bar - BM25 including the fields body, title (file property), anchor text, and extracted title.",
                "With the additional field of extracted title included in BM25 the precision @10 increased from 0.132 to 0.145, or by ~10%.",
                "Thus, it is safe to say that the use of extracted title can indeed improve the precision of document retrieval. 7.",
                "CONCLUSION In this paper, we have investigated the problem of automatically extracting titles from general documents.",
                "We have tried using a machine learning approach to address the problem.",
                "Previous work showed that the machine learning approach can work well for metadata extraction from research papers.",
                "In this paper, we showed that the approach can work for extraction from general documents as well.",
                "Our experimental results indicated that the machine learning approach can work significantly better than the baselines in title extraction from Office documents.",
                "Previous work on metadata extraction mainly used linguistic features in documents, while we mainly used formatting information.",
                "It appeared that using formatting information is a key for successfully conducting title extraction from general documents.",
                "We tried different machine learning models including Perceptron, Maximum Entropy, Maximum Entropy Markov Model, and Voted Perceptron.",
                "We found that the performance of the Perceptorn models was the best.",
                "We applied models constructed in one domain to another domain and applied models trained in one language to another language.",
                "We found that the accuracies did not drop substantially across different domains and across different languages, indicating that the models were generic.",
                "We also attempted to use the extracted titles in document retrieval.",
                "We observed a significant improvement in document ranking performance for search when using extracted title information.",
                "All the above investigations were not conducted in previous work, and through our investigations we verified the generality and the significance of the title extraction approach. 8.",
                "ACKNOWLEDGEMENTS We thank Chunyu Wei and Bojuan Zhao for their work on data annotation.",
                "We acknowledge Jinzhu Li for his assistance in conducting the experiments.",
                "We thank Ming Zhou, John Chen, Jun Xu, and the anonymous reviewers of JCDL05 for their valuable comments on this paper. 9.",
                "REFERENCES [1] Berger, A. L., Della Pietra, S. A., and Della Pietra, V. J.",
                "A maximum entropy approach to natural language processing.",
                "Computational Linguistics, 22:39-71, 1996. [2] Collins, M. Discriminative training methods for hidden markov models: theory and experiments with perceptron algorithms.",
                "In Proceedings of Conference on Empirical Methods in Natural Language Processing, 1-8, 2002. [3] Cortes, C. and Vapnik, V. Support-vector networks.",
                "Machine Learning, 20:273-297, 1995. [4] Chieu, H. L. and Ng, H. T. A maximum entropy approach to information extraction from semi-structured and free text.",
                "In Proceedings of the Eighteenth National Conference on Artificial Intelligence, 768-791, 2002. [5] Evans, D. K., Klavans, J. L., and McKeown, K. R. Columbia newsblaster: multilingual news summarization on the Web.",
                "In Proceedings of Human Language Technology conference / North American chapter of the Association for Computational Linguistics annual meeting, 1-4, 2004. [6] Ghahramani, Z. and Jordan, M. I. Factorial hidden markov models.",
                "Machine Learning, 29:245-273, 1997. [7] Gheel, J. and Anderson, T. Data and metadata for finding and reminding, In Proceedings of the 1999 International Conference on Information Visualization, 446-451,1999. [8] Giles, C. L., Petinot, Y., Teregowda P. B., Han, H., Lawrence, S., Rangaswamy, A., and Pal, N. eBizSearch: a niche search engine for e-Business.",
                "In Proceedings of the 26th Annual International ACM SIGIR Conference on Research and Development in Information Retrieval, 413414, 2003. [9] Giuffrida, G., Shek, E. C., and Yang, J. Knowledge-based metadata extraction from PostScript files.",
                "In Proceedings of the Fifth ACM Conference on Digital Libraries, 77-84, 2000. [10] Han, H., Giles, C. L., Manavoglu, E., Zha, H., Zhang, Z., and Fox, E. A.",
                "Automatic document metadata extraction using support vector machines.",
                "In Proceedings of the Third ACM/IEEE-CS Joint Conference on Digital Libraries, 37-48, 2003. [11] Kobayashi, M., and Takeda, K. Information retrieval on the Web.",
                "ACM Computing Surveys, 32:144-173, 2000. [12] Lafferty, J., McCallum, A., and Pereira, F. Conditional random fields: probabilistic models for segmenting and 153 labeling sequence data.",
                "In Proceedings of the Eighteenth International Conference on Machine Learning, 282-289, 2001. [13] Li, Y., Zaragoza, H., Herbrich, R., Shawe-Taylor J., and Kandola, J. S. The perceptron algorithm with uneven margins.",
                "In Proceedings of the Nineteenth International Conference on Machine Learning, 379-386, 2002. [14] Liddy, E. D., Sutton, S., Allen, E., Harwell, S., Corieri, S., Yilmazel, O., Ozgencil, N. E., Diekema, A., McCracken, N., and Silverstein, J.",
                "Automatic Metadata generation & evaluation.",
                "In Proceedings of the 25th Annual International ACM SIGIR Conference on Research and Development in Information Retrieval, 401-402, 2002. [15] Littlefield, A.",
                "Effective enterprise information retrieval across new content formats.",
                "In Proceedings of the Seventh Search Engine Conference, http://www.infonortics.com/searchengines/sh02/02prog.html, 2002. [16] Mao, S., Kim, J. W., and Thoma, G. R. A dynamic feature generation system for automated metadata extraction in preservation of digital materials.",
                "In Proceedings of the First International Workshop on Document Image Analysis for Libraries, 225-232, 2004. [17] McCallum, A., Freitag, D., and Pereira, F. Maximum entropy markov models for information extraction and segmentation.",
                "In Proceedings of the Seventeenth International Conference on Machine Learning, 591-598, 2000. [18] Murphy, L. D. Digital document metadata in organizations: roles, analytical approaches, and future research directions.",
                "In Proceedings of the Thirty-First Annual Hawaii International Conference on System Sciences, 267-276, 1998. [19] Pinto, D., McCallum, A., Wei, X., and Croft, W. B.",
                "Table extraction using conditional random fields.",
                "In Proceedings of the 26th Annual International ACM SIGIR Conference on Research and Development in Information Retrieval, 235242, 2003. [20] Ratnaparkhi, A. Unsupervised statistical models for prepositional phrase attachment.",
                "In Proceedings of the Seventeenth International Conference on Computational Linguistics. 1079-1085, 1998. [21] Robertson, S., Zaragoza, H., and Taylor, M. Simple BM25 extension to multiple weighted fields, In Proceedings of ACM Thirteenth Conference on Information and Knowledge Management, 42-49, 2004. [22] Yi, J. and Sundaresan, N. Metadata based Web mining for relevance, In Proceedings of the 2000 International Symposium on Database Engineering & Applications, 113121, 2000. [23] Yilmazel, O., Finneran, C. M., and Liddy, E. D. MetaExtract: An NLP system to automatically assign metadata.",
                "In Proceedings of the 2004 Joint ACM/IEEE Conference on Digital Libraries, 241-242, 2004. [24] Zhang, J. and Dimitroff, A. Internet search engines response to metadata Dublin Core implementation.",
                "Journal of Information Science, 30:310-320, 2004. [25] Zhang, L., Pan, Y., and Zhang, T. Recognising and using named entities: focused named entity recognition using machine learning.",
                "In Proceedings of the 27th Annual International ACM SIGIR Conference on Research and Development in Information Retrieval, 281-288, 2004. [26] http://dublincore.org/groups/corporate/Seattle/ 154"
            ],
            "original_annotated_samples": [],
            "translated_annotated_samples": [],
            "translated_text": "",
            "candidates": [],
            "error": []
        },
        "usefulness of extracted title": {
            "translated_key": "utilidad del título extraído",
            "is_in_text": false,
            "original_annotated_sentences": [
                "Automatic Extraction of Titles from General Documents using Machine Learning Yunhua Hu1 Computer Science Department Xian Jiaotong University No 28, Xianning West Road Xian, China, 710049 yunhuahu@mail.xjtu.edu.cn Hang Li, Yunbo Cao Microsoft Research Asia 5F Sigma Center, No.",
                "49 Zhichun Road, Haidian, Beijing, China, 100080 {hangli,yucao}@microsoft.com Qinghua Zheng Computer Science Department Xian Jiaotong University No 28, Xianning West Road Xian, China, 710049 qhzheng@mail.xjtu.edu.cn Dmitriy Meyerzon Microsoft Corporation One Microsoft Way Redmond, WA, USA, 98052 dmitriym@microsoft.com ABSTRACT In this paper, we propose a machine learning approach to title extraction from general documents.",
                "By general documents, we mean documents that can belong to any one of a number of specific genres, including presentations, book chapters, technical papers, brochures, reports, and letters.",
                "Previously, methods have been proposed mainly for title extraction from research papers.",
                "It has not been clear whether it could be possible to conduct automatic title extraction from general documents.",
                "As a case study, we consider extraction from Office including Word and PowerPoint.",
                "In our approach, we annotate titles in sample documents (for Word and PowerPoint respectively) and take them as training data, train machine learning models, and perform title extraction using the trained models.",
                "Our method is unique in that we mainly utilize formatting information such as font size as features in the models.",
                "It turns out that the use of formatting information can lead to quite accurate extraction from general documents.",
                "Precision and recall for title extraction from Word is 0.810 and 0.837 respectively, and precision and recall for title extraction from PowerPoint is 0.875 and 0.895 respectively in an experiment on intranet data.",
                "Other important new findings in this work include that we can train models in one domain and apply them to another domain, and more surprisingly we can even train models in one language and apply them to another language.",
                "Moreover, we can significantly improve search ranking results in document retrieval by using the extracted titles.",
                "Categories and Subject Descriptors H.3.3 [Information Storage and Retrieval]: Information Search and Retrieval - Search Process; H.4.1 [Information Systems Applications]: Office Automation - Word processing; D.2.8 [Software Engineering]: Metrics - complexity measures, performance measures General Terms Algorithms, Experimentation, Performance. 1.",
                "INTRODUCTION Metadata of documents is useful for many kinds of document processing such as search, browsing, and filtering.",
                "Ideally, metadata is defined by the authors of documents and is then used by various systems.",
                "However, people seldom define document metadata by themselves, even when they have convenient metadata definition tools [26].",
                "Thus, how to automatically extract metadata from the bodies of documents turns out to be an important research issue.",
                "Methods for performing the task have been proposed.",
                "However, the focus was mainly on extraction from research papers.",
                "For instance, Han et al. [10] proposed a machine learning based method to conduct extraction from research papers.",
                "They formalized the problem as that of classification and employed Support Vector Machines as the classifier.",
                "They mainly used linguistic features in the model.1 In this paper, we consider metadata extraction from general documents.",
                "By general documents, we mean documents that may belong to any one of a number of specific genres.",
                "General documents are more widely available in digital libraries, intranets and the internet, and thus investigation on extraction from them is sorely needed.",
                "Research papers usually have well-formed styles and noticeable characteristics.",
                "In contrast, the styles of general documents can vary greatly.",
                "It has not been clarified whether a machine learning based approach can work well for this task.",
                "There are many types of metadata: title, author, date of creation, etc.",
                "As a case study, we consider title extraction in this paper.",
                "General documents can be in many different file formats: Microsoft Office, PDF (PS), etc.",
                "As a case study, we consider extraction from Office including Word and PowerPoint.",
                "We take a machine learning approach.",
                "We annotate titles in sample documents (for Word and PowerPoint respectively) and take them as training data to train several types of models, and perform title extraction using any one type of the trained models.",
                "In the models, we mainly utilize formatting information such as font size as features.",
                "We employ the following models: Maximum Entropy Model, Perceptron with Uneven Margins, Maximum Entropy Markov Model, and Voted Perceptron.",
                "In this paper, we also investigate the following three problems, which did not seem to have been examined previously. (1) Comparison between models: among the models above, which model performs best for title extraction; (2) Generality of model: whether it is possible to train a model on one domain and apply it to another domain, and whether it is possible to train a model in one language and apply it to another language; (3) Usefulness of extracted titles: whether extracted titles can improve document processing such as search.",
                "Experimental results indicate that our approach works well for title extraction from general documents.",
                "Our method can significantly outperform the baselines: one that always uses the first lines as titles and the other that always uses the lines in the largest font sizes as titles.",
                "Precision and recall for title extraction from Word are 0.810 and 0.837 respectively, and precision and recall for title extraction from PowerPoint are 0.875 and 0.895 respectively.",
                "It turns out that the use of format features is the key to successful title extraction. (1) We have observed that Perceptron based models perform better in terms of extraction accuracies. (2) We have empirically verified that the models trained with our approach are generic in the sense that they can be trained on one domain and applied to another, and they can be trained in one language and applied to another. (3) We have found that using the extracted titles we can significantly improve precision of document retrieval (by 10%).",
                "We conclude that we can indeed conduct reliable title extraction from general documents and use the extracted results to improve real applications.",
                "The rest of the paper is organized as follows.",
                "In section 2, we introduce related work, and in section 3, we explain the motivation and problem setting of our work.",
                "In section 4, we describe our method of title extraction, and in section 5, we describe our method of document retrieval using extracted titles.",
                "Section 6 gives our experimental results.",
                "We make concluding remarks in section 7. 2.",
                "RELATED WORK 2.1 Document Metadata Extraction Methods have been proposed for performing automatic metadata extraction from documents; however, the main focus was on extraction from research papers.",
                "The proposed methods fall into two categories: the rule based approach and the machine learning based approach.",
                "Giuffrida et al. [9], for instance, developed a rule-based system for automatically extracting metadata from research papers in Postscript.",
                "They used rules like titles are usually located on the upper portions of the first pages and they are usually in the largest font sizes.",
                "Liddy et al. [14] and Yilmazel el al. [23] performed metadata extraction from educational materials using rule-based natural language processing technologies.",
                "Mao et al. [16] also conducted automatic metadata extraction from research papers using rules on formatting information.",
                "The rule-based approach can achieve high performance.",
                "However, it also has disadvantages.",
                "It is less adaptive and robust when compared with the machine learning approach.",
                "Han et al. [10], for instance, conducted metadata extraction with the machine learning approach.",
                "They viewed the problem as that of classifying the lines in a document into the categories of metadata and proposed using Support Vector Machines as the classifier.",
                "They mainly used linguistic information as features.",
                "They reported high extraction accuracy from research papers in terms of precision and recall. 2.2 Information Extraction Metadata extraction can be viewed as an application of information extraction, in which given a sequence of instances, we identify a subsequence that represents information in which we are interested.",
                "Hidden Markov Model [6], Maximum Entropy Model [1, 4], Maximum Entropy Markov Model [17], Support Vector Machines [3], Conditional Random Field [12], and Voted Perceptron [2] are widely used information extraction models.",
                "Information extraction has been applied, for instance, to part-ofspeech tagging [20], named entity recognition [25] and table extraction [19]. 2.3 Search Using Title Information Title information is useful for document retrieval.",
                "In the system Citeseer, for instance, Giles et al. managed to extract titles from research papers and make use of the extracted titles in metadata search of papers [8].",
                "In web search, the title fields (i.e., file properties) and anchor texts of web pages (HTML documents) can be viewed as titles of the pages [5].",
                "Many search engines seem to utilize them for web page retrieval [7, 11, 18, 22].",
                "Zhang et al., found that web pages with well-defined metadata are more easily retrieved than those without well-defined metadata [24].",
                "To the best of our knowledge, no research has been conducted on using extracted titles from general documents (e.g., Office documents) for search of the documents. 146 3.",
                "MOTIVATION AND PROBLEM SETTING We consider the issue of automatically extracting titles from general documents.",
                "By general documents, we mean documents that belong to one of any number of specific genres.",
                "The documents can be presentations, books, book chapters, technical papers, brochures, reports, memos, specifications, letters, announcements, or resumes.",
                "General documents are more widely available in digital libraries, intranets, and internet, and thus investigation on title extraction from them is sorely needed.",
                "Figure 1 shows an estimate on distributions of file formats on intranet and internet [15].",
                "Office and PDF are the main file formats on the intranet.",
                "Even on the internet, the documents in the formats are still not negligible, given its extremely large size.",
                "In this paper, without loss of generality, we take Office documents as an example.",
                "Figure 1.",
                "Distributions of file formats in internet and intranet.",
                "For Office documents, users can define titles as file properties using a feature provided by Office.",
                "We found in an experiment, however, that users seldom use the feature and thus titles in file properties are usually very inaccurate.",
                "That is to say, titles in file properties are usually inconsistent with the true titles in the file bodies that are created by the authors and are visible to readers.",
                "We collected 6,000 Word and 6,000 PowerPoint documents from an intranet and the internet and examined how many titles in the file properties are correct.",
                "We found that surprisingly the accuracy was only 0.265 (cf., Section 6.3 for details).",
                "A number of reasons can be considered.",
                "For example, if one creates a new file by copying an old file, then the file property of the new file will also be copied from the old file.",
                "In another experiment, we found that Google uses the titles in file properties of Office documents in search and browsing, but the titles are not very accurate.",
                "We created 50 queries to search Word and PowerPoint documents and examined the top 15 results of each query returned by Google.",
                "We found that nearly all the titles presented in the search results were from the file properties of the documents.",
                "However, only 0.272 of them were correct.",
                "Actually, true titles usually exist at the beginnings of the bodies of documents.",
                "If we can accurately extract the titles from the bodies of documents, then we can exploit reliable title information in document processing.",
                "This is exactly the problem we address in this paper.",
                "More specifically, given a Word document, we are to extract the title from the top region of the first page.",
                "Given a PowerPoint document, we are to extract the title from the first slide.",
                "A title sometimes consists of a main title and one or two subtitles.",
                "We only consider extraction of the main title.",
                "As baselines for title extraction, we use that of always using the first lines as titles and that of always using the lines with largest font sizes as titles.",
                "Figure 2.",
                "Title extraction from Word document.",
                "Figure 3.",
                "Title extraction from PowerPoint document.",
                "Next, we define a specification for human judgments in title data annotation.",
                "The annotated data will be used in training and testing of the title extraction methods.",
                "Summary of the specification: The title of a document should be identified on the basis of common sense, if there is no difficulty in the identification.",
                "However, there are many cases in which the identification is not easy.",
                "There are some rules defined in the specification that guide identification for such cases.",
                "The rules include a title is usually in consecutive lines in the same format, a document can have no title, titles in images are not considered, a title should not contain words like draft, 147 whitepaper, etc, if it is difficult to determine which is the title, select the one in the largest font size, and if it is still difficult to determine which is the title, select the first candidate. (The specification covers all the cases we have encountered in data annotation.)",
                "Figures 2 and 3 show examples of Office documents from which we conduct title extraction.",
                "In Figure 2, Differences in Win32 API Implementations among Windows Operating Systems is the title of the Word document.",
                "Microsoft Windows on the top of this page is a picture and thus is ignored.",
                "In Figure 3, Building Competitive Advantages through an Agile Infrastructure is the title of the PowerPoint document.",
                "We have developed a tool for annotation of titles by human annotators.",
                "Figure 4 shows a snapshot of the tool.",
                "Figure 4.",
                "Title annotation tool. 4.",
                "TITLE EXTRACTION METHOD 4.1 Outline Title extraction based on machine learning consists of training and extraction.",
                "The same pre-processing step occurs before training and extraction.",
                "During pre-processing, from the top region of the first page of a Word document or the first slide of a PowerPoint document a number of units for processing are extracted.",
                "If a line (lines are separated by return symbols) only has a single format, then the line will become a unit.",
                "If a line has several parts and each of them has its own format, then each part will become a unit.",
                "Each unit will be treated as an instance in learning.",
                "A unit contains not only content information (linguistic information) but also formatting information.",
                "The input to pre-processing is a document and the output of pre-processing is a sequence of units (instances).",
                "Figure 5 shows the units obtained from the document in Figure 2.",
                "Figure 5.",
                "Example of units.",
                "In learning, the input is sequences of units where each sequence corresponds to a document.",
                "We take labeled units (labeled as title_begin, title_end, or other) in the sequences as training data and construct models for identifying whether a unit is title_begin title_end, or other.",
                "We employ four types of models: Perceptron, Maximum Entropy (ME), Perceptron Markov Model (PMM), and Maximum Entropy Markov Model (MEMM).",
                "In extraction, the input is a sequence of units from one document.",
                "We employ one type of model to identify whether a unit is title_begin, title_end, or other.",
                "We then extract units from the unit labeled with title_begin to the unit labeled with title_end.",
                "The result is the extracted title of the document.",
                "The unique characteristic of our approach is that we mainly utilize formatting information for title extraction.",
                "Our assumption is that although general documents vary in styles, their formats have certain patterns and we can learn and utilize the patterns for title extraction.",
                "This is in contrast to the work by Han et al., in which only linguistic features are used for extraction from research papers. 4.2 Models The four models actually can be considered in the same metadata extraction framework.",
                "That is why we apply them together to our current problem.",
                "Each input is a sequence of instances kxxx L21 together with a sequence of labels kyyy L21 . ix and iy represents an instance and its label, respectively ( ki ,,2,1 L= ).",
                "Recall that an instance here represents a unit.",
                "A label represents title_begin, title_end, or other.",
                "Here, k is the number of units in a document.",
                "In learning, we train a model which can be generally denoted as a conditional probability distribution )|( 11 kk XXYYP LL where iX and iY denote random variables taking instance ix and label iy as values, respectively ( ki ,,2,1 L= ).",
                "Learning Tool Extraction Tool 21121 2222122221 1121111211 nknnknn kk kk yyyxxx yyyxxx yyyxxx LL LL LL LL → → → )|(maxarg 11 mkmmkm xxyyP LL )|( 11 kk XXYYP LL Conditional Distribution mkmm xxx L21 Figure 6.",
                "Metadata extraction model.",
                "We can make assumptions about the general model in order to make it simple enough for training. 148 For example, we can assume that kYY ,,1 L are independent of each other given kXX ,,1 L .",
                "Thus, we have )|()|( )|( 11 11 kk kk XYPXYP XXYYP L LL = In this way, we decompose the model into a number of classifiers.",
                "We train the classifiers locally using the labeled data.",
                "As the classifier, we employ the Perceptron or Maximum Entropy model.",
                "We can also assume that the first order Markov property holds for kYY ,,1 L given kXX ,,1 L .",
                "Thus, we have )|()|( )|( 111 11 kkk kk XYYPXYP XXYYP −= L LL Again, we obtain a number of classifiers.",
                "However, the classifiers are conditioned on the previous label.",
                "When we employ the Percepton or Maximum Entropy model as a classifier, the models become a Percepton Markov Model or Maximum Entropy Markov Model, respectively.",
                "That is to say, the two models are more precise.",
                "In extraction, given a new sequence of instances, we resort to one of the constructed models to assign a sequence of labels to the sequence of instances, i.e., perform extraction.",
                "For Perceptron and ME, we assign labels locally and combine the results globally later using heuristics.",
                "Specifically, we first identify the most likely title_begin.",
                "Then we find the most likely title_end within three units after the title_begin.",
                "Finally, we extract as a title the units between the title_begin and the title_end.",
                "For PMM and MEMM, we employ the Viterbi algorithm to find the globally optimal label sequence.",
                "In this paper, for Perceptron, we actually employ an improved variant of it, called Perceptron with Uneven Margin [13].",
                "This version of Perceptron can work well especially when the number of positive instances and the number of negative instances differ greatly, which is exactly the case in our problem.",
                "We also employ an improved version of Perceptron Markov Model in which the Perceptron model is the so-called Voted Perceptron [2].",
                "In addition, in training, the parameters of the model are updated globally rather than locally. 4.3 Features There are two types of features: format features and linguistic features.",
                "We mainly use the former.",
                "The features are used for both the title-begin and the title-end classifiers. 4.3.1 Format Features Font Size: There are four binary features that represent the normalized font size of the unit (recall that a unit has only one type of font).",
                "If the font size of the unit is the largest in the document, then the first feature will be 1, otherwise 0.",
                "If the font size is the smallest in the document, then the fourth feature will be 1, otherwise 0.",
                "If the font size is above the average font size and not the largest in the document, then the second feature will be 1, otherwise 0.",
                "If the font size is below the average font size and not the smallest, the third feature will be 1, otherwise 0.",
                "It is necessary to conduct normalization on font sizes.",
                "For example, in one document the largest font size might be 12pt, while in another the smallest one might be 18pt.",
                "Boldface: This binary feature represents whether or not the current unit is in boldface.",
                "Alignment: There are four binary features that respectively represent the location of the current unit: left, center, right, and unknown alignment.",
                "The following format features with respect to context play an important role in title extraction.",
                "Empty Neighboring Unit: There are two binary features that represent, respectively, whether or not the previous unit and the current unit are blank lines.",
                "Font Size Change: There are two binary features that represent, respectively, whether or not the font size of the previous unit and the font size of the next unit differ from that of the current unit.",
                "Alignment Change: There are two binary features that represent, respectively, whether or not the alignment of the previous unit and the alignment of the next unit differ from that of the current one.",
                "Same Paragraph: There are two binary features that represent, respectively, whether or not the previous unit and the next unit are in the same paragraph as the current unit. 4.3.2 Linguistic Features The linguistic features are based on key words.",
                "Positive Word: This binary feature represents whether or not the current unit begins with one of the positive words.",
                "The positive words include title:, subject:, subject line: For example, in some documents the lines of titles and authors have the same formats.",
                "However, if lines begin with one of the positive words, then it is likely that they are title lines.",
                "Negative Word: This binary feature represents whether or not the current unit begins with one of the negative words.",
                "The negative words include To, By, created by, updated by, etc.",
                "There are more negative words than positive words.",
                "The above linguistic features are language dependent.",
                "Word Count: A title should not be too long.",
                "We heuristically create four intervals: [1, 2], [3, 6], [7, 9] and [9, ∞) and define one feature for each interval.",
                "If the number of words in a title falls into an interval, then the corresponding feature will be 1; otherwise 0.",
                "Ending Character: This feature represents whether the unit ends with :, -, or other special characters.",
                "A title usually does not end with such a character. 5.",
                "DOCUMENT RETRIEVAL METHOD We describe our method of document retrieval using extracted titles.",
                "Typically, in information retrieval a document is split into a number of fields including body, title, and anchor text.",
                "A ranking function in search can use different weights for different fields of 149 the document.",
                "Also, titles are typically assigned high weights, indicating that they are important for document retrieval.",
                "As explained previously, our experiment has shown that a significant number of documents actually have incorrect titles in the file properties, and thus in addition of using them we use the extracted titles as one more field of the document.",
                "By doing this, we attempt to improve the overall precision.",
                "In this paper, we employ a modification of BM25 that allows field weighting [21].",
                "As fields, we make use of body, title, extracted title and anchor.",
                "First, for each term in the query we count the term frequency in each field of the document; each field frequency is then weighted according to the corresponding weight parameter: ∑= f tfft tfwwtf Similarly, we compute the document length as a weighted sum of lengths of each field.",
                "Average document length in the corpus becomes the average of all weighted document lengths. ∑= f ff dlwwdl In our experiments we used 75.0,8.11 == bk .",
                "Weight for content was 1.0, title was 10.0, anchor was 10.0, and extracted title was 5.0. 6.",
                "EXPERIMENTAL RESULTS 6.1 Data Sets and Evaluation Measures We used two data sets in our experiments.",
                "First, we downloaded and randomly selected 5,000 Word documents and 5,000 PowerPoint documents from an intranet of Microsoft.",
                "We call it MS hereafter.",
                "Second, we downloaded and randomly selected 500 Word and 500 PowerPoint documents from the DotGov and DotCom domains on the internet, respectively.",
                "Figure 7 shows the distributions of the genres of the documents.",
                "We see that the documents are indeed general documents as we define them.",
                "Figure 7.",
                "Distributions of document genres.",
                "Third, a data set in Chinese was also downloaded from the internet.",
                "It includes 500 Word documents and 500 PowerPoint documents in Chinese.",
                "We manually labeled the titles of all the documents, on the basis of our specification.",
                "Not all the documents in the two data sets have titles.",
                "Table 1 shows the percentages of the documents having titles.",
                "We see that DotCom and DotGov have more PowerPoint documents with titles than MS.",
                "This might be because PowerPoint documents published on the internet are more formal than those on the intranet.",
                "Table 1.",
                "The portion of documents with titles Domain Type MS DotCom DotGov Word 75.7% 77.8% 75.6% PowerPoint 82.1% 93.4% 96.4% In our experiments, we conducted evaluations on title extraction in terms of precision, recall, and F-measure.",
                "The evaluation measures are defined as follows: Precision: P = A / ( A + B ) Recall: R = A / ( A + C ) F-measure: F1 = 2PR / ( P + R ) Here, A, B, C, and D are numbers of documents as those defined in Table 2.",
                "Table 2.",
                "Contingence table with regard to title extraction Is title Is not title Extracted A B Not extracted C D 6.2 Baselines We test the accuracies of the two baselines described in section 4.2.",
                "They are denoted as largest font size and first line respectively. 6.3 Accuracy of Titles in File Properties We investigate how many titles in the file properties of the documents are reliable.",
                "We view the titles annotated by humans as true titles and test how many titles in the file properties can approximately match with the true titles.",
                "We use Edit Distance to conduct the approximate match. (Approximate match is only used in this evaluation).",
                "This is because sometimes human annotated titles can be slightly different from the titles in file properties on the surface, e.g., contain extra spaces).",
                "Given string A and string B: if ( (D == 0) or ( D / ( La + Lb ) < θ ) ) then string A = string B D: Edit Distance between string A and string B La: length of string A Lb: length of string B θ: 0.1 ∑ × ++− + = t t n N wtf avwdl wdl bbk kwtf FBM )log( ))1(( )1( 25 1 1 150 Table 3.",
                "Accuracies of titles in file properties File Type Domain Precision Recall F1 MS 0.299 0.311 0.305 DotCom 0.210 0.214 0.212Word DotGov 0.182 0.177 0.180 MS 0.229 0.245 0.237 DotCom 0.185 0.186 0.186PowerPoint DotGov 0.180 0.182 0.181 6.4 Comparison with Baselines We conducted title extraction from the first data set (Word and PowerPoint in MS).",
                "As the model, we used Perceptron.",
                "We conduct 4-fold cross validation.",
                "Thus, all the results reported here are those averaged over 4 trials.",
                "Tables 4 and 5 show the results.",
                "We see that Perceptron significantly outperforms the baselines.",
                "In the evaluation, we use exact matching between the true titles annotated by humans and the extracted titles.",
                "Table 4.",
                "Accuracies of title extraction with Word Precision Recall F1 Model Perceptron 0.810 0.837 0.823 Largest font size 0.700 0.758 0.727 Baselines First line 0.707 0.767 0.736 Table 5.",
                "Accuracies of title extraction with PowerPoint Precision Recall F1 Model Perceptron 0.875 0. 895 0.885 Largest font size 0.844 0.887 0.865 Baselines First line 0.639 0.671 0.655 We see that the machine learning approach can achieve good performance in title extraction.",
                "For Word documents both precision and recall of the approach are 8 percent higher than those of the baselines.",
                "For PowerPoint both precision and recall of the approach are 2 percent higher than those of the baselines.",
                "We conduct significance tests.",
                "The results are shown in Table 6.",
                "Here, Largest denotes the baseline of using the largest font size, First denotes the baseline of using the first line.",
                "The results indicate that the improvements of machine learning over baselines are statistically significant (in the sense p-value < 0.05) Table 6.",
                "Sign test results Documents Type Sign test between p-value Perceptron vs. Largest 3.59e-26 Word Perceptron vs. First 7.12e-10 Perceptron vs. Largest 0.010 PowerPoint Perceptron vs. First 5.13e-40 We see, from the results, that the two baselines can work well for title extraction, suggesting that font size and position information are most useful features for title extraction.",
                "However, it is also obvious that using only these two features is not enough.",
                "There are cases in which all the lines have the same font size (i.e., the largest font size), or cases in which the lines with the largest font size only contain general descriptions like Confidential, White paper, etc.",
                "For those cases, the largest font size method cannot work well.",
                "For similar reasons, the first line method alone cannot work well, either.",
                "With the combination of different features (evidence in title judgment), Perceptron can outperform Largest and First.",
                "We investigate the performance of solely using linguistic features.",
                "We found that it does not work well.",
                "It seems that the format features play important roles and the linguistic features are supplements..",
                "Figure 8.",
                "An example Word document.",
                "Figure 9.",
                "An example PowerPoint document.",
                "We conducted an error analysis on the results of Perceptron.",
                "We found that the errors fell into three categories. (1) About one third of the errors were related to hard cases.",
                "In these documents, the layouts of the first pages were difficult to understand, even for humans.",
                "Figure 8 and 9 shows examples. (2) Nearly one fourth of the errors were from the documents which do not have true titles but only contain bullets.",
                "Since we conduct extraction from the top regions, it is difficult to get rid of these errors with the current approach. (3).",
                "Confusions between main titles and subtitles were another type of error.",
                "Since we only labeled the main titles as titles, the extractions of both titles were considered incorrect.",
                "This type of error does little harm to document processing like search, however. 6.5 Comparison between Models To compare the performance of different machine learning models, we conducted another experiment.",
                "Again, we perform 4-fold cross 151 validation on the first data set (MS).",
                "Table 7, 8 shows the results of all the four models.",
                "It turns out that Perceptron and PMM perform the best, followed by MEMM, and ME performs the worst.",
                "In general, the Markovian models perform better than or as well as their classifier counterparts.",
                "This seems to be because the Markovian models are trained globally, while the classifiers are trained locally.",
                "The Perceptron based models perform better than the ME based counterparts.",
                "This seems to be because the Perceptron based models are created to make better classifications, while ME models are constructed for better prediction.",
                "Table 7.",
                "Comparison between different learning models for title extraction with Word Model Precision Recall F1 Perceptron 0.810 0.837 0.823 MEMM 0.797 0.824 0.810 PMM 0.827 0.823 0.825 ME 0.801 0.621 0.699 Table 8.",
                "Comparison between different learning models for title extraction with PowerPoint Model Precision Recall F1 Perceptron 0.875 0. 895 0. 885 MEMM 0.841 0.861 0.851 PMM 0.873 0.896 0.885 ME 0.753 0.766 0.759 6.6 Domain Adaptation We apply the model trained with the first data set (MS) to the second data set (DotCom and DotGov).",
                "Tables 9-12 show the results.",
                "Table 9.",
                "Accuracies of title extraction with Word in DotGov Precision Recall F1 Model Perceptron 0.716 0.759 0.737 Largest font size 0.549 0.619 0.582Baselines First line 0.462 0.521 0.490 Table 10.",
                "Accuracies of title extraction with PowerPoint in DotGov Precision Recall F1 Model Perceptron 0.900 0.906 0.903 Largest font size 0.871 0.888 0.879Baselines First line 0.554 0.564 0.559 Table 11.",
                "Accuracies of title extraction with Word in DotCom Precisio n Recall F1 Model Perceptron 0.832 0.880 0.855 Largest font size 0.676 0.753 0.712Baselines First line 0.577 0.643 0.608 Table 12.",
                "Performance of PowerPoint document title extraction in DotCom Precisio n Recall F1 Model Perceptron 0.910 0.903 0.907 Largest font size 0.864 0.886 0.875Baselines First line 0.570 0.585 0.577 From the results, we see that the models can be adapted to different domains well.",
                "There is almost no drop in accuracy.",
                "The results indicate that the patterns of title formats exist across different domains, and it is possible to construct a domain independent model by mainly using formatting information. 6.7 Language Adaptation We apply the model trained with the data in English (MS) to the data set in Chinese.",
                "Tables 13-14 show the results.",
                "Table 13.",
                "Accuracies of title extraction with Word in Chinese Precision Recall F1 Model Perceptron 0.817 0.805 0.811 Largest font size 0.722 0.755 0.738Baselines First line 0.743 0.777 0.760 Table 14.",
                "Accuracies of title extraction with PowerPoint in Chinese Precision Recall F1 Model Perceptron 0.766 0.812 0.789 Largest font size 0.753 0.813 0.782Baselines First line 0.627 0.676 0.650 We see that the models can be adapted to a different language.",
                "There are only small drops in accuracy.",
                "Obviously, the linguistic features do not work for Chinese, but the effect of not using them is negligible.",
                "The results indicate that the patterns of title formats exist across different languages.",
                "From the domain adaptation and language adaptation results, we conclude that the use of formatting information is the key to a successful extraction from general documents. 6.8 Search with Extracted Titles We performed experiments on using title extraction for document retrieval.",
                "As a baseline, we employed BM25 without using extracted titles.",
                "The ranking mechanism was as described in Section 5.",
                "The weights were heuristically set.",
                "We did not conduct optimization on the weights.",
                "The evaluation was conducted on a corpus of 1.3 M documents crawled from the intranet of Microsoft using 100 evaluation queries obtained from this intranets search engine query logs. 50 queries were from the most popular set, while 50 queries other were chosen randomly.",
                "Users were asked to provide judgments of the degree of document relevance from a scale of 1to 5 (1 meaning detrimental, 2 - bad, 3 - fair, 4 - good and 5 - excellent). 152 Figure 10 shows the results.",
                "In the chart two sets of precision results were obtained by either considering good or excellent documents as relevant (left 3 bars with relevance threshold 0.5), or by considering only excellent documents as relevant (right 3 bars with relevance threshold 1.0) 0 0.05 0.1 0.15 0.2 0.25 0.3 0.35 0.4 0.45 P@10 P@5 Reciprocal P@10 P@5 Reciprocal 0.5 1 BM25 Anchor, Title, Body BM25 Anchor, Title, Body, ExtractedTitle Name All RelevanceThreshold Data Description Figure 10.",
                "Search ranking results.",
                "Figure 10 shows different document retrieval results with different ranking functions in terms of precision @10, precision @5 and reciprocal rank: • Blue bar - BM25 including the fields body, title (file property), and anchor text. • Purple bar - BM25 including the fields body, title (file property), anchor text, and extracted title.",
                "With the additional field of extracted title included in BM25 the precision @10 increased from 0.132 to 0.145, or by ~10%.",
                "Thus, it is safe to say that the use of extracted title can indeed improve the precision of document retrieval. 7.",
                "CONCLUSION In this paper, we have investigated the problem of automatically extracting titles from general documents.",
                "We have tried using a machine learning approach to address the problem.",
                "Previous work showed that the machine learning approach can work well for metadata extraction from research papers.",
                "In this paper, we showed that the approach can work for extraction from general documents as well.",
                "Our experimental results indicated that the machine learning approach can work significantly better than the baselines in title extraction from Office documents.",
                "Previous work on metadata extraction mainly used linguistic features in documents, while we mainly used formatting information.",
                "It appeared that using formatting information is a key for successfully conducting title extraction from general documents.",
                "We tried different machine learning models including Perceptron, Maximum Entropy, Maximum Entropy Markov Model, and Voted Perceptron.",
                "We found that the performance of the Perceptorn models was the best.",
                "We applied models constructed in one domain to another domain and applied models trained in one language to another language.",
                "We found that the accuracies did not drop substantially across different domains and across different languages, indicating that the models were generic.",
                "We also attempted to use the extracted titles in document retrieval.",
                "We observed a significant improvement in document ranking performance for search when using extracted title information.",
                "All the above investigations were not conducted in previous work, and through our investigations we verified the generality and the significance of the title extraction approach. 8.",
                "ACKNOWLEDGEMENTS We thank Chunyu Wei and Bojuan Zhao for their work on data annotation.",
                "We acknowledge Jinzhu Li for his assistance in conducting the experiments.",
                "We thank Ming Zhou, John Chen, Jun Xu, and the anonymous reviewers of JCDL05 for their valuable comments on this paper. 9.",
                "REFERENCES [1] Berger, A. L., Della Pietra, S. A., and Della Pietra, V. J.",
                "A maximum entropy approach to natural language processing.",
                "Computational Linguistics, 22:39-71, 1996. [2] Collins, M. Discriminative training methods for hidden markov models: theory and experiments with perceptron algorithms.",
                "In Proceedings of Conference on Empirical Methods in Natural Language Processing, 1-8, 2002. [3] Cortes, C. and Vapnik, V. Support-vector networks.",
                "Machine Learning, 20:273-297, 1995. [4] Chieu, H. L. and Ng, H. T. A maximum entropy approach to information extraction from semi-structured and free text.",
                "In Proceedings of the Eighteenth National Conference on Artificial Intelligence, 768-791, 2002. [5] Evans, D. K., Klavans, J. L., and McKeown, K. R. Columbia newsblaster: multilingual news summarization on the Web.",
                "In Proceedings of Human Language Technology conference / North American chapter of the Association for Computational Linguistics annual meeting, 1-4, 2004. [6] Ghahramani, Z. and Jordan, M. I. Factorial hidden markov models.",
                "Machine Learning, 29:245-273, 1997. [7] Gheel, J. and Anderson, T. Data and metadata for finding and reminding, In Proceedings of the 1999 International Conference on Information Visualization, 446-451,1999. [8] Giles, C. L., Petinot, Y., Teregowda P. B., Han, H., Lawrence, S., Rangaswamy, A., and Pal, N. eBizSearch: a niche search engine for e-Business.",
                "In Proceedings of the 26th Annual International ACM SIGIR Conference on Research and Development in Information Retrieval, 413414, 2003. [9] Giuffrida, G., Shek, E. C., and Yang, J. Knowledge-based metadata extraction from PostScript files.",
                "In Proceedings of the Fifth ACM Conference on Digital Libraries, 77-84, 2000. [10] Han, H., Giles, C. L., Manavoglu, E., Zha, H., Zhang, Z., and Fox, E. A.",
                "Automatic document metadata extraction using support vector machines.",
                "In Proceedings of the Third ACM/IEEE-CS Joint Conference on Digital Libraries, 37-48, 2003. [11] Kobayashi, M., and Takeda, K. Information retrieval on the Web.",
                "ACM Computing Surveys, 32:144-173, 2000. [12] Lafferty, J., McCallum, A., and Pereira, F. Conditional random fields: probabilistic models for segmenting and 153 labeling sequence data.",
                "In Proceedings of the Eighteenth International Conference on Machine Learning, 282-289, 2001. [13] Li, Y., Zaragoza, H., Herbrich, R., Shawe-Taylor J., and Kandola, J. S. The perceptron algorithm with uneven margins.",
                "In Proceedings of the Nineteenth International Conference on Machine Learning, 379-386, 2002. [14] Liddy, E. D., Sutton, S., Allen, E., Harwell, S., Corieri, S., Yilmazel, O., Ozgencil, N. E., Diekema, A., McCracken, N., and Silverstein, J.",
                "Automatic Metadata generation & evaluation.",
                "In Proceedings of the 25th Annual International ACM SIGIR Conference on Research and Development in Information Retrieval, 401-402, 2002. [15] Littlefield, A.",
                "Effective enterprise information retrieval across new content formats.",
                "In Proceedings of the Seventh Search Engine Conference, http://www.infonortics.com/searchengines/sh02/02prog.html, 2002. [16] Mao, S., Kim, J. W., and Thoma, G. R. A dynamic feature generation system for automated metadata extraction in preservation of digital materials.",
                "In Proceedings of the First International Workshop on Document Image Analysis for Libraries, 225-232, 2004. [17] McCallum, A., Freitag, D., and Pereira, F. Maximum entropy markov models for information extraction and segmentation.",
                "In Proceedings of the Seventeenth International Conference on Machine Learning, 591-598, 2000. [18] Murphy, L. D. Digital document metadata in organizations: roles, analytical approaches, and future research directions.",
                "In Proceedings of the Thirty-First Annual Hawaii International Conference on System Sciences, 267-276, 1998. [19] Pinto, D., McCallum, A., Wei, X., and Croft, W. B.",
                "Table extraction using conditional random fields.",
                "In Proceedings of the 26th Annual International ACM SIGIR Conference on Research and Development in Information Retrieval, 235242, 2003. [20] Ratnaparkhi, A. Unsupervised statistical models for prepositional phrase attachment.",
                "In Proceedings of the Seventeenth International Conference on Computational Linguistics. 1079-1085, 1998. [21] Robertson, S., Zaragoza, H., and Taylor, M. Simple BM25 extension to multiple weighted fields, In Proceedings of ACM Thirteenth Conference on Information and Knowledge Management, 42-49, 2004. [22] Yi, J. and Sundaresan, N. Metadata based Web mining for relevance, In Proceedings of the 2000 International Symposium on Database Engineering & Applications, 113121, 2000. [23] Yilmazel, O., Finneran, C. M., and Liddy, E. D. MetaExtract: An NLP system to automatically assign metadata.",
                "In Proceedings of the 2004 Joint ACM/IEEE Conference on Digital Libraries, 241-242, 2004. [24] Zhang, J. and Dimitroff, A. Internet search engines response to metadata Dublin Core implementation.",
                "Journal of Information Science, 30:310-320, 2004. [25] Zhang, L., Pan, Y., and Zhang, T. Recognising and using named entities: focused named entity recognition using machine learning.",
                "In Proceedings of the 27th Annual International ACM SIGIR Conference on Research and Development in Information Retrieval, 281-288, 2004. [26] http://dublincore.org/groups/corporate/Seattle/ 154"
            ],
            "original_annotated_samples": [],
            "translated_annotated_samples": [],
            "translated_text": "",
            "candidates": [],
            "error": []
        },
        "extracted title usefulness": {
            "translated_key": "utilidad de título extraída",
            "is_in_text": false,
            "original_annotated_sentences": [
                "Automatic Extraction of Titles from General Documents using Machine Learning Yunhua Hu1 Computer Science Department Xian Jiaotong University No 28, Xianning West Road Xian, China, 710049 yunhuahu@mail.xjtu.edu.cn Hang Li, Yunbo Cao Microsoft Research Asia 5F Sigma Center, No.",
                "49 Zhichun Road, Haidian, Beijing, China, 100080 {hangli,yucao}@microsoft.com Qinghua Zheng Computer Science Department Xian Jiaotong University No 28, Xianning West Road Xian, China, 710049 qhzheng@mail.xjtu.edu.cn Dmitriy Meyerzon Microsoft Corporation One Microsoft Way Redmond, WA, USA, 98052 dmitriym@microsoft.com ABSTRACT In this paper, we propose a machine learning approach to title extraction from general documents.",
                "By general documents, we mean documents that can belong to any one of a number of specific genres, including presentations, book chapters, technical papers, brochures, reports, and letters.",
                "Previously, methods have been proposed mainly for title extraction from research papers.",
                "It has not been clear whether it could be possible to conduct automatic title extraction from general documents.",
                "As a case study, we consider extraction from Office including Word and PowerPoint.",
                "In our approach, we annotate titles in sample documents (for Word and PowerPoint respectively) and take them as training data, train machine learning models, and perform title extraction using the trained models.",
                "Our method is unique in that we mainly utilize formatting information such as font size as features in the models.",
                "It turns out that the use of formatting information can lead to quite accurate extraction from general documents.",
                "Precision and recall for title extraction from Word is 0.810 and 0.837 respectively, and precision and recall for title extraction from PowerPoint is 0.875 and 0.895 respectively in an experiment on intranet data.",
                "Other important new findings in this work include that we can train models in one domain and apply them to another domain, and more surprisingly we can even train models in one language and apply them to another language.",
                "Moreover, we can significantly improve search ranking results in document retrieval by using the extracted titles.",
                "Categories and Subject Descriptors H.3.3 [Information Storage and Retrieval]: Information Search and Retrieval - Search Process; H.4.1 [Information Systems Applications]: Office Automation - Word processing; D.2.8 [Software Engineering]: Metrics - complexity measures, performance measures General Terms Algorithms, Experimentation, Performance. 1.",
                "INTRODUCTION Metadata of documents is useful for many kinds of document processing such as search, browsing, and filtering.",
                "Ideally, metadata is defined by the authors of documents and is then used by various systems.",
                "However, people seldom define document metadata by themselves, even when they have convenient metadata definition tools [26].",
                "Thus, how to automatically extract metadata from the bodies of documents turns out to be an important research issue.",
                "Methods for performing the task have been proposed.",
                "However, the focus was mainly on extraction from research papers.",
                "For instance, Han et al. [10] proposed a machine learning based method to conduct extraction from research papers.",
                "They formalized the problem as that of classification and employed Support Vector Machines as the classifier.",
                "They mainly used linguistic features in the model.1 In this paper, we consider metadata extraction from general documents.",
                "By general documents, we mean documents that may belong to any one of a number of specific genres.",
                "General documents are more widely available in digital libraries, intranets and the internet, and thus investigation on extraction from them is sorely needed.",
                "Research papers usually have well-formed styles and noticeable characteristics.",
                "In contrast, the styles of general documents can vary greatly.",
                "It has not been clarified whether a machine learning based approach can work well for this task.",
                "There are many types of metadata: title, author, date of creation, etc.",
                "As a case study, we consider title extraction in this paper.",
                "General documents can be in many different file formats: Microsoft Office, PDF (PS), etc.",
                "As a case study, we consider extraction from Office including Word and PowerPoint.",
                "We take a machine learning approach.",
                "We annotate titles in sample documents (for Word and PowerPoint respectively) and take them as training data to train several types of models, and perform title extraction using any one type of the trained models.",
                "In the models, we mainly utilize formatting information such as font size as features.",
                "We employ the following models: Maximum Entropy Model, Perceptron with Uneven Margins, Maximum Entropy Markov Model, and Voted Perceptron.",
                "In this paper, we also investigate the following three problems, which did not seem to have been examined previously. (1) Comparison between models: among the models above, which model performs best for title extraction; (2) Generality of model: whether it is possible to train a model on one domain and apply it to another domain, and whether it is possible to train a model in one language and apply it to another language; (3) Usefulness of extracted titles: whether extracted titles can improve document processing such as search.",
                "Experimental results indicate that our approach works well for title extraction from general documents.",
                "Our method can significantly outperform the baselines: one that always uses the first lines as titles and the other that always uses the lines in the largest font sizes as titles.",
                "Precision and recall for title extraction from Word are 0.810 and 0.837 respectively, and precision and recall for title extraction from PowerPoint are 0.875 and 0.895 respectively.",
                "It turns out that the use of format features is the key to successful title extraction. (1) We have observed that Perceptron based models perform better in terms of extraction accuracies. (2) We have empirically verified that the models trained with our approach are generic in the sense that they can be trained on one domain and applied to another, and they can be trained in one language and applied to another. (3) We have found that using the extracted titles we can significantly improve precision of document retrieval (by 10%).",
                "We conclude that we can indeed conduct reliable title extraction from general documents and use the extracted results to improve real applications.",
                "The rest of the paper is organized as follows.",
                "In section 2, we introduce related work, and in section 3, we explain the motivation and problem setting of our work.",
                "In section 4, we describe our method of title extraction, and in section 5, we describe our method of document retrieval using extracted titles.",
                "Section 6 gives our experimental results.",
                "We make concluding remarks in section 7. 2.",
                "RELATED WORK 2.1 Document Metadata Extraction Methods have been proposed for performing automatic metadata extraction from documents; however, the main focus was on extraction from research papers.",
                "The proposed methods fall into two categories: the rule based approach and the machine learning based approach.",
                "Giuffrida et al. [9], for instance, developed a rule-based system for automatically extracting metadata from research papers in Postscript.",
                "They used rules like titles are usually located on the upper portions of the first pages and they are usually in the largest font sizes.",
                "Liddy et al. [14] and Yilmazel el al. [23] performed metadata extraction from educational materials using rule-based natural language processing technologies.",
                "Mao et al. [16] also conducted automatic metadata extraction from research papers using rules on formatting information.",
                "The rule-based approach can achieve high performance.",
                "However, it also has disadvantages.",
                "It is less adaptive and robust when compared with the machine learning approach.",
                "Han et al. [10], for instance, conducted metadata extraction with the machine learning approach.",
                "They viewed the problem as that of classifying the lines in a document into the categories of metadata and proposed using Support Vector Machines as the classifier.",
                "They mainly used linguistic information as features.",
                "They reported high extraction accuracy from research papers in terms of precision and recall. 2.2 Information Extraction Metadata extraction can be viewed as an application of information extraction, in which given a sequence of instances, we identify a subsequence that represents information in which we are interested.",
                "Hidden Markov Model [6], Maximum Entropy Model [1, 4], Maximum Entropy Markov Model [17], Support Vector Machines [3], Conditional Random Field [12], and Voted Perceptron [2] are widely used information extraction models.",
                "Information extraction has been applied, for instance, to part-ofspeech tagging [20], named entity recognition [25] and table extraction [19]. 2.3 Search Using Title Information Title information is useful for document retrieval.",
                "In the system Citeseer, for instance, Giles et al. managed to extract titles from research papers and make use of the extracted titles in metadata search of papers [8].",
                "In web search, the title fields (i.e., file properties) and anchor texts of web pages (HTML documents) can be viewed as titles of the pages [5].",
                "Many search engines seem to utilize them for web page retrieval [7, 11, 18, 22].",
                "Zhang et al., found that web pages with well-defined metadata are more easily retrieved than those without well-defined metadata [24].",
                "To the best of our knowledge, no research has been conducted on using extracted titles from general documents (e.g., Office documents) for search of the documents. 146 3.",
                "MOTIVATION AND PROBLEM SETTING We consider the issue of automatically extracting titles from general documents.",
                "By general documents, we mean documents that belong to one of any number of specific genres.",
                "The documents can be presentations, books, book chapters, technical papers, brochures, reports, memos, specifications, letters, announcements, or resumes.",
                "General documents are more widely available in digital libraries, intranets, and internet, and thus investigation on title extraction from them is sorely needed.",
                "Figure 1 shows an estimate on distributions of file formats on intranet and internet [15].",
                "Office and PDF are the main file formats on the intranet.",
                "Even on the internet, the documents in the formats are still not negligible, given its extremely large size.",
                "In this paper, without loss of generality, we take Office documents as an example.",
                "Figure 1.",
                "Distributions of file formats in internet and intranet.",
                "For Office documents, users can define titles as file properties using a feature provided by Office.",
                "We found in an experiment, however, that users seldom use the feature and thus titles in file properties are usually very inaccurate.",
                "That is to say, titles in file properties are usually inconsistent with the true titles in the file bodies that are created by the authors and are visible to readers.",
                "We collected 6,000 Word and 6,000 PowerPoint documents from an intranet and the internet and examined how many titles in the file properties are correct.",
                "We found that surprisingly the accuracy was only 0.265 (cf., Section 6.3 for details).",
                "A number of reasons can be considered.",
                "For example, if one creates a new file by copying an old file, then the file property of the new file will also be copied from the old file.",
                "In another experiment, we found that Google uses the titles in file properties of Office documents in search and browsing, but the titles are not very accurate.",
                "We created 50 queries to search Word and PowerPoint documents and examined the top 15 results of each query returned by Google.",
                "We found that nearly all the titles presented in the search results were from the file properties of the documents.",
                "However, only 0.272 of them were correct.",
                "Actually, true titles usually exist at the beginnings of the bodies of documents.",
                "If we can accurately extract the titles from the bodies of documents, then we can exploit reliable title information in document processing.",
                "This is exactly the problem we address in this paper.",
                "More specifically, given a Word document, we are to extract the title from the top region of the first page.",
                "Given a PowerPoint document, we are to extract the title from the first slide.",
                "A title sometimes consists of a main title and one or two subtitles.",
                "We only consider extraction of the main title.",
                "As baselines for title extraction, we use that of always using the first lines as titles and that of always using the lines with largest font sizes as titles.",
                "Figure 2.",
                "Title extraction from Word document.",
                "Figure 3.",
                "Title extraction from PowerPoint document.",
                "Next, we define a specification for human judgments in title data annotation.",
                "The annotated data will be used in training and testing of the title extraction methods.",
                "Summary of the specification: The title of a document should be identified on the basis of common sense, if there is no difficulty in the identification.",
                "However, there are many cases in which the identification is not easy.",
                "There are some rules defined in the specification that guide identification for such cases.",
                "The rules include a title is usually in consecutive lines in the same format, a document can have no title, titles in images are not considered, a title should not contain words like draft, 147 whitepaper, etc, if it is difficult to determine which is the title, select the one in the largest font size, and if it is still difficult to determine which is the title, select the first candidate. (The specification covers all the cases we have encountered in data annotation.)",
                "Figures 2 and 3 show examples of Office documents from which we conduct title extraction.",
                "In Figure 2, Differences in Win32 API Implementations among Windows Operating Systems is the title of the Word document.",
                "Microsoft Windows on the top of this page is a picture and thus is ignored.",
                "In Figure 3, Building Competitive Advantages through an Agile Infrastructure is the title of the PowerPoint document.",
                "We have developed a tool for annotation of titles by human annotators.",
                "Figure 4 shows a snapshot of the tool.",
                "Figure 4.",
                "Title annotation tool. 4.",
                "TITLE EXTRACTION METHOD 4.1 Outline Title extraction based on machine learning consists of training and extraction.",
                "The same pre-processing step occurs before training and extraction.",
                "During pre-processing, from the top region of the first page of a Word document or the first slide of a PowerPoint document a number of units for processing are extracted.",
                "If a line (lines are separated by return symbols) only has a single format, then the line will become a unit.",
                "If a line has several parts and each of them has its own format, then each part will become a unit.",
                "Each unit will be treated as an instance in learning.",
                "A unit contains not only content information (linguistic information) but also formatting information.",
                "The input to pre-processing is a document and the output of pre-processing is a sequence of units (instances).",
                "Figure 5 shows the units obtained from the document in Figure 2.",
                "Figure 5.",
                "Example of units.",
                "In learning, the input is sequences of units where each sequence corresponds to a document.",
                "We take labeled units (labeled as title_begin, title_end, or other) in the sequences as training data and construct models for identifying whether a unit is title_begin title_end, or other.",
                "We employ four types of models: Perceptron, Maximum Entropy (ME), Perceptron Markov Model (PMM), and Maximum Entropy Markov Model (MEMM).",
                "In extraction, the input is a sequence of units from one document.",
                "We employ one type of model to identify whether a unit is title_begin, title_end, or other.",
                "We then extract units from the unit labeled with title_begin to the unit labeled with title_end.",
                "The result is the extracted title of the document.",
                "The unique characteristic of our approach is that we mainly utilize formatting information for title extraction.",
                "Our assumption is that although general documents vary in styles, their formats have certain patterns and we can learn and utilize the patterns for title extraction.",
                "This is in contrast to the work by Han et al., in which only linguistic features are used for extraction from research papers. 4.2 Models The four models actually can be considered in the same metadata extraction framework.",
                "That is why we apply them together to our current problem.",
                "Each input is a sequence of instances kxxx L21 together with a sequence of labels kyyy L21 . ix and iy represents an instance and its label, respectively ( ki ,,2,1 L= ).",
                "Recall that an instance here represents a unit.",
                "A label represents title_begin, title_end, or other.",
                "Here, k is the number of units in a document.",
                "In learning, we train a model which can be generally denoted as a conditional probability distribution )|( 11 kk XXYYP LL where iX and iY denote random variables taking instance ix and label iy as values, respectively ( ki ,,2,1 L= ).",
                "Learning Tool Extraction Tool 21121 2222122221 1121111211 nknnknn kk kk yyyxxx yyyxxx yyyxxx LL LL LL LL → → → )|(maxarg 11 mkmmkm xxyyP LL )|( 11 kk XXYYP LL Conditional Distribution mkmm xxx L21 Figure 6.",
                "Metadata extraction model.",
                "We can make assumptions about the general model in order to make it simple enough for training. 148 For example, we can assume that kYY ,,1 L are independent of each other given kXX ,,1 L .",
                "Thus, we have )|()|( )|( 11 11 kk kk XYPXYP XXYYP L LL = In this way, we decompose the model into a number of classifiers.",
                "We train the classifiers locally using the labeled data.",
                "As the classifier, we employ the Perceptron or Maximum Entropy model.",
                "We can also assume that the first order Markov property holds for kYY ,,1 L given kXX ,,1 L .",
                "Thus, we have )|()|( )|( 111 11 kkk kk XYYPXYP XXYYP −= L LL Again, we obtain a number of classifiers.",
                "However, the classifiers are conditioned on the previous label.",
                "When we employ the Percepton or Maximum Entropy model as a classifier, the models become a Percepton Markov Model or Maximum Entropy Markov Model, respectively.",
                "That is to say, the two models are more precise.",
                "In extraction, given a new sequence of instances, we resort to one of the constructed models to assign a sequence of labels to the sequence of instances, i.e., perform extraction.",
                "For Perceptron and ME, we assign labels locally and combine the results globally later using heuristics.",
                "Specifically, we first identify the most likely title_begin.",
                "Then we find the most likely title_end within three units after the title_begin.",
                "Finally, we extract as a title the units between the title_begin and the title_end.",
                "For PMM and MEMM, we employ the Viterbi algorithm to find the globally optimal label sequence.",
                "In this paper, for Perceptron, we actually employ an improved variant of it, called Perceptron with Uneven Margin [13].",
                "This version of Perceptron can work well especially when the number of positive instances and the number of negative instances differ greatly, which is exactly the case in our problem.",
                "We also employ an improved version of Perceptron Markov Model in which the Perceptron model is the so-called Voted Perceptron [2].",
                "In addition, in training, the parameters of the model are updated globally rather than locally. 4.3 Features There are two types of features: format features and linguistic features.",
                "We mainly use the former.",
                "The features are used for both the title-begin and the title-end classifiers. 4.3.1 Format Features Font Size: There are four binary features that represent the normalized font size of the unit (recall that a unit has only one type of font).",
                "If the font size of the unit is the largest in the document, then the first feature will be 1, otherwise 0.",
                "If the font size is the smallest in the document, then the fourth feature will be 1, otherwise 0.",
                "If the font size is above the average font size and not the largest in the document, then the second feature will be 1, otherwise 0.",
                "If the font size is below the average font size and not the smallest, the third feature will be 1, otherwise 0.",
                "It is necessary to conduct normalization on font sizes.",
                "For example, in one document the largest font size might be 12pt, while in another the smallest one might be 18pt.",
                "Boldface: This binary feature represents whether or not the current unit is in boldface.",
                "Alignment: There are four binary features that respectively represent the location of the current unit: left, center, right, and unknown alignment.",
                "The following format features with respect to context play an important role in title extraction.",
                "Empty Neighboring Unit: There are two binary features that represent, respectively, whether or not the previous unit and the current unit are blank lines.",
                "Font Size Change: There are two binary features that represent, respectively, whether or not the font size of the previous unit and the font size of the next unit differ from that of the current unit.",
                "Alignment Change: There are two binary features that represent, respectively, whether or not the alignment of the previous unit and the alignment of the next unit differ from that of the current one.",
                "Same Paragraph: There are two binary features that represent, respectively, whether or not the previous unit and the next unit are in the same paragraph as the current unit. 4.3.2 Linguistic Features The linguistic features are based on key words.",
                "Positive Word: This binary feature represents whether or not the current unit begins with one of the positive words.",
                "The positive words include title:, subject:, subject line: For example, in some documents the lines of titles and authors have the same formats.",
                "However, if lines begin with one of the positive words, then it is likely that they are title lines.",
                "Negative Word: This binary feature represents whether or not the current unit begins with one of the negative words.",
                "The negative words include To, By, created by, updated by, etc.",
                "There are more negative words than positive words.",
                "The above linguistic features are language dependent.",
                "Word Count: A title should not be too long.",
                "We heuristically create four intervals: [1, 2], [3, 6], [7, 9] and [9, ∞) and define one feature for each interval.",
                "If the number of words in a title falls into an interval, then the corresponding feature will be 1; otherwise 0.",
                "Ending Character: This feature represents whether the unit ends with :, -, or other special characters.",
                "A title usually does not end with such a character. 5.",
                "DOCUMENT RETRIEVAL METHOD We describe our method of document retrieval using extracted titles.",
                "Typically, in information retrieval a document is split into a number of fields including body, title, and anchor text.",
                "A ranking function in search can use different weights for different fields of 149 the document.",
                "Also, titles are typically assigned high weights, indicating that they are important for document retrieval.",
                "As explained previously, our experiment has shown that a significant number of documents actually have incorrect titles in the file properties, and thus in addition of using them we use the extracted titles as one more field of the document.",
                "By doing this, we attempt to improve the overall precision.",
                "In this paper, we employ a modification of BM25 that allows field weighting [21].",
                "As fields, we make use of body, title, extracted title and anchor.",
                "First, for each term in the query we count the term frequency in each field of the document; each field frequency is then weighted according to the corresponding weight parameter: ∑= f tfft tfwwtf Similarly, we compute the document length as a weighted sum of lengths of each field.",
                "Average document length in the corpus becomes the average of all weighted document lengths. ∑= f ff dlwwdl In our experiments we used 75.0,8.11 == bk .",
                "Weight for content was 1.0, title was 10.0, anchor was 10.0, and extracted title was 5.0. 6.",
                "EXPERIMENTAL RESULTS 6.1 Data Sets and Evaluation Measures We used two data sets in our experiments.",
                "First, we downloaded and randomly selected 5,000 Word documents and 5,000 PowerPoint documents from an intranet of Microsoft.",
                "We call it MS hereafter.",
                "Second, we downloaded and randomly selected 500 Word and 500 PowerPoint documents from the DotGov and DotCom domains on the internet, respectively.",
                "Figure 7 shows the distributions of the genres of the documents.",
                "We see that the documents are indeed general documents as we define them.",
                "Figure 7.",
                "Distributions of document genres.",
                "Third, a data set in Chinese was also downloaded from the internet.",
                "It includes 500 Word documents and 500 PowerPoint documents in Chinese.",
                "We manually labeled the titles of all the documents, on the basis of our specification.",
                "Not all the documents in the two data sets have titles.",
                "Table 1 shows the percentages of the documents having titles.",
                "We see that DotCom and DotGov have more PowerPoint documents with titles than MS.",
                "This might be because PowerPoint documents published on the internet are more formal than those on the intranet.",
                "Table 1.",
                "The portion of documents with titles Domain Type MS DotCom DotGov Word 75.7% 77.8% 75.6% PowerPoint 82.1% 93.4% 96.4% In our experiments, we conducted evaluations on title extraction in terms of precision, recall, and F-measure.",
                "The evaluation measures are defined as follows: Precision: P = A / ( A + B ) Recall: R = A / ( A + C ) F-measure: F1 = 2PR / ( P + R ) Here, A, B, C, and D are numbers of documents as those defined in Table 2.",
                "Table 2.",
                "Contingence table with regard to title extraction Is title Is not title Extracted A B Not extracted C D 6.2 Baselines We test the accuracies of the two baselines described in section 4.2.",
                "They are denoted as largest font size and first line respectively. 6.3 Accuracy of Titles in File Properties We investigate how many titles in the file properties of the documents are reliable.",
                "We view the titles annotated by humans as true titles and test how many titles in the file properties can approximately match with the true titles.",
                "We use Edit Distance to conduct the approximate match. (Approximate match is only used in this evaluation).",
                "This is because sometimes human annotated titles can be slightly different from the titles in file properties on the surface, e.g., contain extra spaces).",
                "Given string A and string B: if ( (D == 0) or ( D / ( La + Lb ) < θ ) ) then string A = string B D: Edit Distance between string A and string B La: length of string A Lb: length of string B θ: 0.1 ∑ × ++− + = t t n N wtf avwdl wdl bbk kwtf FBM )log( ))1(( )1( 25 1 1 150 Table 3.",
                "Accuracies of titles in file properties File Type Domain Precision Recall F1 MS 0.299 0.311 0.305 DotCom 0.210 0.214 0.212Word DotGov 0.182 0.177 0.180 MS 0.229 0.245 0.237 DotCom 0.185 0.186 0.186PowerPoint DotGov 0.180 0.182 0.181 6.4 Comparison with Baselines We conducted title extraction from the first data set (Word and PowerPoint in MS).",
                "As the model, we used Perceptron.",
                "We conduct 4-fold cross validation.",
                "Thus, all the results reported here are those averaged over 4 trials.",
                "Tables 4 and 5 show the results.",
                "We see that Perceptron significantly outperforms the baselines.",
                "In the evaluation, we use exact matching between the true titles annotated by humans and the extracted titles.",
                "Table 4.",
                "Accuracies of title extraction with Word Precision Recall F1 Model Perceptron 0.810 0.837 0.823 Largest font size 0.700 0.758 0.727 Baselines First line 0.707 0.767 0.736 Table 5.",
                "Accuracies of title extraction with PowerPoint Precision Recall F1 Model Perceptron 0.875 0. 895 0.885 Largest font size 0.844 0.887 0.865 Baselines First line 0.639 0.671 0.655 We see that the machine learning approach can achieve good performance in title extraction.",
                "For Word documents both precision and recall of the approach are 8 percent higher than those of the baselines.",
                "For PowerPoint both precision and recall of the approach are 2 percent higher than those of the baselines.",
                "We conduct significance tests.",
                "The results are shown in Table 6.",
                "Here, Largest denotes the baseline of using the largest font size, First denotes the baseline of using the first line.",
                "The results indicate that the improvements of machine learning over baselines are statistically significant (in the sense p-value < 0.05) Table 6.",
                "Sign test results Documents Type Sign test between p-value Perceptron vs. Largest 3.59e-26 Word Perceptron vs. First 7.12e-10 Perceptron vs. Largest 0.010 PowerPoint Perceptron vs. First 5.13e-40 We see, from the results, that the two baselines can work well for title extraction, suggesting that font size and position information are most useful features for title extraction.",
                "However, it is also obvious that using only these two features is not enough.",
                "There are cases in which all the lines have the same font size (i.e., the largest font size), or cases in which the lines with the largest font size only contain general descriptions like Confidential, White paper, etc.",
                "For those cases, the largest font size method cannot work well.",
                "For similar reasons, the first line method alone cannot work well, either.",
                "With the combination of different features (evidence in title judgment), Perceptron can outperform Largest and First.",
                "We investigate the performance of solely using linguistic features.",
                "We found that it does not work well.",
                "It seems that the format features play important roles and the linguistic features are supplements..",
                "Figure 8.",
                "An example Word document.",
                "Figure 9.",
                "An example PowerPoint document.",
                "We conducted an error analysis on the results of Perceptron.",
                "We found that the errors fell into three categories. (1) About one third of the errors were related to hard cases.",
                "In these documents, the layouts of the first pages were difficult to understand, even for humans.",
                "Figure 8 and 9 shows examples. (2) Nearly one fourth of the errors were from the documents which do not have true titles but only contain bullets.",
                "Since we conduct extraction from the top regions, it is difficult to get rid of these errors with the current approach. (3).",
                "Confusions between main titles and subtitles were another type of error.",
                "Since we only labeled the main titles as titles, the extractions of both titles were considered incorrect.",
                "This type of error does little harm to document processing like search, however. 6.5 Comparison between Models To compare the performance of different machine learning models, we conducted another experiment.",
                "Again, we perform 4-fold cross 151 validation on the first data set (MS).",
                "Table 7, 8 shows the results of all the four models.",
                "It turns out that Perceptron and PMM perform the best, followed by MEMM, and ME performs the worst.",
                "In general, the Markovian models perform better than or as well as their classifier counterparts.",
                "This seems to be because the Markovian models are trained globally, while the classifiers are trained locally.",
                "The Perceptron based models perform better than the ME based counterparts.",
                "This seems to be because the Perceptron based models are created to make better classifications, while ME models are constructed for better prediction.",
                "Table 7.",
                "Comparison between different learning models for title extraction with Word Model Precision Recall F1 Perceptron 0.810 0.837 0.823 MEMM 0.797 0.824 0.810 PMM 0.827 0.823 0.825 ME 0.801 0.621 0.699 Table 8.",
                "Comparison between different learning models for title extraction with PowerPoint Model Precision Recall F1 Perceptron 0.875 0. 895 0. 885 MEMM 0.841 0.861 0.851 PMM 0.873 0.896 0.885 ME 0.753 0.766 0.759 6.6 Domain Adaptation We apply the model trained with the first data set (MS) to the second data set (DotCom and DotGov).",
                "Tables 9-12 show the results.",
                "Table 9.",
                "Accuracies of title extraction with Word in DotGov Precision Recall F1 Model Perceptron 0.716 0.759 0.737 Largest font size 0.549 0.619 0.582Baselines First line 0.462 0.521 0.490 Table 10.",
                "Accuracies of title extraction with PowerPoint in DotGov Precision Recall F1 Model Perceptron 0.900 0.906 0.903 Largest font size 0.871 0.888 0.879Baselines First line 0.554 0.564 0.559 Table 11.",
                "Accuracies of title extraction with Word in DotCom Precisio n Recall F1 Model Perceptron 0.832 0.880 0.855 Largest font size 0.676 0.753 0.712Baselines First line 0.577 0.643 0.608 Table 12.",
                "Performance of PowerPoint document title extraction in DotCom Precisio n Recall F1 Model Perceptron 0.910 0.903 0.907 Largest font size 0.864 0.886 0.875Baselines First line 0.570 0.585 0.577 From the results, we see that the models can be adapted to different domains well.",
                "There is almost no drop in accuracy.",
                "The results indicate that the patterns of title formats exist across different domains, and it is possible to construct a domain independent model by mainly using formatting information. 6.7 Language Adaptation We apply the model trained with the data in English (MS) to the data set in Chinese.",
                "Tables 13-14 show the results.",
                "Table 13.",
                "Accuracies of title extraction with Word in Chinese Precision Recall F1 Model Perceptron 0.817 0.805 0.811 Largest font size 0.722 0.755 0.738Baselines First line 0.743 0.777 0.760 Table 14.",
                "Accuracies of title extraction with PowerPoint in Chinese Precision Recall F1 Model Perceptron 0.766 0.812 0.789 Largest font size 0.753 0.813 0.782Baselines First line 0.627 0.676 0.650 We see that the models can be adapted to a different language.",
                "There are only small drops in accuracy.",
                "Obviously, the linguistic features do not work for Chinese, but the effect of not using them is negligible.",
                "The results indicate that the patterns of title formats exist across different languages.",
                "From the domain adaptation and language adaptation results, we conclude that the use of formatting information is the key to a successful extraction from general documents. 6.8 Search with Extracted Titles We performed experiments on using title extraction for document retrieval.",
                "As a baseline, we employed BM25 without using extracted titles.",
                "The ranking mechanism was as described in Section 5.",
                "The weights were heuristically set.",
                "We did not conduct optimization on the weights.",
                "The evaluation was conducted on a corpus of 1.3 M documents crawled from the intranet of Microsoft using 100 evaluation queries obtained from this intranets search engine query logs. 50 queries were from the most popular set, while 50 queries other were chosen randomly.",
                "Users were asked to provide judgments of the degree of document relevance from a scale of 1to 5 (1 meaning detrimental, 2 - bad, 3 - fair, 4 - good and 5 - excellent). 152 Figure 10 shows the results.",
                "In the chart two sets of precision results were obtained by either considering good or excellent documents as relevant (left 3 bars with relevance threshold 0.5), or by considering only excellent documents as relevant (right 3 bars with relevance threshold 1.0) 0 0.05 0.1 0.15 0.2 0.25 0.3 0.35 0.4 0.45 P@10 P@5 Reciprocal P@10 P@5 Reciprocal 0.5 1 BM25 Anchor, Title, Body BM25 Anchor, Title, Body, ExtractedTitle Name All RelevanceThreshold Data Description Figure 10.",
                "Search ranking results.",
                "Figure 10 shows different document retrieval results with different ranking functions in terms of precision @10, precision @5 and reciprocal rank: • Blue bar - BM25 including the fields body, title (file property), and anchor text. • Purple bar - BM25 including the fields body, title (file property), anchor text, and extracted title.",
                "With the additional field of extracted title included in BM25 the precision @10 increased from 0.132 to 0.145, or by ~10%.",
                "Thus, it is safe to say that the use of extracted title can indeed improve the precision of document retrieval. 7.",
                "CONCLUSION In this paper, we have investigated the problem of automatically extracting titles from general documents.",
                "We have tried using a machine learning approach to address the problem.",
                "Previous work showed that the machine learning approach can work well for metadata extraction from research papers.",
                "In this paper, we showed that the approach can work for extraction from general documents as well.",
                "Our experimental results indicated that the machine learning approach can work significantly better than the baselines in title extraction from Office documents.",
                "Previous work on metadata extraction mainly used linguistic features in documents, while we mainly used formatting information.",
                "It appeared that using formatting information is a key for successfully conducting title extraction from general documents.",
                "We tried different machine learning models including Perceptron, Maximum Entropy, Maximum Entropy Markov Model, and Voted Perceptron.",
                "We found that the performance of the Perceptorn models was the best.",
                "We applied models constructed in one domain to another domain and applied models trained in one language to another language.",
                "We found that the accuracies did not drop substantially across different domains and across different languages, indicating that the models were generic.",
                "We also attempted to use the extracted titles in document retrieval.",
                "We observed a significant improvement in document ranking performance for search when using extracted title information.",
                "All the above investigations were not conducted in previous work, and through our investigations we verified the generality and the significance of the title extraction approach. 8.",
                "ACKNOWLEDGEMENTS We thank Chunyu Wei and Bojuan Zhao for their work on data annotation.",
                "We acknowledge Jinzhu Li for his assistance in conducting the experiments.",
                "We thank Ming Zhou, John Chen, Jun Xu, and the anonymous reviewers of JCDL05 for their valuable comments on this paper. 9.",
                "REFERENCES [1] Berger, A. L., Della Pietra, S. A., and Della Pietra, V. J.",
                "A maximum entropy approach to natural language processing.",
                "Computational Linguistics, 22:39-71, 1996. [2] Collins, M. Discriminative training methods for hidden markov models: theory and experiments with perceptron algorithms.",
                "In Proceedings of Conference on Empirical Methods in Natural Language Processing, 1-8, 2002. [3] Cortes, C. and Vapnik, V. Support-vector networks.",
                "Machine Learning, 20:273-297, 1995. [4] Chieu, H. L. and Ng, H. T. A maximum entropy approach to information extraction from semi-structured and free text.",
                "In Proceedings of the Eighteenth National Conference on Artificial Intelligence, 768-791, 2002. [5] Evans, D. K., Klavans, J. L., and McKeown, K. R. Columbia newsblaster: multilingual news summarization on the Web.",
                "In Proceedings of Human Language Technology conference / North American chapter of the Association for Computational Linguistics annual meeting, 1-4, 2004. [6] Ghahramani, Z. and Jordan, M. I. Factorial hidden markov models.",
                "Machine Learning, 29:245-273, 1997. [7] Gheel, J. and Anderson, T. Data and metadata for finding and reminding, In Proceedings of the 1999 International Conference on Information Visualization, 446-451,1999. [8] Giles, C. L., Petinot, Y., Teregowda P. B., Han, H., Lawrence, S., Rangaswamy, A., and Pal, N. eBizSearch: a niche search engine for e-Business.",
                "In Proceedings of the 26th Annual International ACM SIGIR Conference on Research and Development in Information Retrieval, 413414, 2003. [9] Giuffrida, G., Shek, E. C., and Yang, J. Knowledge-based metadata extraction from PostScript files.",
                "In Proceedings of the Fifth ACM Conference on Digital Libraries, 77-84, 2000. [10] Han, H., Giles, C. L., Manavoglu, E., Zha, H., Zhang, Z., and Fox, E. A.",
                "Automatic document metadata extraction using support vector machines.",
                "In Proceedings of the Third ACM/IEEE-CS Joint Conference on Digital Libraries, 37-48, 2003. [11] Kobayashi, M., and Takeda, K. Information retrieval on the Web.",
                "ACM Computing Surveys, 32:144-173, 2000. [12] Lafferty, J., McCallum, A., and Pereira, F. Conditional random fields: probabilistic models for segmenting and 153 labeling sequence data.",
                "In Proceedings of the Eighteenth International Conference on Machine Learning, 282-289, 2001. [13] Li, Y., Zaragoza, H., Herbrich, R., Shawe-Taylor J., and Kandola, J. S. The perceptron algorithm with uneven margins.",
                "In Proceedings of the Nineteenth International Conference on Machine Learning, 379-386, 2002. [14] Liddy, E. D., Sutton, S., Allen, E., Harwell, S., Corieri, S., Yilmazel, O., Ozgencil, N. E., Diekema, A., McCracken, N., and Silverstein, J.",
                "Automatic Metadata generation & evaluation.",
                "In Proceedings of the 25th Annual International ACM SIGIR Conference on Research and Development in Information Retrieval, 401-402, 2002. [15] Littlefield, A.",
                "Effective enterprise information retrieval across new content formats.",
                "In Proceedings of the Seventh Search Engine Conference, http://www.infonortics.com/searchengines/sh02/02prog.html, 2002. [16] Mao, S., Kim, J. W., and Thoma, G. R. A dynamic feature generation system for automated metadata extraction in preservation of digital materials.",
                "In Proceedings of the First International Workshop on Document Image Analysis for Libraries, 225-232, 2004. [17] McCallum, A., Freitag, D., and Pereira, F. Maximum entropy markov models for information extraction and segmentation.",
                "In Proceedings of the Seventeenth International Conference on Machine Learning, 591-598, 2000. [18] Murphy, L. D. Digital document metadata in organizations: roles, analytical approaches, and future research directions.",
                "In Proceedings of the Thirty-First Annual Hawaii International Conference on System Sciences, 267-276, 1998. [19] Pinto, D., McCallum, A., Wei, X., and Croft, W. B.",
                "Table extraction using conditional random fields.",
                "In Proceedings of the 26th Annual International ACM SIGIR Conference on Research and Development in Information Retrieval, 235242, 2003. [20] Ratnaparkhi, A. Unsupervised statistical models for prepositional phrase attachment.",
                "In Proceedings of the Seventeenth International Conference on Computational Linguistics. 1079-1085, 1998. [21] Robertson, S., Zaragoza, H., and Taylor, M. Simple BM25 extension to multiple weighted fields, In Proceedings of ACM Thirteenth Conference on Information and Knowledge Management, 42-49, 2004. [22] Yi, J. and Sundaresan, N. Metadata based Web mining for relevance, In Proceedings of the 2000 International Symposium on Database Engineering & Applications, 113121, 2000. [23] Yilmazel, O., Finneran, C. M., and Liddy, E. D. MetaExtract: An NLP system to automatically assign metadata.",
                "In Proceedings of the 2004 Joint ACM/IEEE Conference on Digital Libraries, 241-242, 2004. [24] Zhang, J. and Dimitroff, A. Internet search engines response to metadata Dublin Core implementation.",
                "Journal of Information Science, 30:310-320, 2004. [25] Zhang, L., Pan, Y., and Zhang, T. Recognising and using named entities: focused named entity recognition using machine learning.",
                "In Proceedings of the 27th Annual International ACM SIGIR Conference on Research and Development in Information Retrieval, 281-288, 2004. [26] http://dublincore.org/groups/corporate/Seattle/ 154"
            ],
            "original_annotated_samples": [],
            "translated_annotated_samples": [],
            "translated_text": "",
            "candidates": [],
            "error": []
        },
        "genre": {
            "translated_key": "género",
            "is_in_text": true,
            "original_annotated_sentences": [
                "Automatic Extraction of Titles from General Documents using Machine Learning Yunhua Hu1 Computer Science Department Xian Jiaotong University No 28, Xianning West Road Xian, China, 710049 yunhuahu@mail.xjtu.edu.cn Hang Li, Yunbo Cao Microsoft Research Asia 5F Sigma Center, No.",
                "49 Zhichun Road, Haidian, Beijing, China, 100080 {hangli,yucao}@microsoft.com Qinghua Zheng Computer Science Department Xian Jiaotong University No 28, Xianning West Road Xian, China, 710049 qhzheng@mail.xjtu.edu.cn Dmitriy Meyerzon Microsoft Corporation One Microsoft Way Redmond, WA, USA, 98052 dmitriym@microsoft.com ABSTRACT In this paper, we propose a machine learning approach to title extraction from general documents.",
                "By general documents, we mean documents that can belong to any one of a number of specific <br>genre</br>s, including presentations, book chapters, technical papers, brochures, reports, and letters.",
                "Previously, methods have been proposed mainly for title extraction from research papers.",
                "It has not been clear whether it could be possible to conduct automatic title extraction from general documents.",
                "As a case study, we consider extraction from Office including Word and PowerPoint.",
                "In our approach, we annotate titles in sample documents (for Word and PowerPoint respectively) and take them as training data, train machine learning models, and perform title extraction using the trained models.",
                "Our method is unique in that we mainly utilize formatting information such as font size as features in the models.",
                "It turns out that the use of formatting information can lead to quite accurate extraction from general documents.",
                "Precision and recall for title extraction from Word is 0.810 and 0.837 respectively, and precision and recall for title extraction from PowerPoint is 0.875 and 0.895 respectively in an experiment on intranet data.",
                "Other important new findings in this work include that we can train models in one domain and apply them to another domain, and more surprisingly we can even train models in one language and apply them to another language.",
                "Moreover, we can significantly improve search ranking results in document retrieval by using the extracted titles.",
                "Categories and Subject Descriptors H.3.3 [Information Storage and Retrieval]: Information Search and Retrieval - Search Process; H.4.1 [Information Systems Applications]: Office Automation - Word processing; D.2.8 [Software Engineering]: Metrics - complexity measures, performance measures General Terms Algorithms, Experimentation, Performance. 1.",
                "INTRODUCTION Metadata of documents is useful for many kinds of document processing such as search, browsing, and filtering.",
                "Ideally, metadata is defined by the authors of documents and is then used by various systems.",
                "However, people seldom define document metadata by themselves, even when they have convenient metadata definition tools [26].",
                "Thus, how to automatically extract metadata from the bodies of documents turns out to be an important research issue.",
                "Methods for performing the task have been proposed.",
                "However, the focus was mainly on extraction from research papers.",
                "For instance, Han et al. [10] proposed a machine learning based method to conduct extraction from research papers.",
                "They formalized the problem as that of classification and employed Support Vector Machines as the classifier.",
                "They mainly used linguistic features in the model.1 In this paper, we consider metadata extraction from general documents.",
                "By general documents, we mean documents that may belong to any one of a number of specific <br>genre</br>s.",
                "General documents are more widely available in digital libraries, intranets and the internet, and thus investigation on extraction from them is sorely needed.",
                "Research papers usually have well-formed styles and noticeable characteristics.",
                "In contrast, the styles of general documents can vary greatly.",
                "It has not been clarified whether a machine learning based approach can work well for this task.",
                "There are many types of metadata: title, author, date of creation, etc.",
                "As a case study, we consider title extraction in this paper.",
                "General documents can be in many different file formats: Microsoft Office, PDF (PS), etc.",
                "As a case study, we consider extraction from Office including Word and PowerPoint.",
                "We take a machine learning approach.",
                "We annotate titles in sample documents (for Word and PowerPoint respectively) and take them as training data to train several types of models, and perform title extraction using any one type of the trained models.",
                "In the models, we mainly utilize formatting information such as font size as features.",
                "We employ the following models: Maximum Entropy Model, Perceptron with Uneven Margins, Maximum Entropy Markov Model, and Voted Perceptron.",
                "In this paper, we also investigate the following three problems, which did not seem to have been examined previously. (1) Comparison between models: among the models above, which model performs best for title extraction; (2) Generality of model: whether it is possible to train a model on one domain and apply it to another domain, and whether it is possible to train a model in one language and apply it to another language; (3) Usefulness of extracted titles: whether extracted titles can improve document processing such as search.",
                "Experimental results indicate that our approach works well for title extraction from general documents.",
                "Our method can significantly outperform the baselines: one that always uses the first lines as titles and the other that always uses the lines in the largest font sizes as titles.",
                "Precision and recall for title extraction from Word are 0.810 and 0.837 respectively, and precision and recall for title extraction from PowerPoint are 0.875 and 0.895 respectively.",
                "It turns out that the use of format features is the key to successful title extraction. (1) We have observed that Perceptron based models perform better in terms of extraction accuracies. (2) We have empirically verified that the models trained with our approach are generic in the sense that they can be trained on one domain and applied to another, and they can be trained in one language and applied to another. (3) We have found that using the extracted titles we can significantly improve precision of document retrieval (by 10%).",
                "We conclude that we can indeed conduct reliable title extraction from general documents and use the extracted results to improve real applications.",
                "The rest of the paper is organized as follows.",
                "In section 2, we introduce related work, and in section 3, we explain the motivation and problem setting of our work.",
                "In section 4, we describe our method of title extraction, and in section 5, we describe our method of document retrieval using extracted titles.",
                "Section 6 gives our experimental results.",
                "We make concluding remarks in section 7. 2.",
                "RELATED WORK 2.1 Document Metadata Extraction Methods have been proposed for performing automatic metadata extraction from documents; however, the main focus was on extraction from research papers.",
                "The proposed methods fall into two categories: the rule based approach and the machine learning based approach.",
                "Giuffrida et al. [9], for instance, developed a rule-based system for automatically extracting metadata from research papers in Postscript.",
                "They used rules like titles are usually located on the upper portions of the first pages and they are usually in the largest font sizes.",
                "Liddy et al. [14] and Yilmazel el al. [23] performed metadata extraction from educational materials using rule-based natural language processing technologies.",
                "Mao et al. [16] also conducted automatic metadata extraction from research papers using rules on formatting information.",
                "The rule-based approach can achieve high performance.",
                "However, it also has disadvantages.",
                "It is less adaptive and robust when compared with the machine learning approach.",
                "Han et al. [10], for instance, conducted metadata extraction with the machine learning approach.",
                "They viewed the problem as that of classifying the lines in a document into the categories of metadata and proposed using Support Vector Machines as the classifier.",
                "They mainly used linguistic information as features.",
                "They reported high extraction accuracy from research papers in terms of precision and recall. 2.2 Information Extraction Metadata extraction can be viewed as an application of information extraction, in which given a sequence of instances, we identify a subsequence that represents information in which we are interested.",
                "Hidden Markov Model [6], Maximum Entropy Model [1, 4], Maximum Entropy Markov Model [17], Support Vector Machines [3], Conditional Random Field [12], and Voted Perceptron [2] are widely used information extraction models.",
                "Information extraction has been applied, for instance, to part-ofspeech tagging [20], named entity recognition [25] and table extraction [19]. 2.3 Search Using Title Information Title information is useful for document retrieval.",
                "In the system Citeseer, for instance, Giles et al. managed to extract titles from research papers and make use of the extracted titles in metadata search of papers [8].",
                "In web search, the title fields (i.e., file properties) and anchor texts of web pages (HTML documents) can be viewed as titles of the pages [5].",
                "Many search engines seem to utilize them for web page retrieval [7, 11, 18, 22].",
                "Zhang et al., found that web pages with well-defined metadata are more easily retrieved than those without well-defined metadata [24].",
                "To the best of our knowledge, no research has been conducted on using extracted titles from general documents (e.g., Office documents) for search of the documents. 146 3.",
                "MOTIVATION AND PROBLEM SETTING We consider the issue of automatically extracting titles from general documents.",
                "By general documents, we mean documents that belong to one of any number of specific <br>genre</br>s.",
                "The documents can be presentations, books, book chapters, technical papers, brochures, reports, memos, specifications, letters, announcements, or resumes.",
                "General documents are more widely available in digital libraries, intranets, and internet, and thus investigation on title extraction from them is sorely needed.",
                "Figure 1 shows an estimate on distributions of file formats on intranet and internet [15].",
                "Office and PDF are the main file formats on the intranet.",
                "Even on the internet, the documents in the formats are still not negligible, given its extremely large size.",
                "In this paper, without loss of generality, we take Office documents as an example.",
                "Figure 1.",
                "Distributions of file formats in internet and intranet.",
                "For Office documents, users can define titles as file properties using a feature provided by Office.",
                "We found in an experiment, however, that users seldom use the feature and thus titles in file properties are usually very inaccurate.",
                "That is to say, titles in file properties are usually inconsistent with the true titles in the file bodies that are created by the authors and are visible to readers.",
                "We collected 6,000 Word and 6,000 PowerPoint documents from an intranet and the internet and examined how many titles in the file properties are correct.",
                "We found that surprisingly the accuracy was only 0.265 (cf., Section 6.3 for details).",
                "A number of reasons can be considered.",
                "For example, if one creates a new file by copying an old file, then the file property of the new file will also be copied from the old file.",
                "In another experiment, we found that Google uses the titles in file properties of Office documents in search and browsing, but the titles are not very accurate.",
                "We created 50 queries to search Word and PowerPoint documents and examined the top 15 results of each query returned by Google.",
                "We found that nearly all the titles presented in the search results were from the file properties of the documents.",
                "However, only 0.272 of them were correct.",
                "Actually, true titles usually exist at the beginnings of the bodies of documents.",
                "If we can accurately extract the titles from the bodies of documents, then we can exploit reliable title information in document processing.",
                "This is exactly the problem we address in this paper.",
                "More specifically, given a Word document, we are to extract the title from the top region of the first page.",
                "Given a PowerPoint document, we are to extract the title from the first slide.",
                "A title sometimes consists of a main title and one or two subtitles.",
                "We only consider extraction of the main title.",
                "As baselines for title extraction, we use that of always using the first lines as titles and that of always using the lines with largest font sizes as titles.",
                "Figure 2.",
                "Title extraction from Word document.",
                "Figure 3.",
                "Title extraction from PowerPoint document.",
                "Next, we define a specification for human judgments in title data annotation.",
                "The annotated data will be used in training and testing of the title extraction methods.",
                "Summary of the specification: The title of a document should be identified on the basis of common sense, if there is no difficulty in the identification.",
                "However, there are many cases in which the identification is not easy.",
                "There are some rules defined in the specification that guide identification for such cases.",
                "The rules include a title is usually in consecutive lines in the same format, a document can have no title, titles in images are not considered, a title should not contain words like draft, 147 whitepaper, etc, if it is difficult to determine which is the title, select the one in the largest font size, and if it is still difficult to determine which is the title, select the first candidate. (The specification covers all the cases we have encountered in data annotation.)",
                "Figures 2 and 3 show examples of Office documents from which we conduct title extraction.",
                "In Figure 2, Differences in Win32 API Implementations among Windows Operating Systems is the title of the Word document.",
                "Microsoft Windows on the top of this page is a picture and thus is ignored.",
                "In Figure 3, Building Competitive Advantages through an Agile Infrastructure is the title of the PowerPoint document.",
                "We have developed a tool for annotation of titles by human annotators.",
                "Figure 4 shows a snapshot of the tool.",
                "Figure 4.",
                "Title annotation tool. 4.",
                "TITLE EXTRACTION METHOD 4.1 Outline Title extraction based on machine learning consists of training and extraction.",
                "The same pre-processing step occurs before training and extraction.",
                "During pre-processing, from the top region of the first page of a Word document or the first slide of a PowerPoint document a number of units for processing are extracted.",
                "If a line (lines are separated by return symbols) only has a single format, then the line will become a unit.",
                "If a line has several parts and each of them has its own format, then each part will become a unit.",
                "Each unit will be treated as an instance in learning.",
                "A unit contains not only content information (linguistic information) but also formatting information.",
                "The input to pre-processing is a document and the output of pre-processing is a sequence of units (instances).",
                "Figure 5 shows the units obtained from the document in Figure 2.",
                "Figure 5.",
                "Example of units.",
                "In learning, the input is sequences of units where each sequence corresponds to a document.",
                "We take labeled units (labeled as title_begin, title_end, or other) in the sequences as training data and construct models for identifying whether a unit is title_begin title_end, or other.",
                "We employ four types of models: Perceptron, Maximum Entropy (ME), Perceptron Markov Model (PMM), and Maximum Entropy Markov Model (MEMM).",
                "In extraction, the input is a sequence of units from one document.",
                "We employ one type of model to identify whether a unit is title_begin, title_end, or other.",
                "We then extract units from the unit labeled with title_begin to the unit labeled with title_end.",
                "The result is the extracted title of the document.",
                "The unique characteristic of our approach is that we mainly utilize formatting information for title extraction.",
                "Our assumption is that although general documents vary in styles, their formats have certain patterns and we can learn and utilize the patterns for title extraction.",
                "This is in contrast to the work by Han et al., in which only linguistic features are used for extraction from research papers. 4.2 Models The four models actually can be considered in the same metadata extraction framework.",
                "That is why we apply them together to our current problem.",
                "Each input is a sequence of instances kxxx L21 together with a sequence of labels kyyy L21 . ix and iy represents an instance and its label, respectively ( ki ,,2,1 L= ).",
                "Recall that an instance here represents a unit.",
                "A label represents title_begin, title_end, or other.",
                "Here, k is the number of units in a document.",
                "In learning, we train a model which can be generally denoted as a conditional probability distribution )|( 11 kk XXYYP LL where iX and iY denote random variables taking instance ix and label iy as values, respectively ( ki ,,2,1 L= ).",
                "Learning Tool Extraction Tool 21121 2222122221 1121111211 nknnknn kk kk yyyxxx yyyxxx yyyxxx LL LL LL LL → → → )|(maxarg 11 mkmmkm xxyyP LL )|( 11 kk XXYYP LL Conditional Distribution mkmm xxx L21 Figure 6.",
                "Metadata extraction model.",
                "We can make assumptions about the general model in order to make it simple enough for training. 148 For example, we can assume that kYY ,,1 L are independent of each other given kXX ,,1 L .",
                "Thus, we have )|()|( )|( 11 11 kk kk XYPXYP XXYYP L LL = In this way, we decompose the model into a number of classifiers.",
                "We train the classifiers locally using the labeled data.",
                "As the classifier, we employ the Perceptron or Maximum Entropy model.",
                "We can also assume that the first order Markov property holds for kYY ,,1 L given kXX ,,1 L .",
                "Thus, we have )|()|( )|( 111 11 kkk kk XYYPXYP XXYYP −= L LL Again, we obtain a number of classifiers.",
                "However, the classifiers are conditioned on the previous label.",
                "When we employ the Percepton or Maximum Entropy model as a classifier, the models become a Percepton Markov Model or Maximum Entropy Markov Model, respectively.",
                "That is to say, the two models are more precise.",
                "In extraction, given a new sequence of instances, we resort to one of the constructed models to assign a sequence of labels to the sequence of instances, i.e., perform extraction.",
                "For Perceptron and ME, we assign labels locally and combine the results globally later using heuristics.",
                "Specifically, we first identify the most likely title_begin.",
                "Then we find the most likely title_end within three units after the title_begin.",
                "Finally, we extract as a title the units between the title_begin and the title_end.",
                "For PMM and MEMM, we employ the Viterbi algorithm to find the globally optimal label sequence.",
                "In this paper, for Perceptron, we actually employ an improved variant of it, called Perceptron with Uneven Margin [13].",
                "This version of Perceptron can work well especially when the number of positive instances and the number of negative instances differ greatly, which is exactly the case in our problem.",
                "We also employ an improved version of Perceptron Markov Model in which the Perceptron model is the so-called Voted Perceptron [2].",
                "In addition, in training, the parameters of the model are updated globally rather than locally. 4.3 Features There are two types of features: format features and linguistic features.",
                "We mainly use the former.",
                "The features are used for both the title-begin and the title-end classifiers. 4.3.1 Format Features Font Size: There are four binary features that represent the normalized font size of the unit (recall that a unit has only one type of font).",
                "If the font size of the unit is the largest in the document, then the first feature will be 1, otherwise 0.",
                "If the font size is the smallest in the document, then the fourth feature will be 1, otherwise 0.",
                "If the font size is above the average font size and not the largest in the document, then the second feature will be 1, otherwise 0.",
                "If the font size is below the average font size and not the smallest, the third feature will be 1, otherwise 0.",
                "It is necessary to conduct normalization on font sizes.",
                "For example, in one document the largest font size might be 12pt, while in another the smallest one might be 18pt.",
                "Boldface: This binary feature represents whether or not the current unit is in boldface.",
                "Alignment: There are four binary features that respectively represent the location of the current unit: left, center, right, and unknown alignment.",
                "The following format features with respect to context play an important role in title extraction.",
                "Empty Neighboring Unit: There are two binary features that represent, respectively, whether or not the previous unit and the current unit are blank lines.",
                "Font Size Change: There are two binary features that represent, respectively, whether or not the font size of the previous unit and the font size of the next unit differ from that of the current unit.",
                "Alignment Change: There are two binary features that represent, respectively, whether or not the alignment of the previous unit and the alignment of the next unit differ from that of the current one.",
                "Same Paragraph: There are two binary features that represent, respectively, whether or not the previous unit and the next unit are in the same paragraph as the current unit. 4.3.2 Linguistic Features The linguistic features are based on key words.",
                "Positive Word: This binary feature represents whether or not the current unit begins with one of the positive words.",
                "The positive words include title:, subject:, subject line: For example, in some documents the lines of titles and authors have the same formats.",
                "However, if lines begin with one of the positive words, then it is likely that they are title lines.",
                "Negative Word: This binary feature represents whether or not the current unit begins with one of the negative words.",
                "The negative words include To, By, created by, updated by, etc.",
                "There are more negative words than positive words.",
                "The above linguistic features are language dependent.",
                "Word Count: A title should not be too long.",
                "We heuristically create four intervals: [1, 2], [3, 6], [7, 9] and [9, ∞) and define one feature for each interval.",
                "If the number of words in a title falls into an interval, then the corresponding feature will be 1; otherwise 0.",
                "Ending Character: This feature represents whether the unit ends with :, -, or other special characters.",
                "A title usually does not end with such a character. 5.",
                "DOCUMENT RETRIEVAL METHOD We describe our method of document retrieval using extracted titles.",
                "Typically, in information retrieval a document is split into a number of fields including body, title, and anchor text.",
                "A ranking function in search can use different weights for different fields of 149 the document.",
                "Also, titles are typically assigned high weights, indicating that they are important for document retrieval.",
                "As explained previously, our experiment has shown that a significant number of documents actually have incorrect titles in the file properties, and thus in addition of using them we use the extracted titles as one more field of the document.",
                "By doing this, we attempt to improve the overall precision.",
                "In this paper, we employ a modification of BM25 that allows field weighting [21].",
                "As fields, we make use of body, title, extracted title and anchor.",
                "First, for each term in the query we count the term frequency in each field of the document; each field frequency is then weighted according to the corresponding weight parameter: ∑= f tfft tfwwtf Similarly, we compute the document length as a weighted sum of lengths of each field.",
                "Average document length in the corpus becomes the average of all weighted document lengths. ∑= f ff dlwwdl In our experiments we used 75.0,8.11 == bk .",
                "Weight for content was 1.0, title was 10.0, anchor was 10.0, and extracted title was 5.0. 6.",
                "EXPERIMENTAL RESULTS 6.1 Data Sets and Evaluation Measures We used two data sets in our experiments.",
                "First, we downloaded and randomly selected 5,000 Word documents and 5,000 PowerPoint documents from an intranet of Microsoft.",
                "We call it MS hereafter.",
                "Second, we downloaded and randomly selected 500 Word and 500 PowerPoint documents from the DotGov and DotCom domains on the internet, respectively.",
                "Figure 7 shows the distributions of the <br>genre</br>s of the documents.",
                "We see that the documents are indeed general documents as we define them.",
                "Figure 7.",
                "Distributions of document <br>genre</br>s.",
                "Third, a data set in Chinese was also downloaded from the internet.",
                "It includes 500 Word documents and 500 PowerPoint documents in Chinese.",
                "We manually labeled the titles of all the documents, on the basis of our specification.",
                "Not all the documents in the two data sets have titles.",
                "Table 1 shows the percentages of the documents having titles.",
                "We see that DotCom and DotGov have more PowerPoint documents with titles than MS.",
                "This might be because PowerPoint documents published on the internet are more formal than those on the intranet.",
                "Table 1.",
                "The portion of documents with titles Domain Type MS DotCom DotGov Word 75.7% 77.8% 75.6% PowerPoint 82.1% 93.4% 96.4% In our experiments, we conducted evaluations on title extraction in terms of precision, recall, and F-measure.",
                "The evaluation measures are defined as follows: Precision: P = A / ( A + B ) Recall: R = A / ( A + C ) F-measure: F1 = 2PR / ( P + R ) Here, A, B, C, and D are numbers of documents as those defined in Table 2.",
                "Table 2.",
                "Contingence table with regard to title extraction Is title Is not title Extracted A B Not extracted C D 6.2 Baselines We test the accuracies of the two baselines described in section 4.2.",
                "They are denoted as largest font size and first line respectively. 6.3 Accuracy of Titles in File Properties We investigate how many titles in the file properties of the documents are reliable.",
                "We view the titles annotated by humans as true titles and test how many titles in the file properties can approximately match with the true titles.",
                "We use Edit Distance to conduct the approximate match. (Approximate match is only used in this evaluation).",
                "This is because sometimes human annotated titles can be slightly different from the titles in file properties on the surface, e.g., contain extra spaces).",
                "Given string A and string B: if ( (D == 0) or ( D / ( La + Lb ) < θ ) ) then string A = string B D: Edit Distance between string A and string B La: length of string A Lb: length of string B θ: 0.1 ∑ × ++− + = t t n N wtf avwdl wdl bbk kwtf FBM )log( ))1(( )1( 25 1 1 150 Table 3.",
                "Accuracies of titles in file properties File Type Domain Precision Recall F1 MS 0.299 0.311 0.305 DotCom 0.210 0.214 0.212Word DotGov 0.182 0.177 0.180 MS 0.229 0.245 0.237 DotCom 0.185 0.186 0.186PowerPoint DotGov 0.180 0.182 0.181 6.4 Comparison with Baselines We conducted title extraction from the first data set (Word and PowerPoint in MS).",
                "As the model, we used Perceptron.",
                "We conduct 4-fold cross validation.",
                "Thus, all the results reported here are those averaged over 4 trials.",
                "Tables 4 and 5 show the results.",
                "We see that Perceptron significantly outperforms the baselines.",
                "In the evaluation, we use exact matching between the true titles annotated by humans and the extracted titles.",
                "Table 4.",
                "Accuracies of title extraction with Word Precision Recall F1 Model Perceptron 0.810 0.837 0.823 Largest font size 0.700 0.758 0.727 Baselines First line 0.707 0.767 0.736 Table 5.",
                "Accuracies of title extraction with PowerPoint Precision Recall F1 Model Perceptron 0.875 0. 895 0.885 Largest font size 0.844 0.887 0.865 Baselines First line 0.639 0.671 0.655 We see that the machine learning approach can achieve good performance in title extraction.",
                "For Word documents both precision and recall of the approach are 8 percent higher than those of the baselines.",
                "For PowerPoint both precision and recall of the approach are 2 percent higher than those of the baselines.",
                "We conduct significance tests.",
                "The results are shown in Table 6.",
                "Here, Largest denotes the baseline of using the largest font size, First denotes the baseline of using the first line.",
                "The results indicate that the improvements of machine learning over baselines are statistically significant (in the sense p-value < 0.05) Table 6.",
                "Sign test results Documents Type Sign test between p-value Perceptron vs. Largest 3.59e-26 Word Perceptron vs. First 7.12e-10 Perceptron vs. Largest 0.010 PowerPoint Perceptron vs. First 5.13e-40 We see, from the results, that the two baselines can work well for title extraction, suggesting that font size and position information are most useful features for title extraction.",
                "However, it is also obvious that using only these two features is not enough.",
                "There are cases in which all the lines have the same font size (i.e., the largest font size), or cases in which the lines with the largest font size only contain general descriptions like Confidential, White paper, etc.",
                "For those cases, the largest font size method cannot work well.",
                "For similar reasons, the first line method alone cannot work well, either.",
                "With the combination of different features (evidence in title judgment), Perceptron can outperform Largest and First.",
                "We investigate the performance of solely using linguistic features.",
                "We found that it does not work well.",
                "It seems that the format features play important roles and the linguistic features are supplements..",
                "Figure 8.",
                "An example Word document.",
                "Figure 9.",
                "An example PowerPoint document.",
                "We conducted an error analysis on the results of Perceptron.",
                "We found that the errors fell into three categories. (1) About one third of the errors were related to hard cases.",
                "In these documents, the layouts of the first pages were difficult to understand, even for humans.",
                "Figure 8 and 9 shows examples. (2) Nearly one fourth of the errors were from the documents which do not have true titles but only contain bullets.",
                "Since we conduct extraction from the top regions, it is difficult to get rid of these errors with the current approach. (3).",
                "Confusions between main titles and subtitles were another type of error.",
                "Since we only labeled the main titles as titles, the extractions of both titles were considered incorrect.",
                "This type of error does little harm to document processing like search, however. 6.5 Comparison between Models To compare the performance of different machine learning models, we conducted another experiment.",
                "Again, we perform 4-fold cross 151 validation on the first data set (MS).",
                "Table 7, 8 shows the results of all the four models.",
                "It turns out that Perceptron and PMM perform the best, followed by MEMM, and ME performs the worst.",
                "In general, the Markovian models perform better than or as well as their classifier counterparts.",
                "This seems to be because the Markovian models are trained globally, while the classifiers are trained locally.",
                "The Perceptron based models perform better than the ME based counterparts.",
                "This seems to be because the Perceptron based models are created to make better classifications, while ME models are constructed for better prediction.",
                "Table 7.",
                "Comparison between different learning models for title extraction with Word Model Precision Recall F1 Perceptron 0.810 0.837 0.823 MEMM 0.797 0.824 0.810 PMM 0.827 0.823 0.825 ME 0.801 0.621 0.699 Table 8.",
                "Comparison between different learning models for title extraction with PowerPoint Model Precision Recall F1 Perceptron 0.875 0. 895 0. 885 MEMM 0.841 0.861 0.851 PMM 0.873 0.896 0.885 ME 0.753 0.766 0.759 6.6 Domain Adaptation We apply the model trained with the first data set (MS) to the second data set (DotCom and DotGov).",
                "Tables 9-12 show the results.",
                "Table 9.",
                "Accuracies of title extraction with Word in DotGov Precision Recall F1 Model Perceptron 0.716 0.759 0.737 Largest font size 0.549 0.619 0.582Baselines First line 0.462 0.521 0.490 Table 10.",
                "Accuracies of title extraction with PowerPoint in DotGov Precision Recall F1 Model Perceptron 0.900 0.906 0.903 Largest font size 0.871 0.888 0.879Baselines First line 0.554 0.564 0.559 Table 11.",
                "Accuracies of title extraction with Word in DotCom Precisio n Recall F1 Model Perceptron 0.832 0.880 0.855 Largest font size 0.676 0.753 0.712Baselines First line 0.577 0.643 0.608 Table 12.",
                "Performance of PowerPoint document title extraction in DotCom Precisio n Recall F1 Model Perceptron 0.910 0.903 0.907 Largest font size 0.864 0.886 0.875Baselines First line 0.570 0.585 0.577 From the results, we see that the models can be adapted to different domains well.",
                "There is almost no drop in accuracy.",
                "The results indicate that the patterns of title formats exist across different domains, and it is possible to construct a domain independent model by mainly using formatting information. 6.7 Language Adaptation We apply the model trained with the data in English (MS) to the data set in Chinese.",
                "Tables 13-14 show the results.",
                "Table 13.",
                "Accuracies of title extraction with Word in Chinese Precision Recall F1 Model Perceptron 0.817 0.805 0.811 Largest font size 0.722 0.755 0.738Baselines First line 0.743 0.777 0.760 Table 14.",
                "Accuracies of title extraction with PowerPoint in Chinese Precision Recall F1 Model Perceptron 0.766 0.812 0.789 Largest font size 0.753 0.813 0.782Baselines First line 0.627 0.676 0.650 We see that the models can be adapted to a different language.",
                "There are only small drops in accuracy.",
                "Obviously, the linguistic features do not work for Chinese, but the effect of not using them is negligible.",
                "The results indicate that the patterns of title formats exist across different languages.",
                "From the domain adaptation and language adaptation results, we conclude that the use of formatting information is the key to a successful extraction from general documents. 6.8 Search with Extracted Titles We performed experiments on using title extraction for document retrieval.",
                "As a baseline, we employed BM25 without using extracted titles.",
                "The ranking mechanism was as described in Section 5.",
                "The weights were heuristically set.",
                "We did not conduct optimization on the weights.",
                "The evaluation was conducted on a corpus of 1.3 M documents crawled from the intranet of Microsoft using 100 evaluation queries obtained from this intranets search engine query logs. 50 queries were from the most popular set, while 50 queries other were chosen randomly.",
                "Users were asked to provide judgments of the degree of document relevance from a scale of 1to 5 (1 meaning detrimental, 2 - bad, 3 - fair, 4 - good and 5 - excellent). 152 Figure 10 shows the results.",
                "In the chart two sets of precision results were obtained by either considering good or excellent documents as relevant (left 3 bars with relevance threshold 0.5), or by considering only excellent documents as relevant (right 3 bars with relevance threshold 1.0) 0 0.05 0.1 0.15 0.2 0.25 0.3 0.35 0.4 0.45 P@10 P@5 Reciprocal P@10 P@5 Reciprocal 0.5 1 BM25 Anchor, Title, Body BM25 Anchor, Title, Body, ExtractedTitle Name All RelevanceThreshold Data Description Figure 10.",
                "Search ranking results.",
                "Figure 10 shows different document retrieval results with different ranking functions in terms of precision @10, precision @5 and reciprocal rank: • Blue bar - BM25 including the fields body, title (file property), and anchor text. • Purple bar - BM25 including the fields body, title (file property), anchor text, and extracted title.",
                "With the additional field of extracted title included in BM25 the precision @10 increased from 0.132 to 0.145, or by ~10%.",
                "Thus, it is safe to say that the use of extracted title can indeed improve the precision of document retrieval. 7.",
                "CONCLUSION In this paper, we have investigated the problem of automatically extracting titles from general documents.",
                "We have tried using a machine learning approach to address the problem.",
                "Previous work showed that the machine learning approach can work well for metadata extraction from research papers.",
                "In this paper, we showed that the approach can work for extraction from general documents as well.",
                "Our experimental results indicated that the machine learning approach can work significantly better than the baselines in title extraction from Office documents.",
                "Previous work on metadata extraction mainly used linguistic features in documents, while we mainly used formatting information.",
                "It appeared that using formatting information is a key for successfully conducting title extraction from general documents.",
                "We tried different machine learning models including Perceptron, Maximum Entropy, Maximum Entropy Markov Model, and Voted Perceptron.",
                "We found that the performance of the Perceptorn models was the best.",
                "We applied models constructed in one domain to another domain and applied models trained in one language to another language.",
                "We found that the accuracies did not drop substantially across different domains and across different languages, indicating that the models were generic.",
                "We also attempted to use the extracted titles in document retrieval.",
                "We observed a significant improvement in document ranking performance for search when using extracted title information.",
                "All the above investigations were not conducted in previous work, and through our investigations we verified the generality and the significance of the title extraction approach. 8.",
                "ACKNOWLEDGEMENTS We thank Chunyu Wei and Bojuan Zhao for their work on data annotation.",
                "We acknowledge Jinzhu Li for his assistance in conducting the experiments.",
                "We thank Ming Zhou, John Chen, Jun Xu, and the anonymous reviewers of JCDL05 for their valuable comments on this paper. 9.",
                "REFERENCES [1] Berger, A. L., Della Pietra, S. A., and Della Pietra, V. J.",
                "A maximum entropy approach to natural language processing.",
                "Computational Linguistics, 22:39-71, 1996. [2] Collins, M. Discriminative training methods for hidden markov models: theory and experiments with perceptron algorithms.",
                "In Proceedings of Conference on Empirical Methods in Natural Language Processing, 1-8, 2002. [3] Cortes, C. and Vapnik, V. Support-vector networks.",
                "Machine Learning, 20:273-297, 1995. [4] Chieu, H. L. and Ng, H. T. A maximum entropy approach to information extraction from semi-structured and free text.",
                "In Proceedings of the Eighteenth National Conference on Artificial Intelligence, 768-791, 2002. [5] Evans, D. K., Klavans, J. L., and McKeown, K. R. Columbia newsblaster: multilingual news summarization on the Web.",
                "In Proceedings of Human Language Technology conference / North American chapter of the Association for Computational Linguistics annual meeting, 1-4, 2004. [6] Ghahramani, Z. and Jordan, M. I. Factorial hidden markov models.",
                "Machine Learning, 29:245-273, 1997. [7] Gheel, J. and Anderson, T. Data and metadata for finding and reminding, In Proceedings of the 1999 International Conference on Information Visualization, 446-451,1999. [8] Giles, C. L., Petinot, Y., Teregowda P. B., Han, H., Lawrence, S., Rangaswamy, A., and Pal, N. eBizSearch: a niche search engine for e-Business.",
                "In Proceedings of the 26th Annual International ACM SIGIR Conference on Research and Development in Information Retrieval, 413414, 2003. [9] Giuffrida, G., Shek, E. C., and Yang, J. Knowledge-based metadata extraction from PostScript files.",
                "In Proceedings of the Fifth ACM Conference on Digital Libraries, 77-84, 2000. [10] Han, H., Giles, C. L., Manavoglu, E., Zha, H., Zhang, Z., and Fox, E. A.",
                "Automatic document metadata extraction using support vector machines.",
                "In Proceedings of the Third ACM/IEEE-CS Joint Conference on Digital Libraries, 37-48, 2003. [11] Kobayashi, M., and Takeda, K. Information retrieval on the Web.",
                "ACM Computing Surveys, 32:144-173, 2000. [12] Lafferty, J., McCallum, A., and Pereira, F. Conditional random fields: probabilistic models for segmenting and 153 labeling sequence data.",
                "In Proceedings of the Eighteenth International Conference on Machine Learning, 282-289, 2001. [13] Li, Y., Zaragoza, H., Herbrich, R., Shawe-Taylor J., and Kandola, J. S. The perceptron algorithm with uneven margins.",
                "In Proceedings of the Nineteenth International Conference on Machine Learning, 379-386, 2002. [14] Liddy, E. D., Sutton, S., Allen, E., Harwell, S., Corieri, S., Yilmazel, O., Ozgencil, N. E., Diekema, A., McCracken, N., and Silverstein, J.",
                "Automatic Metadata generation & evaluation.",
                "In Proceedings of the 25th Annual International ACM SIGIR Conference on Research and Development in Information Retrieval, 401-402, 2002. [15] Littlefield, A.",
                "Effective enterprise information retrieval across new content formats.",
                "In Proceedings of the Seventh Search Engine Conference, http://www.infonortics.com/searchengines/sh02/02prog.html, 2002. [16] Mao, S., Kim, J. W., and Thoma, G. R. A dynamic feature generation system for automated metadata extraction in preservation of digital materials.",
                "In Proceedings of the First International Workshop on Document Image Analysis for Libraries, 225-232, 2004. [17] McCallum, A., Freitag, D., and Pereira, F. Maximum entropy markov models for information extraction and segmentation.",
                "In Proceedings of the Seventeenth International Conference on Machine Learning, 591-598, 2000. [18] Murphy, L. D. Digital document metadata in organizations: roles, analytical approaches, and future research directions.",
                "In Proceedings of the Thirty-First Annual Hawaii International Conference on System Sciences, 267-276, 1998. [19] Pinto, D., McCallum, A., Wei, X., and Croft, W. B.",
                "Table extraction using conditional random fields.",
                "In Proceedings of the 26th Annual International ACM SIGIR Conference on Research and Development in Information Retrieval, 235242, 2003. [20] Ratnaparkhi, A. Unsupervised statistical models for prepositional phrase attachment.",
                "In Proceedings of the Seventeenth International Conference on Computational Linguistics. 1079-1085, 1998. [21] Robertson, S., Zaragoza, H., and Taylor, M. Simple BM25 extension to multiple weighted fields, In Proceedings of ACM Thirteenth Conference on Information and Knowledge Management, 42-49, 2004. [22] Yi, J. and Sundaresan, N. Metadata based Web mining for relevance, In Proceedings of the 2000 International Symposium on Database Engineering & Applications, 113121, 2000. [23] Yilmazel, O., Finneran, C. M., and Liddy, E. D. MetaExtract: An NLP system to automatically assign metadata.",
                "In Proceedings of the 2004 Joint ACM/IEEE Conference on Digital Libraries, 241-242, 2004. [24] Zhang, J. and Dimitroff, A. Internet search engines response to metadata Dublin Core implementation.",
                "Journal of Information Science, 30:310-320, 2004. [25] Zhang, L., Pan, Y., and Zhang, T. Recognising and using named entities: focused named entity recognition using machine learning.",
                "In Proceedings of the 27th Annual International ACM SIGIR Conference on Research and Development in Information Retrieval, 281-288, 2004. [26] http://dublincore.org/groups/corporate/Seattle/ 154"
            ],
            "original_annotated_samples": [],
            "translated_annotated_samples": [
                "Por documentos generales, nos referimos a documentos que pueden pertenecer a cualquiera de una serie de \"géneros\" específicos, incluidas presentaciones, capítulos de libros, documentos técnicos, folletos, informes y cartas.",
                "Por documentos generales, nos referimos a documentos que pueden pertenecer a cualquiera de una serie de \"género\" específico.",
                "Por documentos generales, nos referimos a documentos que pertenecen a uno de cualquier número de \"género\" específico.",
                "La Figura 7 muestra las distribuciones del \"género\" de los documentos.",
                "Distribuciones del documento \"Género\" s."
            ],
            "translated_text": "",
            "candidates": [
                "género",
                "géneros",
                "género",
                "género",
                "género",
                "género",
                "género",
                "género",
                "género",
                "Género"
            ],
            "error": []
        },
        "classifier": {
            "translated_key": "clasificador",
            "is_in_text": true,
            "original_annotated_sentences": [
                "Automatic Extraction of Titles from General Documents using Machine Learning Yunhua Hu1 Computer Science Department Xian Jiaotong University No 28, Xianning West Road Xian, China, 710049 yunhuahu@mail.xjtu.edu.cn Hang Li, Yunbo Cao Microsoft Research Asia 5F Sigma Center, No.",
                "49 Zhichun Road, Haidian, Beijing, China, 100080 {hangli,yucao}@microsoft.com Qinghua Zheng Computer Science Department Xian Jiaotong University No 28, Xianning West Road Xian, China, 710049 qhzheng@mail.xjtu.edu.cn Dmitriy Meyerzon Microsoft Corporation One Microsoft Way Redmond, WA, USA, 98052 dmitriym@microsoft.com ABSTRACT In this paper, we propose a machine learning approach to title extraction from general documents.",
                "By general documents, we mean documents that can belong to any one of a number of specific genres, including presentations, book chapters, technical papers, brochures, reports, and letters.",
                "Previously, methods have been proposed mainly for title extraction from research papers.",
                "It has not been clear whether it could be possible to conduct automatic title extraction from general documents.",
                "As a case study, we consider extraction from Office including Word and PowerPoint.",
                "In our approach, we annotate titles in sample documents (for Word and PowerPoint respectively) and take them as training data, train machine learning models, and perform title extraction using the trained models.",
                "Our method is unique in that we mainly utilize formatting information such as font size as features in the models.",
                "It turns out that the use of formatting information can lead to quite accurate extraction from general documents.",
                "Precision and recall for title extraction from Word is 0.810 and 0.837 respectively, and precision and recall for title extraction from PowerPoint is 0.875 and 0.895 respectively in an experiment on intranet data.",
                "Other important new findings in this work include that we can train models in one domain and apply them to another domain, and more surprisingly we can even train models in one language and apply them to another language.",
                "Moreover, we can significantly improve search ranking results in document retrieval by using the extracted titles.",
                "Categories and Subject Descriptors H.3.3 [Information Storage and Retrieval]: Information Search and Retrieval - Search Process; H.4.1 [Information Systems Applications]: Office Automation - Word processing; D.2.8 [Software Engineering]: Metrics - complexity measures, performance measures General Terms Algorithms, Experimentation, Performance. 1.",
                "INTRODUCTION Metadata of documents is useful for many kinds of document processing such as search, browsing, and filtering.",
                "Ideally, metadata is defined by the authors of documents and is then used by various systems.",
                "However, people seldom define document metadata by themselves, even when they have convenient metadata definition tools [26].",
                "Thus, how to automatically extract metadata from the bodies of documents turns out to be an important research issue.",
                "Methods for performing the task have been proposed.",
                "However, the focus was mainly on extraction from research papers.",
                "For instance, Han et al. [10] proposed a machine learning based method to conduct extraction from research papers.",
                "They formalized the problem as that of classification and employed Support Vector Machines as the <br>classifier</br>.",
                "They mainly used linguistic features in the model.1 In this paper, we consider metadata extraction from general documents.",
                "By general documents, we mean documents that may belong to any one of a number of specific genres.",
                "General documents are more widely available in digital libraries, intranets and the internet, and thus investigation on extraction from them is sorely needed.",
                "Research papers usually have well-formed styles and noticeable characteristics.",
                "In contrast, the styles of general documents can vary greatly.",
                "It has not been clarified whether a machine learning based approach can work well for this task.",
                "There are many types of metadata: title, author, date of creation, etc.",
                "As a case study, we consider title extraction in this paper.",
                "General documents can be in many different file formats: Microsoft Office, PDF (PS), etc.",
                "As a case study, we consider extraction from Office including Word and PowerPoint.",
                "We take a machine learning approach.",
                "We annotate titles in sample documents (for Word and PowerPoint respectively) and take them as training data to train several types of models, and perform title extraction using any one type of the trained models.",
                "In the models, we mainly utilize formatting information such as font size as features.",
                "We employ the following models: Maximum Entropy Model, Perceptron with Uneven Margins, Maximum Entropy Markov Model, and Voted Perceptron.",
                "In this paper, we also investigate the following three problems, which did not seem to have been examined previously. (1) Comparison between models: among the models above, which model performs best for title extraction; (2) Generality of model: whether it is possible to train a model on one domain and apply it to another domain, and whether it is possible to train a model in one language and apply it to another language; (3) Usefulness of extracted titles: whether extracted titles can improve document processing such as search.",
                "Experimental results indicate that our approach works well for title extraction from general documents.",
                "Our method can significantly outperform the baselines: one that always uses the first lines as titles and the other that always uses the lines in the largest font sizes as titles.",
                "Precision and recall for title extraction from Word are 0.810 and 0.837 respectively, and precision and recall for title extraction from PowerPoint are 0.875 and 0.895 respectively.",
                "It turns out that the use of format features is the key to successful title extraction. (1) We have observed that Perceptron based models perform better in terms of extraction accuracies. (2) We have empirically verified that the models trained with our approach are generic in the sense that they can be trained on one domain and applied to another, and they can be trained in one language and applied to another. (3) We have found that using the extracted titles we can significantly improve precision of document retrieval (by 10%).",
                "We conclude that we can indeed conduct reliable title extraction from general documents and use the extracted results to improve real applications.",
                "The rest of the paper is organized as follows.",
                "In section 2, we introduce related work, and in section 3, we explain the motivation and problem setting of our work.",
                "In section 4, we describe our method of title extraction, and in section 5, we describe our method of document retrieval using extracted titles.",
                "Section 6 gives our experimental results.",
                "We make concluding remarks in section 7. 2.",
                "RELATED WORK 2.1 Document Metadata Extraction Methods have been proposed for performing automatic metadata extraction from documents; however, the main focus was on extraction from research papers.",
                "The proposed methods fall into two categories: the rule based approach and the machine learning based approach.",
                "Giuffrida et al. [9], for instance, developed a rule-based system for automatically extracting metadata from research papers in Postscript.",
                "They used rules like titles are usually located on the upper portions of the first pages and they are usually in the largest font sizes.",
                "Liddy et al. [14] and Yilmazel el al. [23] performed metadata extraction from educational materials using rule-based natural language processing technologies.",
                "Mao et al. [16] also conducted automatic metadata extraction from research papers using rules on formatting information.",
                "The rule-based approach can achieve high performance.",
                "However, it also has disadvantages.",
                "It is less adaptive and robust when compared with the machine learning approach.",
                "Han et al. [10], for instance, conducted metadata extraction with the machine learning approach.",
                "They viewed the problem as that of classifying the lines in a document into the categories of metadata and proposed using Support Vector Machines as the <br>classifier</br>.",
                "They mainly used linguistic information as features.",
                "They reported high extraction accuracy from research papers in terms of precision and recall. 2.2 Information Extraction Metadata extraction can be viewed as an application of information extraction, in which given a sequence of instances, we identify a subsequence that represents information in which we are interested.",
                "Hidden Markov Model [6], Maximum Entropy Model [1, 4], Maximum Entropy Markov Model [17], Support Vector Machines [3], Conditional Random Field [12], and Voted Perceptron [2] are widely used information extraction models.",
                "Information extraction has been applied, for instance, to part-ofspeech tagging [20], named entity recognition [25] and table extraction [19]. 2.3 Search Using Title Information Title information is useful for document retrieval.",
                "In the system Citeseer, for instance, Giles et al. managed to extract titles from research papers and make use of the extracted titles in metadata search of papers [8].",
                "In web search, the title fields (i.e., file properties) and anchor texts of web pages (HTML documents) can be viewed as titles of the pages [5].",
                "Many search engines seem to utilize them for web page retrieval [7, 11, 18, 22].",
                "Zhang et al., found that web pages with well-defined metadata are more easily retrieved than those without well-defined metadata [24].",
                "To the best of our knowledge, no research has been conducted on using extracted titles from general documents (e.g., Office documents) for search of the documents. 146 3.",
                "MOTIVATION AND PROBLEM SETTING We consider the issue of automatically extracting titles from general documents.",
                "By general documents, we mean documents that belong to one of any number of specific genres.",
                "The documents can be presentations, books, book chapters, technical papers, brochures, reports, memos, specifications, letters, announcements, or resumes.",
                "General documents are more widely available in digital libraries, intranets, and internet, and thus investigation on title extraction from them is sorely needed.",
                "Figure 1 shows an estimate on distributions of file formats on intranet and internet [15].",
                "Office and PDF are the main file formats on the intranet.",
                "Even on the internet, the documents in the formats are still not negligible, given its extremely large size.",
                "In this paper, without loss of generality, we take Office documents as an example.",
                "Figure 1.",
                "Distributions of file formats in internet and intranet.",
                "For Office documents, users can define titles as file properties using a feature provided by Office.",
                "We found in an experiment, however, that users seldom use the feature and thus titles in file properties are usually very inaccurate.",
                "That is to say, titles in file properties are usually inconsistent with the true titles in the file bodies that are created by the authors and are visible to readers.",
                "We collected 6,000 Word and 6,000 PowerPoint documents from an intranet and the internet and examined how many titles in the file properties are correct.",
                "We found that surprisingly the accuracy was only 0.265 (cf., Section 6.3 for details).",
                "A number of reasons can be considered.",
                "For example, if one creates a new file by copying an old file, then the file property of the new file will also be copied from the old file.",
                "In another experiment, we found that Google uses the titles in file properties of Office documents in search and browsing, but the titles are not very accurate.",
                "We created 50 queries to search Word and PowerPoint documents and examined the top 15 results of each query returned by Google.",
                "We found that nearly all the titles presented in the search results were from the file properties of the documents.",
                "However, only 0.272 of them were correct.",
                "Actually, true titles usually exist at the beginnings of the bodies of documents.",
                "If we can accurately extract the titles from the bodies of documents, then we can exploit reliable title information in document processing.",
                "This is exactly the problem we address in this paper.",
                "More specifically, given a Word document, we are to extract the title from the top region of the first page.",
                "Given a PowerPoint document, we are to extract the title from the first slide.",
                "A title sometimes consists of a main title and one or two subtitles.",
                "We only consider extraction of the main title.",
                "As baselines for title extraction, we use that of always using the first lines as titles and that of always using the lines with largest font sizes as titles.",
                "Figure 2.",
                "Title extraction from Word document.",
                "Figure 3.",
                "Title extraction from PowerPoint document.",
                "Next, we define a specification for human judgments in title data annotation.",
                "The annotated data will be used in training and testing of the title extraction methods.",
                "Summary of the specification: The title of a document should be identified on the basis of common sense, if there is no difficulty in the identification.",
                "However, there are many cases in which the identification is not easy.",
                "There are some rules defined in the specification that guide identification for such cases.",
                "The rules include a title is usually in consecutive lines in the same format, a document can have no title, titles in images are not considered, a title should not contain words like draft, 147 whitepaper, etc, if it is difficult to determine which is the title, select the one in the largest font size, and if it is still difficult to determine which is the title, select the first candidate. (The specification covers all the cases we have encountered in data annotation.)",
                "Figures 2 and 3 show examples of Office documents from which we conduct title extraction.",
                "In Figure 2, Differences in Win32 API Implementations among Windows Operating Systems is the title of the Word document.",
                "Microsoft Windows on the top of this page is a picture and thus is ignored.",
                "In Figure 3, Building Competitive Advantages through an Agile Infrastructure is the title of the PowerPoint document.",
                "We have developed a tool for annotation of titles by human annotators.",
                "Figure 4 shows a snapshot of the tool.",
                "Figure 4.",
                "Title annotation tool. 4.",
                "TITLE EXTRACTION METHOD 4.1 Outline Title extraction based on machine learning consists of training and extraction.",
                "The same pre-processing step occurs before training and extraction.",
                "During pre-processing, from the top region of the first page of a Word document or the first slide of a PowerPoint document a number of units for processing are extracted.",
                "If a line (lines are separated by return symbols) only has a single format, then the line will become a unit.",
                "If a line has several parts and each of them has its own format, then each part will become a unit.",
                "Each unit will be treated as an instance in learning.",
                "A unit contains not only content information (linguistic information) but also formatting information.",
                "The input to pre-processing is a document and the output of pre-processing is a sequence of units (instances).",
                "Figure 5 shows the units obtained from the document in Figure 2.",
                "Figure 5.",
                "Example of units.",
                "In learning, the input is sequences of units where each sequence corresponds to a document.",
                "We take labeled units (labeled as title_begin, title_end, or other) in the sequences as training data and construct models for identifying whether a unit is title_begin title_end, or other.",
                "We employ four types of models: Perceptron, Maximum Entropy (ME), Perceptron Markov Model (PMM), and Maximum Entropy Markov Model (MEMM).",
                "In extraction, the input is a sequence of units from one document.",
                "We employ one type of model to identify whether a unit is title_begin, title_end, or other.",
                "We then extract units from the unit labeled with title_begin to the unit labeled with title_end.",
                "The result is the extracted title of the document.",
                "The unique characteristic of our approach is that we mainly utilize formatting information for title extraction.",
                "Our assumption is that although general documents vary in styles, their formats have certain patterns and we can learn and utilize the patterns for title extraction.",
                "This is in contrast to the work by Han et al., in which only linguistic features are used for extraction from research papers. 4.2 Models The four models actually can be considered in the same metadata extraction framework.",
                "That is why we apply them together to our current problem.",
                "Each input is a sequence of instances kxxx L21 together with a sequence of labels kyyy L21 . ix and iy represents an instance and its label, respectively ( ki ,,2,1 L= ).",
                "Recall that an instance here represents a unit.",
                "A label represents title_begin, title_end, or other.",
                "Here, k is the number of units in a document.",
                "In learning, we train a model which can be generally denoted as a conditional probability distribution )|( 11 kk XXYYP LL where iX and iY denote random variables taking instance ix and label iy as values, respectively ( ki ,,2,1 L= ).",
                "Learning Tool Extraction Tool 21121 2222122221 1121111211 nknnknn kk kk yyyxxx yyyxxx yyyxxx LL LL LL LL → → → )|(maxarg 11 mkmmkm xxyyP LL )|( 11 kk XXYYP LL Conditional Distribution mkmm xxx L21 Figure 6.",
                "Metadata extraction model.",
                "We can make assumptions about the general model in order to make it simple enough for training. 148 For example, we can assume that kYY ,,1 L are independent of each other given kXX ,,1 L .",
                "Thus, we have )|()|( )|( 11 11 kk kk XYPXYP XXYYP L LL = In this way, we decompose the model into a number of classifiers.",
                "We train the classifiers locally using the labeled data.",
                "As the <br>classifier</br>, we employ the Perceptron or Maximum Entropy model.",
                "We can also assume that the first order Markov property holds for kYY ,,1 L given kXX ,,1 L .",
                "Thus, we have )|()|( )|( 111 11 kkk kk XYYPXYP XXYYP −= L LL Again, we obtain a number of classifiers.",
                "However, the classifiers are conditioned on the previous label.",
                "When we employ the Percepton or Maximum Entropy model as a <br>classifier</br>, the models become a Percepton Markov Model or Maximum Entropy Markov Model, respectively.",
                "That is to say, the two models are more precise.",
                "In extraction, given a new sequence of instances, we resort to one of the constructed models to assign a sequence of labels to the sequence of instances, i.e., perform extraction.",
                "For Perceptron and ME, we assign labels locally and combine the results globally later using heuristics.",
                "Specifically, we first identify the most likely title_begin.",
                "Then we find the most likely title_end within three units after the title_begin.",
                "Finally, we extract as a title the units between the title_begin and the title_end.",
                "For PMM and MEMM, we employ the Viterbi algorithm to find the globally optimal label sequence.",
                "In this paper, for Perceptron, we actually employ an improved variant of it, called Perceptron with Uneven Margin [13].",
                "This version of Perceptron can work well especially when the number of positive instances and the number of negative instances differ greatly, which is exactly the case in our problem.",
                "We also employ an improved version of Perceptron Markov Model in which the Perceptron model is the so-called Voted Perceptron [2].",
                "In addition, in training, the parameters of the model are updated globally rather than locally. 4.3 Features There are two types of features: format features and linguistic features.",
                "We mainly use the former.",
                "The features are used for both the title-begin and the title-end classifiers. 4.3.1 Format Features Font Size: There are four binary features that represent the normalized font size of the unit (recall that a unit has only one type of font).",
                "If the font size of the unit is the largest in the document, then the first feature will be 1, otherwise 0.",
                "If the font size is the smallest in the document, then the fourth feature will be 1, otherwise 0.",
                "If the font size is above the average font size and not the largest in the document, then the second feature will be 1, otherwise 0.",
                "If the font size is below the average font size and not the smallest, the third feature will be 1, otherwise 0.",
                "It is necessary to conduct normalization on font sizes.",
                "For example, in one document the largest font size might be 12pt, while in another the smallest one might be 18pt.",
                "Boldface: This binary feature represents whether or not the current unit is in boldface.",
                "Alignment: There are four binary features that respectively represent the location of the current unit: left, center, right, and unknown alignment.",
                "The following format features with respect to context play an important role in title extraction.",
                "Empty Neighboring Unit: There are two binary features that represent, respectively, whether or not the previous unit and the current unit are blank lines.",
                "Font Size Change: There are two binary features that represent, respectively, whether or not the font size of the previous unit and the font size of the next unit differ from that of the current unit.",
                "Alignment Change: There are two binary features that represent, respectively, whether or not the alignment of the previous unit and the alignment of the next unit differ from that of the current one.",
                "Same Paragraph: There are two binary features that represent, respectively, whether or not the previous unit and the next unit are in the same paragraph as the current unit. 4.3.2 Linguistic Features The linguistic features are based on key words.",
                "Positive Word: This binary feature represents whether or not the current unit begins with one of the positive words.",
                "The positive words include title:, subject:, subject line: For example, in some documents the lines of titles and authors have the same formats.",
                "However, if lines begin with one of the positive words, then it is likely that they are title lines.",
                "Negative Word: This binary feature represents whether or not the current unit begins with one of the negative words.",
                "The negative words include To, By, created by, updated by, etc.",
                "There are more negative words than positive words.",
                "The above linguistic features are language dependent.",
                "Word Count: A title should not be too long.",
                "We heuristically create four intervals: [1, 2], [3, 6], [7, 9] and [9, ∞) and define one feature for each interval.",
                "If the number of words in a title falls into an interval, then the corresponding feature will be 1; otherwise 0.",
                "Ending Character: This feature represents whether the unit ends with :, -, or other special characters.",
                "A title usually does not end with such a character. 5.",
                "DOCUMENT RETRIEVAL METHOD We describe our method of document retrieval using extracted titles.",
                "Typically, in information retrieval a document is split into a number of fields including body, title, and anchor text.",
                "A ranking function in search can use different weights for different fields of 149 the document.",
                "Also, titles are typically assigned high weights, indicating that they are important for document retrieval.",
                "As explained previously, our experiment has shown that a significant number of documents actually have incorrect titles in the file properties, and thus in addition of using them we use the extracted titles as one more field of the document.",
                "By doing this, we attempt to improve the overall precision.",
                "In this paper, we employ a modification of BM25 that allows field weighting [21].",
                "As fields, we make use of body, title, extracted title and anchor.",
                "First, for each term in the query we count the term frequency in each field of the document; each field frequency is then weighted according to the corresponding weight parameter: ∑= f tfft tfwwtf Similarly, we compute the document length as a weighted sum of lengths of each field.",
                "Average document length in the corpus becomes the average of all weighted document lengths. ∑= f ff dlwwdl In our experiments we used 75.0,8.11 == bk .",
                "Weight for content was 1.0, title was 10.0, anchor was 10.0, and extracted title was 5.0. 6.",
                "EXPERIMENTAL RESULTS 6.1 Data Sets and Evaluation Measures We used two data sets in our experiments.",
                "First, we downloaded and randomly selected 5,000 Word documents and 5,000 PowerPoint documents from an intranet of Microsoft.",
                "We call it MS hereafter.",
                "Second, we downloaded and randomly selected 500 Word and 500 PowerPoint documents from the DotGov and DotCom domains on the internet, respectively.",
                "Figure 7 shows the distributions of the genres of the documents.",
                "We see that the documents are indeed general documents as we define them.",
                "Figure 7.",
                "Distributions of document genres.",
                "Third, a data set in Chinese was also downloaded from the internet.",
                "It includes 500 Word documents and 500 PowerPoint documents in Chinese.",
                "We manually labeled the titles of all the documents, on the basis of our specification.",
                "Not all the documents in the two data sets have titles.",
                "Table 1 shows the percentages of the documents having titles.",
                "We see that DotCom and DotGov have more PowerPoint documents with titles than MS.",
                "This might be because PowerPoint documents published on the internet are more formal than those on the intranet.",
                "Table 1.",
                "The portion of documents with titles Domain Type MS DotCom DotGov Word 75.7% 77.8% 75.6% PowerPoint 82.1% 93.4% 96.4% In our experiments, we conducted evaluations on title extraction in terms of precision, recall, and F-measure.",
                "The evaluation measures are defined as follows: Precision: P = A / ( A + B ) Recall: R = A / ( A + C ) F-measure: F1 = 2PR / ( P + R ) Here, A, B, C, and D are numbers of documents as those defined in Table 2.",
                "Table 2.",
                "Contingence table with regard to title extraction Is title Is not title Extracted A B Not extracted C D 6.2 Baselines We test the accuracies of the two baselines described in section 4.2.",
                "They are denoted as largest font size and first line respectively. 6.3 Accuracy of Titles in File Properties We investigate how many titles in the file properties of the documents are reliable.",
                "We view the titles annotated by humans as true titles and test how many titles in the file properties can approximately match with the true titles.",
                "We use Edit Distance to conduct the approximate match. (Approximate match is only used in this evaluation).",
                "This is because sometimes human annotated titles can be slightly different from the titles in file properties on the surface, e.g., contain extra spaces).",
                "Given string A and string B: if ( (D == 0) or ( D / ( La + Lb ) < θ ) ) then string A = string B D: Edit Distance between string A and string B La: length of string A Lb: length of string B θ: 0.1 ∑ × ++− + = t t n N wtf avwdl wdl bbk kwtf FBM )log( ))1(( )1( 25 1 1 150 Table 3.",
                "Accuracies of titles in file properties File Type Domain Precision Recall F1 MS 0.299 0.311 0.305 DotCom 0.210 0.214 0.212Word DotGov 0.182 0.177 0.180 MS 0.229 0.245 0.237 DotCom 0.185 0.186 0.186PowerPoint DotGov 0.180 0.182 0.181 6.4 Comparison with Baselines We conducted title extraction from the first data set (Word and PowerPoint in MS).",
                "As the model, we used Perceptron.",
                "We conduct 4-fold cross validation.",
                "Thus, all the results reported here are those averaged over 4 trials.",
                "Tables 4 and 5 show the results.",
                "We see that Perceptron significantly outperforms the baselines.",
                "In the evaluation, we use exact matching between the true titles annotated by humans and the extracted titles.",
                "Table 4.",
                "Accuracies of title extraction with Word Precision Recall F1 Model Perceptron 0.810 0.837 0.823 Largest font size 0.700 0.758 0.727 Baselines First line 0.707 0.767 0.736 Table 5.",
                "Accuracies of title extraction with PowerPoint Precision Recall F1 Model Perceptron 0.875 0. 895 0.885 Largest font size 0.844 0.887 0.865 Baselines First line 0.639 0.671 0.655 We see that the machine learning approach can achieve good performance in title extraction.",
                "For Word documents both precision and recall of the approach are 8 percent higher than those of the baselines.",
                "For PowerPoint both precision and recall of the approach are 2 percent higher than those of the baselines.",
                "We conduct significance tests.",
                "The results are shown in Table 6.",
                "Here, Largest denotes the baseline of using the largest font size, First denotes the baseline of using the first line.",
                "The results indicate that the improvements of machine learning over baselines are statistically significant (in the sense p-value < 0.05) Table 6.",
                "Sign test results Documents Type Sign test between p-value Perceptron vs. Largest 3.59e-26 Word Perceptron vs. First 7.12e-10 Perceptron vs. Largest 0.010 PowerPoint Perceptron vs. First 5.13e-40 We see, from the results, that the two baselines can work well for title extraction, suggesting that font size and position information are most useful features for title extraction.",
                "However, it is also obvious that using only these two features is not enough.",
                "There are cases in which all the lines have the same font size (i.e., the largest font size), or cases in which the lines with the largest font size only contain general descriptions like Confidential, White paper, etc.",
                "For those cases, the largest font size method cannot work well.",
                "For similar reasons, the first line method alone cannot work well, either.",
                "With the combination of different features (evidence in title judgment), Perceptron can outperform Largest and First.",
                "We investigate the performance of solely using linguistic features.",
                "We found that it does not work well.",
                "It seems that the format features play important roles and the linguistic features are supplements..",
                "Figure 8.",
                "An example Word document.",
                "Figure 9.",
                "An example PowerPoint document.",
                "We conducted an error analysis on the results of Perceptron.",
                "We found that the errors fell into three categories. (1) About one third of the errors were related to hard cases.",
                "In these documents, the layouts of the first pages were difficult to understand, even for humans.",
                "Figure 8 and 9 shows examples. (2) Nearly one fourth of the errors were from the documents which do not have true titles but only contain bullets.",
                "Since we conduct extraction from the top regions, it is difficult to get rid of these errors with the current approach. (3).",
                "Confusions between main titles and subtitles were another type of error.",
                "Since we only labeled the main titles as titles, the extractions of both titles were considered incorrect.",
                "This type of error does little harm to document processing like search, however. 6.5 Comparison between Models To compare the performance of different machine learning models, we conducted another experiment.",
                "Again, we perform 4-fold cross 151 validation on the first data set (MS).",
                "Table 7, 8 shows the results of all the four models.",
                "It turns out that Perceptron and PMM perform the best, followed by MEMM, and ME performs the worst.",
                "In general, the Markovian models perform better than or as well as their <br>classifier</br> counterparts.",
                "This seems to be because the Markovian models are trained globally, while the classifiers are trained locally.",
                "The Perceptron based models perform better than the ME based counterparts.",
                "This seems to be because the Perceptron based models are created to make better classifications, while ME models are constructed for better prediction.",
                "Table 7.",
                "Comparison between different learning models for title extraction with Word Model Precision Recall F1 Perceptron 0.810 0.837 0.823 MEMM 0.797 0.824 0.810 PMM 0.827 0.823 0.825 ME 0.801 0.621 0.699 Table 8.",
                "Comparison between different learning models for title extraction with PowerPoint Model Precision Recall F1 Perceptron 0.875 0. 895 0. 885 MEMM 0.841 0.861 0.851 PMM 0.873 0.896 0.885 ME 0.753 0.766 0.759 6.6 Domain Adaptation We apply the model trained with the first data set (MS) to the second data set (DotCom and DotGov).",
                "Tables 9-12 show the results.",
                "Table 9.",
                "Accuracies of title extraction with Word in DotGov Precision Recall F1 Model Perceptron 0.716 0.759 0.737 Largest font size 0.549 0.619 0.582Baselines First line 0.462 0.521 0.490 Table 10.",
                "Accuracies of title extraction with PowerPoint in DotGov Precision Recall F1 Model Perceptron 0.900 0.906 0.903 Largest font size 0.871 0.888 0.879Baselines First line 0.554 0.564 0.559 Table 11.",
                "Accuracies of title extraction with Word in DotCom Precisio n Recall F1 Model Perceptron 0.832 0.880 0.855 Largest font size 0.676 0.753 0.712Baselines First line 0.577 0.643 0.608 Table 12.",
                "Performance of PowerPoint document title extraction in DotCom Precisio n Recall F1 Model Perceptron 0.910 0.903 0.907 Largest font size 0.864 0.886 0.875Baselines First line 0.570 0.585 0.577 From the results, we see that the models can be adapted to different domains well.",
                "There is almost no drop in accuracy.",
                "The results indicate that the patterns of title formats exist across different domains, and it is possible to construct a domain independent model by mainly using formatting information. 6.7 Language Adaptation We apply the model trained with the data in English (MS) to the data set in Chinese.",
                "Tables 13-14 show the results.",
                "Table 13.",
                "Accuracies of title extraction with Word in Chinese Precision Recall F1 Model Perceptron 0.817 0.805 0.811 Largest font size 0.722 0.755 0.738Baselines First line 0.743 0.777 0.760 Table 14.",
                "Accuracies of title extraction with PowerPoint in Chinese Precision Recall F1 Model Perceptron 0.766 0.812 0.789 Largest font size 0.753 0.813 0.782Baselines First line 0.627 0.676 0.650 We see that the models can be adapted to a different language.",
                "There are only small drops in accuracy.",
                "Obviously, the linguistic features do not work for Chinese, but the effect of not using them is negligible.",
                "The results indicate that the patterns of title formats exist across different languages.",
                "From the domain adaptation and language adaptation results, we conclude that the use of formatting information is the key to a successful extraction from general documents. 6.8 Search with Extracted Titles We performed experiments on using title extraction for document retrieval.",
                "As a baseline, we employed BM25 without using extracted titles.",
                "The ranking mechanism was as described in Section 5.",
                "The weights were heuristically set.",
                "We did not conduct optimization on the weights.",
                "The evaluation was conducted on a corpus of 1.3 M documents crawled from the intranet of Microsoft using 100 evaluation queries obtained from this intranets search engine query logs. 50 queries were from the most popular set, while 50 queries other were chosen randomly.",
                "Users were asked to provide judgments of the degree of document relevance from a scale of 1to 5 (1 meaning detrimental, 2 - bad, 3 - fair, 4 - good and 5 - excellent). 152 Figure 10 shows the results.",
                "In the chart two sets of precision results were obtained by either considering good or excellent documents as relevant (left 3 bars with relevance threshold 0.5), or by considering only excellent documents as relevant (right 3 bars with relevance threshold 1.0) 0 0.05 0.1 0.15 0.2 0.25 0.3 0.35 0.4 0.45 P@10 P@5 Reciprocal P@10 P@5 Reciprocal 0.5 1 BM25 Anchor, Title, Body BM25 Anchor, Title, Body, ExtractedTitle Name All RelevanceThreshold Data Description Figure 10.",
                "Search ranking results.",
                "Figure 10 shows different document retrieval results with different ranking functions in terms of precision @10, precision @5 and reciprocal rank: • Blue bar - BM25 including the fields body, title (file property), and anchor text. • Purple bar - BM25 including the fields body, title (file property), anchor text, and extracted title.",
                "With the additional field of extracted title included in BM25 the precision @10 increased from 0.132 to 0.145, or by ~10%.",
                "Thus, it is safe to say that the use of extracted title can indeed improve the precision of document retrieval. 7.",
                "CONCLUSION In this paper, we have investigated the problem of automatically extracting titles from general documents.",
                "We have tried using a machine learning approach to address the problem.",
                "Previous work showed that the machine learning approach can work well for metadata extraction from research papers.",
                "In this paper, we showed that the approach can work for extraction from general documents as well.",
                "Our experimental results indicated that the machine learning approach can work significantly better than the baselines in title extraction from Office documents.",
                "Previous work on metadata extraction mainly used linguistic features in documents, while we mainly used formatting information.",
                "It appeared that using formatting information is a key for successfully conducting title extraction from general documents.",
                "We tried different machine learning models including Perceptron, Maximum Entropy, Maximum Entropy Markov Model, and Voted Perceptron.",
                "We found that the performance of the Perceptorn models was the best.",
                "We applied models constructed in one domain to another domain and applied models trained in one language to another language.",
                "We found that the accuracies did not drop substantially across different domains and across different languages, indicating that the models were generic.",
                "We also attempted to use the extracted titles in document retrieval.",
                "We observed a significant improvement in document ranking performance for search when using extracted title information.",
                "All the above investigations were not conducted in previous work, and through our investigations we verified the generality and the significance of the title extraction approach. 8.",
                "ACKNOWLEDGEMENTS We thank Chunyu Wei and Bojuan Zhao for their work on data annotation.",
                "We acknowledge Jinzhu Li for his assistance in conducting the experiments.",
                "We thank Ming Zhou, John Chen, Jun Xu, and the anonymous reviewers of JCDL05 for their valuable comments on this paper. 9.",
                "REFERENCES [1] Berger, A. L., Della Pietra, S. A., and Della Pietra, V. J.",
                "A maximum entropy approach to natural language processing.",
                "Computational Linguistics, 22:39-71, 1996. [2] Collins, M. Discriminative training methods for hidden markov models: theory and experiments with perceptron algorithms.",
                "In Proceedings of Conference on Empirical Methods in Natural Language Processing, 1-8, 2002. [3] Cortes, C. and Vapnik, V. Support-vector networks.",
                "Machine Learning, 20:273-297, 1995. [4] Chieu, H. L. and Ng, H. T. A maximum entropy approach to information extraction from semi-structured and free text.",
                "In Proceedings of the Eighteenth National Conference on Artificial Intelligence, 768-791, 2002. [5] Evans, D. K., Klavans, J. L., and McKeown, K. R. Columbia newsblaster: multilingual news summarization on the Web.",
                "In Proceedings of Human Language Technology conference / North American chapter of the Association for Computational Linguistics annual meeting, 1-4, 2004. [6] Ghahramani, Z. and Jordan, M. I. Factorial hidden markov models.",
                "Machine Learning, 29:245-273, 1997. [7] Gheel, J. and Anderson, T. Data and metadata for finding and reminding, In Proceedings of the 1999 International Conference on Information Visualization, 446-451,1999. [8] Giles, C. L., Petinot, Y., Teregowda P. B., Han, H., Lawrence, S., Rangaswamy, A., and Pal, N. eBizSearch: a niche search engine for e-Business.",
                "In Proceedings of the 26th Annual International ACM SIGIR Conference on Research and Development in Information Retrieval, 413414, 2003. [9] Giuffrida, G., Shek, E. C., and Yang, J. Knowledge-based metadata extraction from PostScript files.",
                "In Proceedings of the Fifth ACM Conference on Digital Libraries, 77-84, 2000. [10] Han, H., Giles, C. L., Manavoglu, E., Zha, H., Zhang, Z., and Fox, E. A.",
                "Automatic document metadata extraction using support vector machines.",
                "In Proceedings of the Third ACM/IEEE-CS Joint Conference on Digital Libraries, 37-48, 2003. [11] Kobayashi, M., and Takeda, K. Information retrieval on the Web.",
                "ACM Computing Surveys, 32:144-173, 2000. [12] Lafferty, J., McCallum, A., and Pereira, F. Conditional random fields: probabilistic models for segmenting and 153 labeling sequence data.",
                "In Proceedings of the Eighteenth International Conference on Machine Learning, 282-289, 2001. [13] Li, Y., Zaragoza, H., Herbrich, R., Shawe-Taylor J., and Kandola, J. S. The perceptron algorithm with uneven margins.",
                "In Proceedings of the Nineteenth International Conference on Machine Learning, 379-386, 2002. [14] Liddy, E. D., Sutton, S., Allen, E., Harwell, S., Corieri, S., Yilmazel, O., Ozgencil, N. E., Diekema, A., McCracken, N., and Silverstein, J.",
                "Automatic Metadata generation & evaluation.",
                "In Proceedings of the 25th Annual International ACM SIGIR Conference on Research and Development in Information Retrieval, 401-402, 2002. [15] Littlefield, A.",
                "Effective enterprise information retrieval across new content formats.",
                "In Proceedings of the Seventh Search Engine Conference, http://www.infonortics.com/searchengines/sh02/02prog.html, 2002. [16] Mao, S., Kim, J. W., and Thoma, G. R. A dynamic feature generation system for automated metadata extraction in preservation of digital materials.",
                "In Proceedings of the First International Workshop on Document Image Analysis for Libraries, 225-232, 2004. [17] McCallum, A., Freitag, D., and Pereira, F. Maximum entropy markov models for information extraction and segmentation.",
                "In Proceedings of the Seventeenth International Conference on Machine Learning, 591-598, 2000. [18] Murphy, L. D. Digital document metadata in organizations: roles, analytical approaches, and future research directions.",
                "In Proceedings of the Thirty-First Annual Hawaii International Conference on System Sciences, 267-276, 1998. [19] Pinto, D., McCallum, A., Wei, X., and Croft, W. B.",
                "Table extraction using conditional random fields.",
                "In Proceedings of the 26th Annual International ACM SIGIR Conference on Research and Development in Information Retrieval, 235242, 2003. [20] Ratnaparkhi, A. Unsupervised statistical models for prepositional phrase attachment.",
                "In Proceedings of the Seventeenth International Conference on Computational Linguistics. 1079-1085, 1998. [21] Robertson, S., Zaragoza, H., and Taylor, M. Simple BM25 extension to multiple weighted fields, In Proceedings of ACM Thirteenth Conference on Information and Knowledge Management, 42-49, 2004. [22] Yi, J. and Sundaresan, N. Metadata based Web mining for relevance, In Proceedings of the 2000 International Symposium on Database Engineering & Applications, 113121, 2000. [23] Yilmazel, O., Finneran, C. M., and Liddy, E. D. MetaExtract: An NLP system to automatically assign metadata.",
                "In Proceedings of the 2004 Joint ACM/IEEE Conference on Digital Libraries, 241-242, 2004. [24] Zhang, J. and Dimitroff, A. Internet search engines response to metadata Dublin Core implementation.",
                "Journal of Information Science, 30:310-320, 2004. [25] Zhang, L., Pan, Y., and Zhang, T. Recognising and using named entities: focused named entity recognition using machine learning.",
                "In Proceedings of the 27th Annual International ACM SIGIR Conference on Research and Development in Information Retrieval, 281-288, 2004. [26] http://dublincore.org/groups/corporate/Seattle/ 154"
            ],
            "original_annotated_samples": [],
            "translated_annotated_samples": [
                "Formalizaron el problema como el de clasificación y emplearon máquinas de vectores de soporte como el \"clasificador\".",
                "Vieron el problema como el de clasificar las líneas en un documento en las categorías de metadatos y propusieron usar máquinas de vectores de soporte como \"clasificador\".",
                "Como el \"clasificador\", empleamos el modelo Perceptron o de entropía máxima.",
                "Cuando empleamos la percepción del modelo de entropía máxima como un \"clasificador\", los modelos se convierten en un modelo de percepción Markov o modelo de entropía máxima Markov, respectivamente.",
                "En general, los modelos de Markovian funcionan mejor que o sus homólogos \"clasificador\"."
            ],
            "translated_text": "",
            "candidates": [
                "clasificador",
                "clasificador",
                "clasificador",
                "clasificador",
                "clasificador",
                "clasificador",
                "clasificador",
                "clasificador",
                "clasificador",
                "clasificador"
            ],
            "error": []
        },
        "document retrieval": {
            "translated_key": "recuperación de documentos",
            "is_in_text": true,
            "original_annotated_sentences": [
                "Automatic Extraction of Titles from General Documents using Machine Learning Yunhua Hu1 Computer Science Department Xian Jiaotong University No 28, Xianning West Road Xian, China, 710049 yunhuahu@mail.xjtu.edu.cn Hang Li, Yunbo Cao Microsoft Research Asia 5F Sigma Center, No.",
                "49 Zhichun Road, Haidian, Beijing, China, 100080 {hangli,yucao}@microsoft.com Qinghua Zheng Computer Science Department Xian Jiaotong University No 28, Xianning West Road Xian, China, 710049 qhzheng@mail.xjtu.edu.cn Dmitriy Meyerzon Microsoft Corporation One Microsoft Way Redmond, WA, USA, 98052 dmitriym@microsoft.com ABSTRACT In this paper, we propose a machine learning approach to title extraction from general documents.",
                "By general documents, we mean documents that can belong to any one of a number of specific genres, including presentations, book chapters, technical papers, brochures, reports, and letters.",
                "Previously, methods have been proposed mainly for title extraction from research papers.",
                "It has not been clear whether it could be possible to conduct automatic title extraction from general documents.",
                "As a case study, we consider extraction from Office including Word and PowerPoint.",
                "In our approach, we annotate titles in sample documents (for Word and PowerPoint respectively) and take them as training data, train machine learning models, and perform title extraction using the trained models.",
                "Our method is unique in that we mainly utilize formatting information such as font size as features in the models.",
                "It turns out that the use of formatting information can lead to quite accurate extraction from general documents.",
                "Precision and recall for title extraction from Word is 0.810 and 0.837 respectively, and precision and recall for title extraction from PowerPoint is 0.875 and 0.895 respectively in an experiment on intranet data.",
                "Other important new findings in this work include that we can train models in one domain and apply them to another domain, and more surprisingly we can even train models in one language and apply them to another language.",
                "Moreover, we can significantly improve search ranking results in <br>document retrieval</br> by using the extracted titles.",
                "Categories and Subject Descriptors H.3.3 [Information Storage and Retrieval]: Information Search and Retrieval - Search Process; H.4.1 [Information Systems Applications]: Office Automation - Word processing; D.2.8 [Software Engineering]: Metrics - complexity measures, performance measures General Terms Algorithms, Experimentation, Performance. 1.",
                "INTRODUCTION Metadata of documents is useful for many kinds of document processing such as search, browsing, and filtering.",
                "Ideally, metadata is defined by the authors of documents and is then used by various systems.",
                "However, people seldom define document metadata by themselves, even when they have convenient metadata definition tools [26].",
                "Thus, how to automatically extract metadata from the bodies of documents turns out to be an important research issue.",
                "Methods for performing the task have been proposed.",
                "However, the focus was mainly on extraction from research papers.",
                "For instance, Han et al. [10] proposed a machine learning based method to conduct extraction from research papers.",
                "They formalized the problem as that of classification and employed Support Vector Machines as the classifier.",
                "They mainly used linguistic features in the model.1 In this paper, we consider metadata extraction from general documents.",
                "By general documents, we mean documents that may belong to any one of a number of specific genres.",
                "General documents are more widely available in digital libraries, intranets and the internet, and thus investigation on extraction from them is sorely needed.",
                "Research papers usually have well-formed styles and noticeable characteristics.",
                "In contrast, the styles of general documents can vary greatly.",
                "It has not been clarified whether a machine learning based approach can work well for this task.",
                "There are many types of metadata: title, author, date of creation, etc.",
                "As a case study, we consider title extraction in this paper.",
                "General documents can be in many different file formats: Microsoft Office, PDF (PS), etc.",
                "As a case study, we consider extraction from Office including Word and PowerPoint.",
                "We take a machine learning approach.",
                "We annotate titles in sample documents (for Word and PowerPoint respectively) and take them as training data to train several types of models, and perform title extraction using any one type of the trained models.",
                "In the models, we mainly utilize formatting information such as font size as features.",
                "We employ the following models: Maximum Entropy Model, Perceptron with Uneven Margins, Maximum Entropy Markov Model, and Voted Perceptron.",
                "In this paper, we also investigate the following three problems, which did not seem to have been examined previously. (1) Comparison between models: among the models above, which model performs best for title extraction; (2) Generality of model: whether it is possible to train a model on one domain and apply it to another domain, and whether it is possible to train a model in one language and apply it to another language; (3) Usefulness of extracted titles: whether extracted titles can improve document processing such as search.",
                "Experimental results indicate that our approach works well for title extraction from general documents.",
                "Our method can significantly outperform the baselines: one that always uses the first lines as titles and the other that always uses the lines in the largest font sizes as titles.",
                "Precision and recall for title extraction from Word are 0.810 and 0.837 respectively, and precision and recall for title extraction from PowerPoint are 0.875 and 0.895 respectively.",
                "It turns out that the use of format features is the key to successful title extraction. (1) We have observed that Perceptron based models perform better in terms of extraction accuracies. (2) We have empirically verified that the models trained with our approach are generic in the sense that they can be trained on one domain and applied to another, and they can be trained in one language and applied to another. (3) We have found that using the extracted titles we can significantly improve precision of <br>document retrieval</br> (by 10%).",
                "We conclude that we can indeed conduct reliable title extraction from general documents and use the extracted results to improve real applications.",
                "The rest of the paper is organized as follows.",
                "In section 2, we introduce related work, and in section 3, we explain the motivation and problem setting of our work.",
                "In section 4, we describe our method of title extraction, and in section 5, we describe our method of <br>document retrieval</br> using extracted titles.",
                "Section 6 gives our experimental results.",
                "We make concluding remarks in section 7. 2.",
                "RELATED WORK 2.1 Document Metadata Extraction Methods have been proposed for performing automatic metadata extraction from documents; however, the main focus was on extraction from research papers.",
                "The proposed methods fall into two categories: the rule based approach and the machine learning based approach.",
                "Giuffrida et al. [9], for instance, developed a rule-based system for automatically extracting metadata from research papers in Postscript.",
                "They used rules like titles are usually located on the upper portions of the first pages and they are usually in the largest font sizes.",
                "Liddy et al. [14] and Yilmazel el al. [23] performed metadata extraction from educational materials using rule-based natural language processing technologies.",
                "Mao et al. [16] also conducted automatic metadata extraction from research papers using rules on formatting information.",
                "The rule-based approach can achieve high performance.",
                "However, it also has disadvantages.",
                "It is less adaptive and robust when compared with the machine learning approach.",
                "Han et al. [10], for instance, conducted metadata extraction with the machine learning approach.",
                "They viewed the problem as that of classifying the lines in a document into the categories of metadata and proposed using Support Vector Machines as the classifier.",
                "They mainly used linguistic information as features.",
                "They reported high extraction accuracy from research papers in terms of precision and recall. 2.2 Information Extraction Metadata extraction can be viewed as an application of information extraction, in which given a sequence of instances, we identify a subsequence that represents information in which we are interested.",
                "Hidden Markov Model [6], Maximum Entropy Model [1, 4], Maximum Entropy Markov Model [17], Support Vector Machines [3], Conditional Random Field [12], and Voted Perceptron [2] are widely used information extraction models.",
                "Information extraction has been applied, for instance, to part-ofspeech tagging [20], named entity recognition [25] and table extraction [19]. 2.3 Search Using Title Information Title information is useful for <br>document retrieval</br>.",
                "In the system Citeseer, for instance, Giles et al. managed to extract titles from research papers and make use of the extracted titles in metadata search of papers [8].",
                "In web search, the title fields (i.e., file properties) and anchor texts of web pages (HTML documents) can be viewed as titles of the pages [5].",
                "Many search engines seem to utilize them for web page retrieval [7, 11, 18, 22].",
                "Zhang et al., found that web pages with well-defined metadata are more easily retrieved than those without well-defined metadata [24].",
                "To the best of our knowledge, no research has been conducted on using extracted titles from general documents (e.g., Office documents) for search of the documents. 146 3.",
                "MOTIVATION AND PROBLEM SETTING We consider the issue of automatically extracting titles from general documents.",
                "By general documents, we mean documents that belong to one of any number of specific genres.",
                "The documents can be presentations, books, book chapters, technical papers, brochures, reports, memos, specifications, letters, announcements, or resumes.",
                "General documents are more widely available in digital libraries, intranets, and internet, and thus investigation on title extraction from them is sorely needed.",
                "Figure 1 shows an estimate on distributions of file formats on intranet and internet [15].",
                "Office and PDF are the main file formats on the intranet.",
                "Even on the internet, the documents in the formats are still not negligible, given its extremely large size.",
                "In this paper, without loss of generality, we take Office documents as an example.",
                "Figure 1.",
                "Distributions of file formats in internet and intranet.",
                "For Office documents, users can define titles as file properties using a feature provided by Office.",
                "We found in an experiment, however, that users seldom use the feature and thus titles in file properties are usually very inaccurate.",
                "That is to say, titles in file properties are usually inconsistent with the true titles in the file bodies that are created by the authors and are visible to readers.",
                "We collected 6,000 Word and 6,000 PowerPoint documents from an intranet and the internet and examined how many titles in the file properties are correct.",
                "We found that surprisingly the accuracy was only 0.265 (cf., Section 6.3 for details).",
                "A number of reasons can be considered.",
                "For example, if one creates a new file by copying an old file, then the file property of the new file will also be copied from the old file.",
                "In another experiment, we found that Google uses the titles in file properties of Office documents in search and browsing, but the titles are not very accurate.",
                "We created 50 queries to search Word and PowerPoint documents and examined the top 15 results of each query returned by Google.",
                "We found that nearly all the titles presented in the search results were from the file properties of the documents.",
                "However, only 0.272 of them were correct.",
                "Actually, true titles usually exist at the beginnings of the bodies of documents.",
                "If we can accurately extract the titles from the bodies of documents, then we can exploit reliable title information in document processing.",
                "This is exactly the problem we address in this paper.",
                "More specifically, given a Word document, we are to extract the title from the top region of the first page.",
                "Given a PowerPoint document, we are to extract the title from the first slide.",
                "A title sometimes consists of a main title and one or two subtitles.",
                "We only consider extraction of the main title.",
                "As baselines for title extraction, we use that of always using the first lines as titles and that of always using the lines with largest font sizes as titles.",
                "Figure 2.",
                "Title extraction from Word document.",
                "Figure 3.",
                "Title extraction from PowerPoint document.",
                "Next, we define a specification for human judgments in title data annotation.",
                "The annotated data will be used in training and testing of the title extraction methods.",
                "Summary of the specification: The title of a document should be identified on the basis of common sense, if there is no difficulty in the identification.",
                "However, there are many cases in which the identification is not easy.",
                "There are some rules defined in the specification that guide identification for such cases.",
                "The rules include a title is usually in consecutive lines in the same format, a document can have no title, titles in images are not considered, a title should not contain words like draft, 147 whitepaper, etc, if it is difficult to determine which is the title, select the one in the largest font size, and if it is still difficult to determine which is the title, select the first candidate. (The specification covers all the cases we have encountered in data annotation.)",
                "Figures 2 and 3 show examples of Office documents from which we conduct title extraction.",
                "In Figure 2, Differences in Win32 API Implementations among Windows Operating Systems is the title of the Word document.",
                "Microsoft Windows on the top of this page is a picture and thus is ignored.",
                "In Figure 3, Building Competitive Advantages through an Agile Infrastructure is the title of the PowerPoint document.",
                "We have developed a tool for annotation of titles by human annotators.",
                "Figure 4 shows a snapshot of the tool.",
                "Figure 4.",
                "Title annotation tool. 4.",
                "TITLE EXTRACTION METHOD 4.1 Outline Title extraction based on machine learning consists of training and extraction.",
                "The same pre-processing step occurs before training and extraction.",
                "During pre-processing, from the top region of the first page of a Word document or the first slide of a PowerPoint document a number of units for processing are extracted.",
                "If a line (lines are separated by return symbols) only has a single format, then the line will become a unit.",
                "If a line has several parts and each of them has its own format, then each part will become a unit.",
                "Each unit will be treated as an instance in learning.",
                "A unit contains not only content information (linguistic information) but also formatting information.",
                "The input to pre-processing is a document and the output of pre-processing is a sequence of units (instances).",
                "Figure 5 shows the units obtained from the document in Figure 2.",
                "Figure 5.",
                "Example of units.",
                "In learning, the input is sequences of units where each sequence corresponds to a document.",
                "We take labeled units (labeled as title_begin, title_end, or other) in the sequences as training data and construct models for identifying whether a unit is title_begin title_end, or other.",
                "We employ four types of models: Perceptron, Maximum Entropy (ME), Perceptron Markov Model (PMM), and Maximum Entropy Markov Model (MEMM).",
                "In extraction, the input is a sequence of units from one document.",
                "We employ one type of model to identify whether a unit is title_begin, title_end, or other.",
                "We then extract units from the unit labeled with title_begin to the unit labeled with title_end.",
                "The result is the extracted title of the document.",
                "The unique characteristic of our approach is that we mainly utilize formatting information for title extraction.",
                "Our assumption is that although general documents vary in styles, their formats have certain patterns and we can learn and utilize the patterns for title extraction.",
                "This is in contrast to the work by Han et al., in which only linguistic features are used for extraction from research papers. 4.2 Models The four models actually can be considered in the same metadata extraction framework.",
                "That is why we apply them together to our current problem.",
                "Each input is a sequence of instances kxxx L21 together with a sequence of labels kyyy L21 . ix and iy represents an instance and its label, respectively ( ki ,,2,1 L= ).",
                "Recall that an instance here represents a unit.",
                "A label represents title_begin, title_end, or other.",
                "Here, k is the number of units in a document.",
                "In learning, we train a model which can be generally denoted as a conditional probability distribution )|( 11 kk XXYYP LL where iX and iY denote random variables taking instance ix and label iy as values, respectively ( ki ,,2,1 L= ).",
                "Learning Tool Extraction Tool 21121 2222122221 1121111211 nknnknn kk kk yyyxxx yyyxxx yyyxxx LL LL LL LL → → → )|(maxarg 11 mkmmkm xxyyP LL )|( 11 kk XXYYP LL Conditional Distribution mkmm xxx L21 Figure 6.",
                "Metadata extraction model.",
                "We can make assumptions about the general model in order to make it simple enough for training. 148 For example, we can assume that kYY ,,1 L are independent of each other given kXX ,,1 L .",
                "Thus, we have )|()|( )|( 11 11 kk kk XYPXYP XXYYP L LL = In this way, we decompose the model into a number of classifiers.",
                "We train the classifiers locally using the labeled data.",
                "As the classifier, we employ the Perceptron or Maximum Entropy model.",
                "We can also assume that the first order Markov property holds for kYY ,,1 L given kXX ,,1 L .",
                "Thus, we have )|()|( )|( 111 11 kkk kk XYYPXYP XXYYP −= L LL Again, we obtain a number of classifiers.",
                "However, the classifiers are conditioned on the previous label.",
                "When we employ the Percepton or Maximum Entropy model as a classifier, the models become a Percepton Markov Model or Maximum Entropy Markov Model, respectively.",
                "That is to say, the two models are more precise.",
                "In extraction, given a new sequence of instances, we resort to one of the constructed models to assign a sequence of labels to the sequence of instances, i.e., perform extraction.",
                "For Perceptron and ME, we assign labels locally and combine the results globally later using heuristics.",
                "Specifically, we first identify the most likely title_begin.",
                "Then we find the most likely title_end within three units after the title_begin.",
                "Finally, we extract as a title the units between the title_begin and the title_end.",
                "For PMM and MEMM, we employ the Viterbi algorithm to find the globally optimal label sequence.",
                "In this paper, for Perceptron, we actually employ an improved variant of it, called Perceptron with Uneven Margin [13].",
                "This version of Perceptron can work well especially when the number of positive instances and the number of negative instances differ greatly, which is exactly the case in our problem.",
                "We also employ an improved version of Perceptron Markov Model in which the Perceptron model is the so-called Voted Perceptron [2].",
                "In addition, in training, the parameters of the model are updated globally rather than locally. 4.3 Features There are two types of features: format features and linguistic features.",
                "We mainly use the former.",
                "The features are used for both the title-begin and the title-end classifiers. 4.3.1 Format Features Font Size: There are four binary features that represent the normalized font size of the unit (recall that a unit has only one type of font).",
                "If the font size of the unit is the largest in the document, then the first feature will be 1, otherwise 0.",
                "If the font size is the smallest in the document, then the fourth feature will be 1, otherwise 0.",
                "If the font size is above the average font size and not the largest in the document, then the second feature will be 1, otherwise 0.",
                "If the font size is below the average font size and not the smallest, the third feature will be 1, otherwise 0.",
                "It is necessary to conduct normalization on font sizes.",
                "For example, in one document the largest font size might be 12pt, while in another the smallest one might be 18pt.",
                "Boldface: This binary feature represents whether or not the current unit is in boldface.",
                "Alignment: There are four binary features that respectively represent the location of the current unit: left, center, right, and unknown alignment.",
                "The following format features with respect to context play an important role in title extraction.",
                "Empty Neighboring Unit: There are two binary features that represent, respectively, whether or not the previous unit and the current unit are blank lines.",
                "Font Size Change: There are two binary features that represent, respectively, whether or not the font size of the previous unit and the font size of the next unit differ from that of the current unit.",
                "Alignment Change: There are two binary features that represent, respectively, whether or not the alignment of the previous unit and the alignment of the next unit differ from that of the current one.",
                "Same Paragraph: There are two binary features that represent, respectively, whether or not the previous unit and the next unit are in the same paragraph as the current unit. 4.3.2 Linguistic Features The linguistic features are based on key words.",
                "Positive Word: This binary feature represents whether or not the current unit begins with one of the positive words.",
                "The positive words include title:, subject:, subject line: For example, in some documents the lines of titles and authors have the same formats.",
                "However, if lines begin with one of the positive words, then it is likely that they are title lines.",
                "Negative Word: This binary feature represents whether or not the current unit begins with one of the negative words.",
                "The negative words include To, By, created by, updated by, etc.",
                "There are more negative words than positive words.",
                "The above linguistic features are language dependent.",
                "Word Count: A title should not be too long.",
                "We heuristically create four intervals: [1, 2], [3, 6], [7, 9] and [9, ∞) and define one feature for each interval.",
                "If the number of words in a title falls into an interval, then the corresponding feature will be 1; otherwise 0.",
                "Ending Character: This feature represents whether the unit ends with :, -, or other special characters.",
                "A title usually does not end with such a character. 5.",
                "<br>document retrieval</br> METHOD We describe our method of <br>document retrieval</br> using extracted titles.",
                "Typically, in information retrieval a document is split into a number of fields including body, title, and anchor text.",
                "A ranking function in search can use different weights for different fields of 149 the document.",
                "Also, titles are typically assigned high weights, indicating that they are important for <br>document retrieval</br>.",
                "As explained previously, our experiment has shown that a significant number of documents actually have incorrect titles in the file properties, and thus in addition of using them we use the extracted titles as one more field of the document.",
                "By doing this, we attempt to improve the overall precision.",
                "In this paper, we employ a modification of BM25 that allows field weighting [21].",
                "As fields, we make use of body, title, extracted title and anchor.",
                "First, for each term in the query we count the term frequency in each field of the document; each field frequency is then weighted according to the corresponding weight parameter: ∑= f tfft tfwwtf Similarly, we compute the document length as a weighted sum of lengths of each field.",
                "Average document length in the corpus becomes the average of all weighted document lengths. ∑= f ff dlwwdl In our experiments we used 75.0,8.11 == bk .",
                "Weight for content was 1.0, title was 10.0, anchor was 10.0, and extracted title was 5.0. 6.",
                "EXPERIMENTAL RESULTS 6.1 Data Sets and Evaluation Measures We used two data sets in our experiments.",
                "First, we downloaded and randomly selected 5,000 Word documents and 5,000 PowerPoint documents from an intranet of Microsoft.",
                "We call it MS hereafter.",
                "Second, we downloaded and randomly selected 500 Word and 500 PowerPoint documents from the DotGov and DotCom domains on the internet, respectively.",
                "Figure 7 shows the distributions of the genres of the documents.",
                "We see that the documents are indeed general documents as we define them.",
                "Figure 7.",
                "Distributions of document genres.",
                "Third, a data set in Chinese was also downloaded from the internet.",
                "It includes 500 Word documents and 500 PowerPoint documents in Chinese.",
                "We manually labeled the titles of all the documents, on the basis of our specification.",
                "Not all the documents in the two data sets have titles.",
                "Table 1 shows the percentages of the documents having titles.",
                "We see that DotCom and DotGov have more PowerPoint documents with titles than MS.",
                "This might be because PowerPoint documents published on the internet are more formal than those on the intranet.",
                "Table 1.",
                "The portion of documents with titles Domain Type MS DotCom DotGov Word 75.7% 77.8% 75.6% PowerPoint 82.1% 93.4% 96.4% In our experiments, we conducted evaluations on title extraction in terms of precision, recall, and F-measure.",
                "The evaluation measures are defined as follows: Precision: P = A / ( A + B ) Recall: R = A / ( A + C ) F-measure: F1 = 2PR / ( P + R ) Here, A, B, C, and D are numbers of documents as those defined in Table 2.",
                "Table 2.",
                "Contingence table with regard to title extraction Is title Is not title Extracted A B Not extracted C D 6.2 Baselines We test the accuracies of the two baselines described in section 4.2.",
                "They are denoted as largest font size and first line respectively. 6.3 Accuracy of Titles in File Properties We investigate how many titles in the file properties of the documents are reliable.",
                "We view the titles annotated by humans as true titles and test how many titles in the file properties can approximately match with the true titles.",
                "We use Edit Distance to conduct the approximate match. (Approximate match is only used in this evaluation).",
                "This is because sometimes human annotated titles can be slightly different from the titles in file properties on the surface, e.g., contain extra spaces).",
                "Given string A and string B: if ( (D == 0) or ( D / ( La + Lb ) < θ ) ) then string A = string B D: Edit Distance between string A and string B La: length of string A Lb: length of string B θ: 0.1 ∑ × ++− + = t t n N wtf avwdl wdl bbk kwtf FBM )log( ))1(( )1( 25 1 1 150 Table 3.",
                "Accuracies of titles in file properties File Type Domain Precision Recall F1 MS 0.299 0.311 0.305 DotCom 0.210 0.214 0.212Word DotGov 0.182 0.177 0.180 MS 0.229 0.245 0.237 DotCom 0.185 0.186 0.186PowerPoint DotGov 0.180 0.182 0.181 6.4 Comparison with Baselines We conducted title extraction from the first data set (Word and PowerPoint in MS).",
                "As the model, we used Perceptron.",
                "We conduct 4-fold cross validation.",
                "Thus, all the results reported here are those averaged over 4 trials.",
                "Tables 4 and 5 show the results.",
                "We see that Perceptron significantly outperforms the baselines.",
                "In the evaluation, we use exact matching between the true titles annotated by humans and the extracted titles.",
                "Table 4.",
                "Accuracies of title extraction with Word Precision Recall F1 Model Perceptron 0.810 0.837 0.823 Largest font size 0.700 0.758 0.727 Baselines First line 0.707 0.767 0.736 Table 5.",
                "Accuracies of title extraction with PowerPoint Precision Recall F1 Model Perceptron 0.875 0. 895 0.885 Largest font size 0.844 0.887 0.865 Baselines First line 0.639 0.671 0.655 We see that the machine learning approach can achieve good performance in title extraction.",
                "For Word documents both precision and recall of the approach are 8 percent higher than those of the baselines.",
                "For PowerPoint both precision and recall of the approach are 2 percent higher than those of the baselines.",
                "We conduct significance tests.",
                "The results are shown in Table 6.",
                "Here, Largest denotes the baseline of using the largest font size, First denotes the baseline of using the first line.",
                "The results indicate that the improvements of machine learning over baselines are statistically significant (in the sense p-value < 0.05) Table 6.",
                "Sign test results Documents Type Sign test between p-value Perceptron vs. Largest 3.59e-26 Word Perceptron vs. First 7.12e-10 Perceptron vs. Largest 0.010 PowerPoint Perceptron vs. First 5.13e-40 We see, from the results, that the two baselines can work well for title extraction, suggesting that font size and position information are most useful features for title extraction.",
                "However, it is also obvious that using only these two features is not enough.",
                "There are cases in which all the lines have the same font size (i.e., the largest font size), or cases in which the lines with the largest font size only contain general descriptions like Confidential, White paper, etc.",
                "For those cases, the largest font size method cannot work well.",
                "For similar reasons, the first line method alone cannot work well, either.",
                "With the combination of different features (evidence in title judgment), Perceptron can outperform Largest and First.",
                "We investigate the performance of solely using linguistic features.",
                "We found that it does not work well.",
                "It seems that the format features play important roles and the linguistic features are supplements..",
                "Figure 8.",
                "An example Word document.",
                "Figure 9.",
                "An example PowerPoint document.",
                "We conducted an error analysis on the results of Perceptron.",
                "We found that the errors fell into three categories. (1) About one third of the errors were related to hard cases.",
                "In these documents, the layouts of the first pages were difficult to understand, even for humans.",
                "Figure 8 and 9 shows examples. (2) Nearly one fourth of the errors were from the documents which do not have true titles but only contain bullets.",
                "Since we conduct extraction from the top regions, it is difficult to get rid of these errors with the current approach. (3).",
                "Confusions between main titles and subtitles were another type of error.",
                "Since we only labeled the main titles as titles, the extractions of both titles were considered incorrect.",
                "This type of error does little harm to document processing like search, however. 6.5 Comparison between Models To compare the performance of different machine learning models, we conducted another experiment.",
                "Again, we perform 4-fold cross 151 validation on the first data set (MS).",
                "Table 7, 8 shows the results of all the four models.",
                "It turns out that Perceptron and PMM perform the best, followed by MEMM, and ME performs the worst.",
                "In general, the Markovian models perform better than or as well as their classifier counterparts.",
                "This seems to be because the Markovian models are trained globally, while the classifiers are trained locally.",
                "The Perceptron based models perform better than the ME based counterparts.",
                "This seems to be because the Perceptron based models are created to make better classifications, while ME models are constructed for better prediction.",
                "Table 7.",
                "Comparison between different learning models for title extraction with Word Model Precision Recall F1 Perceptron 0.810 0.837 0.823 MEMM 0.797 0.824 0.810 PMM 0.827 0.823 0.825 ME 0.801 0.621 0.699 Table 8.",
                "Comparison between different learning models for title extraction with PowerPoint Model Precision Recall F1 Perceptron 0.875 0. 895 0. 885 MEMM 0.841 0.861 0.851 PMM 0.873 0.896 0.885 ME 0.753 0.766 0.759 6.6 Domain Adaptation We apply the model trained with the first data set (MS) to the second data set (DotCom and DotGov).",
                "Tables 9-12 show the results.",
                "Table 9.",
                "Accuracies of title extraction with Word in DotGov Precision Recall F1 Model Perceptron 0.716 0.759 0.737 Largest font size 0.549 0.619 0.582Baselines First line 0.462 0.521 0.490 Table 10.",
                "Accuracies of title extraction with PowerPoint in DotGov Precision Recall F1 Model Perceptron 0.900 0.906 0.903 Largest font size 0.871 0.888 0.879Baselines First line 0.554 0.564 0.559 Table 11.",
                "Accuracies of title extraction with Word in DotCom Precisio n Recall F1 Model Perceptron 0.832 0.880 0.855 Largest font size 0.676 0.753 0.712Baselines First line 0.577 0.643 0.608 Table 12.",
                "Performance of PowerPoint document title extraction in DotCom Precisio n Recall F1 Model Perceptron 0.910 0.903 0.907 Largest font size 0.864 0.886 0.875Baselines First line 0.570 0.585 0.577 From the results, we see that the models can be adapted to different domains well.",
                "There is almost no drop in accuracy.",
                "The results indicate that the patterns of title formats exist across different domains, and it is possible to construct a domain independent model by mainly using formatting information. 6.7 Language Adaptation We apply the model trained with the data in English (MS) to the data set in Chinese.",
                "Tables 13-14 show the results.",
                "Table 13.",
                "Accuracies of title extraction with Word in Chinese Precision Recall F1 Model Perceptron 0.817 0.805 0.811 Largest font size 0.722 0.755 0.738Baselines First line 0.743 0.777 0.760 Table 14.",
                "Accuracies of title extraction with PowerPoint in Chinese Precision Recall F1 Model Perceptron 0.766 0.812 0.789 Largest font size 0.753 0.813 0.782Baselines First line 0.627 0.676 0.650 We see that the models can be adapted to a different language.",
                "There are only small drops in accuracy.",
                "Obviously, the linguistic features do not work for Chinese, but the effect of not using them is negligible.",
                "The results indicate that the patterns of title formats exist across different languages.",
                "From the domain adaptation and language adaptation results, we conclude that the use of formatting information is the key to a successful extraction from general documents. 6.8 Search with Extracted Titles We performed experiments on using title extraction for <br>document retrieval</br>.",
                "As a baseline, we employed BM25 without using extracted titles.",
                "The ranking mechanism was as described in Section 5.",
                "The weights were heuristically set.",
                "We did not conduct optimization on the weights.",
                "The evaluation was conducted on a corpus of 1.3 M documents crawled from the intranet of Microsoft using 100 evaluation queries obtained from this intranets search engine query logs. 50 queries were from the most popular set, while 50 queries other were chosen randomly.",
                "Users were asked to provide judgments of the degree of document relevance from a scale of 1to 5 (1 meaning detrimental, 2 - bad, 3 - fair, 4 - good and 5 - excellent). 152 Figure 10 shows the results.",
                "In the chart two sets of precision results were obtained by either considering good or excellent documents as relevant (left 3 bars with relevance threshold 0.5), or by considering only excellent documents as relevant (right 3 bars with relevance threshold 1.0) 0 0.05 0.1 0.15 0.2 0.25 0.3 0.35 0.4 0.45 P@10 P@5 Reciprocal P@10 P@5 Reciprocal 0.5 1 BM25 Anchor, Title, Body BM25 Anchor, Title, Body, ExtractedTitle Name All RelevanceThreshold Data Description Figure 10.",
                "Search ranking results.",
                "Figure 10 shows different <br>document retrieval</br> results with different ranking functions in terms of precision @10, precision @5 and reciprocal rank: • Blue bar - BM25 including the fields body, title (file property), and anchor text. • Purple bar - BM25 including the fields body, title (file property), anchor text, and extracted title.",
                "With the additional field of extracted title included in BM25 the precision @10 increased from 0.132 to 0.145, or by ~10%.",
                "Thus, it is safe to say that the use of extracted title can indeed improve the precision of <br>document retrieval</br>. 7.",
                "CONCLUSION In this paper, we have investigated the problem of automatically extracting titles from general documents.",
                "We have tried using a machine learning approach to address the problem.",
                "Previous work showed that the machine learning approach can work well for metadata extraction from research papers.",
                "In this paper, we showed that the approach can work for extraction from general documents as well.",
                "Our experimental results indicated that the machine learning approach can work significantly better than the baselines in title extraction from Office documents.",
                "Previous work on metadata extraction mainly used linguistic features in documents, while we mainly used formatting information.",
                "It appeared that using formatting information is a key for successfully conducting title extraction from general documents.",
                "We tried different machine learning models including Perceptron, Maximum Entropy, Maximum Entropy Markov Model, and Voted Perceptron.",
                "We found that the performance of the Perceptorn models was the best.",
                "We applied models constructed in one domain to another domain and applied models trained in one language to another language.",
                "We found that the accuracies did not drop substantially across different domains and across different languages, indicating that the models were generic.",
                "We also attempted to use the extracted titles in <br>document retrieval</br>.",
                "We observed a significant improvement in document ranking performance for search when using extracted title information.",
                "All the above investigations were not conducted in previous work, and through our investigations we verified the generality and the significance of the title extraction approach. 8.",
                "ACKNOWLEDGEMENTS We thank Chunyu Wei and Bojuan Zhao for their work on data annotation.",
                "We acknowledge Jinzhu Li for his assistance in conducting the experiments.",
                "We thank Ming Zhou, John Chen, Jun Xu, and the anonymous reviewers of JCDL05 for their valuable comments on this paper. 9.",
                "REFERENCES [1] Berger, A. L., Della Pietra, S. A., and Della Pietra, V. J.",
                "A maximum entropy approach to natural language processing.",
                "Computational Linguistics, 22:39-71, 1996. [2] Collins, M. Discriminative training methods for hidden markov models: theory and experiments with perceptron algorithms.",
                "In Proceedings of Conference on Empirical Methods in Natural Language Processing, 1-8, 2002. [3] Cortes, C. and Vapnik, V. Support-vector networks.",
                "Machine Learning, 20:273-297, 1995. [4] Chieu, H. L. and Ng, H. T. A maximum entropy approach to information extraction from semi-structured and free text.",
                "In Proceedings of the Eighteenth National Conference on Artificial Intelligence, 768-791, 2002. [5] Evans, D. K., Klavans, J. L., and McKeown, K. R. Columbia newsblaster: multilingual news summarization on the Web.",
                "In Proceedings of Human Language Technology conference / North American chapter of the Association for Computational Linguistics annual meeting, 1-4, 2004. [6] Ghahramani, Z. and Jordan, M. I. Factorial hidden markov models.",
                "Machine Learning, 29:245-273, 1997. [7] Gheel, J. and Anderson, T. Data and metadata for finding and reminding, In Proceedings of the 1999 International Conference on Information Visualization, 446-451,1999. [8] Giles, C. L., Petinot, Y., Teregowda P. B., Han, H., Lawrence, S., Rangaswamy, A., and Pal, N. eBizSearch: a niche search engine for e-Business.",
                "In Proceedings of the 26th Annual International ACM SIGIR Conference on Research and Development in Information Retrieval, 413414, 2003. [9] Giuffrida, G., Shek, E. C., and Yang, J. Knowledge-based metadata extraction from PostScript files.",
                "In Proceedings of the Fifth ACM Conference on Digital Libraries, 77-84, 2000. [10] Han, H., Giles, C. L., Manavoglu, E., Zha, H., Zhang, Z., and Fox, E. A.",
                "Automatic document metadata extraction using support vector machines.",
                "In Proceedings of the Third ACM/IEEE-CS Joint Conference on Digital Libraries, 37-48, 2003. [11] Kobayashi, M., and Takeda, K. Information retrieval on the Web.",
                "ACM Computing Surveys, 32:144-173, 2000. [12] Lafferty, J., McCallum, A., and Pereira, F. Conditional random fields: probabilistic models for segmenting and 153 labeling sequence data.",
                "In Proceedings of the Eighteenth International Conference on Machine Learning, 282-289, 2001. [13] Li, Y., Zaragoza, H., Herbrich, R., Shawe-Taylor J., and Kandola, J. S. The perceptron algorithm with uneven margins.",
                "In Proceedings of the Nineteenth International Conference on Machine Learning, 379-386, 2002. [14] Liddy, E. D., Sutton, S., Allen, E., Harwell, S., Corieri, S., Yilmazel, O., Ozgencil, N. E., Diekema, A., McCracken, N., and Silverstein, J.",
                "Automatic Metadata generation & evaluation.",
                "In Proceedings of the 25th Annual International ACM SIGIR Conference on Research and Development in Information Retrieval, 401-402, 2002. [15] Littlefield, A.",
                "Effective enterprise information retrieval across new content formats.",
                "In Proceedings of the Seventh Search Engine Conference, http://www.infonortics.com/searchengines/sh02/02prog.html, 2002. [16] Mao, S., Kim, J. W., and Thoma, G. R. A dynamic feature generation system for automated metadata extraction in preservation of digital materials.",
                "In Proceedings of the First International Workshop on Document Image Analysis for Libraries, 225-232, 2004. [17] McCallum, A., Freitag, D., and Pereira, F. Maximum entropy markov models for information extraction and segmentation.",
                "In Proceedings of the Seventeenth International Conference on Machine Learning, 591-598, 2000. [18] Murphy, L. D. Digital document metadata in organizations: roles, analytical approaches, and future research directions.",
                "In Proceedings of the Thirty-First Annual Hawaii International Conference on System Sciences, 267-276, 1998. [19] Pinto, D., McCallum, A., Wei, X., and Croft, W. B.",
                "Table extraction using conditional random fields.",
                "In Proceedings of the 26th Annual International ACM SIGIR Conference on Research and Development in Information Retrieval, 235242, 2003. [20] Ratnaparkhi, A. Unsupervised statistical models for prepositional phrase attachment.",
                "In Proceedings of the Seventeenth International Conference on Computational Linguistics. 1079-1085, 1998. [21] Robertson, S., Zaragoza, H., and Taylor, M. Simple BM25 extension to multiple weighted fields, In Proceedings of ACM Thirteenth Conference on Information and Knowledge Management, 42-49, 2004. [22] Yi, J. and Sundaresan, N. Metadata based Web mining for relevance, In Proceedings of the 2000 International Symposium on Database Engineering & Applications, 113121, 2000. [23] Yilmazel, O., Finneran, C. M., and Liddy, E. D. MetaExtract: An NLP system to automatically assign metadata.",
                "In Proceedings of the 2004 Joint ACM/IEEE Conference on Digital Libraries, 241-242, 2004. [24] Zhang, J. and Dimitroff, A. Internet search engines response to metadata Dublin Core implementation.",
                "Journal of Information Science, 30:310-320, 2004. [25] Zhang, L., Pan, Y., and Zhang, T. Recognising and using named entities: focused named entity recognition using machine learning.",
                "In Proceedings of the 27th Annual International ACM SIGIR Conference on Research and Development in Information Retrieval, 281-288, 2004. [26] http://dublincore.org/groups/corporate/Seattle/ 154"
            ],
            "original_annotated_samples": [],
            "translated_annotated_samples": [
                "Además, podemos mejorar significativamente los resultados de la clasificación de búsqueda en la \"recuperación de documentos\" mediante el uso de los títulos extraídos.",
                "Resulta que el uso de características de formato es la clave para una extracción de título exitosa.(1) Hemos observado que los modelos basados en Perceptron funcionan mejor en términos de precisiones de extracción.(2) Hemos verificado empíricamente que los modelos entrenados con nuestro enfoque son genéricos en el sentido de que pueden ser entrenados en un dominio y aplicados a otro, y pueden ser entrenados en un idioma y aplicarse a otro.(3) Hemos encontrado que utilizando los títulos extraídos podemos mejorar significativamente la precisión de la \"recuperación de documentos\" (en un 10%).",
                "En la Sección 4, describimos nuestro método de extracción de título, y en la Sección 5, describimos nuestro método de \"recuperación de documentos\" utilizando títulos extraídos.",
                "La extracción de información se ha aplicado, por ejemplo, al etiquetado de parto de expresión [20], nombrado reconocimiento de entidad [25] y extracción de tabla [19].2.3 Búsqueda utilizando la información del título La información del título es útil para la \"recuperación de documentos\".",
                "Método de \"recuperación de documentos\" Describimos nuestro método de \"recuperación de documentos\" utilizando títulos extraídos.",
                "Además, los títulos generalmente se les asigna altos pesos, lo que indica que son importantes para la \"recuperación de documentos\".",
                "De la adaptación del dominio y los resultados de la adaptación del lenguaje, concluimos que el uso de información de formato es la clave para una extracción exitosa de documentos generales.6.8 Búsqueda con títulos extraídos realizamos experimentos sobre el uso de la extracción de título para \"recuperación de documentos\".",
                "La Figura 10 muestra diferentes resultados de \"recuperación de documentos\" con diferentes funciones de clasificación en términos de precisión @10, precisión @5 y rango recíproco: • Barra azul - BM25 que incluye el cuerpo de campos, el título (propiedad del archivo) y el texto de anclaje.• Purple Bar: BM25, incluido el cuerpo de campos, el título (propiedad del archivo), el texto de anclaje y el título extraído.",
                "Por lo tanto, es seguro decir que el uso del título extraído puede mejorar la precisión de la \"recuperación de documentos\".7.",
                "También intentamos usar los títulos extraídos en \"Recuperación de documentos\"."
            ],
            "translated_text": "",
            "candidates": [
                "recuperación de documentos",
                "recuperación de documentos",
                "recuperación de documentos",
                "recuperación de documentos",
                "recuperación de documentos",
                "recuperación de documentos",
                "recuperación de documentos",
                "recuperación de documentos",
                "recuperación de documentos",
                "recuperación de documentos",
                "recuperación de documentos",
                "recuperación de documentos",
                "recuperación de documentos",
                "recuperación de documentos",
                "recuperación de documentos",
                "recuperación de documentos",
                "recuperación de documentos",
                "Recuperación de documentos",
                "recuperación de documentos",
                "recuperación de documentos",
                "Recuperación de documentos"
            ],
            "error": []
        },
        "automatic title extraction": {
            "translated_key": "extracción de títulos automáticos",
            "is_in_text": true,
            "original_annotated_sentences": [
                "Automatic Extraction of Titles from General Documents using Machine Learning Yunhua Hu1 Computer Science Department Xian Jiaotong University No 28, Xianning West Road Xian, China, 710049 yunhuahu@mail.xjtu.edu.cn Hang Li, Yunbo Cao Microsoft Research Asia 5F Sigma Center, No.",
                "49 Zhichun Road, Haidian, Beijing, China, 100080 {hangli,yucao}@microsoft.com Qinghua Zheng Computer Science Department Xian Jiaotong University No 28, Xianning West Road Xian, China, 710049 qhzheng@mail.xjtu.edu.cn Dmitriy Meyerzon Microsoft Corporation One Microsoft Way Redmond, WA, USA, 98052 dmitriym@microsoft.com ABSTRACT In this paper, we propose a machine learning approach to title extraction from general documents.",
                "By general documents, we mean documents that can belong to any one of a number of specific genres, including presentations, book chapters, technical papers, brochures, reports, and letters.",
                "Previously, methods have been proposed mainly for title extraction from research papers.",
                "It has not been clear whether it could be possible to conduct <br>automatic title extraction</br> from general documents.",
                "As a case study, we consider extraction from Office including Word and PowerPoint.",
                "In our approach, we annotate titles in sample documents (for Word and PowerPoint respectively) and take them as training data, train machine learning models, and perform title extraction using the trained models.",
                "Our method is unique in that we mainly utilize formatting information such as font size as features in the models.",
                "It turns out that the use of formatting information can lead to quite accurate extraction from general documents.",
                "Precision and recall for title extraction from Word is 0.810 and 0.837 respectively, and precision and recall for title extraction from PowerPoint is 0.875 and 0.895 respectively in an experiment on intranet data.",
                "Other important new findings in this work include that we can train models in one domain and apply them to another domain, and more surprisingly we can even train models in one language and apply them to another language.",
                "Moreover, we can significantly improve search ranking results in document retrieval by using the extracted titles.",
                "Categories and Subject Descriptors H.3.3 [Information Storage and Retrieval]: Information Search and Retrieval - Search Process; H.4.1 [Information Systems Applications]: Office Automation - Word processing; D.2.8 [Software Engineering]: Metrics - complexity measures, performance measures General Terms Algorithms, Experimentation, Performance. 1.",
                "INTRODUCTION Metadata of documents is useful for many kinds of document processing such as search, browsing, and filtering.",
                "Ideally, metadata is defined by the authors of documents and is then used by various systems.",
                "However, people seldom define document metadata by themselves, even when they have convenient metadata definition tools [26].",
                "Thus, how to automatically extract metadata from the bodies of documents turns out to be an important research issue.",
                "Methods for performing the task have been proposed.",
                "However, the focus was mainly on extraction from research papers.",
                "For instance, Han et al. [10] proposed a machine learning based method to conduct extraction from research papers.",
                "They formalized the problem as that of classification and employed Support Vector Machines as the classifier.",
                "They mainly used linguistic features in the model.1 In this paper, we consider metadata extraction from general documents.",
                "By general documents, we mean documents that may belong to any one of a number of specific genres.",
                "General documents are more widely available in digital libraries, intranets and the internet, and thus investigation on extraction from them is sorely needed.",
                "Research papers usually have well-formed styles and noticeable characteristics.",
                "In contrast, the styles of general documents can vary greatly.",
                "It has not been clarified whether a machine learning based approach can work well for this task.",
                "There are many types of metadata: title, author, date of creation, etc.",
                "As a case study, we consider title extraction in this paper.",
                "General documents can be in many different file formats: Microsoft Office, PDF (PS), etc.",
                "As a case study, we consider extraction from Office including Word and PowerPoint.",
                "We take a machine learning approach.",
                "We annotate titles in sample documents (for Word and PowerPoint respectively) and take them as training data to train several types of models, and perform title extraction using any one type of the trained models.",
                "In the models, we mainly utilize formatting information such as font size as features.",
                "We employ the following models: Maximum Entropy Model, Perceptron with Uneven Margins, Maximum Entropy Markov Model, and Voted Perceptron.",
                "In this paper, we also investigate the following three problems, which did not seem to have been examined previously. (1) Comparison between models: among the models above, which model performs best for title extraction; (2) Generality of model: whether it is possible to train a model on one domain and apply it to another domain, and whether it is possible to train a model in one language and apply it to another language; (3) Usefulness of extracted titles: whether extracted titles can improve document processing such as search.",
                "Experimental results indicate that our approach works well for title extraction from general documents.",
                "Our method can significantly outperform the baselines: one that always uses the first lines as titles and the other that always uses the lines in the largest font sizes as titles.",
                "Precision and recall for title extraction from Word are 0.810 and 0.837 respectively, and precision and recall for title extraction from PowerPoint are 0.875 and 0.895 respectively.",
                "It turns out that the use of format features is the key to successful title extraction. (1) We have observed that Perceptron based models perform better in terms of extraction accuracies. (2) We have empirically verified that the models trained with our approach are generic in the sense that they can be trained on one domain and applied to another, and they can be trained in one language and applied to another. (3) We have found that using the extracted titles we can significantly improve precision of document retrieval (by 10%).",
                "We conclude that we can indeed conduct reliable title extraction from general documents and use the extracted results to improve real applications.",
                "The rest of the paper is organized as follows.",
                "In section 2, we introduce related work, and in section 3, we explain the motivation and problem setting of our work.",
                "In section 4, we describe our method of title extraction, and in section 5, we describe our method of document retrieval using extracted titles.",
                "Section 6 gives our experimental results.",
                "We make concluding remarks in section 7. 2.",
                "RELATED WORK 2.1 Document Metadata Extraction Methods have been proposed for performing automatic metadata extraction from documents; however, the main focus was on extraction from research papers.",
                "The proposed methods fall into two categories: the rule based approach and the machine learning based approach.",
                "Giuffrida et al. [9], for instance, developed a rule-based system for automatically extracting metadata from research papers in Postscript.",
                "They used rules like titles are usually located on the upper portions of the first pages and they are usually in the largest font sizes.",
                "Liddy et al. [14] and Yilmazel el al. [23] performed metadata extraction from educational materials using rule-based natural language processing technologies.",
                "Mao et al. [16] also conducted automatic metadata extraction from research papers using rules on formatting information.",
                "The rule-based approach can achieve high performance.",
                "However, it also has disadvantages.",
                "It is less adaptive and robust when compared with the machine learning approach.",
                "Han et al. [10], for instance, conducted metadata extraction with the machine learning approach.",
                "They viewed the problem as that of classifying the lines in a document into the categories of metadata and proposed using Support Vector Machines as the classifier.",
                "They mainly used linguistic information as features.",
                "They reported high extraction accuracy from research papers in terms of precision and recall. 2.2 Information Extraction Metadata extraction can be viewed as an application of information extraction, in which given a sequence of instances, we identify a subsequence that represents information in which we are interested.",
                "Hidden Markov Model [6], Maximum Entropy Model [1, 4], Maximum Entropy Markov Model [17], Support Vector Machines [3], Conditional Random Field [12], and Voted Perceptron [2] are widely used information extraction models.",
                "Information extraction has been applied, for instance, to part-ofspeech tagging [20], named entity recognition [25] and table extraction [19]. 2.3 Search Using Title Information Title information is useful for document retrieval.",
                "In the system Citeseer, for instance, Giles et al. managed to extract titles from research papers and make use of the extracted titles in metadata search of papers [8].",
                "In web search, the title fields (i.e., file properties) and anchor texts of web pages (HTML documents) can be viewed as titles of the pages [5].",
                "Many search engines seem to utilize them for web page retrieval [7, 11, 18, 22].",
                "Zhang et al., found that web pages with well-defined metadata are more easily retrieved than those without well-defined metadata [24].",
                "To the best of our knowledge, no research has been conducted on using extracted titles from general documents (e.g., Office documents) for search of the documents. 146 3.",
                "MOTIVATION AND PROBLEM SETTING We consider the issue of automatically extracting titles from general documents.",
                "By general documents, we mean documents that belong to one of any number of specific genres.",
                "The documents can be presentations, books, book chapters, technical papers, brochures, reports, memos, specifications, letters, announcements, or resumes.",
                "General documents are more widely available in digital libraries, intranets, and internet, and thus investigation on title extraction from them is sorely needed.",
                "Figure 1 shows an estimate on distributions of file formats on intranet and internet [15].",
                "Office and PDF are the main file formats on the intranet.",
                "Even on the internet, the documents in the formats are still not negligible, given its extremely large size.",
                "In this paper, without loss of generality, we take Office documents as an example.",
                "Figure 1.",
                "Distributions of file formats in internet and intranet.",
                "For Office documents, users can define titles as file properties using a feature provided by Office.",
                "We found in an experiment, however, that users seldom use the feature and thus titles in file properties are usually very inaccurate.",
                "That is to say, titles in file properties are usually inconsistent with the true titles in the file bodies that are created by the authors and are visible to readers.",
                "We collected 6,000 Word and 6,000 PowerPoint documents from an intranet and the internet and examined how many titles in the file properties are correct.",
                "We found that surprisingly the accuracy was only 0.265 (cf., Section 6.3 for details).",
                "A number of reasons can be considered.",
                "For example, if one creates a new file by copying an old file, then the file property of the new file will also be copied from the old file.",
                "In another experiment, we found that Google uses the titles in file properties of Office documents in search and browsing, but the titles are not very accurate.",
                "We created 50 queries to search Word and PowerPoint documents and examined the top 15 results of each query returned by Google.",
                "We found that nearly all the titles presented in the search results were from the file properties of the documents.",
                "However, only 0.272 of them were correct.",
                "Actually, true titles usually exist at the beginnings of the bodies of documents.",
                "If we can accurately extract the titles from the bodies of documents, then we can exploit reliable title information in document processing.",
                "This is exactly the problem we address in this paper.",
                "More specifically, given a Word document, we are to extract the title from the top region of the first page.",
                "Given a PowerPoint document, we are to extract the title from the first slide.",
                "A title sometimes consists of a main title and one or two subtitles.",
                "We only consider extraction of the main title.",
                "As baselines for title extraction, we use that of always using the first lines as titles and that of always using the lines with largest font sizes as titles.",
                "Figure 2.",
                "Title extraction from Word document.",
                "Figure 3.",
                "Title extraction from PowerPoint document.",
                "Next, we define a specification for human judgments in title data annotation.",
                "The annotated data will be used in training and testing of the title extraction methods.",
                "Summary of the specification: The title of a document should be identified on the basis of common sense, if there is no difficulty in the identification.",
                "However, there are many cases in which the identification is not easy.",
                "There are some rules defined in the specification that guide identification for such cases.",
                "The rules include a title is usually in consecutive lines in the same format, a document can have no title, titles in images are not considered, a title should not contain words like draft, 147 whitepaper, etc, if it is difficult to determine which is the title, select the one in the largest font size, and if it is still difficult to determine which is the title, select the first candidate. (The specification covers all the cases we have encountered in data annotation.)",
                "Figures 2 and 3 show examples of Office documents from which we conduct title extraction.",
                "In Figure 2, Differences in Win32 API Implementations among Windows Operating Systems is the title of the Word document.",
                "Microsoft Windows on the top of this page is a picture and thus is ignored.",
                "In Figure 3, Building Competitive Advantages through an Agile Infrastructure is the title of the PowerPoint document.",
                "We have developed a tool for annotation of titles by human annotators.",
                "Figure 4 shows a snapshot of the tool.",
                "Figure 4.",
                "Title annotation tool. 4.",
                "TITLE EXTRACTION METHOD 4.1 Outline Title extraction based on machine learning consists of training and extraction.",
                "The same pre-processing step occurs before training and extraction.",
                "During pre-processing, from the top region of the first page of a Word document or the first slide of a PowerPoint document a number of units for processing are extracted.",
                "If a line (lines are separated by return symbols) only has a single format, then the line will become a unit.",
                "If a line has several parts and each of them has its own format, then each part will become a unit.",
                "Each unit will be treated as an instance in learning.",
                "A unit contains not only content information (linguistic information) but also formatting information.",
                "The input to pre-processing is a document and the output of pre-processing is a sequence of units (instances).",
                "Figure 5 shows the units obtained from the document in Figure 2.",
                "Figure 5.",
                "Example of units.",
                "In learning, the input is sequences of units where each sequence corresponds to a document.",
                "We take labeled units (labeled as title_begin, title_end, or other) in the sequences as training data and construct models for identifying whether a unit is title_begin title_end, or other.",
                "We employ four types of models: Perceptron, Maximum Entropy (ME), Perceptron Markov Model (PMM), and Maximum Entropy Markov Model (MEMM).",
                "In extraction, the input is a sequence of units from one document.",
                "We employ one type of model to identify whether a unit is title_begin, title_end, or other.",
                "We then extract units from the unit labeled with title_begin to the unit labeled with title_end.",
                "The result is the extracted title of the document.",
                "The unique characteristic of our approach is that we mainly utilize formatting information for title extraction.",
                "Our assumption is that although general documents vary in styles, their formats have certain patterns and we can learn and utilize the patterns for title extraction.",
                "This is in contrast to the work by Han et al., in which only linguistic features are used for extraction from research papers. 4.2 Models The four models actually can be considered in the same metadata extraction framework.",
                "That is why we apply them together to our current problem.",
                "Each input is a sequence of instances kxxx L21 together with a sequence of labels kyyy L21 . ix and iy represents an instance and its label, respectively ( ki ,,2,1 L= ).",
                "Recall that an instance here represents a unit.",
                "A label represents title_begin, title_end, or other.",
                "Here, k is the number of units in a document.",
                "In learning, we train a model which can be generally denoted as a conditional probability distribution )|( 11 kk XXYYP LL where iX and iY denote random variables taking instance ix and label iy as values, respectively ( ki ,,2,1 L= ).",
                "Learning Tool Extraction Tool 21121 2222122221 1121111211 nknnknn kk kk yyyxxx yyyxxx yyyxxx LL LL LL LL → → → )|(maxarg 11 mkmmkm xxyyP LL )|( 11 kk XXYYP LL Conditional Distribution mkmm xxx L21 Figure 6.",
                "Metadata extraction model.",
                "We can make assumptions about the general model in order to make it simple enough for training. 148 For example, we can assume that kYY ,,1 L are independent of each other given kXX ,,1 L .",
                "Thus, we have )|()|( )|( 11 11 kk kk XYPXYP XXYYP L LL = In this way, we decompose the model into a number of classifiers.",
                "We train the classifiers locally using the labeled data.",
                "As the classifier, we employ the Perceptron or Maximum Entropy model.",
                "We can also assume that the first order Markov property holds for kYY ,,1 L given kXX ,,1 L .",
                "Thus, we have )|()|( )|( 111 11 kkk kk XYYPXYP XXYYP −= L LL Again, we obtain a number of classifiers.",
                "However, the classifiers are conditioned on the previous label.",
                "When we employ the Percepton or Maximum Entropy model as a classifier, the models become a Percepton Markov Model or Maximum Entropy Markov Model, respectively.",
                "That is to say, the two models are more precise.",
                "In extraction, given a new sequence of instances, we resort to one of the constructed models to assign a sequence of labels to the sequence of instances, i.e., perform extraction.",
                "For Perceptron and ME, we assign labels locally and combine the results globally later using heuristics.",
                "Specifically, we first identify the most likely title_begin.",
                "Then we find the most likely title_end within three units after the title_begin.",
                "Finally, we extract as a title the units between the title_begin and the title_end.",
                "For PMM and MEMM, we employ the Viterbi algorithm to find the globally optimal label sequence.",
                "In this paper, for Perceptron, we actually employ an improved variant of it, called Perceptron with Uneven Margin [13].",
                "This version of Perceptron can work well especially when the number of positive instances and the number of negative instances differ greatly, which is exactly the case in our problem.",
                "We also employ an improved version of Perceptron Markov Model in which the Perceptron model is the so-called Voted Perceptron [2].",
                "In addition, in training, the parameters of the model are updated globally rather than locally. 4.3 Features There are two types of features: format features and linguistic features.",
                "We mainly use the former.",
                "The features are used for both the title-begin and the title-end classifiers. 4.3.1 Format Features Font Size: There are four binary features that represent the normalized font size of the unit (recall that a unit has only one type of font).",
                "If the font size of the unit is the largest in the document, then the first feature will be 1, otherwise 0.",
                "If the font size is the smallest in the document, then the fourth feature will be 1, otherwise 0.",
                "If the font size is above the average font size and not the largest in the document, then the second feature will be 1, otherwise 0.",
                "If the font size is below the average font size and not the smallest, the third feature will be 1, otherwise 0.",
                "It is necessary to conduct normalization on font sizes.",
                "For example, in one document the largest font size might be 12pt, while in another the smallest one might be 18pt.",
                "Boldface: This binary feature represents whether or not the current unit is in boldface.",
                "Alignment: There are four binary features that respectively represent the location of the current unit: left, center, right, and unknown alignment.",
                "The following format features with respect to context play an important role in title extraction.",
                "Empty Neighboring Unit: There are two binary features that represent, respectively, whether or not the previous unit and the current unit are blank lines.",
                "Font Size Change: There are two binary features that represent, respectively, whether or not the font size of the previous unit and the font size of the next unit differ from that of the current unit.",
                "Alignment Change: There are two binary features that represent, respectively, whether or not the alignment of the previous unit and the alignment of the next unit differ from that of the current one.",
                "Same Paragraph: There are two binary features that represent, respectively, whether or not the previous unit and the next unit are in the same paragraph as the current unit. 4.3.2 Linguistic Features The linguistic features are based on key words.",
                "Positive Word: This binary feature represents whether or not the current unit begins with one of the positive words.",
                "The positive words include title:, subject:, subject line: For example, in some documents the lines of titles and authors have the same formats.",
                "However, if lines begin with one of the positive words, then it is likely that they are title lines.",
                "Negative Word: This binary feature represents whether or not the current unit begins with one of the negative words.",
                "The negative words include To, By, created by, updated by, etc.",
                "There are more negative words than positive words.",
                "The above linguistic features are language dependent.",
                "Word Count: A title should not be too long.",
                "We heuristically create four intervals: [1, 2], [3, 6], [7, 9] and [9, ∞) and define one feature for each interval.",
                "If the number of words in a title falls into an interval, then the corresponding feature will be 1; otherwise 0.",
                "Ending Character: This feature represents whether the unit ends with :, -, or other special characters.",
                "A title usually does not end with such a character. 5.",
                "DOCUMENT RETRIEVAL METHOD We describe our method of document retrieval using extracted titles.",
                "Typically, in information retrieval a document is split into a number of fields including body, title, and anchor text.",
                "A ranking function in search can use different weights for different fields of 149 the document.",
                "Also, titles are typically assigned high weights, indicating that they are important for document retrieval.",
                "As explained previously, our experiment has shown that a significant number of documents actually have incorrect titles in the file properties, and thus in addition of using them we use the extracted titles as one more field of the document.",
                "By doing this, we attempt to improve the overall precision.",
                "In this paper, we employ a modification of BM25 that allows field weighting [21].",
                "As fields, we make use of body, title, extracted title and anchor.",
                "First, for each term in the query we count the term frequency in each field of the document; each field frequency is then weighted according to the corresponding weight parameter: ∑= f tfft tfwwtf Similarly, we compute the document length as a weighted sum of lengths of each field.",
                "Average document length in the corpus becomes the average of all weighted document lengths. ∑= f ff dlwwdl In our experiments we used 75.0,8.11 == bk .",
                "Weight for content was 1.0, title was 10.0, anchor was 10.0, and extracted title was 5.0. 6.",
                "EXPERIMENTAL RESULTS 6.1 Data Sets and Evaluation Measures We used two data sets in our experiments.",
                "First, we downloaded and randomly selected 5,000 Word documents and 5,000 PowerPoint documents from an intranet of Microsoft.",
                "We call it MS hereafter.",
                "Second, we downloaded and randomly selected 500 Word and 500 PowerPoint documents from the DotGov and DotCom domains on the internet, respectively.",
                "Figure 7 shows the distributions of the genres of the documents.",
                "We see that the documents are indeed general documents as we define them.",
                "Figure 7.",
                "Distributions of document genres.",
                "Third, a data set in Chinese was also downloaded from the internet.",
                "It includes 500 Word documents and 500 PowerPoint documents in Chinese.",
                "We manually labeled the titles of all the documents, on the basis of our specification.",
                "Not all the documents in the two data sets have titles.",
                "Table 1 shows the percentages of the documents having titles.",
                "We see that DotCom and DotGov have more PowerPoint documents with titles than MS.",
                "This might be because PowerPoint documents published on the internet are more formal than those on the intranet.",
                "Table 1.",
                "The portion of documents with titles Domain Type MS DotCom DotGov Word 75.7% 77.8% 75.6% PowerPoint 82.1% 93.4% 96.4% In our experiments, we conducted evaluations on title extraction in terms of precision, recall, and F-measure.",
                "The evaluation measures are defined as follows: Precision: P = A / ( A + B ) Recall: R = A / ( A + C ) F-measure: F1 = 2PR / ( P + R ) Here, A, B, C, and D are numbers of documents as those defined in Table 2.",
                "Table 2.",
                "Contingence table with regard to title extraction Is title Is not title Extracted A B Not extracted C D 6.2 Baselines We test the accuracies of the two baselines described in section 4.2.",
                "They are denoted as largest font size and first line respectively. 6.3 Accuracy of Titles in File Properties We investigate how many titles in the file properties of the documents are reliable.",
                "We view the titles annotated by humans as true titles and test how many titles in the file properties can approximately match with the true titles.",
                "We use Edit Distance to conduct the approximate match. (Approximate match is only used in this evaluation).",
                "This is because sometimes human annotated titles can be slightly different from the titles in file properties on the surface, e.g., contain extra spaces).",
                "Given string A and string B: if ( (D == 0) or ( D / ( La + Lb ) < θ ) ) then string A = string B D: Edit Distance between string A and string B La: length of string A Lb: length of string B θ: 0.1 ∑ × ++− + = t t n N wtf avwdl wdl bbk kwtf FBM )log( ))1(( )1( 25 1 1 150 Table 3.",
                "Accuracies of titles in file properties File Type Domain Precision Recall F1 MS 0.299 0.311 0.305 DotCom 0.210 0.214 0.212Word DotGov 0.182 0.177 0.180 MS 0.229 0.245 0.237 DotCom 0.185 0.186 0.186PowerPoint DotGov 0.180 0.182 0.181 6.4 Comparison with Baselines We conducted title extraction from the first data set (Word and PowerPoint in MS).",
                "As the model, we used Perceptron.",
                "We conduct 4-fold cross validation.",
                "Thus, all the results reported here are those averaged over 4 trials.",
                "Tables 4 and 5 show the results.",
                "We see that Perceptron significantly outperforms the baselines.",
                "In the evaluation, we use exact matching between the true titles annotated by humans and the extracted titles.",
                "Table 4.",
                "Accuracies of title extraction with Word Precision Recall F1 Model Perceptron 0.810 0.837 0.823 Largest font size 0.700 0.758 0.727 Baselines First line 0.707 0.767 0.736 Table 5.",
                "Accuracies of title extraction with PowerPoint Precision Recall F1 Model Perceptron 0.875 0. 895 0.885 Largest font size 0.844 0.887 0.865 Baselines First line 0.639 0.671 0.655 We see that the machine learning approach can achieve good performance in title extraction.",
                "For Word documents both precision and recall of the approach are 8 percent higher than those of the baselines.",
                "For PowerPoint both precision and recall of the approach are 2 percent higher than those of the baselines.",
                "We conduct significance tests.",
                "The results are shown in Table 6.",
                "Here, Largest denotes the baseline of using the largest font size, First denotes the baseline of using the first line.",
                "The results indicate that the improvements of machine learning over baselines are statistically significant (in the sense p-value < 0.05) Table 6.",
                "Sign test results Documents Type Sign test between p-value Perceptron vs. Largest 3.59e-26 Word Perceptron vs. First 7.12e-10 Perceptron vs. Largest 0.010 PowerPoint Perceptron vs. First 5.13e-40 We see, from the results, that the two baselines can work well for title extraction, suggesting that font size and position information are most useful features for title extraction.",
                "However, it is also obvious that using only these two features is not enough.",
                "There are cases in which all the lines have the same font size (i.e., the largest font size), or cases in which the lines with the largest font size only contain general descriptions like Confidential, White paper, etc.",
                "For those cases, the largest font size method cannot work well.",
                "For similar reasons, the first line method alone cannot work well, either.",
                "With the combination of different features (evidence in title judgment), Perceptron can outperform Largest and First.",
                "We investigate the performance of solely using linguistic features.",
                "We found that it does not work well.",
                "It seems that the format features play important roles and the linguistic features are supplements..",
                "Figure 8.",
                "An example Word document.",
                "Figure 9.",
                "An example PowerPoint document.",
                "We conducted an error analysis on the results of Perceptron.",
                "We found that the errors fell into three categories. (1) About one third of the errors were related to hard cases.",
                "In these documents, the layouts of the first pages were difficult to understand, even for humans.",
                "Figure 8 and 9 shows examples. (2) Nearly one fourth of the errors were from the documents which do not have true titles but only contain bullets.",
                "Since we conduct extraction from the top regions, it is difficult to get rid of these errors with the current approach. (3).",
                "Confusions between main titles and subtitles were another type of error.",
                "Since we only labeled the main titles as titles, the extractions of both titles were considered incorrect.",
                "This type of error does little harm to document processing like search, however. 6.5 Comparison between Models To compare the performance of different machine learning models, we conducted another experiment.",
                "Again, we perform 4-fold cross 151 validation on the first data set (MS).",
                "Table 7, 8 shows the results of all the four models.",
                "It turns out that Perceptron and PMM perform the best, followed by MEMM, and ME performs the worst.",
                "In general, the Markovian models perform better than or as well as their classifier counterparts.",
                "This seems to be because the Markovian models are trained globally, while the classifiers are trained locally.",
                "The Perceptron based models perform better than the ME based counterparts.",
                "This seems to be because the Perceptron based models are created to make better classifications, while ME models are constructed for better prediction.",
                "Table 7.",
                "Comparison between different learning models for title extraction with Word Model Precision Recall F1 Perceptron 0.810 0.837 0.823 MEMM 0.797 0.824 0.810 PMM 0.827 0.823 0.825 ME 0.801 0.621 0.699 Table 8.",
                "Comparison between different learning models for title extraction with PowerPoint Model Precision Recall F1 Perceptron 0.875 0. 895 0. 885 MEMM 0.841 0.861 0.851 PMM 0.873 0.896 0.885 ME 0.753 0.766 0.759 6.6 Domain Adaptation We apply the model trained with the first data set (MS) to the second data set (DotCom and DotGov).",
                "Tables 9-12 show the results.",
                "Table 9.",
                "Accuracies of title extraction with Word in DotGov Precision Recall F1 Model Perceptron 0.716 0.759 0.737 Largest font size 0.549 0.619 0.582Baselines First line 0.462 0.521 0.490 Table 10.",
                "Accuracies of title extraction with PowerPoint in DotGov Precision Recall F1 Model Perceptron 0.900 0.906 0.903 Largest font size 0.871 0.888 0.879Baselines First line 0.554 0.564 0.559 Table 11.",
                "Accuracies of title extraction with Word in DotCom Precisio n Recall F1 Model Perceptron 0.832 0.880 0.855 Largest font size 0.676 0.753 0.712Baselines First line 0.577 0.643 0.608 Table 12.",
                "Performance of PowerPoint document title extraction in DotCom Precisio n Recall F1 Model Perceptron 0.910 0.903 0.907 Largest font size 0.864 0.886 0.875Baselines First line 0.570 0.585 0.577 From the results, we see that the models can be adapted to different domains well.",
                "There is almost no drop in accuracy.",
                "The results indicate that the patterns of title formats exist across different domains, and it is possible to construct a domain independent model by mainly using formatting information. 6.7 Language Adaptation We apply the model trained with the data in English (MS) to the data set in Chinese.",
                "Tables 13-14 show the results.",
                "Table 13.",
                "Accuracies of title extraction with Word in Chinese Precision Recall F1 Model Perceptron 0.817 0.805 0.811 Largest font size 0.722 0.755 0.738Baselines First line 0.743 0.777 0.760 Table 14.",
                "Accuracies of title extraction with PowerPoint in Chinese Precision Recall F1 Model Perceptron 0.766 0.812 0.789 Largest font size 0.753 0.813 0.782Baselines First line 0.627 0.676 0.650 We see that the models can be adapted to a different language.",
                "There are only small drops in accuracy.",
                "Obviously, the linguistic features do not work for Chinese, but the effect of not using them is negligible.",
                "The results indicate that the patterns of title formats exist across different languages.",
                "From the domain adaptation and language adaptation results, we conclude that the use of formatting information is the key to a successful extraction from general documents. 6.8 Search with Extracted Titles We performed experiments on using title extraction for document retrieval.",
                "As a baseline, we employed BM25 without using extracted titles.",
                "The ranking mechanism was as described in Section 5.",
                "The weights were heuristically set.",
                "We did not conduct optimization on the weights.",
                "The evaluation was conducted on a corpus of 1.3 M documents crawled from the intranet of Microsoft using 100 evaluation queries obtained from this intranets search engine query logs. 50 queries were from the most popular set, while 50 queries other were chosen randomly.",
                "Users were asked to provide judgments of the degree of document relevance from a scale of 1to 5 (1 meaning detrimental, 2 - bad, 3 - fair, 4 - good and 5 - excellent). 152 Figure 10 shows the results.",
                "In the chart two sets of precision results were obtained by either considering good or excellent documents as relevant (left 3 bars with relevance threshold 0.5), or by considering only excellent documents as relevant (right 3 bars with relevance threshold 1.0) 0 0.05 0.1 0.15 0.2 0.25 0.3 0.35 0.4 0.45 P@10 P@5 Reciprocal P@10 P@5 Reciprocal 0.5 1 BM25 Anchor, Title, Body BM25 Anchor, Title, Body, ExtractedTitle Name All RelevanceThreshold Data Description Figure 10.",
                "Search ranking results.",
                "Figure 10 shows different document retrieval results with different ranking functions in terms of precision @10, precision @5 and reciprocal rank: • Blue bar - BM25 including the fields body, title (file property), and anchor text. • Purple bar - BM25 including the fields body, title (file property), anchor text, and extracted title.",
                "With the additional field of extracted title included in BM25 the precision @10 increased from 0.132 to 0.145, or by ~10%.",
                "Thus, it is safe to say that the use of extracted title can indeed improve the precision of document retrieval. 7.",
                "CONCLUSION In this paper, we have investigated the problem of automatically extracting titles from general documents.",
                "We have tried using a machine learning approach to address the problem.",
                "Previous work showed that the machine learning approach can work well for metadata extraction from research papers.",
                "In this paper, we showed that the approach can work for extraction from general documents as well.",
                "Our experimental results indicated that the machine learning approach can work significantly better than the baselines in title extraction from Office documents.",
                "Previous work on metadata extraction mainly used linguistic features in documents, while we mainly used formatting information.",
                "It appeared that using formatting information is a key for successfully conducting title extraction from general documents.",
                "We tried different machine learning models including Perceptron, Maximum Entropy, Maximum Entropy Markov Model, and Voted Perceptron.",
                "We found that the performance of the Perceptorn models was the best.",
                "We applied models constructed in one domain to another domain and applied models trained in one language to another language.",
                "We found that the accuracies did not drop substantially across different domains and across different languages, indicating that the models were generic.",
                "We also attempted to use the extracted titles in document retrieval.",
                "We observed a significant improvement in document ranking performance for search when using extracted title information.",
                "All the above investigations were not conducted in previous work, and through our investigations we verified the generality and the significance of the title extraction approach. 8.",
                "ACKNOWLEDGEMENTS We thank Chunyu Wei and Bojuan Zhao for their work on data annotation.",
                "We acknowledge Jinzhu Li for his assistance in conducting the experiments.",
                "We thank Ming Zhou, John Chen, Jun Xu, and the anonymous reviewers of JCDL05 for their valuable comments on this paper. 9.",
                "REFERENCES [1] Berger, A. L., Della Pietra, S. A., and Della Pietra, V. J.",
                "A maximum entropy approach to natural language processing.",
                "Computational Linguistics, 22:39-71, 1996. [2] Collins, M. Discriminative training methods for hidden markov models: theory and experiments with perceptron algorithms.",
                "In Proceedings of Conference on Empirical Methods in Natural Language Processing, 1-8, 2002. [3] Cortes, C. and Vapnik, V. Support-vector networks.",
                "Machine Learning, 20:273-297, 1995. [4] Chieu, H. L. and Ng, H. T. A maximum entropy approach to information extraction from semi-structured and free text.",
                "In Proceedings of the Eighteenth National Conference on Artificial Intelligence, 768-791, 2002. [5] Evans, D. K., Klavans, J. L., and McKeown, K. R. Columbia newsblaster: multilingual news summarization on the Web.",
                "In Proceedings of Human Language Technology conference / North American chapter of the Association for Computational Linguistics annual meeting, 1-4, 2004. [6] Ghahramani, Z. and Jordan, M. I. Factorial hidden markov models.",
                "Machine Learning, 29:245-273, 1997. [7] Gheel, J. and Anderson, T. Data and metadata for finding and reminding, In Proceedings of the 1999 International Conference on Information Visualization, 446-451,1999. [8] Giles, C. L., Petinot, Y., Teregowda P. B., Han, H., Lawrence, S., Rangaswamy, A., and Pal, N. eBizSearch: a niche search engine for e-Business.",
                "In Proceedings of the 26th Annual International ACM SIGIR Conference on Research and Development in Information Retrieval, 413414, 2003. [9] Giuffrida, G., Shek, E. C., and Yang, J. Knowledge-based metadata extraction from PostScript files.",
                "In Proceedings of the Fifth ACM Conference on Digital Libraries, 77-84, 2000. [10] Han, H., Giles, C. L., Manavoglu, E., Zha, H., Zhang, Z., and Fox, E. A.",
                "Automatic document metadata extraction using support vector machines.",
                "In Proceedings of the Third ACM/IEEE-CS Joint Conference on Digital Libraries, 37-48, 2003. [11] Kobayashi, M., and Takeda, K. Information retrieval on the Web.",
                "ACM Computing Surveys, 32:144-173, 2000. [12] Lafferty, J., McCallum, A., and Pereira, F. Conditional random fields: probabilistic models for segmenting and 153 labeling sequence data.",
                "In Proceedings of the Eighteenth International Conference on Machine Learning, 282-289, 2001. [13] Li, Y., Zaragoza, H., Herbrich, R., Shawe-Taylor J., and Kandola, J. S. The perceptron algorithm with uneven margins.",
                "In Proceedings of the Nineteenth International Conference on Machine Learning, 379-386, 2002. [14] Liddy, E. D., Sutton, S., Allen, E., Harwell, S., Corieri, S., Yilmazel, O., Ozgencil, N. E., Diekema, A., McCracken, N., and Silverstein, J.",
                "Automatic Metadata generation & evaluation.",
                "In Proceedings of the 25th Annual International ACM SIGIR Conference on Research and Development in Information Retrieval, 401-402, 2002. [15] Littlefield, A.",
                "Effective enterprise information retrieval across new content formats.",
                "In Proceedings of the Seventh Search Engine Conference, http://www.infonortics.com/searchengines/sh02/02prog.html, 2002. [16] Mao, S., Kim, J. W., and Thoma, G. R. A dynamic feature generation system for automated metadata extraction in preservation of digital materials.",
                "In Proceedings of the First International Workshop on Document Image Analysis for Libraries, 225-232, 2004. [17] McCallum, A., Freitag, D., and Pereira, F. Maximum entropy markov models for information extraction and segmentation.",
                "In Proceedings of the Seventeenth International Conference on Machine Learning, 591-598, 2000. [18] Murphy, L. D. Digital document metadata in organizations: roles, analytical approaches, and future research directions.",
                "In Proceedings of the Thirty-First Annual Hawaii International Conference on System Sciences, 267-276, 1998. [19] Pinto, D., McCallum, A., Wei, X., and Croft, W. B.",
                "Table extraction using conditional random fields.",
                "In Proceedings of the 26th Annual International ACM SIGIR Conference on Research and Development in Information Retrieval, 235242, 2003. [20] Ratnaparkhi, A. Unsupervised statistical models for prepositional phrase attachment.",
                "In Proceedings of the Seventeenth International Conference on Computational Linguistics. 1079-1085, 1998. [21] Robertson, S., Zaragoza, H., and Taylor, M. Simple BM25 extension to multiple weighted fields, In Proceedings of ACM Thirteenth Conference on Information and Knowledge Management, 42-49, 2004. [22] Yi, J. and Sundaresan, N. Metadata based Web mining for relevance, In Proceedings of the 2000 International Symposium on Database Engineering & Applications, 113121, 2000. [23] Yilmazel, O., Finneran, C. M., and Liddy, E. D. MetaExtract: An NLP system to automatically assign metadata.",
                "In Proceedings of the 2004 Joint ACM/IEEE Conference on Digital Libraries, 241-242, 2004. [24] Zhang, J. and Dimitroff, A. Internet search engines response to metadata Dublin Core implementation.",
                "Journal of Information Science, 30:310-320, 2004. [25] Zhang, L., Pan, Y., and Zhang, T. Recognising and using named entities: focused named entity recognition using machine learning.",
                "In Proceedings of the 27th Annual International ACM SIGIR Conference on Research and Development in Information Retrieval, 281-288, 2004. [26] http://dublincore.org/groups/corporate/Seattle/ 154"
            ],
            "original_annotated_samples": [],
            "translated_annotated_samples": [
                "No ha estado claro si podría ser posible realizar una \"extracción automática de títulos\" de documentos generales."
            ],
            "translated_text": "",
            "candidates": [
                "extracción de títulos automáticos",
                "extracción automática de títulos"
            ],
            "error": []
        },
        "information extraction": {
            "translated_key": "extracción de información",
            "is_in_text": true,
            "original_annotated_sentences": [
                "Automatic Extraction of Titles from General Documents using Machine Learning Yunhua Hu1 Computer Science Department Xian Jiaotong University No 28, Xianning West Road Xian, China, 710049 yunhuahu@mail.xjtu.edu.cn Hang Li, Yunbo Cao Microsoft Research Asia 5F Sigma Center, No.",
                "49 Zhichun Road, Haidian, Beijing, China, 100080 {hangli,yucao}@microsoft.com Qinghua Zheng Computer Science Department Xian Jiaotong University No 28, Xianning West Road Xian, China, 710049 qhzheng@mail.xjtu.edu.cn Dmitriy Meyerzon Microsoft Corporation One Microsoft Way Redmond, WA, USA, 98052 dmitriym@microsoft.com ABSTRACT In this paper, we propose a machine learning approach to title extraction from general documents.",
                "By general documents, we mean documents that can belong to any one of a number of specific genres, including presentations, book chapters, technical papers, brochures, reports, and letters.",
                "Previously, methods have been proposed mainly for title extraction from research papers.",
                "It has not been clear whether it could be possible to conduct automatic title extraction from general documents.",
                "As a case study, we consider extraction from Office including Word and PowerPoint.",
                "In our approach, we annotate titles in sample documents (for Word and PowerPoint respectively) and take them as training data, train machine learning models, and perform title extraction using the trained models.",
                "Our method is unique in that we mainly utilize formatting information such as font size as features in the models.",
                "It turns out that the use of formatting information can lead to quite accurate extraction from general documents.",
                "Precision and recall for title extraction from Word is 0.810 and 0.837 respectively, and precision and recall for title extraction from PowerPoint is 0.875 and 0.895 respectively in an experiment on intranet data.",
                "Other important new findings in this work include that we can train models in one domain and apply them to another domain, and more surprisingly we can even train models in one language and apply them to another language.",
                "Moreover, we can significantly improve search ranking results in document retrieval by using the extracted titles.",
                "Categories and Subject Descriptors H.3.3 [Information Storage and Retrieval]: Information Search and Retrieval - Search Process; H.4.1 [Information Systems Applications]: Office Automation - Word processing; D.2.8 [Software Engineering]: Metrics - complexity measures, performance measures General Terms Algorithms, Experimentation, Performance. 1.",
                "INTRODUCTION Metadata of documents is useful for many kinds of document processing such as search, browsing, and filtering.",
                "Ideally, metadata is defined by the authors of documents and is then used by various systems.",
                "However, people seldom define document metadata by themselves, even when they have convenient metadata definition tools [26].",
                "Thus, how to automatically extract metadata from the bodies of documents turns out to be an important research issue.",
                "Methods for performing the task have been proposed.",
                "However, the focus was mainly on extraction from research papers.",
                "For instance, Han et al. [10] proposed a machine learning based method to conduct extraction from research papers.",
                "They formalized the problem as that of classification and employed Support Vector Machines as the classifier.",
                "They mainly used linguistic features in the model.1 In this paper, we consider metadata extraction from general documents.",
                "By general documents, we mean documents that may belong to any one of a number of specific genres.",
                "General documents are more widely available in digital libraries, intranets and the internet, and thus investigation on extraction from them is sorely needed.",
                "Research papers usually have well-formed styles and noticeable characteristics.",
                "In contrast, the styles of general documents can vary greatly.",
                "It has not been clarified whether a machine learning based approach can work well for this task.",
                "There are many types of metadata: title, author, date of creation, etc.",
                "As a case study, we consider title extraction in this paper.",
                "General documents can be in many different file formats: Microsoft Office, PDF (PS), etc.",
                "As a case study, we consider extraction from Office including Word and PowerPoint.",
                "We take a machine learning approach.",
                "We annotate titles in sample documents (for Word and PowerPoint respectively) and take them as training data to train several types of models, and perform title extraction using any one type of the trained models.",
                "In the models, we mainly utilize formatting information such as font size as features.",
                "We employ the following models: Maximum Entropy Model, Perceptron with Uneven Margins, Maximum Entropy Markov Model, and Voted Perceptron.",
                "In this paper, we also investigate the following three problems, which did not seem to have been examined previously. (1) Comparison between models: among the models above, which model performs best for title extraction; (2) Generality of model: whether it is possible to train a model on one domain and apply it to another domain, and whether it is possible to train a model in one language and apply it to another language; (3) Usefulness of extracted titles: whether extracted titles can improve document processing such as search.",
                "Experimental results indicate that our approach works well for title extraction from general documents.",
                "Our method can significantly outperform the baselines: one that always uses the first lines as titles and the other that always uses the lines in the largest font sizes as titles.",
                "Precision and recall for title extraction from Word are 0.810 and 0.837 respectively, and precision and recall for title extraction from PowerPoint are 0.875 and 0.895 respectively.",
                "It turns out that the use of format features is the key to successful title extraction. (1) We have observed that Perceptron based models perform better in terms of extraction accuracies. (2) We have empirically verified that the models trained with our approach are generic in the sense that they can be trained on one domain and applied to another, and they can be trained in one language and applied to another. (3) We have found that using the extracted titles we can significantly improve precision of document retrieval (by 10%).",
                "We conclude that we can indeed conduct reliable title extraction from general documents and use the extracted results to improve real applications.",
                "The rest of the paper is organized as follows.",
                "In section 2, we introduce related work, and in section 3, we explain the motivation and problem setting of our work.",
                "In section 4, we describe our method of title extraction, and in section 5, we describe our method of document retrieval using extracted titles.",
                "Section 6 gives our experimental results.",
                "We make concluding remarks in section 7. 2.",
                "RELATED WORK 2.1 Document Metadata Extraction Methods have been proposed for performing automatic metadata extraction from documents; however, the main focus was on extraction from research papers.",
                "The proposed methods fall into two categories: the rule based approach and the machine learning based approach.",
                "Giuffrida et al. [9], for instance, developed a rule-based system for automatically extracting metadata from research papers in Postscript.",
                "They used rules like titles are usually located on the upper portions of the first pages and they are usually in the largest font sizes.",
                "Liddy et al. [14] and Yilmazel el al. [23] performed metadata extraction from educational materials using rule-based natural language processing technologies.",
                "Mao et al. [16] also conducted automatic metadata extraction from research papers using rules on formatting information.",
                "The rule-based approach can achieve high performance.",
                "However, it also has disadvantages.",
                "It is less adaptive and robust when compared with the machine learning approach.",
                "Han et al. [10], for instance, conducted metadata extraction with the machine learning approach.",
                "They viewed the problem as that of classifying the lines in a document into the categories of metadata and proposed using Support Vector Machines as the classifier.",
                "They mainly used linguistic information as features.",
                "They reported high extraction accuracy from research papers in terms of precision and recall. 2.2 <br>information extraction</br> Metadata extraction can be viewed as an application of <br>information extraction</br>, in which given a sequence of instances, we identify a subsequence that represents information in which we are interested.",
                "Hidden Markov Model [6], Maximum Entropy Model [1, 4], Maximum Entropy Markov Model [17], Support Vector Machines [3], Conditional Random Field [12], and Voted Perceptron [2] are widely used <br>information extraction</br> models.",
                "<br>information extraction</br> has been applied, for instance, to part-ofspeech tagging [20], named entity recognition [25] and table extraction [19]. 2.3 Search Using Title Information Title information is useful for document retrieval.",
                "In the system Citeseer, for instance, Giles et al. managed to extract titles from research papers and make use of the extracted titles in metadata search of papers [8].",
                "In web search, the title fields (i.e., file properties) and anchor texts of web pages (HTML documents) can be viewed as titles of the pages [5].",
                "Many search engines seem to utilize them for web page retrieval [7, 11, 18, 22].",
                "Zhang et al., found that web pages with well-defined metadata are more easily retrieved than those without well-defined metadata [24].",
                "To the best of our knowledge, no research has been conducted on using extracted titles from general documents (e.g., Office documents) for search of the documents. 146 3.",
                "MOTIVATION AND PROBLEM SETTING We consider the issue of automatically extracting titles from general documents.",
                "By general documents, we mean documents that belong to one of any number of specific genres.",
                "The documents can be presentations, books, book chapters, technical papers, brochures, reports, memos, specifications, letters, announcements, or resumes.",
                "General documents are more widely available in digital libraries, intranets, and internet, and thus investigation on title extraction from them is sorely needed.",
                "Figure 1 shows an estimate on distributions of file formats on intranet and internet [15].",
                "Office and PDF are the main file formats on the intranet.",
                "Even on the internet, the documents in the formats are still not negligible, given its extremely large size.",
                "In this paper, without loss of generality, we take Office documents as an example.",
                "Figure 1.",
                "Distributions of file formats in internet and intranet.",
                "For Office documents, users can define titles as file properties using a feature provided by Office.",
                "We found in an experiment, however, that users seldom use the feature and thus titles in file properties are usually very inaccurate.",
                "That is to say, titles in file properties are usually inconsistent with the true titles in the file bodies that are created by the authors and are visible to readers.",
                "We collected 6,000 Word and 6,000 PowerPoint documents from an intranet and the internet and examined how many titles in the file properties are correct.",
                "We found that surprisingly the accuracy was only 0.265 (cf., Section 6.3 for details).",
                "A number of reasons can be considered.",
                "For example, if one creates a new file by copying an old file, then the file property of the new file will also be copied from the old file.",
                "In another experiment, we found that Google uses the titles in file properties of Office documents in search and browsing, but the titles are not very accurate.",
                "We created 50 queries to search Word and PowerPoint documents and examined the top 15 results of each query returned by Google.",
                "We found that nearly all the titles presented in the search results were from the file properties of the documents.",
                "However, only 0.272 of them were correct.",
                "Actually, true titles usually exist at the beginnings of the bodies of documents.",
                "If we can accurately extract the titles from the bodies of documents, then we can exploit reliable title information in document processing.",
                "This is exactly the problem we address in this paper.",
                "More specifically, given a Word document, we are to extract the title from the top region of the first page.",
                "Given a PowerPoint document, we are to extract the title from the first slide.",
                "A title sometimes consists of a main title and one or two subtitles.",
                "We only consider extraction of the main title.",
                "As baselines for title extraction, we use that of always using the first lines as titles and that of always using the lines with largest font sizes as titles.",
                "Figure 2.",
                "Title extraction from Word document.",
                "Figure 3.",
                "Title extraction from PowerPoint document.",
                "Next, we define a specification for human judgments in title data annotation.",
                "The annotated data will be used in training and testing of the title extraction methods.",
                "Summary of the specification: The title of a document should be identified on the basis of common sense, if there is no difficulty in the identification.",
                "However, there are many cases in which the identification is not easy.",
                "There are some rules defined in the specification that guide identification for such cases.",
                "The rules include a title is usually in consecutive lines in the same format, a document can have no title, titles in images are not considered, a title should not contain words like draft, 147 whitepaper, etc, if it is difficult to determine which is the title, select the one in the largest font size, and if it is still difficult to determine which is the title, select the first candidate. (The specification covers all the cases we have encountered in data annotation.)",
                "Figures 2 and 3 show examples of Office documents from which we conduct title extraction.",
                "In Figure 2, Differences in Win32 API Implementations among Windows Operating Systems is the title of the Word document.",
                "Microsoft Windows on the top of this page is a picture and thus is ignored.",
                "In Figure 3, Building Competitive Advantages through an Agile Infrastructure is the title of the PowerPoint document.",
                "We have developed a tool for annotation of titles by human annotators.",
                "Figure 4 shows a snapshot of the tool.",
                "Figure 4.",
                "Title annotation tool. 4.",
                "TITLE EXTRACTION METHOD 4.1 Outline Title extraction based on machine learning consists of training and extraction.",
                "The same pre-processing step occurs before training and extraction.",
                "During pre-processing, from the top region of the first page of a Word document or the first slide of a PowerPoint document a number of units for processing are extracted.",
                "If a line (lines are separated by return symbols) only has a single format, then the line will become a unit.",
                "If a line has several parts and each of them has its own format, then each part will become a unit.",
                "Each unit will be treated as an instance in learning.",
                "A unit contains not only content information (linguistic information) but also formatting information.",
                "The input to pre-processing is a document and the output of pre-processing is a sequence of units (instances).",
                "Figure 5 shows the units obtained from the document in Figure 2.",
                "Figure 5.",
                "Example of units.",
                "In learning, the input is sequences of units where each sequence corresponds to a document.",
                "We take labeled units (labeled as title_begin, title_end, or other) in the sequences as training data and construct models for identifying whether a unit is title_begin title_end, or other.",
                "We employ four types of models: Perceptron, Maximum Entropy (ME), Perceptron Markov Model (PMM), and Maximum Entropy Markov Model (MEMM).",
                "In extraction, the input is a sequence of units from one document.",
                "We employ one type of model to identify whether a unit is title_begin, title_end, or other.",
                "We then extract units from the unit labeled with title_begin to the unit labeled with title_end.",
                "The result is the extracted title of the document.",
                "The unique characteristic of our approach is that we mainly utilize formatting information for title extraction.",
                "Our assumption is that although general documents vary in styles, their formats have certain patterns and we can learn and utilize the patterns for title extraction.",
                "This is in contrast to the work by Han et al., in which only linguistic features are used for extraction from research papers. 4.2 Models The four models actually can be considered in the same metadata extraction framework.",
                "That is why we apply them together to our current problem.",
                "Each input is a sequence of instances kxxx L21 together with a sequence of labels kyyy L21 . ix and iy represents an instance and its label, respectively ( ki ,,2,1 L= ).",
                "Recall that an instance here represents a unit.",
                "A label represents title_begin, title_end, or other.",
                "Here, k is the number of units in a document.",
                "In learning, we train a model which can be generally denoted as a conditional probability distribution )|( 11 kk XXYYP LL where iX and iY denote random variables taking instance ix and label iy as values, respectively ( ki ,,2,1 L= ).",
                "Learning Tool Extraction Tool 21121 2222122221 1121111211 nknnknn kk kk yyyxxx yyyxxx yyyxxx LL LL LL LL → → → )|(maxarg 11 mkmmkm xxyyP LL )|( 11 kk XXYYP LL Conditional Distribution mkmm xxx L21 Figure 6.",
                "Metadata extraction model.",
                "We can make assumptions about the general model in order to make it simple enough for training. 148 For example, we can assume that kYY ,,1 L are independent of each other given kXX ,,1 L .",
                "Thus, we have )|()|( )|( 11 11 kk kk XYPXYP XXYYP L LL = In this way, we decompose the model into a number of classifiers.",
                "We train the classifiers locally using the labeled data.",
                "As the classifier, we employ the Perceptron or Maximum Entropy model.",
                "We can also assume that the first order Markov property holds for kYY ,,1 L given kXX ,,1 L .",
                "Thus, we have )|()|( )|( 111 11 kkk kk XYYPXYP XXYYP −= L LL Again, we obtain a number of classifiers.",
                "However, the classifiers are conditioned on the previous label.",
                "When we employ the Percepton or Maximum Entropy model as a classifier, the models become a Percepton Markov Model or Maximum Entropy Markov Model, respectively.",
                "That is to say, the two models are more precise.",
                "In extraction, given a new sequence of instances, we resort to one of the constructed models to assign a sequence of labels to the sequence of instances, i.e., perform extraction.",
                "For Perceptron and ME, we assign labels locally and combine the results globally later using heuristics.",
                "Specifically, we first identify the most likely title_begin.",
                "Then we find the most likely title_end within three units after the title_begin.",
                "Finally, we extract as a title the units between the title_begin and the title_end.",
                "For PMM and MEMM, we employ the Viterbi algorithm to find the globally optimal label sequence.",
                "In this paper, for Perceptron, we actually employ an improved variant of it, called Perceptron with Uneven Margin [13].",
                "This version of Perceptron can work well especially when the number of positive instances and the number of negative instances differ greatly, which is exactly the case in our problem.",
                "We also employ an improved version of Perceptron Markov Model in which the Perceptron model is the so-called Voted Perceptron [2].",
                "In addition, in training, the parameters of the model are updated globally rather than locally. 4.3 Features There are two types of features: format features and linguistic features.",
                "We mainly use the former.",
                "The features are used for both the title-begin and the title-end classifiers. 4.3.1 Format Features Font Size: There are four binary features that represent the normalized font size of the unit (recall that a unit has only one type of font).",
                "If the font size of the unit is the largest in the document, then the first feature will be 1, otherwise 0.",
                "If the font size is the smallest in the document, then the fourth feature will be 1, otherwise 0.",
                "If the font size is above the average font size and not the largest in the document, then the second feature will be 1, otherwise 0.",
                "If the font size is below the average font size and not the smallest, the third feature will be 1, otherwise 0.",
                "It is necessary to conduct normalization on font sizes.",
                "For example, in one document the largest font size might be 12pt, while in another the smallest one might be 18pt.",
                "Boldface: This binary feature represents whether or not the current unit is in boldface.",
                "Alignment: There are four binary features that respectively represent the location of the current unit: left, center, right, and unknown alignment.",
                "The following format features with respect to context play an important role in title extraction.",
                "Empty Neighboring Unit: There are two binary features that represent, respectively, whether or not the previous unit and the current unit are blank lines.",
                "Font Size Change: There are two binary features that represent, respectively, whether or not the font size of the previous unit and the font size of the next unit differ from that of the current unit.",
                "Alignment Change: There are two binary features that represent, respectively, whether or not the alignment of the previous unit and the alignment of the next unit differ from that of the current one.",
                "Same Paragraph: There are two binary features that represent, respectively, whether or not the previous unit and the next unit are in the same paragraph as the current unit. 4.3.2 Linguistic Features The linguistic features are based on key words.",
                "Positive Word: This binary feature represents whether or not the current unit begins with one of the positive words.",
                "The positive words include title:, subject:, subject line: For example, in some documents the lines of titles and authors have the same formats.",
                "However, if lines begin with one of the positive words, then it is likely that they are title lines.",
                "Negative Word: This binary feature represents whether or not the current unit begins with one of the negative words.",
                "The negative words include To, By, created by, updated by, etc.",
                "There are more negative words than positive words.",
                "The above linguistic features are language dependent.",
                "Word Count: A title should not be too long.",
                "We heuristically create four intervals: [1, 2], [3, 6], [7, 9] and [9, ∞) and define one feature for each interval.",
                "If the number of words in a title falls into an interval, then the corresponding feature will be 1; otherwise 0.",
                "Ending Character: This feature represents whether the unit ends with :, -, or other special characters.",
                "A title usually does not end with such a character. 5.",
                "DOCUMENT RETRIEVAL METHOD We describe our method of document retrieval using extracted titles.",
                "Typically, in information retrieval a document is split into a number of fields including body, title, and anchor text.",
                "A ranking function in search can use different weights for different fields of 149 the document.",
                "Also, titles are typically assigned high weights, indicating that they are important for document retrieval.",
                "As explained previously, our experiment has shown that a significant number of documents actually have incorrect titles in the file properties, and thus in addition of using them we use the extracted titles as one more field of the document.",
                "By doing this, we attempt to improve the overall precision.",
                "In this paper, we employ a modification of BM25 that allows field weighting [21].",
                "As fields, we make use of body, title, extracted title and anchor.",
                "First, for each term in the query we count the term frequency in each field of the document; each field frequency is then weighted according to the corresponding weight parameter: ∑= f tfft tfwwtf Similarly, we compute the document length as a weighted sum of lengths of each field.",
                "Average document length in the corpus becomes the average of all weighted document lengths. ∑= f ff dlwwdl In our experiments we used 75.0,8.11 == bk .",
                "Weight for content was 1.0, title was 10.0, anchor was 10.0, and extracted title was 5.0. 6.",
                "EXPERIMENTAL RESULTS 6.1 Data Sets and Evaluation Measures We used two data sets in our experiments.",
                "First, we downloaded and randomly selected 5,000 Word documents and 5,000 PowerPoint documents from an intranet of Microsoft.",
                "We call it MS hereafter.",
                "Second, we downloaded and randomly selected 500 Word and 500 PowerPoint documents from the DotGov and DotCom domains on the internet, respectively.",
                "Figure 7 shows the distributions of the genres of the documents.",
                "We see that the documents are indeed general documents as we define them.",
                "Figure 7.",
                "Distributions of document genres.",
                "Third, a data set in Chinese was also downloaded from the internet.",
                "It includes 500 Word documents and 500 PowerPoint documents in Chinese.",
                "We manually labeled the titles of all the documents, on the basis of our specification.",
                "Not all the documents in the two data sets have titles.",
                "Table 1 shows the percentages of the documents having titles.",
                "We see that DotCom and DotGov have more PowerPoint documents with titles than MS.",
                "This might be because PowerPoint documents published on the internet are more formal than those on the intranet.",
                "Table 1.",
                "The portion of documents with titles Domain Type MS DotCom DotGov Word 75.7% 77.8% 75.6% PowerPoint 82.1% 93.4% 96.4% In our experiments, we conducted evaluations on title extraction in terms of precision, recall, and F-measure.",
                "The evaluation measures are defined as follows: Precision: P = A / ( A + B ) Recall: R = A / ( A + C ) F-measure: F1 = 2PR / ( P + R ) Here, A, B, C, and D are numbers of documents as those defined in Table 2.",
                "Table 2.",
                "Contingence table with regard to title extraction Is title Is not title Extracted A B Not extracted C D 6.2 Baselines We test the accuracies of the two baselines described in section 4.2.",
                "They are denoted as largest font size and first line respectively. 6.3 Accuracy of Titles in File Properties We investigate how many titles in the file properties of the documents are reliable.",
                "We view the titles annotated by humans as true titles and test how many titles in the file properties can approximately match with the true titles.",
                "We use Edit Distance to conduct the approximate match. (Approximate match is only used in this evaluation).",
                "This is because sometimes human annotated titles can be slightly different from the titles in file properties on the surface, e.g., contain extra spaces).",
                "Given string A and string B: if ( (D == 0) or ( D / ( La + Lb ) < θ ) ) then string A = string B D: Edit Distance between string A and string B La: length of string A Lb: length of string B θ: 0.1 ∑ × ++− + = t t n N wtf avwdl wdl bbk kwtf FBM )log( ))1(( )1( 25 1 1 150 Table 3.",
                "Accuracies of titles in file properties File Type Domain Precision Recall F1 MS 0.299 0.311 0.305 DotCom 0.210 0.214 0.212Word DotGov 0.182 0.177 0.180 MS 0.229 0.245 0.237 DotCom 0.185 0.186 0.186PowerPoint DotGov 0.180 0.182 0.181 6.4 Comparison with Baselines We conducted title extraction from the first data set (Word and PowerPoint in MS).",
                "As the model, we used Perceptron.",
                "We conduct 4-fold cross validation.",
                "Thus, all the results reported here are those averaged over 4 trials.",
                "Tables 4 and 5 show the results.",
                "We see that Perceptron significantly outperforms the baselines.",
                "In the evaluation, we use exact matching between the true titles annotated by humans and the extracted titles.",
                "Table 4.",
                "Accuracies of title extraction with Word Precision Recall F1 Model Perceptron 0.810 0.837 0.823 Largest font size 0.700 0.758 0.727 Baselines First line 0.707 0.767 0.736 Table 5.",
                "Accuracies of title extraction with PowerPoint Precision Recall F1 Model Perceptron 0.875 0. 895 0.885 Largest font size 0.844 0.887 0.865 Baselines First line 0.639 0.671 0.655 We see that the machine learning approach can achieve good performance in title extraction.",
                "For Word documents both precision and recall of the approach are 8 percent higher than those of the baselines.",
                "For PowerPoint both precision and recall of the approach are 2 percent higher than those of the baselines.",
                "We conduct significance tests.",
                "The results are shown in Table 6.",
                "Here, Largest denotes the baseline of using the largest font size, First denotes the baseline of using the first line.",
                "The results indicate that the improvements of machine learning over baselines are statistically significant (in the sense p-value < 0.05) Table 6.",
                "Sign test results Documents Type Sign test between p-value Perceptron vs. Largest 3.59e-26 Word Perceptron vs. First 7.12e-10 Perceptron vs. Largest 0.010 PowerPoint Perceptron vs. First 5.13e-40 We see, from the results, that the two baselines can work well for title extraction, suggesting that font size and position information are most useful features for title extraction.",
                "However, it is also obvious that using only these two features is not enough.",
                "There are cases in which all the lines have the same font size (i.e., the largest font size), or cases in which the lines with the largest font size only contain general descriptions like Confidential, White paper, etc.",
                "For those cases, the largest font size method cannot work well.",
                "For similar reasons, the first line method alone cannot work well, either.",
                "With the combination of different features (evidence in title judgment), Perceptron can outperform Largest and First.",
                "We investigate the performance of solely using linguistic features.",
                "We found that it does not work well.",
                "It seems that the format features play important roles and the linguistic features are supplements..",
                "Figure 8.",
                "An example Word document.",
                "Figure 9.",
                "An example PowerPoint document.",
                "We conducted an error analysis on the results of Perceptron.",
                "We found that the errors fell into three categories. (1) About one third of the errors were related to hard cases.",
                "In these documents, the layouts of the first pages were difficult to understand, even for humans.",
                "Figure 8 and 9 shows examples. (2) Nearly one fourth of the errors were from the documents which do not have true titles but only contain bullets.",
                "Since we conduct extraction from the top regions, it is difficult to get rid of these errors with the current approach. (3).",
                "Confusions between main titles and subtitles were another type of error.",
                "Since we only labeled the main titles as titles, the extractions of both titles were considered incorrect.",
                "This type of error does little harm to document processing like search, however. 6.5 Comparison between Models To compare the performance of different machine learning models, we conducted another experiment.",
                "Again, we perform 4-fold cross 151 validation on the first data set (MS).",
                "Table 7, 8 shows the results of all the four models.",
                "It turns out that Perceptron and PMM perform the best, followed by MEMM, and ME performs the worst.",
                "In general, the Markovian models perform better than or as well as their classifier counterparts.",
                "This seems to be because the Markovian models are trained globally, while the classifiers are trained locally.",
                "The Perceptron based models perform better than the ME based counterparts.",
                "This seems to be because the Perceptron based models are created to make better classifications, while ME models are constructed for better prediction.",
                "Table 7.",
                "Comparison between different learning models for title extraction with Word Model Precision Recall F1 Perceptron 0.810 0.837 0.823 MEMM 0.797 0.824 0.810 PMM 0.827 0.823 0.825 ME 0.801 0.621 0.699 Table 8.",
                "Comparison between different learning models for title extraction with PowerPoint Model Precision Recall F1 Perceptron 0.875 0. 895 0. 885 MEMM 0.841 0.861 0.851 PMM 0.873 0.896 0.885 ME 0.753 0.766 0.759 6.6 Domain Adaptation We apply the model trained with the first data set (MS) to the second data set (DotCom and DotGov).",
                "Tables 9-12 show the results.",
                "Table 9.",
                "Accuracies of title extraction with Word in DotGov Precision Recall F1 Model Perceptron 0.716 0.759 0.737 Largest font size 0.549 0.619 0.582Baselines First line 0.462 0.521 0.490 Table 10.",
                "Accuracies of title extraction with PowerPoint in DotGov Precision Recall F1 Model Perceptron 0.900 0.906 0.903 Largest font size 0.871 0.888 0.879Baselines First line 0.554 0.564 0.559 Table 11.",
                "Accuracies of title extraction with Word in DotCom Precisio n Recall F1 Model Perceptron 0.832 0.880 0.855 Largest font size 0.676 0.753 0.712Baselines First line 0.577 0.643 0.608 Table 12.",
                "Performance of PowerPoint document title extraction in DotCom Precisio n Recall F1 Model Perceptron 0.910 0.903 0.907 Largest font size 0.864 0.886 0.875Baselines First line 0.570 0.585 0.577 From the results, we see that the models can be adapted to different domains well.",
                "There is almost no drop in accuracy.",
                "The results indicate that the patterns of title formats exist across different domains, and it is possible to construct a domain independent model by mainly using formatting information. 6.7 Language Adaptation We apply the model trained with the data in English (MS) to the data set in Chinese.",
                "Tables 13-14 show the results.",
                "Table 13.",
                "Accuracies of title extraction with Word in Chinese Precision Recall F1 Model Perceptron 0.817 0.805 0.811 Largest font size 0.722 0.755 0.738Baselines First line 0.743 0.777 0.760 Table 14.",
                "Accuracies of title extraction with PowerPoint in Chinese Precision Recall F1 Model Perceptron 0.766 0.812 0.789 Largest font size 0.753 0.813 0.782Baselines First line 0.627 0.676 0.650 We see that the models can be adapted to a different language.",
                "There are only small drops in accuracy.",
                "Obviously, the linguistic features do not work for Chinese, but the effect of not using them is negligible.",
                "The results indicate that the patterns of title formats exist across different languages.",
                "From the domain adaptation and language adaptation results, we conclude that the use of formatting information is the key to a successful extraction from general documents. 6.8 Search with Extracted Titles We performed experiments on using title extraction for document retrieval.",
                "As a baseline, we employed BM25 without using extracted titles.",
                "The ranking mechanism was as described in Section 5.",
                "The weights were heuristically set.",
                "We did not conduct optimization on the weights.",
                "The evaluation was conducted on a corpus of 1.3 M documents crawled from the intranet of Microsoft using 100 evaluation queries obtained from this intranets search engine query logs. 50 queries were from the most popular set, while 50 queries other were chosen randomly.",
                "Users were asked to provide judgments of the degree of document relevance from a scale of 1to 5 (1 meaning detrimental, 2 - bad, 3 - fair, 4 - good and 5 - excellent). 152 Figure 10 shows the results.",
                "In the chart two sets of precision results were obtained by either considering good or excellent documents as relevant (left 3 bars with relevance threshold 0.5), or by considering only excellent documents as relevant (right 3 bars with relevance threshold 1.0) 0 0.05 0.1 0.15 0.2 0.25 0.3 0.35 0.4 0.45 P@10 P@5 Reciprocal P@10 P@5 Reciprocal 0.5 1 BM25 Anchor, Title, Body BM25 Anchor, Title, Body, ExtractedTitle Name All RelevanceThreshold Data Description Figure 10.",
                "Search ranking results.",
                "Figure 10 shows different document retrieval results with different ranking functions in terms of precision @10, precision @5 and reciprocal rank: • Blue bar - BM25 including the fields body, title (file property), and anchor text. • Purple bar - BM25 including the fields body, title (file property), anchor text, and extracted title.",
                "With the additional field of extracted title included in BM25 the precision @10 increased from 0.132 to 0.145, or by ~10%.",
                "Thus, it is safe to say that the use of extracted title can indeed improve the precision of document retrieval. 7.",
                "CONCLUSION In this paper, we have investigated the problem of automatically extracting titles from general documents.",
                "We have tried using a machine learning approach to address the problem.",
                "Previous work showed that the machine learning approach can work well for metadata extraction from research papers.",
                "In this paper, we showed that the approach can work for extraction from general documents as well.",
                "Our experimental results indicated that the machine learning approach can work significantly better than the baselines in title extraction from Office documents.",
                "Previous work on metadata extraction mainly used linguistic features in documents, while we mainly used formatting information.",
                "It appeared that using formatting information is a key for successfully conducting title extraction from general documents.",
                "We tried different machine learning models including Perceptron, Maximum Entropy, Maximum Entropy Markov Model, and Voted Perceptron.",
                "We found that the performance of the Perceptorn models was the best.",
                "We applied models constructed in one domain to another domain and applied models trained in one language to another language.",
                "We found that the accuracies did not drop substantially across different domains and across different languages, indicating that the models were generic.",
                "We also attempted to use the extracted titles in document retrieval.",
                "We observed a significant improvement in document ranking performance for search when using extracted title information.",
                "All the above investigations were not conducted in previous work, and through our investigations we verified the generality and the significance of the title extraction approach. 8.",
                "ACKNOWLEDGEMENTS We thank Chunyu Wei and Bojuan Zhao for their work on data annotation.",
                "We acknowledge Jinzhu Li for his assistance in conducting the experiments.",
                "We thank Ming Zhou, John Chen, Jun Xu, and the anonymous reviewers of JCDL05 for their valuable comments on this paper. 9.",
                "REFERENCES [1] Berger, A. L., Della Pietra, S. A., and Della Pietra, V. J.",
                "A maximum entropy approach to natural language processing.",
                "Computational Linguistics, 22:39-71, 1996. [2] Collins, M. Discriminative training methods for hidden markov models: theory and experiments with perceptron algorithms.",
                "In Proceedings of Conference on Empirical Methods in Natural Language Processing, 1-8, 2002. [3] Cortes, C. and Vapnik, V. Support-vector networks.",
                "Machine Learning, 20:273-297, 1995. [4] Chieu, H. L. and Ng, H. T. A maximum entropy approach to <br>information extraction</br> from semi-structured and free text.",
                "In Proceedings of the Eighteenth National Conference on Artificial Intelligence, 768-791, 2002. [5] Evans, D. K., Klavans, J. L., and McKeown, K. R. Columbia newsblaster: multilingual news summarization on the Web.",
                "In Proceedings of Human Language Technology conference / North American chapter of the Association for Computational Linguistics annual meeting, 1-4, 2004. [6] Ghahramani, Z. and Jordan, M. I. Factorial hidden markov models.",
                "Machine Learning, 29:245-273, 1997. [7] Gheel, J. and Anderson, T. Data and metadata for finding and reminding, In Proceedings of the 1999 International Conference on Information Visualization, 446-451,1999. [8] Giles, C. L., Petinot, Y., Teregowda P. B., Han, H., Lawrence, S., Rangaswamy, A., and Pal, N. eBizSearch: a niche search engine for e-Business.",
                "In Proceedings of the 26th Annual International ACM SIGIR Conference on Research and Development in Information Retrieval, 413414, 2003. [9] Giuffrida, G., Shek, E. C., and Yang, J. Knowledge-based metadata extraction from PostScript files.",
                "In Proceedings of the Fifth ACM Conference on Digital Libraries, 77-84, 2000. [10] Han, H., Giles, C. L., Manavoglu, E., Zha, H., Zhang, Z., and Fox, E. A.",
                "Automatic document metadata extraction using support vector machines.",
                "In Proceedings of the Third ACM/IEEE-CS Joint Conference on Digital Libraries, 37-48, 2003. [11] Kobayashi, M., and Takeda, K. Information retrieval on the Web.",
                "ACM Computing Surveys, 32:144-173, 2000. [12] Lafferty, J., McCallum, A., and Pereira, F. Conditional random fields: probabilistic models for segmenting and 153 labeling sequence data.",
                "In Proceedings of the Eighteenth International Conference on Machine Learning, 282-289, 2001. [13] Li, Y., Zaragoza, H., Herbrich, R., Shawe-Taylor J., and Kandola, J. S. The perceptron algorithm with uneven margins.",
                "In Proceedings of the Nineteenth International Conference on Machine Learning, 379-386, 2002. [14] Liddy, E. D., Sutton, S., Allen, E., Harwell, S., Corieri, S., Yilmazel, O., Ozgencil, N. E., Diekema, A., McCracken, N., and Silverstein, J.",
                "Automatic Metadata generation & evaluation.",
                "In Proceedings of the 25th Annual International ACM SIGIR Conference on Research and Development in Information Retrieval, 401-402, 2002. [15] Littlefield, A.",
                "Effective enterprise information retrieval across new content formats.",
                "In Proceedings of the Seventh Search Engine Conference, http://www.infonortics.com/searchengines/sh02/02prog.html, 2002. [16] Mao, S., Kim, J. W., and Thoma, G. R. A dynamic feature generation system for automated metadata extraction in preservation of digital materials.",
                "In Proceedings of the First International Workshop on Document Image Analysis for Libraries, 225-232, 2004. [17] McCallum, A., Freitag, D., and Pereira, F. Maximum entropy markov models for <br>information extraction</br> and segmentation.",
                "In Proceedings of the Seventeenth International Conference on Machine Learning, 591-598, 2000. [18] Murphy, L. D. Digital document metadata in organizations: roles, analytical approaches, and future research directions.",
                "In Proceedings of the Thirty-First Annual Hawaii International Conference on System Sciences, 267-276, 1998. [19] Pinto, D., McCallum, A., Wei, X., and Croft, W. B.",
                "Table extraction using conditional random fields.",
                "In Proceedings of the 26th Annual International ACM SIGIR Conference on Research and Development in Information Retrieval, 235242, 2003. [20] Ratnaparkhi, A. Unsupervised statistical models for prepositional phrase attachment.",
                "In Proceedings of the Seventeenth International Conference on Computational Linguistics. 1079-1085, 1998. [21] Robertson, S., Zaragoza, H., and Taylor, M. Simple BM25 extension to multiple weighted fields, In Proceedings of ACM Thirteenth Conference on Information and Knowledge Management, 42-49, 2004. [22] Yi, J. and Sundaresan, N. Metadata based Web mining for relevance, In Proceedings of the 2000 International Symposium on Database Engineering & Applications, 113121, 2000. [23] Yilmazel, O., Finneran, C. M., and Liddy, E. D. MetaExtract: An NLP system to automatically assign metadata.",
                "In Proceedings of the 2004 Joint ACM/IEEE Conference on Digital Libraries, 241-242, 2004. [24] Zhang, J. and Dimitroff, A. Internet search engines response to metadata Dublin Core implementation.",
                "Journal of Information Science, 30:310-320, 2004. [25] Zhang, L., Pan, Y., and Zhang, T. Recognising and using named entities: focused named entity recognition using machine learning.",
                "In Proceedings of the 27th Annual International ACM SIGIR Conference on Research and Development in Information Retrieval, 281-288, 2004. [26] http://dublincore.org/groups/corporate/Seattle/ 154"
            ],
            "original_annotated_samples": [],
            "translated_annotated_samples": [
                "Informaron una alta precisión de extracción de trabajos de investigación en términos de precisión y retiro.2.2 La extracción de metadatos de \"extracción de información\" se puede ver como una aplicación de \"extracción de información\", en la que dada una secuencia de instancias, identificamos una subsecuencia que representa información en la que estamos interesados.",
                "Modelo oculto de Markov [6], modelo de entropía máxima [1, 4], modelo de entropía máxima de Markov [17], máquinas de vectores de soporte [3], campo aleatorio condicional [12] y perceptrón votado [2] se usan ampliamente \"extracción de información\"\"Modelos.",
                "La \"extracción de información\" se ha aplicado, por ejemplo, al etiquetado de parto de expresión [20], el reconocimiento de entidades nombrado [25] y la extracción de tabla [19].2.3 Búsqueda utilizando la información del título La información del título es útil para la recuperación de documentos.",
                "Machine Learning, 20: 273-297, 1995. [4] Chieu, H. L. y Ng, H. T. Un enfoque de entropía máxima para la \"extracción de información\" de texto semiestructurado y libre.",
                "En Actas del primer taller internacional sobre el análisis de imágenes de documentos para bibliotecas, 225-232, 2004. [17] McCallum, A., Freitag, D. y Pereira, F. Modelos máximos de entropía Markov para \"extracción de información\" y segmentación."
            ],
            "translated_text": "",
            "candidates": [
                "extracción de información",
                "extracción de información",
                "extracción de información",
                "extracción de información",
                "extracción de información",
                "extracción de información",
                "extracción de información",
                "extracción de información",
                "extracción de información",
                "extracción de información",
                "extracción de información"
            ],
            "error": []
        },
        "metada extraction": {
            "translated_key": "extracción de metadatos",
            "is_in_text": false,
            "original_annotated_sentences": [
                "Automatic Extraction of Titles from General Documents using Machine Learning Yunhua Hu1 Computer Science Department Xian Jiaotong University No 28, Xianning West Road Xian, China, 710049 yunhuahu@mail.xjtu.edu.cn Hang Li, Yunbo Cao Microsoft Research Asia 5F Sigma Center, No.",
                "49 Zhichun Road, Haidian, Beijing, China, 100080 {hangli,yucao}@microsoft.com Qinghua Zheng Computer Science Department Xian Jiaotong University No 28, Xianning West Road Xian, China, 710049 qhzheng@mail.xjtu.edu.cn Dmitriy Meyerzon Microsoft Corporation One Microsoft Way Redmond, WA, USA, 98052 dmitriym@microsoft.com ABSTRACT In this paper, we propose a machine learning approach to title extraction from general documents.",
                "By general documents, we mean documents that can belong to any one of a number of specific genres, including presentations, book chapters, technical papers, brochures, reports, and letters.",
                "Previously, methods have been proposed mainly for title extraction from research papers.",
                "It has not been clear whether it could be possible to conduct automatic title extraction from general documents.",
                "As a case study, we consider extraction from Office including Word and PowerPoint.",
                "In our approach, we annotate titles in sample documents (for Word and PowerPoint respectively) and take them as training data, train machine learning models, and perform title extraction using the trained models.",
                "Our method is unique in that we mainly utilize formatting information such as font size as features in the models.",
                "It turns out that the use of formatting information can lead to quite accurate extraction from general documents.",
                "Precision and recall for title extraction from Word is 0.810 and 0.837 respectively, and precision and recall for title extraction from PowerPoint is 0.875 and 0.895 respectively in an experiment on intranet data.",
                "Other important new findings in this work include that we can train models in one domain and apply them to another domain, and more surprisingly we can even train models in one language and apply them to another language.",
                "Moreover, we can significantly improve search ranking results in document retrieval by using the extracted titles.",
                "Categories and Subject Descriptors H.3.3 [Information Storage and Retrieval]: Information Search and Retrieval - Search Process; H.4.1 [Information Systems Applications]: Office Automation - Word processing; D.2.8 [Software Engineering]: Metrics - complexity measures, performance measures General Terms Algorithms, Experimentation, Performance. 1.",
                "INTRODUCTION Metadata of documents is useful for many kinds of document processing such as search, browsing, and filtering.",
                "Ideally, metadata is defined by the authors of documents and is then used by various systems.",
                "However, people seldom define document metadata by themselves, even when they have convenient metadata definition tools [26].",
                "Thus, how to automatically extract metadata from the bodies of documents turns out to be an important research issue.",
                "Methods for performing the task have been proposed.",
                "However, the focus was mainly on extraction from research papers.",
                "For instance, Han et al. [10] proposed a machine learning based method to conduct extraction from research papers.",
                "They formalized the problem as that of classification and employed Support Vector Machines as the classifier.",
                "They mainly used linguistic features in the model.1 In this paper, we consider metadata extraction from general documents.",
                "By general documents, we mean documents that may belong to any one of a number of specific genres.",
                "General documents are more widely available in digital libraries, intranets and the internet, and thus investigation on extraction from them is sorely needed.",
                "Research papers usually have well-formed styles and noticeable characteristics.",
                "In contrast, the styles of general documents can vary greatly.",
                "It has not been clarified whether a machine learning based approach can work well for this task.",
                "There are many types of metadata: title, author, date of creation, etc.",
                "As a case study, we consider title extraction in this paper.",
                "General documents can be in many different file formats: Microsoft Office, PDF (PS), etc.",
                "As a case study, we consider extraction from Office including Word and PowerPoint.",
                "We take a machine learning approach.",
                "We annotate titles in sample documents (for Word and PowerPoint respectively) and take them as training data to train several types of models, and perform title extraction using any one type of the trained models.",
                "In the models, we mainly utilize formatting information such as font size as features.",
                "We employ the following models: Maximum Entropy Model, Perceptron with Uneven Margins, Maximum Entropy Markov Model, and Voted Perceptron.",
                "In this paper, we also investigate the following three problems, which did not seem to have been examined previously. (1) Comparison between models: among the models above, which model performs best for title extraction; (2) Generality of model: whether it is possible to train a model on one domain and apply it to another domain, and whether it is possible to train a model in one language and apply it to another language; (3) Usefulness of extracted titles: whether extracted titles can improve document processing such as search.",
                "Experimental results indicate that our approach works well for title extraction from general documents.",
                "Our method can significantly outperform the baselines: one that always uses the first lines as titles and the other that always uses the lines in the largest font sizes as titles.",
                "Precision and recall for title extraction from Word are 0.810 and 0.837 respectively, and precision and recall for title extraction from PowerPoint are 0.875 and 0.895 respectively.",
                "It turns out that the use of format features is the key to successful title extraction. (1) We have observed that Perceptron based models perform better in terms of extraction accuracies. (2) We have empirically verified that the models trained with our approach are generic in the sense that they can be trained on one domain and applied to another, and they can be trained in one language and applied to another. (3) We have found that using the extracted titles we can significantly improve precision of document retrieval (by 10%).",
                "We conclude that we can indeed conduct reliable title extraction from general documents and use the extracted results to improve real applications.",
                "The rest of the paper is organized as follows.",
                "In section 2, we introduce related work, and in section 3, we explain the motivation and problem setting of our work.",
                "In section 4, we describe our method of title extraction, and in section 5, we describe our method of document retrieval using extracted titles.",
                "Section 6 gives our experimental results.",
                "We make concluding remarks in section 7. 2.",
                "RELATED WORK 2.1 Document Metadata Extraction Methods have been proposed for performing automatic metadata extraction from documents; however, the main focus was on extraction from research papers.",
                "The proposed methods fall into two categories: the rule based approach and the machine learning based approach.",
                "Giuffrida et al. [9], for instance, developed a rule-based system for automatically extracting metadata from research papers in Postscript.",
                "They used rules like titles are usually located on the upper portions of the first pages and they are usually in the largest font sizes.",
                "Liddy et al. [14] and Yilmazel el al. [23] performed metadata extraction from educational materials using rule-based natural language processing technologies.",
                "Mao et al. [16] also conducted automatic metadata extraction from research papers using rules on formatting information.",
                "The rule-based approach can achieve high performance.",
                "However, it also has disadvantages.",
                "It is less adaptive and robust when compared with the machine learning approach.",
                "Han et al. [10], for instance, conducted metadata extraction with the machine learning approach.",
                "They viewed the problem as that of classifying the lines in a document into the categories of metadata and proposed using Support Vector Machines as the classifier.",
                "They mainly used linguistic information as features.",
                "They reported high extraction accuracy from research papers in terms of precision and recall. 2.2 Information Extraction Metadata extraction can be viewed as an application of information extraction, in which given a sequence of instances, we identify a subsequence that represents information in which we are interested.",
                "Hidden Markov Model [6], Maximum Entropy Model [1, 4], Maximum Entropy Markov Model [17], Support Vector Machines [3], Conditional Random Field [12], and Voted Perceptron [2] are widely used information extraction models.",
                "Information extraction has been applied, for instance, to part-ofspeech tagging [20], named entity recognition [25] and table extraction [19]. 2.3 Search Using Title Information Title information is useful for document retrieval.",
                "In the system Citeseer, for instance, Giles et al. managed to extract titles from research papers and make use of the extracted titles in metadata search of papers [8].",
                "In web search, the title fields (i.e., file properties) and anchor texts of web pages (HTML documents) can be viewed as titles of the pages [5].",
                "Many search engines seem to utilize them for web page retrieval [7, 11, 18, 22].",
                "Zhang et al., found that web pages with well-defined metadata are more easily retrieved than those without well-defined metadata [24].",
                "To the best of our knowledge, no research has been conducted on using extracted titles from general documents (e.g., Office documents) for search of the documents. 146 3.",
                "MOTIVATION AND PROBLEM SETTING We consider the issue of automatically extracting titles from general documents.",
                "By general documents, we mean documents that belong to one of any number of specific genres.",
                "The documents can be presentations, books, book chapters, technical papers, brochures, reports, memos, specifications, letters, announcements, or resumes.",
                "General documents are more widely available in digital libraries, intranets, and internet, and thus investigation on title extraction from them is sorely needed.",
                "Figure 1 shows an estimate on distributions of file formats on intranet and internet [15].",
                "Office and PDF are the main file formats on the intranet.",
                "Even on the internet, the documents in the formats are still not negligible, given its extremely large size.",
                "In this paper, without loss of generality, we take Office documents as an example.",
                "Figure 1.",
                "Distributions of file formats in internet and intranet.",
                "For Office documents, users can define titles as file properties using a feature provided by Office.",
                "We found in an experiment, however, that users seldom use the feature and thus titles in file properties are usually very inaccurate.",
                "That is to say, titles in file properties are usually inconsistent with the true titles in the file bodies that are created by the authors and are visible to readers.",
                "We collected 6,000 Word and 6,000 PowerPoint documents from an intranet and the internet and examined how many titles in the file properties are correct.",
                "We found that surprisingly the accuracy was only 0.265 (cf., Section 6.3 for details).",
                "A number of reasons can be considered.",
                "For example, if one creates a new file by copying an old file, then the file property of the new file will also be copied from the old file.",
                "In another experiment, we found that Google uses the titles in file properties of Office documents in search and browsing, but the titles are not very accurate.",
                "We created 50 queries to search Word and PowerPoint documents and examined the top 15 results of each query returned by Google.",
                "We found that nearly all the titles presented in the search results were from the file properties of the documents.",
                "However, only 0.272 of them were correct.",
                "Actually, true titles usually exist at the beginnings of the bodies of documents.",
                "If we can accurately extract the titles from the bodies of documents, then we can exploit reliable title information in document processing.",
                "This is exactly the problem we address in this paper.",
                "More specifically, given a Word document, we are to extract the title from the top region of the first page.",
                "Given a PowerPoint document, we are to extract the title from the first slide.",
                "A title sometimes consists of a main title and one or two subtitles.",
                "We only consider extraction of the main title.",
                "As baselines for title extraction, we use that of always using the first lines as titles and that of always using the lines with largest font sizes as titles.",
                "Figure 2.",
                "Title extraction from Word document.",
                "Figure 3.",
                "Title extraction from PowerPoint document.",
                "Next, we define a specification for human judgments in title data annotation.",
                "The annotated data will be used in training and testing of the title extraction methods.",
                "Summary of the specification: The title of a document should be identified on the basis of common sense, if there is no difficulty in the identification.",
                "However, there are many cases in which the identification is not easy.",
                "There are some rules defined in the specification that guide identification for such cases.",
                "The rules include a title is usually in consecutive lines in the same format, a document can have no title, titles in images are not considered, a title should not contain words like draft, 147 whitepaper, etc, if it is difficult to determine which is the title, select the one in the largest font size, and if it is still difficult to determine which is the title, select the first candidate. (The specification covers all the cases we have encountered in data annotation.)",
                "Figures 2 and 3 show examples of Office documents from which we conduct title extraction.",
                "In Figure 2, Differences in Win32 API Implementations among Windows Operating Systems is the title of the Word document.",
                "Microsoft Windows on the top of this page is a picture and thus is ignored.",
                "In Figure 3, Building Competitive Advantages through an Agile Infrastructure is the title of the PowerPoint document.",
                "We have developed a tool for annotation of titles by human annotators.",
                "Figure 4 shows a snapshot of the tool.",
                "Figure 4.",
                "Title annotation tool. 4.",
                "TITLE EXTRACTION METHOD 4.1 Outline Title extraction based on machine learning consists of training and extraction.",
                "The same pre-processing step occurs before training and extraction.",
                "During pre-processing, from the top region of the first page of a Word document or the first slide of a PowerPoint document a number of units for processing are extracted.",
                "If a line (lines are separated by return symbols) only has a single format, then the line will become a unit.",
                "If a line has several parts and each of them has its own format, then each part will become a unit.",
                "Each unit will be treated as an instance in learning.",
                "A unit contains not only content information (linguistic information) but also formatting information.",
                "The input to pre-processing is a document and the output of pre-processing is a sequence of units (instances).",
                "Figure 5 shows the units obtained from the document in Figure 2.",
                "Figure 5.",
                "Example of units.",
                "In learning, the input is sequences of units where each sequence corresponds to a document.",
                "We take labeled units (labeled as title_begin, title_end, or other) in the sequences as training data and construct models for identifying whether a unit is title_begin title_end, or other.",
                "We employ four types of models: Perceptron, Maximum Entropy (ME), Perceptron Markov Model (PMM), and Maximum Entropy Markov Model (MEMM).",
                "In extraction, the input is a sequence of units from one document.",
                "We employ one type of model to identify whether a unit is title_begin, title_end, or other.",
                "We then extract units from the unit labeled with title_begin to the unit labeled with title_end.",
                "The result is the extracted title of the document.",
                "The unique characteristic of our approach is that we mainly utilize formatting information for title extraction.",
                "Our assumption is that although general documents vary in styles, their formats have certain patterns and we can learn and utilize the patterns for title extraction.",
                "This is in contrast to the work by Han et al., in which only linguistic features are used for extraction from research papers. 4.2 Models The four models actually can be considered in the same metadata extraction framework.",
                "That is why we apply them together to our current problem.",
                "Each input is a sequence of instances kxxx L21 together with a sequence of labels kyyy L21 . ix and iy represents an instance and its label, respectively ( ki ,,2,1 L= ).",
                "Recall that an instance here represents a unit.",
                "A label represents title_begin, title_end, or other.",
                "Here, k is the number of units in a document.",
                "In learning, we train a model which can be generally denoted as a conditional probability distribution )|( 11 kk XXYYP LL where iX and iY denote random variables taking instance ix and label iy as values, respectively ( ki ,,2,1 L= ).",
                "Learning Tool Extraction Tool 21121 2222122221 1121111211 nknnknn kk kk yyyxxx yyyxxx yyyxxx LL LL LL LL → → → )|(maxarg 11 mkmmkm xxyyP LL )|( 11 kk XXYYP LL Conditional Distribution mkmm xxx L21 Figure 6.",
                "Metadata extraction model.",
                "We can make assumptions about the general model in order to make it simple enough for training. 148 For example, we can assume that kYY ,,1 L are independent of each other given kXX ,,1 L .",
                "Thus, we have )|()|( )|( 11 11 kk kk XYPXYP XXYYP L LL = In this way, we decompose the model into a number of classifiers.",
                "We train the classifiers locally using the labeled data.",
                "As the classifier, we employ the Perceptron or Maximum Entropy model.",
                "We can also assume that the first order Markov property holds for kYY ,,1 L given kXX ,,1 L .",
                "Thus, we have )|()|( )|( 111 11 kkk kk XYYPXYP XXYYP −= L LL Again, we obtain a number of classifiers.",
                "However, the classifiers are conditioned on the previous label.",
                "When we employ the Percepton or Maximum Entropy model as a classifier, the models become a Percepton Markov Model or Maximum Entropy Markov Model, respectively.",
                "That is to say, the two models are more precise.",
                "In extraction, given a new sequence of instances, we resort to one of the constructed models to assign a sequence of labels to the sequence of instances, i.e., perform extraction.",
                "For Perceptron and ME, we assign labels locally and combine the results globally later using heuristics.",
                "Specifically, we first identify the most likely title_begin.",
                "Then we find the most likely title_end within three units after the title_begin.",
                "Finally, we extract as a title the units between the title_begin and the title_end.",
                "For PMM and MEMM, we employ the Viterbi algorithm to find the globally optimal label sequence.",
                "In this paper, for Perceptron, we actually employ an improved variant of it, called Perceptron with Uneven Margin [13].",
                "This version of Perceptron can work well especially when the number of positive instances and the number of negative instances differ greatly, which is exactly the case in our problem.",
                "We also employ an improved version of Perceptron Markov Model in which the Perceptron model is the so-called Voted Perceptron [2].",
                "In addition, in training, the parameters of the model are updated globally rather than locally. 4.3 Features There are two types of features: format features and linguistic features.",
                "We mainly use the former.",
                "The features are used for both the title-begin and the title-end classifiers. 4.3.1 Format Features Font Size: There are four binary features that represent the normalized font size of the unit (recall that a unit has only one type of font).",
                "If the font size of the unit is the largest in the document, then the first feature will be 1, otherwise 0.",
                "If the font size is the smallest in the document, then the fourth feature will be 1, otherwise 0.",
                "If the font size is above the average font size and not the largest in the document, then the second feature will be 1, otherwise 0.",
                "If the font size is below the average font size and not the smallest, the third feature will be 1, otherwise 0.",
                "It is necessary to conduct normalization on font sizes.",
                "For example, in one document the largest font size might be 12pt, while in another the smallest one might be 18pt.",
                "Boldface: This binary feature represents whether or not the current unit is in boldface.",
                "Alignment: There are four binary features that respectively represent the location of the current unit: left, center, right, and unknown alignment.",
                "The following format features with respect to context play an important role in title extraction.",
                "Empty Neighboring Unit: There are two binary features that represent, respectively, whether or not the previous unit and the current unit are blank lines.",
                "Font Size Change: There are two binary features that represent, respectively, whether or not the font size of the previous unit and the font size of the next unit differ from that of the current unit.",
                "Alignment Change: There are two binary features that represent, respectively, whether or not the alignment of the previous unit and the alignment of the next unit differ from that of the current one.",
                "Same Paragraph: There are two binary features that represent, respectively, whether or not the previous unit and the next unit are in the same paragraph as the current unit. 4.3.2 Linguistic Features The linguistic features are based on key words.",
                "Positive Word: This binary feature represents whether or not the current unit begins with one of the positive words.",
                "The positive words include title:, subject:, subject line: For example, in some documents the lines of titles and authors have the same formats.",
                "However, if lines begin with one of the positive words, then it is likely that they are title lines.",
                "Negative Word: This binary feature represents whether or not the current unit begins with one of the negative words.",
                "The negative words include To, By, created by, updated by, etc.",
                "There are more negative words than positive words.",
                "The above linguistic features are language dependent.",
                "Word Count: A title should not be too long.",
                "We heuristically create four intervals: [1, 2], [3, 6], [7, 9] and [9, ∞) and define one feature for each interval.",
                "If the number of words in a title falls into an interval, then the corresponding feature will be 1; otherwise 0.",
                "Ending Character: This feature represents whether the unit ends with :, -, or other special characters.",
                "A title usually does not end with such a character. 5.",
                "DOCUMENT RETRIEVAL METHOD We describe our method of document retrieval using extracted titles.",
                "Typically, in information retrieval a document is split into a number of fields including body, title, and anchor text.",
                "A ranking function in search can use different weights for different fields of 149 the document.",
                "Also, titles are typically assigned high weights, indicating that they are important for document retrieval.",
                "As explained previously, our experiment has shown that a significant number of documents actually have incorrect titles in the file properties, and thus in addition of using them we use the extracted titles as one more field of the document.",
                "By doing this, we attempt to improve the overall precision.",
                "In this paper, we employ a modification of BM25 that allows field weighting [21].",
                "As fields, we make use of body, title, extracted title and anchor.",
                "First, for each term in the query we count the term frequency in each field of the document; each field frequency is then weighted according to the corresponding weight parameter: ∑= f tfft tfwwtf Similarly, we compute the document length as a weighted sum of lengths of each field.",
                "Average document length in the corpus becomes the average of all weighted document lengths. ∑= f ff dlwwdl In our experiments we used 75.0,8.11 == bk .",
                "Weight for content was 1.0, title was 10.0, anchor was 10.0, and extracted title was 5.0. 6.",
                "EXPERIMENTAL RESULTS 6.1 Data Sets and Evaluation Measures We used two data sets in our experiments.",
                "First, we downloaded and randomly selected 5,000 Word documents and 5,000 PowerPoint documents from an intranet of Microsoft.",
                "We call it MS hereafter.",
                "Second, we downloaded and randomly selected 500 Word and 500 PowerPoint documents from the DotGov and DotCom domains on the internet, respectively.",
                "Figure 7 shows the distributions of the genres of the documents.",
                "We see that the documents are indeed general documents as we define them.",
                "Figure 7.",
                "Distributions of document genres.",
                "Third, a data set in Chinese was also downloaded from the internet.",
                "It includes 500 Word documents and 500 PowerPoint documents in Chinese.",
                "We manually labeled the titles of all the documents, on the basis of our specification.",
                "Not all the documents in the two data sets have titles.",
                "Table 1 shows the percentages of the documents having titles.",
                "We see that DotCom and DotGov have more PowerPoint documents with titles than MS.",
                "This might be because PowerPoint documents published on the internet are more formal than those on the intranet.",
                "Table 1.",
                "The portion of documents with titles Domain Type MS DotCom DotGov Word 75.7% 77.8% 75.6% PowerPoint 82.1% 93.4% 96.4% In our experiments, we conducted evaluations on title extraction in terms of precision, recall, and F-measure.",
                "The evaluation measures are defined as follows: Precision: P = A / ( A + B ) Recall: R = A / ( A + C ) F-measure: F1 = 2PR / ( P + R ) Here, A, B, C, and D are numbers of documents as those defined in Table 2.",
                "Table 2.",
                "Contingence table with regard to title extraction Is title Is not title Extracted A B Not extracted C D 6.2 Baselines We test the accuracies of the two baselines described in section 4.2.",
                "They are denoted as largest font size and first line respectively. 6.3 Accuracy of Titles in File Properties We investigate how many titles in the file properties of the documents are reliable.",
                "We view the titles annotated by humans as true titles and test how many titles in the file properties can approximately match with the true titles.",
                "We use Edit Distance to conduct the approximate match. (Approximate match is only used in this evaluation).",
                "This is because sometimes human annotated titles can be slightly different from the titles in file properties on the surface, e.g., contain extra spaces).",
                "Given string A and string B: if ( (D == 0) or ( D / ( La + Lb ) < θ ) ) then string A = string B D: Edit Distance between string A and string B La: length of string A Lb: length of string B θ: 0.1 ∑ × ++− + = t t n N wtf avwdl wdl bbk kwtf FBM )log( ))1(( )1( 25 1 1 150 Table 3.",
                "Accuracies of titles in file properties File Type Domain Precision Recall F1 MS 0.299 0.311 0.305 DotCom 0.210 0.214 0.212Word DotGov 0.182 0.177 0.180 MS 0.229 0.245 0.237 DotCom 0.185 0.186 0.186PowerPoint DotGov 0.180 0.182 0.181 6.4 Comparison with Baselines We conducted title extraction from the first data set (Word and PowerPoint in MS).",
                "As the model, we used Perceptron.",
                "We conduct 4-fold cross validation.",
                "Thus, all the results reported here are those averaged over 4 trials.",
                "Tables 4 and 5 show the results.",
                "We see that Perceptron significantly outperforms the baselines.",
                "In the evaluation, we use exact matching between the true titles annotated by humans and the extracted titles.",
                "Table 4.",
                "Accuracies of title extraction with Word Precision Recall F1 Model Perceptron 0.810 0.837 0.823 Largest font size 0.700 0.758 0.727 Baselines First line 0.707 0.767 0.736 Table 5.",
                "Accuracies of title extraction with PowerPoint Precision Recall F1 Model Perceptron 0.875 0. 895 0.885 Largest font size 0.844 0.887 0.865 Baselines First line 0.639 0.671 0.655 We see that the machine learning approach can achieve good performance in title extraction.",
                "For Word documents both precision and recall of the approach are 8 percent higher than those of the baselines.",
                "For PowerPoint both precision and recall of the approach are 2 percent higher than those of the baselines.",
                "We conduct significance tests.",
                "The results are shown in Table 6.",
                "Here, Largest denotes the baseline of using the largest font size, First denotes the baseline of using the first line.",
                "The results indicate that the improvements of machine learning over baselines are statistically significant (in the sense p-value < 0.05) Table 6.",
                "Sign test results Documents Type Sign test between p-value Perceptron vs. Largest 3.59e-26 Word Perceptron vs. First 7.12e-10 Perceptron vs. Largest 0.010 PowerPoint Perceptron vs. First 5.13e-40 We see, from the results, that the two baselines can work well for title extraction, suggesting that font size and position information are most useful features for title extraction.",
                "However, it is also obvious that using only these two features is not enough.",
                "There are cases in which all the lines have the same font size (i.e., the largest font size), or cases in which the lines with the largest font size only contain general descriptions like Confidential, White paper, etc.",
                "For those cases, the largest font size method cannot work well.",
                "For similar reasons, the first line method alone cannot work well, either.",
                "With the combination of different features (evidence in title judgment), Perceptron can outperform Largest and First.",
                "We investigate the performance of solely using linguistic features.",
                "We found that it does not work well.",
                "It seems that the format features play important roles and the linguistic features are supplements..",
                "Figure 8.",
                "An example Word document.",
                "Figure 9.",
                "An example PowerPoint document.",
                "We conducted an error analysis on the results of Perceptron.",
                "We found that the errors fell into three categories. (1) About one third of the errors were related to hard cases.",
                "In these documents, the layouts of the first pages were difficult to understand, even for humans.",
                "Figure 8 and 9 shows examples. (2) Nearly one fourth of the errors were from the documents which do not have true titles but only contain bullets.",
                "Since we conduct extraction from the top regions, it is difficult to get rid of these errors with the current approach. (3).",
                "Confusions between main titles and subtitles were another type of error.",
                "Since we only labeled the main titles as titles, the extractions of both titles were considered incorrect.",
                "This type of error does little harm to document processing like search, however. 6.5 Comparison between Models To compare the performance of different machine learning models, we conducted another experiment.",
                "Again, we perform 4-fold cross 151 validation on the first data set (MS).",
                "Table 7, 8 shows the results of all the four models.",
                "It turns out that Perceptron and PMM perform the best, followed by MEMM, and ME performs the worst.",
                "In general, the Markovian models perform better than or as well as their classifier counterparts.",
                "This seems to be because the Markovian models are trained globally, while the classifiers are trained locally.",
                "The Perceptron based models perform better than the ME based counterparts.",
                "This seems to be because the Perceptron based models are created to make better classifications, while ME models are constructed for better prediction.",
                "Table 7.",
                "Comparison between different learning models for title extraction with Word Model Precision Recall F1 Perceptron 0.810 0.837 0.823 MEMM 0.797 0.824 0.810 PMM 0.827 0.823 0.825 ME 0.801 0.621 0.699 Table 8.",
                "Comparison between different learning models for title extraction with PowerPoint Model Precision Recall F1 Perceptron 0.875 0. 895 0. 885 MEMM 0.841 0.861 0.851 PMM 0.873 0.896 0.885 ME 0.753 0.766 0.759 6.6 Domain Adaptation We apply the model trained with the first data set (MS) to the second data set (DotCom and DotGov).",
                "Tables 9-12 show the results.",
                "Table 9.",
                "Accuracies of title extraction with Word in DotGov Precision Recall F1 Model Perceptron 0.716 0.759 0.737 Largest font size 0.549 0.619 0.582Baselines First line 0.462 0.521 0.490 Table 10.",
                "Accuracies of title extraction with PowerPoint in DotGov Precision Recall F1 Model Perceptron 0.900 0.906 0.903 Largest font size 0.871 0.888 0.879Baselines First line 0.554 0.564 0.559 Table 11.",
                "Accuracies of title extraction with Word in DotCom Precisio n Recall F1 Model Perceptron 0.832 0.880 0.855 Largest font size 0.676 0.753 0.712Baselines First line 0.577 0.643 0.608 Table 12.",
                "Performance of PowerPoint document title extraction in DotCom Precisio n Recall F1 Model Perceptron 0.910 0.903 0.907 Largest font size 0.864 0.886 0.875Baselines First line 0.570 0.585 0.577 From the results, we see that the models can be adapted to different domains well.",
                "There is almost no drop in accuracy.",
                "The results indicate that the patterns of title formats exist across different domains, and it is possible to construct a domain independent model by mainly using formatting information. 6.7 Language Adaptation We apply the model trained with the data in English (MS) to the data set in Chinese.",
                "Tables 13-14 show the results.",
                "Table 13.",
                "Accuracies of title extraction with Word in Chinese Precision Recall F1 Model Perceptron 0.817 0.805 0.811 Largest font size 0.722 0.755 0.738Baselines First line 0.743 0.777 0.760 Table 14.",
                "Accuracies of title extraction with PowerPoint in Chinese Precision Recall F1 Model Perceptron 0.766 0.812 0.789 Largest font size 0.753 0.813 0.782Baselines First line 0.627 0.676 0.650 We see that the models can be adapted to a different language.",
                "There are only small drops in accuracy.",
                "Obviously, the linguistic features do not work for Chinese, but the effect of not using them is negligible.",
                "The results indicate that the patterns of title formats exist across different languages.",
                "From the domain adaptation and language adaptation results, we conclude that the use of formatting information is the key to a successful extraction from general documents. 6.8 Search with Extracted Titles We performed experiments on using title extraction for document retrieval.",
                "As a baseline, we employed BM25 without using extracted titles.",
                "The ranking mechanism was as described in Section 5.",
                "The weights were heuristically set.",
                "We did not conduct optimization on the weights.",
                "The evaluation was conducted on a corpus of 1.3 M documents crawled from the intranet of Microsoft using 100 evaluation queries obtained from this intranets search engine query logs. 50 queries were from the most popular set, while 50 queries other were chosen randomly.",
                "Users were asked to provide judgments of the degree of document relevance from a scale of 1to 5 (1 meaning detrimental, 2 - bad, 3 - fair, 4 - good and 5 - excellent). 152 Figure 10 shows the results.",
                "In the chart two sets of precision results were obtained by either considering good or excellent documents as relevant (left 3 bars with relevance threshold 0.5), or by considering only excellent documents as relevant (right 3 bars with relevance threshold 1.0) 0 0.05 0.1 0.15 0.2 0.25 0.3 0.35 0.4 0.45 P@10 P@5 Reciprocal P@10 P@5 Reciprocal 0.5 1 BM25 Anchor, Title, Body BM25 Anchor, Title, Body, ExtractedTitle Name All RelevanceThreshold Data Description Figure 10.",
                "Search ranking results.",
                "Figure 10 shows different document retrieval results with different ranking functions in terms of precision @10, precision @5 and reciprocal rank: • Blue bar - BM25 including the fields body, title (file property), and anchor text. • Purple bar - BM25 including the fields body, title (file property), anchor text, and extracted title.",
                "With the additional field of extracted title included in BM25 the precision @10 increased from 0.132 to 0.145, or by ~10%.",
                "Thus, it is safe to say that the use of extracted title can indeed improve the precision of document retrieval. 7.",
                "CONCLUSION In this paper, we have investigated the problem of automatically extracting titles from general documents.",
                "We have tried using a machine learning approach to address the problem.",
                "Previous work showed that the machine learning approach can work well for metadata extraction from research papers.",
                "In this paper, we showed that the approach can work for extraction from general documents as well.",
                "Our experimental results indicated that the machine learning approach can work significantly better than the baselines in title extraction from Office documents.",
                "Previous work on metadata extraction mainly used linguistic features in documents, while we mainly used formatting information.",
                "It appeared that using formatting information is a key for successfully conducting title extraction from general documents.",
                "We tried different machine learning models including Perceptron, Maximum Entropy, Maximum Entropy Markov Model, and Voted Perceptron.",
                "We found that the performance of the Perceptorn models was the best.",
                "We applied models constructed in one domain to another domain and applied models trained in one language to another language.",
                "We found that the accuracies did not drop substantially across different domains and across different languages, indicating that the models were generic.",
                "We also attempted to use the extracted titles in document retrieval.",
                "We observed a significant improvement in document ranking performance for search when using extracted title information.",
                "All the above investigations were not conducted in previous work, and through our investigations we verified the generality and the significance of the title extraction approach. 8.",
                "ACKNOWLEDGEMENTS We thank Chunyu Wei and Bojuan Zhao for their work on data annotation.",
                "We acknowledge Jinzhu Li for his assistance in conducting the experiments.",
                "We thank Ming Zhou, John Chen, Jun Xu, and the anonymous reviewers of JCDL05 for their valuable comments on this paper. 9.",
                "REFERENCES [1] Berger, A. L., Della Pietra, S. A., and Della Pietra, V. J.",
                "A maximum entropy approach to natural language processing.",
                "Computational Linguistics, 22:39-71, 1996. [2] Collins, M. Discriminative training methods for hidden markov models: theory and experiments with perceptron algorithms.",
                "In Proceedings of Conference on Empirical Methods in Natural Language Processing, 1-8, 2002. [3] Cortes, C. and Vapnik, V. Support-vector networks.",
                "Machine Learning, 20:273-297, 1995. [4] Chieu, H. L. and Ng, H. T. A maximum entropy approach to information extraction from semi-structured and free text.",
                "In Proceedings of the Eighteenth National Conference on Artificial Intelligence, 768-791, 2002. [5] Evans, D. K., Klavans, J. L., and McKeown, K. R. Columbia newsblaster: multilingual news summarization on the Web.",
                "In Proceedings of Human Language Technology conference / North American chapter of the Association for Computational Linguistics annual meeting, 1-4, 2004. [6] Ghahramani, Z. and Jordan, M. I. Factorial hidden markov models.",
                "Machine Learning, 29:245-273, 1997. [7] Gheel, J. and Anderson, T. Data and metadata for finding and reminding, In Proceedings of the 1999 International Conference on Information Visualization, 446-451,1999. [8] Giles, C. L., Petinot, Y., Teregowda P. B., Han, H., Lawrence, S., Rangaswamy, A., and Pal, N. eBizSearch: a niche search engine for e-Business.",
                "In Proceedings of the 26th Annual International ACM SIGIR Conference on Research and Development in Information Retrieval, 413414, 2003. [9] Giuffrida, G., Shek, E. C., and Yang, J. Knowledge-based metadata extraction from PostScript files.",
                "In Proceedings of the Fifth ACM Conference on Digital Libraries, 77-84, 2000. [10] Han, H., Giles, C. L., Manavoglu, E., Zha, H., Zhang, Z., and Fox, E. A.",
                "Automatic document metadata extraction using support vector machines.",
                "In Proceedings of the Third ACM/IEEE-CS Joint Conference on Digital Libraries, 37-48, 2003. [11] Kobayashi, M., and Takeda, K. Information retrieval on the Web.",
                "ACM Computing Surveys, 32:144-173, 2000. [12] Lafferty, J., McCallum, A., and Pereira, F. Conditional random fields: probabilistic models for segmenting and 153 labeling sequence data.",
                "In Proceedings of the Eighteenth International Conference on Machine Learning, 282-289, 2001. [13] Li, Y., Zaragoza, H., Herbrich, R., Shawe-Taylor J., and Kandola, J. S. The perceptron algorithm with uneven margins.",
                "In Proceedings of the Nineteenth International Conference on Machine Learning, 379-386, 2002. [14] Liddy, E. D., Sutton, S., Allen, E., Harwell, S., Corieri, S., Yilmazel, O., Ozgencil, N. E., Diekema, A., McCracken, N., and Silverstein, J.",
                "Automatic Metadata generation & evaluation.",
                "In Proceedings of the 25th Annual International ACM SIGIR Conference on Research and Development in Information Retrieval, 401-402, 2002. [15] Littlefield, A.",
                "Effective enterprise information retrieval across new content formats.",
                "In Proceedings of the Seventh Search Engine Conference, http://www.infonortics.com/searchengines/sh02/02prog.html, 2002. [16] Mao, S., Kim, J. W., and Thoma, G. R. A dynamic feature generation system for automated metadata extraction in preservation of digital materials.",
                "In Proceedings of the First International Workshop on Document Image Analysis for Libraries, 225-232, 2004. [17] McCallum, A., Freitag, D., and Pereira, F. Maximum entropy markov models for information extraction and segmentation.",
                "In Proceedings of the Seventeenth International Conference on Machine Learning, 591-598, 2000. [18] Murphy, L. D. Digital document metadata in organizations: roles, analytical approaches, and future research directions.",
                "In Proceedings of the Thirty-First Annual Hawaii International Conference on System Sciences, 267-276, 1998. [19] Pinto, D., McCallum, A., Wei, X., and Croft, W. B.",
                "Table extraction using conditional random fields.",
                "In Proceedings of the 26th Annual International ACM SIGIR Conference on Research and Development in Information Retrieval, 235242, 2003. [20] Ratnaparkhi, A. Unsupervised statistical models for prepositional phrase attachment.",
                "In Proceedings of the Seventeenth International Conference on Computational Linguistics. 1079-1085, 1998. [21] Robertson, S., Zaragoza, H., and Taylor, M. Simple BM25 extension to multiple weighted fields, In Proceedings of ACM Thirteenth Conference on Information and Knowledge Management, 42-49, 2004. [22] Yi, J. and Sundaresan, N. Metadata based Web mining for relevance, In Proceedings of the 2000 International Symposium on Database Engineering & Applications, 113121, 2000. [23] Yilmazel, O., Finneran, C. M., and Liddy, E. D. MetaExtract: An NLP system to automatically assign metadata.",
                "In Proceedings of the 2004 Joint ACM/IEEE Conference on Digital Libraries, 241-242, 2004. [24] Zhang, J. and Dimitroff, A. Internet search engines response to metadata Dublin Core implementation.",
                "Journal of Information Science, 30:310-320, 2004. [25] Zhang, L., Pan, Y., and Zhang, T. Recognising and using named entities: focused named entity recognition using machine learning.",
                "In Proceedings of the 27th Annual International ACM SIGIR Conference on Research and Development in Information Retrieval, 281-288, 2004. [26] http://dublincore.org/groups/corporate/Seattle/ 154"
            ],
            "original_annotated_samples": [],
            "translated_annotated_samples": [],
            "translated_text": "",
            "candidates": [],
            "error": []
        },
        "machine learn": {
            "translated_key": "aprendizaje automático",
            "is_in_text": true,
            "original_annotated_sentences": [
                "Automatic Extraction of Titles from General Documents using Machine Learning Yunhua Hu1 Computer Science Department Xian Jiaotong University No 28, Xianning West Road Xian, China, 710049 yunhuahu@mail.xjtu.edu.cn Hang Li, Yunbo Cao Microsoft Research Asia 5F Sigma Center, No.",
                "49 Zhichun Road, Haidian, Beijing, China, 100080 {hangli,yucao}@microsoft.com Qinghua Zheng Computer Science Department Xian Jiaotong University No 28, Xianning West Road Xian, China, 710049 qhzheng@mail.xjtu.edu.cn Dmitriy Meyerzon Microsoft Corporation One Microsoft Way Redmond, WA, USA, 98052 dmitriym@microsoft.com ABSTRACT In this paper, we propose a <br>machine learn</br>ing approach to title extraction from general documents.",
                "By general documents, we mean documents that can belong to any one of a number of specific genres, including presentations, book chapters, technical papers, brochures, reports, and letters.",
                "Previously, methods have been proposed mainly for title extraction from research papers.",
                "It has not been clear whether it could be possible to conduct automatic title extraction from general documents.",
                "As a case study, we consider extraction from Office including Word and PowerPoint.",
                "In our approach, we annotate titles in sample documents (for Word and PowerPoint respectively) and take them as training data, train <br>machine learn</br>ing models, and perform title extraction using the trained models.",
                "Our method is unique in that we mainly utilize formatting information such as font size as features in the models.",
                "It turns out that the use of formatting information can lead to quite accurate extraction from general documents.",
                "Precision and recall for title extraction from Word is 0.810 and 0.837 respectively, and precision and recall for title extraction from PowerPoint is 0.875 and 0.895 respectively in an experiment on intranet data.",
                "Other important new findings in this work include that we can train models in one domain and apply them to another domain, and more surprisingly we can even train models in one language and apply them to another language.",
                "Moreover, we can significantly improve search ranking results in document retrieval by using the extracted titles.",
                "Categories and Subject Descriptors H.3.3 [Information Storage and Retrieval]: Information Search and Retrieval - Search Process; H.4.1 [Information Systems Applications]: Office Automation - Word processing; D.2.8 [Software Engineering]: Metrics - complexity measures, performance measures General Terms Algorithms, Experimentation, Performance. 1.",
                "INTRODUCTION Metadata of documents is useful for many kinds of document processing such as search, browsing, and filtering.",
                "Ideally, metadata is defined by the authors of documents and is then used by various systems.",
                "However, people seldom define document metadata by themselves, even when they have convenient metadata definition tools [26].",
                "Thus, how to automatically extract metadata from the bodies of documents turns out to be an important research issue.",
                "Methods for performing the task have been proposed.",
                "However, the focus was mainly on extraction from research papers.",
                "For instance, Han et al. [10] proposed a <br>machine learn</br>ing based method to conduct extraction from research papers.",
                "They formalized the problem as that of classification and employed Support Vector Machines as the classifier.",
                "They mainly used linguistic features in the model.1 In this paper, we consider metadata extraction from general documents.",
                "By general documents, we mean documents that may belong to any one of a number of specific genres.",
                "General documents are more widely available in digital libraries, intranets and the internet, and thus investigation on extraction from them is sorely needed.",
                "Research papers usually have well-formed styles and noticeable characteristics.",
                "In contrast, the styles of general documents can vary greatly.",
                "It has not been clarified whether a <br>machine learn</br>ing based approach can work well for this task.",
                "There are many types of metadata: title, author, date of creation, etc.",
                "As a case study, we consider title extraction in this paper.",
                "General documents can be in many different file formats: Microsoft Office, PDF (PS), etc.",
                "As a case study, we consider extraction from Office including Word and PowerPoint.",
                "We take a <br>machine learn</br>ing approach.",
                "We annotate titles in sample documents (for Word and PowerPoint respectively) and take them as training data to train several types of models, and perform title extraction using any one type of the trained models.",
                "In the models, we mainly utilize formatting information such as font size as features.",
                "We employ the following models: Maximum Entropy Model, Perceptron with Uneven Margins, Maximum Entropy Markov Model, and Voted Perceptron.",
                "In this paper, we also investigate the following three problems, which did not seem to have been examined previously. (1) Comparison between models: among the models above, which model performs best for title extraction; (2) Generality of model: whether it is possible to train a model on one domain and apply it to another domain, and whether it is possible to train a model in one language and apply it to another language; (3) Usefulness of extracted titles: whether extracted titles can improve document processing such as search.",
                "Experimental results indicate that our approach works well for title extraction from general documents.",
                "Our method can significantly outperform the baselines: one that always uses the first lines as titles and the other that always uses the lines in the largest font sizes as titles.",
                "Precision and recall for title extraction from Word are 0.810 and 0.837 respectively, and precision and recall for title extraction from PowerPoint are 0.875 and 0.895 respectively.",
                "It turns out that the use of format features is the key to successful title extraction. (1) We have observed that Perceptron based models perform better in terms of extraction accuracies. (2) We have empirically verified that the models trained with our approach are generic in the sense that they can be trained on one domain and applied to another, and they can be trained in one language and applied to another. (3) We have found that using the extracted titles we can significantly improve precision of document retrieval (by 10%).",
                "We conclude that we can indeed conduct reliable title extraction from general documents and use the extracted results to improve real applications.",
                "The rest of the paper is organized as follows.",
                "In section 2, we introduce related work, and in section 3, we explain the motivation and problem setting of our work.",
                "In section 4, we describe our method of title extraction, and in section 5, we describe our method of document retrieval using extracted titles.",
                "Section 6 gives our experimental results.",
                "We make concluding remarks in section 7. 2.",
                "RELATED WORK 2.1 Document Metadata Extraction Methods have been proposed for performing automatic metadata extraction from documents; however, the main focus was on extraction from research papers.",
                "The proposed methods fall into two categories: the rule based approach and the <br>machine learn</br>ing based approach.",
                "Giuffrida et al. [9], for instance, developed a rule-based system for automatically extracting metadata from research papers in Postscript.",
                "They used rules like titles are usually located on the upper portions of the first pages and they are usually in the largest font sizes.",
                "Liddy et al. [14] and Yilmazel el al. [23] performed metadata extraction from educational materials using rule-based natural language processing technologies.",
                "Mao et al. [16] also conducted automatic metadata extraction from research papers using rules on formatting information.",
                "The rule-based approach can achieve high performance.",
                "However, it also has disadvantages.",
                "It is less adaptive and robust when compared with the <br>machine learn</br>ing approach.",
                "Han et al. [10], for instance, conducted metadata extraction with the <br>machine learn</br>ing approach.",
                "They viewed the problem as that of classifying the lines in a document into the categories of metadata and proposed using Support Vector Machines as the classifier.",
                "They mainly used linguistic information as features.",
                "They reported high extraction accuracy from research papers in terms of precision and recall. 2.2 Information Extraction Metadata extraction can be viewed as an application of information extraction, in which given a sequence of instances, we identify a subsequence that represents information in which we are interested.",
                "Hidden Markov Model [6], Maximum Entropy Model [1, 4], Maximum Entropy Markov Model [17], Support Vector Machines [3], Conditional Random Field [12], and Voted Perceptron [2] are widely used information extraction models.",
                "Information extraction has been applied, for instance, to part-ofspeech tagging [20], named entity recognition [25] and table extraction [19]. 2.3 Search Using Title Information Title information is useful for document retrieval.",
                "In the system Citeseer, for instance, Giles et al. managed to extract titles from research papers and make use of the extracted titles in metadata search of papers [8].",
                "In web search, the title fields (i.e., file properties) and anchor texts of web pages (HTML documents) can be viewed as titles of the pages [5].",
                "Many search engines seem to utilize them for web page retrieval [7, 11, 18, 22].",
                "Zhang et al., found that web pages with well-defined metadata are more easily retrieved than those without well-defined metadata [24].",
                "To the best of our knowledge, no research has been conducted on using extracted titles from general documents (e.g., Office documents) for search of the documents. 146 3.",
                "MOTIVATION AND PROBLEM SETTING We consider the issue of automatically extracting titles from general documents.",
                "By general documents, we mean documents that belong to one of any number of specific genres.",
                "The documents can be presentations, books, book chapters, technical papers, brochures, reports, memos, specifications, letters, announcements, or resumes.",
                "General documents are more widely available in digital libraries, intranets, and internet, and thus investigation on title extraction from them is sorely needed.",
                "Figure 1 shows an estimate on distributions of file formats on intranet and internet [15].",
                "Office and PDF are the main file formats on the intranet.",
                "Even on the internet, the documents in the formats are still not negligible, given its extremely large size.",
                "In this paper, without loss of generality, we take Office documents as an example.",
                "Figure 1.",
                "Distributions of file formats in internet and intranet.",
                "For Office documents, users can define titles as file properties using a feature provided by Office.",
                "We found in an experiment, however, that users seldom use the feature and thus titles in file properties are usually very inaccurate.",
                "That is to say, titles in file properties are usually inconsistent with the true titles in the file bodies that are created by the authors and are visible to readers.",
                "We collected 6,000 Word and 6,000 PowerPoint documents from an intranet and the internet and examined how many titles in the file properties are correct.",
                "We found that surprisingly the accuracy was only 0.265 (cf., Section 6.3 for details).",
                "A number of reasons can be considered.",
                "For example, if one creates a new file by copying an old file, then the file property of the new file will also be copied from the old file.",
                "In another experiment, we found that Google uses the titles in file properties of Office documents in search and browsing, but the titles are not very accurate.",
                "We created 50 queries to search Word and PowerPoint documents and examined the top 15 results of each query returned by Google.",
                "We found that nearly all the titles presented in the search results were from the file properties of the documents.",
                "However, only 0.272 of them were correct.",
                "Actually, true titles usually exist at the beginnings of the bodies of documents.",
                "If we can accurately extract the titles from the bodies of documents, then we can exploit reliable title information in document processing.",
                "This is exactly the problem we address in this paper.",
                "More specifically, given a Word document, we are to extract the title from the top region of the first page.",
                "Given a PowerPoint document, we are to extract the title from the first slide.",
                "A title sometimes consists of a main title and one or two subtitles.",
                "We only consider extraction of the main title.",
                "As baselines for title extraction, we use that of always using the first lines as titles and that of always using the lines with largest font sizes as titles.",
                "Figure 2.",
                "Title extraction from Word document.",
                "Figure 3.",
                "Title extraction from PowerPoint document.",
                "Next, we define a specification for human judgments in title data annotation.",
                "The annotated data will be used in training and testing of the title extraction methods.",
                "Summary of the specification: The title of a document should be identified on the basis of common sense, if there is no difficulty in the identification.",
                "However, there are many cases in which the identification is not easy.",
                "There are some rules defined in the specification that guide identification for such cases.",
                "The rules include a title is usually in consecutive lines in the same format, a document can have no title, titles in images are not considered, a title should not contain words like draft, 147 whitepaper, etc, if it is difficult to determine which is the title, select the one in the largest font size, and if it is still difficult to determine which is the title, select the first candidate. (The specification covers all the cases we have encountered in data annotation.)",
                "Figures 2 and 3 show examples of Office documents from which we conduct title extraction.",
                "In Figure 2, Differences in Win32 API Implementations among Windows Operating Systems is the title of the Word document.",
                "Microsoft Windows on the top of this page is a picture and thus is ignored.",
                "In Figure 3, Building Competitive Advantages through an Agile Infrastructure is the title of the PowerPoint document.",
                "We have developed a tool for annotation of titles by human annotators.",
                "Figure 4 shows a snapshot of the tool.",
                "Figure 4.",
                "Title annotation tool. 4.",
                "TITLE EXTRACTION METHOD 4.1 Outline Title extraction based on <br>machine learn</br>ing consists of training and extraction.",
                "The same pre-processing step occurs before training and extraction.",
                "During pre-processing, from the top region of the first page of a Word document or the first slide of a PowerPoint document a number of units for processing are extracted.",
                "If a line (lines are separated by return symbols) only has a single format, then the line will become a unit.",
                "If a line has several parts and each of them has its own format, then each part will become a unit.",
                "Each unit will be treated as an instance in learning.",
                "A unit contains not only content information (linguistic information) but also formatting information.",
                "The input to pre-processing is a document and the output of pre-processing is a sequence of units (instances).",
                "Figure 5 shows the units obtained from the document in Figure 2.",
                "Figure 5.",
                "Example of units.",
                "In learning, the input is sequences of units where each sequence corresponds to a document.",
                "We take labeled units (labeled as title_begin, title_end, or other) in the sequences as training data and construct models for identifying whether a unit is title_begin title_end, or other.",
                "We employ four types of models: Perceptron, Maximum Entropy (ME), Perceptron Markov Model (PMM), and Maximum Entropy Markov Model (MEMM).",
                "In extraction, the input is a sequence of units from one document.",
                "We employ one type of model to identify whether a unit is title_begin, title_end, or other.",
                "We then extract units from the unit labeled with title_begin to the unit labeled with title_end.",
                "The result is the extracted title of the document.",
                "The unique characteristic of our approach is that we mainly utilize formatting information for title extraction.",
                "Our assumption is that although general documents vary in styles, their formats have certain patterns and we can learn and utilize the patterns for title extraction.",
                "This is in contrast to the work by Han et al., in which only linguistic features are used for extraction from research papers. 4.2 Models The four models actually can be considered in the same metadata extraction framework.",
                "That is why we apply them together to our current problem.",
                "Each input is a sequence of instances kxxx L21 together with a sequence of labels kyyy L21 . ix and iy represents an instance and its label, respectively ( ki ,,2,1 L= ).",
                "Recall that an instance here represents a unit.",
                "A label represents title_begin, title_end, or other.",
                "Here, k is the number of units in a document.",
                "In learning, we train a model which can be generally denoted as a conditional probability distribution )|( 11 kk XXYYP LL where iX and iY denote random variables taking instance ix and label iy as values, respectively ( ki ,,2,1 L= ).",
                "Learning Tool Extraction Tool 21121 2222122221 1121111211 nknnknn kk kk yyyxxx yyyxxx yyyxxx LL LL LL LL → → → )|(maxarg 11 mkmmkm xxyyP LL )|( 11 kk XXYYP LL Conditional Distribution mkmm xxx L21 Figure 6.",
                "Metadata extraction model.",
                "We can make assumptions about the general model in order to make it simple enough for training. 148 For example, we can assume that kYY ,,1 L are independent of each other given kXX ,,1 L .",
                "Thus, we have )|()|( )|( 11 11 kk kk XYPXYP XXYYP L LL = In this way, we decompose the model into a number of classifiers.",
                "We train the classifiers locally using the labeled data.",
                "As the classifier, we employ the Perceptron or Maximum Entropy model.",
                "We can also assume that the first order Markov property holds for kYY ,,1 L given kXX ,,1 L .",
                "Thus, we have )|()|( )|( 111 11 kkk kk XYYPXYP XXYYP −= L LL Again, we obtain a number of classifiers.",
                "However, the classifiers are conditioned on the previous label.",
                "When we employ the Percepton or Maximum Entropy model as a classifier, the models become a Percepton Markov Model or Maximum Entropy Markov Model, respectively.",
                "That is to say, the two models are more precise.",
                "In extraction, given a new sequence of instances, we resort to one of the constructed models to assign a sequence of labels to the sequence of instances, i.e., perform extraction.",
                "For Perceptron and ME, we assign labels locally and combine the results globally later using heuristics.",
                "Specifically, we first identify the most likely title_begin.",
                "Then we find the most likely title_end within three units after the title_begin.",
                "Finally, we extract as a title the units between the title_begin and the title_end.",
                "For PMM and MEMM, we employ the Viterbi algorithm to find the globally optimal label sequence.",
                "In this paper, for Perceptron, we actually employ an improved variant of it, called Perceptron with Uneven Margin [13].",
                "This version of Perceptron can work well especially when the number of positive instances and the number of negative instances differ greatly, which is exactly the case in our problem.",
                "We also employ an improved version of Perceptron Markov Model in which the Perceptron model is the so-called Voted Perceptron [2].",
                "In addition, in training, the parameters of the model are updated globally rather than locally. 4.3 Features There are two types of features: format features and linguistic features.",
                "We mainly use the former.",
                "The features are used for both the title-begin and the title-end classifiers. 4.3.1 Format Features Font Size: There are four binary features that represent the normalized font size of the unit (recall that a unit has only one type of font).",
                "If the font size of the unit is the largest in the document, then the first feature will be 1, otherwise 0.",
                "If the font size is the smallest in the document, then the fourth feature will be 1, otherwise 0.",
                "If the font size is above the average font size and not the largest in the document, then the second feature will be 1, otherwise 0.",
                "If the font size is below the average font size and not the smallest, the third feature will be 1, otherwise 0.",
                "It is necessary to conduct normalization on font sizes.",
                "For example, in one document the largest font size might be 12pt, while in another the smallest one might be 18pt.",
                "Boldface: This binary feature represents whether or not the current unit is in boldface.",
                "Alignment: There are four binary features that respectively represent the location of the current unit: left, center, right, and unknown alignment.",
                "The following format features with respect to context play an important role in title extraction.",
                "Empty Neighboring Unit: There are two binary features that represent, respectively, whether or not the previous unit and the current unit are blank lines.",
                "Font Size Change: There are two binary features that represent, respectively, whether or not the font size of the previous unit and the font size of the next unit differ from that of the current unit.",
                "Alignment Change: There are two binary features that represent, respectively, whether or not the alignment of the previous unit and the alignment of the next unit differ from that of the current one.",
                "Same Paragraph: There are two binary features that represent, respectively, whether or not the previous unit and the next unit are in the same paragraph as the current unit. 4.3.2 Linguistic Features The linguistic features are based on key words.",
                "Positive Word: This binary feature represents whether or not the current unit begins with one of the positive words.",
                "The positive words include title:, subject:, subject line: For example, in some documents the lines of titles and authors have the same formats.",
                "However, if lines begin with one of the positive words, then it is likely that they are title lines.",
                "Negative Word: This binary feature represents whether or not the current unit begins with one of the negative words.",
                "The negative words include To, By, created by, updated by, etc.",
                "There are more negative words than positive words.",
                "The above linguistic features are language dependent.",
                "Word Count: A title should not be too long.",
                "We heuristically create four intervals: [1, 2], [3, 6], [7, 9] and [9, ∞) and define one feature for each interval.",
                "If the number of words in a title falls into an interval, then the corresponding feature will be 1; otherwise 0.",
                "Ending Character: This feature represents whether the unit ends with :, -, or other special characters.",
                "A title usually does not end with such a character. 5.",
                "DOCUMENT RETRIEVAL METHOD We describe our method of document retrieval using extracted titles.",
                "Typically, in information retrieval a document is split into a number of fields including body, title, and anchor text.",
                "A ranking function in search can use different weights for different fields of 149 the document.",
                "Also, titles are typically assigned high weights, indicating that they are important for document retrieval.",
                "As explained previously, our experiment has shown that a significant number of documents actually have incorrect titles in the file properties, and thus in addition of using them we use the extracted titles as one more field of the document.",
                "By doing this, we attempt to improve the overall precision.",
                "In this paper, we employ a modification of BM25 that allows field weighting [21].",
                "As fields, we make use of body, title, extracted title and anchor.",
                "First, for each term in the query we count the term frequency in each field of the document; each field frequency is then weighted according to the corresponding weight parameter: ∑= f tfft tfwwtf Similarly, we compute the document length as a weighted sum of lengths of each field.",
                "Average document length in the corpus becomes the average of all weighted document lengths. ∑= f ff dlwwdl In our experiments we used 75.0,8.11 == bk .",
                "Weight for content was 1.0, title was 10.0, anchor was 10.0, and extracted title was 5.0. 6.",
                "EXPERIMENTAL RESULTS 6.1 Data Sets and Evaluation Measures We used two data sets in our experiments.",
                "First, we downloaded and randomly selected 5,000 Word documents and 5,000 PowerPoint documents from an intranet of Microsoft.",
                "We call it MS hereafter.",
                "Second, we downloaded and randomly selected 500 Word and 500 PowerPoint documents from the DotGov and DotCom domains on the internet, respectively.",
                "Figure 7 shows the distributions of the genres of the documents.",
                "We see that the documents are indeed general documents as we define them.",
                "Figure 7.",
                "Distributions of document genres.",
                "Third, a data set in Chinese was also downloaded from the internet.",
                "It includes 500 Word documents and 500 PowerPoint documents in Chinese.",
                "We manually labeled the titles of all the documents, on the basis of our specification.",
                "Not all the documents in the two data sets have titles.",
                "Table 1 shows the percentages of the documents having titles.",
                "We see that DotCom and DotGov have more PowerPoint documents with titles than MS.",
                "This might be because PowerPoint documents published on the internet are more formal than those on the intranet.",
                "Table 1.",
                "The portion of documents with titles Domain Type MS DotCom DotGov Word 75.7% 77.8% 75.6% PowerPoint 82.1% 93.4% 96.4% In our experiments, we conducted evaluations on title extraction in terms of precision, recall, and F-measure.",
                "The evaluation measures are defined as follows: Precision: P = A / ( A + B ) Recall: R = A / ( A + C ) F-measure: F1 = 2PR / ( P + R ) Here, A, B, C, and D are numbers of documents as those defined in Table 2.",
                "Table 2.",
                "Contingence table with regard to title extraction Is title Is not title Extracted A B Not extracted C D 6.2 Baselines We test the accuracies of the two baselines described in section 4.2.",
                "They are denoted as largest font size and first line respectively. 6.3 Accuracy of Titles in File Properties We investigate how many titles in the file properties of the documents are reliable.",
                "We view the titles annotated by humans as true titles and test how many titles in the file properties can approximately match with the true titles.",
                "We use Edit Distance to conduct the approximate match. (Approximate match is only used in this evaluation).",
                "This is because sometimes human annotated titles can be slightly different from the titles in file properties on the surface, e.g., contain extra spaces).",
                "Given string A and string B: if ( (D == 0) or ( D / ( La + Lb ) < θ ) ) then string A = string B D: Edit Distance between string A and string B La: length of string A Lb: length of string B θ: 0.1 ∑ × ++− + = t t n N wtf avwdl wdl bbk kwtf FBM )log( ))1(( )1( 25 1 1 150 Table 3.",
                "Accuracies of titles in file properties File Type Domain Precision Recall F1 MS 0.299 0.311 0.305 DotCom 0.210 0.214 0.212Word DotGov 0.182 0.177 0.180 MS 0.229 0.245 0.237 DotCom 0.185 0.186 0.186PowerPoint DotGov 0.180 0.182 0.181 6.4 Comparison with Baselines We conducted title extraction from the first data set (Word and PowerPoint in MS).",
                "As the model, we used Perceptron.",
                "We conduct 4-fold cross validation.",
                "Thus, all the results reported here are those averaged over 4 trials.",
                "Tables 4 and 5 show the results.",
                "We see that Perceptron significantly outperforms the baselines.",
                "In the evaluation, we use exact matching between the true titles annotated by humans and the extracted titles.",
                "Table 4.",
                "Accuracies of title extraction with Word Precision Recall F1 Model Perceptron 0.810 0.837 0.823 Largest font size 0.700 0.758 0.727 Baselines First line 0.707 0.767 0.736 Table 5.",
                "Accuracies of title extraction with PowerPoint Precision Recall F1 Model Perceptron 0.875 0. 895 0.885 Largest font size 0.844 0.887 0.865 Baselines First line 0.639 0.671 0.655 We see that the <br>machine learn</br>ing approach can achieve good performance in title extraction.",
                "For Word documents both precision and recall of the approach are 8 percent higher than those of the baselines.",
                "For PowerPoint both precision and recall of the approach are 2 percent higher than those of the baselines.",
                "We conduct significance tests.",
                "The results are shown in Table 6.",
                "Here, Largest denotes the baseline of using the largest font size, First denotes the baseline of using the first line.",
                "The results indicate that the improvements of <br>machine learn</br>ing over baselines are statistically significant (in the sense p-value < 0.05) Table 6.",
                "Sign test results Documents Type Sign test between p-value Perceptron vs. Largest 3.59e-26 Word Perceptron vs. First 7.12e-10 Perceptron vs. Largest 0.010 PowerPoint Perceptron vs. First 5.13e-40 We see, from the results, that the two baselines can work well for title extraction, suggesting that font size and position information are most useful features for title extraction.",
                "However, it is also obvious that using only these two features is not enough.",
                "There are cases in which all the lines have the same font size (i.e., the largest font size), or cases in which the lines with the largest font size only contain general descriptions like Confidential, White paper, etc.",
                "For those cases, the largest font size method cannot work well.",
                "For similar reasons, the first line method alone cannot work well, either.",
                "With the combination of different features (evidence in title judgment), Perceptron can outperform Largest and First.",
                "We investigate the performance of solely using linguistic features.",
                "We found that it does not work well.",
                "It seems that the format features play important roles and the linguistic features are supplements..",
                "Figure 8.",
                "An example Word document.",
                "Figure 9.",
                "An example PowerPoint document.",
                "We conducted an error analysis on the results of Perceptron.",
                "We found that the errors fell into three categories. (1) About one third of the errors were related to hard cases.",
                "In these documents, the layouts of the first pages were difficult to understand, even for humans.",
                "Figure 8 and 9 shows examples. (2) Nearly one fourth of the errors were from the documents which do not have true titles but only contain bullets.",
                "Since we conduct extraction from the top regions, it is difficult to get rid of these errors with the current approach. (3).",
                "Confusions between main titles and subtitles were another type of error.",
                "Since we only labeled the main titles as titles, the extractions of both titles were considered incorrect.",
                "This type of error does little harm to document processing like search, however. 6.5 Comparison between Models To compare the performance of different <br>machine learn</br>ing models, we conducted another experiment.",
                "Again, we perform 4-fold cross 151 validation on the first data set (MS).",
                "Table 7, 8 shows the results of all the four models.",
                "It turns out that Perceptron and PMM perform the best, followed by MEMM, and ME performs the worst.",
                "In general, the Markovian models perform better than or as well as their classifier counterparts.",
                "This seems to be because the Markovian models are trained globally, while the classifiers are trained locally.",
                "The Perceptron based models perform better than the ME based counterparts.",
                "This seems to be because the Perceptron based models are created to make better classifications, while ME models are constructed for better prediction.",
                "Table 7.",
                "Comparison between different learning models for title extraction with Word Model Precision Recall F1 Perceptron 0.810 0.837 0.823 MEMM 0.797 0.824 0.810 PMM 0.827 0.823 0.825 ME 0.801 0.621 0.699 Table 8.",
                "Comparison between different learning models for title extraction with PowerPoint Model Precision Recall F1 Perceptron 0.875 0. 895 0. 885 MEMM 0.841 0.861 0.851 PMM 0.873 0.896 0.885 ME 0.753 0.766 0.759 6.6 Domain Adaptation We apply the model trained with the first data set (MS) to the second data set (DotCom and DotGov).",
                "Tables 9-12 show the results.",
                "Table 9.",
                "Accuracies of title extraction with Word in DotGov Precision Recall F1 Model Perceptron 0.716 0.759 0.737 Largest font size 0.549 0.619 0.582Baselines First line 0.462 0.521 0.490 Table 10.",
                "Accuracies of title extraction with PowerPoint in DotGov Precision Recall F1 Model Perceptron 0.900 0.906 0.903 Largest font size 0.871 0.888 0.879Baselines First line 0.554 0.564 0.559 Table 11.",
                "Accuracies of title extraction with Word in DotCom Precisio n Recall F1 Model Perceptron 0.832 0.880 0.855 Largest font size 0.676 0.753 0.712Baselines First line 0.577 0.643 0.608 Table 12.",
                "Performance of PowerPoint document title extraction in DotCom Precisio n Recall F1 Model Perceptron 0.910 0.903 0.907 Largest font size 0.864 0.886 0.875Baselines First line 0.570 0.585 0.577 From the results, we see that the models can be adapted to different domains well.",
                "There is almost no drop in accuracy.",
                "The results indicate that the patterns of title formats exist across different domains, and it is possible to construct a domain independent model by mainly using formatting information. 6.7 Language Adaptation We apply the model trained with the data in English (MS) to the data set in Chinese.",
                "Tables 13-14 show the results.",
                "Table 13.",
                "Accuracies of title extraction with Word in Chinese Precision Recall F1 Model Perceptron 0.817 0.805 0.811 Largest font size 0.722 0.755 0.738Baselines First line 0.743 0.777 0.760 Table 14.",
                "Accuracies of title extraction with PowerPoint in Chinese Precision Recall F1 Model Perceptron 0.766 0.812 0.789 Largest font size 0.753 0.813 0.782Baselines First line 0.627 0.676 0.650 We see that the models can be adapted to a different language.",
                "There are only small drops in accuracy.",
                "Obviously, the linguistic features do not work for Chinese, but the effect of not using them is negligible.",
                "The results indicate that the patterns of title formats exist across different languages.",
                "From the domain adaptation and language adaptation results, we conclude that the use of formatting information is the key to a successful extraction from general documents. 6.8 Search with Extracted Titles We performed experiments on using title extraction for document retrieval.",
                "As a baseline, we employed BM25 without using extracted titles.",
                "The ranking mechanism was as described in Section 5.",
                "The weights were heuristically set.",
                "We did not conduct optimization on the weights.",
                "The evaluation was conducted on a corpus of 1.3 M documents crawled from the intranet of Microsoft using 100 evaluation queries obtained from this intranets search engine query logs. 50 queries were from the most popular set, while 50 queries other were chosen randomly.",
                "Users were asked to provide judgments of the degree of document relevance from a scale of 1to 5 (1 meaning detrimental, 2 - bad, 3 - fair, 4 - good and 5 - excellent). 152 Figure 10 shows the results.",
                "In the chart two sets of precision results were obtained by either considering good or excellent documents as relevant (left 3 bars with relevance threshold 0.5), or by considering only excellent documents as relevant (right 3 bars with relevance threshold 1.0) 0 0.05 0.1 0.15 0.2 0.25 0.3 0.35 0.4 0.45 P@10 P@5 Reciprocal P@10 P@5 Reciprocal 0.5 1 BM25 Anchor, Title, Body BM25 Anchor, Title, Body, ExtractedTitle Name All RelevanceThreshold Data Description Figure 10.",
                "Search ranking results.",
                "Figure 10 shows different document retrieval results with different ranking functions in terms of precision @10, precision @5 and reciprocal rank: • Blue bar - BM25 including the fields body, title (file property), and anchor text. • Purple bar - BM25 including the fields body, title (file property), anchor text, and extracted title.",
                "With the additional field of extracted title included in BM25 the precision @10 increased from 0.132 to 0.145, or by ~10%.",
                "Thus, it is safe to say that the use of extracted title can indeed improve the precision of document retrieval. 7.",
                "CONCLUSION In this paper, we have investigated the problem of automatically extracting titles from general documents.",
                "We have tried using a <br>machine learn</br>ing approach to address the problem.",
                "Previous work showed that the <br>machine learn</br>ing approach can work well for metadata extraction from research papers.",
                "In this paper, we showed that the approach can work for extraction from general documents as well.",
                "Our experimental results indicated that the <br>machine learn</br>ing approach can work significantly better than the baselines in title extraction from Office documents.",
                "Previous work on metadata extraction mainly used linguistic features in documents, while we mainly used formatting information.",
                "It appeared that using formatting information is a key for successfully conducting title extraction from general documents.",
                "We tried different <br>machine learn</br>ing models including Perceptron, Maximum Entropy, Maximum Entropy Markov Model, and Voted Perceptron.",
                "We found that the performance of the Perceptorn models was the best.",
                "We applied models constructed in one domain to another domain and applied models trained in one language to another language.",
                "We found that the accuracies did not drop substantially across different domains and across different languages, indicating that the models were generic.",
                "We also attempted to use the extracted titles in document retrieval.",
                "We observed a significant improvement in document ranking performance for search when using extracted title information.",
                "All the above investigations were not conducted in previous work, and through our investigations we verified the generality and the significance of the title extraction approach. 8.",
                "ACKNOWLEDGEMENTS We thank Chunyu Wei and Bojuan Zhao for their work on data annotation.",
                "We acknowledge Jinzhu Li for his assistance in conducting the experiments.",
                "We thank Ming Zhou, John Chen, Jun Xu, and the anonymous reviewers of JCDL05 for their valuable comments on this paper. 9.",
                "REFERENCES [1] Berger, A. L., Della Pietra, S. A., and Della Pietra, V. J.",
                "A maximum entropy approach to natural language processing.",
                "Computational Linguistics, 22:39-71, 1996. [2] Collins, M. Discriminative training methods for hidden markov models: theory and experiments with perceptron algorithms.",
                "In Proceedings of Conference on Empirical Methods in Natural Language Processing, 1-8, 2002. [3] Cortes, C. and Vapnik, V. Support-vector networks.",
                "Machine Learning, 20:273-297, 1995. [4] Chieu, H. L. and Ng, H. T. A maximum entropy approach to information extraction from semi-structured and free text.",
                "In Proceedings of the Eighteenth National Conference on Artificial Intelligence, 768-791, 2002. [5] Evans, D. K., Klavans, J. L., and McKeown, K. R. Columbia newsblaster: multilingual news summarization on the Web.",
                "In Proceedings of Human Language Technology conference / North American chapter of the Association for Computational Linguistics annual meeting, 1-4, 2004. [6] Ghahramani, Z. and Jordan, M. I. Factorial hidden markov models.",
                "Machine Learning, 29:245-273, 1997. [7] Gheel, J. and Anderson, T. Data and metadata for finding and reminding, In Proceedings of the 1999 International Conference on Information Visualization, 446-451,1999. [8] Giles, C. L., Petinot, Y., Teregowda P. B., Han, H., Lawrence, S., Rangaswamy, A., and Pal, N. eBizSearch: a niche search engine for e-Business.",
                "In Proceedings of the 26th Annual International ACM SIGIR Conference on Research and Development in Information Retrieval, 413414, 2003. [9] Giuffrida, G., Shek, E. C., and Yang, J. Knowledge-based metadata extraction from PostScript files.",
                "In Proceedings of the Fifth ACM Conference on Digital Libraries, 77-84, 2000. [10] Han, H., Giles, C. L., Manavoglu, E., Zha, H., Zhang, Z., and Fox, E. A.",
                "Automatic document metadata extraction using support vector machines.",
                "In Proceedings of the Third ACM/IEEE-CS Joint Conference on Digital Libraries, 37-48, 2003. [11] Kobayashi, M., and Takeda, K. Information retrieval on the Web.",
                "ACM Computing Surveys, 32:144-173, 2000. [12] Lafferty, J., McCallum, A., and Pereira, F. Conditional random fields: probabilistic models for segmenting and 153 labeling sequence data.",
                "In Proceedings of the Eighteenth International Conference on Machine Learning, 282-289, 2001. [13] Li, Y., Zaragoza, H., Herbrich, R., Shawe-Taylor J., and Kandola, J. S. The perceptron algorithm with uneven margins.",
                "In Proceedings of the Nineteenth International Conference on Machine Learning, 379-386, 2002. [14] Liddy, E. D., Sutton, S., Allen, E., Harwell, S., Corieri, S., Yilmazel, O., Ozgencil, N. E., Diekema, A., McCracken, N., and Silverstein, J.",
                "Automatic Metadata generation & evaluation.",
                "In Proceedings of the 25th Annual International ACM SIGIR Conference on Research and Development in Information Retrieval, 401-402, 2002. [15] Littlefield, A.",
                "Effective enterprise information retrieval across new content formats.",
                "In Proceedings of the Seventh Search Engine Conference, http://www.infonortics.com/searchengines/sh02/02prog.html, 2002. [16] Mao, S., Kim, J. W., and Thoma, G. R. A dynamic feature generation system for automated metadata extraction in preservation of digital materials.",
                "In Proceedings of the First International Workshop on Document Image Analysis for Libraries, 225-232, 2004. [17] McCallum, A., Freitag, D., and Pereira, F. Maximum entropy markov models for information extraction and segmentation.",
                "In Proceedings of the Seventeenth International Conference on Machine Learning, 591-598, 2000. [18] Murphy, L. D. Digital document metadata in organizations: roles, analytical approaches, and future research directions.",
                "In Proceedings of the Thirty-First Annual Hawaii International Conference on System Sciences, 267-276, 1998. [19] Pinto, D., McCallum, A., Wei, X., and Croft, W. B.",
                "Table extraction using conditional random fields.",
                "In Proceedings of the 26th Annual International ACM SIGIR Conference on Research and Development in Information Retrieval, 235242, 2003. [20] Ratnaparkhi, A. Unsupervised statistical models for prepositional phrase attachment.",
                "In Proceedings of the Seventeenth International Conference on Computational Linguistics. 1079-1085, 1998. [21] Robertson, S., Zaragoza, H., and Taylor, M. Simple BM25 extension to multiple weighted fields, In Proceedings of ACM Thirteenth Conference on Information and Knowledge Management, 42-49, 2004. [22] Yi, J. and Sundaresan, N. Metadata based Web mining for relevance, In Proceedings of the 2000 International Symposium on Database Engineering & Applications, 113121, 2000. [23] Yilmazel, O., Finneran, C. M., and Liddy, E. D. MetaExtract: An NLP system to automatically assign metadata.",
                "In Proceedings of the 2004 Joint ACM/IEEE Conference on Digital Libraries, 241-242, 2004. [24] Zhang, J. and Dimitroff, A. Internet search engines response to metadata Dublin Core implementation.",
                "Journal of Information Science, 30:310-320, 2004. [25] Zhang, L., Pan, Y., and Zhang, T. Recognising and using named entities: focused named entity recognition using <br>machine learn</br>ing.",
                "In Proceedings of the 27th Annual International ACM SIGIR Conference on Research and Development in Information Retrieval, 281-288, 2004. [26] http://dublincore.org/groups/corporate/Seattle/ 154"
            ],
            "original_annotated_samples": [],
            "translated_annotated_samples": [
                "49 Zhichun Road, Haidian, Beijing, China, 100080 {Hangli, Yucao}@Microsoft.com Qinghua de Departamento de Ciencias de la Computación Xian Jiaotong No.Microsoft Corporation One Microsoft Way Redmond, WA, EE. UU., 98052 dmitriym@microsoft.com Resumen En este documento, proponemos un enfoque de \"aprendizaje de máquina\" para la extracción de títulos de documentos generales.",
                "En nuestro enfoque, anotamos títulos en documentos de muestra (para Word y PowerPoint respectivamente) y los tomamos como datos de entrenamiento, entrenar modelos de \"Aprendizaje de máquinas\" y realizar la extracción de títulos utilizando los modelos entrenados.",
                "Por ejemplo, Han et al.[10] propuso un método basado en la \"máquina aprendiendo\" para realizar la extracción de los trabajos de investigación.",
                "No se ha aclarado si un enfoque basado en la \"máquina para aprender\" puede funcionar bien para esta tarea.",
                "Tomamos un enfoque de \"Learning\" Ing.",
                "Los métodos propuestos se dividen en dos categorías: el enfoque basado en reglas y el enfoque basado en la \"máquina aprendiendo\".",
                "Es menos adaptativo y robusto en comparación con el enfoque de \"Aprendizaje de la máquina\".",
                "Han et al.[10], por ejemplo, realizó la extracción de metadatos con el enfoque de \"Learning\" de la máquina \".",
                "Método de extracción de título 4.1 Extracción de títulos basada en \"Learning Machine\" que consiste en capacitación y extracción.",
                "Precisiones de extracción del título con precisión PowerPoint Recuerdo del modelo F1 Perceptron 0.875 0. 895 0.885 Tamaño de fuente más grande 0.844 0.887 0.865 líneas de base Primera línea 0.639 0.671 0.655 Vemos que el enfoque de \"Aprendizaje de la máquina\" puede lograr un buen rendimiento en la extracción del título."
            ],
            "translated_text": "",
            "candidates": [
                "Aprender a la máquina",
                "aprendizaje de máquina",
                "Aprender a la máquina",
                "Aprendizaje de máquinas",
                "Aprender a la máquina",
                "máquina aprendiendo",
                "Aprender a la máquina",
                "máquina para aprender",
                "Aprender a la máquina",
                "Learning",
                "Aprender a la máquina",
                "máquina aprendiendo",
                "Aprender a la máquina",
                "Aprendizaje de la máquina",
                "Aprender a la máquina",
                "Learning",
                "Aprender a la máquina",
                "Learning Machine",
                "Aprender a la máquina",
                "Aprendizaje de la máquina"
            ],
            "error": []
        },
        "search": {
            "translated_key": "búsqueda",
            "is_in_text": true,
            "original_annotated_sentences": [
                "Automatic Extraction of Titles from General Documents using Machine Learning Yunhua Hu1 Computer Science Department Xian Jiaotong University No 28, Xianning West Road Xian, China, 710049 yunhuahu@mail.xjtu.edu.cn Hang Li, Yunbo Cao Microsoft Research Asia 5F Sigma Center, No.",
                "49 Zhichun Road, Haidian, Beijing, China, 100080 {hangli,yucao}@microsoft.com Qinghua Zheng Computer Science Department Xian Jiaotong University No 28, Xianning West Road Xian, China, 710049 qhzheng@mail.xjtu.edu.cn Dmitriy Meyerzon Microsoft Corporation One Microsoft Way Redmond, WA, USA, 98052 dmitriym@microsoft.com ABSTRACT In this paper, we propose a machine learning approach to title extraction from general documents.",
                "By general documents, we mean documents that can belong to any one of a number of specific genres, including presentations, book chapters, technical papers, brochures, reports, and letters.",
                "Previously, methods have been proposed mainly for title extraction from research papers.",
                "It has not been clear whether it could be possible to conduct automatic title extraction from general documents.",
                "As a case study, we consider extraction from Office including Word and PowerPoint.",
                "In our approach, we annotate titles in sample documents (for Word and PowerPoint respectively) and take them as training data, train machine learning models, and perform title extraction using the trained models.",
                "Our method is unique in that we mainly utilize formatting information such as font size as features in the models.",
                "It turns out that the use of formatting information can lead to quite accurate extraction from general documents.",
                "Precision and recall for title extraction from Word is 0.810 and 0.837 respectively, and precision and recall for title extraction from PowerPoint is 0.875 and 0.895 respectively in an experiment on intranet data.",
                "Other important new findings in this work include that we can train models in one domain and apply them to another domain, and more surprisingly we can even train models in one language and apply them to another language.",
                "Moreover, we can significantly improve <br>search</br> ranking results in document retrieval by using the extracted titles.",
                "Categories and Subject Descriptors H.3.3 [Information Storage and Retrieval]: Information <br>search</br> and Retrieval - <br>search</br> Process; H.4.1 [Information Systems Applications]: Office Automation - Word processing; D.2.8 [Software Engineering]: Metrics - complexity measures, performance measures General Terms Algorithms, Experimentation, Performance. 1.",
                "INTRODUCTION Metadata of documents is useful for many kinds of document processing such as <br>search</br>, browsing, and filtering.",
                "Ideally, metadata is defined by the authors of documents and is then used by various systems.",
                "However, people seldom define document metadata by themselves, even when they have convenient metadata definition tools [26].",
                "Thus, how to automatically extract metadata from the bodies of documents turns out to be an important research issue.",
                "Methods for performing the task have been proposed.",
                "However, the focus was mainly on extraction from research papers.",
                "For instance, Han et al. [10] proposed a machine learning based method to conduct extraction from research papers.",
                "They formalized the problem as that of classification and employed Support Vector Machines as the classifier.",
                "They mainly used linguistic features in the model.1 In this paper, we consider metadata extraction from general documents.",
                "By general documents, we mean documents that may belong to any one of a number of specific genres.",
                "General documents are more widely available in digital libraries, intranets and the internet, and thus investigation on extraction from them is sorely needed.",
                "Research papers usually have well-formed styles and noticeable characteristics.",
                "In contrast, the styles of general documents can vary greatly.",
                "It has not been clarified whether a machine learning based approach can work well for this task.",
                "There are many types of metadata: title, author, date of creation, etc.",
                "As a case study, we consider title extraction in this paper.",
                "General documents can be in many different file formats: Microsoft Office, PDF (PS), etc.",
                "As a case study, we consider extraction from Office including Word and PowerPoint.",
                "We take a machine learning approach.",
                "We annotate titles in sample documents (for Word and PowerPoint respectively) and take them as training data to train several types of models, and perform title extraction using any one type of the trained models.",
                "In the models, we mainly utilize formatting information such as font size as features.",
                "We employ the following models: Maximum Entropy Model, Perceptron with Uneven Margins, Maximum Entropy Markov Model, and Voted Perceptron.",
                "In this paper, we also investigate the following three problems, which did not seem to have been examined previously. (1) Comparison between models: among the models above, which model performs best for title extraction; (2) Generality of model: whether it is possible to train a model on one domain and apply it to another domain, and whether it is possible to train a model in one language and apply it to another language; (3) Usefulness of extracted titles: whether extracted titles can improve document processing such as <br>search</br>.",
                "Experimental results indicate that our approach works well for title extraction from general documents.",
                "Our method can significantly outperform the baselines: one that always uses the first lines as titles and the other that always uses the lines in the largest font sizes as titles.",
                "Precision and recall for title extraction from Word are 0.810 and 0.837 respectively, and precision and recall for title extraction from PowerPoint are 0.875 and 0.895 respectively.",
                "It turns out that the use of format features is the key to successful title extraction. (1) We have observed that Perceptron based models perform better in terms of extraction accuracies. (2) We have empirically verified that the models trained with our approach are generic in the sense that they can be trained on one domain and applied to another, and they can be trained in one language and applied to another. (3) We have found that using the extracted titles we can significantly improve precision of document retrieval (by 10%).",
                "We conclude that we can indeed conduct reliable title extraction from general documents and use the extracted results to improve real applications.",
                "The rest of the paper is organized as follows.",
                "In section 2, we introduce related work, and in section 3, we explain the motivation and problem setting of our work.",
                "In section 4, we describe our method of title extraction, and in section 5, we describe our method of document retrieval using extracted titles.",
                "Section 6 gives our experimental results.",
                "We make concluding remarks in section 7. 2.",
                "RELATED WORK 2.1 Document Metadata Extraction Methods have been proposed for performing automatic metadata extraction from documents; however, the main focus was on extraction from research papers.",
                "The proposed methods fall into two categories: the rule based approach and the machine learning based approach.",
                "Giuffrida et al. [9], for instance, developed a rule-based system for automatically extracting metadata from research papers in Postscript.",
                "They used rules like titles are usually located on the upper portions of the first pages and they are usually in the largest font sizes.",
                "Liddy et al. [14] and Yilmazel el al. [23] performed metadata extraction from educational materials using rule-based natural language processing technologies.",
                "Mao et al. [16] also conducted automatic metadata extraction from research papers using rules on formatting information.",
                "The rule-based approach can achieve high performance.",
                "However, it also has disadvantages.",
                "It is less adaptive and robust when compared with the machine learning approach.",
                "Han et al. [10], for instance, conducted metadata extraction with the machine learning approach.",
                "They viewed the problem as that of classifying the lines in a document into the categories of metadata and proposed using Support Vector Machines as the classifier.",
                "They mainly used linguistic information as features.",
                "They reported high extraction accuracy from research papers in terms of precision and recall. 2.2 Information Extraction Metadata extraction can be viewed as an application of information extraction, in which given a sequence of instances, we identify a subsequence that represents information in which we are interested.",
                "Hidden Markov Model [6], Maximum Entropy Model [1, 4], Maximum Entropy Markov Model [17], Support Vector Machines [3], Conditional Random Field [12], and Voted Perceptron [2] are widely used information extraction models.",
                "Information extraction has been applied, for instance, to part-ofspeech tagging [20], named entity recognition [25] and table extraction [19]. 2.3 <br>search</br> Using Title Information Title information is useful for document retrieval.",
                "In the system Citeseer, for instance, Giles et al. managed to extract titles from research papers and make use of the extracted titles in metadata <br>search</br> of papers [8].",
                "In web <br>search</br>, the title fields (i.e., file properties) and anchor texts of web pages (HTML documents) can be viewed as titles of the pages [5].",
                "Many <br>search</br> engines seem to utilize them for web page retrieval [7, 11, 18, 22].",
                "Zhang et al., found that web pages with well-defined metadata are more easily retrieved than those without well-defined metadata [24].",
                "To the best of our knowledge, no research has been conducted on using extracted titles from general documents (e.g., Office documents) for <br>search</br> of the documents. 146 3.",
                "MOTIVATION AND PROBLEM SETTING We consider the issue of automatically extracting titles from general documents.",
                "By general documents, we mean documents that belong to one of any number of specific genres.",
                "The documents can be presentations, books, book chapters, technical papers, brochures, reports, memos, specifications, letters, announcements, or resumes.",
                "General documents are more widely available in digital libraries, intranets, and internet, and thus investigation on title extraction from them is sorely needed.",
                "Figure 1 shows an estimate on distributions of file formats on intranet and internet [15].",
                "Office and PDF are the main file formats on the intranet.",
                "Even on the internet, the documents in the formats are still not negligible, given its extremely large size.",
                "In this paper, without loss of generality, we take Office documents as an example.",
                "Figure 1.",
                "Distributions of file formats in internet and intranet.",
                "For Office documents, users can define titles as file properties using a feature provided by Office.",
                "We found in an experiment, however, that users seldom use the feature and thus titles in file properties are usually very inaccurate.",
                "That is to say, titles in file properties are usually inconsistent with the true titles in the file bodies that are created by the authors and are visible to readers.",
                "We collected 6,000 Word and 6,000 PowerPoint documents from an intranet and the internet and examined how many titles in the file properties are correct.",
                "We found that surprisingly the accuracy was only 0.265 (cf., Section 6.3 for details).",
                "A number of reasons can be considered.",
                "For example, if one creates a new file by copying an old file, then the file property of the new file will also be copied from the old file.",
                "In another experiment, we found that Google uses the titles in file properties of Office documents in <br>search</br> and browsing, but the titles are not very accurate.",
                "We created 50 queries to <br>search</br> Word and PowerPoint documents and examined the top 15 results of each query returned by Google.",
                "We found that nearly all the titles presented in the <br>search</br> results were from the file properties of the documents.",
                "However, only 0.272 of them were correct.",
                "Actually, true titles usually exist at the beginnings of the bodies of documents.",
                "If we can accurately extract the titles from the bodies of documents, then we can exploit reliable title information in document processing.",
                "This is exactly the problem we address in this paper.",
                "More specifically, given a Word document, we are to extract the title from the top region of the first page.",
                "Given a PowerPoint document, we are to extract the title from the first slide.",
                "A title sometimes consists of a main title and one or two subtitles.",
                "We only consider extraction of the main title.",
                "As baselines for title extraction, we use that of always using the first lines as titles and that of always using the lines with largest font sizes as titles.",
                "Figure 2.",
                "Title extraction from Word document.",
                "Figure 3.",
                "Title extraction from PowerPoint document.",
                "Next, we define a specification for human judgments in title data annotation.",
                "The annotated data will be used in training and testing of the title extraction methods.",
                "Summary of the specification: The title of a document should be identified on the basis of common sense, if there is no difficulty in the identification.",
                "However, there are many cases in which the identification is not easy.",
                "There are some rules defined in the specification that guide identification for such cases.",
                "The rules include a title is usually in consecutive lines in the same format, a document can have no title, titles in images are not considered, a title should not contain words like draft, 147 whitepaper, etc, if it is difficult to determine which is the title, select the one in the largest font size, and if it is still difficult to determine which is the title, select the first candidate. (The specification covers all the cases we have encountered in data annotation.)",
                "Figures 2 and 3 show examples of Office documents from which we conduct title extraction.",
                "In Figure 2, Differences in Win32 API Implementations among Windows Operating Systems is the title of the Word document.",
                "Microsoft Windows on the top of this page is a picture and thus is ignored.",
                "In Figure 3, Building Competitive Advantages through an Agile Infrastructure is the title of the PowerPoint document.",
                "We have developed a tool for annotation of titles by human annotators.",
                "Figure 4 shows a snapshot of the tool.",
                "Figure 4.",
                "Title annotation tool. 4.",
                "TITLE EXTRACTION METHOD 4.1 Outline Title extraction based on machine learning consists of training and extraction.",
                "The same pre-processing step occurs before training and extraction.",
                "During pre-processing, from the top region of the first page of a Word document or the first slide of a PowerPoint document a number of units for processing are extracted.",
                "If a line (lines are separated by return symbols) only has a single format, then the line will become a unit.",
                "If a line has several parts and each of them has its own format, then each part will become a unit.",
                "Each unit will be treated as an instance in learning.",
                "A unit contains not only content information (linguistic information) but also formatting information.",
                "The input to pre-processing is a document and the output of pre-processing is a sequence of units (instances).",
                "Figure 5 shows the units obtained from the document in Figure 2.",
                "Figure 5.",
                "Example of units.",
                "In learning, the input is sequences of units where each sequence corresponds to a document.",
                "We take labeled units (labeled as title_begin, title_end, or other) in the sequences as training data and construct models for identifying whether a unit is title_begin title_end, or other.",
                "We employ four types of models: Perceptron, Maximum Entropy (ME), Perceptron Markov Model (PMM), and Maximum Entropy Markov Model (MEMM).",
                "In extraction, the input is a sequence of units from one document.",
                "We employ one type of model to identify whether a unit is title_begin, title_end, or other.",
                "We then extract units from the unit labeled with title_begin to the unit labeled with title_end.",
                "The result is the extracted title of the document.",
                "The unique characteristic of our approach is that we mainly utilize formatting information for title extraction.",
                "Our assumption is that although general documents vary in styles, their formats have certain patterns and we can learn and utilize the patterns for title extraction.",
                "This is in contrast to the work by Han et al., in which only linguistic features are used for extraction from research papers. 4.2 Models The four models actually can be considered in the same metadata extraction framework.",
                "That is why we apply them together to our current problem.",
                "Each input is a sequence of instances kxxx L21 together with a sequence of labels kyyy L21 . ix and iy represents an instance and its label, respectively ( ki ,,2,1 L= ).",
                "Recall that an instance here represents a unit.",
                "A label represents title_begin, title_end, or other.",
                "Here, k is the number of units in a document.",
                "In learning, we train a model which can be generally denoted as a conditional probability distribution )|( 11 kk XXYYP LL where iX and iY denote random variables taking instance ix and label iy as values, respectively ( ki ,,2,1 L= ).",
                "Learning Tool Extraction Tool 21121 2222122221 1121111211 nknnknn kk kk yyyxxx yyyxxx yyyxxx LL LL LL LL → → → )|(maxarg 11 mkmmkm xxyyP LL )|( 11 kk XXYYP LL Conditional Distribution mkmm xxx L21 Figure 6.",
                "Metadata extraction model.",
                "We can make assumptions about the general model in order to make it simple enough for training. 148 For example, we can assume that kYY ,,1 L are independent of each other given kXX ,,1 L .",
                "Thus, we have )|()|( )|( 11 11 kk kk XYPXYP XXYYP L LL = In this way, we decompose the model into a number of classifiers.",
                "We train the classifiers locally using the labeled data.",
                "As the classifier, we employ the Perceptron or Maximum Entropy model.",
                "We can also assume that the first order Markov property holds for kYY ,,1 L given kXX ,,1 L .",
                "Thus, we have )|()|( )|( 111 11 kkk kk XYYPXYP XXYYP −= L LL Again, we obtain a number of classifiers.",
                "However, the classifiers are conditioned on the previous label.",
                "When we employ the Percepton or Maximum Entropy model as a classifier, the models become a Percepton Markov Model or Maximum Entropy Markov Model, respectively.",
                "That is to say, the two models are more precise.",
                "In extraction, given a new sequence of instances, we resort to one of the constructed models to assign a sequence of labels to the sequence of instances, i.e., perform extraction.",
                "For Perceptron and ME, we assign labels locally and combine the results globally later using heuristics.",
                "Specifically, we first identify the most likely title_begin.",
                "Then we find the most likely title_end within three units after the title_begin.",
                "Finally, we extract as a title the units between the title_begin and the title_end.",
                "For PMM and MEMM, we employ the Viterbi algorithm to find the globally optimal label sequence.",
                "In this paper, for Perceptron, we actually employ an improved variant of it, called Perceptron with Uneven Margin [13].",
                "This version of Perceptron can work well especially when the number of positive instances and the number of negative instances differ greatly, which is exactly the case in our problem.",
                "We also employ an improved version of Perceptron Markov Model in which the Perceptron model is the so-called Voted Perceptron [2].",
                "In addition, in training, the parameters of the model are updated globally rather than locally. 4.3 Features There are two types of features: format features and linguistic features.",
                "We mainly use the former.",
                "The features are used for both the title-begin and the title-end classifiers. 4.3.1 Format Features Font Size: There are four binary features that represent the normalized font size of the unit (recall that a unit has only one type of font).",
                "If the font size of the unit is the largest in the document, then the first feature will be 1, otherwise 0.",
                "If the font size is the smallest in the document, then the fourth feature will be 1, otherwise 0.",
                "If the font size is above the average font size and not the largest in the document, then the second feature will be 1, otherwise 0.",
                "If the font size is below the average font size and not the smallest, the third feature will be 1, otherwise 0.",
                "It is necessary to conduct normalization on font sizes.",
                "For example, in one document the largest font size might be 12pt, while in another the smallest one might be 18pt.",
                "Boldface: This binary feature represents whether or not the current unit is in boldface.",
                "Alignment: There are four binary features that respectively represent the location of the current unit: left, center, right, and unknown alignment.",
                "The following format features with respect to context play an important role in title extraction.",
                "Empty Neighboring Unit: There are two binary features that represent, respectively, whether or not the previous unit and the current unit are blank lines.",
                "Font Size Change: There are two binary features that represent, respectively, whether or not the font size of the previous unit and the font size of the next unit differ from that of the current unit.",
                "Alignment Change: There are two binary features that represent, respectively, whether or not the alignment of the previous unit and the alignment of the next unit differ from that of the current one.",
                "Same Paragraph: There are two binary features that represent, respectively, whether or not the previous unit and the next unit are in the same paragraph as the current unit. 4.3.2 Linguistic Features The linguistic features are based on key words.",
                "Positive Word: This binary feature represents whether or not the current unit begins with one of the positive words.",
                "The positive words include title:, subject:, subject line: For example, in some documents the lines of titles and authors have the same formats.",
                "However, if lines begin with one of the positive words, then it is likely that they are title lines.",
                "Negative Word: This binary feature represents whether or not the current unit begins with one of the negative words.",
                "The negative words include To, By, created by, updated by, etc.",
                "There are more negative words than positive words.",
                "The above linguistic features are language dependent.",
                "Word Count: A title should not be too long.",
                "We heuristically create four intervals: [1, 2], [3, 6], [7, 9] and [9, ∞) and define one feature for each interval.",
                "If the number of words in a title falls into an interval, then the corresponding feature will be 1; otherwise 0.",
                "Ending Character: This feature represents whether the unit ends with :, -, or other special characters.",
                "A title usually does not end with such a character. 5.",
                "DOCUMENT RETRIEVAL METHOD We describe our method of document retrieval using extracted titles.",
                "Typically, in information retrieval a document is split into a number of fields including body, title, and anchor text.",
                "A ranking function in <br>search</br> can use different weights for different fields of 149 the document.",
                "Also, titles are typically assigned high weights, indicating that they are important for document retrieval.",
                "As explained previously, our experiment has shown that a significant number of documents actually have incorrect titles in the file properties, and thus in addition of using them we use the extracted titles as one more field of the document.",
                "By doing this, we attempt to improve the overall precision.",
                "In this paper, we employ a modification of BM25 that allows field weighting [21].",
                "As fields, we make use of body, title, extracted title and anchor.",
                "First, for each term in the query we count the term frequency in each field of the document; each field frequency is then weighted according to the corresponding weight parameter: ∑= f tfft tfwwtf Similarly, we compute the document length as a weighted sum of lengths of each field.",
                "Average document length in the corpus becomes the average of all weighted document lengths. ∑= f ff dlwwdl In our experiments we used 75.0,8.11 == bk .",
                "Weight for content was 1.0, title was 10.0, anchor was 10.0, and extracted title was 5.0. 6.",
                "EXPERIMENTAL RESULTS 6.1 Data Sets and Evaluation Measures We used two data sets in our experiments.",
                "First, we downloaded and randomly selected 5,000 Word documents and 5,000 PowerPoint documents from an intranet of Microsoft.",
                "We call it MS hereafter.",
                "Second, we downloaded and randomly selected 500 Word and 500 PowerPoint documents from the DotGov and DotCom domains on the internet, respectively.",
                "Figure 7 shows the distributions of the genres of the documents.",
                "We see that the documents are indeed general documents as we define them.",
                "Figure 7.",
                "Distributions of document genres.",
                "Third, a data set in Chinese was also downloaded from the internet.",
                "It includes 500 Word documents and 500 PowerPoint documents in Chinese.",
                "We manually labeled the titles of all the documents, on the basis of our specification.",
                "Not all the documents in the two data sets have titles.",
                "Table 1 shows the percentages of the documents having titles.",
                "We see that DotCom and DotGov have more PowerPoint documents with titles than MS.",
                "This might be because PowerPoint documents published on the internet are more formal than those on the intranet.",
                "Table 1.",
                "The portion of documents with titles Domain Type MS DotCom DotGov Word 75.7% 77.8% 75.6% PowerPoint 82.1% 93.4% 96.4% In our experiments, we conducted evaluations on title extraction in terms of precision, recall, and F-measure.",
                "The evaluation measures are defined as follows: Precision: P = A / ( A + B ) Recall: R = A / ( A + C ) F-measure: F1 = 2PR / ( P + R ) Here, A, B, C, and D are numbers of documents as those defined in Table 2.",
                "Table 2.",
                "Contingence table with regard to title extraction Is title Is not title Extracted A B Not extracted C D 6.2 Baselines We test the accuracies of the two baselines described in section 4.2.",
                "They are denoted as largest font size and first line respectively. 6.3 Accuracy of Titles in File Properties We investigate how many titles in the file properties of the documents are reliable.",
                "We view the titles annotated by humans as true titles and test how many titles in the file properties can approximately match with the true titles.",
                "We use Edit Distance to conduct the approximate match. (Approximate match is only used in this evaluation).",
                "This is because sometimes human annotated titles can be slightly different from the titles in file properties on the surface, e.g., contain extra spaces).",
                "Given string A and string B: if ( (D == 0) or ( D / ( La + Lb ) < θ ) ) then string A = string B D: Edit Distance between string A and string B La: length of string A Lb: length of string B θ: 0.1 ∑ × ++− + = t t n N wtf avwdl wdl bbk kwtf FBM )log( ))1(( )1( 25 1 1 150 Table 3.",
                "Accuracies of titles in file properties File Type Domain Precision Recall F1 MS 0.299 0.311 0.305 DotCom 0.210 0.214 0.212Word DotGov 0.182 0.177 0.180 MS 0.229 0.245 0.237 DotCom 0.185 0.186 0.186PowerPoint DotGov 0.180 0.182 0.181 6.4 Comparison with Baselines We conducted title extraction from the first data set (Word and PowerPoint in MS).",
                "As the model, we used Perceptron.",
                "We conduct 4-fold cross validation.",
                "Thus, all the results reported here are those averaged over 4 trials.",
                "Tables 4 and 5 show the results.",
                "We see that Perceptron significantly outperforms the baselines.",
                "In the evaluation, we use exact matching between the true titles annotated by humans and the extracted titles.",
                "Table 4.",
                "Accuracies of title extraction with Word Precision Recall F1 Model Perceptron 0.810 0.837 0.823 Largest font size 0.700 0.758 0.727 Baselines First line 0.707 0.767 0.736 Table 5.",
                "Accuracies of title extraction with PowerPoint Precision Recall F1 Model Perceptron 0.875 0. 895 0.885 Largest font size 0.844 0.887 0.865 Baselines First line 0.639 0.671 0.655 We see that the machine learning approach can achieve good performance in title extraction.",
                "For Word documents both precision and recall of the approach are 8 percent higher than those of the baselines.",
                "For PowerPoint both precision and recall of the approach are 2 percent higher than those of the baselines.",
                "We conduct significance tests.",
                "The results are shown in Table 6.",
                "Here, Largest denotes the baseline of using the largest font size, First denotes the baseline of using the first line.",
                "The results indicate that the improvements of machine learning over baselines are statistically significant (in the sense p-value < 0.05) Table 6.",
                "Sign test results Documents Type Sign test between p-value Perceptron vs. Largest 3.59e-26 Word Perceptron vs. First 7.12e-10 Perceptron vs. Largest 0.010 PowerPoint Perceptron vs. First 5.13e-40 We see, from the results, that the two baselines can work well for title extraction, suggesting that font size and position information are most useful features for title extraction.",
                "However, it is also obvious that using only these two features is not enough.",
                "There are cases in which all the lines have the same font size (i.e., the largest font size), or cases in which the lines with the largest font size only contain general descriptions like Confidential, White paper, etc.",
                "For those cases, the largest font size method cannot work well.",
                "For similar reasons, the first line method alone cannot work well, either.",
                "With the combination of different features (evidence in title judgment), Perceptron can outperform Largest and First.",
                "We investigate the performance of solely using linguistic features.",
                "We found that it does not work well.",
                "It seems that the format features play important roles and the linguistic features are supplements..",
                "Figure 8.",
                "An example Word document.",
                "Figure 9.",
                "An example PowerPoint document.",
                "We conducted an error analysis on the results of Perceptron.",
                "We found that the errors fell into three categories. (1) About one third of the errors were related to hard cases.",
                "In these documents, the layouts of the first pages were difficult to understand, even for humans.",
                "Figure 8 and 9 shows examples. (2) Nearly one fourth of the errors were from the documents which do not have true titles but only contain bullets.",
                "Since we conduct extraction from the top regions, it is difficult to get rid of these errors with the current approach. (3).",
                "Confusions between main titles and subtitles were another type of error.",
                "Since we only labeled the main titles as titles, the extractions of both titles were considered incorrect.",
                "This type of error does little harm to document processing like <br>search</br>, however. 6.5 Comparison between Models To compare the performance of different machine learning models, we conducted another experiment.",
                "Again, we perform 4-fold cross 151 validation on the first data set (MS).",
                "Table 7, 8 shows the results of all the four models.",
                "It turns out that Perceptron and PMM perform the best, followed by MEMM, and ME performs the worst.",
                "In general, the Markovian models perform better than or as well as their classifier counterparts.",
                "This seems to be because the Markovian models are trained globally, while the classifiers are trained locally.",
                "The Perceptron based models perform better than the ME based counterparts.",
                "This seems to be because the Perceptron based models are created to make better classifications, while ME models are constructed for better prediction.",
                "Table 7.",
                "Comparison between different learning models for title extraction with Word Model Precision Recall F1 Perceptron 0.810 0.837 0.823 MEMM 0.797 0.824 0.810 PMM 0.827 0.823 0.825 ME 0.801 0.621 0.699 Table 8.",
                "Comparison between different learning models for title extraction with PowerPoint Model Precision Recall F1 Perceptron 0.875 0. 895 0. 885 MEMM 0.841 0.861 0.851 PMM 0.873 0.896 0.885 ME 0.753 0.766 0.759 6.6 Domain Adaptation We apply the model trained with the first data set (MS) to the second data set (DotCom and DotGov).",
                "Tables 9-12 show the results.",
                "Table 9.",
                "Accuracies of title extraction with Word in DotGov Precision Recall F1 Model Perceptron 0.716 0.759 0.737 Largest font size 0.549 0.619 0.582Baselines First line 0.462 0.521 0.490 Table 10.",
                "Accuracies of title extraction with PowerPoint in DotGov Precision Recall F1 Model Perceptron 0.900 0.906 0.903 Largest font size 0.871 0.888 0.879Baselines First line 0.554 0.564 0.559 Table 11.",
                "Accuracies of title extraction with Word in DotCom Precisio n Recall F1 Model Perceptron 0.832 0.880 0.855 Largest font size 0.676 0.753 0.712Baselines First line 0.577 0.643 0.608 Table 12.",
                "Performance of PowerPoint document title extraction in DotCom Precisio n Recall F1 Model Perceptron 0.910 0.903 0.907 Largest font size 0.864 0.886 0.875Baselines First line 0.570 0.585 0.577 From the results, we see that the models can be adapted to different domains well.",
                "There is almost no drop in accuracy.",
                "The results indicate that the patterns of title formats exist across different domains, and it is possible to construct a domain independent model by mainly using formatting information. 6.7 Language Adaptation We apply the model trained with the data in English (MS) to the data set in Chinese.",
                "Tables 13-14 show the results.",
                "Table 13.",
                "Accuracies of title extraction with Word in Chinese Precision Recall F1 Model Perceptron 0.817 0.805 0.811 Largest font size 0.722 0.755 0.738Baselines First line 0.743 0.777 0.760 Table 14.",
                "Accuracies of title extraction with PowerPoint in Chinese Precision Recall F1 Model Perceptron 0.766 0.812 0.789 Largest font size 0.753 0.813 0.782Baselines First line 0.627 0.676 0.650 We see that the models can be adapted to a different language.",
                "There are only small drops in accuracy.",
                "Obviously, the linguistic features do not work for Chinese, but the effect of not using them is negligible.",
                "The results indicate that the patterns of title formats exist across different languages.",
                "From the domain adaptation and language adaptation results, we conclude that the use of formatting information is the key to a successful extraction from general documents. 6.8 <br>search</br> with Extracted Titles We performed experiments on using title extraction for document retrieval.",
                "As a baseline, we employed BM25 without using extracted titles.",
                "The ranking mechanism was as described in Section 5.",
                "The weights were heuristically set.",
                "We did not conduct optimization on the weights.",
                "The evaluation was conducted on a corpus of 1.3 M documents crawled from the intranet of Microsoft using 100 evaluation queries obtained from this intranets <br>search</br> engine query logs. 50 queries were from the most popular set, while 50 queries other were chosen randomly.",
                "Users were asked to provide judgments of the degree of document relevance from a scale of 1to 5 (1 meaning detrimental, 2 - bad, 3 - fair, 4 - good and 5 - excellent). 152 Figure 10 shows the results.",
                "In the chart two sets of precision results were obtained by either considering good or excellent documents as relevant (left 3 bars with relevance threshold 0.5), or by considering only excellent documents as relevant (right 3 bars with relevance threshold 1.0) 0 0.05 0.1 0.15 0.2 0.25 0.3 0.35 0.4 0.45 P@10 P@5 Reciprocal P@10 P@5 Reciprocal 0.5 1 BM25 Anchor, Title, Body BM25 Anchor, Title, Body, ExtractedTitle Name All RelevanceThreshold Data Description Figure 10.",
                "<br>search</br> ranking results.",
                "Figure 10 shows different document retrieval results with different ranking functions in terms of precision @10, precision @5 and reciprocal rank: • Blue bar - BM25 including the fields body, title (file property), and anchor text. • Purple bar - BM25 including the fields body, title (file property), anchor text, and extracted title.",
                "With the additional field of extracted title included in BM25 the precision @10 increased from 0.132 to 0.145, or by ~10%.",
                "Thus, it is safe to say that the use of extracted title can indeed improve the precision of document retrieval. 7.",
                "CONCLUSION In this paper, we have investigated the problem of automatically extracting titles from general documents.",
                "We have tried using a machine learning approach to address the problem.",
                "Previous work showed that the machine learning approach can work well for metadata extraction from research papers.",
                "In this paper, we showed that the approach can work for extraction from general documents as well.",
                "Our experimental results indicated that the machine learning approach can work significantly better than the baselines in title extraction from Office documents.",
                "Previous work on metadata extraction mainly used linguistic features in documents, while we mainly used formatting information.",
                "It appeared that using formatting information is a key for successfully conducting title extraction from general documents.",
                "We tried different machine learning models including Perceptron, Maximum Entropy, Maximum Entropy Markov Model, and Voted Perceptron.",
                "We found that the performance of the Perceptorn models was the best.",
                "We applied models constructed in one domain to another domain and applied models trained in one language to another language.",
                "We found that the accuracies did not drop substantially across different domains and across different languages, indicating that the models were generic.",
                "We also attempted to use the extracted titles in document retrieval.",
                "We observed a significant improvement in document ranking performance for <br>search</br> when using extracted title information.",
                "All the above investigations were not conducted in previous work, and through our investigations we verified the generality and the significance of the title extraction approach. 8.",
                "ACKNOWLEDGEMENTS We thank Chunyu Wei and Bojuan Zhao for their work on data annotation.",
                "We acknowledge Jinzhu Li for his assistance in conducting the experiments.",
                "We thank Ming Zhou, John Chen, Jun Xu, and the anonymous reviewers of JCDL05 for their valuable comments on this paper. 9.",
                "REFERENCES [1] Berger, A. L., Della Pietra, S. A., and Della Pietra, V. J.",
                "A maximum entropy approach to natural language processing.",
                "Computational Linguistics, 22:39-71, 1996. [2] Collins, M. Discriminative training methods for hidden markov models: theory and experiments with perceptron algorithms.",
                "In Proceedings of Conference on Empirical Methods in Natural Language Processing, 1-8, 2002. [3] Cortes, C. and Vapnik, V. Support-vector networks.",
                "Machine Learning, 20:273-297, 1995. [4] Chieu, H. L. and Ng, H. T. A maximum entropy approach to information extraction from semi-structured and free text.",
                "In Proceedings of the Eighteenth National Conference on Artificial Intelligence, 768-791, 2002. [5] Evans, D. K., Klavans, J. L., and McKeown, K. R. Columbia newsblaster: multilingual news summarization on the Web.",
                "In Proceedings of Human Language Technology conference / North American chapter of the Association for Computational Linguistics annual meeting, 1-4, 2004. [6] Ghahramani, Z. and Jordan, M. I. Factorial hidden markov models.",
                "Machine Learning, 29:245-273, 1997. [7] Gheel, J. and Anderson, T. Data and metadata for finding and reminding, In Proceedings of the 1999 International Conference on Information Visualization, 446-451,1999. [8] Giles, C. L., Petinot, Y., Teregowda P. B., Han, H., Lawrence, S., Rangaswamy, A., and Pal, N. eBizSearch: a niche <br>search</br> engine for e-Business.",
                "In Proceedings of the 26th Annual International ACM SIGIR Conference on Research and Development in Information Retrieval, 413414, 2003. [9] Giuffrida, G., Shek, E. C., and Yang, J. Knowledge-based metadata extraction from PostScript files.",
                "In Proceedings of the Fifth ACM Conference on Digital Libraries, 77-84, 2000. [10] Han, H., Giles, C. L., Manavoglu, E., Zha, H., Zhang, Z., and Fox, E. A.",
                "Automatic document metadata extraction using support vector machines.",
                "In Proceedings of the Third ACM/IEEE-CS Joint Conference on Digital Libraries, 37-48, 2003. [11] Kobayashi, M., and Takeda, K. Information retrieval on the Web.",
                "ACM Computing Surveys, 32:144-173, 2000. [12] Lafferty, J., McCallum, A., and Pereira, F. Conditional random fields: probabilistic models for segmenting and 153 labeling sequence data.",
                "In Proceedings of the Eighteenth International Conference on Machine Learning, 282-289, 2001. [13] Li, Y., Zaragoza, H., Herbrich, R., Shawe-Taylor J., and Kandola, J. S. The perceptron algorithm with uneven margins.",
                "In Proceedings of the Nineteenth International Conference on Machine Learning, 379-386, 2002. [14] Liddy, E. D., Sutton, S., Allen, E., Harwell, S., Corieri, S., Yilmazel, O., Ozgencil, N. E., Diekema, A., McCracken, N., and Silverstein, J.",
                "Automatic Metadata generation & evaluation.",
                "In Proceedings of the 25th Annual International ACM SIGIR Conference on Research and Development in Information Retrieval, 401-402, 2002. [15] Littlefield, A.",
                "Effective enterprise information retrieval across new content formats.",
                "In Proceedings of the Seventh <br>search</br> Engine Conference, http://www.infonortics.com/searchengines/sh02/02prog.html, 2002. [16] Mao, S., Kim, J. W., and Thoma, G. R. A dynamic feature generation system for automated metadata extraction in preservation of digital materials.",
                "In Proceedings of the First International Workshop on Document Image Analysis for Libraries, 225-232, 2004. [17] McCallum, A., Freitag, D., and Pereira, F. Maximum entropy markov models for information extraction and segmentation.",
                "In Proceedings of the Seventeenth International Conference on Machine Learning, 591-598, 2000. [18] Murphy, L. D. Digital document metadata in organizations: roles, analytical approaches, and future research directions.",
                "In Proceedings of the Thirty-First Annual Hawaii International Conference on System Sciences, 267-276, 1998. [19] Pinto, D., McCallum, A., Wei, X., and Croft, W. B.",
                "Table extraction using conditional random fields.",
                "In Proceedings of the 26th Annual International ACM SIGIR Conference on Research and Development in Information Retrieval, 235242, 2003. [20] Ratnaparkhi, A. Unsupervised statistical models for prepositional phrase attachment.",
                "In Proceedings of the Seventeenth International Conference on Computational Linguistics. 1079-1085, 1998. [21] Robertson, S., Zaragoza, H., and Taylor, M. Simple BM25 extension to multiple weighted fields, In Proceedings of ACM Thirteenth Conference on Information and Knowledge Management, 42-49, 2004. [22] Yi, J. and Sundaresan, N. Metadata based Web mining for relevance, In Proceedings of the 2000 International Symposium on Database Engineering & Applications, 113121, 2000. [23] Yilmazel, O., Finneran, C. M., and Liddy, E. D. MetaExtract: An NLP system to automatically assign metadata.",
                "In Proceedings of the 2004 Joint ACM/IEEE Conference on Digital Libraries, 241-242, 2004. [24] Zhang, J. and Dimitroff, A. Internet <br>search</br> engines response to metadata Dublin Core implementation.",
                "Journal of Information Science, 30:310-320, 2004. [25] Zhang, L., Pan, Y., and Zhang, T. Recognising and using named entities: focused named entity recognition using machine learning.",
                "In Proceedings of the 27th Annual International ACM SIGIR Conference on Research and Development in Information Retrieval, 281-288, 2004. [26] http://dublincore.org/groups/corporate/Seattle/ 154"
            ],
            "original_annotated_samples": [],
            "translated_annotated_samples": [
                "Además, podemos mejorar significativamente los resultados de clasificación de \"búsqueda\" en la recuperación de documentos mediante el uso de los títulos extraídos.",
                "Categorías y descriptores de sujetos H.3.3 [Almacenamiento y recuperación de información]: Información \"Búsqueda\" y recuperación - Proceso de \"búsqueda\";H.4.1 [Aplicaciones de sistemas de información]: Automatización de Office - Procesamiento de textos;D.2.8 [Ingeniería de software]: Métricas: medidas de complejidad, algoritmos de términos generales de medidas de rendimiento, experimentación, rendimiento.1.",
                "Los metadatos de introducción de documentos son útiles para muchos tipos de procesamiento de documentos, como \"búsqueda\", navegación y filtrado.",
                "En este documento, también investigamos los siguientes tres problemas, que no parecían haber sido examinados anteriormente.(1) Comparación entre modelos: entre los modelos anteriores, qué modelo funciona mejor para la extracción de título;(2) Generalidad del modelo: si es posible entrenar un modelo en un dominio y aplicarlo a otro dominio, y si es posible entrenar un modelo en un idioma y aplicarlo a otro idioma;(3) Utilidad de los títulos extraídos: si los títulos extraídos pueden mejorar el procesamiento de documentos como \"búsqueda\".",
                "La extracción de información se ha aplicado, por ejemplo, al etiquetado de parto de expresión [20], nombrado reconocimiento de entidad [25] y extracción de tabla [19].2.3 \"Búsqueda\" utilizando la información del título es útil para la recuperación de documentos.",
                "En el sistema CitaSeer, por ejemplo, Giles et al.logró extraer títulos de trabajos de investigación y hacer uso de los títulos extraídos en los metadatos \"búsqueda\" de documentos [8].",
                "En la \"búsqueda\" web, los campos de título (es decir, las propiedades del archivo) y los textos de anclaje de las páginas web (documentos HTML) pueden verse como títulos de las páginas [5].",
                "Muchos motores de \"búsqueda\" parecen utilizarlos para la recuperación de la página web [7, 11, 18, 22].",
                "Hasta donde sabemos, no se ha realizado investigaciones sobre el uso de títulos extraídos de documentos generales (por ejemplo, documentos de oficina) para \"búsqueda\" de los documentos.146 3.",
                "En otro experimento, encontramos que Google usa los títulos en las propiedades del archivo de los documentos de oficina en \"búsqueda\" y navegación, pero los títulos no son muy precisos."
            ],
            "translated_text": "",
            "candidates": [
                "buscar",
                "búsqueda",
                "Buscar",
                "Búsqueda",
                "búsqueda",
                "buscar",
                "búsqueda",
                "buscar",
                "búsqueda",
                "buscar",
                "Búsqueda",
                "buscar",
                "búsqueda",
                "buscar",
                "búsqueda",
                "buscar",
                "búsqueda",
                "Buscar",
                "búsqueda",
                "buscar",
                "búsqueda"
            ],
            "error": []
        }
    }
}